<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 1]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 1. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Thu, 12 Nov 2020 00:49:18 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-1.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Thu, 12 Nov 2020 00:49:17 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Show HN: Micro 3.0 Released – a platform for microservices development]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25044604">thread link</a>) | @asim
<br/>
November 10, 2020 | https://micro.mu/blog/2020/11/05/micro-v3-aka-m3o.html | <a href="https://web.archive.org/web/*/https://micro.mu/blog/2020/11/05/micro-v3-aka-m3o.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      
      
      

      
      
      

      <p>This is the official announcement for the release of Micro 3.0 better known as M3O - a platform for cloud native development. 
Our 3.0 release is a major refactor and consolidation of the existing tooling into something that addresses the entire workflow 
of build, run, manage and consume all from the developers perspective.</p>

<p>Read on to learn more or go straight to the <a href="https://github.com/micro/micro/releases/latest">latest release</a>. 
Head to <a href="https://m3o.com/">m3o.com</a> for the hosted offering.</p>

<h2 id="overview">Overview</h2>

<p>Micro focuses on developer productivity for the backend. It’s clear that the Cloud has become infinitely more complex 
over the past few years. Micro attempts to create order out of that chaos by distilling it all down to a handful of 
primitives for distributed systems development.</p>

<p>Why should you care? If you’re reading this you’ve no doubt encountered the tedious nature of infrastructure management, 
wrangling a kubernetes cluster on AWS or the thousands of things you need to do to cobble together a platform before 
starting to build a product. We think we’ve nailed the solution for that just as Android did for Mobile. Keep reading 
if you want to find out more.</p>

<h2 id="quick-flashback">Quick Flashback</h2>

<p>Micro started out as a <a href="https://micro.mu/blog/2016/03/20/micro.html">toolkit for microservices</a> development, 
incorporating an api gateway, web dashboard and cli to interact with services built using a Go RPC framework. 
Back then it felt like getting anyone to buy into PaaS again was going to be a losing battle. So we chose 
to write single purpose tools around an RPC framework thinking it might allow people to adopt it piece by piece 
until they saw the need for a platform. It was really straight forward right until it wasn’t.</p>

<p>There was a simple Go framework plus some surrounding 
components to query and interact with them, but like any long lived project, the complexity grew as we 
tried to solve for that platform experience that just couldn’t be done with a swiss army knife. The repo 
exploded with a number of independent libraries. To the creator its obvious what these are all for but to 
the user there is nothing but cognitive overload.</p>

<p>In 2019 we went through a <a href="https://micro.mu/blog/2019/06/10/the-great-consolidation.html">consolidation</a> of all those libraries 
which helped tremendously but there was still always one outstanding question. What’s the difference between 
<a href="https://github.com/micro/micro">micro</a> and <a href="https://github.com/micro/go-micro">go-micro</a>? It’s a good 
question and one we’ve covered before. We saw go-micro as a framework and micro as a toolkit but these 
words were basically empty and meaningless because multiple projects working in coordination really need a 
crisp story that makes sense and we didn’t have one.</p>

<p>In 2020 we’re looking to rectify that but let’s first let’s talk about platforms.</p>

<h2 id="paas-in-2020">PaaS in 2020</h2>

<p>5 years ago the world exploded with a proliferation of “cloud native” tooling as containers and 
container orchestration took centre stage. More specifically, Docker and Kubernetes redefined the 
technology landscape along with a more conscious move towards building software in the cloud.</p>

<p>Micro took a forward looking view even as far back as 2015. It was clear distributed systems and cloud native 
was going to become the dominant model for backend services development over the coming years but, what wasn’t clear 
is just how long we’d spend wrangling all sorts tools like docker, kubernetes, grpc, istio and everything else. 
It felt like we were rebuilding the stack and weren’t really ready to talk about development aspects of it all.</p>

<p>In fact at that time, people mostly wanted to kick the tyres on all these tools and piece something together. 
Running kubernetes yourself became all the rage and even using service mesh as the holy grail for solving 
all your distributed systems problems. Many of us have come to realise while all of this tech is fun 
it’s not actually solving development problems.</p>

<p>We’ve gotten to the point of managed kubernetes and even things like Google Cloud Run or DigitalOcean App 
Platform, but none of these things are helping with a development model for a cloud native era. Our 
frustrations with the existing developer experience have grown and Micro felt like something that 
could solve for all that, but only if we took a drastic step to overhaul it.</p>

<p>We think PaaS 3.0 is not just about running your container or even your source code but something that 
encapsulates the entire developer experience including a model for writing code for the cloud. Based on that 
Micro 3.0 aka M3O is a platform for cloud native development.</p>

<h2 id="what-even-is-cloud-native">What even is Cloud Native?</h2>

<p>What is cloud native? What does it mean to build for the cloud? What is a cloud service?</p>

<p>Cloud native is basically a descriptive term for something that was built to run in the cloud. That’s it. It’s not 
magic, it might sound like a buzzword, but the reality is it simply means, that piece of software was built 
to run in the cloud. How does that differ from the way we used to build before? Well the idea behind the cloud 
is that its ephemeral, scalable and everything can be accessed via an API.</p>

<p>Our expectation for services running in the cloud is that they’re mostly stateless, leveraging external services 
for the persistence, that they are identified by name rather than IP address and they themselves provide an 
API that can be consumed by multiple clients such as web, mobile and cli or other services.</p>

<p>Cloud native applications are horizontally scalable and operate within domain boundaries that divide them as 
separate apps which communicate over the network via their APIs rather than as one monolithic entity. 
We think cloud services require a fundamentally different approach to software creation and why Micro 3.0 
was designed with this in mind.</p>

<h2 id="micro-30-aka-m3o">Micro 3.0 aka M3O</h2>

<p>Micro 3.0 (M3O) reimagines Micro as a platform for cloud native development. What does that mean? Well we think of 
it as PaaS 3.0, a complete solution for source to running and beyond. Micro has moved from just being a Go 
framework to incorporating a standalone server and hosted platform. Our hosted offering is called 
<a href="https://m3o.com/">M3O</a>, a hat tip to Micro 3.0 or M[icr]o, whichever way you want to see it.</p>

<p>Another way to think about it. What Git is to GitHub, Micro is to the M3O platform. Let’s dig into it.</p>

<p>Micro 3.0 includes the following.</p>

<h3 id="server">Server</h3>

<p>The server is our abstraction for cloud infrastructure and underlying systems you might need for writing 
distributed systems. The server encapsulates all of these concerns as gRPC services which you can 
query via any language. The goal here is to say developers don’t really need to be thinking about infrastructure 
but what they do need is design patterns and primitives for building distributed systems.</p>

<p><img src="https://micro.mu/images/micro-3.0.png"></p>

<p>The server includes the following:</p>

<ul>
  <li>
    <p><strong>Authentication</strong>: Auth whether its authentication or authorization is part of the system. Create JWT tokens, define access rules, use one system to govern everything in a simple and straight forward manner. Whether it’s for a user or a service.</p>
  </li>
  <li>
    <p><strong>Configuration</strong>: Dynamic config management allows you to store relevant config that needs to be updated without having to restart services. Throw API keys and business logic related configuration into the secure config service and let your services pick up the changes.</p>
  </li>
  <li>
    <p><strong>Key-Value Storage</strong>: We’re focused on best practices for microservices development which means keeping services mostly stateless. To do this we’re providing persistent storage on the platform. Key-Value allows you to rapidly write code and store data in the format you care about.</p>
  </li>
  <li>
    <p><strong>Event Streaming</strong>: Distributed systems are fundamentally in need of an event driven architecture to breakdown the tight dependencies between them. Using event streaming and pubsub allows you to publish and subscribe to relevant events async.</p>
  </li>
  <li>
    <p><strong>Service Registry</strong>: Micro and M3O bake in service discovery so you can browse a directory of services to explore your service APIs and enable you to query services by name. Micro is all about microservices and multi-service development.</p>
  </li>
  <li>
    <p><strong>Service Network</strong>: Because you don’t want to have to resolve those service names to addresses and deal with the load balancing aspect, the server bakes in a “service mesh” which will handle your inter-service requests (as gRPC) and route to the 
appropriate instance.</p>
  </li>
  <li>
    <p><strong>Identity Proxy</strong>: We include a separate identity proxy for external requests using gRPC via the CLI and other means. This enables you to query from your local machine or anywhere else using valid auth credentials and have it seamlessly work as if 
you were in the platform itself.</p>
  </li>
  <li>
    <p><strong>API Gateway</strong>: Finally there’s an API gateway that automatically exposes your services to the outside world over HTTP. Internally writing service to service using gRPC makes sense, but at the end of the day we want to build APIs consumed from clients via HTTP.</p>
  </li>
</ul>

<h3 id="clients">Clients</h3>

<p>The server provides inter-service communication and two means of external communication with a HTTP API and gRPC proxy but that 
experience is made much better when there’s user experience on the client side that works. Right now we’ve got two ways of doing this.</p>

<ul>
  <li>
    <p><strong>Command Line</strong>: The CLI provides a convenient and simple way to talk to the server via gRPC requests through the proxy. 
The most convenient commands are builtin but every service you write also gets beautiful dynamic generated commands 
for each endpoint.</p>
  </li>
  <li>
    <p><strong>gRPC SDKs</strong>: Every service in the server is accessible via gRPC. We’re code generating clients for the server itself 
so you can access them from any language. What this enables is a wide array of experiences on the client side without 
having to handcraft libraries for each language.</p>
  </li>
  <li>
    <p><strong>Web Interface</strong>: Coming soon is a dynamically generated web interface that creates a simple query mechanism through a 
browser for any of your services. We’ve got a http api, gRPC proxy and command line interface but feel like the browser 
could use some love too.</p>
  </li>
</ul>

<h3 id="framework">Framework</h3>

<p>One thing we really understood from our time working on go-micro was that the developer experience …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://micro.mu/blog/2020/11/05/micro-v3-aka-m3o.html">https://micro.mu/blog/2020/11/05/micro-v3-aka-m3o.html</a></em></p>]]>
            </description>
            <link>https://micro.mu/blog/2020/11/05/micro-v3-aka-m3o.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25044604</guid>
            <pubDate>Tue, 10 Nov 2020 11:00:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The myriad meanings of pwd in Unix systems]]>
            </title>
            <description>
<![CDATA[
Score 17 | Comments 10 (<a href="https://news.ycombinator.com/item?id=25044131">thread link</a>) | @quyleanh
<br/>
November 10, 2020 | https://qmacro.org/2020/11/08/the-meaning-of-pwd-in-unix-systems/ | <a href="https://web.archive.org/web/*/https://qmacro.org/2020/11/08/the-meaning-of-pwd-in-unix-systems/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><em>Last week I ran a poll on Twitter to see what people considered with respect to the meaning of ‘pwd’ in Unix and Linux systems. The results were varied, for perhaps good reason.</em></p>

<p>At the end of Oct 2020 I ran a <a href="https://twitter.com/qmacro/status/1322567992551624705">brief poll on Twitter</a>, on which 82 people voted. Here’s that poll, and the results. They’re quite mixed, which at first might seem surprising. But there are reasons for that, as we’ll find out.</p>

<p><img src="https://qmacro.org/content/images/2020/11/twitter-poll-pwd.png" alt="Poll on Twitter: &quot;Fun Saturday afternoon shell poll. In Unix (and Linux), what do you think the P in $PWD (or pwd) stand for?&quot;"></p>

<p><strong>Print working directory</strong></p>

<p>The most popular option was “print working directory”. At first sight it seems logical: “print out the current working directory, i.e. where I am right now”. Moreover, the description in various versions of the manual for <code>pwd</code> help to drive home that notion. Typically we see sentences like “<a href="https://linux.die.net/man/1/pwd">print name of current/working directory</a>” or “<a href="https://www.mankier.com/1/pwd">print the current directory</a>”.</p>

<p>But there are lots of commands that print stuff, and are described in that way too. Take the <code>id</code> command. Here’s what one man page says: “<a href="https://man7.org/linux/man-pages/man1/id.1.html">print real and effective user and group IDs</a>”. There’s “print” again. But the command isn’t <code>pid</code>, it’s <code>id</code>. When you think about it, many, many commands in Unix send information to STDOUT, i.e. to the terminal. That’s sort of the point of many of them.</p>

<p>This time arguably only superficially definitive, it would seem, the Wikipedia entry states, on the <a href="https://en.wikipedia.org/wiki/Pwd">page for <code>pwd</code></a>: “the pwd command (print working directory) writes the full pathname of the current working directory to the standard output”. As if to underline the hopeful authority of this statement, there are five (!) footnotes that supposedly link to resources that back this up.</p>

<p>Unfortunately, the first footnote points to a Wayback Machine copy of the <a href="https://web.archive.org/web/20050520231659/http://cm.bell-labs.com/7thEdMan/v7vol1.pdf">UNIX PROGRAMMERS MANUAL - Seventh Edition, Volume 1 - January, 1979</a>, wherein there are actually zero references to <code>pwd</code> being short for “print working directory”:</p>

<p><img src="https://qmacro.org/content/images/2020/11/programmers-manual-pwd.png" alt="excerpt from UNIX PROGRAMMERS MANUAL on pwd"></p>

<p>I don’t know about you, but this historic document carries more weight for me than other sources I’ve come across, and it only serves here to undermine the credibility of the Wikipedia entry.</p>

<p>The rest of the footnote links seem dubious at best, except for the one pointing to the <a href="https://www.gnu.org/software/coreutils/manual/coreutils.html#pwd-invocation">GNU Coreutils manual on pwd</a> which has it as “print working directory”. But everything else I’ve seen so far makes me think that this is a misunderstanding that has spread for obvious and innocent reasons. In addition, the one footnote in the Wikipedia page that is not used to back this claim up is a pointer to <a href="https://pubs.opengroup.org/onlinepubs/9699919799/utilities/pwd.html">The Open Group Base Specifications Issue 7, 2018 edition’s information on pwd</a>, which almost seems like it’s actually avoiding using the word “print” at all: “return working directory name” … “The pwd utility shall write to standard output an absolute pathname of the current working directory, which does not contain the filenames dot or dot-dot.”. Very specific, very not-print.</p>

<p>So I’m thinking that “print working directory” isn’t what <code>pwd</code> stands for. In fact, “print working directory” may be common to some man pages, but on this macOS machine, with its <a href="https://en.wikipedia.org/wiki/Berkeley_Software_Distribution">BSD</a> heritage, we have, instead: “pwd – return working directory name”. Moreover, it goes on to say “The pwd utility writes the absolute pathname of the current working directory to the standard output”.</p>

<p><strong>Pathname of working directory</strong></p>

<p>So perhaps it really is “pathname of working directory”. That would, at least to me, make more sense. Not only does it eschew the redundancy of “print”, it also is more specific about the output - if I’m in <code>/home/dja/</code> for example, then invoking pwd will tell me that, i.e. where I am, including the whole path, and not just <code>dja</code>:</p>



<p><strong>Process working directory</strong></p>

<p>As for the other options, I do favour “process working directory”, mostly because it makes a lot of sense to me; every process in Unix has the concept of a current working directory, and that’s exactly what I’m asking for when I’m in my shell process and enter <code>pwd</code> - there’s a part in the video <a href="https://youtu.be/hgFBRZmwpSM?t=165">Unix terminals and shells</a> that explains this very well.</p>

<p>I’d love to be able to point to some old Unix sources that definitively explain the answer, but unfortunately that search has come up with very little - the <code>pwd</code> source in both the <a href="https://minnie.tuhs.org/cgi-bin/utree.pl?file=V5/usr/source/s2/pwd.c">5th</a> and <a href="https://github.com/yisooan/unix-v6/blob/master/source/s2/pwd.c">6th</a> Editions of Unix shed no light on this whatsoever.</p>

<p><strong>Present working directory</strong></p>

<p>What about “present working directory”? Well, that option seems to have legs, in the form of the Korn shell. While <a href="https://northstar-www.dartmouth.edu/doc/solaris-forte/ipe-help/dbx/dbx88cc.html">one source</a> implies that the answer might well be “pathname of current working directory”, in that <code>pwd</code> just emits the value of the <code>$PWD</code> environment variable (and a variable called “print working directory” makes no sense at all) … it would seem that in ksh-land, at least, “present working directory” is what <code>pwd</code> represents. Take, for example, the <a href="https://osr507doc.xinuos.com/en/man/html.C/ksh.C.html">ksh man page</a> which states “PWD - The present working directory set by the cd command”.</p>

<p>There’s a ton of discussion, both direct and indirect, on this very question. Take for example these two entries in the Unix &amp; Linux Stack Exchange forum: <a href="https://unix.stackexchange.com/questions/399026/etymology-of-pwd">Etymology of $PWD</a> and <a href="https://unix.stackexchange.com/questions/174990/what-is-pwd-vs-current-working-directory">What is $PWD? (vs current working directory)</a>. Of course, perhaps the definitive answer will never be found, as computing history is nothing if not varied and prone to forking.</p>

<p><strong>Multics and print_wdir</strong></p>

<p>Talking of history, we could go further back to pre-Unix roots, in the form of Multics, which indirectly gave rise to Unix (originally “Unics”). In the <a href="https://multicians.org/multics-commands.html">list of Multics Commands</a>, we see, nestled amongst other similarly named commands, something that jumps out at us:</p>

<div><div><pre><code>print_mail (pm)	display mail in a mailbox
print_messages (pm)	display interactive messages in a mailbox
print_motd (pmotd)	display message of the day (source)
print_proc_auth (ppa)	display process's sensitivity level and compartments
print_request_types (prt)	display list of I/O daemon request types
print_search_paths (psp)	display search paths
print_search_rules (psr)	display ready messages
print_wdir (pwd)	display working directory
</code></pre></div></div>

<p>There’s <code>pwd</code>, and in fact, just like its sibling <code>pmotd</code>, for example, which is short for <code>print_motd</code>, it’s short for <code>print_wdir</code>. Now, given the context of the original poll being set to Unix and Linux, perhaps we must discount this information. But as someone who is fascinated with Unix history in general - how can I?</p>

<p>I guess there are few things to conclude. The history is rich and diverse, and maybe we’ll never know for sure. Perhaps, in fact, the answer will depend on whom we ask. In the grand scheme of things, it doesn’t really matter … but to those who delight in minutiae, it’s a fun topic worth exploring.</p>

  </div>

</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://qmacro.org/2020/11/08/the-meaning-of-pwd-in-unix-systems/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25044131</guid>
            <pubDate>Tue, 10 Nov 2020 09:30:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[More SaaS and less visibility = offboarding mess]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25044123">thread link</a>) | @andrazrp
<br/>
November 10, 2020 | https://www.cleanshelf.com/resources/complete-offboarding-checklist/ | <a href="https://web.archive.org/web/*/https://www.cleanshelf.com/resources/complete-offboarding-checklist/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-rpi-area=""><p><strong>Employee offboarding is back in the spotlight this year as workforce disruption, layoffs, and resignations unexpectedly hit. IT and HR leaders are swamped transitioning stay-at-home workers and managers. The amount of SaaS these workers use – some sanctioned and some not – adds complexity. Secure offboarding is critical. But companies can’t deprovision what they can’t see.</strong></p><p>Scroll down to preserve data security with our complete offboarding checklist and bonus offboarding insights.</p><p>Or you can hit one of the options below to jump straight to the section:</p><ul><li><a href="#offboarding-risks" "="">Employee offboarding risks</a></li><li><a href="#offboarding-checklist">Employee offboarding checklist</a></li><li><a href="#it-checklist">IT checklist for employee offboarding</a></li><li><a href="#bonus-insights">Bonus insights</a></li></ul><h2 id="offboarding-risks">Employee offboarding risks</h2><p>Information Week shares a sobering stat: <a href="https://www.darkreading.com/vulnerabilities---threats/50--of-ex-employees-can-still-access-corporate-apps/d/d-id/1329672" target="_blank" rel="noopener noreferrer">50% of ex-employees can still access corporate cloud applications</a>. Their findings, based on a study of five hundred IT decision-makers, indicate that few firms have adequate provisioning, deprovisioning, termination, and login management processes in place.</p><p>Notably, 20% of respondents report that “their failure to deprovision employees from corporate applications has contributed to a data breach at their organization.”</p><p>It is now common for companies to deprovision 20-30 licenses per employee, versus the usual four to five in years past, according to the report. But the reality might be worse.</p><p>Cleanshelf’s annual <a href="https://www.cleanshelf.com/resources/business-saas-spend-up-to-30-wasted-every-day/">State of Business SaaS Spend</a> report found that in 2019, the <strong>typical employee at a U.S. enterprise with 800 or more employees used 44 cloud applications</strong>. The companies themselves use licenses from 140 vendors on average. Because so few companies know what they actually own, up to $4 million (or nearly 30% of spend) is wasted yearly.</p><figure id="attachment_2116" aria-describedby="caption-attachment-2116"><a href="https://www.cleanshelf.com/resources/the-state-of-business-saas-spend-2019/"><img src="https://1t7st7202zsklkcbf3yj41y6-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/30-percent-SaaS-Spend-Waste-in-2019-Shadow-2.jpg" alt="Enterprise SaaS Management platform Cleanshelf found that 30 percent of SaaS spend is wasted which poses one more reason to use it for IT termination." width="1368" height="866" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201368%20866'%3E%3C/svg%3E" data-lazy-src="https://1t7st7202zsklkcbf3yj41y6-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/30-percent-SaaS-Spend-Waste-in-2019-Shadow-2.jpg"></a><figcaption id="caption-attachment-2116">Cleanshelf found that enterprises wasted almost 30% of all SaaS spend in 2019.</figcaption></figure><p>The implications of the unawareness is startling. Companies clearly underestimate the <a href="https://www.cleanshelf.com/resources/3-saas-risks-to-look-out-for-in-2020/">SaaS risks</a> and the amount of SaaS used by their employees. <strong>If IT doesn’t know what apps employees are using, how can they turn access off when staff turns over? They can’t.</strong></p><p>A recent <a href="https://gurucul.com/news/1-in-10-tech-employees-plan-to-steal-company-information-before-leaving-a-job" target="_blank" rel="noopener noreferrer">study</a> from Gurucul found 1 in 10 would take as much corporate information with them as possible when they left, while another 15% said they would delete files or change passwords.</p><p>Terminated employee access has financial, legal, and competitive implications.</p><ul><li>An ex-salesperson could access forecasts, contracts, and prospect lists.</li><li>Former operational or financial staff may have access to non-public revenue and performance data.</li><li>Engineers may maintain unauthorized access to code.</li></ul><h3>More SaaS + less visibility = offboarding mess</h3><p>The most complete offboarding workflow is still fragile when IT can’t see what cloud apps employees use. The process of identifying and deprovisioning should take seconds; not hours or days of accessing logs, verifying credit card records, and cross-referencing HR and vendor files to establish who has what. In many cases, these don’t reconcile anyway.</p><p>It’s now common – especially given today’s stay-at-home phenomena – for workers to buy SaaS on their personal cards and seek reimbursement. And beyond the data and compliance benefits, once companies have a full view of their licenses, they can thoughtfully re-deploy unused licenses elsewhere.</p><figure id="attachment_2116" aria-describedby="caption-attachment-2116"><a href="https://www.cleanshelf.com/product"><img src="https://1t7st7202zsklkcbf3yj41y6-wpengine.netdna-ssl.com/wp-content/uploads/2020/06/cleanshelf-app-inventory.jpg" alt="Dashboard of a Cleanshelf's demo account showing app inventory" width="1368" height="866" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201368%20866'%3E%3C/svg%3E" data-lazy-src="https://1t7st7202zsklkcbf3yj41y6-wpengine.netdna-ssl.com/wp-content/uploads/2020/06/cleanshelf-app-inventory.jpg"></a><figcaption id="caption-attachment-2116">With Cleanshelf, companies can have a full view of their licenses so they can thoughtfully re-deploy unused licenses elsewhere.</figcaption></figure><h2 id="offboarding-checklist">Employee offboarding checklist</h2><p>Establish a simple, repeatable, employee offboarding process with a focus on communication and security for the reasons mentioned above.</p><p>The offboarding process is divided into two sections. Steps from 1 and up to 4 are made to check off the general formalities, while step 5 is made for the IT department that will prevent potential security issues.</p><blockquote><p>“The off-boarding process is a step in the system, access, and data lifecycle process. The off-boarding process can be no more secure than the onboarding and control processes. If you properly authorize access to inventory, data, and authentication systems, you can make sure that access is removed.” by Timothy Fawcett, <a href="http://guernsey.us/cybersecurity" target="_blank" rel="noopener noreferrer">Guernsey</a> (Follow him on <a href="https://www.linkedin.com/in/timothy-fawcett/" target="_blank" rel="noopener noreferrer">LinkedIn</a>)</p></blockquote><h3><span>1.</span> Communicate employee’s exit</h3><p>Everyone in the team, their managers, and members should be notified of the employee's exit. This will give them the opportunity to thank the employee for their work.<br> If the employee was in a customer-facing position, let them know of the employee’s departure and who will take over the account.</p><h3><span>2.</span> Resolve paperwork</h3><p>Besides filing the employee’s termination or resignation letter, also file non-disclosure or non-compete agreements.</p><h3><span>3.</span> Finish payroll process</h3><p>Communicate with the payroll department to complete their final pay. Settle other requirements for Benefits, 401K, PTO Balance, Insurance, and Tax related forms.</p><h3><span>4.</span> Exit interview</h3><p>Conducting an open interview to receive honest feedback on what works and what not will eventually help you improve your work environment, leadership, and organization in general.</p><p>As you finish the formalities above it’s time to check-off the IT offboarding points to preserve data security in your enterprise.</p><h3 id="it-checklist"><span>5.</span> IT checklist for employee offboarding</h3><p><strong>Access and inventory control is a process, where the entire lifecycle is important for completeness.</strong></p><p>Below we compiled nine basic steps from the IT perspective where some will depend on the employee and what kind of data they were able to access.</p><p>We recommend full deprovisioning (Step 3) that starts with verifying every app an employee uses. Make sure to review license usage once accounts are deprovisioned (Step 9). This ensures consistent cost management.</p><ol><li><strong>Reclaim all assigned equipment</strong><br> Make someone responsible. If you’re not responsible for recovering the assets then make someone else do it. This way you can follow up and keep track of the assets as they return. Revoke all devices that are associated with company apps and not just a company laptop.</li><li><strong>Revoke access</strong><br> Revoke access to Identity Provider, Single-Sign-On and Production systems, and all internal user accounts to which the employee had access.</li><li><strong>Unassign SaaS or software licenses</strong> <span>(cross-reference in SaaS management tool)</span><br> Verify every app an employee used and deprovision accordingly. Pay attention to <a href="https://www.cleanshelf.com/resources/enterprise-saas-visibility-and-discovery/">shadow IT</a> as it is most often a missed step especially when offboarding employees. If you don’t control your data, you may not be aware of what access to remove.</li><li><strong>Perform a backup of user’s data</strong><br> Perform a complete backup of the user’s data. If you already have it saved and archived as part of your routine manage it according to your internal compliance policy.</li><li><strong>Take forensic images of computing devices</strong><br> High-risk organizations should take forensic images of computing devices, PCs, Phone, Email, etc. for any employee that is terminated for cause or has access to IP including customer lists. Having this data may prove essential if the employee disputes their termination.</li><li><strong>Disable access to email account</strong><br> Create an out-of-office message for email or forward employee’s emails to the superior or person responsible. Remove the email address from generic distribution lists and manage them according to your internal policy.</li><li><strong>Change passwords</strong><br> If there are any passwords that were shared between employees in the organization or department itself, require staff to change those to remove the potential risk of any unauthorized access.</li><li><strong>Update Credit Card payments</strong><br> If an employee leaving had a company credit card, make sure to uncover which SaaS services were paid with it so you don’t get any disturbances in services that would potentially mean loss of data or employee productivity.</li><li><strong>Optimize licensing plans</strong><br> As you offboard employees, the number of users that were initially planned and signed to use SaaS in contracts is now changed. Make sure to optimize your licensing plans when possible (<a href="https://www.cleanshelf.com/resources/saas-negotiations-and-how-understanding-pricing-will-help-you/">negotiate</a>, if needed).</li></ol><figure id="attachment_2116" aria-describedby="caption-attachment-2116"><a href="https://www.cleanshelf.com/product"><img src="https://1t7st7202zsklkcbf3yj41y6-wpengine.netdna-ssl.com/wp-content/uploads/2020/06/cleanshelf-reduce-licenses.jpg" alt="Cleanshel's demo account shows Salesforce CRM SaaS spend by department and it's utilization based on the usage." width="1368" height="866" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201368%20866'%3E%3C/svg%3E" data-lazy-src="https://1t7st7202zsklkcbf3yj41y6-wpengine.netdna-ssl.com/wp-content/uploads/2020/06/cleanshelf-reduce-licenses.jpg"></a><figcaption id="caption-attachment-2116">Uncover the SaaS service utilization and optimize licensing plans accordingly.</figcaption></figure><h2 id="bonus-insights">Bonus insights for effective employee offboarding</h2><p>Our team has helped dozens of companies manage the software side of offboarding. Once you complete the initial IT termination checklist here are few additional insights to improve the experience:</p><ul><li><strong>Be notified in advance</strong><br> IT departments should be <a href="https://securityboulevard.com/2019/07/assessing-the-risk-of-the-former-employee/" target="_blank" rel="noopener noreferrer">notified in advance</a>, when applicable, of an employee termination. This allows preparation for backup, restoration and preservation of any corporate data the employee was responsible for managing.</li><li><strong>Don’t close only tools with active accounts</strong><br> Don’t only close out SaaS tools for which the employee has an active account. Review the tools they’ve used in the past. Long-running sessions or API access tokens may still be in place.</li><li><strong>Pay attention to remote access software</strong><br> Double check that no remote access software remains on company workstations. These ‘backdoors’ – through logging into a VPN or using a remote desktop – are common given remote IT support trends.</li><li><strong>Reassess SaaS contracts</strong><br> Reassess licensing models for popular cloud applications after staff departures. This is most relevant in cases where layoffs hit a department that uses speciality software. For example, if the marketing team loses staff, move off an enterprise license agreement (ELA) and <a href="https://www.cleanshelf.com/resources/five-ideas-for-successful-saas-vendor-negotiation/">negotiate SaaS contract</a> to pay per license for applications like Marketo, Intercom, Buffer, or Mailchimp.</li><li><strong>Double check vulnerable areas</strong><br> Focus on vulnerable areas. Unwinding cloud office suite access (for G-Suite and Office 365) seems simple but may trap even vigilant admins. Double-check: are IMAP/POP sync settings disabled? As well, verify that no automatic email forwarded remains. Also, delete or reassign email aliases. For example, your customer support employee may have received an email at their “name@company.com” domain, while <i>additionally</i> receiving email from “support@company.com” too.</li><li><strong>Review integration logs</strong><br> Review integration logs of business essential tools. A tool like Slack or Zapier may have integrations dependent on a user’s account staying active. First, understand what applications a former employee used, then assign changes to reduce the likelihood of disruption.</li></ul><p>The stakes for effective …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.cleanshelf.com/resources/complete-offboarding-checklist/">https://www.cleanshelf.com/resources/complete-offboarding-checklist/</a></em></p>]]>
            </description>
            <link>https://www.cleanshelf.com/resources/complete-offboarding-checklist/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25044123</guid>
            <pubDate>Tue, 10 Nov 2020 09:28:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On Automation Part 4: Who made it, why, and in what context?]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25044097">thread link</a>) | @nonoesp
<br/>
November 10, 2020 | https://sketch.nono.ma/who-made-it-why-and-in-what-context | <a href="https://web.archive.org/web/*/https://sketch.nono.ma/who-made-it-why-and-in-what-context">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

    <div>

    
          <svg data-name="sketch.nono.ma" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 600 153"><defs></defs><title>Sketch.Nono.MA</title><path d="M182.51,57.42v1.29a8.6,8.6,0,0,0,.48,3.47,1.85,1.85,0,0,0,1.82,1c1.34,0,2-1.05,2-3.14a8,8,0,0,0-.12-1.42,6.27,6.27,0,0,0-.44-1.38,11.32,11.32,0,0,0-.84-1.52c-.36-.54-.81-1.17-1.36-1.9l-2.22-3.23c-.71-1-1.3-1.92-1.77-2.72a18.44,18.44,0,0,1-1.13-2.28,10,10,0,0,1-.61-2,11.93,11.93,0,0,1-.17-2,6.9,6.9,0,0,1,1.79-5,6.43,6.43,0,0,1,4.84-1.85,6,6,0,0,1,3.86,1.23,5.77,5.77,0,0,1,2,3.43c.06.28.11.53.14.74a6.16,6.16,0,0,1,.07.69c0,.25,0,.57,0,1v1.57l-4.4.43c0-.83,0-1.42,0-1.79a6.37,6.37,0,0,0-.12-1.07c-.16-1.33-.78-2-1.86-2-1.24,0-1.86,1-1.86,3a9.55,9.55,0,0,0,.07,1.26,4.32,4.32,0,0,0,.39,1.21,16.05,16.05,0,0,0,1,1.66l1.77,2.66,2.27,3.23a21.45,21.45,0,0,1,2.4,4.58,12.66,12.66,0,0,1,.84,4.34,6.61,6.61,0,0,1-1.7,4.89,6.53,6.53,0,0,1-4.85,1.71,6.34,6.34,0,0,1-6.55-4.73,14.33,14.33,0,0,1-.29-3.26v-.83a8.51,8.51,0,0,1,.05-.88Z"></path><path d="M211.28,66.84h-4.69l-3.33-15L201.32,57v9.8h-4.54V35.11h4.54V47.05l4.4-11.94h4.6l-4.07,10.71Z"></path><path d="M226.51,35.11V39.3h-6.38v8.8h4.45v4.18h-4.45V62.65h6.77v4.19H215.59V35.11Z"></path><path d="M234.59,39.3h-4.21V35.11h13.15V39.3h-4.4V66.84h-4.54Z"></path><path d="M259.92,55.52c0,.85.06,1.51.08,2s0,.91,0,1.36q0,8.46-6.53,8.46-6.33,0-6.33-7.61V42.25q0-7.61,6.33-7.61Q260,34.64,260,43V44a13.8,13.8,0,0,1-.1,1.45h-4.54a12,12,0,0,0,.09-1.21v-1a10.26,10.26,0,0,0-.4-3.54,1.54,1.54,0,0,0-1.58-.93,1.44,1.44,0,0,0-1.43.69,6.93,6.93,0,0,0-.36,2.77V59.67a6.93,6.93,0,0,0,.36,2.77,1.44,1.44,0,0,0,1.43.69,1.56,1.56,0,0,0,1.56-.88,9,9,0,0,0,.42-3.36c0-.53,0-1,0-1.49s0-1.09-.07-1.88Z"></path><path d="M270.13,52.28V66.84h-4.55V35.07h4.55v13h3.57v-13h4.55V66.84H273.7V52.28Z"></path><path d="M283.91,62.08h4.54v4.76h-4.54Z"></path><path d="M309.34,66.84h-4.55l-5.22-16.65V66.84H295V35.11h3.85l5.91,18.7V35.11h4.55Z"></path><path d="M328.34,58.75c0,1,0,1.73,0,2.33s-.08,1.12-.14,1.55a4.63,4.63,0,0,1-.29,1.09,7.46,7.46,0,0,1-.53,1,5.56,5.56,0,0,1-2.25,1.92,7.4,7.4,0,0,1-6.24,0,5.56,5.56,0,0,1-2.25-1.92,7.46,7.46,0,0,1-.53-1,5.11,5.11,0,0,1-.31-1.09,10.43,10.43,0,0,1-.15-1.55c0-.6,0-1.38,0-2.33V43.15c0-.95,0-1.73,0-2.33a10.28,10.28,0,0,1,.15-1.54,5.21,5.21,0,0,1,.31-1.1,7.39,7.39,0,0,1,.53-1,5.63,5.63,0,0,1,2.25-1.88,7.4,7.4,0,0,1,6.24,0,5.63,5.63,0,0,1,2.25,1.88,7.39,7.39,0,0,1,.53,1,4.72,4.72,0,0,1,.29,1.1c.06.42.11.94.14,1.54s0,1.38,0,2.33ZM323.8,41.38a3.47,3.47,0,0,0-.43-2,1.6,1.6,0,0,0-1.41-.6,1.62,1.62,0,0,0-1.39.6,3.29,3.29,0,0,0-.45,2V60.57a3.51,3.51,0,0,0,.42,2,2,2,0,0,0,2.81,0,3.33,3.33,0,0,0,.45-2Z"></path><path d="M348.84,66.84H344.3l-5.23-16.65V66.84h-4.54V35.11h3.85l5.92,18.7V35.11h4.54Z"></path><path d="M367.84,58.75c0,1,0,1.73,0,2.33s-.08,1.12-.14,1.55a4.63,4.63,0,0,1-.29,1.09,7.46,7.46,0,0,1-.53,1,5.56,5.56,0,0,1-2.25,1.92,7.4,7.4,0,0,1-6.24,0,5.56,5.56,0,0,1-2.25-1.92,7.46,7.46,0,0,1-.53-1,5.11,5.11,0,0,1-.31-1.09,10.43,10.43,0,0,1-.15-1.55c0-.6,0-1.38,0-2.33V43.15c0-.95,0-1.73,0-2.33a10.28,10.28,0,0,1,.15-1.54,5.21,5.21,0,0,1,.31-1.1,7.39,7.39,0,0,1,.53-1,5.63,5.63,0,0,1,2.25-1.88,7.4,7.4,0,0,1,6.24,0,5.63,5.63,0,0,1,2.25,1.88,7.39,7.39,0,0,1,.53,1,4.72,4.72,0,0,1,.29,1.1c.06.42.11.94.14,1.54s0,1.38,0,2.33ZM363.3,41.38a3.38,3.38,0,0,0-.43-2,1.6,1.6,0,0,0-1.41-.6,1.62,1.62,0,0,0-1.39.6,3.29,3.29,0,0,0-.45,2V60.57a3.42,3.42,0,0,0,.43,2,1.62,1.62,0,0,0,1.41.59,1.64,1.64,0,0,0,1.39-.59,3.33,3.33,0,0,0,.45-2Z"></path><path d="M374.37,62.08h4.55v4.76h-4.55Z"></path><path d="M397.19,35.11h5.42V66.84h-3.87V44.44L395,66.84H393l-3.77-22.4v22.4h-3.77V35.11h5.32L394,50.9Z"></path><path d="M412.62,66.84h-4.36l4.4-31.73h5.81l4.2,31.73h-4.4l-.82-7.14h-4.06Zm2.8-26-1.55,14.7H417Z"></path><path d="M96.84,97.69c-1.56,0-2.5.76-2.5,1.8s1.21,1.63,2.34,1.9l1.3.32c2.08.5,4,1.59,4.06,4s-1.93,4.09-5.24,4.09-5.26-1.54-5.36-4.29H93.9c.1,1.45,1.31,2.15,2.88,2.15s2.75-.79,2.76-2-1-1.53-2.48-1.91l-1.58-.41c-2.27-.58-3.68-1.72-3.68-3.71,0-2.44,2.17-4.07,5.07-4.07s4.93,1.65,5,4H99.44C99.32,98.38,98.33,97.69,96.84,97.69Z"></path><path d="M103.78,95.76h2.44v7.61h.17l3.73-4.16H113l-4,4.47,4.25,5.89h-2.93l-3.16-4.43-.9,1v3.48h-2.44Z"></path><path d="M113.33,104.45c0-3.19,1.94-5.37,4.91-5.37,2.55,0,4.73,1.6,4.73,5.23v.75h-7.21a2.55,2.55,0,0,0,2.64,2.81,2.16,2.16,0,0,0,2.19-1.33l2.28.25c-.43,1.8-2.09,3-4.5,3C115.24,109.77,113.33,107.7,113.33,104.45Zm7.3-1a2.3,2.3,0,0,0-2.36-2.43,2.5,2.5,0,0,0-2.51,2.43Z"></path><path d="M129.83,101.1h-2v5.36c0,1,.49,1.2,1.11,1.2a3.12,3.12,0,0,0,.71-.1l.41,1.91a4.8,4.8,0,0,1-1.43.24c-1.84.06-3.26-.9-3.24-2.85V101.1h-1.47V99.21h1.47V96.73h2.44v2.48h2Z"></path><path d="M131,104.43c0-3.16,1.91-5.35,5-5.35,2.53,0,4.28,1.47,4.45,3.72H138a2,2,0,0,0-2.09-1.75c-1.5,0-2.51,1.25-2.51,3.34s1,3.39,2.51,3.39A2,2,0,0,0,138,106h2.33c-.18,2.2-1.84,3.74-4.44,3.74C132.82,109.77,131,107.57,131,104.43Z"></path><path d="M144.42,109.57H142V95.76h2.39V101h.12a3,3,0,0,1,3.09-1.89c2.15,0,3.56,1.39,3.56,3.9v6.59H148.7v-6.22a2,2,0,0,0-2-2.21,2.15,2.15,0,0,0-2.24,2.36Z"></path><path d="M152.8,104.45c0-3.19,1.94-5.37,4.91-5.37,2.55,0,4.73,1.6,4.73,5.23v.75h-7.21a2.55,2.55,0,0,0,2.64,2.81,2.16,2.16,0,0,0,2.19-1.33l2.28.25c-.43,1.8-2.09,3-4.5,3C154.71,109.77,152.8,107.7,152.8,104.45Zm7.3-1a2.3,2.3,0,0,0-2.36-2.43,2.49,2.49,0,0,0-2.51,2.43Z"></path><path d="M170.09,102.19a1.81,1.81,0,0,0-1.91-1.29c-1,0-1.79.48-1.78,1.18s.41,1,1.46,1.21l1.77.37c2,.43,2.9,1.33,2.91,2.81,0,2-1.83,3.3-4.42,3.3s-4.15-1.12-4.45-3l2.38-.23a1.86,1.86,0,0,0,2.06,1.41c1.16,0,1.93-.53,1.93-1.24s-.45-1-1.4-1.18l-1.76-.37c-2-.41-2.92-1.41-2.92-2.92,0-1.92,1.69-3.14,4.19-3.14s3.83,1.12,4.17,2.87Z"></path><path d="M178,106.66c0-2.33,1.92-2.93,3.93-3.15,1.83-.19,2.57-.22,2.57-.93v0c0-1-.62-1.59-1.76-1.59a2.06,2.06,0,0,0-2.12,1.31l-2.28-.32c.54-1.89,2.21-2.86,4.39-2.86,2,0,4.21.82,4.21,3.56v6.93h-2.35v-1.42h-.08a3.21,3.21,0,0,1-3,1.63C179.52,109.78,178,108.7,178,106.66Zm6.5-.8v-1.23a7.34,7.34,0,0,1-2.24.51c-1.09.15-1.9.55-1.9,1.48s.72,1.37,1.74,1.37A2.21,2.21,0,0,0,184.53,105.86Z"></path><path d="M191.5,109.57h-2.45V99.21h2.34V101h.12a3.11,3.11,0,0,1,3.09-1.89c2.14,0,3.55,1.41,3.55,3.9v6.59H195.7v-6.22a2,2,0,0,0-2-2.21,2.12,2.12,0,0,0-2.19,2.36Z"></path><path d="M199.83,104.41c0-3.46,1.89-5.33,4.28-5.33a3.08,3.08,0,0,1,3,1.84h.1V95.76h2.45v13.81h-2.4v-1.63h-.15a3.13,3.13,0,0,1-3,1.81C201.66,109.75,199.83,107.82,199.83,104.41Zm7.39,0c0-2-.86-3.31-2.44-3.31s-2.46,1.38-2.46,3.31.85,3.36,2.46,3.36S207.22,106.4,207.22,104.39Z"></path><path d="M222.15,102.19a1.81,1.81,0,0,0-1.91-1.29c-1,0-1.79.48-1.78,1.18s.41,1,1.46,1.21l1.77.37c1.95.43,2.9,1.33,2.91,2.81,0,2-1.83,3.3-4.42,3.3s-4.14-1.12-4.45-3l2.38-.23a1.86,1.86,0,0,0,2.06,1.41c1.16,0,1.93-.53,1.93-1.24s-.45-1-1.4-1.18l-1.76-.37c-2-.41-2.92-1.41-2.92-2.92,0-1.92,1.7-3.14,4.19-3.14s3.83,1.12,4.17,2.87Z"></path><path d="M231.42,101.1h-2v5.36c0,1,.49,1.2,1.11,1.2a3.12,3.12,0,0,0,.71-.1l.41,1.91a4.8,4.8,0,0,1-1.43.24c-1.84.06-3.25-.9-3.24-2.85V101.1h-1.47V99.21h1.47V96.73h2.44v2.48h2Z"></path><path d="M232.53,104.43c0-3.21,1.93-5.35,5-5.35s5,2.14,5,5.35-1.93,5.34-5,5.34S232.53,107.64,232.53,104.43Zm7.45,0c0-1.9-.82-3.42-2.47-3.42s-2.51,1.52-2.51,3.42.83,3.39,2.51,3.39S240,106.32,240,104.43Z"></path><path d="M244.15,99.21h2.36v1.73h.11a2.6,2.6,0,0,1,2.56-1.88,5.79,5.79,0,0,1,.87.07v2.25a4.54,4.54,0,0,0-1.13-.14,2.21,2.21,0,0,0-2.33,2.24v6.09h-2.44Z"></path><path d="M251.11,96.42a1.42,1.42,0,1,1,1.42,1.32A1.38,1.38,0,0,1,251.11,96.42Zm.19,2.79h2.44v10.36H251.3Z"></path><path d="M255.43,104.45c0-3.19,1.94-5.37,4.9-5.37,2.55,0,4.74,1.6,4.74,5.23v.75h-7.22a2.55,2.55,0,0,0,2.64,2.81,2.17,2.17,0,0,0,2.2-1.33l2.28.25c-.44,1.8-2.09,3-4.51,3C257.34,109.77,255.43,107.7,255.43,104.45Zm7.3-1a2.31,2.31,0,0,0-2.36-2.43,2.48,2.48,0,0,0-2.51,2.43Z"></path><path d="M272.72,102.19a1.82,1.82,0,0,0-1.91-1.29c-1,0-1.8.48-1.79,1.18s.41,1,1.46,1.21l1.77.37c2,.43,2.91,1.33,2.92,2.81,0,2-1.84,3.3-4.43,3.3s-4.14-1.12-4.44-3l2.38-.23a1.85,1.85,0,0,0,2.05,1.41c1.16,0,1.93-.53,1.93-1.24s-.44-1-1.39-1.18l-1.77-.37c-2-.41-2.92-1.41-2.91-2.92,0-1.92,1.69-3.14,4.18-3.14s3.84,1.12,4.17,2.87Z"></path><path d="M281.25,95.76h2.44v5.16h.1a3.09,3.09,0,0,1,3-1.84c2.4,0,4.28,1.87,4.28,5.33s-1.83,5.34-4.27,5.34a3.14,3.14,0,0,1-3-1.81h-.14v1.63h-2.4Zm4.83,12c1.61,0,2.46-1.42,2.46-3.36s-.83-3.31-2.46-3.31-2.44,1.3-2.44,3.31S284.52,107.75,286.08,107.75Z"></path><path d="M292.27,113.33l.57-2c1.07.31,1.77.22,2.23-.92l.25-.66-3.76-10.58h2.59l2.39,7.83h.1l2.4-7.83h2.59l-4.18,11.71a3.65,3.65,0,0,1-3.64,2.68A4.33,4.33,0,0,1,292.27,113.33Z"></path><path d="M318.74,109.57h-2.23l-6.5-9.41h-.12v9.41h-2.5V95.76h2.24l6.5,9.41h.12V95.76h2.49Z"></path><path d="M320.53,104.43c0-3.21,1.93-5.35,5-5.35s5,2.14,5,5.35-1.93,5.34-5,5.34S320.53,107.64,320.53,104.43Zm7.45,0c0-1.9-.82-3.42-2.47-3.42s-2.5,1.52-2.5,3.42.82,3.39,2.5,3.39S328,106.32,328,104.43Z"></path><path d="M334.59,109.57h-2.44V99.21h2.33V101h.12a3.11,3.11,0,0,1,3.09-1.89c2.14,0,3.56,1.41,3.55,3.9v6.59H338.8v-6.22a2,2,0,0,0-2-2.21,2.12,2.12,0,0,0-2.19,2.36Z"></path><path d="M342.91,104.43c0-3.21,1.93-5.35,5-5.35s5,2.14,5,5.35-1.92,5.34-5,5.34S342.91,107.64,342.91,104.43Zm7.45,0c0-1.9-.82-3.42-2.48-3.42s-2.5,1.52-2.5,3.42.83,3.39,2.5,3.39S350.36,106.32,350.36,104.43Z"></path><path d="M362,95.76l4.1,10h.16l4.1-10h3.06v13.81h-2.4v-9.49h-.13l-3.81,9.45h-1.8l-3.81-9.47h-.13v9.51H359V95.76Z"></path><path d="M375.47,106.66c0-2.33,1.92-2.93,3.93-3.15,1.83-.19,2.56-.22,2.56-.93v0c0-1-.62-1.59-1.75-1.59a2.06,2.06,0,0,0-2.12,1.31l-2.28-.32c.54-1.89,2.21-2.86,4.39-2.86,2,0,4.21.82,4.21,3.56v6.93h-2.35v-1.42H382a3.21,3.21,0,0,1-3,1.63C377,109.78,375.47,108.7,375.47,106.66Zm6.5-.8v-1.23a7.41,7.41,0,0,1-2.24.51c-1.09.15-1.91.55-1.91,1.48s.73,1.37,1.75,1.37A2.21,2.21,0,0,0,382,105.86Z"></path><path d="M386.49,99.21h2.37v1.73H389a2.58,2.58,0,0,1,2.55-1.88,5.92,5.92,0,0,1,.88.07v2.25a4.54,4.54,0,0,0-1.13-.14,2.21,2.21,0,0,0-2.34,2.24v6.09h-2.44Z"></path><path d="M399.47,101.1h-2.05v5.36c0,1,.5,1.2,1.11,1.2a3.33,3.33,0,0,0,.72-.1l.41,1.91a4.88,4.88,0,0,1-1.44.24c-1.83.06-3.25-.9-3.24-2.85V101.1h-1.47V99.21H395V96.73h2.44v2.48h2.05Z"></path><path d="M401.13,99.21h2.45v10.36h-2.45Zm1.8-4.44h2.39l-2.07,3.08h-1.83Z"></path><path d="M408.15,109.57h-2.44V99.21H408V101h.12a3.11,3.11,0,0,1,3.09-1.89c2.14,0,3.55,1.41,3.55,3.9v6.59h-2.44v-6.22a2,2,0,0,0-2-2.21,2.12,2.12,0,0,0-2.19,2.36Z"></path><path d="M416.47,104.45c0-3.19,1.93-5.37,4.9-5.37,2.55,0,4.73,1.6,4.73,5.23v.75h-7.21a2.55,2.55,0,0,0,2.64,2.81,2.16,2.16,0,0,0,2.19-1.33l2.28.25c-.43,1.8-2.09,3-4.5,3C418.38,109.77,416.47,107.7,416.47,104.45Zm7.29-1A2.3,2.3,0,0,0,421.4,101a2.5,2.5,0,0,0-2.51,2.43Z"></path><path d="M427.66,108l5.34-6.7v-.08h-5.17v-2H436v1.67l-5.09,6.58v.09h5.26v2h-8.5Z"></path><path d="M441.63,109.57l4.87-13.81h3.08l4.87,13.81h-2.67l-1.14-3.4h-5.2l-1.14,3.4Zm8.33-5.41-1.87-5.57H448l-1.87,5.57Z"></path><path d="M458.24,109.57h-2.45V95.76h2.45Z"></path><path d="M459.92,104.43c0-3.21,1.93-5.35,5-5.35s5,2.14,5,5.35-1.93,5.34-5,5.34S459.92,107.64,459.92,104.43Zm7.45,0c0-1.9-.82-3.42-2.47-3.42s-2.5,1.52-2.5,3.42.82,3.39,2.5,3.39S467.37,106.32,467.37,104.43Z"></path><path d="M474,109.57h-2.44V99.21h2.33V101H474a3.11,3.11,0,0,1,3.09-1.89c2.14,0,3.55,1.41,3.55,3.9v6.59h-2.44v-6.22a2,2,0,0,0-2-2.21A2.12,2.12,0,0,0,474,103.5Z"></path><path d="M488.7,102.19a1.81,1.81,0,0,0-1.91-1.29c-1,0-1.79.48-1.78,1.18s.41,1,1.46,1.21l1.77.37c1.95.43,2.91,1.33,2.91,2.81,0,2-1.83,3.3-4.42,3.3s-4.14-1.12-4.45-3l2.38-.23a1.86,1.86,0,0,0,2.06,1.41c1.16,0,1.93-.53,1.93-1.24s-.44-1-1.4-1.18l-1.76-.37c-2-.41-2.92-1.41-2.92-2.92,0-1.92,1.7-3.14,4.19-3.14s3.83,1.12,4.17,2.87Z"></path><path d="M492.35,104.43c0-3.21,1.93-5.35,5-5.35s5,2.14,5,5.35-1.93,5.34-5,5.34S492.35,107.64,492.35,104.43Zm7.45,0c0-1.9-.82-3.42-2.47-3.42s-2.51,1.52-2.51,3.42.83,3.39,2.51,3.39S499.8,106.32,499.8,104.43Z"></path></svg>
    
    

            <p><img src="https://nono.imgix.net/img/u/sketch-191102-cordoba-las-ramblas-alfar-torres-ferreras-ceramic-artisan.jpg?auto=format%2Ccompress&amp;ixlib=php-3.3.0&amp;w=2500"></p>

    <p>Andy Warhol's artworks have sold for millions of dollars. His most famous works—think of Campbell's Soup Cans (1962) and Marylin Diptych (1962)—are limited edition paintings. Campbell's Soup Cans' piece consists of 32 images produced over five months<sup id="fnref:wikipedia-warhol-andy"><a href="#fn:wikipedia-warhol-andy" role="doc-noteref">1</a></sup>, and Marilyn Monroe's artwork consists of 50 portraits.<sup id="fnref:wikipedia-warhol-marilyn"><a href="#fn:wikipedia-warhol-marilyn" role="doc-noteref">2</a></sup></p>
<p>After hand-painting thirty-two soup cans by hand, Warhol moved to photo-silkscreen, a printmaking technique originally invented for commercial use that allowed Warhol and other artists to create reproductions of the same artwork using a silkscreen.<sup id="fnref:warhol-moma-learning"><a href="#fn:warhol-moma-learning" role="doc-noteref">3</a></sup></p>
<p>Warhol painted the soup cans with acrylic paint. Each canvas corresponded to a soup variety sold by Campbell's back in the 1960s.</p>
<p>Screen printing speeds up the reproduction of an artwork. Once the silkscreen is ready, colors are applied, one by one, using a squeegee to push the ink through the mesh screen<sup id="fnref:dickblick-screen-printing"><a href="#fn:dickblick-screen-printing" role="doc-noteref">4</a></sup>, either by hand or automatically with a machine, a process being used at the time to mass-produce advertisements.<sup id="fnref:warhol-moma-learning__2"><a href="#fn:warhol-moma-learning" role="doc-noteref">3</a></sup></p>
<p>"I don't think art should be only for the select few," Warhol claimed, "I think it should be for the mass of the American people."</p>
<p>Nowadays, we could argue this vision is a reality. Large corporations and artisans deploy a wide range of mediums to automate what used to be done by hand, producing goods en masse, lessening their price and uniqueness while improving its quality and availability. You can buy a ready-to-hang print of Vang Goh's&nbsp;<em>The Starry Night</em>&nbsp;at IKEA for $49.99 while the Museum of Modern Art in Midtown Manhattan shields and exhibits the original painting.</p>
<p>Contrary to his statement, Warhol created artwork for the selected few that could pay for it. In 2007, a 1964&nbsp;<em>Large Campbell's Soup Can</em>&nbsp;sold for $7.4 million, and&nbsp;<em>Silver Car Crash</em>&nbsp;sold for $105.4 million in 2013.</p>
<p>Aesthetics and taste aside, it's all about the story behind each piece.</p>
<p>Who made it, why, and in what context?</p>
<!-- References -->



  </div>



  </div><div>
      <p><img src="https://nono.imgix.net/folio/images/veil.gif" data-src="https://nono.imgix.net/img/u/sketch-nono-ma-logo.svg"></p>
<hr>
<p>
My sketches and stories, in&nbsp;your&nbsp;inbox.
</p>

<p><span>One email per week. No spam ever.</span></p>

<p><img src="https://sketch.nono.ma/img/u/profile-nono-ma-sketch.jpg" alt="Pencil sketch of Nono Martínez Alonso.">
</p>


      </div></div>]]>
            </description>
            <link>https://sketch.nono.ma/who-made-it-why-and-in-what-context</link>
            <guid isPermaLink="false">hacker-news-small-sites-25044097</guid>
            <pubDate>Tue, 10 Nov 2020 09:24:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Low Hanging Fruits in Front End Performance Optimization]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25044079">thread link</a>) | @pawurb
<br/>
November 10, 2020 | https://pawelurbanek.com/frontend-performance-optimization | <a href="https://web.archive.org/web/*/https://pawelurbanek.com/frontend-performance-optimization">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  
    <p><img title="Web apps frontend performance is represented by grapes Photo by Amos Bar-Zeev on Unsplash" alt="Web apps frontend performance is represented by grapes Photo by Amos Bar-Zeev on Unsplash" data-src="https://pawelurbanek.com/assets/frontend-optimization-fruits-6ff2f8bc957fe4e1142ab67c3a460ce9dc962eba5b1dc2f10c5125292c558b37.jpg" src="https://pawelurbanek.com/assets/frontend-optimization-fruits-thumb-90ff9c113ed86f833b4e2fa0bbe34e130f2ee5871d3b08aa033fe1e1cbc0e7ad.jpg">
    </p>
  

  

  

  <p>I conduct Rails performance audits for a living. Clients usually approach me with a request to speed up the backend, i.e., optimize the bottleneck API endpoint or tune the database queries. After the initial research, it often turns out that tweaking the frontend will make a better impact on the perceivable performance than fine-tuning the backend.</p>

<p>In this blog post, I describe the often-overlooked techniques that can significantly improve your web app’s overall performance.</p>

<p>These tips apply to all the web technologies like Ruby on Rails, NodeJS, Python Django, or Elixir Phoenix. It does not matter if you render an HTML or serve an API consumed by the JavaScript SPA framework. It all comes down to transferring bytes over the HTTP protocol. Frontend performance optimization is all about making this process as efficient as possible.</p>

<h2 id="why-is-frontend-performance-critical-for-your-websites-success">Why is frontend performance critical for your website’s success?</h2>

<p>I guess that developers often disregard the frontend performance because it doesn’t directly affect the infrastructure costs. Rendering the unoptimized website is offloaded to the visitor’s desktop or mobile device and cannot be measured using backend monitoring tools.</p>

<p>Developers usually work on top-notch desktop computers with a high-speed internet connection. They do not experience poor performance themselves. The UX of visiting your landing page on a 15 inch Mac Book Pro with a fiber connection cannot be compared to an old Android device on a shaky 3G network.</p>

<p>A typical web app issues dozens of requests on initial load. Only a few are backend-related, i.e., website HTML, API calls, etc. The majority of requests are static assets, JavaScript libraries, images. Fine-tuning the frontend-related requests will give a much greater return than shaving a couple of hundered milliseconds off a database query.</p>

<p>Google Bot measures the performance of your website, and it directly affects the SEO rating. Since <a href="https://developers.google.com/search/mobile-sites/mobile-first-indexing" target="_blank" rel="noopener noreferrer">July 2019</a>, Google Bot is using a <em>“Mobile first”</em> approach to assessing your website.</p>

<p>You might not care about frying the CPU and wasting the bandwidth of your mobile users. Maybe landing a sweet spot in Google search results should convince you to focus on your frontend performance?</p>

<h2 id="test-in-your-clients-shoes">Test in your client’s shoes</h2>

<p><em>“If you want to write fast websites, use slow internet.”</em>.</p>

<p>You should regularly throttle the internet speed during the development process to experience first-hand how your app will behave for most users.</p>

<p>On macOS, you can use the <a href="https://nshipster.com/network-link-conditioner/" target="_blank" rel="noopener noreferrer">Network Link Conditioner</a> to do it:</p>

<p><img alt="Simulate mobile network on a desktop computer" title="Simulate mobile network on a desktop computer" loading="lazy" src="https://pawelurbanek.com/assets/3g-network-performance-4273c3bd62edbdaddfdc36d7dad126747f3d69804a7ce8c1227cd8f96ff0a1ed.png"></p>

<p>Also, both Firefox and Chrome developer tools offer the option to throttle the internet speed in the <strong>Network</strong> tab:</p>

<p><img alt="Chrome network throttle setting" title="Chrome network throttle setting" loading="lazy" src="https://pawelurbanek.com/assets/chrome-network-throttle-ed2b0e3cb5163dbf3d6fe89601bd32c072af9a2b7146e82d8004f8e536ca208d.png"></p>

<p>Chrome network throttle</p>

<p><img alt="Firefox network throttle setting" title="Firefox network throttle setting" loading="lazy" src="https://pawelurbanek.com/assets/firefox-network-throttle-d11d6b54034fff903c4cc721f05a66747904fd72d3d9760ab7f1141491875434.png"></p>

<p>Firefox network throttle</p>


<p>Maybe the internal demos of the new features should also be done on the throttled network? Everyone in the company should have the chance to see how the app really works for most users.</p>

<h2 id="reconnaissance">Reconnaissance</h2>

<p>Discovering frontend issues is usually more straightforward than backend ones. You don’t even need admin access to the website. By definition, the frontend issues are in the <em>frontend</em>. You can scan and diagnose every website out there. I use the following tools to perform the initial scan:</p>

<p><a href="https://www.fastorslow.com/" target="_blank" rel="noopener noreferrer">FastOrSlow</a></p>

<p><a href="https://www.webpagetest.org/" target="_blank" rel="noopener noreferrer">WebPageTest</a></p>

<p><a href="https://developers.google.com/speed/pagespeed/insights/" target="_blank" rel="noopener noreferrer">Google PageSpeed Insights</a></p>

<p><a href="https://github.com/GoogleChrome/lighthouse" target="_blank" rel="noopener noreferrer">GoogleChrome lighthouse</a></p>

<p>There’s no reason why ANY website shouldn’t score top on each of those tools. Read on if your score is anywhere below 90%.</p>

<p><img alt="Abot for Slack FastOrSlow score" title="Abot for Slack FastOrSlow score" loading="lazy" src="https://pawelurbanek.com/assets/abot-fastorslow-28336ad9b7b74849a6a1d85a1ad269be81dd6288a960ee3b3ecbe69e6cf6b6a7.png"></p>

<p><img alt="Abot for Slack WebPageTest score" title="Abot for Slack WebPageTest score" loading="lazy" src="https://pawelurbanek.com/assets/abot-webpagetest-e33807bf28e738ced4aa16f48cdf17e44836a986b283aca9d673c8504ff045fa.png"></p>

<p><img alt="Abot for Slack Google speed score" title="Abot for Slack FastOrSlow score" loading="lazy" src="https://pawelurbanek.com/assets/abot-googlespeed-03d18ebbc637cce72357f44e3ed65e1cf60062e633df52bafc2eb178e0cca7ac.png"></p>

<p>The <a href="https://abot.app/" target="_blank">Abot landing page</a> is a dynamic Rails website getting top performance rating</p>





<h2 id="client-side-caching">Client-side caching</h2>

<p>Correctly configuring client-side caching is the most critical frontend optimization. I’ve seen it misconfigured in multiple production apps so far. <a href="https://github.com/webpack/webpack" target="_blank" rel="noopener noreferrer">Webpack</a> comes with a great mechanism to easily leverage client-side caching, i.e., <em>MD5 digest</em>. The production assets generation process must be configured to append the <em>MD5 digest</em> tag to the filename.</p>

<p>It means that in the production environment, the <code>application.js</code> file becomes <code>application-5bf4f97...95c2147.js</code>. The random suffix is generated based on the file contents, so it is guaranteed to change if the file changes. You must add the correct <code>cache-control</code> header to make sure that once downloaded, the file will persist in the browser cache:</p>

<figure><pre><code data-lang="bash">cache-control: public, max-age<span>=</span>31536000, immutable</code></pre></figure>

<p>The <code>immutable</code> parameter ensures that cache is not cleared when the user explicitly refreshes the website on the Chrome browser.</p>

<p>If you’re using NGINX as reverse proxy you can use the following directive:</p>

<figure><pre><code data-lang="nginx"><span>location</span> <span>~</span><span>*</span> <span>\</span><span>.(?:ico|css|js|gif|jpe?g|png|woff2)</span>$ <span>{</span>
  <span>add_header</span> <span>Cache-Control</span> <span>"public,</span> <span>max-age=31536000,</span> <span>immutable"</span><span>;</span>
  <span>try_files</span> <span>$uri</span> <span>=</span><span>404</span><span>;</span>
<span>}</span></code></pre></figure>

<p>I’ve seen many apps using <code>Etag</code> and <code>Last-Modified</code> headers instead of <code>Cache-Control</code>. <code>Etag</code> is also generated based on the file contents, but the client has to talk to the server to confirm that the cached version is still correct. It means that on every page visit, the browser has to issue a request to validate its cache contents and wait for <code>304 Not Modified</code> response. This  completely unnecessary network roundtrip can be avoided if you add a <code>Cache-Control</code> header.</p>

<h2 id="limit-bandwidth-usage">Limit bandwidth usage</h2>

<p>Nowadays, websites are just MASSIVE. It often takes multiple MBs to render a static landing page. Let me point out the most common mistakes that affect it and how they can be resolved.</p>

<h3 id="compress-and-resize-images">Compress and resize images</h3>

<p>There’s no excuse for serving uncompressed images on your website. You must make sure to process all your images with tools like <a href="https://compressor.io/" target="_blank" rel="noopener noreferrer">Compressor.io</a>. There’s often no perceivable difference for images processed with <strong>Lossy</strong> compression, and it usually means ~70% size reduction.</p>

<p>Resizing an image to the size that it actually needs is often overlooked. To check it, visit your website using Firefox on a large desktop screen, right-click the image, and select <strong>View image info</strong>. You’ll see what dimensions the image needs vs. how large it is now:</p>

<p><img alt="Checking real image" title="Checking real image" loading="lazy" src="https://pawelurbanek.com/assets/real-image-size-524abe8437774180a233461deb8ada35092fda22cee4b3dfe85c0d0ad2e757b8.png"></p>

<p>Make sure first to resize the image and only then compress it. Otherwise, you might lose quality.</p>

<h3 id="defer-images-loading">Defer images loading</h3>

<p>You should defer the loading of the images that are not visible in the initial viewport. During the initial load, dozens of requests are competing for network throughput. Delaying the transfer of unnecessary images will leave more resources for necessary assets like CSS stylesheets etc.</p>

<p>There’s <a href="https://github.com/aFarkas/lazysizes" target="_blank" rel="noopener noreferrer">plenty</a> of <a href="https://github.com/tuupola/lazyload" target="_blank" rel="noopener noreferrer">different</a> <a href="https://github.com/vvo/lazyload" target="_blank" rel="noopener noreferrer">JavaScript libraries</a> that offer this feature. Including them means additional bandwidth usage, so I prefer to keep things simple and use a native <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/img#attr-loading" target="_blank" rel="noopener noreferrer"><code>loading='lazy'</code></a> HTML attribute.</p>

<p>It has decent <a href="https://caniuse.com/loading-lazy-attr" target="_blank" rel="noopener noreferrer">browser support</a>. Have a look at how it affected one of my blog posts:</p>

<p><img alt="Checking real image " title="Checking real image " loading="lazy" src="https://pawelurbanek.com/assets/before-lazy-images-a0e8cdb9099f3d5a348f853f4f222b67149aa058e59ecd29d2d5338be81cd8c0.png"></p>

<p>Without lazy loaded images</p>

<p><img alt="Checking real image " title="Checking real image " loading="lazy" src="https://pawelurbanek.com/assets/after-lazy-images-836f1781a9b7bf56cd8883a0ad0d252f52223f74b856e7e994ecef1d52efd029.png"></p>

<p>Lazy loading for images enabled</p>



<p>As you can see, adding <code>loading='lazy'</code> to all the images reduced ten requests and over <em>250kb</em> of transfer on the initial load. That’s a massive deal for slower internet connections!</p>

<h3 id="enough-with-the-gifs-already">Enough with the GIFs already…</h3>

<p>GIFs are HUGE! I understand you want to showcase a fancy UI on your landing page, but maybe you could use a lazy-loaded movie clip instead? <em>10MB</em> GIF can be converted to <em>250kb</em> mp4 file… Twitter automatically changes <em>GIF</em> images to <em>mp4</em> files, so I’d trust them on this one.</p>

<h3 id="cherry-pick-and-measure-dependencies-size">Cherry-pick and measure dependencies size</h3>

<p>Many frontend libraries offer a modular approach to including them in your application. For example, <a href="https://getbootstrap.com/docs/3.4/customize/" target="_blank" rel="noopener noreferrer">Bootstrap</a> allows you to customize the build to include only the components you need.</p>

<p>Some popular libraries have lightweight alternatives. Since recently, <a href="https://twitter.com/addyosmani/status/1304676118822174721" target="_blank" rel="noopener noreferrer">ChromeDevTools suggests them</a>, so make sure to use it for your application.</p>

<h3 id="reconsider-3rd-party-dependencies">Reconsider 3rd party dependencies</h3>

<p>Overusing externally hosted 3rd party JavaScript libraries is the simplest way to kill the performance of your website.</p>

<p>Dropping in yet another <code>&lt;script src="..."&gt;</code> tag might not seem like a big deal. It’s easy to forget that one script can result in a cascade of requests, each including more resources. Here’s the cost of embedding sample 3rd party JavaScript libraries:</p>

<table>
  <tbody><tr>
    <th></th>
    <th>Requests</th>
    <th>Bandwidth (total/gzipped)</th>
  </tr>
  <tr>
    <td><a href="https://www.google.com/analytics/" target="_blank" rel="noopener noreferrer">Google Analytics</a></td>
    <td>4</td>
    <td>104.09 KB / 40.37 KB</td>
  </tr>
  <tr>
    <td><a href="https://simpleanalytics.com/" target="_blank" rel="noopener noreferrer">Simple Analytics</a></td>
    <td>2</td>
    <td>5.29 KB / 3.12 KB</td>
  </tr>
  <tr>
    <td><a href="https://developer.twitter.com/en/docs/twitter-for-websites/follow-button/overview.html" target="_blank" rel="noopener noreferrer">Twitter button</a></td>
    <td>8</td>
    <td>173.68 KB / 59.30 KB</td>
  </tr>
  <tr>
    <td><a href="https://disqus.com/" target="_blank" rel="noopener noreferrer">Disqus</a></td>
    <td>26</td>
    <td>862.55 KB / 271.48 KB</td>
  </tr>
  <tr>
    <td><a href="https://commento.io/" target="_blank" rel="noopener noreferrer">Commento.io</a></td>
    <td>5</td>
    <td>64.73 KB / 19.25 KB</td>
  </tr>
</tbody></table>



<p>The only 3rd party JavaScript dependency I use for this blog is <a href="https://commento.io/" target="_blank" rel="noopener noreferrer">Commento.io</a> for comments. It’s over <strong>10x</strong> lighter than its alternative Disqus.</p>

<p>I’ve switched from using Google Analytics to SimpleAnalytics long ago. Recently I’ve decided I don’t need to track the visitors of this blog at all. Summary visit stats from Cloudflare are enough for me.</p>

<p><img alt="CloudFlare visits stats" title="CloudFlare visits stats" loading="lazy" src="https://pawelurbanek.com/assets/cloudflare-total-stats-2af208b0332a799e70e0aaa5495ef01c05c12ccc24514110accd3a4005817863.png"></p>

<p>All the tracking I need. No JavaScript dependencies required</p>


<p>Including 3rd party libraries from external sources often reduces your ability to set correct caching headers, thus hurting your performance score.</p>

<p>You should always look for the most straightforward tool that meets your requirements and only resort to using 3rd party if you cannot develop the lightweight solution yourself.</p>

<h2 id="http-2">HTTP 2</h2>

<p><em>HTTP 2</em> offers massive performance improvement over <em>HTTP 1.1</em> for loading static assets. Headers are compressed to reduce bandwidth. Even more important is that multiple assets can be loaded in parallel over a single HTTP connection.</p>

<p>It might not be critical for API calls, but for static assets, you should enable <em>HTTP 2</em> and expect serious performance gains.</p>

<p>How to do it depends on your infrastructure. If you’re using custom infrastructure with NGINX reverse proxy, you can check out <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-nginx-with-http-2-support-on-ubuntu-18-04" target="_blank" rel="noopener noreferrer">this tutorial</a>.</p>

<p>If you’re using Heroku, you’re out of luck because currently, it <a href="https://help.heroku.com/JAOCNZ25/does-heroku-have-plans-to-support-http-2" target="_blank" rel="noopener noreferrer">does not support HTTP 2</a>. The simplest way to add HTTP 2 support for Heroku is to proxy your traffic through <a href="https://pawelurbanek.com/cloudflare.com/" target="_blank" rel="noopener noreferrer">Cloudflare</a>.</p>

<p>If you don’t want to move your application to Cloudflare’s DNS, you can always use a custom domain just for serving assets from their CDN.</p>

<h2 id="physical-server-location-and-cdn">Physical server location and CDN</h2>

<p>The usage of CDN (<em>Content Delivery Network</em>) is critical if your user base is spread across the globe. Correctly configured CDN will cache static assets on the edge locations, significantly reducing the request’s duration. We’re talking like <em>50ms</em> vs. <em>800ms</em> (<strong>16x</strong> …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pawelurbanek.com/frontend-performance-optimization">https://pawelurbanek.com/frontend-performance-optimization</a></em></p>]]>
            </description>
            <link>https://pawelurbanek.com/frontend-performance-optimization</link>
            <guid isPermaLink="false">hacker-news-small-sites-25044079</guid>
            <pubDate>Tue, 10 Nov 2020 09:20:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Software development: should we stop? Maybe we should]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25044031">thread link</a>) | @enz
<br/>
November 10, 2020 | http://blog.spencermounta.in/2020/should-we-stop/index.html | <a href="https://web.archive.org/web/*/http://blog.spencermounta.in/2020/should-we-stop/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://blog.spencermounta.in/2020/should-we-stop/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25044031</guid>
            <pubDate>Tue, 10 Nov 2020 09:09:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An Awful Edge Case in Bash's Set -e]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25044030">thread link</a>) | @jbrot
<br/>
November 10, 2020 | http://jbrot.com/blog/dash_e_problems.html | <a href="https://web.archive.org/web/*/http://jbrot.com/blog/dash_e_problems.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
            <header>
                
                
            </header>

            <p>
            The last six months, I've been building out the automated testing infrastructure at a start up.
            Our infrastructure is mostly in Python, but writing Bash scripts is inevitable.
            At the end of the day, automated testing is all about running commands in a row—and Bash is the right tool for the job.
            </p>

            <p>
            There are a <a href="https://mywiki.wooledge.org/BashPitfalls">whole</a> <a href="https://github.com/anordal/shellharden/blob/master/how_to_do_things_safely_in_bash.md">bunch</a> <a href="https://sipb.mit.edu/doc/safe-shell/">of</a> <a href="https://wizardzines.com/comics/bash-errors/">articles</a> about how to write safe Bash scripts, and the standard advice is to add <code>set -euo pipefail</code> to make your scripts "safe."
            In this article, I'm going to describe one edge case where <code>set -e</code> completely fails to work.
            </p>

            <h2> Background </h2>

            <p>
            Suppose we have two projects in a git repo, say MyLibrary and MyApplication, where MyApplication depends on MyLibrary. And suppose each project provides a script <code>test.sh</code> that looks something like this:
            </p>

<pre><code>#!/bin/bash

set -e

./configure
make

python fancy_test_driver.py tests/first_tests
python fancy_test_driver.py --option-1 tests/second_tests
python fancy_test_driver.py --option-2 tests/third_tests
</code></pre>

            <p>
            This is a pretty reasonable script.
            We can now require that all changes pass both <code>my_library/test.sh</code> and <code>my_application/test.sh</code> before being merged.
            </p>

            <p>
            As time goes on, the amount of tests (and projects!) can spiral out of control.
            Eventually, someone (me) gets tasked with trying to optimize things.
            One obvious thing to do is to abort early.
            If MyLibrary fails testing, we don't need to bother with testing MyApplication.
            </p>

            <p>
            Of course, the developers who used to get errors from both projects aren't very happy about this change.
            Now passing the <code>test.sh</code> scripts is like peeling an onion: you resolve the first layer of errors only to find more errors lurking underneath—hidden by the early abort.
            However, there's a middle ground.
            We can test MyApplication only if MyLibrary fails while running test cases.
            If MyLibrary fails during compilation, continuing on is pointless since MyApplication depends on MyLibrary.
            </p>

            <p>
            So how do we distinguish when <code>test.sh</code> fails during compilation from when it fails during testing?
            Exit codes, naturally:
            </p>

<pre><code>#!/bin/bash

set -e

COMPILE_FAILURE_CODE=79

./configure || exit $COMPILE_FAILURE_CODE
make || exit $COMPILE_FAILURE_CODE

python fancy_test_driver.py tests/first_tests
python fancy_test_driver.py --option-1 tests/second_tests
python fancy_test_driver.py --option-2 tests/third_tests
</code></pre>

            <p>
            And we're done!
            An exit code of 0 means we passed the tests, an exit code of 79 means compilation failure (no need to test further projects), and any other exit code means we failed in testing—so we can continue testing the other projects.
            </p>

            <h2> Finding the Problem </h2>

            <p>
            The above solution works fine when we only have two lines that need the special exit code.
            However, it quickly becomes unwieldly when it needs to be applied to more lines:
            </p>

<pre><code>#!/bin/bash

set -e

COMPILE_FAILURE_CODE=79

pushd codegen_tool1 || exit $COMPILE_FAILURE_CODE
./configure || exit $COMPILE_FAILURE_CODE
make || exit $COMPILE_FAILURE_CODE
./codegen_tool1 || exit $COMPILE_FAILURE_CODE
popd || exit $COMPILE_FAILURE_CODE

pushd codegen_tool2 || exit $COMPILE_FAILURE_CODE
./configure || exit $COMPILE_FAILURE_CODE
make || exit $COMPILE_FAILURE_CODE
./codegen_tool2 || exit $COMPILE_FAILURE_CODE
popd || exit $COMPILE_FAILURE_CODE

./configure || exit $COMPILE_FAILURE_CODE
make || exit $COMPILE_FAILURE_CODE

# Run some tests...
</code></pre>

            <p>
            Gross!
            Clearly, we should factor out the <code>|| exit $COMPILE_FAILURE_CODE</code> line and have it apply to all of our lines at once.
            We can easily do this by creating a separate <code>build.sh</code> script:
            </p>

<pre><code>#!/bin/bash

set -e

pushd codegen_tool1
./configure
make
./codegen_tool1
popd

# snip

./configure
make
</code></pre>

            <p>
            And then adjusting <code>test.sh</code> to just have:
            </p>

<pre><code>#!/bin/bash

set -e

COMPILE_FAILURE_CODE=79

./build.sh || exit $COMPILE_FAILURE_CODE

# Run some tests...
</code></pre>

            <p>
            And this, too, works great!
            But wait!
            Why even use a second script?
            Can't we do the exact same thing with a subshell?
            </p>

<pre><code>#!/bin/bash

set -e

COMPILE_FAILURE_CODE=79

(
    pushd codegen_tool1
    ./configure
    make
    ./codegen_tool1
    popd

    # snip

    ./configure
    make
) || exit $COMPILE_FAILURE_CODE

# Run some tests...
</code></pre>

            <p>
            <strong>No!</strong>
            This subshell implementation is dangerously broken.
            The rest of this article will explore how and why the subshell code does not function as expected.
            </p>

            <h2> There's a Problem? </h2>

            <p>
            Let's run some simple bash programs and see what happens.
            First, we'll confirm <code>set -e</code> works as expected.
            </p>

<pre><samp>$ cat script1.sh
#!/bin/bash
echo "Statement 1"
(exit 3)
echo "Statement 2"

$ echo "$?"
0

#!/bin/bash
set -e
echo "Statement 1"
(exit 3)
echo "Statement 2"

$ ./script2.sh
Statement 1

$ echo "$?"
3
</samp></pre>

            <p>
            Yep, that's what we expected.
            Without <code>set -e</code> Bash ran every statement, and with <code>set -e</code> Bash stopped after the first non-zero exit code.
            What if we put our statements in a subshell?
            </p>

<pre><samp>$ cat script3.sh
#!/bin/bash
(
    echo "Statement 1"
    (exit 3)
    echo "Statement 2"
)

$ ./script3.sh
Statement 1
Statement 2

$ echo "$?"
0

$ cat ./script4.sh
#!/bin/bash
set -e
(
    echo "Statement 1"
    (exit 3)
    echo "Statement 2"
)

$ ./script4.sh
Statement 1

$ echo "$?"
3
</samp></pre>

        <p>
        And again, that's what we expected.
        The subshell made no difference.
        Note that <code>set -e</code> does propagate into the subshell.
        Alright. What if we mask the subshell's exit code?
        </p>

<pre><samp>$ cat script5.sh
#!/bin/bash
set -e
(
    echo "Statement 1"
    (exit 3)
    echo "Statement 2"
) || exit 9

$ ./script5.sh
Statement 1
Statement 2

$ echo "$?"
0
</samp></pre>

        <p>And again everything works as...</p>

        <h2> Wait, what? </h2>

        <p>Okay. Maybe <code>set -e</code> doesn't propagate?</p>

<pre><samp>$ cat script6.sh
#!/bin/bash
set -e
(
    set -e
    echo "Statement 1"
    (exit 3)
    echo "Statement 2"
) || exit 9

$ ./script6.sh
Statement 1
Statement 2

$ echo "$?"
0

$ cat script7.sh
#!/bin/bash
(
    set -e
    echo "Statement 1"
    (exit 3)
    echo "Statement 2"
) || exit 9

$ ./script7.sh
Statement 1
Statement 2

$ echo "$?"
0
</samp></pre>

        <p>
        Nope. Still doesn't work.
        Even if we just have <code>set -e</code> in the subshell and not in the outer script, it doesn't work.
        </p>

        <h2>So what's going on?</h2>

        <p>
        Well, if we dig into the Bash man page, we find this excerpt about <code>set -e</code>:
        </p>

        <blockquote>
              Exit  immediately  if a pipeline (which may consist of a single simple command), a list, or a compound command (see SHELL GRAMMAR above), exits with a non-zero status.
              The shell does not exit if the  command  that  fails  is part  of  the command list immediately following a while or until keyword, part of the test following the if or elif reserved  words, <em>part of any command executed in a &amp;&amp; or || list except the command following the final &amp;&amp; or ||,</em> any command in a pipeline but the last, or if the command's return value is being inverted with !.
        </blockquote>

        <p>
        So, the spec says that if you're using <code>&amp;&amp;</code> or <code>||</code>, only the last command's exit code can cause the shell to exit.
        This makes sense, because you expect  <code>command_1 || command_2</code> to execute <code>command_2</code> if <code>command_1</code> fails.
        Without this exception, it would be very hard to have any logical statements when <code>-e</code> is set.
        </p>

        <p>
        The behavior we just witnessed is, therefore, Working as Intended™.
        When we try to mask the subshell's exit code, we put the subshell at the start of an <code>||</code> list.
        So, the subshell's exit code will not cause an exit despite <code>-e</code> being set.
        But, every single command inside the subshell is <em>also</em> considered part of the <code>||</code> list, and thus no exit code anywhere in the subshell can cause the subshell to exit.
        It's as if <code>set +e</code> is being run implicitly in the subshell—only, as we've seen, we can't override it with an explicit <code>set -e</code> in the subshell.
        </p>

        <p>
        Is there anything we can do to fix this?
        Well, you're probably better off with one of the approaches I presented earlier.
        If you need to stay inside the same shell script, the solution with <code>trap</code> below is probably what you want.
        And if you truly need to use a subshell, I was able to come up with this mess:
        </p>

<pre><samp>$ cat script8.sh
#!/bin/bash
set -e
echo "Some stuff with -e set"

set +e
(
    set -e
    echo "Statement 1"
    (exit 3)
    echo "Statement 2"
)
[[ $? -ne 0 ]] &amp;&amp; exit 9
set -e

echo "More code with -e set (unreachable)"

$ ./script8.sh
Some stuff with -e set
Statement 1

$ echo "$?"
9
</samp></pre>

        <h3>Bonus Solution</h3>

        <p>
        I ended up using <code>trap</code> for error masking:
        </p>

<pre><samp>$ cat script9.sh
#!/bin/bash

set -e

trap 'exit 9' ERR

echo "Statement 1"
(exit 3)
echo "Statement 2"

trap - ERR

$ ./script9.sh
Statement 1

$ echo "$?"
9
</samp></pre>

        <p>
        The nice part about this solution is it allows you to stick with just one script (useful if you need to use functions), and the logic is straightforward.
        This option can be harder to make work if you're already using <code>trap ERR</code> for cleanup, though.
        </p>

        </article></div>]]>
            </description>
            <link>http://jbrot.com/blog/dash_e_problems.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25044030</guid>
            <pubDate>Tue, 10 Nov 2020 09:09:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Become Covid Savvy in 10 Steps]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25044022">thread link</a>) | @datashrimp
<br/>
November 10, 2020 | https://adsp.ai/articles/how-to-become-covid-savvy-in-10-steps/ | <a href="https://web.archive.org/web/*/https://adsp.ai/articles/how-to-become-covid-savvy-in-10-steps/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://adsp.ai/articles/how-to-become-covid-savvy-in-10-steps/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25044022</guid>
            <pubDate>Tue, 10 Nov 2020 09:06:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[iOS 14 IDFA changes. Research on change in mobile ad market]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25044000">thread link</a>) | @iwitaly
<br/>
November 10, 2020 | https://blog.adapty.io/ios-14-idfa-attribution-a-global-change-in-the-mobile-advertising-market/ | <a href="https://web.archive.org/web/*/https://blog.adapty.io/ios-14-idfa-attribution-a-global-change-in-the-mobile-advertising-market/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<p>The mobile industry is undergoing one of the most fundamental changes of recent years. Apple has decided that early 2021, app developers will no longer have access to IDFA by default.</p><p>IDFA is a unique device identifier used for ad attribution, retargeting, alike audiences, analytics and other tasks. After the change, in order to receive the IDFA, an app developer must explicitly request the user’s permission (which is similar to allowing push notifications in an app). According to various estimates, the share of users who will provide access to their IDFA doesn’t exceed 10%.</p><p>Apple has provided privacy-friendly alternatives for attribution, but they fail to cover even a small fraction of the tasks that teams working on developing and promoting mobile apps currently have.</p><p>This shift means that mobile marketing (estimated at $80 billion), and by extension the mobile industry, are about to change drastically. In this essay, we will discuss in detail what will change, how it will affect the main players in the mobile advertising market such as developers, ad systems, attribution service providers, and advertisers.</p><figure><img src="https://blog.gopractice.io/wp-content/uploads/2020/09/for-post.jpg"></figure><h2 id="a-short-summary-of-the-key-changes-and-implications-of-ios-14-release-and-limiting-default-access-to-idfa"><strong><strong>A short summary of the key changes and implications of iOS 14 release and limiting default access to IDFA</strong></strong></h2><p><strong><strong># 1 Access restrictions: In iOS 14, IDFA will only be accessible upon user permission.</strong></strong></p><p>IDFA (Identifier for Advertisers) is a unique identifier of an iOS device. It is used in mobile apps for user attribution and tells advertisers where a user came from.</p><p>With iOS 14, to be released in early 2021, every app that wants to use an advertising user ID (IDFA) will have to explicitly ask permission from the user.</p><p>This will work in the same way as requesting permission to send push notifications.</p><figure><img src="https://lh4.googleusercontent.com/ary-L0VrnuIGlsebGQyVjv9IvP--cr9L0G2gMnSXBsP399mw2VnRal5OGOr7u9oqR9oVOOdOuWovqElUe8XyDaPb4gSYrOTRhlSnOqprRQRCEDoDQESdrFvOiMM_GEHTr-3_YMdR"></figure><p>Request to use IDFA will look like this – the text on the popup will ask: “Would you like to give permission to track you across apps and websites owned by other companies?”</p><p>The text is very straightforward, and the button to refuse is located below, making it more convenient to deny access to the IDFA. <a href="https://mobiledevmemo.com/mobile-advertising-without-the-idfa-a-comprehensive-overview/" rel="noopener noreferrer">Most experts</a> agree that 9 out of 10 people will most probably not opt in. After the initial declaration, Apple suggested a milder design for the popup, yet the idea remains largely the same and won’t bring fundamental changes.</p><p>Thus Apple is breaking the existing ad traffic attribution infrastructure under the pretext of privacy concerns. And this will affect everyone: ad networks systems, mobile developers, advertisers, and users.</p><p><strong><strong># 2. Lack of access to IDFA will lead to a decrease in the quality of mobile traffic attribution and an increase in the cost of user acquisition.</strong></strong></p><p>Previously, mobile developers and ad networks could use the IDFA without the explicit consent of the user. But now, the situation is radically changing:</p><p>1. Without access to IDFA, mobile ad attribution services (Appsflyer, Adjust and others) will no longer be able to trace back a significant portion of mobile traffic. It is important to understand that IDFA is now the primary accurate attribution tool. Appsflyer, Adjust and others will be forced to switch to less accurate and less efficient methods of determining the source of installs (e.g., device fingerprinting).</p><p>2. This will reduce the accuracy of traffic attribution, which will complicate things for companies developing and promoting mobile apps. In the future, it could lead to an increase in the cost of attribution.</p><p>3. We can expect a rise in acquisition costs as accurate targeting will become much more limited. Such popular and effective tools as lookalike audiences and retargeting will now be available only for a small portion of users who agreed to honor a request to provide access to the IDFA or used a relevant email or phone number while signing up.</p><p><strong><strong># 3. Apple presented its own attribution system, but it still doesn’t cover all mobile developers’ needs.</strong></strong></p><p>Apple offered the market an alternative, privacy-friendly traffic attribution system. This system makes it possible to send information about installs to advertising networks without explicitly revealing information about the user. But, unfortunately, the capabilities of this system are severely limited and don’t cover basic marketing needs.</p><p>One of the biggest problems is that developers and ad systems will no longer have access to user-level data. They will only see aggregated data in the account.</p><p>Developers will no longer be able to calculate and segment ROI or link attribution data to product events.</p><p><strong><strong>#4. Impact of IOS 14 Changes: rising user acquisition costs, accelerating mobile market consolidation, difficulties for large advertising networks and ad-attribution services</strong></strong></p><p>It’s hard to predict the results of this change. But here are some possible scenarios:</p><ul><li>As iOS 14 takes over (users will be gradually updating their devices to a newer iOS version with a IDFA), the cost of user acquisition will increase. The key factors here will be a lower quality of attribution and limited access to ad-targeting tools like lookalike audiences and retargeting.</li><li>Companies that provide attribution services will find themselves in a difficult position. Prior to iOS 14, they held a central position in the mobile advertising market. Without these companies developers wouldn’t be able to run mobile marketing efficiently. Apple’s decision will shatter their positions in the market. And there’s a possibility that Google will follow suit.</li><li>Large ad networks will also suffer. In its <a href="https://www.facebook.com/audiencenetwork/news-and-insights/preparing-audience-network-for-ios14/" rel="noopener noreferrer">latest report</a>, Facebook has already declared that it sees the changes in iOS 14 as a risk to its advertising business. Google, Twitter, Snapchat, Tiktok, and other big players have voiced <a href="https://www.facebook.com/audiencenetwork/news-and-insights/preparing-audience-network-for-ios14/" rel="noopener noreferrer">similar concerns.</a></li><li>It will become even more difficult for small players to compete with large publishers on the mobile market. They will lose the ability to accurately calculate ROI for ad campaigns. Without the ability to precisely link users’ payments to ad campaigns, it will be impossible to calculate the profit from acquired users. For small players, ineffective marketing can be a disaster, and many will have to be much more careful or even abandon paid channels. Big players have greater error tolerance and more tools to solve the emerging problems, although the problem will be no less relevant to them. Due to this asymmetry, it makes sense to expect an acceleration of the consolidation process on the mobile market.</li></ul><p><strong><strong>If you want to understand in more details what and why will happen, then here is what we will discuss further on:</strong></strong></p><ul><li>How traffic attribution currently works for mobile apps and why IDFA is so important?</li><li>What exactly will change in iOS 14?</li><li>What is the alternative Apple is offering to replace IDFA and existing mobile traffic attribution mechanisms?</li><li>Why do we expect an increase in user acquisition costs?</li><li>What do the leaders of the mobile market say?</li><li>When can we expect this change to take place?</li><li>How to prepare your app for iOS 14?</li><li>What will be the broader implications for mobile advertising and attribution?</li><li>How traffic attribution currently works for mobile apps, and why IDFA is so important?</li></ul><h2 id="how-traffic-attribution-works-for-mobile-apps"><strong><strong>How traffic attribution works for mobile apps</strong></strong></h2><p>Traffic attribution helps find out where a particular user came from. This is a critical task for performance marketing, as without high-quality attribution, it is impossible to determine which advertising campaigns are profitable (i.e., are making money) and which are not.</p><p>On the web, attribution is tackled in a simple way – we just need to add special parameters (usually utm-parameters) to the ad links leading to the site.</p><p>This scheme doesn’t work with mobile apps largely because mobile app stores add an intermediate step to the process and do not provide information about where the user came from.</p><p>Another reason is the policy of a number of leading advertising systems on the market. For example, Facebook doesn’t allow you to add any parameters when promoting mobile apps through its ad network.</p><p>Therefore, there are other methods for traffic attribution for mobile apps out there. Say you have a mobile app or a mobile game. To acquire users to your app, you purchase ad traffic. This is what happens in order to link ad clicks to an app’s install:</p><ul><li>The app developer has to integrate the SDK of a mobile attribution service into the app to track the traffic source of new users. Examples include AppsFlyer, Adjust, and Kochava.</li><li>The developer purchases ads in the advertising network, while using special links from the traffic-tracking service (Appsflyer, Adjust, etc.).</li><li>After clicking on the ad, the user is redirected to a special page, where various information is collected about him, including IDFA and his traffic source (it is transmitted from the advertising network, while access to IDFA is limited on the web). The user is then redirected to the app’s page in App Store or Google Play. Users will see none of this happen.</li><li>When a new user launches the app for the first time, the information about this action is sent to the mobile attribution service (AppsFlyer, Adjust, Kochava), which tries to find a match between the data received and the data collected at the previous step. If there is a match, then this user is attributed to the corresponding ad campaign. Absent a match, the traffic is considered organic.</li><li>The developer must set up a postback from a partner to the ad network, so that the ad system understands which campaigns are working well and which are not, and can optimize the display of ads on its side.</li><li>In some ad networks, the logic differs slightly. For example, the logic is different for Facebook, which is directly integrated with Appsflyer, Adjust and other providers.</li></ul><p>If you are not familiar with this topic, then here is a good <a href="https://www.appagent.co/blog/2019/01/09/the-principles-of-mobile-ad-attribution-analytics-and-tracking/" rel="noopener noreferrer">essay</a>.</p><figure><img src="https://lh4.googleusercontent.com/UfaVT2cgGlrEBil86RyIVr4tzeJi-pS3YLNVluew3DvgRDJ0lYjWotReEUaOZdVs8j8B_2RO09oNWdcplvBLW6agdeLEqyrSiZOMb7BFaHZZlAw-DxeYpbC9niIoi22u5hH8x0NM"></figure><h2 id="why-idfa-is-a-central-element-of-mobile-traffic-attribution">Why IDFA is a central element of mobile traffic attribution</h2><p>As is evident in the process described above, data about users who click on ads is key to the logic behind mobile traffic attribution systems. The IDFA is the central element of the data collected, and if IDFA is missing, the accuracy of this attribution method will drop <a href="https://medium.com/@gsimmons/your-attribution-may-be-more-wrong-than-right-903cde0ce4ca#:~:text=Fingerprinting%20accuracy%3A,result%20as%20a%20deterministic%20match)." rel="noopener noreferrer">dramatically.</a></p><p>Without IDFA, …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.adapty.io/ios-14-idfa-attribution-a-global-change-in-the-mobile-advertising-market/">https://blog.adapty.io/ios-14-idfa-attribution-a-global-change-in-the-mobile-advertising-market/</a></em></p>]]>
            </description>
            <link>https://blog.adapty.io/ios-14-idfa-attribution-a-global-change-in-the-mobile-advertising-market/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25044000</guid>
            <pubDate>Tue, 10 Nov 2020 09:02:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Messaging via SEPAtransfercomments [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25043965">thread link</a>) | @zanfr
<br/>
November 10, 2020 | https://franzkruhm.com/SEPA2020.pdf | <a href="https://web.archive.org/web/*/https://franzkruhm.com/SEPA2020.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://franzkruhm.com/SEPA2020.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25043965</guid>
            <pubDate>Tue, 10 Nov 2020 08:54:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Productivity Guide: All You Need to Know to Be Efficient]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25043892">thread link</a>) | @iuliangulea
<br/>
November 10, 2020 | https://iuliangulea.com/productivity/ | <a href="https://web.archive.org/web/*/https://iuliangulea.com/productivity/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><img src="https://iuliangulea.com/images/productivity.png" alt="Productivity"></p><h2 id="productivity-definition">Productivity Definition</h2><blockquote><p>Productivity is a measure of the efficiency of a person to perform a specific task.</p></blockquote><p>We often think it is a state of constant efficiency that allows us to do everything faster and better, but this is wrong. <em>Productivity is a measurement per individual task.</em></p><p>If you try to measure productivity as the number of tasks you accomplish daily, you still have to count in each individual assignment, which endorses the idea that productivity is a measurement per task.</p><h2 id="productivity-from-within-and-without">Productivity From Within And Without</h2><p>There are different strategies for productivity, and all those strategies can be classified into two main categories: <strong>external productivity</strong> and <strong>internal productivity.</strong></p><p><strong>External productivity</strong> is what usually people put their emphasis on. Also, it is what all those products, apps, and services promise to deliver. External productivity is all about automation, better tools, frameworks, software, and anything that allows you to perform your work better and faster. It has a significant potential to improve your performance by taking advantage of technology and advanced tools.</p><p><strong>Internal productivity,</strong> on the other hand, is often overlooked. Unlike the external one, internal productivity is all about your cognitive and physical performance, about your ability to focus, sustain your attention on the task at hand, manage your energy, and, generally speaking, understand how your body and mind works.</p><h2 id="gaining-productivity-expertise--the-pyramid-of-mastery">Gaining Productivity Expertise — The Pyramid Of Mastery</h2><p><img src="https://iuliangulea.com/images/the-pyramid-of-mastery/the-pyramid-of-mastery-1.png" alt="Pyramid of Mastery"></p><p><a href="https://iuliangulea.com/pyramid-of-mastery/">The Pyramid of Mastery</a> is a model that defines any domain in terms of 4 categories:</p><p><strong>Elements</strong> are the fundamental building blocks that make up a domain. In productivity, elements are abstract: focus, attention, working memory, sensory channels, etc.</p><p><strong>Rules</strong> are the laws by which the elements interact with each other and general principles that govern a domain. Some rules are: goal-directed attention is easily distracted, senses have a different throughput, working memory cannot perform two tasks simultaneously (hence multitasking is a myth), etc.</p><p><strong>Tools</strong> are the instruments that help you operate with the Elements and Rules. The majority of productivity tools nowadays are software apps. One of the best productivity tools is pen and paper.</p><p><strong>Frameworks</strong> are a combination of the previous layers. A Framework is a layer of abstraction that hides the underlying fundamentals behind a friendly facade that is easy to use to achieve a specific goal. Some frameworks in productivity are office suites and various productivity methods (e.g., <a href="https://en.wikipedia.org/wiki/Pareto_principle">80/20 Rule</a>, <a href="https://en.wikipedia.org/wiki/Time_management#The_Eisenhower_Method">The Eisenhower Method</a>, and others).</p><p>All four layers together allow you to be an expert in your field. Leave one out, and there will always be something you do not fully understand. And when you don’t understand something, you cannot be fully efficient at it.</p><p>If you would like to find out more about how the Pyramid of Mastery applies to the field of productivity, I wrote a separate <a href="https://iuliangulea.com/pyramid-of-mastery/productivity/">article</a> on that topic.</p><h2 id="productivity-approaches">Productivity Approaches</h2><p>Generally speaking, you can be more productive by taking one or several of the following approaches:</p><p><strong>1. Delegate/Outsource the task.</strong> This approach is the most efficient from the time standpoint as it frees all your time and allows you to focus on other tasks. Although it is limited in how much you can delegate/outsource, it is crucial to keep in mind that you can and need to delegate.</p><p><strong>2. Automate the task.</strong> If you cannot delegate/outsource, think about whether your work on a task can be either fully or partially automated. There are lots of services, products, and programs that can do the work for you in a broad range of areas. If their cost is smaller than the value of the time you can save using them, then do it.</p><p><strong>3. Use better tools and learn them well.</strong> If automation is also not an option, then it means <em>you</em> need to do it. Having good tools is crucial if you want to be productive. A thorough understanding of their functions can make a big difference. In the case of software, learn its features and the shortcuts of the most frequently used functionality by you.</p><p><strong>4. Understand cognitive processes and learn what works best for you.</strong> Whenever we need to perform mental work, understanding <em>how</em> our <a href="https://iuliangulea.com/human-senses/">senses</a>, <a href="https://iuliangulea.com/attention/">attention</a>, <a href="https://iuliangulea.com/working-memory/">working memory</a>, and other relevant processes work can make a huge difference. As a plant flourishes, when the conditions are right, our brains can be incredibly performant whenever we offer them the right environment in which they can function.</p><h2 id="my-top-productivity-strategies">My Top Productivity Strategies</h2><p><strong>The Rule Of Threes.</strong> This is a meta strategy I came up with some time ago that help me make the right decisions when it comes to taking on new opportunities. No matter how productive you are, if you have too much on your plate, your attention and energy will split into too many places, and that will affect your productivity. The rule is simple: at any one point in time, I should have no more than three ongoing projects, and by an ongoing project, I mean any work that spans for longer than one week. Having more than that will scatter your energy and attention to the detriment of effectiveness.</p><p><strong>Plan Your Day In Advance.</strong> It is much easier to follow a predefined list of steps rather than having only the destination in mind and think about your next course of action after each task. The 10–15 minutes spent in the evening to decide and prioritize what you will work on will save you plenty of energy and time the following day.</p><h2 id="more-productivity-tips-for-every-day">More Productivity Tips For Every Day</h2><p><strong>1. Reduce Distractions As Much As Possible.</strong> If there is something that can distract you, sooner or later, it will distract you. Therefore, if you want to keep focused for a longer time, remove as many distractions as you can. This includes visual distractions on your table and screen (yes, those Facebook and Twitter tabs are hooking your attention pretty easy, aren’t they?), audial distractions (buy yourself a good pair of noise-canceling headphones), and other types of disturbances that distract you regularly.</p><p><strong>2. Put Your Phone Away.</strong> Although it is also a distraction, this tip deserves a separate mention. Put your phone on silent mode and away from your sight (not in your pocket). All those sounds (including notifications, calls, etc.) and flashes are nothing else than stimuli that have their primary goal to grab your attention and distract you from the thing you are focused on. It is also essential to put the phone away, as having it in your area of sight will also urge you to grab it when you see it on the table.</p><p><strong>3. Split Your Tasks Into Manageable Chunks.</strong> If an assignment is too big for you to comprehend, consider splitting it into several smaller subtasks until you get them of a size that you can easily accomplish. A positive side-effect of this is that smaller tasks provide a sense of progress, positively affecting your overall state and mood.</p><p><strong>4. Use Good Tools And Learn Them Properly.</strong> If you use software tools, learn the shortcuts of the programs you work in as it will save you dozens of hours within a year. If you use physical tools, buy high-quality tools, as they will pay off multiple times.</p><p><strong>5. Your Energy Is More Important Than The Allocated Time.</strong> Time Management is overrated. It’s not that timing your tasks is not essential, but <em>time is absolute.</em> It is independent of anything. Consider your energy levels when planning your tasks. Your energy is what matters when working on a job. You can spend 2 hours banging your head against something in the evening when you are tired and then complete that task in 30 minutes the next morning. Know when you are more productive and work on the most important tasks then.</p><p><strong>6. Use Visual Aids.</strong> A pen and a piece of paper are sometimes the best, simple, and most efficient productivity tools you can use. If the task you are working on relies on manipulating multiple pieces of information at a time, write them on paper or draw a diagram. That will free up resources necessary to store them in your working memory so that you can focus on processing and manipulating them instead. This will also involve your visual sense, allowing you to make more potentially relevant connections between ideas.</p><h2 id="all-productivity-articles">All Productivity Articles</h2><p>These are all articles I have written on productivity. Enjoy!</p><ul><li><a href="https://iuliangulea.com/keyboard-shortcuts/">6 Shortcuts That Save Me 62 Hours Each Year</a></li><li><a href="https://iuliangulea.com/pyramid-of-mastery/productivity/">The Ultimate Productivity Guide — Scientifically Proven Techniques To Get Things Done</a></li><li><a href="https://iuliangulea.com/attention/">The Dual Nature Of Attention — 5 Ways To Stay Less Distracted And Be More Productive</a></li><li><a href="https://iuliangulea.com/working-memory/">How People Learn — Working Memory And The 3 Basic Rules Of Productivity</a></li><li><a href="https://iuliangulea.com/the-most-substantial-word/">Your Name — The Most Substantial Word</a></li><li><a href="https://iuliangulea.com/how-i-automated-things/">How I Saved 14 Hours Of Working Time Each Month</a></li><li><a href="https://iuliangulea.com/one-percent-rule/">The One Percent Rule - How Tiny Changes Can Bring Big Results</a></li><li><a href="https://iuliangulea.com/team-processes-what/">Increase Your Team’s Productivity by Establishing Processes - Part III</a></li><li><a href="https://iuliangulea.com/team-processes-how/">Increase Your Team’s Productivity by Establishing Processes - Part II</a></li><li><a href="https://iuliangulea.com/team-processes-why/">Increase Your Team’s Productivity by Establishing Processes - Part I</a></li></ul><hr></div></div>]]>
            </description>
            <link>https://iuliangulea.com/productivity/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25043892</guid>
            <pubDate>Tue, 10 Nov 2020 08:38:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Smartphone Upgrades Are Impacting the Environment]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25043889">thread link</a>) | @scottbucks
<br/>
November 10, 2020 | https://www.thedetechtor.com/post/smartphones-the-true-cost-of-upgrades | <a href="https://web.archive.org/web/*/https://www.thedetechtor.com/post/smartphones-the-true-cost-of-upgrades">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-hook="post-description"><article><div><div data-rce-version="8.3.0"><div dir="ltr"><div><div id="viewer-16t2j"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img"><p><img data-pin-url="https://www.thedetechtor.com/post/smartphones-the-true-cost-of-upgrades" data-pin-media="https://static.wixstatic.com/media/f361a8_da6af0458bd5469fb370e704386f86a3~mv2.jpeg/v1/fit/w_1000%2Ch_635%2Cal_c%2Cq_80/file.png" src="https://static.wixstatic.com/media/f361a8_da6af0458bd5469fb370e704386f86a3~mv2.jpeg/v1/fit/w_300,h_300,al_c,q_5/file.jpeg"></p></div><p><span dir="auto">Photo by Daniel Romero on Unsplash</span></p></div></div></div><p id="viewer-7arqh"><span>Smartphone technology is evolving rapidly. Every year there are better cameras, performance, refresh rates, screens and batteries. Tempted by a host of new features, people can't help but upgrade to the latest model, but what happens to all the smartphones we go through?

</span></p><p id="viewer-5o3kt"><span>The average lifespan of a smartphone is 3 to 4 years, perhaps even 5, but by that time the battery's capacity is likely to have decreased significantly. After an average lifespan has been reached, most people will throw away their smartphone and upgrade to a more recent model.</span></p><blockquote id="viewer-cmiam"><span><em>On average only 12,5% of electronic waste is recycled, with approximately 20 to 50 million metric tons of e-waste disposed of worldwide every year.</em></span></blockquote><p id="viewer-74ml0"><span>This is a huge problem for the environment due to the chemicals in these devices leaching into the groundwater system from landfills, polluting the land, water and air.</span></p><div id="viewer-bm1b6"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img"><p><img data-pin-url="https://www.thedetechtor.com/post/smartphones-the-true-cost-of-upgrades" data-pin-media="https://static.wixstatic.com/media/f361a8_e577402e314d40939016a7c158736c28~mv2.jpeg/v1/fit/w_794%2Ch_528%2Cal_c%2Cq_80/file.png" src="https://static.wixstatic.com/media/f361a8_e577402e314d40939016a7c158736c28~mv2.jpeg/v1/fit/w_300,h_300,al_c,q_5/file.jpeg"></p></div></div></div></div><p id="viewer-87pmv"><span>With companies constantly encouraging people to upgrade by stopping updates for older models, it renders them obsolete.

</span></p><p id="viewer-31crc"><span>Not only is this a problem for the environment, but by throwing away these devices, we are wasting precious metals such as copper, silver, gold, palladium and other raw materials, that would require significant resources to mine and manufacture.</span></p><div id="viewer-d1tql"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img"><p><img data-pin-url="https://www.thedetechtor.com/post/smartphones-the-true-cost-of-upgrades" data-pin-media="https://static.wixstatic.com/media/nsplsh_6a58643246537663527238~mv2.jpg/v1/fit/w_1000%2Ch_1000%2Cal_c%2Cq_80/file.png" src="https://static.wixstatic.com/media/nsplsh_6a58643246537663527238~mv2.jpg/v1/fit/w_300,h_300,al_c,q_5/file.jpg"></p></div></div></div></div><p id="viewer-bqd9o"><span>
This is why it is important to recycle old cell phones and preserve these increasingly scarce materials where possible.


</span></p><p id="viewer-c1me1"><span>Here are some suggestions on how to alleviate these problems:

</span></p><ol><li id="viewer-4p3cu"><p>Instead of buying a new phone, why not change the battery? Often the smartphone is still in good condition.</p></li><li id="viewer-bqu6i"><p>Once you have had your smartphone for several years and have already changed the battery, you could recycle it and buy a new one. Some companies offer trade-ins fo credit to use on your next phone.</p></li><li id="viewer-16hl3"><p>You could buy a refurbished product; they are often as good as brand new with the added benefit it helps the environment and saves you money.</p></li><li id="viewer-1aql"><p>Instead of throwing the phone away you could sell it, or give it to a friend/family member.</p></li></ol><p id="viewer-26bo7"><span>The bottom line is currently we change smartphones too often. There is no specific amount of time that you should keep the same phone but when changing, think about where your phone may end up if you don't recycle it, sell it or pass it on to someone else.</span></p><p id="viewer-5oobg"><span>If you enjoyed this article, why not consider subscribing to our newsletter, or check out some of our <a href="http://thedetechtor.com/all-news" target="_blank" rel="noopener"><u>other posts</u></a><u>.</u> </span></p></div></div></div></div></article></div></div>]]>
            </description>
            <link>https://www.thedetechtor.com/post/smartphones-the-true-cost-of-upgrades</link>
            <guid isPermaLink="false">hacker-news-small-sites-25043889</guid>
            <pubDate>Tue, 10 Nov 2020 08:37:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[This is how I git]]>
            </title>
            <description>
<![CDATA[
Score 80 | Comments 28 (<a href="https://news.ycombinator.com/item?id=25043731">thread link</a>) | @ingve
<br/>
November 10, 2020 | https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/ | <a href="https://web.archive.org/web/*/https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>Every now and then I get questions on how to work with git in a smooth way when developing, bug-fixing or extending curl – or how I do it. After all, I <a href="https://daniel.haxx.se/blog/2020/10/26/working-open-source/" data-type="post" data-id="14901">work on open source full time</a> which means I have very frequent interactions with git (and GitHub). Simply put, I work with git all day long. Ordinary days, I issue git commands several hundred times.</p>



<p>I have a very simple approach and way of working with git in curl. This is how it works.</p>



<h2>command line</h2>



<p>I use git almost exclusively from the command line in a terminal. To help me see which branch I’m working in, I have this little bash helper script.</p>



<pre>brname () {
  a=$(<code>git rev-parse --abbrev-ref HEAD 2&gt;/dev/null</code>)
  if [ -n "$a" ]; then
    echo " [$a]"
  else
    echo ""
  fi
}
PS1="\u@\h:\w\$(brname)$ "</pre>



<p>That gives me a prompt that shows username, host name, the current working directory and the current checked out git branch.</p>



<p>In addition: I use Debian’s <a href="https://salsa.debian.org/debian/bash-completion/-/blob/master/README.md">bash command line completion</a> for git which is also really handy. It allows me to use tab to complete things like git commands and branch names. </p>



<h2>git config</h2>



<p>I of course also have my customized <code>~/.gitconfig</code> file to provide me with some convenient aliases and settings. My most commonly used git aliases are:</p>


<pre title="">st = status --short -uno
ci = commit
ca = commit --amend
caa = commit -a --amend
br = branch
co = checkout
df = diff
lg = log -p --pretty=fuller --abbrev-commit
lgg = log --pretty=fuller --abbrev-commit --stat
up = pull --rebase
latest = log @^{/RELEASE-NOTES:.synced}..
</pre>


<p>The ‘latest’ one is for listing all changes done to curl since the most recent RELEASE-NOTES “sync”. The others should hopefully be rather self-explanatory.</p>



<p>The config also sets <code>gpgsign = true</code>, enables mailmap and a few other things.</p>



<h2>master is clean and working</h2>



<p>The main curl development is done in the single <a href="https://github.com/curl/curl">curl/curl</a> git repository (primarily hosted on GitHub). We keep the master branch the bleeding edge development tree and we work hard to always keep that working and functional. We do our releases off the master branch when that day comes (every eight weeks) and we provide “<a href="https://curl.haxx.se/snapshots/">daily snapshots</a>” from that branch, put together – yeah – daily.</p>



<p>When merging fixes and features into master, we avoid merge commits and use rebases and fast-forward as much as possible. This makes the branch very easy to browse, understand and work with – as it is 100% linear.</p>



<h2>Work on a fix or feature</h2>



<p>When I start something new, like work on a bug or trying out someone’s patch or similar, I first create a local branch off master and work in that. That is, I don’t work directly in the master branch. Branches are easy and quick to do and there’s no reason to shy away from having loads of them!</p>



<p>I typically name the branch prefixed with my GitHub user name, so that when I push them to the server it is noticeable who is the creator (and I can use the same branch name locally as I do remotely).</p>



<pre>$ git checkout -b bagder/my-new-stuff-or-bugfix</pre>



<p>Once I’ve reached somewhere, I commit to the branch. It can then end up one or more commits before I consider myself “done for now” with what I was set out to do.</p>



<p>I try not to leave the tree with any uncommitted changes – like if I take off for the day or even just leave for food or an extended break. This puts the repository in a state that allows me to easily switch over to another branch  when I get back – should I feel the need to. Plus, it’s better to commit and explain the change <em>before</em> the break rather than having to recall the details again when coming back.</p>



<h2>Never stash</h2>



<p>“git stash” is therefore not a command I ever use. I rather create a new branch and commit the (temporary?) work in there as a potential new line of work.</p>



<h2>Show it off and get reviews</h2>



<p>Yes I am the lead developer of the project but I still maintain the same work flow as everyone else. All changes, except the most minuscule ones, are done as pull requests on GitHub.</p>



<p>When I’m happy with the functionality in my local branch. When the bug seems to be fixed or the feature seems to be doing what it’s supposed to do and the test suite runs fine locally.</p>



<p>I then clean up the commit series with “<code>git rebase -i</code>” (or if it is a single commit I can instead use just “<code>git commit --amend</code>“).</p>



<p>The commit series should be a set of logical changes that are related to this change and not any more than necessary, but kept separate if they are separate. Each commit also gets its own proper commit message. Unrelated changes should be split out into its own separate branch and subsequent separate pull request.</p>



<pre>git push origin bagder/my-new-stuff-or-bugfix</pre>



<h2>Make the push a pull request</h2>



<p>On GitHub, I then make the newly pushed branch into a <a href="https://github.com/curl/curl/pulls">pull request</a> (aka “a PR”). It will then become visible in the list of pull requests on the site for the curl source repository, it will be announced in the #curl IRC channel and everyone who follows the repository on GitHub will be notified accordingly.</p>



<p>Perhaps most importantly, a pull request kicks of a flood of CI jobs that will build and test the code in numerous different combinations and on several platforms, and the results of those tests will trickle in over the coming hours. When I write this, we have around 90 different CI jobs – per pull request – and something like 8 different code analyzers will scrutinize the change to see if there’s any obvious flaws in there.</p>



<figure><a href="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/Screenshot_2020-11-05-curl-Project-status-dashboard.png"><img loading="lazy" width="2686" height="1510" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/Screenshot_2020-11-05-curl-Project-status-dashboard.png" alt=""></a><figcaption>CI jobs per platform over time. Graph snapped on November 5, 2020</figcaption></figure>



<h2>A branch in the actual curl/curl repo</h2>



<p>Most contributors who would work on curl would not do like me and make the branch in the curl repository itself, but would rather do them in their own forked version instead. The difference isn’t that big and I <em>could</em> of course also do it that way.</p>



<h2>After push, switch branch</h2>



<p>As it will take some time to get the full CI results from the PR to come in (generally a few hours), I switch over to the next branch with work on my agenda. On a normal work-day I can easily move over ten different branches, polish them and submit updates in their respective pull-requests.</p>



<p>I can go back to the&nbsp;master branch again with ‘<code>git checkout master</code>‘ and there I can “<code>git pull</code>” to get everything from upstream – like when my fellow developers have pushed stuff in the mean time.</p>



<h2>PR comments or CI alerts</h2>



<p>If a reviewer or a CI job find a mistake in one of my PRs, that becomes visible on GitHub and I get to work to handle it. To either fix the bug or discuss with the reviewer what the better approach might be.</p>



<p>Unfortunately, flaky CI jobs is a part of life so very often there ends up one or two red markers in the list of CI jobs that can be ignored as the test failures in them are there due to problems in the setup and not because of actual mistakes in the PR…</p>



<p>To get back to my branch for that PR again, I “<code>git checkout bagder/my-new-stuff-or-bugfix</code>“, and fix the issues.</p>



<p>I normally start out by doing follow-up commits that repair the immediate mistake and push them on the branch:</p>



<pre>git push origin <code>bagder/my-new-stuff-or-bugfix</code></pre>



<p>If the number of fixup commits gets large, or if the follow-up fixes aren’t small, I usually end up doing a squash to reduce the number of commits into a smaller, simpler set, and then force-push them to the branch.</p>



<p>The reason for that is to make the patch series easy to review, read and understand. When a commit series has too many commits that changes the previous commits, it becomes hard to review.</p>



<h2>Ripe to merge?</h2>



<p>When the pull request is ripe for merging (independently of who authored it), I switch over to the master branch again and I merge the pull request’s commits into it. In special cases I cherry-pick specific commits from the branch instead. When all the stuff has been yanked into master properly that should be there, I push the changes to the remote.</p>



<p>Usually, and especially if the pull request wasn’t done by me, I also go over the commit messages and polish them somewhat before I push everything. Commit messages should follow our style and mention not only which PR that it closes but also which issue it fixes and properly give credit to the bug reporter and all the helpers – using the right syntax so that our automatic tools can pick them up correctly!</p>



<p>As already mentioned above, I merge fast-forward or rebased into master. No merge commits.</p>



<h2>Never merge with GitHub!</h2>



<p>There’s a button GitHub that says “rebase and merge” that could theoretically be used for merging pull requests. I <em>never</em> use that (and if I could, I’d disable/hide it). The reasons are simply:</p>



<ol><li>I don’t feel that I have the proper control of the commit message(s)</li><li>I can’t select to squash a subset of the commits, only all or nothing</li><li>I often want to cleanup the author parts too before push, which the UI doesn’t allow</li></ol>



<p>The downside with not using the merge button is that the message in the  PR says “closed by [hash]” instead of “merged in…” which causes confusion to a fair amount of users who don’t realize it means that it actually means the same thing! I consider this is a (long-standing) GitHub UX flaw.</p>



<h2>Post merge</h2>



<p>If the branch has nothing to be kept around more, I delete the local branch again with “<code>git branch -d [name]</code>” and I remove it remotely too since it was completely merged there’s no reason to keep the work version left.</p>



<p>At any given point in time, I have some 20-30 different local branches alive using this approach so things I work on over time all live in their own branches and also submissions from various people that haven’t been merged into master yet exist in branches of various maturity levels. Out of those local branches, the number of concurrent pull requests I have in progress can be somewhere between just a few up to ten, twelve something.</p>



<h2>RELEASE-NOTES</h2>



<p>Not strictly related, but in order to keep interested people informed about what’s happening in the tree, we sync the <a href="https://github.com/curl/curl/blob/master/RELEASE-NOTES">RELEASE-NOTES</a> file every once in a while. Maybe every 5-7 days or so. It …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/">https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/</a></em></p>]]>
            </description>
            <link>https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25043731</guid>
            <pubDate>Tue, 10 Nov 2020 08:08:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Event, 18th Nov: Building a Notion Website Live]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25043693">thread link</a>) | @saviorand
<br/>
November 9, 2020 | http://optemization.com/how-to-build-notion-website | <a href="https://web.archive.org/web/*/http://optemization.com/how-to-build-notion-website">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><article id="block-how-to-build-notion-website"><div id="block-128d927961c546d8870b1a51a5579a93"><picture><source srcset="https://api.super.so/asset/optemization.com/67b219ac-6de7-482c-b615-91d148315e54.png?w=750&amp;f=webp" media="(max-width: 414px)" type="image/webp"><source src="https://api.super.so/asset/optemization.com/67b219ac-6de7-482c-b615-91d148315e54.png?w=750" media="(max-width: 414px)"><source srcset="https://api.super.so/asset/optemization.com/67b219ac-6de7-482c-b615-91d148315e54.png?w=1500&amp;f=webp" type="image/webp"><img src="https://api.super.so/asset/optemization.com/67b219ac-6de7-482c-b615-91d148315e54.png?w=1500" alt="image" loading="lazy"></picture></div><h2 id="block-d2269dc812f14b90932490290a797437"><span id="d2269dc812f14b90932490290a797437"></span><span><span>🖥️ You can do websites on Notion?</span></span></h2><blockquote id="block-2d6de681f9a04563af1656151a4a72ac"><span><span>If you spend anytime on #productivity Twitter or r/Notion you know that building websites on Notion, is the new normal :)  Thanks to projects like Super and Fruition, Notion pages can turn into real websites, with custom domains, analytics, and styling!

My new teammate Valentine, just shared a </span><span><a target="_blank" rel="noopener noreferrer" href="https://optemization.com/notion-landing-page-guide">comprehensive guide</a></span><span> on how to build your own Notion website. 

However, this stuff is really visual, so we thought it'd be super fun to host an </span><span><strong>event where we conceptualize, design and ship a Notion website LIVE</strong></span><span>! 

So two things: RSVP below and tell us what kind of website do you want to build!</span></span></blockquote><h2 id="block-1b34e201f6484d4680c97fe88b965d4c"><span id="1b34e201f6484d4680c97fe88b965d4c"></span><span><span>🖋️ Sign Up</span></span></h2><h2 id="block-b67496c5fc8b4bdcb04d5891dc6baf0b"><span id="b67496c5fc8b4bdcb04d5891dc6baf0b"></span><span><span>😃 Are you excited?</span></span></h2><div id="block-c91d2a953127494f86a21d77c34339ea"><div id="block-f2134537e8d04052915c6399871b09eb"><blockquote id="block-7d5a84aa13624e43ba7e69fbac453dea"><span><span>Share the event on the ze twitter 🙏</span></span></blockquote></div></div></article></div></div></div>]]>
            </description>
            <link>http://optemization.com/how-to-build-notion-website</link>
            <guid isPermaLink="false">hacker-news-small-sites-25043693</guid>
            <pubDate>Tue, 10 Nov 2020 07:56:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[2020 Haskell Is Ready for Prime Time]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25043675">thread link</a>) | @_query
<br/>
November 9, 2020 | https://ihp.digitallyinduced.com/ShowPost?postId=14ed1d41-5ea4-4608-9c96-465443cd6e55 | <a href="https://web.archive.org/web/*/https://ihp.digitallyinduced.com/ShowPost?postId=14ed1d41-5ea4-4608-9c96-465443cd6e55">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><em>by Marc Scholten, 29.10.2020</em></p>

<p>
There’s been a recent blog post <a href="https://www.snoyman.com/blog/2020/10/haskell-bad-parts-1" target="_blank"><q>Haskell: The Bad Parts</q></a> in the haskell community. To keep things in balance and to spread some positive vibes we should also talk about the good parts of the haskell programming language and it’s ecosystem.
</p>

<p>
Here are some of the best parts we encountered while using Haskell at digitally induced. We focus on the advantages in the web dev space because that is what we are currently working on.
</p>

<h2>Type Safety</h2>
<p>
Haskell has one of the most impressive type systems of any programming language in practical use. If you have used TypeScript or other type safe languages in the past, you should be aware of the great advantages of having a type-checked codebase. Now think TypeScript - but 10x better. That’s how Haskell feels like.
</p>

<p>
You save a lot of time debugging runtime errors. Once the compiler approved your code, you can be pretty sure that it is working. This kind of development process is usually a lot more fun than debugging why something is <code>null</code> or <code>undefined</code>.
</p>

<p>
Once your system has hit a certain size and when new feature requests are rolling in you will want to make changes and refactor some parts of your code base. With Haskell you feel empowered to make changes to any part of your codebase.
</p>

<p>
Compare this to the ruby ecosystem: When working with rails you usually need to have lots of tests or otherwise you cannot confidently refactor code after things are running in production. And even then things will break. With the power of the type safety provided by Haskell, we can make refactorings whenever we want.
</p>

<p>
It’s really a blessing.
</p>

<h2>Managed Side Effects</h2>
<p>The way you deal with the file system, external APIs and user input is way different in Haskell than in other less functional programming languages. Your program consists of a main routine that handles the side effects and calls all your pure functions that do the real business logic.</p>

<p>
Systems build this way scale really well because there are less moving parts. Additionally pure functions can be easily tested and changed later on.
</p>

<p>
Most other languages encourage you to do side effects in an unrestricted way. For example when working in Java, a call to an object method might indirectly change the state of many related objects. This means you cannot easily reason about what a method calls does. In Haskell most functions are pure and thus don't trigger side effects like this. And when they do you can see this already by the function's type signature.
</p>

<p>
Haskell forces you to manage your side effects in a more careful way. You can still do IO and have mutable state, you just need to make this explicit inside the type signature. This leads to a far more robust system in overall.
</p>

<h2>Performance</h2>
<p>
Out of the box the performance of Haskell based web applications is great. It just feels faster than your typical Rails or PHP application. Thanks to it’s highly optimized runtime system it can also <a href="https://www.yesodweb.com/blog/2011/03/preliminary-warp-cross-language-benchmarks" target="_blank">handle way more requests than a nodejs application</a>.
</p>

<p>
And you get all that without ever thinking about performance at all. 
</p>

<h2>Tooling</h2>
<p>In 2020 it’s finally good. Thanks to <a href="https://github.com/haskell/haskell-language-server" target="_blank">Haskell Language Server</a> there’s now an easy way to have type information, documentation on hover and smart refactorings inside your text editor.</p>

<p>
With nix, cabal and stack we have the best tools for managing Haskell dependencies. Cabal hell is a thing of the past.
</p>

<p>
Great things are also happening to the Haskell compiler itself. <a href="https://github.com/ghc-proposals/ghc-proposals/pull/282" target="_blank">We soon can write dot expressions as you know from most other programming languages:</a> <code>project.name</code> instead of <code>name project</code>.
</p>

<h2>Hiring Haskell Developers</h2>
<p>
Haskell is a secret super power in that regard. The Haskell community consists of many very smart and talented software engineers. Haskell developers usually learn about Haskell because they care about their craft and about building high quality software products instead of learning about it to get a high paying job. Exactly the kind of people you want in your team.
</p>

<h2>2020 Haskell is Ready for Prime Time</h2>
<p>
For years there has been this trend of growing use of type safety as well as the growing use of functional programming techniques. What language could fill this space better than Haskell. Haskell has really matured in the last years and in 2020 it feels like it’s finally ready to conquer the world.
</p>

<p>
If this post made you interested, <a href="https://ihp.digitallyinduced.com/" target="_blank">check out IHP, our batteries-included haskell web framework.</a>
</p></div></div>]]>
            </description>
            <link>https://ihp.digitallyinduced.com/ShowPost?postId=14ed1d41-5ea4-4608-9c96-465443cd6e55</link>
            <guid isPermaLink="false">hacker-news-small-sites-25043675</guid>
            <pubDate>Tue, 10 Nov 2020 07:50:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Poignant Guide to Ruby]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25043544">thread link</a>) | @creolabs
<br/>
November 9, 2020 | https://poignant.guide/book/chapter-2.html | <a href="https://web.archive.org/web/*/https://poignant.guide/book/chapter-2.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      

<h2>1. Opening This Book</h2>

<p>Pretend that you’ve opened this book (although you probably <em>have</em> opened this
book), just to find a huge onion right in the middle crease of the book. (The
manufacturer of the book has included the onion at my request.)</p>

<p>So you’re like, “Wow, this book comes with an onion!” (Even if you don’t
particularly like onions, I’m sure you can appreciate the logistics of shipping
any sort of produce discreetly inside of an alleged programming manual.)</p>

<p>Then you ask yourself, “Wait a minute. I thought this was a book on Ruby, the
incredible new programming language from Japan. And although I can appreciate
the logistics of shipping any sort of produce discreetly inside of an alleged
programming manual: Why an onion? What am I supposed to do with it?”</p>

<p>No. Please don’t puzzle over it. You don’t need to do anything with the onion.
Set the onion aside and let <em>it</em> do something with <em>you</em>.</p>

<p>I’ll be straight with you. I want you to cry. To weep. To whimper sweetly. This
book is a <strong>poignant</strong> guide to Ruby. That means code so beautiful that tears
are shed. That means gallant tales and somber truths that have you waking up the
next morning in the arms of this book. Hugging it tightly to you all the day
long. If necessary, fashion a makeshift hip holster for <em>Why’s (Poignant) Guide
to Ruby</em>, so you can always have this book’s tender companionship.</p>

<p>You really must sob once. Or at least sniffle. And if not, then the onion will
make it all happen for you.</p>





<h2>2. The Dog Story</h2>

<p>So try this first bit of poignancy on for size:</p>

<p>One day I was walking down one of those busy roads covered with car dealerships
(this was shortly after my wedding was called off) and I found an orphaned dog
on the road. A woolly, black dog with greenish red eyes. I was kind of feeling
like an orphan myself, so I took a couple balloons that were tied to a pole at
the dealership and I relocated them to the dog’s collar. Then, I decided he
would be my dog. I named him Bigelow.</p>

<p>We set off to get some Milkbones for Bigelow and, afterwards, head over to my
place, where we could sit in recliners and listen to Gorky’s Zygotic Mynci. Oh,
and we’d also need to stop by a thrift store and get Bigelow his own recliner.</p>

<p>But Bigelow hadn’t accepted me as his master. So five minutes later, the stupid
dog took a different crosswalk than I did and I never caught up. So whereas he
had previously only been lost once, he was now lost twice. I slowed my pace
towards the life of Milkbones and an extra recliner. I had a dog for five
minutes.</p>

<p>Stupid Benedict Arnold of a dog. I sat on a city bench and threw pine cones at a
statue of three sheep crossing a bridge. After that, I wept for hours. The tears
just came. Now there’s a little something poignant to get you started.</p>

<p>I wonder where he went with all those balloons. That crazy dog must have looked
like a party with legs.</p>

<p>It wasn’t much later that I pulled my own Bigelow. I printed out a bunch of
pages on Ruby. Articles found around the Web. I scanned through them on a train
ride home one day. I flipped through them for five minutes and then gave up. Not
impressed.</p>

<p>I sat, staring out the window at the world, a life-sized blender mixing graffiti
and iron smelts before my eyes. <em>This world’s too big for such a a little
language</em>, I thought. <em>Poor little thing doesn’t stand a chance. Doesn’t have
legs to stand on. Doesn’t have arms to swim.</em></p>

<p>And yet, there I was. One little man on a flimsy little train (and I even still
had a baby tooth to lose at the time) out of billions of people living on a
floating blue rock. How can I knock Ruby? Who’s to say that I’m not going to
happen to choke on my cell phone and die later that evening. Why’s dead, Ruby
lives on.</p>

<p>The gravestone:</p>

<blockquote>
  <p>What’s in his trachea? Oh, look, a Nokia!</p>
</blockquote>

<p>Just my luck. Finally get to have a good, long sleep underground, only to be
constantly disturbed by <em>Pachelbel’s Canon</em> going off in my stomach.</p>



<h2>3. The Red Sun Rises</h2>

<p>So, now you’re wondering why I changed my mind about Ruby. The quick answer is:
we clicked.</p>

<p>Like when you meet Somebody in college and they look like somebody who used to
hit you in the face with paintbrushes when you were a kid. And so, impulsively,
you conclude that this new Somebody is likely a non-friend. You wince at their
hair. You hang up phones loudly during crucial moments in their anecdotes. You
use your pogo stick right there where they are trying to walk!</p>

<p>Six months later, somehow, you and Somebody are sitting at a fountain having a
perfectly good chat. Their face doesn’t look so much like that childhood
nemesis. You’ve met the Good Twin. You clicked.</p>

<p>So whereas I should probably be pounding your teeth in with hype about Ruby and
the tightly-knit cadre of pertinent acronyms that accompany it everywhere
(whetting the collective whistles of your bosses and their bosses’ bosses),
instead I will just let you coast. I’ll let you free-fall through some code,
interjecting occasionally with my own heartfelt experiences. It’ll be quite
easy, quite natural.</p>

<p>I should offer you some sort of motivation, though. So, Smotchkkiss, I’m going
to give my three best reasons to learn Ruby and be done with it.</p>

<ol>
  <li>
    <p><strong>Brain health.</strong></p>

    <p>Vitamin R. Goes straight to the head. Ruby will teach you to <em>express</em> your
ideas through a computer. You will be writing stories for a machine.</p>

    <p>Creative skills, people. Deduction. Reason. Nodding intelligently. The
language will become a tool for you to better connect your mind to the world.
I’ve noticed that many experienced users of Ruby seem to be clear thinkers and
objective. (In contrast to: heavily biased and coarse.)</p>
  </li>
  <li>
    <p><strong>One man on one island.</strong></p>

    <p>Ruby was born in Japan. Which is freaky. Japan is not known for its
software. And since programming languages are largely written in English, who
would suspect a language to come from Japan?</p>

    <p>And yet, here we have Ruby. Against the odds, Yukihiro Matsumoto created
Ruby on February 24, 1993. For the past ten years, he has steadily brought Ruby
to a global audience. It’s triumphant and noble and all that. Support diversity.
Help us tilt the earth just a bit.</p>
  </li>
  <li>
    <p><strong>Free.</strong></p>

    <p>Using Ruby costs nothing. The code to Ruby itself is open for all of the
world to inhale/exhale. Heck, this book is free. It’s all part of a great, big
giveaway that should have some big hitch to it.</p>

    <p>You’d think we’d make you buy vacuums or timeshare or fake Monets. You’d
think there’d be a 90 minute presentation where the owner of the company comes
out at the end and knuckles you into sealing the deal.</p>

    <p>Nope, free.</p>
  </li>
</ol>

<p>With that, it’s time for the book to begin. You can now get out your highlighter
and start dragging it along each captivating word from this sentence on. I think
I have enough hairspray and funny money on my person to keep me sustained until
the final page.</p>



<h2>4. How Books Start</h2>

<p>Now, if you ever have read a book, you know that no book can properly start
without an exorbitant amount of synergy. Yes, synergy. Maybe you didn’t know
this. Synergy means that you and I are supposed to cooperate to make this a
great reading experience.</p>

<p>We start off the book by getting along well in the Introduction. This
togetherness, this <strong>synergy</strong>, propels us through the book, with me guiding you
on your way. You give me a reassuring nod or snicker to indicate your progress.</p>

<p>I’m Peter Pan holding your hand. Come on, Wendy! Second star to the right and on
till morning.</p>

<p>One problem here. I don’t get along well with people. I don’t hold hands very
well.</p>

<p>Any of my staff will tell you. At the Opening Ceremonies of This Book (a catered
event with stadium seating), I discovered that the cucumber sandwiches weren’t
served in tea towels. As a result, the butter hadn’t set with the cucumbers
right… Anyways, I made a big scene and set fire to some of the advertising
trucks outside. I smashed this spotlight to pieces and so on. I had this loud
maniacal laughing thing going on deep into that night. It was a real mess.</p>

<p>But, since I don’t get along well with people, I hadn’t invited anyone but
myself to the Opening Ceremonies of This Book. So it wasn’t really that
embarrassing. I kept it under wraps and no one found out about the whole ordeal.</p>

<p>So you’ve got to know that <strong>synergy</strong> doesn’t actually mean <strong>synergy</strong> in this
book. I can’t do normal <strong>synergy</strong>. No, in this book, <strong>synergy</strong> means
<strong>cartoon foxes</strong>. What I’m saying is: this book will be starting off with an
exorbitant amount of <strong>cartoon foxes</strong>.</p>

<p>And I will be counting on you to turn them into <strong>synergy</strong>.</p>


      <p>
      <a href="https://poignant.guide/book/chapter-3.html">Turn page.</a>
      </p>
    </div></div>]]>
            </description>
            <link>https://poignant.guide/book/chapter-2.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25043544</guid>
            <pubDate>Tue, 10 Nov 2020 07:26:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My new GoBlog-Blog is finally alive]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25043497">thread link</a>) | @jlelse
<br/>
November 9, 2020 | https://jlelse.blog/posts/new-blog-goblog | <a href="https://web.archive.org/web/*/https://jlelse.blog/posts/new-blog-goblog">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>I spent months coding and it’s finally time to say “Hello World”!</p><p>Until a few hours ago I used a complicated setup of Hugo, Drone, Webmentiond, Caddy and some self-programmed tools for my blog to have an IndieWeb compatible blog with support for MicroPub, Webmentions, ActivityPub, IndieAuth, Telegram notifications and more. A use case that actually calls for a dynamic backend.</p><p>The dynamic backend is finally here. Finally no more workarounds!</p><p>I decided to develop my own CMS because it gives me the opportunity to know my code inside and out and in case of problems I know directly where to look for it. I developed the code to the best of my conscience so that everything is done as efficiently as possible. And I will be able to add more features to my blog much easier in the future!</p><p>I am especially happy about the function that I can now finally edit or delete posts. Far too often it happened to me that I had mistakes in the text or made stupid mistakes while creating the post itself. These can now be corrected directly.</p><p>To the technology behind the blog: The CMS is written in Go (my current favorite programming language!) and SQLite is used for database purposes.</p><p>The code is available <a href="https://jlel.se/goblog" target="_blank" rel="noopener">on my Gitea instance</a>. In the next days I will fix bugs I find and add a documentation and license.</p><p>P.S.: Sorry for the name of this project, “GoBlog” is a pretty lame name, feel free to send me better suggestions!</p></div></div>]]>
            </description>
            <link>https://jlelse.blog/posts/new-blog-goblog</link>
            <guid isPermaLink="false">hacker-news-small-sites-25043497</guid>
            <pubDate>Tue, 10 Nov 2020 07:15:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hottest FinTech Accelerators in London]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25043490">thread link</a>) | @cpepper
<br/>
November 9, 2020 | https://codeandpepper.com/fintech-accelerators-in-london/ | <a href="https://web.archive.org/web/*/https://codeandpepper.com/fintech-accelerators-in-london/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
              
<p>London is a sprawling economic hub that boasts a mind-boggling 750 FinTech companies. The “Big Smoke” represents <a rel="nofollow" href="https://www.whitecapconsulting.co.uk/articles/20-essential-stats-about-uk-fintech/">84 percent</a>&nbsp;of the UK’s VC, CVC, and PE investment volume, making it a desirable place for startups to find their financial feet. Looking for investors? Connections? Opportunities? Here are 6 FinTech accelerators in London you need to know about in 2020.&nbsp;</p>



<h2 id="h-a-list-of-fintech-accelerators-for-startups-in-london"><strong>A list of FinTech accelerators for startups in London</strong></h2>



<p>Below you will find the most interesting FinTech accelerators for startups based in London.</p>
<ul>
<li data-amp-original-style="text-decoration: none;">Seedcamp</li>
<p>Where? <a href="https://www.google.com/search?q=5+Bonhill+St%252C+Shoreditch%252C+London+EC2A+4BX&amp;oq=5+Bonhill+St%252C+Shoreditch%252C+London+EC2A+4BX&amp;aqs=chrome..69i57&amp;sourceid=chrome&amp;ie=UTF-8%23" rel="nofollow">5 Bonhill St, Shoreditch, London EC2A 4BX</a></p>
<p>Smack-bang in trendy Shoreditch, <a href="https://seedcamp.com/" rel="nofollow">Seedcamp</a> calls itself “Europe’s seed fund,” and this isn’t an exaggeration. <strong>London FinTech company</strong> identifies and invests in early-stage companies and establishes connections through its network of partners. There’s an emphasis on European founders, but Seedcamp, founded in 2007, has a truly global reach and has powered FinTech companies like Revolut and TransferWise.&nbsp;</p>
<p><a href="https://twitter.com/seedcamp" rel="nofollow">Follow Seedcamp on Twitter</a></p>
<li data-amp-original-style="text-decoration: none;">Ignite</li>
<p><i>Where?</i> <a href="https://www.google.com/search?q=113+Walton+St%2C+Chelsea%2C+London+SW3+2HP&amp;oq=113+Walton+St%2C+Chelsea%2C+London+SW3+2HP&amp;aqs=chrome..69i57&amp;sourceid=chrome&amp;ie=UTF-8#" rel="nofollow"><em>113 Walton St, Chelsea, London SW3 2HP</em></a></p>
<p>Founded in 2011, <a href="https://www.ignite.io/" rel="nofollow">Ignite</a> is one of the leading accelerator programs in the world. It has invested in more than 150 companies so far and even partnered with Google. With backing from the European Union, their three-month pre-accelerator and six-month accelerator programs provide you with the resources you need to scale your business. Ignite offers investment opportunities to FinTech accelerators worldwide, so you don’t need to be a UK-based company.&nbsp;</p>
<p><i>What to expect: £20,000 in exchange for 8 percent equity.&nbsp;</i></p>
<p><a href="https://twitter.com/igniteaccel?lang=en" rel="nofollow">Twitter</a></p>
<figure><amp-img width="2560" height="1707" src="https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-scaled.jpg" alt="FinTech in London - vital companies." srcset="https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-scaled.jpg 2560w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-300x200.jpg 300w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-1024x683.jpg 1024w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-768x512.jpg 768w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-1536x1024.jpg 1536w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-2048x1365.jpg 2048w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-314x209.jpg 314w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-150x100.jpg 150w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-720x480.jpg 720w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-864x576.jpg 864w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-432x288.jpg 432w" sizes="(max-width: 2560px) 100vw, 2560px" layout="intrinsic" disable-inline-width="" i-amphtml-layout="intrinsic"><img loading="lazy" width="2560" height="1707" src="https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-scaled.jpg" alt="FinTech in London - vital companies." srcset="https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-scaled.jpg 2560w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-300x200.jpg 300w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-1024x683.jpg 1024w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-768x512.jpg 768w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-1536x1024.jpg 1536w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-2048x1365.jpg 2048w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-314x209.jpg 314w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-150x100.jpg 150w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-720x480.jpg 720w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-864x576.jpg 864w, https://codeandpepper.com/wp-content/uploads/2020/03/benjamin-davies-Oja2ty_9ZLM-unsplash-1-432x288.jpg 432w" sizes="(max-width: 2560px) 100vw, 2560px" data-old-src="data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9JzE3MDcnIHdpZHRoPScyNTYwJyB4bWxucz0naHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmcnIHZlcnNpb249JzEuMScvPg=="></amp-img></figure>
<li data-amp-original-style="text-decoration: none;">Bethnal Green Ventures</li>
<p><i>Where?</i> <a href="https://www.google.com/search?q=20-30+Whitechapel+Rd%2C+Shadwell%2C+London+E1+1EW&amp;oq=20-30+Whitechapel+Rd%2C+Shadwell%2C+London+E1+1EW&amp;aqs=chrome..69i57&amp;sourceid=chrome&amp;ie=UTF-8#" rel="nofollow"><em>20-30 Whitechapel Rd, Shadwell, London E1 1EW</em></a></p>
<p>If you’re a <strong>FinTech startup</strong> based around a good cause, <a href="https://bethnalgreenventures.com/" rel="nofollow">Bethnal Green Ventures </a>wants to hear from you! This accelerator provides investments to companies that want to change the world. There’s a focus on sustainable and democratic tech, but this accelerator is open to all ideas, with an intensive program that runs twice a year. Launched in 2010, Bethnal Green Ventures offers mentorship and educational opportunities as well as funding.&nbsp;</p>
<p><i>What to expect: £20,000 in return for 6 percent equity.&nbsp;&nbsp;</i></p>
<p><a href="https://twitter.com/bg_ventures" rel="nofollow">Follow Bethnal Freen Ventures on Twitter</a></p>
<li>Startupbootcamp</li>
<p><i>Where? </i><a href="https://www.google.com/search?q=Techspace+Shoreditch+25+Luke+St%2C+London+EC2A+4DS&amp;oq=Techspace+Shoreditch+25+Luke+St%2C+London+EC2A+4DS&amp;aqs=chrome..69i57&amp;sourceid=chrome&amp;ie=UTF-8#" rel="nofollow"><i>Techspace Shoreditch 25 Luke St, London EC2A 4DS</i></a></p>
<p>The name says it all. This accelerator attracts FinTech startups looking for investment and mentoring opportunities in London. The super-intensive 3+3 month program connects 10 selected companies with more than 100 industry experts and office space in the middle of London. <a href="https://www.startupbootcamp.org/accelerator/fintech-london/" rel="nofollow">Startupbootcamp</a> was founded in 2010.</p>
<p><a href="https://twitter.com/sbootcamp" rel="nofollow">Follow Startupbootcamp on Twitter</a></p>
<li>Barclays Accelerator</li>
<p><i>Where?</i><a href="https://www.google.com/search?q=41+Luke+St%2C+Hackney%2C+London+EC2A+4DP&amp;oq=41+Luke+St%2C+Hackney%2C+London+EC2A+4DP&amp;aqs=chrome..69i57&amp;sourceid=chrome&amp;ie=UTF-8#" rel="nofollow"><em> </em><i>41 Luke St, Hackney, London EC2A 4DP</i></a></p>
<p>Powered by Techstars, <a rel="nofollow" href="https://home.barclays/who-we-are/innovation/barclays-accelerator/">Barclays Accelerator</a> is an award-winning 13-week program based in New York City, Tel Aviv, and, of course, London. Lucky startups will get access to Rise, the largest FinTech co-working space in the capital and the chance to meet the world’s most influential investors. Alumni include credit score company Aire and cloud-based payroll service Dotpay, as well as a number of Code &amp; Pepper partners:&nbsp; <a href="https://codeandpepper.com/case-studies/nearshore-it-outsourcing-lusid/">Fibourne</a>, <a href="https://codeandpepper.com/case-studies/insurtech-development-design-nimbla/">Nimbla</a>, <a href="https://codeandpepper.com/case-studies/legaltech-design-development-oathello/">Oathello </a>and <a href="https://www.simudyne.com/">Simudyne</a>.</p>
<p><i>What to expect: Up to £120,000 for 6 percent equity.</i></p>
<p><a rel="nofollow" href="https://twitter.com/ThinkRiseLDN">Follow Rise on Twitter</a></p>
<li>Accenture Fintech Innovation Lab</li>
<p>Where? <a href="https://www.bing.com/maps?where=Accenture%20%2C%20London%2C%20London%20EC3M%203BD%2C%20GB">Accenture, London, London EC3M 3BD, GB</a></p>
<p>The <a rel="nofollow" href="https://www.fintechinnovationlab.com/london/london-fintech-innovation-lab/">FinTech Innovation Lab London </a>has been an annual accelerator program started and conducted by Accenture since 2012. It operates by offering zealous, newly-founded companies business mentoring for three months, as well as chances for networking with banking giants and good advice from experienced entrepreneurs. What drives the London branch of FinTech Innovation Lab is the opportunity to be close to promising projects at the very beginning.</p>
<p>Besides the word “London” in its name, the accelerator program accepts applications from the whole world.</p>
<p><a rel="nofollow" href="https://www.linkedin.com/company/fintech-innovation-lab-london/">Follow FinTech Innovation Lab London on Linkedin</a>.</p>
</ul>



<h2 id="h-fintech-accelerators-in-london-your-thoughts"><strong>FinTech accelerators in London – your thoughts</strong></h2>



<p>So that’s our curated list of 6 places to drop by. Do you have any experiences connected with these accelerators? Or maybe you’ve benefited directly from their initiatives, just like some of our <a href="https://codeandpepper.com/clients/">clients</a>? Tell your story on our Facebook profile! If you feel like this post might prove useful to other founders and business owners you know, feel free to share it with them. Sharing is caring, and we deeply care about the growth of the FinTech community.</p>
          </div></div>]]>
            </description>
            <link>https://codeandpepper.com/fintech-accelerators-in-london/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25043490</guid>
            <pubDate>Tue, 10 Nov 2020 07:13:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Making Mass Effect not require admin rights, or how to not write a boolean check]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25042910">thread link</a>) | @__david__
<br/>
November 9, 2020 | https://www.me3tweaks.com/blog/modding/making-me1-not-require-admin-rights-part-2/ | <a href="https://web.archive.org/web/*/https://www.me3tweaks.com/blog/modding/making-me1-not-require-admin-rights-part-2/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
						<p>Hi all, it’s me again, your favorite modder who publishes a single research blog post a year. Welcome to my new blog, where I will also post maybe once a year! I got fed up with blogger’s endless unfixed bugs. I’m going to leave the content there though for historical sake.</p>
<p>I just finished a hardcore crunch to ship ALOT Installer V4, which is a complete rewrite of ALOT Installer. ALOT Installer is the Mass Effect modding scene’s main texture installation tool, built on top of aquadran’s MassEffectModder program, which can be used to install textures in a more advanced fashion. In V4 of ALOT Installer, I split the main ‘core’ features into a cross-platform .NET Core library so I can also write a frontend that works on Linux. But that’s not why I’m here today – I’m here to follow up on how I fixed Mass Effect on PC to not require elevation for good.</p>
<h2>Mass Effect on PC: About what you’d be expect from a mid 2000’s console port</h2>
<p>For those of you not in the know, Mass Effect came out on PC back in 2008, and was ported from the Xbox 360 by a studio named Demiurge, who also developed Pinnacle Station for Mass Effect. It’s… a really meh port that has not aged very well. It’s passable as a game but it has a lot of problems, even when it came out. Particle LODs not working properly, texture LODs being read backwards, ini settings being randomly reset to their defaults, the problems are pretty numerous, just to name a few. But nothing completely game breaking.</p>
<p>Well, kind of. There is one, but it’s not specifically due to Mass Effect. The big issue is that Mass Effect requires administrator rights to run, because Demiurge seems to have assumed everyone would run the game as administrator – which <em>might</em> have been OK if the game was only really developed when Windows XP existed, but Windows Vista had already been out for over a year by the time the game had released. Even back then though, Windows XP had a concept of LUA (Least User Access) with separated user accounts. For more information on this, you should check out the original post I wrote, <a href="https://www.me3tweaks.com/blog/modding/why-mass-effect-requires-administrator-rights-and-how-we-fixed-origin-not-running-it/">Why Mass Effect on PC requires administrator</a>. It describes a lot of backstory to this post.</p>
<h2>Oh boy, PhysX, my favorite physics library!</h2>
<figure id="attachment_67" aria-describedby="caption-attachment-67"><img loading="lazy" src="https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/ageialogo1.gif" alt="" width="236" height="134"><figcaption id="caption-attachment-67">I may have a slight beef with this SDK.</figcaption></figure><p>
Mass Effect for PC runs on a lightly modified version of Unreal Engine 3, which appears to be dated around late 2006. According to some former BioWare developers, this version of Unreal Engine was not very mature yet, to put it lightly. According to some stories from these developers, it was really difficult to work with because Epic Games was focused on Gears of War and not dedicating much time to their partners who were also using the engine.</p>
<p>Unreal Engine 3 uses PhysX for physics interactions, so Epic Games built a dll that interfaces PhysX to Unreal Engine data formats through a file named PhysXLoader.dll, which loads the PhysX libraries from both parties. PhysX is a physics simulation library that was acquired by AGEIA Technologies in the mid 2000s before AGEIA was sold to Nvidia in early 2008. If you remember Physics Processing Unit cards, or PPU, they were using PhysX before Nvidia promptly killed that idea.</p>
<figure id="attachment_66" aria-describedby="caption-attachment-66"><img loading="lazy" src="https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/2020-11-07_19h50_25.png" alt="" width="360" height="136" srcset="https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/2020-11-07_19h50_25.png 360w, https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/2020-11-07_19h50_25-300x113.png 300w, https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/2020-11-07_19h50_25-250x94.png 250w" sizes="(max-width: 360px) 100vw, 360px"><figcaption id="caption-attachment-66">PhysXLoader.dll, PhysXCore.dll, and NxCooking.dll make up the PhysX dlls for Mass Effect.</figcaption></figure>
<p>All three Mass Effect games use PhysX, but Mass Effect 2 and Mass Effect 3 use the system’s install of PhysX, while Mass Effect uses the local game’s PhysX. Mass Effect 2 and Mass Effect 3 also use the ‘modern’ version of PhysX, rather than the legacy one that was shipped by AGEIA. Nvidia changed some paths under the hood when it took over, which separates Legacy out from it’s ‘modern’ versions. </p>
<p>But that doesn’t seem to stop Legacy PhysX’s uninstaller from deleting modern PhysX’s files/registry keys, so during the course of testing this fix, my other copies of Mass Effect 2/3 didn’t work, even after installing the ‘modern’ PhysX redistributable. It’s really annoying how BioWare couldn’t just ship a 8MB library with the game – they already shipped the installer for PhysX with the game, so it’s not like it saved space!</p>
<p>But anyways…</p>
<h2>The issue with Epic Games’ PhysXLoader.dll is that it can load PhysXCore.dll locally, or from the system’s installed version</h2>
<p>Err… wait, how is that an issue? Can’t you just load the local dll, and if that doesn’t exist, load the system one? How is that an issue exactly?</p>
<figure id="attachment_73" aria-describedby="caption-attachment-73"><img loading="lazy" src="https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/4929c5241f6f230614d6454092c699fe1.jpg" alt="OH BOY HERE WE GO" width="294" height="294" srcset="https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/4929c5241f6f230614d6454092c699fe1.jpg 294w, https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/4929c5241f6f230614d6454092c699fe1-150x150.jpg 150w, https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/4929c5241f6f230614d6454092c699fe1-48x48.jpg 48w, https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/4929c5241f6f230614d6454092c699fe1-250x250.jpg 250w, https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/4929c5241f6f230614d6454092c699fe1-180x180.jpg 180w" sizes="(max-width: 294px) 100vw, 294px"><figcaption id="caption-attachment-73">You won’t believe how many facepalms there were as I making this fix.</figcaption></figure><p>
On boot, Mass Effect writes two values to the Windows HKEY_LOCAL_MACHINE registry:</p>
<blockquote><p>REG_BINARY HKLM\SOFTWARE\AGEIA Technologies enableLocalPhysXCore [mac address, 6 bytes]<br>
REG_DWORD HKLM\SOFTWARE\AGEIA Technologies EpicLocalDllHack [1]</p></blockquote>
<p>*Mass Effect is a 32-bit program, so on 64-bit systems it goes into HKLM\SOFTWARE\WOW6432Node\AGEIA Technologies instead, if you’re looking for yourself.</p>
<p>Remember these registry values, they’re going to be important later!</p>
<p>These registry values are why Mass Effect requires administrative permissions. In my previous blog post linked above, we explored why these writings were enough to make Microsoft put Mass Effect into it’s compatibility database, which forces it to run as admin when matching on certain executable criteria, which we worked around by modifying the executable criteria to no longer match. </p>
<p>We have to modify the executable to enable Large Address Aware, so the game could load higher resolution textures without running out of memory, so there was no way to avoid breaking the signature. This in turn caused Origin to no longer run the game as it would not elevate games without a valid EA signature. But if the game cannot write these registry keys on boot, the game may crash… </p>
<p>So it’s already a big fun chain of problems, but we worked around Mass Effect needing administrative rights by simply giving the user account permissions to that specific AGEIA Technologies registry key. This would let the game process write the values it needed, and would we could go on our merry way. I assumed the game crashed because it was denied write permissions and Demiurge couldn’t be bothered to write a try/catch around the registry writing code.</p>
<h2>You probably shouldn’t name your registry values as a hack if you want me to think this is a good idea</h2>
<p>Our solution to this problem did not change Mass Effect’s behavior – the values it wanted to write to the registry were going to be written one way or another, so we were just letting it do the thing it’s always done, just without administrative rights. There wasn’t really any change in application behavior.</p>
<figure id="attachment_81" aria-describedby="caption-attachment-81"><img loading="lazy" src="https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/2020-11-01_12h55_59.png" alt="" width="362" height="154" srcset="https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/2020-11-01_12h55_59.png 362w, https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/2020-11-01_12h55_59-300x128.png 300w, https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/2020-11-01_12h55_59-250x106.png 250w" sizes="(max-width: 362px) 100vw, 362px"><figcaption id="caption-attachment-81">The two registry values that Mass Effect writes.</figcaption></figure>
<p>mirh, a moderator for <a href="https://www.pcgamingwiki.com/wiki/Home">PC Gaming Wiki</a>, sounded the alarm for years that somehow we were breaking other games in ALOT Installer – even though our application didn’t actually change how Mass Effect was behaving writing these values, so there’s no way our change would break other games.</p>
<p>After many months, he wrote a fairly detailed reason why ALOT Installer (when, in reality, it was Mass Effect) is breaking other games: <b>enableLocalPhysXCore</b> being in the registry <em>is used by other games using Epic Game’s PhysXLoader.dll.</em> When I was writing V4 of ALOT Installer, I told mirh I would take a more serious look into his idea of a solution that would not break other games, even though at the time I did not really understand how a registry key with the system’s MAC address would break other games – or why it even used a MAC address to begin with.</p>
<p>mirh seems to have determined this enableLocalPhysXCore lets Mass Effect use the local directory’s PhysXCore.dll/NxCooking.dll, instead of loading the one from the installed PhysX redistributable. Mass Effect doesn’t install the PhysX redistributable, so it could not rely on it existing, so it needed to use the local libraries.</p>
<p>Hope you’re strapped in because this is where it gets really dumb: </p>
<h4>The MAC address stored in in the registry by MassEffect.exe is read by PhysXLoader.dll and compared against your system’s MAC address to determine if it should load the local directory’s PhysX libraries or the system’s.</h4>
<p>Which MAC address? </p>
<h3>¯\_(ツ)_/¯</h3>
<p>So the way Mass Effect works:</p>
<ol>
<li>Very early in the boot process of MassEffect.exe, your MAC address is read and written to the registry as enableLocalPhysXCore (along with EpicLocalDllHack)</li>
<li>MassEffect.exe loads PhysXLoader.dll</li>
<li>PhysXLoader.dll reads the value of enableLocalPhysXCore and compares your system’s MAC address against it</li>
<li>If it matches, it uses the local folder’s PhysX, if not, it uses the system’s redistributable version of PhysX</li>
</ol>
<p>Yes, you read that right.</p>
<p>It turns out that other games, such as Mirror’s Edge, have a PhysXLoader.dll that also reads these values (as they’re based on the same code), <em>but they don’t include local PhysX libraries</em>. So those games boot up, see enableLocalPhysXCore, and try to load the local library, which fails, and the game doesn’t start. This information is second hand from mirh – I have not tested other games broken by this registry value.</p>
<p>Normally that value wouldn’t exist, and it should use the system PhysX. This behavior can be tested in Mass Effect by denying it write permissions to the registry key, deleting the values, and having Legacy PhysX installed – it will use the system libraries instead. If system PhysX is not installed, the application will not boot – this is why we originally had to let Mass Effect write these keys, otherwise it could appear that the installer broke Mass Effect, when it actually was a terrible implementation by Epic Games.</p>
<figure id="attachment_157" aria-describedby="caption-attachment-157"><img loading="lazy" src="https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/Facepalm-PNG-File1.png" alt="Facepalm" width="782" height="433" srcset="https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/Facepalm-PNG-File1.png 782w, https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/Facepalm-PNG-File1-300x166.png 300w, https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/Facepalm-PNG-File1-768x425.png 768w, https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/Facepalm-PNG-File1-250x138.png 250w, https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/Facepalm-PNG-File1-550x305.png 550w, https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/Facepalm-PNG-File1-325x180.png 325w, https://www.me3tweaks.com/blog/wp-content/uploads/2020/11/Facepalm-PNG-File1-542x300.png 542w" sizes="(max-width: 782px) 100vw, 782px"><figcaption id="caption-attachment-157">It’s hard to imagine any possible scenario where this was a good idea.</figcaption></figure><p>
If you’re interfacing with a library that has exports you can call to initialize/load the PhysX SDK… couldn’t you just, you know, pass a boolean to tell it to locally load? Why does it not locally look to begin with? And what’s up with the MAC address? Why is this in the registry, where it behaves LIKE A GLOBAL SETTING??? </p>
<p>All of these seem like terrible design decisions – and after disassembling the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.me3tweaks.com/blog/modding/making-me1-not-require-admin-rights-part-2/">https://www.me3tweaks.com/blog/modding/making-me1-not-require-admin-rights-part-2/</a></em></p>]]>
            </description>
            <link>https://www.me3tweaks.com/blog/modding/making-me1-not-require-admin-rights-part-2/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25042910</guid>
            <pubDate>Tue, 10 Nov 2020 04:53:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AppleCrate II: A New Apple II-Based Parallel Computer (2015)]]>
            </title>
            <description>
<![CDATA[
Score 79 | Comments 9 (<a href="https://news.ycombinator.com/item?id=25042551">thread link</a>) | @aresant
<br/>
November 9, 2020 | http://michaeljmahon.com/AppleCrateII.html | <a href="https://web.archive.org/web/*/http://michaeljmahon.com/AppleCrateII.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

<b><span face="Arial, helvetica" size="4"><p>AppleCrate II:  A New Apple II-Based Parallel Computer</p>
</span><span face="Arial, helvetica" size="2"><p>
Michael J. Mahon – July 26, 2008<br>
Revised – September 23, 2015</p>

<p><img src="http://michaeljmahon.com/CrateII.jpg" width="500" height="612"></p>

</span><span face="Arial, helvetica"><p>Introduction</p>
</span></b><span face="Arial, helvetica" size="2"><p>
In 2004 I built the first <a href="http://michaeljmahon.com/Applecrate.html">AppleCrate</a>, an 8-board system, as an inexpensive, easy to program
vehicle for experiments in parallel programming—a kind of "blade server" for the Apple II, if
you will!  AppleCrate I (at the time I didn't realize that it was number "I" ;-) was great fun, and it
enabled some very interesting experiments, but over time I discovered some of its shortcomings.</p>

<p>First and foremost, since the boards were supported by only two edges and not clamped in place, it
was relatively fragile and hard to transport.  Second, I had come across situations in which more
than 8 slave processors would have been useful.  Third, my arrangement for collecting audio signals
synthesized by the slaves was quite makeshift and delivered sound with lots of digital "hash"
as background noise.  And finally, the original AppleCrate made no provision for plugging I/O cards
into any of its boards, so it had to be hosted by a separate Apple II, adding to the problem of
transporting it for demonstrations.</p>

<p>The AppleCrate II is designed to be significantly improved in all of the areas that were
problems for the AppleCrate I.</p>

<b></b></span><b><span face="Arial, helvetica"><p>Description</p>
</span></b><span face="Arial, helvetica" size="2">

<p>The AppleCrate II is made from 17 Enhanced Apple //e main boards.  (Fifteen of these boards were
obtained in the same eBay auction that netted the eight unenhanced boards for the original AppleCrate.)
Because they are enhanced ROMs, the original NadaNet boot ROM code would not fit and a new
boot protocol had to be developed, as described below.</p>

<p>Instead of mounting the cards vertically in a frame, as in the original, I decided to mount them
horizontally in a stack secured with standoffs—3/4" long hexagonal rods, each with a screw protruding from
one end and a tapped hole in the other.  The AppleCrate II has nine "columns" of these standoffs—six
metal columns at the back and corners of the boards and three nylon columns interior to the boards
to add stiffness, as shown in the photo below at the 2-board construction stage:</p>

<p><img src="http://michaeljmahon.com/TwoBoards.jpg" width="800" height="554"></p>

<p>This "hi-rise" construction makes the "stack" quite rigid and sturdy, while eliminating the need
for a space-consuming exoskeleton.  It also has the advantage of leaving the top board unobstructed
so that I/O cards can be plugged in, allowing it to serve as the host machine for the AppleCrate.  (In fact,
I used 17 boards so that the top board can serve as master and leave 16 slave machines for parallel
programs.)</p>

<p>The Pushbutton 1 input and Annunciator 1 output bus wires and the AN2-to-PB2 GETID daisy chain wires are connected to
machined-pin sockets inserted into the 16-pin game port connector.  These connections support <a href="http://michaeljmahon.com/NadaNet.html">NadaNet</a>,
which is the only signal connection between the boards.  The network adapter (described below) is shown
with its mounting bracket under what will be the third board.  The power bus card is supported by a
similar angle bracket, and the standoffs immediately beneath them are filed down to accomodate the
bracket thickness.</p>

<p>The boards are powered by a PC AT power supply.  The average power consumed by an Apple //e
board is about 4.2 watts, so the whole 17-board crate consumes only about 70 watts in total,
and both the AppleCrate and the power supply run only a few degrees above ambient temperature.</p>

<p>I decided to use #12 copper bus wires
to distribute power to all boards (visible on the right side of the first photo).  I would have preferred
a connectorized approach, but I could not come up with a connector scheme with a
reasonable mating/unmating force.  As a result, I decided to go with soldered power connections.
(It's a good thing that Apple //e's are so reliable, since replacing one in the middle of the
stack would be relatively difficult!)</p>

<p>The top board is used as the "master" machine with I/O cards and an external keyboard plugged into it.
The Master boots the 16 slave Apples in the AppleCrate II and uses them to run parallel programs.
Once they have been booted and started, they can run independently of the master—though they are clearly
I/O-constrained!</p>

<b></b></span><b><span face="Arial, helvetica"><p>Indicators</p>
</span></b><span face="Arial, helvetica" size="2">

<p>It has proven useful to have some real-time indication of each board's activity.  The stock board contains a
red "power" LED (at the right) and a red "speaker" LED at the left.  Both are easily visible from the back of the
boards (the "front" of the AppleCrate).  The function of the power LED is fixed, but the speaker LED is usable
as an indicator that software running on the board can operate, just by toggling the speaker.  For example,
printing a "beep"—CHR$(07)—causes the speaker LED to flash for 0.1 second, and can be used to indicate some
condition in the software.  (The speaker LED will not light when a speaker is installed, but AppleCrate
boards have no speakers attached.)</p>

<p>Although the Applecrate network interface described below incorporates an LED to show global network activity,
it is very useful to be able to see when any particular board is sending on the network.  This need is met by
using the PDL 3 timer to "stretch" each packet send operation into a visible flash of a green rectangular LED.</p>

<p><img src="http://michaeljmahon.com/SendLED.jpg" width="762" height="263"></p>

<p>These photos show the modification made to the 558 timer chip, in which a 267-ohm resistor (just what I
had handy—any value between 220 and 560 ohms is fine) is connected
between pins 5 and 8, and the "send" signalling LED is connected between pin 8 and ground, with pin 8 going to the
anode.  The rectangular LED is carefully pressed between the cassette input and output jacks.  On some boards,
the jacks were so close together that it was necessary to "shave" the upper plastic swage on the side of the input
jack with an Exacto knife to make room for the LED to press fit between them.  (Note that in these photos the red
wire connected to the anode of the LED has not yet been soldered.)</p>

</span><b><span face="Arial, helvetica"><p>Network Boot in NadaNet 3.x</p>
</span></b><span face="Arial, helvetica" size="2">

<p>Since AppleCrate machines have no I/O capabilities other than
the network, they must be booted from the network.  This requires that the ROMs on the boards be replaced with
EPROMs containing modified RESET code to perform the network boot.</p>

<p>As with the AppleCrate I, replacement of the self-test code was the easiest path, since it is self-contained,
contiguous, and is executed upon power-on reset if no keyboard is connected.  However, the Enhanced //e ROM contains
only $200 bytes of self-test code, just half the size of the unenhanced //e self-test, requiring a new
design for the network boot.</p>

<p>The AppleCrate I used an "active" boot protocol, in which each board enabled by the "GETID daisy chain" (connected from
AN2 of the previous machine to PB2 of the current machine) continuously sent GETID requests to ID 1, until it was assigned
a permanent ID and received a NadaNet boot image.  The complexity of this protocol, requiring both sending and receiving
packets over the network, resulted in a boot ROM requirement of almost $400 bytes—which fit in an <b>Unenhanced</b> //e ROM.</p>

<p>Since the <b>Enhanced</b> //e ROM has only $200 bytes available, a new "passive" boot protocol had to be devised.
The <a href="http://michaeljmahon.com/PASSIVEBOOT.ROM.pdf">new ROM code</a> continuously monitors the network for a broadcast BOOTREQ control packet
containing  the load address and length of the immediately following boot code data.  When the boot image has been correctly
read from the network, control is passed to its starting address.  This passive boot code only needs to <b>read</b> packets from the
net, and so occupies just $190 bytes, which comfortably fits in place of the Enhanced //e ROM self-test code at $C600.</p>

<p>The new boot protocol capitalizes on the fact that boot code is sent as a broadcast transaction, so the
machines being booted do not need IDs to receive boot code.  A page of "second-stage boot" code is added at the
front of the slave machine boot image.  This code is given control immediately after the boot image is received, and,
when enabled by the "GETID daisy chain", it sends a GETID request to the machine that &amp;BOOTed it, making use of the
code in the full NadaNet boot image to do so (see the BOOT2 code in the <a href="http://michaeljmahon.com/NADA.CRATE.pdf">NADA.CRATE</a>
listing for details).</p>

<p>The GETID daisy chain functions just as it did in the AppleCrate I.  The "first" machine is permanently enabled
by connecting its PB2 to ground.  AN2 of each machine is connected
to PB2 of the "next" machine.  The second-stage boot code running in each machine initially sets its AN2.
Then it waits until it sees its PB2 go low, enabling it to send its GETID request.  When its GETID is successful
it drops its AN2, enabling the next machine.  Then it clears its video display, writes a banner showing the
machine ID, and enters its server loop.</p>

<p>This results in permanent
IDs being assigned in the fixed order of the physical daisy chain, while allowing all ROMs to be identical.
An LED on the AppleCrate II NadaNet adapter board is wired to the last machine's AN2, so that when the last
machine drops its AN2, the red LED extinguishes, signalling that all machines have booted successfully.</p>

<p>When a network-booting machine is reset, it first checks the network state.  If the network is low (ZERO),
it performs a cold start.  If the network is being held high (ONE), it checks page 3 to see if it is being cold started or warm reset.  If it is
a warm reset, it re-enters its Server loop.  If it is a cold start, it initializes and enters the ROM boot code, again waiting
for a BOOTREQ packet.  (This approach has the advantage of reliably forcing a reboot on a power cycle, while
still permitting boards to be warm reset while holding the network high.)</p>

<p>As of NadaNet 3.1, all AppleCrate boot ROMs must be <a href="http://michaeljmahon.com/PASSIVEBOOT.ROM.pdf">NadaNet 3.x capable</a>.</p>

</span><b><span face="Arial, helvetica"><p>AppleCrate II NadaNet Interface</p>
</span></b><span face="Arial, helvetica" size="2">
<p><a href="http://michaeljmahon.com/NadaNet.html">NadaNet</a> is a TTL-level serial network in which logic high is represented by a voltage greater than +2 volts
and logic low is represented by a voltage less than +0.7 volts.  The fanout capability of a TTL annunciator output
is sufficient to  drive a dozen or so TTL pushbutton inputs if they are not otherwise …</p></span></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://michaeljmahon.com/AppleCrateII.html">http://michaeljmahon.com/AppleCrateII.html</a></em></p>]]>
            </description>
            <link>http://michaeljmahon.com/AppleCrateII.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25042551</guid>
            <pubDate>Tue, 10 Nov 2020 03:23:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Stakes]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25042480">thread link</a>) | @exolymph
<br/>
November 9, 2020 | https://www.sonyasupposedly.com/stakes/ | <a href="https://web.archive.org/web/*/https://www.sonyasupposedly.com/stakes/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            


                <figure>
                    <img srcset="https://www.sonyasupposedly.com/content/images/size/w300/2020/11/1411922_o2.jpg 300w,
                                https://www.sonyasupposedly.com/content/images/size/w600/2020/11/1411922_o2.jpg 600w,
                                https://www.sonyasupposedly.com/content/images/size/w1200/2020/11/1411922_o2.jpg 1000w,
                                https://www.sonyasupposedly.com/content/images/size/w2000/2020/11/1411922_o2.jpg 2000w" sizes="(max-width: 800px) 400px,
                            (max-width: 1170px) 1170px,
                                2000px" src="https://www.sonyasupposedly.com/content/images/size/w2000/2020/11/1411922_o2.jpg" alt="Stakes">
                </figure>
                <section>
                    <div>
                        <p>Hi friends! What a year, what a year. That's all I have to say about electoral politics. Seriously though, can you believe it's November already? Time to plan the holiday movie roster ❄️</p><p>Perhaps you'd enjoy a serene interlude. <a href="https://wanderverse.org/wanderpath/gazebo/">Visit the Wanderpath gazebo</a> to contemplate "icicles which glimmer and glitter in the faint sunlight." Or run your eyes over the dreamy hues of <a href="https://wanderverse.org/wanderpath/indian_balsam/">flowers pressed in warmer days</a>. (Thank you <a href="http://polyducks.co.uk/">Polyducks</a> 💝)</p><p>Today's soundtrack: <a href="https://lovecrypt.bandcamp.com/album/revelations-ii"><em>Revelations II</em> by Khan &amp; Polo</a>. "alone, you build new structures; some that you live inside, some that live inside you."</p><p>I have something to celebrate. Twelve months ago, I stopped drinking:</p><figure><blockquote data-width="550"><div lang="en" dir="ltr"><p>Hello friends. I want to tell you something, because I need to declare it to make it part of myself.</p><p>I'm quitting alcohol. I commit to never touching another drop.</p><p>I like it too much, and I don't like the person I turn into because of liking alcohol too much.</p></div>— 🎀 sonyasupposedly.com 🤖 (@sonyasupposedly) <a href="https://twitter.com/sonyasupposedly/status/1185916407906738177?ref_src=twsrc%5Etfw">October 20, 2019</a></blockquote>

<figcaption>Helpful support groups: <a href="https://www.reddit.com/r/stopdrinking/">/r/stopdrinking</a> and <a href="https://www.reddit.com/r/dryalcoholics/">/r/dryalcoholics</a>.</figcaption></figure><p>For me, eschewing booze was an unequivocally wonderful decision. But a difficult one, as you might expect. I want to share two hard-won realizations:</p><ol><li>Change requires loss. To <em>be</em> different, you have to <em>do</em> different things, to the exclusion of the things that you used to do. Maybe that sounds obvious, but the actual practice is terrifying (or at least it was for me). Change requires sacrificing the old self — stabbed through the heart and consigned to the grave, a vampire destroyed for its predatory concupiscence! Ideally with compassion, since that used to be you... Nonetheless, the old self's cherished pleasures are precisely what you must relinquish. It will torment you to retaliate for the privation of venal delights. Thus, like the vampire, the old self has gotta go. Ready your stake!</li><li>Change requires <em>choosing</em> that pain and strife over the comfortable familiarity of stasis. It will hurt. But the price is the price —&nbsp;either pay up or accept that you don't want the finer things in life more than you want to avoid what it takes to attain them.</li></ol><p>Related: <a href="https://drmaciver.substack.com/p/dont-just-try-harder">"Don't just try harder,"</a> writes <a href="https://twitter.com/DRMacIver">David MacIver</a>. Do different things!<em> </em>While you're at it, read <em><a href="https://amzn.to/2UcRBM4">The Voyage of the Dawn Treader</a></em>.</p><p>In unrelated happy news, <strong>the <a href="https://www.sonyasupposedly.com/membership/">membership</a> setup is fixed</strong>. Now I can tell you how I broke it: by changing my prices, which scrambled the Ghost-Stripe integration. Anyway, I got that sorted out. The new monthly price is $1.50 cheaper than before —&nbsp;$12/month instead of $13.50, or $120/year versus $135. Hooray for small economies of scale!</p><p>Current members, the price change is prorated, and no action is needed on your part. Potential members, if you'd like to receive monthly <a href="https://www.sonyasupposedly.com/tag/zines/">zines</a> (both printed and digital!) as well as occasional <a href="https://www.sonyasupposedly.com/down-the-line/">Top Secret Special Notes</a>, please <a href="https://www.sonyasupposedly.com/account/">go here</a> to proceed.</p><p>Launches you may like:</p><ul><li><a href="http://threadhelper.com/">ThreadHelper</a>, a power-up for Twitter addicts <a href="https://chrome.google.com/webstore/detail/threadhelper/nfadnflafdfmekapgcgddbccooagpndk">who use Chrome</a>:</li></ul><figure><blockquote data-width="550">— Rival Voices 🦁 (@nosilverv) <a href="https://twitter.com/nosilverv/status/1321518942033186822?ref_src=twsrc%5Etfw">October 28, 2020</a></blockquote>

</figure><ul><li><a href="https://otherlife.co/strauss/">Other Life's course on Leo Strauss</a> (whose Wikipedia page, in screenshot form, graces the cover of <a href="https://www.sonyasupposedly.com/clandestine-motives/">Clandestine Motives</a>, fun fact)</li><li><a href="https://www.boethi.us/">Boethius</a>, a service offering "Classical education for a digital age," by <a href="https://twitter.com/JaysonVirissimo">Jayson Virissimo</a></li><li><a href="https://gumroad.com/l/WEdDl">Other Futures</a>, a sci-fi zine by <a href="https://twitter.com/atroyn">Anton Troynikov</a> 🤖🔮</li></ul><p>Plus one bonus not-quite-launch snagged from <a href="https://tinyletter.com/auerbook/letters/auerbook-election-hangover-william-gass-the-tunnel-enneadecameron-meganets">David Auerbach's newsletter</a>:</p><blockquote>The videos/lectures from my class <a href="http://https//www.youtube.com/playlist?list=PL4kdFWi1bM_HaWZNRHeRVLlbfg3vx6naG">FIVE PARADIGMS OF ARTIFICIAL INTELLIGENCE</a> are available. The course was very rewarding for me and the material is extremely relevant to present-day debates. They also form the historical basis of my forthcoming book from PublicAffairs, MEGANETS, which has been occupying most of my time recently.</blockquote><p>In addition to his TinyLetter and that YouTube playlist, David has a <a href="https://davidauerba.ch/">website</a>, a <a href="https://www.waggish.org/">blog</a>, and <a href="https://twitter.com/AuerbachKeller">Twitter</a>.</p><p>A few things to read:</p><ul><li><a href="https://uncivilizedbooks.com/authors/kaczynski-tom/">Tom Kaczynski's Uncivilized comics</a></li><li><a href="https://theprepared.com/blog/a-giant-waxed-cheese-wheel-is-the-apocalypse-prep-you-didnt-know-you-needed/">"A giant waxed cheese wheel is the apocalypse prep you didn't know you needed"</a> by <a href="https://twitter.com/jonst0kes">Jon Stokes</a></li><li><a href="https://quillette.com/2020/10/29/the-evolutionary-history-of-mans-best-friend-revealed/">"The Evolutionary History of Man's Best Friend Revealed"</a> by <a href="https://twitter.com/razibkhan">Razib Khan</a></li><li><a href="https://www.precursorpoets.com/newsletter-october2020/">"Galatea, pt. 3"</a> by <a href="https://twitter.com/PreCursorPoets">Timothy Wilcox</a></li><li><a href="https://livingideas.substack.com/p/how-culture-drives-human-evolution">"How Culture Drives Human Evolution"</a> by <a href="https://twitter.com/sachinmaini">Sachin Maini</a></li><li><a href="https://solana.substack.com/p/zen-and-the-art-of-political-censorship">"Zen and the Art of Political Censorship"</a> by <a href="https://twitter.com/micsolana">Mike Solana</a></li><li><a href="https://diff.substack.com/p/big-tech-sees-like-a-state">"Big Tech Sees Like a State"</a> by <a href="https://twitter.com/ByrneHobart">Byrne Hobart</a></li></ul><p>That's all for now. Talk soon 😘</p><p>P.S. I dare you to <a href="https://www.sonyasupposedly.com/mystery-zine-bundle/">buy a Mystery Zine Bundle</a>. Great stocking stuffers, jussayin...</p><hr><p><a href="https://www.slam.org/collection/objects/27247/">Still life by Pieter Claesz</a> (1643).</p>
                    </div>
                </section>



        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://www.sonyasupposedly.com/stakes/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25042480</guid>
            <pubDate>Tue, 10 Nov 2020 03:06:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Apple Kills Cname Cloaking on iOS/iPadOS]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25042374">thread link</a>) | @fenier
<br/>
November 9, 2020 | https://cunderwood.dev/2020/11/06/using-cnames-to-bypass-itp-has-been-put-to-torch/ | <a href="https://web.archive.org/web/*/https://cunderwood.dev/2020/11/06/using-cnames-to-bypass-itp-has-been-put-to-torch/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
						
<p>Browsers are mobilizing to combat the use of DNS CNAME records to bypass anti-tracking tech they have built in to their browsers.  November is looking to be a triple whammy of developments in this area.</p>



<h2>iOS and Safari</h2>



<p>I <a href="https://cunderwood.dev/2020/09/09/the-death-of-cname-records-in-analytics/">first wrote </a>about this in September, and now the time is here.   On November 5th 2020 Apple released iOS 14.2 and iPad 14.2.  With these updates now in play, any device running these new versions will have CNAME Cloaking Mitigation enabled by default.  This change was not listed in any patch notes and as of the time of writing Apple has also not updated Webkit’s blog to announce the change.</p>



<h3>Safari vs iOS (present state)</h3>



<p>An example to show what I am talking about may be in order.</p>



<p>Using Mac OS Safari – Let’s look at Apple.com which based on their Network traffic you can tell is leveraging Adobe’s CNAME solution by calling their subdomain <strong>securemetrics.apple.com</strong></p>



<p>If we run a DNS record lookup on this subdomain we can see it points to <strong>appleglobal.102.112.2o7.net</strong>.  The 2o7.net domain is used by Adobe for Adobe Analytics data collection.</p>



<p>How this works is JavaScript on the page would call (in this case) <strong>securemetrics.apple.com</strong> which forwards the request to <strong>appleglobal.102.112.2o7.net</strong>.  which would proceed to set a <strong>s_vi</strong> cookie (containing the Adobe Analytics visitor ID) for a period of two years via the response header.</p>



<p>Response Header:</p>



<p><code>set-cookie: s_vi=[CS]v1|2EAFF145852CC4DD-400009B360002651[CE]; Path=/; Domain=apple.com; Max-Age=63072000; Expires=Sun, 06 Nov 2022 10:07:07 GMT; SameSite=None; Secure</code></p>



<p>Then if we look at the cookies set by the domain we can see that cookie does indeed last for two years.</p>



<figure><img loading="lazy" width="339" height="211" src="https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/vi_cookie.png?resize=339%2C211&amp;ssl=1" alt="" srcset="https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/vi_cookie.png?w=339&amp;ssl=1 339w, https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/vi_cookie.png?resize=300%2C187&amp;ssl=1 300w" sizes="(max-width: 339px) 100vw, 339px" data-recalc-dims="1" data-lazy-srcset="https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/vi_cookie.png?w=339&amp;ssl=1 339w, https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/vi_cookie.png?resize=300%2C187&amp;ssl=1 300w" data-lazy-src="https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/vi_cookie.png?resize=339%2C211&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<h3>iOS</h3>



<p>Now let’s see how this is different on the recent iOS upgrade.</p>



<p>Using Mobile Safari we repeat the above steps and use a Macbook to see what is going on.  So we navigate to apple.com and call <strong>securemetrics.apple.com</strong></p>



<figure><img loading="lazy" src="https://i1.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-2.png?resize=645%2C356&amp;ssl=1" alt="response code results" width="645" height="356" srcset="https://i1.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-2.png?resize=1024%2C566&amp;ssl=1 1024w, https://i1.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-2.png?resize=300%2C166&amp;ssl=1 300w, https://i1.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-2.png?resize=768%2C424&amp;ssl=1 768w, https://i1.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-2.png?w=1430&amp;ssl=1 1430w" sizes="(max-width: 645px) 100vw, 645px" data-recalc-dims="1" data-lazy-srcset="https://i1.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-2.png?resize=1024%2C566&amp;ssl=1 1024w, https://i1.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-2.png?resize=300%2C166&amp;ssl=1 300w, https://i1.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-2.png?resize=768%2C424&amp;ssl=1 768w, https://i1.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-2.png?w=1430&amp;ssl=1 1430w" data-lazy-src="https://i1.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-2.png?resize=645%2C356&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>We can see that indeed, the response header says the <strong>s_vi</strong> cookie should last for two years.  So we move on to check the cookie storage.</p>



<figure><img loading="lazy" width="1024" height="578" src="https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-3.png?resize=1024%2C578&amp;ssl=1" alt="Dev tool results" srcset="https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-3.png?resize=1024%2C578&amp;ssl=1 1024w, https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-3.png?resize=300%2C169&amp;ssl=1 300w, https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-3.png?resize=768%2C433&amp;ssl=1 768w, https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-3.png?w=1400&amp;ssl=1 1400w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" data-lazy-srcset="https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-3.png?resize=1024%2C578&amp;ssl=1 1024w, https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-3.png?resize=300%2C169&amp;ssl=1 300w, https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-3.png?resize=768%2C433&amp;ssl=1 768w, https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-3.png?w=1400&amp;ssl=1 1400w" data-lazy-src="https://i2.wp.com/cunderwood.dev/wp-content/uploads/2020/11/image-3.png?resize=1024%2C578&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>We can see indeed that the cookie’s duration is lowered to 7 days.   This is a sizable change as many vendors (not just Adobe) use CNAME redirection for a number of reasons (not just tracking).  The Webkit feature does not care however, and only looks at if apple.com is the same as 2o7.net, which it clearly isn’t – and so applies the restrictions.</p>



<p>This means as adoption of iOS 14.2 and iPad 14.2 continue to rise – this will become a common scenario.  Since Webkit is the foundation of all browsers on iOS, I strongly suspect that it will apply to all browsers on the platform. I have reached out to Apple and will update if they confirm this is not the case.  </p>



<p>In North America iOS accounts for 53% of all mobile web traffic.  This should become noticeable in data pulls over the next few weeks for any site with notable iPhone / iPad traffic.</p>



<h3>What about Mac OS?</h3>



<p>The change is present in Safari builds for the upcoming Mac OS Big Sur.   At the time of writing Apple has an event planned for November 10th, and so details about the release date could be announced at that time.  I fully expect it to be present in Safari on the new operating system version.</p>



<h2>Brave</h2>



<p>Brave <a href="https://brave.com/privacy-updates-6/">announced </a>on November 17th their browser will also combat CNAME cloaking in a different manner.  Rather than just lower the duration of cookies set via this method, Brave will block the request entirely.</p>



<h2>So what’s this mean?</h2>



<p>The door has closed on using CNAMEs to set cookies and expecting them to have long durations.  This is a change in web architecture that developers need to now account for.</p>



<p>From the business point of view, any service using CNAMEs for cookie setting will either see that traffic disappear entirely (Brave) or have sizable reductions in lookback windows (iOS/iPadOS/Safari Big Sur).</p>



<p>This includes (but is not limited to) 7 days of lookback window for campaign measurement,   possible increase in  new users, possible loss of retention identification, and possible resegmentation of A/B testing cell assignments in scenarios where CNAME was used to set cookies.</p>



<h2>What can be done?</h2>



<p>It’s going to be interesting.</p>



<p>The clearest answer is transitioning from CNAME usage to use A / AAAA DNS records.  Which may be easier said than done.  Otherwise you’d need to leverage some sort of server cookie issuance / persistence layer.  Both of these can be tricky as I am sure this change is going to blindside a number of vendors.</p>



<p>The best advice I have is to in scenarios where you where using CNAME for Analytics or Measurement is to reach out to the vendor(s) involved and see what they say then discuss the answers with your dev team/agency.</p>



<p>Since many brands are now going into code freeze for the shopping season, this is likely to be exceptionally rough on the analytics front as adoption of the upgrade takes hold.  The only advice I have for that is to keep this in mind when analyzing or reporting results.</p>
					</div></div>]]>
            </description>
            <link>https://cunderwood.dev/2020/11/06/using-cnames-to-bypass-itp-has-been-put-to-torch/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25042374</guid>
            <pubDate>Tue, 10 Nov 2020 02:46:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Networking for Introverts]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25042288">thread link</a>) | @davefreiburger
<br/>
November 9, 2020 | https://gradually.co/how-to-network-as-an-introvert/ | <a href="https://web.archive.org/web/*/https://gradually.co/how-to-network-as-an-introvert/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main">
			<article id="post-876">

					<div>
						<div>
	              			<!-- category colored coded display and hide takeaways php -->
	              			<p>																								Health								</p><!-- end cat-wrap -->
						<p><span>&nbsp; •&nbsp; </span>
						<span>Networking for Introverts</span>
						<span> &nbsp;•&nbsp; </span>
						<span>
							November 9, 2020						</span>

						<img width="640" height="312" src="https://gradually.co/wp-content/uploads/2020/11/GD23-Health.gif" alt="" loading="lazy"></p><div>

																					<div>
								<p><a href="https://giphy.com/gifs/giphydiscovery-dogs-VzGQrj8sLH4GLcSiG1" target="_blank">
									[Image source: Giphy]								</a></p><h5>
									<a href="https://medium.com/@byrnehobart/writing-is-networking-for-introverts-5cac14ad4c77" target="_blank">
										Writing is Networking for Introverts									</a>
									 &nbsp;by Byrne Hobart									<br>
								</h5>
								
								<p>
									Takeaways
								</p>
								<div>
									<ul>
<li><span>“Networking is painful, and I’m suspicious of anyone who claims to enjoy it. Unfortunately for me, networking is effective: most good opportunities come from personal connections” — Byrne Hobart</span></li>
<li><span>Networking works, but it doesn’t work very well for people who are bad at striking up and maintaining conversations with strangers.&nbsp;</span></li>
<li><span>“There is a solution: be famous. You lose the ability to filter out who you want to talk to, but at least everyone starts the conversation with some context; you’re outsourcing the extroversion to them.” — Byrne Hobart</span>
<ul>
<li><span>Fame doesn’t just grow on trees. Byrne says there’s a second alternative called being </span><i><span>microfamous</span></i><span>, though. “Microfame is the best kind of fame because it combines an easier task (be famous to fewer people) with a better outcome (be famous to the right people).”</span></li>
<li><span>“If you’re trying to calibrate how hard it is to achieve micro-fame, focus on the micro, not the fame. Micro-fame just means your friends-of-friends have a nonzero chance of knowing who you are, and striking up a conversation with you about something mutually interesting.” — Byrne Hobart</span></li>
</ul>
</li>
<li><span>To achieve microfame for introverts, Byrne believes one way is to write. Specifically, “writing-as-networking strategy is that writing about your other interests gets other people interested. You’re not just identifying neighbors in your intellectual ghetto; you’re recruiting more. If more of your friends do it, you get exposed to more ideas.”&nbsp;</span></li>
<li><span>“This is not for everyone. There are some people who really love the idea of walking into a room full of strangers with a fat stack of business cards and making a bunch of valuable connections. But for those of us who faintly dread the prospect, writing is an alternative. If you put in the effort, you can substitute the worst parts of socializing for time spent alone with your computer.” — Byrne Hobart</span></li>
</ul>
								</div>
								<p><img src="https://gradually.co/wp-content/themes/gradually/img/two-cents.png">
								</p><!-- end half sqaure arrow -->
								<p><span>I certainly identify as an introvert and I used to think it was a weakness (and it somewhat is to a certain extent), but it’s also somewhat of a superpower too. You have the ability to listen more intently, pick up on things others probably wouldn’t, and weaponize your empathy for good. Writing can act as a bat signal for others interested in what you’re interested in. </span></p>
								<p>
								<span>Share</span> (if you're an OG)  &nbsp;  <span></span><span><span>
									</span><span>


							</span></span></p></div><!-- end newsletter wrap content -->
													
						</div><!-- end newsletter wrap content -->
					</div><!-- end newsletter wrap -->

			</div></article><!-- #post-## -->
		</div></div>]]>
            </description>
            <link>https://gradually.co/how-to-network-as-an-introvert/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25042288</guid>
            <pubDate>Tue, 10 Nov 2020 02:26:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[1984 by George Orwell [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25042280">thread link</a>) | @bra-ket
<br/>
November 9, 2020 | https://snewd.com/wp-content/uploads/2020/01/1984-George-Orwell.pdf | <a href="https://web.archive.org/web/*/https://snewd.com/wp-content/uploads/2020/01/1984-George-Orwell.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://snewd.com/wp-content/uploads/2020/01/1984-George-Orwell.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25042280</guid>
            <pubDate>Tue, 10 Nov 2020 02:24:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Best Practices for Writing Clean Interfaces in Go]]>
            </title>
            <description>
<![CDATA[
Score 42 | Comments 7 (<a href="https://news.ycombinator.com/item?id=25042085">thread link</a>) | @lanecwagner
<br/>
November 9, 2020 | https://qvault.io/2020/03/15/best-practices-for-writing-clean-interfaces-in-go/ | <a href="https://web.archive.org/web/*/https://qvault.io/2020/03/15/best-practices-for-writing-clean-interfaces-in-go/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text">
			
<p>Interfaces in Go allow us to treat different types as the same data type temporarily. They are central to a Go programmers toolbelt and are often used improperly by new Go developers… leading to hard to read and buggy code. Let’s take a look at some of the best practices for Golang interfaces.</p>



<h2>Recap on Interfaces</h2>



<p>I often look to the standard library as an example of the way to write clean Go. The standard error interface is simple:</p>



<pre><code lang="go">type error interface {
    Error() string
}</code></pre>



<p>The error interface encapsulates any type that has an <code>Error()</code> method defined on it. That method accepts no parameters, and returns a <em>string</em>. For example, let’s define a struct that represents a network problem:</p>



<pre><code lang="go">type networkProblem struct {
	message string
	code    int
}</code></pre>



<p>Then we define an <code>Error()</code> method:</p>



<pre><code lang="go">func (np networkProblem) Error() string {
	return fmt.Sprintf("network error! message: %s, code: %v", np.message, np.code)
}</code></pre>



<p>Now, we can use an instance of the <code>networkProblem</code> struct wherever an error is accepted.</p>



<pre><code lang="go">func handleErr(err error) {
	fmt.Println(err.Error())
}

np := networkProblem{
	message: "we received a problem",
	code:    404,
}

handleErr(np)

// prints "network error! message: we received a problem, code: 404"</code></pre>



<h2>Keep Interfaces Small</h2>



<p>If there is only one piece of advice that you take away from this article, make it this: <strong>keep interfaces small!</strong> Interfaces are meant to define the <em>minimal</em> behavior necessary to accurately represent an idea or concept. </p>



<p>Here is an example from the standard <a aria-label="HTTP package (opens in a new tab)" href="https://golang.org/pkg/net/http/#pkg-overview" target="_blank" rel="noreferrer noopener nofollow">HTTP package</a> of a larger interface that’s a good example of defining minimal behavior:</p>



<pre><code lang="go">type File interface {
    io.Closer
    io.Reader
    io.Seeker
    Readdir(count int) ([]os.FileInfo, error)
    Stat() (os.FileInfo, error)
}</code></pre>



<p>Any type that satisfies the interface’s behaviors can be considered by the HTTP package as a <em>File</em>. This is convenient because the HTTP package doesn’t need to know if it’s dealing with a file on disk, a network buffer, or a simple <code>[]byte</code>. </p>



<h2>Interfaces Should Have No Knowledge of Satisfying Types </h2>



<p>An interface should define what is necessary for other types to classify as a member of that interface. They shouldn’t be aware of any types that happen to satisfy the interface at design time.</p>



<p>For example, let’s assume we are building an interface to describe the components necessary to define a car.</p>



<pre><code lang="go">type car interface {
	GetColor() string
	GetSpeed() int
	IsFiretruck() bool
}</code></pre>



<p><code>GetColor()</code> and <code>GetSpeed()</code> make perfect sense, they are methods confined to the scope of a car. <code>IsFiretruck()</code> is an anti-pattern. We are forcing all cars to declare whether or not they are firetrucks. In order for this pattern to make any amount of sense, we would need a whole list of possible subtypes. <code>IsPickup()</code>, <code>IsSedan()</code>, <code>IsTank()</code>… where does it end??</p>



<p>Instead, the developer should have relied on the native functionality of <a href="https://yourbasic.org/golang/type-assertion-switch/" target="_blank" aria-label=" (opens in a new tab)" rel="noreferrer noopener nofollow">type assertion</a> to derive the underlying type when given an instance of the <strong>car</strong> interface. Or, if a sub-interface is needed, it can be defined as:</p>



<pre><code lang="go">type firetruck interface {
	car
	HoseLength() int
}</code></pre>



<p>Which inherits the required methods from <code>car</code> and adds one additional required method to make the car a <code>firetruck</code>.</p>



<h2>Interfaces Are Not Classes</h2>



<ul><li>Interfaces are not classes, they are slimmer.</li><li>Interfaces don’t have constructors or deconstructors that require that data is created or destroyed.</li><li>Interfaces aren’t hierarchical by nature, though there is syntactic sugar to create interfaces that happen to be supersets of other interfaces.</li><li>Interfaces define function signatures, but not underlying behavior. Making an interface often won’t <a aria-label="DRY (opens in a new tab)" href="https://en.wikipedia.org/wiki/Don%27t_repeat_yourself" target="_blank" rel="noreferrer noopener nofollow">DRY</a> up your code in regards to struct methods. For example, if five types satisfy the error interface, they all need their own version of the <code>Error()</code> function.</li></ul>







<h2>Related Work</h2>



<ul><li><a href="https://qvault.io/2020/03/29/how-to-separate-library-packages-in-go/">How To Separate Library Packages in Go</a></li><li><a href="https://qvault.io/2020/03/19/golang-mutexes-what-is-rwmutex-for/">Golang Mutexes – What Is RWMutex For?</a></li><li><a href="https://qvault.io/2020/02/20/how-to-build-jwts-in-go-golang/">How To Build JWT’s in Go (Golang)</a></li></ul>
		</div></div>]]>
            </description>
            <link>https://qvault.io/2020/03/15/best-practices-for-writing-clean-interfaces-in-go/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25042085</guid>
            <pubDate>Tue, 10 Nov 2020 01:53:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Debunking an election fraud claim using open data and Dolt]]>
            </title>
            <description>
<![CDATA[
Score 32 | Comments 21 (<a href="https://news.ycombinator.com/item?id=25041998">thread link</a>) | @proverbialbunny
<br/>
November 9, 2020 | https://www.dolthub.com/blog/2020-11-09-debunking-election-fraud/ | <a href="https://web.archive.org/web/*/https://www.dolthub.com/blog/2020-11-09-debunking-election-fraud/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://www.dolthub.com/blog/2020-11-09-debunking-election-fraud/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25041998</guid>
            <pubDate>Tue, 10 Nov 2020 01:34:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Try Design Thinking]]>
            </title>
            <description>
<![CDATA[
Score 44 | Comments 6 (<a href="https://news.ycombinator.com/item?id=25041961">thread link</a>) | @gbasin
<br/>
November 9, 2020 | https://garybasin.com/try-design-thinking/ | <a href="https://web.archive.org/web/*/https://garybasin.com/try-design-thinking/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://garybasin.com/try-design-thinking/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25041961</guid>
            <pubDate>Tue, 10 Nov 2020 01:27:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Replacing my phone battery with a cheap AliExpress knock-off]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25041894">thread link</a>) | @flotwig
<br/>
November 9, 2020 | https://zach.bloomqu.ist/blog/2020/11/aftermarket-cell-phone-battery.html | <a href="https://web.archive.org/web/*/https://zach.bloomqu.ist/blog/2020/11/aftermarket-cell-phone-battery.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> <p>This is a story of one manâ€™s quest for power.</p> <p>I purchased my current phone, a OnePlus 5T, in 2017. This summer, after about two and a half years of ownership, I noticed that it was no longer holding a charge all day. Frequently, the phone would reach 0% and shut off, right in the middle of tracking an evening bike ride or watching Netflix while cooking dinner. Although cell phone battery wear is a well-known issue, I got tired of it pretty quickly.</p> <p>I used the <a href="https://play.google.com/store/apps/details?id=com.digibites.accubattery&amp;hl=en_US&amp;gl=US">AccuBattery</a> app for about two months to try and get a handle on my battery health. It measured my phoneâ€™s amperage draw during the day and used that to estimate that of the 3300 milliamp-hour (mAh) capacity that my battery originally offered, only about 2400 mAh of capacity remained - only about 75% of the batteryâ€™s original health:</p> <p><img src="https://zach.bloomqu.ist/assets/battery/oem-capacity.png" alt="Screenshot of AccuBattery app for OEM battery capacity"></p> <p>This answered the question of â€œwhy does it feel like my phone is shutting off so quickly?â€� pretty clearly. Now, it was up to me to get a replacement battery.</p> <h3 id="attempting-to-get-a-genuine-battery">Attempting to get a genuine battery</h3> <p>My first thought was that I could simply order the OEM OnePlus 5T battery somewhere online. Why not? I found a page on the OnePlus website where prices are listed for replacement parts. The USA page was down at the time of this writing, but the <a href="https://www.oneplus.in/support/pricing/detail?code=7">India support page</a> lists a OnePlus 5T OEM battery replacement as being about $15.</p> <p>This seemed acceptable to me. My first thought was to email OnePlus support asking how to purchase the battery. Unfortunately, according to the service rep, they do not ship or sell OEM batteries without service:</p> <blockquote> <p>We would like to inform you that we do not ship or sell the accessories in any parts of the world, and all the repairs are carried out by our Authorized service centers only. So if you wish to get the device repaired, you can send it to the OnePlus authorized service center and get the same repaired.</p> </blockquote> <p>This is in line with what other OnePlus customers have reported - nobody, as far as I can tell, has ever been able to source OEM batteries from OnePlus, leaving DIY customers like myself to try and find knock-offs elsewhere.</p> <p>I wouldâ€™ve sent my phone in for repairs, but after I received the above email, I had such a long and terrible customer support experience trying to arrange the repair that by the end of it, I no longer trusted OnePlus to reliably service and return my phone. This lack of trust was reinforced by horror stories from other OnePlus customers - one customerâ€™s phone was <a href="https://www.reddit.com/r/oneplus/comments/eleckw/sent_my_oneplus_5_to_fort_worth_tx_for_repair_no/">lost by the Fort Worth, TX service center</a>, anotherâ€™s was <a href="https://www.reddit.com/r/oneplus/comments/depgkg/oneplus_lost_my_coworkers_phone_during_repair_at/">lost and took 4 weeks before being returned</a>, and yet another customer had <a href="https://www.reddit.com/r/oneplus/comments/jke2kd/sent_my_op3t_for_a_battery_replacement_oneplus/">their phone held hostage unless they agreed to repairing EVERYTHING instead of just getting the battery replaced</a>. These stories, combined with my awful customer support experience, convinced me that sending my phone in would be a truly bad idea.</p> <h3 id="buying-an-aftermarket-battery">Buying an aftermarket battery</h3> <p>Many people on the /r/oneplus5t subreddit have recommended purchasing a <a href="https://www.ifixit.com/Store/Android/OnePlus-5-5T-Replacement-Battery/IF330-018?o=2">replacement battery from iFixit</a>, but I felt like iFixit was simply selling cheap Chinese batteries with a nice label on them. I mean, if OnePlus can fix it for $15, why does the iFixit battery cost $30, if not for marketing?</p> <p>So, I hit up eBay and AliExpress, and eventually found the [sic] â€œSpecail Mobilephone Parts Storeâ€�, where they offer a <a href="https://web.archive.org/web/20201109223630/https://www.aliexpress.com/item/4000438352423.html">â€œ4650 mAhâ€� â€œPerfect business batteryâ€�</a> for the OnePlus 5T. With slogans like <a href="https://zach.bloomqu.ist/assets/battery/giant-energy-huge-capacity.webp">â€œGiant energy; huge capacityâ€�</a>, <a href="https://zach.bloomqu.ist/assets/battery/safety-does-not-explode.webp">â€œSafety does not explodeâ€�</a>, and <a href="https://zach.bloomqu.ist/assets/battery/ensure-qualified-and-safe-to-use.webp">â€œEnsure qualified and safe to useâ€�</a>, I felt confident that my $11.87 was going to a good place. I placed the order and, about 3 weeks later, I received the battery in my mailbox.</p> <h3 id="battery-physics-101">Battery Physics 101</h3> <p><img src="https://zach.bloomqu.ist/assets/battery/oem-and-aftermarket.jpg" alt="Photo of the OEM battery and the aftermarket battery side-by-side"> <small>The OEM battery (left) and the aftermarket battery installed (right).</small></p> <p>The first thing I noticed about the replacement battery was that the capacity was even HIGHER than what I ordered. The OnePlus 5T OEM battery is rated at 3300 mAh capacity, the AliExpress product page advertised a battery with 4650 mAh capacity, and the label on the battery I received claimed an astounding <em>5350 mAh</em> capacity - 162% of the OEM capacity. Clearly, I had gotten a great deal!</p> <p>The second thing I noticed was that the aftermarket battery was significantly lighter than the OEM battery. So much lighter that I weighed the batteries out of curiosity. The OEM OnePlus 5T battery weighed 47.0g. The aftermarket OnePlus 5T battery weighed 38.7g, or about 17% less.</p> <p>Itâ€™s amazing that Da Da Xiong was able to achieve 162% capacity with 17% less weight. Too amazing to be true, in fact.</p> <p>Via Wikipedia, I learned that the <a href="https://en.wikipedia.org/wiki/Specific_energy">specific energy</a> of a lithium-ion polymer battery can be up to <a href="https://en.wikipedia.org/wiki/Lithium-ion_battery">265 watt-hours per kilogram (Wh/kg)</a>. The nominal voltage of the lithium-ion polymer batteries here is about 3.8V. We can use <a href="https://en.wikipedia.org/wiki/Ohm%27s_law">Ohmâ€™s law</a> to calculate the maximum possible capacity of each battery based on weight, assuming that each battery is always supplying the nominal 3.8V.</p> <p>Letâ€™s start by calculating the maximum possible Amp-hours (Ah) per kilogram (kg) for a Li-ion poly battery at 3.8V, using Ohmâ€™s law:</p> <div><div><pre><code>265 Wh/kg / 3.8 V = 69 Ah/kg
</code></pre></div></div> <p>Now, we can calculate the maximum physically possible capacity for each battery by multiplying this number by the weights of each battery:</p> <div><div><pre><code>OEM battery:          .047 kg * 69 Ah/kg = 3.2 Ah = 3200 mAh
Aftermarket battery: .0387 kg * 69 Ah/kg = 2.6 Ah = 2600 mAh
</code></pre></div></div> <p>The astute reader might be wondering why this estimate for the maximum capacity of the OEM battery (3200 mAh) is less than the capacity OnePlus advertises (3300 mAh). Why is this? Well, itâ€™s because the assumption we made - that each battery is always supplying the nominal 3.8V - is false. The voltage output of a Li-ion poly battery <a href="https://learn.adafruit.com/li-ion-and-lipoly-batteries/voltages">drops over time</a>, so the calculation shown is only a lower bound approximation of each batteryâ€™s maximum capacity.</p> <p>I donâ€™t have information about the exact chemical composition of these batteries, nor the voltage charts, nor do I know what the upper and lower voltage limits are on the OnePlus 5T charging circuit. However, if we estimate that the voltage drops from 3.8V to 3.0V in a linear fashion (<code>V = 3.8 - .8t, 0 &lt;= t &lt;= 1</code>), we can use integration to arrive at approximately 3600 mAh maximum capacity for the OEM battery and 2900 mAh maximum capacity for the aftermarket battery.</p> <p>Even without exact numbers, these calculations demonstrate that <em>something</em> is fishy about the Da Da Xiong batteryâ€™s mAh claims.</p> <h3 id="real-world-usage">Real-world usage</h3> <p>Anyways, I didnâ€™t buy this shady AliExpress battery just so that I could do a bunch of math. I purchased it to restore my phoneâ€™s ability to last all day, and it has definitely succeeded at that. From a qualitative perspective, I now have enough juice to keep my phoneâ€™s battery fueled all day until I can recharge it at night.</p> <p>From a quantitative perspective, AccuBattery reports that the aftermarket battery has an estimated 3360 mAh capacity, which about matches the capacity of the OEM battery:</p> <p><img src="https://zach.bloomqu.ist/assets/battery/aftermarket-capacity.png" alt="Screenshot of AccuBattery app for aftermarket battery capacity"></p> <p>However, what AccuBattery fails to account for is the fact that once the aftermarket battery reaches 15%, the battery percentage begins to free-fall until it reaches 0% and shuts off. It seems like 15% on the aftermarket battery is equivalent to 1% on the OEM battery. I think this is because the Android OS cannot correctly estimate the batteryâ€™s remaining charge because it has different voltage characteristics than the OEM battery, but it doesnâ€™t really bother me, I just have to make sure that to charge the phone at 15% instead of 1%. This seems to be an extremely common experience with DIY battery replacements - even folks using the iFixit battery run in to this issue.</p> <p>If we take 15% off of AccuBatteryâ€™s estimated capacity, we get 2856 mAh, which is really really close to what a brand new OnePlus 5T reports - AccuBattery estimates the OEM battery as having ~3000 mAh capacity when it is brand new. That about matches my experience - with the Da Da Xiong battery, the phone is staying alive longer, almost like when it was new.</p> <h3 id="conclusions">Conclusions</h3> <ul> <li>Random Chinese batteries do not work as advertised - they will not magically double your phoneâ€™s battery capacity.</li> <li>However, random Chinese batteries work <em>almost as well</em> as brand new OEM batteries, but your battery percentage will forever be miscalibrated.</li> <li>Never trust OnePlus customer service.</li> </ul> </div></div>]]>
            </description>
            <link>https://zach.bloomqu.ist/blog/2020/11/aftermarket-cell-phone-battery.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25041894</guid>
            <pubDate>Tue, 10 Nov 2020 01:14:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Ladders of Wealth Creation]]>
            </title>
            <description>
<![CDATA[
Score 17 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25041361">thread link</a>) | @davefreiburger
<br/>
November 9, 2020 | https://gradually.co/roadmap-to-building-wealth/ | <a href="https://web.archive.org/web/*/https://gradually.co/roadmap-to-building-wealth/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main">
			<article id="post-878">
				<!--<a href="https://gradually.co/roadmap-to-building-wealth/">-->
					<div>
						<div>
	              			<!-- category colored coded display and hide takeaways php -->
	              											<p>																Wealth								</p><!-- end cat-wrap -->
						<p><span>&nbsp; •&nbsp; </span>
						<span>Roadmap to Building Wealth</span>
						<span> &nbsp;•&nbsp; </span>
						<span>
							November 9, 2020						</span>

						<img width="640" height="418" src="https://gradually.co/wp-content/uploads/2020/11/GD23-Wealth-1024x668.png" alt="" loading="lazy" srcset="https://gradually.co/wp-content/uploads/2020/11/GD23-Wealth-1024x668.png 1024w, https://gradually.co/wp-content/uploads/2020/11/GD23-Wealth-300x196.png 300w, https://gradually.co/wp-content/uploads/2020/11/GD23-Wealth-768x501.png 768w, https://gradually.co/wp-content/uploads/2020/11/GD23-Wealth-1536x1002.png 1536w, https://gradually.co/wp-content/uploads/2020/11/GD23-Wealth-2048x1336.png 2048w" sizes="(max-width: 640px) 100vw, 640px"></p><div>

																					<div>
								<p><a href="https://nathanbarry.com/wealth-creation/" target="_blank">
									[Image source: Nathan Barry]								</a></p><h5>
									<a href="https://nathanbarry.com/wealth-creation/" target="_blank">
										The ladders of wealth creation: a step-by-step roadmap to building wealth									</a>
									 &nbsp;by Nathan Barry									<br>
								</h5>
								
								<p>
									Takeaways
								</p>
								<div>
									<ul>
<li><span>“…Making money is a skill—like playing the drums or piano—that you can get better at over time. I wouldn’t expect to be able to sit down at a piano for the first time and immediately play a concerto.” — Nathan Barry</span></li>
</ul>
<ul>
<li><b><i>Your time for money</i></b><span>: the only skills you need are showing up consistently, being reliable, and learning new skills on the job. Nathan goes on to say, “Then in order to take the next step up the ladder you will need to specialize in certain skills (design, copywriting, legal, becoming a nurse, etc) to gain a salaried position.”&nbsp;</span></li>
</ul>
<ul>
<li><b><i>Your own service business</i></b><span>: This next rung on the ladder requires skills such as setting up a company, finding clients, creating proposals, pricing services, hiring employees, establishing an online presence, accounting, finance, business ops, etc.&nbsp;</span></li>
</ul>
<ul>
<li>
<ul>
<li><span>However, these two are probably the most important: follow up with customers and doing what you said you were going to do.&nbsp;</span></li>
</ul>
</li>
</ul>
<ul>
<li><b><i>Productized services</i></b><span>: Nathan mentions, “…To truly reach new levels of income you need to learn a different lesson: how to sell without ever talking to the customer.” This rung of the ladder requires you to learn how to write quality sales copy, design a sales page, process online payments, and create systems to deliver repeatable quality with each service.&nbsp;</span></li>
</ul>
<ul>
<li><b><i>Selling products</i></b><span>: Nathan continues, “A product takes far more work to create upfront, but then each individual sale and the fulfillment of that sale happens without much (or any) additional effort from the business owner.”&nbsp;</span></li>
</ul>
<ul>
<li><span>“All across society extra money—whether from a raise or working extra—disappears into lifestyle inflation or temporary purchases, when it could be put to work so much more effectively.” — Nathan Barry</span></li>
<li><span>You should always trade your time for money if:</span>
<ul>
<li><span>You’re early in your career and just starting out</span></li>
<li><span>You’re getting paid to learn a new skill while growing your earning potential</span></li>
<li><span>It’s a step in getting to a higher rung or on to the next ladder</span></li>
<li><span>You’re building relationships or finding mentors</span></li>
<li><span>The work is rewarding and meaningful in its own right</span></li>
</ul>
</li>
</ul>
								</div>
								<p><img src="https://gradually.co/wp-content/themes/gradually/img/two-cents.png">
								</p><!-- end half sqaure arrow -->
								<p><span>There’s no secret formula for building wealth. It takes a ton of hard work and time. At the end of the day, it boils down to how much of your time you’re willing to give up to make more money. If you’re able to learn how to make more money while giving up less of your time, you’re learning how to build wealth. There are no shortcuts.&nbsp; </span></p>
								<p>
								<span>Share</span> (if you're an OG)  &nbsp;  <span></span><span><span>
									</span><span>


							</span></span></p></div><!-- end newsletter wrap content -->
													
						</div><!-- end newsletter wrap content -->
					</div><!-- end newsletter wrap -->
				<!--</a>-->
			</div></article><!-- #post-## -->
		</div></div>]]>
            </description>
            <link>https://gradually.co/roadmap-to-building-wealth/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25041361</guid>
            <pubDate>Mon, 09 Nov 2020 23:47:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Benchmarking Pulsar and Kafka]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25041342">thread link</a>) | @ubolonton_
<br/>
November 9, 2020 | https://streamnative.io/blog/tech/2020-11-09-benchmark-pulsar-kafka-performance | <a href="https://web.archive.org/web/*/https://streamnative.io/blog/tech/2020-11-09-benchmark-pulsar-kafka-performance">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://streamnative.io/blog/tech/2020-11-09-benchmark-pulsar-kafka-performance</link>
            <guid isPermaLink="false">hacker-news-small-sites-25041342</guid>
            <pubDate>Mon, 09 Nov 2020 23:43:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Forget security champions, what about security advocates?]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25041132">thread link</a>) | @designthinker
<br/>
November 9, 2020 | https://jwgoerlich.com/security-culture-needs-security-advocates-design-monday/ | <a href="https://web.archive.org/web/*/https://jwgoerlich.com/security-culture-needs-security-advocates-design-monday/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
						
<p>“Everything is design. Everything.” — <a href="https://www.paulrand.design/">Paul Rand (1914–1996)</a></p>



<p>Paul Rand is behind so many stories this series has covered. The Olivetti Valentine typewriter designed by <a href="https://jwgoerlich.com/design-monday-valuing-assets/">Ettore Sottsass</a> and used by <a href="https://jwgoerlich.com/security-architecture-principles-design-monday/">Dieter Rams</a> in his documentary? Paul Rand did Olivetti’s US advertising. Speaking of Deiter Rams, the Braun shavers that made Rams famous? Paul Rand bought every model. (Though <a href="https://www.paulrand.design/life/interviews/1988-artograph.html">Rand once said</a> he would “buy just for their beauty and then put them in a drawer.”) IDEO, the birthplace of design thinking? Paul Rand did IDEO’s logo. He collaborated on a team with Charles Eames on <a href="https://www.paulrand.design/life/books-articles/articles/print/2011-the-interface.html">IBM’s Design Program</a>. I like to think some of that work was in the IBM plaza building that <a href="https://jwgoerlich.com/mies-and-ibm-plaza-knowing-when-more-is-more-design-monday/">Ludwig Mies van der Rohe</a> designed. The building, by the way, sported the iconic IBM logo which was, you guessed it, designed by Paul Rand.</p>



<p>Paul Rand was instrumental in creating the culture and discipline of graphic design. He taught the next generation at Yale from 1956 to 1985, with a break in the 1970s. Rand was visiting professor and critic at a number of other institutions. Check out the book <em>Paul Rand: Conversations with Students </em>for a view into that work. “What is design?” Paul would often ask. When he wasn’t creating, Rand was instructing, and through instruction, he was creating culture.</p>



<p>Like Paul Rand fostered designers who brought ideas to wider audiences, security leaders need to foster advocates who will bring security ideas to the wider workforce.</p>



<p>We don’t talk much about advocates. A security advocate is a member of the security team who focuses on getting practices into the hands of the workforce. It’s more common for us to talk about security champions. A security champion is a member of the business itself, who collaborates with the security team on best practices. A fully fleshed out security capability has advocates working with champions to interpret and implement security controls. In a well-run security capability, those controls will be usable and widely adopted, because of the partnership of advocates and champions.</p>



<p>To learn more about cyber security advocates and what they need to succeed, check out the “<a href="https://www.usenix.org/conference/soups2018/presentation/haney-perceptions">It’s Scary…It’s Confusing…It’s Dull</a>” research paper. These professionals “advocate for systems and policies that are usable, minimize requisite knowledge, and compensate for the inevitability of user error.”</p>



<p>Here are four practices from Paul Rand that we can apply to designing a security advocacy program:</p>



<p><strong>(1) Coach on tangible work, not abstract principles</strong>. Rand’s courses were practical not theoretical, with advice given based on the student’s work. He focused stories, literature, examples, and more through the lens of the work at hand.</p>



<p><strong>(2) Coach one-on-one, avoid one size fits all</strong>. Paul Rand worked individually with students, and a session on their work “went on as long as was necessary to set the student on the right track and was laced with stories from Paul’s vast career as they were appropriate to the issue at hand. When he worked with students, he poured his heart and soul into it.”</p>



<p><strong>(3) Use short cycle times</strong>. Typically, the criticism on individual work in Rand’s courses came weekly. Feedback was quick, specific, and direct. Compare this to many security programs where manager feedback comes at annual reviews.</p>



<p><strong>(4) Encourage personalization</strong>. Rand taught designers to build their own set of techniques, their own visual vocabulary, to solve problems. That’s not for the sake of originality. “Don’t try to be original,” Rand often said, “just try to be good.” It’s to develop a sense of the designer’s personal needs and strengths and how to mesh those with the audience’s instincts and intuitions. </p>



<p>When designing a cyber security program, give thought into how leadership will coach advocates. Give thought to how advocates will cultivate security champions. With a nod to Paul Rand, prompt both with a deceptively simple question. “What is security?”</p>



<figure><img src="https://jwgoerlich.com/wp-content/uploads/2020/11/paul-rand-abacus.jpg" alt="" srcset="https://jwgoerlich.com/wp-content/uploads/2020/11/paul-rand-abacus.jpg 514w, https://jwgoerlich.com/wp-content/uploads/2020/11/paul-rand-abacus-241x300.jpg 241w" sizes="(max-width: 514px) 100vw, 514px"><figcaption>Abacus Photogram, Photography by Paul Rand</figcaption></figure>



<hr>



<p><em>This article is part of a series on designing cyber security capabilities. To see other articles in the series, including a full list of design principles,&nbsp;</em><a href="https://jwgoerlich.com/principles-for-designing-security-capabilities/"><em>click here</em></a><em>.</em></p>
																		<p><span>Posted </span><a href="https://jwgoerlich.com/security-culture-needs-security-advocates-design-monday/" title="6:00 am" rel="bookmark"><time datetime="2020-11-09T06:00:00-05:00" pubdate="">November 9, 2020</time></a> by 					</p></div></div>]]>
            </description>
            <link>https://jwgoerlich.com/security-culture-needs-security-advocates-design-monday/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25041132</guid>
            <pubDate>Mon, 09 Nov 2020 23:13:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A positive version of BLACK MIRROR (book of short stories)]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25041043">thread link</a>) | @curatecuriosity
<br/>
November 9, 2020 | https://tinkeredthinking.com/bookstore/products/v1/ | <a href="https://web.archive.org/web/*/https://tinkeredthinking.com/bookstore/products/v1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<div>
			<div>
				<h3>The Lucilius Parables Vol. I</h3><hr>
				<p>34.50 -<span>Illustrated Edition, Hardcover - 284 pages</span></p>
			</div>
		</div>
		
		 
		<div>
			<p>A collections of modern parables from Tinkered Thinking.  </p>

<p>These short parables are for curious intellectuals - people who want to explore the strange possibilities of our time through short, lean narratives.  Read these stories in any order and let them stretch your brain with mind-bending meditations taking place in the past, present and the future.</p>

<p>Follow Lucilius through these tiny snapshots of his many lives, from the burning of the magnificent library of Alexandria, to his quiet ruminations aboard a whaling ship, to his exploration of distant planets and realities.  He exists at no time and all times, at every age, always meditating on that which makes us human and all that might make us more.  He is a selcouthist - a wanderer of human experience.</p>

<p> This edition of the parables has been fully edited, expanded and professionally reviewed.</p>

<p>In addition, each Parable is accompanied by a beautiful hand drawn illustration relating to each story.</p>

<p>Make this a thoughtful gift for those who have yet to discover Tinkered Thinking or carry the adventures of Lucilius with you for a refreshing reminder of the possibilities that exist behind each and every moment.</p>
		</div>
	</div><div>
		


		<div>
			<div>
				<h3>The Lucilius Parables Vol. I</h3><hr>
				<p>34.50</p>
				<p>Illustrated Edition, Hardcover - 284 pages</p>
				<p>A collections of modern parables from Tinkered Thinking.  </p>

<p>These short parables are for curious intellectuals - people who want to explore the strange possibilities of our time through short, lean narratives.  Read these stories in any order and let them stretch your brain with mind-bending meditations taking place in the past, present and the future.</p>

<p>Follow Lucilius through these tiny snapshots of his many lives, from the burning of the magnificent library of Alexandria, to his quiet ruminations aboard a whaling ship, to his exploration of distant planets and realities.  He exists at no time and all times, at every age, always meditating on that which makes us human and all that might make us more.  He is a selcouthist - a wanderer of human experience.</p>

<p> This edition of the parables has been fully edited, expanded and professionally reviewed.</p>

<p>In addition, each Parable is accompanied by a beautiful hand drawn illustration relating to each story.</p>

<p>Make this a thoughtful gift for those who have yet to discover Tinkered Thinking or carry the adventures of Lucilius with you for a refreshing reminder of the possibilities that exist behind each and every moment.</p> <br>
			</div>
			 
		</div>

	</div><p>
		<h5><em>Cost Transparency &amp; Sustainability</em></h5>
	</p><div id="collapseExample">
	  <div>
		    	<h4>Commitment to Sustainability</h4>
				<p>For Every purchase, <span>Tinkered Thinking</span> is allocating a portion of the proceeds to plant a tree.  Through a partnership with <a target="_blank" href="https://onetreeplanted.org/">OneTreePlanted</a> <span>Tinkered Thinking</span> is committed to making a contribution to the health of our planet.  It's thought of this way: a tree is cut down in order to make products like books, so why not plant a tree for every book sold?  This system creates a virtuous cycle where the cutting down of a tree used to make many books results in many more trees, each book like a seed, helping the movement towards a better future take root.</p>
				
				<p><img src="https://tinkeredthinking.com/static/img/OneTreeStamp.png">
				</p>

				<h4>Cost transparency</h4>
				<div><p>The cost of this book is a bit higher than what might be expected.  The reasons for this are several.  As an extension of <span>Tinkered Thinking</span>'s commitment to sustainable practice, the method by which this book is being produced is <em>print-on-demand</em>.  This means there is no stock.  A copy of this book is not physically produced <em>until a purchase is made</em>.  Not only does this make sense from a business perspective, but it eliminates the enormous risk of waste that often results when traditional book production creates a large quantity of books that are never purchased or read.  <em>Print-on-demand</em> solves this issue of waste in a powerful way.  The tradeoff is that when copies are produced individually, the cost is quite a bit higher than the per-unit-price of mass production.  A slight bummer, but the technology is still young and all signs point to <em>print-on-demand</em> as the way of the future.  Chances are high that costs will come down as the technology continues to scale.  </p><p><span>Tinkered Thinking</span> hopes you will consider these important details while perusing the bookstore.  And keep in mind, when you give a <span>Tinkered Thinking</span> Book <em>as a gift to someone</em>, you aren't just giving a book, you also get bragging rights: you get to say you planted a tree in that person's honor.</p></div>
	  </div>
	</div></div>]]>
            </description>
            <link>https://tinkeredthinking.com/bookstore/products/v1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25041043</guid>
            <pubDate>Mon, 09 Nov 2020 23:03:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Totem Alfa Romeo GT Electric]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25041002">thread link</a>) | @simonebrunozzi
<br/>
November 9, 2020 | https://www.totemautomobili.com/exterior/ | <a href="https://web.archive.org/web/*/https://www.totemautomobili.com/exterior/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-elementor-type="wp-page" data-elementor-id="29" data-elementor-settings="[]">
						<div>
							<div>
							<section data-id="7ad74aa" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;,&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
							
							<div>
							<div>
					<div data-id="d7256f5" data-element_type="column">
			<div>
							<div>
						
				
				<div data-id="d7758da" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="heading.default">
				<p>
			<h2>A LEGEND REBORN WITH ELECTRIC SOUL</h2>		</p>
				</div>
				
						</div>
					</div>
		</div>
				
								</div>
					</div>
		</section>
				<section data-id="9afbc47" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;,&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="7dce961" data-element_type="column">
			<div>
							<div>
						
				<div data-id="9167192" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="heading.default">
				<p>
			<h2>With the idea of revamping the car we elegantly redesigned the original lines giving the car a stunning shape whilst maintaining the authentic signature of the Giulia GTA.</h2>		</p>
				</div>
				<section data-id="1c78337" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="54c315c" data-element_type="column">
			<div>
							<div>
						<div data-id="734005a" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<p>Starting from one of the best iconic Italian cars of the 60’s &amp; 70’s, the Alfa Romeo Giulia GTA, we have created one of the most advanced restomod in the business, producing the ever fastest and most fascinating Giulia GTA.&nbsp;<br><span>The Giulia GTA was presented in 1965 and in the following seven&nbsp;</span><br></p>
				</div>
				</div>
						</div>
					</div>
		</div>
				<div data-id="7467847" data-element_type="column">
			<div>
							<div>
						<div data-id="2466c45" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<p><span>years obtained a series of successes and prizes which led this car to be&nbsp;</span><span>considered as a legend.&nbsp;</span>Our goal was to rebuild a car which remembers in spirit and shape the victorious Alfa of the 60ies, emerging as a reference for sportsmanship and craftsmanship.</p>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="a4079dc" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;,&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
							
							<div>
							<div>
					<div data-id="11124bb" data-element_type="column">
			<div>
							<div>
						
				
				<div data-id="957679a" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<div><p>Not only looking back to the past but also glancing at the future, we elegantly redesigned the original lines giving the car a stunning shape whilst maintaining the authentic signature of the Giulia GTA.</p><p><span>Similar considerations applied to the engineering and mechanical elements, planning the very best components and working with a team of Italian experts, we developed the new chassis setup designed to enclose an electric heart, bringing the car to extraordinary performance, whilst projecting it into the future.</span></p></div>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="33563bc" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="8f0a8bb" data-element_type="column">
			<div>
							<div>
						
				
				
				<div data-id="dddf15e" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<p>The design of our GT electric has been reshaped in forms and materials to create a timeless car with a smoother line. The formal search for details has given continuity to the past recovering some original features to apply in the new design.<br>We removed the roof drip molding from the external body, which was reconstructed entirely in carbon fiber, in order to make the car as light as possible and to obtain a perfect surface using new milling molds.</p>
				</div>
				</div>
				<section data-id="25e8e57" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						
		</section>
						</div>
					</div>
		</div>
				<div data-id="3f208ce" data-element_type="column">
			<div>
							<div>
						<div data-id="28a6348" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img width="2560" height="1707" src="https://www.totemautomobili.com/wp-content/uploads/2020/10/IMG_5484-scaled.jpg" alt="" loading="lazy" srcset="https://www.totemautomobili.com/wp-content/uploads/2020/10/IMG_5484-scaled.jpg 2560w, https://www.totemautomobili.com/wp-content/uploads/2020/10/IMG_5484-300x200.jpg 300w, https://www.totemautomobili.com/wp-content/uploads/2020/10/IMG_5484-1024x683.jpg 1024w, https://www.totemautomobili.com/wp-content/uploads/2020/10/IMG_5484-768x512.jpg 768w, https://www.totemautomobili.com/wp-content/uploads/2020/10/IMG_5484-1536x1024.jpg 1536w, https://www.totemautomobili.com/wp-content/uploads/2020/10/IMG_5484-2048x1365.jpg 2048w" sizes="(max-width: 2560px) 100vw, 2560px">											</p>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="7f2162b" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="425b843" data-element_type="column">
			<div>
							<div>
						<section data-id="8cea9f6" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="86eda9b" data-element_type="column">
			<div>
							<div>
						<div data-id="11f0ed2" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img width="475" height="488" src="https://www.totemautomobili.com/wp-content/uploads/2020/10/About.png" alt="" loading="lazy" srcset="https://www.totemautomobili.com/wp-content/uploads/2020/10/About.png 475w, https://www.totemautomobili.com/wp-content/uploads/2020/10/About-292x300.png 292w" sizes="(max-width: 475px) 100vw, 475px">											</p>
				</div>
				</div>
				<div data-id="62bc7be" data-element_type="widget" data-settings="{&quot;_position&quot;:&quot;absolute&quot;}" data-widget_type="image.default">
				<div>
					<p><img width="365" height="379" src="https://www.totemautomobili.com/wp-content/uploads/2020/10/about_2.png" alt="" loading="lazy" srcset="https://www.totemautomobili.com/wp-content/uploads/2020/10/about_2.png 365w, https://www.totemautomobili.com/wp-content/uploads/2020/10/about_2-289x300.png 289w" sizes="(max-width: 365px) 100vw, 365px">											</p>
				</div>
				</div>
						</div>
					</div>
		</div>
				<div data-id="2db8cdc" data-element_type="column">
			<div>
							<div>
						
				
				<div data-id="01606d3" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<div><p>Redesigning the Alfa Romeo Giulia GTA with a revived design that preserves the iconic soul, whilst reaches distinction for the craftsmanship of each component, Totem Giulia GT electric runs the road of the future as a result of advanced electrical technology, which improve its performance.</p>
<p><span>Maintaining 10% of the original chassis, to which a new full aluminum suspensions, specifically designed for the car, has been coupled, we substituted the classic fuel engine with an electric motor, able to grant an unparalleled power, a longer duration and a greener vision.</span></p></div>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="e61219f" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="d0ecafe" data-element_type="column">
			<div>
							<div>
						
				
				
				<div data-id="ad7c83b" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<div><p>The conversion process started from our beloved classic Alfa Romeo Giulia GT Junior 1300/1600, built between 1970 and 1975. We totally disassembled, stripped, unmounted the external panels. The frame was finely tuned and stiffened by hand to grant the significant power increase from the original 192bhp to 518bhp.</p><p>At the front axle we designed new MacPerson suspensions, at the rear axle we incorporated a multilink aluminum system, connected to a new rear sub-frame, which supports the electric motor, in line with the semiaxle.</p></div>
				</div>
				</div>
						</div>
					</div>
		</div>
				<div data-id="ba22c8b" data-element_type="column">
			<div>
							<div>
						<div data-id="4ec10b9" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img width="1600" height="1067" src="https://www.totemautomobili.com/wp-content/uploads/2020/10/Chassis.jpg" alt="" loading="lazy" srcset="https://www.totemautomobili.com/wp-content/uploads/2020/10/Chassis.jpg 1600w, https://www.totemautomobili.com/wp-content/uploads/2020/10/Chassis-300x200.jpg 300w, https://www.totemautomobili.com/wp-content/uploads/2020/10/Chassis-1024x683.jpg 1024w, https://www.totemautomobili.com/wp-content/uploads/2020/10/Chassis-768x512.jpg 768w, https://www.totemautomobili.com/wp-content/uploads/2020/10/Chassis-1536x1024.jpg 1536w" sizes="(max-width: 1600px) 100vw, 1600px">											</p>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="9d5624a" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="b0a8f18" data-element_type="column">
			<div>
							<div>
						<div data-id="ecb2218" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img width="800" height="534" src="https://www.totemautomobili.com/wp-content/uploads/2020/10/IMG_5511-1024x683.jpg" alt="" loading="lazy" srcset="https://www.totemautomobili.com/wp-content/uploads/2020/10/IMG_5511-1024x683.jpg 1024w, https://www.totemautomobili.com/wp-content/uploads/2020/10/IMG_5511-300x200.jpg 300w, https://www.totemautomobili.com/wp-content/uploads/2020/10/IMG_5511-768x512.jpg 768w, https://www.totemautomobili.com/wp-content/uploads/2020/10/IMG_5511-1536x1024.jpg 1536w, https://www.totemautomobili.com/wp-content/uploads/2020/10/IMG_5511-2048x1365.jpg 2048w" sizes="(max-width: 800px) 100vw, 800px">											</p>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="4641253" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;,&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					
				<div data-id="e1b0c30" data-element_type="column">
			<div>
							<div>
						
				
				<div data-id="281d3c3" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<p>The car is stiffened by an internal hidden roll bar, made in steel Fia fe45 40×2.5mm.<br><span>The bull-bar is applied in substitution of the external bumpers to improve its beauty while making it safer and reliable. To preserve quality and assure lifelong corrosion resistance, the whole chassis received a cataphoresis treatment.</span></p>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="a9484a3" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="12ae684" data-element_type="column">
			<div>
							<div>
						
				
				<div data-id="66a9b2e" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<p>Thanks to McFly technology by our partner 2electron, the car reproduces a realistic package of <b>performance</b>, <b>sound</b> and <b>vibration</b> starting from the soul of ICE engines up to completely blank sheet, where the customer can design with us his own experience.</p>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="fb8bbfd" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="7d5271e" data-element_type="column">
			<div>
							<div>
						<div data-id="69dd5fd" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<p>Thanks to the fusion of gaming and ICE calibration, engine torque, gear ratios, number of gear, power band, engine brake, sound and vibration and so many other features are realistic and customizable. We can use a <b>gear lever selector</b> digitally connected to our controller with the same mechanical feeling of a conventional one.</p>
				</div>
				</div>
						</div>
					</div>
		</div>
				<div data-id="f47fe9c" data-element_type="column">
			<div>
							<div>
						<div data-id="9264661" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<p><span>It’s&nbsp;</span><span>easy to change</span><span>: you can switch engines, select the features and drive.<br></span><span>It will be possible to choose from unlimited sound layers, building your own and unique experience, even those that exist only in the creative mind.<br>Very wide possibility of customization in every engine point.</span></p>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="36c455bd" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						
		</section>
				<section data-id="3fbf672" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="cf01059" data-element_type="column">
			<div>
							<div>
						<div data-id="b2497a8" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="heading.default">
				<p>
			<h2>Engine torque, power band, number of gears, gear ratios, rev limiter and other amazing features will be tailor-made designed.</h2>		</p>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="25ab659" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="48a5be6" data-element_type="column">
			<div>
							<div>
						<div data-id="ae5d8d8" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img width="1141" height="557" src="https://www.totemautomobili.com/wp-content/uploads/2020/10/casse-2.jpg" alt="" loading="lazy" srcset="https://www.totemautomobili.com/wp-content/uploads/2020/10/casse-2.jpg 1141w, https://www.totemautomobili.com/wp-content/uploads/2020/10/casse-2-300x146.jpg 300w, https://www.totemautomobili.com/wp-content/uploads/2020/10/casse-2-1024x500.jpg 1024w, https://www.totemautomobili.com/wp-content/uploads/2020/10/casse-2-768x375.jpg 768w" sizes="(max-width: 1141px) 100vw, 1141px">											</p>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="cd5ae48" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						
		</section>
				<section data-id="0dfe93c" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="17ac646" data-element_type="column">
			<div>
							<div>
						
				<div data-id="b377055" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<p>Take full advantage of the torque curves of the emulated engine: the Sw will manage in real time the optimal gear based on your driving style.<br>Get to the limiter when you drive on track or shift to the next gear at low revs if you want to enjoy the view! To have a more aggressive behavior you can use the gear lever to interact even when the mode is active.</p>
				</div>
				</div>
						</div>
					</div>
		</div>
				<div data-id="a44926d" data-element_type="column">
			<div>
							<div>
						<div data-id="603db8d" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img width="2096" height="824" src="https://www.totemautomobili.com/wp-content/uploads/2020/11/Gear-Ratio-GT-electric.png" alt="" loading="lazy" srcset="https://www.totemautomobili.com/wp-content/uploads/2020/11/Gear-Ratio-GT-electric.png 2096w, https://www.totemautomobili.com/wp-content/uploads/2020/11/Gear-Ratio-GT-electric-300x118.png 300w, https://www.totemautomobili.com/wp-content/uploads/2020/11/Gear-Ratio-GT-electric-1024x403.png 1024w, https://www.totemautomobili.com/wp-content/uploads/2020/11/Gear-Ratio-GT-electric-768x302.png 768w, https://www.totemautomobili.com/wp-content/uploads/2020/11/Gear-Ratio-GT-electric-1536x604.png 1536w, https://www.totemautomobili.com/wp-content/uploads/2020/11/Gear-Ratio-GT-electric-2048x805.png 2048w" sizes="(max-width: 2096px) 100vw, 2096px">											</p>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="a11ea71" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="3160b71" data-element_type="column">
			<div>
							<div>
						
				<div data-id="26c8493" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<p>The software controls acceleration based on engine specifications to make the car accelerate smoothly and as fast as possible, avoiding spinning of the drive wheels to ensure you the best standing start procedure matching also with immersive sound and rev limiter feature</p>
				</div>
				</div>
						</div>
					</div>
		</div>
				<div data-id="b266ae3" data-element_type="column">
			<div>
							<div>
						
				<div data-id="6f0e305" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<p>Use the emulated engine braking in downshift to start a drift.<br>The Sw will manage the regenerative braking in order to reproduce the engine inertia increasing the Rpm during the downshift. The behavior is completely customizable by driver.</p>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="2b3e806" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						
		</section>
				<section data-id="43f4f07" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="aaffbee" data-element_type="column">
			<div>
							<div>
						
				
				<div data-id="9f22d6b" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<p>The front and the rear of our GT electric were redesigned to give a specific identity to the car, to enhance the emotional driving experience and its sporty and contemporary character, whilst staying true to the original car personality and its legendary past.</p>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="a32f60c" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;,&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="b1eb9e6" data-element_type="column">
			<div>
							<div>
						<div data-id="3bb9685" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img width="1600" height="1051" src="https://www.totemautomobili.com/wp-content/uploads/2020/10/foto-due-frontale-copia.jpg" alt="" loading="lazy" srcset="https://www.totemautomobili.com/wp-content/uploads/2020/10/foto-due-frontale-copia.jpg 1600w, https://www.totemautomobili.com/wp-content/uploads/2020/10/foto-due-frontale-copia-300x197.jpg 300w, https://www.totemautomobili.com/wp-content/uploads/2020/10/foto-due-frontale-copia-1024x673.jpg 1024w, https://www.totemautomobili.com/wp-content/uploads/2020/10/foto-due-frontale-copia-768x504.jpg 768w, https://www.totemautomobili.com/wp-content/uploads/2020/10/foto-due-frontale-copia-1536x1009.jpg 1536w" sizes="(max-width: 1600px) 100vw, 1600px">											</p>
				</div>
				</div>
						</div>
					</div>
		</div>
				<div data-id="e6d4aed" data-element_type="column">
			<div>
							<div>
						
				
				<div data-id="51cff60" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<p><span>At the front we evoked the competitive fame of the Giulia GTam making it 180mm wider, selecting headlights in LED technology inspired by rally cars and a carbon front grille embellished with geometric patterns.</span></p>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="4bc9b67" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;,&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="21486d3" data-element_type="column">
			<div>
							<div>
						
				
				<div data-id="e08c28d" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="text-editor.default">
				<div>
					<p>The rear was redesigned to provide a curvy shape, while accommodating new LED taillights and two large extractors based on an initial aerodynamic study of the whole coefficient. To balance the design we also agreed to move down the license plate.<br></p>
				</div>
				</div>
						</div>
					</div>
		</div>
				<div data-id="355159e" data-element_type="column">
			<div>
							<div>
						<div data-id="8f54fde" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img width="1600" height="1055" src="https://www.totemautomobili.com/wp-content/uploads/2020/10/3.jpg" alt="" loading="lazy" srcset="https://www.totemautomobili.com/wp-content/uploads/2020/10/3.jpg 1600w, https://www.totemautomobili.com/wp-content/uploads/2020/10/3-300x198.jpg 300w, https://www.totemautomobili.com/wp-content/uploads/2020/10/3-1024x675.jpg 1024w, https://www.totemautomobili.com/wp-content/uploads/2020/10/3-768x506.jpg 768w, https://www.totemautomobili.com/wp-content/uploads/2020/10/3-1536x1013.jpg 1536w" sizes="(max-width: 1600px) 100vw, 1600px">											</p>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="655fe3f" data-element_type="section" data-settings="{&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="9459db0" data-element_type="column">
			<div>
							<div>
						<div data-id="bfb885b" data-element_type="widget" data-widget_type="flip-box.default">
				<div>
					<div>
			
			<div>
			<div>
				<div>
					
											<p>
							The electric motor in place generates 518bhp, with a couple of 940 Nm (or 692 Ft lbs), which
accelerates our Totem GT electric from 0 to 100 Km/h in 3,4”.<br>We implemented also an electronic power control to manage the power, with 3 efficiency settings: D (dynamic), N (natural) and A
(advanced).						</p>
					
								</div>
		</div>
		</div>
		</div>
				</div>
				</div>
						</div>
					</div>
		</div>
				<div data-id="f15b940" data-element_type="column">
			<div>
							<div>
						<div data-id="526cd3c" data-element_type="widget" data-widget_type="flip-box.default">
				<div>
					<div>
			
			<div>
			<div>
				<div>
					
											<p>
							We are proud to present one of the safest and lightest battery packs in the world, with only 350 Kg of weight and 50,4 Kwh.<br>The internal modules are certified UN38.3 (Immersion Cooled Modular
Battery Pack System submerges cells directly in 3M™ Novec™ Engineered Fluid to deliver unprecedented continuous power to electric vehicles).<br>With a full charge pack you will be able to
drive your Totem GT electric for about 320 KM at a standard pace.						</p>
					
								</div>
		</div>
		</div>
		</div>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section data-id="db749be" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;,&quot;ekit_has_onepagescroll_dot&quot;:&quot;yes&quot;}">
						<div>
							<div>
					<div data-id="72bc0f2" data-element_type="column">
			<div>
							<div>
						<div data-id="f7fd38b" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="heading.default">
				<p>
			<h4>Subscribe to our newsletter</h4>		</p>
				</div>
				<div data-id="3e72f97" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;}" data-widget_type="heading.default">
				<p>
			<h5>Don't miss new updates on your email</h5>		</p>
				</div>
				
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
						</div>
						</div>
					</div></div>]]>
            </description>
            <link>https://www.totemautomobili.com/exterior/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25041002</guid>
            <pubDate>Mon, 09 Nov 2020 22:59:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Introduction to ARIA for Web Accessibility]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25040841">thread link</a>) | @kliwo
<br/>
November 9, 2020 | https://kliwo.com/aria-for-web-accessibility/ | <a href="https://web.archive.org/web/*/https://kliwo.com/aria-for-web-accessibility/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			

<p>In this article I will introduce you to ARIA for web accessibility is and how you can implement it to help your community; especially people with a disability; to be included and to be able to access and navigate your blog or website. To not make anyone digitally impaired.</p>



<p>Disability is not uncommon, it is estimated that about 15% of the world’s population suffers from some level of disability; that is about a billion people, mind-boggling isn’t it?</p>



<p>While not all of the billion people’s ability is impaired to interact with the web it is nevertheless important to create an environment that is accessible to everyone.</p>



<h2><span id="what_is_aria_for_web_accessibility"></span>What is ARIA for web accessibility<span></span></h2>



<p>ARIA stands for <a href="https://www.w3.org/WAI/standards-guidelines/aria/" target="_blank" aria-label="Accessible Rich Internet Application (opens in a new tab)" rel="noreferrer noopener">Accessible Rich Internet Application</a> and was created for the main purpose to provide accessibility and usability for users with disabilities who use AT (Assistive Technologies).</p>



<p>An example of an AT is a screen reader; this is a program that reads information out loud meanwhile it permits the user to navigate with a keyboard. A screen reader is the main tool for the visually impaired to navigate on the web.</p>



<p>In other words, ARIA just provides more explicative coding that the AT converts to information about parts of the UI, for example, menus, forms, and pop-up alerts.</p>



<h2><span id="use_aria_for_a_raking_boost"></span>Use ARIA for a raking boost<span></span></h2>



<p>A website that offers accessibility usually uses good principles when coding and in web design and therefore works better on the computer and mobile devices that ultimately rank higher on <a aria-label="SEO (opens in a new tab)" href="https://kliwo.com/on-page-seo/" target="_blank" rel="noreferrer noopener">SEO</a>.</p>



<p>You may not know that accessibility is an important part of SEO; not offering accessibility may lead to ranking penalties from Google and other search engines. By making simple changes to your HTML source code can give your site an SEO boost.</p>



<h2><span id="how_to_implement_aria"></span>How to implement ARIA<span></span></h2>



<p>With only simple changes in your HTML coding you can make your website accessible; adding <strong>alt text</strong> to your images, utilize <strong>headings,</strong> and include keyboard controls for interactive elements.</p>



<p>HTML 5 made it very easy, just using the default syntaxes and using them correctly you already completed the first step of making your webpage ARIA friendly. However the screen readers don’t have the ability to see what’s on the site, yet, so some dynamic, interactive, and javascript elements will be skipped by the screen readers. This is where ARIA can help bridge those gaps.</p>



<h2><span id="html_before_aria"></span>HTML before ARIA<span></span></h2>



<p>Always use HTML syntaxes primarily. HTML is, as you just learned, the foundation of web accessibility; and you should ONLY use ARIA when the semantics you are searching for is not available in HTML.</p>



<p>Screen readers work in such a way that they translate and read out loud HTML code that describes accessibility information for example a &lt;nav&gt; or &lt;button&gt; but not a &lt;div&gt;.</p>



<p>An example of how NOT to use HTML coding is to create a button using a &lt;div&gt;, you could ALWAYS use the correct HTML semantic, &lt;button&gt;.</p>



<h2><span id="aria_roles"></span>ARIA roles<span></span></h2>



<p>Elements on a page such as navigation bar, buttons, and links are defined as roles. The role function is to help screen readers tell the user how to interact with elements and what they do. Roles are divided into four different categories, landmark, document, widgets, and abstract roles.</p>



<h3><span id="landmark_roles"></span>Landmark roles<span></span></h3>



<p>Landmark roles separate a page into different parts, menu, main content search bar, etc. To separate the page into different sections helps the user to easier find information and to navigate.</p>



<h3><span id="document_roles"></span>Document roles<span></span></h3>



<p>The document roles define the content of a webpage, they define particular sections within a page, documents, articles, headings, and lists are some examples.</p>



<h3><span id="widgets_roles"></span>Widgets roles<span></span></h3>



<p>Widget roles are elements and interfaces of a page, they often describe javascript-based interfaces or the more complex parts of a web page, some examples are alerts, buttons, and text boxes.</p>



<h3><span id="abstract_roles"></span>Abstract roles<span></span></h3>



<p>The abstract roles are used by the browser and are the basis of how the other ARIA roles, landmark, document, and widget roles are defined. You don’t bother with them, one thing less to worry about.</p>



<h3><span id="aria_testing"></span>ARIA testing<span></span></h3>



<p>Once you implement ARIA on your web page I strongly recommend that you test it using a screen reader. You would want everything to go smoothly and to provide the best possible experience for all your users. To test your web page you download a free screen reader, eg. ChromeVox if you use Chrome, ideally blindfold so you can get the full user experience.&nbsp;</p>



<p>These are the basics of ARIA, hopefully, it feels not as intimidating as it did in the beginning. Once you break down something into smaller parts it automatically gets much simpler to grasp. However this is just the beginning of ARIA, you would be surprised how much you can do and achieve with ARIA.</p>



<p>I strongly believe to include everyone and everybody can contribute towards that goal; whether you can contribute by using ARIA to make someone feel included or you achieve this goal another way does not matter, you made a difference.</p>
		</div></div>]]>
            </description>
            <link>https://kliwo.com/aria-for-web-accessibility/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25040841</guid>
            <pubDate>Mon, 09 Nov 2020 22:41:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Abstract Machines: Interpreters for Computer Scientists]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25040388">thread link</a>) | @sinistersnare
<br/>
November 9, 2020 | https://drs.is/post/abstract-machines/ | <a href="https://web.archive.org/web/*/https://drs.is/post/abstract-machines/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content"><p>
Nov 09, 2020
· 21 min
</p><nav>
<ul>
<li>
<a href="https://drs.is/tags/plt/">
PLT
</a>
</li>
<li>
<a href="https://drs.is/tags/computer-science/">
Computer Science
</a>
</li>
<li>
<a href="https://drs.is/tags/interpreters/">
Interpreters
</a>
</li>
<li>
<a href="https://drs.is/tags/abstract-machines/">
Abstract Machines
</a>
</li>
<li>
<a href="https://drs.is/tags/continuations/">
Continuations
</a>
</li>
</ul>
</nav>
<hr>


<hr>

<p>So im a PhD student now, so I need to write about cool computer science things!
As part of my learnings, I have been writing a bunch of 'Abstract Machines'.
I think of them as how computer scientists do programming languages research without computers.
I mean, they have to use <em>something</em> to write their super complex papers.</p>
<blockquote>
<p>"Computer Science is no more about computers than astronomy is about telescopes"</p>
<ul>
<li>Edsger Dijkstra</li>
</ul>
</blockquote>
<p>We love computers, but they are merely a tool of computing. The real study
of computation can be done without them, and the theories of how programming
languages function is not excluded from that. So how do computer scientists
study interpreters without a computer? The theory of
abstract machines is one of the more popular ways.</p>

<p>What does it mean to compute something? Humans are pretty good at just looking at things
and formulating an answer. For example, a person does not use a sorting algorithm when matching
socks after their laundry is finished. But computers can't just intuit a solution. They are given
precise instructions on what to do to accomplish something. So how can we model that,
and use it to inform the science behind computation?</p>
<p>'Abstract Machines' were created to model real computation strategies. These are functions that take program states, and return some value. Program states can be composed of many different things. The simplest abstract machines simply use the current point that the program is at. Others include a mapping of variables to values, so we can keep state around. We will describe such machines, and what kind of languages they can describe.</p>
<p>These machines, in practice, are interpreters. They are called 'abstract' because the theory on them is not specific to any exact language. You can make an abstract machine for whatever you could want: Lisp, Java bytecode, RISC-V assembly language...</p>

<p>Operational Semantics are how we can write semantics of a language
using pen and paper. Using a simple syntax, we can get the idea of
'if the syntax looks like this, it can be evaluated into this'.</p>
<p>The idea of semantics is separate from execution of a program. Semantics describe what something 'means', based on how it looks. Here are two different kinds of 'operational semantics', big-step and small-step.</p>
<h2 id="big-step">Big Step</h2>
<p>Big step evaluations have the type <code>State -&gt; Value</code>,
meaning that you give them a state, and it will tell you which value
it exactly evaluates to. These are nice and simple generally, because they
give you the values in a single step.</p>
<p>To use a small example expression, <code>if cond then e_true else e_false</code>
there are two rules that will be used.</p>
<ol>
<li>If cond evaluates to <code>true</code>, then the result is what <code>e_true</code> evaluates to.</li>
<li>If cond evaluates to <code>false</code>, then the result is what <code>e_false</code> evaluates to.</li>
</ol>
<p>These rules assume that we have some way to fully evaluate whatever <code>cond</code> is. But whatever that way is is unimportant to the rule of evaluating <code>if</code> statements. This separation is very important for operational semantics. We can write small rules for specific parts of the language, all of which get composed together into a fully formalized machine.</p>
<p>I like using big step when thinking exactly about what expressions <em>do</em>.</p>
<h2 id="small-step">Small Step</h2>
<p>Small step semantics have the type <code>State -&gt; State</code>. They will
show you what to do step by step to evaluate a term. They work iteratively,
explicitly, to show how a given state is computed.</p>
<p>For example, if the term is <code>if cond then e_true else e_false</code>,
There will be a few different rules.</p>
<ol>
<li>If cond is an atomic value, and that value is <code>true</code>, the resulting state is <code>e_true</code>.</li>
<li>If cond is an atomic value, and that value is <code>false</code>, the resulting state is <code>e_false</code>.</li>
<li>If cond is not an atommic value, then evaluate it to <code>cond'</code> , and return <code>if cond' then e_true else e_false</code>.</li>
</ol>
<p>An atomic value is a value that can not be broken up any more. This means a datatype, not a complex expression. In these simple machines, the only atomic datatype is a number. Don't tell computer scientists about quarks, they may go insane.</p>
<p>By going from a state to a next state, small step mechanics more closely follow
how our computers work. They dont evaluate to values directly, they just... keep going.</p>
<p>This may confuse a new reader, if it keeps going, how do we know its done? Big step rules
directly result in a value, full stop. How do we know a state in small step semantics is the one with the value? We use what is called a 'fixpoint', or more simply, we evaluate until there isnt a meaningful change in the state after running. If we evaluate a math expression enough times,
it will decompose into a single number, upon which evaluation will lead to itself. That means there is no more work to be done, and evaluation stops.</p>
<p>The same is true in a language like <code>C</code>, after the final instruction in <code>main</code>, if we try to evaluate any more, nothing will happen, theres nothing left to do. That is a fixpoint.</p>
<p>Small step semantics are a bit more precise in my opinion, and they are much easier to translate
to <em>real</em> interpreters. But it is very useful to understand both styles, they have different uses. Using small steps, we can also more closely trace how an expression is evaluated.</p>

<p>I dont mean the the C language, especially because I would not use simple to describe it!
C in this case stands for 'control'. You may remember things like 'if' are called 'control flow
operators'. Control is the currently running 'thing' in your program. The <code>if</code> operator, is a way to change the control based on a condition. This machine will be called
the C machine because you only need control to represent the state of the entire program.</p>
<p>C machines are not capable of much, only simple rewriting of expresions, because they dont have any information other than the program's control itself to go off of.
One kind of language we can formalize with a C machine is that of mathematical expressions.</p>
<p>Here is a big-step semantics of type  (takes a math expression and returns a number). This means that the control we choose to use is a math expression. The result of evaluating a math expression is a number, of course, so thats the value.</p>


<p>This rule shows how to add expressions to end up with a number. You can understand it by reading the half under the bar as 'this is what we start and end with' and the half above the bar as 'these must be true to use these semantics'. You read  as 'evalutes to'.</p>
<p>You can read this rule like so:</p>
<ol>
<li>If we have some control that looks like  (this is from the bottom left, before the arrow)</li>
<li>if  evaluates to some number  (the first expression above the bar),</li>
<li>if  evaluates to some number  (the second expression above the bar),</li>
<li>if  added to  is equal to some number  (the third expression above the bar).</li>
<li>THEN we know that the expression in step 1 evaluates to .</li>
</ol>
<p>This may seem a bit backwards, we implement adding by adding? Well, the key is that expressions are complex, they can be composed of other expressions or just values. These rules show how to do math on expressions by first evaluating them to values. Then once they are values, it is quite easy to do math operations on them.</p>
<p>Note the distinction between the arrow  and  here.  is saying "left evaluates to right by virtue of applying this machine's rules." and  is saying "you can substitute left for right".</p>
<p>Authors of semantics like these love to use different looking arrow symbols, they all mean the same thing. Usually in big step they use a cool down-facing arrow like . In small step they will use a more boring arrow like .</p>
<h3 id="example">Example</h3>
<p>Lets evaluate a simple math expression to show how you can use these rules to prove that expressions are evaluated correctly using a machines rules.</p>

<p>Here, at the first, bottom most level,  and . Then we need to prove that <code>7 + 3 = 10</code>, and we do that with another application of the addition rule! We could have chosen <code>3 + 4</code> to be be , but it doesnt matter for addition, and we leave issues like that to a parser. I noted each level with the rule that was used to evaluate it. It is generally showed like this, but I usually dont show them. I only have so much horizontal space on this webpage!</p>
<p>To evaluate simple mathematical expressions, we only need a control for the state. C machines are only capable of evaluating simple programs. What if we want to add a simple programming construct like variables? For that, we need a place to store them. And so, the CE machine is born!</p>

<p>To evaluate variables, we need to be able to keep track of what value they hold at a given program point. in a C machine, if we are given <code>a + 2</code>, we have no way of know what <code>a</code> is, because its not a number, and we can only deal with syntax as we see it. But if we gave a machine both <code>a + 2</code>, the control, and a mapping <code>{a : 4}</code>, an environment, we can evaluate the expression to 6!</p>
<p>So, a big-step CE machine doesnt just have <code>MathExp</code> (C) for state anymore, we need an <code>Env</code> (E) to accompany it! The function is now of type . This means that our machine will take 2 arguments, a math expression and an environment, and it will return a computed number.</p>


<p>This simply states that if the expression is a variable, not an artithmetic expression,
the value it returns is what the environment says it is. We use the greek
letter rho ('ρ') (not the letter 'p') to represent the environment. This is what is used
in the literature, and its always best to follow along with norms to avoid confusion! For the history on why they used  for this task, you will have to ask someone smarter than me.</p>
<p>We dont have variable binding yet, but you can imagine an expression like:
<code>m*c*c</code> with an environment <code>{m : 12 c : 299792458}</code>, which will return some number for us.
We have made a calculator with predefined constants! We can put pi in there,
or tau if you are a lunatic...</p>

<p>Wow, it seems like we can do a lot with just a control and an environment. We could further extend this to things like variable assignment (again, these are big-step …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://drs.is/post/abstract-machines/">https://drs.is/post/abstract-machines/</a></em></p>]]>
            </description>
            <link>https://drs.is/post/abstract-machines/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25040388</guid>
            <pubDate>Mon, 09 Nov 2020 21:50:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Simple approach to time series spike detection, using website visits data]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25040241">thread link</a>) | @querystash
<br/>
November 9, 2020 | https://app.querystash.com/query/d72aac44f94c57cf513d29e625a34201 | <a href="https://web.archive.org/web/*/https://app.querystash.com/query/d72aac44f94c57cf513d29e625a34201">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://app.querystash.com/query/d72aac44f94c57cf513d29e625a34201</link>
            <guid isPermaLink="false">hacker-news-small-sites-25040241</guid>
            <pubDate>Mon, 09 Nov 2020 21:35:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[WebRTC Video Streaming]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25040014">thread link</a>) | @Iwontgo
<br/>
November 9, 2020 | https://antmedia.io/webrtc-video-streaming/ | <a href="https://web.archive.org/web/*/https://antmedia.io/webrtc-video-streaming/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="panel-34528-0-0-1" data-index="1"><div>
<div> <p><span>WebRTC stands for web real-time communications. <a href="https://webrtc.org/" target="_blank" rel="noopener noreferrer nofollow">WebRTC</a> is a very exciting, powerful, and highly disruptive cutting-edge technology and streaming protocol.</span></p> <p><span>WebRTC is HTML5 compatible and you can use it to add real-time media communications directly between browser and devices. And you can do that without the need of any prerequisite of plugins to be installed in the browser. </span><span>And Webrtc is progressively becoming supported by all major modern browser vendors </span><span>including Safari, Google Chrome, Firefox, Opera and others</span><span>.&nbsp;</span></p> <p><span>Thanks to WebRTC technology, you can embed the real-time video directly into your browser-based solution to create an engaging and interactive streaming experience for your audience without worrying about the delay. WebRTC video streaming is just changing the way of engagement in the new normal.</span></p></div></div></div></div>]]>
            </description>
            <link>https://antmedia.io/webrtc-video-streaming/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25040014</guid>
            <pubDate>Mon, 09 Nov 2020 21:15:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Lessons Learned Building an Open Source MLOps Platform]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25039872">thread link</a>) | @ChefboyOG
<br/>
November 9, 2020 | https://www.cortex.dev/post/building-an-open-source-mlops-platform | <a href="https://web.archive.org/web/*/https://www.cortex.dev/post/building-an-open-source-mlops-platform">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div content-type="article"><p>For the last two years, we’ve been working on Cortex, our open source machine learning deployment platform. Over that time, we’ve been really fortunate to see it grow into what it is today, used in production by teams around the world, and supported by a fantastic community of contributors.</p><p>We’ve also had to change our thinking several times along the way. The understanding of the ML ecosystem we had at the beginning has not always turned out to be accurate, and this is reflected in various changes we’ve made to Cortex.</p><p>As interest in MLOps continues to increase, I thought it would be useful (for our sakes as much as anyone else’s) to document a few of the key lessons we’ve learned that’ve come to shape Cortex.</p><p>If you’re working on a production machine learning system, building machine learning infrastructure, or designing your own MLOps tool, hopefully the following lessons (listed in no particular order) are useful for you.</p><h3>1. Production machine learning runs in the cloud</h3><p>When Cortex was still in its idea stage, one of our most frequent discussions was whether or not it should support on-premise deployments. At the time, the worry was that a large portion of the machine learning ecosystem was going to remain on-premise indefinitely due to privacy and cost.</p><p>These worries were enflamed when we initially released Cortex. While we had some excited users, we also had plenty of people writing in requesting on-prem support. We worried that by going all-in on the public clouds, we’d cut off most of the machine learning ecosystem.</p><p>Over the last two years, things have changed. Production machine learning is almost entirely moving to the cloud, and there are a couple reasons why.</p><p>The first is the standard reason for moving to the cloud: scalability. As production machine learning systems become more powerful and responsible for more features, their workloads increase. If you need to autoscale to dozens of GPUs during peak hours, the cloud has obvious advantages.</p><p>The second is the investment by the major clouds into ML-specific offerings. Major clouds now offer both dedicated software and hardware for machine learning. For example, Google and AWS both offer ASICs (TPUs and Inferentia, respectively) that substantially improve machine learning performance, and both are only available on their respective clouds.</p><p>More and more, the cloud is becoming the only realistic way to deploy production machine learning systems.</p><h3>2. It’s too early for end-to-end MLOps tools</h3><p>Another misguided belief we held in Cortex’s early days was that Cortex needed to be an all-inclusive, end-to-end MLOps platform that automated your pipeline from raw data to deployed model.</p><figure id="w-node-a47abffb7609-258b12d0"><p><img src="https://uploads-ssl.webflow.com/5f6030edfd63364a668b1265/5fa9ad5cb4b8083a07ddc765_0*6z89yrFEvqkVBqIg.jpeg" alt=""></p></figure><p>We’ve written a full <a href="https://towardsdatascience.com/we-tried-to-build-an-end-to-end-ml-platform-heres-why-it-failed-190c0f503536" target="_blank">breakdown of why that was the wrong decision,</a> but the short version is that it’s still way too early in the lifespan of MLOps to build that sort of platform.</p><p>Every page of the production machine learning playbook is constantly being rewritten. For example, in the last several years:</p><ul role="list"><li><strong>Our notion of “big” models has exploded.</strong> We thought models with hundreds of millions of parameters were flirting with boundaries of being “too large” to deploy. Then Transformer models like GPT-2 started weighing in the billions—and people still built applications out of them.</li><li><strong>The ways we train models have changed. </strong>Transfer learning, neural architecture search, knowledge distillation—we have more techniques and tools than ever to design, train, and optimize models efficiently.</li><li><strong>The machine learning toolbox has grown rapidly</strong>. PyTorch was only released in 2016, shortly after TF Serving’s initial public release. ONNX came out in 2017. The frameworks, languages, and features that an end-to-end MLOps platform would need to support changes endlessly.</li></ul><p>We ran into all of these problems with our first release of Cortex. We provided a seamless experience—<em>if</em> <em>you used the narrow stack we supported.</em> Because everything (including language, pipeline, frameworks, and even team structure) can vary so wildly across ML orgs, we were almost always “one feature away” from fitting any given team’s stack.</p><p>As a modular platform, focused on one discrete part of the machine learning lifecycle—deployment—without opinions about the rest of the stack, Cortex has been adopted by many more teams at a much faster pace. We’ve seen rapid growth in other MLOps tools with similar “best of breed” approaches at different parts of the stack, including <a href="https://dvc.org/" target="_blank">DVC (Data Version Control) </a>and <a href="https://www.comet.ml/site/" target="_blank">Comet</a>.</p><h3>3. Data science, ML engineering, and ML infrastructure are all different — in theory</h3><p>With Cortex, we use the following high-level model of an ML function and its constituent parts:</p><ul role="list"><li><strong>Data science</strong>. Concerned with the development of models, from exploring the data to conducting experiments to training and optimizing models.</li><li><strong>Machine learning engineering</strong>. Concerned with the deployment of models, from productionizing models to writing inference services to designing inference pipelines.</li><li><strong>Machine learning infrastructure</strong>. Concerned with the design and management of the ML platform, from resource allocation to cluster management to performance monitoring.</li></ul><p>And in theory, these are nicely delineated functions with clear handoff points. Data science creates models which are turned into inference pipelines by ML engineering and deployed to a platform maintained by ML infrastructure.</p><p>But, this is an overview of the theoretical functions in an ML org, not the <em>actual roles</em> people hold. Oftentimes, a data scientist will also do ML engineering work, or an ML engineer will be tasked with managing an inference cluster.</p><p>Building a tool for these different use-cases gets complex, as the optimal ergonomics of an interface for one role can vary drastically from another.</p><p>For example, <a href="https://towardsdatascience.com/why-we-do-machine-learning-engineering-with-yaml-not-notebooks-a2a97f5e04f8" target="_blank">for reasons we’ve explained before</a>, Cortex APIs are written as Python scripts with YAML manifests, not notebooks, and are deployed via a CLI. </p><p>For MLEs, this is comfortable. For data scientists, however, it is often uncomfortable, as YAML and CLIs aren’t common tools in their ecosystem. Because of this, we needed to build a Python client for defining deployments in pure Python in order for some teams to use Cortex successfully.</p><p>Now, people who are more comfortable with CLIs can deploy like this:</p><figure id="w-node-b878c71bee4b-258b12d0"><p><img src="https://uploads-ssl.webflow.com/5f6030edfd63364a668b1265/5fa9ad5c272d5ce9d4c2dd28_0*yV51u9hxfGDvxtF3.png" alt=""></p></figure><p>And people more comfortable with pure Python can do this:</p><figure id="w-node-5864f8b2c1a4-258b12d0"><p><img src="https://uploads-ssl.webflow.com/5f6030edfd63364a668b1265/5fa9ad5c09e66f37c2c56cdf_1*1CO_-hPGhV9qNuH4c3Jxuw.png" alt=""></p></figure><p>The takeaway here is that if you’re building MLOps tooling, remember everyone who will be using it in practice, not just in theory.</p><h3>4. ML native companies have different needs</h3><p>Several years ago, the most common examples of production machine learning were popular products optimized by trained models. Payment processors would sprinkle in fraud detection models, streaming platforms would boost their engagement with recommendation engines, etc.</p><p>Now, however, there is a new wave of companies whose products aren’t enhanced by models—they <em>are</em> models.</p><p>These companies, which we refer to as ML native, operate in different ways. Some sell access to an inference pipeline as an API, as in the case of <a href="https://www.glisten.ai/" target="_blank">Glisten</a>, whose API allows retailers to tag and categorize products instantly:</p><figure id="w-node-dc634e21281f-258b12d0"><p><img src="https://uploads-ssl.webflow.com/5f6030edfd63364a668b1265/5fa47e5ab8c60a850d3f4032_0*mw_tAL1CeDD1Q8e3.png" alt=""></p></figure><p>Others build applications whose core functionality is provided by a trained model. For example, <a href="https://postera.ai/" target="_blank">PostEra’s</a> medicinal chemistry platform uses models to predict the most likely chemical reactions for creating a specific drug, and <a href="https://play.aidungeon.io/main/home" target="_blank">AI Dungeon</a> uses a trained language model to create an endless choose-your-own-adventure:</p><p>These ML native applications have different infrastructure needs. For one, they typically rely on realtime inference, meaning their models need to be deployed and available at all times.</p><p>Ensuring this availability can get very expensive. <a href="https://medium.com/@aidungeon/how-we-scaled-ai-dungeon-2-to-support-over-1-000-000-users-d207d5623de9" target="_blank">AI Dungeon uses a 6 GB model</a> that can only handle a few concurrent requests and requires GPUs for inference. To scale to even a few thousand concurrent users, they need many large GPU instances running at once—something that is costly to sustain for long periods.</p><p>When we first built Cortex, we hadn’t worked with many ML native teams. After working with them, we wound up prioritizing a new set of features, many of which were at least in part aimed at helping control inference costs:</p><ul role="list"><li>Request-based autoscaling to optimally scale each model for spend</li><li>Spot instance support to allow for cheaper base instance prices</li><li>Multi-model caching, live reloading, and multi-model endpoints to increase efficiency</li><li>Inferentia support for more cost-effective and performant instance types</li></ul><p>As the number of ML native companies continues to rise quickly, MLOps tools and platforms are going to have to build for their needs.</p><h3>5. MLOps is production machine learning’s biggest bottleneck</h3><p>This is one of the few things we believed before building Cortex that we still find to be true today. It is the feasibility of building and deploying a production machine learning system prevents teams from using ML. </p><p>Training and retraining models is not cheap. Deploying models to production isn’t cheap either. Building a platform to support those deployments is a full-scale infrastructure project, one that has to be maintained moving forward.</p><p>These costs make machine learning unapproachable for most companies. and the frustrating part is that they aren’t intrinsic qualities of machine learning. We can solve them with better infrastructure—no ML research breakthroughs needed.</p><p>As the MLOps ecosystem matures, new tools will continue to abstract away these parts of infrastructure and nullify the costs that prohibit teams from using ML in production. If you want to accelerate the proliferation of machine learning, consider contributing to any of the many open source MLOps projects—<a href="https://github.com/cortexlabs/cortex" target="_blank">like this one</a>.</p><p>‍</p></div></div>]]>
            </description>
            <link>https://www.cortex.dev/post/building-an-open-source-mlops-platform</link>
            <guid isPermaLink="false">hacker-news-small-sites-25039872</guid>
            <pubDate>Mon, 09 Nov 2020 21:04:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why Developers Love Rust]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25039826">thread link</a>) | @ibraheemdev
<br/>
November 9, 2020 | https://ibraheem.ca/posts/why-devs-love-rust | <a href="https://web.archive.org/web/*/https://ibraheem.ca/posts/why-devs-love-rust">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Rust has been getting a lot of media attention recently. It seems like a "X written in Rust" post makes the front page of hackernews every other day. Rust has been <a href="https://insights.stackoverflow.com/survey/2020#technology-most-loved-dreaded-and-wanted-languages">voted the most loved language</a> for five years running, and it <a href="https://octoverse.github.com/#fastest-growing-languages">grew in use on Github</a> by <strong>235%</strong> from 2018 to 2019. Large companies such as Mozilla, Apple, Amazon, Facebook, Google, Twitter, and Microsoft have began adopting it in their codebases. So, why do so many people love Rust?</p>
<p>Rust was built to solve many of the hassles associated with other popular languages. Let's look at a couple of examples:</p>
<h4 id="memory-safety"><a href="#memory-safety" aria-label="memory safety permalink"></a><strong>Memory Safety</strong></h4>
<p>Rust focuses on speed and safety. It balances speed and safety through many ‘zero-cost abstractions’. This means that in Rust, abstractions cost as little as possible in order to make them work. The ownership system is a prime example of a zero cost abstraction. All of the analysis we’ll talk about in this section is done at compile time. You do not pay any run-time cost for any of these features.</p>
<p>To track the ownership of each value: a value can only be used at most once, after which the compiler refuses to use it again.</p>
<p>For example, the following code:</p>
<div data-language="rust"><pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> original <span>=</span> String<span>::</span><span>from</span><span>(</span><span>"hello"</span><span>)</span><span>;</span>
    <span>takes_ownership</span><span>(</span>original<span>)</span><span>;</span>
    <span>println!</span><span>(</span><span>"{}"</span><span>,</span> original<span>)</span>
<span>}</span> 

<span>fn</span> <span>takes_ownership</span><span>(</span>other<span>:</span> String<span>)</span> <span>{</span>
    <span>println!</span><span>(</span><span>"{}"</span><span>,</span> other<span>)</span><span>;</span>
<span>}</span> </code></pre></div>
<p>Yields an error:</p>
<div data-language="rust"><pre><code>error<span>[</span>E0382<span>]</span><span>:</span> borrow of moved value<span>:</span> `original`
 <span>-</span><span>-&gt;</span> src<span>/</span>main<span>.</span>rs<span>:</span><span>4</span><span>:</span><span>20</span>
<span>3</span> <span>|</span>   <span>takes_ownership</span><span>(</span>original<span>)</span><span>;</span>
  <span>|</span>                     <span>-</span> value moved here
<span>4</span> <span>|</span>   <span>println!</span><span>(</span><span>"{}"</span><span>,</span> original<span>)</span>
  <span>|</span>                    <span>^</span> value borrowed here after mov</code></pre></div>
<p>In the above code, the ownership of <code>original</code> was moved to the <code>take_ownership</code> function. Because the ownership was moved, Rust now cleans up the memory of <code>original</code>. Now, the compiler prevents you from using <code>original</code>. </p>
<p>Rust's ownership model guarantees, at compile time, that your application will be safe from dereferencing null or dangling pointers This prevents the dreaded double-free regularly encountered in C or C++, along with many other memory related issues.</p>
<p>In Rust, functions can <em>borrow</em> ownership of a value. Rust tracks borrowed ownership with the borrow checker. We can modify the example above to borrow <code>original</code>, instead of taking ownership:</p>
<div data-language="rust"><pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> original <span>=</span> String<span>::</span><span>from</span><span>(</span><span>"hello"</span><span>)</span><span>;</span>
    <span>borrow_ownership</span><span>(</span><span>&amp;</span>original<span>)</span><span>;</span>
    <span>println!</span><span>(</span><span>"{}"</span><span>,</span> original<span>)</span>
<span>}</span> 

<span>fn</span> <span>borrow_ownership</span><span>(</span>other<span>:</span> <span>&amp;</span>String<span>)</span> <span>{</span>
    <span>println!</span><span>(</span><span>"{}"</span><span>,</span> other<span>)</span><span>;</span>
<span>}</span></code></pre></div>
<p>Now the code compiles, because the ownership of <code>original</code> stays in the main function. Instead of owning the resource, the function borrows ownership. We call the &amp;T type a ‘reference’. A binding that borrows something does not deallocate the resource when it goes out of scope. This means that after the borrow, we can use our original bindings again.</p>
<p>Rust memory safety comes at the cost of complexity. New developers often complain that getting a program to compile can be quite difficult. It’s pretty common for newcomers to the Rust community to get stuck "fighting the borrow checker". As <a href="https://news.ycombinator.com/item?id=23437202#unv_23437831">Rust learner</a> explained:</p>
<blockquote>
<p>"It's hard but I love it. Dealing with the compiler felt like being the novice in an old kung fu movie who spends day after day being tortured by his new master (rustc) for no apparent reason until one day it clicks and he realizes that he knows kung fu."</p>
</blockquote>
<p>Fighting the borrow checker can be frustrating, but trust me, it's worth it. Rust is often compared to Haskell and Scala in the sense that if your code compiles, you can sleep at night without having to worry about runtime errors. This is even more true after looking at the memory safety Rust enforces through its ownership model.</p>
<p>Rust also has a second language hidden inside it that doesn’t enforce memory safety guarantees: it’s called <em>unsafe Rust</em>. Wrapping code with the <code>unsafe</code> block effectively tells the compiler to shut up, because you know what you are doing. Doing so gives you <em>unsafe superpowers</em>. For example, you can dereference a raw pointer:</p>
<div data-language="go"><pre><code>let mut num <span>=</span> <span>5</span><span>;</span>

let r1 <span>=</span> <span>&amp;</span>num as <span>*</span><span>const</span> i32<span>;</span>
let r2 <span>=</span> <span>&amp;</span>mut num as <span>*</span>mut i32<span>;</span>

unsafe <span>{</span>
  <span>println</span><span>!</span><span>(</span><span>"r1 is: {}"</span><span>,</span> <span>*</span>r1<span>)</span><span>;</span>
  <span>println</span><span>!</span><span>(</span><span>"r2 is: {}"</span><span>,</span> <span>*</span>r2<span>)</span><span>;</span>
<span>}</span></code></pre></div>
<p>If you can't do something in safe Rust, you can implement it yourself with <code>unsafe</code>. However, <code>unsafe</code> should be used with caution. Abusing it can have unwanted consequences. Because of this, Rust forces you to explicitly mark code as unsafe. You cannot use an unsafe function in a safe block. Many developers even opt to mark there entire project with <code>![forbid(unsafe_code)]</code>.</p>
<h4 id="rust-vs-dynamic-languages"><a href="#rust-vs-dynamic-languages" aria-label="rust vs dynamic languages permalink"></a><strong>Rust vs. Dynamic Languages</strong></h4>
<p>Developers coming from dynamically typed languages will find it hard to argue the benefits of static typing. Static type definitions are even being added to many popular dynamic languages, such as javascript's <a href="https://www.typescriptlang.org/">typescript</a>, python's <a href="https://github.com/python/mypy">type hints</a>, and ruby's <a href="https://github.com/ruby/rbs">rbs</a>. Static languages are generally considered more "scalable" and better for larger codebases as the compiler does much of the work for you. Let's look at an example:</p>
<div data-language="ruby"><pre><code><span>def</span> <span><span>silly</span></span><span>(</span>a<span>)</span>
  <span>if</span> a <span>&gt;</span> <span>0</span>
    puts <span>'hello'</span>
  <span>else</span>
    print a <span>+</span> <span>'3'</span>
  <span>end</span>
<span>end</span></code></pre></div>
<p>The code above prints 'hello', right? Let's test it out:</p>

<p>But, when you pass a negative number:</p>
<div data-language="ruby"><pre><code>$ silly<span>(</span><span>-</span><span>1</span><span>)</span>
<span>=</span><span>&gt;</span> <span>TypeError</span> <span>(</span><span>String</span> can't be coerced into <span>Integer</span><span>)</span></code></pre></div>
<p>You get a <code>TypeError</code> at runtime. </p>
<p>A simple mistake like this can cause runtime errors that can be hard to debug without comprehensive test coverage. Since Rust is statically typed, all type errors will be caught at compile time, and this problem never occurs.</p>
<p>Static typing also results in compiled code that executes faster as the compiler knows the exact data types that are in use, and therefore can produce optimized machine code.</p>
<p>The points in this section apply to pretty much all strongly typed languages. Now let's look at some of the things Rust does differently than other statically typed languages.</p>
<h4 id="no-nulls"><a href="#no-nulls" aria-label="no nulls permalink"></a><strong>No Nulls</strong></h4>
<p>Most languages have a concept of null. Any value can either be what you expect, or nothing at all. If you accidentally miss a null check, you code can blow up at runtime. Tony Hoare, the inventor of null references had <a href="https://qconlondon.com/london-2009/qconlondon.com/london-2009/speaker/Tony+Hoare.html">this to say</a> about the concept:</p>
<blockquote>
<p>I call it my billion-dollar mistake. It was the invention of the null reference in 1965. At that time, I was designing the first comprehensive type system for references in an object oriented language (ALGOL W). My goal was to ensure that all use of references should be absolutely safe, with checking performed automatically by the compiler. But I couldn't resist the temptation to put in a null reference, simply because it was so easy to implement. This has led to innumerable errors, vulnerabilities, and system crashes, which have probably caused a billion dollars of pain and damage in the last forty years</p>
</blockquote>
<p>Rust, unlike most other languages, does not have a concept of null. It does not exist! If <code>x = 1</code>, then x <em>is</em> an integer, and will <em>always</em> be an integer.</p>
<p>Rust expresses optional values with an type called <code>Option</code>: </p>
<div data-language="rust"><pre><code><span>pub</span> <span>enum</span> Option<span>&lt;</span>T<span>&gt;</span> <span>{</span>
    None<span>,</span>
    <span>Some</span><span>(</span>T<span>)</span><span>,</span>
<span>}</span></code></pre></div>
<p>An <code>Option</code> is either something, or nothing. This union is expressed succinctly with Rust enum's, which can hold values. You can pattern match on an option enum to access the underlying value:</p>
<div data-language="rust"><pre><code><span>match</span> x <span>{</span>
  None <span>=&gt;</span> <span>handle_none</span><span>(</span><span>)</span><span>,</span>
  <span>Some</span><span>(</span>value<span>)</span> <span>=&gt;</span> <span>return</span> value
<span>}</span></code></pre></div>
<p>But what happens if you forget to check for <code>None</code>? Doesn't this pose the same problems as null? Nope! Rust solves this problem my enforcing exhaustive pattern matching. This means that this code, which does not check for <code>None</code>:</p>
<div data-language="rust"><pre><code><span>match</span> x <span>{</span>
  <span>Some</span><span>(</span>value<span>)</span> <span>=&gt;</span> <span>println!</span><span>(</span><span>"{}"</span><span>,</span> value<span>)</span>
<span>}</span></code></pre></div>
<p>Will not compile:</p>
<div data-language="rust"><pre><code>error<span>[</span>E0004<span>]</span><span>:</span> non<span>-</span>exhaustive patterns<span>:</span> `None` not covered
<span>-</span><span>-&gt;</span> src<span>/</span>main<span>.</span>rs<span>:</span><span>6</span><span>:</span><span>11</span>
  <span>|</span>
<span>6</span> <span>|</span>  <span>match</span> x <span>{</span>
  <span>|</span>  <span>^</span><span>^</span><span>^</span><span>^</span><span>^</span><span>^</span> pattern `None` not covered
  <span>|</span>
  <span>=</span> help<span>:</span> ensure that all possible cases are being handled<span>,</span> 
    possibly by adding wildcards or more <span>match</span> arms</code></pre></div>
<p>In Rust, the code above would never make it to production, and clients would never experience the error because the compiler is so strict. Also note how detailed the error message is, telling you the exact location, problem, and potential solution to the error.</p>
<h4 id="rust-vs-statically-typed-languages"><a href="#rust-vs-statically-typed-languages" aria-label="rust vs statically typed languages permalink"></a><strong>Rust vs. Statically Typed Languages</strong></h4>
<p>Rust does its best to get out of the developer's way when it comes to static typing. Rust has a very smart type inference engine. It looks not only at the type of the value expression during its initialization but also at how the variable is used afterwards to infer its type. However, Rust's use of type inference does not decrease its ability to provide detailed error messages at compile time. Let's see how that type inference works. </p>
<p>We can start by initializing a integer:</p>

<p>Because of the annotation, the compiler knows that elem is of type u8. Now we can create a mutable vector (a growable array):</p>
<div data-language="rust"><pre><code><span>let</span> <span>mut</span> vec <span>=</span> Vec<span>::</span><span>new</span><span>(</span><span>)</span><span>;</span></code></pre></div>
<p>At this point the compiler doesn't know the exact type of the vector. It just knows that it's a vector of something (<code>Vec&lt;_&gt;</code>). But once we insert the element into the vector</p>

<p>Aha! Now the compiler knows that <code>vec</code> is a vector of u8's (<code>Vec&lt;u8&gt;</code>)</p>
<p>No type annotation of variables was needed, the compiler is happy and so is the programmer!</p>
<h4 id="rust-vs-garbage-collected-languages"><a href="#rust-vs-garbage-collected-languages" aria-label="rust vs garbage collected languages permalink"></a><strong>Rust vs. Garbage Collected Languages</strong></h4>
<p>Garbage collection is an automatic memory management system that looks for unused variables and frees their memory. It is a concept employed by many widely used languages, such as Java, Ruby, and Python. However, garbage collection can introduce performance issues at scale.</p>
<p>For example, <a href="https://discord.com/">Discord</a> used Golang, a garbage collected language, for keeping track of which channels and messages a user read. They began experiencing latency and CPU spikes consistently every 2 minutes. This is because Go will force a garbage collection run every 2 minutes, scanning the entire LRU cache to determine which memory needed to be handled by GC.</p>
<p>Here is a before and after of them switching from Go, to Rust. Go is purple, Rust is blue.</p>
<p><img src="https://ibraheem.ca/media/rustvsgo-discord.png"></p>
<p>Read the full post here: <a href="https://blog.discord.com/why-discord-is-switching-from-go-to-rust-a190bbca2b1f">Why Discord is Switching from Go to Rust</a></p>
<p>Why is Rust so much better? Rust is blazingly fast and memory-efficient without needing a garbage collector, due to its ownership model. Here is a simple example:</p>

<p>Thanks to Rust's ownership tracking, the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ibraheem.ca/posts/why-devs-love-rust">https://ibraheem.ca/posts/why-devs-love-rust</a></em></p>]]>
            </description>
            <link>https://ibraheem.ca/posts/why-devs-love-rust</link>
            <guid isPermaLink="false">hacker-news-small-sites-25039826</guid>
            <pubDate>Mon, 09 Nov 2020 20:59:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Haskell: The Bad Parts, part 2]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25039758">thread link</a>) | @anuragsoni
<br/>
November 9, 2020 | https://www.snoyman.com/blog/2020/11/haskell-bad-parts-2 | <a href="https://web.archive.org/web/*/https://www.snoyman.com/blog/2020/11/haskell-bad-parts-2">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><i>See a typo? Have a suggestion?
<a target="_blank" rel="nofollow" href="https://github.com/snoyberg/snoyman.com/edit/master/content/posts/haskell-bad-parts-2.md">Edit this page on Github</a>
</i>
</p>

<p>If you didn’t see it, please check out <a href="https://www.snoyman.com/blog/2020/10/haskell-bad-parts-1">part 1 of this series</a> to understand the purpose of this. Now, for more bad parts!</p>
<h2>Partial functions (in general)</h2>
<p>Laziness very likely belongs in this list. My favorite part of criticizing laziness is how quickly people jump to defend it based on edge cases. So let’s be a bit more nuanced before I later get far <em>less</em> nuanced. Laziness is <strong>obviously</strong> a good thing. Strictness is <strong>obviously</strong> a good thing. They also both suck. It depends on context and purpose. Each of them introduce different kinds of issues. The real question is: what’s a more sensible default? We’ll get to that another time.</p>
<p>I called this section partial functions. Am I having a senior moment? Maybe, but I intentionally started with laziness. In a strict language, function calls can result in exceptions being thrown, segfaulting occurring, or panicking. (And if I write a “Rust: The Bad Parts”, believe me, I’ll be mentioning panicking.) The fact that a function <em>acts</em> like it can successfully perform something, but in fact fails in a predictable way (like failing a <code>HashMap</code> lookup), it should be reflected at the type level. If not, ya dun goofed.</p>
<p>Also, if you have a language that doesn’t let you reflect this information at the type level: ya dun goofed.</p>
<p>Partial functions are the antithesis of this concept. They allow you to say “yeah dude, I can <em>totally</em> give you the first value in an empty list.” Partial functions are like politicians: you can tell they’re lying because their lips are moving. (“But Michael,” you say. “Functions don’t have lips!” Whatever, I’m waxing poetical.)</p>
<p>Alright, so plenty of languages screw this up. Haskell tells those languages “hold my beer.”</p>
<p><img src="https://www.snoyman.com/static/images/holdmybeer.jpg"></p><p>Haskell screws up partial functions way, way worse than other languages:</p>
<ol>
<li>It promotes a whole bunch of them in the standard libraries and <code>Prelude</code>.</li>
<li>Some libraries, like <code>vector</code> (I’m getting to you, don’t worry) make it <em>really</em> confusing by providing an <code>index</code> and <code>unsafeIndex</code> function. Hint: <code>index</code> isn’t really safe, it’s just less unsafe.</li>
<li>There’s no obvious way to search for usages of these partial functions.</li>
<li>And, by far, the worst…</li>
</ol>
<h3>Values are partial too!</h3>
<p>Only in a lazy language does this exist. You call a function. You get a result. You continue working. In any other non-lazy language, that means you have a value. If I have a <code>u32</code> in Rust, I actually have a <code>u32</code> in Rust. Null pointers in languages like C and Java somewhat muddy this situation, but at least primitive types are really there if they say they’re there.</p>
<p>No, not Haskell. <code>x :: Int</code> may in fact not exist. It’s a lie. <code>let x = head [] :: [Int]</code> is a box waiting to explode. And you find out <em>much</em> later. And it’s even worse than that. <code>let alice = Person { name = "Alice", age = someAge }</code> may give you a valid <code>Person</code> value. You can evaluate it. But Cthulhu help you if you evaluate <code>age alice</code>. Maybe, just maybe, <code>someAge</code> is a bottom value. Boom! You’ve smuggled a dirty bomb out.</p>
<p>I’m not advocating for removing laziness in Haskell. In fact I’m not really advocating for much of anything in this series. I’m just complaining, because I like complaining.</p>
<p>But <em>if</em> I was to advocate some changes:</p>
<ul>
<li>Deprecate partial functions</li>
<li>Introduce a naming scheme for partial functions to be more obvious</li>
<li>Introduce a compiler warning to note partial function use (with a pragma to turn off specific usages)</li>
<li>Warn by default on partial pattern matches</li>
<li>Advocate strict data fields by default</li>
</ul>
<h3>But ackshualllly, infinite loops</h3>
<p>Someone’s gonna say it. So I’ll say it. Yes, without major language changes, you can’t prevent partial functions. You can’t even detect them, unless Turing was wrong (and I have my suspicions.) But Haskell community, please, please learn this lesson:</p>
<p><strong>DON’T LET THE PERFECT BE THE ENEMY OF THE GOOD</strong></p>
<p>We can get rid of many of the most common partial functions trivially. We can detect many common cases by looking for partial pattern matches and usage of <code>throw</code> (again, horribly named function). “But we can’t get everything” doesn’t mean “don’t try to get something.”</p>
<h2>Hubris</h2>
<p>Given what I just said, we Haskellers have a lot of hubris. Each time you say “if it compiles it works,” a thunk dies and collapses into a blackhole. We’ve got plenty of messes in Haskell that don’t sufficiently protect us from ourselves. The compiler can only do as good a job as our coding standards and our libraries allow.</p>
<p>“But Haskell’s at least better than languages like PHP.” I mean, obviously I agree with this, or I’d be writing PHP. But since I’m being ridiculously hyperbolic here, let me make a ridiculous claim:</p>
<blockquote>
<p><strong>PHP is better than Haskell, since at least you don’t get a false sense of security</strong></p>
<p><em>- Michael Snoyman, totally 100% what he actually believes, you should totally quote this out of context</em></p>
</blockquote>
<p>I’ve said this so many times. So I’ll say it again. Using a great language with safety features is one tiny piece of the puzzle.</p>
<ul>
<li>Did you get the software requirements right?</li>
<li>Did you leverage the type system to prevent the bugs you’re trying to prevent?</li>
<li>Do your underlying libraries have bugs?</li>
<li>Did you find a way to implement a function with correct types but incorrect semantics?</li>
<li>Did you host the thing on a dinky server sitting under your desk and forget that you have power outages on a daily basis?</li>
<li>Did you forget to write a single test case?</li>
<li>Do your test cases actually test anything meaningful?</li>
</ul>
<p>There are <em>so many ways</em> for software to fail outside the purview of the type system. We’ve got to stop thinking that somehow Haskell (or, for that matter, Rust, Scala, and other strongly typed languages) are some kind of panacea. Seriously: the PHP people at least know their languages won’t protect them from anything. We should bring some of that humility back to Haskell.</p>
<p>Haskell provides me tools to help prevent certain classes of bugs, so I can spend more of my time catching a bunch of other bugs that I’m absolutely going to write. Because I’m dumb. And we need to remember: we’re all dumb.</p>
<h2>More partial functions!</h2>
<p>You know what’s worse than partial functions? Insidiously partial functions. We’ve all been screaming about <code>head</code> and <code>tail</code> for years. My hackles rise every time I see a <code>read</code> instead of <code>readMaybe</code>. I can’t remember the last time I saw the <code>!!</code> operator in production code.</p>
<p>But there are plenty of other functions that are just as dangerous, if not more so. More dangerous because they aren’t well known to be partial. They are commonly used. People don’t understand why they’re dangerous. And they fail only in edge cases that people aren’t thinking about.</p>
<p>Exhibit A: I present <code>decodeUtf8</code>. (Thanks <a href="https://twitter.com/kerckhove_ts/status/1321390954172063745?s=20">Syd</a>.)</p>
<p>Go ahead, search your codebase. Be dismayed that you’ve found it present.</p>
<p>What’s wrong with <code>decodeUtf8</code>? As we established last time, character encoding crap breaks stuff in production. UTF-8 works about 99% of the time, especially for people in Western countries. You’ll probably forget to even test for it. And that function looks so benign: <code>decodeUtf8 :: ByteString -&gt; Text</code>.</p>
<p><strong>DO NOT BE FOOLED</strong></p>
<p>This function is a ticking time bomb. Use <code>decodeUtf8'</code> (yes, it’s named that badly, just like <code>foldl'</code>) and explicitly handle error cases. Or use I/O functions that explicitly handle UTF-8 decoding errors and throw a runtime exception.</p>
<p>“I can’t believe Michael still thinks runtime exceptions are a good idea.” I’ll get to that another time. I don’t really believe they’re a good idea. I believe they are omnipresent, better than bottom values, and our least-bad-option.</p>
<h2>Law-abiding type classes</h2>
<p>Now I’ve truly lost it. What in tarnation could be wrong with law-abiding type classes? They’re good, right? Yes, they are! The section heading is complete clickbait. Haha, fooled you!</p>
<p>There’s a concept in the Haskell community that all type classes should be law-abiding. I’ve gone to the really bad extreme opposing this in the past with early versions of <code>classy-prelude</code>. In my defense: it was an experiment. But it was a bad idea. I’ve mostly come around to the idea of type classes being lawful. (Also, the original namespacing issues that led to <code>classy-prelude</code> really point out a much bigger bad part of Haskell, which I’ll get to later. Stay tuned! Hint: Rust beat us again.)</p>
<p>Oh, right. Speaking of Rust: they do <em>not</em> believe in law-abiding type classes. There are plenty of type classes over there (though they call them <code>trait</code>s) that are completely ad-hoc. I’m looking at you, <code>FromIterator</code>. This is Very, Very Bad of course. Or so my Haskell instincts tell me. And yet, it makes code Really, Really Good. So now I’m just confused.</p>
<p>Basically: I think we need much more nuanced on this in the Haskell community. I’m leaning towards my <em>very</em> original instincts having been spot on. So:</p>
<ul>
<li>Law abiding type classes: great</li>
<li>Flippantly non-law-abiding type classes ala the original <code>classy-prelude</code>: bad</li>
<li>“You know what I meant” typeclasses like <code>ToContent</code> in Yesod: also great</li>
</ul>
<p>This isn’t exactly in line with a “bad part” of Haskell. Up until now I’ve been giving a nuanced reflection on my journeys in Haskell. Let me try something better then. Ahem.</p>
<p><strong>DON’T LECTURE ME ON LAW ABIDING TYPE CLASSES AND FLAGRANTLY VIOLATE LAWS</strong></p>
<p>I’m staring at you, <code>Eq Double</code>. No, you cannot do equality on a <code>Double</code>. (And thanks again to Syd for this idea.) Rust, again, Got It Right. See <code>PartialEq</code> vs <code>Eq</code>. Floating point values do not allow for total equality. This makes things like <code>Map Double x</code> dangerous. Like, super dangerous. Though maybe not as dangerous as <code>HashMap Double x</code>, which deserves its own rant later.</p>
<p>So come down from your high horses. We don’t have law abiding type classes. We have “if I close my eyes and pretend enough then maybe I have law abiding type classes.”</p>
<h2>Unused import warnings</h2>
<p>Haskell has a dumb set of default warnings enabled. (“I think you mean GHC, one implementation of Haskell, not Haskell the language itself.” Uh-huh.) How can we <em>not</em> generate a warning for a partial pattern match? Come on! ADTs and pattern matching is <em>the</em> killer feature to first expose people to. And it’s a total lie: the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.snoyman.com/blog/2020/11/haskell-bad-parts-2">https://www.snoyman.com/blog/2020/11/haskell-bad-parts-2</a></em></p>]]>
            </description>
            <link>https://www.snoyman.com/blog/2020/11/haskell-bad-parts-2</link>
            <guid isPermaLink="false">hacker-news-small-sites-25039758</guid>
            <pubDate>Mon, 09 Nov 2020 20:54:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Statusly - Automate slack DND when you join a zoom meeting/gCal event]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25039692">thread link</a>) | @statusly
<br/>
November 9, 2020 | https://www.statusly.app/why-statusly?ref=hn | <a href="https://web.archive.org/web/*/https://www.statusly.app/why-statusly?ref=hn">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.statusly.app/why-statusly?ref=hn</link>
            <guid isPermaLink="false">hacker-news-small-sites-25039692</guid>
            <pubDate>Mon, 09 Nov 2020 20:48:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Benchmark Godot with Rust]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25039494">thread link</a>) | @todsacerdoti
<br/>
November 9, 2020 | https://blog.extrawurst.org/general/gamedev/rust/2020/11/07/godot-rust-benchmark.html | <a href="https://web.archive.org/web/*/https://blog.extrawurst.org/general/gamedev/rust/2020/11/07/godot-rust-benchmark.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>I recently started looking into using Rust and the <a href="https://godotengine.org/">Godot</a> Game Engine for developing games. A very quick experiment recently was to compare performance of GDScript, Visual scripting and Rust for the same task.</p>



<p>It is simple: Let us just draw a ton of lines to have a lot of traffic between our code and the engine:</p>

<p><img src="http://extrawurst.github.io/assets/godot-rust-benchmark/result.png" alt="res"></p>

<p>In fact it is so many lines that we end up with a filled circle. Nothing beautiful, this is just a benchmark afterall 👌</p>



<p>Let us look at the contenders:</p>
<ul>
  <li>gdscript</li>
  <li>visual script</li>
  <li>gdnative (rust)</li>
</ul>

<p><strong>GDScript</strong> is the official scripting language shipping with Godot.</p>

<p><strong>Visual Script</strong> is the official node based way to visually script in Godot (think Unreal Blueprint or Unity Bolt)</p>

<p><strong>GDNative</strong> is the official C-Api to access Godot from escentially any language. We are using <a href="https://github.com/godot-rust/godot-rust">Godot-Rust</a> to be able to compile shared libraries from Rust code to interface with Godot.</p>

<hr>

<p>The entire test with all three options can be found on github: <a href="https://github.com/extrawurst/godot-rust-benchmark">extrawurst/godot-rust-benchmark</a></p>

<p>Rerun it for yourself :)</p>

<h2 id="gdscript">GDScript</h2>

<p>Let’s start with the official way of doing things in Godot - using GDScript:</p>

<div><div><pre><code><span>extends</span> <span>CanvasItem</span>

<span># how many lines to draw? (this can be adjusted from the editor UI)</span>
<span>export</span> <span>var</span> <span>cnt</span> <span>=</span> <span>6000</span>
<span># this is the center point</span>
<span>export</span> <span>var</span> <span>start</span> <span>=</span> <span>Vector2</span><span>(</span><span>250</span><span>,</span><span>250</span><span>)</span>
<span># radius (line length)</span>
<span>export</span> <span>var</span> <span>rad</span> <span>=</span> <span>200</span>

<span>func</span> <span>_draw</span><span>():</span>
	<span># lets measure the runtime</span>
	<span>var</span> <span>startTime</span> <span>=</span> <span>OS</span><span>.</span><span>get_ticks_usec</span><span>()</span>

	<span>var</span> <span>cntf</span> <span>=</span> <span>float</span><span>(</span><span>cnt</span><span>)</span>
	<span>for</span> <span>n</span> <span>in</span> <span>range</span><span>(</span><span>cnt</span><span>):</span>
		<span>var</span> <span>x</span> <span>=</span> <span>sin</span><span>(</span><span>n</span><span>/</span><span>cntf</span> <span>*</span> <span>360.0</span><span>)</span><span>*</span><span>rad</span>
		<span>var</span> <span>y</span> <span>=</span> <span>cos</span><span>(</span><span>n</span><span>/</span><span>cntf</span> <span>*</span> <span>360.0</span><span>)</span><span>*</span><span>rad</span>
		<span>draw_line</span><span>(</span>
			<span>start</span><span>,</span> 
			<span>start</span><span>+</span><span>Vector2</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>),</span> 
			<span>Color</span><span>(</span><span>255</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>),</span> 
			<span>1</span><span>,</span>
			<span>false</span><span>)</span>
	
	<span>print</span><span>(</span><span>"bench: "</span> <span>+</span> <span>String</span><span>(</span><span>OS</span><span>.</span><span>get_ticks_usec</span><span>()</span> <span>-</span> <span>startTime</span><span>))</span>

<span>func</span> <span>_process</span><span>(</span><span>_delta</span><span>):</span>
	<span>update</span><span>()</span>
</code></pre></div></div>

<h2 id="visual-script">Visual Script</h2>

<p>The following screenshot shows the same logic in a visual node based way:</p>

<p><img src="http://extrawurst.github.io/assets/godot-rust-benchmark/visualscript.png" alt="vs"></p>

<p>We immediately see how this is more verbose but at least it is possible and it even just crashed once on me 🙈</p>

<h2 id="gdnative-rust">GDNative (Rust)</h2>

<p>We are using <a href="https://github.com/godot-rust/godot-rust">Godot-Rust</a> for this.</p>

<div><div><pre><code><span>#[export]</span>
<span>fn</span> <span>_</span><span>draw</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>owner</span><span>:</span> <span>&amp;</span><span>CanvasItem</span><span>)</span> <span>{</span>
	<span>let</span> <span>start_time</span> <span>=</span> <span>OS</span><span>::</span><span>godot_singleton</span><span>()</span><span>.get_ticks_usec</span><span>();</span>

	<span>let</span> <span>cntf</span> <span>=</span> <span>self</span><span>.cnt</span> <span>as</span> <span>f32</span><span>;</span>

	<span>for</span> <span>n</span> <span>in</span> <span>0</span><span>..</span><span>self</span><span>.cnt</span> <span>{</span>
		<span>let</span> <span>x</span> <span>=</span> <span>f32</span><span>::</span><span>sin</span><span>(</span><span>n</span> <span>as</span> <span>f32</span> <span>/</span> <span>cntf</span> <span>*</span> <span>360.0</span><span>)</span> <span>*</span> <span>self</span><span>.rad</span><span>;</span>
		<span>let</span> <span>y</span> <span>=</span> <span>f32</span><span>::</span><span>cos</span><span>(</span><span>n</span> <span>as</span> <span>f32</span> <span>/</span> <span>cntf</span> <span>*</span> <span>360.0</span><span>)</span> <span>*</span> <span>self</span><span>.rad</span><span>;</span>
		<span>let</span> <span>target</span> <span>=</span> <span>Vector2</span><span>::</span><span>new</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>)</span> <span>+</span> <span>self</span><span>.start</span><span>;</span>

		<span>owner</span><span>.draw_line</span><span>(</span>
			<span>self</span><span>.start</span><span>,</span> 
			<span>target</span><span>,</span> 
			<span>Color</span><span>::</span><span>rgb</span><span>(</span><span>0.0</span><span>,</span> <span>0.0</span><span>,</span> <span>1.0</span><span>),</span> 
			<span>1.0</span><span>,</span> 
			<span>false</span><span>)</span>
	<span>}</span>

	<span>godot_print!</span><span>(</span>
		<span>"bench: {}"</span><span>,</span>
		<span>OS</span><span>::</span><span>godot_singleton</span><span>()</span><span>.get_ticks_usec</span><span>()</span> <span>-</span> <span>start_time</span>
	<span>);</span>
<span>}</span>
</code></pre></div></div>



<p>I am not going to further comment on the ergonomics of either language. I really did this for two reasons: 1) can we do all we need in the visual script and 2) how does performance compare between the alternatives</p>

<p>Here are the timings:</p>

<table>
  <thead>
    <tr>
      <th>type</th>
      <th>usecs</th>
      <th>slowdown</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>gdnative (rust)</td>
      <td>~980 usec</td>
      <td>-</td>
    </tr>
    <tr>
      <td>gdscript</td>
      <td>~5112 usec</td>
      <td>5x</td>
    </tr>
    <tr>
      <td>visual script</td>
      <td>~7099 usec</td>
      <td>7x</td>
    </tr>
  </tbody>
</table>

<p><sub><sup>(executed on a macbook 2016 3,3 GHz i7, 16 GB Ram, Intel Iris 550 and Godot 3.2.3, avg. over 10 runs)</sup></sub></p>

<p>On twitter people noted that this might change with Godot 4.0 and the support of type checking in gdscript. This could be interesting to measure once 4.0 is released.</p>

<p>For now my conclusion is:</p>

<ul>
  <li>GDScript is easy and quick to learn</li>
  <li>Visual Scripting in Godot works although it feels a little instable</li>
  <li>Godot-Rust is a clear alternative to write entire Godot games in</li>
</ul>

<p>Of course point 3) is limited to people coming with a Rust background otherwise the Rust part in it is a clear challenge to learn first.</p>

      </div></div>]]>
            </description>
            <link>https://blog.extrawurst.org/general/gamedev/rust/2020/11/07/godot-rust-benchmark.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25039494</guid>
            <pubDate>Mon, 09 Nov 2020 20:29:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The election of the doge]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25039470">thread link</a>) | @flannery
<br/>
November 9, 2020 | https://generalist.academy/2020/11/06/the-election-of-the-doge/ | <a href="https://web.archive.org/web/*/https://generalist.academy/2020/11/06/the-election-of-the-doge/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-4574">
	
		<p>
By  on <a href="https://generalist.academy/2020/11/06/the-election-of-the-doge/" title="7:00 am" rel="bookmark"><time datetime="2020-11-06T07:00:00+13:00">November 6, 2020</time></a>	• 
	</p>
	<section>

<p>The ruler of Medieval Venice was chosen by an exceptionally complex ten-step process of alternating random lots and elections.</p>



<div><figure><img loading="lazy" data-attachment-id="4580" data-permalink="https://generalist.academy/kms3898/" data-orig-file="https://thegeneralistacademy.files.wordpress.com/2020/11/grandcouncil.jpg" data-orig-size="2043,1200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;Statens Museum for Kunst&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;Canaletto (1697-1768), Dogen og det store raad forsamlede i Sala del consiglio maggior i Dogepaladset, About 1763&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;Public Domain (CC0)&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;kms3898&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="kms3898" data-image-description="" data-medium-file="https://thegeneralistacademy.files.wordpress.com/2020/11/grandcouncil.jpg?w=300" data-large-file="https://thegeneralistacademy.files.wordpress.com/2020/11/grandcouncil.jpg?w=656" src="https://thegeneralistacademy.files.wordpress.com/2020/11/grandcouncil.jpg?w=1024" alt="Grand Council" width="768" height="451" srcset="https://thegeneralistacademy.files.wordpress.com/2020/11/grandcouncil.jpg?w=1024 1024w, https://thegeneralistacademy.files.wordpress.com/2020/11/grandcouncil.jpg?w=768 768w, https://thegeneralistacademy.files.wordpress.com/2020/11/grandcouncil.jpg?w=1536 1536w, https://thegeneralistacademy.files.wordpress.com/2020/11/grandcouncil.jpg?w=150 150w, https://thegeneralistacademy.files.wordpress.com/2020/11/grandcouncil.jpg?w=300 300w" sizes="(max-width: 768px) 100vw, 768px"><figcaption><a href="https://commons.wikimedia.org/wiki/File:Canaletto_-_The_Doge_and_Grand_Council_in_Sala_del_Maggior_Consiglio_-_KMS3898_-_Statens_Museum_for_Kunst.jpg">Canaletto</a>, Public domain, via Wikimedia Commons</figcaption></figure></div>



<p>A few weeks ago I wrote about modern <a href="https://generalist.academy/2020/10/17/electoral-fairness/">democratic electoral systems</a>, and a couple of days ago I wrote about the complexities of the <a href="https://generalist.academy/2020/11/04/the-unpopular-president/">American Electoral College</a>. Today I wanted to go even further back, to the Medieval Venetian Republic. There, the selection of a new leader – the doge – was one of the more complex and baffling electoral processes in history. And even so, though this be madness, yet there is method in’t.</p>



<p>The Great Council of Venice was a large legislative body made up of a relatively small number of noble families. Obviously, everyone wanted to be the doge, but the council was very keen to avoid behind-the-scenes bribery, dirty deals, intrigue, and extended and contentious campaigns. To achieve this, the election of the doge went through multiple steps, all designed to reduce power consolidation.</p>



<p>First, thirty members of the Great Council were chosen at random. Then nine of those thirty were chosen, again randomly. Those nine members picked the next set: forty people from the Great Council. And those forty? Twelve, randomly picked from their number, moved on to the next step. Those twelve chose twenty-five; those twenty-five were randomly pared down to just nine. Having fun yet?</p>



<p>This set of nine members chose forty-five more; eleven were picked – again at random – from those forty-five. The eleven chose forty-one members. Those forty-one (finally!) voted for the doge. </p>



<p>There were some additional checks against skulduggery. Each noble family couldn’t have more than one member in each group, and members couldn’t vote for their own relatives. Every time a set of members voted for the next group, more than a simple majority was required: around three quarters of the voting group had to agree. (For the final election, just 25 of the 41 had to agree.)</p>



<p>To recap, this is the process:<br>Great Council &gt; 30 &gt; 9 &lt; 40 &gt; 12 &lt; 25 &gt; 9 &lt; 45 &gt; 11 &lt; 41 &gt; 1.</p>



<p>Because of this complexity, the chances of rigging or buying the election were greatly reduced, minority concerns were not buried by the majority, but neither was the majority tyrannized by the minority. Today we only use this kind of random process in jury selection and citizen’s assemblies.</p>



<p>[Thanks to Alistair S. for suggesting this topic.]</p>



<ul><li><a href="https://en.wikipedia.org/wiki/Doge_of_Venice">Doge of Venice</a></li><li><a href="https://en.wikipedia.org/wiki/Sortition">Sortition</a></li><li><a href="https://doi.org/10.1007/s10602-019-09290-6">How the Republic of Venice chose its doge: Lot-based elections and supermajority rule</a></li></ul>
		<p>Categories: <a href="https://generalist.academy/category/places/europe/" rel="category tag">Europe</a> <a href="https://generalist.academy/category/history/" rel="category tag">History</a> <a href="https://generalist.academy/category/history/medieval-history/" rel="category tag">Medieval history</a> <a href="https://generalist.academy/category/places/" rel="category tag">Places</a> <a href="https://generalist.academy/category/politics-law/" rel="category tag">Politics &amp; law</a>		</p>
	<div>
		<p><img alt="" src="https://0.gravatar.com/avatar/f7eb82f9df252be8cad1a3993809331d?s=100&amp;d=https%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D100&amp;r=G" height="100" width="100"></p><h3>The Generalist</h3>
		<p>I live in Auckland, New Zealand, and am curious about most things.</p>
	</div>
	</section>
</article></div>]]>
            </description>
            <link>https://generalist.academy/2020/11/06/the-election-of-the-doge/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25039470</guid>
            <pubDate>Mon, 09 Nov 2020 20:27:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Canada Is 2nd Place]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25039402">thread link</a>) | @Kortaggio
<br/>
November 9, 2020 | https://billmei.net/blog/canada | <a href="https://web.archive.org/web/*/https://billmei.net/blog/canada">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
      
      
      
        <p><img src="https://d33wubrfki0l68.cloudfront.net/6f6580be6496271f0228cb98a1db81f2e403a0c5/df9b6/assets/blog/img/canada--main.jpg" alt="Canada is second place"></p>
      
      <p>We’ll always be in the shadow of our neighbour, the United States; but that’s okay, because mediocrity is what makes Canada awesome.</p>

<p>Our healthcare is worse than the UK’s. TIFF is not as prestigious as the Golden Globes. We are the Shopify to your Amazon, the BlackBerry to your Apple. The landmass spans three oceans and yet still is only the second largest. We have less oil than Texas. Vancouver real estate is not as expensive as San Francisco. The most talented people in the world apply to immigrate here, after they were rejected by the USA.</p>

<p>Canadians don’t desire to be the first, the best, or the greatest. We’re pretty okay with being number two. We’re so unremarkable that we rarely make international news, and that’s fine because most Canadians don’t want that spotlight anyway.</p>

<p>Canadian history is a list of things we helped French and British people accomplish.</p>

<p>Our winter doesn’t stop Napoleons, it just stops your car batteries in the morning.</p>

<p><a href="https://medium.com/conversations-with-tyler/tyler-cowen-margaret-atwood-writer-author-poetry-handmaids-tale-b3ddd258cf81" target="_blank" rel="noopener">Canada is really big</a>. So you can’t make generic statements that apply to all Canadians. You are guaranteed to find someone whose experience contradicts what I’ve written here. Perhaps the only thing that unites Canadians is we tell ourselves at least we’re not American. Canada doesn’t have a strong national identity, or a unique marketing angle.</p>

<p>Canada is the generic store brand of countries.</p>

<p>In an era where technology <a href="http://www.paulgraham.com/re.html" target="_blank" rel="noopener">pulls apart</a> the <a href="https://billmei.net/books/average-is-over/">tails</a>, Canada remains refreshingly average. The best part of a country is also its <a href="https://markmanson.net/5-life-lessons-5-years-traveling-world" target="_blank" rel="noopener">worst part</a>, and the worst part about Canada is that it has no best part.</p>

<p>The lack of any single dominating force is what makes Canada so amazingly diverse. You can just be you, and don’t need to conform to a peer group, because everyone around you doesn’t work for just one industry, or likes just one type of media, or participates in just one cultural bubble. It’s a place where I feel safe to be mediocre; I don’t feel like a failure for not curing cancer or inventing AI or doing rocket science.</p>

<p>No matter how much I like to believe, the truth is that I’ll probably live a normal, unremarkable life that people will forget about once I’m gone. Canada accepts your inherent worth as a person, and doesn’t demand that you achieve, consume, or do something special to deserve your dignity. Canada is a wonderful place to live despite being second place and this reminds you that it’s fine not to be first place. You are valuable because you are you.</p>

<p>Canada is mediocre, and so am I. That’s why I love you, Canada. 🇨🇦</p>

<p><em>I only publish half of my writing publicly. The rest are posted exclusively on my private email list: <a href="https://billmei.net/email">billmei.net/email</a> (Subscribing is free, no spam ever, and you can safely unsubscribe anytime)</em></p>

      <small>
  
  <span>Image credit: Jp Valery</span>
  
  <time datetime="2020-09-28T00:00:00+00:00">Published September 2020</time>
</small>

    </article></div>]]>
            </description>
            <link>https://billmei.net/blog/canada</link>
            <guid isPermaLink="false">hacker-news-small-sites-25039402</guid>
            <pubDate>Mon, 09 Nov 2020 20:20:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Goodbye USA]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25039397">thread link</a>) | @polote
<br/>
November 9, 2020 | https://larrysalibra.com/goodbye-usa/ | <a href="https://web.archive.org/web/*/https://larrysalibra.com/goodbye-usa/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
<div>
<article>

<figure>
<img srcset="https://larrysalibra.com/content/images/size/w300/2020/11/IMG_1166.jpeg 300w,
                            https://larrysalibra.com/content/images/size/w600/2020/11/IMG_1166.jpeg 600w,
                            https://larrysalibra.com/content/images/size/w1000/2020/11/IMG_1166.jpeg 1000w,
                            https://larrysalibra.com/content/images/size/w2000/2020/11/IMG_1166.jpeg 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://larrysalibra.com/content/images/size/w2000/2020/11/IMG_1166.jpeg" alt="Goodbye USA">
</figure>
<section>
<div>
<p>A few weeks ago, when I was trying to open an account at a financial institution here in Hong Kong, someone noticed I was born in the USA and my account was denied.</p><p>They wrote “I regret to inform you at this time we are unable to accept bank wire deposits/withdrawals from US citizens. For more information, please refer to our article: Restricted nationalities and countries.” While this is a common experience for Americans living abroad, one I’ve experienced many times before, it was a mistake, a mistake that reminded me to write this blog post.</p><p>A few days ago, I received a rather passive-aggressive email from one of my banks asking me to “let us know about your tax connection to the United States” with the threat of closing my account if I don’t. Another common experience for Americans living abroad, another mistake and another reminder to write this post.</p><figure><img src="https://larrysalibra.com/content/images/2020/11/Screen-Shot-2020-10-22-at-5.52.02-PM.png" alt=""></figure><p>Tomorrow, Americans will vote to choose one of two candidates no one really likes. I’m not allowed to vote this time around because two years ago, I finally rid myself of the burden that is American citizenship.</p><h2 id="a-hard-story-to-tell">A hard story to tell</h2><p>I’ve tried to write this blog post a number of times over the past two years. I’ve written entire drafts of this post from scratch more than once and never published them, unhappy with the tone or the content or the narrative or worried about the reaction of people I’ve never met on the internet. Perhaps it was the years of brainwashing…I mean, “<a href="https://en.wikipedia.org/wiki/Civic_education_in_the_United_States">civic education</a>”…in US public schools that has scared my psyche and made me afraid of what the tribe will do when it finds out I’ve deserted them. Having to say the "Pledge of Allegiance" every day during one's childhood before he knows what the words "pledge" or "allegiance" even mean really does a job on one's mind.</p><p>I want to tell my story because I hope that it will benefit others on their journey towards personal freedom just the stories of others benefited me.</p><p>Our story starts about a year ago, in October 2019.</p><p>It's 2am on a Saturday morning in Hong Kong and I can't sleep. My body thinks it's still in Paris after two weeks in Europe. One year ago today, I also couldn't sleep but for a different reason. I was tossing and turning from stress the night before what I thought was going to be the biggest decision of my life: to give up US citizenship.</p><p>After over 11 years living outside of the USA, I had made an appointment on October 12, 2018, at the US Consulate in Hong Kong to go through the process known as "expatriation" or "renouncing US citizenship."</p><p>This was the second appointment in the process. The first had been a few weeks prior, on August 31, 2018. You see, the United States wants you to believe that giving up US citizenship is a Big Deal, and in many ways it is! You lose a whole bunch of rights granted by citizenship such as no longer being able live or work in the country unless you get a visa. In this day and age of passports, visas and borders, (<a href="https://www.nationalgeographic.com/travel/features/a-history-of-the-passport/">it didn't used to be like this</a>! travel used to be free as in freedom!) if you didn't have another citizenship, you could become "stateless" with nowhere to go and no way to get past borders to get there. To someone like me who was born and raised in suburban Ohio, this felt like a Big Deal.</p><p>What they don’t tell you, is that you’re also freed from a number of coercive obligations you never agreed to that other countries don’t impose on their citizens by opting-out of US citizenship. They tell you it's all pain with no gain.</p><p>To drive home the Big Dealness of the decision, the State Department made me make two appointments at the consulate to make sure I got their point. And I had to make the appointments well in advance as they were fully booked for months. I wasn’t the only American in Hong Kong trying to exit.</p><p>At the first appointment, a nice vice consul named Rachel took me in to a tiny interview room complete with an American flag in the nondescript Garden Road compound. Our interview began with Rachel hitting her head on the phone on the wall quite hard as she was sitting down and me asking her with a bit of a shock, “OMG, are you okay?!?!" After a minute or so of rubbing her head and grimacing, she was fine and we started with our interview.</p><p>She roughly followed a script that is specified in the <a href="https://fam.state.gov/fam/07fam/07fam1260.html">US State Department's Foreign Affairs Manual</a>. She asked me questions about myself and my decision. Where was I from? (Ohio) How long have I lived abroad? (Since May, 2007) Where does my family live? (My parents live in the US and my brother lives in Bulgaria). Did I have another citizenship and passport? (Yes, Italian) Why did I want to give up US citizenship? (It's complicated, but I didn't feel American anymore and when visiting the US anymore I felt like a tourist).</p><p>After the questions she started reading me a list of what came off as warnings and disclaimers to make sure I knew what I was getting myself into:</p><ul><li>Any children I had wouldn't become US citizens</li><li>Might not be able to travel to the US again</li><li>Wouldn't be able to live or work in the US an appropriate visa</li><li>I would still have to file for and pay taxes that I owe</li><li>If I didn't have another citizenship I could be come stateless</li></ul><p>She then instructed me to take a couple of weeks for a "period of reflection" (this is required according to State Department regulations) and handed me a packet of papers to review listing the consequences giving up my blue passport and outlining the tax implications.</p><p>Where better to go to reflect and get some clarity on my decision to exit the US than to visit a country struggling with its own exit decision, the United Kingdom? So I headed off to the UK for a short vacation followed by a work trip in London.</p><p>By the time I made my first August appointment, I'd already made the decision - I'd been thinking about it for years and had done a ton of research and due diligence. I'd read a lot about other people's experiences and thinking. I was particularly inspired by bitcoin investor Roger Ver's story and Balajis Srinivasan's talk on <a href="https://www.youtube.com/watch?v=cOubCHLXT6A">Voice vs Exit</a>.</p><p>I was most concerned about losing the right to live and work in the US - it seems so many people want this right and that they're willing to go through great pain and inconvenience to get it. However, I hadn't ever used this right in the almost 20 years of my adult life. I’d always competed on the global market without the benefit of “work authorization” protecting me from competition of “aliens.” As life long entrepreneur and a strong advocate of remote work, I didn't ever see myself wanting to sign up for some office job in the States. But would I want to live there?</p><h2 id="a-tourist-in-my-own-country">A tourist in my own country</h2><p>In 2016, I flew to California and rented a car to do some due diligence on the country I was thinking of leaving. I spent a month driving across the US and back, taking in the sites and visiting places I thought I might like to live. The natural beauty of America and her parks is breathtaking. While it is amazing to visit and even vacation for a few weeks, it wasn’t someplace I'd want to live. New York and Chicago were my favorite cities, but cold climate and snow is a turn off as is the crumbling infrastructure and confiscatory tax rates.</p><p>A number of tech friends moved to Austin for the low tax and warm weather, so I stopped there for a few days. After over a decade in Asia, Austin hardly seems big enough to even be called a city. My impression was that it is a bunch of suburban sprawl with some bigger buildings in the middle. And I much prefer being on islands surrounded by water instead of landlocked.</p><figure><img src="https://larrysalibra.com/content/images/2020/11/IMG_5406.jpeg" alt=""></figure><p>In June of 2018, I took a vacation to America’s only state that is “a bunch of islands surrounded by water,” Hawaii. I had found memories of visiting the Aloha state as a child and wanted to make sure that I wasn't missing out on some future destiny where Larry becomes a Hawaiian like George Clooney in <em>The Descendants</em>. The nature was nice, but the poverty, crime and remoteness from everywhere made it not very attractive as a future home.</p><p>I'd discussed the decision with family and friends. Friends ranged from very unhappy to mildly supportive to downright enthusiastic. My brother, also an entrepreneur who has lived outside of the States for more than a decade, was understanding of the challenges the USA imposes on its entrepreneurs abroad and very supportive. My parents were skeptical that it was the right decision. They'd lived their whole lives in the USA and grown up on stories of World War II valor and of their grandparents who had struggled to immigrate to the United States for a better life. They shared their views with me but never gave me any pressure and let me make my own decision.</p><h2 id="the-oath">The oath</h2><p>October 12, 2018, my second appointment in the renunciation process, quickly arrived. I couldn’t really sleep the night before. I was nervous about both the appointment and a <a href="https://blog.blockstack.org/a-path-to-decentralization/">big work announcement</a>. Lots of things were changing.</p><p>In the morning, I woke up and took a taxi to the US consulate for my appointment. Here’s a picture of me before the appointment. You can see the stress in my face.</p><figure><img src="https://larrysalibra.com/content/images/2020/11/IMG_8562.jpeg" alt=""></figure><p>After entering the consulate and going through security, the first step of the appointment was paying the fee: US$2,350. You can’t free yourself from the Land of the Free without having the cash money to buy your freedom. Think of the renunciation fee as an emancipation fee, a lot of money, but a small price to pay for freedom.</p><figure><img src="https://larrysalibra.com/content/images/2020/11/https---blogs-images.forbes.com-robertwood.jpg" alt=""></figure><p>The actual renunciation ceremony was sort of an out-of-body blur. I felt like I was watching myself in the little room with the American flag renouncing my allegiance (which of course the US education system forces children who don’t know what the word “allegiance” means to “pledge”) to the USA.</p><p>The consular official told me that they would take my US passport and hold on to it until the State Department approved my renunciation and return it to me canceled along with my Certificate of Loss of Nationality (CLN) at some later date. He advised that if I needed to travel to the US I might be able to borrow the passport for a trip if the …</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://larrysalibra.com/goodbye-usa/">https://larrysalibra.com/goodbye-usa/</a></em></p>]]>
            </description>
            <link>https://larrysalibra.com/goodbye-usa/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25039397</guid>
            <pubDate>Mon, 09 Nov 2020 20:19:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Recalculate a Spreadsheet]]>
            </title>
            <description>
<![CDATA[
Score 272 | Comments 64 (<a href="https://news.ycombinator.com/item?id=25039393">thread link</a>) | @todsacerdoti
<br/>
November 9, 2020 | https://lord.io/blog/2020/spreadsheets/ | <a href="https://web.archive.org/web/*/https://lord.io/blog/2020/spreadsheets/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>Let’s say I’m ordering burritos for my two friends while they quar up in Jersey City, and want to calculate the total price of my order:</p>
<p><img alt="screenshot of spreadsheet; burrito price is listed as $7, burrito price w ship as burrito price plus $3, num burritos is 2, and total is num burritos times burrito price w ship, for a total of $20" src="https://lord.io/images/2020/anchors_0.png"></p>
<p>It’s a little confusing to follow the flow of data in a spreadsheet when it’s written like that, so I hope you don’t mind this equivalent diagram that represents it as a graph:</p>
<p><img alt="the previous spreadsheet represented as a graph, with arrows from one cell to another replacing the spreadsheet cell references" src="https://lord.io/images/2020/anchors_1.png"></p>
<p>We’re rounding the cost of an El Farolito super vegi burrito to $8, so assuming the per-burrito delivery toll remains at just $2 per burrito, it looks like the total for our two burritos will be $20.</p>
<p>Oh no, I completely forgot! One of my friends loves to wolf down multiple burritos at a time, so I actually want to place an order for three burritos. If I update <code>Num Burritos</code>, a naïve spreadsheet engine might recompute the entire document, recalculating first the cells with no inputs, and then recalculating any cell whose inputs are ready until we’ve finished every cell. In this case, we’d first calculate <code>Burrito Price</code> and <code>Num Burritos</code>, then <code>Burrito Price w Ship</code>, and then a new final <code>Total</code> of $30.</p>
<p><img alt="same as previous graph, but num burritos is updated to 3, and every cell is tagged as &quot;recalc&quot;" src="https://lord.io/images/2020/anchors_2.png"></p>
<p>This simple strategy of recalculating the whole document may sound wasteful, but it’s actually already <em>better</em> than VisiCalc, the first spreadsheet software ever made, and the first so-called “killer app”, responsible for popularizing the Apple II. VisiCalc would repeatedly recalculate cells from left-to-right and top-to-bottom, sweeping over them again and again until none of them changed. Despite this “interesting” algorithm, VisiCalc remained the dominant spreadsheet software for four years. Its reign ended in 1983, when Lotus 1-2-3 swept the market with “natural-order recalculation”, <a href="https://aresluna.org/attached/computerhistory/articles/spreadsheets/tenyearsofrowsandcolumns">as described by Tracy Robnett Licklider in Byte Magazine</a>:</p>
<blockquote>
<p>Lotus 1-2-3 exploited natural-order recalculation, although it also supported VisiCalc’s row- and column-order modes. Natural-order recalculation maintained a cell dependency list and recalculated a cell before recalculating cells that depended on it.</p>
</blockquote>
<p>Lotus 1-2-3 implemented the “recalculate everything” strategy we’ve shown above, and for the first decade of spreadsheets, that was as good as it got. Yes, we recalculate every cell in the document, but at least we only recalculate every cell once.</p>
<h2>but what about “burrito price w ship”</h2>
<p>Great point, header 2. In my three burrito example there’s no reason to recompute <code>Burrito Price w Ship</code>, because changing the number of burritos we order can’t possibly influence the per-burrito price. In 1989, one of Lotus’ competitors realized this, and created SuperCalc5, presumably naming it after the theory of super burritos at the core of this algorithm. SuperCalc5 recalculated “only cells dependent on changed cells”, which would make updating the burrito count look more like this:</p>
<p><img alt="same as prior graph, with burrito count updated from 2 to 3, but now only the two affected cells &quot;num burritos&quot; and &quot;total&quot; are tagged as recalc" src="https://lord.io/images/2020/anchors_3.png"></p>
<p>By only updating a cell when one of its inputs changes, we can avoid recalculating <code>Burrito Price w Ship</code>. In this case, it saves just a single addition, but on larger spreadsheets it can save quite a bit of time! Unfortunately, we now have another problem. Let’s say my friends now want meat burritos, which cost a dollar more, and simultaneously El Farolito adds a $2 fee paid per-order, regardless of how many burritos you order. Before any formula outputs are recalculated, our graph might look like this:</p>
<p><img alt="same as prior graph (after burrito count update finished calculation), but now burrito price is being updated from $8 to $9, and simultaneously total is updated from &quot;burrito price w ship * num burritos&quot; to &quot;burrito price w ship * num burritos + $2 fee&quot;" src="https://lord.io/images/2020/anchors_4.png"></p>
<p>Since there are two updated cells here, we have a problem. Should we recalculate <code>Burrito Price</code> first, or <code>Total</code>? Ideally, we first calculate <code>Burrito Price</code>, notice that its output has changed, then recalculate <code>Burrito Price w Ship</code>, and finally recalculate <code>Total</code>. However, if we instead recalculate <code>Total</code> first, we’ll have to recalculate it a second time once the new $9 burrito price propagates down. If we don’t calculate cells in the right order, this algorithm isn’t better than recalculating the whole document. In some cases, it’s as slow as VisiCalc!</p>
<p>Clearly, it’s important for us to figure out the right order to update our cells. Broadly, there are two solutions to this problem: dirty marking and topological sorting.</p>
<p>This first solution involves marking all cells downstream from an edit as dirty. For instance, when we update <code>Burrito Price</code>, we would mark the downstream cells <code>Burrito Price w Ship</code> and <code>Total</code> as dirty, even before doing any recalculations:</p>
<p><img alt="same as prior graph with the two updates, but now three nodes are tagged as dirty: &quot;burrito price&quot;, &quot;burrito price w ship&quot;, and &quot;total&quot;. would also like to apologize for the rather confusing image alt text so far; it's really hard to write these for graph diagrams!! if you are a screen reader user and have advice on better ways to do this, would love to hear from you." src="https://lord.io/images/2020/anchors_5.png"></p>
<p>Then, in a loop, we find a dirty cell that has no dirty inputs, and recalculate it. When there are no dirty cells left, we’re done! This solves our ordering problem. There’s one downside though — if a cell is recalculated and we find its new output to be the same as its previous output, we’ll still recalculate downstream cells! A little bit of extra logic can avoid actually running the formula trouble in this case, but we unfortunately still waste time marking and unmarking a lot of cells as dirty.</p>
<p>The second solution is topological sorting. If a cell has no inputs, we mark its height as 0. If a cell has inputs, we mark its height as the maximum of the heights of its inputs, plus one. This guarantees all cells have a greater height than any of their inputs, so we just keep track of all cells with a changed input, always choosing the cell with the lowest height to recalculate first:</p>
<p><img alt="same as prior graph with the two updates, but instead of dirty tags, now every node has a height tag. &quot;burrito price&quot; and &quot;num burritos&quot;, the two cells with no in-nodes, have height 0. &quot;burrito price w ship&quot; has height 1. &quot;total&quot; has height 2." src="https://lord.io/images/2020/anchors_6.png"></p>
<p>In our double-update example, <code>Burrito Price</code> and <code>Total</code> would be initially added to the recalculation heap. <code>Burrito Price</code> has lesser height, and would be recalculated first. Since its output changes, we then would add <code>Burrito Price w Ship</code> to the recalculation heap, and since it too has less height than <code>Total</code>, it would be recalculated before we finally recalculate <code>Total</code>.</p>
<p>This has a big advantage over the first solution: no cell is ever marked dirty unless one of its inputs actually change. However, it requires we keep all cells pending recalculation in sorted order. If we use a heap, this results in an <code>O(n log n)</code> slowdown, so in the worst case, asymptotically slower than Lotus 1-2-3’s strategy of recalculating everything.</p>
<p>Modern-day Excel uses <a href="https://docs.microsoft.com/en-us/office/client-developer/excel/excel-recalculation">a combination of dirty marking and topological sorting</a>, which you can read more about in their docs.</p>
<h2>demand-driven complications</h2>
<p>We’ve now more or less reached the algorithms used in modern-day spreadsheet recalculation. Unfortunately, I suspect there is basically no business case to be made for ever improving it further. The few people with the problem “my Excel spreadsheet is too slow” have already written enough Excel formulas that migration to any other platform is impossible. Fortunately, I have no understanding of business, and so we’re going to look at further improvements anyway.</p>
<p>Beyond caching, one of the cool aspects of a spreadsheet-style computation graph is we can only calculate the cells that we’re interested in. This is sometimes called lazy computation, or demand-driven computation. As a more concrete example, here’s a slightly expanded burrito spreadsheet graph. This example is the same as before, but we’ve added what is best described as “salsa calculations”. Each burrito contains 40 grams of salsa, and we perform a quick multiplication to know how much salsa is in our entire order. In this case, since our order has three burritos, there’s a total of 120 grams of salsa in our entire order.</p>
<p><img alt="a new graph. similar structure to the old graph, but there are two new nodes: &quot;salsa per burrito&quot;, which is set to the constant &quot;40 grams&quot;, and &quot;salsa in order&quot;, which is &quot;salsa per burrito&quot; times &quot;num burritos&quot;" src="https://lord.io/images/2020/anchors_7.png"></p>
<p>Of course, astute readers will have spotted the problem here already: knowing the total weight of salsa in an order is a pretty useless measurement. Who cares that it’s 120 grams? What am I supposed to do with this information?? Unfortunately, a regular spreadsheet would waste cycles calculating <code>Salsa In Order</code>, even if we don’t want it recalculated most of the time.</p>
<p>This is where demand-driven recalculation can help. If we could somehow specify that we’re only interested in the output of <code>Total</code>, we could only recompute that cell and its dependencies, and skip touching <code>Salsa In Order</code> and <code>Salsa Per Burrito</code>. Let’s call <code>Total</code> an <em>observed</em> cell, since we’re trying to look at its output. We can also call both <code>Total</code> and its three dependencies <em>necessary</em> cells, since they’re necessary to compute some observed cell. <code>Salsa In Order</code> and <code>Salsa Per Burrito</code> would be aptly described as <em>unnecessary</em>.</p>
<p>Some folks on the Rust team created the <a href="https://github.com/salsa-rs/salsa">Salsa</a> framework to solve this problem, clearly naming it after the unnecessary salsa calculations their computers were wasting cycles on. Salsa is really cool, and I’m sure <a href="https://www.youtube.com/watch?v=i_IhACacPRY">they can explain</a> how it works better than I can. Very roughly, they use revision numbers to track whether a cell needs recalculation. Any mutation to a formula or input increments the global revision number, and every cell tracks two revisions: <code>verified_at</code> to track the revision its output was last brought up-to-date, and <code>changed_at</code> to track the revision its output last actually changed.</p>
<p><img alt="our new graph, but now there's a title of &quot;current revision: R6&quot;. each cell is tagged with a change revision and verified at revision. all change revisions are R1, except &quot;salsa per burrito&quot;, which is R6. all verified at revisions are R6, except &quot;salsa in order&quot;, which is R1." src="https://lord.io/images/2020/anchors_8.png"></p>
<p>When the user indicates they’d like a fresh value for <code>Total</code>, we’d first recursively recalculate any cell necessary to <code>Total</code>, skipping cells if their <code>last_updated</code> revision is equal to the global revision. Once the dependencies of <code>Total</code> are up-to-date, we only rerun the actual formula in <code>Total</code> if either <code>Burrito Price w Ship</code> or <code>Num Burrito</code> have a <code>changed_at</code> revision greater than the <code>verified_at</code> revision of <code>Total</code>. This is great for Salsa’s purposes in the rust-analyzer, where simplicity is important and each cell takes a significant amount of time to compute. However, we can see the disadvantages in our burrito graph above — if <code>Salsa Per Burrito</code> constantly changes, our global revision number will frequently tick up. This will make each observation of <code>Total</code> walk the three cells necessary to it, even though none of those cells have actually changed. No formulas will be recalculated, but if the graph is large, repeatedly walking all of a cell’s dependencies could get expensive.</p>
<h2>faster demand-driven solutions</h2>
<p>Instead of inventing new algorithms for demand-driven spreadsheets, what if we instead draw from the two classical spreadsheet algorithms mentioned earlier: dirty marking and topological sorting? As you might imagine, a demand-driven model complicates both of these, but both are still viable.</p>
<p>Let’s first look at dirty marking. As before, when we change a cell’s formula, we mark all downstream cells as …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://lord.io/blog/2020/spreadsheets/">https://lord.io/blog/2020/spreadsheets/</a></em></p>]]>
            </description>
            <link>https://lord.io/blog/2020/spreadsheets/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25039393</guid>
            <pubDate>Mon, 09 Nov 2020 20:19:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Learn how to dive into a new codebase (Front end vs. back end)]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25039359">thread link</a>) | @morchen
<br/>
November 9, 2020 | https://swimm.io/blog/2020-08-23-top-down-vs-bottom-up-how-to-deep-dive-into-a-codebase/ | <a href="https://web.archive.org/web/*/https://swimm.io/blog/2020-08-23-top-down-vs-bottom-up-how-to-deep-dive-into-a-codebase/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section data-v-f94c7cca=""> <!----> <div data-v-0cf44990="" data-v-f94c7cca=""><div><p>It’s not impossible to cruise through a programming task on a new codebase and actually complete it without understanding its logic, purpose, or context. Yet there are really only a couple of main approaches to sailing into uncharted waters and coming out the other end alive.</p>
<p>I find it helpful to imagine piecing together a puzzle; we can always start by tackling a dominant piece and working our way from it or rather study the big picture before getting started. Should the approach be the same with a 26-piece-puzzle or a 5K-piece puzzle? Do we write (automation or behavioral) tests, learn the business logic and draw diagrams or rely on conversations with colleagues first?</p>
<blockquote><p lang="en" dir="ltr">Read the documentation? Nobody has time for that!<a href="https://twitter.com/hashtag/100DaysOfCode?src=hash&amp;ref_src=twsrc%5Etfw">#100DaysOfCode</a> <a href="https://twitter.com/hashtag/CodeNewbie?src=hash&amp;ref_src=twsrc%5Etfw">#CodeNewbie</a> <a href="https://twitter.com/hashtag/programming?src=hash&amp;ref_src=twsrc%5Etfw">#programming</a> <a href="https://t.co/aOXy2uJCeU">pic.twitter.com/aOXy2uJCeU</a></p>— Danny Thompson (@DThompsonDev) <a href="https://twitter.com/DThompsonDev/status/1296059274553106432?ref_src=twsrc%5Etfw">August 19, 2020</a></blockquote> 
<p>Whether you are contributing to a new open source project or need to dive into a new repo for work, the approach you opt for will impact your success rate or your teams’. It’s important to cherry-pick the right steps at the right time. Below I list some tips and quick wins I learned along the way for getting into a new repository.</p>
<h3>Bridging the Gap: Top-Down vs Bottom-Up</h3>
<p>Where would you start, if you just arrived at a new team and you were tasked with a backlog issue, for example fixing a bug, creating a button on screen or adding an integration somewhere?</p>
<p><strong>The bottom-up approach</strong>, means trying to get right to the code lines that handle the task at hand. For example, looking for a unique string to pinpoint the specific code area closest to the feature or problem you’re looking to tackle. This approach can lead you to quickly solve the issue at hand. On the other hand, you may unknowingly implement components that have already been implemented, or not understand the impact of your changes on other parts of the codebase.</p>
<p><strong>With top-down</strong>, we look for the larger picture - starting off with modules, , understanding each module’s responsibility, and understanding the main logic. But there are levels of Top-down. For example, you may have a micro-top-down process looking into a specific module (like the module responsible for DB access), or you might have a higher up module - such as the entire backend. The cons of doing an exaggerated top down on a monolithic codebase, is that it’s, plainly speaking, inefficient. It’s easy to hit the end of your exploration capacity and lose sight of the bigger picture when you’re not being hands-on.</p>
<p>While both approaches have merit, they are two extreme ends on the same continuum that are especially problematic for first-timers facing a daunting monolithic codebase. The truth is probably somewhere in the middle (as always). Otherwise, these relatively tempting dichotomous approaches will prove stressful and ineffective when facing delivery.</p>
<p><br>
<strong>Rules of thumbs I try to follow when approaching my first task on a new codebase:</strong></p>
<h3>Get super specific context [Readme, tests and context].</h3>
<p>Even if I want to get to know a very specific code area to complete a task on a repo, I’ll first get some general understanding by looking at the codebase’s Readme files and docs, and even try to look at recent bug-fixes and breakage points to understand the logic. Run the tests and make sure all the frameworks are working and suites are running without errors before starting the task at hand. For a better understanding of the code area, start writing your own tests. If part of a team, I would ask my team lead or mentor which modules and components my first task touches upon.</p>
<h3>Big Picture Components.</h3>
<p>There are some things you should really understand before tackling the task itself. If you’re not sure, ask your team leader / mentor. Why are you writing this code? Why is this important and what’s the objective? What modules will your code interact with? Get a good overview as part of your onboarding but without aimlessly parsing through the codebase. Essentially, this means - start to look for the big picture of the components.</p>
<h3>Quick History Check or Git Extras.</h3>
<p>We love to get frustrated with legacy code, but we also have to learn why some code came to be like that and why it’s still there. Check busy areas of the code through commit messages (look at <a href="https://github.com/tj/git-extras">Git Extras</a> and specifically git effort command to see the number of commit files with the most activity). Another thing is understanding: What libraries and utilities do I need to know? What has already been implemented?</p>
<p>
                <iframe src="https://player.vimeo.com/video/45506445" frameborder="0" allowfullscreen="">
                </iframe>
            </p>
<h3>Check for Interdependencies and Interfaces.</h3>
<p><strong>You can ask: “If I change X, what other modules do I need to change accordingly?”.</strong></p>
<p>At <a href="http://swimm.io/">Swimm</a> for example, users create and run Unit files within the CLI. Users can then collaborate and document information from the repo to highlight different areas of the codebase for other users. We gave a new engineer on our team a task - to save the name of the author of such a Unit upon its creation. This appears to be a super-simple and focused task. Nevertheless, he needed to learn that we have another area that makes sure that what we save in the database is in a certain format so that we don’t send information from our clients (that we do not save by design).</p>
<p>The developer that received this task had to understand the interaction of three different areas - how to save the Unit’s author name (the CLI) without saving client information to the database (DB interaction), and display it in the front end (front end).</p>
<h3>High Exposure = Patterns and Structure.</h3>
<p>Once you’ve explored several areas and functionalities, even poorly documented legacy code, you're bound to start making sense of the mess. You’ll find patterns, see how the code is organized, and gain an understanding of how the different components are connected and inevitably the codebase as a whole. <a href="https://selftaughtcoders.com/how-to-quickly-and-effectively-read-other-peoples-code/">This has to do with high quantity exposure</a> and the way our brain can make up for missing pieces of data. In her new book, <a href="https://www.amazon.com/Badass-Making-Awesome-Kathy-Sierra/dp/1491919019">Badass: Making Users Awesome</a>, Kathy Sierra states:</p>
<blockquote>
<p><strong>“after enough exposure with feedback, your brain [begins] detecting patterns and underlying structures, without your conscious awareness.”</strong></p>
</blockquote>
<h3>Frontend vs Backend: Dive In Like You’re the Captain</h3>
<p>So far I described some high-level tips to apply for any module or code snippet you have to tackle. Yet, there are some tips that better apply to frontend or backend code, as I describe below.</p>
<p><strong>Frontend tips:</strong></p>
<blockquote><div lang="en" dir="ltr"><p>After spending 80 hours on the frontend...</p><p>"Whew! I almost died. Ok let me knock out the API. Gimme 2 hours"</p></div>— Angie Jones (@techgirl1908) <a href="https://twitter.com/techgirl1908/status/1296922565076635648?ref_src=twsrc%5Etfw">August 21, 2020</a></blockquote> 
<ul>
<li>Before you start looking at the code - <strong>interact with it</strong>! Sign up and become a regular user. Try to imagine how each event or action happens in the backend. Play more with the app /website to see how all the changes you will make look and feel constantly. This will also help you consider what your code might affect.</li>
<li><strong>Try to reuse existing components</strong> as much as possible. Be sure to ask your mentor or team lead if there are components you will find helpful in your first task(s).</li>
<li><strong>Try the “friends and family test”.</strong> See how they experience and learn from their user-logic.</li>
</ul>
<p><strong>Backend tips:</strong></p>
<ul>
<li><strong>Research the dependencies</strong>, the functions and main features you must deal with in the backend. Do some frontend investigating as well. It will help you understand how the changes you make in the backend might affect the user’s experience. You’ll need this along the way.</li>
<li><strong>Find all places in the frontend</strong> that interact with your backend code. See if the behavior makes sense before and after your changes.</li>
<li><strong>Skim some areas on git</strong> and <a href="https://github.com/islomar/your-code-as-a-crime-scene">Treat your Code as a Crime Scene</a>. Understand the areas that you touch upon, and look at the code history. Using <a href="https://github.com/tj/git-extras/blob/master/Commands.md">git extra’s commands</a> is a good place to start.</li>
<li><strong>Read the existing</strong> <a href="https://dev.to/perigk/how-to-get-familiar-with-a-new-codebase-i9f">automation tests</a>.</li>
</ul>
<p>
                <iframe type="text/html" src="https://www.youtube.com/embed/qJ_hplxTYJw?autoplay=false" frameborder="0" allowfullscreen="">
                </iframe>
            </p>
<h3>Summarizing thoughts:</h3>
<blockquote>
<p>Experts are not what they know, but what they do</p>
</blockquote>
<p>- <a href="https://medium.com/building-winning-products/kathy-sierra-on-designing-for-badass-ba92cd5fad96">Kathy Sierra on Designing for Badass</a></p>
<ul>
<li>
<p><strong>Write code that others simply get.</strong> That means that your problem solving approach and line of thought are clearly reflected in the code.</p>
</li>
<li>
<p><strong><a href="https://www.forbes.com/sites/forbesproductgroup/2018/03/05/five-critical-steps-to-successful-codebase-on-boarding/#42a08d83e176">Talk to your colleagues</a>.</strong> Successful codebase-onboarding may be heavily influenced by good communication, remote or not, by mentoring or pairing programs, the quality of the tasks you receive, and learning more about the company culture and the people you work with.</p>
</li>
<li>
<p><strong>While onboarding new codebases</strong> <a href="https://swimm.io/blog/2020-07-29-on-the-verge-of-a-new-codebase-5-smart-moves-to-make-when-starting-a-new-job/">we have a rare opportunity to show immediate value</a> to the team by contributing to missing documentation. Once you start to become an expert on a specific area you’ll grow more attentive to missing documentation and you'll have reached the level of Ultimate Onboardee if you’re able to add steps to <a href="http://read.me/"></a>Readme files or deprecated documents.</p>
<p>***</p>
<p>Omer Rosenbaum, is Swimm’s Co-Founder and Chief Technology Officer. Cyber training expert and Founder of Checkpoint Security Academy. Author of <a href="https://data.cyber.org.il/networks/networks.pdf">Computer Networks (in Hebrew)</a>. Visit My <a href="https://www.youtube.com/watch?v=79jlgESHzKQ&amp;list=PL9lx0DXCC4BMS7dB7vsrKI5wzFyVIk2Kg">YouTube Channel</a>.</p>
</li>
</ul>
</div></div> </section></div>]]>
            </description>
            <link>https://swimm.io/blog/2020-08-23-top-down-vs-bottom-up-how-to-deep-dive-into-a-codebase/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25039359</guid>
            <pubDate>Mon, 09 Nov 2020 20:17:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Connect your on-premises databases to Kubernetes in the cloud]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25039310">thread link</a>) | @alexellisuk
<br/>
November 9, 2020 | https://inlets.dev/blog/2020/11/06/hybrid-cloud-with-inlets.html | <a href="https://web.archive.org/web/*/https://inlets.dev/blog/2020/11/06/hybrid-cloud-with-inlets.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p>Learn how to connect private on-premises services to the public cloud with inlets</p>

<h2 id="what-is-hybrid-cloud-anyway">What is “hybrid cloud” anyway?</h2>

<p>Before we get started, let’s have a clear idea what “hybrid cloud” is all about.</p>

<blockquote>
  <p>“<strong>Hybrid Cloud</strong> is a composition of a public cloud and a private environment, such as a private cloud or on-premises resources, offering the benefits of multiple deployment models. … For example, an organization may store sensitive client data in house on a private cloud application, but interconnect that application to services provided on a public cloud as a software service.” – <a href="https://en.wikipedia.org/wiki/Cloud_computing#Hybrid_cloud">Wikipedia</a></p>
</blockquote>

<p>A hybrid cloud strategy can give a huge benefit for your business by moving workloads to a public cloud, leveraging the flexibility and robustness of managed services, while keeping sensitive data on a private cloud or local data center.</p>

<p>In this post, we’ll demonstrate how you can bring your on-premises services or databases into a Kubernetes cluster running on a public cloud.</p>

<p>This model applies for different use-cases:</p>
<ul>
  <li>perhaps you are in the middle of a digital transformation where some parts of the architecture is deployed on a public cloud, but they still need to integrate with some legacy services</li>
  <li>you have some sensitive data to be kept in a private data center due to data residency regulation</li>
</ul>

<h2 id="tutorial">Tutorial</h2>

<p>You’ll need:</p>
<ul>
  <li>A Kubernetes cluster running on a public cloud (e.g. GKE, AKS, EKS, DOKS, …)</li>
  <li><code>kubectl</code>, configured to connect to the cluster</li>
  <li>A domain and access to your DNS admin panel to create a sub-domain</li>
  <li>A service, like a database, running locally</li>
  <li>An inlets PRO license, start <a href="https://docs.google.com/forms/d/e/1FAIpQLScfNQr1o_Ctu_6vbMoTJ0xwZKZ3Hszu9C-8GJGWw1Fnebzz-g/viewform?usp=sf_link">a 14-day free trial</a>.</li>
</ul>

<p>As an example, we will connect a WordPress instance running in the cloud with a MySQL server running locally. Still, this solution is perfectly applicable to other databases or services like e.g. an Oracle database, a MinIO cluster or a RabbitMQ service.</p>

<p><img src="https://inlets.dev/images/2020-11-06-hybrid-cloud-with-inlets/mysql-wordpress.png" alt="hybrid-mysql-wordpress"></p>

<blockquote>
  <p>Picture above: our target architecture, a WordPress in the cloud connecting to a MySQL on-prem via inlets PRO</p>
</blockquote>

<h3 id="create-the-inlets-pro-exit-server">Create the inlets PRO exit server</h3>

<p>Before we start an inlets-pro exit service, create a Kubernetes secret with a token:</p>

<div><div><pre><code>kubectl create secret generic inlets-token <span>--from-literal</span><span>=</span><span>token</span><span>=</span>&lt;a random token&gt;
</code></pre></div></div>

<p>First, start an inlets-pro exit server pod and make it public with a LoadBalancer service:</p>

<div><div><pre><code><span>apiVersion</span><span>:</span> <span>apps/v1</span>
<span>kind</span><span>:</span> <span>Deployment</span>
<span>metadata</span><span>:</span>
  <span>name</span><span>:</span> <span>inlets-pro-server</span>
<span>spec</span><span>:</span>
  <span>replicas</span><span>:</span> <span>1</span>
  <span>selector</span><span>:</span>
    <span>matchLabels</span><span>:</span>
      <span>app</span><span>:</span> <span>inlets-pro-server</span>
  <span>template</span><span>:</span>
    <span>metadata</span><span>:</span>
      <span>labels</span><span>:</span>
        <span>app</span><span>:</span> <span>inlets-pro-server</span>
    <span>spec</span><span>:</span>
      <span>containers</span><span>:</span>
        <span>-</span> <span>name</span><span>:</span> <span>inlets-pro</span>
          <span>image</span><span>:</span> <span>inlets/inlets-pro:0.7.2</span>
          <span>imagePullPolicy</span><span>:</span> <span>IfNotPresent</span>
          <span>command</span><span>:</span> <span>[</span> <span>"</span><span>inlets-pro"</span> <span>]</span>
          <span>args</span><span>:</span>
            <span>-</span> <span>"</span><span>server"</span>
            <span>-</span> <span>"</span><span>--auto-tls"</span>
            <span>-</span> <span>"</span><span>--common-name=inlets.example.com"</span>
            <span>-</span> <span>"</span><span>--token-from=/etc/inlets/token"</span>
          <span>volumeMounts</span><span>:</span>
            <span>-</span> <span>name</span><span>:</span> <span>temp-volume</span>
              <span>mountPath</span><span>:</span> <span>/tmp</span>
            <span>-</span> <span>name</span><span>:</span> <span>inlets-token</span>
              <span>mountPath</span><span>:</span> <span>/etc/inlets</span>
              <span>readOnly</span><span>:</span> <span>true</span>   
      <span>volumes</span><span>:</span>
        <span>-</span> <span>name</span><span>:</span> <span>temp-volume</span>
          <span>emptyDir</span><span>:</span> <span>{}</span>        
        <span>-</span> <span>name</span><span>:</span> <span>inlets-token</span>
          <span>secret</span><span>:</span>
            <span>secretName</span><span>:</span> <span>inlets-token</span>
</code></pre></div></div>

<p>After applying this on the cluster, a exit server pod is available with:</p>

<ul>
  <li><code>auto-tls</code> enabled, meaning a TLS certificate for the <code>common-name</code> is automatically generated</li>
  <li>the default control port 8123</li>
  <li>the token available in the previously created secret</li>
</ul>

<p>Now expose the exit server with a LoadBalancer service:</p>

<div><div><pre><code><span>apiVersion</span><span>:</span> <span>v1</span>
<span>kind</span><span>:</span> <span>Service</span>
<span>metadata</span><span>:</span>
  <span>name</span><span>:</span> <span>inlets-pro-server</span>
  <span>labels</span><span>:</span>
    <span>app</span><span>:</span> <span>inlets-pro-server</span>
<span>spec</span><span>:</span>
  <span>type</span><span>:</span> <span>LoadBalancer</span>
  <span>ports</span><span>:</span>
    <span>-</span> <span>name</span><span>:</span> <span>control</span>
      <span>port</span><span>:</span> <span>8123</span>
      <span>targetPort</span><span>:</span> <span>8123</span>
  <span>selector</span><span>:</span>
    <span>app</span><span>:</span> <span>inlets-pro-server</span>
</code></pre></div></div>

<blockquote>
  <p>Instead of using a LoadBalancer service, a Kubernetes Ingress can also be used here, especially when bringing multiple services into your cluster.</p>
</blockquote>

<p>As you can see, we’ll only expose the control port 8123 to the outside world.
This is actually a good thing, as our database will only reachable from within our Kubernetes cluster, making it more secure.</p>

<p>Wait a little bit until the load balancer is created, grab it’s public IP address and point your domain (remember the common-name) to it.</p>

<div><div><pre><code><span>$ </span>kubectl get service inlets-pro-server
NAME                TYPE           CLUSTER-IP       EXTERNAL-IP       PORT<span>(</span>S<span>)</span>          AGE
inlets-pro-server   LoadBalancer   192.168.197.17   185.136.232.105   8123:31981/TCP   8m11s
</code></pre></div></div>

<blockquote>
  <p>TIP: Some cloud providers honor the <code>loadBalancerSourceRanges</code> field in the Service spec, which allows you to provide a list of IP CIDR blocks allowed to connect to the load balancer. By creating firewall rules, only connections coming from your on-prem data center are allowed.</p>
</blockquote>

<h3 id="start-the-inlets-pro-client">Start the inlets-pro client</h3>

<p>Now that the server part of the tunnel is running, it is time to start the client in our private data center.
Let’s say we have a MySQL instance available with an internal IP address <code>10.1.0.50</code>, start the inlets-pro client:</p>

<div><div><pre><code><span>$ </span>inlets-pro client <span>--license-file</span> ~/inlets-license <span>--port</span> 3306 <span>--url</span> wss://inlets.example.com:8123/connect <span>--upstream</span> 10.1.0.50 <span>--token</span> &lt;your token&gt; 
2020/11/05 13:23:21 Welcome to inlets-pro! Client version 0.7.2
2020/11/05 13:23:21 Licensed to: Johan Siebens &lt;xxxx@gmail.com&gt;, expires: xxx day<span>(</span>s<span>)</span>
2020/11/05 13:23:21 Upstream server: 10.1.0.50, <span>for </span>ports: 3306
inlets-pro client. Copyright Alex Ellis, OpenFaaS Ltd 2020
INFO[2020/11/05 13:23:21] Connecting to proxy                           <span>url</span><span>=</span><span>"wss://inlets.example.com:8123/connect"</span>
</code></pre></div></div>

<p>Perfect! Now the client made the connection, port 3306 of the server pod in our public cloud is accepting connection and will tunnel traffic to the MySQL instance.</p>

<h3 id="create-a-mysql-service">Create a MySQL service</h3>

<p>When we deploy WordPress, we could configure it to connect directly to the inlets-pro server pod, but it is better to create Kubernetes Service:</p>

<div><div><pre><code><span>apiVersion</span><span>:</span> <span>v1</span>
<span>kind</span><span>:</span> <span>Service</span>
<span>metadata</span><span>:</span>
  <span>name</span><span>:</span> <span>mysql</span>
  <span>labels</span><span>:</span>
    <span>app</span><span>:</span> <span>mysql</span>
<span>spec</span><span>:</span>
  <span>ports</span><span>:</span>
    <span>-</span> <span>name</span><span>:</span> <span>mysql</span>
      <span>port</span><span>:</span> <span>3306</span>
      <span>targetPort</span><span>:</span> <span>3306</span>
  <span>selector</span><span>:</span>
    <span>app</span><span>:</span> <span>inlets-pro-server</span>
</code></pre></div></div>

<p>The set of Pods targeted by this Service is determined by the same selector as the previous service, but this time it is a service of type ClusterIP, making it only accessible from inside the cluster.</p>

<h3 id="deploy-wordpress">Deploy WordPress</h3>

<p>The only thing left for our example is deploying a WordPress instance, connecting to the MySQL database via the inlets-pro tunnel:</p>

<div><div><pre><code><span>apiVersion</span><span>:</span> <span>apps/v1</span>
<span>metadata</span><span>:</span>
  <span>name</span><span>:</span> <span>wordpress</span>
  <span>labels</span><span>:</span>
    <span>app</span><span>:</span> <span>wordpress</span>
<span>spec</span><span>:</span>
  <span>selector</span><span>:</span>
    <span>matchLabels</span><span>:</span>
      <span>app</span><span>:</span> <span>wordpress</span>
  <span>template</span><span>:</span>
    <span>metadata</span><span>:</span>
      <span>labels</span><span>:</span>
        <span>app</span><span>:</span> <span>wordpress</span>
    <span>spec</span><span>:</span>
      <span>containers</span><span>:</span>
      <span>-</span> <span>image</span><span>:</span> <span>wordpress</span>
        <span>name</span><span>:</span> <span>wordpress</span>
        <span>env</span><span>:</span>
        <span>-</span> <span>name</span><span>:</span> <span>WORDPRESS_DB_HOST</span>
          <span>value</span><span>:</span> <span>mysql</span>
        <span>ports</span><span>:</span>
        <span>-</span> <span>containerPort</span><span>:</span> <span>80</span>
          <span>name</span><span>:</span> <span>wordpress</span>
</code></pre></div></div>

<blockquote>
  <p>note: this WordPress is not production-ready as it is missing the required volumes for the content</p>
</blockquote>

<p>Mission accomplished! Our WordPress application, running in a public cloud environments is using the MySQL server located in the private data center.</p>

<h2 id="wrapping-up">Wrapping up</h2>

<p>This tutorial gives us a short introduction on how inlets PRO can help us to build a hybrid cloud between existing servers and public cloud.
As a cheaper, easier alternative to a data-center uplink or managed product like AWS Direct Connect or Azure Express Route it is a very lightweight, but powerful, tool to bring your on-prem services to a cloud workload.</p>

<p>For the example we chose WordPress, but the same technique can be applied to any other applications that use TCP traffic.</p>

<ul>
  <li>Resource heavy ETL processes on the cloud, combining multiple data sources like private legacy databases and event streams in the public cloud.</li>
  <li>Data migrations from and to on-prem databases</li>
  <li>Connect your new application to legacy service during a digital transformation</li>
  <li>Keep your LDAP side on-premises in Active Directory and connect to a SaaS IDP product like Auth0. That way anyone can log into a website using their corporate identity without having to migrate Active Directory to the cloud.</li>
</ul>

<p>Further resources:</p>

<ul>
  <li><a href="https://docs.inlets.dev/">Read tutorials and documentation for inlets PRO and OSS</a></li>
  <li><a href="https://inlets.dev/">Kick the tires with free 14-day trial of inlets PRO</a></li>
  <li><a href="https://twitter.com/inletsdev/">Follow @inletsdev on Twitter</a></li>
</ul>

        </div></div>]]>
            </description>
            <link>https://inlets.dev/blog/2020/11/06/hybrid-cloud-with-inlets.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25039310</guid>
            <pubDate>Mon, 09 Nov 2020 20:13:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Play Dots and Boxes Against AlphaZero in JavaScript]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25039225">thread link</a>) | @carlosaguayo
<br/>
November 9, 2020 | https://carlos-aguayo.github.io/alphazero/ | <a href="https://web.archive.org/web/*/https://carlos-aguayo.github.io/alphazero/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
              <h4>About</h4>
              <p>WebApp that implements AlphaZero, an exciting and novel Reinforcement Learning Algorithm, used to beat world-champions in games like Go and Chess. In this WebApp, AlphaZero was trained to play the <a href="https://en.wikipedia.org/wiki/Dots_and_Boxes">Dots and Boxes game</a>. This WebApp was written for this <a href="https://towardsdatascience.com/alphazero-a-novel-reinforcement-learning-algorithm-deployed-in-javascript-56018503ad18">Medium blog post</a>.</p>
            </div></div>]]>
            </description>
            <link>https://carlos-aguayo.github.io/alphazero/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25039225</guid>
            <pubDate>Mon, 09 Nov 2020 20:04:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Nginx conf snippet for blocking bad bots]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25039051">thread link</a>) | @rogue_man_coder
<br/>
November 9, 2020 | https://www.michaellapan.com/blocking-bad-bots | <a href="https://web.archive.org/web/*/https://www.michaellapan.com/blocking-bad-bots">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-v-1a93d359="" data-v-0291b861=""> <p>A co-worker of mine recently brought up an issue to me where one of our clients were exceeding their monthly ShipperHQ api quota. The wordpress site in question was exceeding their 10k api limit from ShipperHQ for 2 months in a row. The combined traffic and sales from the site did not justify hitting 10k api calls monthly. This is where I came in to see if there was a deeper issue.</p> <p>My first steps were to identify when the api call was sent to ShipperHQ on the site. After some navigating though some pages and keeping an eye on the network tab. I determined the call was being sent out when visiting the checkout.</p> <p>With that known it was time to see where the hits were coming from.</p> <ol><li><p>My first though was to check if the shopping checkout was indexed somewhere on google. A quick google dork later <code>site:site-url inurl:checkout</code> I had found that there were a few entries for the shopping checkout url on google. We removed them and edited the robots.txt</p></li> <li><p>Ensured ShipperHQ was not on the cart page.</p></li> <li><p>lastly, after analyzing the logs I had found a large number of bots that did not respect the robots.txt. Becuase of this they were adding items to carts and navigating to the checkout as they crawled the site. This is where I believed the majortity of the ShipperHQ api hits were coming from. Now its time to block these bots entirly from the site.</p></li></ol> <h4>Nginx Config</h4> <p>To block these malicious bots from crawling the site you can do the following in your nginx conf. While this isnt perfect, as the person with the crawler can change the user_agent. It will atleast get the majortity of them</p> <p>If you need to find the user_agent just go to your nginx access.log and it will show it there. Copy and past it to this list (+restart nginx) and it will start blocking.</p> <p>Specifically for nginx we send back http code 444 for any matching user agents. Code 444 is</p> <blockquote><p>CONNECTION CLOSED WITHOUT RESPONSE</p></blockquote> <pre><code>  if ($http_user_agent ~* (360Spider|80legs.com|Abonti|AcoonBot|Acunetix|adbeat_bot|AddThis.com|adidxbot|ADmantX|AhrefsBot|AngloINFO|Antelope|BaiduSpider|BeetleBot|billigerbot|binlar|bitlybot|BlackWidow|BLP_bbot|BoardReader|Bolt\ 0|BOT\ for\ JCE|Bot\ mailto\:craftbot@yahoo\.com|casper|CazoodleBot|CCBot|checkprivacy|ChinaClaw|chromeframe|Clerkbot|Cliqzbot|clshttp|CommonCrawler|comodo|CPython|crawler4j|Crawlera|CRAZYWEBCRAWLER|Curious|Custo|CWS_proxy|Default\ Browser\ 0|diavol|DigExt|Digincore|DIIbot|discobot|DISCo|DoCoMo|DotBot|Download\ Demon|DTS.Agent|EasouSpider|eCatch|ecxi|EirGrabber|Elmer|EmailCollector|EmailSiphon|EmailWolf|Exabot|ExaleadCloudView|ExpertSearchSpider|ExpertSearch|Express\ WebPictures|ExtractorPro|extract|EyeNetIE|Ezooms|F2S|FastSeek|feedfinder|FeedlyBot|FHscan|finbot|Flamingo_SearchEngine|FlappyBot|FlashGet|flicky|Flipboard|g00g1e|Genieo|genieo|GetRight|GetWeb\!|GigablastOpenSource|GozaikBot|Go\!Zilla|Go\-Ahead\-Got\-It|GrabNet|grab|Grafula|GrapeshotCrawler|GTB5|GT\:\:WWW|harvest|heritrix|HMView|HomePageBot|HTTP\:\:Lite|HTTrack|HubSpot|ia_archiver|icarus6|IDBot|id\-search|IlseBot|Image\ Stripper|Image\ Sucker|Indigonet|Indy\ Library|integromedb|InterGET|InternetSeer\.com|Internet\ Ninja|IRLbot|ISC\ Systems\ iRc\ Search\ 2\.1|jakarta|Java|JetCar|JobdiggerSpider|JOC\ Web\ Spider|Jooblebot|kanagawa|KINGSpider|kmccrew|larbin|LeechFTP|libwww|Lingewoud|LinkChecker|linkdexbot|LinksCrawler|LinksManager\.com_bot|linkwalker|LinqiaRSSBot|LivelapBot|ltx71|LubbersBot|lwp\-trivial|Mail.RU_Bot|masscan|Mass\ Downloader|maverick|Maxthon$|Mediatoolkitbot|MegaIndex|MegaIndex|megaindex|MFC_Tear_Sample|Microsoft\ URL\ Control|microsoft\.url|MIDown\ tool|miner|Missigua\ Locator|Mister\ PiX|mj12bot|Mozilla.*Indy|Mozilla.*NEWT|MSFrontPage|msnbot|Navroad|NearSite|NetAnts|netEstate|NetSpider|NetZIP|Net\ Vampire|NextGenSearchBot|nutch|Octopus|Offline\ Explorer|Offline\ Navigator|OpenindexSpider|OpenWebSpider|OrangeBot|Owlin|PageGrabber|PagesInventory|panopta|panscient\.com|Papa\ Foto|pavuk|pcBrowser|PECL\:\:HTTP|PeoplePal|Photon|PHPCrawl|planetwork|PleaseCrawl|PNAMAIN.EXE|PodcastPartyBot|prijsbest|proximic|psbot|purebot|pycurl|QuerySeekerSpider|R6_CommentReader|R6_FeedFetcher|RealDownload|ReGet|Riddler|Rippers\ 0|rogerbot|RSSingBot|rv\:1.9.1|RyzeCrawler|SafeSearch|SBIder|Scrapy|Scrapy|SeaMonkey$|search.goo.ne.jp|SearchmetricsBot|search_robot|SemrushBot|Semrush|SentiBot|SEOkicks|SeznamBot|ShowyouBot|SightupBot|SISTRIX|sitecheck\.internetseer\.com|siteexplorer.info|SiteSnagger|skygrid|Slackbot|Slurp|SmartDownload|Snoopy|Sogou|Sosospider|spaumbot|Steeler|sucker|SuperBot|Superfeedr|SuperHTTP|SurdotlyBot|Surfbot|tAkeOut|Teleport\ Pro|TinEye-bot|TinEye|Toata\ dragostea\ mea\ pentru\ diavola|Toplistbot|trendictionbot|TurnitinBot|turnit|URI\:\:Fetch|urllib|Vagabondo|Vagabondo|vikspider|VoidEYE|VoilaBot|WBSearchBot|webalta|WebAuto|WebBandit|WebCollage|WebCopier|WebFetch|WebGo\ IS|WebLeacher|WebReaper|WebSauger|Website\ eXtractor|Website\ Quester|WebStripper|WebWhacker|WebZIP|Web\ Image\ Collector|Web\ Sucker|Wells\ Search\ II|WEP\ Search|WeSEE|Wget|Widow|WinInet|woobot|woopingbot|worldwebheritage.org|Wotbox|WPScan|WWWOFFLE|WWW\-Mechanize|Xaldon\ WebSpider|XoviBot|yacybot|YandexBot|Yandex|YisouSpider|zermelo|Zeus|zh-CN|ZmEu|ZumBot|ZyBorg) ) {
      return 444;
  }
</code></pre> <h4>Wordpress modification</h4> <p>If your are unable to modify your nginx conf. Another option would be to have shipping calculation on a different page.</p> <p>Or, using jQuery, to only load the ShipperHQ calculations after the shipping address is filled out.</p></div></div>]]>
            </description>
            <link>https://www.michaellapan.com/blocking-bad-bots</link>
            <guid isPermaLink="false">hacker-news-small-sites-25039051</guid>
            <pubDate>Mon, 09 Nov 2020 19:48:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How does consciousness even make sense?]]>
            </title>
            <description>
<![CDATA[
Score 22 | Comments 26 (<a href="https://news.ycombinator.com/item?id=25039045">thread link</a>) | @algoholix
<br/>
November 9, 2020 | http://niklasbuehler.com/blog/consciousness.html | <a href="https://web.archive.org/web/*/http://niklasbuehler.com/blog/consciousness.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

<header>

<span><a href="http://niklasbuehler.com/">Home</a> / <a href="http://niklasbuehler.com/rss.xml">RSS</a> / <a href="http://niklasbuehler.com/contact.html">Contact me</a></span>
</header>
<p><em>08.11.2020</em></p>
<!--
## How does consciousness even make sense?
<a id='1604845992' href='/log/#1604845992'>#1604845992 2020 Nov 08, 15:33</a>
-->
<p>I don’t think the current state of “artificial intelligence” really has proven that it earns the great title of <em>intelligence</em>, as I believe it’s all still just a sophisticated application of statistics on large amounts of data. I prefer the title “machine learning”, as in my opinion that describes the process of adjusting the parameters of statistical methods based on the given data adequately.</p>
<p>I’m not even sure if intelligence and consciousness can be simulated by a computer. Because if they could, then the speed of execution surely wouldn’t matter to that fact, right? And if speed didn’t matter, one could just as well represent the (deterministic!) calculations of a finite computer on a piece of paper or by arranging some stones on a large field. Granted, it’d be somewhat slower than a modern computer and the paper would have to be sufficiently large, but in the end flipping bits, drawing on paper, and moving rocks in a systematic way is just the same when it comes to representing computation. So that’d mean if we arranged a bunch of stones on a large field in a certain pattern and then used some fancy (but deterministic) rules to move them around, we’d create consciousness?! I can’t really believe that’s true.</p>
<p><em>But how is a human brain any different??</em> In the end it’s also just biological wires exchanging electricity (+ some chemistry added to the process)…<br>
I can’t really grasp that. Do my thoughts make sense? Where’s the flaw?</p>
<hr>
<h3 id="join-the-discussion-on-hacker-news">Join the discussion on Hacker News</h3>
<p>There’s an interesting discussion about this text on <a href="https://news.ycombinator.com/item?id=25039045">Hacker News</a>.</p>

<hr>

<h3>Want to leave a comment?</h3>
<p>
If you want to give me some feedback or share your opinion, please contact me via <a href="mailto:hi@niklasbuehler.com?subject=Comment%20on%20Blog:%20consciousness" target="_blank">email</a>.
</p>

<hr>

<p>
<span>© Niklas Bühler, 2020</span>
<span><a href="http://niklasbuehler.com/rss.xml">RSS</a> / <a href="http://niklasbuehler.com/contact.html">Contact me</a></span>
</p>



</div>]]>
            </description>
            <link>http://niklasbuehler.com/blog/consciousness.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25039045</guid>
            <pubDate>Mon, 09 Nov 2020 19:48:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I wrote every day for 365 days]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25038739">thread link</a>) | @patwalls
<br/>
November 9, 2020 | https://patwalls.com/365-blog-posts-in-365-days | <a href="https://web.archive.org/web/*/https://patwalls.com/365-blog-posts-in-365-days">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://patwalls.com/365-blog-posts-in-365-days</link>
            <guid isPermaLink="false">hacker-news-small-sites-25038739</guid>
            <pubDate>Mon, 09 Nov 2020 19:28:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AWS Security Maturity Roadmap [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 74 | Comments 5 (<a href="https://news.ycombinator.com/item?id=25038422">thread link</a>) | @sciurus
<br/>
November 9, 2020 | https://summitroute.com/downloads/aws_security_maturity_roadmap-Summit_Route.pdf | <a href="https://web.archive.org/web/*/https://summitroute.com/downloads/aws_security_maturity_roadmap-Summit_Route.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://summitroute.com/downloads/aws_security_maturity_roadmap-Summit_Route.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25038422</guid>
            <pubDate>Mon, 09 Nov 2020 19:07:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Contemplating Becoming a Monk]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25038254">thread link</a>) | @laybak
<br/>
November 9, 2020 | https://knowledgeartist.org/article/becoming-a-monk | <a href="https://web.archive.org/web/*/https://knowledgeartist.org/article/becoming-a-monk">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <p><span>I contemplated becoming a Buddhist monk. </span></p> <p><span>For a period of time, I was feeling unmotivated. Depressed, perhaps. It seemed like nothing I did would matter. What's the point of doing anything? </span></p> <p><span>Leaving behind worldly attachments seemed appealing. It was a romantic idea I entertained. </span></p> <p><span>Well, I didn't end up becoming a monk. At least not yet. This post documents my thinking through the decision. And my renewed passion for life.</span></p>  <p><h3><span>Teachings that Resonated</span></h3></p> <p><span>Much of Buddhist teachings rings true to me. Freeing oneself from suffering. Attaining a calm mind through spiritual practice. Living a peaceful life. </span></p> <p><span>It resonated both on a conceptual and a visceral level. And I don't know how much of that came from hearing about it over time from my Buddhist mom.</span></p> <p><span>I could always use more concentration. I could always use more </span> <a href="https://en.wikipedia.org/wiki/Noble_Eightfold_Path" target="_blank"><span>mindfulness</span></a> <span>.</span></p>  <p><h3><span>Leaving It All Behind</span></h3></p> <p><span>The life of a Buddhist monk would be simple. It would be a journey towards enlightenment. </span></p> <p><span>It would mean letting go of my ego and desires. My hopes and dreams. My worries and anxiety. All of it. </span></p> <p><span>It would mean walking away from my possessions and wealth. My comfortable urban lifestyle. My family and friends. </span></p> <p><span>I would be at peace. </span></p>  <p><h3><span>Testing the Idea</span></h3></p> <p><span>But of course, I wasn't going to just take the plunge without validating it in a low-risk way. After all, I had </span> <a href="https://knowledgeartist.org/article/about-me" target="_blank"><span>front-loaded my retirement</span></a> <span> before I started working full-time.</span></p> <p><span>There was only so much I could learn about the Buddhist way of life from reading about it. To validate the idea, I visited several temples. Including a stay at </span> <a href="https://en.wikipedia.org/wiki/Baegyangsa" target="_blank"><span>Baekyangsa</span></a> <span>.</span></p> <p><span>﻿</span> <a href="https://en.wikipedia.org/wiki/Baegyangsa" target="_blank"><span>﻿</span></a> <span>In the process, I spoke with many monks about their experience. I learned a lot more about Buddhist practice. And got to eat some exquisite temple food. 😋</span></p> <p><img src="https://storage.googleapis.com/rumin-gcs-bucket/newsletter/jeong-kwan-mushroom.png"></p>   <p><h3><span>Calming Routines</span></h3></p> <p><span>The routines at a temple didn't seem too far from what I was used to. Over the years, several friends had remarked that I "lived like a monk" anyway.</span></p> <p><span>Waking up at 4 AM was a bit early. But it was doable. And I quite enjoyed the tranquility that the early mornings afforded. I didn't mind the chores. In fact, it felt good to take responsibility to help maintain the temple.</span></p> <p><span>What was the most powerful though, was the morning practice. In a dark room dimly lit with candles, we meditated. We performed prostrations. We chanted in unison. Simple acts, but they moved my soul. </span></p>  <p><h3><span>Conversations With a Young Monk</span></h3></p> <p><span>At one of the temples, I met a young monk around my age. He went to college in New Zealand and was fluent in English. </span></p> <p><span>Having a common language and cultural understanding made him extra relatable. Our conversations were a valuable resource for learning about practicing Buddhism in a modern context.</span></p> <p><span>For him, Buddhist practice was not about strictly following the teachings and the scripture. It was the cultivation of the mind that he was after.</span></p> <p><span>That resonated. It also made the idea of becoming a monk more personal. I could totally see myself becoming more like this guy.</span></p> <p><span>And fun fact, I brought up the </span> <a href="https://www.simulation-argument.com/" target="_blank"><span>simulation argument</span></a> <span> and he too thought that it was likely we are living in a simulation. </span></p>  <p><h3><span>Material Possessions </span></h3></p> <p><span>Having adopted a minimalist lifestyle, living an austere life would not be a big issue. </span></p> <p><span>I didn't own much stuff to begin with. I owned (and still do) a total of five shirts (two of which are identical). And most of my material belongings fit inside a backpack.</span></p> <p><span>I might smuggle in my </span> <a href="https://knowledgeartist.org/article/croissant-way-of-life" target="_blank"><span>morning croissant</span></a> <span> once in a while. But I would be OK for the most part. </span></p>  <p><h3><span>Holding Onto Relationships</span></h3></p> <p><span>But one aspect of life that I have chosen not to leave behind was my earthly relationships.</span></p> <p><span>Some of my happiest moments were with people I loved. Even plain rice tasted better when shared with loved ones. I also remembered how reading Tim Urban's "</span> <a href="https://waitbutwhy.com/2015/12/the-tail-end.html" target="_blank"><span>The Tail End</span></a> <span>" brought me to tears.</span></p> <p><span>I cherished the relationships in my life. And I refused to give them up. </span></p> <p><span>[🚨 Spoiler Alert] In a way, I felt like </span> <a href="https://youtu.be/cH-HT9WCtiQ?t=453" target="_blank"><span>Aang (in Avatar) when he was trying to open his seventh chakra</span></a> <span> to get into the all-powerful Avatar State at will. The chakra was blocked by his earthly attachment. In Aang's case, it was Katara, his lover. For him, the decision was clear: </span> <em>"Why would I choose cosmic energy over Katara?"</em> <span> ﻿</span></p> <p><img src="https://storage.googleapis.com/rumin-gcs-bucket/newsletter/aang-chakra.png"></p>  <p><h3><span>More Fully Participating in Life</span></h3></p> <p><span>I thought about it some more. It became clear to me that I did not want to sit out on life. I did not wish to retreat, to disengage, to let go.</span></p> <p><span>Instead, I wanted to fully embrace life. All the pleasure and pain that it brings, in all its intensity. I wanted to feel the fiery passions. Taste the full spectrum of flavours. Experience the alternate states of consciousness.</span></p> <p><span>Unsuppressed. Unbounded.</span></p> <p><span>I wanted to not just understand timeless abstract principles. But to fully act out my role in my lifetime.</span></p>  <p><h3><span>Creating My Own Meaning</span></h3></p> <p><span>My purpose in life is to feel. It is to create. It is to acquire wisdom and share it. </span></p> <p><span>I want to serve others through my creations. Each day is a challenge I have to overcome, on my never-ending path to mastery. </span></p> <p><span>I would rather live to become a village elder. Loved by the tribe. Scarred by his experiences in life. Respected for wisdom and contributions. That archetype appeals to me more.</span></p> <p><span>There will be gains and losses. Ups and downs. Moments of pure joy. And misery and suffering. </span></p> <p><span>So be it.</span></p>        


          
            
            <p><em>Each week, I send out a newsletter where I share my learnings on product, automation, productivity, and other learnings.</em></p>
            <p><em>Enter your email below to subscribe.</em></p>

            
          
        </div></div>]]>
            </description>
            <link>https://knowledgeartist.org/article/becoming-a-monk</link>
            <guid isPermaLink="false">hacker-news-small-sites-25038254</guid>
            <pubDate>Mon, 09 Nov 2020 18:55:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Boost Visual Accessibility by Auto Flipping Text Color Based on Background Color]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25038248">thread link</a>) | @karenying7
<br/>
November 9, 2020 | https://www.blog.karenying.com/posts/boost-visual-accessibility-by-auto-flipping-text-color | <a href="https://web.archive.org/web/*/https://www.blog.karenying.com/posts/boost-visual-accessibility-by-auto-flipping-text-color">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><span>
      <a href="https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/d43b4/boost-visual-accessibility-by-changing-your-text-color.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
        <source srcset="https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/8ac56/boost-visual-accessibility-by-changing-your-text-color.webp 240w,
https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/d3be9/boost-visual-accessibility-by-changing-your-text-color.webp 480w,
https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/e46b2/boost-visual-accessibility-by-changing-your-text-color.webp 960w,
https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/ccc09/boost-visual-accessibility-by-changing-your-text-color.webp 1202w" sizes="(max-width: 960px) 100vw, 960px" type="image/webp">
        <source srcset="https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/8ff5a/boost-visual-accessibility-by-changing-your-text-color.png 240w,
https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/e85cb/boost-visual-accessibility-by-changing-your-text-color.png 480w,
https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/d9199/boost-visual-accessibility-by-changing-your-text-color.png 960w,
https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/d43b4/boost-visual-accessibility-by-changing-your-text-color.png 1202w" sizes="(max-width: 960px) 100vw, 960px" type="image/png">
        <img src="https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/d9199/boost-visual-accessibility-by-changing-your-text-color.png" alt="gradient.png" title="gradient.png" loading="lazy">
      </picture>
  </a>
    </span><em><a href="http://gradient-png.netlify.app/" target="_blank">gradient.png</a></em></p>
<p><strong>If you’re only looking for implementation, <a href="#implementation">skip ahead</a>.</strong></p>
<p>Often times, an app, website, or diagram will write text over a colored background. If the text is white, and the background is light colored, then it’s always hard to read. Visual accessibility is becoming an increasingly hot topic. In this post, we’ll quantify this contrast between two colors, define a standard for the minimum allowed contrast, and implement a way to dynamically change text color based on background color!</p>
<h2 id="wcag-and-contrast-ratio"><a href="#wcag-and-contrast-ratio" aria-label="wcag and contrast ratio permalink"></a>WCAG and Contrast Ratio</h2>
<p>The <a href="https://www.w3.org/WAI/standards-guidelines/wcag/" target="_blank">Web Content Accessibility Guidelines</a> (WCAG) aims to provide a set of standards for developers around the world — to make web content more accessible to people with disabilities.</p>
<p>Perhaps the most obvious applications of the WCAG are in visual accessibility. In this post we’ll dive into the world of color contrast, specifically looking at scenarios where you would want to dynamically change text color based on the background color to increase readability.</p>
<p>In order to understand the motivation behind this, we have to first understand how to quantify the contrast between two colors.</p>
<h3 id="mathematical-representation-of-colors"><a href="#mathematical-representation-of-colors" aria-label="mathematical representation of colors permalink"></a>Mathematical Representation of Colors</h3>
<p>A crash course:</p>
<ul>
<li>
<p>Each color can be represented as a triplet of red, green, and blue values, with values between 0 and 255 — this is the <a href="https://en.wikipedia.org/wiki/RGB_color_space" target="_blank">RGB color space</a></p>
<ul>
<li>Red is <code>(255, 0, 0)</code>, green is <code>(0, 255, 0)</code>, blue is <code>(0, 0, 255)</code></li>
<li>Eggplant purple is equal parts red and blue so it’s <code>(128, 0, 128)</code></li>
</ul>
</li>
<li>
<p>To digitize this RGB triplet, we have <a href="https://en.wikipedia.org/wiki/Web_colors#Hex_triplet" target="_blank">HTML (Hex) color codes</a> which HTML/CSS uses</p>
<ul>
<li>Hex codes are just the concatenation of RGB values in hexadecimal, often preceded by a <code>#</code></li>
<li>Red = <code>#ff0000</code>, green = <code>#00ff00</code>, blue = <code>#0000ff</code>, eggplant purple = <code>#800080</code></li>
</ul>
</li>
<li>Hex codes only represent the RGB color space. There are many other color spaces such as <a href="https://en.wikipedia.org/wiki/HSL_and_HSV" target="_blank">HSL/HSV/HSB</a>, <a href="https://en.wikipedia.org/wiki/CIELAB_color_space" target="_blank">CIELAB</a>, and <a href="https://en.wikipedia.org/wiki/CMYK_color_model" target="_blank">CMYK</a>. Each serve different purposes but we’ll be focusing on RGB for this post.</li>
</ul>
<h3 id="luminance"><a href="#luminance" aria-label="luminance permalink"></a>Luminance</h3>
<p>Now that we have a way to quantify colors, we can talk about <a href="https://en.wikipedia.org/wiki/Relative_luminance" target="_blank">luminance</a>, the preceived brightness of a color. This is the L value in the HSL color space. The mathematical model for <a href="https://planetcalc.com/7779/" target="_blank">calculating luminance</a> is rather convoluted so I’ll be glazing over it. All we need to know is that</p>
<ul>
<li>it can be derived from an RGB triplet — it’s roughly a weighted average of the three values</li>
<li>can be represented as a percentage, or as a value from 0 - 1 (what we’ll be using)</li>
<li>white is 1, black is 0</li>
</ul>
<h3 id="contrast-ratio"><a href="#contrast-ratio" aria-label="contrast ratio permalink"></a>Contrast Ratio</h3>
<p>The final piece is to calculate the <a href="https://webaim.org/articles/contrast/" target="_blank">contrast ratio</a> between two colors, namely between a text color and its background color here.</p>
<p>The formula is just the quotient between the brighter color’s luminance (the bigger number) and the dark color’s luminance (the smaller number):</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>ratio</mtext><mo>=</mo><mfrac><mrow><mi>l</mi><mi>u</mi><mi>m</mi><mo stretchy="false">(</mo><mrow><mtext mathvariant="bold">brighter</mtext><mtext>&nbsp;color)</mtext></mrow><mo>+</mo><mn>0.05</mn></mrow><mrow><mi>l</mi><mi>u</mi><mi>m</mi><mo stretchy="false">(</mo><mrow><mtext mathvariant="bold">darker</mtext><mtext>&nbsp;color</mtext></mrow><mo stretchy="false">)</mo><mo>+</mo><mn>0.05</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">\textrm{ratio} = \frac{lum(\textrm{\textbf{brighter} color)} + 0.05}{lum(\textrm{\textbf{darker} color}) + 0.05}</annotation></semantics></math></span></span></span></p><p>The contrast ratio of any two colors ranges between 1 (two of the same colors) to 21 (black and white).</p>
<p><strong>The WCAG requires a contrast ratio of at least 4.5.</strong></p>
<p>There are three exceptions to the above rule:</p>
<ul>
<li><strong>Large text</strong>: larger text is easier to read. Thus the required ratio is at least 3.0 for bigger text.</li>
<li><strong>Incidental</strong>: text that is for decorative/design purposes, not really meant to be read</li>
<li><strong>Logos</strong></li>
</ul>
<h4 id="examples"><a href="#examples" aria-label="examples permalink"></a>Examples</h4>
<ul>
<li>Contrast ratio of <strong>12.98</strong> (good job Karen)</li>
<li><span>Contrast ratio of <b>4.77</b> (I could do better for links)</span></li>
<li><span>Contrast ratio of <b>4.54</b> (barely passable by WCAG standards)</span></li>
<li><span>Contrast ratio of <b>1.61</b> (terrible)</span></li>
<li><span>Contrast ratio of <b>1.15</b> (brb my eyes are crying)</span></li>
</ul>
<h2 id="use-cases"><a href="#use-cases" aria-label="use cases permalink"></a>Use Cases</h2>
<p>Before we get started coding, let’s run through a couple of examples of where you would want to dynamically changed the text color based on its background color.</p>
<h3 id="facebook-messenger"><a href="#facebook-messenger" aria-label="facebook messenger permalink"></a>Facebook Messenger</h3>
<p>Messenger is what spurred this post. Facebook lets you change the chat default color from the typical blue to a variety of different options. This theme color the background color of all the messages <em>you</em> send. The messages you receive are typically with a dark gray background.</p>
<p><span><span>
      <a href="https://www.blog.karenying.com/static/a8f769be03dbd5ce10de3aaa45a70b46/f6386/fb-blue.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
        <source srcset="https://www.blog.karenying.com/static/a8f769be03dbd5ce10de3aaa45a70b46/8ac56/fb-blue.webp 240w,
https://www.blog.karenying.com/static/a8f769be03dbd5ce10de3aaa45a70b46/d3be9/fb-blue.webp 480w,
https://www.blog.karenying.com/static/a8f769be03dbd5ce10de3aaa45a70b46/7dbce/fb-blue.webp 686w" sizes="(max-width: 686px) 100vw, 686px" type="image/webp">
        <source srcset="https://www.blog.karenying.com/static/a8f769be03dbd5ce10de3aaa45a70b46/8ff5a/fb-blue.png 240w,
https://www.blog.karenying.com/static/a8f769be03dbd5ce10de3aaa45a70b46/e85cb/fb-blue.png 480w,
https://www.blog.karenying.com/static/a8f769be03dbd5ce10de3aaa45a70b46/f6386/fb-blue.png 686w" sizes="(max-width: 686px) 100vw, 686px" type="image/png">
        <img src="https://www.blog.karenying.com/static/a8f769be03dbd5ce10de3aaa45a70b46/f6386/fb-blue.png" alt="Facebook Messenger blue" title="Facebook Messenger blue" loading="lazy">
      </picture>
  </a>
    </span></span><br><em>Default Messenger blue</em></p>
<p><span><span>
      <a href="https://www.blog.karenying.com/static/d49276e5ec176aedd3bc5ed0ceb03fe2/6c745/fb-palette.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
        <source srcset="https://www.blog.karenying.com/static/d49276e5ec176aedd3bc5ed0ceb03fe2/8ac56/fb-palette.webp 240w,
https://www.blog.karenying.com/static/d49276e5ec176aedd3bc5ed0ceb03fe2/d3be9/fb-palette.webp 480w,
https://www.blog.karenying.com/static/d49276e5ec176aedd3bc5ed0ceb03fe2/f0cd5/fb-palette.webp 893w" sizes="(max-width: 893px) 100vw, 893px" type="image/webp">
        <source srcset="https://www.blog.karenying.com/static/d49276e5ec176aedd3bc5ed0ceb03fe2/8ff5a/fb-palette.png 240w,
https://www.blog.karenying.com/static/d49276e5ec176aedd3bc5ed0ceb03fe2/e85cb/fb-palette.png 480w,
https://www.blog.karenying.com/static/d49276e5ec176aedd3bc5ed0ceb03fe2/6c745/fb-palette.png 893w" sizes="(max-width: 893px) 100vw, 893px" type="image/png">
        <img src="https://www.blog.karenying.com/static/d49276e5ec176aedd3bc5ed0ceb03fe2/6c745/fb-palette.png" alt="Facebook Messenger palette" title="Facebook Messenger palette" loading="lazy">
      </picture>
  </a>
    </span></span><br> <em>Messenger solid palette. They change this up pretty often. Gradient options are also available 😍</em></p>
<p>However, regardless of what color you pick, the text color is infuriatingly white.</p>
<p><span><span>
      <a href="https://www.blog.karenying.com/static/f7dbc143cde958ed091a7992aa0e58da/91e7e/fb-yellow.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
        <source srcset="https://www.blog.karenying.com/static/f7dbc143cde958ed091a7992aa0e58da/8ac56/fb-yellow.webp 240w,
https://www.blog.karenying.com/static/f7dbc143cde958ed091a7992aa0e58da/d3be9/fb-yellow.webp 480w,
https://www.blog.karenying.com/static/f7dbc143cde958ed091a7992aa0e58da/f686e/fb-yellow.webp 692w" sizes="(max-width: 692px) 100vw, 692px" type="image/webp">
        <source srcset="https://www.blog.karenying.com/static/f7dbc143cde958ed091a7992aa0e58da/8ff5a/fb-yellow.png 240w,
https://www.blog.karenying.com/static/f7dbc143cde958ed091a7992aa0e58da/e85cb/fb-yellow.png 480w,
https://www.blog.karenying.com/static/f7dbc143cde958ed091a7992aa0e58da/91e7e/fb-yellow.png 692w" sizes="(max-width: 692px) 100vw, 692px" type="image/png">
        <img src="https://www.blog.karenying.com/static/f7dbc143cde958ed091a7992aa0e58da/91e7e/fb-yellow.png" alt="Facebook Messenger yellow" title="Facebook Messenger yellow" loading="lazy">
      </picture>
  </a>
    </span></span><br> <em>Yellow theme in a group chat</em></p>
<p>Remember this example from before? <span>Contrast ratio of <b>1.61</b> (terrible)</span> 🙁</p>
<h3 id="gradientpng"><a href="#gradientpng" aria-label="gradientpng permalink"></a>gradient.png</h3>
<p>I made a gradient generating <a href="http://gradient-png.netlify.app/" target="_blank">app</a> a while back. The app lets you choose colors for a gradient, displaying the hex codes on the current colors.</p>
<p><span>
      <a href="https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/d43b4/boost-visual-accessibility-by-changing-your-text-color.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
        <source srcset="https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/8ac56/boost-visual-accessibility-by-changing-your-text-color.webp 240w,
https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/d3be9/boost-visual-accessibility-by-changing-your-text-color.webp 480w,
https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/e46b2/boost-visual-accessibility-by-changing-your-text-color.webp 960w,
https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/ccc09/boost-visual-accessibility-by-changing-your-text-color.webp 1202w" sizes="(max-width: 960px) 100vw, 960px" type="image/webp">
        <source srcset="https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/8ff5a/boost-visual-accessibility-by-changing-your-text-color.png 240w,
https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/e85cb/boost-visual-accessibility-by-changing-your-text-color.png 480w,
https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/d9199/boost-visual-accessibility-by-changing-your-text-color.png 960w,
https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/d43b4/boost-visual-accessibility-by-changing-your-text-color.png 1202w" sizes="(max-width: 960px) 100vw, 960px" type="image/png">
        <img src="https://www.blog.karenying.com/static/56417ad67026c9015db7e7d117a45a6f/d9199/boost-visual-accessibility-by-changing-your-text-color.png" alt="gradient.png" title="gradient.png" loading="lazy">
      </picture>
  </a>
    </span><em>Imperfect implementation for <a href="http://gradient-png.netlify.app/" target="_blank">gradient.png</a></em></p>
<p>I calculated luminance for the colors and choose dark text if the luminance was below 50%. However, I didn’t apply the contrast ratio formula, so this is an imperfect implementation. Still better than nothing though?</p>
<h3 id="charts-and-diagrams"><a href="#charts-and-diagrams" aria-label="charts and diagrams permalink"></a>Charts and Diagrams</h3>
<p><img src="https://d2mvzyuse3lwjc.cloudfront.net/doc/en/UserGuide/images/Bar_Of_Pie_Chart/Bar_Of_Pie_Chart.png?v=83483" alt="Pie chart"><em>Rando pie chart I found <a href="https://www.originlab.com/doc/Origin-Help/Bar-Of-Pie" target="_blank">online</a></em></p>
<p>This concept might be most useful when you’re displaying data with different colors, and you want to write text over each section.</p>
<h2 id="implementation"><a href="#implementation" aria-label="implementation permalink"></a>Implementation</h2>
<h3 id="-1-prereqs"><a href="#-1-prereqs" aria-label=" 1 prereqs permalink"></a>-1. Prereqs</h3>
<p>This tutorial assumes you have some knowledge of JavaScript and React. All good? Let’s get started 👍🏼</p>
<h3 id="0-getting-started"><a href="#0-getting-started" aria-label="0 getting started permalink"></a>0. Getting Started</h3>
<p>We’ll use <a href="https://create-react-app.dev/docs/getting-started/" target="_blank">Create React App</a> to create, bundle, and run the project:</p>
<div data-language="bash"><pre><code>$ npx create-react-app dyn-change-text-color
$ <span>cd</span> dyn-change-text-color
$ <span>npm</span> start</code></pre></div>
<h3 id="1-colorbox-component"><a href="#1-colorbox-component" aria-label="1 colorbox component permalink"></a>1. ColorBox Component</h3>
<p>We’re going to create a <code>ColorBox</code> component which takes a hex code string as a prop. For consistency sake, we will always store hex codes variables without the pound sign, only adding it when necessary for CSS/HTML. The hex code prop will determine the background color of the component.</p>
<p>Create a new file called <code>ColorBox.js</code>:</p>

<div data-language="jsx"><pre><code><span>import</span> <span>'./ColorBox.css'</span><span>;</span>

<span>const</span> <span>ColorBox</span> <span>=</span> <span>(</span><span><span>{</span> backgroundHex <span>}</span></span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>return</span> <span>(</span>
    <span><span><span>&lt;</span>div</span>
      <span>className</span><span><span>=</span><span>'</span>colorbox-container<span>'</span></span>
      <span>style</span><span><span>=</span><span>{</span><span>{</span> backgroundColor<span>:</span> <span><span>`</span><span>#</span><span><span>${</span>backgroundHex<span>}</span></span><span>`</span></span> <span>}</span><span>}</span></span>
    <span>&gt;</span></span><span>
      </span><span>{</span><span><span>`</span><span>#</span><span><span>${</span>backgroundHex<span>}</span></span><span>`</span></span><span>}</span><span>
    </span><span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
  <span>)</span><span>;</span>
<span>}</span><span>;</span>

<span>export</span> <span>default</span> ColorBox<span>;</span></code></pre></div>
<p>Here we use inline styling to dynamically change the background color based on the component’s prop. We also render the hex code as text in the component.</p>
<p>Let’s import <code>ColorBox</code> to <code>App.js</code> and pass in black as the <code>backgroundHex</code> prop:</p>

<div data-language="jsx"><pre><code><span>function</span> <span>App</span><span>(</span><span>)</span> <span>{</span>
  <span>return</span> <span>(</span>
    <span><span><span>&lt;</span>div</span> <span>className</span><span><span>=</span><span>'</span>App<span>'</span></span><span>&gt;</span></span><span>
      </span><span><span><span>&lt;</span><span>ColorBox</span></span> <span>backgroundHex</span><span><span>=</span><span>'</span>2a2b2e<span>'</span></span> <span>/&gt;</span></span><span>
    </span><span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
  <span>)</span><span>;</span>
<span>}</span></code></pre></div>
<p>Add some styling:</p>

<div data-language="css"><pre><code><span>.colorbox-container</span> <span>{</span>
  <span>display</span><span>:</span> flex<span>;</span>
  <span>align-items</span><span>:</span> center<span>;</span>
  <span>justify-content</span><span>:</span> center<span>;</span>
  <span>height</span><span>:</span> 70px<span>;</span>
  <span>width</span><span>:</span> 200px<span>;</span>
  <span>border-radius</span><span>:</span> 5px<span>;</span>
  <span>padding</span><span>:</span> 20px<span>;</span>
  <span>text-align</span><span>:</span> center<span>;</span>
<span>}</span></code></pre></div>
<p>If we run the app, we should see:</p>
<p><span><span>
      <a href="https://www.blog.karenying.com/static/539ba39070de853652df053dcd90deef/7527b/colorbox-1.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
        <source srcset="https://www.blog.karenying.com/static/539ba39070de853652df053dcd90deef/8ac56/colorbox-1.webp 240w,
https://www.blog.karenying.com/static/539ba39070de853652df053dcd90deef/d3be9/colorbox-1.webp 480w,
https://www.blog.karenying.com/static/539ba39070de853652df053dcd90deef/20eb0/colorbox-1.webp 754w" sizes="(max-width: 754px) 100vw, 754px" type="image/webp">
        <source srcset="https://www.blog.karenying.com/static/539ba39070de853652df053dcd90deef/8ff5a/colorbox-1.png 240w,
https://www.blog.karenying.com/static/539ba39070de853652df053dcd90deef/e85cb/colorbox-1.png 480w,
https://www.blog.karenying.com/static/539ba39070de853652df053dcd90deef/7527b/colorbox-1.png 754w" sizes="(max-width: 754px) 100vw, 754px" type="image/png">
        <img src="https://www.blog.karenying.com/static/539ba39070de853652df053dcd90deef/7527b/colorbox-1.png" alt="ColorBox" title="ColorBox" loading="lazy">
      </picture>
  </a>
    </span></span><br><em><code>ColorBox</code> component with black as <code>backgroundHex</code> prop. Terrible contrast ratio with default black text</em></p>
<h3 id="2-so-oop-much-modularization"><a href="#2-so-oop-much-modularization" aria-label="2 so oop much modularization permalink"></a>2. So OOP, Much Modularization</h3>
<h4 id="21-templates"><a href="#21-templates" aria-label="21 templates permalink"></a>2.1 Templates</h4>
<p>To better organize our code, we’re gonna create a <code>Color</code> class as well as some helper methods. Let’s set these up:</p>

<div data-language="js"><pre><code><span>import</span> <span>{</span> textColors<span>,</span> contrastRatioPair<span>,</span> getLuminance <span>}</span> <span>from</span> <span>'./helper'</span><span>;</span>

<span>export</span> <span>default</span> <span>class</span> <span>Color</span> <span>{</span>
  
  <span>constructor</span><span>(</span><span>hex</span><span>)</span> <span>{</span>
    <span>this</span><span>.</span>hex <span>=</span> hex<span>;</span>
  <span>}</span>

  
  <span>get</span> <span>luminance</span><span>(</span><span>)</span> <span>{</span><span>}</span>

  
  <span>contrastRatioWith</span><span>(</span><span>hex2</span><span>)</span> <span>{</span><span>}</span>

  
  <span>get</span> <span>textColor</span><span>(</span><span>)</span> <span>{</span><span>}</span>
<span>}</span></code></pre></div>

<div data-language="js"><pre><code><span>export</span> <span>const</span> textColors <span>=</span> <span>{</span>
  <span>BLACK</span><span>:</span> <span>'000000'</span><span>,</span>
  <span>WHITE</span><span>:</span> <span>'ffffff'</span><span>,</span>
<span>}</span><span>;</span>


<span>export</span> <span>function</span> <span>contrastRatioPair</span><span>(</span><span>hex1<span>,</span> hex2</span><span>)</span> <span>{</span><span>}</span>


<span>function</span> <span>hexToRGB</span><span>(</span><span>hex</span><span>)</span> <span>{</span><span>}</span>


<span>export</span> <span>function</span> <span>getLuminance</span><span>(</span><span>hex</span><span>)</span> <span>{</span><span>}</span></code></pre></div>
<p>Great, now we can start filling out these functions.</p>
<h4 id="22-luminance"><a href="#22-luminance" aria-label="22 luminance permalink"></a>2.2. Luminance</h4>
<p>As mentioned before, the luminance calculation is a bit messy. We need to first convert our 6 bit hex string to RGB values. To do so, we splice the string, and parse the substrings from hex to decimal:</p>

<div data-language="js"><pre><code>
<span>function</span> <span>hexToDecimal</span><span>(</span><span>hex_string</span><span>)</span> <span>{</span>
  <span>return</span> <span>parseInt</span><span>(</span>hex_string<span>,</span> <span>16</span><span>)</span><span>;</span>
<span>}</span>


<span>function</span> <span>hexToRGB</span><span>(</span><span>hex</span><span>)</span> <span>{</span>
  <span>const</span> r <span>=</span> <span>hexToDecimal</span><span>(</span>hex<span>.</span><span>substring</span><span>(</span><span>0</span><span>,</span> <span>2</span><span>)</span><span>)</span><span>;</span>
  <span>const</span> g <span>=</span> <span>hexToDecimal</span><span>(</span>hex<span>.</span><span>substring</span><span>(</span><span>2</span><span>,</span> <span>4</span><span>)</span><span>)</span><span>;</span>
  <span>const</span> b <span>=</span> <span>hexToDecimal</span><span>(</span>hex<span>.</span><span>substring</span><span>(</span><span>4</span><span>,</span> <span>6</span><span>)</span><span>)</span><span>;</span>

  <span>return</span> <span>{</span> r<span>,</span> g<span>,</span> b <span>}</span><span>;</span>
<span>}</span></code></pre></div>
<p>We can then call <code>hexToRGB</code> in our luminance calculation. If you want to read more about exactly how to calculate, you can check out this <a href="https://planetcalc.com/7779/" target="_blank">calculator</a> or <a href="https://en.wikipedia.org/wiki/Relative_luminance" target="_blank">Wikipedia</a>. But if you just wanna trust me on this one, here’s the gross code:</p>

<div data-language="js"><pre><code>
<span>export</span> <span>function</span> <span>getLuminance</span><span>(</span><span>hex</span><span>)</span> <span>{</span>
  <span>const</span> rgb <span>=</span> <span>hexToRGB</span><span>(</span>hex<span>)</span><span>;</span>

  <span>for</span> <span>(</span><span>const</span> key <span>in</span> rgb<span>)</span> <span>{</span>
    <span>let</span> c <span>=</span> rgb<span>[</span>key<span>]</span><span>;</span>
    c <span>/=</span> <span>255</span><span>;</span>

    c <span>=</span> c <span>&gt;</span> <span>0.03928</span> <span>?</span> Math<span>.</span><span>pow</span><span>(</span><span>(</span>c <span>+</span> <span>0.055</span><span>)</span> <span>/</span> <span>1.055</span><span>,</span> <span>2.4</span><span>)</span> <span>:</span> <span>(</span>c <span>/=</span> <span>12.92</span><span>)</span><span>;</span>

    rgb<span>[</span>key<span>]</span> <span>=</span> c<span>;</span>
  <span>}</span>

  <span>return</span> <span>0.2126</span> <span>*</span> rgb<span>.</span>r <span>+</span> <span>0.7152</span> <span>*</span> rgb<span>.</span>g <span>+</span> <span>0.0722</span> <span>*</span> rgb<span>.</span>b<span>;</span>
<span>}</span></code></pre></div>
<h4 id="23-contrast-ratio"><a href="#23-contrast-ratio" aria-label="23 contrast ratio permalink"></a>2.3. Contrast Ratio</h4>
<p>With our luminance function done, we can call it to calculate the contrast ratio between two colors with our division formula from before:</p>

<div data-language="js"><pre><code>
<span>export</span> <span>function</span> <span>contrastRatioPair</span><span>(</span><span>hex1<span>,</span> hex2</span><span>)</span> <span>{</span>
  <span>const</span> lum1 <span>=</span> <span>getLuminance</span><span>(</span>hex1<span>)</span><span>;</span>
  <span>const</span> lum2 <span>=</span> <span>getLuminance</span><span>(</span>hex2<span>)</span><span>;</span>

  <span>return</span> <span>(</span>Math<span>.</span><span>max</span><span>(</span>lum1<span>,</span> lum2<span>)</span> <span>+</span> <span>0.05</span><span>)</span> <span>/</span> <span>(</span>Math<span>.</span><span>min</span><span>(</span>lum1<span>,</span> lum2<span>)</span> <span>+</span> <span>0.05</span><span>)</span><span>;</span>
<span>}</span></code></pre></div>
<h4 id="24-filling-out-colorjs"><a href="#24-filling-out-colorjs" aria-label="24 filling out colorjs permalink"></a>2.4. Filling out <code>Color.js</code></h4>
<p>We can now fill out the methods of the <code>Color</code> class with our helper methods:</p>

<div data-language="js"><pre><code>  
  <span>get</span> <span>luminance</span><span>(</span><span>)</span> <span>{</span>
    <span>return</span> <span>getLuminance</span><span>(</span><span>this</span><span>.</span>hex<span>)</span><span>;</span>
  <span>}</span>

  
  <span>contrastRatioWith</span><span>(</span><span>hex2</span><span>)</span> <span>{</span>
    <span>return</span> <span>contrastRatioPair</span><span>(</span><span>this</span><span>.</span>hex<span>,</span> hex2<span>)</span><span>;</span>
  <span>}</span>

  
  <span>get</span> <span>textColor</span><span>(</span><span>)</span> <span>{</span>
    <span>const</span> <span>{</span> <span>BLACK</span><span>,</span> <span>WHITE</span> <span>}</span> <span>=</span> textColors<span>;</span>

    <span>return</span> <span>this</span><span>.</span><span>contrastRatioWith</span><span>(</span><span>BLACK</span><span>)</span> <span>&gt;</span> <span>this</span><span>.</span><span>contrastRatioWith</span><span>(</span><span>WHITE</span><span>)</span>
      <span>?</span> <span>BLACK</span>
      <span>:</span> <span>WHITE</span><span>;</span>
  <span>}</span></code></pre></div>
<p><code>textColor</code> computes the contrast ratio of the current color with black and white, and returns the color (black or white) that yields the highest contrast ratio.</p>
<h3 id="3-pulling-it-all-together"><a href="#3-pulling-it-all-together" aria-label="3 pulling it all together permalink"></a>3. Pulling it all Together</h3>
<p>Now we can turn back to our <code>ColorBox</code> component.</p>
<p>All we need to do is create a new <code>Color</code> object with the <code>backgroundHex</code> prop and call its appropriate properties/methods:</p>

<div data-language="jsx"><pre><code><span>const</span> backgroundColor <span>=</span> <span>new</span> <span>Color</span><span>(</span>backgroundHex<span>)</span><span>;</span>
<span>const</span> <span>{</span> textColor <span>}</span> <span>=</span> backgroundColor<span>;</span></code></pre></div>
<p>Then we can set the <code>color</code> CSS property of the div as <code>textColor</code>. I also added a couple of lines to display the current contrast ratio:</p>

<div data-language="jsx"><pre><code>  <span>return</span> <span>(</span>
    <span><span><span>&lt;</span>div</span>
      <span>className</span><span><span>=</span><span>'</span>colorbox-container<span>'</span></span>
      <span>style</span><span><span>=</span><span>{</span><span>{</span> backgroundColor<span>:</span> <span><span>`</span><span>#</span><span><span>${</span>backgroundHex<span>}</span></span><span>`</span></span><span>,</span> color<span>:</span> <span><span>`</span><span>#</span><span><span>${</span>textColor<span>}</span></span><span>`</span></span> <span>}</span><span>}</span></span>
    <span>&gt;</span></span><span>
      </span><span>{</span><span><span>`</span><span>#</span><span><span>${</span>backgroundHex<span>}</span></span><span>`</span></span><span>}</span><span>
      </span><span><span><span>&lt;</span>br</span> <span>/&gt;</span></span><span>
      </span><span>{</span><span><span>`</span><span>Contrast ratio: </span><span><span>${</span>backgroundColor
        <span>.</span><span>contrastRatioWith</span><span>(</span>textColor<span>)</span>
        <span>.</span><span>toFixed</span><span>(</span><span>2</span><span>)</span><span>}</span></span><span>`</span></span><span>}</span><span>
    </span><span><span><span>&lt;/</span>div</span><span>&gt;</span></span></code></pre></div>
<p>Now if you check out the app, it should look like this:</p>
<p><span><span>
      <a href="https://www.blog.karenying.com/static/60068a87c2a2002a02f7ec090c19bf53/8ae3e/colorbox-2.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
        <source srcset="https://www.blog.karenying.com/static/60068a87c2a2002a02f7ec090c19bf53/8ac56/colorbox-2.webp 240w,
https://www.blog.karenying.com/static/60068a87c2a2002a02f7ec090c19bf53/d3be9/colorbox-2.webp 480w,
https://www.blog.karenying.com/static/60068a87c2a2002a02f7ec090c19bf53/b5834/colorbox-2.webp 756w" sizes="(max-width: 756px) 100vw, 756px" type="image/webp">
        <source srcset="https://www.blog.karenying.com/static/60068a87c2a2002a02f7ec090c19bf53/8ff5a/colorbox-2.png 240w,
https://www.blog.karenying.com/static/60068a87c2a2002a02f7ec090c19bf53/e85cb/colorbox-2.png 480w,
https://www.blog.karenying.com/static/60068a87c2a2002a02f7ec090c19bf53/8ae3e/colorbox-2.png 756w" sizes="(max-width: 756px) 100vw, 756px" type="image/png">
        <img src="https://www.blog.karenying.com/static/60068a87c2a2002a02f7ec090c19bf53/8ae3e/colorbox-2.png" alt="ColorBox" title="ColorBox" loading="lazy">
      </picture>
  </a>
    </span></span><br><em>The …</em></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.blog.karenying.com/posts/boost-visual-accessibility-by-auto-flipping-text-color">https://www.blog.karenying.com/posts/boost-visual-accessibility-by-auto-flipping-text-color</a></em></p>]]>
            </description>
            <link>https://www.blog.karenying.com/posts/boost-visual-accessibility-by-auto-flipping-text-color</link>
            <guid isPermaLink="false">hacker-news-small-sites-25038248</guid>
            <pubDate>Mon, 09 Nov 2020 18:55:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[API with NestJS #17. Offset and keyset pagination with PostgreSQL and TypeORM]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25038192">thread link</a>) | @mwanago
<br/>
November 9, 2020 | https://wanago.io/2020/11/09/api-nestjs-offset-keyset-pagination-postgresql-typeorm/ | <a href="https://web.archive.org/web/*/https://wanago.io/2020/11/09/api-nestjs-offset-keyset-pagination-postgresql-typeorm/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><article id="post-3395"><div><div> <p><time datetime="2020-11-09"> November 9, 2020 </time></p><ul><li>1. <a href="https://wanago.io/2020/05/11/nestjs-api-controllers-routing-module/" title="API with NestJS #1. Controllers, routing and the module structure">API with NestJS #1. Controllers, routing and the module structure</a></li><li>2. <a href="https://wanago.io/2020/05/18/api-nestjs-postgresql-typeorm/" title="API with NestJS #2. Setting up a PostgreSQL database with TypeORM">API with NestJS #2. Setting up a PostgreSQL database with TypeORM</a></li><li>3. <a href="https://wanago.io/2020/05/25/api-nestjs-authenticating-users-bcrypt-passport-jwt-cookies/" title="API with NestJS #3. Authenticating users with bcrypt, Passport, JWT, and cookies">API with NestJS #3. Authenticating users with bcrypt, Passport, JWT, and cookies</a></li><li>4. <a href="https://wanago.io/2020/06/01/api-nestjs-error-handling-validation/" title="API with NestJS #4. Error handling and data validation">API with NestJS #4. Error handling and data validation</a></li><li>5. <a href="https://wanago.io/2020/06/08/api-nestjs-serializing-response-interceptors/" title="API with NestJS #5. Serializing the response with interceptors">API with NestJS #5. Serializing the response with interceptors</a></li><li>6. <a href="https://wanago.io/2020/06/15/api-with-nestjs-6-looking-into-dependency-injection-and-modules/" title="API with NestJS #6. Looking into dependency injection and modules">API with NestJS #6. Looking into dependency injection and modules</a></li><li>7. <a href="https://wanago.io/2020/06/22/api-nestjs-relationships-postgres-typeorm/" title="API with NestJS #7. Creating relationships with Postgres and TypeORM">API with NestJS #7. Creating relationships with Postgres and TypeORM</a></li><li>8. <a href="https://wanago.io/2020/07/06/api-nestjs-unit-tests/" title="API with NestJS #8. Writing unit tests">API with NestJS #8. Writing unit tests</a></li><li>9. <a href="https://wanago.io/2020/07/13/api-nestjs-testing-services-controllers-integration-tests/" title="API with NestJS #9. Testing services and controllers with integration tests">API with NestJS #9. Testing services and controllers with integration tests</a></li><li>10. <a href="https://wanago.io/2020/08/03/api-nestjs-uploading-public-files-to-amazon-s3/" title="API with NestJS #10. Uploading public files to Amazon S3">API with NestJS #10. Uploading public files to Amazon S3</a></li><li>11. <a href="https://wanago.io/2020/08/10/api-nestjs-private-files-amazon-s3/" title="API with NestJS #11. Managing private files with Amazon S3">API with NestJS #11. Managing private files with Amazon S3</a></li><li>12. <a href="https://wanago.io/2020/09/07/api-nestjs-elasticsearch/" title="API with NestJS #12. Introduction to Elasticsearch">API with NestJS #12. Introduction to Elasticsearch</a></li><li>13. <a href="https://wanago.io/2020/09/21/api-nestjs-refresh-tokens-jwt/" title="API with NestJS #13. Implementing refresh tokens using JWT">API with NestJS #13. Implementing refresh tokens using JWT</a></li><li>14. <a href="https://wanago.io/2020/10/19/nestjs-performance-postgres-database-indexes/" title="API with NestJS #14. Improving performance of our Postgres database with indexes">API with NestJS #14. Improving performance of our Postgres database with indexes</a></li><li>15. <a href="https://wanago.io/2020/10/26/api-nestjs-transactions-postgresql-typeorm/" title="API with NestJS #15. Defining transactions with PostgreSQL and TypeORM">API with NestJS #15. Defining transactions with PostgreSQL and TypeORM</a></li><li>16. <a href="https://wanago.io/2020/11/02/api-nestjs-array-data-type-postgresql-typeorm/" title="API with NestJS #16. Using the array data type with PostgreSQL and TypeORM">API with NestJS #16. Using the array data type with PostgreSQL and TypeORM</a></li><li>17. API with NestJS #17. Offset and keyset pagination with PostgreSQL and TypeORM</li></ul><p>As our database grows, so do the results of our queries. Returning a lot of data in our API might not be the best approach performance-wise. Dividing our content into multiple pages and solutions like infinite scrolling have been around for quite some time. In this article, we explore ways of implementing pagination and point out their pros and cons.</p><p>You can find all of the code from this series in <a href="https://github.com/mwanago/nestjs-typescript">this repository</a>.</p><h2>Offset and Limit</h2><p>Let’s start with the following, straightforward query:</p><div id="crayon-5fac86a19deeb310150628" data-settings=" no-popup minimize scroll-mouseover wrap"><div><table><tbody><tr><td data-settings="show"></td><td><div><p><span>SELECT</span><span> </span>*<span> </span><span>FROM</span><span> </span>post</p><p><span>ORDER</span><span> </span><span>BY</span><span> </span>id<span> </span><span>ASC</span></p></div></td></tr></tbody></table></div></div><p>The above returns all of the records from the <span id="crayon-5fac86a19def6300614608"><span><span>post</span></span></span> table. To be sure about the order of the results, we sort them by id.</p><p>The first step in implementing pagination would be to limit the number of results. We can do that using the <span id="crayon-5fac86a19def9816331456"><span><span>LIMIT</span></span></span> statement.</p><div id="crayon-5fac86a19defb699092640" data-settings=" no-popup minimize scroll-mouseover wrap"><div><table><tbody><tr><td data-settings="show"></td><td><div><p><span>SELECT</span><span> </span>*<span> </span><span>FROM</span><span> </span>post</p><p><span>ORDER</span><span> </span><span>BY</span><span> </span>id<span> </span><span>ASC</span></p><p><span>LIMIT</span><span> </span>10</p></div></td></tr></tbody></table></div></div><p>Now, instead of getting all of the posts, we get just the first ten of them. This results in getting elements with ids from 1 to 10.</p><p>To have fully functional pagination, we need to specify the starting point of our query. To do that, we can use the <span id="crayon-5fac86a19defd902337993"><span><span>OFFSET</span></span></span> keyword. With it, we can say how many rows we want to skip.</p><div id="crayon-5fac86a19df00474629618" data-settings=" no-popup minimize scroll-mouseover wrap"><div><table><tbody><tr><td data-settings="show"></td><td><div><p><span>SELECT</span><span> </span>*<span> </span><span>FROM</span><span> </span>post</p><p><span>ORDER</span><span> </span><span>BY</span><span> </span>id<span> </span><span>ASC</span></p><p><span>OFFSET</span><span> </span>10</p><p><span>LIMIT</span><span> </span>10</p></div></td></tr></tbody></table></div></div><p>We omit the first ten posts with the above while still getting just ten posts as a result. This gives us elements with ids from 11 to 20.</p><p>If we would like to change the way we order elements while paginating, we need to modify our <span id="crayon-5fac86a19df02434118718"><span><span>ORDER </span><span>BY</span></span></span> clause.</p><h3>Implementing offset and limit with TypeORM</h3><p>We want the users to provide the offset and the limit through query params. To implement this, let’s use the knowledge we’ve gained in previous parts of this series. This includes the usage of the <span id="crayon-5fac86a19df04907059821"><span><span>class</span><span>-</span><span>validator</span></span></span> and the <span id="crayon-5fac86a19df06853152756"><span><span>class</span><span>-</span><span>transformer</span></span></span>.</p><div id="crayon-5fac86a19df08567647316" data-settings=" no-popup minimize scroll-mouseover wrap"><div><table><tbody><tr><td data-settings="show"></td><td><div><p><span>import</span><span> </span><span>{</span><span> </span><span>IsNumber</span><span>,</span><span> </span><span>Min</span><span>,</span><span> </span><span>IsOptional</span><span> </span><span>}</span><span> </span><span>from</span><span> </span><span>'class-validator'</span><span>;</span></p><p><span>import</span><span> </span><span>{</span><span> </span><span>Type</span><span> </span><span>}</span><span> </span><span>from</span><span> </span><span>'class-transformer'</span><span>;</span></p><p><span>export</span><span> </span><span>class</span><span> </span><span>PaginationParams</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>IsOptional</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>Type</span><span>(</span><span>(</span><span>)</span><span> </span><span>=</span><span>&gt;</span><span> </span><span>Number</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>IsNumber</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>Min</span><span>(</span><span>0</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>offset</span><span>?</span><span>:</span><span> </span><span>number</span><span>;</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>IsOptional</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>Type</span><span>(</span><span>(</span><span>)</span><span> </span><span>=</span><span>&gt;</span><span> </span><span>Number</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>IsNumber</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>Min</span><span>(</span><span>1</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>limit</span><span>?</span><span>:</span><span> </span><span>number</span><span>;</span></p><p><span>}</span></p></div></td></tr></tbody></table></div></div><p>We can now use the <span id="crayon-5fac86a19df0a170055039"><span><span>@</span><span>Query</span><span>(</span><span>)</span></span></span> decorator to inject the above parameters into our controller.</p><div id="crayon-5fac86a19df0c178554853" data-settings=" no-popup minimize scroll-mouseover wrap"><div><table><tbody><tr><td data-settings="show"><div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p></div></td><td><div><p><span>import</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;</span><span>Controller</span><span>,</span></p><p><span>&nbsp;&nbsp;</span><span>Get</span><span>,</span></p><p><span>&nbsp;&nbsp;</span><span>UseInterceptors</span><span>,</span></p><p><span>&nbsp;&nbsp;</span><span>ClassSerializerInterceptor</span><span>,</span></p><p><span>&nbsp;&nbsp;</span><span>Query</span><span>,</span></p><p><span>}</span><span> </span><span>from</span><span> </span><span>'@nestjs/common'</span><span>;</span></p><p><span>import </span><span>PostsService </span><span>from</span><span> </span><span>'./posts.service'</span><span>;</span></p><p><span>import</span><span> </span><span>{</span><span> </span><span>PaginationParams</span><span> </span><span>}</span><span> </span><span>from</span><span> </span><span>'../utils/types/paginationParams'</span><span>;</span></p><p><span>@</span><span>Controller</span><span>(</span><span>'posts'</span><span>)</span></p><p><span>@</span><span>UseInterceptors</span><span>(</span><span>ClassSerializerInterceptor</span><span>)</span></p><p><span>export</span><span> </span><span>default</span><span> </span><span>class</span><span> </span><span>PostsController</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;</span><span>constructor</span><span>(</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>private</span><span> </span><span>readonly </span><span>postsService</span><span>:</span><span> </span><span>PostsService</span></p><p><span>&nbsp;&nbsp;</span><span>)</span><span> </span><span>{</span><span>}</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>Get</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>async </span><span>getPosts</span><span>(</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>@</span><span>Query</span><span>(</span><span>'search'</span><span>)</span><span> </span><span>search</span><span>:</span><span> </span><span>string</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>@</span><span>Query</span><span>(</span><span>)</span><span> </span><span>{</span><span> </span><span>offset</span><span>,</span><span> </span><span>limit</span><span> </span><span>}</span><span>:</span><span> </span><span>PaginationParams</span></p><p><span>&nbsp;&nbsp;</span><span>)</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>if</span><span> </span><span>(</span><span>search</span><span>)</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>return</span><span> </span><span>this</span><span>.</span><span>postsService</span><span>.</span><span>searchForPosts</span><span>(</span><span>search</span><span>,</span><span> </span><span>offset</span><span>,</span><span> </span><span>limit</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>}</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>return</span><span> </span><span>this</span><span>.</span><span>postsService</span><span>.</span><span>getAllPosts</span><span>(</span><span>offset</span><span>,</span><span> </span><span>limit</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp;</span><span>}</span></p><p><span>&nbsp;&nbsp;</span><span>// ...</span></p><p><span>}</span></p></div></td></tr></tbody></table></div></div><p>Implementing offset-based pagination is very easy with TypeORM. Aside from returning an array of posts, we also want to return a number of them. Thanks to that, our frontend can estimate the number of pages available.</p><p>Although we could use the <span id="crayon-5fac86a19df0e928311767"><span><span>postsRepository</span><span>.</span><span>count</span><span>(</span><span>)</span></span></span> and <span id="crayon-5fac86a19df10664363321"><span><span>postsRepository</span><span>.</span><span>find</span><span>(</span><span>)</span></span></span> methods separately, this would result in making two queries to the database. We can improve that by using <span id="crayon-5fac86a19df13426025799"><span><span>postsRepository</span><span>.</span><span>findAndCount</span></span></span>.</p><div id="crayon-5fac86a19df15217600257" data-settings=" no-popup minimize scroll-mouseover wrap"><div><table><tbody><tr><td data-settings="show"></td><td><div><p><span>async </span><span>getAllPosts</span><span>(</span><span>offset</span><span>?</span><span>:</span><span> </span><span>number</span><span>,</span><span> </span><span>limit</span><span>?</span><span>:</span><span> </span><span>number</span><span>)</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;</span><span>const</span><span> </span><span>[</span><span>items</span><span>,</span><span> </span><span>count</span><span>]</span><span> </span><span>=</span><span> </span><span>await </span><span>this</span><span>.</span><span>postsRepository</span><span>.</span><span>findAndCount</span><span>(</span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>relations</span><span>:</span><span> </span><span>[</span><span>'author'</span><span>]</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>order</span><span>:</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>id</span><span>:</span><span> </span><span>'ASC'</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>}</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>skip</span><span>:</span><span> </span><span>offset</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>take</span><span>:</span><span> </span><span>limit</span></p><p><span>&nbsp;&nbsp;</span><span>}</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp;</span><span>return</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>items</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>count</span></p><p><span>&nbsp;&nbsp;</span><span>}</span></p><p><span>}</span></p></div></td></tr></tbody></table></div></div><h3>Implementing offset and limit with Elasticsearch</h3><p>In <a href="https://wanago.io/2020/09/07/api-nestjs-elasticsearch/">one of the previous parts of this series</a>, we’ve integrated our posts with Elasticsearch. Fortunately, it is effortless to add the offset-based pagination to it. We need to pass the additional <span id="crayon-5fac86a19df17330325246"><span><span>offset</span></span></span> and <span id="crayon-5fac86a19df19346054453"><span><span>size</span></span></span> parameters.</p><div id="crayon-5fac86a19df1b340492864" data-settings=" no-popup minimize scroll-mouseover wrap"><div><table><tbody><tr><td data-settings="show"><div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p></div></td><td><div><p><span>async </span><span>search</span><span>(</span><span>text</span><span>:</span><span> </span><span>string</span><span>,</span><span> </span><span>offset</span><span>?</span><span>:</span><span> </span><span>number</span><span>,</span><span> </span><span>limit</span><span>?</span><span>:</span><span> </span><span>number</span><span>)</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;</span><span>const</span><span> </span><span>{</span><span> </span><span>body</span><span> </span><span>}</span><span> </span><span>=</span><span> </span><span>await </span><span>this</span><span>.</span><span>elasticsearchService</span><span>.</span><span>search</span><span>&lt;</span><span>PostSearchResult</span><span>&gt;</span><span>(</span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>index</span><span>:</span><span> </span><span>this</span><span>.</span><span>index</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>from</span><span>:</span><span> </span><span>offset</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>size</span><span>:</span><span> </span><span>limit</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>body</span><span>:</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>query</span><span>:</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>multi_match</span><span>:</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>query</span><span>:</span><span> </span><span>text</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>fields</span><span>:</span><span> </span><span>[</span><span>'title'</span><span>,</span><span> </span><span>'paragraphs'</span><span>]</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>}</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>}</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>sort</span><span>:</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>id</span><span>:</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>order</span><span>:</span><span> </span><span>'asc'</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>}</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>}</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>}</span></p><p><span>&nbsp;&nbsp;</span><span>}</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>const</span><span> </span><span>count</span><span> </span><span>=</span><span> </span><span>body</span><span>.</span><span>hits</span><span>.</span><span>total</span><span>.</span><span>value</span><span>;</span></p><p><span>&nbsp;&nbsp;</span><span>const</span><span> </span><span>hits</span><span> </span><span>=</span><span> </span><span>body</span><span>.</span><span>hits</span><span>.</span><span>hits</span><span>;</span></p><p><span>&nbsp;&nbsp;</span><span>const</span><span> </span><span>results</span><span> </span><span>=</span><span> </span><span>hits</span><span>.</span><span>map</span><span>(</span><span>(</span><span>item</span><span>)</span><span> </span><span>=</span><span>&gt;</span><span> </span><span>item</span><span>.</span><span>_source</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp;</span><span>return</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>count</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>results</span></p><p><span>&nbsp;&nbsp;</span><span>}</span></p><p><span>}</span></p></div></td></tr></tbody></table></div></div><h3>Disadvantages</h3><p>The solution with offset and limit seems to be the most widely used. Unfortunately, its performance might fall short of our expectations.</p><p>An essential thing to keep in mind is that the database still needs to compute the rows skipped by the <span id="crayon-5fac86a19df1d752640953"><span><span>OFFSET</span></span></span>. First, the database sorts all of the rows according to our <span id="crayon-5fac86a19df1f252281564"><span><span>ORDER </span><span>BY</span></span></span> clause. Then, Postgres drops the number of rows specified in the <span id="crayon-5fac86a19df21079711547"><span><span>OFFSET</span></span></span>. This might require quite a bit of work.</p><p>Aside from the performance, another important thing to consider is consistency. We want an element to appear in the results exactly once. Let’s imagine the following situation:</p><ol><li>one user fetches page number one with posts</li><li>meanwhile, the second user creates a new post – after sorting, it ends up on page number one</li><li>the first user fetches the second page</li></ol><p>The last element of the first page is now again seen on the second page because of the above. What’s even worse, the user missed the element that has been added to the first page.</p><h3>Advantages</h3><p>While the offset approach has its cons, it is still common. Due to its simplicity, it is straightforward to implement. Also, it is easy to change the column that we use for sorting, including the usage of multiple columns. Because of that, it is a viable solution in many cases. Especially if the offset is expected not to be big, and the result inconsistencies are acceptable.</p><h2>Keyset pagination</h2><p>While the offset-based pagination can be useful, its performance might not be the best. Sometimes we might want to avoid it.</p><p>One of the ways to do so is to implement keyset pagination. Instead of using the <span id="crayon-5fac86a19df23279688663"><span><span>OFFSET</span></span></span> clause, we use the <span id="crayon-5fac86a19df25716074903"><span><span>WHERE</span></span></span> command to select the data we haven’t fetched yet.</p><p>Let’s start with a simple query:</p><div id="crayon-5fac86a19df27797612944" data-settings=" no-popup minimize scroll-mouseover wrap"><div><table><tbody><tr><td data-settings="show"></td><td><div><p><span>SELECT</span><span> </span><span>*</span><span> </span><span>FROM </span><span>post</span></p><p><span>ORDER </span><span>BY </span><span>id </span><span>ASC</span></p><p><span>LIMIT</span><span> </span><span>10</span></p></div></td></tr></tbody></table></div></div><p>The above query gets us the first ten posts. Let’s assume that the id of the last post was&nbsp;<strong>20</strong>. With this assumption, we can run this query:</p><div id="crayon-5fac86a19df29030589953" data-settings=" no-popup minimize scroll-mouseover wrap"><div><table><tbody><tr><td data-settings="show"></td><td><div><p><span>SELECT</span><span> </span><span>*</span><span> </span><span>FROM </span><span>post</span></p><p><span>WHERE </span><span>id</span><span> </span><span>&gt;</span><span> </span><span>20</span></p><p><span>ORDER </span><span>BY </span><span>id </span><span>ASC</span></p><p><span>LIMIT</span><span> </span><span>10</span></p></div></td></tr></tbody></table></div></div><p>The above query gets us ten posts with id bigger than 20. Now, we can take the last post and rerun the query, changing the id. Doing that creates us simple and efficient pagination mechanism.</p><p>This exposes the biggest drawback of the keyset pagination, though. To get a page, we need to know the last element of the previous set of results. This makes traversing multiple pages at once impossible.</p><p>Fortunately, most of the time, the users got straight to the next page. To cover all of the cases, we can implement both the offset-based approach and the keyset pagination.</p><p>If we would like to change the column that we order our elements by, we need to change both the <span id="crayon-5fac86a19df2c524374775"><span><span>ORDER </span><span>BY</span></span></span> and <span id="crayon-5fac86a19df2e824081496"><span><span>WHERE</span></span></span> clauses.</p><h3>Implementing keyset pagination with TypeORM</h3><p>Adding keyset pagination is not difficult with TypeORM. First, let’s add another query parameter called <span id="crayon-5fac86a19df30148276937"><span><span>startId</span></span></span>&nbsp;to our <span id="crayon-5fac86a19df32753656789"><span><span>PaginationParams</span></span></span>.</p><div id="crayon-5fac86a19df34449900762" data-settings=" no-popup minimize scroll-mouseover wrap"><div><table><tbody><tr><td data-settings="show"><div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p></div></td><td><div><p><span>import</span><span> </span><span>{</span><span> </span><span>IsNumber</span><span>,</span><span> </span><span>Min</span><span>,</span><span> </span><span>IsOptional</span><span> </span><span>}</span><span> </span><span>from</span><span> </span><span>'class-validator'</span><span>;</span></p><p><span>import</span><span> </span><span>{</span><span> </span><span>Type</span><span> </span><span>}</span><span> </span><span>from</span><span> </span><span>'class-transformer'</span><span>;</span></p><p><span>export</span><span> </span><span>class</span><span> </span><span>PaginationParams</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>IsOptional</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>Type</span><span>(</span><span>(</span><span>)</span><span> </span><span>=</span><span>&gt;</span><span> </span><span>Number</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>IsNumber</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>Min</span><span>(</span><span>1</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>startId</span><span>?</span><span>:</span><span> </span><span>number</span><span>;</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>IsOptional</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>Type</span><span>(</span><span>(</span><span>)</span><span> </span><span>=</span><span>&gt;</span><span> </span><span>Number</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>IsNumber</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>Min</span><span>(</span><span>0</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>offset</span><span>?</span><span>:</span><span> </span><span>number</span><span>;</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>IsOptional</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>Type</span><span>(</span><span>(</span><span>)</span><span> </span><span>=</span><span>&gt;</span><span> </span><span>Number</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>IsNumber</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>@</span><span>Min</span><span>(</span><span>1</span><span>)</span></p><p><span>&nbsp;&nbsp;</span><span>limit</span><span>?</span><span>:</span><span> </span><span>number</span><span>;</span></p><p><span>}</span></p></div></td></tr></tbody></table></div></div><p>Along the way, we will face a small issue with the count of our elements. The <span id="crayon-5fac86a19df36417347837"><span><span>postsRepository</span><span>.</span><span>findAndCount</span></span></span> with a <span id="crayon-5fac86a19df38978616482"><span><span>WHERE</span></span></span> clause will return only the number of matching posts. We need to count them separately.</p><div id="crayon-5fac86a19df39888348551" data-settings=" no-popup minimize scroll-mouseover wrap"><div><table><tbody><tr><td data-settings="show"><div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p></div></td><td><div><p><span>async </span><span>getAllPosts</span><span>(</span><span>offset</span><span>?</span><span>:</span><span> </span><span>number</span><span>,</span><span> </span><span>limit</span><span>?</span><span>:</span><span> </span><span>number</span><span>,</span><span> </span><span>startId</span><span>?</span><span>:</span><span> </span><span>number</span><span>)</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;</span><span>const</span><span> where:</span><span> </span><span>FindManyOptions</span><span>&lt;</span><span>Post</span><span>&gt;</span><span>[</span><span>'where'</span><span>]</span><span> </span><span>=</span><span> </span><span>{</span><span>}</span><span>;</span></p><p><span>&nbsp;&nbsp;</span><span>let </span><span>separateCount</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span></p><p><span>&nbsp;&nbsp;</span><span>if</span><span> </span><span>(</span><span>startId</span><span>)</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>where</span><span>.</span><span>id</span><span> </span><span>=</span><span> </span><span>MoreThan</span><span>(</span><span>startId</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>separateCount</span><span> </span><span>=</span><span> </span><span>await </span><span>this</span><span>.</span><span>postsRepository</span><span>.</span><span>count</span><span>(</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp;</span><span>}</span></p><p><span>&nbsp;&nbsp;</span><span>const</span><span> </span><span>[</span><span>items</span><span>,</span><span> </span><span>count</span><span>]</span><span> </span><span>=</span><span> </span><span>await </span><span>this</span><span>.</span><span>postsRepository</span><span>.</span><span>findAndCount</span><span>(</span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>where</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>relations</span><span>:</span><span> </span><span>[</span><span>'author'</span><span>]</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>order</span><span>:</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>id</span><span>:</span><span> </span><span>'ASC'</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>}</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>skip</span><span>:</span><span> </span><span>offset</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>take</span><span>:</span><span> </span><span>limit</span></p><p><span>&nbsp;&nbsp;</span><span>}</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp;</span><span>return</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>items</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>count</span><span>:</span><span> </span><span>startId</span><span> </span><span>?</span><span> </span><span>separateCount</span><span> </span><span>:</span><span> </span><span>count</span></p><p><span>&nbsp;&nbsp;</span><span>}</span></p><p><span>}</span></p></div></td></tr></tbody></table></div></div><h3>Implementing keyset pagination with Elasticsearch</h3><p>We can also achieve the above result with Elasticsearch by adding the id of a post to our query.</p><p>In this very simple example, we separately count the matching posts. If you feel like using other pagination approaches due to performance reasons, Elasticsearch has <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/paginate-search-results.html">other built-in methods of pagination</a>.</p><div id="crayon-5fac86a19df3c451416213" data-settings=" no-popup minimize scroll-mouseover wrap"><div><table><tbody><tr><td data-settings="show"></td><td><div><p><span>async </span><span>count</span><span>(</span><span>query</span><span>:</span><span> </span><span>string</span><span>,</span><span> </span><span>fields</span><span>:</span><span> </span><span>string</span><span>[</span><span>]</span><span>)</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;</span><span>const</span><span> </span><span>{</span><span> </span><span>body</span><span> </span><span>}</span><span> </span><span>=</span><span> </span><span>await </span><span>this</span><span>.</span><span>elasticsearchService</span><span>.</span><span>count</span><span>&lt;</span><span>PostCountResult</span><span>&gt;</span><span>(</span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>index</span><span>:</span><span> </span><span>this</span><span>.</span><span>index</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>body</span><span>:</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>query</span><span>:</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>multi_match</span><span>:</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>query</span><span>,</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>fields</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>}</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;…</span></p></div></td></tr></tbody></table></div></div></div></div></article></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://wanago.io/2020/11/09/api-nestjs-offset-keyset-pagination-postgresql-typeorm/">https://wanago.io/2020/11/09/api-nestjs-offset-keyset-pagination-postgresql-typeorm/</a></em></p>]]>
            </description>
            <link>https://wanago.io/2020/11/09/api-nestjs-offset-keyset-pagination-postgresql-typeorm/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25038192</guid>
            <pubDate>Mon, 09 Nov 2020 18:51:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A hidden gem in sound symmetry]]>
            </title>
            <description>
<![CDATA[
Score 176 | Comments 43 (<a href="https://news.ycombinator.com/item?id=25037784">thread link</a>) | @gbh444g
<br/>
November 9, 2020 | https://soundshader.github.io/hn/acf/index.html | <a href="https://web.archive.org/web/*/https://soundshader.github.io/hn/acf/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      
      
      

      

<blockquote>
  <p><em><a href="https://pages.mtu.edu/~suits/autocorrelation.html">Autocorrelation</a> is used to compare a signal with a time-delayed version of itself. If a signal is periodic, then the signal will be perfectly correlated with a version of itself if the time-delay is an integer number of periods. That fact, along with related experiments, has implicated autocorrelation as a potentially important part of signal processing in human hearing.</em></p>
</blockquote>

<p>ACF is a simple method to visualize music that produces surprisingly good results. Perhaps the most unexpected property of ACF is that it accurately transfers the subjective “harmony level” from music to images. It’s almost an unreasonable property, if you think about it. Images below are ACF height maps in polar coordinates.</p>

<table>
  <thead>
    <tr>
      <th>Female vocal</th>
      <th>David Parsons</th>
      <th>Piano</th>
      <th>Bird</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="https://soundshader.github.io/pics/song-2.jpg" alt=""></td>
      <td><img src="https://soundshader.github.io/pics/bowl-3.jpg" alt=""></td>
      <td><img src="https://soundshader.github.io/pics/piano-p.jpg" alt=""></td>
      <td><img src="https://soundshader.github.io/pics/bird-2.jpg" alt=""></td>
    </tr>
  </tbody>
</table>

<p>More examples: <a href="https://soundshader.github.io/gallery/">soundshader.github.io/gallery</a> (beware of large images).</p>

<p>Live demo: <a href="https://soundshader.github.io/">soundshader.github.io</a></p>



<p>Contrary to what you might think, our ears don’t seem to rely on an FFT-like process to extract isolated frequencies. Instead, our ears detect periodic parts in the signal, although in most cases those periodic parts closely match the FFT frequencies. There is a simple <a href="https://auditoryneuroscience.com/pitch/missing-fundamental-stimuli">experiment</a> that proves this point:</p>

<p><img src="https://auditoryneuroscience.com/sites/default/files/missingFundamental2.png" alt=""></p>

<p>As can be clearly seen on the FFT image, the A signal is a pure sinusoidal tone, while B is a mix of tones. Despite each tone in B is higher than A, our ears perceive B as a lower tone. If we plot both waveforms, we’ll see that A has about 9 peaks in a 20 ms window, while B has only 5. The definition of “peak” is moot, but it doesn’t stop our ears from counting them and using the “number of peaks per second” as a proxy to the tone height.</p>

<p>ACF detects those peaks. ACF sees that there are 5 equally spaced time shifts where <code>B[t] * B[t + shift]</code> reaches the maximum, so on the ACF output we’ll see those 5 peaks.</p>

<blockquote>
  <p>Given that I’ve shamelessly stolen the experiment’s illustration above, I feel obligated to recommend the book where the illustration came from: <a href="https://auditoryneuroscience.com/book-preview">Auditory Neuroscience</a>.</p>
</blockquote>

<p>One downside of ACF is that it drops the phase component of the input signal, and thus ACF is not reversible. This means that images that only render ACF, lose about 50% of the information from the sound and those 50% are important, e.g. dropping the phase from recorded speech makes that speech indiscernible. Real world sounds, such as voice, heavily use nuanced amplitude and phase modulation. ACF captures the former, but ignores the latter.</p>



<p>ACF of a sound sample <code>X[0..N-1]</code> can be computed with two FFTs:</p>

<div><div><pre><code>S = |FFT[X]|^2
ACF[X] = FFT[S]
</code></pre></div></div>

<p>And thus ACF contains exactly the same information as the spectral density <code>S</code> (the well known spectrogram).</p>

<blockquote>
  <p>If you’re familiar with the ACF definition, you’ll notice that I should’ve used the inverse FFT in the last step. There is no mistake. The inverse FFT can be computed as <code>FFT[X*]*</code>, where <code>X*</code> is complex conjugate, but since <code>S[i]</code> is real-valued (and positive, in fact), the conjugate has no effect on it, and since ACF is also real valued in this case, the second conjugate has no effect either.</p>
</blockquote>

<p>ACF is a periodic and even function and so it can be naturally rendered in polar coordinates. In most cases, ACF has a very elaborate structure. Below are some examples, where red = ACF &gt; 0 and blue = ACF &lt; 0.</p>

<table>
  <thead>
    <tr>
      <th>conventional music</th>
      <th>a bird song</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="https://soundshader.github.io/pics/acf-c-1.jpg" alt=""></td>
      <td><img src="https://soundshader.github.io/pics/acf-c-3.jpg" alt=""></td>
    </tr>
  </tbody>
</table>

<p>Looking at the first example, we can tell that there are 5 prominent peaks in a 20 ms sound sample, which corresponds to 250 Hz. This means that our ears would necesserarily perceive this sound as a 250 Hz tone, regardless of what its spectrogram says. If it was a pure 250 Hz tone, we’d see perfectly round shapes of the <code>r = cos(250Hz * t)</code> line, but it’s not the case here: we see that the 5 peaks are modulated with small wavelets: there is one big wavelet in the middle (which consists of 3 smaller wavelets) and 4 smaller wavelets. Our ears would hear the big wavelet as the 2nd harmonic of the 250 Hz tone (i.e. it would be a 500 Hz tone with a smaller amplitude) and the 4 small wavelets as the 5th harmonic (1000 Hz) at barely discernible volume. In addition to that, the 500 Hz harmonic is also modulated by the 3 tiny wavelets, which means we’d hear a 1500 Hz tone, almost inaudible. We can say all this without even looking at the spectrogram or hearing the sound.</p>



<p>Music is a temporal ornament. There are many types of ornaments, e.g. the 17 types of wallpaper tesselations, but few of them look like music. However there is one particular type of ornament that resembles music a lot - I mean those “mandala” images. I don’t know how and why those are produced, but I noticed a connection between those images and music:</p>

<ul>
  <li>The 1st obvious observation is that a mandala is drawn in polar coordinates and is <code>2*PI</code> periodic. Sound is periodic too, so I thought the two facts are related.</li>
  <li>The 2nd observation is that patterns on those images evolve over the radial axis. Ans so is music is a sequence of evolving sound patterns.</li>
  <li>The 3rd observation is that a <code>2*PI</code> periodic function trivially corresponds to a set of frequencies. We usually use FFT to extract the frequencies and another FFT to restore the <code>2*PI</code> periodic function. Thus, a single radial slice of a mandala could encode a set of frequencies. If this is correct, a mandala is effectively an old school vinyl disk.</li>
</ul>

<p>Putting these observations together we naturally arrive with the ACF idea.</p>



<p>Open an issue on github or shoot me a email at ssgh@aikh.org</p>



<p>AGPLv3</p>


      
    </div></div>]]>
            </description>
            <link>https://soundshader.github.io/hn/acf/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25037784</guid>
            <pubDate>Mon, 09 Nov 2020 18:19:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Is it Pokemon or Big Data? (2016)]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25037755">thread link</a>) | @boogies
<br/>
November 9, 2020 | https://pixelastic.github.io/pokemonorbigdata/ | <a href="https://web.archive.org/web/*/https://pixelastic.github.io/pokemonorbigdata/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    
    
    
    
    
    
    
    
    
    <p>
      Made by <a href="https://twitter.com/pixelastic/">@pixelastic</a>, 
      inspired by <a href="https://docs.google.com/forms/d/1kckcq_uv8dk9-W5rIdtqRwCHN4Uh209ELPUjTEZJDxc/viewform">this google form</a>.
    </p>
  


</div>]]>
            </description>
            <link>https://pixelastic.github.io/pokemonorbigdata/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25037755</guid>
            <pubDate>Mon, 09 Nov 2020 18:17:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An Illustrated Guide to Superlearning]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25037740">thread link</a>) | @prostoalex
<br/>
November 9, 2020 | https://www.khstats.com/blog/sl/superlearning/ | <a href="https://web.archive.org/web/*/https://www.khstats.com/blog/sl/superlearning/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
      


<blockquote>
<p>Why use <em>one</em> machine learning algorithm when you could use all of them?! This post contains a step-by-step walkthrough of how to build a superlearner prediction algorithm in <code>R</code>.</p>
</blockquote>


<title>
HTML Image as link
</title>


<p><img alt="cheatsheet" src="https://www.khstats.com/img/Superlearning.jpg" width="100%&quot;"></p><figcaption>
<strong><em>A Visual Guide…</em></strong> Over the winter, I read <a href="https://www.springer.com/gp/book/9781441997814"><em>Targeted Learning</em></a> by Mark van der Laan and Sherri Rose. This “visual guide” I made for <em>Chapter 3: Superlearning</em> by Rose, van der Laan, and Eric Polley is a condensed version of the following tutorial. It is available as an <a href="https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/Superlearner.pdf">8.5x11" pdf on Github</a>, should you wish to print it out for reference (or desk decor).
</figcaption>



<div id="supercuts-of-superlearning">

<ul>
<li><p><strong>Superlearning</strong> is a technique for prediction that involves <strong>combining many individual statistical algorithms</strong> (commonly called “data-adaptive” or “machine learning” algorithms) to <strong>create a new, single prediction algorithm</strong> that is expected to <strong>perform at least as well as any of the individual algorithms</strong>.</p></li>
<li><p>The superlearner algorithm “decides” how to combine, or weight, the individual algorithms based upon how well each one <strong>minimizes a specified loss function</strong>, for example, the mean squared error (MSE). This is done using cross-validation to avoid overfitting.</p></li>
<li><p>The motivation for this type of “ensembling” is that <strong>a mix of multiple algorithms may be more optimal for a given data set than any single algorithm</strong>. For example, a tree based model averaged with a linear model (e.g.&nbsp;random forests and LASSO) could smooth some of the model’s edges to improve predictive performance.</p></li>
<li><p>Superlearning is also called stacking, stacked generalizations, and weighted ensembling by different specializations within the realms of statistics and data science.</p></li>
</ul>
<p><img src="https://www.khstats.com/img/spiderman_meme.jpg"></p>
</div>
<div id="superlearning-step-by-step">

<p>First I’ll go through the algorithm one step at a time using a simulated data set.</p>
<div id="initial-set-up-load-libraries-set-seed-simulate-data">
<h2>Initial set-up: Load libraries, set seed, simulate data</h2>
<p>For simplicity I’ll show the concept of superlearning using only four variables (AKA features or predictors) to predict a continuous outcome. Let’s first simulate a continuous outcome, <code>y</code>, and four potential predictors, <code>x1</code>, <code>x2</code>, <code>x3</code>, and <code>x4</code>.</p>
<pre><code>library(tidyverse)
library(knitr)
set.seed(7)</code></pre>
<pre><code>n &lt;- 5000
obs &lt;- tibble(
  id = 1:n,
  x1 = rnorm(n),
  x2 = rbinom(n, 1, plogis(10*x1)),
  x3 = rbinom(n, 1, plogis(x1*x2 + .5*x2)),
  x4 = rnorm(n, mean=x1*x2, sd=.5*x3),
  y = x1 + x2 + x2*x3 + sin(x4)
)
kable(head(obs), digits=3, caption = "Simulated data set")</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-2">Table 1: </span>Simulated data set</caption>
<thead>
<tr>
<th>id</th>
<th>x1</th>
<th>x2</th>
<th>x3</th>
<th>x4</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2.287</td>
<td>1</td>
<td>1</td>
<td>1.385</td>
<td>5.270</td>
</tr>
<tr>
<td>2</td>
<td>-1.197</td>
<td>0</td>
<td>0</td>
<td>0.000</td>
<td>-1.197</td>
</tr>
<tr>
<td>3</td>
<td>-0.694</td>
<td>0</td>
<td>0</td>
<td>0.000</td>
<td>-0.694</td>
</tr>
<tr>
<td>4</td>
<td>-0.412</td>
<td>0</td>
<td>1</td>
<td>-0.541</td>
<td>-0.928</td>
</tr>
<tr>
<td>5</td>
<td>-0.971</td>
<td>0</td>
<td>0</td>
<td>0.000</td>
<td>-0.971</td>
</tr>
<tr>
<td>6</td>
<td>-0.947</td>
<td>0</td>
<td>1</td>
<td>-0.160</td>
<td>-1.107</td>
</tr>
</tbody>
</table>


<h2>
<strong>Step 1: Split data into K folds
</strong></h2><p><img src="https://www.khstats.com/img/sl_steps/step1.png">
The superlearner algorithm relies on K-fold cross-validation (CV) to avoid overfitting. We will start this process by splitting the data into 10 folds. The easiest way to do this is by creating indices for each CV fold.</p>
<pre><code>k &lt;- 10 # 10 fold cv
cv_index &lt;- sample(rep(1:k, each = n/k)) # create indices for each CV fold. We need each fold K to contain n (all the rows of our data set) divided by k rows. in our example this is 5000/10 = 500 rows in each fold</code></pre>


<h2>
<strong>Step 2: Fit base learners for first CV-fold
</strong></h2><p><img src="https://www.khstats.com/img/sl_steps/step2.png"></p>
<p>Recall that in K-fold CV, each fold serves as the validation set one time. In this first round of CV, we will train all of our base learners on all the CV folds (k = 1,2,…,9) <em>except</em> for the very last one: <code>cv_index == 10</code>.</p>
<p>The individual algorithms or <strong>base learners</strong> that we’ll use here are three linear regressions with differently specified parameters:</p>
<ol>
<li><p><strong>Learner A</strong>: <span>\(Y=\beta_0 + \beta_1 X_2 + \beta_2 X_4 + \epsilon\)</span></p></li>
<li><p><strong>Learner B</strong>: <span>\(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_3 + \beta_4 sin(X_4) + \epsilon\)</span></p></li>
<li><p><strong>Learner C</strong>: <span>\(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_1 X_2 + \beta_5 X_1 X_3 + \beta_6 X_2 X_3 + \beta_7 X_1 X_2 X_3 + \epsilon\)</span></p></li>
</ol>
<pre><code>cv_train_1 &lt;- obs[-which(cv_index == 10),] # make a data set that contains all observations except those in k=1
fit_1a &lt;- glm(y ~ x2 + x4, data=cv_train_1) # fit the first linear regression on that training data
fit_1b &lt;- glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=cv_train_1) # second LR fit on the training data
fit_1c &lt;- glm(y ~ x1*x2*x3, data=cv_train_1) # and the third LR</code></pre>
<p>I am <em>only</em> using the linear regressions so that code for running more complicated regressions does not take away from understanding the general superlearning algorithm.</p>
<p>Superlearning actually works best if you use a diverse set, or <strong>superlearner library</strong>, of base learners. For example, instead of three linear regressions, we could use a least absolute shrinkage estimator (LASSO), random forest, and multivariate adaptive splines (MARS). Any parametric or non-parametric supervised machine learning algorithm can be included as a base learner.</p>


<h2>
<strong>Step 3: Obtain predictions for first CV-fold
</strong></h2><p><img src="https://www.khstats.com/img/sl_steps/step3.png"></p>
<p>We can then get use our validation data, <code>cv_index == 10</code>, to obtain our first set of cross-validated predictions.</p>
<pre><code>cv_valid_1 &lt;- obs[which(cv_index == 10),] # make a data set that only contains observations except in k=10
pred_1a &lt;- predict(fit_1a, newdata = cv_valid_1) # use that data set as the validation for all the models in the SL library
pred_1b &lt;- predict(fit_1b, newdata = cv_valid_1) 
pred_1c &lt;- predict(fit_1c, newdata = cv_valid_1)</code></pre>
<p>Since we have 5000 <code>obs</code>ervations, that gives us three vectors of length 500: a set of predictions for each of our Learners A, B, and C.</p>
<pre><code>length(pred_1a) # double check we only have n/k predictions ...we do :-)</code></pre>
<pre><code>## [1] 500</code></pre>
<pre><code>knitr::kable(head(cbind(pred_1a, pred_1b, pred_1c)), digits= 2, caption = "First CV round of predictions") </code></pre>
<table>
<caption><span id="tab:unnamed-chunk-6">Table 2: </span>First CV round of predictions</caption>
<thead>
<tr>
<th>pred_1a</th>
<th>pred_1b</th>
<th>pred_1c</th>
</tr>
</thead>
<tbody>
<tr>
<td>-1.39</td>
<td>-0.77</td>
<td>-0.40</td>
</tr>
<tr>
<td>-1.27</td>
<td>-0.34</td>
<td>-0.11</td>
</tr>
<tr>
<td>2.16</td>
<td>1.32</td>
<td>1.10</td>
</tr>
<tr>
<td>4.27</td>
<td>4.26</td>
<td>3.98</td>
</tr>
<tr>
<td>3.31</td>
<td>3.98</td>
<td>3.78</td>
</tr>
<tr>
<td>2.29</td>
<td>2.42</td>
<td>2.83</td>
</tr>
</tbody>
</table>


<h2>
<strong>Step 4: Obtain CV predictions for entire data set
</strong></h2><p><img src="https://www.khstats.com/img/sl_steps/step4.png"></p>
<p>We’ll want to get those predictions for <em>every</em> fold. So, using your favorite <code>for</code> loop, <code>apply</code> statement, or <code>map</code>ping function, fit the base learners and obtain predictions for each of them, so that there are 1000 predictions – one for every point in <code>obs</code>ervations.</p>
<p>The way I chose to code this was to make a generic function that combines Step 2 (base learners fit to the training data) and Step 3 (predictions on the validation data), then use <code>map_dfr()</code> from the <code>purrr</code> package to repeat over all 10 CV folds. I saved the results in a new data frame called <code>cv_preds</code>.</p>
<pre><code>cv_folds &lt;- as.list(1:k)
names(cv_folds) &lt;- paste0("fold",1:k)

get_preds &lt;- function(fold){   # function that does the same procedure as step 2 and 3 for any CV fold
  cv_train &lt;- obs[-which(cv_index == fold),]  # make a training data set that contains all data except fold k
  fit_a &lt;- glm(y ~ x2 + x4, data=cv_train)  # fit all the base learners to that data
  fit_b &lt;- glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=cv_train)
  fit_c &lt;- glm(y ~ x1*x2*x3, data=cv_train)
  cv_valid &lt;- obs[which(cv_index == fold),]  # make a validation data set that only contains data from fold k
  pred_a &lt;- predict(fit_a, newdata = cv_valid)  # obtain predictions from all the base learners for that validation data
  pred_b &lt;- predict(fit_b, newdata = cv_valid)
  pred_c &lt;- predict(fit_c, newdata = cv_valid)
  return(data.frame("obs_id" = cv_valid$id, "cv_fold" = fold, pred_a, pred_b, pred_c))  # save the predictions and the ids of the observations in a data frame
}

cv_preds &lt;- purrr::map_dfr(cv_folds, ~get_preds(fold = .x)) # map_dfr loops through every fold (1:k) and binds the rows of the listed results together

cv_preds %&gt;% arrange(obs_id) %&gt;% head() %&gt;% kable(digits=2, caption = "All CV predictions for all three base learners") </code></pre>
<table>
<caption><span id="tab:unnamed-chunk-7">Table 3: </span>All CV predictions for all three base learners</caption>
<thead>
<tr>
<th></th>
<th>obs_id</th>
<th>cv_fold</th>
<th>pred_a</th>
<th>pred_b</th>
<th>pred_c</th>
</tr>
</thead>
<tbody>
<tr>
<td>1…1</td>
<td>1</td>
<td>4</td>
<td>3.73</td>
<td>5.42</td>
<td>5.28</td>
</tr>
<tr>
<td>1…2</td>
<td>2</td>
<td>8</td>
<td>-0.77</td>
<td>-1.19</td>
<td>-1.20</td>
</tr>
<tr>
<td>1…3</td>
<td>3</td>
<td>2</td>
<td>-0.78</td>
<td>-0.81</td>
<td>-0.69</td>
</tr>
<tr>
<td>1…4</td>
<td>4</td>
<td>10</td>
<td>-1.39</td>
<td>-0.77</td>
<td>-0.40</td>
</tr>
<tr>
<td>1…5</td>
<td>5</td>
<td>6</td>
<td>-0.78</td>
<td>-1.01</td>
<td>-0.97</td>
</tr>
<tr>
<td>1…6</td>
<td>6</td>
<td>7</td>
<td>-0.96</td>
<td>-1.04</td>
<td>-0.94</td>
</tr>
</tbody>
</table>


<h2>
<strong>Step 5: Choose and compute loss function of interest via metalearner
</strong></h2><p><img src="https://www.khstats.com/img/sl_steps/step5.png"></p>
<blockquote>
<p>This is the key step of the superlearner algorithm: we will use a new learner, a <strong>metalearner</strong>, to take information from all of the base learners and create that new algorithm.</p>
</blockquote>
<p>Now that we have cross-validated predictions for every observation in the data set, we want to merge those CV predictions back into our main data set…</p>
<pre><code>obs_preds &lt;- 
  full_join(obs, cv_preds, by=c("id" = "obs_id"))</code></pre>
<p>…so that we can minimize a final loss function of interest between the true outcome and each CV prediction. This is how we’re going to optimize our overall prediction algorithm: we want to make sure we’re “losing the least” in the way we combine our base learners’ predictions to ultimately make final predictions. We can do this efficiently by choosing a new learner, a metalearner, which reflects the final loss function of interest.</p>
<p>For simplicity, we’ll use another linear regression as our metalearner. Using a linear regression as a metalearner will minimize the Cross-Validated Mean Squared Error (CV-MSE) when combining the base learner predictions. Note that we could use a variety of parametric or non-parametric regressions to minimize the CV-MSE.</p>
<p>No matter what metalearner we choose, the predictors will always be the cross-validated predictions from each base learner, and the outcome will always be the true outcome, <code>y</code>.</p>
<pre><code>sl_fit &lt;- glm(y ~ pred_a + pred_b + pred_c, data = obs_preds)
kable(broom::tidy(sl_fit), digits=3, caption = "Metalearner regression coefficients") </code></pre>
<table>
<caption><span id="tab:unnamed-chunk-9">Table 4: </span>Metalearner regression coefficients</caption>
<thead>
<tr>
<th>term</th>
<th>estimate</th>
<th>std.error</th>
<th>statistic</th>
<th>p.value</th>
</tr>
</thead>
<tbody>
<tr>
<td>(Intercept)</td>
<td>-0.003</td>
<td>0.002</td>
<td>-1.447</td>
<td>0.148</td>
</tr>
<tr>
<td>pred_a</td>
<td>-0.017</td>
<td>0.004</td>
<td>-4.739</td>
<td>0.000</td>
</tr>
<tr>
<td>pred_b</td>
<td>0.854</td>
<td>0.007</td>
<td>128.241</td>
<td>0.000</td>
</tr>
<tr>
<td>pred_c</td>
<td>0.165</td>
<td>0.005</td>
<td>30.103</td>
<td>0.000</td>
</tr>
</tbody>
</table>
<p>This metalearner provides us with the coefficients, or weights, to apply to each of the base learners. In other words, if we have a set of predictions from Learner A, B, and C, we can obtain our best possible predictions by starting with an intercept of -0.003, then adding -0.017 <span>\(\times\)</span> predictions from Learner A, 0.854 <span>\(\times\)</span> predictions from Learner B, and 0.165 <span>\(\times\)</span> predictions from Learner C.</p>
<p><em>For more information on the metalearning step, check out the <a href="#appendix">Appendix</a>.</em></p>


<h2>
<strong>Step 6: Fit base learners on entire data set
</strong></h2><p><img src="https://www.khstats.com/img/sl_steps/step6.png"></p>
<p>After we fit the metalearner, we officially have our superlearner algorithm, so it’s time to input data and …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.khstats.com/blog/sl/superlearning/">https://www.khstats.com/blog/sl/superlearning/</a></em></p>]]>
            </description>
            <link>https://www.khstats.com/blog/sl/superlearning/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25037740</guid>
            <pubDate>Mon, 09 Nov 2020 18:16:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Rust emit=asm Can Be Misleading]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25037663">thread link</a>) | @todsacerdoti
<br/>
November 9, 2020 | https://siliconsprawl.com/2020/11/09/rust-emit-asm.html | <a href="https://web.archive.org/web/*/https://siliconsprawl.com/2020/11/09/rust-emit-asm.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <h2 id="the-short-version">The short version</h2>

<p>Cargo builds like:</p>
<div><div><pre><code><span>$ RUSTFLAGS</span><span>=</span><span>"--emit asm"</span> cargo build <span>--release</span>
<span>$ </span>cargo rustc <span>--release</span> <span>--</span> <span>--emit</span> asm
</code></pre></div></div>

<p>Do not always output assembly equivalent to the machine code you’d get from:</p>


<p>Possibly <code>rustc --emit=asm</code> has some uses, like examining a single file with
no external dependencies, but it’s not useful for my normal case of wanting
to look at the asm for an arbitrary release build.</p>

<h2 id="the-long-version">The long version</h2>

<p><a href="https://siliconsprawl.com/2020/11/06/simd-ray-tracer.html">Previously</a> I rewrote my ray tracer
to use <code>crossbeam::scope</code> and <code>crossbeam::queue</code> instead of rayon. Internally
rayon leans heavily on <code>crossbeam::deque</code> for its work-stealing implementation, so
my expectation was that this change would be neutral or a slight improvement, 
depending on how good of a job the compiler had been doing to condense
rayon’s abstractions.</p>

<p>Instead it was a ~15% regression.</p>

<h3 id="looking-at-the-asm-pt-1">Looking at the asm, pt. 1</h3>

<p>The asm output appeared sane. I saw no expensive indirection, calls, etc. -
things were getting properly inlined and optimized.</p>

<h3 id="understanding-rayon">Understanding rayon</h3>

<p>I first questioned my understanding of rayon and spent some time digging
through its guts. It’s well-engineered, and it’s impressive that clang’s able
to condense all of its abstractions down into basically no overhead - but I also
didn’t see anything fundamentally novel or surprising going on that would give it a significant performance edge. The
splitting/work assignment portion of the vec codepath looked like it would
lead to slightly more even partitioning than my hand-built crossbeam method,
but not by a lot, and definitely not by 15%. So that was bust. I did notice that
crossbeam needed to heap allocate the closure I was using for my thread body,
so perhaps that caused some additional overhead, but it should have been
negligible.</p>

<h3 id="cpu-profiling">CPU profiling</h3>

<p>At this point I dumped both versions into Instruments and did some basic CPU
profiling. rayon’s a bit annoying to poke around in because you end up with
extremely deep stacks of <code>join</code> frames, but nothing really stood out. The
crossbeam version was simply slower with no major red flags.</p>

<h3 id="more-in-depth-cpu-profiling">More in-depth CPU profiling</h3>

<p>I’d been looking for an excuse to try <a href="https://software.intel.com/content/www/us/en/develop/tools/vtune-profiler.html">Intel
VTune</a>
for awhile, but since it’s only supported on Windows and Linux and is best
run on bare-metal, it had always been slightly too much effort to stand up
for smaller projects. It seemed warranted for this one! I had an existing
Windows bootcamp partition, so figured I’d see just how much hassle it was to
get everything working in that before I dusted off something to run Linux.</p>

<p>Sidebar: turns out Rust on Windows is… really nice. I’m not a Windows dev. There are
things I admire about the ecosystem (like a good first-party
debugger and some decent OS APIs), but apart from some Java way back in high
school I’ve never even tried to compile software on a Windows machine. It
always looked like a nightmare for C/C++ projects - I’m familiar enough with
the code side of cross-platform support, but as for actually
building things… I think cmake can spit out a Visual Studio project? And I
keep hearing about WSL? So I went in with significant trepidation. Turns out
it took all of ten minutes to install the VS C++ tools, rustup, a rust
toolchain, vtune, and get everything building and working together. Pretty
impressive.</p>

<p>VTune itself is a complex beast. Most (all?) of the data in it is stuff you
could get out of <code>perf</code>, but the collection and workflow is streamlined - it
does a good job of keeping track of previous runs, grouping them in a way so
you don’t lose anything, surfacing useful information based on top-level
categories (eg. “I want to look at memory access”), and providing a 
diff view between runs. It looks particularly useful for guiding iterative optimization
and refinement. It’s a bit less useful when I’m comparing the
performance of two fairly different programs, because many of the stack
traces are unique to either the rayon or crossbeam version, so “you have 100%
more of these rayon stack traces in this run” is not helpful. Looking through
the data I saw that I was getting flagged on uarch perf, retiring
instructions maybe 5% worse in the crossbeam version. Thinking that could be
stalling waiting on memory, I ran a memory access profile and saw:</p>

<p><img src="https://siliconsprawl.com/assets/images/rrt/vtune_macc.png" alt=""></p>

<p>Crossbeam version is on the left, rayon version is on the right. Okay, 3s
runtime difference - that’s commensurate with the perf regression I’m seeing.
Interesting, we’re memory bound twice as frequently. That’s strange because
our memory access pattern should be pretty similar. We’re doing over twice as
many stores. We’re doing some additional loads. We’re…</p>

<p>Wait.</p>

<p>We’re doing over twice as many stores?! That doesn’t make sense.</p>

<h3 id="replacing-crossbeamscope">Replacing crossbeam::scope</h3>

<p>Perhaps heap allocating the closures was more expensive than I thought, or
had bad knock-on effects. It’s a long shot, but the whole point of side
projects is following some of those random tangents. I set about eliminating
<code>crossbeam::scope</code> and using <code>std::thread</code> directly instead. This was a quick
and dirty test: the entire point of <code>scope</code> is to create an abstraction that
communicates to the borrow checker that threads we’ve spun off have been
joined, otherwise it doesn’t know when a thread’s borrow is guaranteed to
have ended and requires that data references from a thread’s closure are all
static lifetime. In this case I’m manually joining the threads, so I can do a
transmute to placate the compiler. Don’t ship code like this, it defeats the
purpose of using Rust in the first place - you’d have a better experience
with C++. But it can be really handy to circumvent these sorts of checks when
doing quick prototyping/performance analysis to decide if it’s worth the time
to build out a safe abstraction. I would welcome a “just build this without
the borrow checker” mode for cases like this, though I’m probably in the
minority and I don’t expect that would be an easy feature to add.</p>

<p>My testing code looked roughly like this:</p>
<div><div><pre><code><span>let</span> <span>pixels</span> <span>=</span> <span>unsafe</span> <span>{</span> 
    <span>mem</span><span>::</span><span>transmute</span><span>::</span><span>&lt;&amp;</span><span>mut</span> <span>[</span><span>V3</span><span>],</span> <span>&amp;</span><span>'static</span> <span>mut</span> <span>[</span><span>V3</span><span>]</span><span>&gt;</span><span>(</span><span>pixels</span><span>)</span>
<span>};</span>
<span>let</span> <span>handle</span> <span>=</span> <span>std</span><span>::</span><span>thread</span><span>::</span><span>spawn</span><span>(</span><span>move</span> <span>||</span> <span>{</span>
    <span>// code that uses &amp;pixels</span>
<span>});</span>
<span>handle</span><span>.join</span><span>()</span><span>.unwrap</span><span>();</span>
</code></pre></div></div>

<p>As expected, no significant performance gains were had.</p>

<h3 id="looking-at-the-asm-pt-2">Looking at the asm, pt. 2…</h3>

<p>Something isn’t adding up so I want to look at the assembly again, but I’d like to
clearly distinguish between my unchanged business logic and the
rayon/crossbeam coordination code. The majority of my business logic is
behind a single function named <code>cast</code>; adding <code>#[inline(never)]</code> to that single ray processing function
should give me a nice seam between rayon and my business logic.</p>

<p>Build, run and the rayon version slows down… in fact it runs exactly as slow as the crossbeam
version.</p>

<p>I try adding <code>#[inline(always)]</code> to the <code>cast</code> function in the crossbeam
version, and lo and behold it speeds up to match the original rayon version,
my regression disappears.</p>

<p>But, how’s that possible? The <em>first</em> thing I did was look at inlining. Maybe
my quick once-over missed it, maybe I misread and this whole circuitous path
is all my fault?</p>

<p>I generated assembly output for both the inlined and noninlined versions of the crossbeam ray tracer:</p>
<div><div><pre><code><span>$ </span>rg ecl_rt4cast inline.s 
21293:	.asciz	<span>"_ZN6ecl_rt4cast17hc1100eade04dff75E"</span>
<span>$ </span>rg ecl_rt4cast noinline.s 
21293:	.asciz	<span>"_ZN6ecl_rt4cast17hc1100eade04dff75E"</span>
</code></pre></div></div>

<p>I’m building release with symbols, so that string is expected. But neither
version, not even the non-inlined version, is making calls to <code>cast()</code>.
Curious.</p>
<div><div><pre><code><span>$ </span><span>wc</span> <span>-l</span> inline.s 
203969 inline.s
<span>$ </span><span>wc</span> <span>-l</span> noinline.s 
203969 noinline.s
</code></pre></div></div>

<p>Now I feel like I’m being gaslighted. These are the exact same length. A diff
shows that the only changes are some arbitrary IDs in debug info. I have a
difficult relationship with optimizing compilers, so my first thought is maybe
clang’s being clang again and I should go validate the binaries instead…</p>

<div><div><pre><code><span>$ </span>objdump <span>-d</span> ecl_rt_inline | rg ecl_rt4cast
&lt;no output&gt;
<span>$ </span>objdump <span>-d</span> ecl_rt_noinline | rg ecl_rt4cast
0000000100003190 __ZN6ecl_rt4cast17hc1100eade04dff75E:
100003299: e9 af 01 00 00              	jmp	431 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x2bd&gt;
1000034a2: eb 1f                       	jmp	31 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x333&gt;
1000034c6: 74 38                       	je	56 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x370&gt;
1000034e5: 0f 82 f5 00 00 00           	jb	245 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x450&gt;
1000034ee: 72 1d                       	jb	29 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x37d&gt;
1000034f0: e9 eb 00 00 00              	jmp	235 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x450&gt;
100003503: 0f 83 d7 00 00 00           	jae	215 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x450&gt;
100003515: 0f 87 16 03 00 00           	ja	790 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x6a1&gt;
10000351e: 0f 82 1f 03 00 00           	jb	799 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x6b3&gt;
100003527: 0f 82 2b 03 00 00           	jb	811 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x6c8&gt;
100003530: 0f 82 37 03 00 00           	jb	823 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x6dd&gt;
100003539: 0f 82 40 03 00 00           	jb	832 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x6ef&gt;
100003590: 0f 84 1a ff ff ff           	je	<span>-230</span> &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x320&gt;
1000035db: e9 d0 fe ff ff              	jmp	<span>-304</span> &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x320&gt;
10000360a: 0f 86 a1 01 00 00           	jbe	417 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x621&gt;
100003637: 0f 87 57 02 00 00           	ja	599 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x704&gt;
100003668: 0f 86 3d 02 00 00           	jbe	573 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x71b&gt;
100003682: 0f 84 41 01 00 00           	je	321 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x639&gt;
100003707: 0f 85 93 fb ff ff           	jne	<span>-1133</span> &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x110&gt;
10000371a: 0f 86 9f 01 00 00           	jbe	415 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x72f&gt;
100003723: 0f 86 a8 01 00 00           	jbe	424 &lt;__ZN6ecl_rt4cast17hc1100eade04dff75E+0x741&gt;
10000372c: 0f 86 b1 01 00 00           	jbe	433 &lt;_…</code></pre></div></div></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://siliconsprawl.com/2020/11/09/rust-emit-asm.html">https://siliconsprawl.com/2020/11/09/rust-emit-asm.html</a></em></p>]]>
            </description>
            <link>https://siliconsprawl.com/2020/11/09/rust-emit-asm.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25037663</guid>
            <pubDate>Mon, 09 Nov 2020 18:09:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Journalist vs. Facebook]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25037615">thread link</a>) | @leoschwartz
<br/>
November 9, 2020 | https://restofworld.org/2020/the-journalist-vs-facebook/ | <a href="https://web.archive.org/web/*/https://restofworld.org/2020/the-journalist-vs-facebook/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
<p>Maria Ressa has a cheerful way of making apocalyptic pronouncements. “The world as we knew it is rubble,” she says over a Zoom call from Manila. “Facts are debatable, we have alternate realities — the White House will say we have alternate realities.”</p>



<p>Ressa is the CEO of Rappler, a prominent digital news outlet in the Philippines. Over the past few years, she has watched from the front row as the internet helped demolish civil discourse and fracture society’s shared sense of reality. In 2016, Ressa witnessed a flood of digital disinformation help elect Rodrigo Duterte, the firebrand populist who as president launched a “war on drugs” that has killed thousands of Filipinos. That year, the same trends helped put in office U.S. President Donald Trump and push the United Kingdom to leave the European Union.</p>



<p>During Duterte’s 2016 campaign, Rappler analyzed fraudulent social media networks that promoted the president and attacked his opponents, drawing on Ressa’s experience mapping terrorist organizations as an investigative reporter in the 1990s and 2000s.</p>



<p>Over the following four years, Ressa became not only a chronicler of the government’s falsehoods but a target of them. Through intimidation and lawsuits, the Duterte administration has <a href="https://www.nytimes.com/2020/05/05/world/asia/philippines-abs-cbn-duterte.html">worked to muzzle</a> a number of independent media organizations in the Philippines. Ressa has been singled out: She and her company were charged with a series of spurious crimes, including tax evasion, all of which she has denied. In June, she and a former colleague <a href="https://www.nytimes.com/2020/06/14/business/maria-ressa-verdict-philippines-rappler.html">were convicted</a> of cyberlibel, a charge that stemmed from an article Rappler published in 2012. Ressa is now out on bail pending an appeal but could ultimately face years in prison.&nbsp;</p>



<p>Rather than silence her, these attacks have only given Ressa a bigger platform. Named a <em>Time </em>magazine <a href="https://time.com/5793800/maria-ressa-the-guardians-100-women-of-the-year/">person of the year</a> in 2019, she has become a global symbol of press freedom and an avatar of courage in the face of rising authoritarianism. As her profile has grown, so, too, has her mission. Today, Ressa’s focus has shifted away from Duterte, whom she sees as only the symptom of a larger problem: Facebook.</p>



<p>Ressa is one of the founding members of the <a href="https://www.theverge.com/interface/2020/9/29/21472092/real-facebook-oversight-board-stunt-activism-limitations">Real Facebook Oversight Board</a>, a group of academics, journalists, and activists formed as a tongue-in-cheek counterpoint to the company’s actual <a href="https://about.fb.com/news/2020/05/welcoming-the-oversight-board/">Oversight Board</a>, launched in May. That board was designed to function as a “Supreme Court” of sorts, with the power to decide if the company’s content-moderation decisions “were made in accordance with Facebook’s stated values and policies.” Ressa contends that those values and policies are<em> themselves</em> the problem, and says that Facebook’s persistent failure to effectively combat disinformation and hate speech poses an existential threat to democracy around the world.</p>



<p>“We’re fighting huge powers. Duterte, Zuckerberg,” she says. “Who would have thought you would have put the two of them in the same breath, but that’s what I’ve been living with for the last four years.”</p>



<p>Rappler began as a Facebook page in 2011 and launched as a stand-alone website the following year. Ressa says she was initially enthusiastic about the power of “social media for good.” Rappler even partnered with Facebook’s Internet.org initiative, which gives consumers in developing countries free access to certain websites. The program was instrumental in getting millions of Filipinos online, and <a href="https://nymag.com/intelligencer/2018/09/how-facebooks-free-internet-helped-elect-a-dictator.html">also on Facebook</a>: Today, nearly all internet users in the country have a Facebook account. It was the 2016 presidential election in the Philippines that eventually changed Ressa’s mind about the social network. “When the problems began, we were the first to feel it,” she says.&nbsp;</p>



<p>Rappler identified dozens of Facebook accounts that were creating and spreading disinformation in support of Duterte. Some posted fictionalized accounts of terrorist attacks and murders supposedly committed by drug addicts, which fed into the newly elected president’s law-and-order agenda. Ressa alerted Facebook, but she says it failed to take action. So <a href="https://www.rappler.com/nation/propaganda-war-weaponizing-internet">Rappler exposed the accounts</a> in a series of articles, instantly making the site a target of the same troll armies it was reporting on. (Facebook says it removes any content that violates its rules.)</p>


		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/11/h_3.01374802-40x60.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/11/h_3.01374802-600x1066.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/11/h_3.01374802-400x601.jpg 400w, https://restofworld.org/wp-content/uploads/2020/11/h_3.01374802-600x902.jpg 600w, https://restofworld.org/wp-content/uploads/2020/11/h_3.01374802-1000x1503.jpg 1000w, " sizes="(max-width: 640px) 100vw, 300px" alt="“The platform itself is biased against facts. It’s really biased against journalism,” said Maria Ressa.">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				<span itemprop="copyrightHolder">Tania/Contrasto/Redux</span>
			</figcaption>
		</figure>


<p>Since 2016, Ressa has become increasingly convinced that Facebook needs to profoundly change how it’s designed and governed. She believes the platform’s algorithms and content-moderation policies are inherently prejudiced against reasoned debate based on settled truths. “The platform itself is biased against facts. It’s really biased against journalism,” she says. “Social media platforms have atomized meaning to meaninglessness. They have completely deconstructed context.”</p>



<p>Her opinions are backed by a growing body of academic research, which shows that social media sites often <a href="https://jonahberger.com/wp-content/uploads/2013/02/ViralityB.pdf">reward emotional messages over rational analysis</a>, funnel users toward <a href="https://www.scientificamerican.com/article/biases-make-people-vulnerable-to-misinformation-spread-by-social-media/">content that reinforces their</a> preexisting beliefs, and<a href="https://science.sciencemag.org/content/359/6380/1146"> spread lies more rapidly and widely than they do the truth</a>.</p>



<p>Ressa says one of Facebook’s most alarming shortcomings is its reluctance to moderate <a href="https://restofworld.org/2020/in-the-philippines-fake-news-can-get-you-killed/?utm_medium=Social&amp;utm_source=Twitter#Echobox=1603977263">disinformation posted by governments and politicians</a>. The company has justified its restraint by arguing that statements from public figures should remain online for public scrutiny. Although Facebook has removed state-backed propaganda in some instances, Ressa, along with other activists, say that these actions frequently amount to too little, too late. They say Facebook’s inaction has allowed propaganda and disinformation to spread unchecked, overwhelming and delegitimizing the news media.</p>



<p>“What we saw [in the Philippines] was that news organizations were being pushed to the periphery, and the center of the conversation was being taken over by the pro-government, state-sponsored disinformation,” Ressa says.</p>



<p>Without checks and balances on social media, Ressa says, authoritarian governments like Duterte’s can impose their own narratives — that drug addicts and communists run the country, and that journalists like Ressa are criminals and conspirators.</p>



<p>Ressa, who speaks in long, rapid-fire monologues, apologizes throughout her interview for her anger. “It’s very emotional for me,” she says. “I have real skin in the game. If they don’t fix this, this is how the government will normalize the possibility of jailing me. I could go to jail because [Facebook] refuses to address these problems that they created.”</p>



<p>The idea for the Real Facebook Oversight Board began taking shape last year, after Ressa met with Carole Cadwalladr, the journalist who, in 2018, first reported that political consulting firm <a href="https://www.theguardian.com/news/series/cambridge-analytica-files">Cambridge Analytica had harvested Facebook data</a> from millions of users. Since then, a number of high-profile academics and activists have joined the project, including Harvard professor Shoshana Zuboff, who wrote the best-selling book <em>The Age of Surveillance Capitalism</em>, as well as former Estonian President Thomas Hendrik Ilves and NAACP President Derrick Johnson.</p>



<p>The board was created to force Facebook into taking responsibility for the damage it has caused. The group <a href="https://www.axios.com/facebook-critics-take-on-its-oversight-board-06c496a2-355f-4a10-a8b4-d8f9b041a611.html">plans</a> to “use stunts, viral video, celebrity endorsement, and skillful media management” to put a spotlight on the threats social media companies pose to democracy. Ressa says she hasn’t fully discounted Facebook’s ability to do the right thing. “They just need to get off their butts and fix it before it is completely broken,” she says.</p>



<p>Ressa is still processing her evolution from journalism to activism, but she says she sees an obvious connection between the two, especially in an environment where facts are under constant attack. “Journalism is activism when it is a battle for truth,” she says. “This is a time when anyone living in a democracy, if you care about democracy, you have to sit there and answer the same question I was forced to answer four years ago, which is: What are you willing to sacrifice for the truth?”</p>
		</div></div>]]>
            </description>
            <link>https://restofworld.org/2020/the-journalist-vs-facebook/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25037615</guid>
            <pubDate>Mon, 09 Nov 2020 18:06:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Work faster and safer on untested code with Overcommitting]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25037561">thread link</a>) | @nicoespeon
<br/>
November 9, 2020 | https://understandlegacycode.com/blog/work-faster-safer-untested-code-overcommitting/ | <a href="https://web.archive.org/web/*/https://understandlegacycode.com/blog/work-faster-safer-untested-code-overcommitting/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><undefined>
  <a href="https://understandlegacycode.com/static/2c60e52fc6710a2397962652219eef72/b8ccf/overcommitting-preview.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="Comic that recaps the content of this post" title="" src="https://understandlegacycode.com/static/2c60e52fc6710a2397962652219eef72/799d3/overcommitting-preview.png" srcset="https://understandlegacycode.com/static/2c60e52fc6710a2397962652219eef72/00d96/overcommitting-preview.png 148w,https://understandlegacycode.com/static/2c60e52fc6710a2397962652219eef72/0b23c/overcommitting-preview.png 295w,https://understandlegacycode.com/static/2c60e52fc6710a2397962652219eef72/799d3/overcommitting-preview.png 590w,https://understandlegacycode.com/static/2c60e52fc6710a2397962652219eef72/b8ccf/overcommitting-preview.png 750w" sizes="(max-width: 590px) 100vw, 590px">
    </span>
  </span>
  
  </a>
    </undefined></p><p><em>Get this comic in SVG + high-res PNG by subscribing to my weekly newsletter at the bottom of this post 👇</em></p><hr><p>I’m sure you’ve got into this paradox:</p><ul><li>People tell you to write tests <em>before</em> you refactor, so you know if you break something</li><li>But the code is so messy that you need to refactor it <em>before</em> you can write tests</li></ul><p>It sounds like a <a href="https://en.wikipedia.org/wiki/Chicken_or_the_egg">chicken-and-egg problem</a>. Where do you even start?</p><p>Well, usually you don’t have the luxury to wait until you find out. You estimated this issue would take 2 days and now this estimation has turned into a commitment. So you fall back to what you know to do: changing the code, trying not to do any mistake, and testing manually a few common scenarios to verify everything is still working.</p><p>If that’s you, I have a technique for you that will preserve your speed and make your changes safer. Mastering it will make you go faster when things are blurry. You’ll be a <a href="https://fr.wikipedia.org/wiki/Speedrun">speedrunner</a> through the maze of Legacy Code.</p><p><img src="https://understandlegacycode.com/acdf66f62dec2dd5d4b3659e6ca43245/speedrun.gif" alt="Mario speedrun"></p><p>If you’re versioning your code, you already have the tool you need. If you’re not, I suggest <a href="https://git-scm.com/downloads">you install git</a>, run <code>git init</code> in your source code repository, commit everything, and start from here.</p><h2 id="how-overcommitting-can-help"><a href="#how-overcommitting-can-help" aria-label="how overcommitting can help permalink"></a>How overcommitting can help</h2><p>Among all the benefits of source code version control, one is to save the state of your code at different moments of the development. This is what you do when you commit. It allows you to go back to a previous state (commit) if needed.</p><p>Commits are like checkpoints. When the game is particularly difficult, having more checkpoints helps.</p><p>When you realize you did something wrong, commits allow you to go back to a state where things were still working, so you can revisit your changes or try again.</p><p>When you don’t have automated tests, the feedback loop is notoriously longer. You won’t know if you break something until a few minutes, maybe even hours. If you commit as you use to do, you’ll lose dozen of minutes of code, if I’m being optimistic.</p><p>But if you were committing every minute or so, you would have many more checkpoints to start from. You’d be able to find out precisely when things got sour, and start from here!</p><p><strong>Overcommitting allows you to work safely.</strong></p><h2 id="what-does-it-look-like"><a href="#what-does-it-look-like" aria-label="what does it look like permalink"></a>What does it look like</h2><p>When change is risky, think about committing often. Then, commit even more than that!</p><p>You need to commit absurdly frequently. If it feels stupidly frequent, you’re doing it right.</p><p>You’re not used to committing that often. So you need to deliberately push yourself out of your habits to build a new one.</p><p>As you can’t both think carefully about the code and think carefully about committing, I recommend you use a timer. 2 minutes is a good chunk to get started. In my experience, developers commit way less often than that.</p><ol><li>Start a 2 minutes timer.</li><li>When it rings, commit what you have.</li><li>Repeat.</li></ol><p>It’s designed to be a short loop. You must repeat this again and again during your whole coding session.</p><p><strong>Don’t bother with the commit message yet.</strong> That will feel wrong, but that’s normal. You can’t come up with a good name if you commit everything you have every 2 minutes (or you’d be very lucky). The goal is to do safe changes for the moment, focus on that.</p><p>If you’re worried commits won’t pass the review, I advise you to take 5min at the end of each hour to <a href="http://gitready.com/advanced/2009/02/10/squashing-commits-with-rebase.html">squash your commits together</a> and give them more appropriate messages. You should have ~30 small commits to rebase, which shouldn’t take long.</p><h2 id="why-does-it-work"><a href="#why-does-it-work" aria-label="why does it work permalink"></a>Why does it work?</h2><p>The obvious reason is that you create much, much more checkpoints. It makes it easier to detect the exact moment a bug was introduced. You waste less time debugging when you realize there’s a problem.</p><p>Because you don’t focus on the commit message while you’re committing, it preserves your speed. The time is taking care of telling you to commit. When it rings, you just do that and move on.</p><p>But there’s more, and it has to do with safety.</p><p>J. B. Rainsberger told me this recently: <em>why do we have breaks on a car?</em></p><p>Most people will tell you that’s so you can stop. The truth is: <strong>that’s so you can go fast</strong>! If there were no breaks, you would drive slowly and carefully because you can’t react promptly to what’s around the corner. Breaks give you the power to go faster because you can react to obstacles.</p><p>If you have automated tests, you’ll refactor the code more because you know it’s safe to do it. If you don’t have them, you’ll not feel comfortable changing the code and try to work around as much as you can, usually making things worse.</p><p>Sometimes, refactoring the code is necessary so you can add some tests. Sometimes also, you don’t know how to test this code but you still have to change it before tomorrow. In these cases, overcommit will bring you enough safety so you can work faster!</p><p>Finally, the time you’d have saved would be wisely re-invested in writing the automated tests you’re missing!</p><p><strong>Don’t forget to grab your Overcommitting Comic!</strong></p></div></div>]]>
            </description>
            <link>https://understandlegacycode.com/blog/work-faster-safer-untested-code-overcommitting/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25037561</guid>
            <pubDate>Mon, 09 Nov 2020 18:01:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[This Month in Rust GameDev #15 – October 2020]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25037471">thread link</a>) | @ozkriff
<br/>
November 9, 2020 | https://rust-gamedev.github.io/posts/newsletter-015 | <a href="https://web.archive.org/web/*/https://rust-gamedev.github.io/posts/newsletter-015">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
            
    <article itemscope="" itemtype="http://schema.org/BlogPosting">
        

        <div itemprop="articleBody">
            <p>Welcome to the 15th issue of the Rust GameDev Workgroup's
monthly newsletter.
<a href="https://rust-lang.org/">Rust</a> is a systems language pursuing the trifecta:
safety, concurrency, and speed.
These goals are well-aligned with game development.
We hope to build an inviting ecosystem for anyone wishing
to use Rust in their development process!
Want to get involved? <a href="https://github.com/rust-gamedev/wg#join-the-fun">Join the Rust GameDev working group!</a></p>
<p>You can follow the newsletter creation process
by watching <a href="https://github.com/rust-gamedev/rust-gamedev.github.io/issues?q=label%3Acoordination">the coordination issues</a>.
Want something mentioned in the next newsletter?
<a href="https://github.com/rust-gamedev/rust-gamedev.github.io">Send us a pull request</a>.
Feel free to send PRs about your own projects!</p>
<p>Table of contents:</p>
<ul>
<li><a href="https://rust-gamedev.github.io/posts/newsletter-015/#annual-survey-from-the-rust-gamedev-wg">Annual Survey from the Rust GameDev WG</a></li>
<li><a href="https://rust-gamedev.github.io/posts/newsletter-015/#game-updates">Game Updates</a></li>
<li><a href="https://rust-gamedev.github.io/posts/newsletter-015/#learning-material-updates">Learning Material Updates</a></li>
<li><a href="https://rust-gamedev.github.io/posts/newsletter-015/#library-tooling-updates">Library &amp; Tooling Updates</a></li>
<li><a href="https://rust-gamedev.github.io/posts/newsletter-015/#popular-workgroup-issues-in-github">Popular Workgroup Issues in Github</a></li>
<li><a href="https://rust-gamedev.github.io/posts/newsletter-015/#requests-for-contribution">Requests for Contribution</a></li>
</ul>
<!--
Ideal section structure is:

```
### [Title]

![image/GIF description](image link)
_image caption_

A paragraph or two with a summary and [useful links].

_Discussions:
[/r/rust](https://reddit.com/r/rust/todo),
[twitter](https://twitter.com/todo/status/123456)_

[Title]: https://first.link
[useful links]: https://other.link
```

If needed, a section can be split into subsections with a "------" delimiter.
-->
<h2 id="annual-survey-from-the-rust-gamedev-wg"><a href="https://surveymonkey.com/r/F2JYRFF">Annual Survey from the Rust GameDev WG</a>&nbsp;
</h2>
<p>As we did <a href="https://rust-gamedev.github.io/posts/survey-01">last year</a>, we are once again running
a Rust Game Development Ecosystem Survey. It'll only take 10 minutes,
and your responses help us better understand the state of our ecosystem
and where we should try to focus our collective efforts.</p>
<h2 id="game-updates">Game Updates&nbsp;
</h2>
<h3 id="veloren"><a href="https://veloren.net/">Veloren</a>&nbsp;
</h3>
<p><img src="https://rust-gamedev.github.io/posts/newsletter-015/veloren_clouds.jpeg" alt="Landscape">
<em>Endless mountains to explore</em></p>
<p><a href="https://veloren.net/">Veloren</a> is an open world, open-source voxel RPG inspired by Dwarf
Fortress and Cube World.</p>
<p>In October, lots of work was done on the UI, and a buff system. There was an
overhaul done to the staff item that gives it new primary and secondary attacks.
There has also been work done on the axe and bow. The cloud system was
overhauled and brought a cheaper way to compute the 3D noise that the system uses.
The skill bar was overhauled to implement a new design that could handle the new
buff system. This was also the first overhaul in over a year. A SFX system is in
the works to allow effects to be mapped to blocks, for sounds like crickets or
birds.</p>
<p>You can read more about some specific topics from October:</p>
<ul>
<li><a href="https://veloren.net/devblog-88#gemu">Modelling Process</a></li>
<li><a href="https://veloren.net/devblog-89#staff-overhaul-by-sam">Staff Overhaul</a></li>
<li><a href="https://veloren.net/devblog-89#new-skillbar-and-buffs-visuals-pfau">New Skillbar and Buffs Visuals</a></li>
<li><a href="https://veloren.net/devblog-90#cloud-improvements-by-zesterer">Cloud Improvements</a></li>
<li><a href="https://veloren.net/devblog-91#buffs-by-sam">Buffs</a></li>
<li><a href="https://veloren.net/devblog-91#alignment-and-hostility-by-adam">Alignment and Hostility</a></li>
<li><a href="https://veloren.net/devblog-91#fixing-ci-by-xmac94x">Fixing CI</a></li>
</ul>
<p>October's full weekly devlogs: "This Week In Veloren...":
<a href="https://veloren.net/devblog-88">#88</a>,
<a href="https://veloren.net/devblog-89">#89</a>,
<a href="https://veloren.net/devblog-90">#90</a>,
<a href="https://veloren.net/devblog-91">#91</a>.</p>
<p>In November, Veloren will release 0.8. Veloren will also be speaking at
MiniDebConf on November 22nd.</p>
<p><img src="https://rust-gamedev.github.io/posts/newsletter-015/veloren_wolves.jpeg" alt="Healing sceptre">
<em>Team hunt</em></p>
<h3 id="crate-before-attack"><a href="https://cratebeforeattack.com/">Crate Before Attack</a>&nbsp;
</h3>
<p><a href="https://cratebeforeattack.com/"><img src="https://rust-gamedev.github.io/posts/newsletter-015/crate-before-attack.gif" alt="Leaderboard Histogram"></a>
<em>Interactive Histogram with Race Results</em></p>
<p><a href="https://cratebeforeattack.com/">Crate Before Attack</a> by <a href="https://twitter.com/CrateAttack">koalefant (@CrateAttack)</a>
is a skill-based multiplayer game where frogs fight and race using their sticky
tongues as grappling hooks.</p>
<p>A <a href="https://cratebeforeattack.com/play">browser build</a> can be played online.</p>
<p>Changes since the last update:</p>
<ul>
<li>Added a global leaderboard that visualizes Race and Training results in an
interactive histogram.</li>
<li>Tweaked frogs physics to make them more bouncy, added an option that would
keep tongue connected as long as a key is being pressed.</li>
<li><a href="https://youtu.be/j87I8akUTkc">Online Ghosts</a> were added. One can now compete with real
players instead of AI when playing Race mode.</li>
<li>Improved load-times: level graphics is now quantized with an 8-bit palette,
signed distance fields that are used for collisions are now generated offline.
Downloads are cached in an IndexedDB, so subsequent starts are even faster.</li>
<li>Multiple bugs were fixed.</li>
</ul>
<p>More details are in <a href="https://cratebeforeattack.com/posts/20201001-september-update">September</a> and
<a href="https://cratebeforeattack.com/posts/20201029-october-update">October</a> DevLog entries and in
<a href="https://www.youtube.com/channel/UC_xMilPTLuuE5iLs1Ml9zow">YouTube-channel</a>.</p>
<h3 id="egregoria"><a href="https://github.com/Uriopass/Egregoria">Egregoria</a>&nbsp;
</h3>
<p><img src="https://rust-gamedev.github.io/posts/newsletter-015/egregoria.jpg" alt="Egregoria roads at night"></p>
<p><a href="https://github.com/Uriopass/Egregoria">Egregoria</a>'s objective is to become a granular society simulation,
filled with fully autonomous agents interacting with their world in real-time.</p>
<p>The <a href="http://douady.paris/blog/egregoria_6.html">6th devlog</a> was published. Updates include:</p>
<ul>
<li>Island generation.</li>
<li>Day/night cycle.</li>
<li>Human AI via utility systems.</li>
<li>Specs to <a href="https://github.com/amethyst/legion">legion 0.3</a> port.</li>
</ul>
<p>See also <a href="https://www.youtube.com/watch?v=mfvAuvC-XLg">the recent video</a> showcasing very basic AI.</p>
<p>Join <a href="https://discord.gg/CAaZhUJ">Egregoria's Discord server</a>.</p>
<p><em>Discussions:
<a href="https://reddit.com/r/rust_gamedev/comments/jkcllc/egregoria_devblog_6">/r/rust_gamedev</a></em></p>
<h3 id="a-b-street"><a href="https://abstreet.org/">A/B Street</a>&nbsp;
</h3>
<p><img src="https://rust-gamedev.github.io/posts/newsletter-015/abstreet.png" alt="A/B Street on the web"></p>
<p><a href="https://abstreet.org/">A/B Street</a> is a traffic simulation game exploring how small changes
to roads affect cyclists, transit users, pedestrians, and drivers. Any city
with OpenStreetMap coverage can be used!</p>
<p>Some of this month's updates:</p>
<ul>
<li><a href="http://abstreet.s3-website.us-east-2.amazonaws.com/dev/">web version</a> launched, powered by <code>winit</code>, <code>glow</code>, and other
dependencies having support for WebAssembly;</li>
<li>an <a href="http://abstreet.s3-website.us-east-2.amazonaws.com/osm_demo/">OpenStreetMap viewer</a> with 100 cities imported;</li>
<li>"thought bubbles" for cars looking for parking, by <a href="https://github.com/michaelkirk">Michael</a>;</li>
<li>slow portions of a trip highlighted in the info panel, by <a href="https://github.com/NoSuchThingAsRandom/">Sam</a>;</li>
</ul>
<h3 id="worship-the-sun">Worship The Sun&nbsp;
</h3>
<p><img src="https://rust-gamedev.github.io/posts/newsletter-015/worship-sun.jpg" alt="Worship The Sun">
<em>One of the many unique and confounding puzzles in the game</em></p>
<p>Worship The Sun is a dark, mysterious 2D puzzle-platform game with computer
science themes. It introduces the player to a rich language of puzzle elements
and challenges them to solve difficult puzzles that require experimentation,
comprehension, and internalisation of the game's mechanics.</p>
<p>The game is built using a custom engine that sits on top of <a href="https://github.com/amethyst/legion">legion</a>,
<a href="https://github.com/gfx-rs/wgpu">wgpu</a>, and a handful of other crates. It features dynamic lighting, a
flexible particle system, bespoke collision behaviour, and a Vim-inspired level
editor. The majority of game assets are hand drawn in <a href="https://procreate.art/">Procreate</a>
and painstakingly animated.</p>
<p>The game is a few months into development with a release target of late 2021.
You can read about how swimming was added to the game in <a href="https://tuzz.tech/blog/taking-the-plunge">GameDev Note 1:
Taking the Plunge</a> which contains a sneak peek at some of the levels.
For updates and possible playtesting opportunities, follow
<a href="https://twitter.com/chrispatuzzo">@chrispatuzzo</a> and a <a href="https://reddit.com/r/WorshipTheSunGame">/r/WorshipTheSunGame</a> subreddit.</p>
<h3 id="garden"><a href="https://www.cyberplant.xyz/">Garden</a>&nbsp;
</h3>
<p><img src="https://rust-gamedev.github.io/posts/newsletter-015/garden_scaled_1.png" alt="Garden">
<em>A couple of trees growing and basking in the sun</em></p>
<p><a href="https://www.cyberplant.xyz/">Garden</a> is a procedural tree-growing, strategical ecosystem-restoration
and biological simulation game with an infinite amount of plant species where
every leaf is simulated, and the natural resources are scarce.
Every specimen is unique, as the plants grow by responding to the live changes in
the environment.
The player has to balance many complex mechanics to sustain life and go
forward in the game.
The game and the custom engine are developed in Rust with an OpenGL backend.</p>
<p>Garden developers (temporary name) are preparing for a demo release in a couple
of months by tying everything together into a coherent experience.
The game is also continually optimized to run on less powerful GPUs,
so that everyone can enjoy it.</p>
<p>Some of the <a href="https://cyberplant.xyz/posts/october_2020">updates from the October devlog</a>:</p>
<ul>
<li>Near-infinite variety of plant species
achieved through treating branch segments as Markov chains
(enabling different growth speeds and probabilities
for other segment types' growth from one another)
and simulating photosynthesis as an electrical circuit
(enabling sugar storage in the form of root vegetables, for example).</li>
<li>Concrete brick destruction mechanics were implemented.
Dust particles for the animation that appears upon breaking,
as well as the debris, were also added to the game.</li>
<li>Saving and loading are almost complete.</li>
<li>A watering can was added.</li>
<li>Smoother soil and debris outlines.</li>
</ul>
<p>Follow the developers <a href="https://twitter.com/logicsoup">@logicsoup</a> and <a href="https://twitter.com/epcc10">@epcc10</a> on Twitter for more updates.</p>
<h3 id="akigi"><a href="https://akigi.com/">Akigi</a>&nbsp;
</h3>
<p><a href="https://akigi.com/">Akigi</a> is a WIP online multiplayer game.</p>
<p>In October, more progress was made on the editor tool for placing entity spawn
points. Work was started on prototyping the hunting skill. Functionality was
added to allow focusing for TextAreas in the user interface. Improvements were
made to the engine's asset management code to make it more generalized.</p>
<p>Full devlogs:
<a href="https://devjournal.akigi.com/october-2020/087-2020-10-04.html">#087</a>,
<a href="https://devjournal.akigi.com/october-2020/088-2020-10-11.html">#088</a>,
<a href="https://devjournal.akigi.com/october-2020/089-2020-10-18.html">#089</a>,
<a href="https://devjournal.akigi.com/october-2020/090-2020-10-25.html">#090</a>.</p>
<h3 id="sun-prison"><a href="https://github.com/ropewalker/sun_prison">Sun Prison</a>&nbsp;
</h3>
<p><img src="https://rust-gamedev.github.io/posts/newsletter-015/sun_prison.gif" alt="Sun Prison gameplay"></p>
<p><a href="https://github.com/ropewalker/sun_prison">Sun Prison</a> by <a href="https://twitter.com/dmitrywithouti">Dima Lazarev</a> is a WIP turn-based
meditation on Rubik's cube, <a href="https://github.com/ropewalker/bevy_sokoban">Sokoban</a>, and roguelikes, being
implemented with <a href="https://bevyengine.org/">Bevy engine</a>.
The game is in the very early stages of development,
but it is already possible to <a href="https://twitter.com/dmitrywithouti/status/1309025584039768064">get lost in the dark</a>
or to be <a href="https://twitter.com/dmitrywithouti/status/1309982656260648960">eaten by zombies</a>.</p>
<p>Follow <a href="https://twitter.com/dmitrywithouti">@dmitrywithouti</a> on Twitter for updates.</p>
<h3 id="camp-misty"><a href="https://github.com/ReeCocho/camp-misty">Camp Misty</a>&nbsp;
</h3>
<p><img src="https://rust-gamedev.github.io/posts/newsletter-015/camp-misty.PNG" alt="Camp Misty Title Screen"></p>
<p><a href="https://github.com/ReeCocho/camp-misty">Camp Misty</a> is an asymmetric
multiplayer game played on the command line. The game is played with two
people. One of the players is a helpless victim searching for car parts. If
they find all of the parts, they can repair their car and escape the camp. The
other player is a ruthless killer who is trying to hunt down the victim.</p>
<p>The game was created as a learning exercise in about two weeks by
<a href="https://github.com/ReeCocho">@ReeCocho</a>, with contributions from the many helpful members of <a href="https://reddit.com/r/rust">/r/rust</a>.</p>
<h3 id="antorum-online"><a href="https://ratwizard.dev/dev-log/antorum">Antorum Online</a>&nbsp;
</h3>
<p><img src="https://rust-gamedev.github.io/posts/newsletter-015/antorum-online-10-29-2020.jpg" alt="A small marketplace area with a few merchants"></p>
<p>Antorum Online is a micro-multiplayer online role-playing game by <a href="https://twitter.com/dooskington">@dooskington</a>.
The game server is written in Rust, and the current "official" client is being
developed in Unity. The server can be self-hosted, and the network protocol is
open, so even custom clients that adhere to the protocol can connect and play.</p>
<p>Two more devlogs were published this month, regarding work done to implement
shops, character creation, and a few other features:</p>
<ul>
<li><a href="https://ratwizard.dev/dev-log/antorum/21">21: Belmart, Shops, And Bartering</a></li>
<li><a href="https://ratwizard.dev/dev-log/antorum/22">22: Character Creation And Customization</a></li>
</ul>
<h3 id="the-honor-sagas"><a href="https://khonsulabs.itch.io/honorsagas">The Honor Sagas</a>&nbsp;
</h3>
<p><img src="https://rust-gamedev.github.io/posts/newsletter-015/honor.png" alt="game's banner"></p>
<p><a href="https://khonsulabs.itch.io/honorsagas">The Honor Sagas</a> is an early-in-development 2d MMORPG project.
October was the first month of development, and <a href="https://twitter.com/ectonDev">@ectonDev</a> wrote
<a href="https://khonsulabs.itch.io/honorsagas/devlog/192252/the-honor-sagas-devtober-postmortem">a postmortem</a> of the progress made while participating
in <a href="https://itch.io/jam/devtober-2020">#Devtober</a>.</p>
<h3 id="project-yawc">Project YAWC&nbsp;
</h3>
<p><img src="https://rust-gamedev.github.io/posts/newsletter-015/yawc.png" alt="An in-progress game of Project YAWC."></p>
<p>Project YAWC is a turn-based strategy game in the style of Advance Wars in
development by junkmail. October saw the release of Alpha 3, including
dynamically generated info cards and minor networking changes. For inquiries or
if you are interested in playtesting, contact projectyawc(at)gmail.com.</p>
<h3 id="power-kick"><a href="https://kakoeimon.itch.io/power-kick">Power Kick</a>&nbsp;
</h3>
<p><img src="https://rust-gamedev.github.io/posts/newsletter-015/power-kick-shot.png" alt="Power Kick"></p>
<p><a href="https://kakoeimon.itch.io/power-kick">Power Kick</a> is a one screen platform game inspired by similar old arcade games
like Bubble Bobble and SnowBros.
Your task is to hit the enemies till they get dizzy and then kick them out of
their misery to proceed to the next stage. The kicked enemies will hit the
colliding enemies with a possibility to create a chain reaction
(similar to the pushed snowball in SnowBros).</p>
<p>The game has 20 stages and in stage 10 and 20 you will face a helicopter boss.</p>
<p>Can be played solo on the web through WebAssembly or up to two players
in the downloadable version:
the first player with the keyboard and the second one with a joypad.</p>
<p>The development took around two weeks thanks to <a href="https://github.com/not-fl3/macroquad">macroquad</a> and <a href="https://crates.io/crates/hecs">hecs</a>.</p>
<h3 id="rymd"><a href="https://profan.itch.io/rymd">rymd</a>&nbsp;
</h3>
<p><img src="https://rust-gamedev.github.io/posts/newsletter-015/rymd_2020_11_05.gif" alt="rymd animated combat"></p>
<p><a href="https://profan.itch.io/rymd">rymd</a> by <a href="https://twitter.com/_profan">@_profan</a> is a space shooter prototype made with <a href="https://github.com/not-fl3/macroquad">macroquad</a>.
Intended as a test platform for trying out rust for prototyping games and
particularly for game AI programming purposes.</p>
<p>Development started at the end of October, recent additions include:</p>
<ul>
<li>Basic enemy AI behaviour mostly based on steering behaviours.</li>
<li>Possibly the world's most nauseating physics-driven camera.</li>
<li>Too many particles.</li>
</ul>
<h3 id="pglowrpg"><a href="https://github.com/roalyr/pglowrpg">pGLOWrpg</a>&nbsp;
</h3>
<p><img src="https://rust-gamedev.github.io/posts/newsletter-015/pglowrpg.gif" alt="walking through a forest"></p>
<p>The <a href="https://twitter.com/pglowrpg">@pGLOWrpg</a> (Procedurally Generated Living …</p></div></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://rust-gamedev.github.io/posts/newsletter-015">https://rust-gamedev.github.io/posts/newsletter-015</a></em></p>]]>
            </description>
            <link>https://rust-gamedev.github.io/posts/newsletter-015</link>
            <guid isPermaLink="false">hacker-news-small-sites-25037471</guid>
            <pubDate>Mon, 09 Nov 2020 17:54:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Manager Advice: Prepare to Be Scrutinized]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25037454">thread link</a>) | @hackitup7
<br/>
November 9, 2020 | https://staysaasy.com/management/2020/09/18/management-scrutiny.html | <a href="https://web.archive.org/web/*/https://staysaasy.com/management/2020/09/18/management-scrutiny.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>One of the weird things that people don’t tell you about management is the degree to which people will sometimes scrutinize your behavior.</p>

<p>This is completely opposite to how most human interactions work. Generally speaking people overestimate how much attention others are paying to them. It’s easy to be embarrassed if your voice cracks while meeting someone new, if you say something silly in front of your in-laws, or if you have spaghetti sauce on your shirt. In most cases nobody notices because their own life is being pumped into their brain in stereo.</p>

<p>But this isn’t always true. People actually <em>do</em> scrutinize the behavior of people who have some measure of control over their careers (such as the CEO) carefully, and in some cases will even document what they say for a rainy day. This pattern becomes more extreme as titles and organizations grow, to the point where the job of a Fortune 500 CEO almost overlaps with Broadway theater.</p>

<p>The reasons for this are obvious. People aren’t paying attention out of any sort of expectation of wisdom – but when someone has an impact on your job, it’s rational and reasonable to analyze them to help your career. However, I find that few new managers are aware of this phenomenon, which can have far-ranging consequences.</p>

<h2 id="the-awkward-lunch">The Awkward Lunch</h2>

<p>I remember the first time that I learned this lesson. I was eating lunch with my team as we lightly discussed our upcoming customer conference, and made a quick, dismissive offhand comment about how I felt that one of our marketing teams, led by Steve, was going in circles and wasting time on trivial details. This was 100% wrong – you should never “otherize” other teams – but I didn’t say anything particularly incendiary or critical. Nobody made any mention and I certainly forgot what I had said almost immediately.</p>

<p>Two weeks later, a member of my team offered out of the blue to handle an upcoming planning meeting with Steve’s group. I was confused – why was she making this offer? “Well, you mentioned that you thought that Steve had a tendency to waste time, and I know that you’re presenting at that conference in a week so you’re probably busy. I figured I could help cover a meeting with a team you don’t like.”</p>

<p>A team I don’t like? Where did that come from?! I dimly recalled the lunch in which I had made that comment, and realized that the offhand comment which I had immediately deleted from my mind had stuck with my lunchmates. With a growing sense of dread, I also realized that versions of this situation had probably been happening without my realizing it.</p>

<h2 id="managing-under-a-microscope">Managing Under a Microscope</h2>

<p>Advice that I had received earlier in my career, especially after becoming a new manager: assume that someone might remember everything you say or do. For extra fun, also expect that about 10% of the juiciest details that get remembered will be at least somewhat inaccurate.</p>

<p>Since it can be jarring to realize that your team is watching you a little bit more closely than you might like, I try to convey the general advice below to first time managers on my team:</p>

<ul>
  <li>Treat everything that you say like an email – assume that it may live on in someone’s memory for a long time after you say it.</li>
  <li>Be intentional in how you operate or communicate, as some people (particularly your team) may emulate some of your patterns. If you are really harsh when you communicate with other teams, they’ll tend to be harsher as well. If you’re extremely accommodating, they’ll also tend to be more accommodating.</li>
  <li>Build the mental muscle of being less reactive to stressful situations. Try not to visibly frown or look skeptical unless you really mean to express that emotion as people will likely pick up on it. This is true even if you’re stressed because you’re thinking about how your corgi Mr. Snuffles is sick, and your bad mood has nothing to do with work.</li>
  <li>Be extra vigilant not to say things that could be misconstrued as disrespectful or offensive.</li>
  <li>Don’t get wasted at your company’s holiday party, or at other types of events. While I find that most people will give you a pass, it’s really easy to start rumors after hours.</li>
</ul>

<p>There is no way to stop people from scrutinizing – it’s human nature mixed with rational behavior. But if you’re intentional in your actions, you should at least be able to avoid putting your foot in your mouth by saying something dumb at lunch.</p>

    




  </div></div>]]>
            </description>
            <link>https://staysaasy.com/management/2020/09/18/management-scrutiny.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25037454</guid>
            <pubDate>Mon, 09 Nov 2020 17:52:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I Git]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25037365">thread link</a>) | @jerodsanto
<br/>
November 9, 2020 | https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/ | <a href="https://web.archive.org/web/*/https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>Every now and then I get questions on how to work with git in a smooth way when developing, bug-fixing or extending curl – or how I do it. After all, I <a href="https://daniel.haxx.se/blog/2020/10/26/working-open-source/" data-type="post" data-id="14901">work on open source full time</a> which means I have very frequent interactions with git (and GitHub). Simply put, I work with git all day long. Ordinary days, I issue git commands several hundred times.</p>



<p>I have a very simple approach and way of working with git in curl. This is how it works.</p>



<h2>command line</h2>



<p>I use git almost exclusively from the command line in a terminal. To help me see which branch I’m working in, I have this little bash helper script.</p>



<pre>brname () {
  a=$(<code>git rev-parse --abbrev-ref HEAD 2&gt;/dev/null</code>)
  if [ -n "$a" ]; then
    echo " [$a]"
  else
    echo ""
  fi
}
PS1="\u@\h:\w\$(brname)$ "</pre>



<p>That gives me a prompt that shows username, host name, the current working directory and the current checked out git branch.</p>



<p>In addition: I use Debian’s <a href="https://salsa.debian.org/debian/bash-completion/-/blob/master/README.md">bash command line completion</a> for git which is also really handy. It allows me to use tab to complete things like git commands and branch names. </p>



<h2>git config</h2>



<p>I of course also have my customized <code>~/.gitconfig</code> file to provide me with some convenient aliases and settings. My most commonly used git aliases are:</p>


<pre title="">st = status --short -uno
ci = commit
ca = commit --amend
caa = commit -a --amend
br = branch
co = checkout
df = diff
lg = log -p --pretty=fuller --abbrev-commit
lgg = log --pretty=fuller --abbrev-commit --stat
up = pull --rebase
latest = log @^{/RELEASE-NOTES:.synced}..
</pre>


<p>The ‘latest’ one is for listing all changes done to curl since the most recent RELEASE-NOTES “sync”. The others should hopefully be rather self-explanatory.</p>



<p>The config also sets <code>gpgsign = true</code>, enables mailmap and a few other things.</p>



<h2>master is clean and working</h2>



<p>The main curl development is done in the single <a href="https://github.com/curl/curl">curl/curl</a> git repository (primarily hosted on GitHub). We keep the master branch the bleeding edge development tree and we work hard to always keep that working and functional. We do our releases off the master branch when that day comes (every eight weeks) and we provide “<a href="https://curl.haxx.se/snapshots/">daily snapshots</a>” from that branch, put together – yeah – daily.</p>



<p>When merging fixes and features into master, we avoid merge commits and use rebases and fast-forward as much as possible. This makes the branch very easy to browse, understand and work with – as it is 100% linear.</p>



<h2>Work on a fix or feature</h2>



<p>When I start something new, like work on a bug or trying out someone’s patch or similar, I first create a local branch off master and work in that. That is, I don’t work directly in the master branch. Branches are easy and quick to do and there’s no reason to shy away from having loads of them!</p>



<p>I typically name the branch prefixed with my GitHub user name, so that when I push them to the server it is noticeable who is the creator (and I can use the same branch name locally as I do remotely).</p>



<pre>$ git checkout -b bagder/my-new-stuff-or-bugfix</pre>



<p>Once I’ve reached somewhere, I commit to the branch. It can then end up one or more commits before I consider myself “done for now” with what I was set out to do.</p>



<p>I try not to leave the tree with any uncommitted changes – like if I take off for the day or even just leave for food or an extended break. This puts the repository in a state that allows me to easily switch over to another branch  when I get back – should I feel the need to. Plus, it’s better to commit and explain the change <em>before</em> the break rather than having to recall the details again when coming back.</p>



<h2>Never stash</h2>



<p>“git stash” is therefore not a command I ever use. I rather create a new branch and commit the (temporary?) work in there as a potential new line of work.</p>



<h2>Show it off and get reviews</h2>



<p>Yes I am the lead developer of the project but I still maintain the same work flow as everyone else. All changes, except the most minuscule ones, are done as pull requests on GitHub.</p>



<p>When I’m happy with the functionality in my local branch. When the bug seems to be fixed or the feature seems to be doing what it’s supposed to do and the test suite runs fine locally.</p>



<p>I then clean up the commit series with “<code>git rebase -i</code>” (or if it is a single commit I can instead use just “<code>git commit --amend</code>“).</p>



<p>The commit series should be a set of logical changes that are related to this change and not any more than necessary, but kept separate if they are separate. Each commit also gets its own proper commit message. Unrelated changes should be split out into its own separate branch and subsequent separate pull request.</p>



<pre>git push origin bagder/my-new-stuff-or-bugfix</pre>



<h2>Make the push a pull request</h2>



<p>On GitHub, I then make the newly pushed branch into a <a href="https://github.com/curl/curl/pulls">pull request</a> (aka “a PR”). It will then become visible in the list of pull requests on the site for the curl source repository, it will be announced in the #curl IRC channel and everyone who follows the repository on GitHub will be notified accordingly.</p>



<p>Perhaps most importantly, a pull request kicks of a flood of CI jobs that will build and test the code in numerous different combinations and on several platforms, and the results of those tests will trickle in over the coming hours. When I write this, we have around 90 different CI jobs – per pull request – and something like 8 different code analyzers will scrutinize the change to see if there’s any obvious flaws in there.</p>



<figure><a href="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/Screenshot_2020-11-05-curl-Project-status-dashboard.png"><img loading="lazy" width="2686" height="1510" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/Screenshot_2020-11-05-curl-Project-status-dashboard.png" alt=""></a><figcaption>CI jobs per platform over time. Graph snapped on November 5, 2020</figcaption></figure>



<h2>A branch in the actual curl/curl repo</h2>



<p>Most contributors who would work on curl would not do like me and make the branch in the curl repository itself, but would rather do them in their own forked version instead. The difference isn’t that big and I <em>could</em> of course also do it that way.</p>



<h2>After push, switch branch</h2>



<p>As it will take some time to get the full CI results from the PR to come in (generally a few hours), I switch over to the next branch with work on my agenda. On a normal work-day I can easily move over ten different branches, polish them and submit updates in their respective pull-requests.</p>



<p>I can go back to the&nbsp;master branch again with ‘<code>git checkout master</code>‘ and there I can “<code>git pull</code>” to get everything from upstream – like when my fellow developers have pushed stuff in the mean time.</p>



<h2>PR comments or CI alerts</h2>



<p>If a reviewer or a CI job find a mistake in one of my PRs, that becomes visible on GitHub and I get to work to handle it. To either fix the bug or discuss with the reviewer what the better approach might be.</p>



<p>Unfortunately, flaky CI jobs is a part of life so very often there ends up one or two red markers in the list of CI jobs that can be ignored as the test failures in them are there due to problems in the setup and not because of actual mistakes in the PR…</p>



<p>To get back to my branch for that PR again, I “<code>git checkout bagder/my-new-stuff-or-bugfix</code>“, and fix the issues.</p>



<p>I normally start out by doing follow-up commits that repair the immediate mistake and push them on the branch:</p>



<pre>git push origin <code>bagder/my-new-stuff-or-bugfix</code></pre>



<p>If the number of fixup commits gets large, or if the follow-up fixes aren’t small, I usually end up doing a squash to reduce the number of commits into a smaller, simpler set, and then force-push them to the branch.</p>



<p>The reason for that is to make the patch series easy to review, read and understand. When a commit series has too many commits that changes the previous commits, it becomes hard to review.</p>



<h2>Ripe to merge?</h2>



<p>When the pull request is ripe for merging (independently of who authored it), I switch over to the master branch again and I merge the pull request’s commits into it. In special cases I cherry-pick specific commits from the branch instead. When all the stuff has been yanked into master properly that should be there, I push the changes to the remote.</p>



<p>Usually, and especially if the pull request wasn’t done by me, I also go over the commit messages and polish them somewhat before I push everything. Commit messages should follow our style and mention not only which PR that it closes but also which issue it fixes and properly give credit to the bug reporter and all the helpers – using the right syntax so that our automatic tools can pick them up correctly!</p>



<p>As already mentioned above, I merge fast-forward or rebased into master. No merge commits.</p>



<h2>Never merge with GitHub!</h2>



<p>There’s a button GitHub that says “rebase and merge” that could theoretically be used for merging pull requests. I <em>never</em> use that (and if I could, I’d disable/hide it). The reasons are simply:</p>



<ol><li>I don’t feel that I have the proper control of the commit message(s)</li><li>I can’t select to squash a subset of the commits, only all or nothing</li><li>I often want to cleanup the author parts too before push, which the UI doesn’t allow</li></ol>



<p>The downside with not using the merge button is that the message in the  PR says “closed by [hash]” instead of “merged in…” which causes confusion to a fair amount of users who don’t realize it means that it actually means the same thing! I consider this is a (long-standing) GitHub UX flaw.</p>



<h2>Post merge</h2>



<p>If the branch has nothing to be kept around more, I delete the local branch again with “<code>git branch -d [name]</code>” and I remove it remotely too since it was completely merged there’s no reason to keep the work version left.</p>



<p>At any given point in time, I have some 20-30 different local branches alive using this approach so things I work on over time all live in their own branches and also submissions from various people that haven’t been merged into master yet exist in branches of various maturity levels. Out of those local branches, the number of concurrent pull requests I have in progress can be somewhere between just a few up to ten, twelve something.</p>



<h2>RELEASE-NOTES</h2>



<p>Not strictly related, but in order to keep interested people informed about what’s happening in the tree, we sync the <a href="https://github.com/curl/curl/blob/master/RELEASE-NOTES">RELEASE-NOTES</a> file every once in a while. Maybe every 5-7 days or so. It …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/">https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/</a></em></p>]]>
            </description>
            <link>https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25037365</guid>
            <pubDate>Mon, 09 Nov 2020 17:46:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Lox: A word that hasn't changed sound or meaning in 8k years]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25037325">thread link</a>) | @fanf2
<br/>
November 9, 2020 | http://m.nautil.us/blog/-the-english-word-that-hasnt-changed-in-sound-or-meaning-in-8000-years | <a href="https://web.archive.org/web/*/http://m.nautil.us/blog/-the-english-word-that-hasnt-changed-in-sound-or-meaning-in-8000-years">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			

			<figure data-alt=""><img src="http://static.nautil.us/16021_16b2399ccd1419de9e098d7abf025eb6.jpg" width="733" alt=""><figcaption><span><i>The word</i> lox <i>was one of the clues that eventually led linguists to discover who the Proto-Indo-Europeans were, and where they lived.</i></span><span>Photograph by Helen Cook / Flickr</span></figcaption></figure><p><span>O</span>ne of my favorite words is <i>lox</i>,” says Gregory Guy, a professor of linguistics at New York University. There is hardly a more quintessential New York food than a lox bagel—a century-old popular appetizing store, Russ &amp; Daughters, <a href="https://www.eater.com/2014/6/30/6201785/the-classic-bagel-and-salmon-sandwich-at-russ-daughters-in-new-york" target="_blank">calls</a> it “The Classic.” But Guy, who has lived in the city for the past 18 years, is passionate about lox for a different reason. “The pronunciation in the Proto-Indo-European was probably ‘lox,’ and that’s exactly how it is pronounced in modern English,” he says. “Then, it meant salmon, and now it specifically means ‘smoked salmon.’ It’s really cool that that word hasn’t changed its pronunciation at all in 8,000 years and still refers to a particular fish.”</p><p>How scholars have traced the word’s pronunciation over thousands of years is also really cool. The story goes back to Thomas Young, also known as “The Last Person Who Knew Everything.” The 18th-century British polymath came up with the wave theory of light, first described astigmatism, and played a key role in deciphering the Rosetta Stone. Like some people before him, Young noticed eerie similarities between Indic and European languages. He went further, analyzing 400 languages spread across continents and millennia and proved that the overlap between some of them was too extensive to be an accident. A single coincidence meant nothing, but each additional one increased the chance of an underlying connection. In 1813, Young declared that all those languages belong to one family. He named it “Indo-European.”</p><p>Today, roughly half the world’s population speaks an Indo-European language. That family includes 440 languages spoken across the globe, including English. The word <i>yoga</i>, for example, which comes from Sanskrit, the language of ancient India, is a distant relative of the English word <i>yoke</i>. The nature of this relationship puzzled historical linguists for two centuries.</p><p>In modern English, well over half of all words are borrowed from other languages. To trace how language changes over time, linguists developed an ingenious toolkit. “Some parts of vocabulary are more stable and don’t change as much. The linguistic term [for these words] is ‘a core vocabulary.’ These are numbers, colors, family relations like ‘mother,’ ‘father,’ ‘sister,’ ‘brother,’ and basic verbs like ‘walk’ and ‘see,’ says Guy. “If you look at words of that sort in different languages, it becomes fairly clear which ones are related and which ones are not. For example, take the English word for number <i>two</i>, which is <i>dva</i> in Russian and <i>deux</i> in French, or the word <i>night</i>, which is <i>nacht</i> in German and <i>noch</i> in Russian.”</p><blockquote><p>“The sounds that change across time are unpredictable, and differ from language to language, and some may not happen to change at all.”</p> </blockquote><p>Analyzing the patterns of change that words undergo, moving from one language to another, showed how to unwind these changes and identify the possible originals. “Reconstructed vocabulary of Indo-European is based on a comparison of descendant languages,” explains Guy. “You collect words that mean more or less the same thing in all the languages, and if they look like each other in terms of their pronunciation, then it’s a good candidate for a descendant from a common ancestor.” The English word <i>honey</i> is <i>madhu</i> in Sanskrit and <i>myod</i> in Russian. Sanskrit and Russian haven’t shared a common ancestor since Indo-European, so these words had to come from the same source. (There are also the words <i>mead</i> in English, <i>met</i> in German and <i>mjød</i> in Danish that refer to an alcoholic drink made from honey.)<br></p><p>After discovering a word that might have existed in the Indo-European, linguists compared how its pronunciations changed from language to language. For example, sound [k] changes to [h] from Latin to Germanic, and the Latin word <i>casa</i> transforms into the English <i>house </i>while&nbsp;the French word <i>cœur</i> transforms into the English <i>heart</i>.*&nbsp;With hints like that, linguists could undo the sound changes and trace the original pronunciation. In several thousand years, most words change beyond recognition, like the word <i>wheel</i>, which initially might have sounded “kʷékʷlos.” But there were some remarkable exceptions—like the timeless <i>lox</i>.</p><p>The <a href="https://upload.wikimedia.org/wikipedia/commons/4/4f/IndoEuropeanTree.svg" target="_blank">family tree</a> of the Indo-European languages sprawls across Eurasia, including such different species as English and Tocharian B, an extinct language once spoken on the territory of Xinjiang in modern China. In Tocharian B, the word for “fish/salmon” is <i>laks</i>, similar to German <i>lachs</i>, and Icelandic <i>lax</i>—the only ancestor all these languages share is the Proto-Indo-European. In Russian, Czech, Croatian, Macedonian, and Latvian, the [k] sound changed to [s,] resulting in the word <i>losos</i>. </p><div id="inpagesub">
	<p>Get the <span>Nautilus</span> newsletter</p>
<p>
	The newest and most popular articles delivered right to your inbox!
</p>
			<!-- Begin MailChimp Signup Form -->
			




</div><p>This kind of millennia-long semantic consistency also appears in other words. For example, the Indo-European <i>porkos</i>, similar to modern English <i>pork</i>, meant a young pig. “What is interesting about the word <i>lox</i> is that it simply happened to consist of sounds that didn’t undergo changes in English and several other daughter languages descended from Proto-Indo-European,” says Guy. “The sounds that change across time are unpredictable, and differ from language to language, and some may not happen to change at all.”</p><p>The word <i>lox</i> was one of the clues that eventually led linguists to discover who the Proto-Indo-Europeans were, and where they lived. The fact that those distantly related Indo-European languages had almost the same pronunciation of a single word meant that the word—and the concept behind it—had most likely existed in the Proto-Indo-European language. “If they had a word for it, they must have lived in a place where there was salmon,” explains Guy. “Salmon is a fish that lives in the ocean, reproduces in fresh water and swims up to rivers to lay eggs and mate. There are only a few places on the planet where that happens.”</p><p>In reconstructed Indo-European, there were words for <i>bear</i>, <i>honey</i>, <i>oak tree</i>, and <i>snow</i>, and, which is also important, no words for <i>palm tree</i>, <i>elephant</i>, <i>lion</i>, or <i>zebra</i>. Based on evidence like that, linguists reconstructed what their homeland was. The only possible geographic location turned out to be in a narrow band between Eastern Europe and the Black Sea where animals, trees, and insects matched the ancient Indo-European words.</p><p>In the 1950s, archaeological discoveries backed up this theory with remnants of an ancient culture that existed in that region from 6,000 to 8,000 years ago. Those people used to build kurgans, burial mountains, that archaeologists excavated to study cultural remains. In that process, scholars not only learned more about the Proto-Indo-Europeans but also why they were able to migrate across Europe and Asia.</p><p>In turned out that, in the past, the grassy plains of steppe that run from Western China to the Black Sea had large herds of wild horses. Early humans hunted them for food, but the Proto-Indo-Europeans were probably the first people who <a href="https://www.pnas.org/content/pnas/109/21/8202.full.pdf" target="_blank">domesticated</a> the ancestors of modern-day domestic horses. That brought them an enormous advantage, allowing them to move a lot faster than any other human group. Then, they adopted—or, less likely, invented—wheeled vehicles and attached these to horses. “That’s probably the moment when they suddenly managed to expand into the Middle East, into India, and across Europe,” says Guy. “Within the next thousands of years, they expanded like no other human group that we know about in history. Because [now] they had mobility, which nobody else had.”</p><p>In his book <i>The Power of Babel</i>, Columbia University linguist John McWhorter wrote, “Everything about a language is eternally and inherently changeable, not just the slang and the occasional cultural destination, but the very sound and meaning of basic words, and the word order and grammar.” It’s nice to know, though, that some words never change—<i>lox</i> being one of the most surprising.</p><p><i>Sevindj Nurkiyazova is a journalist from Kyrgyzstan. Follow her on Medium&nbsp;<a href="https://medium.com/@calempir" target="_blank">@calempir</a>.</i></p><p><i>*This sentence was changed so as not to imply that Germanic languages descend from Latin ones.</i></p><p><i>This classic Facts So Romantic&nbsp;post was originally&nbsp;published in May 2019.</i></p>

		</div></div>]]>
            </description>
            <link>http://m.nautil.us/blog/-the-english-word-that-hasnt-changed-in-sound-or-meaning-in-8000-years</link>
            <guid isPermaLink="false">hacker-news-small-sites-25037325</guid>
            <pubDate>Mon, 09 Nov 2020 17:43:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why Rust Is the Future of Game Development]]>
            </title>
            <description>
<![CDATA[
Score 152 | Comments 224 (<a href="https://news.ycombinator.com/item?id=25037147">thread link</a>) | @adamnemecek
<br/>
November 9, 2020 | https://thefuntastic.com/blog/why-rust-is-the-future-game-dev? | <a href="https://web.archive.org/web/*/https://thefuntastic.com/blog/why-rust-is-the-future-game-dev?">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-v-10b1bd0a="" data-v-2ae295f5=""><p><em>Rust, not related to the video game also called Rust, is a promising systems programming language with novel features ideally suited for game development. Exposure and awareness within the game developer community, however, remains limited. In this post, I provide a gentle introduction to Rust and attempt to justify its place on your radar.</em></p>
<h2 id="a-short-history-lesson">A Short History Lesson</h2>
<p>What is Rust, and where did it come from? In <a href="https://www.youtube.com/watch?v=HiWkMFE8uRE" target="_blank" rel="nofollow noopener noreferrer">this fantastic talk</a>, James Munns gives us a detailed oral history. Way back around 2010, Mozilla was frustrated by the state of development in Firefox, a massive software project  written mostly in C++. Despite best practices and an abundance of engineering talent, writing high-performance, parallelised, and memory-safe code, at that scale of complexity, remained fraught and error-prone.</p>
<p>Bear in mind, this predates the advent of C++11 (aka the 2011 edition) which heralded efforts to somewhat modernise the language. Even so, manual memory manipulation is easy to get wrong, and <a href="https://msrc-blog.microsoft.com/2019/07/18/we-need-a-safer-systems-programming-language/" target="_blank" rel="nofollow noopener noreferrer">research</a> from <a href="https://www.zdnet.com/article/chrome-70-of-all-security-bugs-are-memory-safety-issues/" target="_blank" rel="nofollow noopener noreferrer">multiple vendors</a> describes this category of error as responsible for 70% of security vulnerabilities. </p>
<p>Into this context steps Graydon Hoare, a Mozilla employee, introducing <a href="https://en.wikipedia.org/wiki/Rust_(programming_language)#History" target="_blank" rel="nofollow noopener noreferrer">a potential solution</a> to the roadblock: Rust, the hobby language he'd been tinkering with since 2006. In 2012, Mozilla would formally announce Servo, an experimental research project to re-imagine a browser engine built with memory safety and concurrency as first principles. And alongside it, Rust, the companion language to make it all possible.</p>
<p>These early days of Rust are described as a Cambrian explosion of ideas and wild experimentation. Concepts were liberally stolen from other languages, from C++ to OCaml, Haskell, Erlang, ML, C#, Ruby and more, reflecting the diverse pool of engineers working on the language at the time. Still, most in the industry, while admiring the optimism in taking such an ambitious moon shot, <a href="http://dtrace.org/blogs/bmc/2018/09/18/falling-in-love-with-rust/" target="_blank" rel="nofollow noopener noreferrer">remained pessimistic</a> about the prospects of success. </p>
<p>2015 saw a major milestone, with the release of Rust v1.0. Perhaps as significant as the feature list, was the number of failed experiments left behind on the cutting room floor, the team unafraid to pare down the language to its quintessential elements. This was also the first time stability guarantees would be offered, a quality notoriously absent before. </p>
<p>Soon after, in 2016, Firefox <a href="https://hacks.mozilla.org/2016/07/shipping-rust-in-firefox/" target="_blank" rel="nofollow noopener noreferrer">shipped its first production Rust code</a>. The industry and community started to take notice, and Rust began its impressive, and as yet unbroken, <del>four</del> <a href="https://stackoverflow.blog/2020/01/20/what-is-rust-and-why-is-it-so-popular/" target="_blank" rel="nofollow noopener noreferrer">five year streak</a> as Stack Overflow's most beloved language. [<em>Thank you James Munns for pointing out it's now actually five years</em>]. </p>
<p>Right from the outset, Rust set out with a clear focus on   building an inclusive community. They, in turn, have contributed to Rust's impressive technical aptitude, but have also fostered a sense of reverence and fondness not often witnessed in other languages. Are they crazy zealots or onto something?</p>
<h2 id="why-rust">Why Rust?</h2>
<blockquote>
<p><em>The performance of C++ with the convenience of C#</em></p>
</blockquote>
<p>This was the first time Rust hijacked my attention. C++ enjoys a long-standing ubiquity, in part, due to its ability to express zero cost abstractions. As explained by Bjarne Stroustrup, the creator of C++:</p>
<blockquote>
<p><em>What you don't use, you don't pay for. And further: What you do use, you couldn't hand code any better.</em></p>
</blockquote>
<p>It's easy to spot the relevancy to games. Making frame-rate while simulating entire worlds is a daunting performance challenge. Indeed, C++ underpins the bulk of game engines. There simply is <a href="https://www.youtube.com/watch?v=ltCgzYcpFUI" target="_blank" rel="nofollow noopener noreferrer">no other industrial language</a> that offers the same speed and low-level control, whilst writing programs in the large. </p>
<p>C++, however, suffers from the weight of its legacy. The accumulation of features over 40 years makes for a complex and intricate language. In the last decade modernisation of the standard has done well to uplift it from its C roots, but the experienced programmer must build up an arcane lore of which features are blessed, and which machinery is dangerous. As Stroustrup again describes:  </p>
<blockquote>
<p>Within C++, there is a much smaller and cleaner language struggling to get out.   </p>
</blockquote>
<p>This makes the language daunting and difficult to approach for beginners. In <a href="https://boats.gitlab.io/blog/post/zero-cost-abstractions/" target="_blank" rel="nofollow noopener noreferrer">this blog post</a>, Rust contributor <em>withoutboats</em> defines an import quality about abstraction:  </p>
<blockquote>
<p>A zero cost abstraction, like all abstractions, must actually offer a better experience than the alternative.</p>
</blockquote>
<p>So yes, of course, C++ offers a better time than your own hand wrought assembly. However, this is making the subtle point that it's competing against a secondary force: a more expensive abstraction that justifies its cost by being more comfortable and convenient.</p>
<p>We see this writ large in the rise of popular game engines that eschew the complexity of C++, the most notable being Unity. End users write code in C#, a more forgiving and ergonomic language, creating a boon in developer productivity and a reduction in iteration time.  </p>
<p><img src="https://thefuntastic.com/blog/2020-10-Unity-Interest.png" title="Unity interest over time in search trends"></p>
<p>In large codebases though, near the edge of the performance envelope, this trade-off begins to bite. The garbage collector eliminates an entire category of errors by removing responsibility for memory management from the end-user. However as its workload grows, so do periodic performance spikes antithetical to smooth gameplay.  </p>
<p><img src="https://thefuntastic.com/blog/2020-11-GC-spikes.png" title="Unity interest over time in search trends"></p>
<p>The experienced developer can still create a performant experience, however, this demands plugging the leaks in the abstraction. They must build a mental model of the machinery behind the curtain, a collection of arcane wisdom that bans many of the original conveniences, lest they disturb the garbage collector. </p>
<p>So development teams face a choice. Better resourced AAA studios generally choose Unreal or in-house engine tech built on C++, able to absorb the overhead for long term gain. Less resourced studios optimise for time to market, choosing Unity, or one of the many other accessible game making tools (Godot, Haxe, Game Maker, etc.). They often postpone performance concerns until after business eligibility is secured.   </p>
<p>Rust, however, for the first time, promises a third way. A world where it's possible to write zero cost abstractions without sacrificing higher-order elegance. </p>
<h3 id="ownership-based-memory">Ownership based memory</h3>
<p>To understand Rust's special sauce, we're going have to talk about ownership and how it handles memory. This is only a simple sketch, but <a href="http://intorust.com/tutorial/ownership/" target="_blank" rel="nofollow noopener noreferrer">in-depth resources</a> exist for the curious. </p>
<p>Writing optimised code is often about taking the way we, as humans, naturally think of an idea or algorithm, and instead expressing it in terms that favour the computer. This act often harms the legibility and understanding of a program, which makes it much harder for us, the humans, to reason about its correctness. </p>
<p>In a manually managed language, like C, the hapless programmer is left responsible for the machinations of the machine. They must take great care to ensure data is appropriately loaded into memory before operation, and then responsibly disposed of afterwards. A difficult dance in which missteps either cause dramatic crashes or else subtle and hard to detect vulnerabilities. But these are the very same tools that allow careful users to tune performance.  </p>
<p>At the other end of the spectrum, garbage collection promises the programmer it will automatically deal with the problem on their behalf. They are now free to express code naturally, but in doing so, it ties hands behind their back. They no longer have, at least not without indirection, the levers needed to wring out maximal performance.</p>
<p>Rust begins from a different premise. Rather than hiding this complexity, it accepts that computers are hard for humans, and instead tries to save us from the dangerous bits. Users can still tune the machine, but with less rope to wrap around their necks. </p>
<p>In the same way that static typing exists, very clever people have figured out how to make the compiler eliminate a whole category of memory and concurrency errors. To achieve this, Rust makes a bargain with the developer: </p>
<blockquote>
<p>"I'm going to keep track of the lifetime of every piece of memory in your program for you. This way, I can detect the moment you're no longer using it and safely free it on your behalf. But in return, I'm going to need you to follow <a href="https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html" target="_blank" rel="nofollow noopener noreferrer">strict rules</a> about the ownership of that memory. If you try to use it outside of the scope that owns it, my humourless friend here, the borrow checker, is going to make sure you don't hurt yourself."</p>
</blockquote>
<p>However, like static typing, this lunch isn't free. Rust is known to have a steep learning curve, "fighting the borrow checker" becomes a right of passage. It takes time to learn this new paradigm. Ownership makes some familiar patterns difficult or impossible and demands new ones be learnt in their place. Perhaps we should revise our earlier statement as: "The performance of C++ with the <del>convenience</del> safety of C#"</p>
<h2 id="unpacking-rusts-popularity">Unpacking Rust's Popularity</h2>
<p>Early adopters have a selfish reason to extol the virtues of their chosen technology, as widespread adoption enhances the return on their risky investment. In this respect, while interest is high but opportunities for real-world exposure are limited, is it possible that Rust is cresting a wave of unearned hype? <a href="https://matklad.github.io/2020/09/20/why-not-rust.html" target="_blank" rel="nofollow noopener noreferrer">Not every javascript or python developer</a> interested in the language, for example, has a use case that merits the additional complexity.</p>
<p>To a developer standing on the shores of 2010, <code>git</code>, a new version control system with a steep learning curve, may have seemed like a risky investment. But, in the ensuing world of Github, it's hard to argue the effort was wasted, even if some workloads (i.e. large games) still require alternatives.</p>
<p>In a similar vein, how can we qualify Rust's popularity as a meaningful signal? Ultimately, we will only know by the volume of mud we've dug through in the trenches, and admittedly, it is far too early to collect this data for games. </p>
<p>In other industries, though, early reports of Rust are effusive. <a href="https://medium.com/the-innovation/how-microsoft-is-adopting-rust-e0f8816566ba" target="_blank" rel="nofollow noopener noreferrer">Mircosoft</a>, <a href="https://developers.libra.org/docs/community/coding-guidelines" target="_blank" rel="nofollow noopener noreferrer">Facebook</a>, <a href="https://aws.amazon.com/blogs/opensource/aws-sponsorship-of-the-rust-project/" target="_blank" rel="nofollow noopener noreferrer">Amazon</a>, <a href="https://www.wired.com/2016/03/epic-story-dropboxs-exodus-amazon-cloud-empire/" target="_blank" rel="nofollow noopener noreferrer">Dropbox</a>, <a href="https://blog.cloudflare.com/tag/rust/" target="_blank" rel="nofollow noopener noreferrer">Cloudflare</a> all have Rust deployed in production. The <a href="https://www.phoronix.com/scan.php?page=news_item&amp;px=Linux-Kernel-Rust-Path-LPC2020" target="_blank" rel="nofollow noopener noreferrer">Linux Kernel</a> and <a href="https://www.chromium.org/Home/chromium-security/memory-safety/rust-and-c-interoperability" target="_blank" rel="nofollow noopener noreferrer">Chrom…</a></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://thefuntastic.com/blog/why-rust-is-the-future-game-dev?">https://thefuntastic.com/blog/why-rust-is-the-future-game-dev?</a></em></p>]]>
            </description>
            <link>https://thefuntastic.com/blog/why-rust-is-the-future-game-dev?</link>
            <guid isPermaLink="false">hacker-news-small-sites-25037147</guid>
            <pubDate>Mon, 09 Nov 2020 17:27:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Peeking Inside the Black Box: Explaining Artificial Intelligence Models]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25037136">thread link</a>) | @asafg6
<br/>
November 9, 2020 | https://www.turtle-techies.com/peeking-inside-the-black-box/ | <a href="https://web.archive.org/web/*/https://www.turtle-techies.com/peeking-inside-the-black-box/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h4>Explaining Artificial Intelligence models</h4><p><h5>2020-11-08</h5></p></div><div><div itemprop="articleBody"><h2 id="a-brief-introduction">A Brief Introduction <a href="#a-brief-introduction"><svg height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>We live in the age of data and new technologies.
More and more, Artificial Intelligence is coming to our lives, from the image processing after a picture is shot in our phones to the recommendation algorithm in most content places.
And the number of companies that want to introduce some AI process to their workflow increases by the minute.</p><p>Soon, AI systems will be diagnosing illnesses, granting mortgages, etc.
But then doubts arise.
A lot of those AI systems will be black box systems, most likely Neural Networks or Ensembles.
Basically a lot of automatic learned equations and parameters that are going to tell everybody if they are fit to buy a house, or if they have this or that illness.<br>But can we really trust these systems? They have been proven wrong in the past, showing incredible and unexpected
<a href="https://metro.co.uk/2017/07/13/racist-soap-dispensers-dont-work-for-black-people-6775909/" target="_blank">biases</a>.</p><p>Here is why the trend is moving towards making AI fair and understandable.
There are great initiatives like
<a href="https://www.fast.ai/" target="_blank">fast.ai</a> that focus on unbiased AI, but here we are going to use some of the latest framework to <strong>explain existing models</strong>.<br>Whether you have an AI pipeline in your company or you are learning how to use the latest Neural Network models, this tutorial will explain you a bit more about what is going on inside that process.</p><h2 id="anchor">Anchor <a href="#anchor"><svg height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>There are plenty of papers researching XAI (eXplainable Artificial Intelligence), but there are not so many frameworks that can currently take an existing model and explain its internal processes.
This kind of <em>post-hoc explanation</em> is very interesting because it does not have to reimplement and retrain the existing pipelines, but rather can work as a new addition to those pipelines.</p><p>In this article we will explain
<a href="https://github.com/marcotcr/anchor" target="_blank">Anchor</a>, a state-of-the-art library programmed in Python which has proven efficiency and is considered the rival to beat when developing new algorithms.
Anchor is an open-source library that learns a model-agnostic model (this is, it works with any kind of machine learning algorithm).
This model generates a set of rules (or anchors, hence the name) that classify and explain each particular example. Anchor can work with a variety of data, from text to images or tabular.
In this article, we will explain how to use it on a tabular database.</p><h2 id="installing-anchor">Installing Anchor <a href="#installing-anchor"><svg height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>To install Anchor, we will need Python 3.7 or greater. To install the package from
<a href="https://pypi.org/" target="_blank">PyPI</a> just type:</p><p>Or clone their repository and install the package:</p><div><pre><code data-lang="Bash">
git clone https://github.com/marcotcr/anchor.git
python setup.py install

</code></pre></div><h2 id="installing-additional-elements">Installing Additional Elements <a href="#installing-additional-elements"><svg height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>First step will be to choose our database and to train our model. In this article we will use the very well known
<a href="https://archive.ics.uci.edu/ml/datasets/iris" target="_blank">iris database</a> that we can find packaged inside
<a href="https://scikit-learn.org/stable/" target="_blank">Scikit Learn</a>.</p><p>This is a flower database with 150 registers of different iris flowers. Each register measures both sepal and petal lengths in centimeters, and the target class is the type of iris (setosa, versicolor or virginica).</p><p>To install Scikit-Learn type in the terminal:</p><p>To train a model, we will use one of the best Python libraries out there:
<a href="https://xgboost.readthedocs.io/en/latest/" target="_blank">XGBoost</a>.</p><p>This will train a tree ensemble that grants a great accuracy.
Again, to install it type:</p><h2 id="training-our-model">Training our model <a href="#training-our-model"><svg height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>Now we are ready to train our model.
The first step will be to import everything we need:</p><div><pre><code data-lang="Python">
<span>import</span> xgboost <span>as</span> xgb

<span>from</span> sklearn <span>import</span> datasets
<span>from</span> sklearn.model_selection <span>import</span> train_test_split

<span>from</span> anchor <span>import</span> anchor_tabular

</code></pre></div><p>Here we have imported:</p><ul><li>The library to train our model (xgboost)</li><li>The datasets</li><li>A function to automatically generate the train and the test sets to properly validate the model</li><li>Anchor</li></ul><p>Next step will be to load the database:</p><div><pre><code data-lang="Python">
iris <span>=</span> datasets<span>.</span>load_iris()

X_train, X_test, y_train, y_test <span>=</span> train_test_split(iris<span>.</span>data, iris<span>.</span>target, test_size<span>=</span><span>0.3</span>, random_state<span>=</span><span>42</span>)

</code></pre></div><p>In this step, we load the iris dataset from the scikit package.<br>In this format, <code>iris.data</code> contains a series of <code>numpy</code> arrays with the features of the 150 registers, and <code>iris.target</code> contains an array of the prediction (as integers).
Then, we split the data in train and test sets.
This way, we have a random 70% of the database that will be used to train the model, and the 30% remaining that we will use to test the score.
We will also use it later to get some registers unknown to the model that we will be able to classify and explain.</p><p>Now it is time to train the model:</p><div><pre><code data-lang="Python">
xgb_model <span>=</span> xgb<span>.</span>XGBClassifier(random_state<span>=</span><span>42</span>)
xgb_model<span>.</span>fit(X_train, y_train)

</code></pre></div><p>The library <code>XGBoost</code> implements a
<a href="https://en.wikipedia.org/wiki/Gradient_boosting" target="_blank">Gradient Boosting</a> algorithm.<br>What it does is to generate an ensemble of weak models that combine to generate a strong classifier.</p><p>This is typically done by training very shallow decision trees with random sets of data so that they are different from each other.
When they combine by
<a href="https://en.wikipedia.org/wiki/Boosting_%28machine_learning%29" target="_blank">boosting</a> they generate a very powerful model.</p><p>This is a classification problem, so we use a Classifier (not a Regressor).</p><blockquote><p>For this database we can leave the default parameters as is.<br>For other databases, you might need to fine-tune the model.
You can get all you need from the
<a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html" target="_blank">docs</a>.</p></blockquote><p>The model is trained just with the train data. Let’s see how it performs:</p><div><pre><code data-lang="Python">
<span>print</span>(xgb_model<span>.</span>score(X_test, y_test)) <span># 0.98</span>

</code></pre></div><p>If everything is correct, the score should be <code>0.98</code> or higher, given the randomness of some parts of the algorithm.</p><p>Impressive, but this will not tell us <strong>why a particular register is classified in some way</strong>.<br>For this, we will need to apply Anchor.</p><h2 id="applying-anchor-to-our-model">Applying Anchor to our model <a href="#applying-anchor-to-our-model"><svg height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>The first step will be to create the <code>explainer</code>.
This is the element that will take the model and explain the registers:</p><div><pre><code data-lang="Python">
explainer <span>=</span> anchor_tabular<span>.</span>AnchorTabularExplainer(class_names<span>=</span>iris<span>.</span>target_names,
                                                    feature_names<span>=</span>iris<span>.</span>feature_names,
                                                    train_data<span>=</span>X_train)

</code></pre></div><p>The <code>explainer</code> takes three parameters:</p><ul><li>The name of each feature</li><li>The name of each class value</li><li>The same data we have used to train the model.</li></ul><p>Note that we are not giving any data from our test set, the explainer does not use that to train.
This way, the explainer doesn’t know the test data, just like the model.</p><p>Let’s take the register with index 30 (at random) from out test data.<br>Pass it through our model:</p><div><pre><code data-lang="Python">
idx <span>=</span> <span>30</span>
np<span>.</span>random<span>.</span>seed(<span>1</span>)
<span>print</span>(<span>'Prediction: '</span>, explainer<span>.</span>class_names[xgb_model<span>.</span>predict(X_test[idx]<span>.</span>reshape(<span>1</span>, <span>-</span><span>1</span>))[<span>0</span>]])

</code></pre></div><pre><code>
Prediction:  setosa

</code></pre><p>Our <code>xgb_model</code> predicts that it will be a <code>setosa</code> output, but why?</p><p>Let’s generate the explanation:</p><div><pre><code data-lang="Python">
exp <span>=</span> explainer<span>.</span>explain_instance(X_test[idx], xgb_model<span>.</span>predict, threshold<span>=</span><span>0.95</span>)

</code></pre></div><p>Here is where the magic happens.<br>We pass the explainer the register we want to explain and the <code>predict</code> function from our model.
This is the good thing about Anchor: as long as the <code>predict</code> function can take a numpy array and return an integer as a prediction, it will work with absolutely any model out there! Even with custom ones you can program.
Note that there is a <code>threshold</code> parameter.
This means that the predictions for the explanation it has generated will hold at least 95% of the time.
But how to show that explanation?</p><p>With this code:</p><div><pre><code data-lang="Python">
<span>print</span>(<span>'Anchor: </span><span>%s</span><span>'</span> <span>%</span> (<span>' AND '</span><span>.</span>join(exp<span>.</span>names())))
<span>print</span>(<span>'Precision: </span><span>%.2f</span><span>'</span> <span>%</span> exp<span>.</span>precision())
<span>print</span>(<span>'Coverage: </span><span>%.2f</span><span>'</span> <span>%</span> exp<span>.</span>coverage())

</code></pre></div><pre><code>
Anchor: 1.70 &lt; petal length (cm) &lt;= 5.10 AND petal width (cm) &lt;= 1.30 AND sepal length (cm) &gt; 5.80
Precision: 1.00
Coverage: 0.06

</code></pre><p>Here we are.
Now we know that our register is a <code>setosa</code> because its petal length is smaller than 1.7 and so on.
We also know that 100% of the registers that match this rule are <code>setosa</code> and that 6% of the training registers match this rule.
With this, we have not only explained what is happening in the model, but also how confident we are in these rules.</p><h2 id="what-is-an-anchor">What is an Anchor <a href="#what-is-an-anchor"><svg height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>All right, so now we have a rule or <em>anchor</em> that can explain the registry.<br>But how are they computed? Well, the deep mathematics of the anchor are a bit complicated, but the gist of it is as follows.<br>First, the system determines a probability for a certain <em>anchor</em> to get its precision.<br>Then, using a
<a href="https://en.wikipedia.org/wiki/Greedy_algorithm" target="_blank">greedy algorithm</a>, it starts adding features, values and splits (such as [<code>petal length (cm)</code>, <code>1.7</code>, <code>&lt;</code>]) and computing the best next addition.<br>The system keeps searching the best <em>anchor</em> according to the precision threshold, so that it returns the <strong>shortest</strong> anchor for the specified threshold.
It is not very explainable if the rule has 70 features, is it?<br>The nitty gritty part of the search is more complex, but if you want to have a look feel free to check the original
<a href="https://homes.cs.washington.edu/~marcotcr/aaai18.pdf" target="_blank">article</a>, where everything is explained.</p><h2 id="closing-words">Closing words <a href="#closing-words"><svg height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>In these days where AI is taking over more and more functions and processes, it is very easy to just let it decide for us without knowing why.
But there are aspects in life too important to just blindly trust a machine.
With Anchor, we have a library that is easy to add to any machine learning pipeline where the trust of each answer is as important as the answer itself.</p></div></div></div>]]>
            </description>
            <link>https://www.turtle-techies.com/peeking-inside-the-black-box/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25037136</guid>
            <pubDate>Mon, 09 Nov 2020 17:27:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[18th Century England Had No Police]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25037076">thread link</a>) | @willbobaggins
<br/>
November 9, 2020 | https://narrativespodcast.com/2020/11/09/narratives-podcast-episode-15-economics-law-and-the-future-with-david-friedman/ | <a href="https://web.archive.org/web/*/https://narrativespodcast.com/2020/11/09/narratives-podcast-episode-15-economics-law-and-the-future-with-david-friedman/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1924">

                    
                    <div>
                        
<p>This week on the podcast, we have David Friedman. David holds a PhD in physics from the University of Chicago, he is chiefly known for his scholarly contributions to economics and law. He is the author of five books of non‐​fiction as well as three novels. We discuss the future, legal systems very different from our own, how technology drives progress, and what the future might look like.&nbsp;</p>



<figure></figure>
                                            </div>

                </article></div>]]>
            </description>
            <link>https://narrativespodcast.com/2020/11/09/narratives-podcast-episode-15-economics-law-and-the-future-with-david-friedman/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25037076</guid>
            <pubDate>Mon, 09 Nov 2020 17:21:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Canadian government pledges to connect 98% of Canadians via High-Speed Internet]]>
            </title>
            <description>
<![CDATA[
Score 96 | Comments 80 (<a href="https://news.ycombinator.com/item?id=25037074">thread link</a>) | @aDfbrtVt
<br/>
November 9, 2020 | https://www.cbc.ca/news/politics/broadband-internet-1.5794901 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/politics/broadband-internet-1.5794901">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>The Liberal government is promising to spend more than a billion dollars to connect most Canadian to high-speed internet by 2026.</p><div><figure><div><p><img loading="lazy" alt="" srcset="" sizes="" src="https://i.cbc.ca/1.4962390.1546282434!/cpImage/httpImage/image.jpg_gen/derivatives/16x9_780/broadband-expansion.jpg"></p></div><figcaption>The Liberal government has been promising to do something to approve broadband internet service in rural areas.<!-- --> <!-- -->(Toby Talbot/Associated Press)</figcaption></figure><p><span><p>After some pandemic-related delays, the Liberal government says it's now&nbsp;on track to connect 98 per cent of Canadians to high-speed internet by 2026.</p>  <p>The announcement comes as more Canadians find themselves living online while stuck at home due to COVID-19 restrictions.</p>  <p>Prime Minister Justin Trudeau and a handful of cabinet ministers&nbsp;held a news conference in Ottawa to launch the $1.75 billion universal broadband fund — a program unveiled in the federal government's 2019 budget and highlighted on the campaign trail and in&nbsp;September's throne speech. Most of the money&nbsp;was&nbsp;announced in last year's budget.</p>  <p>"We were ready to go&nbsp;in March&nbsp;with the new Universal Broadband Fund and then the pandemic hit,"&nbsp;Rural Economic Development Minister Maryam Monsef told reporters.</p>  <p>The prime minister said the government is now on track to connect&nbsp;98 per cent of Canadians&nbsp;to high-speed&nbsp;by 2026 — an increase over&nbsp;the previously promised 95 per cent benchmark — and to link up&nbsp;the rest by 2030.</p>  <p>"These are ambitious targets&nbsp;and we're ready to meet them,"&nbsp;Trudeau said.</p>  <p><em><strong>WATCH |&nbsp;Trudeau announces a large investment in broadband services for rural Canadians</strong></em></p>  <p><span><span><div><div role="button" tabindex="0" title="Trudeau announces a large investment in broadband services for rural Canadians"><div><p><img src="https://thumbnails.cbc.ca/maven_legacy/thumbnails/573/747/ftr%20TRUDEAU%20broadband_frame_0.jpg" alt=""></p></div></div></div><span>Prime Minister Justin Trudeau spoke with reporters during a media briefing in Ottawa on Monday.<!-- --> <!-- -->2:47</span></span></span></p>  <p>About&nbsp;$150 million from the fund will be freed up to fund projects aimed at getting communities connected by next fall.</p>  <p>Senior officials with the department of&nbsp;Innovation, Science and Economic Development&nbsp;said applications will be reviewed on an ongoing basis until Jan. 15, 2021, with a goal of having projects completed by mid-November, 2021.</p>  <p>Deciding who gets upgraded connectivity first will depend on the service providers applying, they said.</p>  <p>Josh Tabish is&nbsp;corporate communications manager at the Canadian Internet Registration Authority, the not-for-profit agency that manages the .ca internet domain. He said he's hoping&nbsp;that a&nbsp;rapid build will bring relief to many Canadians over the next year.</p>    <p>"In terms of action, I think&nbsp;this is great news for Canadians who are stuck at home suffering from slow, crappy internet," he said.&nbsp;</p>  <p>But Tabish also said he hopes the government will look at need when deciding which projects should get approval first.&nbsp;His group has been working to identify the&nbsp;communities that&nbsp;have the slowest&nbsp;rates in Canada.</p>  <p>"What we really want to see happen is communities who are suffering with slow, sluggish connectivity get those upgrades first," he said.</p>  <p>The prime minister said the government also&nbsp;has reached a $600 million agreement with Telesat for satellite capacity to improve broadband service in remote areas and in the North.</p>    <p>"Good reliable internet isn't a luxury. It's a basic service," he said.</p>  <p>"Now more than ever, a video chat cutting out during a meeting or a connection that's too slow to upload a school assignment — that's not just a hassle, that's a barrier."</p>  <h2>Tories call out timelines</h2>  <p>The Opposition Conservatives criticized the government's timelines, arguing Canadians need better access now more than ever.</p>  <p>"This is absolutely unacceptable and a slap in the face to the nearly one million Canadians who don't have internet access at home, much less a reliable cell phone signal," said MP John Nater, Conservative critic&nbsp;for rural economic development.</p>  <p>"For months, Canada's Conservatives have been demanding concrete action to connect Canadians.&nbsp;We will continue to advocate for lower cell phone prices and for real improvements to broadband internet services, so that Canadians living in rural and remote areas have consistent access to these essential services."</p>  <p>The&nbsp;CRTC <a href="https://www.cbc.ca/news/politics/crtc-internet-essential-service-1.3906664">declared</a> broadband internet a basic telecommunications service in 2016.&nbsp;But its data suggest&nbsp;just&nbsp;<a href="https://crtc.gc.ca/eng/internet/internet.htm">40.8 per cent of rural Canadian households have access to </a>download speeds of&nbsp;at least 50 megabits per second (Mbps) and upload speeds of&nbsp;10 Mbps.</p>  <p>The government said those speeds will allow Canadians to work and learn online and access telehealth services.</p>  <p><em><strong>WATCH | Rural Canadians react to today's announcement</strong></em></p>  <p><span><span><div><div role="button" tabindex="0" title="Liberals promise to connect 98% of Canadians by 2030"><div><p><img src="https://thumbnails.cbc.ca/maven_legacy/thumbnails/732/431/politics_THURTON_broadband_money_7000kbps_1280x720_1817544259879.jpg" alt=""></p></div></div></div><span>After some pandemic-related delays, the Liberal government says it's now on track to connect 98 per cent of Canadians to high-speed internet by 2026. The announcement comes as more Canadians find themselves living online while stuck at home due to COVID-19 restrictions.<!-- --> <!-- -->1:47</span></span></span></p>  </span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/politics/broadband-internet-1.5794901</link>
            <guid isPermaLink="false">hacker-news-small-sites-25037074</guid>
            <pubDate>Mon, 09 Nov 2020 17:21:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Converting Utzoo-Wiseman Usenet Tapes to PostgreSQL Back End Using Python]]>
            </title>
            <description>
<![CDATA[
Score 40 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25036780">thread link</a>) | @kxrm
<br/>
November 9, 2020 | https://www.joe0.com/2020/10/07/converting-utzoo-wiseman-netnews-archive-to-postgresql-using-python-3-8/ | <a href="https://web.archive.org/web/*/https://www.joe0.com/2020/10/07/converting-utzoo-wiseman-netnews-archive-to-postgresql-using-python-3-8/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><main id="content" role="main" itemprop="mainEntityOfPage" itemscope="itemscope" itemtype="http://schema.org/Blog"> <article id="post-4678" itemscope="itemscope" itemtype="http://schema.org/BlogPosting" itemprop="blogPost"><div> <!-- .entry-header --><div itemprop="articleBody"><p>Recently, I came across a resource that allowed me to download the entire collection of UTZOO NetNews Archive of the earliest USENET posts. These were essentially the earliest available discussions posted to the Internet by people working at various Universities who were already connected to the Internet. There were approximately 2.1 million posts in these archives created between Feb 1981 and June of 1991. This article describes the journey of converting those tapes into fully searchable PostgreSQL database and later also into the <a href="https://usenetarchives.com/groups.php?c=utzoo" target="_blank" rel="noopener noreferrer">usenetarchives.com</a> website.</p><p>Until 2001, these early Usenet discussions were considered being lost, but miraculously <a href="https://en.wikipedia.org/wiki/Henry_Spencer" target="_blank" rel="noopener noreferrer">Henry Spencer</a> from the University of Toronto, Department of Zoology was backing it up onto magnetic tapes and kept them stored for all these years (apparently at a great cost).</p><p><a href="https://www.joe0.com/wp-content/uploads/2019/02/2019-02-17_9-14-15.png"><img loading="lazy" src="https://www.joe0.com/wp-content/uploads/2019/02/2019-02-17_9-14-15.png" alt="" width="325" height="259" srcset="https://www.joe0.com/wp-content/uploads/2019/02/2019-02-17_9-14-15.png 1282w, https://www.joe0.com/wp-content/uploads/2019/02/2019-02-17_9-14-15-300x239.png 300w, https://www.joe0.com/wp-content/uploads/2019/02/2019-02-17_9-14-15-768x613.png 768w, https://www.joe0.com/wp-content/uploads/2019/02/2019-02-17_9-14-15-1024x817.png 1024w" sizes="(max-width: 325px) 100vw, 325px"></a>H. Spencer had altogether 141 of these magnetic tapes, but there were of no use, so eventually, him and a couple of motivated people such as David Wiseman (who dragged 141 tapes back and forth in his a pickup truck), Lance Bailey, Bruce Jones, Bob Webber, Brewster Kahle, and Sue Thielen; embarked on a process of converting all of these tapes into the regular format, accessible to everyone.</p><p>And that’s the copy I downloaded. What a treasure, right?</p><p>Well, not so fast, once I unzipped the data, I realized that the TGZ format contains literally millions of small text files (each post in its own file). While it was certainly nice to have, it wasn’t something that I or anyone else could read. Certainly not in a forum like discussion format. It wasn’t obvious which post is the one that starts the discussion or which ones are the replies to the thread. And forget about searching through these files, that was utterly not possible. Just to put things into perspective, it took me over 5 hours to un-tar the archives.</p><p>That said, it didn’t take long for me to decide to develop a Python-based converter that would allow me to convert the entire collection from millions of flat files into a fully searchable PostgreSQL database. The following post talks about the process and also includes the Python code of the solution released as open source.</p><p>The UTZOO Usenet archive can be downloaded here:</p><ul><li>http://www.skrenta.com/rt/utzoo-usenet/</li><li>http://shiftleft.com/mirrors/utzoo-usenet/</li><li>https://ipfs.io/ipfs/QmTo7fRxpXwxv6Uw4TAAtyLWEmvugKaggrHSKNBTRHzWcA/</li><li>Or using this torrent: <a href="https://www.joe0.com/wp-content/uploads/2020/10/utzoo-wiseman-usenet-archive_archive.zip">utzoo-wiseman-usenet-archive_archive</a></li></ul><p>Once downloaded you’ll see that archive contains 161 x TAR Archive files. It looks like this:</p><p><a href="https://www.joe0.com/wp-content/uploads/2019/02/img_5c69574363b87.png"><img loading="lazy" src="https://www.joe0.com/wp-content/uploads/2019/02/img_5c69574363b87.png" alt="" width="596" height="531" srcset="https://www.joe0.com/wp-content/uploads/2019/02/img_5c69574363b87.png 832w, https://www.joe0.com/wp-content/uploads/2019/02/img_5c69574363b87-300x268.png 300w, https://www.joe0.com/wp-content/uploads/2019/02/img_5c69574363b87-768x685.png 768w" sizes="(max-width: 596px) 100vw, 596px"></a></p><p>So, I grabbed a copy of the 7-Zip archiver from <a href="https://www.7-zip.org/">https://www.7-zip.org</a> and started decompressing the files.</p><p>I ended up with over <strong>2,104,828</strong>&nbsp;flat text files in <strong>56,988</strong> folders, which was the entire copy of Henry Spencer’s Usenet archive.</p><p>For those who like numbers, here is each Utzoo tape along with its size, number of files and folders:</p><p id="MLqhONH"><img loading="lazy" width="602" height="2294" src="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e6e422f3ee.png" alt="" srcset="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e6e422f3ee.png 602w, https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e6e422f3ee-79x300.png 79w, https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e6e422f3ee-269x1024.png 269w, https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e6e422f3ee-403x1536.png 403w" sizes="(max-width: 602px) 100vw, 602px"></p><h3>File Issues</h3><p>While examining the extract, I realized that Magnetic Tape 118 is uncompressed in \utzoo-wiseman-usenet-archive\news118f1 folder, named tape118, so I had rename it to tape118.tar and extracted it manually, only to realize it’s a copy of files which I already have. Someone creating the original archive forgotten to remove that file. There are 3 files in these folders that need to have.tar extension added and decompressed as well:</p><ul><li>\utzoo-wiseman-usenet-archive\news118f1\tape118</li><li>\utzoo-wiseman-usenet-archive\news120f1\tape120</li><li>\utzoo-wiseman-usenet-archive\news121f1\tape121</li></ul><p>If you opened one of the folders and navigated down to one of the many subfolders, you’d find a file that contained the message. For example, going into&nbsp;\utzoo-wiseman-usenet-archive\news006f1\b15\net\aviation folder, I was now apparently in the <strong>net.aviation</strong> Usenet group. But the only way to find out was to open one of the files and look at the content. Here I highlighted what it looked like.&nbsp;As you can see, each file seems to consist of a header, then a single empty line and the body of the message:</p><p id="RYYsysr"><a href="https://www.joe0.com/wp-content/uploads/2019/02/img_5c6967453ae2c.png"><img loading="lazy" src="https://www.joe0.com/wp-content/uploads/2019/02/img_5c6967453ae2c.png" alt="" width="1110" height="759" srcset="https://www.joe0.com/wp-content/uploads/2019/02/img_5c6967453ae2c.png 1110w, https://www.joe0.com/wp-content/uploads/2019/02/img_5c6967453ae2c-300x205.png 300w, https://www.joe0.com/wp-content/uploads/2019/02/img_5c6967453ae2c-768x525.png 768w, https://www.joe0.com/wp-content/uploads/2019/02/img_5c6967453ae2c-1024x700.png 1024w" sizes="(max-width: 1110px) 100vw, 1110px"></a></p><p>So, I decided to build a Python parser, that went through all these files reading the header portion of each message and grouping all unique results together, giving me all the possible headers such as (From, Subject, Newsgroup, etc.). I found that there were about 79 x different types of headers. So it appeared that not all messages adhered to the same basic structure. Going through the headers, all had the standard set that was common across all posts.</p><p>Once I had the common field, I’ve created a Postgres database called ‘utzoo’</p><pre>create database utzoo;</pre><p>And a new schema called all_messages</p><pre>create schema all_messages;


</pre><p>The above database and schema were the pre-requisites. Everything else, like table creation, inserting the posts, etc. is part of the Python script and fully automated.</p><p>In terms of table creation, the script automatically creates 5 tables for each detected newsgroup:</p><ul><li>headers – parsed headers</li><li>references – references for each message</li><li>body – text of the message</li><li>from – who posted the message</li><li>subjects – list of unique subject lines</li></ul><p>This is what the script auto-creates for each unique Group name:</p><pre>create table all_messages.<strong>GroupName_headers</strong>
(
    id         bigserial not null
        constraint GroupName_headers_pk primary key,
    dateparsed timestamp,
    subj_id    bigint,
    ref        smallint,
    msg_id     text,
    msg_from   bigint,
    enc        text,
    contype    text,
    processed  timestamp default CURRENT_TIMESTAMP
);
alter table all_messages.GroupName_headers
    owner to postgres;


create table all_messages.<strong>GroupName_refs</strong>
(
    id      bigint,
    ref_msg text default null
);
alter table all_messages.GroupName_refs
    owner to postgres;

create table all_messages.<strong>GroupName_body</strong>
(
    id   bigint primary key,
    data text default null
);
alter table all_messages.GroupName_body
    owner to postgres;

create table all_messages.<strong>GroupName_from</strong>
(
    id   serial not null
        constraint GroupName_from_pk primary key,
    data text
);
alter table all_messages.GroupName_from
    owner to postgres;

create table all_messages.<strong>GroupName_subjects</strong>
(
    id      serial not null
        constraint GroupName_subjects_pk primary key,
    subject text
);
alter table all_messages.GroupName_subjects
    owner to postgres;</pre><p>Those will be the tables where the Python parser will dump all the data and make sure posts are properly lined up between tables.</p><p>The python script also creates indexes to make the inserting and later reading of the posts faster:</p><pre>create unique index GroupName_headers_uiidx on all_messages.GroupName_headers(id);
create unique index GroupName_headers_umidx on all_messages.GroupName_headers(msg_id);
create unique index GroupName_body_idx on all_messages.GroupName_body(id);; 
create unique index GroupName_from_idx on all_messages.GroupName_from(data);
create unique index GroupName_subjects_idx on all_messages.GroupName_subjects(subject);

</pre><p>Once created, the structure per group looks like this:</p><p id="kdsmyQE"><img loading="lazy" width="362" height="703" src="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e3e7d98df4.png" alt="" srcset="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e3e7d98df4.png 362w, https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e3e7d98df4-154x300.png 154w" sizes="(max-width: 362px) 100vw, 362px"></p><p>The following screenshot explains how it’s all wired up. I didn’t do any hardcoded relationships, but you can change the script if you want that.</p><p id="THgecCD"><img loading="lazy" width="601" height="496" src="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e40111a6ef.png" alt="" srcset="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e40111a6ef.png 601w, https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e40111a6ef-300x248.png 300w" sizes="(max-width: 601px) 100vw, 601px"></p><p>The date is an integral part of each message and I had to do some data conversion massaging in Python to get the proper date, as dates were coming in a variety of formats. I’ve tried various libraries but dateutil.parser.parse standard date and time library for Python did the best job.</p><p>However, I still needed to account for various labelling of data fields in the headers, so if data wasn’t found in the ‘date’ header, I had to look into other header parts such as ‘NNTP-Posting-Date’, ‘X-Article-Creation-Date’,&nbsp;‘Posted’,&nbsp;or ‘Received’ fields.</p><p>Well and then it was all about creating a Python parser, start the PostgreSQL, point it to an archive directory, and wait :)</p><p>At the bottom of this article is the code of the Python solution. It’s about 1,000 lines, and it took altogether about 1 day to create and test it. The script is smart enough to keep the track of where it started, so if it needs to be interrupted, it’ll know where to continue from to get the job done.</p><p>The source code is available on GitHub as open-source under MIT license:</p><p><a href="https://github.com/JozefJarosciak/python_mbox_parser/blob/master/utzoo2postgres.py" target="_blank" rel="noopener noreferrer">https://github.com/JozefJarosciak/python_mbox_parser/blob/master/utzoo2postgres.py</a></p><p>The final solution artifact is called ‘<strong>utzoo2postgres.py</strong>‘ , and it was tested on Python 3.8.</p><p>Open the script and define the path to un-tared Utzoo archive directories.</p><p>Examples:</p><pre># for Windows
positionFilePath = "E:\\Usenet\\Utzoo\\"
# for linux:
# positionFilePath = "/Usenet/Utzoo/"</pre><p>Also, define the particulars of your PostgreSQL database:</p><pre>db_connection = psycopg2.connect(host="localhost", user="", password="", port="5432", database="utzoo")</pre><p>And then just execute the script!</p><pre>python 3 utzoo2postgres.py</pre><p><em>Note: In case you need to stop the program and run it later, the script is smart to resume from the last spot it was processing.</em></p><p>The script will process all Utzoo Archive messages in about 6 hours (depending on the speed of your machine).</p><p>Screenshot from processing:</p><p id="UwdOjId"><img loading="lazy" width="713" height="585" src="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7dc52b05827.png" alt="" srcset="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7dc52b05827.png 713w, https://www.joe0.com/wp-content/uploads/2020/10/img_5f7dc52b05827-300x246.png 300w" sizes="(max-width: 713px) 100vw, 713px"></p><p>Here is a screenshot of the database after only a couple of minutes of conversion:</p><p id="JQYnVLo"><img loading="lazy" width="432" height="642" src="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e3dc6ce1c7.png" alt="" srcset="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e3dc6ce1c7.png 432w, https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e3dc6ce1c7-202x300.png 202w" sizes="(max-width: 432px) 100vw, 432px"></p><p>As you can see, the conversion utility produces a database with 5 tables per group where messages are linked to each other through auto-created indexes.</p><p id="kdsmyQE">Let’s say we want to look up all discussions in the<strong> net.physics</strong> discussions; and sort them out by the number of replies.</p><p>This is how you can do that:</p><p id="ymEaJie"><a href="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e48cfddb5b.png"><img loading="lazy" src="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e48cfddb5b.png" alt="" width="1198" height="625" srcset="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e48cfddb5b.png 1198w, https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e48cfddb5b-300x157.png 300w, https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e48cfddb5b-1024x534.png 1024w, https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e48cfddb5b-768x401.png 768w" sizes="(max-width: 1198px) 100vw, 1198px"></a></p><p>Now, we can look up a particular discussion by the ID. For example, we want the ID: 1648 from the screenshot above, the discussion with the subject: “<strong>Question on FTL and quantum mechanics</strong>“. That’s not so hard either:</p><p id="rcwUUqq"><a href="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e49dfdfd6d.png"><img loading="lazy" src="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e49dfdfd6d.png" alt="" width="1697" height="847" srcset="https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e49dfdfd6d.png 1697w, https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e49dfdfd6d-300x150.png 300w, https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e49dfdfd6d-1024x511.png 1024w, https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e49dfdfd6d-768x383.png 768w, https://www.joe0.com/wp-content/uploads/2020/10/img_5f7e49dfdfd6d-1536x767.png 1536w" sizes="(max-width: 1697px) 100vw, 1697px"></a></p><p>It’s nice to have a database full of posts, but it’s hardly usable that way. I needed something that would allow me to easily access these posts.</p><p>So, once everything was done, I built a PHP script around this code and registered <a href="http://usenetarchives.com/" target="_blank" rel="noopener noreferrer">https://usenetarchives.com</a> to make all these archives available online, in an easy to read and search (forum-like) web site.</p><p>The PHP code is not part of this article, but you can head over to <a href="https://usenetarchives.com/groups.php?c=utzoo" target="_blank" rel="noopener noreferrer"><strong>https://usenetarchives.…</strong></a></p></div></div></article></main></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.joe0.com/2020/10/07/converting-utzoo-wiseman-netnews-archive-to-postgresql-using-python-3-8/">https://www.joe0.com/2020/10/07/converting-utzoo-wiseman-netnews-archive-to-postgresql-using-python-3-8/</a></em></p>]]>
            </description>
            <link>https://www.joe0.com/2020/10/07/converting-utzoo-wiseman-netnews-archive-to-postgresql-using-python-3-8/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25036780</guid>
            <pubDate>Mon, 09 Nov 2020 16:56:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Towards a Lightweight Jamstack]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25036750">thread link</a>) | @phacks
<br/>
November 9, 2020 | https://orbit.love/blog/towards-a-lightweight-jamstack | <a href="https://web.archive.org/web/*/https://orbit.love/blog/towards-a-lightweight-jamstack">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p><em>Note: this article is an edited transcript of my talk of the same name at the Jamstack Berlin meetup. You can watch the video <a href="https://youtu.be/taOyVmLgym4">here</a>.</em><br></p><p>Jamstack is thriving. There is a plethora of languages, frameworks, libraries, and services that allow you to take full advantage of static websites while being able to leverage the JavaScript and Serverless ecosystems to build rich, dynamic, and whimsical experiences.</p><p>This often comes at a cost—as most Jamstack frameworks are based on JavaScript frameworks (NextJS and Gatsby on React, NuxtJS, and Gridsome on Vue), <a href="https://timkadlec.com/remembers/2020-04-21-the-cost-of-javascript-frameworks/">the JavaScript tax</a> takes a toll on performance, and, ultimately, on your users.</p><p>This article aims to give directions to curb that JS tax—whether by optimizing your existing JavaScript-framework-based website or by going for an alternative: Eleventy.</p><p>But first, let’s take a trip down memory lane to understand how we got where we are today.</p><h2>From Jekyll to Gatsby</h2><p>The Static/Jamstack ecosystem has evolved <em>a lot</em> over the past decade. This evolution has had a deep impact on the way we conceive websites and on the way our users experience them.</p><p>We’re going to cover three major steps through that journey from the (fictional) point of view of a casual discussion between a server and a user.</p><p>Our user will try and access a website, and our server will give her what she needs to <em>view</em> and <em>interact</em> with it. Those two important steps (the user can see the content, the user can interact with it) will be denoted with associated red badges, highlighting the moment in the conversation when they become possible.</p><p>Buckle up! We’re going all the way back to 2008.</p><h3><br>Pure Static (e.g. Jekyll)</h3><p><a href="https://jekyllrb.com/">Jekyll</a>, created in 2008, has long been the most popular Static Site Generator. Following the “<a href="http://www.aaronsw.com/weblog/000404">bake, don’t fry</a>” adage, it pre-computed all the pages of the website to have them readily available for its visitors.</p><p><img src="https://cdn.sanity.io/images/cad8jutx/production/4da9af4b2075d28a8d8e43004d0da1dfa53663ad-1876x802.png" alt="A discussion goes like this. User: “hey could you pass me that index.html file you got there?”. Server: “sure thing here you go”. User: “thanks”"></p><p>It’s straightforward. Simple. No shenanigans. The desired page is served immediately to the user, and it is immediately available to view and interact with. Should she navigate to another page, her browser would fetch it in the same way it did the first.</p><p>As years went by, however, the user experience this solution provided somehow lagged behind what users got used to with mobile applications. Transitions, offline-mode, all the bells and whistles of the mobile revolution were nowhere to be found.</p><p>JavaScript frameworks, Angular, React and Vue among them, offered a new proposition that was to bring native-like experiences to the web, bringing us to our next stop: Client-Side Rendered websites.</p><h3><br>Client-Side Rendering (e.g. React, Vue)</h3><p>To make websites feel native-like, the solution offered by new JavaScript frameworks circa 2015 was to embed a JS-based engine that would create and update the HTML markup and associated styles dynamically. The upfront price to download, parse, and execute that engine would supposedly be offset by faster subsequent navigation and a richer user experience.</p><p><img src="https://cdn.sanity.io/images/cad8jutx/production/b6ac711b81b742038bfa1d6503e6768a046c83dd-1872x1150.png" alt="Discussion goes like this. User: “oooh this new online publication looks cool can i see it?”. Server: “you know what why don’t you take all the raw materials and figure it out yourself”. User: “ wow rude but ok”. User computes the page for a moment. User: “ it works!”"></p><p>The major shift that happened with that evolution is that <strong>the cost of building web pages passed from the server to the user’s browser</strong>. This cost is indicated in the discussion with the gear icon, during which the screen is mostly blank or showing a loading indicator.<br></p><p>As a result, the performance of Client-Side Rendered websites depend widely on the specs of the device of the user, as JavaScript is CPU-intensive. The following video, by <a href="https://joshwcomeau.com/">Josh Comeau</a>, shows a 28 seconds difference (!) in load time for the Washington Post between an iPhone and a $100 Xiaomi Redmi 8.</p><h3>Server-Side Rendering (e.g. Next.js, Nuxt.js)</h3><p>A few years after this paradigm was introduced, these issues lead the pendulum to swing back towards the server, with the introduction of Server-Side Rendering.</p><p>Server-Side Rendering was introduced to fix one of the most annoying aspects of Client-Side Rendering: that the content the user came for would not be visible until after the (usually large) JavaScript code is downloaded, parsed, and executed. Those seconds can be <a href="https://www.thinkwithgoogle.com/marketing-strategies/app-and-mobile/mobile-page-speed-new-industry-benchmarks/">the difference between the visitor staying or leaving the website</a>.</p><p>Frameworks like Next.js and Nuxt.js appeared to try and bring the server back to its original role: building web pages. The approach would be different from Jekyll’s, though: in the Server-Side Rendering paradigm, the server acts as a web browser and renders the page using JavaScript—not Ruby.<br></p><p><img src="https://cdn.sanity.io/images/cad8jutx/production/515d9fcb7a96cf4f41c6c090977e5fbbb0cec9cc-1716x1364.png" alt="Discussion goes like this. User: “i heard they worked on performance on this publication”. Server: “they did! here it is: i’m building the page very quicky…”. Server computes the page for a moment. Server: “and now send you the raw materials to build it yourself BUT now you have a nice picture of the finished page to look at meanwhile!”. Server: “and now send you the raw materials to build it yourself BUT now you have a nice picture of the finished page to look at meanwhile!”. User: “sweet! i can see the content first…”. User computes the page for a moment. User: “and now click around!”"></p><p><br>This approach leverages the power of the server to show the content to the visitor immediately. However, as interactivity still relies on client-side JavaScript, a delay is being introduced between the <em>availability</em> of the content and its <em>readiness</em>. From the point of view of the visitor, this can induce <em>rage clicks</em>: clicking on a button or a link has no effect for several seconds, as illustrated below.</p><p><img src="https://cdn.sanity.io/images/cad8jutx/production/46db90282489daeae289e6307f4dab205b1b79d4-800x445.gif" alt="An animated gif showing two pages loading (one resembling AirBnB, the other Amazon). The pages look like they are ready, and a fictional user clicks on them for around 20 seconds until it has an effect. The fictional user gets annoyed, then angry, then despaired at the situation."></p><p><em>Source: Addy Osmany, <a href="https://medium.com/@addyosmani/the-cost-of-javascript-in-2018-7d8950fbb5d4">The Cost Of JavaScript in 2018</a></em>​</p><p>There would be much more to say on the topic, as some of the frameworks have additional optimizations (prerendering, link-prefetching…), but we already covered enough ground to know that JavaScript has an impact on the user experience for Jamstack websites. This impact has been dubbed the <em>JavaScript tax</em>.</p><h2>Curbing the JavaScript tax</h2><p>Without entering into more details, we can follow the simple approximation that the less JavaScript is used, the lower the tax will be. Shipping less JavaScript then becomes a powerful approach to enhance the performance of our websites.</p><p>What if we could remove it altogether?</p><h3>Removing the JavaScript of JavaScript frameworks</h3><p><a href="http://nextjs.org/">Next.js</a> and <a href="https://www.gatsbyjs.com/">Gatsby</a> are two popular Server-Side Rendered JavaScript frameworks used on many Jamstack websites. They both use React as the underlying JS library to manage state and UI.</p><p>For this section, I’ll take <a href="https://phacks.dev/">my personal blog</a> as an example. I chose to build it with Gatsby, as I wanted to learn more about how it worked and could leverage my React experience to ship it quickly.</p><p><img src="https://cdn.sanity.io/images/cad8jutx/production/1a51c0e83d712f7944362eea3142c9f42e60e64f-2756x1164.png" alt="A screenshot of my personal blog"></p><p>The Developer Experience was a delight and I’m overall pretty happy with it, but something felt <em>off</em>. My blog is pretty basic: an index of all the articles, a page for each, and a few other pages here and there. Yet it was powered by the same technology that powers Facebook, AirBnB and many other extremely complex websites: React. Any overengineering questions put aside, I still required any reader to download, parse, and execute React for <em>no benefits at all</em>. There are no smart widgets or complex UI to justify React. Only text and images.</p><p>My (outstanding) developer experience had an impact on my reader’s user experience. <a href="https://twitter.com/getify/status/1139625725504512003">There is no such thing as trickle-down UX</a>.</p><p>Well, it turns out that there <em>is</em> a way to get the best of both worlds. If, like my blog, your Gatsby website does not require React (or any other JavaScript) to run, you can <em>disable it</em>. The <a href="https://github.com/itmayziii/gatsby-plugin-no-javascript">Gatsby Plugin No Javascript</a> community plugin will let you enjoy the DX of Gatsby without taking a toll on user experience. A <a href="https://github.com/vercel/next.js/pull/11949">similar (experimental) plugin</a> also exists for Next.js.</p><p>Of course, not every website is as simple as my blog—that would be pretty boring. A lot of Gatsby and Next.js websites out there rely on React for their user experience: pretty animations, shopping carts, newsletter sign-ups, and the likes. Is there something we can do on those websites to make them lighter?</p><h3>(P)react</h3><p>When React is required for a website to run properly, we can’t just get rid of JavaScript. What we can do, however, is look for ways to reduce its footprint.</p><p><a href="https://preactjs.com/">Preact</a> is an alternative to React that has the same functionality, the same modern API, for a tenth of its size. The Preact team managed to drastically reduce the footprint by dropping support for some old browsers and legacy React APIs.</p><p>Although there are some slight differences (<a href="https://preactjs.com/guide/v10/differences-to-react#main-differences">see the list</a>), Preact can be used instead of React for many websites without any impact on the end-user, and barely any on the developers.</p><p>We switched from React to Preact on the Orbit app without issues and shaved off half of our JavaScript footprint in the process. If you’d like to try, there are plugins for <a href="https://www.gatsbyjs.org/packages/gatsby-plugin-preact/">Gatsby</a> and <a href="https://github.com/vercel/next.js/tree/canary/examples/using-preact">Next.js</a>, and <a href="https://preactjs.com/guide/v10/switching-to-preact">a guide for switching manually</a>.</p><p>Now, say you are in a situation where you have to create a brand new website. You want to use the Jamstack because you’re convinced of the benefits. You want to be mindful of the user experience, also because you’re convinced of the benefits. Say the website you want to create is similar to this very one, <a href="https://orbit.love/">orbit.love</a>.</p><p>What would you choose for a Lightweight Jamstack? What <em>did I</em> choose for a Lightweight Jamstack?</p><h3>Building orbit.love with Tailwind, Eleventy, and Alpine.js</h3><p>The Orbit website does not have a complex UI—interactivity is limited to a mobile nav menu, modals, and sign-ups for our newsletter and early access. I knew from the get-go that reaching out to (P)react would be heavy handed, so I looked for lightweight alternatives.</p><p>I went for the following: TailwindCSS, for styling, Eleventy, for the static site generation, and Alpine.js, for interactivity.</p><p><a href="http://11ty.dev/">Eleventy</a> is a JavaScript-based Static Site Generator that, despite being written in JavaScript, shares a lot more with Jekyll than with Gatsby. Indeed, Eleventy (also known as 11ty) <em>does not ship any JavaScript by default</em>. You are free to add any, of course, but it does not force you to use any library or framework.</p><p>Not having to use a JavaScript framework also meant that HTML, not JSX or Vue components, is now front and center in the code you write. This helped me avoid the usual traps when writing React: the infamous <a href="https://www.chillybin.com.sg/would-you-like-another-bowl-of-div-soup/#:~:text=What%20is%20Soup%3F,to%20make%20your%20eyes%20bleed.">div soup</a>, inaccessible components, or non-semantic tags.</p><p><a href="https://tailwindcss.com/">TailwindCSS</a> is a utility-first CSS framework, which means that instead of writing CSS for your components (class="navbar__mobile"), you combine utility classes that each do one specific thing (class="flex flex-row justify-center w-full").</p><p>I find this approach incredibly productive once you learn the grammar, and it makes for a resilient CSS architecture at the admitted cost of some duplication in your code. What makes it a great match with Eleventy is that you rarely, if ever, leave your HTML components when writing code. It helps me focus on the task at hand by …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://orbit.love/blog/towards-a-lightweight-jamstack">https://orbit.love/blog/towards-a-lightweight-jamstack</a></em></p>]]>
            </description>
            <link>https://orbit.love/blog/towards-a-lightweight-jamstack</link>
            <guid isPermaLink="false">hacker-news-small-sites-25036750</guid>
            <pubDate>Mon, 09 Nov 2020 16:54:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to build successful Machine Learning teams]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25036630">thread link</a>) | @benkoller
<br/>
November 9, 2020 | https://blog.maiot.io/MLOps-Learning-from-history/ | <a href="https://web.archive.org/web/*/https://blog.maiot.io/MLOps-Learning-from-history/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section id="post-content">
    <div id="blogpost">
        <p>TL;DR: Running successful ML teams and projects requires cross-functional collaboration within the executing team.</p>

<p>That sentence alone does not help anyone. It feels unsubstantiated, and someone playing bullshit bingo might luck out just on that one sentence - and yet, itâ€™s true. Why? Because itâ€™s taking the main driver behind the DevOps revolution in software development and applies it to Machine Learning.</p>

<p>If youâ€™re familiar with what DevOps stands for, where it came from and why itâ€™s such a game-changer, feel free to skip to <a href="#a-better-way">the conclusion</a>.</p>

<p>For everyone else, a quick refresher.</p>

<h2 id="why-did-software-people-come-up-with-devops">Why did â€œsoftware peopleâ€� come up with DevOps?</h2>
<p>Flashback to the 2000s (sorry if this triggers PTSD for my fellow Ops guys out there). Software Development is a barren wasteland. Developers write code until it works on their computer, send an email to QA and Ops to please do their thing now. Nobody knows who the people in QA are, all they are doing is complaining about bugs that canâ€™t be reproduced. Ops are the angry guys from the basement, and somehow they get mad just because code sometimes SEGFAULTs, but it canâ€™t have been a fault on our side, it never SEGFAULTs on my machine.</p>

<p>This is not a healthy way to run a business. The late 2000s were full of stories of companies that failed, simply because their internal software development process was too slow to keep up with the market.</p>

<p>Meet â€œDevOpsâ€�: A new and revolutionary way of dealing with software development. What if we could deliver our software more often, and faster, to our customers? And what if it broke less in production? What if our Devs, QA, and Ops could be happier at work? And, what if we would build up less technical debt along the way?</p>

<h2 id="but-how">But, how?</h2>

<p>How, you ask? Let developers, testers, AND operators work in unison, together, and own the entire process, rather than separate them into silos!
From an orbital perspective, software development can be roughly broken down into a handful of stages:</p>

<ul>
  <li>Coding</li>
  <li>Building / Packaging</li>
  <li>Testing</li>
  <li>Deployment</li>
  <li>Monitoring</li>
</ul>

<p>Allowing teams to own the entire process, rather than just a single aspect, created a deeper understanding of all software lifecycle aspects across the functions in a team. Suddenly, everyone was a part of the development team. Testers had to be able to understand and write code because tests had to be automated. Ops guys built abstractions to the underlying hardware so that others could run software independently. Developers had to acknowledge that software is only done when being exposed successfully to customers.</p>

<p>Yes, there are many ways to mess this up. You might say itâ€™s even impossible to get it right. These ideas have spawned many closely related methodologies and team designs (e.g. SREs at Google, Netflixâ€™s Platform Engineering team, etc. pp.). But evidence confirms: getting all lifecycle functions to work (more) cross-functional will leave you better off than in a siloed environment.</p>

<h2 id="a-pessimistic-look-at-the-state-of-machine-learning-teams">A pessimistic look at the state of Machine Learning teams</h2>
<p>Obviously, Iâ€™m trying to go somewhere with this. Machine Learning as a discipline is exactly where software development was in the early 2000s: just before the dawn of DevOps. Unless youâ€™re in some hotshot startup, you own all data, and every one of your engineers is a core contributor to Kubeflow, your reality will look a bit bleak:</p>

<ul>
  <li>ML teams have little to no influence over the data they get. The data might be owned by a different team, or worse, a customer.</li>
  <li>There is no close communication between ML and data owners.</li>
  <li>Schemas and feature quality changes.</li>
  <li>The delivery of data is not standardized.</li>
  <li>ML teams have to run their own infrastructure. Support from your Platform team ended at â€œHere, I created an AWS IAM account for youâ€�.</li>
  <li>Nobody in your ML team has prior experience as a Software Developer.</li>
</ul>

<h2 id="a-better-way">A better way</h2>
<p>Nothing Iâ€™m about to say should sound like a revelation. Iâ€™ll paint the picture by using an example: A team running the search engine on an e-commerce platform. They need engineers with an understanding of high-performance processing, databases, and API design. They will need to be able to rely on the upstream product data staying consistent. They need to understand how their search engine is used in the frontend, and what performance metrics they need to be able to provide. They need to own the APIs used by downstream teams, e.g. the Frontend or your mobile app. Abstracted, they need to get</p>

<ul>
  <li>The right cross-functional resources/skills</li>
  <li>Structured upstream data as input</li>
  <li>Clear business requirements</li>
  <li>Full ownership of their service</li>
</ul>

<p>The same applies to Machine Learning teams if theyâ€™re supposed to be successful:</p>

<h3 id="skills">Skills</h3>
<p>Your team will be responsible for preprocessing input data it receives, training ML models, evaluating fulfillment of business criteria, and delivering results to downstream stakeholders. Therefore, the skills your team will need are:</p>

<ul>
  <li>Software Engineering to write performant standardized preprocessing code and to expose results as a service</li>
  <li>Operations/SRE to build reproducible ML pipelines and reliable model serving services, both with potentially high resource demand</li>
  <li>Machine Learning to build performant models</li>
  <li>Data Science to evaluate input data and output results</li>
</ul>

<h3 id="upstream-input">Upstream input</h3>

<p>Data needs to be reliable if a team is supposed to generate value from it. The team and the upstream data provider need to agree on the frequency of data provision, data format, and data quality. The easiest and best agreement can be found in code, e.g. through data ingestion pipelines with strong data validation built-in, but it can be beneficial later on to establish a feature store for your Machine Learning team. Horizontally aligned Data Operations teams can help if more than one team is reliant on similar data.</p>

<h3 id="business-objectives">Business objectives</h3>
<p>Surprisingly many projects fail due to muddy business objectives. Business stakeholders and the ML team need to come together and thoroughly define what the business is trying to achieve, and how it affects the model performance indicators. At a minimum, the business and the team need to answer these questions:</p>

<ul>
  <li>How to deal with false-positives, false-negatives?</li>
  <li>Do predictions need to be real-time, or is batch inference fine?</li>
  <li>What biases need to be accounted for?</li>
  <li>Which downstream services need to consume model results?</li>
</ul>

<h3 id="ownership">Ownership</h3>
<p>Probably the most controversial point in this blogpost concerns ownership. After transitioning multiple teams across companies towards full ownership of their software development lifecycle Iâ€™m a firm believer in the approach. Teams need to be responsible for providing the actual value, not just the artifact itself. It drives healthy decision-making (e.g. how big can the final model be, how much resources can it use), and it breeds a deeper understanding of the actual needs of downstream consumers. After all, if the served model does not create a positive impact on someone downstream, the efforts of the team should have been applied elsewhere.</p>

<h2 id="what-now">What now?</h2>
<p>Your teams are now cross-functional, and everyone is eager to jump to work. But what now? How does the work of a successful ML team look like in production? We got you covered on that front, too - check out our blog post on the <a href="https://blog.maiot.io/12-factors-of-ml-in-production/">12 factors of reproducible Machine Learning in production</a>.</p>


    </div>
</section><section id="blog-signup">
    <hr>
    <div>
        <div>
            <p>If you want to keep in touch with the latest blog posts, please subscribe to our <a href="https://blog.maiot.io/feed.xml" target="_blank">RSS Feed </a></p>
        </div>
    </div>
</section></div>]]>
            </description>
            <link>https://blog.maiot.io/MLOps-Learning-from-history/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25036630</guid>
            <pubDate>Mon, 09 Nov 2020 16:43:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Astronomy Picture of the Day]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25036130">thread link</a>) | @jayass
<br/>
November 9, 2020 | https://misspellede.com/us/cosmos/ | <a href="https://web.archive.org/web/*/https://misspellede.com/us/cosmos/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <section>
                    <article>
            <a href="https://misspellede.com/us/colors-of-the-moon/">
                <img loading="lazy" src="https://apod.nasa.gov/apod/image/2011/MoonColors_Pace_960.jpg" alt="Colors of the Moon">
            </a>
            <div>
                <p><a href="https://misspellede.com/us/cosmos/">cosmos</a>
                <a href="https://misspellede.com/us/colors-of-the-moon/">Colors of the Moon</a></p><p>Marcella Giulia Pace • 2020-11-11</p>
                <p>What color is the Moon? It depends on the night.  Outside of the Earth's atmosphere, the dark Moon, which shines by reflected sunlight, appears a magn...</p>
                <p><a href="https://misspellede.com/us/colors-of-the-moon/">Continue reading <i></i></a>
            </p></div>
        </article>
                                <article>
            <a href="https://misspellede.com/us/the-central-soul-nebula-without-stars/">
                <img loading="lazy" src="https://apod.nasa.gov/apod/image/2011/IC1848_Guenzel_960.jpg" alt="The Central Soul Nebula Without Stars">
            </a>
            
        </article>
                    <article>
            <a href="https://misspellede.com/us/in-green-company-aurora-over-norway/">
                <img loading="lazy" src="https://apod.nasa.gov/apod/image/2011/greencompany_rive_960.jpg" alt="In Green Company: Aurora over Norway">
            </a>
            
        </article>
                    <article>
            <a href="https://misspellede.com/us/martian-moon-phobos-from-mars-express/">
                <img loading="lazy" src="https://apod.nasa.gov/apod/image/2011/phoboslimb_marsexpress_960.jpg" alt="Martian Moon Phobos from Mars Express">
            </a>
            
        </article>
                    <article>
            <a href="https://misspellede.com/us/the-hercules-cluster-of-galaxies/">
                <img loading="lazy" src="https://apod.nasa.gov/apod/image/2011/Abell2151_Howard_Trottier_2020_FFTelescope1024.jpg" alt="The Hercules Cluster of Galaxies">
            </a>
            
        </article>
                    <article>
            <a href="https://misspellede.com/us/moon-over-iss/">
                <img loading="lazy" src="https://apod.nasa.gov/apod/image/2011/ISSlunartransit110320closeup1024.jpg" alt="Moon over ISS">
            </a>
            <div>
                <p><a href="https://misspellede.com/us/cosmos/">cosmos</a>
                <a href="https://misspellede.com/us/moon-over-iss/">Moon over ISS</a></p><p>Derek Demeter • 2020-11-06</p>
                <p>Completing one orbit of our fair planet in 90 minutes the International Space Station can easily be spotted by eye as a very bright star moving throug...</p>
                <p><a href="https://misspellede.com/us/moon-over-iss/">Continue reading <i></i></a>
            </p></div>
        </article>
                    <article>
            <a href="https://misspellede.com/us/north-of-orions-belt/">
                <img loading="lazy" src="https://apod.nasa.gov/apod/image/2011/M78_LDN1622_BarnardsLoop_SEP27_28_Oct15_final1024.jpg" alt="North of Orion's Belt">
            </a>
            <div>
                <p><a href="https://misspellede.com/us/cosmos/">cosmos</a>
                <a href="https://misspellede.com/us/north-of-orions-belt/">North of Orion's Belt</a></p><p>Terry Hancock • 2020-11-05</p>
                <p>Bright stars, interstellar clouds of dust and glowing nebulae fill this cosmic scene, a skyscape just north of Orion's belt. Close to the plane of our...</p>
                <p><a href="https://misspellede.com/us/north-of-orions-belt/">Continue reading <i></i></a>
            </p></div>
        </article>
                    <article>
            <a href="https://misspellede.com/us/fifty-gravitational-wave-events-illustrated/">
                <img loading="lazy" src="https://apod.nasa.gov/apod/image/2011/GWaveSources2020Oct_LigoVIrgo_960.jpg" alt="Fifty Gravitational Wave Events Illustrated">
            </a>
            
        </article>
                    <article>
            <a href="https://misspellede.com/us/tagging-bennu-the-movie/">
                <img loading="lazy" src="https://www.youtube.com/embed/F6Tkb8syTK8?rel=0" alt="Tagging Bennu: The Movie">
            </a>
            
        </article>
                    <article>
            <a href="https://misspellede.com/us/half-sun-with-prominence/">
                <img loading="lazy" src="https://apod.nasa.gov/apod/image/2011/HalfSunProm_Colacurcio_960.jpg" alt="Half Sun with Prominence">
            </a>
            <div>
                <p><a href="https://misspellede.com/us/cosmos/">cosmos</a>
                <a href="https://misspellede.com/us/half-sun-with-prominence/">Half Sun with Prominence</a></p><p>Rainee Colacurcio • 2020-11-02</p>
                <p>What's happening to the Sun? Clearly, the Sun's lower half is hidden behind a thick cloud.  Averaging over the entire Earth, clouds block the Sun abou...</p>
                <p><a href="https://misspellede.com/us/half-sun-with-prominence/">Continue reading <i></i></a>
            </p></div>
        </article>
                    </section>
            
                    </div></div>]]>
            </description>
            <link>https://misspellede.com/us/cosmos/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25036130</guid>
            <pubDate>Mon, 09 Nov 2020 16:01:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Europe enforces IM-Services to store encryption master key for government access]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25036061">thread link</a>) | @lilatentakel
<br/>
November 9, 2020 | https://fm4.orf.at/stories/3008930/ | <a href="https://web.archive.org/web/*/https://fm4.orf.at/stories/3008930/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="ss-storyText" role="article">
     
     <p><strong>Im EU-Ministerrat wurde binnen fünf Tagen eine Resolution beschlussfertig gemacht, die Plattformbetreiber wie WhatsApp, Signal und Co. künftig dazu verpflichtet, Generalschlüssel zur Überwachbarkeit von E2E-verschlüsselten Chats und Messages anzulegen.</strong></p>
     



     


     <p>Von <a href="http://fm4.orf.at/tags/erichmoechel">Erich Moechel</a></p><p>Der Terroranschlag in Wien wird im EU-Ministerrat dazu benützt, um ein Verbot sicherer Verschlüsselung für Services wie WhatsApp, Signal und viele andere im Schnellsiedeverfahren durchzusetzen. Das geht aus einem mit 6. November datierten internen Dokument der deutschen Ratspräsidentschaft an die Delegationen der Mitgliedsstaaten im Rat hervor, das ORF.at vorliegt.</p><p>Das sollte nun unter den „weiteren Schritten gegen den Terrorismus“ zu verstehen sein, die Frankreichs Präsident Emmanuel Macron mit Bundeskanzler Sebastian Kurz (ÖVP) im Rahmen einer Videokonferenz zu Wochenbeginn besprechen will. Der Beschluss ist bereits so weit akkordiert, dass er in der Videotagung der Innen- und Justizminister Anfang Dezember ohne weitere Diskussion verabschiedet werden kann.</p><div><p><img src="https://tubestatic.orf.at/static/images/site/tube/20201145/eu_ministerrat_cover.5944853.jpg" alt="Text" title="© EU Ministerrat" width="800" height="560"></p><p>EU Ministerrat</p><p>Rechts sind die Ratsarbeitsgruppen aufgelistet, an die dieser Text erging, dessen erste revidierte Fassung offenbar am Freitag fertig wurde. Wie im Ministerrat üblich, wurde das Dokument als „limite“ klassifiziert. Da es aus diesem Grund abseits des Rats nirgendwo für die Öffentlichkeit einsehbar ist, wird es hier zur Verfügung gestellt: <a href="https://files.orf.at/vietnam2/files/fm4/202045/783284_fh_st12143-re01en20_783284.pdf">[PDF]</a></p></div><h2>Analogien zur Vorratsdatenspeicherung</h2><p>Aus dem ursprünglich für Anfang kommender Woche geplanten Besuch Macrons wurde pandemiebedingt eine Videokonferenz „zur Bekämpfung des islamistischen Terrorismus“. Weiters steht ein Besuch des EU-Ratspräsidenten Charles Michel in Wien für Montag an, der ebenfalls mit Bundeskanzler Kurz Gespräche führen wird. Zudem empfängt Europaministerin Karoline Edtstadler (ÖVP) den französischen Europastaatssekretär Clement Beaune im Bundeskanzleramt. Alleine um Kondolenzbezeugungen geht es dabei natürlich nicht.</p><p>Mittlerweile wird zwar immer klarer, dass offenbar haarsträubende Ermittlungsfehler im BVT den Anschlag erst ermöglicht hatten und nicht fehlende digitale Überwachungsbefugnisse. Ob irgendein solcher Zusammenhang zur Tat besteht, ist allerdings unerheblich. In Brüssel wird so ein Anlass seit 25 Jahren mit schnöder Regelmäßigkeit dafür missbraucht, längst geplante Überwachungsvorhaben durchzusetzen. Auf diese Weise wurde die fünf Jahre lang in der EU umstrittene Vorratsdatenspeicherung nach den Zugsanschlägen in Madrid (2004) und London durch Islamisten (2005) durch den Ministerrat und das Parlament geschleust.</p><div><p><img src="https://tubestatic.orf.at/static/images/site/tube/20201145/ohne-titel-2.5944858.jpg" alt="Text" title="© EU Ministerrat" width="800" height="360"></p><p>EU Ministerrat</p><p>An den letzten Änderungen (fett und unterstrichen) sieht man, welche Formulierungen von einzelnen Mitgliedsstaaten in den Text reklamiert worden waren. Zuletzt eingefügt wurden „Terrorismus“ und eine unscheinbare Änderung im Wording. Statt der in allen Dokumenten seit 1995 üblichen Strafverfolger („law enforcement“) ist nun konsequent von „competent authorities“ die Rede. Wer damit gemeint ist, steht weiter unten.</p></div><h2>Verabschiedung ohne weitere Diskussionen</h2><p>Diese Resolution des Ministerrats ist laut Dokument - da wird um allfällige letzte Einwände gebeten -  nicht nur fast fertig ausformuliert. Sie ist im Rat offenbar auch bereits fertig abgestimmt. Am 19. November soll sie dann in der Ratsarbeitsgruppe zur Kooperation im nationalen Sicherheitsbereich (COSI) verabschiedet werden, am 25. ist die Vorlage im Rat der ständigen Vertreter der EU-Mitgliedsstaaten (COREPER) geplant. Dort hat der Ratsbeschluss bereits den Status eines I-Items, damit kann er ohne weitere Diskussion passieren.</p><p>In einer für Anfang Dezember geplanten virtuellen Sitzung des Rats der Innen- und Justizminister soll der Beschluss dann abgefeiert werden. Was folgen wird, ist klar, nämlich ein Auftrag des Ministerrats an die EU-Kommission, einen Entwurf für eine Verordnung zu erstellen, die dann das übliche Prozedere durch Parlament und Rat durchlaufen wird. Angesichts der offenbaren Einstimmigkeit wäre es im Ministerrat allerdings möglich, die geplante Regulation in ihrem Kern auch ohne Mitwirkung des Parlaments durchzuziehen. Auch das hat es in Zusammenhang mit Überwachung schon gegeben. So wurde der berühmte Beschluss im Fischereiausschuss des Rats von 1995 zur Überwachbarkeit der damals neuen GSM-Netze als A-Item (beschlossene Sache) durchgezogen, von dem das EU-Parlament erst nach seinem Inkrafttreten 1996 Kenntnis erhielt.</p><div><p><img src="https://tubestatic.orf.at/static/images/site/tube/20201145/ohne-titel-1.5944852.jpg" alt="Text" title="© Public | FiveEyes" width="1280" height="255"></p><p>Public | FiveEyes</p><p>Diese Passage sieht dem EU-Ministerratsbeschluss zwar zum Verwechseln ähnlich, stammt jedoch nicht aus Europa. <a href="https://www.justice.gov/opa/pr/international-statement-end-end-encryption-and-public-safety">Sie findet sich vielmehr in einer Resolution der Innen- und Justizminister</a> aus den „Five Eyes“-Staaten, datiert mit 11. Oktober. Die Spionageallianz ist neben Europol und diversen europäischen Diensten eine der treibenden Kräfte, auf die der aktuelle Ministerratsbeschluss zurückzuführen ist.</p></div><h2>Treibende Kräfte im Hintergrund</h2><p>Frankreich treibt das ursprünglich von Großbritannien angestoßene Vorgehen gegen sichere Verschlüsselung auf Plattformen wie WhatsApp bereits das ganze Jahr auf EU-Ebene voran. Der Boden dafür wurde seit 2015 in einer ganze Serie von Kampagnen vorbereitet, die abwechselnd von Europol und FBI bzw. den Diensten der „Five Eyes“-Spionageallianz samt den dafür zuständigen Ministern gefahren wurden. Erst Anfang Oktober hatten die Innenminister dieser fünf Staaten - Großbritannien, USA, Australien, Neuseeland und Kanada - die Internetkonzerne erneut aufgefordert, ihre IT-Netze mit Hintertüren für die Strafverfolger auszustatten.</p><p>Sekundiert wurden sie dabei von ihren Amtskollegen in Japan und in Indien. Warum sich die Geheimdienstallianz so auffällig um die bedauernswerten Strafverfolger jahrelang öffentlich gesorgt hat, ist eigentlich selbsterklärend. Sie sind die übrigen „competent authorities“ die ebenfalls Zugang erhalten werden.</p><h2>„Competent authorities“ lassen grüßen</h2><p>Laut weiteren Informationen, die ORF.at vorliegen, soll die Überwachungsmethode „Exceptional Access“ gewählt werden, das geht indirekt bereits aus diesem  nicht technischen Resolutionstext hervor. Unter acht möglichen Modellvorschlägen, die allesamt aus technischen Szenarien verschiedener Geheimdienste stammen, wurde jener aus dem britischen „National Cyber Security Center“ (NCSC) ausgewählt. Das NCSC ist eine Abteilung des britischen Militärgeheimdienstes GCHQ. Plattformbetreiber wie WhatsApp, Signal und Co, die alle E2E-Verschlüsselung benützen, sollen verpflichtet werden, zusätzlich Generalschlüssel anzulegen und diese zu hinterlegen.</p><div><p><img src="https://tubestatic.orf.at/static/images/site/tube/20200939/eu_ministerrat_diskussionspapier_ea.5941753.jpg" alt="Skizzen aus Dokumenten" title="© EU-Ministerrat" width="800" height="409"></p><p>EU-Ministerrat</p><p>Hier wird ein Nachschlüssel für Dritte in den Verschlüsselungsprozess zweier Chatteilnehmer geschmuggelt, es ist die Methode „Exceptional Access“ des GCHQ. Mit sicherer Verschlüsselung hat diese wie alle anderen in diesem Dokument enthaltenen Varianten nichts zu tun, es sind einfach nur verschiedene Arten von „Man in the middle“ -Angriffen auf sichere Kommunikation. Die Studie wurde im Auftrag der deutschen Ratspräsidentschaft erstellt und im August <a href="https://www.politico.eu/wp-content/uploads/2020/09/SKM_C45820090717470-1_new.pdf">vom Fachmagazin Politico</a> veröffentlicht.</p></div><p>Das nämlich sind die „competent authorities“: GCHQ, DGSE, BND usw., deren Staubsaugermethoden an den Glasfasern wegen zunehmender Transportverschlüsselung immer weniger verarbeitbare Daten einbringen. Um diese drohende Datenarmut abzuwenden, wurden jetzt Generalschlüssel verlangt - und wie es aussieht, wird das im Rat auch bewilligt. Dann kann das BVT, das es nicht einmal schafft, einen Terroristen auszuschalten, der von zwei anderen Diensten zweimal auf dem Silbertablett serviert wird, künftig auch in Chatverläufen wochenlang nicht ermitteln.</p>
     
     



   </div></div>]]>
            </description>
            <link>https://fm4.orf.at/stories/3008930/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25036061</guid>
            <pubDate>Mon, 09 Nov 2020 15:56:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Separating User Database and Authorization from Apps with Istio and FusionAuth]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25035985">thread link</a>) | @mooreds
<br/>
November 9, 2020 | https://reachablegames.com/oidc-fusionauth-istio/ | <a href="https://web.archive.org/web/*/https://reachablegames.com/oidc-fusionauth-istio/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://reachablegames.com/content/images/size/w300/2020/11/security.jpg 300w,
                            https://reachablegames.com/content/images/size/w600/2020/11/security.jpg 600w,
                            https://reachablegames.com/content/images/size/w1000/2020/11/security.jpg 1000w,
                            https://reachablegames.com/content/images/size/w2000/2020/11/security.jpg 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://reachablegames.com/content/images/size/w2000/2020/11/security.jpg" alt="Separating your User Database and Authorization from Applications with Istio and FusionAuth">
            </figure>

            <section>
                <div>
                    <p>Kubernetes is a tremendously powerful cluster management system. &nbsp;There are many pluggable technologies to choose from that exhibit the features you desire, which is great because you have options--but also comes with the down-side that the likelihood someone else has done exactly what you are trying to do is slim. &nbsp;Eventually, there will be some consolidation as developers gather around the best solutions, but for the moment, there are lots of interesting projects to choose from (and not a ton of great examples for certain configs). Today, I'm sharing a slightly challenging setup and hoping it helps the community.</p><p>I am using Istio as my L7 ingress and routing controller, which is based on Envoy. &nbsp;It is a highly scalable L7 proxy with excellent performance characteristics and relatively mature feature set. When it came time to implement a basic <code>/admin</code> route on a project, I came up with the list of features that I wanted to achieve. &nbsp;My desired config:</p><ul><li>Applications should not have access to user passwords or necessarily email</li><li>Applications should not re-implement role-based access control (RBAC) security, as every application will need it</li><li>Users should be able to login without creating yet another username/password to remember, but support it if they prefer</li><li>Users should be able to self-register, password reset if necessary, and manage what remote authentications are associated with their account without needing support (Google keyword CIAM)</li><li>Application Admins should be able to edit the role of users, either explicitly or by group permissions, with a visual interface for non-technical people to control access</li><li>K8s Ops should be able to change what RBAC rules protect individual routes to applications without a redeploy</li><li>Fully self-hosted, to limit external dependencies and have auditable security around user data</li></ul><p>The simplest way to get started is to follow the excellent walkthrough on <a href="https://www.blog.jetstack.io/blog/istio-oidc/">Jetstack.io</a> that explains in reasonably good detail how to cover your whole ingress with JWT handling. &nbsp;I won't go over all that detail here. &nbsp;Instead, I will present the exact YAML manifests necessary to directly deploy a working config, as well as show screenshots of relevant bits in FusionAuth of exactly how to configure the application so it communicates properly with these manifests.</p><h3 id="why-fusionauth">Why FusionAuth?</h3><p>I usually try out two or three alternative technologies before settling into one I like. Although I did start with KeyCloak, it felt a little unpolished and left a lot to be desired when it came to explaining how to configure it if the terminology wasn't familiar (eg. people who aren't security professionals). &nbsp;I studied several other options and it came down to Gluu or FusionAuth. &nbsp;The main deciding factor for me in favor of FusionAuth was the amount of documentation and tutorials (with much appreciated touches of humor). &nbsp;There is also a clear effort made by the developers to provide official docker images and Kubernetes examples that show real world use. &nbsp;I have been remarkably satisfied with this decision.</p><h3 id="quick-architecture-overview">Quick Architecture Overview</h3><p>Ok, so let's talk about the architecture of how this works together. &nbsp;Like any other traffic using Istio, a request will come into an application by following the routing rules of a <code>VirtualService</code> to a <code>Service</code>, then to a Deployment's <code>Pod</code>. &nbsp;To use the Istio security features, this pod needs to have the Sidecar Proxy running, otherwise the rules don't do anything. (This is unfortunate, as it has been my experience that the sidecar can cause connectivity issues with certain workloads, so just be aware it can cause side effects and you may need to explicitly create and configure the <code>Sidecar</code> for this namespace). The easiest way to get this working is to enable automatic sidecar proxy injection on a new <code>Namespace</code> and deploy the application there. &nbsp;By declaring a <code>RequestAuthentication</code> rule, we configure Istio to refuse any traffic that doesn't have a validly signed Json Web Token (JWT). &nbsp;And by declaring an <code>AuthorizationPolicy</code> rule, we configure Istio to accept or deny traffic by matching specific HTTP paths or user roles, etc. &nbsp;That's great! &nbsp;Right?</p><p>Well, Istio isn't quite mature enough to speak Open ID Connect. &nbsp;It's only smart enough to expect a validly decoded JWT and do some simple pattern matching against its contents. &nbsp;When those rules fail, you just get <code>RBAC: access denied</code> as a response to your request. &nbsp;There's no redirection logic to send the browser to the auth server login page. &nbsp;So, let's teach it to do that with a simple <code>EnvoyFilter</code> rule that is injected on <code>SIDECAR_INBOUND</code>. This lets us target specific applications to protect only the routes we care about without impacting anything else. </p><p>A few critical details: <code>RequestAuthentication</code> only accepts a &nbsp;JWT that is signed with an RSA key, because HMAC is a symmetrical key and anyone who can decode it can also sign it. &nbsp;This means it needs to know where to get the public RSA key, which is supplied in the <code>issuer</code> field. &nbsp;Assuming this checks out, Istio then looks at any <code>AuthorizationPolicy</code> rules and either <code>ALLOW</code> or <code>DENY</code> traffic based on matching or non-matching details. &nbsp;In this case, I have provided a basic rule that allows anyone who has been verified to have an account with this application, and further restrict the <code>/admin/</code> path to accounts that have the <code>admin</code> role. &nbsp;Should anything go wrong, Istio just says <code>RBAC: access denied</code> . &nbsp;To diagnose, just delete these rules and try hitting the endpoint to see what errors pop up. &nbsp;If these rules are removed and you are still getting Unauthorized messages, it's oauth2-proxy refusing the user--check the config and logging to see why.</p><p>Here's the YAML we've all been waiting for. &nbsp;This fully describes a working config where the <code>VirtualService</code> is in the default namespace but everything else is in <code>auth</code> just to keep it away from everything else. &nbsp;The application is hosted at <code>https://auth-example.reachablegames.com</code>. &nbsp;Certain difficult and undocumented details that cause problems if not configured properly have been commented below--please pay attention before changing or simplifying things.</p><pre><code># create namespace where applications can have sidecar injection
apiVersion: v1
kind: Namespace
metadata:
  labels:
    app: auth
    istio-injection: enabled
  name: auth
---
# This rule makes sure the JWT is decoded and passed through to the web server as HTTP_PAYLOAD base64 encoded.
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: auth-example
  namespace: auth
spec:
  selector:
    matchLabels:
      app: auth-example
  jwtRules:
  - issuer: "https://fusionauth.reachablegames.com"
    # this passes the full bearer token as the "authorization" header
    forwardOriginalToken: true        
    # this passes just the decoded JWT as "payload" header
    outputPayloadToHeader: "payload"  
---
# This rule verifies the user is an authenticated user (requestPrincipals) and also authorized (request.auth.claims)
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: auth-example
  namespace: auth
spec:
  selector:
    matchLabels:
      app: auth-example
  action: ALLOW
  rules:
  - from:  # limit admin path to users with admin role
    - source:
        requestPrincipals: ["*"]
    to:
    - operation:
        paths: ["/admin/*"]
    when:
    - key: request.auth.claims[roles]
      values: ["admin"]
  - from:  # allow anyone who is authorized to access the site to access anything other than /admin
    - source:
        requestPrincipals: ["*"]
    to:
    - operation:
        notPaths: ["/admin/*"]
---
# This intercepts and sends the traffic directly to the oauth2-proxy if there isn't a JWT cookie in the header.
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: auth-example
  namespace: auth
spec:
  workloadSelector:
    labels:
      app: auth-example
  configPatches:
  - applyTo: HTTP_FILTER
    match:
      context: SIDECAR_INBOUND
      listener:
        portNumber: 80
        filterChain:
          filter:
            name: envoy.http_connection_manager
            subFilter:
              name: envoy.filters.http.jwt_authn
    patch:
      operation: INSERT_BEFORE
      value:
        name: envoy.filters.http.ext_authz
        typed_config:
          "@type": type.googleapis.com/envoy.config.filter.http.ext_authz.v2.ExtAuthz
          http_service:
            server_uri: # Note, this absolutely must be the FQDN for the service.  Does not work as a shortname.
              uri: http://auth-example-oauthproxy.auth.svc.cluster.local:8081
              cluster: outbound|8081||auth-example-oauthproxy.auth.svc.cluster.local
              timeout: 10s
            authorizationRequest:
              allowedHeaders:
                patterns:
                - exact: cookie
            authorizationResponse:
              allowedUpstreamHeaders:
                patterns:
                - exact: authorization
---
# Critical: spell out the FQDN because this VirtualService is in "default" but the Service is in "auth"
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: auth-example
  namespace: default
  labels:
    app: auth-example
spec:
  hosts:
  - "auth-example.reachablegames.com"
  gateways:
  - istio-gw
  http:
  - route:
    - destination:
        host: auth-example.auth.svc.cluster.local  # this refers to a Service with name="auth-example"
        port:
          number: 80
---
# Sends traffic to the auth-example deployment pods, which is our application we are trying to secure
apiVersion: v1
kind: Service
metadata:
  name: auth-example
  namespace: auth
  labels:
    app: auth-example
spec:
  ports:
  - port: 80
    name: http-web
    targetPort: http-web
    protocol: TCP
  selector:
    app: auth-example  # send traffic to the auth-example pods
  sessionAffinity: None
  type: ClusterIP
---
# Sends …</code></pre></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://reachablegames.com/oidc-fusionauth-istio/">https://reachablegames.com/oidc-fusionauth-istio/</a></em></p>]]>
            </description>
            <link>https://reachablegames.com/oidc-fusionauth-istio/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035985</guid>
            <pubDate>Mon, 09 Nov 2020 15:51:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Art of the Seed Pitch]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035887">thread link</a>) | @paraj
<br/>
November 9, 2020 | https://parajmathur.com/2020/10/24/art-of-seed-pitch/ | <a href="https://web.archive.org/web/*/https://parajmathur.com/2020/10/24/art-of-seed-pitch/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<figure><img data-attachment-id="71" data-permalink="https://parajmathur.com/2020/10/24/art-of-seed-pitch/coach-speaking-before-audience/" data-orig-file="https://parajvc.files.wordpress.com/2020/10/the-seed-pitch-graphic.jpg" data-orig-size="8000,5000" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;Coach speaking before audience. Mentor presenting charts and reports, Employees meeting at business training, seminar or conference. Vector illustration for presentation, lecture, education concept&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;Coach speaking before audience&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="Coach speaking before audience" data-image-description="" data-medium-file="https://parajvc.files.wordpress.com/2020/10/the-seed-pitch-graphic.jpg?w=300" data-large-file="https://parajvc.files.wordpress.com/2020/10/the-seed-pitch-graphic.jpg?w=1024" src="https://parajvc.files.wordpress.com/2020/10/the-seed-pitch-graphic.jpg?w=1024" alt="Designed by pch.vector / Freepik" title="Designed by pch.vector / Freepik" srcset="https://parajvc.files.wordpress.com/2020/10/the-seed-pitch-graphic.jpg?w=1024 1024w, https://parajvc.files.wordpress.com/2020/10/the-seed-pitch-graphic.jpg?w=2048 2048w, https://parajvc.files.wordpress.com/2020/10/the-seed-pitch-graphic.jpg?w=150 150w, https://parajvc.files.wordpress.com/2020/10/the-seed-pitch-graphic.jpg?w=300 300w, https://parajvc.files.wordpress.com/2020/10/the-seed-pitch-graphic.jpg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>I meet 20+ seed and pre-seed founders every week. Every founder has a unique presentation style. Some like to go through the deck slide by slide. Others prefer a more dynamic conversation without a deck. After studying hundreds of pitches, the most effective and efficient pitches follow this framework — Founder-Product-Market-Fit.</p>



<p><amp-fit-text layout="fixed-height" min-font-size="6" max-font-size="72" height="80"><strong>Founder – why you are the right team to invest in</strong></amp-fit-text></p>



<p>A pre-seed/seed investment is mostly a bet on the founding team and their ability to move fast, execute, learn, and overcome adversity. Because this is potentially the beginning of a 10+ year relationship (hopefully), start your pitch by talking about you and the team. Tell your story. Highlight your professional accomplishments, industry and functional areas of expertise, and any unique insights you have about the problem you are solving. In this segment, some founders rely on logos (Stanford, Google, McKinsey), others rely on pedigree and experience (X years leading B2B Sales teams), and some on results and exits. The main goal is to help the investor understand why you are the right person to bet on, right now.</p>



<p><strong>Product –&nbsp; your solution to an existing problem</strong></p>



<p>After convincing the investor that you have done other cool things <em>before this</em>, the next step is showing that you can do <strong><em>this. </em></strong>To do that, you need to answer two questions. First, is it even possible to build this solution? Second, can <strong>you</strong> build this? If the answer to either of those is no, it might be hard to raise money for your idea. Otherwise, this is the perfect time to talk about the specifics of your product. What it does. How it works. How it solves the problem. How much of it have you built so far. This last part is important because it reflects your ability to move fast and execute. If you started the company in 2018, and it’s 2020, and you don’t have an MVP yet, it raises red flags. If you can convince the investor that your product truly solves the problem and you have made meaningful progress towards putting this product in the hands of the customers, this section is successful.</p>



<p><strong>Market – customers care about your solution</strong></p>



<p>If the (all too) recent demise of Quibi proves anything, it is that building a cool new product isn’t enough to attract customers. As a result, you need to first highlight the segments that face the problem you have set out to solve. It’s okay if it’s not a precisely defined market, it’s helpful to show your thinking around how you are slicing and dicing the market and how you are creating your ideal customer profile (VCs will use this in their internal market sizing).&nbsp; Once you show target customers, the next step is to show your progress in reaching them, and their ability and willingness to pay for your solution. The best way to do this is revenue. If you are really early you can use a combo: show how much the problem costs them in dollars and time, and how much you found they are willing to pay through customer interviews. You want the investor to walk away from this segment thinking that you understand the customer profile, their needs and wants, have a strategy for how to reach them, a preliminary understanding of how much the solution is worth to them, and how much traction you have so far with your early adopters.</p>



<p><strong>Fit – okay so you did it once but can you do it again, and again?</strong></p>



<p>Once I hit a full court basketball shot. Is the NBA calling? No, I have never been able to do it again. To succeed as a startup founder you need to sell your product to a customer. Then do it again. Then do it a million more times, atleast. The final key piece of the puzzle is discussing how you can repeatedly get customers to use and pay for your product. In VC-speak this is your go to market strategy. Walk through your process for acquiring customers and how you can repeat it, even if it is not ironclad. If you have a few different channels, mention them all and then highlight your favorite one and why you like it. The secret to sticking the landing here, is to show that you have figured out a way to repeatedly get customers.</p>



<p>By the end of this pitch, ideally, you showcase your founder highlight reel, prove you can build a product that actually solves a problem, demonstrate that customers care about it enough to pay for it, and that you can do it again, and again, and again. Thinking through your pitch in this format will help you modify your pitch for any situation, from the elevator pitch, to a 30 min intro call, to an hour long partner meeting.</p>
	</div><div>
			<!-- .entry-auhtor -->
		<p><strong>Published</strong>
			<time datetime="2020-10-24T21:23:16-04:00">October 24, 2020</time><time datetime="2020-10-26T09:40:30-04:00">October 26, 2020</time>		</p><!-- .site-posted-on -->
	</div></div>]]>
            </description>
            <link>https://parajmathur.com/2020/10/24/art-of-seed-pitch/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035887</guid>
            <pubDate>Mon, 09 Nov 2020 15:43:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Oneapi.jl – Native Julia Support for Intel GPUs]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25035875">thread link</a>) | @KenoFischer
<br/>
November 9, 2020 | https://juliagpu.org/2020-11-05-oneapi_0.1/ | <a href="https://web.archive.org/web/*/https://juliagpu.org/2020-11-05-oneapi_0.1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><main id="main"><i data-feather="calendar"></i><time datetime="2020-11-05">Nov 5, 2020</time><br><i data-feather="edit-2"></i>Tim Besard<p>We’re proud to announce the first version of oneAPI.jl, a Julia package for programming
accelerators with the <a href="https://www.oneapi.com/">oneAPI programming model</a>. It is currently
available for select Intel GPUs, including common integrated ones, and offers a similar
experience to CUDA.jl.</p><p>The initial version of this package, v0.1, consists of three key components:</p><ul><li>wrappers for the oneAPI Level Zero interfaces;</li><li>a compiler for Julia source code to SPIR-V IR;</li><li>and an array interface for convenient data-parallel programming.</li></ul><p>In this post, I’ll briefly describe each of these. But first, some essentials.</p><h2 id="installation">Installation</h2><p>oneAPI.jl is currently only supported on 64-bit Linux, using a sufficiently recent kernel,
and requires Julia 1.5. Furthermore, it currently only supports a limited set of Intel GPUs:
Gen9 (Skylake, Kaby Lake, Coffee Lake), Gen11 (Ice Lake), and Gen12 (Tiger Lake).</p><p>If your Intel CPU has an integrated GPU supported by oneAPI, you can just go ahead and
install the oneAPI.jl package:</p><pre><code>pkg&gt; add oneAPI
</code></pre><p>That’s right, no additional drivers required! oneAPI.jl ships its own copy of the <a href="https://github.com/intel/compute-runtime">Intel
Compute Runtime</a>, which works out of the box on
any (sufficiently recent) Linux kernel. The initial download, powered by Julia’s artifact
subsystem, might take a while to complete. After that, you can import the package and start
using its functionality:</p><pre><code>julia&gt; using oneAPI

julia&gt; oneAPI.versioninfo()
Binary dependencies:
- NEO_jll: 20.42.18209+0
- libigc_jll: 1.0.5186+0
- gmmlib_jll: 20.3.2+0
- SPIRV_LLVM_Translator_jll: 9.0.0+1
- SPIRV_Tools_jll: 2020.2.0+1

Toolchain:
- Julia: 1.5.2
- LLVM: 9.0.1

1 driver:
- 00007fee-06cb-0a10-1642-ca9f01000000 (v1.0.0, API v1.0.0)

1 device:
- Intel(R) Graphics Gen9
</code></pre><h2 id="the-onearray-type">The <code>oneArray</code> type</h2><p>Similar to CUDA.jl’s <code>CuArray</code> type, oneAPI.jl provides an array abstraction that you can
use to easily perform data parallel operations on your GPU:</p><pre><code>julia&gt; a = oneArray(zeros(2,3))
2×3 oneArray{Float64,2}:
 0.0  0.0  0.0
 0.0  0.0  0.0

julia&gt; a .+ 1
2×3 oneArray{Float64,2}:
 1.0  1.0  1.0
 1.0  1.0  1.0

julia&gt; sum(ans; dims=2)
2×1 oneArray{Float64,2}:
 3.0
 3.0
</code></pre><p>This functionality builds on the <a href="https://github.com/JuliaGPU/GPUArrays.jl/">GPUArrays.jl</a>
package, which means that a lot of operations are supported out of the box. Some are still
missing, of course, and we haven’t carefully optimized for performance either.</p><h2 id="kernel-programming">Kernel programming</h2><p>The above array operations are made possible by a compiler that transforms Julia source code
into SPIR-V IR for use with oneAPI. Most of this work is part of
<a href="https://github.com/JuliaGPU/GPUCompiler.jl">GPUCompiler.jl</a>. In oneAPI.jl, we use this
compiler to provide a kernel programming model:</p><pre><code>julia&gt; function vadd(a, b, c)
           i = get_global_id()
           @inbounds c[i] = a[i] + b[i]
           return
       end

julia&gt; a = oneArray(rand(10));

julia&gt; b = oneArray(rand(10));

julia&gt; c = similar(a);

julia&gt; @oneapi items=10 vadd(a, b, c)

julia&gt; @test Array(a) .+ Array(b) == Array(c)
Test Passed
</code></pre><p>Again, the <code>@oneapi</code> macro resembles <code>@cuda</code> from CUDA.jl. One of the differences with the
CUDA stack is that we use OpenCL-style built-ins, like <code>get_global_id</code> instead of
<code>threadIdx</code> and <code>barrier</code> instead of <code>sync_threads</code>. Other familiar functionality, e.g. to
reflect on the compiler, is available as well:</p><pre><code>julia&gt; @device_code_spirv @oneapi vadd(a, b, c)
; CompilerJob of kernel vadd(oneDeviceArray{Float64,1,1},
;                            oneDeviceArray{Float64,1,1},
;                            oneDeviceArray{Float64,1,1})
; for GPUCompiler.SPIRVCompilerTarget

; SPIR-V
; Version: 1.0
; Generator: Khronos LLVM/SPIR-V Translator; 14
; Bound: 46
; Schema: 0
               OpCapability Addresses
               OpCapability Linkage
               OpCapability Kernel
               OpCapability Float64
               OpCapability Int64
               OpCapability Int8
          %1 = OpExtInstImport "OpenCL.std"
               OpMemoryModel Physical64 OpenCL
               OpEntryPoint Kernel
               ...
               OpReturn
               OpFunctionEnd
</code></pre><h2 id="level-zero-wrappers">Level Zero wrappers</h2><p>To interface with the oneAPI driver, we use the <a href="https://github.com/oneapi-src/level-zero">Level Zero
API</a>. Wrappers for this API is available under the
<code>oneL0</code> submodule of oneAPI.jl:</p><pre><code>julia&gt; using oneAPI.oneL0

julia&gt; drv = first(drivers())
ZeDriver(00000000-0000-0000-1642-ca9f01000000, version 1.0.0)

julia&gt; dev = first(devices(drv))
ZeDevice(GPU, vendor 0x8086, device 0x1912): Intel(R) Graphics Gen9
</code></pre><p>This is a low-level interface, and importing this submodule should not be required for the
vast majority of users. It is only useful when you want to perform very specific operations,
like submitting an certain operations to the command queue, working with events, etc. In
that case, you should refer to the <a href="https://spec.oneapi.com/level-zero/latest/index.html">upstream
specification</a>; The wrappers in the
<code>oneL0</code> module closely mimic the C APIs.</p><h2 id="status">Status</h2><p>Version 0.1 of oneAPI.jl forms a solid base for future oneAPI developments in Julia. Thanks
to the continued effort of generalizing the Julia GPU support in packages like GPUArrays.jl
and GPUCompiler.jl, this initial version is already much more usable than early versions of
CUDA.jl or AMDGPU.jl ever were.</p><p>That said, there are crucial parts missing. For one, oneAPI.jl does not integrate with any
of the vendor libraries like oneMKL or oneDNN. That means several important operations, e.g.
matrix-matrix multiplication, will be slow. Hardware support is also limited, and the
package currently only works on Linux.</p><p>If you want to contribute to oneAPI.jl, or run into problems, check out the GitHub
repository at <a href="https://github.com/JuliaGPU/oneAPI.jl">JuliaGPU/oneAPI.jl</a>. For questions,
please use the <a href="https://discourse.julialang.org/c/domain/gpu">Julia Discourse forum</a> under
the GPU domain and/or in the #gpu channel of the <a href="https://julialang.org/community/">Julia
Slack</a>.</p></main></div></div>]]>
            </description>
            <link>https://juliagpu.org/2020-11-05-oneapi_0.1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035875</guid>
            <pubDate>Mon, 09 Nov 2020 15:42:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Architecture Playbook]]>
            </title>
            <description>
<![CDATA[
Score 237 | Comments 46 (<a href="https://news.ycombinator.com/item?id=25035752">thread link</a>) | @yarapavan
<br/>
November 9, 2020 | https://nocomplexity.com/documents/arplaybook/index.html | <a href="https://web.archive.org/web/*/https://nocomplexity.com/documents/arplaybook/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>
        
          By Maikel Mardjan<br>
        
            © Copyright 2018,2019, 2020 BM-Support.org. Created by Maikel Mardjan. This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License (cc-by-sa).<br>
      </p>
  </div></div>]]>
            </description>
            <link>https://nocomplexity.com/documents/arplaybook/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035752</guid>
            <pubDate>Mon, 09 Nov 2020 15:31:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[SC20: Supercomputing Virtual Conference Now Live]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035671">thread link</a>) | @ArtWomb
<br/>
November 9, 2020 | https://www.eventscribe.net/2020/SC20/index.asp? | <a href="https://web.archive.org/web/*/https://www.eventscribe.net/2020/SC20/index.asp?">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
						<p>This session is not in your schedule.</p>
					</div></div>]]>
            </description>
            <link>https://www.eventscribe.net/2020/SC20/index.asp?</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035671</guid>
            <pubDate>Mon, 09 Nov 2020 15:23:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Start with “No”]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035670">thread link</a>) | @jerodsanto
<br/>
November 9, 2020 | https://www.dylanpaulus.com/start-with-no/ | <a href="https://web.archive.org/web/*/https://www.dylanpaulus.com/start-with-no/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><section><div><blockquote>
<p>Our default response to any idea that comes up should be: “Interesting. Maybe some day.” In other words, a very soft “no” that leaves all our options open. We don’t put it in a backlog. We give it space so we can learn whether it’s really important and what it might entail.</p>
</blockquote>

<p>Story time. At a previous employer I had a boss who I thought was pretty tough. Not to me, but our users. No matter the request his response was always the same, "No." Eventually, we implemented around 40% of the requests, but the constant rejection of these requests stuck with me. Now that I lead a team I notice myself using the same mindset. "No" isn't negative, and it's not dismissive. "No" is a way of protecting the team, the project, and the users.</p>
<h3 id="No"><a href="#No" aria-label="No permalink"></a>No?</h3>
<p>There is a natural progression to projects that looks something like this. First, the honeymoon phase. The pastures are green; the project has no technical debt and every feature is a great idea. As we add functionality our user base grows. If we're lucky enough to get traction we move to phase two, the monolith. Our application now handles a bunch of use-cases. It does your taxes, picks up your dog's poop, and reminds you to call your mom. Only power users can fully grasp the purpose of the application. The technical debt is increasing with every new feature added. Finally, we get to the last phase. The application has become so bloated that it's hard to develop new features. New users aren't willing to learn the ins-and-outs of the product. A new startup comes around with a trimmed down version of your application. It's so easy to use! Around this time users are jumping ship.</p>
<figure>
    <span>
      <span></span>
  <picture>
        <source srcset="https://www.dylanpaulus.com/static/2c6a45f0b16beb2d0378247e433e93e4/c85cb/feature-graph.webp 300w,
https://www.dylanpaulus.com/static/2c6a45f0b16beb2d0378247e433e93e4/b0a15/feature-graph.webp 500w" sizes="(max-width: 500px) 100vw, 500px" type="image/webp">
        <source srcset="https://www.dylanpaulus.com/static/2c6a45f0b16beb2d0378247e433e93e4/f93b5/feature-graph.jpg 300w,
https://www.dylanpaulus.com/static/2c6a45f0b16beb2d0378247e433e93e4/41099/feature-graph.jpg 500w" sizes="(max-width: 500px) 100vw, 500px" type="image/jpeg">
        <img src="https://www.dylanpaulus.com/static/2c6a45f0b16beb2d0378247e433e93e4/41099/feature-graph.jpg" alt="Diagram showing a product's lifecycle" title="Diagram showing a product's lifecycle" loading="lazy">
      </picture>
    </span>
    <figcaption>Diagram showing a product's lifecycle</figcaption>
  </figure>
<p>Saying "no" is a buffer in our development process. It gives us, the developer, time to think of an optimal solution. Could a feature being asked for be solved in a way that satisfies multiple feature requests? Is there some domain logic we have yet to understand? Does the feature really make sense in <em>this</em> application? All these questions help reduce the lifecycle described above. It reduces feature churn by keeping the application simple, and by solving issues the users actually have.</p>
<p>Be mindful of how features interacts within your application. Nothing comes without a cost--especially in software development.</p>
<h3 id="No-1"><a href="#No-1" aria-label="No 1 permalink"></a>No.</h3>
<p>"No" is a way of protecting people--the users and the development team. There is a gradient of experience when it comes to using your application. Users will be complete beginners, and others will be power users. As a product owner we need to balance these two extremes. If the application is too power-user-centric, then it's harder for new users to grok. When the application is too hand-holding, power users become frustrated. Saying "no" to one extreme is a way of keeping the scales balanced. But, be mindful of what you say "no" to. Some features may be vital to the application's success. This is part of the art.</p>
<p>"No" gives time to find the optimal solution to a problem. Users ask for a solution to <strong>X</strong> and suggest implementing <strong>Y</strong>, but what they really need is <strong>K</strong>. Feature requests give us an opportunity to improve processes and workflows. Our user's are domain experts, but are prone to do things they way they've "always done them". We can come in and find ways to simplify. Ask the user, "do we really need this step?" or "Could the system handle that instead of a person?" Once code is written and users get accustomed to the feature, it's really hard to change. Do this step upfront.</p>
<p>"No" keeps the development team doing meaningful work. Spending a sprint or two implementing a feature nobody uses or finds frustrating is demoralizing. Think about the workflow before writing any code. Question the feature and the users. Does the application need this? Could we implement it in a simple way? Reduce code churn; reduce developer churn.</p>
<h3 id="No-2"><a href="#No-2" aria-label="No 2 permalink"></a>No!</h3>
<p>Work needs to get done. At some point you need to say "yes", but instead of taking on every task, you get to be mindful on what to prioritize. You get to take on the tasks that add the most business value and delight users. Use "no" as a tool.</p></div></section></section></div>]]>
            </description>
            <link>https://www.dylanpaulus.com/start-with-no/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035670</guid>
            <pubDate>Mon, 09 Nov 2020 15:23:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Are Airlines Ready to Transport Vaccines?]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035662">thread link</a>) | @ilamont
<br/>
November 9, 2020 | https://airlineweekly.com/are-airlines-ready-to-transport-vaccines/ | <a href="https://web.archive.org/web/*/https://airlineweekly.com/are-airlines-ready-to-transport-vaccines/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><div><article id="main-article" itemscope="" itemtype="https://schema.org/CreativeWork"><div itemprop="text"><div><p>The answer is “maybe.” The International Air Transport Association (IATA) <a href="https://www.iata.org/en/pressroom/pr/2020-09-09-01/">says</a> the airline industry could be ready to air ship billions of doses of Covid-19 vaccines, when they’re ready to be administered, but only if governments around the world coordinate now on transport policies.</p><p>The logistical challenge of transporting billions of vaccines is enormous. First, the sheer capacity required. IATA estimates that 8,000 cargo-only Boeing 747s would be needed to carry enough doses for the world’s 7.6 billion people. It’s almost needless to say that there aren’t 8,000 B747 freighters (given that Boeing only built just under 1,600 B747s of all types in the 50 years since the aircraft launched).</p><p>Most air freight is carried not by freighters but in the belly holds of passenger aircraft. Since the pandemic started, airlines have slashed international routes, resulting in a cargo capacity crunch just as cargo demand and profits started to rise. Many airlines, including United, American, <a href="https://airlineweekly.com/cargo-buoys-turkish-airlines/">Turkish</a>, Emirates, and others, have operated special cargo-only flights since the pandemic begin, converting passenger cabins to temporary cargo holds (and in fact, cargo led to rare profits in the industry for <a href="https://airlineweekly.com/cargo-fuels-korean-asiana-profits/">Korean and Asiana</a>). Global air freight capacity was down 31% in July, the latest month for which data are available, from 2019, <a href="https://www.iata.org/en/pressroom/pr/2020-08-31-01/">IATA said</a>. </p><p>A second challenge is maintaining the “cold chain,” or keeping the vaccines cool from manufacture to delivery. It’s not a matter of life and death if the cold chain fails for a shipment of fresh fruit. It could be if a shipment of vaccines is exposed to high temperatures, during land transport to the airport, on the tarmac, or even at the delivery airport if the shipment is held up by customs. Early in the pandemic, reports that personal protective equipment was stalled on tarmacs raised outrage, even though most of the equipment survived. Vaccines would not survive days on the tarmac, the industry warns. Freight industry analysts have said the cold chain already is stretched and could further imperil the safe transport of vaccines.</p><p>And speaking of safety, airlines transporting vaccines will be charged with the safety of what will arguably be the world’s most valuable commodity. Security at all points of the logistics chain needs to be strengthened and can’t be solely airlines’ responsibility, IATA said.</p><p>The good news is that airlines have parked thousands of aircraft — including hundreds of large widebodies — that can be pressed into vaccine-transport duty. But it takes time to return aircraft parked in the desert to service. And it takes employees to return those aircraft to service and to fly them, just as airlines around the world are in the process of furloughing tens of thousands of workers. </p><p>None of this is insurmountable, IATA argues. Governments need to act and coordinate on security, mandating cargo flights, and loosening some quarantine and travel restrictions to allow flight crews more flexibility to operate cargo flights. But the time to act is now, not when vaccines are ready for distribution. “If borders remain closed, travel curtailed, fleets grounded and employees furloughed, the capacity to deliver life-saving vaccines will be very much compromised,” said IATA Director General Alexandre de Juniac. &nbsp;&nbsp;</p><p><a href="https://airlineweekly.com/subscribe/">Subscribe Now to Airline Weekly</a></p></div></div> </article></div></div></div></div></div>]]>
            </description>
            <link>https://airlineweekly.com/are-airlines-ready-to-transport-vaccines/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035662</guid>
            <pubDate>Mon, 09 Nov 2020 15:22:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I don't care what Elon Musk thinks anymore]]>
            </title>
            <description>
<![CDATA[
Score 58 | Comments 66 (<a href="https://news.ycombinator.com/item?id=25035658">thread link</a>) | @avthar
<br/>
November 9, 2020 | https://avthar.com/blog/dont-outsource-thinking | <a href="https://web.archive.org/web/*/https://avthar.com/blog/dont-outsource-thinking">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-f18334b57a4385ee2a76"><div><p><em>This blog originally appeared in my </em><a href="https://avthar.substack.com/"><em>weekly newsletter</em></a><em>, where I share ideas I’m reflecting upon, experiments I’m trying and lessons I’ve learned, all to help you level up your own life. To get posts like this straight to your inbox, </em><a href="https://avthar.substack.com/"><em>subscribe here</em></a><em>.</em></p><p>—</p><p>Youtube recommended to me&nbsp;<a href="https://www.youtube.com/watch?v=vVnDE8wSrVo">what Elon Musk would work on if he was 22 years old today</a>. The video has almost 3 million views. In the past, I would’ve immediately watched the whole video, gotten inspired by what Elon thought were industries important to the future of humanity and then spent a ton of time working on that and telling everyone about it. All because Elon Musk thought it was important.&nbsp;</p><p><strong>This is outsourcing your thinking</strong>. This trick served me well in highschool and throughout college, but it’s not sustainable for long term success and happiness.&nbsp;</p><p>There are two problems with outsourcing your thinking. Continuing with the example of past me, there are things I’m already interested in, have&nbsp;<a href="https://nav.al/specific-knowledge">specific knowledge</a>&nbsp;about or possess a competitive advantage in, that won’t be mentioned on Elon’s list. Consequently, the first problem is that I will look down on those things as less meaningful and important and not pursue them, despite my better suitability and chances of success in those areas.</p><p>The second problem with outsourcing your thinking is that once the novelty of the problem fades away, I’d be faced with navigating difficulty, naysayers and the friction of creating something new, without an internal compass to guide me toward the correct paths to take. Put simply, Elon Musk isn’t there to talk me through what he thinks the path forward to be. This all stems from the issue that I pursued something, not because I had interest in that thing, actually enjoyed it or thought it was important, but because Elon Musk (or whoever else) thought it was important to work on. And I followed his thinking, rather than thinking for myself.</p><p>The reason this is important is because most success in business is having&nbsp;<strong>product-market-founder fit</strong>, not just about working on what’s world changing or hot. It’s about building a product that solves a burning problem for the right market and&nbsp;<strong>being the right person, with the right intuition</strong>&nbsp;to bring that product to market, operate that company and delight those customers. Even if you’re working on an important problem, if you don’t have conviction that comes from your own mental models, you’ll get burned when chaos hits. Just ask all the crypto ‘experts’ of 2016/17.&nbsp;<strong>It’s better to build something that’s an expression of yourself, rather than something others think is smart.</strong></p><p>Outsourcing your thinking is a manifestation of the error of trusting others more than we trust ourselves.&nbsp;<a href="https://avthar.com/blog/jw-curation">Josh Waitzkin</a>&nbsp;talks about how this phenomenon of outsourcing your thinking happens all the time in the investing world:</p><blockquote><p>“<em>If you take investors, there might be an investment, which one from the outside we think is objectively good. But it really isn't objectively good. It has to fit into one's portfolio of investments in a way that emerges from one's own mental models. Otherwise, it is not a form of self expression. Then, when you enter volatility, you're not gonna know what to do with it.</em>” -&nbsp;<a href="https://avthar.com/blog/jw-curation">Josh Waitzkin</a></p></blockquote><p>Elon Musk is a placeholder for anyone telling you what you should do or think. That could be entrepreneurs or VCs you idolize or maybe your parents (especially true if you’re brown). The reality is that you should not care what Elon Musk or anyone else says is important, you should decide for yourself what you should work on, based on following your own curiosity and interest.&nbsp;<strong>Do the hard work of experimenting, exploring and thinking for yourself. Don’t outsource your thinking.</strong></p><p><a href="https://avthar.substack.com/"><em>Subscribe here</em></a><em> to get posts like this straight to your inbox, </em></p></div></div></div>]]>
            </description>
            <link>https://avthar.com/blog/dont-outsource-thinking</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035658</guid>
            <pubDate>Mon, 09 Nov 2020 15:21:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Dying for movies: Suicide highlights labour issues in Canada's VFX sector]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035628">thread link</a>) | @alibarber
<br/>
November 9, 2020 | https://montreal.ctvnews.ca/mobile/dying-for-movies-suicide-highlights-labour-issues-in-canada-s-visual-effects-sector-1.5175793 | <a href="https://web.archive.org/web/*/https://montreal.ctvnews.ca/mobile/dying-for-movies-suicide-highlights-labour-issues-in-canada-s-visual-effects-sector-1.5175793">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p>MONTREAL -- 
	Last April, Malcolm Angell, a 46-year-old New Zealander who moved to Montreal to work in the city's famed visual effects industry, was taken to hospital after attempting suicide.</p>
<p>
	He was back at work two days later at Montreal-based visual effects studio Mill Film, according to his brother, Ivan. A month later -- shortly after learning his mother had a brain tumour and didn't have long to live -- Angell tried to kill himself again. This time he died.</p>
<p>
	Angell's former colleagues allege the work environment at Mill Film was toxic. They say 80-hour workweeks were common, and that Angell was regularly humiliated by his bosses. Ivan says he's certain his brother would have quit -- were it not for a clause in Angell's contract requiring he pay a $35,000 penalty.</p>
<p>
	The story told by Angell's colleagues is not uncommon in Canada's visual effects and animation sectors, according to industry insiders. Long overtime hours, often unpaid, are seen as normal, they say. And employees in these industries are vulnerable -- particularly foreign workers -- who toil on short-term contracts and are afraid to speak up out of fear of not getting hired again.</p>
<p>
	For Vanessa Kelly, a former animator and union organizer in Vancouver's animation industry, Angell's suicide is a sign that something is deeply wrong with the visual effects sector. She said similar issues exist in animation and within video game companies across Canada.</p>
<p>
	"These are movies. Why are people dying for movies?" said Kelly, general director of the Art Babbitt Appreciation Society, which is trying to organize animators in Vancouver.</p>
<p>
	Angell had nearly 20 years experience in film, and got his start working on set during the production of The Lord of The Rings. In August 2019, he moved to Montreal to work in the city's visual effects industry -- one of the largest in the world. Colleagues and friends say it was not long before the job started to get to him.</p>
<p>
	The Canadian Press spoke to three of Angell's former colleagues, who painted a picture of a workplace where Angell was under extreme pressure and where bosses yelled at him during meetings. Complaints to human resources and to senior mangers, they said, went nowhere.</p>
<p>
	The Canadian Press has agreed not to identify those workers because they fear repercussions. All three said people in the industry who speak out against work conditions are frequently blacklisted.</p>
<p>
	"Work kinda sucks," Angell wrote in an email to a friend in New York City in early September 2019. By November, in an email to the same friend, he said he was doing the work of two people. A planned trip to New Zealand for a wedding in February, 2020, was cancelled, his brother Ivan said in a recent interview, because Angell couldn't get the time off work.</p>
<p>
	Ivan Angell said friends noticed a change in his brother by December. The man who was described in an obituary as a "superfriend" who was always smiling, had become a "shadow of himself," he said.</p>
<p>
	Julia Neville, with the International Alliance of Theatrical Stage Employees, said fears of being blacklisted for speaking out in the visual effects industry are legitimate. Visual effects artists are precarious workers, Neville said, because their contracts are typically for one project at a time. "There's always that underlying insecurity," she said. Foreign workers, such as Angell, are "particularly vulnerable."</p>
<p>
	"They can't just cross the street and work for another visual effects house -- their whole ability to work in Canada is tied to a specific employer," Neville said. Much of the film industry is unionized, while the large majority of visual effects artists are not, she explained. Long hours and unpaid overtime "are very common," she said, adding that unfair labour practices are frequent in other entertainment sectors, such as animation, reality television and in commercials.</p>
<p>
	Neville said visual effects companies try to underbid each other for work on projects produced by major movie studios. "That pressure is exerted downward onto the worker," she said. "What ends up happening is there's never enough time allotted to accomplish what you need done."</p>
<p>
	Angell's former colleagues said he was under extreme pressure to complete his part in the movie "Bios," starring Tom Hanks. They said Angell and the team he oversaw had been told by his bosses at Mill Film to finish additional work but hadn't been given more time or money to get it done.</p>
<p>
	Another element that tied Angell to his employer was his contract, a copy of which The Canadian Press viewed. The contract included a clause stating he was liable to pay Mill Film a $35,000 indemnity should he leave in the middle of a project.</p>
<p>
	The indemnity clause identified Angell as a "key member" of the team and indicated that the company would be contractually committing Angell's services to its client. The contract said that for "certain very exceptional and serious" reasons the company could decide to waive the indemnification clause.</p>
<p>
	Adelle Blackett, a law professor at McGill University and labour law expert, said that clause "is deeply disturbing." Quebec's labour standards require employers to provide working conditions that "safeguard employees' dignity, health and well-being," she wrote in an email. "An employee working in conditions of freedom must be able to terminate an employment contract with only minimally necessary restrictions."</p>
<p>
	Technicolor, Mill Film's parent company, did not make anyone available to speak on the record. In an emailed statement, the company said Angell's death was a "traumatic and tragic event for his family, friends and for our team. We mourn his passing and continue to express our deepest condolences to his family."</p>
<p>
	The company said it has introduced a new program aimed at supporting employee mental health since Angell's death -- due to the "severity and isolating nature of the pandemic." Another program has been launched encouraging employees to "call out" inappropriate behaviour, the company said.</p>
<p>
	"Technicolor has had longstanding and robust anti-harassment policies in place in Canada. This specifically includes broad anti-bullying and related anti-retaliation policies, among others," it said. The company said it takes complaints seriously and that it didn't receive any formal complaints about Angell's treatment at Mill Film.</p>
<p>
	Kelly -- who quit animation work in 2017 to pursue a science degree -- said unpaid overtime is common. "In animation and (visual effects) we have major skilled labour shortages," she said, adding that companies often don't have the budget to hire more people. "We have to fill in those gaps with overtime and they don't want to pay us for it."</p>
<p>
	Kelly said she got involved with union organizing after working on a project as a storyboard artist. She said her workload suddenly doubled but her deadline remained the same. The work -- which required hours of unpaid overtime -- damaged her wrist and her eyesight, she said.</p>
<p>
	"My physical body was being harmed, my mental capacity was being harmed and my relationships were being harmed. And I looked around and I said, what is this for? A PBS show for children?"</p>
<p>
	For many workers in animation, the job is a part of their identity, Kelly said. "People don't do this because they just want a job, they do this because they have a skill and a passion.</p>
<p>
	"They eat, live and breathe this." But behind the scenes, she said, "there's blood on the screen."</p>

<p>
	<em>-- this report by The Canadian Press was first published Nov. 5, 2020.</em></p>
                                              </div></div>]]>
            </description>
            <link>https://montreal.ctvnews.ca/mobile/dying-for-movies-suicide-highlights-labour-issues-in-canada-s-visual-effects-sector-1.5175793</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035628</guid>
            <pubDate>Mon, 09 Nov 2020 15:19:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to price your SaaS product]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035529">thread link</a>) | @moeamaya
<br/>
November 9, 2020 | https://www.lennyrachitsky.com/p/saas-pricing-strategy | <a href="https://web.archive.org/web/*/https://www.lennyrachitsky.com/p/saas-pricing-strategy">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><em>👋 Hello, I’m&nbsp;<a href="https://twitter.com/lennysan">Lenny</a>&nbsp;and welcome to a ✨&nbsp;<strong>once-a-month-free-edition&nbsp;</strong>✨ of my newsletter. Each week I humbly tackle reader questions about product, growth, working with humans, and anything else that’s stressing them out at the office.</em></p><p><em>If you’re not a paid subscriber, here’s what you missed this month:</em></p><ol><li><p><em><a href="https://www.lennyrachitsky.com/p/moving-from-ic-product-manager-to">Moving from IC product manager to manager of product managers</a></em></p></li><li><p><em><a href="https://www.lennyrachitsky.com/p/top-5-most-interesting-things-about">Top 5 most interesting things about Booking.com's early growth strategy</a></em></p></li><li><p><em><a href="https://www.lennyrachitsky.com/p/the-most-important-bottom-up-saas">The most important bottom-up SaaS metrics to track (and how to best visualize them)</a></em></p></li></ol><blockquote><h2>Q: I'm building a SaaS product and don't know where to start when pricing it. How should I approach my pricing strategy?</h2></blockquote><p>When this question came in, I took to Twitter to find the smartest person in the world on SaaS pricing…</p><p>Many suggestions came though but one name came up again and again: <a href="https://twitter.com/Patticus">@Patticus</a>, aka Patrick Campbell.</p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F225c2988-0cd8-431b-b69b-a8e7dd1de832_2400x1350.png"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F225c2988-0cd8-431b-b69b-a8e7dd1de832_2400x1350.png" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/225c2988-0cd8-431b-b69b-a8e7dd1de832_2400x1350.png&quot;,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:657394,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null}" alt=""></a></figure></div><p>I cold DM’d Patrick and asked him if he’d be up for writing a guest post. He instantly agreed 🙌🙌🙌</p><p>For those that don’t know Patrick, he is CEO of <a href="https://www.profitwell.com/">ProfitWell</a>, and generally regarded guru on anything SaaS. Unrelated to this post, I started using ProfitWell a couple of months back to track my newsletter metrics, and the amount of insight you get into your subscription business is unreal. AND IT’S FREE.  If you’re running a SaaS business,  definitely <a href="https://www.profitwell.com/">check it out.</a> This is not a sponsored post — I just love the product.</p><p><em>🚨 <strong>Bonus</strong>: Patrick Campbell is doing a live AMA at 5pm PT today (10/27) in our subscriber-only Slack group. Ask Patrick any question you have about pricing your product live!</em></p><p>With that out of the way, let’s dive in!</p><p>—</p><p><em>by Patrick Campbell</em></p><p>Pricing is one of those topics that sits at the nexus of uncomfortable and long-term, which means companies often don’t think about it for far too long. Even when they eventually figure it out, they don’t touch it again for years.</p><p><strong>The most successful companies optimize monetization in some manner every quarter</strong>. You may be thinking, “they change their price every 3 months!?” No, and that's the first lesson of monetization: pricing goes so much further than the actual price. Let me explain.</p><p>If we go to a thirty thousand foot view, you have to think about what you're actually doing with pricing. No matter the business you're in — non-profit, retail, SaaS, DTC, B2B, whatever — you've created some sort of value. You attach a unit of measurement to the value you created: your price. Put simply, your price is the exchange rate on the value you're creating in the world.</p><p>But price doesn’t live in a vacuum. Everything in your business — from sales and marketing to product and finance — is used to drive someone to buy the product at the price you're offering. Dozens of aspects of your business influence your price, and how effectively it converts customers:</p><ol><li><p><strong>The segment and vertical you are targeting</strong>: You can go upmarket to customers who have higher willingness to pay, shift to a vertical that sees more value in your offering, or even change the ideal customer profile entirely.</p></li><li><p><strong>Your product, positioning, and packaging</strong>: You can come out with new features, move features to different tiers, pull features out and make them add-ons, change up your value propositions, etc.</p></li><li><p><strong>Your price</strong>: You can move your price up or down, which will impact conversion obviously, but will also impact the perception of your brand.</p></li></ol><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F637dba83-56b6-434f-906e-330f363cbde7_1600x900.png"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F637dba83-56b6-434f-906e-330f363cbde7_1600x900.png" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/637dba83-56b6-434f-906e-330f363cbde7_1600x900.png&quot;,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null}" alt=""></a></figure></div><p>These are not the only axes when it comes to pricing, but the point is anything that influences the value of your product is involved in pricing and monetization. Cool – I've now given you a college lecture worthy of a tweed blazer, so let's dig into where you should start.</p><p><strong>In the beginning, the actual number you're charging isn't that important. </strong></p><p>There are some exceptions, but for the most part, you should first be figuring out the range you're in: a $10 product, $100 product, $1k product, etc. Don't waste time debating $500 vs. $505, because this doesn't matter as much until you have a stronger foundation beneath you.</p><p>What matters much more is two other questions:</p><ol><li><p>Your <strong>value metric</strong></p></li><li><p>Your ideal <strong>customer profiles and segments</strong></p></li></ol><p>These two elements are the foundation of your monetization and pricing strategy. Let’s explore them individually.</p><h3>Step 1: Determine your Value Metric</h3><p>A “value metric” is essentially what you charge for. For example: per seat, per 1,000 visits, per CPA, per GB used, per transaction, etc. <strong>If you get everything else wrong in pricing, but you get your value metric right, you'll do ok</strong>. It's that important. Partly because it bakes lower churn and higher expansion revenue into your monetization.</p><p>Pricing based on a value metric (vs. a tiered monthly fee) is important because it allows you to make sure you're not charging a large customer the same as you'd charge a small customer.</p><p>If you remember back to your high school or college economics class, the professor put a point on a demand curve for the perfect price and said “the revenue a firm gets is the area under that point.” The problem here is — what about all that other area under the curve? <strong>You’re missing out on that revenue by charging a flat monthly fee.&nbsp;</strong></p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F971baee8-62bc-4007-8a62-b2179faef927_1600x900.png"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F971baee8-62bc-4007-8a62-b2179faef927_1600x900.png" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/971baee8-62bc-4007-8a62-b2179faef927_1600x900.png&quot;,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null}" alt=""></a></figure></div><p>“Good, better, best” pricing is a bit more advantageous, because you end up with three points on our trusty demand curve, and thus more revenue potential. You see this in a lot of retail products who are constrained by being physical goods — the car with the basic package vs. the car with the stereo and sunroof vs. the car with everything. In software, it’s thankfully dying out, but you’ll still see it with mass-market products:&nbsp; Netflix, Adobe Creative Cloud, etc.</p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F24c3c7aa-aab8-40d1-9d5b-147b44cc2669_1600x900.png"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F24c3c7aa-aab8-40d1-9d5b-147b44cc2669_1600x900.png" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/24c3c7aa-aab8-40d1-9d5b-147b44cc2669_1600x900.png&quot;,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null}" alt=""></a></figure></div><p>A value metric however allows you to have essentially infinite price points — maximizing your revenue potential. In practice, you’ll never show infinite price points on your pricing page, sales deck, or mobile conversion page, but you may have a customer come in at a certain level and then grow.</p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6d0315d-e46a-4f3e-b03e-3bc46c2609d5_1600x900.png"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6d0315d-e46a-4f3e-b03e-3bc46c2609d5_1600x900.png" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/e6d0315d-e46a-4f3e-b03e-3bc46c2609d5_1600x900.png&quot;,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null}" alt=""></a></figure></div><p>Value metrics also bake growth directly into how you charge because as usage or the amount of value received goes up (and those are not the same thing), the customer pays more. If they end up using or consuming less, they pay less (and thus avoid churning). This is why companies using value metrics are typically growing at <a href="https://www.profitwell.com/recur/all/outcome-based-value-metrics-for-growth">double the rate with half the churn and 2x the expansion revenue</a> when compared to companies that charge a flat fee or where the only difference between their pricing tiers are features.&nbsp;</p><h4><strong>How to determine your value metric</strong></h4><p>To determine your value metric, think about the <em>ideal essence of value</em> for your product&nbsp;— what value are you directly providing your customer? </p><p>In B2B, it's likely going to be money saved, revenue gained, time saved, etc. In DTC, it may be the joy you bring them, fitness achieved, increased efficiency, etc. Obviously, we can't measure all of these, but if you can, <em>and</em> your customer trusts your measurement (meaning you say you saved them $100 and they agree you saved them $100), that’s your value metric.</p><p>As an example, the perfect value metric for ProfitWell Retain (our churn recovery product) is how much churn we recover for you. We can measure this, and our customers agree to the measurement, so we can charge on that axis. Other pure value metric products include <a href="https://mainstreet.us/">MainStreet</a>, which handles government paperwork to automatically get you back tax credits — you pay a percentage of the money saved.&nbsp;</p><p>Most of you won't have a pure value metric, so the next step is to find a proxy for that metric. Take for example <a href="https://www.hubspot.com/">HubSpot</a>’s marketing product. Their pure value metric is the amount of revenue their tool drives for your business. This is hard to measure and hard for the customer to agree to in terms of what percentage of credit HubSpot deserves for revenue from a blog post. Proxies for HubSpot are things like the number of contacts, number of visits, number of users, etc.&nbsp;</p><p>To find the right proxy metric, you want to come up with 5-10 proxies and then talk to your customers and prospects. You’ll typically find 1-2 of these pricing metrics will be most preferred amongst your target customers. You then want to make sure those 1-2 also make sense from a growth perspective. Your larger customers should be using/getting more of the metric, whereas your smaller customers should be using/getting less of the metric. You also want to make sure the metric encourages retention.</p><p>When we look at HubSpot, if they were to primarily price on “number of seats”, folks could share a login and HubSpot wouldn’t make much more money on large customers vs. small. Ironically they wouldn’t get as many people invested into HubSpot, because there’d be friction to adding additional seats. Instead, if they give unlimited seats and price based on “number of contacts” there’s minimal friction to getting as many people into HubSpot as possible to do activities (e.g. blog posts, email campaigns, landing pages, etc.)&nbsp;that then produce contacts.</p><p>The result: HubSpot’s marketing product’s value metric is “contacts”, which ensures growth is baked directly into how they make money. The usage drives the metric, which therein drives revenue. Most importantly customers small, medium, and large are all paying at the point they see value and then can grow.</p><p>Some other examples:</p><ol><li><p><a href="https://wistia.com/">Wistia</a> charges by the number of videos or channels you use/have</p></li><li><p><a href="https://zapier.com/">Zapier</a> invented the concept of zap (connection of software) and charge based on time to connect</p></li><li><p><a href="https://www.bbc.com/news/technology-29551380">Theater in Barcelona charged based on the number of laughs</a></p></li><li><p><a href="https://www.husqvarna.com/">Husqvarna</a> charges based on time for lawn care products vs. making you buy them</p></li><li><p><a href="https://www.rolls-roycemotorcars.com/en_US/home.html">Rolls Royce</a> charges per mile for airplane engines. They own the engines on the plane you own and do all the maintenance. Cool model.</p></li><li><p><a href="https://www.freshpatch.com/">Fresh Patch</a> charges based on the amount of grass you want per month for your dog — yes they deliver grass to you monthly</p></li></ol><p>As a side note, you should stop pricing based on seats for products where each seat doesn’t provide a unique experience. For instance, in a CRM if I log in to the AE sitting next to me’s account, I can’t really do my work because I’m only seeing their leads and accounts. Conversely, if I log in to our marketing manager’s …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.lennyrachitsky.com/p/saas-pricing-strategy">https://www.lennyrachitsky.com/p/saas-pricing-strategy</a></em></p>]]>
            </description>
            <link>https://www.lennyrachitsky.com/p/saas-pricing-strategy</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035529</guid>
            <pubDate>Mon, 09 Nov 2020 15:09:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Aaron Swartz would have been 34 years old today]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035500">thread link</a>) | @paulcarroty
<br/>
November 9, 2020 | https://www.aaronswartzday.org/remembering-aaron-in-2020/ | <a href="https://web.archive.org/web/*/https://www.aaronswartzday.org/remembering-aaron-in-2020/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<p><em><strong>This year’s <a href="https://www.aaronswartzday.org/invitation/">Aaron Swartz Day and International Hackathon</a> will take place online from 10am-5pm PST on November 14, 2020. </strong></em></p>
<p><strong>It’s free and </strong><span data-key="1936"><em data-slate-leaf="true"><strong data-slate-leaf="true">livestreamed on our </strong></em></span><a href="https://www.youtube.com/channel/UCAX6qlJGN1g1EAXtRoZVN7w" data-key="1937"><span data-key="1938"><em data-slate-leaf="true"><strong data-slate-leaf="true">YouTube</strong></em></span></a><span data-key="1939"><em data-slate-leaf="true"><strong data-slate-leaf="true"> and </strong></em></span><a href="https://www.facebook.com/AaronSwartzDay/" data-key="1940"><span data-key="1941"><em data-slate-leaf="true"><strong data-slate-leaf="true">Facebook</strong></em></span></a><span data-key="1942"><em data-slate-leaf="true"><strong data-slate-leaf="true"> channels. </strong></em></span></p>
<p><em><strong><a href="https://www.eventbrite.com/e/aaron-swartz-day-and-international-hackathon-2020-tickets-128384147441">Register on Eventbrite</a> to have the streaming links emailed to you in the morning on Saturday, November 14. You can also just check our <a href="https://twitter.com/aaronswartzday">@AaronSwartzDay </a>Twitter or look on our home page here at <a href="http://aaronswartzday.org/">aaronswartzday.org</a> on that morning.<br>
</strong></em></p>
<p><a href="https://www.aaronswartzday.org/wp-content/uploads/2014/11/Aaron.png"><img loading="lazy" src="https://www.aaronswartzday.org/wp-content/uploads/2014/11/Aaron.png" alt="" width="566" height="443" srcset="https://www.aaronswartzday.org/wp-content/uploads/2014/11/Aaron.png 566w, https://www.aaronswartzday.org/wp-content/uploads/2014/11/Aaron-300x234.png 300w" sizes="(max-width: 566px) 100vw, 566px"></a></p>
<p>Today would have been Aaron’s 34th birthday, but instead we mourn our friend and wonder what could have been, had he not taken his own life seven years ago after being terrorized by a career-driven prosecutor and U.S. Attorney who decided to just make shit up, make an example out of Aaron, impress their bosses and further their own careers.</p>
<p>As it turns out though, <strong><a href="https://www.aaronswartzday.org/aaron-swartz-was-authorized/">Aaron’s downloading wasn’t even illegal</a></strong>, as he was a Harvard Ethics Fellow at the time and Harvard and MIT had contractual agreements allowing Aaron to access those materials en masse.</p>
<p>But all this didn’t come to light until it was too late.</p>
<p>Aaron was careful not to tell his friends too much about his case for fear he would involve them in the quagmire. In truth, we wouldn’t have minded doing anything we could to help him, but we didn’t realize he needed help, and that his grand jury’s runaway train had gone so far off the rails.</p>
<p>We should have known though, as Grand Juries are <strong><a href="https://www.releasechelsea.com/faq/e/">a dangerous, outdated practice</a></strong> that <strong><a href="https://www.releasechelsea.com/faq/h/">give prosecutors unlimited power</a></strong>, making it easy to manipulate the way that witnesses and evidence are presented to the Grand Jury and convince jurors of almost anything. These kinds of proceedings also often violate <strong><a href="https://www.releasechelsea.com/faq/i/">the subject and witness’ constitutional rights</a></strong> in different ways. For these reasons, most civilized countries have transitioned away from them<strong> <a href="https://www.releasechelsea.com/faq/j/">in favor of preliminary hearings</a></strong>.</p>
<p>We learned many other lessons from his case, after the smoke had cleared. We learned that Aaron’s Grand Jury prosecutor, Assistant U.S. Attorney Stephen Heymann, and the U.S. Attorney in charge of his case, Carmen Ortiz, were so obsessed with trying to make names for themselves, they were&nbsp; willing to fabricate charges and evidence in order get indictments that would otherwise be unachievable.</p>
<p>As Dan Purcell explained:</p>
<blockquote><p>“Steve Heymann did what bureaucrats and functionaries often choose to do. He wanted make a big case to justify his existence and justify his budget. The casualties be damned…</p>
<p>Our bottom line was going to be that Aaron had done only what MIT permitted him to do. He hadn’t gained unauthorized access to anything. He had gained access to JSTOR with full authorization from MIT. Just like anyone in the jury pool, anyone reading Boing Boing, or anyone in the country could have done.</p>
<p>We hoped that the jury would understand that and would acquit Aaron, and it quickly became obvious to us that there really wasn’t going to be opportunity to resolve the case short of trial because Steve Heymann was unreasonable.”</p></blockquote>
<p>We also learned that MIT was more concerned with their own reputation than standing up for the truth or protecting Aaron. In fact, we learned that MIT decided to <em>assist the government with its case</em> <em>against Aaron</em>, rather than helping him by pressuring to Feds to drop the case, even after JSTOR had made it clear it did not wish to prosecute.</p>
<p>We know all this because <strong><a href="https://www.aaronswartzday.org/kevin-poulsen-2014/">Kevin Poulsen explained to us how he had to sue the Department of Homeland Security</a></strong> to get access to documents in <strong><a href="http://swartzfiles.com/">Aaron’s FBI file</a></strong>, and that <strong><a href="https://boingboing.net/2013/07/18/mit-blocking-release-of-aaron.html">MIT blocked their release</a></strong> – intervening as a third party – and demanding to get a chance to further redact them before they were released to Kevin – and the Judge granted their request! Only time will tell what MIT was so worried about, but its behavior suggests that there may have been some kind of cover-up&nbsp; regarding its involvement in Aaron’s case.</p>
<p>Most recently, thanks to Property of the People’s Ryan Shapiro, we learned that <strong><a href="https://www.aaronswartzday.org/jan11rawthought/#Aarondocs">Aaron had an erroneous code in his FBI record</a></strong>&nbsp; that meant “International Terrorism involving Al Qaeda” – deriving from his sending a single email to the University of Pittsburg, which might explain why the FBI was so suspicious of him during his case.</p>
<p>There are still many pieces of the puzzle missing, but we won’t stop trying to put it all together. We hope you will join us on November 14th to honor him and learn about his projects and ideas that are still bearing fruit to this day, such as <strong><a href="https://securedrop.org/">SecureDrop</a></strong>, <strong><a href="https://openlibrary.org/">Open Library</a></strong>, and the <strong><a href="https://www.aaronswartzday.org/psp/">Aaron Swartz Day Police Surveillance Project</a></strong>.</p>
<p>Until then, we will continue to come together to help each other and share information, knowledge and resources, and to try to make things better in our world.</p>
<p>Please email us at aaronswartzday at gmail.com or <strong><a href="https://twitter.com/aaronswartzday">DM us on Twitter</a></strong> if you would like to contribute a project to the hackathon – or want to be on the hackathon or speaker Jitsi calls<strong>. </strong></p>
<p>We hope to see you on November 14th!</p>
<h4><strong><em>&nbsp;<a href="https://www.aaronswartzday.org/howl-for-aaron-swartz/">Howl For Aaron Swartz</a> (by Brewster Kahle)</em></strong></h4>
<h5>Howl for Aaron Swartz</h5>
<p><em>Written by Brewster Kahle, shortly after Aaron’s Death, on January 11, 2013.<br>
</em></p>
<p>Howl for Aaron Swartz<br>
New ways to create culture<br>
Smashed by lawsuits and bullying<br>
Laws that paint most of us criminal</p>
<p>Inspiring young leaders<br>
Sharing everything<br>
Living open source lives<br>
Inspiring communities selflessly</p>
<p>Organizing, preserving<br>
Sharing, promoting<br>
Then crushed by government<br>
Crushed by politicians, for a modest fee<br>
Crushed by corporate spreadsheet outsourced business development</p>
<p>New ways<br>
New communities<br>
Then infiltrated, baited<br>
Set-up, arrested</p>
<p>Celebrating public spaces<br>
Learning, trying, exploring<br>
Targeted by corporate security snipers<br>
Ending up in databases<br>
Ending up in prison</p>
<p>Traps set by those that promised change<br>
Surveillance, wide-eyes, watching everyone now<br>
Government surveillance that cannot be discussed or questioned<br>
Corporate surveillance that is accepted with a click</p>
<p>Terrorists here, Terrorists there<br>
More guns in schools to promote more guns, business<br>
Rendition, torture<br>
Manning, solitary, power</p>
<p>Open minds<br>
Open source<br>
Open eyes<br>
Open society</p>
<p>Public access to the public domain<br>
Now closed out of our devices<br>
Closed out of owning books<br>
Hands off<br>
Do not open<br>
Criminal prosecution</p>
<p>Traps designed by the silicon wizards<br>
With remarkable abilities to self-justify<br>
Traps sprung by a generation<br>
That vowed not to repeat<br>
COINTELPRO and dirty tricks and Democratic National Conventions</p>
<p>Government-produced malware so sophisticated<br>
That career engineers go home each night thinking what?<br>
Saying what to their families and friends?</p>
<p>Debt for school<br>
Debt for houses<br>
Debt for life<br>
Credit scores, treadmills, with chains</p>
<p>Inspiring and optimistic explorers navigating a sea of traps set by us<br>
I see traps ensnare our inspiring generation<br>
Leaders and discoverers finding new ways and getting crushed for it</p>

	</div></div>]]>
            </description>
            <link>https://www.aaronswartzday.org/remembering-aaron-in-2020/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035500</guid>
            <pubDate>Mon, 09 Nov 2020 15:07:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Elixir Flavoured Erlang: Erlang to Elixir Transpiler]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035483">thread link</a>) | @marianoguerra
<br/>
November 9, 2020 | http://marianoguerra.org/posts/elixir-flavoured-erlang-an-erlang-to-elixir-transpiler/ | <a href="https://web.archive.org/web/*/http://marianoguerra.org/posts/elixir-flavoured-erlang-an-erlang-to-elixir-transpiler/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Last year I was invited to <a href="https://en.elixirconf.la/">ElixirConf Latin America in Colombia</a> to give a talk,
I proposed to also give a tutorial about <a href="https://riak-core-lite.github.io/">Riak Core</a>
and they said that it should be in Elixir, so I started looking into Elixir to
translate my Riak Core material to it.</p><p>This year I was invited to give another talk about languages on the Erlang virtual machine at <a href="https://www.codebeambr.com/">Code BEAM Brasil 2020</a> and I thought it would be a good idea
to continue working on it and maybe announce it at the talk.</p><p>To measure progress I built some scripts that would transpile the Erlang
standard library to Elixir and then try compiling the resulting modules with
the Elixir compiler, I would pick one compiler error, fix it and try again.</p><p>With this short feedback loop and a counter that told me how many modules
compiled successful it was just a matter of finding errors and fixing them.
At the beginning each fix would remove lot of compiler errors and some times
surface new ones, after a while each error was a weird corner case and progress
slowed.</p><p>Some days before the talk I managed to transpile all of Erlang/OTP and 91% of
the Elixir translations compiled successfully.</p><p>The result is of course <a href="https://github.com/marianoguerra/efe">Elixir Flavoured Erlang</a>, but
as a side effect I have Erlang/OTP in Elixir, so I decided to publish it too.</p><p>The objective of this repository is to allow Elixir programmers to read Erlang
code for projects they use, most of the code compiles but I can't ensure that
it behaves identically to the original source.</p><p>While writing the readme of efe I needed some example that wasn't OTP so I
decided to also transpile a widely used project on Erlang and Elixir: the <a href="https://github.com/marianoguerra/otp.ex/tree/main/cowboy/src">Cowboy web server</a></p><div id="the-match-operator-in-elixir">
<h2>The ^ match operator in Elixir</h2>
<p>In Elixir variable bindings by default rebind to the new value, if they are
already bound and you want to pattern match on the current value you have to
add the <em>^</em> operator in front:</p>
<pre><a name="rest_code_fd61af65dfe74189aeb889ef8462540c-1"></a><span>iex</span><span>(</span><span>1</span><span>)</span><span>&gt;</span> <span>a</span> <span>=</span> <span>1</span>
<a name="rest_code_fd61af65dfe74189aeb889ef8462540c-2"></a><span>1</span>
<a name="rest_code_fd61af65dfe74189aeb889ef8462540c-3"></a><span>iex</span><span>(</span><span>2</span><span>)</span><span>&gt;</span> <span>a</span> <span>=</span> <span>2</span>
<a name="rest_code_fd61af65dfe74189aeb889ef8462540c-4"></a><span>2</span>
<a name="rest_code_fd61af65dfe74189aeb889ef8462540c-5"></a><span>iex</span><span>(</span><span>3</span><span>)</span><span>&gt;</span> <span>a</span>
<a name="rest_code_fd61af65dfe74189aeb889ef8462540c-6"></a><span>2</span>
<a name="rest_code_fd61af65dfe74189aeb889ef8462540c-7"></a><span>iex</span><span>(</span><span>4</span><span>)</span><span>&gt;</span> <span>^</span><span>a</span> <span>=</span> <span>3</span>
<a name="rest_code_fd61af65dfe74189aeb889ef8462540c-8"></a><span>**</span> <span>(</span><span>MatchError</span><span>)</span> <span>no</span> <span>match</span> <span>of</span> <span>right</span> <span>hand</span> <span>side</span> <span>value</span><span>:</span> <span>3</span>
</pre>
<p>In Erlang variables are bound once and then always pattern match, the easy
part of the translation is that I know that when a variable is bound and in
match position I have to add the <em>^</em>, the thing is that I can't add the <em>^</em>
on the first binding and I have to know where variables are in match position.</p>
<p>For this I do <a href="https://github.com/marianoguerra/efe/blob/main/src/efe_var_ann.erl">a pass on the Erlang Abstract Syntax Tree and I add annotations
on variables</a> to know if it's already bound and if it's in match possition, the
pretty printer in the second pass checks those annotations to know if it has
to add the <em>^</em> or not.</p>
</div><div id="why-some-modules-don-t-compile">
<h2>Why some modules don't compile?</h2>
<p>Here's a list of reasons why the remaining modules don't compile after being
transpiled.</p>
<div id="for-comprehensions-must-start-with-a-generator">
<h3>For comprehensions must start with a generator</h3>
<p>There's a weird trick in Erlang where you can generate an empty list if a
condition is false or a list with one item if a condition is true by having a
list comprehension that has no generator but has a filter.</p>
<p>I've been told that it's an artifact of how list comprehensions used to be
translated to other code in the past.</p>
<pre><a name="rest_code_2bdc186984394a22b651c88739f592c2-1"></a><span>1</span><span>&gt;</span> <span>[</span><span>ok</span> <span>||</span> <span>true</span><span>].</span>
<a name="rest_code_2bdc186984394a22b651c88739f592c2-2"></a><span>[</span><span>ok</span><span>]</span>
<a name="rest_code_2bdc186984394a22b651c88739f592c2-3"></a>
<a name="rest_code_2bdc186984394a22b651c88739f592c2-4"></a><span>2</span><span>&gt;</span> <span>[</span><span>ok</span> <span>||</span> <span>false</span><span>].</span>
<a name="rest_code_2bdc186984394a22b651c88739f592c2-5"></a><span>[]</span>
</pre>
<p>The fact is that it's valid Erlang and is used in some places in the standard library.</p>
<p>For simple cases in efe I insert a dummy generator:</p>
<pre><a name="rest_code_0369c5665dce49749f10a1cf9ea48854-1"></a><span>for</span> <span>_</span> <span>&lt;-</span> <span>[</span><span>:EFE_DUMMY_GEN</span><span>],</span> <span>true</span> <span>do</span>
<a name="rest_code_0369c5665dce49749f10a1cf9ea48854-2"></a>    <span>:ok</span>
<a name="rest_code_0369c5665dce49749f10a1cf9ea48854-3"></a><span>end</span>
<a name="rest_code_0369c5665dce49749f10a1cf9ea48854-4"></a>
<a name="rest_code_0369c5665dce49749f10a1cf9ea48854-5"></a><span>for</span> <span>_</span> <span>&lt;-</span> <span>[</span><span>:EFE_DUMMY_GEN</span><span>],</span> <span>false</span> <span>do</span>
<a name="rest_code_0369c5665dce49749f10a1cf9ea48854-6"></a>    <span>:ok</span>
<a name="rest_code_0369c5665dce49749f10a1cf9ea48854-7"></a><span>end</span>
</pre>
<p>For more advanced cases with many filters I have to analyze if inserting a
generator at the beginning doesn't change the result, that's why some cases are
left as is.</p>
</div>
<div id="erlang-records-dont-evaluate-default-expressions-elixir-defrecord-do">
<h3>Erlang records don’t evaluate default expressions, Elixir defrecord do</h3>
<p><a href="http://erlang.org/doc/reference_manual/records.html">Erlang records</a> are not
part of the language, they are expanded by the <a href="http://erlang.org/doc/man/epp.html">Erlang Preprocessor</a>.</p>
<p>What the preprocessor does is to insert the default values "as is" on the places
where a record is created, this means that if the default is a function call it
won't be evaluated during definition, there will be a function call for each
instantiation of the record.</p>
<p>Elixir has <a href="https://hexdocs.pm/elixir/master/Record.html">a module to deal with Erlang Records</a> using macros, the thing is that Elixir will evaluate the defaults when they are defined,
this means that if the call doesn't return a constant the behavior won't be the same.
If the call returns a value that can't be represented as a constant in the code it won't compile either.</p>
<p>Another issue is if the function being called is declared after the record is
defined, it will fail with an error saying that the function doesn't exit.</p>
<p>There could be a solution here by creating another module that tries to emulate
the way default values behave in Erlang (they behave as "quoted" expressions)
but I don't know so much about Elixir macros to know how to do it.</p>
</div>
<div id="named-lambda-functions">
<h3>Named lambda functions</h3>
<p>In Erlang <a href="https://erlang.org/doc/reference_manual/expressions.html#fun-expressions">lambda functions can have names to allow recursion</a>, in Elixir this is not supported, there's
no way to automatically change the code in a local/simple way, it's easy to
change the code by hand so I decided to transpile it as if Elixir supported
named lambda functions and get a compiler error.</p>
</div>
<div id="expressions-in-bitstrings">
<h3>Expressions in bitstrings</h3>
<p>In Elixir <a href="https://elixir-lang.org/getting-started/binaries-strings-and-char-lists.html#bitstrings">size in bitstring expects an integer or a variable as argument</a>, Erlang allows any expression there, it's easy to fix by hand by extracting the expression into a variable and putting the variable there, it could be doable but for now I just leave the expression in place and get a compiler error.</p>
</div>
<div id="variable-defined-inside-scope-and-used-outside">
<h3>Variable defined inside scope and used outside</h3>
<p>In Erlang variables introduced within the if, case or receive expressions are implicitly exported from the bodies, this means this works:</p>
<pre><a name="rest_code_6cfc4681f2944f1785c8eee1cdfb7414-1"></a><span>case</span> <span>1</span> <span>of</span> <span>A</span> <span>-&gt;</span> <span>ok</span> <span>end</span><span>,</span> <span>A</span><span>.</span>
<a name="rest_code_6cfc4681f2944f1785c8eee1cdfb7414-2"></a><span>% or this</span>
<a name="rest_code_6cfc4681f2944f1785c8eee1cdfb7414-3"></a><span>case</span> <span>1</span> <span>of</span> <span>1</span> <span>-&gt;</span> <span>B</span> <span>=</span> <span>2</span> <span>end</span><span>,</span> <span>B</span><span>.</span>
</pre>
<p>Elixir has more strict scoping rules and that is not allowed, this is highly discouraged in Erlang but used in some places in the standard library.</p>
</div>
</div><div id="corner-cases-all-the-way-down">
<h2>Corner cases all the way down</h2>
<p>Here's a list of small differences that I had to fix.</p>
<div id="erlang-vs-elixir-imports">
<h3>Erlang vs Elixir imports</h3>
<p>In Erlang you can import functions from a module in multiple imports and they "add up".</p>
<p>In Elixir later imports for the same module "shadow" previous ones.</p>
<p>The solution is to group imports for the same module and emit only one import
per module.</p>
<p>In Erlang you can import a function more than once, in Elixir it's a compiler
error, the solution is to deduplicate function imports.</p>
</div>

<div id="lowercase-variables-that-become-keywords">
<h3>Lowercase variables that become keywords</h3>
<p>Erlang variables start with uppercase, Elixir variables with lowercase, this
means in Erlang variable names can't clash with language keywords but the lowercase
versions can, that's why I have to <a href="https://github.com/marianoguerra/efe/blob/main/src/efe_pp.erl#L1738">check if the variable is a keyword</a> and add a suffix to them.</p>
</div>
<div id="local-calls-and-kernel-autoimports">
<h3>Local calls and Kernel autoimports</h3>
<p>Elixir auto import functions from the <a href="https://hexdocs.pm/elixir/Kernel.html">Kernel</a> module
that may clash with local functions in the current Erlang module, for this case I have to <a href="https://github.com/marianoguerra/efe/blob/21ac93fb9eecfb8b164787d0e9935dae6ba7119e/src/efe_pp.erl#L1838">detect Kernel functions and macros</a> that are also local functions and add an expression to avoid auto importing them, like this:</p>
<pre><a name="rest_code_004b69f2fa9941fe83cdf5cee61ad893-1"></a><span>import</span> <span>Kernel</span><span>,</span> <span>except</span><span>:</span> <span>[</span><span>to_string</span><span>:</span> <span>1</span><span>,</span> <span>send</span><span>:</span> <span>2</span><span>]</span>
</pre>
</div>
<div id="private-on-load-function">
<h3>Private on_load function</h3>
<p>Erlang allows to define a private function to be run when the module loads,
Elixir only allowed public functions, this has been reported and fixed in
Elixir but not yet released.</p>
</div>
<div id="function-capture-calls-with-dynamic-values">
<h3>Function capture/calls with dynamic values</h3>
<p>In Erlang the syntax to pass a reference to a function is uniform for constants
and variables:</p>
<pre><a name="rest_code_9c91d656991a4f0a99b43d0cbba79657-1"></a><span>fun</span> <span>calls</span><span>/</span><span>3</span>
<a name="rest_code_9c91d656991a4f0a99b43d0cbba79657-2"></a><span>fun</span> <span>cornercases</span><span>:</span><span>calls</span><span>/</span><span>3</span>
<a name="rest_code_9c91d656991a4f0a99b43d0cbba79657-3"></a><span>fun</span> <span>M</span><span>:</span><span>F</span><span>/</span><span>Arity</span>
<a name="rest_code_9c91d656991a4f0a99b43d0cbba79657-4"></a><span>fun</span> <span>M</span><span>:</span><span>calls</span><span>/</span><span>3</span>
<a name="rest_code_9c91d656991a4f0a99b43d0cbba79657-5"></a><span>fun</span> <span>M</span><span>:</span><span>F</span><span>/</span><span>3</span>
<a name="rest_code_9c91d656991a4f0a99b43d0cbba79657-6"></a><span>fun</span> <span>cornercases</span><span>:</span><span>F</span><span>/</span><span>Arity</span>
<a name="rest_code_9c91d656991a4f0a99b43d0cbba79657-7"></a><span>fun</span> <span>cornercases</span><span>:</span><span>calls</span><span>/</span><span>Arity</span>
<a name="rest_code_9c91d656991a4f0a99b43d0cbba79657-8"></a><span>fun</span> <span>M</span><span>:</span><span>calls</span><span>/</span><span>Arity</span><span>}</span>
</pre>
<p>In Elixir I had to special case when any part is a variable.</p>
<pre><a name="rest_code_8a20e56ef08c4fb8b32a4bfaca4a1070-1"></a><span>&amp;</span><span>calls</span><span>/</span><span>3</span>
<a name="rest_code_8a20e56ef08c4fb8b32a4bfaca4a1070-2"></a><span>&amp;</span><span>:cornercases</span><span>.</span><span>calls</span><span>/</span><span>3</span>
<a name="rest_code_8a20e56ef08c4fb8b32a4bfaca4a1070-3"></a><span>Function</span><span>.</span><span>capture</span><span>(</span><span>m</span><span>,</span> <span>f</span><span>,</span> <span>arity</span><span>)</span>
<a name="rest_code_8a20e56ef08c4fb8b32a4bfaca4a1070-4"></a><span>Function</span><span>.</span><span>capture</span><span>(</span><span>m</span><span>,</span> <span>:calls</span><span>,</span> <span>3</span><span>)</span>
<a name="rest_code_8a20e56ef08c4fb8b32a4bfaca4a1070-5"></a><span>Function</span><span>.</span><span>capture</span><span>(</span><span>m</span><span>,</span> <span>f</span><span>,</span> <span>3</span><span>)</span>
<a name="rest_code_8a20e56ef08c4fb8b32a4bfaca4a1070-6"></a><span>Function</span><span>.</span><span>capture</span><span>(</span><span>:cornercases</span><span>,</span> <span>f</span><span>,</span> <span>arity</span><span>)</span>
<a name="rest_code_8a20e56ef08c4fb8b32a4bfaca4a1070-7"></a><span>Function</span><span>.</span><span>capture</span><span>(</span><span>:cornercases</span><span>,</span> <span>:calls</span><span>,</span> <span>arity</span><span>)</span>
<a name="rest_code_8a20e56ef08c4fb8b32a4bfaca4a1070-8"></a><span>Function</span><span>.</span><span>capture</span><span>(</span><span>m</span><span>,</span> <span>:calls</span><span>,</span> <span>arity</span><span>)</span>
</pre>
<p>Something similar happens with function calls:</p>
<pre><a name="rest_code_9183c2e1ec3c4a6ca42e5c6e94251da2-1"></a><span>M</span> <span>=</span> <span>erlang</span>
<a name="rest_code_9183c2e1ec3c4a6ca42e5c6e94251da2-2"></a><span>F</span> <span>=</span> <span>max</span>
<a name="rest_code_9183c2e1ec3c4a6ca42e5c6e94251da2-3"></a><span>M</span><span>:</span><span>max</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>
<a name="rest_code_9183c2e1ec3c4a6ca42e5c6e94251da2-4"></a><span>M</span><span>:</span><span>F</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>
<a name="rest_code_9183c2e1ec3c4a6ca42e5c6e94251da2-5"></a><span>erlang</span><span>:</span><span>F</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>
<a name="rest_code_9183c2e1ec3c4a6ca42e5c6e94251da2-6"></a><span>erlang</span><span>:</span><span>max</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>
<a name="rest_code_9183c2e1ec3c4a6ca42e5c6e94251da2-7"></a><span>max</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>
</pre>
<p>vs</p>
<pre><a name="rest_code_40785744968a44b196ab0dffde38b6c8-1"></a><span>m</span> <span>=</span> <span>:erlang</span>
<a name="rest_code_40785744968a44b196ab0dffde38b6c8-2"></a><span>f</span> <span>=</span> <span>:max</span>
<a name="rest_code_40785744968a44b196ab0dffde38b6c8-3"></a><span>m</span><span>.</span><span>max</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>
<a name="rest_code_40785744968a44b196ab0dffde38b6c8-4"></a><span>apply</span><span>(</span><span>m</span><span>,</span> <span>f</span><span>,</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>])</span>
<a name="rest_code_40785744968a44b196ab0dffde38b6c8-5"></a><span>apply</span><span>(</span><span>:erlang</span><span>,</span> <span>f</span><span>,</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>])</span>
<a name="rest_code_40785744968a44b196ab0dffde38b6c8-6"></a><span>:erlang</span><span>.</span><span>max</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>
<a name="rest_code_40785744968a44b196ab0dffde38b6c8-7"></a><span>max</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>
</pre>
</div>
<div id="binary-operators">
<h3>Binary operators</h3>
<p>In Erlang <a href="http://erlang.org/doc/reference_manual/expressions.html#arithmetic-expressions">binary operators</a> are builtin.</p>
<p>In Elixir they are macros from the <a href="https://hexdocs.pm/elixir/Bitwise.html">Bitwise</a> module.</p>
<p>The fix was easy, just use the module.</p>
</div>
<div id="call-expressions">
<h3>Call Expressions</h3>
<p>In Erlang there's no extra syntax to call a function that is the result of an
expression:</p>
<pre><a name="rest_code_6b808e85e84a4da6aa619e274c1eb8ba-1"></a><span>fun</span> <span>()</span> <span>-&gt;</span> <span>ok</span> <span>end</span><span>().</span>
<a name="rest_code_6b808e85e84a4da6aa619e274c1eb8ba-2"></a><span>% or</span>
<a name="rest_code_6b808e85e84a4da6aa619e274c1eb8ba-3"></a><span>(</span><span>return_fn</span><span>())().</span>
</pre>
<p>In Elixir it has to be wrapped in parenthesis and a dot added before the call:</p>
<pre><a name="rest_code_a7611ce6437f4112b8095f769d3914e5-1"></a><span>(</span><span>fn</span> <span>()</span> <span>-&gt;</span> <span>:ok</span> <span>end</span><span>)</span><span>.</span><span>()</span>
<a name="rest_code_a7611ce6437f4112b8095f769d3914e5-2"></a><span># or</span>
<a name="rest_code_a7611ce6437f4112b8095f769d3914e5-3"></a><span>(</span><span>return_fn</span><span>())</span><span>.</span><span>()</span>
</pre>
</div>
<div id="weird-function-names">
<h3>Weird function names</h3>
<p>In Erlang to declare or call function names whose names are not valid identifiers
the name has to be in single quotes:</p>
<pre><a name="rest_code_e525dfbd2faa468dab783017a1e1faf2-1"></a><span>'substring-after'</span><span>()</span> <span>-&gt;</span>
<a name="rest_code_e525dfbd2faa468dab783017a1e1faf2-2"></a>    <span>wxMenu</span><span>:</span><span>'Destroy'</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>).</span>
</pre>
<p>In Elixir the declaration is different from the call.</p>
<pre><a name="rest_code_511d8c2441144a3f847e5f7b9b36594f-1"></a><span>def</span> <span>unquote</span><span>(</span><span>:"substring-after"</span><span>)()</span> <span>do</span>
<a name="rest_code_511d8c2441144a3f847e5f7b9b36594f-2"></a>    <span>:wxMenu</span><span>.</span><span>'Destroy'</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span>
<a name="rest_code_511d8c2441144a3f847e5f7b9b36594f-3"></a><span>end</span>
</pre>
<p>When the function is a keyword in Elixir the declaration is the same but a
local call must be prefixed with the module to be valid syntax:</p>
<pre><a name="rest_code_5db18959ea2c451fbc16653aa3c92d9d-1"></a><span>keyword_methods</span><span>()</span> <span>-&gt;</span>
<a name="rest_code_5db18959ea2c451fbc16653aa3c92d9d-2"></a>    <span>{</span><span>nil</span><span>(),</span> <span>in</span><span>()}.</span>
<a name="rest_code_5db18959ea2c451fbc16653aa3c92d9d-3"></a>
<a name="rest_code_5db18959ea2c451fbc16653aa3c92d9d-4"></a><span>nil</span><span>()</span> <span>-&gt;</span> <span>nil</span><span>.</span>
<a name="rest_code_5db18959ea2c451fbc16653aa3c92d9d-5"></a><span>in</span><span>()</span> <span>-&gt;</span> <span>in</span><span>.</span>
</pre>
<p>vs</p>
<pre><a name="rest_code_2b8527328040454ea27b6cc1c8ac541c-1"></a><span>def</span> <span>keyword_methods</span><span>()</span> <span>do</span>
<a name="rest_code_2b8527328040454ea27b6cc1c8ac541c-2"></a>    <span>{</span><span>__MODULE__</span><span>.</span><span>nil</span><span>(),</span> <span>__MODULE__</span><span>.</span><span>in</span><span>()}</span>
<a name="rest_code_2b8527328040454ea27b6cc1c8ac541c-3"></a><span>end</span>
<a name="rest_code_2b8527328040454ea27b6cc1c8ac541c-4"></a>
<a name="rest_code_2b8527328040454ea27b6cc1c8ac541c-5"></a><span>def</span> <span>unquote</span><span>(</span><span>:nil</span><span>)()</span> <span>do</span>
<a name="rest_code_2b8527328040454ea27b6cc1c8ac541c-6"></a>    <span>nil</span>
<a name="rest_code_2b8527328040454ea27b6cc1c8ac541c-7"></a><span>end</span>
<a name="rest_code_2b8527328040454ea27b6cc1c8ac541c-8"></a>
<a name="rest_code_2b8527328040454ea27b6cc1c8ac541c-9"></a><span>def</span> <span>unquote</span><span>(</span><span>:in</span><span>)()</span> <span>do</span>
<a name="rest_code_2b8527328040454ea27b6cc1c8ac541c-10"></a>    <span>:in</span>
<a name="rest_code_2b8527328040454ea27b6cc1c8ac541c-11"></a><span>end</span>
</pre>
</div>
<div id="erlang-non-short-circuit-boolean-operators">
<h3>Erlang non short circuit boolean operators</h3>
<p>For historical reasons Erlang's boolean operators <em>and</em> and <em>or</em> do not short
circuit, this means they evaluate both sides before evaluating itself, for short
circuit versions the newer and recommended <em>andalso</em> and <em>orelse</em> operators
exist. Still the old versions are used in some places.</p>
<p>Elixir only has short circuit versions, to solve this I replace calls to those
operators to the functions in the Erlang module that do the same, since I need
to force the evaluation of both sides and function calls evaluate the arguments
before calling it does what I need.</p>
<pre><a name="rest_code_afb80f254e8f4854ba75cc53416f3fb5-1"></a><span>o_and</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>)</span> <span>-&gt;</span> <span>A</span> <span>and</span> <span>B</span><span>.</span>
<a name="rest_code_afb80f254e8f4854ba75cc53416f3fb5-2"></a><span>o_or</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>)</span>  <span>-&gt;</span> <span>A</span> <span>or</span> <span>B</span><span>.</span>
<a name="rest_code_afb80f254e8f4854ba75cc53416f3fb5-3"></a><span>o_xor</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>)</span> <span>-&gt;</span> <span>A</span> <span>xor</span> <span>B</span><span>.</span>
</pre>
<p>vs</p>
<pre><a name="rest_code_c615e15c0ee24d6ea1a26a0cdf7c4f43-1"></a><span>def</span> <span>o_and</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span> <span>do</span>
<a name="rest_code_c615e15c0ee24d6ea1a26a0cdf7c4f43-2"></a>  <span>:erlang</span><span>.</span><span>and</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span>
<a name="rest_code_c615e15c0ee24d6ea1a26a0cdf7c4f43-3"></a><span>end</span>
<a name="rest_code_c615e15c0ee24d6ea1a26a0cdf7c4f43-4"></a>
<a name="rest_code_c615e15c0ee24d6ea1a26a0cdf7c4f43-5"></a><span>def</span> <span>o_or</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span> <span>do</span>
<a name="rest_code_c615e15c0ee24d6ea1a26a0cdf7c4f43-6"></a>  <span>:erlang</span><span>.</span><span>or</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span>
<a name="rest_code_c615e15c0ee24d6ea1a26a0cdf7c4f43-7"></a><span>end</span>
<a name="rest_code_c615e15c0ee24d6ea1a26a0cdf7c4f43-8"></a>
<a name="rest_code_c615e15c0ee24d6ea1a26a0cdf7c4f43-9"></a><span>def</span> <span>o_xor</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span> <span>do</span>
<a name="rest_code_c615e15c0ee24d6ea1a26a0cdf7c4f43-10"></a>  <span>:erlang</span><span>.</span><span>xor</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span>
<a name="rest_code_c615e15c0ee24d6ea1a26a0cdf7c4f43-11"></a><span>end</span>
</pre>
<p>The problem is in guards, where only a subset of functions can be used, in
Erlang since <em>and</em> and <em>or</em> are operators they are allowed, but in Elixir the
function calls are not, only in this case I replace the non short circuit
version for the short circuit ones since …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://marianoguerra.org/posts/elixir-flavoured-erlang-an-erlang-to-elixir-transpiler/">http://marianoguerra.org/posts/elixir-flavoured-erlang-an-erlang-to-elixir-transpiler/</a></em></p>]]>
            </description>
            <link>http://marianoguerra.org/posts/elixir-flavoured-erlang-an-erlang-to-elixir-transpiler/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035483</guid>
            <pubDate>Mon, 09 Nov 2020 15:05:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why is there so much surprise around Trump's approach to losing the US Election]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035356">thread link</a>) | @twords
<br/>
November 9, 2020 | https://twords.com/view-article/thgwigmore-Why-is-there-so-much-surprise-around-Trump-s-approach-to-losing-the-US-Election-2020- | <a href="https://web.archive.org/web/*/https://twords.com/view-article/thgwigmore-Why-is-there-so-much-surprise-around-Trump-s-approach-to-losing-the-US-Election-2020-">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="root"><div><div><div><div><div><header><div><p><a tag="[object Object]" href="https://twords.com/"><img src="https://twords.com/static/media/inverse_cropped.0601fa81.svg"></a></p><div><div><div></div></div></div><div><p><a tag="[object Object]" href="https://twords.com/authentication/login">Login</a></p></div></div></header></div><div class="page"><div><div><p><a href="https://twords.com/#" data-rb-event-key="/" role="button"></a><a href="https://twords.com/">Home</a></p></div><div><p><a href="https://twords.com/#" data-rb-event-key="/trending" role="button"></a><a href="https://twords.com/trending">Trending</a></p></div><div><p><a href="https://twords.com/#" data-rb-event-key="/my-feed" role="button"></a><a href="https://twords.com/my-feed">My Feed</a></p></div></div><div><nav><a href="https://twords.com/"></a></nav></div><div><div><a href="https://twords.com/view-article/null"><div><p>Continue reading...</p></div></a></div><div></div></div><div><div><p><h3>Latest Articles</h3></p><div><div><p><span>Loading..</span></p></div><p><span>Loading...</span></p></div></div></div></div><div><nav><div><p><a tag="[object Object]" href="https://twords.com/about-us">About Us</a></p></div><div><p><a tag="[object Object]" href="https://twords.com/contact-us">Contact Us</a></p></div><div><p><a tag="[object Object]" href="https://twords.com/faq">FAQ</a></p></div><div><div><p><a href="https://twitter.com/official_twords">Twitter</a></p></div></div></nav></div></div></div></div></div></div></div>]]>
            </description>
            <link>https://twords.com/view-article/thgwigmore-Why-is-there-so-much-surprise-around-Trump-s-approach-to-losing-the-US-Election-2020-</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035356</guid>
            <pubDate>Mon, 09 Nov 2020 14:52:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Vulnerabilities Discovered in Tcl Android TVs]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035350">thread link</a>) | @moviuro
<br/>
November 9, 2020 | https://sick.codes/extraordinary-vulnerabilities-discovered-in-tcl-android-tvs-now-worlds-3rd-largest-tv-manufacturer/ | <a href="https://web.archive.org/web/*/https://sick.codes/extraordinary-vulnerabilities-discovered-in-tcl-android-tvs-now-worlds-3rd-largest-tv-manufacturer/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                                
<p>The following piece is the culmination of a three-month long investigation into Smart TVs running Android. Having lived through this research experience, I can wholeheartedly say that there were multiple moments that I, and another security researcher that I met along the way, couldn’t believe what was happening. On multiple occasions I found myself feeling as though, “you couldn’t even make this up…”</p>



<p>I’m a security researcher, a freelance developer, and a hacker.</p>



<p>Please follow me on Twitter <a href="https://twitter.com/sickcodes" target="_blank" rel="noreferrer noopener">@sickcodes</a> here: <a href="https://twitter.com/sickcodes" target="_blank" rel="noreferrer noopener">https://twitter.com/sickcodes</a></p>



<p>The second researcher in this story is John Jackson: <a href="https://twitter.com/johnjhacking" target="_blank" rel="noreferrer noopener">https://twitter.com/johnjhacking</a>, an Application Security Engineer with Shutterstock, and a hacker.</p>



<p>We met about half way through this, and I have included his experience too.</p>



<h2>Initial Research</h2>



<p>Near the end of September, while conducting research into low-end Android boxes, I came across a number of serious flaws in the way in which these devices were being designed.</p>



<p>Without delving into the nuances of each device, all of the Smart TV products are Android based.</p>



<p>There are four types of TV products in the TV market:</p>



<ol><li>TV Sticks</li><li>TV Boxes</li><li>Smart TVs</li><li>Android TVs</li></ol>



<p>All of them are ARM based single board computers (SBCs). Most of the dies are 32bit, some are 64bit, but all of them are like a little Raspberry Pi competitor, focusing on GPU performance through the small, but powerful, Mali GPUs.</p>



<p>Some of the products that I investigated were “factory-flawed” and deliberately insecure.</p>



<h2>First Blood</h2>



<p>On 2020-09-20, I discovered some ridiculous security shortfalls in the TV Sticks.</p>



<p><em>NOTE: TCL does not make TV sticks that are vulnerable. Only TCL Android TVs are affected. The following vulnerabilities refer to other products that I was testing at the time before finding the TCL vulnerability that is discussed in depth after the nmap scaps below.</em></p>



<p>Each stick that I tested had at least one of the following major security flaws.</p>



<ul><li>Port 22 open and allowing SSH access as root:root out of the box</li><li>Port 5555 open and allowing unauthenticated android (adb) as root:root out of the box</li><li>Rooted device, with world-executable su binaries in multiple locations</li><li>Open WiFi network with adb and ssh daemons running</li></ul>



<p>In effect, if you had a thousand of these devices, you could worm through all of them, taking advantage of the dual WiFi, plain-text WAN router credentials, and the ability to then hop from the TV stick, to the router, MITM the router, search for more vulnerable devices from the larger, more powerful router, and truly “surf the internet”.</p>



<p><strong>Proof of Concept</strong></p>



<pre># connect to the device's open WiFi network without any password
adb connect 192.168.1.1
adb shell
su
whoami
# root</pre>



<p>Having witnessed how dismal the security was on these devices, or lack thereof, my plan was to write a really big proof of concept, in the form of an actual shell based worm, that would hop between the 4 or 5 TV sticks that I had.</p>



<p>Speaking to an associate about my idea, we ended up chatting about real Android TVs.</p>



<p>Suddenly, I thought, “If these sticks are the same, just little Rockchip &amp; Amlogic CPUs, then what is so special about Smart TVs?”</p>



<p>Since I don’t actually have an Android Television to test, I asked my friend what type of Smart TV does he have and is it running Android?</p>



<p>His answer was, “TCL and not sure.”</p>



<p>I hadn’t really heard much about TCL, but it turns out TCL is a huge Chinese electronics manufacturing company.</p>



<p>TCL has been growing their global market share, at a remarkable rate.</p>



<p>According to a Forbes article, they only launched in the United States in 2013 and sales began on Amazon: <a href="https://www.forbes.com/sites/sethporges/2016/11/14/how-a-no-name-chinese-tv-brand-came-to-dominate-the-amazon-charts/?sh=15fd0d52f096" target="_blank" rel="noreferrer noopener">https://www.forbes.com/sites/sethporges/2016/11/14/how-a-no-name-chinese-tv-brand-came-to-dominate-the-amazon-charts/?sh=15fd0d52f096</a></p>



<div><figure><img loading="lazy" width="928" height="545" src="https://sick.codes/wp-content/uploads/2020/11/tv-market-share-2008-2020.png" alt="TV market share 2008 2020" srcset="https://sick.codes/wp-content/uploads/2020/11/tv-market-share-2008-2020.png 928w, https://sick.codes/wp-content/uploads/2020/11/tv-market-share-2008-2020-300x176.png 300w, https://sick.codes/wp-content/uploads/2020/11/tv-market-share-2008-2020-768x451.png 768w, https://sick.codes/wp-content/uploads/2020/11/tv-market-share-2008-2020-750x440.png 750w" sizes="(max-width: 928px) 100vw, 928px"><figcaption>TV market share 2008 to 2020</figcaption></figure></div>



<p>With their Amazon success, TCL began targeting other large markets.</p>



<p>The key point here is that they aren’t Samsung or LG, but they ARE selling millions of TV sets…</p>



<p>We did a remote desktop session and I ran a trivial nmap scan on the TV to see what it was running out of the box.</p>



<p>Here is the nmap scan:</p>



<pre>Starting Nmap 7.91 ( https://nmap.org ) at 2020-10-16 21:55 UTC
…
Scanning 10.0.0.117 [65535 ports]
Discovered open port 6550/tcp on 10.0.0.117
Discovered open port 8012/tcp on 10.0.0.117
Discovered open port 6466/tcp on 10.0.0.117
Discovered open port 8009/tcp on 10.0.0.117
Discovered open port 9000/tcp on 10.0.0.117
Discovered open port 8443/tcp on 10.0.0.117
Discovered open port 10101/tcp on 10.0.0.117
Discovered open port 46211/tcp on 10.0.0.117
Discovered open port 7989/tcp on 10.0.0.117
Discovered open port 6467/tcp on 10.0.0.117
Discovered open port 6559/tcp on 10.0.0.117
Discovered open port 6553/tcp on 10.0.0.117
Discovered open port 4332/tcp on 10.0.0.117
Discovered open port 8008/tcp on 10.0.0.117
Completed SYN Stealth Scan at 21:56, 20.40s elapsed (65535 total ports)
Initiating Service scan at 21:56
Scanning 14 services on 10.0.0.117
…
Completed Service scan at 21:58, 156.41s elapsed (14 services on 1 host)
Not shown: 65521 closed ports</pre>



<p>If you nmap your Android mobile phone, you will generally find 0 open TCP ports.</p>



<p><strong>Zero.</strong></p>



<p>So why does a TV need so many open ports?</p>



<p>While there are some reasons why TVs should have open ports, some of the above services warranted much deeper investigation.</p>



<p>Since I was in a remote desktop session, I just entered all the URLs manually into his web browser.</p>



<pre>http://10.0.0.117:6550
http://10.0.0.117:8012
http://10.0.0.117:6466
http://10.0.0.117:8009
http://10.0.0.117:9000
http://10.0.0.117:8443
http://10.0.0.117:10101
http://10.0.0.117:46211
http://10.0.0.117:7989
http://10.0.0.117:6467
http://10.0.0.117:6559
http://10.0.0.117:6553
http://10.0.0.117:4332
http://10.0.0.117:8008</pre>



<p>I also tested the https:// editions:</p>



<pre>https://10.0.0.117:6550
https://10.0.0.117:8012
https://10.0.0.117:6466
https://10.0.0.117:8009
https://10.0.0.117:9000
https://10.0.0.117:8443
https://10.0.0.117:10101
https://10.0.0.117:46211
https://10.0.0.117:7989
https://10.0.0.117:6467
https://10.0.0.117:6559
https://10.0.0.117:6553
https://10.0.0.117:4332
https://10.0.0.117:8008</pre>



<p>Some of the pages were blank white pages. This can indicate an API endpoint.</p>



<p>Some of the pages just hang the browser.</p>



<p>Then the rest of the nmap scan came through…</p>



<pre>PORT STATE SERVICE VERSION
4332/tcp open getty-focus?
6466/tcp open ssl/unknown
| ssl-cert: Subject: commonName=atvremote/BeyondTV2/BeyondTV/BeyondTV2/unknown
| Subject Alternative Name: email:<a href="https://sick.codes/cdn-cgi/l/email-protection" data-cfemail="1b7a757f6974727f366f6d36697e76746f7e36686e6b6b74696f5b7c74747c777e35787476">[email&nbsp;protected]</a>
| Issuer: commonName=atvremote/BeyondTV2/BeyondTV/BeyondTV2/unknown
| Public Key type: rsa
| Public Key bits: 2048
| Signature Algorithm: sha256WithRSAEncryption
…
6467/tcp open tcpwrapped
6550/tcp open fg-sysupdate?
| fingerprint-strings:
| NULL:
|_ Version 4
6553/tcp open unknown
6559/tcp open unknown
| fingerprint-strings:
| GenericLines:
…
7989/tcp open unknown
| fingerprint-strings:
| FourOhFourRequest:
| HTTP/1.1 404 Not Found
| Content-Type: text/plain
| Date: Fri, 16 Oct 2020 10:57:17 GMT
| Accept-Ranges: bytes
| Connection: keep-alive
| Content-Length: 26
| Error 404, file not found.
| GenericLines:
| HTTP/1.1 400 Bad Request
| Content-Type: text/plain
| Date: Fri, 16 Oct 2020 10:56:27 GMT
| Connection: keep-alive
| Content-Length: 56
| REQUEST: Syntax error. Usage: GET /example/file.html
| SIPOptions:
| HTTP/1.1 404 Not Found
| Content-Type: text/plain
| Date: Fri, 16 Oct 2020 10:57:37 GMT
| Accept-Ranges: bytes
| Connection: keep-alive
| Content-Length: 26
|_ Error 404, file not found.
8008/tcp open http?
|_http-title: Site doesn\'t have a title (text/html).
8009/tcp open ssl/castv2 Ninja Sphere Chromecast driver
|_ajp-methods: Failed to get a valid response for the OPTION request
8012/tcp open unknown
8443/tcp open ssl/https-alt\?
|_http-title: Site doesn\'t have a title (text/html).
| ssl-cert: Subject: commonName=/organizationName=Google Inc/stateOrProvinceName=Washington/countryName=US
| Issuer: commonName=TCL TV BeyondTV Realtek RTD2851 Cast ICA/organizationName=Google Inc/stateOrProvinceName=Washington/countryName=US
9000/tcp open ssl/cslistener?
10101/tcp open ssl/ezmeeting-2?
46211/tcp open tcpwrapped</pre>



<p>Port 7989 was showing a 404 error, yet when I visit 10.0.0.117:7989 in the browser, an error is shown.</p>



<p>http://10.0.0.117:7989 did not return a page in the browser.</p>



<div><figure><img loading="lazy" width="1007" height="664" src="https://sick.codes/wp-content/uploads/2020/11/tcl-directory-file-structure.png" alt="TCL directory file structure" srcset="https://sick.codes/wp-content/uploads/2020/11/tcl-directory-file-structure.png 1007w, https://sick.codes/wp-content/uploads/2020/11/tcl-directory-file-structure-300x198.png 300w, https://sick.codes/wp-content/uploads/2020/11/tcl-directory-file-structure-768x506.png 768w, https://sick.codes/wp-content/uploads/2020/11/tcl-directory-file-structure-750x495.png 750w" sizes="(max-width: 1007px) 100vw, 1007px"><figcaption>TCL directory file structure</figcaption></figure></div>



<p>What kind of special web server doesn’t show an index page, but shows deeper pages?</p>



<p>I had recently done research on an IoT device that was serving CGI scripts from the / directory, so the first page I thought to test was init.rc</p>



<p><em>http://10.0.0.117:7989/init.rc</em></p>



<p>403 Forbidden.</p>



<p>Yikes.</p>



<p>This means that the file is exists but we are not authorized to view it.</p>



<p>Naturally, I tested some other Android directories:</p>



<p><em>http://10.0.0.117:7989/sdcard</em></p>



<figure><img loading="lazy" width="916" height="691" src="https://sick.codes/wp-content/uploads/2020/11/tcl-directory-file-structure-vulnerability.png" alt="TCL directory file structure vulnerability" srcset="https://sick.codes/wp-content/uploads/2020/11/tcl-directory-file-structure-vulnerability.png 916w, https://sick.codes/wp-content/uploads/2020/11/tcl-directory-file-structure-vulnerability-300x226.png 300w, https://sick.codes/wp-content/uploads/2020/11/tcl-directory-file-structure-vulnerability-768x579.png 768w, https://sick.codes/wp-content/uploads/2020/11/tcl-directory-file-structure-vulnerability-750x566.png 750w" sizes="(max-width: 916px) 100vw, 916px"><figcaption>TCL directory file structure vulnerability</figcaption></figure>



<p>If you work with computers, no matter whether it be mobile apps, web apps, websites, back-end, front-end, upend…</p>



<p>Ask yourself this question right now:</p>



<figure><blockquote><p><em>When in the history of your career…</em><br><em>Have you ever needed to serve the entire filesystem…</em><br><em>over http?</em></p></blockquote></figure>



<p>This becomes a really import question because this custom vendor firmware is currently installed in millions of TCL Android TVs around the world.</p>



<p>My friend who was actually on the phone to me while we were doing the remote desktop, was fairly surprised.</p>



<p>“Why can we see all the files in the TV?”</p>



<p>Port 7989 is not on the list of standard TCP/UDP ports by the Internet Assigned Numbers Authority (IANA), https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.txt</p>



<p>This means, without scanning all 65,535 ports, most scanners will skip that port.</p>



<p>Secondly, the actual root page is blank.</p>



<p>So in order to scan more than 1 page per port, port scan times will exponentially increase…</p>



<p>Curiously, I checked the IANA list for other …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://sick.codes/extraordinary-vulnerabilities-discovered-in-tcl-android-tvs-now-worlds-3rd-largest-tv-manufacturer/">https://sick.codes/extraordinary-vulnerabilities-discovered-in-tcl-android-tvs-now-worlds-3rd-largest-tv-manufacturer/</a></em></p>]]>
            </description>
            <link>https://sick.codes/extraordinary-vulnerabilities-discovered-in-tcl-android-tvs-now-worlds-3rd-largest-tv-manufacturer/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035350</guid>
            <pubDate>Mon, 09 Nov 2020 14:52:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A US Visa in 937 Days]]>
            </title>
            <description>
<![CDATA[
Score 286 | Comments 142 (<a href="https://news.ycombinator.com/item?id=25035307">thread link</a>) | @caution
<br/>
November 9, 2020 | https://daniel.haxx.se/blog/2020/11/09/a-us-visa-in-937-days/ | <a href="https://web.archive.org/web/*/https://daniel.haxx.se/blog/2020/11/09/a-us-visa-in-937-days/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>Here’s the complete timeline of events. From my first denial to travel to the US until I eventually received a tourist visa. And then I can’t go anyway.</p>



<h2>December 5-11, 2016</h2>



<p>I spent a week on Hawaii with Mozilla – my employer at the time. This was my 12th visit to the US over a period of 19 years. I went there on ESTA, the visa waiver program Swedish citizens can use. I’ve used it many times, there was nothing special this time. The typical procedure with ESTA is that we apply online: fill in a form, pay a 14 USD fee and get a confirmation within a few days that we’re good to go.</p>



<figure><a href="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/hawaii-2016.jpg"><img loading="lazy" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/hawaii-2016.jpg" alt="" width="558" height="414"></a><figcaption>I took this photo at the hotel we stayed at during the Mozilla all-hands on Hawaii 2016.</figcaption></figure>



<h2>June 26, 2017</h2>



<p>In the early morning one day by the check-in counter at Arlanda airport in Sweden, I was refused to board my flight. Completely unexpected and out of the blue! I thought I was going to San Francisco via London with British Airways, but instead I had to turn around and go back home – slightly shocked. According to the lady behind the counter there was “something wrong with my ESTA”. I used the same ESTA and passport as I used just fine back in December 2016. They’re made to last two years and it had not expired.</p>



<figure><a href="https://twitter.com/bagder/status/879198063998513152"><img loading="lazy" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/Screenshot_2020-11-06-Twitter-Publish.png" alt="" width="553" height="199"></a><figcaption>Tweeted by me, minutes after being stopped at Arlanda.</figcaption></figure>



<p>People engaged by Mozilla to help us out could not figure out or get answers about what the problem was (questions and investigations were attempted both in the US and in Sweden), so we put our hopes on that it was a human mistake somewhere and decided to just try again next time.</p>



<h2>April 3, 2018</h2>



<p>I missed the following meeting (in December 2017) for other reasons but in the summer of 2018 another Mozilla all-hands meeting was coming up (in Texas, USA this time) so I went ahead and applied for a new ESTA in good time before the event – as I was a bit afraid there was going to be problems. I was right and I got denied ESTA very quickly. “Travel Not Authorized”.</p>



<figure><img loading="lazy" src="https://daniel.haxx.se/media/ESTA-travel-not-authorized.png" alt="" width="642" height="458"><figcaption>Rejected from the ESTA program.</figcaption></figure>



<h2>Day 0 – April 17, 2018</h2>



<p><strong>Gaaah</strong>. It meant it was no mistake last year, they actually mean this. I switched approach and instead applied for a tourist visa. I paid 160 USD, filled in a ridiculous amount of information about me and my past travels over the last 15 years and I visited the US embassy for an in-person interview and fingerprinting.</p>



<figure><img loading="lazy" src="https://daniel.haxx.se/media/Administrative-Processing.png" alt="" width="577" height="289"></figure>



<p>This is day 0 in the <a href="https://daniel.haxx.se/blog/2018/07/28/administrative-purgatory/" data-type="post" data-id="11076">visa process</a>, 296 days after I was first stopped at Arlanda.</p>



<h2>Day 90 – July 2018</h2>



<p>I missed the all-hands meeting in San Francisco when I didn’t get the visa in time.</p>



<h2>Day 240 – December 2018</h2>



<p>I <a href="https://daniel.haxx.se/blog/2018/11/18/im-leaving-mozilla/">quit Mozilla</a>, so I then had no more reasons to go to their company all-hands…</p>



<h2>Day 365 – April 2019</h2>



<p><a href="https://daniel.haxx.se/blog/2019/04/17/one-year-in-still-no-visa/" data-type="post" data-id="12216">A year passed</a>. “someone is working on it” the embassy email person claimed when I asked about progress.</p>



<h2>Day 651- January 28, 2020</h2>



<p>I emailed the embassy to query about the process</p>



<figure><img loading="lazy" src="https://daniel.haxx.se/media/651-days-email.png" alt="" width="611" height="166"><figcaption>Screenshotted email</figcaption></figure>



<p>The reply came back quickly:</p>



<blockquote><p>Dear Sir, </p><p>All applications are processed in the most expeditious manner possible. While we understand your frustration, we are required to follow immigration law regarding visa issuances. This process cannot be expedited or circumvented. Rest assured that we will contact you as soon as the administrative processing is concluded.</p></blockquote>



<h2>Day 730 – April 2020</h2>



<p><a href="https://daniel.haxx.se/blog/2020/04/17/two-years-in/" data-type="post" data-id="13456">Another year had passed</a> and I had given up all hope. Now it turned into a betting game and science project. How long can they actually drag out this process without saying either yes or no?</p>



<h2>Day 871 – September 3, 2020</h2>



<p>A friend of mine, a US citizen, contacted his Congressman – <a href="https://en.wikipedia.org/wiki/Gerry_Connolly">Gerry Connolly</a> – about my situation and asked for help. His office then subsequently sent a question to the US embassy in Stockholm asking about my case. While the response that arrived on September 17 was rather negative…</p>



<pre>your case is currently undergoing necessary administrative processing and regrettably it is not possible to predict when this processing will be completed.</pre>



<p>… I think the following turn of events indicates it had an effect. It unclogged something.</p>



<h2>Day 889 – September 22, 2020</h2>



<p>After 889 days since my interview on the embassy (only five days after the answer to the congressman), the embassy contacted me over email. <em> For the first time since that April day in 2018.</em></p>



<pre>Your visa application is still in administrative processing. However, we regret to inform you that because you have missed your travel plans, we will require updated travel plans from you.</pre>



<p>My travel plans – that had been out of date for the last 800 days or so – suddenly needed to be updated! As I was already so long into this process and since I feared that giving up now would force me back to square one if I would stop now and re-attempt this again at a later time, I decided to arrange myself some updated travel plans. After all, I work for an American company and I have a friend or two there.</p>



<h2>Day 900 – October 2, 2020</h2>



<p>I replied to the call for travel plan details with an official invitation letter attached, inviting me to go visit my colleagues at <a href="https://www.wolfssl.com/">wolfSSL</a> signed by our CEO, Larry. I really want to do this at some point, as I’ve never met most of them so it wasn’t a made up reason. I could possibly even get some other friends to invite me to get the process going but I figured this invite should be enough to keep the ball rolling.</p>



<h2>Day 910 – October 13, 2020</h2>



<p>I got another email. Now at 910 days since the interview. The embassy asked for my passport “for further processing”.</p>



<h2>Day 913 – October 16, 2020</h2>



<p>I posted my passport to the US embassy in Stockholm. I also ordered and paid for “return postage” as instructed so that they would ship it back to me in a safe way.</p>



<h2>Day 934 – November 6, 2020</h2>



<p>At 10:30 in the morning my phone lit up and showed me a text telling me that there’s an incoming parcel being delivered to me, shipped from “the Embassy of the United State” (bonus points for the typo).</p>



<figure><a href="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/embassy-text.png"><img loading="lazy" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/embassy-text.png" alt="" width="496" height="211"></a></figure>



<h2>Day 937 – November 9, 2020</h2>



<p>I received my passport. Inside, there’s a US visa that is valid for ten years, until November 2030.</p>



<div><figure><a href="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/VISA.jpg"><img loading="lazy" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/VISA.jpg" alt="" width="441" height="221"></a><figcaption>The upper left corner of the visa page in my passport…</figcaption></figure></div>



<p>As a bonus, the visa also comes with a NIE (National Interest<br>Exception) that allows me a single entry to the US during the PP (Presidential Proclamations) – which is restricting travels to the US from the European Schengen zone. In other words: I am actually allowed to travel right away!</p>



<p>The timing is fascinating. The last time I was in the US, Trump hadn’t taken office yet and I get the approved visa in my hands just days after Biden has been announced as the next president of the US.</p>



<h2>Will I travel?</h2>



<p>Covid-19 is still over us and there’s no end in sight of the pandemic. I will of course not travel to the US or any other country until it can be deemed safe and sensible.</p>



<p>When the pandemic is under control and traveling becomes viable, I am sure there will be opportunities. Hopefully the situation will improve before the visa expires.</p>



<h2>Thanks to</h2>



<p>All my family and friends, in the US and elsewhere who have supported me and cheered me up through this entire process. Thanks for keeping inviting me to fun things in the US even though I’ve not been able to participate. Thanks for pushing for events to get organized outside of the US! I’m sorry I’ve missed social gatherings, a friend’s marriage and several conference speaking opportunities. Thanks for all the moral support throughout this long journey of madness.</p>



<p>A special thanks go to David (you know who you are) for contacting Gerry Connolly’s office. I honestly think this was the key event that finally made things move in this process.</p>
	</div></div>]]>
            </description>
            <link>https://daniel.haxx.se/blog/2020/11/09/a-us-visa-in-937-days/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035307</guid>
            <pubDate>Mon, 09 Nov 2020 14:47:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Apache Kafka – 8 things to check before going live]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25035239">thread link</a>) | @ariskk
<br/>
November 9, 2020 | https://ariskk.com/kafka-8-things | <a href="https://web.archive.org/web/*/https://ariskk.com/kafka-8-things">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p><span>
      <span></span>
  <img alt="A Kafka System" title="A Kafka System" src="https://ariskk.com/static/89caf78b2f29f9e66a1780486740f932/18e3b/kafka-system.jpg" srcset="https://ariskk.com/static/89caf78b2f29f9e66a1780486740f932/46946/kafka-system.jpg 240w,https://ariskk.com/static/89caf78b2f29f9e66a1780486740f932/55489/kafka-system.jpg 480w,https://ariskk.com/static/89caf78b2f29f9e66a1780486740f932/18e3b/kafka-system.jpg 960w,https://ariskk.com/static/89caf78b2f29f9e66a1780486740f932/60e21/kafka-system.jpg 1440w,https://ariskk.com/static/89caf78b2f29f9e66a1780486740f932/198e7/kafka-system.jpg 1485w" sizes="(max-width: 960px) 100vw, 960px" loading="lazy">
    </span></p><p>Apache Kafka is a beautiful system. It scales well, it is stable and it provides phenomenal system architecture flexibility.
After 5 years of running production Kafka clusters, I have collected a list of tips and pitfalls. Some of them were learnt the hard way.
If you work in a small team rolling out Kafka to production, those might prove useful.
The article assume familiarity with basic Kafka concepts, such as brokers, topics, producers and consumers.
What is more, the following points should be valid for up to Kafka 2.6.0.
Without further ado.</p><h3>1. Key all the messages!</h3><p>Kafka topics consist of a (configurable) number of partitions.
If the partition number is not provided by the user, <code>KafkaProducer</code>
chooses the partition for the message using the <code>key</code> in the <code>ProducerRecord</code> instance passed to it.</p><p>Check out the default <a href="https://github.com/apache/kafka/blob/2.6.0/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java#L65">implementation</a>:
It checks if the key is <code>null</code>, and if it isn't it, computes a <code>murmur2</code> hash modulo the number of partitions.
This is consistent; it will yield the same result for messages sharing the same key.
If the key is <code>null</code> though, it uses a <a href="https://github.com/apache/kafka/blob/2.6.0/clients/src/main/java/org/apache/kafka/clients/producer/internals/StickyPartitionCache.java#L60">sticky partitioner</a>
that chooses a partition randomly at every batch.
In practical terms, if no key is passed, the producer will choose the partition randomly.</p><p>This has important implications because <strong>Kafka only provides delivery ordering guarantees within a partition</strong>.
Messages in the same partition will be delivered in the order they were committed.
Messages in different partitions will be delivered in non-deterministic order.
If the messages have any form of <strong>causal relationship</strong> between them, and they are <strong>not</strong> in the same partition,
then any downstream consumer will have to collect all messages for a key before processing them, else causal consistency might be violated; whatever <em>all</em> means in the context of
an infinite stream.</p><p>As a simple example to illustrate the above, think of a simple event like the following:</p><div data-language=""><pre data-linenumber="true"><p><span>1</span><span>case class EmailSubscription(</span></p><p><span>2</span><span>  email: Email,</span></p><p><span>3</span><span>  active: Boolean,</span></p><p><span>4</span><span>  createdOn: DateTime</span></p><p><span>5</span><span>)</span></p><p><span>6</span><span></span></p><p><span>7</span><span>def storeToDB(sub: EmailSubscription) = ???</span></p><p><span>8</span><span>val events: Stream[EmailSubscription] = ???</span></p></pre></div><p>If emails arrive in causal order, then we can map the events statelessly:</p><p>If per-email causal order is not guaranteed though, we need to maintain enough state to know
if the event we see is indeed the latest. Else if the order is reversed, we might send an email to a user who
has unsubscribed.
For example:</p><div data-language=""><pre data-linenumber="true"><p><span>1</span><span>events</span></p><p><span>2</span><span>  .groupMapReduce(_.email)(identity)(</span></p><p><span>3</span><span>    (e1, e2) =&gt; if (e1.createdOn.isAtfer(e2.createdOn)) e1 else e2</span></p><p><span>4</span><span>  )</span></p><p><span>5</span><span>    .values</span></p><p><span>6</span><span>    .map(storeToDB)</span></p></pre></div><p>If we key messages using <code>email</code>, then Kafka will deliver all messages for a single email in the order they were inserted.
Our consumer can be completely stateless; it can fetch messages from Kafka and store them to a datastore.
If the key is <code>null</code> (none provided), our stateless consuer will happily store a message from the past.
To mitigate that, at the very minimum we need to maintain the latest <code>createdOn</code> date for every email.
Relying on wall clocks for causality <a href="https://github.com/aphyr/distsys-class#clocks">is a very bad idea</a>.</p><p>Assuming per key ordering can be guaranteed, downstream reducer bounds can be reduced from a <code>Semilattice</code> to a <code>Semigroup</code>.
In practical terms, by taking advantage of this property we can drop the commutativity requirement which unlocks easier implementations.
If designing reducers for Kafka consumers sounds interesting, let me know and I will write about it.</p><h3>2. Ensure all producers are using the same partitioner</h3><p>Providing a key is not enough. Partitioners (ie the function f: (key) =&gt; partition) are configurable.
Kafka provides a few and users can roll out their own. Do <strong>NOT</strong> assume all producers are using the same partitioner.</p><p>In a complex system where Go services, Python services, Spark and other wild animals all share the same Kafka cluster, all sorts of different implementations might exist.
If different services are pushing data to the same topics, an integration test would be very useful.
If things go wrong, delivery to consumers will be non-deterministic and debugging it can be pure hell.</p><h3>3. Topic versioning</h3><p>The beauty of Kafka is that data can be reprocessed as many times as needed.
This forgives a lot of errors. To illustrate this one, let's assume the same dummy model:</p><div data-language=""><pre data-linenumber="true"><p><span>1</span><span>case class EmailSubscription(</span></p><p><span>2</span><span>  email: Email,</span></p><p><span>3</span><span>  active: Boolean,</span></p><p><span>4</span><span>  createdOn: DateTime</span></p><p><span>5</span><span>)</span></p></pre></div><p>Let's now assume messages are serialized to <code>json</code> before being pushed to Kafka.
Due to a json serialization bug, the following is pushed to the <code>emailsubscriptions</code> topic:</p><div data-language=""><pre data-linenumber="true"><p><span>1</span><span>{</span></p><p><span>2</span><span>  "email": {</span></p><p><span>3</span><span>    "value": "aris@aris.com"</span></p><p><span>4</span><span>  },</span></p><p><span>5</span><span>  "active": false,</span></p><p><span>6</span><span>  "createdOn": 1600000000</span></p><p><span>7</span><span>}</span></p></pre></div><p>Instead of the expected:</p><div data-language=""><pre data-linenumber="true"><p><span>1</span><span>{</span></p><p><span>2</span><span>  "email": "aris@aris.com",</span></p><p><span>3</span><span>  "active": false,</span></p><p><span>4</span><span>  "createdOn": 1600000000</span></p><p><span>5</span><span>}</span></p></pre></div><p>Downstream consumers try to deserialize the message and fail. What can be done?</p><p>One solution would be to create a custom deserializer for those buggy instances and to add it to all the consumers.
That's non trivial and error prone code; useful just for this instance.</p><p>Another solution would be to implement a consumer that would read from <code>emailsubscriptions-1</code>, fix the issue, and write to <code>emailsubscriptions-2</code>.
Once the offsets of the two topics are identical, producers and consumers can switch from <code>emailsubscriptions-1</code> to <code>emailsubscriptions-2</code> without having to update any code.
What's great about this is that those migrations can fail with no major consequences. If <code>emailsubscriptions-2</code> is no good, we can run again and produce <code>emailsubscriptions-3</code> and so on.
This trick also works for non-trivial schema changes, migrations and other data enrichments.
Avro and Profobuf can help in some cases, but bugs will occur and requirements will evolve in unpredictable ways.
In any case, "fixing" a topic's data by reading from it and publishing to it is rarely a good idea.
Topics should be immutable and versioning them can help in many situations where a topic's content have been corrupted.</p><h3>4. Treat ZooKeeper like royalty</h3><p>Up until at least 2.6.0, Kafka relies on ZooKeeper.
Losing connection to ZooKeeper means no ISRs (In-Sync-Replicas, more on that later), no partition leader election and eventually the brokers shut down.
Thankfully <a href="https://twitter.com/fpjunqueira?lang=en">@fpjunqueira</a> and his team who created ZooKeeper are real pros, and that won't happen without reason.
In fact, ZooKeeper is one of the most reliable distributed systems (that I have seen at least).</p><p>The two following mess ups have occurred though:</p><ol><li>Due to a bug in the provisioning Ansible script, 2/3rds of a cluster ended up in the same availability zone, with sequential IPs (that usually means in the same rack on AWS).
They all disappeared at the same time. No consensus, hell broke loose.</li><li>A QA environment ran for long enough for all nodes to run out of disk space (ZooKeeper creates backup snapshots of the transaction log over time and someone/something external has to deal with deleting them).
At the same time. Bringing this env back to life required editing znodes manually, and still data was lost.</li></ol><p>To clean up older transaction log snapshots in ZooKeeper 3.4.x, ZooKeeper provides the following tool:</p><div data-language=""><pre data-linenumber="true"><p><span>1</span><span>java -cp zookeeper-3.4.x.jar:lib/*:conf org.apache.zookeeper.server.PurgeTxnLog \</span></p><p><span>2</span><span>  /var/lib/zookeeper /var/lib/zookeeper -n 5</span></p></pre></div><p>Ideally on a cron.</p><p>Those are just two examples. A lot more can go wrong. Because of the consequences of failure, proper JMX metric monitoring and real time log aggregation,
all hooked up to a form of PagerDuty, are very highly recommended.</p><h3>5. Unclean elections and minimum ISRs</h3><p>This is essentially a trade off between availability and durability. Let's start with unclean elections.</p><p>Let's assume we have a topic with a single partition and a single replica. Data is happily flowing in.
If the replica is "in-sync" (aka identical to the leader and in the ISR set in ZooKeeper),
then if the leader partition becomes unavailable (eg the broker crashes) then the replica can pick up, accept writes and continue with no downtime.
If the replica lags behind though, the leader will remove it from the ISRs in ZooKeeper. If then the leader goes down, there are two options:</p><ol><li>The lagging replica picks up, accepts writes and whatever excess writes the old leader had are <em>lost</em>. Essentially, the replica gets elected as leader without being in-sync. This is the "unclean" part.</li><li>The partition becomes unavailable and new writes are rejected.</li></ol><p>It entirely depends on the kind of data the topic holds. If the topic holds system metrics,
then maybe the most recent data is more valuable and thus losing some older writes might be acceptable.
If the topic contains bank transactions, going down until a human intervenes might be a better option.
This is a broker level config that can be overridden per topic.</p><p>The second part of this equation is <code>min.insync.replicas</code>, which represents the minimum number of replicas that have to be in-sync for a write to go through.
This is configurable at the broker level, topic level and even at the producer level (ie <code>acks</code>). Same considerations as above,
if the topic holds payments, having just 1 replica with all the data might be risky.</p><p>Legendary distributed systems researcher Kyle Kingsbury, aka Aphyr, did an <a href="https://aphyr.com/posts/293-call-me-maybe-kafka">excellent analysis on Kafka's replication mechanism</a> some 7 years ago.
If you wish to dig deeper into this trade off, reading Aphyr's piece is very highly recommeneded. As far as I understand, the basic trade offs discussed still hold true today.</p><h3>6. Memory Maps</h3><p>Kafka uses a LOT of those. Running out of them leads to a fatal runtime exception that will kill the broker.
If the OS defaults are used, it is extremely likely that those will be reached as soon as the cluster has a few tens of thousand segments per broker.
What's worse is that in a well balanced cluster where brokers hold similar numbers of partitions, those failures will occur roughly at the same time.
Let's look a bit closer:</p><p><code>vm.max_map_count</code>: is the maximum number of memory map areas a process can have.</p><p>From the linux kernel <a href="https://www.kernel.org/doc/html/latest/admin-guide/sysctl/vm.html?highlight=max_map_count#max-map-count">docs</a>:</p><blockquote><p>max_map_count:<br>
<!-- -->This file contains the maximum number of memory map areas a process
may have. Memory map areas are used as a side-effect of …</p></blockquote></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ariskk.com/kafka-8-things">https://ariskk.com/kafka-8-things</a></em></p>]]>
            </description>
            <link>https://ariskk.com/kafka-8-things</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035239</guid>
            <pubDate>Mon, 09 Nov 2020 14:39:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Physicists develop an efficient modem for the future quantum internet]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035235">thread link</a>) | @rbanffy
<br/>
November 9, 2020 | https://www.mpq.mpg.de/modem-quantum-internet | <a href="https://web.archive.org/web/*/https://www.mpq.mpg.de/modem-quantum-internet">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  
  
  <p>Physicists at the Max Planck Institute of Quantum Optics have developed the basic technology for a new "quantum modem". It will allow users to connect to a future quantum internet that is based on the existing fibre optic network infrastructure.</p>
  

  

  <p>The first quantum revolution brought about semiconductor electronics, the laser and finally the internet. The coming, second quantum revolution promises spy-proof communication, extremely precise quantum sensors and quantum computers for previously unsolvable computing tasks. But this revolution is still in its infancy. A central research object is the interface between local quantum devices and light quanta that enable the remote transmission of highly sensitive quantum information. The Otto-Hahn group "Quantum Networks" at the Max-Planck-Institute of Quantum Optics in Garching is researching such a "quantum modem". The team has now achieved a first breakthrough in a relatively simple but highly efficient technology that can be integrated into existing fibre optic networks. The work is published this week in "Physical Review X".</p>
  
  
<figure data-description="The Garching quantum modem: The crystal disk with the quantum bits of erbium atoms (arrows) is in the middle, the back and forth reflected infrared light is indicated by the red disks." data-picture="base64;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">
      
    

    
    <figcaption>
        <p>
          The Garching quantum modem: The crystal disk with the quantum bits of erbium atoms (arrows) is in the middle, the back and forth reflected infrared light is indicated by the red disks.
        </p>
        <p>
           Christoph Hohmann (MCQST)
        </p>
    </figcaption>
</figure>


<p>The Corona pandemic is a daily reminder of how important the internet has become. The World Wide Web, once a by-product of basic physical research, has radically changed our culture. Could a quantum internet become the next major innovation out of physics?</p>
<p>It is still too early to answer that question, but basic research is already working on the quantum internet. Many applications will be more specialised and less sensual than video conferencing, but the importance of absolutely spy-proof long-distance communication is understandable to everyone. "In the future, a quantum internet could be used to connect quantum computers located in different places," Andreas Reiserer says, "which would considerably increase their computing power!” The physicist heads the independent Otto-Hahn research group "Quantum Networks" at the Max-Planck-Institute of Quantum Optics in Garching.</p>
<p>A quantum internet is thus essentially about the global networking of new technologies that make a much more consequent use of quantum physics than ever before. However, this requires suitable interfaces for the extremely sensitive quantum information. This is an enormous technical challenge, which is why such interfaces are a central focus of fundamental research. They must ensure that stationary quantum bits - qubits for short - interact efficiently with "flying" qubits for long-distance communication without destroying the quantum information. Stationary qubits will be located in local devices, for example as the memory or processor of a quantum computer. Flying qubits are typically light quanta, photons, that transport the quantum information through the air, a vacuum of space or through fibre optic networks.</p>
<h2>Delicate connection between quantum bits</h2>
<p>The "quantum modem" is designed to efficiently establish a connection between flying and stationary qubits. For this purpose, the team around doctoral student Benjamin Merkel has developed a new technology and has just demonstrated its basic functionality. Its crucial advantage is that it could be integrated into the existing telecommunications fibre-optic network. This would be the fastest way to advance a functioning long-distance networking of quantum technologies.</p>
<p>For this system to work, the photons sent or received by the modem as quantum information carriers must be matched precisely to the infrared wavelength of the laser light used for telecommunications. This means that the modem must have qubits at rest that can react precisely to these infrared photons with a quantum leap. Only in this way the sensitive quantum information can be transmitted directly between the qubits at rest and the flying qubits.&nbsp;</p>
<p>Extensive research by the Garching-based group showed that the element erbium is best suited for this purpose. Its Electrons can perform a perfectly matching quantum leap. Unfortunately, the erbium atoms are very reluctant to make this quantum leap. Therefore,&nbsp; they must be fixated in anenvironment that forces them to react more quickly. To solve this problem, the erbium atoms and the infrared photons are locked up in a suitable space for as long as possible. "You can think of it as a party, which should stimulate the best possible communication between, let’s say, ten guests," Reiserer explains. The size of the space is crucial here. "In a football stadium the guests would get lost, a telephone box in turn would &nbsp;be too small," the physicist continues, "but a living room would do just fine.”</p>
<p>The party, however, would quickly be over because the photons travel at the speed of light and are therefore highly volatile and always tempted to leave. This is why the Garching quantum modem uses a tiny mirror cabinet as a "living room" Thereto,the team packed the atoms into a transparent crystal made of an yttrium silicate compound, which is five times thinner than a human hair. This crystal, in turn, is placed like a sandwich spread between two almost perfect mirrors. To eliminate the heat wobbling of the atoms, which is destructive to quantum information, the entire ensemble is cooled to minus 271 °C.</p>
<h2>Photon ping-pong in the mirror cabinet</h2>

<figure data-description="Approximately in the centre of the picture, the &quot;mirror cabinet&quot; can be seen from outside, which creates the connection between flying and stationary qubits." data-picture="base64;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">
      
    

    
    <figcaption>
        <p>
          Approximately in the centre of the picture, the "mirror cabinet" can be seen from outside, which creates the connection between flying and stationary qubits.
        </p>
    </figcaption>
</figure>


<p>The photons trapped between the mirrors are reflected back and forth through the crystal like ping-pong balls. They pass the erbium atoms so often so that &nbsp;the atoms have enough time to react with a quantum leap. Compared to a situation without a mirror cabinet, this happens much more efficiently and almost sixty times faster. Since the mirrors, despite their perfection, are also slightly permeable to the photons, the modem can connect to the network.</p>
<p>"We are very happy about this success," Reiserer says. As a next step, he wants to improve the experiment such that individual erbium atoms can be addressed as qubits via laser light. This is not only an important step towards a usable quantum modem. Erbium atoms as qubits in a crystal may even serve directly as a quantum processor, which is the central part of a quantum computer. This would make the modem easily compatible with such quantum terminals.</p>
<p>With such an elegant solution, comparatively simply constructed "quantum repeaters" would also become possible. Every hundred kilometres, the devices would have to compensate the increasing losses of quantum information transported by photons in the fibre-optic network. Such “quantum repeaters” are also the focus of international research. "Although such a device based on our technology would cost about a hundred thousand euros, widespread use would not be unrealistic," Reiserer says.</p>
<p>The Garching quantum modem is still purely fundamental research. But it has the potential to advance the technical realisation of a quantum internet.</p>
<p>&nbsp;(RW/MPQ)</p>
  
</div></div>]]>
            </description>
            <link>https://www.mpq.mpg.de/modem-quantum-internet</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035235</guid>
            <pubDate>Mon, 09 Nov 2020 14:39:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I ended up with 1k GitHub repos while testing the GitHub API with CATS]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035195">thread link</a>) | @ludovicianul
<br/>
November 9, 2020 | https://ludovicianul.github.io/2020/10/05/github-api-testing/ | <a href="https://web.archive.org/web/*/https://ludovicianul.github.io/2020/10/05/github-api-testing/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
  
  <p><span>05 Oct 2020</span></p>
<p><span><strong>!!! WARNING !!! If you choose to run the steps in this article, please note that you will end up with a significant number of dummy GitHub repos under your username. Over 1k in my case.
There is a script at the end of the article that you can use to delete them. Be careful not to delete your real repos though!</strong></span></p>

<p><img src="https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/github_repos.png" alt="Repos"></p>


<p>Building good APIs is hard. There are plenty of resources out there with plenty of good advice on how to achieve this. While some of the things are a must, like following the <a href="https://cheatsheetseries.owasp.org/cheatsheets/REST_Security_Cheat_Sheet.html">OWASP REST Security Practices</a>,
some others might be debatable, based on preference, “like using <code>snake_case</code> or <code>camelCase</code> for naming JSON objects”. (I plan to write a more detailed article in the next weeks on what I consider good practices.)
As the number of APIs usually grows significantly, even when dealing with simple systems, it’s very important to have quick ways to make sure the APIs are meeting good practices consistently.</p>

<p>I’ve shown in a <a href="https://ludovicianul.github.io/2020/09/09/cats/">previous article</a> how easy it is to use a tool like <a href="https://github.com/Endava/cats">CATS</a> to quickly verify OpenAPI endpoints 
while covering a significant number of tests cases. But that was a purely didactic showcase using the OpenAPI demo <code>petstore</code> app. Which was obviously not built as a production ready service.
Today I’ll pick a real-life API, specifically the GitHub API, which recently published <a href="https://github.com/github/rest-api-description/blob/main/descriptions/ghes-2.22/ghes-2.22.yaml">their OpenAPI specs</a>.</p>

<p>I’ve downloaded the 2.22 version and saved the file locally as <code>github.yml</code>. Before we start, we need to <a href="https://github.com/settings/tokens/new">create an access token</a> in order to be able to call the APIs. Make sure it has proper scopes for repo creation (and deletion when using the script at the end of the article).
Also, as the API is quite rich (the file has 59k lines), I’ve only selected the <code>/user/repos</code> path for this showcase. You’ll see that there are plenty of findings only using this endpoint.</p>

<p>You can run <code>CATS</code> as a blackbox testing tool and incrementally add minimal bits of context until you end up with consistent issues or a green suite.</p>

<p>As shown in the <a href="https://ludovicianul.github.io/2020/09/09/cats/">previous article</a>, running <a href="https://github.com/Endava/cats">CATS</a> is quite simple:</p>

<div><div><pre><code>./cats.jar <span>--contract</span><span>=</span>github.yml <span>--server</span><span>=</span>https://api.github.com <span>--paths</span><span>=</span><span>"/user/repos"</span> <span>--headers</span><span>=</span>headers_github.yml
</code></pre></div></div>

<p>With the <code>headers_github.yml</code> having the following content:</p>

<div><div><pre><code><span>all</span><span>:</span>
  <span>Authorization</span><span>:</span> <span>token XXXXXXXXXXXXX</span>
</code></pre></div></div>

<p>Let’s see what we get on a first run:</p>

<p><img src="https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/first_run_github.png" alt="First Run"></p>

<p>We have <code>42 warnings</code> and <code>156 errors</code>. Let’s go through the errors first. Looking at the result of <code>Test 118</code> we see that a request failed due to the name of the repository not being unique. 
Indeed, <code>CATS</code>, for each Fuzzer, preserves an initial payload that will be used to fuzz each of the request fields. This means that we need a way to force <code>CATS</code> to send unique names for the <code>name</code> field. Noted!</p>

<p><img src="https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/test_118_github.png" alt="Test 118"></p>

<p><code>Test 426</code> says that <code>If you specify visibility or affiliation, you cannot specify type.</code>. Let’s note this down also.</p>

<p><img src="https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/test_426_github.png" alt="Test 426"></p>

<p>Considering the above 2 problems are reported consistently, let’s give it another go with a <a href="https://github.com/Endava/cats#reference-data-file">Reference Data File</a>.
This is the <code>refData_github.yml</code> file that will be used:</p>

<div><div><pre><code><span>/user/repos</span><span>:</span>
  <span>name</span><span>:</span> <span>"</span><span>T(org.apache.commons.lang3.RandomStringUtils).random(5,true,true)"</span>
  <span>type</span><span>:</span> <span>"</span><span>cats_remove_field"</span>
</code></pre></div></div>

<p><code>CATS</code> supports <a href="https://github.com/Endava/cats#dynamic-values-in-configuration-files">dynamic values in properties values</a> via the <a href="https://docs.spring.io/spring-framework/docs/3.0.x/reference/expressions.html">Spring Expression Language</a>.
Using the above <code>refData</code> file, <code>CATS</code> will now generate a new random <code>name</code> everytime it will execute a request to the GitHub API.
Also, using the <code>cats_remove_field</code> value, <code>CATS</code> will remove this field from all requests before sending them to the endpoint. More details on this feature <a href="https://github.com/Endava/cats#removing-fields">here</a>.</p>

<p>Running <code>CATS</code> again:</p>

<div><div><pre><code>./cats.jar <span>--contract</span><span>=</span>github.yml <span>--server</span><span>=</span>https://api.github.com <span>--paths</span><span>=</span><span>"/user/repos"</span> <span>--headers</span><span>=</span>headers_github.yml <span>--refData</span><span>=</span>refData_github.yml
</code></pre></div></div>

<p>We now get <code>17 warnings</code> and <code>90 errors</code>. Again, looking though the tests failures/warnings there are some tests which are failing due to the fact the <code>since</code> and <code>before</code> are not sent in ISO8061 timestamp format (more on this inconsistency in the <a href="#Findings">Findings</a> section).</p>

<p><img src="https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/second_run_github.png" alt="Second Run"></p>

<p>We’ll now update the <code>refData</code> file to look as follows:</p>

<div><div><pre><code><span>/user/repos</span><span>:</span>
  <span>before</span><span>:</span> <span>"</span><span>T(java.time.Instant).now().toString()"</span>
  <span>since</span><span>:</span> <span>"</span><span>T(java.time.Instant).now().minusSeconds(86400).toString()"</span>
  <span>name</span><span>:</span> <span>"</span><span>T(org.apache.commons.lang3.RandomStringUtils).random(5,true,false)"</span>
  <span>type</span><span>:</span> <span>"</span><span>cats_remove_field"</span>
</code></pre></div></div>

<p>and run <code>CATS</code> again:</p>

<div><div><pre><code>./cats.jar <span>--contract</span><span>=</span>github.yml <span>--server</span><span>=</span>https://api.github.com <span>--paths</span><span>=</span><span>"/user/repos"</span> <span>--headers</span><span>=</span>headers_github.yml <span>--refData</span><span>=</span>refData_github.yml
</code></pre></div></div>

<p><img src="https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/third_run_github.png" alt="Third Run"></p>

<p>We now get <code>5 warnings</code> and <code>83 errors</code>. Looking through the errors and warnings, there is a significant amount which I consider legit issues while some are debatable points, depending on preference/standards being followed. 
Let’s go through the findings.</p>


<h2 id="invalid-values-for-boolean-fields-implicitly-converted-to-false">Invalid values for boolean fields implicitly converted to false</h2>
<p>One of the Fuzzers that <code>CATS</code> has is the <code>BooleanFieldsFuzzer</code>. 
This Fuzzer works on the assumption that if you send an invalid value into a <code>boolean</code> field, the service should return a validation error.
Obviously, the GitHub API does not do this, but is rather silently converting the value to <code>false</code>. It’s true this is a consistent behaviour, applying for all boolean fields like <code>auto_init</code>, <code>allow_merge_commits</code>, etc, but I would personally choose to return a validation error in these cases.</p>

<p>This is in contradiction with the <a href="https://cheatsheetseries.owasp.org/cheatsheets/REST_Security_Cheat_Sheet.html#input-validation">OWASP recommendation</a> around strong input validation and data type enforcing.</p>

<h2 id="invalid-values-for-enumerated-fields-implicitly-converted-to-the-default-value-">Invalid values for enumerated fields implicitly converted to the default value (?)</h2>
<p>The <code>InvalidValuesInEnumsFieldsFuzzer</code> will send invalid values in enum fields. It expects a validation error in return. 
The GitHub API does not seem to reject invalid values, but rather convert them to a default value and respond successfully to the request.</p>

<p>This is in contradiction with the <a href="https://cheatsheetseries.owasp.org/cheatsheets/REST_Security_Cheat_Sheet.html#input-validation">OWASP recommendation</a> around strong input validation and data type enforcing.</p>

<h2 id="integer-fields-accepting-decimal-or-large-negative-or-positive-values">Integer fields accepting decimal or large negative or positive values</h2>
<p>The <code>DecimalValuesInIntegerFieldsFuzzer</code> expects an error when it sends a <code>decimal</code> value inside an <code>integer</code> field. The GitHub API seems to accept these invalid values in the <code>team_id</code> field without returning any error, but rather resulting in a successful processing of the request.
Same applies for <code>ExtremePositiveValueInIntegerFieldsFuzzer</code> and <code>ExtremeNegativeValueIntegerFieldsFuzzer</code> which will send values such as <code>9223372036854775807</code> or <code>-9223372036854775808</code> in the <code>team_id</code> field.
Strings also seem to be accepted in the <code>team_id</code> field.</p>

<p>This is in contradiction with the <a href="https://cheatsheetseries.owasp.org/cheatsheets/REST_Security_Cheat_Sheet.html#input-validation">OWASP recommendation</a> around strong input validation and data type enforcing.</p>

<h2 id="accepts-unsupported-or-dummy-content-type-headers">Accepts unsupported or dummy Content-Type headers</h2>
<p>The GitHub API seems to successfully accept and process requests containing unsupported (according to the OpenAPI contract) <code>Content-Type</code> headers such as: <code>image/gif</code>, <code>multipart/related</code>, etc. or dummy ones such as <code>application/cats</code>.</p>

<p>This is in contradiction with the <a href="https://cheatsheetseries.owasp.org/cheatsheets/REST_Security_Cheat_Sheet.html#validate-content-types">OWASP recommendation</a> around validation around content types.</p>


<p>The GitHub API does not reject requests that contain duplicate headers. The HTTP standard itself allows duplicate headers for specific cases, but allowing duplicate headers might lead to hidden bugs.</p>

<h2 id="spaces-are-not-trimmed-from-values">Spaces are not trimmed from values</h2>
<p>If the request fields are prefixed or trailed with spaces, they are rejected as invalid values. For example sending a <code>Haskell</code> space-prefixed value in the <code>gitignore_template</code> will cause the service to return an error.
Although this is inconsistent with the fact that if you trail or prefix the <code>since</code> and <code>before</code> fields with spaces, the values get trimmed successfully and converted to dates.
As a common approach I think that services should consistently trim spaces by default (maybe with some business-driven special cases) for all request fields and perform the validation after.</p>

<h2 id="accepting-new-fields-in-the-request">Accepting new fields in the request</h2>
<p>The GitHub API seems to allow injection of new json fields inside the requests. The <code>NewFieldsFuzzer</code> adds a new <code>catsField</code> inside a request, but GitHub API accepts it as valid.
This is again in contradiction with the <a href="https://cheatsheetseries.owasp.org/cheatsheets/REST_Security_Cheat_Sheet.html#input-validation">OWASP recommendation</a> which suggests rejecting unexpected content.</p>

<h2 id="doesnt-make-proper-use-of-enumerated-values">Doesn’t make proper use of enumerated values</h2>
<p>There are cases when it makes sense to use enums rather than free text. Some examples are the <code>gitignore_template</code> field or the <code>license_template</code> field, which are rejecting invalid values. They are obviously having a pre-defined list of values, but do not enforce this in any way in the contract.
Having them listed as enums will also make it easier to understand what are all supported templates for example.</p>


<p>There are 2 fields called <code>since</code> and <code>before</code> which seem to actually be a <code>date-time</code>, although in the OpenAPI contract they are only marked as <code>string</code> without any additional <code>format</code> information. In the description of the fields it states that this is actually an ISO date, but it will also be good to leverage <a href="https://swagger.io/docs/specification/data-models/data-types/">the features of OpenAPI</a> and mark them accordingly.</p>

<p><img src="https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/date_time_github.png" alt="Date-Time Error"></p>

<h2 id="mismatch-of-validation-between-front-end-driven-calls-and-direct-api-calls">Mismatch of validation between front-end driven calls and direct API calls</h2>
<p>The <code>homepage</code> field seems to accept any values when doing a direct API call, but when you try to set this from the UI, you will get an error saying that you need to enter a valid URL.</p>

<p><img src="https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/ui_error_github.png" alt="Validation Error from the UI"></p>

<p>Having the same level of validation for backend and frontend is another good practice for making sure you don’t end up with inconsistent data.</p>


<p>There are some other failures which might seem debatable or not applicable:</p>
<ul>
  <li><code>description</code> accepts very large strings (50k characters sent by CATS), although the GitHub API doesn’t actually have constraint information in the contract; again this is not necessarily a problem, but it’s important for the contract to enforce constraints</li>
  <li>The <code>RecommendedHeadersFuzzer</code> expects a <code>CorrelationId/TraceId</code> to be defined in the headers, but this being a public API, it’s not actually applicable</li>
  <li>The <code>CheckSecurityHeadersFuzzer</code> expects a <code>Cache-Control: no-store</code> as per the OWASP recommendations, but the endpoint does not operate critical data, so allowing caching of the information is fine</li>
</ul>


<p><strong>Before proceeding, please be careful to not delete your real repos</strong>.
This is the script I’ve used to delete the repos. …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ludovicianul.github.io/2020/10/05/github-api-testing/">https://ludovicianul.github.io/2020/10/05/github-api-testing/</a></em></p>]]>
            </description>
            <link>https://ludovicianul.github.io/2020/10/05/github-api-testing/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035195</guid>
            <pubDate>Mon, 09 Nov 2020 14:34:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Finding passwords in the public Gists feed]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035158">thread link</a>) | @passflow
<br/>
November 9, 2020 | https://gistsecrets.io/home | <a href="https://web.archive.org/web/*/https://gistsecrets.io/home">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    <div>
      <p><a href="https://gistsecrets.io/home">
        <img src="https://gistsecrets.io/images/header.svg" alt="GistSecrets.io">
      </a></p><div>
        <p><a href="https://gistsecrets.io/about">About</a></p>
        <p><a href="https://twitter.com/intent/tweet?text=Alert%20people%20who%20may%20have%20saved%20their%20plaintext%20passwords%20with:%20https://gistsecrets.io">Tweet</a></p>
        <p><a href="https://www.linkedin.com/in/florian-overfelt">LinkedIn</a></p>
        <p><a href="https://www.buymeacoffee.com/floverfelt">Donate</a></p>
      </div>
    </div>
    <div>
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
    </div>

    
    <div>
      <p><a href="https://gistsecrets.io/home">
        <span>Â©</span><img src="https://gistsecrets.io/images/header.svg" alt="GistSecrets.io">
      </a>
    </p></div>
  
</div>]]>
            </description>
            <link>https://gistsecrets.io/home</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035158</guid>
            <pubDate>Mon, 09 Nov 2020 14:31:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Cowboys and Villagers. Simple Thoughts on Hiring]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035124">thread link</a>) | @etherio
<br/>
November 9, 2020 | https://devinfee.com/cowboys-villagers-6b3e0740f56e | <a href="https://web.archive.org/web/*/https://devinfee.com/cowboys-villagers-6b3e0740f56e">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section><div><div><h2 id="e53f">Simple Thoughts on Hiring</h2><div><div><div><p><a href="https://medium.com/@dfee?source=post_page-----6b3e0740f56e--------------------------------" rel="noopener"><img alt="Devin Fee" src="https://miro.medium.com/fit/c/96/96/1*eQw2G_FuHx8Ta00xEvlJEg.png" width="48" height="48"></a></p></div></div></div><figure><div role="button" tabindex="0"><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/3360/1*KfFjWyfbhqsxEpsYt3kS1A.jpeg" width="1680" height="1050" srcset="https://miro.medium.com/max/552/1*KfFjWyfbhqsxEpsYt3kS1A.jpeg 276w, https://miro.medium.com/max/1104/1*KfFjWyfbhqsxEpsYt3kS1A.jpeg 552w, https://miro.medium.com/max/1280/1*KfFjWyfbhqsxEpsYt3kS1A.jpeg 640w, https://miro.medium.com/max/1400/1*KfFjWyfbhqsxEpsYt3kS1A.jpeg 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/1*KfFjWyfbhqsxEpsYt3kS1A.jpeg?q=20"></p></div></div></div></figure><p id="113b">Your first hires were probably mistakes. Mine were disasters; they demonstrated humanity’s fascinating ability to self-implode. Hiring is hard.</p><p id="3694">You’ll probably begin by hiring people who look like you. Not demographically (right?), but people who share the same traits you share: hard working, perfectionist, and maybe even unforgiving. Or, you’ve hired people you want to be friends with — people you’d enjoy going to a concert with or perhaps carrying on a different type of relationship with (i.e. platonic).</p><p id="1a3b">So while I was digging a bit deeper into <a href="https://www.saastr.com/" rel="noopener">SaaStr</a> this evening I came across an interesting discussion between Sarah Lacey and <a href="https://www.slack.com/" rel="noopener">Slack’s</a> CEO, Stewart Butterfield. Slack is a team of 40 year-old plus employees. Slack doesn’t screen for age, but the company does focus on building a village. Not a frat. How many times do we as builders have to learn these lessons?</p><figure><div></div></figure><p id="a15f">This clip highlights the struggle of managing towards a culture with balance and purpose. Sarah discusses with Stewart the work / life balance at <a href="https://www.techcrunch.com/" rel="noopener">TechCrunch</a> with grueling hours and punishing schedules. This thought is then juxtaposed against <a href="https://www.pandodaily.com/" rel="noopener">PandoDaily</a>, where she takes the marathon approach of business building.</p><p id="38c1">These two conclude by discussing the design of a business foundation that is mature and respectful: hallmark traits of a dependability and resilience.</p><p id="14b8">Deviating from the narrative they’ve laid, my experience in hiring and development of employees leads to my understanding that short-term benefits of bringing on cowboys — my way or the highway folk — comes at the cost of any long term benefits. The rapidity of responsibility delegation is only matched by the rate at which they hand it back on their way out. Cowboys don’t stick around; they’re cowboys!</p><p id="8017">Following from that philosophy, there are two hiring questions I fall back on during the candidate screening process:</p><ul><li id="1a87">what do <strong>you</strong> want out of this experience?</li><li id="358e">what resources do <strong>you</strong> need to call on to be successful?</li></ul><p id="9a58">The first question is about alignment and fit. If my needs (as a hiring manager) and your desires are aligned, we can do business today and work in a common direction. If there is a mismatch here and yet I advance the candidate down the hiring-pipeline, I ought to soul-search about what I’m trying to accomplish. I (read: you) need a reality check.</p><p id="0b3c">The second question — really a topic for another day — should help you determine whether the candidate has been there before or whether they’re just a <a href="https://en.wikipedia.org/wiki/Peter_principle" rel="noopener">Peter principle</a> candidate.</p><p id="23a2">Building a village in your business is about hiring function-focused staff who are able to provide excellence and depth for today’s needs. The villagers take pride in their work and stay firmly focused on improving themselves and the village. They demonstrate ownership and long-term buy-in. They often idolize the cowboy, but realize those dreamy nights spent under the stars are often spent alone.</p><p id="ceff">Remember, you’re also a villager. Yes you, the founder, the exec, the CEO. It is up to you to as mayor to bring in good citizens. More good citizens will follow.</p><p id="6d6a">Or don’t. Develop organizations that lack the fundamentals of dependability and resilience. Your villagers and best talent will exit alongside the cowboys. They’ll just be headed in separate directions.</p><p id="3355"><em>Originally published on October 7th, 2015.</em></p></div></div></section></div></div>]]>
            </description>
            <link>https://devinfee.com/cowboys-villagers-6b3e0740f56e</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035124</guid>
            <pubDate>Mon, 09 Nov 2020 14:28:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Prometheus Pushgateways – Everything You Need to Know]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035108">thread link</a>) | @botayhard
<br/>
November 9, 2020 | https://www.metricfire.com/blog/prometheus-pushgateways-everything-you-need-to-know/ | <a href="https://web.archive.org/web/*/https://www.metricfire.com/blog/prometheus-pushgateways-everything-you-need-to-know/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          
          

    

          <h2 id="Intro">Intro</h2>
<p>You have your MetricFire Prometheus instance(s) scraping your scraping targets for every scrape_interval time period, and that is all fine and dandy. But, what if some of your applications do not live long enough to be discovered and scraped by Prometheus? For example if you are using Kubernetes service discovery, and your pods do not live enough to be picked up? Then the typical pull model of Prometheus does not fit anymore because Prometheus cannot find or scrape all of the targets. Some kind of solution is needed.</p>
<p>This is where <a href="https://github.com/prometheus/pushgateway">Prometheus Pushgateways</a> come in. You can send metrics data to the Pushgateway from your short-lived scripts, then the metrics will be eventually ingested by Prometheus. This article on MetricFire’s blog will tell you everything that you need to know so that you can quickly start using Pushgateways. Everything below has been tested for the version 1.2.0 of Prometheus Pushgateway.</p>
<p>To follow along with the blog on your own Prometheus instance, try setting up the MetricFire hosted Prometheus. You can <a href="https://metricfire.com/trial-demo">sign on to our free trial</a>, and get all of the benefits of Prometheus and Grafana with none of the hassle of installation.</p>

<h2 id="Basic-Usage">Basic Usage</h2>
<h3 id="Running-a-Pushgateway">Running a Pushgateway</h3>
<p>The first and most important question - what do I actually need to run a Prometheus Pushgateway? The software is written in Go, so it is distributed as a self-contained binary that you can run on any of Go’s supported operating systems and architecture. If it is most convenient for you, then you can go to the github page for <a href="https://github.com/prometheus/pushgateway/releases">Prometheus Pushgateway releases</a>, and download those binaries there. Here is a handy bash script to download, extract, and run the specified version:</p>

<pre><code>#!/bin/bash
VERSION="1.2.0"
wget "https://github.com/prometheus/pushgateway/releases/download/v${VERSION}/pushgateway-${VERSION}.linux-amd64.tar.gz"
tar xvzf "pushgateway-${VERSION}.linux-amd64.tar.gz" "pushgateway-${VERSION}.linux-amd64/pushgateway"
rm -f "pushgateway-${VERSION}.linux-amd64.tar.gz"
mv "pushgateway-${VERSION}.linux-amd64/pushgateway" ./
rmdir "pushgateway-${VERSION}.linux-amd64"</code></pre>

<p>Obviously, you ought to change the operating system and architecture in this snippet if it is different on your machine. This will give you a binary pushgateway in your current working directory. You can use it to start your own Pushgateway.</p>
<p>If you would rather use a container then there are premade Docker images already waiting for you in the Docker hub. A simple:</p>

<pre><code>docker run -it -p 9091:9091 --rm prom/pushgateway&nbsp;</code></pre>
<p>‍&nbsp;</p>
<p>will give you a Pushgateway container that gets deleted automatically on shutdown on localhost:9091:</p>
<p>‍&nbsp;</p>
<p><img src="https://cdn.buttercms.com/JDQRJvG4RAyraAGXExUR" alt="undefined"></p>

<p>Finally, let’s talk about Kubernetes. <a href="https://helm.sh/">Helm</a> is the ubiquitous package manager for Kubernetes that we will use. It has a nice chart for Pushgateway that you can install with the following command:</p>

<pre><code>helm install stable/prometheus-pushgateway</code></pre>

<p>By default, the chart will create a Service that is also listening on port 9091. You can find the reference list of all of the options <a href="https://hub.helm.sh/charts/stable/prometheus-pushgateway">here</a>. What’s more, the chart even integrates with the <a href="https://github.com/coreos/prometheus-operator">Prometheus Operator</a>. What this integration means is that if you add serviceMonitor.enabled to that chart, then your Pushgateway will be automatically scraped by Prometheus. This is a huge topic in itself, and it is more related to Kubernetes than Pushgateways, so we will skip it in this article. Feel free to explore the links above to learn more about it.</p>
<p>The default options given by Pushgateway should work well in most of the cases. Here they are with the explanation of each of them:</p>
<ul>
<li><em>--web.listen-address=:9091</em>, IP (optional) and port pair on which to listen for requests;</li>
<li><em>--web.telemetry-path=/metrics</em>, Path under which the metrics (both user-sent and internal ones) of the Pushgateway will be exposed;</li>
<li><em>--web.external-url=</em>, URL on which this Pushgateway is externally available. Useful if you expose it via some domain name;</li>
<li><em>--web.route-prefix=</em>, if specified then uses this as a prefix for all of the routes. Defaults to <em>--web.external-url</em>’s prefix;</li>
<li><em>--web.enable-lifecycle</em>, if specified then lets you shutdown the Pushgateway via the <a href="https://github.com/prometheus/pushgateway#management-api">API</a>;</li>
<li><em>--web.enable-admin-api</em>, if specified then enables the Admin API. It lets you perform certain destructive actions. More on that in the following sections;</li>
<li><em>--persistence.file=</em>, if specified then Pushgateway writes its state to this file every <em>--persistence.interval</em> time period;</li>
<li><em>--persistence.interval=5m</em>, how often the state should be written to the previously specified file;</li>
<li><em>--push.disable-consistency-check</em>, if specified then the metrics are not checked for correctness at ingestion time. Should not be specified in the absolute majority of cases;</li>
<li><em>--log.level=info</em>, one of: <em>debug</em>, <em>info</em>, <em>warn</em>, <em>error</em>. Only prints messages with levels higher than that;</li>
<li><em>--log.format=logfmt</em>, possible values: <em>logfmt</em>, <em>json</em>. Specify <em>json</em> if you want structured logs that could be used with, for example, <a href="https://www.elastic.co/">Elasticsearch</a>.</li>
</ul>
<p>‍&nbsp;</p>
<h3 id="Sending-Metrics">Sending Metrics</h3>
<p>Now we have a Pushgateway up and running. However, how do you send your own metrics to it from your ephemeral batch jobs? There are two ways:</p>
<ol>
<li>You can use a program which can do web requests, or&nbsp;</li>
<li>You can use a client library.</li>
</ol>
<p>‍</p>
<p>The former is more applicable in the cases where your batch job is written in a language such as Powershell or Bash.</p>
<p>With Powershell you can use the canonical <em>Invoke-WebRequest</em> which sends a web request to the given URL and given data. To send metrics data to a Pushgateway on Windows Powershell you can use this snippet:</p>
<p>‍&nbsp;</p>
<pre><code>$metrics = "
# TYPE some_metric 
gaugesome_metric 42
# TYPE awesomeness_total counter
# HELP awesomeness_total How awesome is this article.
awesomeness_total 99999999
"Invoke-WebRequest-Uri "http://localhost:9091/metrics/job/metricfire/instance/article" -Body $metrics -Method Post</code></pre>

<p>Running the above snippet will give you the following result in the user interface at <em>localhost:9091</em>:</p>
<p>‍&nbsp;</p>
<p><img src="https://cdn.buttercms.com/YajDBBbnS2aFgKJ2oSVG" alt="undefined"></p>

<p>Consequently, the <em>/metrics</em> end-point now has the following data:</p>
<p>‍&nbsp;&nbsp;</p>
<p><img src="https://cdn.buttercms.com/4DkYcx6mSSODkC8kgQNo" alt="undefined"></p>
<p>&nbsp;&nbsp;‍</p>

<p><img src="https://cdn.buttercms.com/fFsMQLDLSgiuaZuumava" alt="undefined"></p>



<p>As you can see, there are two labels, job and instance. If Prometheus can't find them, it will attach them by itself if the configuration value <em>honor_labels</em> is <em>true</em>. In the snippet above they have been added to the metrics data via the URL that we have used to make the request.</p>
<p>For Linux and other Unix(-like) operating systems we can use the widespread cURL to do this request. The following snippet achieves the same result as the previous one:</p>
<p>‍&nbsp;</p>
<pre><code>cat &lt;&lt;EOF | curl --data-binary @- http://localhost:9091/metrics/job/metricfire/instance/article
# TYPE some_metric gauge
some_metric 42
# TYPE awesomeness_total counter
# HELP awesomeness_total How awesome is this article.
awesomeness_total 
99999999
EOF</code></pre>

<p>The labels specified in the URL are used for grouping metrics together. This enables them to be easily deleted later on because we can refer to small groups of metrics together.</p>
<p>In the case where your job or other labels contain a slash (/) character, then you will need to do extra work. You can find more information <a href="https://github.com/prometheus/pushgateway#url">here</a>.</p>
<p>The only needed label that is necessary to pass metrics via the URL is the <em>job</em> label. Also, note that any of the labels passed via the URL will overwrite whatever labels that the metrics passed via the body might have. If you have any doubts then you can always push some example metrics and check what is being exposed to Prometheus by checking the /metrics (or some other path, if you have it changed via <em>--web.telemetry-path</em>): <a href="http://localhost:9091/metrics">http://localhost:9091/metrics</a>.&nbsp;</p>
<p>If everything succeeds then you will get a response with 200 HTTP status code. If you have disabled the consistency check then you might get a 202 HTTP status code, which means that your metrics have been queued for inclusion into the next scrape, and that they haven’t been checked yet. The actual scraping might fail if you’ve pushed some invalid metrics. For example, if you have pushed them with different types, even if you have written the name exactly the same.&nbsp;</p>
<p>Finally, a 400 HTTP status code means that you’ve pushed some invalid metrics and they have been rejected. In such a case, the HTTP response indicates what is wrong. For example: pushed metrics are invalid or inconsistent with existing metrics: <em>pushed metrics are invalid or inconsistent with existing metrics: collected metric "some_metric" { label:&lt;name:"instance" value:"article" &gt; label:&lt;name:"job" value:"metricfire" &gt; label:&lt;name:"label" value:"test" &gt; gauge:&lt;value:12345 &gt; } was collected before with the same name and label values</em>.</p>
<p>This means that the same metric has been defined twice in one request. It is invalid to redefine the metric’s value in the same request or scrape.</p>


<p>&nbsp; ‍&nbsp;</p>
<h3 id="Scraping-Metrics">Scraping Metrics</h3>
<p>The configuration that you might have is highly variable depending on the different service discovery mechanism(-s) that you have at your disposal. Explore the different configuration options available <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config">here</a>. The simplest option is to statically define a Pushgateway target from which to scrape metrics:</p>
<p>‍&nbsp;</p>
<pre><code>scrape_configs:
- job_name: pushgateway
  honor_labels: false
  static_configs:
  - targets: ['localhost:9091']
    labels:
      pushgateway_instance: metricfire</code></pre>

<p>This will make your Prometheus scrape a Pushgateway instance at <em>localhost:9091</em> if you are running a Pushgateway locally. Another thing to consider is enabling the <em>honor_labels</em> parameter. Having it enabled means that Prometheus would opt to choose whatever comes from the Pushgateway instead of what Prometheus is trying to attach when it tries to add certain reserved labels or labels which have you specified in the scraping configuration. For example, if you had a metric in your Pushgateway <em>awesomeness{pushgateway_instance=”notmetricfire”}</em> then it would use the value <em>notmetricfire</em> for that label instead of <em>metricfire</em> as defined in the scraping configuration that you can see in the code snippet above. Without it, the original label will be renamed to <em>exported_pushgateway_instance</em> which would have the value <em>notmetricfire</em>.</p>
<p>Enabling <em>honor_label</em> is useful in cases where you want to …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.metricfire.com/blog/prometheus-pushgateways-everything-you-need-to-know/">https://www.metricfire.com/blog/prometheus-pushgateways-everything-you-need-to-know/</a></em></p>]]>
            </description>
            <link>https://www.metricfire.com/blog/prometheus-pushgateways-everything-you-need-to-know/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035108</guid>
            <pubDate>Mon, 09 Nov 2020 14:27:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A look at S&P 500's real excess return over Treasuries over long time horizons]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25035020">thread link</a>) | @fa
<br/>
November 9, 2020 | https://fasiha.github.io/post/excess-returns/ | <a href="https://web.archive.org/web/*/https://fasiha.github.io/post/excess-returns/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><figure><img src="https://fasiha.github.io/post/excess-returns/The_Monitor_and_Merrimac.jpg"></figure>  <ul>
    <li><a href="https://fasiha.github.io/">Blog</a></li>
    <li><a href="https://fasiha.github.io/#contact">Contact</a></li>
    <li><a href="https://fasiha.github.io/atom.xml">Feed</a></li>
  </ul><p><em>Updated on Sun, 08 Nov 2020 03:01:01 GMT, tagged with ‘finance’.</em></p><p>Imagine. It's 1871. A promising young American has just entered the workforce and makes it a point to buy $100 of the S&amp;P 500 index every month, with dividends reinvested, over a forty year career. It's now 1911. Our American, about to retire, stops these monthly purchases and asks, "What is the real return I achieved in excess of risk-free Treasuries over my forty-year investing horizon?"</p>
<p><strong><em>Answer: 3.8%.</em></strong></p>
<blockquote>
<p>You want details, I got details. You can read the <a href="https://github.com/fasiha/shiller-heat/blob/8cac0574702320ffc41302c8392b4929855ed321/shiller.ts#L121-L125">code</a> or the <a href="https://drive.google.com/file/d/1jzBiJ4OAIDo35Nom0U6a655TJAnqeoMf/view?usp=sharing">spreadsheet</a> (start at cell W5), but I use Robert Shiller's <a href="http://www.econ.yale.edu/~shiller/data.htm">online dataset</a>. It contains monthly numbers for the S&amp;P 500 index's price (dollars per share), dividends (dollars per month), CPI (consumer price index, to discount inflation), and 10-year Treasury yields, all starting in 1871. I assume you invested $1 at the beginning of each month at the real CPI-adjusted price of the stock index, reinvesting the dividends that accrued over the previous month. After 480 such buying sessions, I calculate the internal rate of return (XIRR) by assuming the entire portfolio was liquidated, which is just an accounting choice to answer the question, "what real return did the S&amp;P 500 yield over this forty year horizon after monthly dollar cost averaging".</p>
<p>I then do a similar exercise with Treasuries: every month I assume you put that real $1 into a savings account-like vehicle that pays interest monthly at the same rate as the 10-year T-note's (CPI-adjusted). XIRR again computes the internal rate of return, over the same time horizon. <em>Excess</em> return is just the S&amp;P's real return minus the Treasuries' real return, and is expressed in a percentage just like any rate of return.</p>
</blockquote>
<p>Imagine now that every year after 1871, we can find one such promising young American to join the work force and to do the same thing: monthly-dollar-cost average into the S&amp;P 500 index for forty years.</p>
<p>Seven years later, the investor who began dollar-cost averaging in 1878 and asks in 1918 what their real excess rate of return was, gets a shocking number.</p>
<p><strong><em>0.3%.</em></strong></p>
<p>This investor retiring in 1918; the next one retiring in 1919, and 1920, and on, up to 1924: each of these see an excess real rate of return between <strong><em>-1.3%</em></strong> and <strong><em>0.3%</em></strong>. In 1925, the retiree who began in 1885, sees a <strong><em>0.8%</em></strong> excess real return, and only after them does each successive year's retiree see a nice positive excess real return.</p>
<p>The graph below plots this time series: the excess real return each year's retiree saw, from 1911 to 2020. Thanks to Plotly, it's interactive so you can click, tap, zoom, pan, pinch, etc. You can see it starts out at the 3.8% mentioned above, drops to -1.3% in the early 1920s, and wanders between -2.4% and 6.6%, as each year's retiree does a bit better or worse than the previous year's. The <em>median</em> excess real return for all our retirees: 1.6%.</p>

<p>From 1925 to 1981, each retiree saw a positive real excess return.</p>
<p>Then, from 1981 to 2013, thirty-one retirees during this thirty-three year interval saw negative excess real returns over forty years of monthly dollar-cost averaging. (There was a brief blip into positive territory during 1999 and 2000, i.e., the Tech Bubble.) That's a whole generation: a parent and their child could both have seen zero real excess return, over a career's worth of investing.</p>
<p>As I'm writing this, in late 2020, I see this year's retiree is looking back on 3.1% real excess return of the S&amp;P 500 over Treasuries, over forty years of monthly dollar-cost averaging. I'm about fifteen years into my career. You might be thirty-five years into yours, or just three years. We don't yet know what the graph will look like for us when we retire, in five, twenty-five, and thirty-seven years hence—but seeing this graph, with its plateaus and gyrations, and imagining at each point a retiree looking back on <em>a lifetime</em> of following solid retirement advice, gives me pause: so many of them were <em>unrewarded</em> for investing in stocks—they could have just bought Treasuries and relaxed.</p>
<p>I'm caricaturing retirement conventional wisdom a little bit—although its simplest tenet is to buy and hold a diversified basket of equities and reap its risk premium over the long-term, we haven't simulated a glide path to bonds, or explored any alternatives. Nevertheless, for me personally, answering this simple question about equities' excess real returns following this commonly-recommended strategy was very illuminating, because it makes me feel that retirement savings is less of a solved topic than I thought.</p>
<p><strong>Postscript</strong> A friend recently asked me, "What should I invest in for my newborn's college fund?" Although the universe of assets worth considering is much larger than just S&amp;P 500 ETF vs a money market fund, the above analysis can apply: what is the excess real return of the S&amp;P 500 over Treasuries over all <em>twenty</em> year horizons of monthly dollar-cost averaging, with dividends reinvested?</p>
<p>That's the graph below (along with several other horizons).</p>

<p>Answer: it ranges from between 10.4% to -10% annualized rate of return, with a <em>median</em> of 2.4%. If we imagine an annual series of parent–investors adopting this strategy, the first's child turning twenty in 1891, this median means that half of them saw excess returns greater than 2.4% and half of them less. Do these returns justify putting your child's college fund in the S&amp;P 500? I would feel sufficiently uneasy about this and seek to explore other options.</p>
<blockquote>
<p><strong>Footnote</strong> Earlier versions of this post were circulated in February 2019, and again in March 2020 which led to finding and fixing a bug in the calculation. The <a href="https://drive.google.com/file/d/1jzBiJ4OAIDo35Nom0U6a655TJAnqeoMf/view?usp=sharing">Google Sheets version</a> of Shiller's dataset includes calculations for a single data point. I also sought some feedback from the readers of <a href="https://money.stackexchange.com/q/121010">Personal Finance Stack Exchange</a>.</p>
</blockquote>
<p>(Banner: a crop from "The Monitor and Merrimac: The First Fight Between Ironclads", chromolithograph by Louis Prang &amp; Co., 1886. <a href="https://commons.wikimedia.org/wiki/File:The_Monitor_and_Merrimac.jpg">Wikimedia</a>)</p>


<p>
<small>Previous: <a href="https://fasiha.github.io/post/risk-for-kids-and-grownups/">Learning about risk, for kids and grownups</a><br></small>
</p>
</div>]]>
            </description>
            <link>https://fasiha.github.io/post/excess-returns/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25035020</guid>
            <pubDate>Mon, 09 Nov 2020 14:19:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Tiny CI System]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25034823">thread link</a>) | @yule
<br/>
November 9, 2020 | https://www.0chris.com/tiny-ci-system.html | <a href="https://web.archive.org/web/*/https://www.0chris.com/tiny-ci-system.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        

<p>2020-11-08</p>
<p>This is a little demonstration of how little you need to host your own git
repositories and have a modest <a href="https://en.wikipedia.org/wiki/Continuous_integration">Continuous Integration</a>
system for them. All you need is a unixy
server you can ssh into, but arguably you can try this out locally as well.
We will use Redis at one point to queue tasks, but
strictly speaking this can be achieved without additional software. To keep things
simple this will only work with one repository, since this is only describing a
pattern.</p>
<p>The source code to all of that follows below can be found <a href="https://git.sr.ht/%7Estchris/tiny-ci">here</a>.</p>
<h2>Hosting bare git repositories</h2>
<p>Assuming you can ssh into a server and create a directory, this is all you need
to create a shareable git repository:</p>
<pre><code>$ git init --bare
</code></pre>
<p>Ideally you are using a distinct user for it (named <code>git</code>) and have it set to
use <code>git-shell</code> as its default shell. By convention bare repositories are stored
in directories which end in <code>.git</code>. You can now clone this repository from your
machine with:</p>
<pre><code>$ git clone ssh://git@host.example.com/~git/repo.git
</code></pre>
<h2>post-receive hooks</h2>
<p>A <a href="https://git-scm.com/docs/githooks#post-receive">post-receive hook</a> is an executable which can do some work as soon as something new was pushed to the repository. We will use an executable shell script which needs to go inside the <code>hooks</code> directory of the (bare) repository on the server side.</p>
<p>Now the most trivial thing to do would be to do the actual work in here, but this would block the <code>git push</code> on the client side, so we just want to enqueue a new job, return a handle and exit. If what you do takes only a short amount of time, you can stop here. Alternatively you can use this repository for deployments only, by defining it as a separate remote. But the goal here is to have tests run on every push, so we will split the job creation from the actual run.</p>
<p>This is where Redis comes into play for the job queueing. We will assume redis is installed and running and we will use redis-cli to access it from the script. We will use two data structures: a list of jobs waiting to be executed, referenced by a UUID we will generate and a hash where we can store the git revision and the state associated to a given job, as well as its output.</p>
<p>Note that git is passing three arguments to the script via stdin: the old revision before the push, the new revision and the current ref.</p>
<pre><code>#!/bin/bash
while read -r _ newrev ref
do
	id=$(uuid)
	echo "Starting CI job $id"
	redis-cli hset "$id" rev "$newrev" &gt;/dev/null
	redis-cli hset "$id" ref "$ref" &gt;/dev/null
	redis-cli lpush jobs "$id" &gt;/dev/null
done
</code></pre>
<h2>Defining build jobs</h2>
<p>By convention our system will run whatever is in an executable script named <code>ci.sh</code>. The drawback is that this only works with trusted systems and access to the repository needs to be guarded to prevent random code execution. The big advantage is that we don't need to come up with a job definition DSL or cumbersome file format.</p>
<p>Our convention will also be that the script will be passed one argument: the name of the git ref, so we can decide what to do based on the branch we are on.</p>
<p>Let's just put this into a file named <code>ci.sh</code>:</p>
<pre><code>#!/usr/bin/env bash

# the git ref gets passed in as the only argument
ref="$1"

# pretend we're running tests
echo "running tests"

# only deploy if we're on the main branch
[[ "$ref" == "refs/heads/main" ]] &amp;&amp; echo "Deploying"
</code></pre>
<h2>The build runner</h2>
<p>Now that jobs are queued the last piece missing is a job runner. We will make use of Redis' <a href="https://redis.io/commands/blpop">BLPOP command</a> to block until the jobs list has a new job for us. That job id will give us the revision we need to check out and will allow us to write back the output and status of the job.</p>
<p>Note that, as discussed, this assumes a repository called <code>test</code> is already checked out right next to the script.</p>
<p>tiny-ci.sh</p>
<pre><code>#!/usr/bin/env bash

# ./runner.sh is supposed to run on the server where your git repository lives

# the logic in here will run in an infinite loop:
# * (block and) wait for a job
# * run it
while :
do

# Announce that we're waiting
echo "Job runner waiting"

# We are using https://redis.io/commands/blpop to block until we have a new
# message on the "jobs" list. We use `tail` to get the last line because the
# output of BLPOP is of the form "list-that-got-an-element\nelement"
jobid=$(redis-cli blpop jobs 0 | tail -n 1)

# The message we received will have the job uuid
echo "Running job $jobid"

# Get the git revision we're supposed to check out
rev=$(redis-cli hget "${jobid}" "rev")
echo Checking out revision "$rev"

# Get the git ref
ref=$(redis-cli hget "${jobid}" "ref")

# Prepare the repository (hardcoded path) by getting that commit
cd test || exit; git fetch &amp;&amp; git reset --hard "$rev";

# Actually runs the job and saves the output
if ! output=$(./ci.sh "$ref" 2&gt;&amp;1);
then
    status="failed";
else
    status="success";
fi;

# Update the result status
redis-cli hset "${jobid}" "status" $status;

# Update the job output
redis-cli hset "${jobid}" "output" "$output";

echo "Job ${jobid} done"

done
</code></pre>
<h2>Running it</h2>
<p>Summing up:</p>
<ul>
<li>there's a bare git repository somewhere, called <code>test.git</code></li>
<li>we can clone the empty repo (or create a new one and add the respective remote)</li>
<li>on the server hosting the git repository we clone <code>test.git</code> into <code>test</code> and place <code>tiny-ci.sh</code> next to it</li>
<li>we run builds by starting <code>tiny-ci.sh</code> on the server hosting the repository</li>
</ul>
<p>Now if we <code>git push</code> a new commit to the <code>main</code> branch with the <code>ci.sh</code> file from above, the output will return the job id</p>
<pre><code>Enumerating objects: 5, done.
...
remote: Starting CI job dab82634-21cc-11eb-b3b3-9b8767dff47c
</code></pre>
<h2>Checking build status</h2>
<p>Knowing a job uuid, the easiest way to get the status
of a build is by using the <code>--csv</code> style output of the <a href="https://redis.io/commands/hgetall">HGETALL</a> command of redis.</p>
<pre><code>$ ssh example.com redis-cli --csv hgetall $JOB_UUID
"rev","f0706ea18a22031f84619b1161c8fbdb0dcd6850","ref","refs/heads/master","status","success","output","running tests\nDeploying"
</code></pre>
<h2>Possible further improvements</h2>
<ul>
<li>
<p><strong>multi-repo support</strong></p>
<p>This would mean changes to the <code>post-receive</code> hook to put jobs in a list named <code>job-${REPONAME}</code> and then have the worker also react based on that. Notice how <code>redis-cli blpop</code> takes several lists to watch and will also return the name of the list.</p>
</li>
<li>
<p><strong>job cleanup</strong></p>
<p>Creating a key for every job pollutes the redis database unnecesarily. Enqueuing the job could be done via <a href="https://redis.io/commands/setex">SETEX</a> so that the keys go away after one hour / one day / one week. The purpose of Redis here is short term storage and not long-term archival of job results</p>
</li>
<li>
<p><strong>more workers</strong></p>
<p>Scaling to multiple workers on the same machine would need different working folders (and some process isolation depending on the tasks run in there). Scaling to multiple machines would need access to a central redis instance for job distribution.</p>
</li>
<li>
<p><strong>worker isolation / sandboxing</strong></p>
<p>For more complex tasks some kind of process and file-system isolation is necessary. The worker could spin up VMs or Docker containers. The build system used on <a href="https://builds.sr.ht/">builds.sr.ht</a> for instance uses a <a href="https://man.sr.ht/builds.sr.ht/installation.md#security-model">Docker container run as an unprivileged user in a KVM qemu machine</a>.</p>
</li>
<li>
<p><strong>timestamps</strong></p>
<p>For convenience you would definitely want timestamps for every operation. This also allows to list queries like "the last five jobs" or to do maintenance on job results based on their time.</p>
</li>
<li>
<p><strong>notifications</strong></p>
<p>Any CI system will have some form of notifications and the simplest form would be to do something in the script, right at the end. But this covers only the success case, so a better approach would be to create a notification queue and have a notification worker react on that.</p>
</li>
</ul>
<p><a href="https://lobste.rs/s/fbc6wl/tiny_ci_system">Discuss on lobste.rs</a></p>


<ul>
  
</ul>

    </div></div>]]>
            </description>
            <link>https://www.0chris.com/tiny-ci-system.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034823</guid>
            <pubDate>Mon, 09 Nov 2020 13:56:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[No Free Features]]>
            </title>
            <description>
<![CDATA[
Score 72 | Comments 83 (<a href="https://news.ycombinator.com/item?id=25034809">thread link</a>) | @alangibson
<br/>
November 9, 2020 | https://landshark.io/2020/11/09/no-free-features.html | <a href="https://web.archive.org/web/*/https://landshark.io/2020/11/09/no-free-features.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	<p><a href="https://news.ycombinator.com/item?id=25032105">This thread recently hit the top of Hacker news</a>.</p>

<blockquote>
  <p>No More Free Work from Marak: Pay Me or Fork This</p>

  <p>Respectfully, I am no longer going to support Fortune 500s
( and other smaller sized companies ) with my free work.</p>
</blockquote>

<p>The gist is that Marak, who’s on the brink of homelessness after his apartment building caught fire, is no longer interested in doing unpaid work for businesses using his <a href="https://github.com/Marak/faker.js">faker.js</a> project. He seems to be getting a lot of support from the open source developer community.</p>

<h2 id="unpaid-interns">Unpaid Interns</h2>

<p>The foundational principle of open source is “fix your problem, then give the world a copy of the solution.” So let’s get one thing straight: <em>open source developers are not volunteering to fix your problem.</em> They are fixing their own problems, then letting you use the solution too because it costs them nothing. That near-zero cost of replicating software is why open source works.</p>

<p>Because of this, I don’t think developers claiming to do open source should expect compensation for features that they need for themselves. But developing a new feature that they don’t need is something different entirely. In IT we call that a Change Request, and CRs come with a fee to cover them. ‘Near-zero cost’ doesn’t apply anymore because now they’re taking on a lot of work they otherwise wouldn’t have done.</p>

<p>Not recognizing this difference has led to a situation where for-profit entities are using open-source devs as unpaid interns. Well it’s worse really: at least interns get resume filler.</p>

<h2 id="no-more-free-features">No More Free Features</h2>

<p>I look forward to a day when asking anyone to do unpaid labor is considered unethical by our industry. That goes for feature requests on open source projects, on unpaid internships, and on unpaid ‘take home’ interview assignments.</p>

<p>Requesting work in an economic context without offering compensation in some form is morally indefensible. It’s wrong because unpaid labor is wrong. It’s wrong because presuming on anyone’s helpful nature is wrong. We shouldn’t be using bounties to move our change requests to the head of the line because we shouldn’t even be making requests without a bounty attached.</p>

<p>(<a href="https://news.ycombinator.com/item?id=25034809">Official Hacker News discussion thread</a>)</p>

</div></div>]]>
            </description>
            <link>https://landshark.io/2020/11/09/no-free-features.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034809</guid>
            <pubDate>Mon, 09 Nov 2020 13:55:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[JIT Compiler of PCRE2]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25034716">thread link</a>) | @elvis70
<br/>
November 9, 2020 | https://zherczeg.github.io/sljit/pcre2_jit.html | <a href="https://web.archive.org/web/*/https://zherczeg.github.io/sljit/pcre2_jit.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://zherczeg.github.io/sljit/pcre2_jit.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034716</guid>
            <pubDate>Mon, 09 Nov 2020 13:46:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why you should care about privacy (even if no one else around you does)]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25034710">thread link</a>) | @betaman0
<br/>
November 9, 2020 | https://www.cupwire.com/why-you-should-care/ | <a href="https://web.archive.org/web/*/https://www.cupwire.com/why-you-should-care/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <p>There's a pattern I've noticed over the last couple years that goes something like this. </p><ul><li>Person has a personal revelation or their curiosity is piqued and are interested in privacy</li><li>Person reads articles and researches ways to become more private after learning how companies are abusing our data</li><li>Person finds an online forum, such as Reddit, and ends up sprinting down the rabbit hole of alternatives, fixes, and 'get private quick' actions</li><li>Person does too much, too fast and becomes burnt out</li><li>Person questions if it's all worth it because it's hard and no one else around them is doing it so why should they</li><li>Person goes back to old, comfortable habits</li></ul><p>It happens like clockwork and you can always tell what stage the person is at after talking to them for a moment or two. &nbsp;But, there's a specific step I want to hone in on for this post. &nbsp;The "no one else around me is doing it, so why should I bother" part.</p><p>Usually, this mentality comes during or directly after they try to change their daily habits, both IRL and online, all at once and become overwhelmed. They changed all of their passwords, email service, web browser, downloaded Signal, deleted Facebook and SnapChat, went and registered for a PO box, and shredded their credit cards all in the same week and are trying to cold turkey their way into privacy.</p><p>After spending hours of their time and a chunk of cash, they're sitting there wondering if it's all worth it. Their brain begins to cast doubt on their actions and look for justifications to drop everything and go back to how it was. &nbsp;Mom and dad are out living their life happily even though they use Facebook, Windows, and Chrome. &nbsp;Your best friend runs a successful local shop and uses Google for everything. &nbsp;Heck, even your younger sister is all over social media and signs up for every contest known to man, yet, her life doesn't seem any different than yours when you've gone through all of this work to become more private. </p><p>So why care about privacy when everyone around you doesn't? &nbsp;</p><p>Here's four reasons why you should care and why it's important you do.</p><h2 id="you-have-to-normalize-it">You have to normalize it</h2><p>Headlines about privacy and data abuse are at an all-time high but we still see the same arguments show up time after time.</p><div><p>“I have nothing to hide” <br>“They already know everything about me” <br>“I like Facebook and Google” </p><p>These statements don't always come from a place of ignorance. People, generally, know that Facebook and Google collects their data. &nbsp;Maybe not to the extent that they do, but people know they're giving up information to use their services. Often time, these comments come from a place of disinterest or general skepticism. </p></div><p>There's a perception that if you're concerned about privacy, you're trying to hide something or you bear a tin foil hat and shout conspiracies on the corner. Basically, being private is currently seen as "not normal". &nbsp;</p><p>Some of this is self inflicted, with privacy enthusiasts jokingly calling themselves paranoid, weirdos, or nut jobs. &nbsp;It may seem like it's all in good fun but over time, it begins to reinforce that these beliefs and habits are atypical when they aren't.</p><p>When we look at someone wearing a coat, we don't suddenly wonder what they're hiding. &nbsp;We think "oh, that person must be cold". Same thing with masks. Wearing a mask? &nbsp;You must either be sick or doing your part to prevent the spread of a global pandemic that's killed over 215,000 people in the US <em>cough</em>wearyourmask<em>cough</em>. The point is that it's no longer seen as odd to walk around everywhere with a medical grade mask on your face. It's been normalized. </p><p>This is the point we need to get privacy to and we do it by taking these actions ourselves. &nbsp;You might tell 10 people to use Signal and maybe one person does. Even though the other nine aren't using it, you've planted a seed in their mind. Next time one of their other friends or family member mentions downloading Signal, they'll remember that you mentioned it a while back and will start to think "is this something I should be checking out?" </p><p>One we individually begin to normalize these actions, behaviors, and services, <a href="https://en.wikipedia.org/wiki/Social_proof">social proof</a> will take over.</p><p>And why do you have to take a stand and do you part to help normalize it? Because...</p><h2 id="most-people-won-t-do-it-alone">Most people won't do it alone</h2><p>We are creatures of habit and it almost always takes some sort of external force to get us to change our ways. That could be as simple as stepping on the scale one day and going “holy crap, I didn’t realize I put on so much weight” or as serious as a near-death experience that forces us to quit drinking and appreciate the small things in life. It could also be as simple as a close friend or family member asking you to download a messenger app or explaining to you why Facebook is bad. </p><p>Chances are you didn’t just wake up one day and think “I should care about my privacy.” You probably read an article, saw a documentary, had a chat with a friend, or were a victim of some sort of data abuse. Your friends won’t just wake up one day and start using Signal either. You have to guide them to it.</p><p>This doesn't mean monologue for 15 straight minutes of borderline conspiracy at them. &nbsp;Nobody, including &nbsp;yourself, enjoys someone preaching or talking at them. You also don't always need ultimatums either. &nbsp;I know those are somewhat popular in the privacy community (ex: talk to me on Signal or don't talk to me at all) but that can easily backfire. Telling family and friends that you found an awesome new messager that's better than regular texting and you want to try it out with them is much more effective because...</p><h2 id="most-people-will-humor-you">Most People Will Humor You</h2><p>More often than not, your friends and family value you and respect your values even if they don’t share them. Your friends may not humor you if you ask them to delete Facebook, but if you ask them to switch to a user-friendly app like Signal or Wire or ask them to use an encrypted email provider like ProtonMail, they'll probably take you up on the offer. </p><p>Friends and family are, generally, willing to jump into their app store of choice and download an app to try it out. People do it all the time for random games, apps that autotune your voice, or ones that make your head look like an alien. As long as you aren't asking them to set up a server or some complex onboarding process, most people will download an app, jump through a two or three step registration, and test it out for you.</p><p>They may only use these apps with youand that's perfectly fine. It opens the door for you to explain to them why you want to use these apps, how it benefits them, and why they should get their friends and family to use them as well.</p><h2 id="herd-immunity">Herd Immunity</h2><p>The reason why every company harvests so much data is because there's so much value in it thanks to the large amount of people who willingly give it up. &nbsp;Names, addresses, email accounts, usernames, social media accounts, SSN, birthdays, where they're born, pictures of themselves, kids, and friends, browsing history, shopping habits, location data, etc. </p><p>Because there's so much data on so many people, it becomes valuable. Not just monetary value in terms of buying/selling data either. It makes advertising better and more targeted, its <a href="https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did">predictions more accurate</a>, and <a href="https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal">mass manipulation more damaging.</a></p><p>But what happens when people start to use services where Google can't read your messages and email or see your search history? Or ones where Facebook can't see every thought, opinion, or stance you have? Or use cash so every bank and retailer can't see each and every purchase you make?</p><p>What happens is that the data becomes less accurate, which lowers the value. And when the value reaches a specific threshold, it becomes worthless and the company will have to shift its efforts to something that makes them money or they die. It's a win for you because you've made the choice to use privacy respecting services but it also is a win for those who can't or don't want to switch because the company drops the collection due to being a low return on investment.</p><p>Even by yourself, you're ever so slightly lowering the amount of data they have, the accuracy of their data, and the value their data has. &nbsp;But the wonderful thing is, you aren't alone. &nbsp;There are millions of people who have taken steps to regain their privacy and the movement is growing every day. &nbsp;You may not personally know anyone who has taken any privacy focused steps, but the world is slowly moving that way. </p><p>ProtonMail hit <a href="https://www.inverse.com/article/49041-protonmail-ceo-andy-yen-interview">5 million active users a couple years back</a>. Facebook is <a href="https://www.businessinsider.com/facebook-decline-2-million-daily-users-us-canada-q3-earnings-2020-10">down 2 million users in the US and Canada.</a> Signal is <a href="https://time.com/5893114/signal-app-privacy/">growing by leaps and bounds</a>. DuckDuckGo is <a href="https://duckduckgo.com/traffic">consistently seeing increases in traffic every month.</a></p><p>Privacy is slowly building a wall that big tech and others can't get around. &nbsp;You aren't alone in this. &nbsp;Millions of individual people have taken a stance and made a change. &nbsp;You don't have to do everything all at once. &nbsp;Rome wasn't built in a day and neither is your privacy. &nbsp;Take the steps, even if no one else you know currently is, because not only are you making the world better for yourself, you're making better for everyone.</p><hr><p><em>This post was based on the post from <a href="https://write.as/thenewoil/why-you-yes-you-reading-this-need-to-take-the-lead-in-privacy-and-security">TheNewOil</a>.</em></p><p><em>Want to join the discussion? &nbsp;Check out this post, and others, over at the <a href="https://www.reddit.com/r/cupwire">CupWire subreddit</a> and leave a comment.</em></p>
  </div></div>]]>
            </description>
            <link>https://www.cupwire.com/why-you-should-care/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034710</guid>
            <pubDate>Mon, 09 Nov 2020 13:45:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Good Writing Is About Logic, Not Words]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25034706">thread link</a>) | @anacleto
<br/>
November 9, 2020 | https://pulseasync.com/operators/share-written-ideas/ | <a href="https://web.archive.org/web/*/https://pulseasync.com/operators/share-written-ideas/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="article"><p>Today it's almost obvious to state that good written communication creates a business advantage.</p>
<p>I've probably read more articles on written communication in the workplace in the last two months than over the last 10 years. </p>
<p>Most of these essays gave tips on how to become better writers.</p>
<p>I couldn't help but notice that many (if not all) of them questioned the same thing: the prose.</p>
<p>Everyone advised to keep sentences short and to the point, use active voice and not passive&nbsp;voice, use fewer commas and more periods, avoid acronyms, etc.</p>
<p>Let's be clear: none of these are mistaken or wrong by any means. But this made me realize how huge the misconceptions are on what good business writing actually means.</p>
<p>In fact, what separates most people from good writing has very little to do with style, grammar, local sentences structure, word selection, or even content per se.&nbsp;</p>
<p>Most people can't write well because they don't know how to control the logical sequence in which they present their ideas.</p>
<p>And that is the single most important act necessary to clear writing.</p>
<p>In this essay we're going to dig into how you can effectively share written ideas in a way that value time and effort for others.</p>
<hr>
<h2 id="understanding-how-we-think"><a href="#understanding-how-we-think" aria-label="understanding how we think permalink"></a>Understanding How We Think</h2>
<p>Writing an idea is always the result of two macro-steps. First, decide the point that we want want to make, and then put into words.</p>
<p>To understand how we can effectively share written ideas, we need to understand first how we formulate them in the first place.</p>
<p>Deciding the point you want to make is the result of a 5-step process.</p>
<ul>
<li><em>Unbundling</em> a concept</li>
<li><em>Noticing</em> something</li>
<li><em>Articulating</em>/<em>Developing</em> an idea</li>
<li><em>Re-Bundling</em>&nbsp;</li>
<li><em>Reframing</em></li>
</ul>
<p><img src="https://pulseasync.com/assets/generate-idea-thinking-process.png" alt="State diagram to share the 5-step process one use to generate an idea."></p>
<h3 id="unbundling-a-concept"><a href="#unbundling-a-concept" aria-label="unbundling a concept permalink"></a>Unbundling A Concept</h3>
<p>Every idea begins with an unbundling process. Unbundling is an act of exploration that leads to the decoupling of all the individual items of a certain subject.</p>
<p>Unbundling something doesn't imply a deep understanding of it. It's more a perception of full awareness.</p>
<p>In fact, you might not even know how every individual piece works, but you know they all exist in separate forms. No hidden parts.</p>
<h3 id="noticing-something"><a href="#noticing-something" aria-label="noticing something permalink"></a>Noticing Something</h3>
<p>Unbundling enhances our ability to observe, and this can lead us to noticing something. This can be a pattern, an insight, a novelty, or even a minor detail.</p>
<p>If <em>unbundling</em> is the flint, <em>noticing</em> is the spark that really makes the fire.</p>
<h3 id="articulatingdeveloping-an-idea"><a href="#articulatingdeveloping-an-idea" aria-label="articulatingdeveloping an idea permalink"></a>Articulating/Developing An Idea</h3>
<p>Only when we notice something can we start developing an idea. That's where the creative part begins. That's where you try to develop your initial cue into a fully formed idea.&nbsp;</p>
<p>For instance, if you noticed that something was unnecessary or too complicated, you might run a simplification process. If you noticed something was missing, you go through an addition or reinforcement process.</p>
<h3 id="re-bundling-and-reframing"><a href="#re-bundling-and-reframing" aria-label="re bundling and reframing permalink"></a>Re-bundling And Reframing</h3>
<p>Once you finish including your idea, you go through re-bundling. This is a reconstruction process. This is where you try to recompose a world that now contemplates your newly inserted idea.&nbsp;</p>
<p>Bundling is a fundamental part because it's where you can verify if the your new world still holds up. If not, that's a signal you need to put more work in the articulation phase or what you noticed didn't lead to anything meaningful at all.</p>
<p>If at the end of the re-bundling process your world does hold up, you go through a reframing process.</p>
<p>On paper, this seems to be a very logical and clear process, but in reality it's much more complicated as you constantly repeat these steps of of unbundling, editing your idea, and re-bundling until you find a viable path.</p>
<p><img src="https://pulseasync.com/assets/generate-idea-complexity.png" alt="The complex process of idea generation"></p>
<p>Now that you have an idea, you need to decide how to put it into words that you can share with others.</p>
<h2 id="understanding-how-we-write"><a href="#understanding-how-we-write" aria-label="understanding how we write permalink"></a>Understanding How We Write</h2>
<p>From a broader point, the single goal of writing is to get some information into someone's head.</p>
<p>Think about it. It's almost a simulation act.</p>
<p>You need to reproduce your reader's thinking process using your brain. And know how to build up your information in a way that feels logical and makes sense to them.</p>
<p>This process has very little to do with what you went through when you came up with the idea in the first place. In fact, forcing the reader through the exact same original path you took will have the opposite effect and create more confusion.</p>
<p>Here's how it often goes:</p>
<ol>
<li>You have an idea <em>x</em></li>
<li>You write down a set of words <em>m</em> that lead you to <em>x</em></li>
<li>Because of the pre-existing narrative that led you to the idea, you associate <em>m -&gt; x</em></li>
<li>When you proofread it, you mentally get it. Not because <em>m -&gt; x</em> but because of all the pre-existing associations.</li>
</ol>
<p>This is how most people write. And it's exactly why most people's writing sucks.</p>
<p>Not because they use too many passive forms or weak verbs (that doesn't help either), but because they aren't able to write from the reader's perspective. Most writings lacks the basic logical order and structure.</p>
<h2 id="what-is-good-business-writing"><a href="#what-is-good-business-writing" aria-label="what is good business writing permalink"></a>What is Good Business Writing</h2>
<p>Good business writing is a combination of two things:</p>
<ul>
<li>Information Context</li>
<li>Information Resolution</li>
</ul>
<h3 id="how-to-build-context-the-scqa-framework"><a href="#how-to-build-context-the-scqa-framework" aria-label="how to build context the scqa framework permalink"></a>How to build Context: The SCQA Framework</h3>
<p>Context is the "<em>you're here</em>" red arrow that you can see on almost any maps. Good information context helps the reader set the frame to understand what they're about to read next.</p>
<p>Shared context helps the participants make judgments calls using the same pair of lenses.</p>
<p>A lack of a shared understanding on the basic principles can easily result in a partial understanding or conflict on what follows.</p>
<p>What's the best way to build information context?</p>
<p>Barbara Minto, a McKinsey consultant in the 70s, solved this problem with what she called the <em>SCQA framework</em>.</p>
<p>She named this framework the <em>Situation — Complication — Question — Answer</em> framework. You can unpack more on this topic in her book “The Pyramid Principle”.</p>
<p>According to Minto, context is the result of these four sub-ingredients:</p>
<ol>
<li>Situation,</li>
<li>A Complication,</li>
<li>A Question,</li>
<li>... and an Answer.</li>
</ol>
<p>The <strong>Situation</strong> is a non-controversial statement on a subject that you know the reader will agree because it's something he already knows. By summarizing what he already knows, the situation establishes the relevance of the questions that your document is going to answer.&nbsp;</p>
<p>The <strong>Complication</strong> describes an alteration of a stable situation. Keep in mind, this alteration is purely fact-based. The Situation-Complication combination should lead the reader to an immediate question.</p>
<p>The <strong>Question</strong> represents an intuitive response to the complication. The best Situation-Complication scenario makes the question sound totally superfluous to state. The best questions aren't posed, they emerge.</p>
<p>The <strong>Answer</strong> is the summary of your main idea. Beware, it's the solution, not the explanation of it. Good answers are typically represented by 3/4 bullet points. No more.</p>
<p>If you squint at it, you realise that the SCQA framework turned out initial schema upside down.</p>
<p>Ideas comes up in a bottom-up fashion, but they need to be told top-down.</p>
<p>This is not how we think. But it's how we should write.</p>
<p><img src="https://pulseasync.com/assets/scqa-reversed-thinking-frmework.png" alt="SCQA reversed thinking framework"></p>
<p>Another interesting benefit of building information context using the SCQA framework is that once you've gotten the initial part out of the way, you can focus all your energy on making and supporting the case for why it's true.</p>
<h3 id="how-to-increase-resolution-whyhow-dialogues"><a href="#how-to-increase-resolution-whyhow-dialogues" aria-label="how to increase resolution whyhow dialogues permalink"></a>How to increase Resolution: Why/How Dialogues</h3>
<p>Ensuring you and your reader are in the same place before you lead him through your thinking is a necessary but non-sufficient condition. </p>
<p>Once he's aware of the gist of your main idea (Answer), you need to argue and support it. That's when you need to focus on information resolution.</p>
<p>Think of <em>information resolution</em> as the density level of details that you're able to provide to the reader. The bolder the answer, the higher resolution levels it requires.</p>
<p>If you gloss over key important passages, people will not follow your thinking and might have a partial understanding of the message.</p>
<p>How do you increase information resolution?</p>
<p>You support your initial <em>Answer</em> using the form of Why/How dialogues. Making an initial statement that the reader doesn't know will automatically raise a logical question in his mind. <em>How is that possible?</em> <em>Why do you say that?</em> In your following answers, you're likely to tell him other details he doesn't know and this will raise further questions that you're going to answer. And so on.</p>
<p>The author will continue to write, raising and answering questions, until he reaches a point at which he judges the reader will have no more logical questions.</p>
<p>The vertical relationship of a why/how dialogue helps capture the reader's attention. It permits you, as an author, to establish an inner dialog that will pull him with great interest through your reasoning. The reader will be forced to respond logically to your ideas.</p>
<p>As you can see we've not made a full circle. We're now in the (previously discussed) unbundling part.</p>
<h2 id="examples-of-good-business-writing"><a href="#examples-of-good-business-writing" aria-label="examples of good business writing permalink"></a>Examples of Good Business Writing</h2>
<p>Armed with our SCQA framework and the Why/How vertical development, let's look at the skeleton of some real examples.</p>

<p>It's one of the most common examples. common examples. A salesperson, after speaking with a customers, irrupts in the #product or #engineering Slack channels requesting the implementation of a given feature.</p>
<p>If you've been in this position, here's how you can Minto-ize an internal feature request for your product:</p>
<blockquote>
<p><em>Situation:</em>
We have never allowed customers to customize XYZ in order to keep complexity low.</p>
<p><em>Complication:</em>
However, competitor X has now shipped such a customization feature, and we’ve been losing deals because of that.</p>
<p><em>Questions:</em>
We now need to decide whether we want to allow some kind of customization as well.</p>
<p><em>Answer:</em>
[...]</p>
</blockquote>

<p>Directives are the most common internal memo. Executives write them to request something of someone.</p>
<blockquote>
<p><em>Situation:</em>&nbsp;
As you know we're repositioning product <em>x</em> for mid-sized enterprise. We need to teach you how to see <em>x</em> for organizations between 100 and 200 employees.</p>
<p><em>Complication:</em>
We've never sold to this type of customer before. Hence, we need to construct a new customer profile from scratch</p>
<p><em>Questions:</em> (How can we do the profile?)</p>
<p><em>Answer:</em>
We're going to host:</p>
<ol>
<li>A …</li></ol></blockquote></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pulseasync.com/operators/share-written-ideas/">https://pulseasync.com/operators/share-written-ideas/</a></em></p>]]>
            </description>
            <link>https://pulseasync.com/operators/share-written-ideas/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034706</guid>
            <pubDate>Mon, 09 Nov 2020 13:44:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Practical Introduction to Quantum Computing: Qubits to Quantum ML and Beyond]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25034692">thread link</a>) | @blopeur
<br/>
November 9, 2020 | https://indico.cern.ch/event/970903/ | <a href="https://web.archive.org/web/*/https://indico.cern.ch/event/970903/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div itemprop="description">
        <p><strong>*** The webcast is now over - The recording link is posted at the bottom of this page ***</strong></p>

<p><strong>General description of the course</strong></p>

<p><span><span>Quantum computing is one the most promising new trends in information processing. In this course, we will introduce from scratch the basic concepts of the quantum circuit model (qubits, gates and measures) and use them to study some of the most important quantum algorithms and protocols, including those that can be implemented with a few qubits (BB84, quantum teleportation, superdense coding...) as well as those that require multi-qubit systems (Deutsch-Jozsa, Grover, Shor..). We will also cover some of the most recent applications of quantum computing in the fields of optimization and simulation (with special emphasis on the use of quantum annealing, the quantum approximate optimization algorithm and the variational quantum eigensolver) and quantum machine learning (for instance, through the use of quantum support vector machines and quantum variational classifiers). We will also give examples of how these techniques can be used in chemistry simulations and high energy physics problems.</span></span></p>

<p><span><span>The focus of the course will be on the practical aspects of quantum computing and on the implementation of algorithms in quantum simulators and actual quantum computers (as the ones available on the IBM Quantum Experience and D-Wave Leap). No previous knowledge of quantum physics is required and, from the mathematical point of view, only a good command of basic linear algebra is assumed. Some familiarity with the python programming language would be helpful, but is not required either.&nbsp;</span></span></p>

<p>====</p>

<p><strong>Lecture 1: Introduction</strong></p>

<p><strong><span><span>What is quantum computing? Applications of quantum computing. Hardware and software for quantum computing. Elements of the quantum circuit model. Introduction to the IBM Quantum Experience</span></span></strong></p>

<p>===</p>

<p><strong>Biography of the speaker</strong></p>

<p><span><span>Elías F. Combarro holds degrees&nbsp;from the University of Oviedo (Spain) in both Mathematics (1997, award for second highest grades in the country) and Computer Science (2002, award for highest grades in the country). After some research stays at the Novosibirsk State University (Russia), he obtained a Ph.D. in Mathematics (Oviedo, 2001) with a dissertation on the properties of some computable predicates under the supervision of Prof. Andrey Morozov. Since 2009, Elías F. Combarro has been an associate professor at the Computer Science Department of the University of Oviedo. He has published more than 50 research papers in international&nbsp;journals on topics such as Computability Theory, Machine Learning, Fuzzy Measures and Computational Algebra. His current research focuses on the application Quantum Computing to algebraic, optimization and machine learning problems. From July 2020 he has been a Cooperation Associate at CERN openlab.</span></span></p>


    </div>
</div></div>]]>
            </description>
            <link>https://indico.cern.ch/event/970903/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034692</guid>
            <pubDate>Mon, 09 Nov 2020 13:43:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Schedule your appointment for your clients using these tools]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25034681">thread link</a>) | @startupcheckr
<br/>
November 9, 2020 | https://www.startupcheckr.com/appointmentscheduling | <a href="https://web.archive.org/web/*/https://www.startupcheckr.com/appointmentscheduling">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.startupcheckr.com/appointmentscheduling</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034681</guid>
            <pubDate>Mon, 09 Nov 2020 13:41:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Using “Virtio-Fs” on a Unikernel]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25034668">thread link</a>) | @ingve
<br/>
November 9, 2020 | https://www.qemu.org/2020/11/03/osv-virtio-fs/ | <a href="https://web.archive.org/web/*/https://www.qemu.org/2020/11/03/osv-virtio-fs/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main">
		<div>
			<!-- Main -->
	<section>
		<header>
			
			<p>03 Nov 2020 — by Fotis Xenakis</p>
		</header>
		<p>This article provides an overview of <a href="https://virtio-fs.gitlab.io/">virtio-fs</a>,
a novel way for sharing the host file system with guests and
<a href="https://github.com/cloudius-systems/osv">OSv</a>, a specialized, lightweight
operating system (unikernel) for the cloud, as well as how these two fit
together.</p>

<h2 id="virtio-fs">virtio-fs</h2>

<p>Virtio-fs is a new host-guest shared filesystem, purpose-built for local file
system semantics and performance. To that end, it takes full advantage of the
host’s and the guest’s colocation on the same physical machine, unlike
network-based efforts, like virtio-9p.</p>

<p>As the name suggests, virtio-fs builds on virtio for providing an efficient
transport: it is included in the (currently draft, to become v1.2) virtio
<a href="https://github.com/oasis-tcs/virtio-spec">specification</a> as a new device. The
protocol used by the device is a slightly extended version of
<a href="https://github.com/libfuse/libfuse">FUSE</a>, providing a solid foundation for
all file system operations native on Linux. Implementation-wise, on the QEMU
side, it takes the approach of splitting between the guest interface (handled
by QEMU) and the host file system interface (the device “backend”). The latter
is handled by virtiofsd (“virtio-fs daemon”), running as a separate process,
utilizing the
<a href="https://www.qemu.org/docs/master/interop/vhost-user.html">vhost-user</a> protocol
to communicate with QEMU.</p>

<p>One prominent performance feature of virtio-fs is the DAX (Direct Access)
window. It’s a shared memory window between the host and the guest, exposed as
device memory (a PCI BAR) to the second. Upon request, the host (QEMU) maps file contents to the window for the guest to access directly. This bears performance
gains due to taking VMEXITs out of the read/write data path and bypassing the
guest page cache on Linux, while not counting against the VM’s memory (since
it’s just device memory, managed on the host).</p>

<p><img src="https://gitlab.com/virtio-fs/virtio-fs.gitlab.io/-/raw/master/architecture.svg" alt="virtio-fs DAX architecture"></p>

<p>Virtio-fs is under active development, with its community focussing on a pair of
device implementation in QEMU and device driver in Linux. Both components are
already available upstream in their initial iterations, while upstreaming
continues further e.g. with DAX window support.</p>

<h2 id="osv">OSv</h2>

<p>OSv is a <a href="https://en.wikipedia.org/wiki/Unikernel">unikernel</a> (framework). The
two defining characteristics of a unikernel are:</p>

<ul>
  <li><strong>Application-specialized</strong>: a unikernel is an executable machine image,
consisting of an application and supporting code (drivers, memory management,
runtime etc.) linked together, running in a single address space (typically
in guest “kernel mode”).</li>
  <li><strong>Library OS</strong>: each unikernel only contains the functionality mandated by its
application in terms of non-application code, i.e. no unused drivers, or even
whole subsystems (e.g. networking, if the application doesn’t use the
network).</li>
</ul>

<p>OSv in particular strives for binary compatibility with Linux, using a <a href="https://github.com/cloudius-systems/osv/wiki/Dynamic-Linker">dynamic
linker</a>. This means
that applications built for Linux should run as OSv unikernels without requiring
modifications or even rebuilding, at least most of the time. Of course, not the
whole Linux ABI is supported, with system calls like <code>fork()</code> and relatives
missing by design in all unikernels, which lack the notion of a process. Despite
this limitation, OSv is quite full featured, with full SMP support, virtual
memory, a virtual file system (and many filesystem implementations, including
ZFS) as well as a mature networking stack, based on the FreeBSD sources.</p>

<p>At this point, one is sure to wonder “Why bother with unikernels?”. The problem
they were originally
<a href="http://unikernel.org/files/2013-asplos-mirage.pdf">introduced</a> to solve is the
bloated software stack in modern cloud computing. Running general-purpose
operating systems as guests, typically for a single application/service, on top
of a hypervisor which already takes care of isolation and provides a standard
device model means duplication, as well as loss of efficiency. This is were
unikernels come in, trying to be just enough to support a single application
and as light-weight as possible, based on the assumption that they are executing
inside a VM. Below is an illustration of the comparison between
general-purpose OS, unikernels and containers (as another approach to the same
problem, for completeness).</p>

<p><img src="https://www.qemu.org/screenshots/2020-11-04-unikernel-vs-gpos.svg" alt="Unikernels vs GPOS vs containers"></p>

<h2 id="osv-meet-virtio-fs">OSv, meet virtio-fs</h2>

<p>As is apparent e.g. from the container world, it is very common for applications
running in isolated environments (such as containers, or unikernels even more
so) to require host file system access. Whereas containers sharing the host
kernel thus have an obvious, controlled path to the host file system, with
unikernels this has been more complex: all solutions were somewhat heavyweight,
requiring a network link or indirection through network protocols. Virtio-fs
then provided a significantly more attractive route: straight-forward mapping of
fs operations (via FUSE), reusing the existing virtio transport and decent
performance without high memory overhead.</p>

<p>The OSv community quickly identified the opportunity and came up with a
read-only implementation on its side, when executing under QEMU. This emphasized
being lightweight complexity-wise, while catering to many of its applications’
requirements (they are stateless, think e.g. serverless). Notably, it includes
support for the DAX window (even before that’s merged in upstream QEMU),
providing <a href="https://github.com/foxeng/diploma">excellent performance</a>, directly
rivalling that of its local (non-shared) counterparts such as ZFS and ROFS (an
OSv-specific read-only file system).</p>

<p>One central point is OSv’s support for booting from virtio-fs: this enables
deploying a modified version or a whole new application <strong>without rebuilding</strong>
the image, just by adjusting its root file system contents on the host. Last,
owing to the DAX window practically providing low-overhead access to the host’s
page cache, scalability is also expected to excel, with it being a common
concern due to the potentially high density of unikernels per host.</p>

<p>For example, to build the <code>cli</code> OSv image, bootable from virtio-fs, using the
core OSv <a href="https://github.com/cloudius-systems/osv#building-osv-kernel-and-creating-images">build
system</a>:</p>
<div><div><pre><code>scripts/build fs=virtiofs export=all image=cli
</code></pre></div></div>
<p>This results in a minimal image (just the initramfs), while the root fs contents
are placed in a directory on the host (<code>build/export</code> here, by default).</p>

<p><a href="https://github.com/cloudius-systems/osv#running-osv">Running</a> the above image
is just a step away (may want to use the virtio-fs development version of
<a href="https://gitlab.com/virtio-fs/qemu/-/tree/virtio-fs-dev">QEMU</a>, e.g. for DAX
window support):</p>
<div><div><pre><code>scripts/run.py --virtio-fs-tag=myfs --virtio-fs-dir=$(pwd)/build/export
</code></pre></div></div>
<p>This orchestrates running both virtiofsd and QEMU, using the contents of
<code>build/export</code> as the root file system. Any changes to this directory, directly
from the host will be visible in the guest without re-running the previous build
step.</p>

<h2 id="conclusion">Conclusion</h2>

<p>OSv has gained a prominent new feature, powered by virtio-fs and its QEMU
implementation. This allows efficient, lightweight and performant access to the
host’s file system, thanks to the native virtio transport, usage of the FUSE
protocol and the DAX window architecture. In turn, it enables use cases like
rapid unikernel reconfiguration.</p>

		<ul>
		
		<li><span></span><a href="https://www.qemu.org/blog/category/storage/index.html">storage</a></li>
		
		<li><span></span><a href="https://www.qemu.org/blog/category/virtio-fs/index.html">virtio-fs</a></li>
		
		<li><span></span><a href="https://www.qemu.org/blog/category/unikernel/index.html">unikernel</a></li>
		
		<li><span></span><a href="https://www.qemu.org/blog/category/osv/index.html">OSv</a></li>
		
		</ul>
	</section>

		</div>
	</div></div>]]>
            </description>
            <link>https://www.qemu.org/2020/11/03/osv-virtio-fs/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034668</guid>
            <pubDate>Mon, 09 Nov 2020 13:40:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Which FinTech Investment Trends Are Popular and Why?]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25034472">thread link</a>) | @cpepper
<br/>
November 9, 2020 | https://codeandpepper.com/fintech-investment-trends/ | <a href="https://web.archive.org/web/*/https://codeandpepper.com/fintech-investment-trends/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
              
<p>In business, good enough is often not enough. Many people are turning away from incumbent financial institutions because traditional banking can’t provide the level of services they are actively seeking. Meanwhile, FinTech startups introduce innovative service offerings almost every month. Investors keep their eyes open, they don’t want to miss a potential unicorn. Here’s a list of important FinTech investment trends to watch – maybe they’ll bring another industry star.</p>



<figure><ul><li><figure><amp-img width="864" height="450" src="https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why.jpg" alt="" data-id="14569" data-full-url="https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why.jpg" data-link="https://codeandpepper.com/?attachment_id=14569" srcset="https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why.jpg 864w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why-300x156.jpg 300w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why-768x400.jpg 768w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why-361x188.jpg 361w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why-192x100.jpg 192w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why-720x375.jpg 720w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why-432x225.jpg 432w" sizes="(max-width: 864px) 100vw, 864px" layout="intrinsic" disable-inline-width="" i-amphtml-layout="intrinsic"><img loading="lazy" width="864" height="450" src="https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why.jpg" alt="" srcset="https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why.jpg 864w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why-300x156.jpg 300w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why-768x400.jpg 768w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why-361x188.jpg 361w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why-192x100.jpg 192w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why-720x375.jpg 720w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why-432x225.jpg 432w" sizes="(max-width: 864px) 100vw, 864px" data-old-src="data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9JzQ1MCcgd2lkdGg9Jzg2NCcgeG1sbnM9J2h0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnJyB2ZXJzaW9uPScxLjEnLz4="></amp-img></figure></li></ul></figure>







<p>First, let’s take a look at the foundations of the industry. Phenomenons like <a href="https://codeandpepper.com/services/open-banking">open banking</a> or <a href="https://codeandpepper.com/services/insurance-insurtech-software-development">InsurTech</a> took both the financial sector and social media by storm. These new digital services are what people want and need in their daily activities. Being relevant is exactly what drives the growth of FinTech companies. Trends like Big Data improve customer relationships and give people personalized experience, expanding brands’ market reach.</p>



<p>The stakes are high. <a href="https://www.marketdataforecast.com/market-reports/fintech-market" target="_blank" rel="noreferrer noopener nofollow">According to Market Data Forecast</a>, the global financial technology market will reach the value of $305 billion by 2025. That’s a big pie and everyone wants their share. Tech solutions have the potential to improve the customer experience. They are also well-positioned for the application’s future growth. Companies like <a href="https://www.fintechmagazine.com/digital-payments/klarna-how-can-we-enrich-customer-experience">Klarna</a>, a FinTech payments giant, look for customer relationships to enhance the business. Especially now, when people stay at home and buy from home. Even more than before the pandemic. The market values such an approach – Klarna is currently worth over $10.5 billion.</p>



<p>How does it translate into FinTech sectors? The pandemic <a href="https://sifted.eu/articles/fintech-funding-data-corona" target="_blank" rel="noreferrer noopener nofollow">collapse of early 2020</a> will not slow down macro trends. <a href="https://pitchbook.com/news/reports/q2-2020-emerging-tech-research-fintech?utm_medium=nl-na&amp;utm_source=reports&amp;utm_campaign=q2-2020-emerging-tech-research-fintech" target="_blank" rel="noreferrer noopener nofollow">According to PitchBook</a>, European investors are confident that long-term tendencies will “broadly favor” the sector. Which trends and companies are getting traction and why?</p>



<h2 id="h-top-fintech-investment-trends">Top FinTech investment trends</h2>



<p>Traditional banking has much to think about. The trends below clearly show there’s no turning back from innovation. Consumers think that security and convenience brought by new financial products have changed their lives for the better. The mobile is the wallet, the social impact of satisfied customers is big. This is how it looks in practice.</p>



<h3 id="h-insurtech-is-on-the-rise">InsurTech is on the rise</h3>



<p>Four trends are shaping the <a href="https://codeandpepper.com/insurtech-industry-guide/">InsurTech</a> industry:</p>



<ul><li>human agents are replaced with artificial intelligence (AI) and machine learning</li><li>Big Data intelligence and Internet of Things (IoT) increase customer retention</li><li>increased use of blockchain help in better risk assessment</li><li>digital ecosystems drive revenue</li></ul>



<p>Companies in this sector vary dramatically. Investors pour their money in companies like <a href="https://www.threatinformer.com/" target="_blank" rel="noreferrer noopener nofollow">ThreatInformer</a>, a next level InsurTech providing risk intelligence to the industry. Such data-driven solutions are especially important in a world ruled by access to information. The startup combines threat data, security assessments, and environmental factors to help clients estimate insurance risks. On the other hand, we have more “traditional” InsurTech startups, like <a href="https://www.homelyfe.com/" target="_blank" rel="noreferrer noopener nofollow">Homelyfe</a>. The app lets users manage their policies in one place.</p>



<h3 id="h-online-payment-processing-still-challenging-traditional-banking">Online payment processing still challenging traditional banking</h3>



<p>There are many online payment processing platforms, yet investors still believe in them. They don’t seem to be afraid of market saturation. Many apps have unique features, targeting nations or even local problems. To the point, where traditional financial institutions can’t efficiently compete.</p>



<p>Companies like <a rel="noreferrer noopener nofollow" href="https://stripe.com/" target="_blank">Stripe</a>, <a rel="noreferrer noopener nofollow" href="https://www.adyen.com/" target="_blank">Adyen</a>, or the previously mentioned Klarna, help people manage their payments on-the-go. Investing in these companies means tapping into people’s spending potential. It also gives managers insights into customer behaviours, invaluable in doing business.</p>



<div><figure><amp-img width="864" height="450" src="https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2.jpg" alt="" srcset="https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2.jpg 864w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2-300x156.jpg 300w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2-768x400.jpg 768w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2-361x188.jpg 361w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2-192x100.jpg 192w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2-720x375.jpg 720w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2-432x225.jpg 432w" sizes="(max-width: 864px) 100vw, 864px" layout="intrinsic" disable-inline-width="" i-amphtml-layout="intrinsic"><img loading="lazy" width="864" height="450" src="https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2.jpg" alt="" srcset="https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2.jpg 864w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2-300x156.jpg 300w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2-768x400.jpg 768w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2-361x188.jpg 361w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2-192x100.jpg 192w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2-720x375.jpg 720w, https://codeandpepper.com/wp-content/uploads/2020/10/Which-FinTech-Investment-Trends-are-Popular-and-Why_2-432x225.jpg 432w" sizes="(max-width: 864px) 100vw, 864px" data-old-src="data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9JzQ1MCcgd2lkdGg9Jzg2NCcgeG1sbnM9J2h0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnJyB2ZXJzaW9uPScxLjEnLz4="></amp-img></figure></div>



<h3 id="h-peer-to-peer-lending-solutions-as-a-future-proof-fintech-investment">Peer-to-peer lending solutions as a future-proof FinTech investment</h3>



<p>One of the most powerful tools used by traditional banks is lending. However, their monopoly in that domain has been broken. FinTech loans are among the most interesting financial products in the eyes of investors. Not everyone is eligible for a bank loan but there are plenty of people with even small capital who will gladly help in time of need.</p>



<p>Connecting individuals or entrepreneurs with potential lenders is a social, financial, and technological phenomenon. Brands like <a href="https://www.lu.com/" target="_blank" rel="noreferrer noopener nofollow">Lufax</a>, <a href="https://www.prosper.com/" target="_blank" rel="noreferrer noopener nofollow">Prosper</a> or <a href="https://www.commonbond.co/" target="_blank" rel="noreferrer noopener nofollow">CommonBond</a> grow by making their offers tailored to specific groups. The latter thrives because it has an offer addressed exclusively to students – a demographic notorious for having money problems. Peer-to-peer payment solutions engage users and support communities, making them a perfect remedy for the times of uncertainty.</p>



<h3 id="h-fintech-investments-come-to-the-unbanked">FinTech investments come to the unbanked</h3>



<p>According to <a href="https://globalfindex.worldbank.org/" target="_blank" rel="noreferrer noopener nofollow">The World Bank report</a>, 1.7 billion people across the world don’t have access to any formal financial system. 60% of them simply don’t have enough money, 30% never felt like they needed a bank, 26% think of banking accounts as too expensive.</p>



<p>At the same time, there is a growing number of startups serving those who are considered too poor by traditional banking. <a href="https://www.planet-fintech.com/file/163178/" target="_blank" rel="noreferrer noopener nofollow">FinTech investments</a> are now made not only in the USA or London, the European FinTech capital. Latin America, Africa, Southeast Asia are full of companies that bring money to those who have very little. Companies like <a href="https://www.lenddo.com/" target="_blank" rel="noreferrer noopener nofollow">Lenddo</a> or <a href="https://www.ayannah.com/" target="_blank" rel="noreferrer noopener nofollow">Ayannah</a> efficiently activate people in emerging markets.</p>



<h3 id="h-bigger-investments-in-cybersecurity-solutions">Bigger investments in cybersecurity solutions</h3>



<p>Traditional financial institutions are shifting from making their own FinTech products into buying the existing ones. In fact, <a href="https://www.businessinsider.com/1-in-5-european-banks-would-buy-fintech-startups-2016-6?IR=T" target="_blank" rel="noreferrer noopener nofollow">one in five European banks would buy a FinTech startup</a>. Security, for some companies in the middle of the to-do list, is a must for big banks. That’s why the banking sector will look for solutions with an established security track record. Or at least those which can be easily turned into safe application.</p>



<h3 id="h-active-search-for-apps-with-potential-for-ecosystem-building">Active search for apps with potential for ecosystem building</h3>



<p>To effectively compete and meet customers’ expectations, many business owners decide to cooperate with various industries. Creating an <a href="https://codeandpepper.com/innovations-improving-fintech-customer-experience/">ecosystem of mutually beneficial applications</a>, drives added value for users. It also attracts investors. The more companies can offer, the more users they can attract and the more value they have in the eyes of people who want to invest.</p>



<p>The future of FinTech investment lies in applications that are platforms, not just solutions. Think of WeChat, the Chinese behemoth. It launched in 2011, allowing to send text and voice messages to family and friends. Local email market penetration was low, forcing the app owners to innovate. By 2015, <a href="https://www.wsj.com/articles/BL-CJB-28569" target="_blank" rel="noreferrer noopener nofollow">90% of Chinese Internet users were browsing the net on mobile</a>. Today, WeChat serves as a platform to pay for utilities, use city services, order movie tickets, and much more. You can pay for products and services, even support a favorite charity.</p>



<p>WeChat is for China what Facebook Messenger, Instagram, and Apple Pay are for the West. And much more. According to <a href="https://go.appannie.com/rs/071-QED-284/images/2001_State_of_Mobile_2020_Main_EN.pdf" target="_blank" rel="noreferrer noopener nofollow">an AppAnnie report</a>, the average number of installed mobile apps in 2020 is 40. At the same time, <a href="https://www.forbes.com/sites/blakemorgan/2019/05/02/when-it-comes-to-customer-engagement-loyalty-matters-at-citi/#6ca973fe7fb6" target="_blank" rel="noreferrer noopener nofollow">Citi reported</a> that 83% of consumers and 94% of millennials are more likely to participate in a loyalty program if it’s accessible on mobile.</p>



<p>Conclusions? The future digital economy will most likely favour cooperation between the FinTech industry and 3rd party apps. A feature-rich ecosystem is valuable for users and it brings revenue.</p>



<h2 id="h-fintech-investment-trends-are-clear">FinTech investment trends are clear</h2>



<p>If good enough is not enough, do more! FinTech investment trends are coming into focus, especially in the light of the COVID-19 pandemic. There are detailed guides on how to invest, written for example by <a href="https://www.investors.com/news/technology/fintech-companies-to-buy-and-watch" target="_blank" rel="noreferrer noopener nofollow">Investor’s Business Daily</a>. None of them, however, can prepare you for what really needs to be done.</p>



<p>Users’ trust is built on reliability. Outsourcing FinTech software development can save time and money, helping you build an app fit for the future. Financial products are valued for design, performance, and integrations. Need a boost for growth potential? <a href="https://codeandpepper.com/services/api-development/">Create a platform, not a solution</a>.</p>
          </div></div>]]>
            </description>
            <link>https://codeandpepper.com/fintech-investment-trends/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034472</guid>
            <pubDate>Mon, 09 Nov 2020 13:20:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Benefits of an integrated eCommerce marketing platform]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25034333">thread link</a>) | @xxlcloudinc
<br/>
November 9, 2020 | https://codecoda.com/en/blog/entry/6-benefits-of-an-integrated-ecommerce-marketing-platform | <a href="https://web.archive.org/web/*/https://codecoda.com/en/blog/entry/6-benefits-of-an-integrated-ecommerce-marketing-platform">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="description">
<p>Few technological advancements and innovations have affected the world of commerce as deeply as <strong>the advent of eCommerce</strong>. One cannot overstress the profound ease of doing business online, especially in an increasingly mobile-friendly digital era. In turn, this development has motivated brands and businesses to adapt their marketing strategies accordingly. Inevitably, many seem to have come to <em>the data-driven conclusion that an integrated eCommerce marketing platform is both necessary and lucrative.</em> </p>
<p>The most fundamental benefit of such a platform should be immediately obvious in this context. An <em>ever-growing, vast, tech-savvy, often international audience</em> should allure any brand or business. However, tapping into such a sizeable audience entails a notable practical burden, unprecedented for many. Fortunately, such platforms tackle this burden by <strong><a href="https://codecoda.com/en/blog/entry/ecommerce-guide-from-idea-to-a-working-online-solution">providing tailor-made solutions</a></strong> to each of the challenges at hand. However, to analyze such solutions in-depth, one would need to carefully examine <strong>the emerging challenges alongside the benefits such platforms offer</strong>.</p>
<h2>The benefits of an integrated eCommerce marketing platform</h2>
<p>Arguably, the most fundamental challenge that comes with such a volume of transactions is strictly practical; <strong>handling and processing transactions themselves</strong>. Concurrently, the need to handle back-end jobs remains, while their scope and intensity only increase. Furthermore, <em>such a volume of data can hamper marketing efforts due to fragmented data, and thus inhibit sales and growth.</em> To consolidate such challenges, one could identify them as the following core issues; </p>
<ul>
<li>Miscommunication between incoming orders and internal accounting systems </li>
<li>Siloed data which can hamper team/department cohesion   </li>
<li>Mismanagement of potential leads and engagement opportunities </li>
<li>Time mismanagement </li>
<li>Worse customer experience and service </li>
</ul>
<p>By definition, all of the aforementioned factors can have a visible adverse effect on any brand or business. <strong>Increased strain</strong>, especially for the less prepared, can range from mildly disorganizing to borderline catastrophic. Therefore, an integrated <strong><a href="https://codecoda.com/en/blog/entry/best-ecommerce-platforms-2020">eCommerce marketing platform</a></strong> can be an invaluable asset towards tackling these challenges and ensuring growth, simply because such platforms strive to tackle those same challenges. Most significantly, <em>they aim to let brands and businesses streamline their operations</em>, optimizing their time, and ultimately delivering a concise, enticing customer experience.</p>
<div>
<blockquote>
<p>If you do build a great experience, customers tell each other about that. Word of mouth is very powerful.</p>
</blockquote>

</div>
<p>The exact benefits of an integrated eCommerce marketing platform can be numerous and worth exploring individually and in-depth. In the interest of time and consolidated information, however, one should safely distill them down to <strong>6 key benefits</strong>.</p>
<div><div data-appear-animation="fadeInUpShorter" data-appear-animation-delay="200"><div><h4>Eliminating human errors and duplicate data entries</h4><div><p>The very first challenge of sustaining a vast ERP system for eCommerce ventures lies, expectedly, in <strong>human error</strong>. Processing large quantities of orders manually lends itself to oversights and mistakes such as the following;</p></div><ul>
<li>Duplicate data entries</li>
<li>Transposed numbers</li>
<li>Wrong or misspelled data entries</li>
<li>Lost orders</li>
</ul><p>While such mistakes may not incur immediately visible damage, their effect and quantity <em>only scale with size</em>. As one’s backlog expands, the room for such errors naturally increases as employees struggle to manage the added strain. Furthermore, potential mismanagement of data and orders can deteriorate brand loyalty and jeopardize growth in the context of eCommerce. Online avenues and retail sites offer public, visible customer reviews as a testament of quality – which can conversely amplify even the slightest imperfection.<br>As such, an integrated eCommerce marketing platform can overcome this challenge by <strong>automating data entry</strong>. It can process any volume of orders with the same efficiency and swiftness while providing a <em>nearly error-free process</em>. While it may not be absolutely infallible, as no software solutions truly are, it can certainly offer more confidence than manual data entry.</p>
</div></div><div data-appear-animation="fadeInUpShorter" data-appear-animation-delay="200"><div><h4>Automated order cycles and timely data updates</h4><div><p>By the same token, manual order cycles and data updates also offer room for error. Moreover, they become increasingly inefficient in terms of time investment on a scale. On both accounts, manually updating data can be <strong>inefficient, error-prone, and needlessly time-consuming</strong>. An integrated eCommerce marketing platform can address both through similar yet distinct functionalities.</p><p><b>An integrated eCommerce marketing platform can automate order cycles</b><br>Traditionally, sizeable volumes of orders may require manual management, <strong>exporting and importing them into back-end accounting systems</strong>. The massive potential volume of eCommerce orders can encumber employees regarding this task, slowing down order processing speed and shipping. In response, such platforms can automate this process and enable seamless data entry.</p><p><b>An integrated eCommerce marketing platform can update customer and inventory data in a timely manner</b><br>Similarly, manual data updates can present a challenge and entail errors. One needs to <em>update inventory after each incoming order, and customer data changes fairly regularly</em>. Needless to say, errors on either front can damage operations and one’s image. Furthermore, both of those processes can demand unreasonable effort and precious time that one could invest elsewhere. As such, these solutions address this challenge by automating data updates; <strong>bi-directional integration</strong> can keep inventory and customer data synchronized and up-to-date.</p></div>
</div></div><div data-appear-animation="fadeInUpShorter" data-appear-animation-delay="200"><div><h4>Data integration and customer segmentation</h4><div><p>Moreover, still on the subject of data management and analysis, <strong><a href="https://codecoda.com/en/blog/entry/things-to-consider-when-choosing-an-ecommerce-solution">data integration</a></strong> is an irreplaceable asset. <strong>Siloed data</strong> inaccessible by specific employees or departments can hamper internal cooperation and thus jeopardize the final customer experience. Additionally, fragmented data can prevent <strong>proper lead analysis</strong>, which can, in turn, lead to multiple shortcomings;</p></div><ul>
<li>Poor lead acquisition and profitability analysis</li>
<li>Disjointed marketing efforts</li>
<li>Lower ROI across marketing and sales endeavors</li>
</ul><p>An integrated platform can address such challenges by providing <strong>centralized, cross-channel data</strong> accessible by multiple devices. Additionally, it offers powerful insights into <strong>customer behavior, interaction history, and specific touchpoints</strong>. Thus, brands and businesses can use such data to segment customers according to their demographics and other characteristics, behavior, and profitability. In turn, they can provide highly personalized marketing campaigns and purchase funnels, <em>which yield significantly better results.</em></p>
</div></div><div data-appear-animation="fadeInUpShorter" data-appear-animation-delay="200"><div><h4>Improved customer experience and customer service</h4><div><p>With the aforementioned benefits of consolidated data and optimized internal cooperation, another benefit of an integrated eCommerce marketing platform lies in <strong>customer satisfaction</strong>. In the digital age, customers expect a seamless experience and impeccable customer support. A consolidated, detailed database can facilitate both to a significant degree. </p><p>In terms of customer service, <strong>swift access to customer interaction history</strong> and past inquiries can expedite the process. Common and frequent inquiries can be identified more easily, and the appropriate agents for each incoming communication can be notified more effectively. By the same token, such a trove of valuable, actionable data can significantly <strong>improve customer experience</strong>. Consider the following data-driven initiatives;</p></div><ul>
<li>Optimal lead acquisition and conversion through well-timed communication </li>
<li>An increased volume of user-generated content, such as public comments and reviews, through post-purchase requests </li>
<li>More engagement and loyalty incentives, such as tailored content, loyalty points programs, and referrals </li>
</ul><p>An integrated eCommerce marketing platform can facilitate the above, and more, to <strong>holistically improve operations</strong>. In turn, such efforts can unclog the back-end of any brand or business, and by extension, improve the customers’ final experience.</p>
</div></div>
<div data-appear-animation="fadeInUpShorter" data-appear-animation-delay="200"><div><h4>Reduced operational costs</h4><p>Furthermore, <strong>automated accounting processes</strong> can help reduce operational costs. Eliminating human error can be beneficial by itself, as outlined above, <em>but it also has the byproduct of reducing labor costs</em>. As such, fewer employees and administrators need to take up the burden of manual data entry and management.<br>Simultaneously, outside of accounting automation, <strong>data-driven choices can reduce marketing costs</strong> as well. With more data in hand to inform focused marketing decisions, one can reduce the costs of misplaced or mismanaged marketing efforts.</p>
</div></div>
<div data-appear-animation="fadeInUpShorter" data-appear-animation-delay="200"><div><h4>Time efficiency</h4><div><p>Lastly, the data, automation, and overall operational efficiency that an integrated eCommerce marketing platform can offer can only conclude with <strong>time efficiency</strong>. Time is a vital, valuable resource that can dictate growth rates and even sustainability. From reducing the administrative burden of accounting to streamlining data entry and collection, automation can indeed streamline one’s time investments.</p><p>In turn, time saved from redundant, automatable tasks can be invested in more productive activities, such as <strong>producing high-converting marketing content</strong>. Lead analysis can highlight the most valuable marketing platforms, content types, and lead generation strategies for one’s intended audience – <em>equally time-intensive but arguably far more productive tasks to focus on.</em></p></div>
</div></div>

</div>
<h2>Beyond an integrated eCommerce marketing platform: business software solutions</h2>
<p>Finally, it is equally noteworthy that <strong>further business software options</strong> also exist. As long as one can guarantee integration, such options are not mutually exclusive; rather, they can function as complementary assets. <strong>Customer relationship management (CRM)</strong> software is likely among the most prominent holistic solutions, but various business software solutions can handle different vital tasks; </p>
<ul>
<li>Word and spreadsheet processing software </li>
<li>Asset management software </li>
<li>Accounting, billing, and payroll software </li>
<li>Internal communication software </li>
</ul>
<p>All …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://codecoda.com/en/blog/entry/6-benefits-of-an-integrated-ecommerce-marketing-platform">https://codecoda.com/en/blog/entry/6-benefits-of-an-integrated-ecommerce-marketing-platform</a></em></p>]]>
            </description>
            <link>https://codecoda.com/en/blog/entry/6-benefits-of-an-integrated-ecommerce-marketing-platform</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034333</guid>
            <pubDate>Mon, 09 Nov 2020 13:04:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Engineering Manager Event Loop (2018)]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25034254">thread link</a>) | @mooreds
<br/>
November 9, 2020 | https://www.chriseigner.com/engineering-manager-event-loop/ | <a href="https://web.archive.org/web/*/https://www.chriseigner.com/engineering-manager-event-loop/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
	<article>
		<div>

			<section>
				<div><p>In prepping for a new Engineering Manager role I'll be starting in the next several weeks, I stumbled on a great video featuring <a href="https://twitter.com/dloft">David Loftesness</a> (former Director of Eng at Twitter, now at eero) talking about the transition from Engineer to Manager.</p>
<p>I liked this matrix enough that I turned it into a downloadable PDF for other folks.</p>
<p><img src="https://s3-us-west-2.amazonaws.com/chris-eigner-site/2018/10/Screen-Shot-2018-10-24-at-5.37.09-PM.png" alt="Screen-Shot-2018-10-24-at-5.37.09-PM"></p>
<p><a href="https://www.dropbox.com/s/b35bm43aecdb9cu/Engineering%20Manager%20Event%20Loop.pdf?dl=0">Click the link to download</a></p>
<p>Source: <a href="https://www.youtube.com/watch?v=qaHEy1I2M5Q">https://www.youtube.com/watch?v=qaHEy1I2M5Q</a></p>
</div>
			</section>

			<section>

				

				

				

						

			</section>


			<section>
				<a id="show-disqus">Show Comments</a>
			    
			</section>

            <section>
                <form method="post" action="/subscribe/">
    

    
    
    


</form>


                <p>Get the latest posts delivered right to your inbox.</p>
            </section>

			


		</div>
	</article>
</div></div>]]>
            </description>
            <link>https://www.chriseigner.com/engineering-manager-event-loop/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034254</guid>
            <pubDate>Mon, 09 Nov 2020 12:57:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Cyrillic orthography for the Polish language]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25034182">thread link</a>) | @keiferski
<br/>
November 9, 2020 | http://steen.free.fr/cyrpol/index.html | <a href="https://web.archive.org/web/*/http://steen.free.fr/cyrpol/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

<!---------------- Header ------------------->





<!---------------- Body ------------------->



<p><small><i>See also: </i> <a href="http://steen.free.fr/interslavic/index.html">Interslavic</a>, <a href="http://steen.free.fr/wenedyk/index.html">Wenedyk</a>, <a href="http://steen.free.fr/poilschi/index.html">Poilschi</a></small></p>

<a name="introduction"></a>

<h2>Ortografia cyrylicka dla języka polskiego</h2>
<h2>A Cyrillic orthography for the Polish language</h2>

<p><big>E</big>ver wondered what Polish would look like if it were written in Cyrillic? Perhaps you have. Or not. In any case, I have. That's what happens when you spend half of your life working on language projects that one way or another are related to Polish or the Slavic languages in general. Toying around with Polish, Slavic, as well as with several Slavic orthographies, it is hard not to think about the possibilities of a Cyrillic orthography for Polish.</p>

<p>Many people have argued that Cyrillic would be unsuitable for Polish. I disagree with that opinion. Granted, Polish phonology differs from that of the other Slavic languages in several ways, but these two facts remain: Polish is a completely Slavic language by any standard, and Cyrillic, unlike the Latin alphabet, was made especially to fit Cyrillic phonology, and therefore is perfectly suited for it. Therefore, I am convinced that Polish and Cyrillic are a perfect match. Much more so, in fact, than Polish and the Latin alphabet. Latin orthographies of Slavic languages always have one of the following two disadvantages: either they are full of diacritical marks, or they look horribly like English or another Western language. Slovene manages best, but still has <b>š</b>, <b>ž</b> and <b>č</b>. Other languages have more of those babies. Polish orthography has managed to avoid hačeks, but has a whole bunch of other diacritics instead: <b>ą</b>, <b>ę</b>, <b>ł</b>, <b>ż</b>, <b>ć</b>, <b>ń</b>, <b>ó</b>, <b>ś</b>, <b>ź</b>. Besides, Polish in addition tends to favour digraphs like <b>sz</b> and <b>ie</b>, so Polish words tend to be appear longer than they actually are.</p>

<p>Now, I am quite fond of Polish orthography, and therefore my Cyrillic orthography of Polish should by no means be treated as a serious proposal to replace Polish orthography. If anyone would ever make such a proposal, I would be the first to stand up against it. This project, therefore, is primarily a thought experiment, my answer to the question if such an orthography would be possible at all.</p>

<p>The idea, by the way, is not new at all. If we have to believe Wikipedia, Russia's czar Nikolay I intended to cyrillify Polish in the mid-19th century as a means for russification, although at last nothing came of his plans. Here is a sample:</p>

<table><tbody><tr><td>
<div><p>Поврóтъ Таты, <i>пр̌езъ А. Мицкевича</i></p><p>
Пóйдзьце о дзятки, пóйдзьце вшистке разэм<br>
За място, подъ слупъ на взгóрэкъ,<br>
Тамъ пр̌едъ цудовнымъ клęкнийце образэмъ,<br>
Побожне змóвце пацёрэкъ.</p><p>
Тато не враца ранки и вечоры<br>
Вэ Лзах го чекамъ и трводзэ;<br>
Розлялы р̌еки, пэлнэ звер̌а боры,<br>
И пэлно збóйцóвъ на дродзэ;-</p></div>
</td><td>
<div><p>Слышąцъ то дзятки бегнą вшистке разэмъ<br>
За място подъ слупъ на взгóрэкъ,<br>
Тамъ пр̌едъ цудовнымъ клęкая̨ образемъ,<br>
И зачиная̨ пацёрэкъ.</p><p>
Цалуя̨ земę, потэмъ въ Имę Ойца,<br>
Сына и Духа свęтэго,<br>
Бąдзь похвалёна пр̌енайсьвęтша Трóйца<br>
Тэразъ и часу вшелькего.</p><p>
(...)</p></div></td></tr></tbody></table>

<p>A few pecularities in this text deserve our attention:
</p><ul>
<li>the use of the letter <b>р̌</b> for Polish <b>rz</b>;
</li><li>the hard sign <b>ъ</b> at the end of many words (a feature common in prerevolutionary Russian);
</li><li>the fact that Polish <b>ó</b> remains untouched;
</li><li>this orthography inherits the Polish ogonek and adds it to Cyrillic letters;
</li><li>the use of <b>ць</b> and <b>дзь</b> where Polish has <b>ć</b> and <b>dź</b>, a feature also present in contemporary Belarusian.
</li></ul>

<p>My own Cyrillic orthography for Polish is largely based on the same premises, but there are a few differences as well, which I will describe below. By the way, it should be noted that the transcription quoted above is not the only attempt at a Cyrillic alphabet for Polish. Several people have played with the idea, seriously or less seriously. An interesting example is <a href="http://varpho.livejournal.com/2006/11/17/">Jusowica (Юсовица)</a>, created by Szymon Pawlas.</p>

<hr>

<p><big>T</big>he biggest problem related with the Cyrillisation of Polish are sounds that do not exist in other languages, nor do they correspond closely with anything else that exist in them: the nasal vowels <b>ą</b> and <b>ę</b>. The 19th century Russian solution is in fact a pretty funny one: it simply teleports the ogonek to Cyrillic, thus producing four characters that have never seen before in Cyrillic: <b>а̨</b>, <b>э̨</b>, <b>я̨</b> and <b>е̨</b> (the latter two representing <b>ją</b> and <b>ję</b> respectively). A funny solution indeed! And an unnecessary one to that, because Old Church Slavonic has precisely four Cyrillic characters for exactly these four sounds: <b>ѫ</b>, <b>ѧ</b>, <b>ѭ</b> and <b>ѩ</b>. True, they are uncommon, because the only living Slavic language that preserved these sounds is Polish, a language that happens to be written in Latin alphabet. But since these letters are around, why shouldn't we simply use them? After all, they exist, and are indefinitely more Cyrillic than Cyrillic letters with ogoneks beneath them. Besides, the choice for <b>а̨</b> and <b>я̨</b> is equally unlogical as the Polish letter <b>ą</b> itself, since it is pronounced as nasalised <b>o</b>; it is not for nothing that the Latin transcription of Old Church Slavonic uses <b>ǫ</b>.</p>

<p>Another specifically Polish letter is the <b>ó</b>, pronounced as [u] (its Czech equivalent is <b>ů</b>). The transcription mentioned above conveniently keeps it. But why would we? It has no pronunciation of its own; the only thing that distinguishes it from <b>u</b> is that it alternates with <b>o</b>. Incidentally, mixing up those two is the most common spelling mistake in Polish. As far as I am concerned, there is no reason to keep it. Since <i>miasto</i> alternates with <i>mieście</i> (and not with <i>miæście</i> or something), why can't <i>grud</i> alternate with <i>grodzie</i>? So let's be bold and use <b>у</b> instead.</p>

<p>The characters <b>ć</b> and <b>dź</b> could of course be rendered like Belarusian (and in a way, Polish) does, by using <b>ць</b> and <b>дзь</b>, but I'd much prefer <b>ть</b> and <b>дь</b>. Etymologically speaking, this is more correct; after all <b>ć/dź</b> are the softened equivalents of <b>t/d</b>, not of <b>c/dz</b>. Sequences like <b>ti</b> and <b>di</b> are rare in Polish and occur only in foreign words. In these rare cases, we could write <i>радио</i> and <i>тиара</i> (a Pole will know that they are to be read as <i>radio</i> and <i>tiara</i> and not like <i>radzio</i> or <i>ciara</i>). Or, if we want to be really sure that the <b>t</b> will not be softened in these cases, we could use the hard sign and write <i>радъио</i> and <i>тъиара</i>.<br>
Using <b>ть</b>/<b>дь</b> instead of <b>ць</b>/<b>дзь</b> has one more advantage: now at least will not have to worry about the sequence <b>cja</b>, which is unambiguously rendered as <b>ця</b>.</p>

<p>Same goes for the digraphy <b>rz</b>, which in Polish is pronounced like <b>ż</b>. Another common source of spelling errors. Yet, I wouldn't propose transcribing it to <b>ж</b>, for the same etymological reasons: <b>rz</b> comes from softened <b>r</b>, while <b>ż</b> comes from softened <b>g</b>. The fact that it sounds very different does not change that fact. Therefore, we simply use <b>рь</b> (and not this weird creation from the 19th century, <b>р̌</b>). Just like <b>ti</b> and <b>di</b>, <b>ri</b> is a rare sequence in Polish that occurs only in foreign words, so I propose the same solution for it as well.</p>

<p>And then we have the letter <b>e</b>. Because in Polish palatalising <b>e</b> is way more numerous than its non-palatalising equivalent, we will use Cyrillic <b>e</b> for the former (usually rendered as <b>je</b> or <b>ie</b>) and <b>э</b> for the latter. This is also what the 19th century version does.</p>

<p>The choice for other Cyrillic letters is merely a matter of picking an option. For example, how do we represent <b>i</b> and <b>y</b>? Do we follow the Russian model and pick <b>и/ы</b> or do we prefer the Ukrainian model and pick <b>і/и</b>? Both are possible, but I've decided to follow the Russian model. Also, when preceded by <b>cz</b>, <b>sz</b> or <b>ż</b> we write <b>и</b> instead of <b>ы</b> – just like Russian does. Again, a matter of etymology.</p>

<p>So, let's see now what Cyrylica Polska looks like.</p>

<a name="alphabet"><hr></a><h2>Alphabet</h2>

<p><big>C</big>yrylica Polska has 37 letters. Exactly the same as the 33 letters of the Russian alphabet, with four additional characters for the nasals:</p>

<p><p><b><span size="+1">А Б В Г Д Е Ë Ж З И Й К Л М Н О П Р С Т У Ф Х Ц Ч Ш Щ Ъ Ы Ь Э Ю Я Ѧ Ѫ Ѩ Ѭ</span></b></p></p>

<a name="vowels"><hr></a><h2>Vowels</h2>

<p><big>E</big>very vowel has a hardening and a softening version. Both can occur in two possitions: either it follows a consonant, or it doesn't (in that case it is either word-initial or after another vowel). In Polish orthography, when a softening vowel follows a consonant, it is preceded by <b>i</b>, unless the consonant in question is inherently soft. In other positions this vowel is preceded by <b>j</b>. The only exceptions are <b>i</b>, which is softening by definition, and <b>y</b>, which is never softening. <br>
Just like <b>i</b> and <b>y</b> form a pair, in Cyrillic all vowels come in pairs, as you can see in the table below:</p>

<p><table><colgroup><col><col><col><col>
</colgroup><tbody><tr><th colspan="2">Latin</th><th colspan="2">Cyrylica</th></tr>
<tr><th> <i>hard</i> </th><th> <i>soft</i> </th><th> <i>hard</i> </th><th> <i>soft</i> </th></tr>
<tr><td>	a	</td><td>	ia/ja		</td><td>	а	</td><td>	я	</td></tr>
<tr><td>	e	</td><td>	ie/je		</td><td>	э	</td><td>	е	</td></tr>
<tr><td>	y	</td><td>	i		</td><td>	ы	</td><td>	и	</td></tr>
<tr><td>	o	</td><td>	io/jo		</td><td>	о	</td><td>	ë	</td></tr>
<tr><td>	ó<br>u	</td><td>	ió/jó<br>iu/ju	</td><td>	у	</td><td>	ю	</td></tr>
<tr><td>	ą	</td><td>	ią/ją		</td><td>	ѫ	</td><td>	ѭ	</td></tr>
<tr><td>	ę	</td><td>	ię/ję		</td><td>	ѧ	</td><td>	ѩ	</td></tr>
</tbody></table></p>

<a name="consonants"><hr></a><h2>Consonants</h2>

<p><big>N</big>ow that the question of palatalised vs. non-palalalised consonant has been resolved by the vowels that follow them, the consonants have suddenly become very simple to handle. Here goes:</p>

<p><table><tbody><tr><td>
<table><colgroup><col width="40%"><col width="60%">
</colgroup><tbody><tr><th>Latin</th><th>Cyrylica</th></tr>
<tr><td>	p		</td><td>	п	</td></tr>
<tr><td>	b		</td><td>	б	</td></tr>
<tr><td>	f		</td><td>	ф	</td></tr>
<tr><td>	w		</td><td>	в	</td></tr>
<tr><td>	t, ć		</td><td>	т	</td></tr>
<tr><td>	d, dź		</td><td>	д	</td></tr>
<tr><td>	s, ś		</td><td>	с	</td></tr>
<tr><td>	z, ź		</td><td>	з	</td></tr>
<tr><td>	k		</td><td>	к	</td></tr>
<tr><td>	g		</td><td>	г	</td></tr>
<tr><td>	ch<br>h		</td><td>	х	</td></tr>
</tbody></table>
</td><td>
<table><colgroup><col width="40%"><col width="60%">
</colgroup><tbody><tr><th>Latin</th><th>Cyrylica</th></tr>
<tr><td>	sz		</td><td>	ш	</td></tr>
<tr><td>	ż		</td><td>	ж	</td></tr>
<tr><td>	cz		</td><td>	ч	</td></tr>
<tr><td>	szcz		</td><td>	щ	</td></tr>
<tr><td>	c		</td><td>	ц	</td></tr>
<tr><td>	m		</td><td>	м	</td></tr>
<tr><td>	n		</td><td>	н	</td></tr>
<tr><td>	ł, l		</td><td>	л	</td></tr>
<tr><td>	r, rz		</td><td>	р	</td></tr>
<tr><td>	j		</td><td>	й	</td></tr>
<tr><td>	ь		</td><td>	soft sign	</td></tr>
<tr><td>	ъ		</td><td>	hard sign	</td></tr>
</tbody></table>
</td></tr></tbody></table></p>

<p>A few notes:
</p><ul>
<li>Most consonants can be soft (palatalised) or hard. Whether a Cyrillic <b>д</b> should be read as <b>d</b> or <b>dź</b> is decided by the consonant that follows it: <b>дэ</b> should be read as <b>de</b>, <b>де</b> should be read as <b>dzie</b>.
</li><li>If a soft consonant is not followed by a vowel, i.e. when it is word- or syllable-final, it is followed by the soft sign: <b>bat</b> becomes <b>бат</b>, <b>bać</b> becomes <b>бать</b>.
</li><li>In reality, the soft sign will occur only after <b>т</b>, <b>д</b>, <b>н</b>, <b>л</b>, and <b>р</b>. However, in a few cases it can be placed after another consonant as well, although that wouldn't affect pronunciation. For example, take these two Polish cities: Kraków and Wrocław. When declined, the former has a hard <b>w</b>, the latter a soft <b>w</b>, and so their genitives are <i>Krakowa</i> and <i>Wrocławia</i> respectively. In Cyrillic, we could easily write <b>Вроцлавь</b> for "Wrocław", to make this fact predictable.
</li><li>Most consonant clusers as palatalised as a whole, and only in a few cases consonants in such a cluster are palatalised individually. Therefore, <b>śmiałość</b> is written <b>смялость</b>, and not <b>сьмялосьть</b>.
</li><li>The consonant clusters <b>śr</b> and <b>źr</b> (historically from <i>ser-/zer- &gt; srze-/zrze-</i>, in some dialects <i>st…</i></li></ul></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://steen.free.fr/cyrpol/index.html">http://steen.free.fr/cyrpol/index.html</a></em></p>]]>
            </description>
            <link>http://steen.free.fr/cyrpol/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034182</guid>
            <pubDate>Mon, 09 Nov 2020 12:49:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Testing Maslow's Hierarchy of Needs (1999) [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25034104">thread link</a>) | @throw0101a
<br/>
November 9, 2020 | https://www.miqols.org/howb/wp-content/uploads/2016/06/Hagerty-M._Maslows-Needs-Hierarchy-Natl-QOL_1998.pdf | <a href="https://web.archive.org/web/*/https://www.miqols.org/howb/wp-content/uploads/2016/06/Hagerty-M._Maslows-Needs-Hierarchy-Natl-QOL_1998.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.miqols.org/howb/wp-content/uploads/2016/06/Hagerty-M._Maslows-Needs-Hierarchy-Natl-QOL_1998.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034104</guid>
            <pubDate>Mon, 09 Nov 2020 12:39:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Urgency Illusion: How to stay present when big things happen in the world]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25034043">thread link</a>) | @LeonW
<br/>
November 9, 2020 | https://leowid.com/the-urgency-illusion-how-to-stay-present-when-big-things-happen-in-the-world/ | <a href="https://web.archive.org/web/*/https://leowid.com/the-urgency-illusion-how-to-stay-present-when-big-things-happen-in-the-world/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <header>    
                
            </header>

            <section>
            <div>
                <div><p>Last week I felt way more on edge than usual. There were new lockdown restrictions here in Austria due to COVID, the US elections kept me checking the news every few minutes and a terrorist attack shook up the country. My belly was tight and the world felt gray. Quickly, I felt preoccupied with so many things at once.</p>
<p>My experience is that when events like these happen, I can feel an urge to drop everything and somehow get involved and do things related to these events. Or to fall into apathy. Some of that feels good, yet often the intention it’s coming from is one of my underlying fear, anger, hatred, or sadness. I witness this in others often too. The more I reflect on this, the more it becomes clear to me that this is usually a dead-end. The more helpful way forward seems to be to attend to my emotions and state of being first, before I’m doing anything with it.</p>
<p>Hard, yet simple. Especially when so much is coming at us all at once.</p>
<h2>Being touched, but not led astray</h2>
<p>My friend Matthias told me recently on a hike through the forest, how he spent some time meditating on the terrorist attacks. His intention was to see the causes and conditions that led to it, casting a net as wide as possible. To me, this was a great example of letting ourselves be touched by current events, but not led astray. I would illustrate it like this:</p>
<p><img loading="lazy" src="https://leowid.com/wp-content/uploads/2020/11/ideas-8.jpg" alt="" width="2157" height="1668" srcset="https://leowid.com/wp-content/uploads/2020/11/ideas-8.jpg 2157w, https://leowid.com/wp-content/uploads/2020/11/ideas-8-300x232.jpg 300w, https://leowid.com/wp-content/uploads/2020/11/ideas-8-1024x792.jpg 1024w, https://leowid.com/wp-content/uploads/2020/11/ideas-8-768x594.jpg 768w, https://leowid.com/wp-content/uploads/2020/11/ideas-8-1536x1188.jpg 1536w, https://leowid.com/wp-content/uploads/2020/11/ideas-8-2048x1584.jpg 2048w" sizes="(max-width: 2157px) 100vw, 2157px"></p>
<p>The patterns I see the most and know well from myself, that I believe aren’t very helpful are two extreme reactions to current events:</p>
<ul>
<li><strong>apathy or “I don’t care…”</strong>: I simply carry on with my life, pretending that nothing happened or ignoring any major current events that have shaken up the world. This tends to keep me focused, but also makes my work and attention kind of lifeless, apathetic, and overall feel disconnected from my own intentions and dreams.</li>
<li><strong>flooded or “OMG, drop everything &amp; let’s do something!”</strong>: Here I have such a strong reaction that I want to take to the streets immediately, express my anger, hurt, sadness, and pain in the hopes that it will improve the situation. I feel reactive and righteous that I’m doing something about the situation.</li>
</ul>
<p>I believe that neither of these reactions helps us create the life and world that we ultimately want to see. And I think there’s a middle way, that uses the wisdom of both of these more extreme directions:</p>
<ul>
<li><strong>Care</strong>: When there is a major external event after we’ve gotten to safety, whether it’s from disease, attack, or something else, we first need care. By care, I mean our ability to tend to the emotional and inner states that have been evoked from the event. Tending to our anger, hurt, pain, feeling our sadness, tears, and frustration. This can take some time and the more support we have to feel through these elements of life, the more enjoyable this part can be for us.</li>
<li><strong>Integration with the life you want</strong>: Once the big emotions have settled, we can turn our attention to integration and meaning-making. What does it mean that we have experienced this? How does this connect with our bigger intention of living the life we want? An example from my own life is that through plenty of reflection on the coronavirus crisis and the amount of physical distancing and disconnection has birthed a new idea of a product to help us reconnect in a meaningful way, even when we’re not in the same room together. I’ve deeply enjoyed working on this the last weeks, it doesn’t feel reactive, yet it seems to be aligned with what is happening in the wider world and my dream of creating more presence and aliveness for myself and the people I meet.</li>
</ul>
<h2>Letting the urgency illusion pass and acting with power</h2>
<p>As the urgency illusion passes, there comes a window for all of us where we can be present to what happened and at the same time have enough space inside for ourselves and our dreams. This is the sweet spot, where we can make useful sense of ourselves and the world around us.</p>
<p>If you’re stuck along the journey towards integration right now, here’re some questions that you might find helpful to journal with:</p>
<ul>
<li><strong>Care-questions</strong>: What needs to be tended to on the inside? How are you doing? What level of care would support you the most right now?</li>
<li><strong>Integration-questions</strong>: What are the 3 most important values for you to live into in this life? Where is the overlap between those and what happened in the world? How can you move forward in integrity with what happened, without throwing everything overboard?</li>
</ul>
<p>From there, see which actions naturally arise for you as you give yourself space and time to let everything integrate.</p>
<p>Whatever you’re going through, keep going, sending love and care your way!</p>
</div>
            </div>
        </section><section>
                <div>
                    <p><img data-src="https://leowid.com/wp-content/uploads/2020/07/Leo-Profile.jpg" data-srcset="https://leowid.com/wp-content/uploads/2020/07/Leo-Profile.jpg 400w, https://leowid.com/wp-content/uploads/2020/07/Leo-Profile-300x300.jpg 300w, https://leowid.com/wp-content/uploads/2020/07/Leo-Profile-150x150.jpg 150w, https://leowid.com/wp-content/uploads/2020/07/Leo-Profile-180x180.jpg 180w" data-sizes="(max-width: 100px) 100vw, 100px" alt="" src="https://leowid.com/wp-content/uploads/2020/07/Leo-Profile.jpg" srcset="https://leowid.com/wp-content/uploads/2020/07/Leo-Profile.jpg 400w, https://leowid.com/wp-content/uploads/2020/07/Leo-Profile-300x300.jpg 300w, https://leowid.com/wp-content/uploads/2020/07/Leo-Profile-150x150.jpg 150w, https://leowid.com/wp-content/uploads/2020/07/Leo-Profile-180x180.jpg 180w">
                        <span>Leo Widrich</span>
                        <span>Leo Widrich coaches extraordinary people. In his previous life, he co-founded Buffer, a $20m+ revenue software company. He also lived in Buddhist monasteries for close to two years, trained as a trauma therapist and now lives in Vienna near the forest. He tweets <a href="https://twitter.com/LeoWid">@leowid</a>. To learn about working with him, <a href="https://leowid.com/working-with-me/">go here</a>.</span>
                    </p>
                </div>
            </section><section>
        <div>
            <div><h3>Receive my most vulnerable and powerful lessons from meeting life.</h3><p>Add your details below for my weekly newsletter.</p></div>
        </div>
    </section>
        </div></div>]]>
            </description>
            <link>https://leowid.com/the-urgency-illusion-how-to-stay-present-when-big-things-happen-in-the-world/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034043</guid>
            <pubDate>Mon, 09 Nov 2020 12:29:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[When remote work doesn't cut it]]>
            </title>
            <description>
<![CDATA[
Score 63 | Comments 47 (<a href="https://news.ycombinator.com/item?id=25034037">thread link</a>) | @FlyingSnake
<br/>
November 9, 2020 | https://samkhawase.com/blog/remote-work/ | <a href="https://web.archive.org/web/*/https://samkhawase.com/blog/remote-work/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                <p>The COVID-19 crisis, while disrupting the global world unlike anything before, has opened up an unexpected window to remote work. Nearly all major<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>tech<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> giants<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> have<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> allowed their workers to do home office. Many people are considering this as a sign of the advent of <strong>Work 2.0</strong>, where physical offices spaces will be irrelevant, and people can work from their cozy dens. There are however significant challenges in adoption of generalized remote work and things will be back as usual once the COVID-19 ends.</p>
<p>The <strong>challenges surrounding remote work outweigh it’s promises</strong>. Not every company is a FAANG, and companies will struggle to transition given their limited resources.</p>
<h2 id="regulations">Regulations</h2>
<p>The most significant hurdle in hiring a global remote team is <strong>regulation</strong>. Labor regulations are wildly different amongst countries, and could be cumbersome for some companies. Some major hurdles include:</p>
<ul>
<li>
<p><strong>Payroll taxes</strong>, retirement bonuses: Germany has rentenversicherung, sozialversicherung whereas USA has 401k contributions. Can a German company afford to <strong>setup payroll</strong> taxes for a remote workers hired from India, Chile or US? Or will it lead to worker abuse through <strong>laissez-faire abuse</strong> through freelance contracts?</p>
</li>
<li>
<p><strong>Notice periods</strong>: Europeans (on average) have 3 months notice period while US Americans have 2 weeks. How would a US company deal with it? On top of that, several countries have <strong>protection against unlawful termination</strong>, and how can a Slovakian employee avail that benefit against a German company?</p>
</li>
<li>
<p><strong>IP protection</strong>: It’s hard to <strong>protect IP</strong> if employees are not in the same jurisdiction. A company operation from Czechia would find it hard to settle trade disputes with a remote worker from South Africa. Another example is of <strong>TISAX compliance</strong> that is required for specialized hardware projects for Automotive industries. Remote work fails to make a dent in this situation.</p>
</li>
</ul>
<h2 id="hardware-cant-remote">Hardware can’t remote</h2>
<p>Patio11’s law<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> states that the <strong>economy is much bigger than you think</strong>. There are companies which have widely different business models and they often have a hardware related product. My <a href="https://www.salonlab-server.de/en-GB/">current project</a> is an IoT device that talks to an iPad app. The <strong>hardware team</strong> needs <strong>specialized tools</strong> to work on the IoT device, and these tools can’t be moved to home office. Remote work is a strict no-no for such products.</p>
<h2 id="swim-against-the-tide">Swim against the tide</h2>
<p>The biggest hurdle employees face in remote/home offices is <strong>lack of focus and direction</strong>. Humans have evolved over thousands of years to collaborate based on interpersonal cues, and a video call simply does not have the same effect. Humans need <strong>feedback</strong> and <strong>constructive communication</strong> whereas isolation kills the spirit. People who are new to remote work often feel <strong>rudderless</strong> because <strong>self discipline is hard</strong> when there’s no structure. I’m doing remote work on-and-off since 2018, and it took me a lot of discipline to get productive. The simple fact is that remote work is not natural, and not suited for all work streams in a typical company. Add to it the fact that many <strong>families</strong> simply don’t have space to work from home, and on top of that there might be kids around.</p>
<p>Remote work might be one of the few positive outcomes of the COVID-19 crisis but unless we tend to it carefully, we’ll end up creating a unhappy and unproductive workspace.</p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://www.wsj.com/articles/facebook-to-shift-permanently-toward-more-remote-work-after-coronavirus-11590081300">https://www.wsj.com/articles/facebook-to-shift-permanently-toward-more-remote-work-after-coronavirus-11590081300</a> <a href="#fnref:1" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://www.cnbc.com/2020/08/07/atlassian-tells-employees-they-can-work-from-home-indefinitely.html">https://www.cnbc.com/2020/08/07/atlassian-tells-employees-they-can-work-from-home-indefinitely.html</a> <a href="#fnref:2" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><a href="https://www.wsj.com/articles/google-to-keep-employees-home-until-summer-2021-amid-coronavirus-pandemic-11595854201">https://www.wsj.com/articles/google-to-keep-employees-home-until-summer-2021-amid-coronavirus-pandemic-11595854201</a> <a href="#fnref:3" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p><a href="https://www.buzzfeednews.com/article/alexkantrowitz/twitter-will-allow-employees-to-work-at-home-forever">https://www.buzzfeednews.com/article/alexkantrowitz/twitter-will-allow-employees-to-work-at-home-forever</a> <a href="#fnref:4" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p><a href="https://secondbreakfast.co/patio11-s-law">https://secondbreakfast.co/patio11-s-law</a> <a href="#fnref:5" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>

            </div></div>]]>
            </description>
            <link>https://samkhawase.com/blog/remote-work/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034037</guid>
            <pubDate>Mon, 09 Nov 2020 12:28:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Free Typography Logo Maker]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25034017">thread link</a>) | @hosshams
<br/>
November 9, 2020 | https://formito.com/tools/logo | <a href="https://web.archive.org/web/*/https://formito.com/tools/logo">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://formito.com/tools/logo</link>
            <guid isPermaLink="false">hacker-news-small-sites-25034017</guid>
            <pubDate>Mon, 09 Nov 2020 12:25:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[OpenBSD Router Guide]]>
            </title>
            <description>
<![CDATA[
Score 18 | Comments 5 (<a href="https://news.ycombinator.com/item?id=25033925">thread link</a>) | @upofadown
<br/>
November 9, 2020 | https://www.unixsheikh.com/tutorials/openbsd-router-guide/ | <a href="https://web.archive.org/web/*/https://www.unixsheikh.com/tutorials/openbsd-router-guide/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
<table>
    <tbody><tr>
        <td><img src="https://www.unixsheikh.com/includes/img/openbsd-icon.png" alt="OpenBSD icon"></td>
        <td>
            
            <h4>Network segmenting firewall, DHCP, DNS with Unbound, domain blocking and much more<br>
                <span>OpenBSD: 6.8 · Published: 2020-11-05 · Updated: 2020-11-11 · Version: 1.3.4</span>
            </h4>
        </td>
    </tr>
</tbody></table>

<h2>Introduction</h2>

<div><p>In this guide we're going to take a look at how we can use cheap and "low end" hardware to build an amazing OpenBSD router with firewalling capabilities, segmented local area networks, DNS with domain blocking, DHCP and more.</p><p>We will use a setup in which the router segments the local area network (LAN) into three separate networks, one for the grown-ups in the house, one for the children, and one for public facing servers, such as a private web server or mail server. We will also look at how we can use DNS to block out ads, porn, and other websites on the Internet. The OpenBSD router can also be used on small to mid-size offices.</p></div>

<p>Table of contents</p>
<ul>
    <li><a href="#why-a-firewall">Why a firewall?</a></li>
    <li><a href="#the-hardware">The hardware</a></li>
    <li><a href="#why-openbsd">Why OpenBSD?</a></li>
    <li><a href="#the-network">The network</a>
    <ul>
        <li><a href="#setting-up-the-network">Setting up the network</a></li>
    </ul>
    </li>
    <li><a href="#dhcp">DHCP</a></li>
    <li><a href="#a-packet-filtering-firewall">PF - A packet filtering firewall</a>
    <ul>
        <li><a href="#pf-setup">PF setup</a></li>
        <li><a href="#clarifications">Clarifications</a></li>
        <li><a href="#pf-domain-name-resolution">Domain name or hostname resolution</a></li>
        <li><a href="#the-ruleset">The ruleset</a>
            <ul>
                <li><a href="#whitelist">The children's whitelist</a>
                    <ul>
                        <li><a href="#persistent-table">Using a persistent table</a></li>
                    </ul>
                </li>
            </ul>
        </li>
        <li><a href="#loading-ruleset">Loading the rules</a></li>
        <li><a href="#logging">Logging and monitoring</a></li>
    </ul>
    </li>
    <li><a href="#domain-name-service">DNS</a>
    <ul>
        <li><a href="#unbound">I present to you, Unbound</a></li>
        <li><a href="#blocking-with-dns">Blocking with DNS</a>
            <ul>
                <li><a href="#nxdomain">NXDOMAIN vs redirecting</a></li>
            </ul>
        </li>
        <li><a href="#doh">The problem with DNS over HTTPS (DoH)</a></li>
        <li><a href="#unbound-setup">Setting up Unbound</a>
            <ul>
                <li><a href="#basic-settings">Basic settings</a></li>
                <li><a href="#lets-block-some-domains">Let's block some domains!</a></li>
            </ul>
        </li>
        <li><a href="#dns-security">DNS security</a>
            <ul>
                <li><a href="#dns-hijacking">DNS hijacking</a>
                    <ul>
                        <li><a href="#dns-hijacking-prevention">DNS hijacking prevention</a></li>
                    </ul>
                </li>
                <li><a href="#dns-spoofing">DNS spoofing</a>
                    <ul>
                        <li><a href="#dns-spoofing-prevention">DNS spoofing prevention</a></li>
                    </ul>
                </li>
            </ul>
        </li>
    </ul>
    </li>
    <li><a href="#appendix">Appendix</a>
        <ul>
            <li><a href="#inspecting-doh">Inspecting DNS over HTTPS (DoH)</a></li>
            <li><a href="#blocking-doh">Blocking DNS over HTTPS (DoH)</a></li>
            <li><a href="#recommended-reading">Recommended reading</a></li>
            <li><a href="#how-to-contribute">How to contribute to the guide?</a></li>
            <li><a href="#todo">TODO</a></li>
        </ul>
    </li>
</ul>

<h2 id="why-a-firewall">Why a firewall?</h2>
<p>Almost no matter how you connect to the Internet from your home or office, you need a real firewall between you and the modem or router that your ISP has provided you with.</p>
<p>Very rarely do consumer-grade modems or routers get firmware updates and they are often vulnerable to <a href="https://en.wikipedia.org/wiki/Home_router#Security">network attacks</a> that turns these devices into <a href="https://en.wikipedia.org/wiki/Botnet">botnets</a>, such like the <a href="https://en.wikipedia.org/wiki/Mirai_(malware)">Mirai malware</a>. Many consumer-grade modems and routers is to blame for some of the largest <a href="https://en.wikipedia.org/wiki/Distributed_denial_of_service_attack">distributed denial of service (DDoS) attacks</a>.</p>
<p>A firewall between you and your ISP modem or router cannot protect your modem or router device against attacks, but it can protect your computers and devices on the inside of the network, and it can help you monitor and control the traffic that comes and goes to and from your local network.</p>
<p>Without a firewall between your local network and the ISP modem or router you could basically consider this an open door policy, like leaving the door to your house wide open, because you cannot trust the equipment from your ISP.</p>
<p>It is always a really good idea to put a real firewall between your local network and the Internet, and with OpenBSD you get an very solid solution.</p>
<p><b>NOTE:</b><br>Currently this guide only deals with IPv4 as most people still don't use IPv6 and many ISPs also still only use IPv4, but IPv6 is planned for a future update of the guide.</p>

<h2 id="the-hardware">The hardware</h2>
<p>You don't have to buy expensive hardware to get an effective router and firewall for your house or office. Even with cheap and "low end" hardware you can get a very solid solution.</p>
<p>I have build multiple solutions with the <a href="https://www.asrock.com/mb/Intel/Q1900DC-ITX/">ASRock Q1900DC-ITX</a> motherboard that comes with an Intel Quad-Core Celeron processor.</p>
<p><img src="https://www.unixsheikh.com/includes/img/asrock-q1900dc-itx.png" alt="ASRock Q1900DC-ITX motherboard"></p>
<p>I'll admit, it's a pretty "crappy" motherboard, but it gets the job done and I have several builds that have run very solid for many years on gigabit networks with full saturation and the firewall, DNS, etc. working "overtime" and the CPU hardly breaks a sweat.</p>
<p>The ASRock Q1900DC-ITX motherboard has the advantage that it comes with a DC-In Jack that is compatible with a 9~19V power adapter, making it very power saving. Unfortunatly the ASRock Q1900DC-ITX motherboard is no longer made, but I'm just using it as an example, I have used several other cheap boards as well.</p>
<p>I have also used the ASRock Q1900-ITX (it doesn't come with the DC-In Jack) combined with a PicoPSU.</p>
<p><img src="https://www.unixsheikh.com/includes/img/picopsu.png" alt="PicoPSU power supply"></p>
<p>You can find different brands and versions of the PicoPSU, some are better quality than others. I have two different brands, the original and a cheaper knockoff, both performs very well and they save quite a bit of power contrary to running with a normal power supply.</p>
<p>Last, I am using a cheap Intel knockoff quad port NIC found on Ebay like this one:</p>
<p><img src="https://www.unixsheikh.com/includes/img/intel-quad-nic.png" alt="Intel Quad NIC"></p>
<p>I know it is better to use quality hardware, especially on a network that you care about, but this tutorial is about how you can get away with using fairly cheep hardware and still get an extremely useful product that will continue to serve you well for many years - at least that is my experience.</p>
<p>I recommend that you look for a low power mini ITX board with hardware <a href="https://www.openbsd.org/amd64.html">supported by OpenBSD</a>, such as an Intel Celeron or Intel i3 processor. These boards are typically cheap, less power hungry, and they don't take up much space. I don't recommend using the Intel Atom CPU if you have a gigabit network as they usually choke because they can't handle the amount of traffic, but your mileage may vary.</p>
<p>You might also need a couple of cheap gigabit switches for the segmented local network, at least if you have more than one computer you want to connect to the same LAN :)</p>

<h2 id="why-openbsd">Why OpenBSD?</h2>
<p>In truth, you can get a similar setup with one of the other <a href="https://en.wikipedia.org/wiki/Comparison_of_BSD_operating_systems">BSD flavors</a> or one of the many different <a href="https://en.wikipedia.org/wiki/Linux_distribution">Linux distribution</a>, but <a href="https://www.openbsd.org/">OpenBSD</a> is specifically very well suited and designed for this kind of task. Not only does it come with all the needed software in the base install, but it also has significantly better security and tons of improved mitigations already build-in into the operating system. I <a href="https://www.unixsheikh.com/articles/openbsd-is-fantastic.html">highly recommend</a> OpenBSD over any other operating system for this kind of task.</p>
<p>This guide is not going to show you how to install OpenBSD. If you haven't done that before I recommend you spin up some kind of virtual machine or see if you have some unused and supported hardware laying around you can play with. OpenBSD is one of the easiest and quickest operating systems to install. Don't be afraid of the non-gui approach, once you have tried it you will really appreciate the simplicity. Use the default settings when in doubt.</p>
<p>Before you endeavor on this journey make sure to reference the OpenBSD documentation! Not only is everything very well documented, but you will most likely find all the answers you need right there. Read the <a href="https://www.openbsd.org/faq/index.html">OpenBSD FAQ</a> and take a look at the different <a href="https://man.openbsd.org/">manual pages</a> for the software we're going to use.</p>
<p>Another really useful place to find general information about OpenBSD is the <a href="https://marc.info/?l=openbsd-misc">OpenBSD mailing list archives</a>. Also make sure to stay up to date with relevant information by subscribing to the <a href="https://www.openbsd.org/mail.html">Announcements and security advisories</a> mailing list.</p>
<p>Last, but not least, please consider <a href="https://www.openbsd.org/donations.html">supporting OpenBSD</a>! Even if you don't use OpenBSD on a daily basis, but perhaps make use of <a href="https://www.openssh.com/">OpenSSH</a> on Linux, then you're really using software from the OpenBSD project. Consider making a small, but steady donation to support the further development of all the great software the OpenBSD developers make!</p>

<h2 id="the-network">The network</h2>
<p>A router is basically a device that regulate network traffic between two or more separate networks. The router will ensure that network traffic intended for the local network doesn't run out into the wild on the Internet, and traffic on the Internet, that is not intended for your local network, stays on the Internet.</p>
<p><b>NOTE:</b><br>A router is sometimes also referred to as a gateway, which generally is alright, but in truth a real gateway joins dissimilar systems, while a router joins similar networks. An example of a gateway would be a device that joins a PC network with a telecommunications network.</p>
<p>In this tutorial we're building a router and we have 4 networks of the same type to work with. One is the Internet and the other three are the internally segmented local area networks (LANs). Some people prefer to work with virtual LANs, but in this tutorial we're going to use the quad port NIC from the illustration above. You can achieve the same result by using multiple one port NICs if you prefer that, you just have to make sure that you have enough room and free PCI slots on the motherboard. You can also use the Ethernet port on the motherboard itself, but it depends on the driver and support for the device. I have had no problems using the Realtek PCI gigabit Ethernet controller that normally comes with many motherboards even though I recommend Intel over Realtek.</p>
<p>Of course you don't have to segment the network into several parts if you don't need that, and it will be very easy to change the settings from this guide, but I have decided to use this approach in order to show you how you can protect your children by segmenting their network into a separate LAN that not only gets ad and porn blocking using DNS blocking (all the segments gets that), but you can even whitelist the parts of the Internet you want them to have access to. The last part about whitelisting is difficult and generally not recommended unless your children requires only very limited access, but it is doable with some work, and the guide is going to show you one way you can do that.</p>
<p>This is an illustration of the network we're going to setup:</p>
<pre><code>
                       Internet
                          |
                    xxx.xxx.xxx.xxx
                    ISP Modem (WAN)
                      10.24.0.23
                          |
                       OpenBSD
                      10.24.0.50
                  (router/firewall)
                          |
     -------------------------------------------
     |                    |                    |
    NIC1                 NIC2                 NIC3
192.168.1.1          192.168.2.1          192.168.3.1
LAN1 switch          LAN2 switch          LAN3 switch
     |                    |                    |
     -- 192.168.1.x       -- …</code></pre></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.unixsheikh.com/tutorials/openbsd-router-guide/">https://www.unixsheikh.com/tutorials/openbsd-router-guide/</a></em></p>]]>
            </description>
            <link>https://www.unixsheikh.com/tutorials/openbsd-router-guide/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033925</guid>
            <pubDate>Mon, 09 Nov 2020 12:13:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pfizer and BioNTech Announce Vaccine Candidate Against Covid-19 Achieved Success]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033852">thread link</a>) | @doener
<br/>
November 9, 2020 | https://investors.biontech.de/news-releases/news-release-details/pfizer-and-biontech-announce-vaccine-candidate-against-covid-19?mobile=1 | <a href="https://web.archive.org/web/*/https://investors.biontech.de/news-releases/news-release-details/pfizer-and-biontech-announce-vaccine-candidate-against-covid-19?mobile=1">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article role="article">

  <div>
                <ul type="disc"><li><em>Vaccine candidate was found to be more than 90% effective in preventing COVID-19 in participants without evidence of prior SARS-CoV-2 infection in the first interim efficacy analysis</em></li><li><em>Analysis evaluated</em><em> 94 confirmed cases of COVID-19 in trial participants </em></li><li><em>Study enrolled 43,538 participants, with 42% having diverse backgrounds, and no serious safety concerns have been observed; safety and additional efficacy data continue to be collected </em></li><li><em>Submission for Emergency Use Authorization (EUA) to the U.S. Food and Drug Administration (FDA) planned soon after the required safety milestone is achieved, which is currently expected to occur in the third week of November </em></li><li><em>Clinical trial to continue through to final analysis at 164 confirmed cases in order to collect further data and characterize the vaccine candidate’s performance against other study endpoints</em></li></ul><p><strong>NEW YORK and MAINZ, GERMANY, November 9, 2020</strong> — <a href="https://www.globenewswire.com/Tracker?data=PJ8PG63hVRh2bkt5smgOlFAGa-RXpa2iRwzAO6IqH_YcfyX-R7Tdk96PAniX7YGQoMAdF-jDHoeL-YX46MLSDw==" rel="nofollow" target="_blank"><u>Pfizer Inc.</u></a> (NYSE: PFE) and <a href="https://www.globenewswire.com/Tracker?data=XfaLG8yfG5HYcb1T5NR2xBQd-D3R4PCcCCpFaLX9rM23piCwE-K7u65TmqKeemgOsm9Iwv7SBS6CLe_lhfQmFA==" rel="nofollow" target="_blank"><u>BioNTech SE</u></a> (Nasdaq: BNTX) today announced their mRNA-based vaccine candidate, BNT162b2, against SARS-CoV-2 has demonstrated evidence of efficacy against COVID-19 in participants without prior evidence of SARS-CoV-2 infection, based on the first interim efficacy analysis conducted on November 8, 2020 by an external, independent Data Monitoring Committee (DMC) from the Phase 3 clinical study. After discussion with the FDA, the companies recently elected to drop the 32-case interim analysis and conduct the first interim analysis at a minimum of 62 cases. Upon the conclusion of those discussions, the evaluable case count reached 94 and the DMC performed its first analysis on all cases. </p>  <p>The case split between vaccinated individuals and those who received the placebo indicates a vaccine efficacy rate above 90%, at seven days after the second dose. This means that protection is achieved 28 days after the initiation of the vaccination, which consists of a 2-dose schedule. As the study continues, the final vaccine efficacy percentage may vary. The DMC has not reported any serious safety concerns and recommends that the study continues to collect additional safety and efficacy data as planned. The data will be discussed with regulatory authorities worldwide. </p>  <p>“Today is a great day for science and humanity. The first set of results from our Phase 3 COVID-19 vaccine trial provides the initial&nbsp;evidence of our vaccine’s ability to prevent COVID-19,” said <strong>Dr. Albert Bourla, Pfizer Chairman and CEO.</strong> “We are reaching this critical milestone in our vaccine development program at a time when the world needs it most with infection rates setting new records, hospitals nearing over-capacity and economies struggling to reopen. With today’s news, we are a significant step closer to providing people around the world with a much-needed breakthrough to help bring an end to this global health crisis. We look forward to sharing additional efficacy and safety data generated from thousands of participants in the coming weeks.”</p>  <p>“I want to thank the thousands of people who volunteered to participate in the clinical trial, our academic collaborators and investigators at the study sites, and our colleagues and collaborators around the world who are dedicating their time to this crucial endeavor,” added <strong>Bourla.</strong> “We could not have come this far without the tremendous commitment of everyone involved.”</p>  <p>“The first interim analysis of our global Phase 3 study provides evidence that a vaccine may effectively prevent COVID-19. This is a victory for innovation, science and a global collaborative effort,” said <strong>Prof. Ugur Sahin, BioNTech Co-founder and CEO.</strong> “When we embarked on this journey 10 months ago this is what we aspired to achieve. Especially today, while we are all in the midst of a second wave and many of us in lockdown, we appreciate even more how important this milestone is on our path towards ending this pandemic and for all of us to regain a sense of normality. We will continue to collect further data as the trial continues to enroll for a final analysis planned when a total of 164 confirmed COVID-19 cases have accrued. I would like to thank everyone who has contributed to make this important achievement possible.”</p>  <p>The Phase 3 clinical trial of BNT162b2 began on July 27 and has enrolled 43,538 participants to date, 38,955 of whom have received a second dose of the vaccine candidate as of November 8, 2020. Approximately 42% of global participants and 30% of U.S. participants have racially and ethnically diverse backgrounds. The trial is continuing to enroll and is expected to continue through the final analysis when a total of 164 confirmed COVID-19 cases have accrued. The study also will evaluate the potential for the vaccine candidate to provide protection against COVID-19 in those who have had prior exposure to SARS-CoV-2, as well as vaccine prevention against severe COVID-19 disease. In addition to the primary efficacy endpoints evaluating confirmed COVID-19 cases accruing from seven days after the second dose, the final analysis now will include, with the approval of the FDA, new secondary endpoints evaluating efficacy based on cases accruing 14 days after the second dose as well. The companies believe that the addition of these secondary endpoints will help align data across all COVID-19 vaccine studies and allow for cross-trial learnings and comparisons between these novel vaccine platforms. The companies have posted an updated version of the study protocol at <a href="https://www.globenewswire.com/Tracker?data=bE4RvEXc3amHdJ0LinFQkPyUCRxShL94rKlQWnVIuOebbWs0t_t1F1qSmc0deSfk8TsQTmqiZHqgYUA4_HoIG8cgZ-MTBQVX5ZbQ89L9eImlSxNuQzgmZ36Fp4CVqIu8sjS4aT-HPEktSLWmEhtRsQ==" rel="nofollow" target="_blank"><u>https://www.pfizer.com/science/coronavirus</u></a>. </p>  <p>Pfizer and BioNTech are continuing to accumulate safety data and currently estimate that a median of two months of safety data following the second (and final) dose of the vaccine candidate – the amount of safety data specified by the FDA in its guidance for potential Emergency Use Authorization – will be available by the third week of November. Additionally, participants will continue to be monitored for long-term protection and safety for an additional two years after their second dose.</p>  <p>Along with the efficacy data generated from the clinical trial, Pfizer and BioNTech are working to prepare the necessary safety and manufacturing data to submit to the FDA to demonstrate the safety and quality of the vaccine product produced. Based on supply projections, we expect to supply globally up to 50 million vaccine doses in 2020 and manufacture up to 1.3 billion doses in 2021. Pfizer and BioNTech plan to submit data from the full Phase 3 trial for scientific peer-review publication.</p>  <p><strong>About Pfizer: Breakthroughs That Change Patients’ Lives</strong></p>  <p>At Pfizer, we apply science and our global resources to bring therapies to people that extend and significantly improve their lives. We strive to set the standard for quality, safety and value in the discovery, development and manufacture of health care products, including innovative medicines and vaccines. Every day, Pfizer colleagues work across developed and emerging markets to advance wellness, prevention, treatments and cures that challenge the most feared diseases of our time. Consistent with our responsibility as one of the world's premier innovative biopharmaceutical companies, we collaborate with health care providers, governments and local communities to support and expand access to reliable, affordable health care around the world. For more than 150 years, we have worked to make a difference for all who rely on us. We routinely post information that may be important to investors on our website at <u><a href="https://www.globenewswire.com/Tracker?data=4FbrwG1rPf9jwYvPniD1rUMbj6s_Wqek0iGxXtCmV7zUg7CMpYiSUA1zc-r5E0Nf_ZNaQlSWnZ5e8_mFWG8XYA==" rel="nofollow" target="_blank">www.Pfizer.com</a></u>. In addition, to learn more, please visit us on <u><a href="https://www.globenewswire.com/Tracker?data=4FbrwG1rPf9jwYvPniD1rf9AtACzzG5su0IsLCtDLy0Q4vyLC1u2a07goDfiO7HGejXmxyveSXGtjTX9Jbf1aw==" rel="nofollow" target="_blank">www.Pfizer.com</a></u> and follow us on Twitter at <a href="https://www.globenewswire.com/Tracker?data=nZQmHBaz29Df7F7-i_Dx5Ci9JAIKZm1fs38JsJ0UDRndZf2WfVJLut17r7ky8GafcpFUih6abC7JiIHrL1K6Bw==" rel="nofollow" target="_blank"><u>@Pfizer</u></a> and <a href="https://www.globenewswire.com/Tracker?data=nZQmHBaz29Df7F7-i_Dx5EIUSI0zr1bjnR6DBf-Egt2of100u8SheiSFC3c0Qbr_yoniSSHE2UwRTyWYI9JOS5L8WV3tP6hHSIv-ym0XDP0=" rel="nofollow" target="_blank"><u>@Pfizer News</u></a>, <a href="https://www.globenewswire.com/Tracker?data=UH05W4ZxYdmUeMyOlJKhJmRsqWd0fvhHaFnVv0fo-CIWUdkF6HDUEv2p868nekUmyUYROhkLqSYY41Pcuq02MsL4ZoYkXD237abwsIit60U=" rel="nofollow" target="_blank"><u>LinkedIn</u></a>, <a href="https://www.globenewswire.com/Tracker?data=ir8lky4stO2XkFLpjU_Ut0YeVDly5-CLV1tWNdWEYeM0oZHykh1Sm3s7CRgIHdfPWelsTpFq7j2P9dXmnvMVig==" rel="nofollow" target="_blank"><u>YouTube</u></a> and like us on Facebook at <a href="https://www.globenewswire.com/Tracker?data=LTOch18I9XnaHE4qzjVzrNxzuXYwnNH0v4XrrVbCqgnXUaKX5nOdqwvgTZru4193lMFjeY16tudZKniE9EdlpQSPhDWRORO-8flQBehdsh8=" rel="nofollow" target="_blank"><u>Facebook.com/Pfizer</u></a>.</p>  <p><strong>Pfizer Disclosure Notice</strong></p>  <p>The information contained in this release is as of November 9, 2020. Pfizer assumes no obligation to update forward-looking statements contained in this release as the result of new information or future events or developments.</p>  <p>This release contains forward-looking information about Pfizer’s efforts to combat COVID-19, the collaboration between BioNTech and Pfizer to develop a potential COVID-19 vaccine, the BNT162 mRNA vaccine program, and modRNA candidate BNT162b2 (including qualitative assessments of available data, potential benefits, expectations for clinical trials, anticipated timing of clinical trial readouts and regulatory submissions and anticipated manufacturing, distribution and supply), that involves substantial risks and uncertainties that could cause actual results to differ materially from those expressed or implied by such statements. Risks and uncertainties include, among other things, the uncertainties inherent in research and development, including the ability to meet anticipated clinical endpoints, commencement and/or completion dates for clinical trials, regulatory submission dates, regulatory approval dates and/or launch dates, as well as risks associated with preliminary and interim data, (including the Phase 3 interim data that is the subject of this release), including the possibility of unfavorable new preclinical or clinical trial data and further analyses of existing preclinical or clinical trial data; the risk that clinical trial data are subject to differing interpretations and assessments, including during the peer review/publication process, in the scientific community generally, and by regulatory authorities; whether and when data from the BNT162 mRNA vaccine program will be published in scientific journal publications and, if so, when and with what modifications; whether regulatory authorities will be satisfied with the design of and results from these and future preclinical and clinical studies; whether and when any biologics license and/or emergency use authorization applications may be filed in any jurisdictions for BNT162b2 or any other potential vaccine candidates; whether and when any such applications may be approved by regulatory authorities, which will depend on myriad factors, including making …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://investors.biontech.de/news-releases/news-release-details/pfizer-and-biontech-announce-vaccine-candidate-against-covid-19?mobile=1">https://investors.biontech.de/news-releases/news-release-details/pfizer-and-biontech-announce-vaccine-candidate-against-covid-19?mobile=1</a></em></p>]]>
            </description>
            <link>https://investors.biontech.de/news-releases/news-release-details/pfizer-and-biontech-announce-vaccine-candidate-against-covid-19?mobile=1</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033852</guid>
            <pubDate>Mon, 09 Nov 2020 11:59:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why Knowledge Is The Real Wealth]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033771">thread link</a>) | @ozres1
<br/>
November 9, 2020 | https://ruizhidong.com/why-knowledge-is-ultimately-more-important-than-money/ | <a href="https://web.archive.org/web/*/https://ruizhidong.com/why-knowledge-is-ultimately-more-important-than-money/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<hr><p>The real source of wealth is not money. Money is abundant and plentiful. Fiat can be printed at will. It’s a means to an end.</p><p>Knowledge that advances civilization on the other hand is <em>scarce</em>. Great thinkers like Newton enable progress.</p><p>Consider for a moment what $1 million would have bought you 500 years ago… 100 years ago… today…</p><p>and 100 years into the future.</p><p>Being a John Rockefeller 100 years ago wouldn’t have gotten you an iPhone no matter how much money you spent.</p><p>The problem isn’t one of having enough money to organize labour and resources.</p><p><strong>The bigger problem is in <em>knowing</em> what to do. </strong></p><p>This has been the real bottleneck in unleashing human potential. Until now.</p><p><a href="https://giphy.com/gifs/5VKbvrjxpVJCM" target="_blank" rel="noopener">via GIPHY</a></p><p>AI and the advancement of thinking tools to assist in research, development and general decision making will greatly alleviate this strain and enable greater productivity.</p><div><div><div><figure><img data-attachment-id="1647" data-permalink="https://ruizhidong.com/why-knowledge-is-ultimately-more-important-than-money/naval-ravikant/" data-orig-file="https://ruizhidong.com/wp-content/uploads/2020/11/naval-ravikant.jpg" data-orig-size="250,250" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="naval-ravikant" data-image-description="" data-medium-file="https://ruizhidong.com/wp-content/uploads/2020/11/naval-ravikant.jpg" data-large-file="https://ruizhidong.com/wp-content/uploads/2020/11/naval-ravikant.jpg" width="250" height="250" src="https://cdn.shortpixel.ai/spai/q_lossless+ret_img/https://ruizhidong.com/wp-content/uploads/2020/11/naval-ravikant.jpg" data-spai-eager="1" alt="" srcset="https://cdn.shortpixel.ai/spai/q_lossless+ret_img/https://ruizhidong.com/wp-content/uploads/2020/11/naval-ravikant.jpg 250w, https://cdn.shortpixel.ai/spai/q_lossless+ret_img/https://ruizhidong.com/wp-content/uploads/2020/11/naval-ravikant-150x150.jpg 150w, https://cdn.shortpixel.ai/spai/q_lossless+ret_img/https://ruizhidong.com/wp-content/uploads/2020/11/naval-ravikant-80x80.jpg 80w" loading="lazy" sizes="(max-width: 250px) 100vw, 250px" data-old-src="data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjUwIDI1MCIgd2lkdGg9IjI1MCIgaGVpZ2h0PSIyNTAiIGRhdGEtdT0iaHR0cHMlM0ElMkYlMkZydWl6aGlkb25nLmNvbSUyRndwLWNvbnRlbnQlMkZ1cGxvYWRzJTJGMjAyMCUyRjExJTJGbmF2YWwtcmF2aWthbnQuanBnIiBkYXRhLXc9IjI1MCIgZGF0YS1oPSIyNTAiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+PC9zdmc+" data-lazy-src="https://ruizhidong.com/wp-content/uploads/2020/11/naval-ravikant.jpg?is-pending-load=1"><figcaption>Naval Ravikant</figcaption></figure></div><blockquote><p>Society, business, &amp; money are downstream of technology, which is itself downstream of science. Science applied is the engine of humanity. <br></p><p>Corollary: Applied Scientists are the most powerful people in the world. This will be more obvious in the coming years.</p><cite>— Naval Ravikant</cite></blockquote><hr></div></div><hr><h3>An example using an isolated island</h3><figure><p><span><iframe width="900" height="507" src="https://www.youtube.com/embed/9EdnEOWA9w4?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en-US&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span></p></figure>
</div></div>]]>
            </description>
            <link>https://ruizhidong.com/why-knowledge-is-ultimately-more-important-than-money/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033771</guid>
            <pubDate>Mon, 09 Nov 2020 11:45:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA['It was crazy,' says California kayaker who was engulfed in a whale's mouth]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033732">thread link</a>) | @pseudolus
<br/>
November 9, 2020 | https://www.cbc.ca/radio/asithappens/as-it-happens-thursday-edition-1.5790998/it-was-crazy-says-california-kayaker-who-was-engulfed-in-a-whale-s-mouth-1.5791001 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/radio/asithappens/as-it-happens-thursday-edition-1.5790998/it-was-crazy-says-california-kayaker-who-was-engulfed-in-a-whale-s-mouth-1.5791001">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Julie McSorley&nbsp;says she&nbsp;learned an important lesson after she and her friend were nearly swallowed by a humpback: "Whales need their space."</p><div><figure><div><p><img loading="lazy" alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5791307.1604607440!/fileImage/httpImage/image.jpeg_gen/derivatives/16x9_780/whale.jpeg"></p></div><figcaption>Sam Mcmillan of Atuscadero, Calif., was out snapping photos of humpback whales in San Luis Obispo Bay when he saw one whale breach the surface directly under a pair of kayakers. <!-- --> <!-- -->(Sam McMillan Photography)</figcaption></figure><p><span><div><div role="button" tabindex="0" title="'It was crazy,' says California kayaker who was engulfed in a whale's mouth"><div><p><img src="https://thumbnails.cbc.ca/maven_legacy/thumbnails/425/375/AsItHappens-podcast-640x360.jpg" alt=""></p><p><span>As It Happens</span><span>6:28</span><span>'It was crazy,' says California kayaker who was engulfed in a whale's mouth</span></p></div></div></div></span></p><p><span><p>Julie McSorley&nbsp;says she&nbsp;learned an important lesson after she and her friend ended up in a humpback's mouth:&nbsp;"Whales need their space."</p>  <p>McSorley and Liz Cottriel were kayaking together in California's San Luis Obispo Bay on Monday morning,&nbsp;watching the whales feed on silverfish, when one of the massive sea creatures surfaced beneath them, toppling their kayak and knocking them into the water.&nbsp;</p>  <p>Videos and photos from&nbsp;other kayakers and paddlers appear&nbsp;to show the women and their kayak&nbsp;being momentarily engulfed in the whale's mouth — though the two friends&nbsp;say it all happened too fast for them to be sure.&nbsp;</p>  <p>"It's definitely woke me up to the realization that, you know, our place is not in the feeding zone of whales," McSorley&nbsp;told <em>As It Happens</em> host Carol Off.&nbsp;</p>  <p>"We didn't think we were that close, but we definitely were right in the area that we shouldn't have been — so I've learned my lesson, big time."</p>  <ul>   <li><strong><em>The following video&nbsp;contains strong language:</em></strong></li>  </ul>  <p><span><span><iframe src="https://www.youtube.com/embed/3X2C46--2lY" frameborder="no" title="YouTube content" allowfullscreen=""></iframe></span></span></p>  <hr>  <p>Humpback&nbsp;whales have been active lately in Luis Obispo Bay near Avila Beach,&nbsp;drawing kayakers and paddlers to the area to watch them feed.</p>  <p>McSorley had already been out to watch them once, so when her friend came to town for a visit, she asked her if she wanted to go.</p>  <p>"Her reaction was, 'No, I don't like the ocean. I'm scared of sharks. I'm scared of anything I can't see in the water.' And I so ignorantly told her, 'Oh, they're never going to dump you over. The kayaks are very stable. I've never had an issue,'" McSorley said.</p>  <p>"And so she reluctantly came with me just to have a new experience."</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5791324.1604608023!/fileImage/httpImage/image.jpeg_gen/derivatives/original_300/kayaking.jpeg 300w,https://i.cbc.ca/1.5791324.1604608023!/fileImage/httpImage/image.jpeg_gen/derivatives/original_460/kayaking.jpeg 460w,https://i.cbc.ca/1.5791324.1604608023!/fileImage/httpImage/image.jpeg_gen/derivatives/original_620/kayaking.jpeg 620w,https://i.cbc.ca/1.5791324.1604608023!/fileImage/httpImage/image.jpeg_gen/derivatives/original_780/kayaking.jpeg 780w,https://i.cbc.ca/1.5791324.1604608023!/fileImage/httpImage/image.jpeg_gen/derivatives/original_1180/kayaking.jpeg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5791324.1604608023!/fileImage/httpImage/image.jpeg_gen/derivatives/original_780/kayaking.jpeg"></p></div><figcaption>This photo by Sam Mcmillan of Atuscadero, Calif., shows Liz Cottriel, right, and Julie McSorley, left, kayaking in California’s San Luis Obispo Bay.<!-- --> <!-- -->(Sam McMillan Photography)</figcaption></figure></span></p>  <p>For the first hour or so, the friends followed a pair of humpbacks as they fed. They would spot the swarms of fish — or "bait balls" — at a distance, watch the whales&nbsp;surface for a munch, wait a few minutes, and then move to the place the whales had just been.</p>  <p>They were sitting peacefully in their&nbsp;kayak waiting to see where the next bait ball&nbsp;would show up, when the little fish suddenly appeared all around them.&nbsp;</p>  <p>"So I knew it was going to be very close, but again, I'd seen whales breach right next kayaks before. So my mind was like, this is going to be, you know, super cool," McSorley said.</p>  <p>"And then all of a sudden the boat lifted up and we were dumped in the water very, very quickly."</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5791316.1604608039!/fileImage/httpImage/image.jpeg_gen/derivatives/original_300/whale.jpeg 300w,https://i.cbc.ca/1.5791316.1604608039!/fileImage/httpImage/image.jpeg_gen/derivatives/original_460/whale.jpeg 460w,https://i.cbc.ca/1.5791316.1604608039!/fileImage/httpImage/image.jpeg_gen/derivatives/original_620/whale.jpeg 620w,https://i.cbc.ca/1.5791316.1604608039!/fileImage/httpImage/image.jpeg_gen/derivatives/original_780/whale.jpeg 780w,https://i.cbc.ca/1.5791316.1604608039!/fileImage/httpImage/image.jpeg_gen/derivatives/original_1180/whale.jpeg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5791316.1604608039!/fileImage/httpImage/image.jpeg_gen/derivatives/original_780/whale.jpeg"></p></div><figcaption>This shot by Sam Mcmillan of Atuscadero, Calif., shows a kayak paddle sticking out of the whale's mouth after it surfaced between Cottriel and McSorley. <!-- --> <!-- -->(Sam McMillan Photography)</figcaption></figure></span></p>  <p>Cottriel could see the inside of the whale's mouth coming down on them, but mistook it at the time for its belly. Panicked and confused,&nbsp;she threw up her hand to stop it.&nbsp;</p>  <p>"I'm thinking to myself, 'I'm going to&nbsp;push. Like, I'm going to push a whale out of the way. It was the weirdest thought. I'm thinking, 'I'm dead. I'm dead.' I thought it was going&nbsp;land on me," Cottriel&nbsp;<a href="https://kmph.com/news/local/kayakers-get-knocked-over-by-humpback-whales-at-avila-beach">told the local Fox News affiliate</a>.</p>  <p>"Next thing I know, I'm under water."</p>  <p>McSorley says it all happened so fast that the only thing she remembers is feeling the boat rise, and then finding herself beneath the surface.&nbsp;</p>  <p>"Once we were in the water, we didn't know where we were — if we were under the whale, if we were sucked down with the whales," she said.</p>  <p>"So both of us ... ended up popping up right next to the kayak and next to each other. It was crazy."</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5791336.1604612953!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/liz-cottriel-and-julie-mcsorley.jpg 300w,https://i.cbc.ca/1.5791336.1604612953!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/liz-cottriel-and-julie-mcsorley.jpg 460w,https://i.cbc.ca/1.5791336.1604612953!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/liz-cottriel-and-julie-mcsorley.jpg 620w,https://i.cbc.ca/1.5791336.1604612953!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/liz-cottriel-and-julie-mcsorley.jpg 780w,https://i.cbc.ca/1.5791336.1604612953!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/liz-cottriel-and-julie-mcsorley.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5791336.1604612953!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/liz-cottriel-and-julie-mcsorley.jpg"></p></div><figcaption>Cottriel, left, and McSorley, right.<!-- --> <!-- -->(Submitted by Julie McSorley and Liz Cottriel)</figcaption></figure></span></p>  <p>Sam Mcmillan&nbsp;was nearby&nbsp;taking photos of the whales at the time. He told <em>As It Happens</em> that he just knew he was going to get a good shot&nbsp;when he saw the size of the bait ball.&nbsp;</p>  <p>But it wasn't until he heard people shouting "Are you OK?" that he realized there were two&nbsp;people and a kayak mixed in with the fish and the whale.</p>  <p>"I&nbsp;checked on Julie and Liz to make sure they were OK, but it wasn't until I got home and saw the photos that I had taken, where you can see that they were right in the whale's mouth, that I realized just what had happened," he said.&nbsp;</p>  <p>McSorley&nbsp;says she and Cottriel didn't realize it either, until other kayakers came to their rescue.</p>  <p>"They were telling us, 'You were in the mouth, you were in the whale's&nbsp;mouth!"&nbsp;McSorley said. "But we didn't have any idea at that time. And it didn't really hit us until we watched the video later."</p>    <p>McSorley, meanwhile, says she won't be kayaking again&nbsp;when the whales are out unless she can keep a football field's distance from the creatures.</p>  <p>"I'll definitely kayak in the ocean by dolphins and otters and seals and all the others," she said. "But I think the whales need their space."</p>  <hr>  <p><em>Written by Sheena Goodyear. Interview produced by Sarah Cooper.&nbsp;</em></p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/radio/asithappens/as-it-happens-thursday-edition-1.5790998/it-was-crazy-says-california-kayaker-who-was-engulfed-in-a-whale-s-mouth-1.5791001</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033732</guid>
            <pubDate>Mon, 09 Nov 2020 11:40:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Extract full news article content from any RSS feed using Extract API]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033704">thread link</a>) | @imshashank
<br/>
November 9, 2020 | https://pipfeed.com/2020/11/09/tutorial-extract-full-news-article-content-from-any-rss-feed-using-extract-api/ | <a href="https://web.archive.org/web/*/https://pipfeed.com/2020/11/09/tutorial-extract-full-news-article-content-from-any-rss-feed-using-extract-api/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div itemprop="articleBody"><p>Learn how to extract all fields from any RSS feed or given a list of URLs. For this example, we will be using Medium’s RSS feed. The code will be in python but can easily be adapted for other languages.</p><p>Lets start by importing the packages. We will be using “feedparser” to extract Medium Rss feed.</p><pre><code lang="bash">pip install feedparser
pip install requests</code></pre><p>Let’s begin by first extracting links from the RSS feed. For this example, we will be extracting the articles from “Towards Data Science”. “Towards Data Science” is one of the leading blogs when it comes to Data Science, Machine Learning &amp; Artificial Intelligence.</p><pre><code lang="python">import feedparser

NewsFeed = feedparser.parse("https://towardsdatascience.com/feed")
print("Total entries found in feed: "+ str(len(NewsFeed.entries)) +"\n")
i =0
for entry in NewsFeed.entries:
print(str(i) + ": Got url: " + entry.link)
i = i +1</code></pre><p>We are able to extract the links, now we want to extract the entire content, summary, metadata and other details for each news article in the feed.</p><p>To extract we will be using Pipfeed’s extract API: <a href="https://promptapi.com/marketplace/description/pipfeed-api" rel="nofollow noopener external noreferrer" target="_blank" data-wpel-link="external">https://promptapi.com/marketplace/description/pipfeed-api</a> You can get a free API key from prompt API.</p><pre><code lang="python">import requests

url = "https://api.promptapi.com/pipfeed"

payload = "https://towardsdatascience.com/topic-model-evaluation-3c43e2308526"
headers= {
  "apikey": "YOUR_API_KEY"
}

response = requests.request("POST", url, headers=headers, data = payload)

status_code = response.status_code
result = response.text
print(result)</code></pre><p>The above code will extract the given URL and return all the fields. Below is the response we get for the above code. DO NOT forget to replace the API key with your own API keys generated from prompt API.</p><p>“Summary” &amp; “predictedCategories” are generated using Pipfeed’s AI models. Rest of the fields are extracted from the article HTML itself.</p><pre><code lang="json">{
"publishedAt": "2020-11-09T05:15:23.001Z",
"title": "Topic Model Evaluation",
"authors": [
"Giri Rabindranath"
],
"description": "Evaluation is the key to understanding topic models - This article explains what topic model evaluation is, why it's important and how to do it",
"language": "en",
"url": "https://towardsdatascience.com/topic-model-evaluation-3c43e2308526",
"mainImage": "https://miro.medium.com/max/1200/1*wvlqQPpOHFK7xQ1XOhe6xg.jpeg",
"category": "machine-learning",
"categories": null,
"predictedCategories": [
"machine-learning",
"data-science",
"programming"
],
"tags": [],
"keywords": [
"coherence",
"evaluation",
"human",
"model",
"models",
"topic",
"topics",
"way",
"word",
"words"
],
"summary": "In this article, we\u2019ll look at topic model evaluation, what it is and how to do it.\nWhat is topic model evaluation?\nTopic model evaluation is the process of assessing how well a topic model does what it is designed for.\nThis is why topic model evaluation matters.\nHow to evaluate topic models \u2014 RecapThis article has hopefully made one thing clear \u2014 topic model evaluation isn\u2019t easy!",
"images": [
"https://miro.medium.com/fit/c/140/140/1*74Yrxu8s4sOtTECtixv9Fg.jpeg",
"https://miro.medium.com/max/60/1*<a href="https://pipfeed.com/cdn-cgi/l/email-protection" data-cfemail="84ccd2d4cddecab7d1eecaf3cec1b3dec7e1fee0dcd0c5c4b6fcaaeef4e1e3">[email&nbsp;protected]</a>?q=20",
"https://miro.medium.com/fit/c/140/140/0*l_zfjU9IKMa47tfy",
"https://miro.medium.com/fit/c/56/56/2*b2y5uCYazQ9FgiUQEUHT6Q.jpeg",
"https://miro.medium.com/max/60/1*mpyrgqwMjfclV2oN1U2VIA.jpeg?q=20",
"https://miro.medium.com/max/698/1*E4oPMmq5jTKuStZJuyDGpw.jpeg",
"https://miro.medium.com/max/12032/1*wvlqQPpOHFK7xQ1XOhe6xg.jpeg",
"https://miro.medium.com/max/60/1*_MXaw5BKgIsm8J3dOUNHMg.jpeg?q=20",
"https://miro.medium.com/max/224/1*AGyTPCaRzVqL77kFwUwHKg.png",
"https://miro.medium.com/max/270/1*W_RAPQ62h0em559zluJLdQ.png",
"https://miro.medium.com/max/60/1*E4oPMmq5jTKuStZJuyDGpw.jpeg?q=20",
"https://miro.medium.com/max/1200/1*wvlqQPpOHFK7xQ1XOhe6xg.jpeg",
"https://miro.medium.com/max/60/0*aP8H1qpRN_OR1x5r?q=20",
"https://miro.medium.com/max/60/0*NIpOoYo9iHt4lMbg?q=20",
"https://miro.medium.com/max/60/0*l_zfjU9IKMa47tfy?q=20",
"https://miro.medium.com/max/270/1*Crl55Tm6yDNMoucPo1tvDg.png",
"https://miro.medium.com/max/784/1*_MXaw5BKgIsm8J3dOUNHMg.jpeg",
"https://miro.medium.com/fit/c/140/140/1*FTG-junI6KJzojC_xRVNXg.png",
"https://miro.medium.com/max/60/0*fG5RLd48iOZezB_y.jpeg?q=20",
"https://miro.medium.com/fit/c/140/140/0*NIpOoYo9iHt4lMbg",
"https://miro.medium.com/fit/c/140/140/1*<a href="https://pipfeed.com/cdn-cgi/l/email-protection" data-cfemail="145c42445d4e5a27417e5a635e51234e57716e704c405554266c3a7e647173">[email&nbsp;protected]</a>",
"https://miro.medium.com/max/60/1*wvlqQPpOHFK7xQ1XOhe6xg.jpeg?q=20",
"https://miro.medium.com/fit/c/140/140/1*mpyrgqwMjfclV2oN1U2VIA.jpeg",
"https://miro.medium.com/fit/c/140/140/0*fG5RLd48iOZezB_y.jpeg",
"https://miro.medium.com/fit/c/140/140/0*aP8H1qpRN_OR1x5r",
"https://miro.medium.com/max/60/1*74Yrxu8s4sOtTECtixv9Fg.jpeg?q=20",
"https://miro.medium.com/max/60/1*FTG-junI6KJzojC_xRVNXg.png?q=20"
],
"blogName": null,
"blogLogoUrl": null,
"html": "&lt;div class=\"page\" id=\"readability-page-1\"&gt;&lt;section&gt;&lt;div&gt;&lt;div&gt;&lt;h2 id=\"ef6b\"&gt;DATA SCIENCE EXPLAINED&lt;/h2&gt;&lt;h2 id=\"e375\"&gt;Here\u2019s what you need to know about evaluating topic models&lt;/h2&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;a rel=\"noopener\" href=\"https://medium.com/@g_rabi?source=post_page-----3c43e2308526--------------------------------\"&gt;&lt;div&gt;&lt;p&gt;&lt;img height=\"28\" width=\"28\" src=\"https://miro.medium.com/fit/c/56/56/2*b2y5uCYazQ9FgiUQEUHT6Q.jpeg\" alt=\"Giri Rabindranath\"&gt;&lt;/p&gt;&lt;/div&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;p id=\"8bff\"&gt;&lt;em&gt;Topic models are widely used for analyzing unstructured text data, but they provide no guidance on the quality of topics produced. Evaluation is the key to understanding topic models. In this article, we\u2019ll look at what topic model evaluation is, why it\u2019s important and how to do it.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div&gt;&lt;h2 id=\"324c\"&gt;Contents&lt;/h2&gt;&lt;ul&gt;&lt;li id=\"dd12\"&gt;&lt;a rel=\"noopener\" href=\"#f0ce\"&gt;&lt;em&gt;What is topic model evaluation&lt;/em&gt;&lt;/a&gt;?&lt;/li&gt;&lt;li id=\"ceba\"&gt;&lt;a rel=\"noopener\" href=\"#d1ae\"&gt;&lt;em&gt;How to evaluate topic models&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=\"ea5d\"&gt;&lt;a rel=\"noopener\" href=\"#2932\"&gt;&lt;em&gt;Evaluating topic models \u2014 Human judgment&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=\"6275\"&gt;&lt;a rel=\"noopener\" href=\"#9b50\"&gt;&lt;em&gt;Evaluating topic models \u2014 Quantitative metrics&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=\"ea38\"&gt;&lt;a rel=\"noopener\" href=\"#19ff\"&gt;&lt;em&gt;Calculating coherence using Gensim in Python&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=\"95a3\"&gt;&lt;a rel=\"noopener\" href=\"#1756\"&gt;&lt;em&gt;Limitations of coherence&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=\"251a\"&gt;&lt;a rel=\"noopener\" href=\"#63c4\"&gt;&lt;em&gt;How to evaluate topic models \u2014 Recap&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=\"e448\"&gt;&lt;a rel=\"noopener\" href=\"#31aa\"&gt;&lt;em&gt;Conclusion&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p id=\"6f84\"&gt;Topic modeling is a branch of &lt;a rel=\"noopener nofollow\" href=\"https://highdemandskills.com/natural-language-processing-explained-simply/\"&gt;natural language processing&lt;/a&gt; that\u2019s used for exploring text data. It works by identifying key themes \u2014 or topics \u2014 based on the words or phrases in the data that have a similar meaning. Its versatility and ease-of-use have led to a variety of applications.&lt;/p&gt;&lt;p id=\"3772\"&gt;Be&lt;span id=\"rmm\"&gt;i&lt;/span&gt;ng a form of unsupervised learning, topic modeling is useful when annotated or labeled data isn\u2019t available. This is helpful, as the majority of emerging text data isn\u2019t labeled, and labeling is time-consuming and expensive to do.&lt;/p&gt;&lt;p id=\"030c\"&gt;For an easy-to-follow, intuitive explanation of topic modeling and its applications, see &lt;a rel=\"noopener nofollow\" href=\"https://highdemandskills.com/topic-modeling-intuitive/\"&gt;this article&lt;/a&gt;.&lt;/p&gt;&lt;p id=\"fb4a\"&gt;One of the shortcomings of topic modeling is that there\u2019s no guidance about the quality of topics produced. If you want to learn about how meaningful the topics are, you\u2019ll need to evaluate the topic model.&lt;/p&gt;&lt;p id=\"b937\"&gt;In this article, we\u2019ll look at topic model evaluation, what it is and how to do it. It\u2019s an important part of the topic modeling process that sometimes gets overlooked. For a topic model to be truly useful, some sort of evaluation is needed to understand how relevant the topics are for the purpose of the model.&lt;/p&gt;&lt;p id=\"b85d\"&gt;Topic model evaluation is the process of assessing how well a topic model does what it is designed for.&lt;/p&gt;&lt;p id=\"44ee\"&gt;When you run a topic model, you usually do it with a specific purpose in mind. It may be for document classification, to explore a set of unstructured texts, or some other analysis. As with any model, if you wish to know how effective it is at doing what it\u2019s designed for, you\u2019ll need to evaluate it. This is why topic model evaluation matters.&lt;/p&gt;&lt;p id=\"e9c9\"&gt;Evaluating a topic model can help you decide if the model has captured the internal structure of a corpus (a collection of text documents). This can be particularly useful in tasks like e-discovery, where the effectiveness of a topic model can have implications for legal proceedings or other important matters.&lt;/p&gt;&lt;p id=\"a51a\"&gt;More generally, topic model evaluation can help you answer questions like:&lt;/p&gt;&lt;ul&gt;&lt;li id=\"b7ef\"&gt;Are the identified topics understandable?&lt;/li&gt;&lt;li id=\"1d2d\"&gt;Are the topics coherent?&lt;/li&gt;&lt;li id=\"325e\"&gt;Does the topic model serve the purpose it is being used for?&lt;/li&gt;&lt;/ul&gt;&lt;p id=\"da03\"&gt;Without some form of evaluation, you won\u2019t know how well your topic model is performing or if it\u2019s being used properly.&lt;/p&gt;&lt;p id=\"c559\"&gt;Evaluating a topic model isn\u2019t always easy, however.&lt;/p&gt;&lt;p id=\"3adc\"&gt;If a topic model is used for a measurable task, such as classification, then its effectiveness is relatively straightforward to calculate (eg. measure the proportion of successful classifications). But if the model is used for a more qualitative task, such as exploring the semantic themes in an unstructured corpus, then evaluation is more difficult.&lt;/p&gt;&lt;p id=\"ff58\"&gt;In this article, we\u2019ll focus on evaluating topic models that do not have clearly measurable outcomes. These include topic models used for document exploration, content recommendation and e-discovery, amongst other use cases.&lt;/p&gt;&lt;p id=\"dc38\"&gt;E…</code></pre></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pipfeed.com/2020/11/09/tutorial-extract-full-news-article-content-from-any-rss-feed-using-extract-api/">https://pipfeed.com/2020/11/09/tutorial-extract-full-news-article-content-from-any-rss-feed-using-extract-api/</a></em></p>]]>
            </description>
            <link>https://pipfeed.com/2020/11/09/tutorial-extract-full-news-article-content-from-any-rss-feed-using-extract-api/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033704</guid>
            <pubDate>Mon, 09 Nov 2020 11:36:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Understanding Representation Learning with Autoencoder]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033673">thread link</a>) | @patrycjaneptune
<br/>
November 9, 2020 | https://neptune.ai/blog/understanding-representation-learning-with-autoencoder-everything-you-need-to-know-about-representation-and-feature-learning | <a href="https://web.archive.org/web/*/https://neptune.ai/blog/understanding-representation-learning-with-autoencoder-everything-you-need-to-know-about-representation-and-feature-learning">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                    <article class="page">
	
<p>Machine learning is a subfield of Artificial Intelligence, where we try to build intelligent systems that have the function and behavior of our brain. Through ML, we try to build machines that can compute, extract patterns, automate routine tasks, diagnose biological anomalies, and prove scientific theories and hypotheses.</p>



<p>Because machine learning is a subset of AI, it doesn’t rely on hard-coded algorithms to find its way to the core solution, but it strengthens AI with the idea that it can extract knowledge from given information, and finds its way to the core idea without being hardcoded.&nbsp;</p>



<blockquote><p><strong>With the advent of machine learning, we can now design algorithms that can “tackle problems involving knowledge of the real world and make decisions that appear subjective.”</strong> </p><p>(Ian Goodfellow, “Deep Learning”)</p></blockquote>



<p>Data plays a key role in all machine learning problems.</p>



<p>Why is it so important?&nbsp;</p>



<p>Data is a discrete arrangement of information that offers a continuous series of events. In this arrangement, the whole patterns of representation are hidden. If a machine can extract patterns that represent a particular event, we can say that the machine has learned that information, and if new data or information is fed into it then it can provide appropriate solutions and predictions.</p>






<h2>Representation Learning</h2>



<p>Imagine an engineer designing an ML algorithm to predict malignant cells based on brain scans. To design the algorithm, the engineer has to rely heavily on patient data, because that’s where all the answers are.&nbsp;</p>



<p>Each observation or feature in that data describes the attributes of the patient. The machine learning algorithm that predicts the outcome has to learn how each feature correlates with the different outcomes: benign or malignant.</p>



<p>So in case of any noise or discrepancies in the data, the outcome can be totally different, which is the problem with most machine learning algorithms. Most machine learning algorithms have a superficial understanding of the data.&nbsp;</p>



<p>So what is the solution?</p>



<p><strong>Provide the machine with a more abstract representation of the data.&nbsp;</strong></p>



<p>For many tasks, it is impossible to know what features should be extracted. Alan Turing and his colleagues deciphering the enigma code observed the patterns that were regularly appearing in the messages. This is where the<a href="https://opensource.com/article/17/9/representation-learning" target="_blank" rel="noreferrer noopener nofollow"> idea of representation learning </a>truly comes into view.&nbsp;</p>



<p>In representation learning, the machine is provided with data and it learns the representation by itself. It’s a method of finding a representation of the data – the features, the distance function, the similarity function– that dictates how the predictive model will perform.&nbsp;</p>



<p>Representation learning works by reducing high-dimensional data into low-dimensional data, making it easier to find patterns, anomalies, and also giving us a better understanding of the behavior of the data altogether.&nbsp;</p>



<p>It also reduces the complexity of the data, so the anomalies and noise are reduced. This reduction in noise can be very useful for supervised learning algorithms.&nbsp;</p>






<h3><strong>Invariance and disentangling</strong></h3>



<p>The problem with representation learning is that it’s very difficult to get representations that can solve a given problem. Luckily, deep learning and deep neural networks started to prove very much goal-oriented and efficient.&nbsp;</p>



<p>The idea that deep neural networks can build complex concepts out of simple concepts lies at the core of deep learning [Ian Goodfellow, “Deep learning”].&nbsp;</p>



<p><strong>So where does the idea of representation learning fit in?</strong></p>



<p>People try to understand the success of deep learning. The two main conclusions are<a href="https://opensource.com/article/17/9/representation-learning" target="_blank" rel="noreferrer noopener nofollow"> <strong>representation learning </strong></a>and<strong> optimization</strong>.&nbsp;</p>



<p>Deep learning is often seen as a black box, where a finite number of functions are used to find parameters that yield good generalization. This is achieved by optimization where the algorithm tries to correct itself by evaluating the model output with the ground truth.&nbsp;</p>



<hr>



<p><strong><sup>EDITOR’S NOTE<br></sup></strong>Check also: <a href="https://neptune.ai/blog/the-ultimate-guide-to-evaluation-and-selection-of-models-in-machine-learning" target="_blank" rel="noreferrer noopener nofollow">The Ultimate Guide to Evaluation and Selection of Models in Machine Learning</a></p>



<hr>



<p>This process is done until the optimization function reaches a minimum point called the global minima. Most deep learning networks are heavily over-parameterized and suggest that they may overfit – where the algorithm performs well on training data but fails to perform well in new data. Recent work suggests that this is related to properties of the loss landscape, and to the implicit regularization performed by stochastic gradient descent (SGD), but the overall output is still noisy [Zhang et al., 2017].</p>



<p>Representation learning, on the other hand, focuses on the properties of the representation learned by the layers of the network (the activations) while remaining largely agnostic to the particular optimization process used [Emergence of Invariance and Disentanglement in Deep Representations, 2018].&nbsp;</p>



<p>Two major factors that usually occur in any data distribution are <strong>variance</strong> and <strong>entanglement</strong>. These two factors need to be eliminated to get a good representation from the data. Variance in the data can also be considered the sensitivity, and these sensitivities can turn the outcome upside down. Any model that we build has to be robust to variance, i.e. it has to be invariant because this can greatly harm the outcome of a deep learning model.</p>



<p>Entanglement is the way a vector in the data is connected or correlated to other vectors in the data. These connections make the data very complex and hard to decipher. What we are supposed to do is look for variables where the relationship is simple. It’s an easy way to transform high dimensional data into low dimensional data, or to transform high-dimensional data in a way that can be easily separated.&nbsp;</p>



<p>From the previous section, we learned that the ability of representation learning is that it learns abstract patterns that make sense to the data, while deep learning is often ascribed the ability of deep networks to learn representations that are invariant (insensitive) to nuisance such as translations, rotations, occlusions, and also “disentangled”,&nbsp;or separating factors in the high-dimensional space of data [Bengio, 2009]. But it is still important to learn to simplify a complex arrangement of data by creating models that are invariant and untangled.</p>



<blockquote><p>“If neither the architecture nor the loss function explicitly enforce invariance and disentangling, how can these properties emerge consistently in deep networks trained by simple generic optimization?”</p></blockquote>



<p>It turns out that we can answer this question by showing two things:&nbsp;</p>



<ol><li>Using classical notions of statistical decision and information theory, we show that invariance in a deep neural network is equivalent to the minimum of the representation it computes, and can be achieved by <strong>stacking layers</strong> and injecting noise in the computation, under realistic and empirically validated assumptions.</li><li>Using an Information Decomposition of the empirical loss, we show that overfitting can be reduced by <strong>limiting the information content stored in the weights</strong>. [Emergence of Invariance and Disentanglement in Deep Representations, 2018].</li></ol>






<h3><strong>The Information Bottleneck</strong></h3>



<p>Information Bottleneck (IB) was introduced by Tishby et al. (1999). It was introduced with a hypothesis that it can extract relevant information by compressing the amount of information that can traverse the full network, forcing a learned compression of the input data.&nbsp;</p>



<p>This compressed representation not only reduces dimensions but along with it reduces the complexity of the data as well [Deep Learning and the Information Bottleneck Principle, Tishby 2015]. The idea is that a network rids noisy input data of extraneous details as if by squeezing the information through a bottleneck, leaving only the features most relevant to general concepts.&nbsp;</p>



<div><figure><img loading="lazy" src="https://i1.wp.com/neptune.ai/wp-content/uploads/information-bottleneck.png?resize=512%2C384&amp;ssl=1" alt="information bottleneck" width="512" height="384" srcset="https://i1.wp.com/neptune.ai/wp-content/uploads/information-bottleneck.png?resize=1024%2C768&amp;ssl=1 1024w, https://i1.wp.com/neptune.ai/wp-content/uploads/information-bottleneck.png?resize=300%2C225&amp;ssl=1 300w, https://i1.wp.com/neptune.ai/wp-content/uploads/information-bottleneck.png?resize=768%2C576&amp;ssl=1 768w, https://i1.wp.com/neptune.ai/wp-content/uploads/information-bottleneck.png?resize=1536%2C1152&amp;ssl=1 1536w, https://i1.wp.com/neptune.ai/wp-content/uploads/information-bottleneck.png?w=1600&amp;ssl=1 1600w" sizes="(max-width: 512px) 100vw, 512px" data-recalc-dims="1"></figure></div>



<p>Tishby’s findings have the AI community buzzing. “I believe that the information bottleneck idea could be very important in future deep neural network research”, said Alex Alemi of Google Research, who has already developed new approximation methods for applying an information bottleneck analysis to large deep neural networks. The bottleneck could serve “not only as a theoretical tool for understanding why our neural networks work as well as they do currently but also as a tool for constructing new objectives and architectures of networks,” Alemi said.</p>






<h3><strong>Latent variables</strong></h3>



<p>A latent variable is a random variable that cannot be observed directly, but it lays the foundation of how the data is distributed. Latent variables also give us a low-level representation of high-dimensional data. They give us an abstract representation of how the data is distributed.&nbsp;</p>



<div><figure><img loading="lazy" width="495" height="200" src="https://i0.wp.com/neptune.ai/wp-content/uploads/latent-variables.png?resize=495%2C200&amp;ssl=1" alt="latent variables" srcset="https://i0.wp.com/neptune.ai/wp-content/uploads/latent-variables.png?w=495&amp;ssl=1 495w, https://i0.wp.com/neptune.ai/wp-content/uploads/latent-variables.png?resize=300%2C121&amp;ssl=1 300w" sizes="(max-width: 495px) 100vw, 495px" data-recalc-dims="1"></figure></div>



<p>So why do we need latent variables?</p>



<p>All machine learning has a definite problem of learning complicated probability distribution <em>p(x)</em>. And these distributions are <strong>constrained</strong>, with only a limited set of high-dimensional data points <em>x</em> drawn from this distribution.&nbsp;</p>



<p>For example, to learn the probability distribution over images of cats we need to define a <strong>distribution that</strong> can model complex correlations between all pixels which form each image. Modelling this distribution directly is a tedious and challenging task, even unfeasible infinite time. Instead of modelling <em>p(x) </em>directly, we can introduce an (unobserved) latent variable z and define a conditional distribution <em>p(x | z)</em> for the data, which is called a likelihood. In probabilistic terms, <em>z</em> can be interpreted as a continuous random variable. For the example of cat images, z could contain a hidden representation of the type of cat, its color, or shape.</p>



<p>Having z, we can further introduce a<strong> prior distribution</strong> <em>p(z</em>) over the latent variables to compute the joint distribution over observed and latent variables <em>p(x,z) = p(x|z)p(z)</em>.&nbsp;</p>



<p>To obtain the data distribution <em>p(x) </em>we need to marginalize over the latent variables.&nbsp;</p>



<div><figure><img src="https://latex.codecogs.com/gif.latex?%5Cdpi%7B120%7D%20%5Clarge%20p%28x%29%20%3D%20%5Cint%20p%28x%2Cz%29dz%20%3D%20%5Cint%20p%28x%7Cz%29p%28z%29dz" alt="" title="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program."></figure></div>



<p>Prior to that we can compute <strong>posterior …</strong></p></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://neptune.ai/blog/understanding-representation-learning-with-autoencoder-everything-you-need-to-know-about-representation-and-feature-learning">https://neptune.ai/blog/understanding-representation-learning-with-autoencoder-everything-you-need-to-know-about-representation-and-feature-learning</a></em></p>]]>
            </description>
            <link>https://neptune.ai/blog/understanding-representation-learning-with-autoencoder-everything-you-need-to-know-about-representation-and-feature-learning</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033673</guid>
            <pubDate>Mon, 09 Nov 2020 11:32:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Brief History of Python]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033660">thread link</a>) | @JKooll
<br/>
November 9, 2020 | https://bestproductsreviews.com.au/a-brief-history-of-python/ | <a href="https://web.archive.org/web/*/https://bestproductsreviews.com.au/a-brief-history-of-python/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div><ul><li><a data-class="popup" data-network="facebook" href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fbestproductsreviews.com.au%2Fa-brief-history-of-python%2F" target="_blank" rel="nofollow"><span></span> </a></li><li><a data-class="popup" data-network="twitter" href="https://twitter.com/intent/tweet?text=A%20Brief%20History%20of%20Python&amp;url=https%3A%2F%2Fbestproductsreviews.com.au%2Fa-brief-history-of-python%2F" target="_blank" rel="nofollow"><span></span> </a></li><li><a data-class="popup" data-network="google-plus" href="https://plus.google.com/share?url=https%3A%2F%2Fbestproductsreviews.com.au%2Fa-brief-history-of-python%2F" target="_blank" rel="nofollow"><span></span> </a></li><li><a data-class="popup" data-network="pinterest" href="https://pinterest.com/pin/create/button/?url=https%3A%2F%2Fbestproductsreviews.com.au%2Fa-brief-history-of-python%2F&amp;description=Python+is+my+favourite+language%2C+concise%2C+beautiful%2C+and+easy+to+use.%26nbsp%3BTwo+days+ago%2C+I+was+very+excited+to+promote+the+benefits+of+Python+to+my+friends.+After+listening+to+it%2C+my+friend+asked+me%3A+Okay%2C+I+admit+that+Python+is+good%2C+but+why+is+it+called+Python%3F+I%27m+not+sure%3A+Well%2C+it+seems+to+be+the+name+of+a+TV+show.+The+friend+asked+again%3A+Is+Guido+you+mentioned+as+an+American%3F%26nbsp%3B%28Guido+von+Rossum%2C+author+of+Python%29+I%27m+not+sure+again%3A+he+switched+from+Google+to+work+in+Dropbox%2C+but+his+name+is+Dutch+%28with+a+von+in+the+middle%29.&amp;media=https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06121511-82e43957fefe4c13ac06bd02a5e9d97a.png" target="_blank" rel="nofollow"><span></span> </a></li><li><a data-class="popup" data-network="fintel" href="https://fintel.io/share?url=https%3A%2F%2Fbestproductsreviews.com.au%2Fa-brief-history-of-python%2F" target="_blank" rel="nofollow"><span></span></a></li></ul>
<p><a href="https://bestproductsreviews.com.au/awesome-python-books/">Python</a> is my favourite language, concise, beautiful, and easy to use.&nbsp;Two days ago, I was very excited to promote the benefits of Python to my friends.</p>
<p>After listening to it, my friend asked me: Okay, I admit that Python is good, but why is it called Python?</p>
<p>I’m not sure: Well, it seems to be the name of a TV show.</p>
<p>The friend asked again: Is Guido you mentioned as an American?&nbsp;(Guido von Rossum, author of Python)</p>
<p>I’m not sure again: he switched from Google to work in Dropbox, but his name is Dutch (with a von in the middle).</p>
<p>So, I spent some time investigating the history of Python later.&nbsp;This is good learning.&nbsp;I have seen the source of many functions in Python and the design philosophy of Python, such as which functions are leftover from history, which functions are repetitive, how to add functions… Moreover, Python is also a successful case of the open-source movement.&nbsp;From the history of Python, we can get a glimpse of the concepts and achievements of open source development.</p>
<h2>The origin of Python</h2>
<p>The author of Python, Guido von Rossum, is indeed Dutch.&nbsp;In 1982, Guido received a master’s degree in mathematics and computer science from the University of Amsterdam.&nbsp;However, although he can be regarded as a mathematician, he enjoys the fun of computers even more.&nbsp;In his words, although he possesses both mathematics and computer qualifications, he always tends to do computer-related work and is keen to do any programming-related work.</p>
<div><figure><img width="400" height="600" src="https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06100633-c2ce8755002945df846b5dad1dc25cdd.jpg" alt="python, Best Products Reviews - Tita" srcset="https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06100633-c2ce8755002945df846b5dad1dc25cdd.jpg 400w, https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06100633-c2ce8755002945df846b5dad1dc25cdd-200x300.jpg 200w" sizes="(max-width: 400px) 100vw, 400px" title="A Brief History of Python 2" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20400%20600'%3E%3C/svg%3E" data-lazy-srcset="https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06100633-c2ce8755002945df846b5dad1dc25cdd.jpg 400w, https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06100633-c2ce8755002945df846b5dad1dc25cdd-200x300.jpg 200w" data-lazy-src="https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06100633-c2ce8755002945df846b5dad1dc25cdd.jpg"><figcaption>Guido von Rossum</figcaption></figure></div>
<p>At that time, he was exposed to and used languages ​​such as Pascal, C, Fortran, etc.&nbsp;The basic design principle of these languages ​​is to make machines run faster.&nbsp;In the 1980s, although IBM and Apple had set off a wave of personal computers, the configuration of these personal computers was very low (in today’s view).&nbsp;For example, in the early Macintosh, only 8MHz CPU frequency and 128KB RAM, a large array can fill the memory.&nbsp;The core of all compilers is to optimize so that the program can run.&nbsp;In order to increase efficiency, language also forces programmers to think like computers so that they can write programs that are more in line with machine tastes.&nbsp;In that era, programmers could not wait to squeeze every inch of computer power with their hands.&nbsp;Some people even think that C language pointers are a waste of memory.&nbsp;As for dynamic typing, automatic memory management, object-oriented… Don’t think about it, it will paralyze your computer.</p>
<p>However, this way of thinking makes Guido feel distressed.&nbsp;Guido knows how to write a function in C language, but the whole writing process takes a lot of time (even if he already knows exactly how to implement it).&nbsp;His other option is the shell.&nbsp;Bourne Shell has long existed as an interpreter for UNIX systems.&nbsp;UNIX administrators often use shells to write simple scripts to perform some system maintenance tasks, such as regular backups, file system management, and so on.&nbsp;The shell can be like glue, connecting many functions under UNIX together.&nbsp;Many programs with hundreds of lines in the C language can be completed in just a few lines in the shell.&nbsp;However, the essence of the shell is to invoke commands.&nbsp;It is not a real language.&nbsp;For example, the shell has no numeric data types, and addition operations are very complicated.&nbsp;In short, the shell cannot fully mobilize the computer’s functions.</p>
<p>(For shell, you can refer to&nbsp;<a aria-label=" (opens in a new tab)" href="https://amzn.to/3khj35T" target="_blank" rel="noreferrer noopener nofollow">How Linux Works</a>&nbsp;and&nbsp;<a aria-label=" (opens in a new tab)" href="https://amzn.to/35cJzJK" target="_blank" rel="noreferrer noopener nofollow">The Linux Command Line</a>&nbsp;)</p>
<p>Guido hopes that there is a language that can fully call the computer’s functional interfaces like C language, and can be easily programmed like a shell.&nbsp;ABC language gave Guido hope.&nbsp;ABC was developed by CWI (&nbsp;Centrum Wiskunde &amp; Informatica, Institute of Mathematics and Computers) in the&nbsp;Netherlands.&nbsp;Guido works at CWI and participated in the development of the ABC language.&nbsp;ABC language is for the purpose of teaching.&nbsp;Unlike most languages ​​at the time, the goal of ABC language was to&nbsp;“make users feel better.&nbsp;”&nbsp;ABC language hopes to make the language&nbsp;easy to read, easy to use, easy to remember, easy to learn&nbsp;and to stimulate people’s interest in learning programming.&nbsp;For example, the following is an ABC program from Wikipedia, this program is used to count the total number of words that appear in the text:</p>
<pre><code>HOW TO RETURN words document:
   PUT {} IN collection
   FOR line IN document:
      FOR word IN split line:
         IF word not.in collection:
            INSERT word IN collection
   RETURN collection</code></pre>
<p>HOW TO is used to define a function.&nbsp;A Python programmer should easily understand this program.&nbsp;ABC language uses colons (:) and indentation to indicate program blocks (C language uses {} to indicate program blocks).&nbsp;There is no semicolon at the end of the line.&nbsp;There are no parentheses () in the for and if structures.&nbsp;If you change HOW TO to def, change the PUT line to collection = [] and change the INSERT line to collection.append(word), this is almost a standard Python function.&nbsp;The above function reads like a natural text.</p>
<p>Despite having good readability and ease of use, the ABC language did not become popular in the end.&nbsp;At that time, the ABC language compiler needed a relatively high-end computer to run.&nbsp;The users of these computers are usually proficient in computers, and they consider the efficiency of the program more than the difficulty of learning.&nbsp;In addition to hardware difficulties, the design of the ABC language also has some fatal problems:</p>
<ul><li><strong>Poor scalability</strong>.&nbsp;The ABC language is not a modular language.&nbsp;If you want to add features to the ABC language, such as graphical support, you must change many places.</li><li><strong>Cannot directly perform IO</strong>.&nbsp;The ABC language cannot directly manipulate the file system.&nbsp;Although you can import data through methods such as text streaming, ABC cannot read and write files directly.&nbsp;The difficulty of input and output is fatal to computer languages.&nbsp;Can you imagine a sports car that can’t open the door?</li><li><strong>Over-renovation</strong>.&nbsp;ABC uses natural language to express the meaning of the program, such as HOW TO in the program above.&nbsp;However, for programmers, they are more accustomed to using the function or define to define a function.&nbsp;Similarly, programmers are also used to assigning variables with the equal sign (=).&nbsp;Although this makes the ABC language special, it actually increases the difficulty of learning for programmers (most programmers master more than one language).</li><li><strong>Difficult to spread</strong>.&nbsp;The ABC compiler is large and must be stored on tape.&nbsp;When Guido was visiting, he had to have a large tape to install the ABC compiler for others.&nbsp;In this way, ABC language is difficult to spread quickly.</li></ul>
<div><figure><img width="469" height="666" src="https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06111717-51622dbe8fbb4e54ae64f834584180c0.gif" alt="python, Best Products Reviews - Tita" title="A Brief History of Python 3" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20469%20666'%3E%3C/svg%3E"></figure></div>
<p>In 1989, in order to pass the Christmas holiday, Guido began to write a compiler/interpreter for the Python language.&nbsp;Python comes from Guido’s beloved TV series Monty Python’s Flying Circus (BBC 1960s-70s indoor sitcoms, based on British life at the time).&nbsp;He hopes that this new language called Python can realize his idea (a language between C and shell, with comprehensive functions, easy to learn, easy to use, and extensible).&nbsp;As a language design lover, Guido has already had (not very successful) attempts to design languages.&nbsp;This time, it was just a pure hacking behavior.</p>
<h2>The birth of Python</h2>
<p>In 1991, the first Python compiler (also an interpreter) was born.&nbsp;It is implemented in C language and can call C library (.so file).&nbsp;From its birth, Python has: classes, functions, exception handling, core data types including lists and dictionaries, and module-based Expand the system.</p>
<div><figure><img width="348" height="96" src="https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06121511-82e43957fefe4c13ac06bd02a5e9d97a.png" alt="python, Best Products Reviews - Tita" srcset="https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06121511-82e43957fefe4c13ac06bd02a5e9d97a.png 348w, https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06121511-82e43957fefe4c13ac06bd02a5e9d97a-300x83.png 300w" sizes="(max-width: 348px) 100vw, 348px" title="A Brief History of Python 4" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20348%2096'%3E%3C/svg%3E" data-lazy-srcset="https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06121511-82e43957fefe4c13ac06bd02a5e9d97a.png 348w, https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06121511-82e43957fefe4c13ac06bd02a5e9d97a-300x83.png 300w" data-lazy-src="https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06121511-82e43957fefe4c13ac06bd02a5e9d97a.png"><figcaption>The original Python logo: Designed by Guido’s brother Just von Rossum</figcaption></figure></div>
<p>A lot of Python syntax comes from C, but it is strongly influenced by the ABC language.&nbsp;Some regulations from the ABC language are still controversial today, such as mandatory indentation.&nbsp;However, these provisions allow Python syntax&nbsp;content&nbsp;easy to read&nbsp;.&nbsp;On the other hand, Python cleverly chooses to obey some conventions (especially the conventions of the C language).&nbsp;For example, use the equal sign to assign values ​​and use def to define functions.&nbsp;Guido believes that if something is established on “common sense”, there is no need to over-entangle it.</p>
<p>Python has been particularly concerned&nbsp;with&nbsp;extensibility&nbsp;from the beginning.&nbsp;Python can be extended on multiple levels.&nbsp;At a high level, you can import .py files.&nbsp;At the bottom, you can reference C language libraries.&nbsp;Python programmers can quickly use Python to write .py files as extension modules.&nbsp;But when performance is an important factor to consider, Python programmers can go deep into the bottom layer, write C programs, compile them into .so files and import them into Python.&nbsp;Python is like using steel to build a house, first prescribe a big frame.&nbsp;The programmer can expand or change quite freely under this framework.</p>
<p>The original Python was developed entirely by Guido himself.&nbsp;Python is welcomed by Guido colleagues.&nbsp;They quickly gave feedback on usage opinions and participated in the improvement of Python.&nbsp;Guido and some colleagues form the core team of Python.&nbsp;They spend most of their free time hacking Python (including work time because they use Python for work).&nbsp;Subsequently, Python expanded beyond CWI.&nbsp;Python hides many details on the machine level and hands them to the compiler for processing, and highlights the logic of programming thinking.&nbsp;Python programmers can spend more time thinking about the logic of the program, rather than the specific implementation details (Guido has a T-shirt that says: Life is short, I use Python).&nbsp;This feature has attracted the majority of programmers.&nbsp;Python became popular.</p>
<p>We had to pause our Python time and take a look at the computer overview at this time.&nbsp;In the early 1990s, personal computers began to enter ordinary households.&nbsp;Intel released the 486 processor, and Windows released a series of window systems starting with Window 3.0.&nbsp;The performance of the computer is greatly improved.&nbsp;Programmers began to pay attention to the ease of use of computers (such as graphical interfaces).</p>
<div><figure><img width="640" height="480" src="https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06121437-2bed48b285d746c2a147d1d63cc05483.png" alt="python, Best Products Reviews - Tita" srcset="https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06121437-2bed48b285d746c2a147d1d63cc05483.png 640w, https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06121437-2bed48b285d746c2a147d1d63cc05483-300x225.png 300w" sizes="(max-width: 640px) 100vw, 640px" title="A Brief History of Python 5" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20640%20480'%3E%3C/svg%3E" data-lazy-srcset="https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06121437-2bed48b285d746c2a147d1d63cc05483.png 640w, https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06121437-2bed48b285d746c2a147d1d63cc05483-300x225.png 300w" data-lazy-src="https://bestproductsreviews.com.au/wp-content/uploads/2020/11/06121437-2bed48b285d746c2a147d1d63cc05483.png"><figcaption>Win…</figcaption></figure></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://bestproductsreviews.com.au/a-brief-history-of-python/">https://bestproductsreviews.com.au/a-brief-history-of-python/</a></em></p>]]>
            </description>
            <link>https://bestproductsreviews.com.au/a-brief-history-of-python/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033660</guid>
            <pubDate>Mon, 09 Nov 2020 11:30:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Train Your Own Object Detector Using TensorFlow Object Detection API]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033619">thread link</a>) | @patrycjaneptune
<br/>
November 9, 2020 | https://neptune.ai/blog/how-to-train-your-own-object-detector-using-tensorflow-object-detection-api | <a href="https://web.archive.org/web/*/https://neptune.ai/blog/how-to-train-your-own-object-detector-using-tensorflow-object-detection-api">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                    <article class="page">
	
<p>Object detection is a computer vision task that has recently been influenced by the progress made in Machine Learning.&nbsp;</p>



<p>In the past, creating a custom object detector looked like a time-consuming and challenging task. Now, with tools like <a href="https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/index.html" target="_blank" rel="noreferrer noopener nofollow">TensorFlow Object Detection API</a>, we can create reliable models quickly and with ease.</p>



<div><figure><img loading="lazy" src="https://i2.wp.com/neptune.ai/wp-content/uploads/object-detection-tensorflow.png?resize=769%2C385&amp;ssl=1" alt="object detection tensorflow" width="769" height="385" srcset="https://i2.wp.com/neptune.ai/wp-content/uploads/object-detection-tensorflow.png?w=1025&amp;ssl=1 1025w, https://i2.wp.com/neptune.ai/wp-content/uploads/object-detection-tensorflow.png?resize=300%2C150&amp;ssl=1 300w, https://i2.wp.com/neptune.ai/wp-content/uploads/object-detection-tensorflow.png?resize=768%2C384&amp;ssl=1 768w" sizes="(max-width: 769px) 100vw, 769px" data-recalc-dims="1"></figure></div>



<p><em>Object Detection task solved by TensorFlow | Source: <a href="https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.html" target="_blank" rel="noreferrer noopener nofollow">TensorFlow 2 meets the Object Detection API</a></em></p>



<p>In this article we will focus on the second generation of the TensorFlow Object Detection API, which:</p>



<ul><li>supports TensorFlow 2,&nbsp;&nbsp;</li><li>lets you employ state of the art model architectures for object detection,&nbsp;</li><li>gives you a simple way to configure models.</li></ul>



<p>If you’re interested to know all of the features available in TensorFlow 2 and its API, you can find them in the <a href="https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.html" target="_blank" rel="noreferrer noopener nofollow">official announcement from Google</a>.</p>



<p><strong>After reading this article, you should be able to create your own custom object detector.&nbsp;</strong></p>



<p>We’ll be using the <a href="https://ai.googleblog.com/2020/04/efficientdet-towards-scalable-and.html" target="_blank" rel="noreferrer noopener nofollow">EfficientDet based model</a> as an example, but you will also learn how to use <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md" target="_blank" rel="noreferrer noopener nofollow">any architecture</a> of your choice to get a model up and running. Stay tuned! Your own object detector is just around the corner.</p>



<div><figure><img loading="lazy" src="https://i1.wp.com/neptune.ai/wp-content/uploads/object-detection-api.jpg?resize=768%2C512&amp;ssl=1" alt="object detection api" width="768" height="512" srcset="https://i1.wp.com/neptune.ai/wp-content/uploads/object-detection-api.jpg?resize=1024%2C682&amp;ssl=1 1024w, https://i1.wp.com/neptune.ai/wp-content/uploads/object-detection-api.jpg?resize=300%2C200&amp;ssl=1 300w, https://i1.wp.com/neptune.ai/wp-content/uploads/object-detection-api.jpg?resize=768%2C512&amp;ssl=1 768w, https://i1.wp.com/neptune.ai/wp-content/uploads/object-detection-api.jpg?w=1351&amp;ssl=1 1351w" sizes="(max-width: 768px) 100vw, 768px" data-recalc-dims="1"></figure></div>



<p><em>Output example for a model trained using TF Object Detection API. | Source: <a href="https://github.com/tensorflow/models/tree/master/research/object_detection" target="_blank" rel="noreferrer noopener nofollow">Official TF Object Detection API GitHub page</a></em></p>






<h2>Before you start</h2>



<p>Let me briefly talk about the prerequisites that are essential to proceed towards your own object detector:&nbsp;</p>



<ul><li>You should have Python installed on your computer. In case you need to install it, I recommend <a href="https://docs.anaconda.com/anaconda/install/" target="_blank" rel="noreferrer noopener nofollow">following this official guide by Anaconda</a>.</li><li>If your computer has a CUDA-enabled GPU (a GPU made by NVIDIA), then a few relevant libraries are needed in order to support GPU-based training. In case you need to enable GPU support, check the <a href="https://docs.nvidia.com/cuda/archive/10.1/index.html#installation-guides" target="_blank" rel="noreferrer noopener nofollow">guidelines</a> on NVIDIA’s website. Your goal is to install the latest version of both the CUDA Toolkit, and cuDNN for your operating system.</li></ul>






<h2>Installation and setup</h2>



<p>Let’s first make sure that we have everything needed to start working with the TensorFlow Object Detection API. I’ll go over the entire setup process, and explain every step to get things working.</p>



<p>If you’ve already worked with the TF API, you can still have a quick glance over this part, just to make sure that we’re following the same direction.&nbsp;</p>



<p>But if it is your first time installing Tensorflow Object detection API, I would highly recommend completing all of the steps in this section. Let’s jump in!</p>



<hr>



<p><strong><span><span><sup>EDITOR’S NOTE</sup><br></span></span></strong><a href="https://neptune.ai/integrations/tensorflow" target="_blank" rel="noreferrer noopener nofollow">Did you know that you can use TensorFlow for training deep learning models and Neptune for experiment tracking?</a></p>



<hr>



<h3><strong>1. Creating a project directory</strong></h3>



<p>Under a path of your choice, create a new folder. Name it <code>Tensorflow</code>.</p>



<h3><strong>2. Creating a new virtual environment </strong></h3>



<ul><li>Open a <em>Terminal </em>window<em> </em>and use the <code>cd</code> command to navigate to the <code>Tensorflow</code> folder created in step 1.</li></ul>



<ul><li>Create a new virtual environment using the <code>venv</code> library:</li></ul>



<p>If you already have <code>venv</code> installed on your machine (or you prefer managing environments with another tool like <em><a href="https://www.anaconda.com/" target="_blank" rel="noreferrer noopener nofollow">Anaconda</a></em>), then proceed directly to new environment creation.&nbsp;</p>



<p>In case you don’t know what <code>venv</code> is or don’t have it installed, you can do it by typing the following command in your <em>Terminal</em> window:</p>



<pre>pip install venv                                                         
</pre>



<p>In order to create a new environment using <code>venv</code>, type the following command in your <em>Terminal</em> window:</p>



<pre>python -m venv tf2_api_env 
</pre>



<p>Once executed, a new virtual environment named <code>tf2_api_env</code> will be created by <code>venv</code>.</p>



<ul><li>Activate newly created virtual environment:</li></ul>



<p>In order to activate the virtual environment that we’ve just created, you first need to make sure that your current working directory is <code>Tensorflow</code>. You can check your current working directory by typing and executing the following command in your <em>Terminal</em> window:</p>



<pre>pwd 
</pre>



<p>In order to activate your virtual environment, run the following command from you <em>Terminal</em> window:</p>



<pre>source tf2_api_env/bin/activate</pre>



<p>If you see the name of your environment at the beginning of the command line within your <em>Terminal</em> window, then you are all set. It should look like this:</p>



<div><figure><img loading="lazy" width="731" height="485" src="https://i0.wp.com/neptune.ai/wp-content/uploads/virtual-environment-activation.png?resize=731%2C485&amp;ssl=1" alt="virtual environment activation" srcset="https://i0.wp.com/neptune.ai/wp-content/uploads/virtual-environment-activation.png?w=731&amp;ssl=1 731w, https://i0.wp.com/neptune.ai/wp-content/uploads/virtual-environment-activation.png?resize=300%2C199&amp;ssl=1 300w" sizes="(max-width: 731px) 100vw, 731px" data-recalc-dims="1"></figure></div>



<p><em>Successful virtual environment activation in the Terminal window</em></p>



<ul><li>Install core library</li></ul>



<p>It’s time to install TensorFlow in our environment. Make sure that your environment is activated, and do the installation by executing the following command:</p>



<pre>pip install tensorflow==<span>2.</span>* 
</pre>



<p><strong>NOTE: </strong>as I’m writing this article, the latest TensorFlow version is 2.3. You can use this version, but it’s not a requirement. Everything we do in this guide is compatible with 2.3, and it might also work with later updates. It’s up to you to try. In case of any problems, you can always downgrade to 2.3 and move on.</p>



<h3><strong>3. Download and extract TensorFlow Model Garden</strong></h3>



<p>Model Garden is an official TensorFlow repository on <a href="http://github.com/" target="_blank" rel="noreferrer noopener nofollow">github.com</a>. In this step we want to clone this repo to our local machine.</p>



<ul><li>Make sure that within your <em>Terminal </em>window you’re located in the <code>Tensorflow</code> directory.</li></ul>



<ul><li>In your web browser, go to <a href="https://github.com/tensorflow/models" target="_blank" rel="noreferrer noopener nofollow">Model Garden Repo</a> and click on the <em>Code</em> button in order to select a cloning method that’s best for you (the options are HTTPS, SSH or GitHub CLI).</li></ul>



<div><figure><img loading="lazy" width="1024" height="645" src="https://i2.wp.com/neptune.ai/wp-content/uploads/cloning-method.png?resize=1024%2C645&amp;ssl=1" alt="cloning metod" srcset="https://i2.wp.com/neptune.ai/wp-content/uploads/cloning-method.png?resize=1024%2C645&amp;ssl=1 1024w, https://i2.wp.com/neptune.ai/wp-content/uploads/cloning-method.png?resize=300%2C189&amp;ssl=1 300w, https://i2.wp.com/neptune.ai/wp-content/uploads/cloning-method.png?resize=768%2C484&amp;ssl=1 768w, https://i2.wp.com/neptune.ai/wp-content/uploads/cloning-method.png?w=1372&amp;ssl=1 1372w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></figure></div>



<p><em>Selecting a cloning method for an official Model Garder Tensorflow repo</em></p>



<ul><li>Once you select the cloning method, clone the repo to your local <code>Tensorflow</code> directory. In case you need extra help with cloning, check <a href="https://docs.github.com/en/enterprise/2.13/user/articles/cloning-a-repository" target="_blank" rel="noreferrer noopener nofollow">this official GitHub guide</a>.</li></ul>



<p>By now you should have the following structure under the <code>Tensorflow</code> directory:</p>



<pre>Tensorflow/
└─ tf2_api_env/
   ├─ bin/
   ├─ include/
   └── …
└─ models/
   ├─ community/
   ├─ official/
   ├─ orbit/
   └── …
</pre>



<h3><strong>4. Download, install and compile Protobuf</strong></h3>



<p>By default, the TensorFlow Object Detection API uses Protobuf to configure model and training parameters, so we need this library to move on.</p>



<ul><li>Go to the <a href="https://github.com/protocolbuffers/protobuf/releases" target="_blank" rel="noreferrer noopener nofollow">official protoc release page</a> and download an archive for the latest protobuf version compatible with your operation system and processor architecture.&nbsp;</li></ul>



<p>For example, I’m using <em>Ubuntu</em>. My CPU is <em>AMD64</em> (64-bit processor). As I’m writing this article, the latest protoc version is <em>3.13.0</em>. Given all of that information, I am downloading <em>protoc-3.13.0-linux-x86_64.zip</em> file from the official protoc release page.</p>



<ul><li>In the <code>Tensorflow</code> project directory, create a new folder called <code>protoc</code>. Extract the content of the downloaded archive to the <code>Tensorflow/protoc</code> directory.&nbsp;</li></ul>



<p>Now your <code>Tensorflow</code> directory structure should look like this:</p>



<pre>Tensorflow/
└─ protoc/
   ├─ bin/
   ├─ include/
   ├─ readme.txt
└─ tf2_api_env/
   ├─ bin/
   ├─ include/
   └── …
└─ models/
   ├─ community/
   ├─ official/
   ├─ orbit/
   └── …
</pre>



<ul><li>Compile all proto files</li></ul>



<p>Make sure that in your <em>Terminal </em>window, you’re located in the <code>Tensorflow</code> directory. To compile proto files, execute this command:</p>



<pre>protoc/bin/protoc models/research/object_detection/protos/*.proto
--python_out=.
</pre>



<h3><strong>5. Install COCO API</strong></h3>



<p>COCO API is a dependency that does not go directly with the Object Detection API. You should install it separately. Manual installation of COCO API introduces a few new features (e.g. set of popular <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/evaluation_protocols.md" target="_blank" rel="noreferrer noopener nofollow">detection or/and segmentation metrics</a> becomes available for model evaluation). Installation goes as follows:</p>



<p>If you’re using Windows:</p>



<ul><li>Make sure that within your <em>Terminal </em>window you’re located in the <code>Tensorflow</code> directory. Run the following commands one by one:</li></ul>



<pre>pip install cython
pip install git+https://github.com/philferriere/cocoapi.git
</pre>



<p>If you’re using Linux:</p>



<ul><li>Make sure that within your <em>Terminal </em>window you’re located in the <code>Tensorflow</code> directory. Run the following commands one by one:</li></ul>



<pre>pip install cython
git clone https://github.com/cocodataset/cocoapi.git
cd cocoapi/PythonAPI
make
cp -r pycocotools ./models/research/
</pre>



<p>By the end of this step, your <code>Tensorflow</code> directory structure should look like this:</p>



<pre>Tensorflow/
└─ cocoapi/
   ├─ common/
   ├─ LuaAPI/
   └── …
└─ protoc/
   ├─ bin/
   ├─ include/
   ├─ readme.txt
└─ tf2_api_env/
   ├─ bin/
   ├─ include/
   └── …
└─ models/
   ├─ community/
   ├─ official/
   ├─ orbit/
   └── …
</pre>



<h3><strong>6. Object Detection API installation</strong></h3>



<p>This is the final step of our Installation and Setup block! We’re going to install the Object Detection API itself. You do this by installing the <em>object_detection</em> package. Here’s how:</p>



<ul><li>Make sure that within your <em>Terminal </em>window you’re located in the <code>Tensorflow</code> directory.</li></ul>



<ul><li>Change the current working directory from <code>Tensorflow</code> to <code>Tensorflow/models/research</code> using the <code>cd</code> command</li></ul>



<ul><li>Run the following commands one by one in your <em>Terminal </em>window:</li></ul>



<pre>cp object_detection/packages/tf2/setup.py .
python -m pip install .
</pre>



<p><strong>NOTE: </strong>the<strong> </strong>second command might give you an error. No worries at all. Just run it one more time until you see a completed installation.</p>



<ul><li>Test if your installation is successful by running the following command from <code>Tensorflow/models/research</code> directory in your <em>Terminal </em>window:</li></ul>



<pre>python object_detection/builders/model_builder_tf2_test.py</pre>



<p>Once tests are finished, you will see a message printed out in your <em>Terminal </em>window. If all 20 tests were run and the status for them is “OK” (some might be skipped, that’s perfectly fine), then you are all set with the installation!&nbsp;</p>



<div><figure><img loading="lazy" width="498" height="225" src="https://i1.wp.com/neptune.ai/wp-content/uploads/congratulations.gif?resize=498%2C225&amp;ssl=1" alt="congratulations" data-recalc-dims="1"></figure></div>



<p>That was a lot of work, so congratulations! Well done!</p>






<h2>Data preparation</h2>



<p>When you finish all installation steps, you need to think about the data that you’ll feed into your custom object detection model later.&nbsp;</p>



<p>Models based on the TensorFlow object detection API need a special format for all input data, called <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" target="_blank" rel="noreferrer noopener nofollow"><em>TFRecord</em></a>. We’ll talk about how to transform your data into the <em>TFRecord </em>format (to get a better sense of what the <em>TFRecord </em>format is, I highly recommend reading <a href="https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564" target="_blank" rel="noreferrer noopener nofollow">this article</a>), but first let’s talk about a few assumptions about your data availability and its annotations. Specifically, we assume that:</p>



<ul><li>You already <strong>have data (images) collected</strong> for model training, validation and testing,</li><li>Your <strong>i…</strong></li></ul></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://neptune.ai/blog/how-to-train-your-own-object-detector-using-tensorflow-object-detection-api">https://neptune.ai/blog/how-to-train-your-own-object-detector-using-tensorflow-object-detection-api</a></em></p>]]>
            </description>
            <link>https://neptune.ai/blog/how-to-train-your-own-object-detector-using-tensorflow-object-detection-api</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033619</guid>
            <pubDate>Mon, 09 Nov 2020 11:24:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Useful Values / Guiding Principles from Ethics]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033563">thread link</a>) | @hunglee2
<br/>
November 9, 2020 | https://lowercaseopinions.com/useful-values | <a href="https://web.archive.org/web/*/https://lowercaseopinions.com/useful-values">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      
      <p>What we value is foundational to how we approach our lives, how we prioritise our time, and how we interact with one another. In society, in companies, in communities like families and friendship groups.</p>

<p>Our values already exist: in every group, there’s a set of shared or overlapping values. Perhaps you value being demonstratively affectionate to one another: your group chat is likely kind words, animal gifs, wishing each other luck and good vibes on stressful days, or arranging karaoke dates so you can sweetly serenade one another. Maybe you value learning, and that manifests through book recommendations, DMing each other links to thought-provoking articles, attending salons and watching documentaries together.</p>

<p>Often these values shift as the group changes, and as you change the group that you’re in. I find myself leaning harder into some of my values with a particular group of friends, and a different set with my family or my coworkers.</p>

<p>In companies, it’s fundamental that we’re able to talk coherently about what we value, especially if we want to preserve some of this sense of belonging as we scale. Culture is the manifestation of values, it’s the glue that binds us; to reinforce and steer this, we need to have a clear, shared understanding of what’s important to us all in how we work together.</p>

<p>A company’s mission, or a team’s mission, establishes what problem space is addressed; values and culture become the <em>how</em> underlying the business strategy to achieve the company’s mission.</p>

<p>I think it’s really interesting to observe the language that we use when talking about company values. People refer to “the right behaviours” or being a “good” employee. We’ve borrowed the language of ethics - and I don’t think this is a coincidence at all.</p>

<p>In fact, there are three frameworks in ethics that have striking parallels to how we set out what’s good or right in a company or business. One of these lends itself exceptionally well to framing company values.</p>



<p>Before we dive into my weird Good-Place-esque rundown of metaethics as applied to companies, I want to explore a little bit what makes a good set of values.</p>

<p>There’s a trope that every startup should have five pithy values that can be neatly painted onto the wall (and then a couple of pages explaining what each of them actually means!). And that is often the endpoint - something polished by a copywriter that gets thrown onto t-shirts or notebooks or painted on a wall back when a physical office felt important.</p>

<p>But often we miss a step. We get the polished pithy phrases, but don’t draw them from the right place. This process of understanding and surfacing what your team values has to be <em>descriptive</em>. It’s a weird, difficult, fun, generative, iterative journey of discovery. You’ll find the intrinsic qualities that you deem valuable in the people you all want to work with (smart, considerate), and the extrinsic qualities that you value in how you interact with one another (straightforward, open to early feedback).</p>

<p>This discovery process is so important to building a healthy and meaningful company culture. Values are so, so important - they’re what the culture is built on, they’re what bring us joy and meaning every day. But we dismiss them because so often they do end up just being a set of catchy but empty phrases painted on a wall.</p>

<p>I have two examples of pithy phrases masquerading as values which just aren’t useful:</p>

<p><em>Get shit done.</em>
This one baffles me because it provides no direction. What shit should I be getting done? What’s the best way to get it done? At what point is my shit done enough? Especially in the context of a company, where I get paid to do a job - my side of the employment contract is about getting shit done! You can tell me to value doing shit as much as you like, but this adds zero value when you consider that, contractually, as soon as I stop getting shit done, you’ll stop paying me!</p>

<p><em>Don’t be evil.</em>
I have a bunch of issues with this as a thing to value. Firstly, how are we defining evil? Secondly, can I do evil without being evil? What if it’s unintentional? Thirdly, is not being evil enough, or should I strive to be good? And if so, what is good? I used to say “be good, have fun” when I dropped my kid off at school in the mornings, but I’ve switched to “have fun, learn something” because it feels more directionally useful. I think “be good” sometimes reads as “don’t get told off”, and I’ve definitely been told off for doing what was right.</p>

<p>Honestly compared to these, if you’re going to paint something on your wall, I’d actually favour “live, laugh, love”. It’s more like a useful value statement than either of these - it’s clear that love and laughter are states to optimise for, which can at least guide the behaviours we choose. Tell me to “live, laugh, love” and my constraints are tighter than “don’t be evil”. Plus there’s already a bunch of wall decals on etsy!</p>



<p>Our values affect how we go about achieving things and interacting with others. They capture the quirks, traits and essences of the things that make your company and your community who they are.  For me, useful values have two properties: they are opinionated, and they guide behaviour.</p>

<p>The things that <em>you in particular</em> value have to take a perspective and a point of view. And it needs to be a meaningful point of view - where you can envisage a world where many very reasonable people take a conflicting point of view.</p>

<p>You know you have pinpointed genuinely useful values if you can use them to make decisions about what you do and how you do it. Particularly in high stress or difficult situations: for instance, how you approach interpersonal conflict or even conflicting priorities.</p>

<p>Both “don’t be evil” and “get shit done” fail on both counts - I can’t imagine anyone pasting the opposite on an office wall, and I can’t imagine solving a problem by using either. They are about as generic as you can be. Likewise “integrity” is not especially meaningful as a value; what would it mean to be a reasonable person who holds a value that’s contradictory to that?</p>

<p>“Move fast”, on the other hand, is absolutely meaningful. Depending on the context, you could choose to move fast; to be cautious; to take thoughtful and deliberate action; to prioritise perfection over speed. “Build to last” would take an opposing point of view, but be meaningful - as would “excellence always”. None of these three stances are objectively better than the others, which is great! Pick one and you now have a value that not only takes a perspective, but also influences how you work.</p>



<p>Our values delineate what we encourage and praise, and what is frowned upon or even punished. They set out what is right and what is wrong within the company. Values layer over the top of the ethical and moral norms of the society where you operate - like, the laws and regulatory principles and business governance.</p>

<p>When we internalise values as a sort of ethical system, it makes a lot of sense to adopt an ethics framework in order to establish how we want to think about values. Ethical <em>systems</em> concern themselves less with what specific acts are moral or immoral, and more with how we systematically determine this. So I want to focus less on what specific things we value, and more on how we frame values.</p>

<p>I’ll briefly outline three approaches to ethics from the western philosophical canon, and how the combination of the three gives us a strong framework for thinking about right and wrong in a company.</p>

<h2 id="categorical-imperatives-and-the-law">Categorical imperatives and the law</h2>

<p>One approach to ethics is through the <a href="https://plato.stanford.edu/entries/kant-moral/#CatHypImp">categorical imperative</a>: ethics should be a system of commands which are always, unconditionally, universally the correct and right way to act.</p>

<p><em>“Act only according to that maxim whereby you can, at the same time, will that it should become a universal law.”</em> Judaism and Christianity’s Ten Commandments take the form of categorical imperatives.</p>

<p>There are no exceptions to the imperative. If, as a collective, we set out that lying is wrong, then it is not ever permissible for anyone to lie. This is super clear cut, which appeals to those who like simplicity! But humans are messy. The workplace is ambiguous, the decisions we make in our jobs are nuanced. There are a lot of behaviours that are context-dependent, or require a trade-off. So the universal commandment approach can’t capture the delightful quirks of one company over another - they are adopted by everyone.</p>

<p>But there is something useful in considering this approach. Let’s assume that there is a subset of actions which are either good or bad, based on whether everyone should do it, or everyone should not do it. In a company context, this approach is really useful in two places: the first is laws, regulation and governance. For example, it is never acceptable to commit fraud. The second is process and policies. For example, in our company, it is never acceptable to bully or harass others.</p>

<p>I referenced “don’t be evil” earlier, and for me that falls into the categorical imperative category, as does “only hire the best”. But there’s a whole layer of nuance beneath these that we need to unpick to determine what they mean within a particular company - “the best” is subjective, we characterise it through traits that we deem valuable, and those just won’t apply to everyone.</p>

<h2 id="utilitarianism-and-stakeholder-benefit">Utilitarianism and stakeholder benefit</h2>

<p><em>“It is the greatest happiness of the greatest number that is the measure of right and wrong.”</em></p>

<p>A second approach to ethics is <a href="https://plato.stanford.edu/entries/bentham/#PaiPle">utilitarianism</a> - where an act is good based entirely on its <em>consequences</em> and which way they swing happiness. In the context of a company, the clearest parallel is where a lot of what you do - the goals you set - optimise for stakeholder benefit (which includes profit and user happiness).</p>

<p>One of the difficulties with utilitarianism is that it is entirely based on consequences, intention has no role to play at all. This theory of ethics would judge someone’s actions good if …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://lowercaseopinions.com/useful-values">https://lowercaseopinions.com/useful-values</a></em></p>]]>
            </description>
            <link>https://lowercaseopinions.com/useful-values</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033563</guid>
            <pubDate>Mon, 09 Nov 2020 11:13:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hand-drawn animated tram ride (web experiment)]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033545">thread link</a>) | @parisianka
<br/>
November 9, 2020 | https://alexanderperrin.com.au/paper/shorttrip/ | <a href="https://web.archive.org/web/*/https://alexanderperrin.com.au/paper/shorttrip/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="credits">
<div>
<p>
<strong>Short Trip - Alexander Perrin, 2017</strong>
</p>
<p>Hold left or right to move.</p>
<p>Read more about the project
<a href="http://alexanderperrin.com.au/portfolio/short-trip/" target="_blank">here.</a>
</p>
<p>Thank you to
<a href="https://twitter.com/domwillmott" target="_blank">Dom Willmott</a> for
<br>audio support.</p>
<p>If you would like to support Short Trip, you're welcome (but not obliged) to make a contribution
<a href="https://paypal.me/alexanderperrin">here.</a>
</p>
<p>Quality Mode:
<span id="default-button">Default</span>
<span id="eco-button">Eco</span>
</p>
<p>Sound:
<span id="mute-off">On</span>
<span id="mute-on">Off</span>
</p>
</div>
</div></div>]]>
            </description>
            <link>https://alexanderperrin.com.au/paper/shorttrip/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033545</guid>
            <pubDate>Mon, 09 Nov 2020 11:10:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Instantly refactor your Python code in Vim]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033399">thread link</a>) | @brendanator
<br/>
November 9, 2020 | https://sourcery.ai/blog/sourcery-vim/ | <a href="https://web.archive.org/web/*/https://sourcery.ai/blog/sourcery-vim/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><span>
      <span></span>
  <img alt="Sourcery and Vim" title="Sourcery and Vim" src="https://sourcery.ai/static/d9a1fa03d905634aaaa161e3752f8ff2/5a190/sourcery-vim.png" srcset="https://sourcery.ai/static/d9a1fa03d905634aaaa161e3752f8ff2/772e8/sourcery-vim.png 200w,
https://sourcery.ai/static/d9a1fa03d905634aaaa161e3752f8ff2/e17e5/sourcery-vim.png 400w,
https://sourcery.ai/static/d9a1fa03d905634aaaa161e3752f8ff2/5a190/sourcery-vim.png 800w,
https://sourcery.ai/static/d9a1fa03d905634aaaa161e3752f8ff2/c1b63/sourcery-vim.png 1200w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy">
    </span></p>
<p>We're delighted to announce that Sourcery is now available in Vim 🎉.</p>
<p>This means you can use all of the great features of Sourcery directly in Vim:</p>
<ul>
<li><em>Instant refactoring suggestions</em>.  We're the only tool that will refactor your code for you! As you edit, Sourcery will analyse your code and suggest improvements which you can preview and then apply with a single command.</li>
<li><em>We don't break your code</em>.  Sourcery uses extensive static analysis to ensure that its refactorings don't change the existing functionality - backed up by testing on open source repositories.</li>
<li><em>Code Quality Metrics</em>.  View the quality of any function and be shown warnings when the quality drops below a threshold.</li>
</ul>
<h2>Installation</h2>
<p>Sourcery uses the the excellent coc.nvim LSP plugin to provide its functionality. There are complete installation instruction on our <a href="https://github.com/sourcery-ai/sourcery/wiki/Vim-Usage#installation">wiki</a>.</p>
<h2>Sourcery in Action</h2>
<p>Here's a Sourcery refactoring suggestion shown using <code>:call CocAction('doHover')</code></p>
<p><span>
      <span></span>
  <img alt="Suggested diff" title="Suggested diff" src="https://sourcery.ai/static/f0507d4539c1878b600f5c8bd406049a/5a190/suggestion.png" srcset="https://sourcery.ai/static/f0507d4539c1878b600f5c8bd406049a/772e8/suggestion.png 200w,
https://sourcery.ai/static/f0507d4539c1878b600f5c8bd406049a/e17e5/suggestion.png 400w,
https://sourcery.ai/static/f0507d4539c1878b600f5c8bd406049a/5a190/suggestion.png 800w,
https://sourcery.ai/static/f0507d4539c1878b600f5c8bd406049a/764be/suggestion.png 806w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy">
    </span></p>
<p>Run <code>:CocFix</code> to view the options to fix:</p>
<p><span>
      <span></span>
  <img alt="Options to fix" title="Options to fix" src="https://sourcery.ai/static/ad7d66732663b341ba56f3785d4252bc/5a190/fix-options.png" srcset="https://sourcery.ai/static/ad7d66732663b341ba56f3785d4252bc/772e8/fix-options.png 200w,
https://sourcery.ai/static/ad7d66732663b341ba56f3785d4252bc/e17e5/fix-options.png 400w,
https://sourcery.ai/static/ad7d66732663b341ba56f3785d4252bc/5a190/fix-options.png 800w,
https://sourcery.ai/static/ad7d66732663b341ba56f3785d4252bc/e1031/fix-options.png 803w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy">
    </span></p>
<p>Press <code>1</code> and your code is instantly refactored:</p>
<p><span>
      <span></span>
  <img alt="Refactored code" title="Refactored code" src="https://sourcery.ai/static/cc000aacc919619f5f31baaa3cfb7d35/5a190/refactored.png" srcset="https://sourcery.ai/static/cc000aacc919619f5f31baaa3cfb7d35/772e8/refactored.png 200w,
https://sourcery.ai/static/cc000aacc919619f5f31baaa3cfb7d35/e17e5/refactored.png 400w,
https://sourcery.ai/static/cc000aacc919619f5f31baaa3cfb7d35/5a190/refactored.png 800w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy">
    </span></p>
<h2>Documentation</h2>
<p>Check out our <a href="https://github.com/sourcery-ai/sourcery/wiki/Vim-Usage">wiki</a> for more usage instructions and configuration options.</p>
<h2>Thanks</h2>
<p>Thanks to <a href="https://github.com/marcoaaguiar">Marco Aguiar</a> for working out how to use Sourcery in Vim 🙏!</p></div></div>]]>
            </description>
            <link>https://sourcery.ai/blog/sourcery-vim/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033399</guid>
            <pubDate>Mon, 09 Nov 2020 10:44:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The power of HTTP headers and examples]]>
            </title>
            <description>
<![CDATA[
Score 53 | Comments 5 (<a href="https://news.ycombinator.com/item?id=25033398">thread link</a>) | @loweisz
<br/>
November 9, 2020 | https://www.lorenzweiss.de/the_power_of_http_headers_with_four_examples/ | <a href="https://web.archive.org/web/*/https://www.lorenzweiss.de/the_power_of_http_headers_with_four_examples/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>Almost everything in the web is sent with <strong>http</strong> and even non-developers have seen it when using the internet as keyword
inside urls or links.</p>
<p>Http stands for <strong>Hypertext Transfer Protocol</strong> and gives us the ability to transfer hypertext between a browser and a server.
This is a great technology that has been around almost since the invention of the web and is constantly evolving and
<a href="https://en.wikipedia.org/wiki/HTTP/2">offering more and more great features</a></p>

<p>As a developer you probably heard of http headers, at least in the moment you heard about the CORS policy.
This is a problem you must have heard about when developing websites.
But what exactly are http headers and what other ways are there to use them?</p>
<p>Let us first find out what they do and how you could use them. </p>
<p>When a browser requests a resource, for example a page of this blog, it asks the server with a request.
This request looks something like this: </p>
<div data-language="js"><pre><code><span>fetch</span><span>(</span><span>"https://www.lorenzweiss.de/race_conditions_explained/"</span><span>,</span> <span>{</span>
  credentials<span>:</span> <span>"include"</span><span>,</span>
  headers<span>:</span> <span>{</span>
    accept<span>:</span>
      <span>"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3"</span><span>,</span>
    <span>"accept-language"</span><span>:</span> <span>"en,en-US;q=0.9,de-DE;q=0.8,de;q=0.7"</span><span>,</span>
    <span>"cache-control"</span><span>:</span> <span>"max-age=0"</span><span>,</span>
    <span>"sec-fetch-mode"</span><span>:</span> <span>"navigate"</span><span>,</span>
    <span>"sec-fetch-site"</span><span>:</span> <span>"same-origin"</span><span>,</span>
    <span>"sec-fetch-user"</span><span>:</span> <span>"?1"</span><span>,</span>
    <span>"upgrade-insecure-requests"</span><span>:</span> <span>"1"</span><span>,</span>
  <span>}</span><span>,</span>
  referrerPolicy<span>:</span> <span>"no-referrer-when-downgrade"</span><span>,</span>
  body<span>:</span> <span>null</span><span>,</span>
  method<span>:</span> <span>"GET"</span><span>,</span>
  mode<span>:</span> <span>"cors"</span><span>,</span>
<span>}</span><span>)</span><span>;</span></code></pre></div>
<p>So you can see the URL or location of the resource, some information about the request and also a lot of headers with some information about the request.
This is how your browser tells the server some more information about the request. For example what kind of data type it accepts or
how the client is handling the cache.</p>
<p>After sending the request, the server replies, and it also sets some headers in the reply, which could look like this: </p>
<div data-language="text"><pre><code>:authority: www.lorenzweiss.de
:method: GET
:path: /race_conditions_explained/
:scheme: https
accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3
accept-encoding: gzip, deflate, br
accept-language: en,en-US;q=0.9,de-DE;q=0.8,de;q=0.7
cache-control: max-age=0
cookie: _ga=GA1.2.1173972759.1584812492; _gid=GA1.2.2076192721.1594044231
sec-fetch-mode: navigate
sec-fetch-site: same-origin
sec-fetch-user: ?1
upgrade-insecure-requests: 1
user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36</code></pre></div>
<p>There is also some information that the server wants to tell the browser what to do with the resource, for example
if there are cookies, it must be determined which encoding was used, etc</p>
<p>Basically, in the http-context the headers for the communication of the browser and the server are used to extend the simple
Requests for resources. You could see it as the sheet of paper that is added on top of a package that you oder from an online store,
giving you more information about the context and the resource that you ordered.
Most of the headers have quite good defaults which you don't need to think of, but there are some headers that
can get quite important, like CORS headers. But there are so much more headers that you might never heard of which are very useful
and good to know how to use. </p>

<p>Do not worry, this article will not deal with CORS headers. The following http headers are those that are rarely used, but
can be really powerful and helpful to significantly improve the communication between a server and the browser. </p>
<p>So let's dig into it. Here are some headers that you can set and that are very useful and practical.</p>
<h2 id="if-range"><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/If-Range">If-Range</a><a href="#if-range" aria-label="if range permalink"></a></h2>
<h3>What and why?</h3>
<p>Imagine you start downloading a large resource, such as a video, an image, etc., and stop in between because of connection problems.
With <code>If-Range</code> you can tell the server if the representation is unchanged, to send the part(s) that are requested in Range.
Which means only the parts that were missing and not again the whole thing.</p>
<p>This can be very helpful when dealing with large resources and often bad connections as with mobile devices.
Because the resource can be downloaded in parts even if the connection is interrupted in between. </p>
<h4>How to use</h4>
<p>It can either be used with a date when the resources were last modified, or with an <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ETag">ETag</a>, which is a key to help if the resources was invalidated</p>
<div data-language="text"><pre><code>If-Range: &lt;day-name&gt;, &lt;day&gt; &lt;month&gt; &lt;year&gt; &lt;hour&gt;:&lt;minute&gt;:&lt;second&gt; GMT
If-Range: &lt;etag&gt;</code></pre></div>
<h4>Example</h4>
<div data-language="text"><pre><code>If-Range: Wed, 21 Oct 2015 07:28:00 GMT </code></pre></div>
<h2 id="vary"><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Vary">Vary</a><a href="#vary" aria-label="vary permalink"></a></h2>
<p><code>Vary</code> Comes from a time when the web or http was used for a variety of things and not just for web pages.<br>
It is based on the idea of using http to exchange information in many different formats.
How does it do that? Well, it tells the server in which header to find the information, how to present the information. </p>
<p>Nowadays it can be really helpful if you have different resources for different customers, for example
mobile, tablet or desktop.
Imagine three different images for the same resource are stored on the server, depending on the device.
Then you can simply use the <code>Vary</code> header to tell the server to check the device and then decide which image size to send. </p>
<h4>Example</h4>
<p>For the example with the device dependent images, you can simply pass the 'user agent' to tell the server
that it should check the user-agent for device information. </p>

<h4>How to use</h4>

<p>Just enter the header, the server must check before deciding which resource to send.</p>
<h2 id="content-disposition"><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Content-Disposition">Content-Disposition</a><a href="#content-disposition" aria-label="content disposition permalink"></a></h2>
<p>If we go back to the example of a request to a server, for example to load this website, it is clear to the browser,
that it must <strong>display</strong> the resource of the answer.
But it can also be the case that the server sends a resource that the browser should automatically download to the user's computer,
like a picture or pdf etc.
A server can tell the browser what the browser should do with the attached resource via the <code>Content Disposition</code> header.</p>
<h4>Example</h4>
<p>With defining the <code>Content-disposition</code> to <code>attachment</code> the browser knows that this is a resource to download instead of just
show. </p>
<div data-language="text"><pre><code>Content-Disposition: attachment; filename="data.pdf"</code></pre></div>
<h4>How to use</h4>
<p>You can define the header as <code>inline</code> or <code>attachment</code>, where `inline is always the default.  </p>
<div data-language="text"><pre><code>Content-Disposition: &lt;inline | attachment&gt;</code></pre></div>
<h2 id="feature-policy"><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Feature-Policy">Feature-Policy</a><a href="#feature-policy" aria-label="feature policy permalink"></a></h2>
<p>This is a fairly new header and therefore only supported by modern browsers (sorry to all IE users). However
I want to mention this anyway because I think it can be really helpful for some use cases.<br>
Basically, the <code>feature-policy tells the browser which features or apis the browser should provide to the document and its</code>iframes` to be used. </p>
<p>For example, it can ban all scripts or iframes etc. within this website to allow sensitive apis like the camera or microphone.</p>
<h4>How to use</h4>
<div data-language="text"><pre><code>Feature-Policy: &lt;directive&gt; &lt;allowlist&gt;</code></pre></div>
<p>The <code>directive</code> is the name of the feature. You can see the full <a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Feature-Policy#Directives">list of features here</a>
The <code>allowlist</code> defines the origins which are allowed to use the directive.</p>
<h3>Example</h3>
<p>Suppose we want our website to use neither the microphone nor the camera. With this header the
document or a contained iframe cannot access these functions.</p>
<div data-language="text"><pre><code>Feature-Policy: microphone 'none'; camera 'none'</code></pre></div>
<h3>More Headers:</h3>
<p>Here are some more headers that are worth mentioning: </p>
<ul>
<li><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Upgrade-Insecure-Requests">Upgrade-Insecure-Requests</a></li>
<li><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Age">Age</a></li>
<li><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Trailer">Trailer</a></li>
<li><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Location">Location</a></li>
</ul>
<h2 id="conclusion">Conclusion<a href="#conclusion" aria-label="conclusion permalink"></a></h2>
<p>Https headers are great and also very useful! But sometimes they can be quite complex, and it's really hard to get an overview of what headers are available and what benefits they bring.
Also when developing a website, especially in the frontend, you don't come in contact with them too often, except maybe with the CORS headers.
But I think that this missed some possibilities. http headers represent the communication between the server and the
customers much better, and we all know that communication is the key to a good relationship.</p>
<p>I hope I could shed some light on the darkness of http headers for you. In case I missed a good and helpful header,
please do not hesitate to send me a mail or contact me in any way.</p></div></div>]]>
            </description>
            <link>https://www.lorenzweiss.de/the_power_of_http_headers_with_four_examples/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033398</guid>
            <pubDate>Mon, 09 Nov 2020 10:44:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Message from the Center for Presidential Transition]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033361">thread link</a>) | @scapecast
<br/>
November 9, 2020 | https://presidentialtransition.org/publications/message-from-the-center-advisory-board/ | <a href="https://web.archive.org/web/*/https://presidentialtransition.org/publications/message-from-the-center-advisory-board/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
<article id="post-8006" role="article">
    <h3>A MESSAGE FROM THE CENTER FOR PRESIDENTIAL TRANSITION ADVISORY BOARD</h3>
    <p>
      November 8, 2020    </p>
    

    
<p>&nbsp;<strong>November 8, 2020&nbsp;</strong></p>



<p>The Partnership for Public Service’s Center for Presidential Transition is the nation’s premier nonpartisan source of information and resources designed to help presidential candidates and their teams lay the groundwork for a new administration or for a president’s second term. The Center has been active in transition activities on a bipartisan basis for four cycles.&nbsp;</p>



<p>We congratulate Vice President Joe Biden and Senator Kamala Harris on their successful and historic campaign for the White House. In our role we have observed the seriousness with which they have taken the transition planning process. They embraced transition planning early, recruited a seasoned and disciplined team and resourced their transition effort commensurate with the challenges that President-elect Biden will face on January 20. While there will be legal disputes requiring adjudication, the outcome is sufficiently clear that the transition process must now begin.&nbsp;</p>



<p>As candidate Biden becomes President-elect Biden, he and his transition team will quickly shift from campaigning to governing. To build an effective government ready to address the urgent needs of our great country, the new president will have to recruit 4,000 political appointees, including 1,250 who require Senate confirmation; prepare a $4.7 trillion budget; implement a strong policy agenda; and assume leadership of a workforce of 2 million civilian employees and 2 million active duty and reserve troops.&nbsp;</p>



<p>We want to also applaud the two other key stakeholders necessary for a successful transition – the White House staff and the career officials throughout the federal government with responsibility for transition planning under the Presidential Transition Act. The White House staff took implementation of the Presidential Transition Act seriously, met every statutory milestone and worked closely with the career officials responsible for transition planning. The career federal officials with responsibility for transition planning, led by the GSA, did exactly what one would expect from highly qualified, experienced career officials – they planned and prepared methodically for either eventuality – a Trump re-election or a Biden win.&nbsp;</p>



<p>Now the real challenge begins. We urge the Trump administration to immediately begin the post-election transition process and the Biden team to take full advantage of the resources available under the Presidential Transition Act. This was a hard-fought campaign, but history is replete with examples of presidents who emerged from such campaigns to graciously assist their successors. “Your success now is our country’s success,” George H.W. Bush wrote in 1993 to the incoming president who involuntarily retired him, “I am rooting hard for you.”&nbsp;</p>



<p><strong>Josh Bolten</strong>, White House Chief of Staff and Director of the Office of Management and Budget, George W. Bush Administration (Republican)&nbsp;</p>



<p><strong>Michael Leavitt</strong>, Secretary of Health and Human Services and Administrator, Environmental Protection Agency, George W. Bush Administration, Governor of Utah (Republican)&nbsp;</p>



<p><strong>Thomas F. (Mack) McLarty</strong>, White House Chief of Staff, Clinton Administration (Democrat)&nbsp;</p>



<p><strong>Penny S. Pritzker, </strong>Secretary of Commerce, Obama Administration (Democrat)&nbsp;</p>
<!-- AddThis Advanced Settings above via filter on the_content --><!-- AddThis Advanced Settings below via filter on the_content --><!-- AddThis Advanced Settings generic via filter on the_content --><!-- AddThis Share Buttons above via filter on the_content --><!-- AddThis Share Buttons below via filter on the_content --><!-- AddThis Share Buttons generic via filter on the_content -->
                
    </article></div>

</div></div>]]>
            </description>
            <link>https://presidentialtransition.org/publications/message-from-the-center-advisory-board/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033361</guid>
            <pubDate>Mon, 09 Nov 2020 10:38:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Zero-Days in Desktop Web Browsers]]>
            </title>
            <description>
<![CDATA[
Score 119 | Comments 73 (<a href="https://news.ycombinator.com/item?id=25033290">thread link</a>) | @svenfaw
<br/>
November 9, 2020 | https://www.radsix.com/dashboard1/ | <a href="https://web.archive.org/web/*/https://www.radsix.com/dashboard1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.radsix.com/dashboard1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033290</guid>
            <pubDate>Mon, 09 Nov 2020 10:24:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Biden-Harris plan to create union jobs by tackling the climate crisis]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033253">thread link</a>) | @_Microft
<br/>
November 9, 2020 | https://buildbackbetter.com/priorities/climate-change/ | <a href="https://web.archive.org/web/*/https://buildbackbetter.com/priorities/climate-change/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
	

		<section id="content">

	  
<div data-module="">
  <div>
	<div>
	  <p>From coastal towns to rural farms to urban centers, climate change poses an existential threat — not just to our environment, but to our health, our communities, our national security, and our economic well-being. It also damages our communities with storms that wreak havoc on our towns and cities and our homes and schools. It puts our national security at risk by leading to regional instability that will require U.S military-supported relief activities and could make areas more vulnerable to terrorist activities.</p>
	</div>
  </div>
</div>

<div data-module="" id="climate-change-2" data-new-section="false" data-id="climate-change-2">
  <div>
	<div>
	  <div>
	   <p>The current COVID-19 pandemic reminds us how profoundly the energy and environmental policy decisions of the past have failed communities — allowing systemic shocks, persistent stressors, and pandemics to disproportionately impact communities of color and low-income communities.</p>
<p>At this moment of profound crisis, we have the opportunity to build a more resilient, sustainable economy — one that will put the United States on an irreversible path to achieve net-zero emissions, economy-wide, by no later than 2050. Biden is working to seize that opportunity and, in the process, create millions of good-paying jobs that provide workers with the choice to join a union and bargain collectively with their employers.</p>
<p>President-elect Biden is leading the world to address the climate emergency and leading through the power of example. Biden knows how to stand with America’s allies, stand up to adversaries, and level with any world leader about what must be done. He will not only recommit the United States to the Paris Agreement on climate change – he will go much further than that. He is working to lead an effort to get every major country to ramp up the ambition of their domestic climate targets.</p>
	  </div>
	</div>
  </div>
</div>

<!-- .module.block-quote -->

<div data-module="" id="climate-change-4" data-new-section="false" data-id="climate-change-4">
  <div>
	<div>
	  <div>
	   <p>President-elect Biden will ensure that — coming out of this profound public health and economic crisis, and facing the persistent climate crisis — we are never caught flat-footed again. He is working to launch a national effort aimed at creating the jobs we need to build modern, sustainable infrastructure now and deliver an equitable clean energy future.</p>
<p>The current coronavirus crisis destroyed millions of American jobs, including hundreds of thousands in clean energy. It has exacerbated historic environmental injustices. Biden will immediately invest in engines of sustainable job creation — new industries and re-invigorated regional economies spurred by innovation from our national labs and universities; commercialized into new and better products that can be manufactured and built by American workers; and put together using feedstocks, materials, and parts supplied by small businesses, family farms, and job creators all across our country.</p>
<p>President-elect Biden is working to make far-reaching investments in:</p>
<ul>
<li><strong>Infrastructure:</strong> Create millions of good, union jobs rebuilding America’s crumbling infrastructure – from roads and bridges to green spaces and water systems to electricity grids and universal broadband – to lay a new foundation for sustainable growth, compete in the global economy, withstand the impacts of climate change, and improve public health, including access to clean air and clean water.</li>
<li><strong>Auto Industry:</strong> Create 1 million new jobs in the American auto industry, domestic auto supply chains, and auto infrastructure, from parts to materials to electric vehicle charging stations, positioning American auto workers and manufacturers to win the 21st century; and invest in U.S. auto workers to ensure their jobs are good jobs with a choice to join a union.</li>
<li><strong>Transit:</strong> Provide every American city with 100,000 or more residents with high-quality, zero-emissions public transportation options through flexible federal investments with strong labor protections that create good, union jobs and meet the needs of these cities — ranging from light rail networks to improving existing transit and bus lines to installing infrastructure for pedestrians and bicyclists.</li>
<li><strong>Power Sector:</strong> Move ambitiously to generate clean, American-made electricity to achieve a carbon pollution-free power sector by 2035. This will enable us to meet the existential threat of climate change while creating millions of jobs with a choice to join a union.</li>
<li><strong>Buildings:</strong> Upgrade 4 million buildings and weatherize 2 million homes over 4 years, creating at least 1 million good-paying jobs with a choice to join a union; and also spur the building retrofit and efficient-appliance manufacturing supply chain by funding direct cash rebates and low-cost financing to upgrade and electrify home appliances and install more efficient windows, which will cut residential energy bills.</li>
<li><strong>Housing:</strong> Spur the construction of 1.5 million sustainable homes and housing units.</li>
<li><strong>Innovation:</strong> Drive dramatic cost reductions in critical clean energy technologies, including battery storage, negative emissions technologies, the next generation of building materials, renewable hydrogen, and advanced nuclear – and rapidly commercialize them, ensuring that those new technologies are made in America.</li>
<li><strong>Agriculture and Conservation:</strong> Create jobs in climate-smart agriculture, resilience, and conservation, including 250,000 jobs plugging abandoned oil and natural gas wells and reclaiming abandoned coal, hardrock, and uranium mines — providing good work with a choice to join or continue membership in a union in hardhit communities, including rural communities, reducing leakage of toxics, and preventing local environmental damage.</li>
<li><strong>Environmental Justice:</strong> Ensure that environmental justice is a key consideration in where, how, and with whom we build — creating good, union, middle-class jobs in communities left behind, righting wrongs in communities that bear the brunt of pollution, and lifting up the best ideas from across our great nation — rural, urban, and tribal.</li>
</ul>
	  </div>
	</div>
  </div>
</div>



	</section>

  </article></div>]]>
            </description>
            <link>https://buildbackbetter.com/priorities/climate-change/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033253</guid>
            <pubDate>Mon, 09 Nov 2020 10:17:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why Rust Is the Future of Game Development]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25033247">thread link</a>) | @TheFuntastic
<br/>
November 9, 2020 | https://thefuntastic.com/blog/why-rust-is-the-future-game-dev | <a href="https://web.archive.org/web/*/https://thefuntastic.com/blog/why-rust-is-the-future-game-dev">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-v-10b1bd0a="" data-v-2ae295f5=""><p><em>Rust, not related to the video game also called Rust, is a promising systems programming language with novel features ideally suited for game development. Exposure and awareness within the game developer community, however, remains limited. In this post, I provide a gentle introduction to Rust and attempt to justify its place on your radar.</em></p>
<h2 id="a-short-history-lesson">A Short History Lesson</h2>
<p>What is Rust, and where did it come from? In <a href="https://www.youtube.com/watch?v=HiWkMFE8uRE" target="_blank" rel="nofollow noopener noreferrer">this fantastic talk</a>, James Munns gives us a detailed oral history. Way back around 2010, Mozilla was frustrated by the state of development in Firefox, a massive software project  written mostly in C++. Despite best practices and an abundance of engineering talent, writing high-performance, parallelised, and memory-safe code, at that scale of complexity, remained fraught and error-prone.</p>
<p>Bear in mind, this predates the advent of C++11 (aka the 2011 edition) which heralded efforts to somewhat modernise the language. Even so, manual memory manipulation is easy to get wrong, and <a href="https://msrc-blog.microsoft.com/2019/07/18/we-need-a-safer-systems-programming-language/" target="_blank" rel="nofollow noopener noreferrer">research</a> from <a href="https://www.zdnet.com/article/chrome-70-of-all-security-bugs-are-memory-safety-issues/" target="_blank" rel="nofollow noopener noreferrer">multiple vendors</a> describes this category of error as responsible for 70% of security vulnerabilities. </p>
<p>Into this context steps Graydon Hoare, a Mozilla employee, introducing <a href="https://en.wikipedia.org/wiki/Rust_(programming_language)#History" target="_blank" rel="nofollow noopener noreferrer">a potential solution</a> to the roadblock: Rust, the hobby language he'd been tinkering with since 2006. In 2012, Mozilla would formally announce Servo, an experimental research project to re-imagine a browser engine built with memory safety and concurrency as first principles. And alongside it, Rust, the companion language to make it all possible.</p>
<p>These early days of Rust are described as a Cambrian explosion of ideas and wild experimentation. Concepts were liberally stolen from other languages, from C++ to OCaml, Haskell, Erlang, ML, C#, Ruby and more, reflecting the diverse pool of engineers working on the language at the time. Still, most in the industry, while admiring the optimism in taking such an ambitious moon shot, <a href="http://dtrace.org/blogs/bmc/2018/09/18/falling-in-love-with-rust/" target="_blank" rel="nofollow noopener noreferrer">remained pessimistic</a> about the prospects of success. </p>
<p>2015 saw a major milestone, with the release of Rust v1.0. Perhaps as significant as the feature list, was the number of failed experiments left behind on the cutting room floor, the team unafraid to pare down the language to its quintessential elements. This was also the first time stability guarantees would be offered, a quality notoriously absent before. </p>
<p>Soon after, in 2016, Firefox <a href="https://hacks.mozilla.org/2016/07/shipping-rust-in-firefox/" target="_blank" rel="nofollow noopener noreferrer">shipped its first production Rust code</a>. The industry and community started to take notice, and Rust began its impressive, and as yet unbroken, <del>four</del> <a href="https://stackoverflow.blog/2020/01/20/what-is-rust-and-why-is-it-so-popular/" target="_blank" rel="nofollow noopener noreferrer">five year streak</a> as Stack Overflow's most beloved language. [<em>Thank you James Munns for pointing out it's now actually five years</em>]. </p>
<p>Right from the outset, Rust set out with a clear focus on   building an inclusive community. They, in turn, have contributed to Rust's impressive technical aptitude, but have also fostered a sense of reverence and fondness not often witnessed in other languages. Are they crazy zealots or onto something?</p>
<h2 id="why-rust">Why Rust?</h2>
<blockquote>
<p><em>The performance of C++ with the convenience of C#</em></p>
</blockquote>
<p>This was the first time Rust hijacked my attention. C++ enjoys a long-standing ubiquity, in part, due to its ability to express zero cost abstractions. As explained by Bjarne Stroustrup, the creator of C++:</p>
<blockquote>
<p><em>What you don't use, you don't pay for. And further: What you do use, you couldn't hand code any better.</em></p>
</blockquote>
<p>It's easy to spot the relevancy to games. Making frame-rate while simulating entire worlds is a daunting performance challenge. Indeed, C++ underpins the bulk of game engines. There simply is <a href="https://www.youtube.com/watch?v=ltCgzYcpFUI" target="_blank" rel="nofollow noopener noreferrer">no other industrial language</a> that offers the same speed and low-level control, whilst writing programs in the large. </p>
<p>C++, however, suffers from the weight of its legacy. The accumulation of features over 40 years makes for a complex and intricate language. In the last decade modernisation of the standard has done well to uplift it from its C roots, but the experienced programmer must build up an arcane lore of which features are blessed, and which machinery is dangerous. As Stroustrup again describes:  </p>
<blockquote>
<p>Within C++, there is a much smaller and cleaner language struggling to get out.   </p>
</blockquote>
<p>This makes the language daunting and difficult to approach for beginners. In <a href="https://boats.gitlab.io/blog/post/zero-cost-abstractions/" target="_blank" rel="nofollow noopener noreferrer">this blog post</a>, Rust contributor <em>withoutboats</em> defines an import quality about abstraction:  </p>
<blockquote>
<p>A zero cost abstraction, like all abstractions, must actually offer a better experience than the alternative.</p>
</blockquote>
<p>So yes, of course, C++ offers a better time than your own hand wrought assembly. However, this is making the subtle point that it's competing against a secondary force: a more expensive abstraction that justifies its cost by being more comfortable and convenient.</p>
<p>We see this writ large in the rise of popular game engines that eschew the complexity of C++, the most notable being Unity. End users write code in C#, a more forgiving and ergonomic language, creating a boon in developer productivity and a reduction in iteration time.  </p>
<p><img src="https://thefuntastic.com/blog/2020-10-Unity-Interest.png" title="Unity interest over time in search trends"></p>
<p>In large codebases though, near the edge of the performance envelope, this trade-off begins to bite. The garbage collector eliminates an entire category of errors by removing responsibility for memory management from the end-user. However as its workload grows, so do periodic performance spikes antithetical to smooth gameplay.  </p>
<p><img src="https://thefuntastic.com/blog/2020-11-GC-spikes.png" title="Unity interest over time in search trends"></p>
<p>The experienced developer can still create a performant experience, however, this demands plugging the leaks in the abstraction. They must build a mental model of the machinery behind the curtain, a collection of arcane wisdom that bans many of the original conveniences, lest they disturb the garbage collector. </p>
<p>So development teams face a choice. Better resourced AAA studios generally choose Unreal or in-house engine tech built on C++, able to absorb the overhead for long term gain. Less resourced studios optimise for time to market, choosing Unity, or one of the many other accessible game making tools (Godot, Haxe, Game Maker, etc.). They often postpone performance concerns until after business eligibility is secured.   </p>
<p>Rust, however, for the first time, promises a third way. A world where it's possible to write zero cost abstractions without sacrificing higher-order elegance. </p>
<h3 id="ownership-based-memory">Ownership based memory</h3>
<p>To understand Rust's special sauce, we're going have to talk about ownership and how it handles memory. This is only a simple sketch, but <a href="http://intorust.com/tutorial/ownership/" target="_blank" rel="nofollow noopener noreferrer">in-depth resources</a> exist for the curious. </p>
<p>Writing optimised code is often about taking the way we, as humans, naturally think of an idea or algorithm, and instead expressing it in terms that favour the computer. This act often harms the legibility and understanding of a program, which makes it much harder for us, the humans, to reason about its correctness. </p>
<p>In a manually managed language, like C, the hapless programmer is left responsible for the machinations of the machine. They must take great care to ensure data is appropriately loaded into memory before operation, and then responsibly disposed of afterwards. A difficult dance in which missteps either cause dramatic crashes or else subtle and hard to detect vulnerabilities. But these are the very same tools that allow careful users to tune performance.  </p>
<p>At the other end of the spectrum, garbage collection promises the programmer it will automatically deal with the problem on their behalf. They are now free to express code naturally, but in doing so, it ties hands behind their back. They no longer have, at least not without indirection, the levers needed to wring out maximal performance.</p>
<p>Rust begins from a different premise. Rather than hiding this complexity, it accepts that computers are hard for humans, and instead tries to save us from the dangerous bits. Users can still tune the machine, but with less rope to wrap around their necks. </p>
<p>In the same way that static typing exists, very clever people have figured out how to make the compiler eliminate a whole category of memory and concurrency errors. To achieve this, Rust makes a bargain with the developer: </p>
<blockquote>
<p>"I'm going to keep track of the lifetime of every piece of memory in your program for you. This way, I can detect the moment you're no longer using it and safely free it on your behalf. But in return, I'm going to need you to follow <a href="https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html" target="_blank" rel="nofollow noopener noreferrer">strict rules</a> about the ownership of that memory. If you try to use it outside of the scope that owns it, my humourless friend here, the borrow checker, is going to make sure you don't hurt yourself."</p>
</blockquote>
<p>However, like static typing, this lunch isn't free. Rust is known to have a steep learning curve, "fighting the borrow checker" becomes a right of passage. It takes time to learn this new paradigm. Ownership makes some familiar patterns difficult or impossible and demands new ones be learnt in their place. Perhaps we should revise our earlier statement as: "The performance of C++ with the <del>convenience</del> safety of C#"</p>
<h2 id="unpacking-rusts-popularity">Unpacking Rust's Popularity</h2>
<p>Early adopters have a selfish reason to extol the virtues of their chosen technology, as widespread adoption enhances the return on their risky investment. In this respect, while interest is high but opportunities for real-world exposure are limited, is it possible that Rust is cresting a wave of unearned hype? <a href="https://matklad.github.io/2020/09/20/why-not-rust.html" target="_blank" rel="nofollow noopener noreferrer">Not every javascript or python developer</a> interested in the language, for example, has a use case that merits the additional complexity.</p>
<p>To a developer standing on the shores of 2010, <code>git</code>, a new version control system with a steep learning curve, may have seemed like a risky investment. But, in the ensuing world of Github, it's hard to argue the effort was wasted, even if some workloads (i.e. large games) still require alternatives.</p>
<p>In a similar vein, how can we qualify Rust's popularity as a meaningful signal? Ultimately, we will only know by the volume of mud we've dug through in the trenches, and admittedly, it is far too early to collect this data for games. </p>
<p>In other industries, though, early reports of Rust are effusive. <a href="https://medium.com/the-innovation/how-microsoft-is-adopting-rust-e0f8816566ba" target="_blank" rel="nofollow noopener noreferrer">Mircosoft</a>, <a href="https://developers.libra.org/docs/community/coding-guidelines" target="_blank" rel="nofollow noopener noreferrer">Facebook</a>, <a href="https://aws.amazon.com/blogs/opensource/aws-sponsorship-of-the-rust-project/" target="_blank" rel="nofollow noopener noreferrer">Amazon</a>, <a href="https://www.wired.com/2016/03/epic-story-dropboxs-exodus-amazon-cloud-empire/" target="_blank" rel="nofollow noopener noreferrer">Dropbox</a>, <a href="https://blog.cloudflare.com/tag/rust/" target="_blank" rel="nofollow noopener noreferrer">Cloudflare</a> all have Rust deployed in production. The <a href="https://www.phoronix.com/scan.php?page=news_item&amp;px=Linux-Kernel-Rust-Path-LPC2020" target="_blank" rel="nofollow noopener noreferrer">Linux Kernel</a> and <a href="https://www.chromium.org/Home/chromium-security/memory-safety/rust-and-c-interoperability" target="_blank" rel="nofollow noopener noreferrer">Chrom…</a></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://thefuntastic.com/blog/why-rust-is-the-future-game-dev">https://thefuntastic.com/blog/why-rust-is-the-future-game-dev</a></em></p>]]>
            </description>
            <link>https://thefuntastic.com/blog/why-rust-is-the-future-game-dev</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033247</guid>
            <pubDate>Mon, 09 Nov 2020 10:16:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Continued fractions and the square root of 3]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033224">thread link</a>) | @ColinWright
<br/>
November 9, 2020 | https://www.flyingcoloursmaths.co.uk/continued-fractions-and-the-square-root-of-3/ | <a href="https://web.archive.org/web/*/https://www.flyingcoloursmaths.co.uk/continued-fractions-and-the-square-root-of-3/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
						<p>I’m a Big Fan of both <span data-cites="standupmaths">@standupmaths</span> and <span data-cites="sparksmaths">@sparksmaths</span>, two mathematicians who fight the good fight.</p>
<p>I was interested to see Ben <a href="https://www.youtube.com/watch?v=qknGhrmBZvA&amp;feature=youtu.be">tackling the square root of 3</a> using the ‘long division’ method. It’s a method I’ve tried hard to love. It’s a method I just can’t bring myself to do or recommend. <span data-cites="colinthemathmo">@colinthemathmo</span> has an explainer <a href="https://www.solipsys.co.uk/new/SquareRootByLongDivision.html?RSS">here</a>, probably as nice an explainer as the method permits.</p>
<p>I have two alternative methods for finding the square root of three.</p>
<p>The first, Ben alludes to in the video: use the binomial expansion. My instinct is to find a number $3k^2$ (where $k$ is an integer) that’s fairly close to an even power of 10 - for example, $108 = 3(6^2)$.</p>
<p>Now you can apply the binomial expansion: $6\sqrt{3} = \left( 10^2 +8\right)^{1/2} = 10 + \frac{1}{2}\left(\frac{1}{10}\right)(8) + \frac{-1}{8}\left(\frac{1}{1000}\right)(64) + \dots$. This will converge pretty quickly, at least one decimal digit per term, but the divisions quickly become cumbersome and if you’re going to use fractions continually, you may as well…</p>
<p>… use continued fractions. Or a matrix version thereof.</p>
<p>Like Ben, I’m going to skip the derivation and jump straight to the method: the key matrix is $\mathbf {M}_n =  \mattwotwo{1}{1}{2}{3}^n$ - for a large value of $n$.</p>
<p>Applying this matrix to $\mattwotwo{0}{1}{1}{1}$ gives a two-by-two matrix in which the bottom-right element divided by the bottom-left one is a good approximation to $\sqrt{3}$. Perhaps more simply, this is just the sum of the bottom row of $\bb M$ divided by its bottom right element.</p>
<p>And we can be clever about calculating $\mathbf M_n$: repeatedly squaring the matrix gives us answers that approach $\sqrt{3}$ quite rapidly - although the multiplications can get big quite quickly.</p>
<p>The approximation from $\bb M_1$ is $\frac{5}{3}$, which is not a bad starting point.</p>
<p>From $\bb M_2$, it’s $\frac{19}{11} \approx 1.727$, which is not at all far from the true answer.</p>
<p>Squaring $\bb M_2$ gives $\bb M_4$, and the approximation from there is $\frac{265}{153}$ - the numbers are already getting big, but we’re within $2.5\times 10^{-5}$ of the true answer.</p>
<p>Squaring $\bb M_4$ gives $\bb M_8$, where numbers are in the ten-thousands; the estimate is now $\frac{51409}{29681}$, which is around $6\times10^{-10}$ away from $\sqrt{3}$.</p>
<p>The operations here are not <em>trivial</em><a href="#footnote_0_8385" id="identifier_0_8385" title="hush, sensei">1</a>, but they’re more tedious than difficult. Multiplying and adding large numbers is typically easier than long division – and in fact, multiplying repeatedly by $\bb M_1$ is not all that hard, even with big numbers (I believe each extra matrix gives roughly one decimal place of accuracy).</p>
<p>You’re left with one big division to do at the end, which <em>is</em> a bit more difficult than the other sums – but it’s just one division!</p>
<p>Now, Sir Isaac may not have had continued fractions available to him - but the maths involved here is certainly achievable on quill and parchment. I’m definitely not saying this is the method Ben should have used in the video (I mean, the whole point was to do it an absurd and 17th-century way), but figured it was a nice method to share.</p>
<div>
                        <p><a href="https://www.flyingcoloursmaths.co.uk/author/admin/"><img alt="" src="https://secure.gravatar.com/avatar/2882d12afb7b20f6db30d794567b21a1?s=80&amp;d=mm&amp;r=pg" srcset="https://secure.gravatar.com/avatar/2882d12afb7b20f6db30d794567b21a1?s=160&amp;d=mm&amp;r=pg 2x" height="80" width="80" loading="lazy"></a></p><h2>Colin</h2>
                        <p>Colin is a Weymouth maths tutor, author of several Maths For Dummies books and A-level maths guides. He started Flying Colours Maths in 2008.

He lives with an espresso pot and nothing to prove.</p></div>

<ol><li id="footnote_0_8385">hush, sensei [<a href="#identifier_0_8385">↩</a>]</li></ol>					</div></div>]]>
            </description>
            <link>https://www.flyingcoloursmaths.co.uk/continued-fractions-and-the-square-root-of-3/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033224</guid>
            <pubDate>Mon, 09 Nov 2020 10:11:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Leaky academia: digital intimacy and open secrets in times of Covid-19]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033222">thread link</a>) | @tokai
<br/>
November 9, 2020 | https://www.identitiesjournal.com/the-viral-condition-virtual-symposium/leaky-academia-digital-intimacy-and-open-secrets-in-times-of-covid-19 | <a href="https://web.archive.org/web/*/https://www.identitiesjournal.com/the-viral-condition-virtual-symposium/leaky-academia-digital-intimacy-and-open-secrets-in-times-of-covid-19">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><em>Nanna Bonde Thylstrup, <span>Copenhagen Business School, Denmark;</span>&nbsp;Zeerak&nbsp;Waseem, University of Sheffield, UK;&nbsp;and Daniela Agostinho, University of Copenhagen, Denmark</em></p><p><em>Within the context of academia, much like in other sectors of society, the ongoing pandemic has exposed inequalities that we here describe as 'open secrets'. The disclosure of such open secrets, or 'hidden truths' that were never hidden to begin with, is being facilitated through the digitally networked spaces that bring us together more than ever. Engaging with the 'viral condition' foregrounded by this virtual symposium, this essay thinks through how the virality of digital media is currently intersecting with the unfolding viral pandemic. As we find ourselves connecting in new ways, we suggest that it is time to consider the challenges posed by digital networks, the troubled intimacies they generate, and their potential to forge alliances and solidarities amidst stark and growing inequality.&nbsp;</em>&nbsp;</p><div><p>COVID-19 turned the familiar humdrum of the academic year upside down, abruptly putting a stop to everyday routines and demanding new practices. The effects of COVID-19 have been brutal across society: unemployment rates have skyrocketed, economies have gone to shambles, and hundreds of thousands have died and will continue to die and fall ill from this disease.</p><p>&nbsp;Many writers before us have pointed out how the unfolding pandemic has magnified existing inequalities, laying bare realities that were there all along. Such inequalities have also become highly visible in digitally-enabled academia, as higher education institutions have rushed to respond to the spread of the virus.</p><p>&nbsp;Across privileged Scandinavia, from where we write, professors retreated to spacious summerhouses away from urban centres, quietly enjoying their sudden liberation from academic reproductive labour, tedious meetings and departmental obligations. Some even enjoyed a surge in productivity or used their time to catch up on their reading lists. Others seized the chance to add a COVID-19 grant to their already extensive grant collection. Beyond Scandinavia, journal submissions from male authors went up (Flaherty 2020). Executive branches of academia saw new opportunities in the sudden shift to online teaching, framing the analogue-to-digital conversion as a positive disruptive force (Kandri 2020). Tech companies strongly encouraged this embrace and gained new strongholds in the educational sector, fueling disaster capitalism on campus (Turiano 2020). From this perspective, COVID-19 was not only experienced as a social tragedy, but also as an opportunity for retreat, advancement and profit.</p><p>&nbsp;Meanwhile, with their work rhythms, social lives and study habits upended, students struggle to keep afloat in small and shared accommodations. A massive pre-existing mental health crisis among the student population suddenly hits the headlines (Pedersen 2020). Without a systemic response in place, some universities reacted with quickly improvised tips on how to keep a routine and promises of a mindfulness app (Munk 2020), offering technological fixes in lieux of politically-informed responses. Feminist, anti-racist and critical disability communities quickly came together to perform the unpaid and academically unrecognised labour of gathering best practices for online teaching (Davidson 2020; Hamraie 2020; Wernimont 2020). Yet, as student and teacher frustration with online instruction accumulates over time, the already devalued labour performed by critical digital pedagogy is even more frowned upon than before the pandemic. Moreover, the socio-political critiques that underlie this body of scholarship are evacuated by utilitarian uptakes, disregarding the critique of sexism, racism and ableism in academia in favour of uncritical implementation of digital technologies. Meanwhile, the divide between tenured faculty and short-term employees is widening (Zahneiss 2020). Researchers on temporary contracts witness their working hours and job prospects disappear like grains of sand in a precarious hourglass, while universities avoid committing to contract extensions or even use the opportunity to fire temporary employees (Collini 2020). Fieldwork stalls. And journal submissions by women drop (Wiegand et al. 2020; Andersen et al. 2020).</p><p>&nbsp;Within the context of academia, much like in other sectors of society, the ongoing pandemic has exposed inequalities that we here describe as 'open secrets'. The disclosure of such open secrets, or 'hidden truths' that were never hidden to begin with, is being facilitated through the digitally networked spaces that bring us together more than ever. Engaging with the 'viral condition' foregrounded by this virtual symposium, this essay thinks through how the virality of digital media is currently intersecting with the unfolding viral pandemic. As we find ourselves connecting in new ways, we suggest that it is time to consider the challenges posed by digital networks, the troubled intimacies they generate, and their potential to forge alliances and solidarities amidst stark and growing inequality.</p><p>&nbsp;<strong>Leaky conditions – ‘hidden’ truths</strong><br>As we practice containment – stuck in rooms, apartments, buildings, cities and countries – we also experience new modes of intimacy. Through the porous digital networks that bind us, we leak into each others’ homes and lives. We screenshot and close-examine colleagues’ bookshelves and domestic backgrounds. We observe meeting participants who, forgetting they are on screen, fill dishwashers, pick noses or go to the restroom. We leak into rooms and backyards of students while our children, partners, parents and pets photobomb our lectures. We even leak into our own field of vision, our tired faces – normally out of sight – staring back at us on screen.</p><p>&nbsp;These new digital intimacies (Wiehn 2020) are not reserved for close friends and colleagues. We also leak into wider communities and data aggregates through video conferencing platforms, contact tracing apps and new higher-ed platforms. Professors conducting online classes about China in one end of the world leak into Chinese censorship apparatuses through Zoom (@letahong 2020). Researchers developing contact tracing apps in the health sector find themselves entangled in regimes of surveillance and policing (Amnesty 2020). And the rushed adoption of digital technologies in the classroom sediments infrastructures that create new value flows between big tech and higher education (Walsh 2020). Academia’s apparently contained spaces, previously upheld by physical walls and normative epistemological boundaries between the public and private spheres, now turn into intimate membranes that leak through digital networks.</p><p>&nbsp;Digital intimacies have given rise to a string of viral stories about digital transgressions: people unwittingly broadcasting their toilet visits and intimate affairs to department meetings and online classes (Vincent 2020; Smith 2020; Feldman 2020). And in turn, the racialised and sexualised abuse that occurs offline now leak into once safe spaces. Rather than merely exposing flawed privacy settings or digital illiteracy, these stories, in which the boundaries between public and private dissolve, tend to confirm the inherently porous nature of digital technologies. The leakiness of digital technologies is not accidental or anecdotal; it is built into the digital networks that bind us. Rather than premised on sealed infrastructures that shield and protect, digital technologies are meant to leak at all times (Agostinho and Thylstrup 2019; Chun 2016). Crucially, this leaky nature not only exposes domestic intimacies to the wider world; it also enables and upholds the economic model of surveillance capitalism, as it allows for massive and continuous data flows across platforms.</p><p>&nbsp;Exhausted by lockdowns and fatigued by digital screens, many of us long to return to more contained spaces: meeting rooms, classrooms, hallways and canteens that will allow us to maintain the (imagined) boundaries between public and private and navigate safe and unsafe spaces physically. But this longing for contained spaces also reveals a conservative nostalgia for spaces where privilege can thrive without being confronted by precarity and vulnerability. A space where the pre-existing inequalities are less dramatically seen and felt. Where academia’s dirty secrets can be thrown back into the closet.</p><p>&nbsp;Here we draw on queer theorist Eve Sedgwick and her landmark book <em>Epistemology of the Closet</em> (1990), where she challenges the binary ‘secrecy/disclosure’ that forms the backbone of modern society. Following Michel Foucault, Sedgwick examines sexuality (its secrecy and disclosure) as the structure of modern ways of knowing. She suggests that modern power is premised on the knowledge and withholding of secrets, or as she puts it, modern power is organized around the figure of the closet. The closet here functions as a contained space: what it contains (what is closeted) and what it spills or leaks (the act of outing) structures the modern organisation of knowledge, what is supposed to be known and what is supposed to remain unknown. As Claire Hemmings puts it, the 'closet is the open secret through which difference and inequality are both obscured and played out in front of our eyes in plain sight' (Hemmings 2020).</p><p>&nbsp;The closet of society’s open secrets has been further challenged by the intersection of the pandemic with digital connectivity. Within academia, the shared (if unequally felt) condition of COVID-19 and the unprecedented intimacy of digital media laid bare the 'hidden' truths of academic inequality, both locally and globally. We use scare quotes around 'hidden' to emphasise how these inequalities were never actually <em>hidden</em>. Instead they were hiding in plain sight, but only the privileged could afford to look away: the unequal distribution of reproductive labour, falling along gendered and racialised lines; the previous exclusion …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.identitiesjournal.com/the-viral-condition-virtual-symposium/leaky-academia-digital-intimacy-and-open-secrets-in-times-of-covid-19">https://www.identitiesjournal.com/the-viral-condition-virtual-symposium/leaky-academia-digital-intimacy-and-open-secrets-in-times-of-covid-19</a></em></p>]]>
            </description>
            <link>https://www.identitiesjournal.com/the-viral-condition-virtual-symposium/leaky-academia-digital-intimacy-and-open-secrets-in-times-of-covid-19</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033222</guid>
            <pubDate>Mon, 09 Nov 2020 10:10:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Think Piece on Privacy and Big Data]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033030">thread link</a>) | @Rohitha_Perera
<br/>
November 9, 2020 | https://talk.hyvor.com/blog/privacy-and-big-data/ | <a href="https://web.archive.org/web/*/https://talk.hyvor.com/blog/privacy-and-big-data/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p><span></span>
<span>116</span>
</p>
<p>The majority of us in the present and in the immediate future <a href="https://www.beyondtrust.com/blog/entry/exactis-data-breach-paving-road-data-dystopia-us-gdpr">will face the issue of Privacy</a>. Consider this basic thought, which may sound like science fiction, but is actually quite present today: Thanks to the devices we wear now, <a href="https://www.beyondtrust.com/blog/entry/exactis-data-breach-paving-road-data-dystopia-us-gdpr">the harvesting of our biometric data</a> is a possibility. It is this thought process that led to this think piece on privacy and big data.</p>
<p>Corporations can get to know us far better than we know ourselves. They can then not just predict our feelings but also manipulate our feelings. Monitoring of our biometrics can make episodes like that of Cambridge Analytica’s data hacks prehistoric in comparison. </p>
<p>Remember that <a href="https://www.cheatsheet.com/money-career/heres-much-google-facebook-really-think-youre-worth.html/">you are worth quite a bit of money to the social channels </a>you use. The podcast detailing the <a href="https://podcasts.google.com/?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9IY09mTE41Mg&amp;ep=14&amp;episode=MjM0YjM1OTgtZTQ1Mi00NGZhLTkzYjUtMTQxMGJjZTY1NGE4">Congressional Antitrust Investigation on Tech Monopolies: Google, Facebook, Amazon, and Apple</a> gives a serious look at how powerful these big data companies are. The scale is astronomical. Amazon captures 70% of all retails in the United States. They literally have seven times the revenue of their next largest competitor. </p>
<p>Now imagine their cloud computing capabilities. Take stock of the number of iPhones that are there. We know that what you share on social media, and the information that you surrender is a Big Data Issue; and, consider the number of search results Google controls. There’s information about you being harvested. You should know how your information is being used. </p>
<h2>Some Background</h2>
<p>We hear of how <a href="https://www.theguardian.com/us-news/2018/mar/22/steve-bannon-on-cambridge-analytica-facebook-data-is-for-sale-all-over-the-world">Steve Bannon used Facebook</a> to change politics and change culture. Facebook data, algorithms and narratives were his key weapons. These tools were used by the <a href="https://www.reuters.com/article/us-facebook-cambridge-analytica-kogan-idUSKBN1GX2F6">Cambridge Analytica team to identify the dark triad</a> — Narcissism, Machiavellianism and Psychopathy — in people. We now know about the Russian interference in American politics. We know how data had been manipulated to channel the latent proclivities of racism and anti-Semitism within America to divide it. </p>
<p>The same podcast makes mention of a great knowledge-infused book, which is Shoshana Zuboff’s <a href="https://youtu.be/QL4bz3QXWEo">The Age of Surveillance Capitalism</a>. In this book, Zuboff details the rise of a new form of power which will forever change our lives. By collecting behavioral data from their users, corporations have amassed an incomprehensibly large and detailed picture of our personal lives. They use this data to expand their corporate power and profitability. This, of course, has tremendous consequences for our privacy, but also for our political system.</p>
<figure><img src="https://talk.hyvor.com/blog/wp-content/uploads/2020/10/big-5-personality.jpg" alt="" data-src="https://talk.hyvor.com/blog/wp-content/uploads/2020/10/big-5-personality.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="><figcaption>Surveillance Capitalism talks of <a href="https://www.simplypsychology.org/big-five-personality.html">The Five Factor Personality Test</a>, which helps companies like Facebook infer our political proclivities and sexuality. This is based on the predictive signals based on the punctuation we use on a Facebook status. </figcaption></figure>
<p>Surveillance Capitalism is where it universally claims our private human experience as their free source of raw material. They take the rich predictive signals in our behavior and convert it into data. We hear of <a href="https://arstechnica.com/information-technology/2017/05/facebook-helped-advertisers-target-teens-who-feel-worthless/#:~:text=Leaked%202017%20document%20reveals%20FB,exploit%20teens'%20words%2C%20images.&amp;text=Facebook's%20secretive%20advertising%20practices%20became,of%20the%20company's%20Australian%20office.">Facebook executives who promote&nbsp;advertising campaigns that exploit Facebook users’ emotional states</a>. Facebook’s algorithms can determine, and&nbsp;allow&nbsp;advertisers to pinpoint, “moments when young people need a confidence boost.”&nbsp;97% of Facebook’s revenue comes from its online targeted advertising markets. These are wholly-owned and operated in this surveillance capitalist economic logic. </p>
<p>Readers of Cathy O’Neil’s Weapons of Math Destruction will be compelled to believe the potential dangers of big data. O’Neil, a mathematician, analyses how the use of big data and algorithms in a variety of fields. These include insurance, advertising, education, and policing. They can lead to decisions that harm the poor, reinforce racism, and amplify inequality. Mathematicians and statisticians were for a very long time studying our desires, movements, and spending power. This is the Big Data economy we are living in. </p>
<h2>Trust is Important </h2>
<p>Consumers are more conscious of their data privacy than ever. A recent <a href="https://tealium.com/resource/whitepaper/how-brands-can-prioritize-privacy-in-the-age-of-data/">Tealium study</a> on consumer data privacy found that 97% of consumers surveyed said they are somewhat or very concerned about protecting their data. <a href="https://www.accenture.com/t20171220T024439Z__w__/us-en/_acnmedia/PDF-68/Accenture-Global-Anthem-POV.pdf#zoom=50">Research by Accenture</a>&nbsp;shows that&nbsp;88% of<strong> </strong>consumers say companies that provide personalized experiences without compromising their trust are more appealing and can relate to their needs better than others.</p>
<p>We are focusing on Facebook on this particular blog post to quite a degree since it is the one singular social medium that is growing exponentially. One of the ways in which Facebook garners your data is with you revealing your data and your intentions via the act of publishing status updates and even commenting. You see, the act of commenting fulfills just one touchpoint in the process of these tech giants harvesting of data. Facebook built&nbsp;<a rel="noreferrer noopener" href="https://developers.facebook.com/docs/plugins/comments/" target="_blank">comments plugin</a>&nbsp;to allow users to leave comments on websites, blogs and forums through their Facebook accounts. It was expected to provide high-quality conversations over the internet but instead ended up spamming popular sites.</p>
<p>If you do use the Facebook Comments plugin, remember that your comments are a valuable content asset that shouldn’t be subject to <a href="https://ducttapemarketing.com/how-and-why-i-use-the-facebook-comments-plugin/">Facebook’s Terms of Service</a>, which basically says they can do whatever they want with them. An increasing amount of spam raises questions about how well the policy of malicious content online is going on. There are many misleading and offensive comments, usually attracting and persuading users towards a specific link to click it. These comments are often repetitive and can easily be identified as spam.</p>
<p>According to an estimation by&nbsp;<a href="https://www.similartech.com/technologies/facebook-comments">Similartech</a>&nbsp;more than 360,000 unique domains have installed Facebook Comments plugin. It is still not clear why and how the spam filters of Facebook failed to filter spam comments. <a href="https://www.similartech.com/technologies/facebook-comments">In 2015</a>, one of the security firms, Symantec reported scammers had been trying to affect the comments sections of Facebook to spread malware. </p>
<p>For more than two years now, Facebook has been working on its content-moderation efforts and the spamming in Facebook Comment boxes shows that problematic content still finds its way to escape the loopholes. Moreover, <a href="https://www-dailymail-co-uk.cdn.ampproject.org/v/s/www.dailymail.co.uk/sciencetech/article-2525227/amp/Facebook-tracks-type-DONT-post-update-comment.html?amp_js_v=a6&amp;amp_gsa=1&amp;usqp=mq331AQFKAGwASA%3D#aoh=16034519192504&amp;referrer=https%3A%2F%2Fwww.google.com&amp;amp_tf=From%20%251%24s&amp;ampshare=https%3A%2F%2Fwww.dailymail.co.uk%2Fsciencetech%2Farticle-2525227%2FFacebook-tracks-type-DONT-post-update-comment.html">Facebook can track what </a><a href="https://www-dailymail-co-uk.cdn.ampproject.org/v/s/www.dailymail.co.uk/sciencetech/article-2525227/amp/Facebook-tracks-type-DONT-post-update-comment.html?usqp=mq331AQFKAGwASA%3D&amp;amp_js_v=0.1#aoh=16034519192504&amp;referrer=https%3A%2F%2Fwww.google.com&amp;amp_tf=From%20%251%24s&amp;ampshare=https%3A%2F%2Fwww.dailymail.co.uk%2Fsciencetech%2Farticle-2525227%2FFacebook-tracks-type-DONT-post-update-comment.html">you</a><a href="https://www-dailymail-co-uk.cdn.ampproject.org/v/s/www.dailymail.co.uk/sciencetech/article-2525227/amp/Facebook-tracks-type-DONT-post-update-comment.html?amp_js_v=a6&amp;amp_gsa=1&amp;usqp=mq331AQFKAGwASA%3D#aoh=16034519192504&amp;referrer=https%3A%2F%2Fwww.google.com&amp;amp_tf=From%20%251%24s&amp;ampshare=https%3A%2F%2Fwww.dailymail.co.uk%2Fsciencetech%2Farticle-2525227%2FFacebook-tracks-type-DONT-post-update-comment.html"> type</a>, even if you never post it.&nbsp;Data scientists can determine that a status or comment has been typed by tracking code in the HTML form element of each page.</p>
<h2>Read The Terms and Conditions</h2>
<p>We live in a world where the concept of privacy already seems outdated. But that is largely because we’ve decided not to inquire about what happens when we trade it for convenience. The more connected you, and billions of others, are to Facebook, the more money Facebook makes by selling your personal information, and the more powerful it becomes.</p>
<p>The terms of service state,&nbsp;<em>We use the data we have — for example, about the connections you make, the choices and settings you select, and what you share and do on and off our Products — to personalize your experience.</em></p>
<figure><img loading="lazy" width="746" height="634" src="https://talk.hyvor.com/blog/wp-content/uploads/2020/10/DcrJMbSW0AAxJXC.jpg" alt="" srcset="https://talk.hyvor.com/blog/wp-content/uploads/2020/10/DcrJMbSW0AAxJXC.jpg 746w, https://talk.hyvor.com/blog/wp-content/uploads/2020/10/DcrJMbSW0AAxJXC-300x255.jpg 300w" sizes="(max-width: 746px) 100vw, 746px" data-srcset="https://talk.hyvor.com/blog/wp-content/uploads/2020/10/DcrJMbSW0AAxJXC.jpg 746w, https://talk.hyvor.com/blog/wp-content/uploads/2020/10/DcrJMbSW0AAxJXC-300x255.jpg 300w" data-src="https://talk.hyvor.com/blog/wp-content/uploads/2020/10/DcrJMbSW0AAxJXC.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="><figcaption><a href="https://qz.com/1266835/facebooks-terms-of-service-translated-so-you-understand-your-data-and-privacy-settings/">Be wary of nebulous terms and promises</a></figcaption></figure>
<p>Basically, this means that Facebook uses every bit of personal information it can, collected&nbsp;<a href="https://www.consumerreports.org/privacy/how-facebook-tracks-you-even-when-youre-not-on-facebook/">both on and off Facebook</a>, to entice advertisers. The better the company knows you through the personal information you share with your friends and family, the more likely they are to be able to sell you stuff you want.</p>
<h2>Choose The Right to Privacy</h2>
<p>Big data is big business and value is created from customer insight. But, where is the moral line? What happens when companies cross that line? What if consumers could flip the equation to offer their data directly to the companies they trust? The future could be customer-monetized data.</p>
<p>We are the authors of our own destruction here since we don’t choose to be aware. If you participate in Facebook, should you not have some semblance of an expectation of privacy. The former Federal Trade Commission Chairperson Jon Leibowitz publicly stated, “We all agree that consumers don’t read privacy policies.”</p>
<p><a href="https://talk.hyvor.com/docs/gdpr">Ensure you choose privacy</a> and are aware of how technology plans on using your data. The only solution is being non-participatory. The solution is choosing not to be part of a pernicious agenda that can be defined as Surveillance Capitalism. </p>
<div><div><div><h4>
Need a privacy-focused commenting platform for your website?
</h4>

</div></div></div> </div></div>]]>
            </description>
            <link>https://talk.hyvor.com/blog/privacy-and-big-data/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033030</guid>
            <pubDate>Mon, 09 Nov 2020 09:37:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pijul: Towards 1.0]]>
            </title>
            <description>
<![CDATA[
Score 159 | Comments 102 (<a href="https://news.ycombinator.com/item?id=25032956">thread link</a>) | @lelf
<br/>
November 9, 2020 | https://pijul.org/posts/2020-11-07-towards-1.0/ | <a href="https://web.archive.org/web/*/https://pijul.org/posts/2020-11-07-towards-1.0/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p>
                We are looking for <strong>VC funding</strong>. If you are interested in helping us build the future of collaboration (for code and other documents), shoot us an email at <a href="mailto:contact@pijul.org">contact@pijul.org</a>.
                
            </p>


<p>Saturday, November 7, 2020</p>
<p>After fixing the performance and scalability problems, we’re on our way to getting a stable Pijul. In this post, I explain what I’ve been up to in the recent months.</p>
<h2 id="context">Context</h2>
<p>Pijul has always been advertised as a research project, trying to implement a theory of patches that would be sound and fast. This is an ambitious goal, and became even more ambitious than initially envisioned.</p>
<p>One of the hardest challenges is that source code is by essence stateful, which makes it much harder to iterate over algorithm designs, like normal research projecst need to. For example, in order to get from our last published version to our current design, we have gone through many different variants, and there wasn’t much to publish.</p>
<p>Moreover, the UX aspect is what matters most in the end, and testing it on a real world project is the only way to get it there. However, unlike in a compiler, where bootstrapping is done one step at a time, and previous versions are always available to compile your current one, a version control system has the additional problem that the previous versions might not always be easily accessible if there is a bug.</p>
<p>One of the criticisms I’ve heard since I realised that better datastructures were possible is that I was “working secretely”. I certainly understand this feeling, but this is based on a misunderstanding of how research works. When I first had the idea that I’m explaining in this post, I realised that a complete rewrite would be needed. But for a very long time, almost nothing other than unusable, unreadable prototypes happened.</p>
<p>Back then, there wasn’t much to show, since it wasn’t even clear that the basic datastructure would work. And even when they started working at a large enough scale, it took me quite a bit of testing on large repositories before they started actually working.</p>
<p>This also implies that there wasn’t much to show for quite a while, since the new algorithm wasn’t usable until very recently, and any repository started before now would have become obsolete in a matter of days.</p>
<p>There were also <a href="#a-personal-note">persoprofessional reasons</a> for this silence, described at the end of this post.</p>

<p>Pijul depends on two other projects I’ve started.</p>
<h3 id="sanakirja">Sanakirja</h3>
<p>One of these projects is Sanakirja, which is “just” a key-value store, but has the extra feature that databases can be cloned efficiently. I would have loved to just use an existing library, but there just isn’t any that has this cloning feature. However, the scope of Sanakirja is still quite modest, it does one thing and does it well. Obviously, it took some time to find the memory-management bugs, but I have good confidence that this is now done.</p>
<p>In previous releases of Pijul, databases were implemented with a single mmapped file containing the binary representation of B Trees. Despite their lower writing performance (compared to alternatives such as <em>Log-structured merge-trees</em>), and the complexity of the code for deletions, B Trees are very well suited to this use case: indeed, since they are trees, reference-counting the nodes is enough to implement efficient clones.</p>
<p>One of the remaining issues was that in order to grow the database, we needed to un-mmapped the file, grow it, and mmap it again. Since applying a single change in Pijul must be an atomic operation, we needed to cancel the transaction when that happened, and restart it with a bigger file.</p>
<p>Another issue is that I wanted the next libpijul to compile on platforms that don’t have mmap, such as WASM. However, if reallocating an mmapped file has a very low complexity (even though it does have a non-zero cost in terms of system calls), reallocating a chunk of memory often requires copying everything. This completely defeats the point of the algorithms in Pijul, which rely on a particular representation of the datastructures on the disk.</p>
<p>The main innovation in Sanakirja 0.13 is to use a vector of memory blocks (either in memory or mmapped from a file), of exponentially-increasing size. The overhead is just one extra indirection, the complexity of adding items is the same (since the operation of creating an extra block is $O(1)$). The exponentially-increasing sizes mean that the allocated memory is always at least half-full.</p>
<h3 id="thrussh">Thrussh</h3>
<p>The other one is Thrussh. That library implements the SSH protocol, and tries to handle a number of key formats. The former is a surprisingly easy goal, and keeping up with Tokio versions has historically been the hardest bit, while the latter is the most horrendous hydra-like task, with new heads and legacy formats showing up every time you think you’re done.</p>
<h2 id="how-repositories-used-to-work-and-still-do-to-some-extent">How repositories used to work (and still do, to some extent)</h2>
<p>Old-style repositories represented a single file by a directed graph $G = (V, E)$ of lines, where each vertex $v\in V$ represented a line, and an edge from $u \in V$ to $v\in V$, labelled by some change (also called patch) number $c$, could be read as “according to change $c$, line $u$ comes before $v$”.</p>
<p>This means that changes could introduce vertices and lines, as in the following example, where a line $D$ is introduced between $A$ and $B$:</p>
<p><img src="https://pijul.org/img/repos-line-add.svg">
</p>
<p>Here, the thick line represents the change from the file containing the lines $A$, $B$, $C$ to the file with the new line $D$.
An important feature to note is that <strong>vertices are uniquely identified</strong>, by the hash of the change that introduced them, along with a position in that change. This means that two lines with the same content, introduced by different changes, will be different. It also means that a lines keeps its identity, even if the change is applied in a totally different context.</p>
<p>Moreover, this system is append-only, in the sense that <em>deletions</em> are handled by a more sophisticated labelling of the edges. In the example above, if we want to delete line $D$, we just need to make a change mapping the edge introduced by $c_0$ to a deleted edge, which we label by the name $c_1$ of the change that introduces it:</p>
<p><img src="https://pijul.org/img/repos-line-del.svg">
</p>
<p>From now on, we call the full edges <strong>alive</strong>, and the dashed ones <strong>dead</strong>.</p>
<p>We have just described the two basic kinds of actions in Pijul. There are no other. One kind adds vertices to the graph, along with “alive” edges around them, and the other kind maps an existing edge label onto a different one.
In order to fully described the system, I also need to mention that the edge labels are given by two parameters: their status (alive, deleted, and a few others related to multiple files and technical details explained below) and the change that introduced them.</p>
<h3 id="dependencies">Dependencies</h3>
<p>This scheme allows to defines dependencies between changes:</p>
<ul>
<li>
<p>If a change $c$ adds a vertex, we must have its <em>“context”</em>, i.e. the lines before and after it, hence the changes that introduced these lines are in the dependencies of $c$.</p>
</li>
<li>
<p>If a change $c$ deletes a vertex, or in other words maps an existing edge introduced by a change $d$, then $c$ must depend on $d$.</p>
</li>
</ul>
<p>Of course, this is just the minimal set of dependencies needed to make sense of the text edits. Hooks and scripts may add extra language-dependent dependencies based on semantics.</p>
<h3 id="are-edge-labels-minimal">Are edge labels minimal?</h3>
<p>Our goals is to find the smallest possible system, both for reasons of mathematical aesthetics (why store useless stuff?) and the other one for performance. Therefore, one immediate question comes to mind: why even keep the change number on the edges?</p>
<p>In order to answer that question, suppose we don’t keep the labels, meaning that the maps happen between statuses only. Then, consider the following two situations:</p>
<ul>
<li>
<p><strong>Change inverses</strong></p>
<p>The first issue happens when two authors delete a line in parallel, and one of the authors reverts their change. Applying these changes yields the following diagram, where the two deletions get merged into one, and the inverse applies to both:</p>
 <p><img src="https://pijul.org/img/inverse2.svg">
 </p>
<p>However, this is not what we expect, since one of the authors explicitly reverted the deletion, while the other performed the same deletion in parallel.
By keeping the labels, this is what we get instead:</p>
 <p><img src="https://pijul.org/img/inverse3.svg">
 </p>
</li>
<li>
<p><strong>Missing contexts</strong></p>
<p>For the sake of clarity, in the rest of this post, we name two users Alice (with pronouns “she/her”) and Bob (with pronouns “he/his”).</p>
<p>This situation, where Alice writes something in the middle of a paragraph $p$, while Bob deletes $p$ in parallel.
One issue here, is that the situation is not symmetric: when Bob applies Alice’s change, he can tell immediately that something is wrong, because the context of Alice’s edits is labelled as deleted in his repository.</p>
 <p><img src="https://pijul.org/img/known-vertices1.svg">
 </p>
<p>However, Alice’s situation is different: indeed, consider the case where instead of deleting $p$ <em>in parallel</em> of her changes, Bob deleted $p$ after applying Alice’s change. The edges deleted are exactly the same, but this is not a conflict, as shown in the following diagram:</p>
 <p><img src="https://pijul.org/img/known-vertices2.svg">
 </p>
<p>The situation is further complicated by the fact that this system doesn’t behave symmetrically with the contexts above and below the new line. Indeed, if Bob deleted the <em>down context</em> of the line (i.e. if he deleted line $C$) instead of the <em>up context</em> (line $B$), Alice could detect the conflict, since in that case, $C$ would have both an alive and a dead edge pointing to it ($C$ is called a “zombie vertex” internally), as shown in the following diagram:</p>
 <p><img src="https://pijul.org/img/known-vertices0.svg">
 </p>
<p>Keeping the change identifiers on each edge allows us to solve this. In Pijul 0.12, Bob would add the labels of all the edges around the deleted lines to the dependencies of his change. Then, Alice can tell whether Bob knows of her change before applying it. The changes are conflict if and only if Bob doesn’t know of the new lines.</p>
<p>However, this behaviour was counter-intuitive, <a href="https://discourse.pijul.org/t/why-these-patches-dont-commute/449">as noted by @tae</a>.</p>
<p>A finer analysis of what dependencies are led to a different behaviour in the new Pijul. Changes now have two different sets of …</p></li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pijul.org/posts/2020-11-07-towards-1.0/">https://pijul.org/posts/2020-11-07-towards-1.0/</a></em></p>]]>
            </description>
            <link>https://pijul.org/posts/2020-11-07-towards-1.0/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032956</guid>
            <pubDate>Mon, 09 Nov 2020 09:24:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[This is how I Git]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25032951">thread link</a>) | @stargrave
<br/>
November 9, 2020 | https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/ | <a href="https://web.archive.org/web/*/https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>Every now and then I get questions on how to work with git in a smooth way when developing, bug-fixing or extending curl – or how I do it. After all, I <a href="https://daniel.haxx.se/blog/2020/10/26/working-open-source/" data-type="post" data-id="14901">work on open source full time</a> which means I have very frequent interactions with git (and GitHub). Simply put, I work with git all day long. Ordinary days, I issue git commands several hundred times.</p>



<p>I have a very simple approach and way of working with git in curl. This is how it works.</p>



<h2>command line</h2>



<p>I use git almost exclusively from the command line in a terminal. To help me see which branch I’m working in, I have this little bash helper script.</p>



<pre>brname () {
  a=$(<code>git rev-parse --abbrev-ref HEAD 2&gt;/dev/null</code>)
  if [ -n "$a" ]; then
    echo " [$a]"
  else
    echo ""
  fi
}
PS1="\u@\h:\w\$(brname)$ "</pre>



<p>That gives me a prompt that shows username, host name, the current working directory and the current checked out git branch.</p>



<p>In addition: I use Debian’s <a href="https://salsa.debian.org/debian/bash-completion/-/blob/master/README.md">bash command line completion</a> for git which is also really handy. It allows me to use tab to complete things like git commands and branch names. </p>



<h2>git config</h2>



<p>I of course also have my customized <code>~/.gitconfig</code> file to provide me with some convenient aliases and settings. My most commonly used git aliases are:</p>


<pre title="">st = status --short -uno
ci = commit
ca = commit --amend
caa = commit -a --amend
br = branch
co = checkout
df = diff
lg = log -p --pretty=fuller --abbrev-commit
lgg = log --pretty=fuller --abbrev-commit --stat
up = pull --rebase
latest = log @^{/RELEASE-NOTES:.synced}..
</pre>


<p>The ‘latest’ one is for listing all changes done to curl since the most recent RELEASE-NOTES “sync”. The others should hopefully be rather self-explanatory.</p>



<p>The config also sets <code>gpgsign = true</code>, enables mailmap and a few other things.</p>



<h2>master is clean and working</h2>



<p>The main curl development is done in the single <a href="https://github.com/curl/curl">curl/curl</a> git repository (primarily hosted on GitHub). We keep the master branch the bleeding edge development tree and we work hard to always keep that working and functional. We do our releases off the master branch when that day comes (every eight weeks) and we provide “<a href="https://curl.haxx.se/snapshots/">daily snapshots</a>” from that branch, put together – yeah – daily.</p>



<p>When merging fixes and features into master, we avoid merge commits and use rebases and fast-forward as much as possible. This makes the branch very easy to browse, understand and work with – as it is 100% linear.</p>



<h2>Work on a fix or feature</h2>



<p>When I start something new, like work on a bug or trying out someone’s patch or similar, I first create a local branch off master and work in that. That is, I don’t work directly in the master branch. Branches are easy and quick to do and there’s no reason to shy away from having loads of them!</p>



<p>I typically name the branch prefixed with my GitHub user name, so that when I push them to the server it is noticeable who is the creator (and I can use the same branch name locally as I do remotely).</p>



<pre>$ git checkout -b bagder/my-new-stuff-or-bugfix</pre>



<p>Once I’ve reached somewhere, I commit to the branch. It can then end up one or more commits before I consider myself “done for now” with what I was set out to do.</p>



<p>I try not to leave the tree with any uncommitted changes – like if I take off for the day or even just leave for food or an extended break. This puts the repository in a state that allows me to easily switch over to another branch  when I get back – should I feel the need to. Plus, it’s better to commit and explain the change <em>before</em> the break rather than having to recall the details again when coming back.</p>



<h2>Never stash</h2>



<p>“git stash” is therefore not a command I ever use. I rather create a new branch and commit the (temporary?) work in there as a potential new line of work.</p>



<h2>Show it off and get reviews</h2>



<p>Yes I am the lead developer of the project but I still maintain the same work flow as everyone else. All changes, except the most minuscule ones, are done as pull requests on GitHub.</p>



<p>When I’m happy with the functionality in my local branch. When the bug seems to be fixed or the feature seems to be doing what it’s supposed to do and the test suite runs fine locally.</p>



<p>I then clean up the commit series with “<code>git rebase -i</code>” (or if it is a single commit I can instead use just “<code>git commit --amend</code>“).</p>



<p>The commit series should be a set of logical changes that are related to this change and not any more than necessary, but kept separate if they are separate. Each commit also gets its own proper commit message. Unrelated changes should be split out into its own separate branch and subsequent separate pull request.</p>



<pre>git push origin bagder/my-new-stuff-or-bugfix</pre>



<h2>Make the push a pull request</h2>



<p>On GitHub, I then make the newly pushed branch into a <a href="https://github.com/curl/curl/pulls">pull request</a> (aka “a PR”). It will then become visible in the list of pull requests on the site for the curl source repository, it will be announced in the #curl IRC channel and everyone who follows the repository on GitHub will be notified accordingly.</p>



<p>Perhaps most importantly, a pull request kicks of a flood of CI jobs that will build and test the code in numerous different combinations and on several platforms, and the results of those tests will trickle in over the coming hours. When I write this, we have around 90 different CI jobs – per pull request – and something like 8 different code analyzers will scrutinize the change to see if there’s any obvious flaws in there.</p>



<figure><a href="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/Screenshot_2020-11-05-curl-Project-status-dashboard.png"><img loading="lazy" width="2686" height="1510" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/Screenshot_2020-11-05-curl-Project-status-dashboard.png" alt=""></a><figcaption>CI jobs per platform over time. Graph snapped on November 5, 2020</figcaption></figure>



<h2>A branch in the actual curl/curl repo</h2>



<p>Most contributors who would work on curl would not do like me and make the branch in the curl repository itself, but would rather do them in their own forked version instead. The difference isn’t that big and I <em>could</em> of course also do it that way.</p>



<h2>After push, switch branch</h2>



<p>As it will take some time to get the full CI results from the PR to come in (generally a few hours), I switch over to the next branch with work on my agenda. On a normal work-day I can easily move over ten different branches, polish them and submit updates in their respective pull-requests.</p>



<p>I can go back to the&nbsp;master branch again with ‘<code>git checkout master</code>‘ and there I can “<code>git pull</code>” to get everything from upstream – like when my fellow developers have pushed stuff in the mean time.</p>



<h2>PR comments or CI alerts</h2>



<p>If a reviewer or a CI job find a mistake in one of my PRs, that becomes visible on GitHub and I get to work to handle it. To either fix the bug or discuss with the reviewer what the better approach might be.</p>



<p>Unfortunately, flaky CI jobs is a part of life so very often there ends up one or two red markers in the list of CI jobs that can be ignored as the test failures in them are there due to problems in the setup and not because of actual mistakes in the PR…</p>



<p>To get back to my branch for that PR again, I “<code>git checkout bagder/my-new-stuff-or-bugfix</code>“, and fix the issues.</p>



<p>I normally start out by doing follow-up commits that repair the immediate mistake and push them on the branch:</p>



<pre>git push origin <code>bagder/my-new-stuff-or-bugfix</code></pre>



<p>If the number of fixup commits gets large, or if the follow-up fixes aren’t small, I usually end up doing a squash to reduce the number of commits into a smaller, simpler set, and then force-push them to the branch.</p>



<p>The reason for that is to make the patch series easy to review, read and understand. When a commit series has too many commits that changes the previous commits, it becomes hard to review.</p>



<h2>Ripe to merge?</h2>



<p>When the pull request is ripe for merging (independently of who authored it), I switch over to the master branch again and I merge the pull request’s commits into it. In special cases I cherry-pick specific commits from the branch instead. When all the stuff has been yanked into master properly that should be there, I push the changes to the remote.</p>



<p>Usually, and especially if the pull request wasn’t done by me, I also go over the commit messages and polish them somewhat before I push everything. Commit messages should follow our style and mention not only which PR that it closes but also which issue it fixes and properly give credit to the bug reporter and all the helpers – using the right syntax so that our automatic tools can pick them up correctly!</p>



<p>As already mentioned above, I merge fast-forward or rebased into master. No merge commits.</p>



<h2>Never merge with GitHub!</h2>



<p>There’s a button GitHub that says “rebase and merge” that could theoretically be used for merging pull requests. I <em>never</em> use that (and if I could, I’d disable/hide it). The reasons are simply:</p>



<ol><li>I don’t feel that I have the proper control of the commit message(s)</li><li>I can’t select to squash a subset of the commits, only all or nothing</li><li>I often want to cleanup the author parts too before push, which the UI doesn’t allow</li></ol>



<p>The downside with not using the merge button is that the message in the  PR says “closed by [hash]” instead of “merged in…” which causes confusion to a fair amount of users who don’t realize it means that it actually means the same thing! I consider this is a (long-standing) GitHub UX flaw.</p>



<h2>Post merge</h2>



<p>If the branch has nothing to be kept around more, I delete the local branch again with “<code>git branch -d [name]</code>” and I remove it remotely too since it was completely merged there’s no reason to keep the work version left.</p>



<p>At any given point in time, I have some 20-30 different local branches alive using this approach so things I work on over time all live in their own branches and also submissions from various people that haven’t been merged into master yet exist in branches of various maturity levels. Out of those local branches, the number of concurrent pull requests I have in progress can be somewhere between just a few up to ten, twelve something.</p>



<h2>RELEASE-NOTES</h2>



<p>Not strictly related, but in order to keep interested people informed about what’s happening in the tree, we sync the <a href="https://github.com/curl/curl/blob/master/RELEASE-NOTES">RELEASE-NOTES</a> file every once in a while. Maybe every 5-7 days or so. It …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/">https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/</a></em></p>]]>
            </description>
            <link>https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032951</guid>
            <pubDate>Mon, 09 Nov 2020 09:23:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On the Beauty of Python's ExitStack (2015)]]>
            </title>
            <description>
<![CDATA[
Score 124 | Comments 29 (<a href="https://news.ycombinator.com/item?id=25032924">thread link</a>) | @polm23
<br/>
November 9, 2020 | https://www.rath.org/on-the-beauty-of-pythons-exitstack.html | <a href="https://web.archive.org/web/*/https://www.rath.org/on-the-beauty-of-pythons-exitstack.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>I believe Python's <a href="http://docs.python.org/3/library/contextlib.html#contextlib.ExitStack">ExitStack</a> feature does not get the recognition
it deserves. I think part of the reason for this is that its
documentation is somewhere deep down in the (already obscure)
<a href="http://docs.python.org/3/library/contextlib.html">contextlib</a> module because formally ExitStack is just one of many
available context managers for Python's <a href="http://docs.python.org/3/reference/compound_stmts.html#the-with-statement">with statement</a>. But
ExitStack deserves far more prominent notice than that. This post will
hopefully help with that.</p>
<p>So what makes ExitStack so important? In short, it's the best way to
handle allocation and release of external resources in Python.</p>
<div id="the-problem">
<h2>The Problem</h2>
<p>The main challenge with external resources is that you have to release
them when you don't need them anymore -- and in particular you must
not forget to do so in all the alternate execution paths that may be
entered in case of error conditions.</p>
<p>Most languages implement error conditions as "exceptions" that can be
"caught" and handled (Python, Java, C++), or as special return values
that you need to check to determine if an error occured (C, Rust,
Go). Typically, code that needs to acquire and release external
resources then looks like this:</p>
<div><pre><span></span><span>res1</span> <span>=</span> <span>acquire_resource_one</span><span>()</span>
<span>try</span><span>:</span>
    <span># do stuff with res1</span>
    <span>res2</span> <span>=</span> <span>acquire_resource_two</span><span>()</span>
    <span>try</span><span>:</span>
        <span># do stuff with res1 and res2</span>
    <span>finally</span><span>:</span>
        <span>release_resource</span><span>(</span><span>res2</span><span>)</span>
<span>finally</span><span>:</span>
   <span>release_resource</span><span>(</span><span>res1</span><span>)</span>
</pre></div>
<p>or, if the language doesn't have exceptions:</p>
<div><pre><span></span><span>res1</span> <span>=</span> <span>acquire_resource_one</span><span>();</span>
<span>if</span><span>(</span><span>res</span> <span>==</span> <span>-</span><span>1</span><span>)</span> <span>{</span>
   <span>retval</span> <span>=</span> <span>-</span><span>1</span><span>;</span>
   <span>goto</span> <span>error_out1</span><span>;</span>
<span>}</span>
<span>// do stuff with res1</span>
<span>res2</span> <span>=</span> <span>acquire_resource_two</span><span>();</span>
<span>if</span><span>(</span><span>res</span> <span>==</span> <span>-</span><span>1</span><span>)</span> <span>{</span>
   <span>retval</span> <span>=</span> <span>-</span><span>2</span><span>;</span>
   <span>goto</span> <span>error_out2</span><span>;</span>
<span>}</span>
<span>// do stuff with res1 and res2</span>
<span>retval</span> <span>=</span> <span>0</span><span>;</span> <span>// ok</span>

<span>error_out2</span><span>:</span>
  <span>release_resource</span><span>(</span><span>res2</span><span>);</span>
<span>error_out1</span><span>:</span>
  <span>release_resource</span><span>(</span><span>res1</span><span>);</span>
<span>return</span> <span>retval</span><span>;</span>
</pre></div>
<p>This approach has three big problems:</p>
<ol>
<li>The cleanup code is far away from the allocation code.</li>
<li>When the number of resources increases, indentation levels (or jump
labels) accumulate, making things hard to read.</li>
<li>Managing a dynamic number of resources this way is impossible.</li>
</ol>
<p>In Python, some of these issues can be alleviated by using the
<tt>with</tt> statement:</p>
<div><pre><span></span> <span>@contextlib.contextmanager</span>
 <span>def</span> <span>my_resource</span><span>(</span><span>id_</span><span>):</span>
     <span>res</span> <span>=</span> <span>acquire_resource</span><span>(</span><span>id_</span><span>)</span>
     <span>try</span><span>:</span>
         <span>yield</span> <span>res</span>
     <span>finally</span><span>:</span>
         <span>release_source</span><span>(</span><span>res</span><span>)</span>

<span>with</span> <span>my_resource</span><span>(</span><span>RES_ONE</span><span>)</span> <span>as</span> <span>res1</span><span>,</span> \
   <span>my_resource</span><span>(</span><span>RES_TWO</span><span>)</span> <span>as</span> <span>res2</span><span>:</span>
    <span># do stuff with res1</span>
    <span># do stuff with res1 and res2</span>
</pre></div>
<p>However, this solution is far from optimal: you need to implement
resource-specific context managers (note that in the above example we
silently assumed that both resources can be acquired by the same
function), you can get rid of extra indentation only if you allocate
all the resources at the same time and live with an ugly continuation
line (no parenthesis allowed in this context), and you still need to
know the number of required resources ahead of time.</p>
<p>Over in the world of exception-less programming languages (no pun
intended), <a href="http://www.golang.org/">Go</a> has developed a different remedy: the <a href="http://golang.org/ref/spec#Defer_statement">defer statement</a>
defers execution of an expression until the enclosing
function returns. Using <tt>defer</tt>, the above example can be written
as:</p>
<div><pre><span></span><span>res1</span> <span>=</span> <span>acquire_resource_one</span><span>()</span>
<span>if</span><span>(</span><span>res</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
    <span>return</span> <span>-</span><span>1</span>
<span>}</span>
<span>defer</span> <span>release_resource</span><span>(</span><span>res1</span><span>)</span>
<span>// do stuff with res1</span>
<span>res2</span> <span>=</span> <span>acquire_resource_two</span><span>()</span>
<span>if</span><span>(</span><span>res</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
    <span>return</span> <span>-</span><span>2</span>
<span>}</span>
<span>defer</span> <span>release_resource</span><span>(</span><span>res2</span><span>)</span>
<span>// do stuff with res1 and res2</span>
<span>return</span> <span>0</span>
</pre></div>
<p>This is pretty nice: allocation and cleanup are kept close together,
no extra indentation or jump labels are required, and converting this
to a loop that dynamically acquires multiple resources would be
straightforward. But there are still some drawbacks:</p>
<ul>
<li>To control when exactly a group of resources is getting released you
have to factor out into separate functions all parts of code that
access the respective resources.</li>
<li>You cannot "cancel" a deferred expression, so there is no way to
e.g. return a resource to the caller if no error occured.</li>
<li>There is no way to handle errors from the cleanup functions.</li>
<li><tt>defer</tt> is available in Go, but not in Python.</li>
</ul>
</div>
<div id="exitstack-to-the-rescue">
<h2>ExitStack to the rescue</h2>
<p><a href="http://docs.python.org/3/library/contextlib.html#contextlib.ExitStack">ExitStack</a> fixes all of the above issues, and adds some benefits on
top of it. An ExitStack is (as the name suggests) a stack of clean-up
functions. Adding a callback to the stack is the equivalent of calling
Go's <tt>defer</tt> statement. However, clean-up functions are not executed
when the function returns, but when execution leaves the <tt>with</tt>
block - and until then, the stack can also be emptied again.</p>
<p>Finally, clean-up functions itself may raise exceptions without
affecting execution of other clean-up functions. Even if multiple
clean-ups raise exceptions, you are will get a usable stacktrace.</p>
<p>Here's how to acquire multiple resources:</p>
<div><pre><span></span><span>with</span> <span>ExitStack</span><span>()</span> <span>as</span> <span>cm</span><span>:</span>
    <span>res1</span> <span>=</span> <span>acquire_resource_one</span><span>()</span>
    <span>cm</span><span>.</span><span>callback</span><span>(</span><span>release_resource</span><span>,</span> <span>res1</span><span>)</span>
    <span># do stuff with res1</span>
    <span>res2</span> <span>=</span> <span>acquire_resource_two</span><span>()</span>
    <span>cm</span><span>.</span><span>callback</span><span>(</span><span>release_resource</span><span>,</span> <span>res2</span><span>)</span>
    <span># do stuff with res1 and res2</span>
</pre></div>
<p>Note that</p>
<ul>
<li>acquisition and release are close to each other</li>
<li>there's no extra indentation,</li>
<li>the pattern and it easily scales up to many resources (including a
dynamic number that's acquired in a loop)</li>
</ul>
<p>If there already is a context manager for your resource, there's also
a shortcut function:</p>
<div><pre><span></span><span>with</span> <span>ExitStack</span><span>()</span> <span>as</span> <span>cm</span><span>:</span>
    <span>res1</span> <span>=</span> <span>cm</span><span>.</span><span>enter</span><span>(</span><span>open</span><span>(</span><span>'first_file'</span><span>,</span> <span>'r'</span><span>))</span>
    <span># do stuff with res1</span>
    <span>res2</span> <span>=</span> <span>cm</span><span>.</span><span>enter</span><span>(</span><span>open</span><span>(</span><span>'second_file'</span><span>,</span> <span>'r'</span><span>))</span>
    <span># do stuff with res1 and res2</span>
</pre></div>
<p>To open a bunch of files and return them to the caller (without
leaking already opened files if a subsequent open fails):</p>
<div><pre><span></span><span>def</span> <span>open_files</span><span>(</span><span>filelist</span><span>):</span>
    <span>fhs</span> <span>=</span> <span>[]</span>
    <span>with</span> <span>ExitStack</span><span>()</span> <span>as</span> <span>cm</span><span>:</span>
        <span>for</span> <span>name</span> <span>in</span> <span>filelist</span><span>:</span>
            <span>fhs</span><span>.</span><span>append</span><span>(</span><span>cm</span><span>.</span><span>enter</span><span>(</span><span>open</span><span>(</span><span>name</span><span>,</span> <span>'r'</span><span>)))</span>
        <span>cm</span><span>.</span><span>pop_all</span><span>()</span>
        <span>return</span> <span>fhs</span>
</pre></div>
<p>Disclaimer: the <a href="https://bugs.python.org/issue13585">original idea for ExitStack</a> came from me.</p>
</div>
</div></div>]]>
            </description>
            <link>https://www.rath.org/on-the-beauty-of-pythons-exitstack.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032924</guid>
            <pubDate>Mon, 09 Nov 2020 09:19:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Programmatically perform personalized sales out-reach to Fortune 500 companies]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 4 (<a href="https://news.ycombinator.com/item?id=25032865">thread link</a>) | @nubela
<br/>
November 9, 2020 | https://nubela.co/blog/send-personalized-emails-to-decision-makers-scrape-linkedin-company-profile/ | <a href="https://web.archive.org/web/*/https://nubela.co/blog/send-personalized-emails-to-decision-makers-scrape-linkedin-company-profile/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
<div>
<article>

<figure>
<img srcset="https://accountgram-production.sfo2.cdn.digitaloceanspaces.com/nubelaco_ghost/2020/11/TLC_How_I_sent_personalized_emails_with_25_follow-ups_to_decision_makers_of_200_Fortune_500_companies_light_bg.png 300w,
                            https://accountgram-production.sfo2.cdn.digitaloceanspaces.com/nubelaco_ghost/2020/11/TLC_How_I_sent_personalized_emails_with_25_follow-ups_to_decision_makers_of_200_Fortune_500_companies_light_bg.png 600w,
                            https://accountgram-production.sfo2.cdn.digitaloceanspaces.com/nubelaco_ghost/2020/11/TLC_How_I_sent_personalized_emails_with_25_follow-ups_to_decision_makers_of_200_Fortune_500_companies_light_bg.png 1000w,
                            https://accountgram-production.sfo2.cdn.digitaloceanspaces.com/nubelaco_ghost/2020/11/TLC_How_I_sent_personalized_emails_with_25_follow-ups_to_decision_makers_of_200_Fortune_500_companies_light_bg.png 2000w" sizes="(max-width: 800px) 400px,
                            (max-width: 1170px) 700px,
                            1400px" src="https://accountgram-production.sfo2.cdn.digitaloceanspaces.com/nubelaco_ghost/2020/11/TLC_How_I_sent_personalized_emails_with_25_follow-ups_to_decision_makers_of_200_Fortune_500_companies_light_bg.png" alt="How to programmatically send personalized emails to 200 decision makers of Fortune 500 companies (with code samples)">
</figure>
<section>
<div>
<h2 id="i-want-to-close-bigger-deals-by-reaching-decision-makers-directly">I want to close bigger deals by reaching decision-makers directly</h2><p>I am the founder of Nubela. I lead sales for Proxycurl, and I do this alone. So it is vital that whatever I do is leveraged and effective. Luckily, I can code.<br>I figured that general emails do not get to decision-makers in large(r) companies, and I want to move up the value chain. I want to close bigger deals. I think I can achieve that by reaching decision-makers directly and with emails personalized with a unique problem-solution statement for their company. To say, I want to do what other sales reps are doing and more.<br>To be very specific, I want to reach 200 decision-makers at a single burst. I want to send 25 personalized follow-ups per decision-maker.<br>In this blog post, I will share how I accomplished this with code.</p><h2 id="getting-a-list-of-companies-with-crunchbase-pro">Getting a list of companies with Crunchbase Pro</h2><p>I want to target larger companies that have the budget to purchase Proxycurl to build data-driven products. In particular, I have shortlisted companies that belong to the likes of sales automation tools, job boards, and talent sourcing companies to be our target market. Crunchbase Pro is excellent for building a list like such.</p><p>With Crunchbase Pro, I started a Company Search for a list of companies that</p><ol><li>matched Proxycurl's target industries</li><li>have revenues that are more than 5+M per annum</li></ol><p>Then, I exported the search results into a CSV file, ensuring that I have a column of data that includes the company's Linkedin Profile.</p><p>Once I have the list, I want to enrich the data with company names and their corresponding corporate website. This is the Python script I used to enrich data for the list of companies I had exported from Crunchbase Pro:</p><pre><code>async def get_company(company_profile_url: str):
    api_endpoint = f'{PROXYCURL_HOST}/api/linkedin/company'
    header_dic = {'Authorization': 'Bearer ' + PROXYCURL_API_KEY}

    for _ in range(RETRY_COUNT):
        try:
            async with httpx.AsyncClient() as client:
                r = await client.get(api_endpoint,
                                     params={'url': company_profile_url},
                                     headers=header_dic,
                                     timeout=PROXYCURL_XHR_DEFAULT_TIMEOUT)
                assert r.status_code == 200
                return r.json()
        except:
            continue

    return None


async def enrich_companies(lis):
    for profile_url in lis:
        coy = await get_company(profile_url)
        if coy is None:
            return None
        website = coy.get('website', None)
        coy_name = coy.get('name', None)

        # todo - (task for reader) save `website` and `coy_name` in a file
</code></pre>
<h2 id="find-decision-makers-with-proxycurl-api">Find decision makers with Proxycurl API</h2><p>Now that I have companies, I need decision-makers. Decision-makers in this exercise mean people in the roles of CEO, COO, CTO, and VP of Product.</p><p>To accomplish this, I will search for them on Google. For example, if I want to find the Linkedin profile of the CEO of Cognism, I will enter the following search phrase in Google:</p><blockquote>linkedin.com/in ceo cognism</blockquote><p>Chances are, the correct profile will be in the search result. I will then repeat the query with different roles till I have a list of profiles. And it works.</p><p>To perform Google searches at scale, I will use Proxycurl's <a href="https://nubela.co/proxycurl/docs#crawling-other-pages">"Crawling other pages" endpoint</a>. This is how I programmatically make Google Search queries with Proxycurl:</p><pre><code>async def google_search_async(search_term, retry_count=5) -&gt; List[str]:
    """
    Perform a Google Search via Overlord and return a list of results in terms of URLs in the first page.
    """
    for _ in range(retry_count):
        try:
            search_url = f"https://www.google.com/search?q={quote(search_term)}"
            payload = {'url': search_url,
                       "type": 'xhr',
                       }

            async with httpx.AsyncClient() as client:
                r = await client.post(f"{OVERLORD_ENDPOINT}/message",
                                      auth=(OVERLORD_USERNAME,
                                            OVERLORD_PASSWD),
                                      json=payload,
                                      timeout=PROXYCURL_XHR_DEFAULT_TIMEOUT)
                if r.status_code != 200:
                    print(
                        f"Google search failed with {r.status_code}, retrying.")
                assert r.status_code == 200

            html_src = r.json()['data']
            soup = BeautifulSoup(html_src, features="html.parser")
            result_lis = soup.select(".g a[ping]")
            href_lis = []
            for result in result_lis:
                href = result['href']
                if '//webcache.googleusercontent.com/search' in href:
                    continue
                if 'https://translate.google.com/translate' in href:
                    continue
                href_lis += [href]
            if len(href_lis) == 0:
                continue
            return href_lis
        except:
            traceback.print_exc()
            continue
    raise Exception
</code></pre>
<p>However, my computer is not smart enough to understand when a CEO is the same as "Chief Executive Officer." Or that "Engineering Head" and "Chief Engineering" are very much alike. For that, I have an algorithm which I call <code>is_string_similar()</code>. You can find the algorithm to check if two strings are similar <a href="https://giki.wiki/@nubela/Software-Engineering/similar-string">here</a>.</p><p>Once I have a list of Linkedin profiles, I need to ensure that:</p><ol><li>The profile's current employment belongs to the company that I am googling for (Google gets this wrong sometimes)</li><li>The profile's current role at the company matches the decision making roles.</li></ol><p>To perform the checks above, I will:</p><ol><li>Enrich the Linkedin profiles with <a href="https://nubela.co/proxycurl/docs#linkedin-person-profile-endpoint">Proxycurl's Person Profile Endpoint</a> to get the profile's list of experiences.</li><li>Verify that his/her active employment matches up.</li></ol><p>This is how I accomplish the above in Python code:</p><pre><code>async def get_person_profile(profile_url):
    api_endpoint = f'{PROXYCURL_HOST}/api/v2/linkedin'
    header_dic = {'Authorization': 'Bearer ' + PROXYCURL_API_KEY}

    for _ in range(RETRY_COUNT):
        try:
            async with httpx.AsyncClient() as client:
                r = await client.get(api_endpoint,
                                     params={'url': profile_url},
                                     headers=header_dic,
                                     timeout=PROXYCURL_XHR_DEFAULT_TIMEOUT)
                if r.status_code == 404:
                    return None
                assert r.status_code == 200
                return r.json()
        except:
            continue

    print(f"{profile_url} retried {RETRY_COUNT} times but still failing")
    return None


async def google_search_async(search_term, retry_count=3) -&gt; List[str]:
    """
    Perform a Google Search via Overlord and return a list of results in terms of URLs in the first page.
    """
    for _ in range(retry_count):
        try:
            search_url = f"https://www.google.com/search?q={quote(search_term)}"
            payload = {'url': search_url,
                       "type": 'xhr',
                       }

            async with httpx.AsyncClient() as client:
                r = await client.post(f"{OVERLORD_ENDPOINT}/message",
                                      auth=(OVERLORD_USERNAME,
                                            OVERLORD_PASSWD),
                                      json=payload,
                                      timeout=PROXYCURL_XHR_DEFAULT_TIMEOUT)
                if r.status_code != 200:
                    print(
                        f"Google search failed with {r.status_code}, retrying.")
                assert r.status_code == 200

            html_src = r.json()['data']
            soup = BeautifulSoup(html_src, features="html.parser")
            result_lis = soup.select(".g a[ping]")
            href_lis = []
            for result in result_lis:
                href = result['href']
                if '//webcache.googleusercontent.com/search' in href:
                    continue
                if 'https://translate.google.com/translate' in href:
                    continue
                href_lis += [href]
            return href_lis
        except:
            traceback.print_exc()
            continue
    raise Exception


async def find_people_in_roles(coy_name: str, li_coy_profile_url: str = None) -&gt; List[str]:
    MAX_WORKERS = 10
    ROLES = ['ceo',
             'cto',
             'coo',
             'vp engineering'
             ]

    def does_role_match(role: str, person_profile: Dict) -&gt; bool:
        for exp in person_profile['experiences']:
            if not (exp['ends_at'] is None and util.is_string_similar(coy_name, exp['company'])):
                continue

            if not util.is_string_similar(role, exp['title']):
                continue

            return True
        return False

    async def search_li_profile(role: str) -&gt; List[str]:
        url_result_lis = await google_search_async(f"linkedin.com/in {role} {coy_name}", retry_count=RETRY_COUNT)
        profile_url_lis = list(filter(lambda x: 'linkedin.com/in' in x,
                                      url_result_lis))
        return (role, profile_url_lis)

    print("Performing google search for Linkedin profiles")
    tasks = [search_li_profile(role) for role in ROLES]
    search_results = await asyncio.gather(*tasks)

    profile_url_lis = []
    for _, profile_lis in search_results:
        for profile_url in profile_lis:
            if profile_url not in profile_url_lis:
                profile_url_lis += [profile_url]
    print(f"Total of {len(profile_url_lis)} profiles to query")

    profile_dic = {}
    working_lis = []
    for idx, profile_url in enumerate(profile_url_lis):
        working_lis += [profile_url]

        if (idx &gt; 0 and len(working_lis) &gt; 0 and idx % MAX_WORKERS == 0) or idx == (len(profile_url_lis) - 1):
            print(f"Working on {len(working_lis)} profiles..")
            tasks = …</code></pre></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://nubela.co/blog/send-personalized-emails-to-decision-makers-scrape-linkedin-company-profile/">https://nubela.co/blog/send-personalized-emails-to-decision-makers-scrape-linkedin-company-profile/</a></em></p>]]>
            </description>
            <link>https://nubela.co/blog/send-personalized-emails-to-decision-makers-scrape-linkedin-company-profile/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032865</guid>
            <pubDate>Mon, 09 Nov 2020 09:12:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Time Loop Software (2013)]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25032563">thread link</a>) | @netgusto
<br/>
November 9, 2020 | https://marak.com/blog/2013-05-13-time-loop-software | <a href="https://web.archive.org/web/*/https://marak.com/blog/2013-05-13-time-loop-software">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div>
<p>What if it were possible to write software capable of time travel? What if we could write software that was able to retrieve results from a computation solved sometime in the near future? What would this software look like? What problems could be solved?</p>
<p><a href="https://en.wikipedia.org/wiki/Novikov_self-consistency_principle#Time_loop_logic">Time loop logic</a> is a hypothetical system of computation that exploits the <a href="https://en.wikipedia.org/wiki/Novikov_self-consistency_principle">Novikov self-consistency principle</a>. In this system the computer is able to send the result of a computation backwards through time and rely upon the self-consistency principle to force the sent result to be correct. This futuristic concept might seem impossible now but I'd imagine trying to explain nuclear fission to a 3rd century blacksmith would seem equally impossible.</p>
<h2 id="writing-time-loop-software">Writing time loop software</h2>
<p>Building on the concept of time loop logic we are able to implement theoretical programming constructs to help better understand the concept of time travel in software. In the following examples we demonstrate what a time loop logic program might look like.</p>
<h3 id="an-event-loop">An event loop</h3>
<p>In the follow examples we'll be using the JavaScript programing language. JavaScript provides a single thread of execution for code to run in. The JavaScript virtual machine is constantly running an event loop. Each tick of this event loop represents a single cycle of code execution. Once this cycle is completed the next tick in the event loop will occur. In the popular <a href="https://nodejs.org/">Node.js</a> framework <a href="https://nodejs.org/api/process.html#process_process_nexttick_callback">an API is provided</a> to defer the execution of a block of code until the nextTick of the event loop occurs.</p>
<h4 id="node-js-process-nexttick-example">node.js process.nextTick() example</h4>
<pre><code><span><span>function</span> <span>foo</span>(<span></span>) </span>{
  <span>console</span>.log(<span>'foo'</span>);
}

process.nextTick(foo);
<span>console</span>.log(<span>'bar'</span>);
</code></pre><p>This will output:</p>
<pre><code><span>bar
</span><span>foo</span>
</code></pre><p>The same effect of <code>process.nextTick</code> can also be achieved using JavaScript's setTimeout command</p>
<pre><code>setTimeout<span>(<span>foo</span>, <span>0</span>)</span>
</code></pre><h4 id="node-js-process-prevtick-example">node.js process.prevTick() example</h4>
<p>Now let's imagine that instead of deferring a line of code until the next tick of the event loop we could instead push that code <em>backwards</em> to the <em>previous</em> tick of the event loop.</p>
<pre><code><span><span>function</span> <span>foo</span>(<span></span>) </span>{
  <span>console</span>.log(<span>'foo'</span>);
}

<span>console</span>.log(<span>'bar'</span>);
process.prevTick(foo);
</code></pre><p>Outputs:</p>
<pre><code><span>foo</span>
bar
</code></pre><p>The same effect of <code>process.prevTick</code> can also be achieved using setTimeout with a negative value</p>
<pre><code>setTimeout<span>(<span>foo</span>, <span>-1</span>)</span>
</code></pre><p>Since all we are doing is logging a simple string to the console, this is a contrived example. However; building on the concept of <code>process.prevTick</code> we can begin to implement more complex time loop programs.</p>
<h2 id="brute-force-cracking-with-time-loops">Brute force cracking with time loops</h2>
<p>Let's assume a simple <a href="https://en.wikipedia.org/wiki/Brute-force_search">brute-force search</a> password cracking scenario. Imagine there is a login function which expects a password. We have access to a very large word dictionary in which our cracking software will sequentially attempt logins using every word in the dictionary as a password until a match is found.</p>
<p>Here is the code for our brute-force program</p>
<p><em>Note: It's important to remember that Novikov's self-consistency principle guarantees that the sequence of events generating the paradox in the following code has zero probability.</em></p>


<h2 id="prime-factors-with-time-loops">Prime Factors with time loops</h2>
<p>Using time-loop logic  prime factors can be calculated in polynomial time.</p>


<h2 id="zero-lag-instant-communication">Zero-lag / Instant Communication</h2>
<p>The theoretical application of time-loop logic is endless. Imagine a time-loop based communication protocol. This would mean zero millisecond latency. Imagine gaming, video broadcasting, and file sharing with instantaneous transfer and zero lag. Through exploiting self-consistency we know that data will be sent in the immediate future ( since the data has begun transferring from the source ) and that eventually the transmission will arrive at it's destination. As long as the data will eventually be received, we are able to send the result back from the future into the immediate present, removing the notion of latency or lag.</p>
<h2 id="time-loop-logic-and-novikov-s-self-consistency-principle">Time Loop Logic and Novikov's Self-Consistency Principle</h2>
<p>How is it actually possible to program a time loop? Based on the self-consistency principle and continuing advancements in quantum entanglement these types of mind-bending constructs are not very far away. It's very possible we'll see this type of software actively being developed within the next hundred years.</p>
<p>Time loop logic was first written about by <a href="https://en.wikipedia.org/wiki/Hans_Moravec">Hans Moravec</a> who is best known for his work in robotics and artificial intelligence at Carnegie Mellon University. You can find Hans' original paper from 1991, "Time Travel and Computing", here: <a href="https://frc.ri.cmu.edu/~hpm/project.archive/general.articles/1991/TempComp.html">https://frc.ri.cmu.edu/~hpm/project.archive/general.articles/1991/TempComp.html</a>. I recommend reading the entire paper.</p>
<p>What we know from <a href="https://en.wikipedia.org/wiki/Closed_timelike_curve#General_relativity">general relativity</a> is that at a quantum level backwards time-travel is mathematically possible in certain solutions containing <a href="https://en.wikipedia.org/wiki/Closed_timelike_curve">closed timelike curves</a>. A closed timelike curve is a <a href="https://en.wikipedia.org/wiki/World_line">world-line</a> in a <a href="https://en.wikipedia.org/wiki/Lorentzian_manifold#Lorentzian_manifold">Lorentzian manifold</a>. </p>
<p>Closed timelike curves ( CTCs ) pose a problem for physicists. The existence of CTCs introduces the notion of time travel being possible. If time travel is possible, we have now introduced the notion of <a href="https://en.wikipedia.org/wiki/Grandfather_paradox">time travel paradoxes</a> which can violate <a href="https://en.wikipedia.org/wiki/Causality_(physics)">causality</a>. Since it's generally accepted that we cannot violate causality in our universe we must be able to explain how closed time-like curves can exist.</p>
<p>In his self-consistency principle Novikov asserts that if an event exists that would give rise to a paradox, or to any "change" to the past whatsoever, then the probability of that event is zero. In short, it says that it is impossible to create time travel paradoxes. You can find the original paper here: <a href="http://authors.library.caltech.edu/3737">http://authors.library.caltech.edu/3737</a>. I recommend starting with reading the <a href="https://en.wikipedia.org/wiki/Novikov_self-consistency_principle#History_of_the_principle">history of the principle</a>.</p>

<p>In order for time loop logic to return an answer instantaneously, we <em>must</em> ensure that the problem will run long enough into the future to <em>actually</em> calculate the result. If a problem takes sixty seconds to solve, the program must run for at least sixty seconds. Time-loop logic does <em>not</em> violate causality. We are able to retrieve the answer instantly because we have committed to spending sixty seconds in the future calculating the answer and sending it back.</p>
<p>This turns debugging time-loop logic into somewhat of an impossibility. Any bugs in a time loop indicate that sometime in the future a problem has occurred. <strong>This event may or may not be related to software.</strong> </p>
<p>Imagine a computer that utilized a time loop to brute force crack passwords ( as our code posted above did). I turn the machine on and request it cracks the password. The program doesn't work. Frustrated, I turn off the machine and complain to my co-worker Josh.</p>
<p>Josh turns on the machine and requests the password. The software works instantly cracking the password in under 1ms.</p>
<p>Bewildered, I ask Josh why the machine worked for him but not for me.</p>
<p>Josh replies, "It's actually quite simple. Using that computer it's going to take approximately 400 hours to brute force the password. After that 400 hours the CPU must recursively return the cracked password back in time until it reaches right now. I was able to get the answer instantly because I have decided to not turn this computer off for another 399 hours and 59 minutes. Simply put, you turned off the computer too quickly"</p>
<p><em>The consequences of unplugging the computer</em></p>
</div></div></div>]]>
            </description>
            <link>https://marak.com/blog/2013-05-13-time-loop-software</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032563</guid>
            <pubDate>Mon, 09 Nov 2020 08:27:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Productivity vs. Privacy]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25032481">thread link</a>) | @jessems
<br/>
November 9, 2020 | https://jessems.com/productivity-vs-privacy | <a href="https://web.archive.org/web/*/https://jessems.com/productivity-vs-privacy">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><div><p>In recent years there's been a steady growth in privacy focused companies. Some examples that have reached large-scale adoption are <a href="https://protonmail.com/">ProtonMail</a>, <a href="https://signal.com/">Signal</a>, and <a href="https://duckduckgo.com/">DuckDuckGo</a>. These are companies that have put privacy front and center to their value proposition and can be considered <em>privacy-preserving products</em>. I've come to beleive a goal of preserving user privacy is often inherently in tension with the goal of advancing user productivity.</p><p>What these services have in common is that they promise their users a higher degree of privacy relative to their competitors. Instead of the usual encryption in transit (protection from eavesdroppers) and encryption at rest (protection against unauthorized users), services like Signal and ProtonMail enable their users to hide data from anyone except the intended recipient, which — crucially — includes the service providers themselves.</p><p>This category of encryption is known as end-to-end encryption (e2e) and has found adopters in anyone from principled libertarians to journalists and human rights activitists whose lives may depend on their conversations remaining unwiretapped.</p><h2>Early privacy-preserving software was too difficult to use</h2><p>The canonical implementation of e2e for email is known as Pretty Good Privacy (PGP) and its reference implementation is GPG. GPG never reached mass adoption and there seems to be a myriad of reasons for that. The most salient reason, however, seems to be that to this day, it continues to be difficult to use. As the founder of Signal, Moxie Marlinspike <a href="https://moxie.org/2015/02/24/gpg-and-me.html">explains</a>, the spirit behind GPG was the following:</p><blockquote><p>Instead of developing opinionated software with a simple interface, GPG was written to be as powerful and flexible as possible.</p></blockquote><p>Powerful, flexible software written by nerds, unfortunately also tends to be prohibitively complex for normal users. Combined with the fact that <a href="https://signal.org/blog/the-ecosystem-is-moving/">decentralized technology seems unable to quickly adapt to change</a>, the result has been a clunky solution that has, quite frankly, stayed clunky. With no feasible privacy-preserving alternative <!-- -->[1]<!-- -->, non-privacy preserving email providers became the norm.</p><h2>Surveillance capitalist companies will not encrypt your data, because they rely on being able to read it</h2><p>One such email provider, Gmail by Google, gained millions of users by offering a free plan. Their initial monetization strategy was scanning your emails and serving you personalized ads. Although they've stopped personalizing the ads, they're still scanning your email's contents to serve you a better experience across their services. Similarly, Facebook tracks what you do to shape your experience and keep you glued (they would say 'engaged') to their platform.</p><p>What unites platforms like Google and Facebook, is described by Professor Shoshana Zuboff as “<a href="https://en.wikipedia.org/wiki/Surveillance_capitalism">surveillance capitalism</a>”. The business model of surveillance capitalist companies is to harvest personal data about you to build a model that predicts your behavior. These prediction models are packaged and sold as advertisement opportunities to companies eager to buy your attention. You might be the user, but you're not the customer — the advertisers are.</p><p>It should come as no surprise then, that none of these platforms has shipped with end-to-end encryption by default. Doing so would go against the incentives that undergird their very business model. Their ability to predict your behavior, and sell ads based on those predictions, hinges on their ability to harvest your data.</p><h2>Data is also collected to improve the service</h2><p>A company like Google has other business models of course. Google Workspace, aimed at businesses, is a collection of collaboration and productivity tools. This ranges from Google Docs, to chat, to video conferencing, and more. By offering this as a paid service, Google exposes itself to a different incentive, one where the customer and the user are now one and the same.</p><p>Even if you're both the user and the customer, your data is still being harvested. This data might not feed into personalized ads (because that’s no longer the primary business model) but rather into improving your experience. But as a business user, when does your experience improve? And as a service provider, how do you know what improves the experience?</p><h2>Improvements are productivity gains</h2><p>There's an inclination to think of improvements as things that help you do the thing you want to do quicker, better and/or with less frustration. We can go one step further and borrow some of the thinking used in economics and treat productivity simply as the ratio between outputs (salaries and corporate profits) and inputs (hours worked). Productivity increases if inputs can be decreased (for equal outputs) or outputs can be increased (for equal inputs). What's more, we would expect this quantity to improve along with advances in technology.</p><p>How does technology lead to increases in productivity? One obvious way is by making us more efficient. If some new technology saves us time doing a certain task (decreased input), all other things being equal, we’ll end up seeing those gains reflected in our outputs.</p><h2>Productivity gains are discovered, not planned</h2><p>What exactly are the things that increase efficiency? Here's where it gets tricky. In the realm of knowledge work, we don't always know where the gains will come from — that is, before they are made. We are still discovering new ways in which we can be more productive and especially so in the domain of collaborative productivity. An illustrative example of how productivity gains are discovered comes from Kevin A. Kwok's description of Figma's road to success.</p><p>In "Why Figma Wins", <a href="https://kwokchain.com/2020/06/19/why-figma-wins/">Kwok details</a> how the product team discovered a way to enable more efficient collaboration in the design process. That this potential existed wasn't at all  obvious to even those within the scene. While Sketch had broken new ground with their vector based design tool geared towards product designers, Figma took it to another level by taking many of the same (dare I say revolutionary) UX patterns and offering them in a web-native, multiplayer web application.</p><blockquote><p>The core insight of Figma is that design is larger than just designers. Design is all of the conversations between designers and PMs about what to build. It is the mocks and prototypes and the feedback on them. It is the handoff of specs and assets to engineers and how easy it is for them to implement them.</p></blockquote><p>As Kevin explains, Figma brought together the disparate disciplines that are involved in a design process into a synced browser window for everybody. This helped democratize design and remove a lot of friction that had existed before.</p><p>Not only did Figma push the frontier of productivity into new territory, it wasn’t obvious beforehand what that territory would look like. The lesson is that productivity improvements are won through a process of <em>discovery</em>. Kevin explains:</p><blockquote><p>As disciplines evolve, they figure out the social norms needed to operate better, build tools that can be shared across the industry, and invent abstractions that allow offloading more and more of the workload. They learn how to collaborate better, not just with each other but with all the other functions as well.</p></blockquote><p>Although there's some inherent uncertainty about what the productivity gains will look like (and where to look for them), there's no uncertainty about whether they will be made at all. If one thing can be counted on, it's the tech industry's relentless march towards higher productivity. The big tech platforms know this and don't shy away from investing heavily in innovation (discovery) in that direction.</p><h2>Productivity gains are unlocked by harvesting data</h2><p>Although there is some inherent tension between preserving privacy vs. allowing for a multiplayer mode like Figma, we can find even stronger tensions when it comes to harvesting data in favor of productivity gains.</p><p>A search feature relies on indexing your data. A recommendation feature relies on mining your browsing history. An autocomplete feature relies on what you (or other users) typed before.</p><p>All these potential features which are made possible through harvesting your user data are not available to privacy-preserving products. The user data isn't readable to them — and that's the whole point.</p><p>This creates a trade-off from the user's perspective. Whatever your particular motivation might be, as soon as you opt for a privacy-preserving service you're opting for a service that is not able to read your data, and by extension, not able to harvest it. Because the harvesting of data is what is driving many of the improvements in productivity, in choosing to preserve user privacy, these services are forgoing their ability to provide additional gains in productivity.</p><p>Historically, as we saw with the origins of GPG, there has always been additional friction involved in replicating a workflow in a privacy-preserving manner. Although using e2e services such as Signal and ProtonMail has become nearly frictionless, they lack many features their non-privacy preserving counterparts offer.</p><h2>The productivity gap between privacy-preserving and non-preserving services</h2><p>If you compare the productivity gains between privacy-preserving and non-preserving products from the perspective of the user, it's hard not to arrive at the conclusion that there’s a gap between the two — and it appears to be growing.</p><p>There is perhaps no better example of a feature which hinges on the ability to read user data than search. Although ProtonMail is reminiscent of Gmail in many ways, one area where it falls short is the absence of any ability to  search the contents of your emails. Search only works if the provider of such functionality can scan and index your content. It works even better if the provider is able to harvest search queries and use those to build predictive models (e.g. autocomplete and smart suggestions). These are features which make Gmail users more productive but aren't available to ProtonMail users <!-- -->[3]<!-- -->.</p><p>The absence of search might not be a dealbreaker for a …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://jessems.com/productivity-vs-privacy">https://jessems.com/productivity-vs-privacy</a></em></p>]]>
            </description>
            <link>https://jessems.com/productivity-vs-privacy</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032481</guid>
            <pubDate>Mon, 09 Nov 2020 08:13:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Does laser cutting count as woodworking?]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 5 (<a href="https://news.ycombinator.com/item?id=25032259">thread link</a>) | @Mimowork
<br/>
November 8, 2020 | https://www.mimowork.com/news/Can%20laser%20cutting%20be%20considered%20as%20woodworking.html | <a href="https://web.archive.org/web/*/https://www.mimowork.com/news/Can%20laser%20cutting%20be%20considered%20as%20woodworking.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><span>If you clicked on this, you must be interested in woodworking and maybe you also have one of those confusions:&nbsp;</span><strong>Does laser cutting count as woodworking?</strong></p><p><strong><strong><img src="https://www.mimowork.com/data/upload/ueditor/20201102/5f9f6ed9121ab.jpg" title="Does laser cutting count as woodworking?cid=6" alt="Does laser cutting count as woodworking?cid=6" width="700" height=""></strong></strong></p><p><span>Some may say woodworking is all about skilled manual labor, however, some may say technology has progressed, and sometimes manual labor is unnecessary.</span></p><p><span>Here are some thoughts we would want to share with you:</span></p><h2><span>1. Wood is the medium</span></h2><p><span>As with any industry, there are changing trends and evolving technology. The definition of handmade is becoming blurry.</span></p><p><span>For example, like the difference between forging and milling, both use metal as a medium but get the same result. Therefore, no matter what you want to do, engraving, cutting, labeling, and no matter what you want to use, a circular saw, table saw, jigsaw, or a laser cutting machine, it is just different pathways to achieve the same goal.</span></p><p><img src="https://www.mimowork.com/data/upload/ueditor/20201102/5f9f9d1607475.jpg" title="Does laser cutting count as woodworking?cid=6" alt="Does laser cutting count as woodworking?cid=6" width="700" height=""></p><h2><span>2. Skill is the key</span></h2><p><span>We can reach an agreement that the idea of “skill” is more based on how much someone put themselves into a project. So if someone uses a 3D printer to print out an “artworks” he/she downloaded, then it should not really be praised as craftsmanship, regardless of how fancy it would be.</span></p><p><a href="https://www.mimowork.com/flatbed-laser-cutting-machine/desktop-laser-engraver-70.html" target="_blank"><span>CNC laser cutting</span></a><span>&nbsp;is a different skill set. A considerable part of the wooden creation is from the skill and ability in operating the CNC laser cutting system.</span></p><p><span>Therefore, as long as you put the time in your woodworking, whatever is learning how to run a CNC laser cutting machine or using a jigsaw, even without the design work, just takes a certain amount of skill that should be appreciated.</span></p><p><span><img src="https://www.mimowork.com/data/upload/ueditor/20201102/5f9f9d83cfc6a.jpg" title="Does laser cutting count as woodworking?cid=6" alt="Does laser cutting count as woodworking?cid=6" width="700" height=""></span></p><h2>3. Technology is a tool</h2><p><span>Some musicians faced criticism for their use of synthesizers or electronic devices. Witnessing the success of Giorgio Moroder, Pink Floyd, etc, still and all, some people felt this was unskilled or unmusical.</span></p><p><span>Same idea with woodworking. Using CNC does not mean that the user lays a board down and CNC will give you a completed piece of work. It is just a tool to shorten your working hours and realize your creativity. There are more high-tech productivity tools yet to come. As tools evolve, unique and brand-new ways to express your artworks will also evolve.</span></p><p><span>Fortunately, the art world now is including “digital art” as a respected and well-developed genre.</span></p><p><img src="https://www.mimowork.com/data/upload/ueditor/20201102/5f9f9ea5567e2.jpg" title="Does laser cutting count as woodworking?cid=6" alt="Does laser cutting count as woodworking?cid=6" width="700" height=""></p><p><img src="https://www.mimowork.com/data/upload/ueditor/20201102/5f9f9ed0187eb.jpg" title="Does laser cutting count as woodworking?cid=6" alt="Does laser cutting count as woodworking?cid=6" width="700" height=""></p><p><span>If you have an idea to create, then you are already miles ahead of most people. Do not let anyone ever dull your sparkle!</span></p></div></div>]]>
            </description>
            <link>https://www.mimowork.com/news/Can%20laser%20cutting%20be%20considered%20as%20woodworking.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032259</guid>
            <pubDate>Mon, 09 Nov 2020 07:41:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Structured Concurrency]]>
            </title>
            <description>
<![CDATA[
Score 44 | Comments 12 (<a href="https://news.ycombinator.com/item?id=25032133">thread link</a>) | @ingve
<br/>
November 8, 2020 | https://ericniebler.com/2020/11/08/structured-concurrency/ | <a href="https://web.archive.org/web/*/https://ericniebler.com/2020/11/08/structured-concurrency/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<p>TL;DR: <strong>“Structured concurrency” refers to a way to structure async computations so that child operations are guaranteed to complete before their parents, just the way a function is guaranteed to complete before its caller.</strong> This sounds simple and boring, but in C++ it’s anything but. Structured concurrency — most notably, C++20 coroutines — has profound implications for the correctness and the simplicity of async architecture. It brings the <a href="https://docs.microsoft.com/en-us/cpp/cpp/welcome-back-to-cpp-modern-cpp?view=msvc-160">Modern C++ style</a> to our async programs by making async lifetimes correspond to ordinary C++ lexical scopes, eliminating the need for reference counting to manage object lifetime.</p>
<h2>Structured Programming and C++</h2>
<p>Back in the 1950’s, the nascent computing industry discovered structured programming: that high-level programming languages with lexical scopes, control structures, and subroutines resulted in programs that were far easier to read, write, and maintain than programming at the assembly level with test-and-jump instructions and <code>goto</code>. The advance was such a quantum leap that nobody talks about structured programming anymore; it’s just “programming”.</p>
<p>C++, more so than any other language, leverages structured programming to the hilt. The semantics of object lifetime mirror — and are tied to — the strict nesting of scopes; i.e., the <em>structure</em> of your code. Function activations nest, scopes nest, and object lifetimes nest. Objects’ lifetimes end with a scope’s closing curly brace, and objects are destroyed in the reverse order of their construction to preserve the strict nesting.</p>
<p>The Modern C++ programming style is built on this structured foundation. Objects have <em>value semantics</em> — they behave like the ints — and resources are cleaned up in destructors deterministically, which guarantees structurally that resources aren’t used after their lifetimes have ended. This is <em>very</em> important.</p>
<p>When we abandon this strict nesting of scopes and lifetimes — say, when we reference count an object on the heap, or when we use the singleton pattern — we are fighting against the strengths of the language rather than working with them.</p>
<h2>The Trouble With Threads</h2>
<p>Writing correct programs in the presence of concurrency is far more difficult than in single-threaded code. There are lots of reasons for this. One reason is that threads, like singletons and dynamically allocated objects, scoff at your puny nested scopes. Although you can use the Modern C++ style <em>within</em> a thread, when logic and lifetimes are scattered across threads, the hierarchical structure of your program is lost. The tools we use to manage complexity in single-threaded code — in particular, nested lifetimes tied to nested scopes — simply don’t translate to async code.</p>
<p>To see what I mean, let’s look at what happens when we take a simple synchronous function and make it asynchronous.</p>
<pre>void computeResult(State &amp; s);

int doThing() {
  State s;
  computeResult(s);
  return s.result;
}
</pre>
<p><code>doThing()</code> is simple enough. It declares some local state, calls a helper, then returns some result. Now imagine that we want to make both functions async, maybe because they take too long. No problem, let’s use Boost futures, which support continuation chaining:</p>
<pre>boost::future&lt;void&gt; computeResult(State &amp; s);

boost::future&lt;int&gt; doThing() {
  State s;
  auto fut = computeResult(s);
  return fut.then(
    [&amp;](auto&amp;&amp;) { return s.result; }); // OOPS
}
</pre>
<p>If you’ve programmed with futures before, you’re probably screaming, <em>“Nooooo!”</em> The <code>.then()</code> on the last line queues up some work to run after <code>computeResult()</code> completes. <code>doThing()</code> then returns the resulting future. The trouble is, when <code>doThing()</code> returns, the lifetime of the <code>State</code> object ends, <em>and the continuation is still referencing it</em>. That is now a dangling reference, and will likely cause a crash.</p>
<p>What has gone wrong? Futures let us compute with results that aren’t available yet, and the Boost flavor lets us chain continuations. But the continuation is a separate function with a separate scope. We often need to share data across those separate scopes. No more tidy nested scopes, no more nested lifetimes. We have to manage the lifetime of the state manually, something like this:</p>
<pre>boost::future&lt;void&gt;
computeResult(shared_ptr&lt;State&gt; s); // addref
                                    // the state

boost::future&lt;int&gt; doThing() {
  auto s = std::make_shared&lt;State&gt;();
  auto fut = computeResult(s);
  return fut.then(
    [s](auto&amp;&amp;) { return s.result; }); // addref
                                       // the state
}
</pre>
<p>Since both async operations refer to the state, they both need to share responsibility to keep it alive.</p>
<p>Another way to think about this is: <em>what is the lifetime of this asynchronous computation?</em> It starts when <code>doThing()</code> is called, but it doesn’t end until the continuation — the lambda passed to <code>future.then()</code> — returns. <em>There is no lexical scope that corresponds to that lifetime.</em> And that is the source of our woes.</p>
<h2>Unstructured Concurrency</h2>
<p>The story gets more complicated yet when we consider executors. Executors are handles to executions contexts that let you schedule work onto, say, a thread or thread pool. Many codebases have some notion of an executor, and some let you schedule things with a delay or with some other policy. This lets us do cool things, like move a computation from an IO thread pool to a CPU thread pool, or retry an async operation with a delay. Handy, but like <code>goto</code> it is a very low-level control structure that tends to obfuscate rather than clarify.</p>
<p>For instance, I recently came across an algorithm that uses executors and callbacks (called Listeners here) that retries the async allocation of some resource. Below is a greatly abridged version. It is described after the break.</p>
<pre>// This is a continuation that gets invoked when
// the async operation completes:
struct Manager::Listener : ListenerInterface {
  shared_ptr&lt;Manager&gt; manager_;
  executor executor_;
  size_t retriesCount_;

  void onSucceeded() override {
    /* ...yay, allocation succeeded... */
  }
  void onFailed() override {
    // When the allocation fails, post a retry
    // to the executor with a delay
    auto alloc = [manager = manager_]() {
      manager-&gt;allocate();
    };
    // Run "alloc" at some point in the future:
    executor_.execute_after(
      alloc, 10ms * (1 &lt;&lt; retriesCount_));
  }
};

// Try asynchronously allocating some resource
// with the above class as a continuation
void Manager::allocate() {
  // Have we already tried too many times?
  if (retriesCount_ &gt; kMaxRetries) {
    /* ...notify any observers that we failed */
    return;
  }

  // Try once more:
  ++retriesCount_;
  allocator_.doAllocate(
    make_shared&lt;Listener&gt;(
      shared_from_this(),
      executor_,
      retriesCount_));
}
</pre>
<p>The <code>allocate()</code> member function first checks to see if the operation has already been retried too many times. If not it calls a helper <code>doAllocate()</code> function, passing in a callback to be notified on either success or failure. On failure, the handler posts deferred work to the executor, which will call <code>allocate()</code> back, thus retrying the allocation with a delay.</p>
<p>This is a heavily stateful and rather circuitous async algorithm. The logic spans many functions and several objects, and the control and data flow is not obvious. Note the intricate ref-counting dance necessary to keep the objects alive. Posting the work to an executor makes it even harder. Executors in this code have no notion of continuations, so errors that happen during task execution have nowhere to go. The <code>allocate()</code> function can’t signal an error by throwing an exception if it wants any part of the program to be able to recover from the error. Error handling must be done manually and out-of-band. Ditto if we wanted to support cancellation.</p>
<p>This is <strong>unstructured concurrency</strong>: we queue up async operations in an <em>ad hoc</em> fashion; we chain dependent work, use continuations or “strand” executors to enforce sequential consistency; and we use strong and weak reference counts to keep data alive until we are certain it’s no longer needed. There is no formal notion of task A being a child of task B, no way to enforce that child tasks complete before their parents, and no one place in the code that we can point to and say, “Here is the algorithm.”</p>
<blockquote>
<p><strong>If you don’t mind the analogy, the hops through the executor are a bit like <code>goto</code> statements that are non-local in both time and space: “Jump to this point in the program, <em>X</em> milliseconds from now, on this particular thread.”</strong></p>
</blockquote>
<p>That non-local discontinuity makes it hard to reason about correctness and efficiency. Scale unstructured concurrency up to whole programs handling lots of concurrent real-time events, and the incidental complexity of manually handling out-of-band asynchronous control and data flow, controlling concurrent access to shared state, and managing object lifetime becomes overwhelming.</p>
<h2>Structured Concurrency</h2>
<p>Recall that in the early days of computing, unstructured programming styles rapidly gave way to structured styles. With the addition of coroutines to C++, we are seeing a similar phase shift happening today to our asynchronous code. If we were to rewrite the above retry algorithm in terms of coroutines (using Lewis Baker’s popular <a href="https://github.com/lewissbaker/cppcoro">cppcoro</a> library), it might look something like this:</p>
<pre>// Try asynchronously allocating some resource
// with retry:
cppcoro::task&lt;&gt; Manager::allocate() {
  // Retry the allocation up to kMaxRetries
  // times:
  for (int retriesCount = 1;
       retriesCount &lt;= kMaxRetries;
       ++retriesCount) {
    try {
      co_await allocator_.doAllocate();
      co_return; // success!
    } catch (...) {}

    // Oops, it failed. Yield the thread for a
    // bit and then retry:
    co_await scheduler_.schedule_after(
      10ms * (1 &lt;&lt; retriesCount));
  }

  // Error, too many retries
  throw std::runtime_error(
    "Resource allocation retry count exceeded.");
}
</pre>
<blockquote>
<p>Aside: This replaces the <code>executor_</code> with a <code>scheduler_</code> that implements cppcoro’s <a href="https://github.com/lewissbaker/cppcoro#delayedscheduler-concept">DelayedScheduler</a> concept.</p>
</blockquote>
<p>Let’s …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ericniebler.com/2020/11/08/structured-concurrency/">https://ericniebler.com/2020/11/08/structured-concurrency/</a></em></p>]]>
            </description>
            <link>https://ericniebler.com/2020/11/08/structured-concurrency/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032133</guid>
            <pubDate>Mon, 09 Nov 2020 07:15:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Choose a Programming Language Guide 2021]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25032086">thread link</a>) | @kmhmubin
<br/>
November 8, 2020 | https://mubinsodyssey.com/how-to-choose-a-programming-language-guide-2021 | <a href="https://web.archive.org/web/*/https://mubinsodyssey.com/how-to-choose-a-programming-language-guide-2021">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text"><p>With the blessing of the Internet, people can learn anything anytime, anywhere without any hassle. Programming Language is one of the examples. Nowadays, not only students but also general people learn programming language as hobbies or to make a career on it as programming could be fun and buy you a Lamborghini at the same time. No kidding, As of Nov 1, 2020, the average annual pay for a <a target="_blank" href="https://www.ziprecruiter.com/Salaries/Software-Developer-Salary">Software Developer in the United States is $86,523 a year</a>. Don’t worry, and you don’t need talent, just passion; you can earn too.</p>
<p>However, Choosing a programming language is challenging when you’re getting started. Whenever you search about it, they lead to new recommend blogs, articles, or youtube videos, which could be very confusing. It's not only you; almost everyone faces this common problem. Even I was confused when I started my computer engineering degree.</p>
<p><strong>Table of content</strong></p>
<ul>
<li><p><a href="#no-more-confusion">No More Confusion</a></p>
</li>
<li><p><a href="#choose-a-development-field">Choose A Development Field</a></p>
</li>
<li><p><a href="#choose-a-programming-language">Choose A Programming Language</a></p>
</li>
<li><p><a href="#work-on-projects">Work On Projects</a></p>
</li>
<li><p><a href="#what’s-next">What’s Next</a></p>
</li>
</ul>
<p>Don’t worry. This article will help you to pick the best language for what you want to learn and become. To learn without any stress, follow the steps below.</p>
<h2 id="no-more-confusion"><strong>No More Confusion</strong>😕</h2>
<p>Every time you search on google or watch videos, you would be like, WTF??!! So, in short, just to let you know,</p>
<p>Python is king!😎</p>
<p>Java is down!😪</p>
<p>C++ is hard!🤐</p>
<p>Javascript is boss!😎</p>
<p>PHP is no more! 😰</p>
<p><img src="https://media.giphy.com/media/5t9wJjyHAOxvnxcPNk/giphy.gif" alt="confused"></p>
<p>All these languages make you confused?!! Forget what you have read before about this and make your mind and head clear now.</p>

<h2 id="choose-a-development-field"><strong>Choose A Development Field</strong> ⛏️</h2>
<p>Make your mind about which field you want to work with to be less confused. There are many fields such as,</p>
<ul>
<li>Web Development</li>
<li>Mobile App Development</li>
<li>Desktop App Development</li>
<li>Machine Learning</li>
<li>Security (Software / Network)</li>
</ul>
<p>And many more. Just Pick one of them. Let me explain a little bit to understand those fields.</p>
<p>💠 <strong>Web Development</strong> </p>
<p>Web development is the building and maintenance of websites; it’s the work that happens behind the scenes to make a website look great, work fast, and perform well with a seamless user experience.</p>
<p>💠 <strong>Mobile App Development</strong></p>
<p>Mobile app development is creating software intended to run on mobile devices and optimized to take advantage of those products' unique features and hardware.</p>
<p>💠 <strong>Desktop App Development</strong></p>
<p>Desktop Applications are run stand alone on the user’s laptops and systems. The term used for these applications desktop differs from mobile applications, which are in the trend. The key features of desktop applications are the high efficiency of the application, and these are highly customized as per user requirements and flexibility.</p>
<p>💠 <strong>Machine Learning</strong></p>
<p>Machine learning is an application of artificial intelligence (AI) that provides systems the ability to learn and improve from experience without being explicitly programmed automatically. Machine learning focuses on developing computer programs that can access data and use them to learn for themselves.</p>
<p>💠 <strong> Software Security</strong></p>
<p>Software security is an idea implemented to protect software against malicious attacks and other hacker risks so that the software continues to function correctly under such potential risks. Security is necessary to provide integrity, authentication, and availability.</p>
<p>💠 <strong> Network Security</strong></p>
<p>Network security is a broad term that covers a multitude of technologies, devices, and processes. In its simplest term, it is a set of rules and configurations designed to protect the integrity, confidentiality, and accessibility of computer networks and data using both software and hardware technologies.</p>

<h2 id="choose-a-programming-language"><strong>Choose A Programming Language</strong>🛠️</h2>
<p>Choose a language based on the platform. Why, if you ask? It will help you to learn faster, and you can become more productive. Now the main problem, There’s a lot of programming languages. Wikipedia has a list of over <a target="_blank" href="https://en.wikipedia.org/wiki/List_of_programming_languages">700 programming languages</a>.</p>
<p><img src="https://media.giphy.com/media/3o6YglDndxKdCNw7q8/giphy.gif" alt="what"></p>
<p>Wait. WHAT!! 😲</p>
<p>Are you kidding me? How can I choose a programming language over 700 languages?</p>
<p>Hold your horse, man. Don’t worry.</p>
<p>Before that, Check out this gem.</p>
<p><img src="https://i.imgur.com/7iR9fH4.jpg" alt="programmic joke"></p>
<p>I will give you a better explanation. Don’t get confused after seeing the list. We don’t need to know about it anyway. I will provide a good summary of programming language that can use for personal work or company work.</p>
<p>🔷 <strong><a target="_blank" href="https://www.java.com/en/">Java</a></strong></p>
<p>Popularity: Very high</p>
<p>Ease of Learning: Moderate to Difficult</p>
<p>Use Cases: General Use and Specialty</p>
<ul>
<li>Web applications</li>
<li>Mobile</li>
<li>Embedded systems</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://www.cprogramming.com/">C</a></strong></p>
<p>Popularity: Medium</p>
<p>Ease of Learning: Moderate</p>
<p>Use Cases: General Use and Specialty</p>
<ul>
<li>Embedded systems</li>
<li>Hardware drivers</li>
<li>Local Applications</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://python.org/">Python</a></strong></p>
<p>Popularity: Very High</p>
<p>Ease of Learning: Easy to Moderate</p>
<p>Use Cases: General Use and Specialty</p>
<ul>
<li>Web Applications</li>
<li>Artificial Intelligence</li>
<li>Machine Learning</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://isocpp.org/">C++</a></strong></p>
<p>Popularity: High</p>
<p>Ease of Learning: Difficult</p>
<p>Use Cases: General Use, Specialty</p>
<ul>
<li>Local Applications</li>
<li>Web Services</li>
<li>Proprietary Services</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://docs.microsoft.com/en-us/dotnet/csharp/">C#(Sharp)</a></strong></p>
<p>Popularity: High</p>
<p>Ease of Learning: Moderate</p>
<p>Use Cases: General Use</p>
<ul>
<li>Web Applications</li>
<li>Local Applications</li>
<li>Services/Microservices</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://www.w3schools.com/">HTML</a></strong></p>
<p>Popularity: High</p>
<p>Ease of Learning: Easy</p>
<p>Use Cases: Web Sites and Application</p>
<ul>
<li>Web Development</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/JavaScript">Java Script</a></strong></p>
<p>Popularity: Very High</p>
<p>Ease of Learning: Moderate</p>
<p>Use Cases: General Use</p>
<ul>
<li>Local Applications</li>
<li>Web Applications</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://www.php.net/">PHP</a></strong></p>
<p>Popularity: High</p>
<p>Ease of Learning: Easy</p>
<p>Use Cases: General Use</p>
<ul>
<li>Web Applications</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://www.mysql.com/">SQL</a></strong></p>
<p>Popularity: Very High</p>
<p>Ease of Learning: Easy to Moderate</p>
<p>Use Cases: Specialty</p>
<ul>
<li>Database Queries</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://developer.apple.com/library/archive/documentation/Cocoa/Conceptual/ProgrammingWithObjectiveC/Introduction/Introduction.html">Objective-C</a></strong></p>
<p>Popularity: High</p>
<p>Ease of Learning: Difficult</p>
<p>Use Cases: Mobile Applications</p>
<ul>
<li>Apple iOS devices: iPhone, iPad</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://www.ruby-lang.org/en/">Ruby</a></strong></p>
<p>Popularity: High</p>
<p>Ease of Learning: Easy to Moderate</p>
<p>Use Cases: General</p>
<ul>
<li>Web Applications</li>
<li>Scripting</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://developer.apple.com/swift/">Swift</a></strong></p>
<p>Popularity: Medium</p>
<p>Ease of Learning: Moderate to Difficult</p>
<p>Use Cases: Apple Mobile and Desktop applications</p>
<ul>
<li>MacBook</li>
<li>iPhone</li>
<li>iPad</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://golang.org/">GO</a></strong></p>
<p>Popularity: Low</p>
<p>Ease of Learning: Moderate</p>
<p>Use Cases: General</p>
<ul>
<li>Web Applications</li>
<li>Local Applications</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://www.perl.org/">Perl</a></strong></p>
<p>Popularity: High</p>
<p>Ease of Learning: Easy to Moderate</p>
<p>Use Cases: General</p>
<ul>
<li><p>Local Applications</p>
</li>
<li><p>Web Applications</p>
</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://dart.dev/">Dart</a></strong></p>
<p>Popularity: Niche</p>
<p>Ease of Learning: Moderate</p>
<p>Use Cases: General</p>
<ul>
<li>Web Applications</li>
<li>Mobile Applications</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://kotlinlang.org/">Kotlin</a></strong></p>
<p>Popularity: Medium</p>
<p>Ease of Learning: Moderate</p>
<p>Use Cases: Mobile Development</p>
<ul>
<li>Android Applications</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://visualstudio.microsoft.com/vs/features/net-development/">Visual Basic .NET</a></strong></p>
<p>Popularity: Low</p>
<p>Ease of Learning: Moderate</p>
<p>Use Cases: General Use</p>
<ul>
<li>Web Applications</li>
<li>Local Applications</li>
</ul>
<p><img src="http://carlcheo.com/wp-content/uploads/2014/12/which-programming-language-should-i-learn-first-infographic.png">
<em>Image Credit: <a href="http://carlcheo.com/" target="_blank">Carlcheo.com</a> </em></p>
<p>Choose only one. Start from the very basic and practice every day for at least 2 hours. And try to solve problems.</p>

<h2 id="work-on-projects">Work On Projects🗄️</h2>
<p>After choosing a project or where you want to work, choose a language. And after that, <strong>Start working on small projects</strong>. Like building a simple calculator. Little by little, go from small to big projects. Projects will help you to understand what is lacking. Challenge yourself to create new things.</p>
<p>Best things to do, Build 30 things in 30 days of the challenge.</p>
<p><strong>#build30thingschallenge #mubinsodyssey</strong> on Twitter with us.</p>
<p>Here some project Ideas, if you don’t have for yourself. I would say work on the common and small projects first, then go for your ones. It will be more productive and efficient for you.</p>
<ul>
<li>Guess The Number</li>
<li>Rock, Paper, Scissors Game</li>
<li>Password Generator</li>
<li>Dice Rolling Simulator</li>
<li>Hangman Game</li>
<li>Digital Cloak</li>
<li>Word Counter Tool</li>
<li>Percentage Calculator</li>
<li>Height &amp; Weight Converter Calculator</li>
<li>Temperature Conversion tool</li>
<li>Restaurant Bill Management System</li>
<li>ATM Management System</li>
<li>Movie Ticket Booking System</li>
<li>Attendance Management System</li>
<li>Tic Tac Toe Game</li>
<li>Banking System</li>
<li>Library Management System</li>
<li>Student Report Card Generator</li>
<li>Contact Management system</li>
<li>Pacman Game</li>
<li>Personal Diary management system</li>
<li>Quiz Game</li>
<li>Typing Tutor</li>
</ul>
<p>And many more. Just google small projects.</p>

<h2 id="whats-next">What’s Next ⏭</h2>
<p>After that, chose one of the below options that suit best for you.</p>
<ul>
<li>Apply for a job; which will help you to get real-world experiences in the programming world</li>
<li>Freelancing; which will introduce you to other programmers where you can enrich your programming knowledge</li>
<li>Startup; It will develop your own idea.</li>
</ul>
<p>Here some best websites you can search for your desired jobs.</p>
<ul>
<li><a target="_blank" href="https://jobs.github.com/positions?description=&amp;location=Remote">Github</a></li>
<li><a target="_blank" href="https://stackoverflow.com/jobs">Stack overflow</a></li>
<li><a target="_blank" href="https://jsremotely.com/">JSRemotely</a></li>
<li><a target="_blank" href="https://authenticjobs.com/">Authentic Jobs</a></li>
<li><a target="_blank" href="https://www.indeed.com/">Indeed</a></li>
<li><a target="_blank" href="https://jobbatical.com/explore">Jobbatical</a></li>
<li><a target="_blank" href="https://weworkremotely.com/">We work remotely</a></li>
<li><a target="_blank" href="https://bigcloud.io/">Bigcloud</a></li>
<li><a target="_blank" href="https://remoteok.io/">Remote Ok</a></li>
<li><a target="_blank" href="https://www.androidjobs.io/">AndroidJobs</a></li>
<li><a target="_blank" href="https://landing.jobs/">Landing Jobs</a></li>
<li><a target="_blank" href="https://ai-jobs.net/">Ai-jobs</a></li>
<li><a target="_blank" href="https://www.toptal.com/">Toptal</a></li>
<li><a target="_blank" href="https://www.upwork.com/">Upwork</a></li>
<li><a target="_blank" href="http://www.guru.com/">Guru</a></li>
<li><a target="_blank" href="https://www.freelancer.com/">Freelancer</a></li>
<li><a target="_blank" href="https://www.fiverr.com/">Fiverr</a></li>
</ul>

<hr>
<p>🚩👉 If it was useful to you, please Like/Share to reach others as well. Please hit the <strong><em>Subscribe</em></strong> button at the top of the page to get an email notification on my latest posts.</p>
<p>I talk about web development and UI design on <strong>Twitter</strong> <a target="_blank" href="https://twitter.com/kmhmubin">@kmhmubin</a>, come to talk with me there!</p>
<p>The cover image is an improvisation on top of the work from <a target="_blank" href="https://www.freepik.com/vectors/arrow">Freepik</a>.</p>
</div></div>]]>
            </description>
            <link>https://mubinsodyssey.com/how-to-choose-a-programming-language-guide-2021</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032086</guid>
            <pubDate>Mon, 09 Nov 2020 07:01:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I'm going to experiment by being Blind and Alone for 24 Hours]]>
            </title>
            <description>
<![CDATA[
Score 82 | Comments 75 (<a href="https://news.ycombinator.com/item?id=25031774">thread link</a>) | @Osiris30
<br/>
November 8, 2020 | https://dormin.org/2020/11/08/the-blind-alone-and-confused-for-24-hours-challenge/ | <a href="https://web.archive.org/web/*/https://dormin.org/2020/11/08/the-blind-alone-and-confused-for-24-hours-challenge/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-896">

	
	<!-- .entry-header -->


			<div>

			
<figure><img data-attachment-id="901" data-permalink="https://dormin.org/bird-box/" data-orig-file="https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg" data-orig-size="2000,1050" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="bird-box" data-image-description="" data-medium-file="https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg?w=300" data-large-file="https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg?w=760" src="https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg?w=1024" alt="" srcset="https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg?w=1024 1024w, https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg?w=150 150w, https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg?w=300 300w, https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg?w=768 768w, https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg 2000w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>For 24 hours I will be blind and alone in my apartment. I eventually want to try being blind for a week, but I’ll need seven days with no other obligations, and I won’t have that for a while. For now, I’ll suffice with a smaller-scale experiments with a few extra provisions for added difficulty.</p>







<ol type="1"><li>I must leave my blindfold on for 24 hours.<ul><li>If I remove the blindfold, I have failed the experiment</li><li>If the blindfold falls off or I can get partial sight, I have failed the experiment.</li><li>I am only allowed to readjust my blindfold if I can see light.</li></ul></li><li>I must not be in contact with any other people for 24 hours.<ul><li>I cannot answer my phone or any other messaging system.</li><li>I cannot receive in-person visitors.</li><li>If someone knocks at the door, I cannot answer verbally or physically.</li></ul></li><li>I will set an alarm for 24 hours. I cannot set any other alarms or use any other means to ascertain the time.<ul><li>It is up to me to keep my phone charged so the alarm goes off.</li></ul></li><li>I cannot leave my apartment.</li></ol>











<p>I have no good reason. I just want to see if I am capable of doing it and what will happen. Some things I’m curious about:</p>



<ul><li>Do I have the willpower to get through the experiment?</li><li>Will I become disoriented from losing all sense of time?</li><li>Will I be able to stave off boredom with podcasts, audiobooks, and music on my phone?</li><li>Will I enter some sort of meditative state due to a lack of sensory input?</li><li><a href="https://www.discovermagazine.com/the-sciences/scientists-made-people-wear-blindfolds-for-4-days-the-resulting-hallucinations-were-incredible">Will I hallucinate</a>?</li><li>Will my non-sight senses heighten?</li><li>Will I hurt myself by falling or banging into something?</li><li>Will I sleep?</li><li>Will I eat? Is consuming caffeine a good idea (for entertainment) or a bad idea (energy with no direction)?</li><li>Will this experience make me more interested in being blind for a week? Or less?</li></ul>



<figure><img src="https://digitalimpact.io/wp-content/uploads/2014/08/Blind.png" alt="Modern CEOs Are Blindfolded - Digital Impact"></figure>







<p>Attempt One started at 10:30 AM and failed at 1:13 PM. I purposefully took off my blindfold because I was worried that my multiple failures to input my Iphone’s password had resulted in a permanent lock or data wipe. But the password screen was just locked for a minute and all was well.</p>



<p>Given that I failed in the early afternoon, I considered restarting the experiment on another day in the morning. But I had already carved out a 24 hour period when I wouldn’t do any work or be disturbed, and it might have been a week or two longer before I got that opportunity again.</p>



<p>So I checked my messages, briefly went on Reddit, and then restarted.</p>



<div><figure><img data-attachment-id="903" data-permalink="https://dormin.org/t86752/" data-orig-file="https://dorminorg.files.wordpress.com/2020/11/blindfold-2.jpg" data-orig-size="521,610" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;CSA Images \/ CSA Images&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;Blindfolded Woman&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;\u00a9 CSA Images \/ CSA Images&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;T86752&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="T86752" data-image-description="" data-medium-file="https://dorminorg.files.wordpress.com/2020/11/blindfold-2.jpg?w=256" data-large-file="https://dorminorg.files.wordpress.com/2020/11/blindfold-2.jpg?w=521" src="https://dorminorg.files.wordpress.com/2020/11/blindfold-2.jpg?w=521" alt="" srcset="https://dorminorg.files.wordpress.com/2020/11/blindfold-2.jpg 521w, https://dorminorg.files.wordpress.com/2020/11/blindfold-2.jpg?w=128 128w, https://dorminorg.files.wordpress.com/2020/11/blindfold-2.jpg?w=256 256w" sizes="(max-width: 521px) 100vw, 521px"></figure></div>







<p>Attempt Two was successful. I put on my blindfold at 1:23 PM on Thursday, November 5, 2020. I removed it at 1:28 PM on Friday, November 6.</p>



<p>It was an… interesting experience. I don’t recommend it, but I’m glad I did it. I’m not sure where to begin in describing it, especially since I couldn’t take notes, and part of the challenge was being confused. But I’ll do my best to break down the experience.</p>



<div><figure><img src="https://cdn.shopify.com/s/files/1/0818/3417/products/Les_Sublimes_Cashmere_Scarf_Dark_Blue_packshot_2048x.jpg?v=1539973736" alt="Large Cashmere Scarf in Dark Blue | Les Sublimes" width="427" height="427"></figure></div>



<h2><strong><span>Blindfold</span></strong></h2>



<p>To simulate blindness, I used a dark blue scarf as a blindfold. One layer wasn’t quite dark enough, so I folded it in half for extra light defense.</p>



<p>With the blindfold securely on, my vision was the same whether my eyes were open or closed. I kept my eyes closed 99.9% of the time since it was usually more comfortable and helped limit light. I occasionally opened my eyes to check the brightness level and to… I guess you could call it <em>stretch my eyelids.</em> They don’t feel good if you leave them closed for too long.</p>



<p>I couldn’t get a perfect scarf seal around my eyes, so sometimes when I tilted my head back while sitting I noticed a little light come into the bottom of my vision. To limit this, I often pinched the scarf around my nose in that position. But many/most blind people can see some light anyway, so I don’t think this was a significant violation of the experiment.</p>



<p>My eyes got quite dry under the scarf, so I applied moisturizer to this lids and sockets four or five times. I wanted to use eyedrops too, but there was no way to do so without failing the experiment.</p>



<figure><img loading="lazy" data-attachment-id="906" data-permalink="https://dormin.org/image/" data-orig-file="https://dorminorg.files.wordpress.com/2020/11/image.jpeg" data-orig-size="225,225" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-medium-file="https://dorminorg.files.wordpress.com/2020/11/image.jpeg?w=225" data-large-file="https://dorminorg.files.wordpress.com/2020/11/image.jpeg?w=225" src="https://dorminorg.files.wordpress.com/2020/11/image.jpeg?w=225" alt="BLACK N BLACK - #blackouttuesday ✊🏻✊🏼✊🏽✊🏾✊🏿 | Facebook" width="786" height="786" srcset="https://dorminorg.files.wordpress.com/2020/11/image.jpeg 225w, https://dorminorg.files.wordpress.com/2020/11/image.jpeg?w=150 150w" sizes="(max-width: 786px) 100vw, 786px"></figure>



<h2><strong><span>Blindness</span></strong></h2>



<p>Initially, everything was black, but as the day went on and the sun went down, I could tell it was nighttime even through two layers of scarf and my eyelids. I’m not sure if I could tell because my eyes had adjusted to become extremely sensitive to light, or if there were other subtle signals (ie. noises, air temperature, circadian rhythms, etc.) which my body picked up on. As evidence of the latter, I could not see any difference between the tv being on or off, nor the refrigerator being opened or closed, even when I was sitting right in front of either.</p>



<p>What I saw depended on how I applied my focus. If I did focus on my vision, I’d see the typical blackness you get from closing your eyes, but it was never perfectly black nor uniform; there was always some odd movement and occasional coloring (whiteness, pale blue, or sometimes red). The most common distortions were a swirling or flowing whiteness, sort of like cream in coffee. I hoped that being blindfolded for so long would make the distortions more extreme, but for the most part it looked no different than what you’d see if you closed your eyes right now for ten minutes.</p>



<p>There was one exception. It must have been about 20+ hours into the experiment, and my eyes were itching, so I rubbed both of them at the same time over the scarf. If you rub your closed eyes and focus on your sight any time you can see some weird stuff, but this was far more extreme than usual. I remember my entire vision filling up with white bubbles which then broke and briefly returned to black. Then white lightning bolt shapes stretched across my sight, expanded to make my vision purely white, and then slowly faded back to black. The strangest thing about it was the <em>brightness</em>. I literally felt like I was staring into lights despite being blindfolded in a dark room. Unfortunately, it only lasted about 30 seconds, but my heart was racing.</p>



<p>More notable than what I saw was what I didn’t see. By default, I was lost in thought and I focused on nothing. In such a state, I didn’t even register my vision or notice the darkness. I <em>think</em> this made my imagination and mental visualization more acute. On occasion, I’d be deep in thought and I’d get the <em>brightness</em> sensation again because I’d be mentally picturing something so vividly that the inevitable return to darkness felt like shutting off the lights in my brain. I’ll explain more about this in the <strong>Three Phases</strong> section.</p>



<p>Sadly, I did not hallucinate, or at least not as far as I could tell.</p>



<figure><img src="https://i1.wp.com/www.intelligentliving.co/wp-content/uploads/2014/07/sloth-sleeping.jpg?fit=1024%2C698&amp;ssl=1" alt="Fighting Bacteria With Sloth Fur"></figure>



<h2><strong><span>Energy</span></strong></h2>



<p>This was the most surprising aspect of the experiment.</p>



<p>I read that <a href="https://abcnews.go.com/Health/story?id=117902&amp;page=1#:~:text=Without%20light%20cues%20that%20the,as%20a%20result%2C%20researchers%20say.">blind people have trouble getting to sleep</a> because they don’t access any/enough light for their circadian rhythms. I seem to have the exact opposite problem. Without light, my body always thinks it’s time to sleep and has trouble doing anything else. Throughout most of the experiment, I felt extremely lethargic, lazy, and had to fight to stay awake.</p>



<p>I started my first failed experiment attempt at 10:30 AM. I had gotten 7.5 solid hours of sleep, I hadn’t done anything tiring the previous day, and I generally felt fine. Then I put on my blindfold, and within thirty minutes I was nodding off. I semi-slept for two hours before deciding to get an energy drink to get myself out of the funk. That worked, but as soon as it wore off, I was back in semi-sleep mode.</p>



<p>Even when I was firmly awake, I generally felt weak and lethargic. Movement around the apartment was annoying of course, but made so much more difficult by my energy levels. I ended up lying perfectly still in my comfy computer chair with my feet on a table 95% of the time. That is, when I wasn’t lying in bed.</p>



<p>On the other hand, when I removed my blindfold after 24 hours, I experienced a <em>burst</em> of energy. Seriously, it was like I had downed a double shot of espresso. It was like a switch had been flicked. The haziness and cobwebs were gone in an instant, and I felt the energy coursing through my body. I guess light has a big impact on me.</p>



<div><figure><img data-attachment-id="908" data-permalink="https://dormin.org/blind-man-2/" data-orig-file="https://dorminorg.files.wordpress.com/2020/11/blind-man-2.jpg" data-orig-size="615,479" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1604789422&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="blind-man-2" data-image-description="" data-medium-file="https://dorminorg.files.wordpress.com/2020/11/blind-man-2.jpg?w=300" data-large-file="https://dorminorg.files.wordpress.com/2020/11/blind-man-2.jpg?w=615" src="https://dorminorg.files.wordpress.com/2020/11/blind-man-2.jpg?w=615" alt="" srcset="https://dorminorg.files.wordpress.com/2020/11/blind-man-2.jpg 615w, https://dorminorg.files.wordpress.com/2020/11/blind-man-2.jpg?w=150 150w, https://dorminorg.files.wordpress.com/2020/11/blind-man-2.jpg?w=300 300w" sizes="(max-width: 615px) 100vw, 615px"></figure></div>



<h2><strong><span>Movement</span></strong></h2>



<p>I moved exactly how you’d expect… clumsily.</p>



<p>For the most part, I slowly walked around my apartment with a hand out to feel for walls and edges. Sometimes I’d get lazy and crawl just so it was easier. I know my apartment well enough that it wasn’t hard to get around, but every once in awhile I’d lose track of where I was and would be left slowly swinging my arm around searching for anything. It’s not a pleasant sensation.</p>



<p>Before the experiment, I had planned to pace around for fun, or maybe even do some exercise with the free time. But the confusion and especially the lethargy stopped all that. I just sat in my chair and didn’t move unless I needed to get a drink, go to the bathroom, or sleep.</p>



<p>I kind of wish I had done the experiment in an unfamiliar environment to add to the movement challenge, but oh well.</p>



<div><figure><img src="https://secure.img1-fg.wfcdn.com/im/99629273/compr-r85/1167/116715839/dual-flush-elongated-one-piece-toilet-seat-included.jpg" alt="DeerValley Dual-Flush Elongated One-Piece Toilet (Seat Included) &amp; Reviews  | Wayfair" width="729" height="729"></figure></div>



<h2><strong><span>Necessities</span></strong></h2>



<p>For food, I ate a big lunch at 10 AM before the experiment and then munched on dark chocolate throughout the night. I felt the heavy lethargy well before the lack of calories was an issue. I probably should have put some prepackaged meals in my fridge to eat, but I was worried about making a mess and not being able to clean up. Do I want ants? Because that’s how I get ants.</p>



<p>For drinks, I could manage to get to the kitchen and fill a cup with water when I needed to. I never took a full cup back to my chair just in case I knocked it over (clean up would be a nightmare). I also had some diet coke to serve as entertainment and put a little caffeine in me.</p>



<p>For the bathroom, I (a man) peed sitting down. I’m not ashamed to admit it.</p>



<div><figure><img src="https://imgaz2.staticbg.com/thumb/large/oaupload/banggood/images/F2/09/b934b522-e3e3-491d-b758-d0b92c259f0c.jpg" alt="Novel surreal melting distorted wall clock surrealist salvador dali style  wall clock amazing home decoration gift Sale - Banggood.com" width="802" height="801"></figure></div>



<h2><strong><span>Time</span></strong></h2>



<p>As part of the experiment, I never knew what time it was. This was intended to confuse me throughout the 24 hours, and it did, but it may have helped too. With no sense of time, it was easy to sit back and not think about it. Time drifted by and I existed. That was that.</p>



<p>I actually did ask Siri for the time once… it was late in the experiment, and it felt like I had put on the blindfold forever ago. As you’d expect, …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dormin.org/2020/11/08/the-blind-alone-and-confused-for-24-hours-challenge/">https://dormin.org/2020/11/08/the-blind-alone-and-confused-for-24-hours-challenge/</a></em></p>]]>
            </description>
            <link>https://dormin.org/2020/11/08/the-blind-alone-and-confused-for-24-hours-challenge/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031774</guid>
            <pubDate>Mon, 09 Nov 2020 05:39:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Postgres Constraints]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25031762">thread link</a>) | @mkfeuhrer
<br/>
November 8, 2020 | https://www.mohitkhare.com/blog/postgres-constraints | <a href="https://web.archive.org/web/*/https://www.mohitkhare.com/blog/postgres-constraints">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-v-373a3b57=""><div data-v-373a3b57=""><div data-v-373a3b57=""><div data-v-373a3b57=""><div data-v-373a3b57=""><p data-v-373a3b57="">Postgres Constraints</p> <div data-v-373a3b57=""><div data-v-373a3b57=""><div data-v-373a3b57=""><div data-v-373a3b57=""><a href="https://www.mohitkhare.com/categories/postgres" data-v-373a3b57=""><p>Postgres</p></a></div><div data-v-373a3b57=""><a href="https://www.mohitkhare.com/categories/programming" data-v-373a3b57=""><p>Programming</p></a></div></div> <p><span data-v-373a3b57="">7 Nov 2020</span></p></div></div></div> <p><img data-src="/_nuxt/img/postgres-constraints.8f3344a.webp" alt="Postgres Constraints" data-v-25298ab3="" src="https://www.mohitkhare.com/_nuxt/img/postgres-constraints.8f3344a.webp"></p></div></div> <div data-v-1bfa12aa="" data-v-373a3b57=""><p data-v-1bfa12aa="">Get latest articles directly in your inbox</p> <div data-v-1bfa12aa=""><form action="https://usetaski.us18.list-manage.com/subscribe/post?u=2974614c11e6abca644007be7&amp;id=3b5ecce493" method="post" name="mc-embedded-subscribe-form" target="_blank" novalidate="" data-v-1bfa12aa=""><div data-v-1bfa12aa=""> </div></form></div></div>  <div data-v-5c76b055="" data-v-373a3b57=""><p data-v-5c76b055="">Liked the content? Do support :)</p> <div data-v-5c76b055=""><p><a href="https://www.paypal.me/mkfeuhrer" aria-label="Paypal - Mohit Khare" data-v-5c76b055=""><img src="https://www.mohitkhare.com/_nuxt/9922499ba185bcccc368872c4cf7b0ea-320.png" alt="Paypal - Mohit Khare" width="125px" data-v-5c76b055=""></a></p> <p><a target="_blank" href="https://www.buymeacoffee.com/chHAzigTb" aria-label="Buy me a coffee - Mohit Khare" data-v-5c76b055=""><img src="https://www.mohitkhare.com/_nuxt/img/bmc.87873ba.svg" width="175px" alt="Buy me a coffee" data-v-5c76b055=""></a></p></div></div>  <div data-v-373a3b57=""></div></div> <div data-v-373a3b57=""><div data-v-e9f7aa1a="" data-v-373a3b57=""><p data-v-e9f7aa1a="">Explore more</p> <div data-v-e9f7aa1a=""><div data-v-e9f7aa1a=""><div data-v-36f5b510="" data-v-e9f7aa1a=""><div data-v-36f5b510=""><div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/personal-okrs" data-v-36f5b510=""><p><img data-src="/_nuxt/0b543a8b05ad63f5c8bf88ee97692842-613.png" data-loading="data:image/png;base64,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" alt="Personal OKRs for Success" data-v-25298ab3="" src="https://www.mohitkhare.com/_nuxt/0b543a8b05ad63f5c8bf88ee97692842-613.png"></p></a></div> <div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/personal-okrs" data-v-36f5b510=""><p data-v-36f5b510="">Personal OKRs for Success</p></a></div></div></div><div data-v-36f5b510="" data-v-e9f7aa1a=""><div data-v-36f5b510=""><div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/making-decisions-the-right-way" data-v-36f5b510=""><p><img data-src="/_nuxt/c04ef31c75ad70464207ddcf44872feb-1000.png" data-loading="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAIAAAADnC86AAAACXBIWXMAAAsSAAALEgHS3X78AAAFHUlEQVRYw+2W+09TdxTA+09MGTDkMR52wtg0BgqlLZTe3va+2t7e8hDDFKPy2ARRhhRaufSW12Jw04y5OQXceMz34qyoqDhhY0sQGMrMEoNITOYmTggOKN1p76yLulEQ2C+cfHPy7b3f7/mcc773e04FerzsfxmCJfASeAm8BF58ME1Y9bhFj7OLCgYqoyjQq0160jo39qzBDM5qMGsquj0vMmrd61FatVlPcHNgzxYMAEsqkn9MhLQLVbYVcUYhpsFZZiHBTtNJOItj3D4FMxhD9Abi/SHkWR9xeuxmmqpgsFLXGnaewQxZbsDLKKoyWba1A8X6I/CT4fgFIdblK2dDURVZxhAcQ1gYj9PuEZjGLVp5PomXZoo21gpjbiHY2VimfXCwqba+dZm4wUdUGInRmJlCjTploYdHPiOYpQkuVVlQFRzDvYm2BSHNwrhfVJoLb1MnWk7X51vavCRHfMS21ySfrdGYwxONq1QUyTEvHzGDsSRZkS/L6A5Bmv0kF5fhjcK4W5i2JwQ77yNt95N3BarrvKNPL1e0+isvhctPBSJauGbEzHdsBrABZwmyoliWfiMUPfyqqOUVSX2o+Cam6V5J9K3UXA+jOgLQQ95RX3pJWrzFVyMQmz9CJ+TqyfKXBcM90ZLlGfJsW2B0/ipShxiz1mq7EfSncOpaENoVinWuQKr9xLQ8L02SWbcqvjYonlSbGGdFm4ePy0JjpWmidJ3KnEqYDoTJrsgUA6u1dcGxtuDEi36KWq/VOVEbcN2eFFlOijST9qyWeXyPSasWL9+C5FwTyo9Gym7GaurCYk8GyL7ylZ7yFnHhOtJ1l2iPK6inYAO2W4tbN6C530WgXwdIv49WHgmLa/KN/cI7psNfXhJpICmrAWqIs4zMbwGBw8at6ej27gikOQCpWaNqDI7at3ztByGJ/f5y41tJBFUOV2D+S6azNxDWDOX2pjckqfE7kvD8XKl0vRgjUGONECuJ0BGaygUBu1ohl4LuSpNlUUTlOmzbFjpZr2B00J2Uu9ZLc+hZ9sfZdSeonToo2oQFTzQdq2vdmbmfUJgY0qqbfVeewx8B1kBa1AnFPddvV3EtMIGfi9CPoQuV6dRsqq784cjY8eZvUVlREsXxz2HCD2hTswMbSKcJWs265y6LFl7zI1nDYfJiU2G9w+G40T+oVZXCQwPF0RgL0ePyEtAkYv5vNngp+OcP2IPJS8CEc3+8kSdRSjO4AmepkhnV8cV4YgmWUNxxtd/hkt1FDQmiAlW8ESwc/MTW1Tnwzemu7E37YL2B4p4tBqSFIZ02IWdPwaTCXGVp6eu5PXz3txt9g3urTwCDQsxb3tkL2duW+fG5Mz+2X+4r2nGIMzcC0m6fBn1n8Ne2892HP23lXbHb7aD/eDiWtekj8MadNkBCPOANkWgCnayxCvinEOjnB87BnulppzmHS5063nm5rffx44mhO/dHHozyIY6P/zn6aNzxIpmassOYnJiCeW/P7a0b9sJBAAPaM+QSJu/nHSzMO5iXXfvzwJAAAteqdmdu/JA3NzkJu537+YDcAg7xdqed4ph+8hKWTf293u5eDBZA3xv+HWAKcSFkrqb6xN2h+xOTUxMTU2Njj+GtANII7/ZUHnUnyi3ghNMPl2m3H4B9xqcXCmwEbTvzAxxfU8OlJ1467fAxCMCdne8duDf84GmeF0Cetyx4d/P+kZHRBaJCfHyIz78SXLnUy38XjsUVwSPXN7VgOf5X+QuyMiOTQYnkogAAAABJRU5ErkJggg==" alt="Making Decisions: The right way" data-v-25298ab3="" src="https://www.mohitkhare.com/_nuxt/c04ef31c75ad70464207ddcf44872feb-1000.png"></p></a></div> <div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/making-decisions-the-right-way" data-v-36f5b510=""><p data-v-36f5b510="">Making Decisions: The right way</p></a></div></div></div><div data-v-36f5b510="" data-v-e9f7aa1a=""><div data-v-36f5b510=""><div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/productivity-chrome-extensions" data-v-36f5b510=""><p><img data-src="/_nuxt/6162e8bc88982ff3c2db9ce53102c07a-800.png" data-loading="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAIAAAADnC86AAAACXBIWXMAAAsSAAALEgHS3X78AAAEEUlEQVRYw+1WW0hbSRgOFNanug+CVXxYlWi9BrO1+BBpC6XQy1OfFJaCpcIKuwuLUGhrK4oYtcULWu9RXBcvVbNGI6s16x2lrZfE1tbGKxVtjTG9JEZNTs45/TzT1ZgYm9K+FM73MJlz8s985/vnn29GcNwNhISECIXCqKiotbU1lmUZhmG/GgKemCfmiXni7484ODhY+DkEBQUFBASIRKJvSRwZGRkdHX3iUCBALBbHxMTodLpdYpPJ9MFtINiRWCqV0jSt1+vfugb+tVqtNTU18/PzZNjW1lZERISHh4enp+fRQ4EAhIWHh2OIfbYEBQUF+KEo6pC02Gw2tE1NTXNzc+TN9va2RCLx9fX19/f/yQ4BTggMDPTz84uNjcWQfcT5+fl7xNxbBhnY2qStFub/Nw7EZDBaBDIcyCMAWZsHgcjdl+o9Ym78pnr0nfT2+h8Jhuu/GR/8bds0H6gYme/q6hoZGRkeHh4cHBwYGOjv7+/u7iaRbhXXJ2KLBa3h0cDrsxKdRPz+4inDhZMrkgh9+i2aYWknYijIyMiQyWS1tbXV1dUlJSWFhYV9fX1fUNWEmKZs26ztZtu18Wjh/K+/P6jvHZHnmTv8TP/8QOnbaS7UQXFbm0Iul3d2dv7LAepHR0ehG/QKhWJ8fBz9hxw0Go1LYpZhV94vn+68eC/tTHFl3+n7T+IrplZexLHPBdSbVJsTMbZHXV1dRUVFcXFxeXl5aWkpOtDd0NCA5OODWltb6+vrlUrl1NQUvmBHG00fTKwzrZ2rvXBZcelGsyLqT+WlrIf6Z+dZjYB6LXUmRqqxtCqVCuLa29uheGlpqbe3t6enp6OjA4uNztDQEOSiFBYWFpxtZ19VX1feEBWJr7Yl/FKXK1MlsE+PmB8fpTbUNONIjPiioqL09HTCmpmZqdVqoRJpSE1NzcvLq6qqyuWQmJiIT3Sp2EpZ0b4yLMX/dSUs9+czMtFzlYfpvx+NCyVcVVPOxJhOrVajhT5S2KQzMTEBrRCKxZ6ennZlEnuKGXZH1zuzQfa4UT50x/Iixax/xMUwztsJa5yTk1NWVoaXEJeVlZWcnIw6T0tLq6ysRIXjMSUlpaWlZXc3OhIjG8SJ8PcOPXEDZIYzDxsHC7fZGhsb7Q0EqaM47DoJwlDtmIrYhZWDy+2EpXLTMpubm4lXE5MyGAxw/42NDXTW19ctHEBpNBqJ+ZOzYYUDzMuROC4uDpnJzs6+6xrIKlYkKSlpeXl51yw1avXY2NhLrXZ2dha7FuuNzurqKjb04uIi+jMzM4ifnJzEquOkcShsAZzd29v72KHw8fHx8vLCjQA69o23m8jZwO1t/IBUh4aGhn0OONRwCcGRbH8eO0z6pbcD/rLHE/PEPDFP7DY+Aj8diF6OEZ/ZAAAAAElFTkSuQmCC" alt="Boost Productivity with Chrome Extensions" data-v-25298ab3="" src="https://www.mohitkhare.com/_nuxt/6162e8bc88982ff3c2db9ce53102c07a-800.png"></p></a></div> <div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/productivity-chrome-extensions" data-v-36f5b510=""><p data-v-36f5b510="">Boost Productivity with Chrome Extensions</p></a></div></div></div><div data-v-36f5b510="" data-v-e9f7aa1a=""><div data-v-36f5b510=""><div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/reading-101" data-v-36f5b510=""><p><img data-src="/_nuxt/9eb30de577280fbbf9bf2f184e8fba97-800.png" data-loading="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAIAAAADnC86AAAACXBIWXMAAAsSAAALEgHS3X78AAAEnklEQVRYw2NQJwJoaGioqKgYGBi8evXq//////79+08xYBi1eNTiUYtHLR61mBSL1UBAHYRgNqmpo7BpYrGGhromEGioa6iraaipaoDt0gSywfaBgDpISklJSU9Pn5oW5yaFH10z+ci62Tsa3TdPzlnVm7mmLuT0jOqTS8tWNiXNyLE4syh/SYH1smKLgiC9h0+pZ3FHXfH36zt/Pjz9ZGHYw93997Z1PVxT9v/m7v+3Z96dl763wfz/pe6bU4KfLwmfnmN15+ELuMVfv379TDT4+uXLf1TnMtTlxz/cPuH5rik3eiwvrap5cmz13b1z359Z/f9s591pQQfr9b9syzzUYLO/xaY1wfD+45cQbT9+/DA0NOTl5RURERHGC4AKgMqA6eP79+/IocXgZSydZ8na4CV6fGLUoaU9hxe2XVjZfmp+2erm6EBdbk9NzjAj3iwrjjADTjdj+Wcv38AtNjMzAxoqIyMjJSUlKSkJJyEAwpaWlgYqEBUVNTc3R7c4z9/4Up/nwQUNq2f2zmvK3D2t6OzcnFkFbuUR1jEu+iFutllh7ulWvBuq7Upjne4/fo6s+e/fv7/B4C8YAMUhIj9//vwHBt++ffsKBkAGehz3tDf8un902YS60sTA1qzA3oKgzmyfJQ2xVzb171vY3lWWOrkutzlQ/vai2O6CgLsPnkK0/fr1a+eOnQcPHLh06dKVK1d27dp16tSpLVu2nDhx4vLly3fv3j1y+PDBgweAAYMzcXXWFtzfPWtpX8W66U3HVk04smrSrgWdG6ZUnlw77eGFo6e3LS2NdKryEDnW6dSc5nrv0TOINqAPaqpr2traent7p06dWlVVNW/evKysrEkTJ65bt+7AgQP9/f09PT1fgGkKRy5gKMlOfHz99I1jW/ZtWLRgWs/8KV2bl82+fHjzrWMbH53c8P/9vYV1sdE6DO7qnHYGSi9evUYLaiD48+cPMHgh4j9//QJygWxgaEMYOH3s7Wyxc/PKFZv3lNS0FFXUFpZVlpZXNtTXzZkxae+m5TfP7r+yoX+SN2uoIY+dvhrcYiB48+bNx48fgX56BwZAm4ABC0xBnz59ArJfv34NjFqg+LOnT58/f44Z5gwtFbmfr2y7c/Xc60e3vz+5+uPp1Z9Pr/x8cvHr/TNf7526tGf5zAj1+V4Msea8trqqL2EWAz164fz5M6dP37xx4/atW+fOnj1//vzt27dfvHhx/Pjx+/fvQ2IayLhw4QKQDXQiWpgzNNeU/X937+nlI/du3Xj/8NLHm4ffXj/66vbxN7f2v7+99+r2abOi1FttGNItWd2NlJ6/fI2qH2EQRBDkpj9/IGxIOscZ1K21pf/f3vl45/SHO2dfXT/x+e6pz/fOfLp3+uv9ky+v7F/Snr26yrPJnfdsiUF5tOWDJy+Q7UB2BKnlKIOPp1tNaX5pflZZfmZxTjqIUQBiA1FJboa3o5Wvg0mkg0ahn5m3k+mz5y+pVlYrKipJSklLScsAkbSMLIQB48ooKqsoKqsqqGhIyClp6+hTt1rU0MQNINUikFJXUwWWty9fjjZ9Ri0etXjU4lGL6W8xAIU/JG106R/JAAAAAElFTkSuQmCC" alt="Improving Reading 101" data-v-25298ab3="" src="https://www.mohitkhare.com/_nuxt/9eb30de577280fbbf9bf2f184e8fba97-800.png"></p></a></div> <div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/reading-101" data-v-36f5b510=""><p data-v-36f5b510="">Improving Reading 101</p></a></div></div></div><div data-v-36f5b510="" data-v-e9f7aa1a=""><div data-v-36f5b510=""><div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/transactions-postgres-golang" data-v-36f5b510=""><p><img data-src="/_nuxt/6431c37e5906bbb06dc74faf07dfbcfd-800.png" data-loading="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAIAAAADnC86AAAACXBIWXMAAAsSAAALEgHS3X78AAAE8klEQVRYw+1W6U/bZRxv4F8giEAMJBhgkAnJwjLfmMzEbS9MFuWFJm4sgEOQTWRcy4CBAoVSekEvrq4ctqWFFhgttJylXHITbgQ55CwDFghYQPBDf66rBZnHErOknzTf3/P7Pk/7+d5PSe5/Ax4eHm5ubj4+Pmtra8fHx0dHR8f/GSQrsZXYSmwlfvOI8aMXXgUvLy8c8/X1XV1dfW3Erq6ubxvh4OAA+ZYRhMbJycnZ2RkLR0dHKD09PfV6vYkY8rd/AkvigIAAkUgkFBaJxeLS0pIyiUQslpQUF+PD5/F5XE5p6Q/FJSeIiIiYm5t7bR6zWCw8nunXdLrWZm1rW1t7b3dXQ1OzVqttam4pq1Q2NzWu609SW1xcPDk5icXh4SFkZ2dnUFBQWFhY6LnAgeDg4JSUFAunSTQafd+wl5RO/ySOc/lLSjRTnMCWXAmh3ojMThaobqWWXotgJaRlra2uiMTiiYkJfGd/fx8yPz+fRCLZ2tqSzoWNjQ0kSsRgMJhHi8ThcAd6u28/5nsGsfxCc77hPPUOZngH0n3uMr0DaZHsqg8eFNyMYTc2aBQKxdjYGL5D2L6+vt5pRFtbW0dHBxaQWLe3t2PR1dVlUgKDg4OWHnO5XJ1O5/+Q6x3E+Cg6L0feFpervBFbcDGYdjWSz6rQRfGVHz9g6bTNcrncnHhjY2NkZGRpaWlvb29lZWVxcRHNtru7iwKEcnx8HK9Qwj5UhnlV/kHM5nCG+nsDk/Pc79Cux+QnPVFHcqsfFdS+81laNO9pSpHmHrvmi1jq0vxMmVRGhNrkcXZ2NplMzsjISE1NjYuLY7PZFAoFKZAZwePxaDRaenp6+L1woVBoScxksXaeb4Qm0i/ezb4WnfcVvdwnmB7Dr3n3FuXz70vDmfKo3NqEdDqOikRiorgI4s3NzYaGBrVa3d3drao9QX19vVKlQkbqNRqEuq6urrKqSqNWz8zMIDyWVc1gMPDIYPKu3Od9GMn7lltzPU4A+gt3qJdCmJSy1gKZUiB4gjNFRUXmVY2wM5lMKpUqlUorKysLCwtz+XxoYA3W+Nn4+HiqEfPzC6ebkEShZOJRIZN++pDnG8ZP5pVJlC0hDMV7QTT32xkx+Wp5jYZKow8MDJSXlyNzJo9nZ2fhYktLi0qlgtPT09OQRHFNTU3By76+PlRPY2Mjts4gzsw8If555qfYFFoYWUDOzMrIkzBlWr9QllcgLYGvSM0RJqZSJicnkDaCmPAYNFFRUfAMsUWaq6urcQBTAU6j45FppP9xUhIkjDOZ+5IYoSDMGRwckMukFYpKgbDoUSb/cjjX72tOxHeM4f4f9cYBguFFhNpkOywgxiGhMbzAnhEHBwfYwuvpefmSGIdMqt3t52RuyaUwzvsReYmZ7M1nekJvQbyzs4PC3tra2t7eXl5exit6CTToHEhUE7Yg0VpotjNmNRFqHAU3JJzYN/wqFEmv3mfejGaVycoPjXqiuJA883bq6ekZGhpCmw4PD2OBKQFlf3//6OgocozUQkKPVyI7Z8xqk8eEN78szNeqNRg78OMkpMbd06E+H0dG/OUl4e/vjzbHBCC/QFpaWlZWFlJAp9MxENKMwCuuhIWFBfNr0QLmpr/SOJKLiwvuWoc/g7iV7e3tiS1cyXZ2dhj0pyffv78Wcb3/P/9ArP8yrcRWYivxG0f8O49FUp+RTJP8AAAAAElFTkSuQmCC" alt="Transactions on Postgres with Golang" data-v-25298ab3="" src="https://www.mohitkhare.com/_nuxt/6431c37e5906bbb06dc74faf07dfbcfd-800.png"></p></a></div> <div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/transactions-postgres-golang" data-v-36f5b510=""><p data-v-36f5b510="">Transactions on Postgres with Golang</p></a></div></div></div><div data-v-36f5b510="" data-v-e9f7aa1a=""><div data-v-36f5b510=""><div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/productivity-in-vscode" data-v-36f5b510=""><p><img data-src="/_nuxt/c00f15909b09287ab6dcd91cb9702fc0-800.png" data-loading="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAIAAAADnC86AAAACXBIWXMAAAsSAAALEgHS3X78AAAEiUlEQVRYw+1WSUwbZxSeQntp1KoHxBJUBCpKIeQAUqscorRVlaKqt1YcKvWAWhSpBzZFIKoACkLBZjNpIRKrTezYgCEoFAqBIlybTQaMHfadmMVgA8YYXC94mX7MUMcMiNI2l6r+NHrzzz8z//e+997/Zoj3L4CIiIjw8PDo6OitrS2SJF0uF/mvQXiJvcReYi/xf48Yi179K0RFReGxmJgYnU7nJoZ1XBhOp5NJHBYWFhQUdPksYD6QuhUcHOzv7x8ZGbm9vf3KFMfHxzc0NHC53NqT4PJ4IgFfLOQ/qq2t4fJEdXWpqakrKyt4BwpgFQpFcnJyenp62rnAAykpKRwOhyGaKCsrw8lut3vO0orW9x1jBqebSSgUzs/PY3B4eAhbXV1NEISvry9xLnx8fGBRKDabzTNaRFFREU5Wq81uP87HIeWEQr17q/DXm5UzqTJL56IJMwKBYH5hAQPad61W29PTI5VKe3t7ZTKZlIJEIsEYFuPfKEgoDA0NMRUXFxe7FcMbh/PII+nc1o0CybXsXz7lzsX+bIltMhhIUlwnnKMU015bLJbNzU3U+eLiIlKgo2A0GtVqNSzmURCwGK+uruJhZo5p4nK5tmJ420Gt2PJcE53b9V5mxwe5HXHi1din5s+fGHROsrH+mJiO/OjoaE5OTmJiIlYoLCxksVjl5eWIP9I5Ozvb2dnJ4/GQR8xnZGSoVCp3qI6JSzgcnD6qnL50T5n2TFMhWwy/2xqY3nqzSDLyYueB0vah0Hir8QQx/f7S0lJbW1tXV9fIyEh/f39fX9/k5CQI6GjPzMzAs46OjpaWFqVSabVamYqLKMUv9OYvRcv+7IkQliLkh/bvBUOavaPgsIYtMULjJ2KD1kk2nVQMPkgsKCioqqrKy8srLS1FuUDlwMCAWCyuqakRiUTNzc1yuRx+MOS+DDXpdNxpVL2T2ReYP3mZ/fzhoI6+nSu3XBMYb9QbNj2I6SWQS0hB+bS3t8NOTEwgwt3d3dC9vLyMW1AMJ2DHxsZO736CQ4X6m+pB4rv66/c7vxIuvJmjejtX9W2z2mp35its4bXG6yLDhoOpeHh4OCkpqaSkBMGEbiQV9Gw2u6mpCdXO5/MrKipQBFlZWRifoZjO8Rc/ST97IFvV/+5wkbefqt/IHvXNVn5cOf11q/7Ko/2Yx7uexO6WaTab7X8C61ooWCkcHByYTCYzBUaTOBHqA7PNbHPQOwpH2rP11zKVr9+Vv/vj0tXHppBK/doh+eQk8Wn8rVb6ch+7aCAm1Ov3pVoiffCt/IUrfFNyzz5m6kTHnevV9Gq6c6Gfub8k9qPjKB8PBzS32zZUW7i20Z1rwaNzoTOggej1+r29vbW1tX0KCOz6+jqCvLOzo6egpnDGdsI2ON2rGWD0app4d3cXpYtiRmOanp4eHx9H9YIJg6mpKY1Gg3a2sbGBMeocDjGJ4+LiUJnYiOxTYLHcBwsZSUhIgLKLh/r8x4jQ0NCAgIDAc4EPs5+fH/4FGN9j1ym4Jz3pz/SAwOf9n/2BeH/2vMReYi/x/474D5XblxQw5EkNAAAAAElFTkSuQmCC" alt="Improve your productivity with VS Code" data-v-25298ab3="" src="https://www.mohitkhare.com/_nuxt/c00f15909b09287ab6dcd91cb9702fc0-800.png"></p></a></div> <div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/productivity-in-vscode" data-v-36f5b510=""><p data-v-36f5b510="">Improve your productivity with VS Code</p></a></div></div></div></div></div></div> <div data-v-373a3b57=""><div data-v-76d4e02f="" data-v-373a3b57=""><p data-v-76d4e02f="">
    Liked the content? <br data-v-76d4e02f="">
    Do support :)
  </p> <div data-v-76d4e02f=""><p><a href="https://www.paypal.me/mkfeuhrer" aria-label="Paypal - Mohit Khare" data-v-76d4e02f=""><img src="https://www.mohitkhare.com/_nuxt/9922499ba185bcccc368872c4cf7b0ea-320.png" alt="Paypal - Mohit Khare" width="125px" data-v-76d4e02f=""></a></p> <p><a target="_blank" href="https://www.buymeacoffee.com/chHAzigTb" aria-label="Buy me a coffee - Mohit Khare" data-v-76d4e02f=""><img src="https://www.mohitkhare.com/_nuxt/img/bmc.87873ba.svg" width="175px" alt="Buy me a coffee" data-v-76d4e02f=""></a></p></div></div></div></div></div></div>]]>
            </description>
            <link>https://www.mohitkhare.com/blog/postgres-constraints</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031762</guid>
            <pubDate>Mon, 09 Nov 2020 05:35:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Nix(OS) Thoughts]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25031754">thread link</a>) | @one-punch
<br/>
November 8, 2020 | https://blog.knightsofthelambdacalcul.us/posts/2020-06-20-nix-nixos-thoughts/ | <a href="https://web.archive.org/web/*/https://blog.knightsofthelambdacalcul.us/posts/2020-06-20-nix-nixos-thoughts/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<p>This post is relatively scatterbrained, and if you’re familiar with Nix, there’s
not any explicitly new ground to tread here. However, I have enjoyed my
experience with NixOS so much that I felt compelled to write this post, although
there’s already a plethora of posts drilling the same points.</p>
<p>From time to time, I find software that immediately seems to <strong>click</strong> with me,
and I start integrating it almost irreversibly into my workflow, to the point
where it’s difficult to think outside of its scope. Emacs is one of these: when
I began using it, I started integrating most of my software into Emacs, be it
IRC or RSS.</p>
<p>My recent experience with NixOS, though not my first (more on that later), was
like this. As of today, my two laptops and my server all run NixOS, and they all
use the same configuration – just with different things enabled/disabled across
different machines. From both the perspective of a system administrator and the
perspective as someone with a <em>meticulous</em> set of dotfiles, this is one of the
best decisions I’ve ever made.</p>
<h2 id="the-bad-parts">The bad parts</h2>
<p>Figured I’d get this out of the way first, as doing Nix-everything hasn’t been
<em>entirely</em> painless. Namely, some of the things I find notable are:</p>
<ul>
<li>Doing things “the normal way” is either ill-advised or impossible – in
general, a lot of system-level things can <em>only</em> be accomplished declaratively
(through Nix) via mutability</li>
<li>Running binaries from the internet is a pain because they don’t know where to
find shared libraries – a workaround for this is, oddly enough, using the
Steam runtime (packaged as <code>steam-run</code>)</li>
<li>The Nix language is syntactically very ugly and has a very distinct learning
curve – this was lessened by personal Haskell knowledge, which helped me
understand its overall paradigm</li>
<li>The documentation oftentimes is bad or nonexistent – you will often find
yourself reading packages’ Nix expressions to understand exactly how they work</li>
</ul>
<p>The most notable of these is the lack of documentation. Much of my configuration
would be extremely difficult had I not just loaded up <code>nix repl</code> and played
around with things in <code>builtins</code> and <code>pkgs.lib</code>. The Nix language itself is
extremely obtuse if you have no FP knowledge, and while efforts such as
<a href="https://nixos.org/nixos/nix-pills/">Nix Pills</a> have <em>helped</em>, it’s not even close to enough, in my opinion.</p>
<h2 id="first-impressions">First impressions</h2>
<p>My first experience with NixOS was in early 2017, as I was doing Haskell
development at the time and had heard Nix was a good build system as an
alternative or complement to Cabal, which is a <strong>garbage fire</strong> of a tool. At this
time, <code>home-manager</code> did not exist, and most of my dots were managed
traditionally via GNU Stow or just not at all. I did things as I always had –
installing packages with the package manager on the command-line, and stowing
my dotfiles – all of which was done in an imperative fashion.</p>
<p>NixOS is terrible at being a “traditional Linux” like this. I ended up with a
mess of declarative/imperative work, none of which was reproducible, which is
one of the promises of Nix as a whole – and my system was cluttered with trash.
I hated NixOS for this reason, and didn’t return to it for a long time.</p>
<p>I would later learn that the more you buy into NixOS’s declarative model, the
more utopian it becomes.</p>
<h2 id="second-attempt-and-thoughts-on-home-manager">Second attempt, and thoughts on <code>home-manager</code></h2>
<p>Recently, I got a new laptop – a Lenovo ThinkPad T495. I opted to try NixOS
again on the merit that I saw people talking about a tool caled <code>home-manager</code>,
which after reading up on it, appeared to alleviate my former problems of doing
the majority of things imperatively. Additionally, I was armed with more
knowledge of functional programming as a whole, meaning I was better able to
(ab)use the Nix expression language.</p>
<p><a href="https://github.com/rycee/home-manager"><code>home-manager</code></a> is a tool for managing a user’s environment with Nix – this
means what would be traditionally known as “dotfiles” (even though they aren’t
actually “dots” here) can now be encoded and managed with Nix. It also means I
am able to rollback my dots, which I’ve never explicitly needed to do, but is
nice should I accidentally/intentionally break something.</p>
<p>I set up my new NixOS system with <code>home-manager</code> immediately and avoided using
<code>nix-env -iA</code> (imperative package management) at all costs. With my FP
knowledge, the Nix expression language came very naturally to me – it felt like
an uglier, simpler version of ML (perhaps this feel comes from the <code>let... in</code>
convention). While encoding my configs, I began to find tricks here and there to
add abstraction to my configurations – writing functions, making variables,
even in forms of configuration that formerly didn’t support this kind of work.</p>
<p>As a former fan of programmable window managers like xmonad and dwm, forced off
them by Wayland’s promises of no screen tearing (which it absolutely fulfills),
I realized that with Nix, <em>everything was like xmonad</em>. I could configure things
with the power of nearly a full programming language, FP knowledge in hand. This
was the first thing that really caused me to love Nix.</p>
<p>Additionally, I do CTFs, and for these challenges you often have to have
esoteric software that you’re unlikely to touch again. Nix solves this problem
very well by allowing creation of a temporary environment – just run
<code>nix-shell -p &lt;package&gt;</code> and you’re dropped into an environment with the package
available. This avoids cluttering your system with random trash.</p>
<h2 id="reproducibility">Reproducibility</h2>
<p>As a test, I took my old laptop (which ran Void Linux), and decided to slap my
NixOS configuration onto it. I had to do some modularization such that I
wouldn’t copy system-specific settings (such as partition layout), but after
that, my mind was <em>absolutely blown</em>.</p>
<p>With a proper declarative configuration, I was up and running with all my
software, dotfiles, and all on a brand new system in less than an hour, even
with some software compiled from source. It felt somehow utopian – the promise
that NixOS made of reproducible configurations was made. As someone who puts far
too much work into their dotfiles, this was what I had been looking for all
along – the <em>ultimate</em> dotfile manager.</p>
<h2 id="nixos-on-the-server">NixOS on the server</h2>
<p>Recently, I switched from a Raspberry Pi 4 running Alpine to a PCEngines APU2,
namely because it’s x86, has AES-NI, is quad-core, and is overall <em>faster</em>. I’ve
noticed significant improvements with Nextcloud, namely, after moving to it.</p>
<p>In my opinion, server settings are where NixOS shines the most! When setting up
the server, I was able to merely take my existing configuration, disable the
graphical session in my system-specific settings, and deploy it – instant user
account, instant shell configuration, et cetera. Setting up services was a
breeze as well: for example, here’s the entirety of a configuration to set up
Nextcloud over an nginx reverse proxy with HTTPS (unmodularized, but
modularization is pretty trivial):</p>
<div><pre><code data-lang="nix">{ config<span>,</span> pkgs<span>,</span> lib<span>,</span> <span>.</span><span>.</span><span>.</span> }:
<span>with</span> lib; {
  services<span>.</span>nextcloud <span>=</span> {
    enable <span>=</span> <span>true</span>;
    hostName <span>=</span> <span>"</span><span>c</span><span>l</span><span>o</span><span>u</span><span>d</span><span>.</span><span>q</span><span>t</span><span>p</span><span>2</span><span>t</span><span>.</span><span>c</span><span>l</span><span>u</span><span>b</span><span>"</span>;

    nginx<span>.</span>enable <span>=</span> nginxCfg<span>.</span>enable;
    https <span>=</span> nginxCfg<span>.</span>ssl;
    maxUploadSize <span>=</span> <span>"</span><span>5</span><span>G</span><span>"</span>;

    config <span>=</span> {
      dbtype <span>=</span> <span>"</span><span>p</span><span>g</span><span>s</span><span>q</span><span>l</span><span>"</span>;
      dbuser <span>=</span> <span>"</span><span>n</span><span>e</span><span>x</span><span>t</span><span>c</span><span>l</span><span>o</span><span>u</span><span>d</span><span>"</span>;
      dbhost <span>=</span> <span>"</span><span>/</span><span>r</span><span>u</span><span>n</span><span>/</span><span>p</span><span>o</span><span>s</span><span>t</span><span>g</span><span>r</span><span>e</span><span>s</span><span>q</span><span>l</span><span>"</span>;
      dbname <span>=</span> <span>"</span><span>n</span><span>e</span><span>x</span><span>t</span><span>c</span><span>l</span><span>o</span><span>u</span><span>d</span><span>"</span>;
      dbpassFile <span>=</span> <span>"</span><span>/</span><span>e</span><span>t</span><span>c</span><span>/</span><span>n</span><span>e</span><span>x</span><span>t</span><span>c</span><span>l</span><span>o</span><span>u</span><span>d</span><span>-</span><span>d</span><span>b</span><span>-</span><span>p</span><span>a</span><span>s</span><span>s</span><span>"</span>;

      adminuser <span>=</span> <span>"</span><span>h</span><span>a</span><span>z</span><span>e</span><span>l</span><span>"</span>;
      adminpassFile <span>=</span> <span>"</span><span>/</span><span>e</span><span>t</span><span>c</span><span>/</span><span>n</span><span>e</span><span>x</span><span>t</span><span>c</span><span>l</span><span>o</span><span>u</span><span>d</span><span>-</span><span>p</span><span>a</span><span>s</span><span>s</span><span>"</span>;
    };
  };

  services<span>.</span>postgresql <span>=</span> {
    enable <span>=</span> <span>true</span>;
    ensureDatabases <span>=</span> [ <span>"</span><span>n</span><span>e</span><span>x</span><span>t</span><span>c</span><span>l</span><span>o</span><span>u</span><span>d</span><span>"</span> ];
    ensureUsers <span>=</span> [
      { name <span>=</span> <span>"</span><span>n</span><span>e</span><span>x</span><span>t</span><span>c</span><span>l</span><span>o</span><span>u</span><span>d</span><span>"</span>;
        ensurePermissions<span>.</span><span>"</span><span>D</span><span>A</span><span>T</span><span>A</span><span>B</span><span>A</span><span>S</span><span>E</span><span> </span><span>n</span><span>e</span><span>x</span><span>t</span><span>c</span><span>l</span><span>o</span><span>u</span><span>d</span><span>"</span> <span>=</span> <span>"</span><span>A</span><span>L</span><span>L</span><span> </span><span>P</span><span>R</span><span>I</span><span>V</span><span>I</span><span>L</span><span>E</span><span>G</span><span>E</span><span>S</span><span>"</span>;
      }
    ];
  };

  systemd<span>.</span>services<span>.</span><span>"</span><span>n</span><span>e</span><span>x</span><span>t</span><span>c</span><span>l</span><span>o</span><span>u</span><span>d</span><span>-</span><span>s</span><span>e</span><span>t</span><span>u</span><span>p</span><span>"</span> <span>=</span> {
    requires <span>=</span> [ <span>"</span><span>p</span><span>o</span><span>s</span><span>t</span><span>g</span><span>r</span><span>e</span><span>s</span><span>q</span><span>l</span><span>.</span><span>s</span><span>e</span><span>r</span><span>v</span><span>i</span><span>c</span><span>e</span><span>"</span> ];
    after <span>=</span> [ <span>"</span><span>p</span><span>o</span><span>s</span><span>t</span><span>g</span><span>r</span><span>e</span><span>s</span><span>q</span><span>l</span><span>.</span><span>s</span><span>e</span><span>r</span><span>v</span><span>i</span><span>c</span><span>e</span><span>"</span> ];
  };

  services<span>.</span>nginx<span>.</span>virtualHosts<span>.</span><span>"</span><span>c</span><span>l</span><span>o</span><span>u</span><span>d</span><span>.</span><span>q</span><span>t</span><span>p</span><span>2</span><span>t</span><span>.</span><span>c</span><span>l</span><span>u</span><span>b</span><span>"</span> <span>=</span> {
    forceSSL <span>=</span> <span>true</span>;
    enableACME <span>=</span> <span>true</span>;
  };
}
</code></pre></div><p>Merely writing this expression was enough to create a fully functional Nextcloud
instance. I never had to touch the actual Postgres prompt, I never had to touch
<code>occ</code> – just enabling this module immediately got everything up and running. It
even automatically fetched HTTPS certificates for me via LetsEncrypt, and
automatically created the required database.</p>
<p>This approach applies to the majority of services under Nix – even the
derivations I had to write myself (for example, for <a href="https://git.qtp2t.club/hazel/perihelion">my webring manager</a>) were far
easier with Nix than without. Everything was unified under one language!</p>
<p>Furthermore, the fact that my desktop and server run the same dotfiles, just
with different things enabled/disabled, means that I can have an instant
environment akin to my “production” server (if you can call it that).</p>
<h2 id="nix-as-a-build-system">Nix as a build system</h2>
<p>Nix at its fundamental level is just a way to create reproducible builds, and
NixOS is its application at an extreme level. It makes sense, then, that Nix
makes a good system for reproducible builds. While none of the projects I work
on truly <em>need</em> to be reproducible, it’s nice to not have my system cluttered
with garbage – I can have libraries or entire compilers only available in the
context of one project.</p>
<p>Notable tools that complement Nix here are:</p>
<ul>
<li><a href="https://direnv.net/">direnv</a>, which allows to have an environment specific to a directory – this
allows being dropped into a Nix shell without an explicit step</li>
<li><a href="https://github.com/target/lorri">lorri</a>, which is a replacement for <code>nix-shell</code> with tight <code>direnv</code> integration</li>
<li><a href="https://github.com/nmattia/niv">niv</a>, which is useful for pinning the entirety of Nixpkgs to a certain commit</li>
</ul>
<p>My workflow/setup for Nix-based projects is something like this:</p>
<ul>
<li>
<p>Run <code>lorri init</code> to create an environment and <code>direnv allow</code> to use it</p>
</li>
<li>
<p>Run <code>niv init</code> to pin Nixpkgs, and switch the branch to NixOS 20.03</p>
</li>
<li>
<p>Write a <code>shell.nix</code>. For a Racket project, for example, it would look like:</p>
<div><pre><code data-lang="nix">  <span>let</span>
    sources <span>=</span> <span>import</span> <span>./nix/sources.nix</span>;
    pkgs <span>=</span> <span>import</span> sources<span>.</span>nixpkgs {};
  <span>in</span>
  pkgs<span>.</span>mkShell {
    buildInputs <span>=</span> <span>with</span> pkgs; [
      racket
    ];
  }
</code></pre></div><p>This automatically pulls the Racket interpreter, regardless of whether the
target system has Racket installed. This is a simplistic example – more
complex projects would have more complex dependencies. Regardless, with
<code>direnv</code>,</p>
</li>
<li>
<p>Work on the project! As dependencies flow in, add them to <code>shell.ni…</code></p></li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.knightsofthelambdacalcul.us/posts/2020-06-20-nix-nixos-thoughts/">https://blog.knightsofthelambdacalcul.us/posts/2020-06-20-nix-nixos-thoughts/</a></em></p>]]>
            </description>
            <link>https://blog.knightsofthelambdacalcul.us/posts/2020-06-20-nix-nixos-thoughts/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031754</guid>
            <pubDate>Mon, 09 Nov 2020 05:33:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Monitoring a grain dryer remotely]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25031661">thread link</a>) | @vaillancourtmax
<br/>
November 8, 2020 | https://maximevaillancourt.com/blog/monitoring-a-grain-dryer-via-the-internet | <a href="https://web.archive.org/web/*/https://maximevaillancourt.com/blog/monitoring-a-grain-dryer-via-the-internet">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    
    <p>A relative of mine called a few months ago: “Hey, I have a grain dryer that I’d like to monitor from my phone. Is that something you could set up in time for harvest season?”</p>

<p>I have no idea how grain dryers work, especially not Internet-enabled grain dryers — because <em>of course</em> that’s a thing — but I figured I’d give it a shot anyway.</p>

<p>For context, a grain dryer is literally a combination of a huge furnace and fans that, you guessed it, dries harvested grain for optimal (lack of) moisture.</p>

<div>
<p><img alt="A GSI grain dryer next to a grain silo." src="https://d33wubrfki0l68.cloudfront.net/5a21714733e72c8597435c3ac8150c2fdf955de6/5cfa4/assets/dryer/gsi-1220.jpg">
</p>

</div>

<p>Such dryers typically have an embedded computer running some version of Windows and a touchscreen display that allows monitoring and controlling the dryer’s temperature, timers, and other parameters to get the perfect output.</p>

<div>
<p><img alt="A GSI grain dryer's control panel." src="https://d33wubrfki0l68.cloudfront.net/1b66a66084b20383b3d61ebe1da9a9aa038dd8f7/cccbd/assets/dryer/controller.png">
</p>
<p>
  A GSI grain dryer's control panel. Image © <a href="https://www.grainsystems.com/master/products/conditioning/watchdog-technology.html">GSI</a>
</p>
</div>

<p>However, because these things run for hours at a time, and because agriculture workers during harvest season usually end up in a sleep-deprived state that lasts for multiple weeks, having the ability to monitor these grain dryers remotely lets workers get more precious sleep time (instead of having to walk/drive up to the grain dryer to see if everything’s okay in the middle of the night).</p>

<p>The desired outcome is to monitor the grain dryer remotely using the web interface:</p>

<div>
<p><img alt="An iPhone with the grain dryer's web monitoring interface open in the browser." src="https://d33wubrfki0l68.cloudfront.net/2ec442d267d87ac17aa3ace20b6c0a536e74fcc5/89508/assets/dryer/web-ui.jpg">
</p>
<p>
  The web interface to monitor the grain dryer's current state. Image © <a href="https://www.grainsystems.com/content/dam/Brands/GSI/Brochures/Conditioning/gs013_Modular-TSeries-Dryers-2017_7.pdf/_jcr_content/renditions/original">GSI</a>
</p>
</div>

<p>It’s now harvest season 2020, so I connected a grain dryer to the Internet yesterday. It was fun (and weird) and I learned a few things. Here’s how we did it.</p>

<h3 id="what-youll-need">What you’ll need</h3>

<ul>
  <li>A GSI grain dryer with a <a href="https://www.grainsystems.com/master/products/conditioning/watchdog-technology.html">Watchdog module</a> installed (I was lucky that the dryer already had this module installed)</li>
  <li>A cellular modem (we purchased a <a href="https://www.netgear.com/home/products/mobile-broadband/lte-modems/LB1120.aspx">Netgear LB1120</a>)</li>
  <li>A cellular data plan with a public static IP (we went with Telus, a Canadian cellular carrier, and needed to open a business account to get access to a public static IP for 15$/month per month on top of the base data plan)</li>
  <li>A laptop with an Ethernet port</li>
  <li>A few Ethernet cables</li>
</ul>

<h3 id="topology">Topology</h3>

<p>Here’s how the network topology looks like:</p>

<div><div><pre><code>                   (via public static IP)
                        +----------+
         +--------------&gt; Internet &lt;--------------+
         |              +----------+              |
 +-------v--------+                       +-------v-------+
 | Cellular modem |                       | Mobile device |
 +-------^--------+                       +---------------+
         |
+--------v----------+
|    Grain dryer    |
| (Watchdog module) |
+-------------------+
</code></pre></div></div>

<p>Essentially, we’ll expose the grain dryer’s web interface to the Internet via a public static IP using a cellular modem. Once it’s exposed, the operator will be able to navigate to that static IP from their mobile device to access the grain dryer’s web interface.</p>

<h3 id="subscribing-for-a-data-plan-and-a-static-ip">Subscribing for a data plan and a static IP</h3>

<p>I called various cellular carriers here in Canada to learn more about their data-only plans and the possibility of adding a public static IP option to the plan: the only carrier that seemed like they knew what they were talking about was Telus. Others either didn’t offer the public static IP option, or didn’t know what I was talking about.</p>

<p>With that, we went ahead and signed up for a business account at Telus (required to get access to the public static IP option), bought a SIM card, and a data-only plan for the modem.</p>

<h3 id="setting-up-the-cellular-modem">Setting up the cellular modem</h3>

<p>First up, we need to make sure we’re able to connect to the nearest cellular tower to connect to the Internet. The Netgear LB1120 is fairly easy to set up on paper: pop a SIM card in there and we’re done, right? Well, for our use case, not quite. There are a few things to do to set it up to use the static IP assigned to the data plan from the cellular carrier. More importantly, the modem needs to be set up in “bridge” mode (instead of “router” mode) to act as a simple bridge (rather than a router) to connect it directly to the grain dryer.</p>

<div>
<p><img alt="Laptop, modem, cables, and various manuals spread out on concrete." src="https://d33wubrfki0l68.cloudfront.net/d011a2ceee600cafd7f0ee676b393623f84bbef4/f13bd/assets/dryer/setup-modem.jpg">
</p>
<p>
  On-premise setup to configure the modem and the dryer. That's my trusty ThinkPad X220.
</p>
</div>

<p>Days before arriving on the premises, <a href="https://community.netgear.com/t5/Mobile-Routers-Hotspots-Modems/LB1120-Bridge-Mode-No-Connectivity/m-p/1404666#M3431">I read online that the firmware that ships out of the box is broken</a>: “bridge” mode doesn’t function at all, and an update is required to fix it. This made me a bit nervous because I didn’t have the modem with me, and wasn’t sure if I was going to be able to get the update to work properly. Thankfully, upon inserting the SIM card, turning the modem on, plugging it into the computer via an Ethernet cable, and accessing the modem’s setup interface at <code>http://192.168.5.1</code>, the modem’s web UI suggests downloading the latest firmware over the air. Neat!</p>

<p>From what I read, you must use a firmware more recent than <code>NTG9X07C_12.09.05.27</code> to make sure “bridge” mode works fine. In our case, we updated to <code>NTG9X07C_12.09.05.30</code>, so we’re good.</p>

<div>
<p><img alt="Updating the modem's firmware over the air." src="https://d33wubrfki0l68.cloudfront.net/7d6dd2d298b3cf861f6144dc01dc774b85ac9ff3/a036b/assets/dryer/firmware-update.jpg">
</p>
<p>
  Updating the modem's firmware over the air.
</p>
</div>

<p>Once the update completes and the modem reboots, we can go in the settings and change the operation mode to “bridge”:</p>

<div>
<p><img alt="Changing the modem's operation mode to Bridge mode." src="https://d33wubrfki0l68.cloudfront.net/34c50316b7c48bdcf08962a436d697d2ff1b9c27/a56a3/assets/dryer/bridge-mode.png">
</p>
<p>
  Changing the modem's operation mode to Bridge mode. Image © <a href="https://kb.netgear.com/31163/How-to-change-4G-LTE-Modem-from-router-mode-to-bridge-mode">Netgear</a>
</p>
</div>

<p>Doing so “turns off the router function of the device and assigns the network IP address directly to the attached host”, which is exactly what we need to connect the modem directly to the dryer’s Watchdog module. Save and let the modem reboot.</p>

<p>Now, the SIM is in the modem, which has the latest firmware and is in “bridge” mode, but there’s one last thing to fix: the cellular carrier is assigning a dynamic IP to the modem, which isn’t what we want: we should be getting the static IP for the data plan we’re paying for. To resolve this issue, we need to tweak the modem’s access point name (APN).</p>

<p>I got stuck on this issue for around 30 minutes before remembering that APNs are a thing and that I could possibly tweak it in the modem settings. I’m glad I remembered!</p>

<p>By default, the modem auto-detects the APN from the cellular carrier. For most cases, this works fine, but in this particular case, we want to use a different APN that allows us to use the static IP assigned to our data plan. APN settings vary by cellular carrier, and I ended up searching for Telus’ APN settings while on premise. I found <a href="https://usatcorp.com/faqs/common-access-point-names-apn-carrier/">USAT Corp’s website to include various APN settings</a>, so I highly recommend trying those values out for your particular cellular carrier.</p>

<p>In our specific case, we needed to use the following settings to be able to get the static IP assigned to the modem:</p>

<ul>
  <li>Access point name (APN): <code>staticipeast.telus.com</code> (we’re on the east coast)</li>
  <li>Authentication: None</li>
  <li>Packet data profile (PDP): IPv4</li>
</ul>

<div>
<p><img alt="Configuring the APN to use the static IP assigned to the account." src="https://d33wubrfki0l68.cloudfront.net/a9161e0b08165d260829bf6fd22337043b63bffa/40217/assets/dryer/apn-settings.png">
</p>
<p>
  In the modem settings, we need to configure a new APN to ask the cellular carrier to assign the account's static IP to the modem.
</p>
</div>

<p>The LB1120 has a setting to automatically connect to the Internet upon booting: I recommend turning this on to have it connect automatically in the case of a power failure.</p>

<p>Let’s reboot the modem one more time. At this point, the modem should be using the static IP assigned to your data plan by the cellular carrier.</p>

<div>
<p><img alt="Signal is equivalent to 2 bars out of 5." src="https://d33wubrfki0l68.cloudfront.net/5c248b8917f0fe46445757c0dee3636422b25f1b/b2044/assets/dryer/2-bars.jpg">
</p>
<p>
  The Netgear LB1120 cellular modem, all set up and ready to go. Even though it sits at 2 bars out of 5, it's plenty for the low throughput use case of reading data from a dryer.
</p>
</div>

<p>A tip to know if the modem is configured correctly: with the computer (device A) connected to the modem via Ethernet, use another device (device B) to access the expected static IP address (I used my smartphone). On device A, serve something (anything) on port 80 (I did <code>sudo python -m SimpleHTTPServer 80</code>), and see if you get that on device B when visiting the static IP address. If you do, then the modem is all set up and ready to go. If not, something’s wrong (invalid APN, incorrect data plan, no public static IP configured by the cellular carrier, some sort of NAT that prevents direct access, etc.).</p>

<p>Now that the modem is configured properly, let’s move on to configuring the grain dryer itself.</p>

<h3 id="configuring-the-dryer">Configuring the dryer</h3>

<p>The process to configure the dryer is fairly similar to configuring the modem: we essentially connect the dryer to the laptop via Ethernet to configure it via a web interface. But first, let’s look inside the dryer’s computer compartment.</p>

<p>It all peels up like an onion: the outermost layer is the protective door to prevent water and debris from hitting switches and the touchscreen, and the middle layer is the computer and touchscreen itself along with a PLC. The Watchdog module is fixed inside the box itself.</p>

<div>
<p><img alt="The dryer's computer and PLC internals exposed." src="https://d33wubrfki0l68.cloudfront.net/a74f45dc1610d35fecb4981dae16685049b56b5c/11a3d/assets/dryer/onion.jpg">
</p>
<p>
  The dryer's PLC. The Watchdog module is not pictured (it's fixed on the right).
</p>
</div>

<p>When looking inside the box, the Watchdog module is fixed inside and connected to the rest of the PLC via an RS232 serial port. There are two Ethernet ports: one WAN port (the one on the left in the picture), and a LAN port (the one on the right).</p>

<div>
<p><img alt="The Watchdog module fixed inside the computer compartment of the dryer." src="https://d33wubrfki0l68.cloudfront.net/e3234c6b71810bcce6c0ab3b7e561fee69d4a57e/1e608/assets/dryer/watchdog-module.jpg">
</p>
<p>
  The Watchdog module. The WAN port is directly connected to the modem.
</p>
</div>

<p>To configure the dryer’s network settings, we need to connect the Watchdog’s LAN port to the computer via Ethernet and access the web interface at <code>http://10.0.0.1/setup</code>. On this screen, you’ll need to click “Configure”, which will bring up the possibility to use DHCP or a static IP address. In our case, we select the “static IP” option.</p>

<div>
<p><img alt="The Watchdog network configuration screen, to configure the static IP address used to connect to the dryer." src="https://d33wubrfki0l68.cloudfront.net/2ceb582c1539a2d312fde85322f070fa42c21e38/ea792/assets/dryer/network-setup.png">
</p>
<p>
  Configuring the network settings on the grain dryer.
</p>
</div>

<p>On the next screen, we fill out the following fields:</p>

<ul>
  <li>Static IP address</li>
  <li>Subnet mask</li>
  <li>Gateway</li>
  <li>DNS1</li>
  <li>DNS2</li>
</ul>

<p>Assuming that the static IP address provided by the cellular carrier is <code>241.2.31.59</code>, we’d fill out the fields as such:</p>

<ul>
  <li>Static IP address: <code>241.2.31.59</code> (static IP from the cellular carrier)</li>
  <li>Subnet mask: <code>255.255.255.0</code> (pretty standard stuff)</li>
  <li>Gateway: <code>241.2.31.1</code> (same as static IP, but last component is <code>1</code>)</li>
  <li>DNS1: <code>241.2.31.1</code> (we use the gateway as the DNS provider)</li>
  <li>DNS2: <code>8.8.8.8</code> (this is Google DNS, because why not)</li>
</ul>

<p>Once that’s done, save the network configuration, and turn off the grain dryer.</p>

<p>You may now connect the modem to the Watchdog module’s WAN port (as pictured a few paragraphs above).</p>

<p>Now, the moment of truth: turn on the dryer. You may …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://maximevaillancourt.com/blog/monitoring-a-grain-dryer-via-the-internet">https://maximevaillancourt.com/blog/monitoring-a-grain-dryer-via-the-internet</a></em></p>]]>
            </description>
            <link>https://maximevaillancourt.com/blog/monitoring-a-grain-dryer-via-the-internet</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031661</guid>
            <pubDate>Mon, 09 Nov 2020 05:12:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[4 Billion USD ICO From 2017: Clues Emerging, Finally]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25031582">thread link</a>) | @npguy
<br/>
November 8, 2020 | https://doublespend.io/2020/10/28/breaking-object-floating-in-atlantic-could-be-carrying-funds-raised-in-eos-ico/ | <a href="https://web.archive.org/web/*/https://doublespend.io/2020/10/28/breaking-object-floating-in-atlantic-could-be-carrying-funds-raised-in-eos-ico/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
<p>Travelers aboard a luxury TransAtlantic cruise have shared pictures that many in the crypto community believe could provide clues on the missing 4 billion dollars raised during the EOS ICO. </p>



<p>DoubleSpend’s own analysis based on the size of the box show that it could very well contain the amount in USDs that was raised in the year-long ICO.</p>
<div><p><a href="https://twitter.com/share?url=https://doublespend.io/2020/10/28/breaking-object-floating-in-atlantic-could-be-carrying-funds-raised-in-eos-ico/&amp;text=Object%20Floating%20In%20Atlantic%20Ocean%20Could%20Contain%20EOS%E2%80%99%20ICO%20Funds%3A%20Sources" title="Share on Twitter" target="_blank" rel="nofollow noopener noreferrer" data-postid="409" data-social-network="Twitter" data-social-action="Tweet" data-social-target="https://doublespend.io/2020/10/28/breaking-object-floating-in-atlantic-could-be-carrying-funds-raised-in-eos-ico/"><span><span><svg version="1.1" xmlns="http://www.w3.org/2000/svg" width="29.71875" height="32" viewBox="0 0 951 1024"><path d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"></path></svg></span><span>Tweet</span></span><span>0</span></a></p></div>		</div></div>]]>
            </description>
            <link>https://doublespend.io/2020/10/28/breaking-object-floating-in-atlantic-could-be-carrying-funds-raised-in-eos-ico/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031582</guid>
            <pubDate>Mon, 09 Nov 2020 04:55:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Strategies to working remotely and smashing goals]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25031490">thread link</a>) | @veebuv
<br/>
November 8, 2020 | https://www.remoteworkly.co/blog/22-tips-to-working-remotely-and-smash-your-goals | <a href="https://web.archive.org/web/*/https://www.remoteworkly.co/blog/22-tips-to-working-remotely-and-smash-your-goals">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-w-id="45d72c63-4641-3f9f-f271-8df897413c12"><p>Alright the future of work is here to stay and we all need to prepare for the best remote work practices. Regardless of what happens, work has changed forever, people will go back to the office surely, but the dynamic will now be a hybrid mode.<br>‍</p><p>So learning how to work remotely will be a skill in your arsenal that might just propel you to your next career promotion. If you've asked yourself the question "how to work from home" - you've reached the right place.<br>‍</p><h4>1) Create a routine</h4><p>If you want to succeed working from home or working remotely this is a must, which is why it's the top position. You need to build a system that builds a body clock. I used to get into the zone when I got onto the train and grabbed by morning coffee at my office, when I couldn't do this anymore I spiraled. This was until I started building an atomic habit again, micro habits that signal the mind for whats about to happen next. Nowadays my trigger is early morning juice, similarly so, build a routine. Get out of bed, go to the gym, have a process.<br>‍</p><h4>2) Have your creative work space</h4><p>Separate your living, from your office space. Even if it means working from a coffee shop. You need to keep your area of comfort for leisure and work for work. This is an important work tip as separation of concerns as well as "environments" make a significant difference to motivation. Pretty much the same reason people go to libraries when they can read at home</p><p>‍</p><h4>3) Set time boundaries</h4><p>Its very easy to be sucked into 24 hour work when working from home. One of the keys to working remotely is creating time boundaries. Employers may feel like you're available to message after work hours. In addition you might get FOMO from all the slack notifications. Fight the urge, set your boundaries. 80% of 130 people I spoke to said they were constantly fatigued from working from home. This is a very important tip amongst other work from home tips and tricks. Stay disciplined to time<br>‍</p><h4>4) Leave home</h4><p>How to be productive working from home ? Leave your home. Ironic - I know but cabin fever can creep into you. As humans we need social interaction and movement. Step away from your home, go for a walk, meet a friend or better yet exercise. This will stimulate both your body muscles as well as your brain. Feeling fresh air graze your face makes a big difference, especially when you're glued to your screen<br>‍</p><h4>5) Plan meetings with work colleagues</h4><p>We've been remote for nearly 1.5 years now and I always advice social meetings for working remotely. Catch up with any team-mate in the same city as you and make it a monthly if not weekly thing. Building bonds with people in real life not only helps you create long lasting relationships, but also helps you connect deeper with another person. 70% of communication is non - verbal, which you miss out over zoom calls.<br>‍</p><h4>6) Reduce distractions</h4><p>"Minimise the number of distractions you have in your office" - simple yet sage advice for working remotely. We've got a concentration span of 12 seconds, with slack messages going off every moment combined with our innate nature of not wanting to miss out on important conversations leads us to doing absolutely no work at all. I encourage you to embrace an async first work culture, tools like <a href="http://remoteworkly.co/">remoteworkly.co</a> help with async video meetings or even loom for screen recording.<br>‍</p><h4>7) Have no meeting Wednesdays</h4><p>Meets really hurt productivity. 72% of managers say meetings are a complete waste and 60% of employees think they will work better without meetings. This is the same reason Zoom fatigue is becoming a big issue, because you need to have your camera on and focus the entire time. Try encourage a culture where you have certain days with no meetings. Any conversation that needs to happen can occur in an asynchronous way, weather thats via tools like <a href="http://remoteworkly.co/">remoteworkly.co</a> or vidyard. If you're reporting bugs, use feedback tools like bugheard or <a href="http://remoteworkly.co/show">remoteworkly.co/show</a></p><h4><br>8)Avoid unwanted meetings with conversation bloat</h4><p>Focus on getting work done, try set a decorum where you can leave meetings you're not needed in anymore. Teams that embrace the concept of async communication will win. This will reduce anxiety within teams, improve culture and make sure the meetings that do happen are purely focused on value and outcome. This can be done using tools like <a href="http://remoteworkly.co/">remoteworkly.co</a> for async meetings and startups or even voice notes like recordify</p><h4><br>9)Start with the toughest task</h4><p>There's a science behind this but it works. We often think starting with easy tasks gives us the feeling of accomplishment and builds up momentum. There is some truth to this, however by the time you reach the tough/long task to complete you're left drained of energy and delay it to tomorrow, which never comes. Start with the toughest tasks, its the best way to be productive working from home<br>‍</p><h4>10)Over communicate</h4><p>There will be obvious disconnect between you and your team mates when you're remote. That is the nature of VoIP communication. So make sure you overcommunicate. This does not mean to spam people with 100 messages, but rather use video and tools that capture as much data as possible for you to make it easier for the other person to understand what you're doing without having to reach back to you."</p><h4><br>11)Leverage asynchronous communication</h4><p>My favorite tip in my list of remote work best practices. I strongly believe async communication is the way of the future. Leverage using async communication whenever and wherever possible without organizing impromptu/time blocking calls. There's several tools out there that let you take full advantage of async communication, <a href="http://remoteworkly.co/">remoteworkly</a>, <a href="http://loom.com/">loom</a>, <a href="http://vidyard.com/">vidyard</a>, <a href="http://marker.io/">marker</a> (for website feedback), <a href="http://remoteworkly.co/show">remoteworkly</a> (for QA feedback), <a href="http://trello.com/">trello</a>, <a href="http://asana.com/">asana<br>‍</a></p><h4>12)Use productivity hacks like pomodoro</h4><p>The pomodoro technique is one of the best productivity tips I came across. Its an old technique that lets you cut down your work into an investment reward balance. This builds another atomic habit loop where you learn to enjoy difficult tasks in anticipation of the reward. Your day is broken down into 25 minute work chunks of pure focus with 5 minute breaks, do this 4 times and then take a 20 minute break, rinse and repeat. Checkout timechi.com</p><h4><br>13)Share your project progress</h4><p>The easiest way to start working when you're feeling down or demotivated is by sharing your progress. Social accountability plays a big role in human motivation, something about putting our reputation at risk that kicks us in the behind. In addition, sharing your progress becomes a incredible feedback cycle, where you encourage others with your progress or cause them to reach out to you and motivate you to push faster. Lastly, having progress updates is a great way to look back and see how far you come, consequentially motivating you to work better</p><h4><br>14)Avoid jumping on impromptu phonecalls</h4><p>This rolls back to my earlier point about async conversations, Paul Graham mentioned that one of the downfalls of remote work is impromptu meetings where a large number of incredible ideas are usually generated is being robbed from todays workforce. Yes this is true, but on the flip side, most of the inefficiencies in working from an office came from these "tap on the shoulder" interruptions. I no longer pick up calls unless the message following up says "this is urgent", your concentration is sacred in 2020. Protect it at all costs.</p><h4><br>15)Set clear expectations for each day</h4><p>Employing a daily manifest of long term goals, short term goals, micro tasks as well as schedule has been a game changer in working remote. You almost get to "grade" each day in terms of the success you wanted to achieve and what you did achieve. This lets you work out what is the most effective work from home schedule for you. Each week, analyze the good days and bad days, pick up patterns that lead to bad days and those that lead to good days and optimize to focus on the good patterns.</p><h4><br>16Share with video whenever you can</h4><p>To prevent the feeling of isolation, use video software whenever you can. <a href="http://loom.com/">Loom</a> is a great place to begin, or even zoom for work meetings. I know it can be daunting at times, but it provides a great path to building deeper and more meaningful relationships with teams. PS - when you're chatting, look into the camera and not your screen. Makes double the impact</p><h4><br>17) Ask for one on one checkins</h4><p>Do NOT cancel your one on ones with your team mates, they're more important than every given there is no face to face relationship with your direct supervisor or subordinate. Use a template with a clear structure to find out how your team is doing, how things can be done better. This builds trust and encourages them to keep motivated even when people feel disconnected.<br>‍</p><h4>18) Bond with your team beyond work, get to learn about their family</h4><p>Use tools like <a href="http://donut.com/">donut</a> to encourage your team members to learn about each other. This is vital for new employees who don't have a chance to meet new people given you're restricted to a computer, donut runs introduction meetings between people. Encouraging that communication between new colleagues is a great way to improve team bond, morale and company culture</p><h4><br>19) Exercise</h4><p>Yes, do it. As human's we're not made to be seat potatoes. Despite how tasty potatoes are, you can't aspire to be one. Get out, hit the gym and exercise. The endorphins release as well as adrenaline rush you get post exercise is fundamental for peak performance. You won't find one successful person who doesn't preach for a daily exercise schedule</p><h4><br>20) Use noise filtering software</h4><p>Working at home with kids or even a dog can be very disturbing and sometimes embarrassing. Even though everyone is working remote, the noise of a grinder in the background whilst you're delivering your Q4 results can be quite distractive. Use tools like <a href="http://krisp.ai/">krisp</a> that magically cut out background noise and give you the confidence that regardless of who's speaking at the back, your colleagues won't be able to hear it.<br>‍</p><h4>21) Check in with 5 of your friends</h4><p>I assure you many of your friends will be …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.remoteworkly.co/blog/22-tips-to-working-remotely-and-smash-your-goals">https://www.remoteworkly.co/blog/22-tips-to-working-remotely-and-smash-your-goals</a></em></p>]]>
            </description>
            <link>https://www.remoteworkly.co/blog/22-tips-to-working-remotely-and-smash-your-goals</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031490</guid>
            <pubDate>Mon, 09 Nov 2020 04:34:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Start with pen and paper]]>
            </title>
            <description>
<![CDATA[
Score 159 | Comments 86 (<a href="https://news.ycombinator.com/item?id=25031483">thread link</a>) | @sethetter
<br/>
November 8, 2020 | https://sethetter.com/posts/start-with-pen-and-paper/ | <a href="https://web.archive.org/web/*/https://sethetter.com/posts/start-with-pen-and-paper/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
            


<article>
    
    <section>
        <p>tl;dr — If you feel unfocused, grab a pen and paper and start writing your thoughts down.</p>
<p>Is there a question you are stuck on? A ambiguous goal you're trying to accomplish? Maybe a task you know you need to do but from which you keep getting distracted?</p>
<p><strong>Nothing will provide focus like pen and paper.</strong></p>
<p>It's all too easy these days to have a clear intention only to be sidetracked by the whirlpool of apps and services clawing for our attention on our devices.</p>
<p>It helps to take the time to groom our notification settings for importance and timeliness, but a digital device that we can use to complete nearly any task will never stand up to pen and paper in terms of it's ability to provide focus.</p>
<p>My thoughts are a constant whirlwind, I'm certainly more distractible than most, and interacting with nearly any online service only fuels that fire. So how can I get anything done if I'm unable to control my focus?</p>
<p><strong>Pen and paper.</strong> Whenever I catch myself stuck in the whirlpool, feeling not-great because I <em>know</em> I'm not doing what I want to be doing, or what I should be doing, I step away, grab pen and paper, and start writing.</p>
<p>The simple act of writing can focus my thoughts and attention in a way that nothing else can. Free from distractions, just a canvas to pour my thoughts into, and turn them into something with a sense of direction and purpose.</p>
<p>Writing is like a superpower to me. If there's any task I want to accomplish, the first step is always to write it down. Anytime I need to recenter myself on that task, I simply return to paper.</p>
<p>Throughout my life I've found that the simplest solutions are often the most powerful. So far I've found no simpler solution to start tackling any problem than to simply write it down, and then keep on writing.</p>

    </section>
</article>


        </div></div>]]>
            </description>
            <link>https://sethetter.com/posts/start-with-pen-and-paper/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031483</guid>
            <pubDate>Mon, 09 Nov 2020 04:33:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Cool Machine Learning Books]]>
            </title>
            <description>
<![CDATA[
Score 238 | Comments 15 (<a href="https://news.ycombinator.com/item?id=25031455">thread link</a>) | @ridddle
<br/>
November 8, 2020 | http://matpalm.com/blog/cool_machine_learning_books/ | <a href="https://web.archive.org/web/*/http://matpalm.com/blog/cool_machine_learning_books/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    
  <p>awhile ago i posted
   <a href="http://matpalm.com/blog/2010/08/06/my-list-of-cool-machine-learning-books/">my list of cool machine learning books</a>,
   but it's been awhile so it's probably time to update it...
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/mml.jpg"></p>
<p><b><a href="https://mml-book.github.io/">Mathematics for Machine Learning</a>
   by Marc Peter Deisenroth, A. Aldo Faisal &amp; Cheng Soon Ong.</b>
</p>
<p>this is my personal favorite book on the general math required for machine learning,
   the way things are described really resonate with me.
   available as a free pdf but i got a paper copy to support the authors after reading the
   first half.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/laalfd.jpg"></p>
<p><b><a href="http://math.mit.edu/~gs/learningfromdata/">Linear Algebra and Learning from Data</a>
   by Gilbert Strang.</b>
</p>
<p>this is gilbert's most recent work. it's really great, he's such a good teacher, and
   <a href="https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/">his freely available lectures</a>
   are even better. it's a shorter text than his other classic intro below with
   more of a focus on how things are connected to modern machine learning techniques.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/itla.jpg"></p>
<p><b><a href="https://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a>
   by Gilbert Strang.</b>
</p>
<p>this was my favorite linear algebra book for a long time before his 'learning from
   data' came out. this is a larger book with a more comprehensive view of linear algebra.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/ts.jpg"></p>
<p><b><a href="https://greenteapress.com/wp/think-stats-2e/">Think Stats: Probability and Statistics for Programmers</a> by Allen Downey.</b>
</p>
<p>this book focuses on practical computation methods for probability and statistics.
   i got a lot out of working through this one.
   it's all in python and available for free.
   ( exciting update! as part of writing this post i've discovered there's a new edition
   to read!)
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/dbda.jpg"></p>
<p><b><a href="https://sites.google.com/site/doingbayesiandataanalysis/">Doing Bayesian Data Analysis</a>
   by John Kruscgke</b>
</p>
<p>on the bayesian side of things this is the book i've most enjoyed working through.
   i've only got the first edition which was R and
   <a href="https://en.wikipedia.org/wiki/OpenBUGS">BUGS</a> but i see
   the second edition is R,
   <a href="http://mcmc-jags.sourceforge.net/">JAGS</a> and
   <a href="https://mc-stan.org/">Stan</a>.
   it'd be fun i'm sure to work through it doing
   everything in <a href="https://github.com/pyro-ppl/numpyro">numpyro</a>. i might do that in all
   my free time. haha. "free time" hahaha. sob.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/eosl.jpg"></p>
<p><b><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning</a>
   by Hastie, Tibshirani and Friedman</b>
</p>
<p>this is still one of the most amazing fundamental machine learning books i've ever had.
   in fact i've purchased this book <em>twice</em> and given it away both times :/ i might buy another
   copy some time soon, even though it's been freely available to download for ages. an
   amazing piece of work.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/pgm.jpg"></p>
<p><b>
   <a href="https://mitpress.mit.edu/books/probabilistic-graphical-models">Probabilistic Graphical Models</a>
   by Daphne Koller &amp; Nir Friedman</b>
</p>
<p>this is an epic textbook that i'd love to understand better. i've read a couple of sections in
   detail but not the entire tome yet.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/praml.jpg"></p>
<p><b>
   <a href="https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/">Pattern Recognition and Machine Learning</a>
   by Christopher Bishop</b>
</p>
<p>this is probably the best overall machine learning text book i've ever read. such a beautiful book
   and <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">the pdf is FREE FOR DOWNLOAD!!!</a>
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/mlapp.jpg"></p>
<p><b><a href="https://mitpress.mit.edu/books/machine-learning-1">Machine Learning: A Probabilistic Perspective</a> by Kevin Murphy</b>
</p>
<p>this is my second favorite general theory text on machine learning.
   i got kevin to sign my copy when he was passing my desk once but
   someone borrowed it and never gave it back :(
   so if you see a copy with my name on the spine let me know!
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/homl.jpg"></p>
<p><b><a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</a> by Aurélien Géron</b>
</p>
<p>this is the book i point most people to when they are interested in getting up
   to speed with modern applied machine learning without too much concern for the
   theory. it's very up to date (as much as a book can be) with the latest libraries
   and, most importantly, provides a good overview of not just neural stuff but fundamental
   <a href="https://scikit-learn.org/stable/">scikit-learn</a> as well.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/mle.jpg"></p>
<p><b><a href="http://www.mlebook.com/wiki/doku.php">Machine Learning Engineering</a> by Andriy Burkov</b>
</p>
<p>a great book focussing on the operations side of running a machine learning system. i'm a bit
   under half way through the free online version and very likely to buy a physical copy to finish
   it and support the author. great stuff and, in many ways, a more impactful book than any of
   the theory books here.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/itdm.jpg"></p>
<p><b><a href="https://www-users.cs.umn.edu/~kumar001/dmbook/index.php">Introduction to Data Mining</a>
   by Pang-Ning Tan, Michael Steinbach &amp; Vipin Kumar</b>
</p>
<p>this is another one that was also on my list from ten years ago and though it's section
   on neural networks is a bit of chuckle these days there is still a bunch of really
   great fundamental stuff in this book. very practical and easy to digest. i also see there's
   a second edition now. i reckon this would compliment the "hands on" book above very well.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/salp.jpg"></p>
<p><b><a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a>
   by Dan Jurafsky &amp; James Martin</b>
</p>
<p>still the best overview of NLP there is (IMHO). can't wait to read the 3rd edition which
   apparently will cover more modern stuff (e.g. transformers) but until then, for the
   love of god though, please don't be one of those "this entire book is
   irrelevant now! just fine tune BERT" people :/
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/no.jpg"></p>
<p><b><a href="https://link.springer.com/book/10.1007/978-0-387-40065-5">Numerical Optimization</a>
   by Jorge NocedalStephen J. Wright</b>
</p>
<p>this book is super hard core and maybe more an operations
   research book than machine learning. though i've not read it cover to cover the
   couple of bits i've worked through really taught me a lot. i'd love to understand
   the stuff in this text better; it's so so fundamental to machine learning (and more)
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/dl.jpg"></p>
<p><b><a href="https://www.deeplearningbook.org/">Deep Learning</a>
   by Ian Goodfellow</b>
</p>
<p>writing a book specifically on deep learning is very dangerous since things move so fast but
   if anyone can do it, ian can... i think ian's approach to explaining neural networks
   from the ground up is one of my favorites. i got the first edition hardback but it's free to
   download from the website.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/pr.jpg"></p>
<p><b><a href="https://mitpress.mit.edu/books/probabilistic-robotics">Probabilistic Robotics</a>
   by Sebastian Thrun, Wolfram Burgard and Dieter Fox</b>
</p>
<p>when i first joined a robotics group i bought a stack of ML/robotics books and this
   was by far the best. it's good intro stuff, and maybe already dated in places given
   it's age (the 2006 edition i have) but i still got a bunch from it.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/tml.jpg"></p>
<p><b><a href="https://www.oreilly.com/library/view/tinyml/9781492052036/">TinyML</a>
   by Pete Warden &amp; Daniel Situnayake</b>
</p>
<p>this was a super super fun book to tech review! neural networks on microcontrollers?!?
   yes please!
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/ec.jpg"></p>
<p><b><a href="https://www.wiley.com/en-us/Evolutionary+Computation%3A+Toward+a+New+Philosophy+of+Machine+Intelligence%2C+3rd+Edition-p-9780471669517">Evolutionary Computation</a> by David Fogel</b>
</p>
<p>this is still by favorite book on evolutionary algorithms; i've had this for a loooong
   time now. i still feel like evolutionary approaches are due for a big big comeback
   any time soon....
</p>
<hr>


<h2>in the mail...</h2>
<p>the good thing about writing a list is you get people telling you cool ones you've missed :)
</p>
<p>the top three i've chosen (that are in the mail) are...
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/ciis.jpg"></p>
<p><b><a href="http://bayes.cs.ucla.edu/PRIMER/">Causal Inference in Statistics</a> by
   Judea Pearl, Madelyn Glymour &amp; Nicholas P. Jewell</b>
</p>
<p>recommended by <a href="https://twitter.com/animesh_garg">animesh</a> who quite rightly points out
   the lack of causality in machine learning books in the books above.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/itiala.jpg"></p>
<p><b><a href="https://www.cambridge.org/au/academic/subjects/computer-science/pattern-recognition-and-machine-learning/information-theory-inference-and-learning-algorithms?format=HB&amp;isbn=9780521642989">Information Theory, Inference and Learning Algorithms</a> by David MacKay</b>
</p>
<p>i've seen this book mentioned a number of times and was most recently recommended by
   my colleague <a href="https://twitter.com/danesherbs">dane</a> so it's time to get it.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/bmlpa.jpg"></p>
<p><b><a href="https://www.oreilly.com/library/view/building-machine-learning/9781492045106/">Building Machine Learning Powered Applications</a> by Emmanuel Ameisen</b>
</p>
<p>a number of people i worked with have enjoyed this. first recommended by another
   colleague <a href="https://twitter.com/davidcolls">dave</a>.
   looks to be on the practical side rather than the theory but that's ok some times :)
</p>

  </div></div>]]>
            </description>
            <link>http://matpalm.com/blog/cool_machine_learning_books/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031455</guid>
            <pubDate>Mon, 09 Nov 2020 04:26:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How does the event loop work in JavaScript? [video]]]>
            </title>
            <description>
<![CDATA[
Score 35 | Comments 4 (<a href="https://news.ycombinator.com/item?id=25031384">thread link</a>) | @krayonatan
<br/>
November 8, 2020 | https://yonatankra.com/how-does-the-event-loop-work/ | <a href="https://web.archive.org/web/*/https://yonatankra.com/how-does-the-event-loop-work/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="primary"><main id="main" role="main"><article id="post-599"><div><!-- .entry-meta --><div> <p><span><span>Estimated Reading Time: </span> <span>&lt; 1</span> <span>minute</span></span></p><figure><p><span><iframe width="640" height="360" data-src="https://www.youtube.com/embed/Nqx3rtv_dko?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en-US&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span></p><figcaption>The event loop and your code talk from WarsawJS</figcaption></figure><p>On august 2020 I spoke at <a rel="noreferrer noopener" href="https://warsawjs.com/" data-type="URL" data-id="https://warsawjs.com/" target="_blank">WarsawJS</a>, explaining about the event loop and how it works.  I hope you will enjoy this talk.</p><p>If you prefer to read – <a href="https://yonatankra.com/the-event-loop-and-your-code/" data-type="post" data-id="299">here’s the blog post this talk is based on</a>.</p><p id="jp-relatedposts"><h3><em>Related</em></h3></p></div><!-- .entry-content --><!-- .entry-footer --></div></article><!-- #post-## --><p><h3>Enjoyed the article?</h3><h4>Sign up to my newsletter to enjoy more content:</h4></p>  <nav role="navigation" aria-label="Posts"><h2>Post navigation</h2></nav><!-- #comments --></main><!-- #main --></div></div>]]>
            </description>
            <link>https://yonatankra.com/how-does-the-event-loop-work/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031384</guid>
            <pubDate>Mon, 09 Nov 2020 04:12:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The choice of ML modeling library does not matter]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25031356">thread link</a>) | @chmaynard
<br/>
November 8, 2020 | https://www.shreya-shankar.com/modeling-libraries/ | <a href="https://web.archive.org/web/*/https://www.shreya-shankar.com/modeling-libraries/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>When building the first machine learning pipelines for my company, I agonized over which modeling libraries to include in our stack. What would most model developers want to use? I felt strongly about scikit-learn and PyTorch, but what would be the consequences of imposing my opinions on ML frameworks on our company’s infrastructure? Which modeling library would “win” in the long-term? What if I wrote modeling code in a DSL that would become obsolete in a few years?</p>
<p>In 2016, I took an introductory deep learning class with assignments all in Tensorflow; my most recent deep learning course was completely conducted in PyTorch. Four years later, <a href="https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/">it seems like all ML researchers I know use PyTorch</a>. The few who don’t use PyTorch use TF 1.0, with the “some day I’ll switch to TF 2.0 or PyTorch” mantra. What happened?</p>
<p>Over several months, I realized that <strong>the choice of library does not matter, as modeling is just a tiny step in the machine learning pipeline.</strong> Other steps are equally, if not more, challenging to maintain: for example, <a href="https://www.oracle.com/technetwork/middleware/oedq/successful-data-migration-wp-1555708.pdf">it is much harder to migrate data pipeline code</a> than rewrite basic TF modeling code in PyTorch. I’ve written models in TensorFlow, PyTorch, XGBoost, scikit-learn, and LightGBM for different tasks for my company. I’ve even written non-Python models in Scala. When I iterate on machine learning pipelines for a prediction task, I avoid changing the model architectures as much as possible since <a href="https://www.shreya-shankar.com/making-ml-work/">I’d rather change parts of the pipeline I understand better</a>, like data ingestion and better featurization. My company’s pull requests show that people hardly touch their modeling code compared to pipeline code. What matters is having the infrastructure to “plug and play” ML model trainers and predictors, since there is almost never one programming library that meets all needs. </p>
<p>Some would point to the trends of researchers overwhelmingly preferring PyTorch and JAX and argue that a winner here actually does matter, because researchers turn into data scientists at companies, these data scientists build models, and the models will get productionized and used “forever.” But as a field, we’re still struggling with productionizing models, aligning their outputs with human incentives, iterating on these systems, and trusting these pipelines. For any ML practitioners outside “big tech,” their biggest problems are model pipelines and value alignment between customers, themselves, and machines. After all, these are essential to product development. Even if we built frameworks for these central problems, the modeling library still won’t matter because <a href="https://softwareengineering.stackexchange.com/a/390687">multiple software frameworks for a problem can happily coexist</a>. People are smart and can easily learn a different framework — the fact that there was such a large, rapid <a href="https://www.quora.com/Why-are-people-shifting-from-TensorFlow-to-PyTorch">transition from TensorFlow to PyTorch</a> within a few years proves that developers will find the best tool for their job. It matters more that they have the correct foundation for software they build.</p>
<p>Additionally, business considerations can override the choice of framework or even build new frameworks, particularly in startups. My company’s codebase for a particular ML problem has experienced something in this vein: first, I wrote experimental code in my DSLs of choice to “solve” the problem. Then when we had to build a product, I <a href="https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/">rewrote</a> the pipeline. Then when we pivoted slightly, I <a href="https://refactoring.com/">refactored</a> the pipeline to produce the live ML product we’re regularly releasing today. As I gained more clarity on the current version of the product and how other stakeholders (technical or nontechnical) might interact with it, I realized these business considerations drove pipeline development more than the modeling libraries or my opinions on other DSLs.</p>
<p>So the horse race of modeling libraries is misleading, and the most challenging problems in “real-world” ML right now revolve around business values, productionization, miniaturization, and pipelining for repeated training and inference. But since <a href="https://www.cs.princeton.edu/courses/archive/fall09/cos109/06langs.pdf">programming languages</a> and infrastructure <a href="https://www.forbes.com/sites/oracle/2015/05/20/javas-20-years-of-innovation">drive innovation in software</a>, it’s still worth thinking about the evolution of modeling libraries. In <a href="https://www.amazon.com/Mythical-Man-Month-Software-Engineering-Anniversary/dp/0201835959">The Mythical Man-Month: Essays on Software Engineering</a>, Fred Brooks introduces the concept of the <em><a href="https://en.wikipedia.org/wiki/Second-system_effect">second-system effect</a></em> to be “the tendency of small, elegant, and successful systems to be succeeded by over-engineered, bloated systems, due to inflated expectations and overconfidence.” Famous examples include the IBM System/360 operating system (which succeeded the IBM 700/7000 series from the 1950s), and the Multics operating system (which succeeded Compatible Time-Sharing System from the late 1960s). </p>
<p>I consider TF 1.0 a success: it accelerated a lot of deep learning research, was fairly narrow and thoughtful in scope, and spearheaded innovation in the hardware vertical with XLA compilation, TPUs, and more. But over time, as hundreds of TensorFlow engineers tried to address the software’s limitations and turn TensorFlow into a machine learning library for everybody, it suffered from the second-system effect and became TF 2.0, a machine learning library for nobody (possibly except for Google). One set of problems they tried to address is “making models work in production settings:” TFX is a great example of an <a href="https://blog.tensorflow.org/2020/09/brief-history-of-tensorflow-extended-tfx.html">overhyped</a> and <a href="https://pypistats.org/packages/tfx">underused</a> TF 2.0 tool. Compare the PyPI stats with <a href="https://pypistats.org/packages/kfp">Kubeflow</a>’s for context; TFX built their framework to fit nicely with Kubeflow and Kubeflow users still don’t want to use TFX. This is not to say the problems with production ML aren’t real; rather, it seems TFX currently isn’t <em>the</em> solution to many of these incredibly challenging problems. Having tutorials doesn’t help if the <a href="https://neptune.ai/blog/deep-dive-into-ml-models-in-production-using-tfx-and-kubeflow">UX is counterintuitive and engineers need to become professional error log parsers</a> to become proficient with the tool.</p>
<p>All this being said about my criticism for the TensorFlow UX, I actually use TF 2.0 at work — mainly out of laziness. The Spark to TFRecord to TFData to TF model pipeline is <a href="https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-connector">partially</a> <a href="https://docs.databricks.com/applications/machine-learning/load-data/tfrecords-save-load.html">documented</a>, whereas the Spark to TFRecord to something to PyTorch model pipeline is only <a href="https://github.com/uber/petastorm">barely</a> <a href="https://databricks.com/blog/2020/06/16/simplify-data-conversion-from-apache-spark-to-tensorflow-and-pytorch.html">documented</a>. But the DSL for my models is hardly something I think about on a day-to-day basis, since most of my problems aren’t “how quickly can I code up a transformer or convnet architecture from scratch.” People in this industry rarely build things from scratch. Software products are built on the <a href="https://effectivesoftwaredesign.com/2014/11/02/the-minimum-viable-product-and-incremental-software-development/">principle of incrementality</a>; code and features accumulate over time. </p>
<p>So my answer to my original question is that it’s not worth worrying about which modeling library will “win” in the long run, because <strong>multiple libraries can win if they each do something important</strong>, such as championing the dataflow paradigm or easy autodifferentiation. If you’re an engineer, don’t build your pipelines around a specific modeling library. If you’re a researcher or data scientist, don’t worry about learning all the modeling libraries or whatever libraries the company’s job description mentions. Software history indicates that the modeling framework bloat is inevitable, and for as long as these libraries’ biggest priorities are to compete with each other, they will all converge to the same solutions to mission-critical modeling problems — <a href="https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html">eager execution</a>, ease of <a href="https://www.tensorflow.org/tutorials/quickstart/beginner">building a model from scratch</a>, ability to <a href="https://pytorch-lightning.readthedocs.io/en/latest/loggers.html">nicely view loss curves</a>, and more. But these are only a fraction of most “real-world” machine learning problems, and you, as a machine learning practitioner, at the end of the day aren’t hired only for your expertise in training a model once; you’re hired to make a machine learning system consistently deliver value to an end user.</p>
<p><em>Thanks to <a href="https://people.eecs.berkeley.edu/~pathakr/">Reese Pathak</a>, <a href="https://www.linkedin.com/in/jayant-bhambhani-31b71821/">Jay Bhambhani</a> and <a href="https://twitter.com/debnilsur">Debnil Sur</a> for their feedback on multiple drafts.</em></p></div></div>]]>
            </description>
            <link>https://www.shreya-shankar.com/modeling-libraries/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031356</guid>
            <pubDate>Mon, 09 Nov 2020 04:04:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Covid-19 The Biden-Harris plan to beat Covid-19]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25031354">thread link</a>) | @hkhn
<br/>
November 8, 2020 | https://buildbackbetter.com/priorities/covid-19/ | <a href="https://web.archive.org/web/*/https://buildbackbetter.com/priorities/covid-19/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
	

		<section id="content">

	  
<div data-module="">
  <div>
	<div>
	  <p><span>The American people deserve an urgent, robust, and professional response to the growing public health and economic crisis caused by the coronavirus (COVID-19) outbreak. President-elect Biden believes that the federal government must act swiftly and aggressively to help protect and support our families, small businesses, first responders, and caregivers essential to help us face this challenge, those who are most vulnerable to health and economic impacts, and our broader communities – not to blame others or bail out corporations. </span></p>
	</div>
  </div>
</div>

<div data-module="" id="covid-19-2" data-new-section="false" data-id="covid-19-2">
  <div>
	<div>
	  <div>
	   <p>The Biden-Harris administration will always:</p>
<ul>
<li><strong>Listen to science</strong></li>
<li><strong>Ensure public health decisions are informed by public health professionals</strong></li>
<li><strong>Promote trust, transparency, common purpose, and accountability in our government</strong></li>
</ul>
<p>President-elect Biden and Vice President-elect Harris have a seven-point plan to beat COVID-19.</p>
<p><strong>Ensure all Americans have access to regular, reliable, and free testing.</strong></p>
<ul>
<li>Double the number of drive-through testing sites.</li>
<li>Invest in next-generation testing, including at home tests and instant tests, so we can scale up our testing capacity by orders of magnitude.</li>
<li>Stand up a Pandemic Testing Board like Roosevelt’s War Production Board. It’s how we produced tanks, planes, uniforms, and supplies in record time, and it’s how we will produce and distribute tens of millions of tests.</li>
<li>Establish a U.S. Public Health Jobs Corps to mobilize at least 100,000 Americans across the country with support from trusted local organizations in communities most at risk to perform culturally competent approaches to contact tracing and protecting at-risk populations.</li>
</ul>
<p><strong>Fix personal protective equipment (PPE) problems for good.</strong></p>
<p>President-elect Joe Biden is taking responsibility and giving states, cities, tribes, and territories the critical supplies they need.</p>
<ul>
<li>Fully use the Defense Production Act to ramp up production of masks, face shields, and other PPE so that the national supply of personal protective equipment exceeds demand and our stores and stockpiles — especially in hard-hit areas that serve disproportionately vulnerable populations — are fully replenished.</li>
<li>Build immediately toward a future, flexible American-sourced and manufactured capability to ensure we are not dependent on other countries in a crisis.</li>
</ul>
<p><strong>Provide clear, consistent, evidence-based guidance for how communities should navigate the pandemic – and the resources for schools, small businesses, and families to make it through.</strong></p>
<ul>
<li>Social distancing is not a light switch. It is a dial. President-elect Biden will direct the CDC to provide specific evidence-based guidance for how to turn the dial up or down relative to the level of risk and degree of viral spread in a community, including when to open or close certain businesses, bars, restaurants, and other spaces; when to open or close schools, and what steps they need to take to make classrooms and facilities safe; appropriate restrictions on size of gatherings; when to issue stay-at-home restrictions.</li>
<li>Establish a renewable fund for state and local governments to help prevent budget shortfalls, which may cause states to face steep cuts to teachers and first responders.</li>
<li>Call on Congress to pass an emergency package to ensure schools have the additional resources they need to adapt effectively to COVID-19.</li>
<li>Provide a “restart package” that helps small businesses cover the costs of operating safely, including things like plexiglass and PPE.</li>
</ul>
	  </div>
	</div>
  </div>
</div>

<!-- .module.block-quote -->

<div data-module="" id="covid-19-4" data-new-section="false" data-id="covid-19-4">
  <div>
	<div>
	  <div>
	   <p><strong>Plan for the effective, equitable distribution of treatments and vaccines — because development isn’t enough if they aren’t effectively distributed.</strong></p>
<ul>
<li>Invest $25 billion in a vaccine manufacturing and distribution plan that will guarantee it gets to every American, cost-free.</li>
<li>Ensure that politics plays no role in determining the safety and efficacy of any vaccine. The following 3 principles will guide the Biden-Harris administration: Put scientists in charge of all decisions on safety and efficacy; publicly release clinical data for any vaccine the FDA approves; and authorize career staff to write a written report for public review and permit them to appear before Congress and speak publicly uncensored.</li>
<li>Ensure everyone — not just the wealthy and well-connected — in America receives the protection and care they deserve, and consumers are not price gouged as new drugs and therapies come to market.</li>
</ul>
<p><strong>Protect older Americans and others at high risk.</strong></p>
<p>President-elect Biden understands that older Americans and others at high-risk are most vulnerable to COVID-19.</p>
<ul>
<li>Establish a COVID-19 Racial and Ethnic Disparities Task Force, as proposed by Vice President-elect Harris, to provide recommendations and oversight on disparities in the public health and economic response. At the end of this health crisis, it will transition to a permanent Infectious Disease Racial Disparities Task Force.</li>
<li>Create the Nationwide Pandemic Dashboard that Americans can check in real-time to help them gauge whether local transmission is actively occurring in their zip codes. This information is critical to helping all individuals, but especially older Americans and others at high risk, understand what level of precaution to take.</li>
</ul>
<p><strong>Rebuild and expand defenses to predict, prevent, and mitigate pandemic threats, including those coming from China.</strong></p>
<ul>
<li>Immediately restore the White House National Security Council Directorate for Global Health Security and Biodefense, originally established by the Obama-Biden administration.</li>
<li>Immediately restore our relationship with the World Health Organization, which — while not perfect — is essential to coordinating a global response during a pandemic.</li>
<li>Re-launch and strengthen U.S. Agency for International Development’s pathogen-tracking program called PREDICT.</li>
<li>Expand the number of CDC’s deployed disease detectives so we have eyes and ears on the ground, including rebuilding the office in Beijing.</li>
</ul>
<p><strong>Implement mask mandates nationwide by working with governors and mayors and by asking the American people to do what they do best: step up in a time of crisis.</strong></p>
<p>Experts agree that tens of thousands of lives can be saved if Americans wear masks. President-elect Biden will continue to call on:</p>
<ul>
<li>Every American to wear a mask when they are around people outside their household.</li>
<li>Every Governor to make that mandatory in their state.</li>
<li>Local authorities to also make it mandatory to buttress their state orders.</li>
</ul>
<p>Once we succeed in getting beyond this pandemic, we must ensure that the millions of Americans who suffer long-term side effects from COVID don’t face higher premiums or denial of health insurance because of this new pre-existing condition. The Biden-Harris Administration will work to ensure that the protections for those with pre-existing conditions that were won with Obamacare are protected. And, they will work to lower health care costs and expand access to quality, affordable health care through a Medicare-like public option.</p>
	  </div>
	</div>
  </div>
</div>



	</section>

  </article></div>]]>
            </description>
            <link>https://buildbackbetter.com/priorities/covid-19/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031354</guid>
            <pubDate>Mon, 09 Nov 2020 04:04:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A fun website that simulates fluid]]>
            </title>
            <description>
<![CDATA[
Score 302 | Comments 47 (<a href="https://news.ycombinator.com/item?id=25031304">thread link</a>) | @svikashk
<br/>
November 8, 2020 | https://paveldogreat.github.io/WebGL-Fluid-Simulation/ | <a href="https://web.archive.org/web/*/https://paveldogreat.github.io/WebGL-Fluid-Simulation/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                        <p>Try Fluid Simulation app!</p>
                        <p><a id="apple_link" target="_blank">
                                <img alt="Download on the App Store" src="https://paveldogreat.github.io/WebGL-Fluid-Simulation/app_badge.png">
                            </a>
                            <a id="google_link" target="_blank">
                                <img alt="Get it on Google Play" src="https://paveldogreat.github.io/WebGL-Fluid-Simulation/gp_badge.png">
                            </a>
                        </p>
                    </div></div>]]>
            </description>
            <link>https://paveldogreat.github.io/WebGL-Fluid-Simulation/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031304</guid>
            <pubDate>Mon, 09 Nov 2020 03:54:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Where were you when the US election was called? A map of the stars]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25031179">thread link</a>) | @juanre
<br/>
November 8, 2020 | https://greaterskies.com/free-map/harris-2020 | <a href="https://web.archive.org/web/*/https://greaterskies.com/free-map/harris-2020">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-v-21cede04=""><div><h2>About your star map</h2> <p>It is an accurate image of the sky as seen from a particular place and time, including thousands of stars, the Moon, the Sun and the planets.</p> <p>We have been making these maps since 2006, and are proud to make the very best available.  You can learn more about them
      <a href="https://greaterskies.com/star-map/" target="_blank">here</a>.</p> <p>We really hope you will love it!</p> <p>The GreaterSkies team</p></div></div></div>]]>
            </description>
            <link>https://greaterskies.com/free-map/harris-2020</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031179</guid>
            <pubDate>Mon, 09 Nov 2020 03:25:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[SSH Tunneling Basics]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25031064">thread link</a>) | @bswamina
<br/>
November 8, 2020 | https://www.polarsparc.com/xhtml/SSH-Tunnel.html | <a href="https://web.archive.org/web/*/https://www.polarsparc.com/xhtml/SSH-Tunnel.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    <br>
    
    <br>
    
    
    <hr> 
    <p>Overview</p>
    <p>In networking, a <span>Tunnel</span> is used to encapsulate a communication protocol that is not supported
        by the network inside a protocol that is supported by the network.</p>
    <p>The following are some of terms used in this article:</p>
    <table id="col2-table">
      <thead>
        <tr>
          <th>Term</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><span>SSH</span></td>
          <td>short for <span>S</span>ecure <span>SH</span>ell is a protocol that sets
            up an encrypted connection between two nodes over an unsecured network using a Client-Server architecture</td>
        </tr>
        <tr>
          <td><span>SSH Server</span></td>
          <td>a server that listens on the TCP port <span>22</span> for incoming SSH client
            requests, then authenticates those client requests, and provides a command prompt</td>
        </tr>
        <tr>
          <td><span>SSH Client</span></td>
          <td>a client used to connect to the remote SSH Server on a specific network node</td>
        </tr>
        <tr>
          <td><span>SSH Tunnel</span></td>
          <td>a method of encapsulating and transmitting arbitrary networking data over an encrypted SSH
            connection between a client node and a server node</td>
        </tr>
        <tr>
          <td><span>SSH Port Forwarding</span></td>
          <td>another name for <span>SSH Tunnel</span></td>
        </tr>
      </tbody>
    </table>
    <div id="para-div">
      <p>So, why do we need <span>tunneling</span> ??? The following are some of the reasons:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p>To allow access to legacy applications or unsecure services such as IMAP, POP3, VNC, etc</p>
        </li>
        <li>
          <p>To implement a virtual private network (<span>VPN</span>)</p>
        </li>
        <li>
          <p>To allow access to services behind a firewall</p>
        </li>
      </ul>
    </div>
    <div id="para-div">
      <p>There are <span>3</span> types of <span>SSH Tunnel</span> options as listed below:</p>
      <ol id="blue-ol">
        <li>
          <p><span>Local</span> Port Forwarding</p>
        </li>
        <li>
          <p><span>Remote</span> Port Forwarding</p>
        </li>
        <li>
          <p><span>Dynamic</span> Port Forwarding</p>
        </li>
      </ol>
    </div>
    <p>We will discuss and demonstrate each of the above options in the following sections.</p>
    <p>Setup</p>
    <p>The setup will be on a Ubuntu 20.04 LTS based Linux desktop. For the demonstrations, we will create an environment with
        3 virtual machines running on the hypervisor <span>VirtualBox</span>.</p>
    <p>The following diagram illustrates the environment setup:</p>
    <div id="img-outer-div"> <p><img src="https://www.polarsparc.com/xhtml/images/ssh-tunnel-2.png" alt="Environment"></p><p>Environment</p>
    </div>
    <br>
    <div id="para-div">
      <p>The following are some of the highlights of the 3 virtual machines:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p><span>vm-1</span> :: 1 vCPU, 2GB RAM, 20GB storage, Ubuntu 20.04 OS, and uses a single virtual network
            interface with <span>NAT</span> networking (<span>10.0.2.15</span>)</p>
        </li>
        <li>
          <p><span>vm-2</span> :: 1 vCPU, 2GB RAM, 20GB storage, Ubuntu 20.04 OS, and uses a single virtual network
            interface with <span>Host-only</span> networking (<span>192.168.56.104</span>)</p>
        </li>
        <li>
          <p><span>vm-3</span> :: 1 vCPU, 2GB RAM, 20GB storage, Ubuntu 20.04 OS, and uses a two separate virtual
            network interfaces - one with <span>NAT</span> networking (<span>10.0.2.4</span>) and the
            other with <span>Host-only</span> networking (<span>192.168.56.103</span>)</p>
        </li>
      </ul>
    </div>
    <p>Open a Terminal window in each of the 3 virtual machines <span>vm-1</span> thru <span>vm-3</span>
        and install <span>Python Flask</span>, <span>Net Tools</span>, and <span>SSH Server
        </span> by executing the following command:</p>
    <p>$ sudo apt install python3-flask net-tools openssh-server -y</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  javascript-common libjs-jquery ncurses-term openssh-sftp-server python3-itsdangerous python3-jinja2 python3-markupsafe
  python3-openssl python3-pyinotify python3-werkzeug ssh-import-id
Suggested packages:
  apache2 | lighttpd | httpd molly-guard monkeysphere ssh-askpass python-flask-doc python-jinja2-doc python-openssl-doc
  python3-openssl-dbg python-pyinotify-doc ipython3 python-werkzeug-doc python3-lxml python3-termcolor python3-watchdog
The following NEW packages will be installed:
  javascript-common libjs-jquery ncurses-term net-tools openssh-server openssh-sftp-server python3-flask python3-itsdangerous
  python3-jinja2 python3-markupsafe python3-openssl python3-pyinotify python3-werkzeug ssh-import-id
0 upgraded, 14 newly installed, 0 to remove and 0 not upgraded.
Need to get 1,478 kB of archives.
After this operation, 9,096 kB of additional disk space will be used.
Get:1 http://us.archive.ubuntu.com/ubuntu focal/main amd64 javascript-common all 11 [6,066 B]
Get:2 http://us.archive.ubuntu.com/ubuntu focal/main amd64 libjs-jquery all 3.3.1~dfsg-3 [329 kB]
Get:3 http://us.archive.ubuntu.com/ubuntu focal/main amd64 ncurses-term all 6.2-0ubuntu2 [249 kB]
Get:4 http://us.archive.ubuntu.com/ubuntu focal/main amd64 net-tools amd64 1.60+git20180626.aebd88e-1ubuntu1 [196 kB]
Get:5 http://us.archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-sftp-server amd64 1:8.2p1-4ubuntu0.1 [51.5 kB]
Get:6 http://us.archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-server amd64 1:8.2p1-4ubuntu0.1 [377 kB]
Get:7 http://us.archive.ubuntu.com/ubuntu focal/main amd64 python3-itsdangerous all 1.1.0-1 [14.6 kB]
Get:9 http://us.archive.ubuntu.com/ubuntu focal/main amd64 python3-markupsafe amd64 1.1.0-1build2 [13.9 kB]
Get:9 http://us.archive.ubuntu.com/ubuntu focal/main amd64 python3-jinja2 all 2.10.1-2 [95.5 kB]
Get:10 http://us.archive.ubuntu.com/ubuntu focal/main amd64 python3-werkzeug all 0.16.1+dfsg1-2 [183 kB]
Get:11 http://us.archive.ubuntu.com/ubuntu focal/main amd64 python3-flask all 1.1.1-2 [80.3 kB]
Get:12 http://us.archive.ubuntu.com/ubuntu focal/main amd64 python3-openssl all 19.0.0-1build1 [43.3 kB]
Get:13 http://us.archive.ubuntu.com/ubuntu focal/main amd64 python3-pyinotify all 0.9.6-1.2ubuntu1 [24.8 kB]
Get:14 http://us.archive.ubuntu.com/ubuntu focal/main amd64 ssh-import-id all 5.10-0ubuntu1 [10.0 kB]
Fetched 1,478 kB in 0s (5,095 kB/s)  
Preconfiguring packages ...
Selecting previously unselected package javascript-common.
(Reading database ... 192970 files and directories currently installed.)
Preparing to unpack .../00-javascript-common_11_all.deb ...
Unpacking javascript-common (11) ...
Selecting previously unselected package libjs-jquery.
Preparing to unpack .../01-libjs-jquery_3.3.1~dfsg-3_all.deb ...
Unpacking libjs-jquery (3.3.1~dfsg-3) ...
Selecting previously unselected package ncurses-term.
Preparing to unpack .../02-ncurses-term_6.2-0ubuntu2_all.deb ...
Unpacking ncurses-term (6.2-0ubuntu2) ...
Preparing to unpack .../net-tools_1.60+git20180626.aebd88e-1ubuntu1_amd64.deb ...
Unpacking net-tools (1.60+git20180626.aebd88e-1ubuntu1) ...
Selecting previously unselected package openssh-sftp-server.
Preparing to unpack .../03-openssh-sftp-server_1%3a8.2p1-4ubuntu0.1_amd64.deb ...
Unpacking openssh-sftp-server (1:8.2p1-4ubuntu0.1) ...
Selecting previously unselected package openssh-server.
Preparing to unpack .../04-openssh-server_1%3a8.2p1-4ubuntu0.1_amd64.deb ...
Unpacking openssh-server (1:8.2p1-4ubuntu0.1) ...
Selecting previously unselected package python3-itsdangerous.
Preparing to unpack .../05-python3-itsdangerous_1.1.0-1_all.deb ...
Unpacking python3-itsdangerous (1.1.0-1) ...
Selecting previously unselected package python3-markupsafe.
Preparing to unpack .../06-python3-markupsafe_1.1.0-1build2_amd64.deb ...
Unpacking python3-markupsafe (1.1.0-1build2) ...
Selecting previously unselected package python3-jinja2.
Preparing to unpack .../07-python3-jinja2_2.10.1-2_all.deb ...
Unpacking python3-jinja2 (2.10.1-2) ...
Selecting previously unselected package python3-werkzeug.
Preparing to unpack .../08-python3-werkzeug_0.16.1+dfsg1-2_all.deb ...
Unpacking python3-werkzeug (0.16.1+dfsg1-2) ...
Selecting previously unselected package python3-flask.
Preparing to unpack .../09-python3-flask_1.1.1-2_all.deb ...
Unpacking python3-flask (1.1.1-2) ...
Selecting previously unselected package python3-openssl.
Preparing to unpack .../10-python3-openssl_19.0.0-1build1_all.deb ...
Unpacking python3-openssl (19.0.0-1build1) ...
Selecting previously unselected package python3-pyinotify.
Preparing to unpack .../11-python3-pyinotify_0.9.6-1.2ubuntu1_all.deb ...
Unpacking python3-pyinotify (0.9.6-1.2ubuntu1) ...
Selecting previously unselected package ssh-import-id.
Preparing to unpack .../12-ssh-import-id_5.10-0ubuntu1_all.deb ...
Unpacking ssh-import-id (5.10-0ubuntu1) ...
Setting up javascript-common (11) ...
Setting up net-tools (1.60+git20180626.aebd88e-1ubuntu1) ...
Setting up openssh-sftp-server (1:8.2p1-4ubuntu0.1) ...
Setting up openssh-server (1:8.2p1-4ubuntu0.1) ...
Creating config file /etc/ssh/sshd_config with new version
Creating SSH2 RSA key; this may take some time ...
3072 SHA256:OVmIaDeM2PCBtB6O5tddPIC3q4nuZVdqfcs/7A3QM5A root@vm-3 (RSA)
Creating SSH2 ECDSA key; this may take some time ...
256 SHA256:qDrGgauXE9LwZ6S1j4fjbY0LIPyrL+YSU8iq+PbR7jM root@vm-3 (ECDSA)
Creating SSH2 ED25519 key; this may take some time ...
256 SHA256:WBk+gqOXD47VoAJrw+JeZLxQlzBWdaKFRxi5xfPAkYg root@vm-3 (ED25519)
Created symlink /etc/systemd/system/sshd.service â†’ /lib/systemd/system/ssh.service.
Created symlink /etc/systemd/system/multi-user.target.wants/ssh.service â†’ /lib/systemd/system/ssh.service.
rescue-ssh.target is a disabled or a static unit, not starting it.
Setting up python3-openssl (19.0.0-1build1) ...
Setting up ssh-import-id (5.10-0ubuntu1) ...
Attempting to convert /etc/ssh/ssh_import_id
Setting up python3-pyinotify (0.9.6-1.2ubuntu1) ...
Setting up python3-itsdangerous (1.1.0-1) ...
Setting up python3-markupsafe (1.1.0-1build2) ...
Setting up python3-jinja2 (2.10.1-2) ...
Setting up libjs-jquery (3.3.1~dfsg-3) ...
Setting up ncurses-term (6.2-0ubuntu2) ...
Setting up python3-werkzeug (0.16.1+dfsg1-2) ...
Setting up python3-flask (1.1.1-2) ...
Processing triggers for systemd (245.4-4ubuntu3.3) ...
Processing triggers for man-db (2.9.1-1) ...
Processing triggers for ufw (0.36-6) ...</pre>
    </div>
    <p>Local Port Forwarding</p>
    <p>Assuming <span>vm-3</span> is hosting a useful web application on <span>10.0.2.4</span>, it is
        *ONLY* accessible within the <span>10.0.2.x</span> network. What if a client on the <span>
        vm-2</span> wants to access the web application ???</p>
    <p>In this situation, one could use Local Port Forwarding SSH Tunnel option to allow the client <span>vm-2</span>
        running on <span>192.168.56.104</span> to access the web application server running on the
        <span>10.0.2.x</span> network.</p>
    <p>The following is the code for the simple <span>Python</span> based web application:</p>
    <fieldset id="sc-fieldset"> <legend>Web.py</legend>
      <pre>import sys
from datetime import datetime
from flask …</pre></fieldset></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.polarsparc.com/xhtml/SSH-Tunnel.html">https://www.polarsparc.com/xhtml/SSH-Tunnel.html</a></em></p>]]>
            </description>
            <link>https://www.polarsparc.com/xhtml/SSH-Tunnel.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031064</guid>
            <pubDate>Mon, 09 Nov 2020 03:02:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Program Development in Limbo for Inferno OS]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25030961">thread link</a>) | @marttt
<br/>
November 8, 2020 | https://seh.dev/limbo-intro/ | <a href="https://web.archive.org/web/*/https://seh.dev/limbo-intro/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  
    <div>
<h2 id="motivation">Motivation</h2>
<p>Resources covering software development under Inferno are fairly scarce.</p>
<p>As such, this post aims to provide a start-to-finish demonstration of program development in Limbo inside Inferno.</p>
<h2 id="introduction">Introduction</h2>
<p>This post assumes you’re using Inferno, specifically <a href="https://code.9front.org/hg/purgatorio/">purgatorio</a>, hosted under <code>linux/amd64</code> or similar.</p>
<p>It’s also possible to use Inferno under Docker as per the <code>INSTALL</code> file.</p>
<p>Other platforms are supported, but steps may differ here or there.</p>
<p>The rune <code>$</code> indicates a unix shell command under <code>bash</code>, probably.</p>
<p>The rune <code>;</code> or <code>%</code> indicates a command to be run from inside Inferno.</p>
<p>The final source from this post: <a href="https://github.com/henesy/socketh-limbo">https://github.com/henesy/socketh-limbo</a></p>
<p>This post will be an implementation of <a href="https://github.com/henesy/SocketH">SocketH</a> which was originally written in Go and has a few other implementations:</p>
<ul>
<li><a href="https://github.com/henesy/socketh-myr">https://github.com/henesy/socketh-myr</a></li>
<li><a href="https://github.com/henesy/SocketS">https://github.com/henesy/SocketS</a></li>
</ul>
<p>The original code isn’t great, but it gives a target for what we want to create.</p>
<h2 id="getting-started">Getting started</h2>
<p>Many, if not all, of these development steps prior to <em>running</em> the final Dis bytecode can be done from outside of Inferno.</p>
<p>The limbo compiler can be called as <code>limbo</code> and with the right workflow development may be more pleasant.</p>
<p>This post assumes:</p>
<ul>
<li>Development occurs inside of Inferno for the purpose of consistency</li>
<li>Some knowledge about imperative, C-like, language programming</li>
<li>Some knowledge about how unix-like systems work</li>
<li>Some knowledge about how C-like compiler and linker flows work</li>
<li>Knowledge about how to interact with a unix-like shell</li>
<li>Vague knowledge about Inferno, such as the fact Inferno exists ☺</li>
</ul>
<h3 id="build-inferno">Build Inferno</h3>
<p>Steps provided are targeted for <code>linux/amd64</code> as a host for Inferno.</p>
<p>The official <a href="https://bitbucket.org/inferno-os/inferno-os/">Inferno</a> tree is hosted over <a href="https://git-scm.com/">Git</a>.</p>
<p>The <a href="https://code.9front.org/hg/purgatorio/">purgatorio</a> fork is hosted by the 9front project over <a href="https://www.mercurial-scm.org/">Mercurial</a>.</p>
<p>Cloning:</p>
<div><pre><code data-lang="text">$ hg clone https://code.9front.org/hg/purgatorio
destination directory: purgatorio
requesting all changes
adding changesets
adding manifests
adding file changes
added 86 changesets with 10904 changes to 10545 files
new changesets 78950db8e089:749c484c1b9c
updating to branch default
9584 files updated, 0 files merged, 0 files removed, 0 files unresolved
$ cd purgatorio/
$ ls
acme                     FreeBSD  libdynld     libprefab      mkfile       scripts
AIX                      icons    libfreetype  libsec         mkfiles      services
appl                     include  libinterp    libtk          module       Solaris
bitbucket-pipelines.yml  Inferno  libkern      limbo          NetBSD       tools
CHANGES                  INSTALL  libkeyring   Linux          NOTICE       usr
dis                      Irix     liblogfs     locale         Nt           utils
doc                      keydb    libmath      MacOSX         OpenBSD
Dockerfile               lib      libmemdraw   makemk-AIX.sh  os
DragonFly                lib9     libmemlayer  makemk.sh      Plan9
emu                      libbio   libmp        man            POSTINSTALL
fonts                    libdraw  libnandfs    mkconfig       README.md
$
</code></pre></div><p><strong>Read the</strong> <code>INSTALL</code> <strong>file!</strong></p>
<p>Update our <code>$HOME/.profile</code> to reflect the Inferno install, adapt this to your directories:</p>
<div><pre><code data-lang="text">export EMU='-g1280x960 -c1'
export INFERNO=$HOME/repos/purgatorio
export PATH=$PATH:$INFERNO/Linux/386/bin
</code></pre></div><p>Reload our shell currently in the purgatorio root tree:</p>
<div><pre><code data-lang="text">$ source $HOME/.profile
$
</code></pre></div><p>Update the <code>mkconfig</code> file to reflect our environment, adapt this as needed:</p>
<div><pre><code data-lang="text">ROOT=$HOME/repos/purgatorio

TKSTYLE=std

CONF=emu

SYSHOST=Linux		# build system OS type (Hp, Inferno, Irix, Linux, MacOSX, Nt, Plan9, Solaris)
SYSTARG=$SYSHOST	# target system OS type (Hp, Inferno, Irix, Linux, Nt, Plan9, Solaris)

OBJTYPE=386

OBJDIR=$SYSTARG/$OBJTYPE

&lt;$ROOT/mkfiles/mkhost-$SYSHOST			# variables appropriate for host system
&lt;$ROOT/mkfiles/mkfile-$SYSTARG-$OBJTYPE	# variables used to build target object type
</code></pre></div><p>Enable multi-arch support on debian-based distributions if on amd64 (64-bit) as Inferno is 32-bit only:</p>
<div><pre><code data-lang="text">$ dpkg --add-architecture i386
$ apt-get update
</code></pre></div><p>Install dependencies required to compile Inferno, this example shows dependencies for debian-based (Ubuntu) distributions:</p>
<div><pre><code data-lang="text">$ apt install libc6-dev-i386 libxext6:i386 libx11-dev:i386 libxext-dev:i386 libfontconfig1-dev:i386
…
$
</code></pre></div><p>Build <code>mk</code> which will be used to bootstrap the rest of the process:</p>
<p>Build and install Inferno!</p>
<div><pre><code data-lang="text">$ mk mkdirs
…
$ mk clean
…
$ mk install
…
$
</code></pre></div><h3 id="start-inferno">Start Inferno</h3>
<p>A graphical environment should appear.</p>
<p>You can make the gui window for Inferno larger by passing in a different size to <code>emu</code> as per <a href="http://man.postnix.pw/purgatorio/1/emu">the manual</a>:</p>
<div><pre><code data-lang="text">-gXsizexYsize
	Define screen width and height in pixels.  The default
	values are 640x480 and the minimum values are 64x48.
	Values smaller than the minimum or greater than the
	available display size are ignored.
</code></pre></div><p>thus:</p>
<p>and so forth.</p>
<p>Some programs can be found under the start menu in the bottom left corner decorated with the <a href="https://seh.dev/limbo-intro/vitanuova.com/">Vita Nuova</a> logo:</p>
<p><img src="http://www.vitanuova.com/images/vitanuova.jpg" alt="Vita Nuova’s logo"></p>
<p>The <code>Shell</code> entry in the start menu will provide a shell-interpreter window from which further commands can be run inside Inferno.</p>
<h3 id="preparation">Preparation</h3>
<div><pre><code data-lang="text">% cd $home/appl
% os git clone https://github.com/henesy/socketh-limbo
% cd socketh-limbo
% lc
.git/     LICENSE   README.md
% touch .gitignore socketh.b
% acme socketh.b
</code></pre></div><p><code>.gitignore</code>:</p>
<p>Limbo ‘libraries’, known as ‘modules’, and ‘programs’ are one and the same in terms of semantics, bar ‘libraries’ having module <code>.m</code> files which are similar to header <code>.h</code> files in C.</p>
<p>As such, the boilerplate for most Limbo programs is very similar. We can initialize our main file as follows.</p>
<p><code>socketh.b</code>:</p>
<div><pre><code data-lang="c">implement SocketH;

include <span>"sys.m"</span>;
	<span>sys</span>: Sys;

include <span>"draw.m"</span>;
include <span>"arg.m"</span>;

<span>SocketH</span>: module {
	<span>init</span>: fn(<span>nil</span>: ref Draw<span>-&gt;</span>Context, <span>argv</span>: list of string);
};


<span># An implementation of the SocketH chat protocol
</span><span></span>init(<span>nil</span>: ref Draw<span>-&gt;</span>Context, <span>argv</span>: list of string) {
	sys <span>=</span> load Sys Sys<span>-&gt;</span>PATH;
	<span>arg</span> :<span>=</span> load Arg Arg<span>-&gt;</span>PATH;
	<span>if</span>(arg <span>==</span> nil)
		raise <span>"could not load arg"</span>;



	exit;
}
</code></pre></div><p>We can break this down a bit.</p>
<p><code>implement</code> declares a module by name.</p>
<p>A module definition must be provided indicating exported functions from the module:</p>
<div><pre><code data-lang="text">SocketH: module {
	init: fn(nil: ref Draw-&gt;Context, argv: list of string);
};
</code></pre></div><p>Note how a variable name of <code>nil</code> is used to drop assignment of a value.</p>
<p>The <code>init</code> function is special in shell-loaded Limbo programs and its signature <em>must</em> match what the shell expects the init function interface to be.</p>
<p>Functionally, <code>init</code> is equivalent to <code>main</code> in most other languages.</p>
<p><code>include</code> imports an external module’s definitions into our scope.</p>
<p><code>load</code> performs the dynamic loading of a module at runtime.</p>
<p><code>exit</code> performs the dynamic un-loading of a module at runtime.</p>
<p><code>raise</code> will throw an exception with a given string as its content.</p>
<p>We refer to names inside a module using the <code>-&gt;</code> operator.</p>
<p>We can jointly assign and declare in one step using the <code>:=</code> operator.</p>
<p>Curly braces are optional.</p>
<p>Semicolons are not.</p>
<p>Note the absence of a reserved <code>main</code> module. This is due to each <code>.dis</code> file, potentially an independent module, being theoretically loadable in its own right. A reserved name would cause significant issues with namespaces ☺.</p>
<h3 id="setting-up-a-workflow">Setting up a workflow</h3>
<p>Compiling our program should be as straightforward as running the Limbo compiler against our source file:</p>
<div><pre><code data-lang="text">% limbo socketh.b
% lc
.git/		LICENSE		socketh.b
.gitignore	README.md	socketh.dis
% socketh.dis
%
</code></pre></div><p>This program does nothing right now, but that’s fine.</p>
<p>Note how we can omit the <code>./</code> when running <code>.dis</code> programs.</p>
<p>Calling the limbo compiler each time is a bit of a pain, and if we start using commandline flags this will become tedious to type.</p>
<p>In acme, we could type the text we want to run in a tag or window and middle-click said text to run the compilation (or more!) on-demand. In Inferno, acme comes with a <code>Limbo</code> command in the default window tag, but that only works for one file.</p>
<p>We can simplify this process by writing a <s>makefile</s> <a href="http://doc.cat-v.org/bell_labs/mk/">mkfile</a>!</p>
<p><code>mkfile</code>:</p>
<div><pre><code data-lang="text">&lt;/mkconfig

DISBIN = /dis

TARG = socketh.dis

&lt;/mkfiles/mkdis
</code></pre></div><p>Mk semantics are similar to make with some changes.</p>
<p>How mk will behave inside Inferno using the <code>mkdis</code> mkfile as the trailing import:</p>
<ul>
<li>Mk can import outside mkfiles using the <code>&lt;</code> operator</li>
<li><code>mk</code> will call <code>mk all</code> which resolves to the <code>all</code> (default) target</li>
<li><code>mk install</code> calls the <code>all</code> target and copies the <code>TARG</code> file(s) to the <code>DISBIN</code> destination directory</li>
<li><code>mk clean</code> removes files such as <code>.dis</code> and <code>.sbl</code> from the working directory</li>
<li><code>mk nuke</code> calls the <code>clean</code> target as well as delete the ‘target’ files such as the <code>/dis/socketh</code> binary if the <code>install</code> target has been called</li>
</ul>
<p>A demonstration:</p>
<div><pre><code data-lang="text">% lc
.git/		LICENSE		mkfile
.gitignore	README.md	socketh.b
% mk
limbo -I/module -gw socketh.b
socketh.b:15: warning: argument argv not referenced
% lc
.git/		LICENSE		mkfile		socketh.dis
.gitignore	README.md	socketh.b	socketh.sbl
% mk install
rm -f /dis/socketh.dis &amp;&amp; cp socketh.dis /dis/socketh.dis
% mk clean
rm -f *.dis *.sbl
% whatis socketh
/dis/socketh.dis
% mk nuke
rm -f *.dis *.sbl
cd /dis; rm -f socketh.dis
% whatis socketh.dis
socketh.dis: not found
% lc
.git/		LICENSE		mkfile
.gitignore	README.md	socketh.b
%
</code></pre></div><p>Note the Limbo compiler flags being passed by default now for the <code>all</code> target.</p>
<p>At this point, I usually add <code>mk clean &amp;&amp; mk</code> to my acme tag and run that for multi-file or more complex Limbo programs. This flow is very similar to how I do development under Plan 9.</p>
<h3 id="common-patterns">Common patterns</h3>
<h4 id="commandline-flags">Commandline flags</h4>
<p>We can use <a href="https://seh.dev/limbo-intro/man.postnix.pw/purgatorio/2/arg">arg(2)</a> to process commandline flags:</p>
<div><pre><code data-lang="c"><span>…</span>

<span>chatty</span>: <span>int</span>	<span>=</span> <span>0</span>;	<span>#</span> Verbose debug output


<span># An implementation of the SocketH chat protocol
</span><span></span>init(<span>nil</span>: ref Draw<span>-&gt;</span>Context, <span>argv</span>: list of string) {
	sys <span>=</span> load Sys Sys<span>-&gt;</span>PATH;
	<span>arg</span> :<span>=</span> load Arg Arg<span>-&gt;</span>PATH;
	<span>if</span>(arg <span>==</span> nil)
		raise <span>"could not load arg"</span>;

	<span>addr</span>: string <span>=</span> <span>"tcp!*!9090"</span>;

	arg<span>-&gt;</span>init(argv);
	arg<span>-&gt;</span>setusage(<span>"socketh [-D] [-a addr]"</span>);

	<span>while</span>((<span>c</span> :<span>=</span> arg<span>-&gt;</span>opt()) <span>!=</span> <span>0</span>)
		<span>case</span> c {
		<span>'D'</span> <span>=&gt;</span>
			chatty<span>++</span>;

		<span>'a'</span> <span>=&gt;</span>
			addr <span>=</span> arg<span>-&gt;</span>earg();

		<span>*</span> <span>=&gt;</span>
			arg<span>-&gt;</span>usage();
		}

	argv <span>=</span> arg<span>-&gt;</span>argv();



	exit;
}
</code></pre></div><p>We can see how these flags are parsed and how these functions act:</p>
<div><pre><code data-lang="text">% mk
mk: 'all' is up to date
% socketh -h
usage: socketh [-D] [-a addr]
% socketh -D
% socketh -a
usage: socketh [-D] [-a addr]
% socketh -a -D
% socketh -D -a
usage: socketh [-D] [-a …</code></pre></div></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://seh.dev/limbo-intro/">https://seh.dev/limbo-intro/</a></em></p>]]>
            </description>
            <link>https://seh.dev/limbo-intro/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030961</guid>
            <pubDate>Mon, 09 Nov 2020 02:43:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Attention Is My Most Valuable Asset for Productivity as a Software Developer]]>
            </title>
            <description>
<![CDATA[
Score 602 | Comments 237 (<a href="https://news.ycombinator.com/item?id=25030938">thread link</a>) | @zwbetz
<br/>
November 8, 2020 | https://zwbetz.com/attention-is-my-most-valuable-asset-for-productivity-as-a-software-developer/ | <a href="https://web.archive.org/web/*/https://zwbetz.com/attention-is-my-most-valuable-asset-for-productivity-as-a-software-developer/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main-content">
      <div>
        <div>
          <div>
            
  
  
  <p>
    
    
    
    <strong>Published: </strong>2020-11-08

    
    
      
      
        • <strong>Lastmod: </strong>2020-11-09

      
    
    
    
    
      <br>
      <span>
        <strong>Tags: </strong>
        
          
          
          
        
          
          
          
        
          
          
          
        
        <a href="https://zwbetz.com/tags/life/">life</a> • <a href="https://zwbetz.com/tags/attention/">attention</a> • <a href="https://zwbetz.com/tags/productivity/">productivity</a>
      
    </span>
  </p>
  
  
  
  

<p>Like a tightly written function, I prefer to exit early if no work should be done. So, if you disagree with these definitions and assumptions, now’s a good time to stop reading.</p>
<ul>
<li><strong>Sustainable productivity:</strong> The maximum rate of quality work output, without loss to the wellbeing of the developer</li>
<li><strong>Quality work:</strong> Software that meets requirements, is valuable to users, is maintainable, and is as bug free as possible</li>
<li><strong>Attention:</strong> The limited mental capacity to focus on a task</li>
<li>Sustainable productivity is desired</li>
<li>Attention is essential to sustainable productivity</li>
</ul>
<p>My high-level workflow looks something like this: identify the problem to solve; think on the problem and let ideas percolate; research, discuss, and experiment with these ideas; implement and test the solution; deliver and maintain the solution.</p>
<p>This cycle could repeat many times in a day. Or I could spend days stuck on a single cycle step. Every step in this cycle requires attention. The more attention I can devote, the more cycles I can complete, and the more productive I am.</p>
<p>How long you can focus on a task varies by person. Some people are very good at it out of the box, some people, not so much. Regardless of the hand you were dealt, I believe that focus (the act of devoting your attention) is a skill, and like any skill, can be improved with practice.</p>
<p>So, how can you increase your attention reserves? The most bang for your buck is to organize your outside world in such a way that it’s distraction free as possible. Once you do that, you’ll have more time to practice, and therefore more time to get better.</p>
<p><strong>Build physical strength.</strong> The damage done by sitting 8+ hours a day is underrated. You need a way to offset this damage, especially if you plan to work in this field for decades. Opinions abound on this topic, but I personally prefer deadlifts. There are few movements more primal than picking a heavy object off the ground and standing up with it. You can <a href="https://www.youtube.com/watch?v=wYREQkVtvEc">learn correct technique in little time</a>. I most like deadlifts because you can do them safely, at high weights, into old age. I also like the hand, back, and hip strength they give, to make it that much harder for sitting damage to have its way with you.</p>
<p><strong>Make your place of work boring and tidy.</strong> My office is a spare bedroom. The walls are blank. There’s no tv. There’s a desk, chair, laptop, laptop stand, keyboard, mouse, and mouse pad. There’s a window, which lets enough light in so that I don’t feel like I’m missing a beautiful day, but not too much light to cause screen glare. If I need to work with paper, it’s immediately filed somewhere when done. Like I said, boring and tidy.</p>
<p><strong>Make your smart phone dumb.</strong> My phone has all notifications disabled, except for calls and text messages. Well, and National Hurricane Center alerts, since I live in Louisiana. Unless you’re my wife, you know that I don’t respond to text messages immediately, that’s just how it is. I disabled my social media accounts some time ago. But if you have them, turning off notifications should help curve the urge to compulsively check them.</p>
<p><strong>Be an OS minimalist.</strong> Apps I use less commonly are a keypress combo away. Given this, my dock has only the apps I use on a daily basis:</p>
<ul>
<li>File system explorer</li>
<li>Internet browser</li>
<li>Terminal</li>
<li>Text editor for front-end code and notes</li>
<li>IDE for back-end code</li>
<li>IDE for database</li>
<li>Visual file differ for version control</li>
<li>Email client</li>
<li>Instant message client</li>
</ul>
<p>My desktop alternates between clean and dirty states. Files I’m currently working with live on the desktop. Then they’re filed away into sensible folders when done.</p>
<p><strong>Organize your browser bookmarks.</strong> When I read something useful that I may need to reference later, I file it under a general archive folder. Then more specific items get their own folders. Frequently accessed links are visible on my bookmarks bar under their own folder.</p>
<p><strong>Minimize meetings.</strong> Look, I know some things make sense to discuss face to face, or voice to voice. But if they don’t, then you don’t need a meeting. An email or instant message will suffice.</p>
<p><strong>Finally, use the <a href="https://en.wikipedia.org/wiki/Time_management#The_Eisenhower_Method">The Eisenhower Method</a> to categorize your tasks.</strong> Imagine a grid of 4 quadrants:</p>
<ul>
<li>Important and Urgent</li>
<li>Important and Not Urgent</li>
<li>Not Important and Urgent</li>
<li>Not Important and Not Urgent</li>
</ul>
<p>Important and Urgent tasks have to be dealt with. For me, these are usually major production issues.</p>
<p>Important and Not Urgent tasks should absorb the bulk of your time. For me, this is the plain old development work of implementing features, fixing bugs, and making existing code more maintainable and performant. Also included are building relationships with others and planning ahead.</p>
<p>Not Important and Urgent tasks are nasty attention thieves. They shout out to you in immediacy, but offer little value in return. You know what these are for you. For me, these are most often lazily asked questions, where the asker did not do their due diligence, and expects a top-notch answer immediately. Also included are last-minute meetings, and over-talkative coworkers.</p>
<p>Not Important and Not Urgent tasks are usually not known to your users. Take internal documentation updates as an example. Thing is, they’re an investment in yourself, which means a more productive future “you”. So don’t forget to show them some love in your spare moments.</p>
<p><strong>Further reading.</strong> If you don’t know who Cal Newport is, you’re missing out. He has a whole blog dedicated to this type of thing, and has written books such as <em>Deep Work</em> and <em>Digital Minimalism</em>. Here are some of my favorite articles by him:</p>
<ul>
<li><a href="https://www.calnewport.com/blog/2009/02/04/have-we-lost-our-tolerance-for-a-little-boredom/">Have We Lost Our Tolerance For a Little Boredom?</a></li>
<li><a href="https://www.calnewport.com/blog/2010/06/10/is-allowing-your-child-to-study-while-on-facebook-morally-equivalent-to-drinking-while-pregnant/">Is Allowing Your Child to Study While on Facebook Morally Irresponsible?</a></li>
<li><a href="https://www.calnewport.com/blog/2008/04/07/monday-master-class-how-to-reduce-stress-and-get-more-done-by-building-an-autopilot-schedule/">Monday Master Class: How to Reduce Stress and Get More Done By Building an Autopilot Schedule</a></li>
<li><a href="https://www.calnewport.com/blog/2009/11/24/are-passions-serendipitously-discovered-or-painstakingly-constructed/">Are Passions Serendipitously Discovered or Painstakingly Constructed?</a></li>
<li><a href="https://www.calnewport.com/blog/2018/06/08/jerry-seinfelds-closed-door/">Jerry Seinfeld’s Closed Door</a></li>
</ul>



  
  
  

  
  




          </div>
        </div>
      </div>
    </div></div>]]>
            </description>
            <link>https://zwbetz.com/attention-is-my-most-valuable-asset-for-productivity-as-a-software-developer/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030938</guid>
            <pubDate>Mon, 09 Nov 2020 02:39:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Windows 10 Installer Dystopia]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25030752">thread link</a>) | @brenns10
<br/>
November 8, 2020 | https://brennan.io/2020/11/08/windows-10-nightmare-edition/ | <a href="https://web.archive.org/web/*/https://brennan.io/2020/11/08/windows-10-nightmare-edition/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

  
<p><em>Stephen Brennan • 08 November 2020</em></p><p>A few days ago I had the displeasure of helping a friend reinstall Windows on
their laptop, which had previously contained Ubuntu. The reason for their switch
isn’t that important – although I helpfully suggested keeping Linux, it was
their machine and their decision. I didn’t expect the process to be particularly
difficult. After all, I work on operating systems for a living now, so I didn’t
expect any trouble. But to my surprise, I encountered a nearly dystopian
situation before I even got to the desktop.</p>

<p>I started the process by creating a bootable USB from the ISO downloaded from
Microsoft’s <a href="https://www.microsoft.com/en-us/software-download/windows10ISO">download page</a>. It feels weird writing that, but yes, the ISO
seems to be freely, easily downloaded. No product key was required to download,
or even install. The USB creation process was not easy (Microsoft suggests using
Windows to create the bootable USB, a chicken-and-egg problem if ever there was
one). It seems that the standard <code>dd</code> process used by every Linux vendor does
not work here – instead you need to get the correct magic incantations of
partition types and filesystems, and then copy files from the ISO file into the
USB. I ended up falling back to a tool called <a href="https://github.com/slacka/WoeUSB">WoeUSB</a> to do this process,
after three failed manual attempts.</p>

<p>The real fun started after I (finally) successfully booted from the USB and
started through the installation wizard. Cortana loudly greeted me, telling me
she’d walk me through the installation process using my voice. I must say that,
while I don’t really care to have a voice assistant guide me through OS
installation, I can see it helping a lot of folks out there, if it works
properly (I did not test it). I’m glad that Microsoft is at least trying this
out!</p>

<p>I went through the (impressively quick) installation process, and the laptop
automatically rebooted. It prompted me to connect to the Internet, which I
foolishly did. Directly after connecting to WiFi, the wizard asked me to login
with a Microsoft account!</p>

<p>I chuckled internally. “Classic Microsoft, asking for a silly cloud login just
to use Windows,” I thought. I don’t know my friend’s MS account login, and even
if I did I wouldn’t link their OS account to some cloud account!</p>

<p>I searched for the cancel button, but couldn’t find one. I tried to submit the
form with empty username and password, but that didn’t work. Realizing that I
might be trapped, I got my phone and fired up Google.  Surely, Microsoft
wouldn’t make it <em>impossible</em> to setup a new PC without linking it to their
cloud, right?</p>

<p>I found an <a href="https://helpdeskgeek.com/windows-10/how-to-setup-windows-10-without-a-microsoft-account/">article</a> which said that, by disabling the Internet connection I
had just configured, I could skip the login process. So, I hit the back button
on the installer. The wizard animated for a moment as if it was working, and
then showed me the same login screen. No matter how many times I hit the back
button, the wizard did not let me go back to the Internet configuration page!</p>

<p>“They haven’t got me yet,” I thought. I held down the power button and rebooted
the computer. Certainly on reboot I would restart the process, and could skip
the Internet configuration, right?</p>

<p>The laptop rebooted to a Microsoft Account login page.</p>

<p>So, I did what any self-respecting, conscientious friend would do for a friend:
<strong>I reinstalled Windows all over again.</strong>  This time, during the setup wizard
after the reboot, I skipped configuring an Internet connection. I was greeted
with this page:</p>

<p><img src="https://brennan.io/images/win10-nointernet.png" alt="win10-nointernet"></p>

<p>This, to me, felt kind of chilling. After all, it’s not like I asked not to use
a MS account. All I did was decide not to configure Internet on my first boot,
which has nothing to do with linking a MS account. After all, maybe I just don’t
have Internet access at the moment, or maybe I forgot the WiFi password.  Why
should the installer lecture me about the benefits of a MS account when simply I
did not configure WiFi? It felt obvious that this was a bald-faced statement:
“we know you’re avoiding our login process, and in a few years we’ll get rid of
this loophole too. Welcome to the future!”</p>

<p>I clicked the text (which wasn’t highlighted as a link or as a button) which
said “Continue with limited setup”. This was an odd phrasing, given that none of
the operating system features I’m familiar with (scheduling processes, providing
a unified interface to hardware devices, etc) requires a cloud account.</p>

<p>At this point, I was allowed to create a “local account” for my friend, and
finish the setup. I was presented with a list of preferences, all helpfully
enabled by default:</p>

<p><img src="https://brennan.io/images/win10-privacy.png" alt="win10-privacy"></p>

<p>The irony here is beautiful. Ads “may be less relevant to you”. The only entity
this harms is Microsoft, being able to avertise at you less (within your very
<em>operating system</em>, no less). Why should they bill this as a negative?</p>

<p>After disabling all of the toggles, the desktop loaded for the first time, I
noticed the following at the bottom right:</p>

<p><img src="https://brennan.io/images/win10-edge.png" alt="win10-edge"></p>

<p>I used MS Edge to install Firefox, and closed it out. On reboot, the login
screen contained two advertisements (!!!) for MS Edge. I returned the laptop to
my friend, grateful I didn’t have to use this horror show of an operating
system.</p>

<h2 id="why-does-this-even-matter">Why does this even matter?</h2>

<p>I spend my workday working on operating systems. Don’t get me wrong, I’m new to
the field, and I have a lot to learn. But as far as I know, <strong>there is no
feature in a modern operating system which requires a cloud account login.</strong> (I
would love to be educated if this claim is false, please get in touch!)</p>

<p>I used to spend my career working on machine learning and data analysis. One
thing I remember from my “past life” is that <strong>there’s nothing better than
linking different types of identifiers together.</strong> If Microsoft can track you by
your “Windows installation ID” and also by your “Microsoft Account”, then <em>of
course</em> they want to link those two identifiers together.</p>

<p>More links means more data about you. What applications you run, what sites you
visit, etc. An operating system as at the root of what you trust when you use a
computer. Do you use online banking? Your operating system can read the password
to your bank account, the balances, and more, directly out of memory! I’m not
suggesting that Windows does that – I just want to illustrate the sort of trust
you implicitly use every time you login to your bank account on Windows (or Mac
OS for that matter). But maybe Microsoft just looks at how frequently you login
to your computer, or what sites you’re interested in. What DNS queries does your
OS resolve? What IP addresses have you used in the last 90 days?</p>

<p>All of the data which is obvious to your operating system, can be linked to your
personal identity when you connect it to a cloud account. Don’t get me wrong,
even if you don’t connect it to a cloud account, you still are getting
incredible amounts of telemetry and tracking recording your every move. But why
would you voluntarily give more links and data to Microsoft?</p>

<p>I don’t think most people understand the sort of data they’re giving over to
Microsoft when they login and use Windows. These dark patterns that Microsoft
employs are sickeningly obvious, and really difficult to avoid. Why would I
trust a company that tries to manipulate its customers into such total data
collection, to be responsible with the data it receives?</p>

<p>I can’t imagine how frustrating it must be to be an operating system developer
at Microsoft. I have a lot of respect for the operating system kernel they make.
It seems to be one of the few major non-Unix like kernels out there. It seems
fascinating and I’d love to learn more about it. But it must be frustrating to
see the product of your hard work go out packaged with software capable of
collecting and tracking your users’ every move, and thrown together with an
installer intent on forcing them to submit to this data collection.</p>



<hr>



  
  

  </div></div>]]>
            </description>
            <link>https://brennan.io/2020/11/08/windows-10-nightmare-edition/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030752</guid>
            <pubDate>Mon, 09 Nov 2020 02:02:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Observations from listening and producing 350 startup podcasts]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25030613">thread link</a>) | @JollyMerchant
<br/>
November 8, 2020 | https://viralwegrow.com/blog/observations-from-over-350-startup-podcasts/ | <a href="https://web.archive.org/web/*/https://viralwegrow.com/blog/observations-from-over-350-startup-podcasts/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

        




<main id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://viralwegrow.com/blog/content/images/size/w300/2020/11/istockphoto-165518488-170667a-1.jpg 300w,
                            https://viralwegrow.com/blog/content/images/size/w600/2020/11/istockphoto-165518488-170667a-1.jpg 600w,
                            https://viralwegrow.com/blog/content/images/size/w1000/2020/11/istockphoto-165518488-170667a-1.jpg 1000w,
                            https://viralwegrow.com/blog/content/images/size/w2000/2020/11/istockphoto-165518488-170667a-1.jpg 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://viralwegrow.com/blog/content/images/size/w2000/2020/11/istockphoto-165518488-170667a-1.jpg" alt="Observations from listening and producing 350+ startup podcasts">
            </figure>

            <section>
                <div>
                    <div><p>After listening to over 100 Nathan Latka Podcasts, observing 200 Indiehacker podcasts and watching 50 Microconf talks AND recording 80 episodes myself.</p><p>Here's what I've observed:</p><p><strong>SEO</strong><br>Over 70% of founders credited SEO for being their best source of growth. Invest into ASAP and build up that MOAT.</p><p><strong>FB / Social Media</strong><br>Its a hit or miss. Don't waste time curating the perfect Ad or post. Tim Doyle of Eucalyptus shared that his most profitable Ad was not some high quality video but rather a Doge meme.</p><p><strong>Velocity is everything</strong><br>When building companies you NEED to move fast, there is no alternative to it.</p><p><strong>Product</strong><br>Product led growth is the BEST type, its natural and doesn't feel forced.</p><p><strong>Distribution</strong><br>This is probably just as much if not more important than the content or product itself. Build with distribution in mind, articles / SEO or products.</p><p><strong>SLC</strong><br>NO ONE wants to use an MVP - stop using that mindset. Literally no one other than your Mum/Dad will use a "minimum" "viable" thing you build. Its 2020, the whole patchy product cycle doesn't exist. Aim for Simple, Loveable and Complete (or what I call Minimal Product for Impact) Introduce simple features that meet your north star and make sure they're complete.</p><p><strong>Cold Email</strong><br>Learn to master this, its a great skillset to have in building a company and distributing content.</p><p><strong>Feature Validation</strong><br>Before you build a feature, literally build a simple landing page, run some GAds to it for lifetime deals (this doesn't work always, but for some apps).</p><p><strong>Communicate</strong><br>Build every channel possible to communicate with your user as often as possible for as long as possible.</p><p><strong>No CC No Bueno</strong><br>UNTIL someone gives you their CC, you don't have validation. Do not take anything else as validation other than their CC.</p><p><strong>Timeframe</strong><br>Before you start, set a goal to hit, if you don't hit that goal, be quick to reflect. Build more or move on Last but the MOST important.</p><p><strong>Audience</strong><br>Almost 90% of all the "Super successful" founders attributed having a previous built audience as their reason for success. They built this audience through, podcasts, blogs, Youtube, Tiktok or even Newsletters. BUILD. AN. AUDIENCE. You have a higher chance of succeeding with a shit product and a large audience than vice versa For those that are keen.</p><p>All the best peeps!</p></div><p>-Vaibhav<br></p><blockquote><em><strong>Vaibhav</strong></em> has built several startups into Million Dollar businesses serving Millions of customers across the globe via five2one. He's commonly found on stage talking about AI/ML or Product engineering whilst building his 2nd startup cenario.</blockquote>
                </div>
            </section>

                <section>
    <h3>Subscribe to ViralWeGrow</h3>
    <p>Gain a personal advantage with our weekly insights and analysis into the startup hacks world.</p>
    <form data-members-form="subscribe">
        
        <p><strong>Great!</strong> Check your inbox and click the link to confirm your subscription.
        </p>
        <p>
            Please enter a valid email address!
        </p>
    </form>
</section>

        </article>

    </div>
</main>






        

    </div><p><span></span>
        Could not sign up! Invalid sign up link.
    </p></div>]]>
            </description>
            <link>https://viralwegrow.com/blog/observations-from-over-350-startup-podcasts/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030613</guid>
            <pubDate>Mon, 09 Nov 2020 01:33:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to easily understand Flexbox CSS – (Part 1)]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25030527">thread link</a>) | @alanmontgomery
<br/>
November 8, 2020 | https://blog.alanmontgomery.co.uk/how-to-easily-understand-flexbox-css-part-1 | <a href="https://web.archive.org/web/*/https://blog.alanmontgomery.co.uk/how-to-easily-understand-flexbox-css-part-1">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1604884175281/Wd5pu-Bw3.png?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div><div><div><p>Subscribe to <!-- -->my<!-- --> newsletter and never miss <!-- -->my<!-- --> upcoming articles</p></div></div></div><div itemprop="text"><p>In this mini-series, I will be taking you through how to use flexbox CSS, in a simple, easy to understand way. You can really empower your projects and websites with the use of flexbox CSS and reduce your lines of code drastically. I have also released a video-based tutorial series over on my <a target="_blank" href="https://bit.ly/alanmontgomerycoding">YouTube</a> so if you'd prefer a video, check that below! Ok... Let's get into it.</p>


<p>Flexbox CSS is a one dimensional layout model/method used for laying out items (HTML elements) in a row or column fashion (horizontal or vertical).</p>
<h2 id="why-flexbox-css">Why Flexbox CSS?</h2>
<p>Long gone are the days of using <code>float</code> and <code>position</code> in CSS to create layouts and responsive layouts. Utilising flexbox allows you to create fully <strong>responsive</strong>, mobile first layouts without needing to write lots of different media queries.</p>
<h2 id="display">Display</h2>
<p>The most important CSS property in flexbox is the <strong>display</strong> property. This is how we define a "flexbox container". Familiar display properties include <em>block</em>, <em>inline</em>, <em>inline-block</em> etc etc. Flex is no different, it can be defined like this:</p>
<pre><code><span>.container</span> {

    <span>display</span>: flex;
}
</code></pre>
<h2 id="container">Container</h2>
<pre><code><span>&lt;<span>div</span> <span>class</span>=<span>"container"</span>&gt;</span>
    <span>&lt;<span>div</span> <span>class</span>=<span>"item"</span>&gt;</span><span>&lt;/<span>div</span>&gt;</span>
    <span>&lt;<span>div</span> <span>class</span>=<span>"item"</span>&gt;</span><span>&lt;/<span>div</span>&gt;</span>
    <span>&lt;<span>div</span> <span>class</span>=<span>"item"</span>&gt;</span><span>&lt;/<span>div</span>&gt;</span>
<span>&lt;/<span>div</span>&gt;</span>
</code></pre>
<p>Our flex container is typically a block level element, usually like a div for example. Refer to the below image; I have defined a flex container and inside my flex container, I have three more divs.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1604881496142/fnYjCju9S.png?auto=format&amp;q=60" alt="Flexbox container / Flex container"></p>
<h2 id="items">Items</h2>
<p>These are called flex items. Each child inside a flex container is referred to as an "item". You can style these individually again, using flexbox CSS if wanted and we'll get into the specifics of this in the next part.
<img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1604881547911/oHOA4hd7z.png?auto=format&amp;q=60" alt="Flexbox items / Flex items"></p>
<h2 id="flex-direction">Flex Direction</h2>
<pre><code><span>.container</span> {

    <span>display</span>: flex;
    <span>flex-direction</span>:
}
</code></pre>
<p>The next important CSS property in terms of flexbox is the <code>flex-direction</code> property. This allows you to specify which direction you want to place the items inside the flex container. There are four values which can be used for the <code>flex-direction</code> property, either in a row or a column (<em>e.g. horizontally or vertically</em>).</p>
<h3 id="row">Row</h3>
<pre><code><span>.container</span> {

    <span>display</span>: flex;
    <span>flex-direction</span>: row;
}
</code></pre>
<p>By default, a flex container will automatically have the flex direction of row. Items inside a row direction flex container will be displayed <em>left to right</em> (LTR), your first item will be the first placed element on the display.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1604881578319/WinXtcAhZ.png?auto=format&amp;q=60" alt="Flex direction row"></p>
<h3 id="row-reverse">Row Reverse</h3>
<pre><code><span>.container</span> {

    <span>display</span>: flex;
    <span>flex-direction</span>: row-reverse;
}
</code></pre>
<p>The <code>row-reverse</code> value for <code>flex-direction</code> is similar to the <code>row</code> value, however the items inside a row-reverse direction flex container will be displayed <em>right to left</em> (RTL), so your first item will now become the last for example.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1604881606985/r6yklzUvS.png?auto=format&amp;q=60" alt="Flex direction row-reverse"></p>
<h3 id="column-and-column-reverse">Column and Column Reverse</h3>
<pre><code><span>.container</span> {

    <span>display</span>: flex;
    <span>flex-direction</span>: column | column-reverse;
}
</code></pre>
<p>The next type of <code>flex-direction</code> you can specify is in the vertical axis in the form of columns. First of all with the <code>column</code> value, similar to the <code>row</code> value but placed <em>top to bottom</em> (TTB). Alternatively you can use the <code>column-reverse</code> value which again, is similar to <code>row-reverse</code> but displayed <em>bottom to top</em> (BTT).</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1604881638561/5U3hUwFeJ.png?auto=format&amp;q=60" alt="Flex direction column and column reverse"></p>
<p>Hope you enjoyed part 1 of my easy to understand flexbox CSS tutorial. It really is that simple to get started using it. In the next part we'll look at some more specific properties we can use to <code>justify-content</code> to space out flex items inside our flex containers!</p>
<p>Please leave a comment and a reaction to this post if you enjoyed it! Also, I have a <strong>YouTube</strong> channel where I'm posting lots of coding tutorials, tips, tricks, reviews and more, would love to see you there:</p>

<p>See you in the next part!</p>
</div></div></section></div></div>]]>
            </description>
            <link>https://blog.alanmontgomery.co.uk/how-to-easily-understand-flexbox-css-part-1</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030527</guid>
            <pubDate>Mon, 09 Nov 2020 01:12:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Big-O]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25030390">thread link</a>) | @dleskosky
<br/>
November 8, 2020 | https://www.danielleskosky.com/big-o/ | <a href="https://web.archive.org/web/*/https://www.danielleskosky.com/big-o/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://www.danielleskosky.com/big-o/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030390</guid>
            <pubDate>Mon, 09 Nov 2020 00:35:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Mapping the Underground: Supervised Discovery of Cybercrime Supply Chains [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25030329">thread link</a>) | @stjo
<br/>
November 8, 2020 | https://damonmccoy.com/papers/ecrime2019.pdf | <a href="https://web.archive.org/web/*/https://damonmccoy.com/papers/ecrime2019.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>Å”•PíTÃjÜ¿Ì9‚tœ†äWŸë_ûuVÓ#?‹ÒUY'ü{Ä^5t3},5Ý©Å�Â�YÜQŒµ‹;ðR(‘îh¡‡Y]çãS„i‹Ãq5èç´®ÞáîÀæŸ‰ÌpÞàüó_°ZNöÁ
¸xy5~­uH­�ÔÜêU[-x»ºÙú–e©–0ø‘
[¤3N=V´\ûuíjŽGw±t ð[ÏR!ÀKöžÁç÷÷N°�„Ä:Ám9-·¢‘ß´Z¤¾™Õ·Þiê5
«4w¦-&nbsp;.ÞR*Abd…âH@‘òWx­¶¥‹¿ÙB	KDÐ!|GD«e÷)
î2BæÆi
&gt;R 4&nbsp;†Þ·E»Øµ‡½PŠmÀ&nbsp;#eHÓàë-íHÊðjR_dW�]�Ï³²˜ä§](©‚EJ÷<wtÓ€ )!v‘¢_«¨—�œd“�Íi="">Œ‹Ëºø”wÊGH�n	Ñ" S­¡=ÄQUW×Ù(£ì)+³áõ¤èÂö$ü”¤ÊÆ!iÊ—Û#¼Ì&amp;õÐ€0.Àò¤ÅŠò¼—.ÕÀ©‚T'˜“jò‘0ÚQžM®ÆÝàI	nã¨aáe©`AlY§mcÍºq)áÛƒ?‹R	±š¯»v…“v‰ñÀ†ÐÙuqIfPÖq*;Í1�¯
ÂØÓE5¬É±äŸ‘T–m
&gt;/È:…-âS1©5ºl8¬NÉ¯¸ú"/bÌ±Í¶A{h§ù _Ó&gt;8-àòòÓ¢î¸°%)oTÈ¾iGBHe&lt;Ø]\—&lt;¢C¡¢@*\Naª�øZ˜˜„K°µ R›‡)eòT× *kGß‡a•žk$`T¤Ò;E6KDE#~¨u“ðY%°(sb]J§TC£j�I¯3&lt;+Zeâ·mm\Œò““.µdh´])–ìÉ(Ú®”ÃHÚàûýýO[¿õ_½Ýÿ£Ÿ:IŽÈ®f�Í‚¦G[‚{}VU°~ ”]
+pônÀXƒM‰O`¤w3Æy.ÛÕ›¥À�ð=ðýM½©eÜ„G²bSý»S3ðÆ £ƒýx`CV‘{-Û }“ :|œVÆ;`Ó§dþÛÀRÅFè$Ó&gt;eíòg��s&gt;È.;çîV;®‰ü¤H˜
’-z_.à×Î”¾ì	C‰ÊÐ[MN/
¶Ñ$E�u°„n÷j&lt;Îkb%~ÿ4/ëâì!ëp¿“?Aø÷v	¢„3^äú5I¯¢Ø*µe	�î"½Í�Ýy˜Ÿ`]Škl�Ò»U*KµÇ¯7Õäl�k§×p^®Ö˜ò‘»v·ÛJ�6z«ýõÆˆü‚Äf5áË�´
’ÄÒàŽøðaÊÓèmrƒ?º¨FÙ„¼—ßzµÿnKƒhgyú«#î{£ƒ¸@…šŸ‘Î€Qê/&lt;œZìYq~5^ú´.K�*GÚ‹©Ëµ‚{O›BR&nbsp;ÿþ<mƒa8:,#è-�‘<�sˆ «¾¾Ùç!Ù%�°±k="" r‚ú="" ìû“ÅÏ‹y»jÌ6v.ÆÕ¨¸áÙauú­nØì§w¬h£ÿ±ÈÒ†9ób="">ûËÖ{[�GÙ�õÖyÇó�m&lt;ï½Ù�zoþ–þ¾§'o˜ÄM›Õã«¼Us_÷l’'§Ø;-œ¼Íÿ|s/ûGÐ^&gt;äåiVÖÔd’^~LÝ"þª û+Ï¡§êëÍ×
5�à¨ú­,Ð0g3<ulš²-Ûbhag·=©óÑ~yv%«þy.Í»1Ÿ÷™êy–}Í6¶o«“t_»º¼æ#´À¼dòé¨úu÷ »\bÆß“ªhþÝy}Ög\ÚÅrÐ›Úzuc½÷i}@ì€Ñtî‰6¬9ÆÆ¶e="" �w‰`àÈÌ‚žhkg�à?4¾="">fŽŽþ ƒVb…˜Aow›�46Øg�é9ýª›3/î–ˆŽš#¨Rë`iO/¡#!Ò	:Õl½£b(ž¥šüƒ¨iÉÞk�¿¨€³Ë×yq~QCM
´?BŸ³}Q§„_‹€§·7ÌÎ'ÌMg{ù²úŒ©6-±fK]6áÉAMÓ‘+A§æîƒž•¨Ï^1ä9'Âƒ·Ù(¿SÖûu6,Ûåù0'™ô!õß™×„c¶˜&nbsp;±ì†¬žË
¥xì%Þj¬0ÖK¹n¨¹ÕíqCÍ½¡âñ"•\©ÿb÷¿Î‡ŸòºÀ—wÅ/ÛÆ)*FyÕƒz0º5ý`ZÓiCk¾ íƒap­1¸ocøÆ~ð­ç^�³}cîÐzîÈZcî'Šþ&gt;®ý�¦”4¦B.uo�Eß/©Ù1(×T�RDäL5šxýEl8`Z64dXP-Ñd~‹+ñ£[Øh-D¢+SBö¬)YBÉ&nbsp;
¥ÓzÁ-šYÜ£W&nbsp;,ÎGCŒÇÌšÊt&nbsp;AÇÐì°Ùžýÿ—bÏ‰µÒ'2 GB_ê±™õ<o¨n€ÇøÐ0¨ç¾xÝi±.üo-Öt[lœ¹‹;—ùôáq~µmxt7œØl¹Í¸5];¼&h^Ã%s?ÕðËóà%ntþ2ì}5¼=a®_z ófÌøª$}s’af`ª´b="" Žáæ�|yÐð^Ðr«›é="">í†‘@ÏËt_Eƒ0~žî#!º‘ïG+¸£¬žmš ècèÿüè�Q U"‰S’Û0EÑÌúo6*½3Û§ÿT²Ö`“=ïz†zôzÆÓ/QþøKT?þõ�¿Dó�–øäþTÙÇõ§O¿@÷ü/)cpendstream
endobj
180 0 obj
&lt;&lt; /Filter /FlateDecode /S 186 /Length 183 &gt;&gt;
stream
xœc```b`ëd`e`šÁ Æ`620°Xèp09°1p8±L`°~°§œ!¶Œ�çsÜÓsF\azÉæ
,-s\.oìÇtp'GEekxˆ‰‹ëškS¢®wº®Nït]džÑ@|}J8P°ÂõzKxKø0$Å|&nbsp;Ë||Í%�4'ó��}Ž�Ÿ�ÁeÂëƒçüs¾”�,Q˜¤p5îäåÏèäÞHw
endstream
endobj
181 0 obj
&lt;&lt; /Annots [ 191 0 R 192 0 R 193 0 R 194 0 R 195 0 R 196 0 R 197 0 R 198 0 R 199 0 R 200 0 R ] /Contents 182 0 R /MediaBox [ 0 0 612 792 ] /Parent 387 0 R /Resources 203 0 R /Type /Page &gt;&gt;
endobj
182 0 obj
&lt;&lt; /Filter /FlateDecode /Length 4519 &gt;&gt;
stream
xÚ¥Z[“ãÆ­~÷¯˜‡ã*Í©Í»È&lt;ÙY¯ÏÙT¼veÇq¥Ö~è![3©ðâñì¯&gt;MRÎ&amp;©¼Hhô�&gt;&nbsp;éßnü›ÿûÂ¿úÿãý_}DþMyyàG7÷û›8óò4¾Ù…¾—Ñÿ}yóqó½9Ÿ«æp»�âx3-ˆhóSSÚî6È6‡®›òRýa&lt;Ûî·ª·¥´ú¶ê‹ö6H6¿áÇvÏÒ¬Ýßþzÿ§› L¼ Jo¶áÎËòT¦{óü`»¢«N˜(ñ1ä¹~úÍÑTM�¾²òÀ÷ò$Ë±òmšyqº»ÙF�e¡ŒõÓW�FúþñhjÛ™V{ûþÍÎËw~€Î»ÈÃÝ
É!É¥ç/¾­Î{»(¤Yæ¦w2þ÷æ÷'[×Rø¦®Ì¹ëªí×æ£’—ñ7á;šCEtO¦ªUÐkû½(YÎöüÊTaº:ÕSº¹¾Ùwí§µ9Rß‹‚l9É§ÿl’oÍ©mT–ÅÚŒ?-ór3‰¯ŠŽÍC/�I	‚Ðs:°lx““~‡)†ÉèÈ±Œ4PÅ{o!¿'YÁßnópÓv�Rú©©~ƒ¶û›¾²[Œy­Êw;ZYwBä3Mø¼¶¬�·‹ó‹u]MÎ¢$Âœºª&lt;&nbsp;Ö®­š–¥ÿÙº&gt;ý{ëz×¶kÌPµ�QµÓžÎ#qUkŠÊ6…^ãwM?TU®­r¦©!\f2AÇ·÷áøuƒƒy=[Žª+'3_°•j\
ïâV|]Ô^aNž)¼ñQ[õÐê¯«¢¯&lt;29�,Ãš•àù6M6ËéŠBôñr.ÚÉ«Š­Å70£°Ÿ´52m¬ïdêrÝØ7ýÐ™bp¢Hí#º)Aâì2KliûÁ4%[á$JÅ
'ÑnÓÛŒ�q/|¨Øg¶Ì-ñ{WÕX[’Uæk“NÐ‘oq‘L4´Ô,6d¾÷·aBª†‘¿Vƒªe¯êµ®‹
*€‚ö–‚Ýïm1TÝo¢¢Òš33¨PïñÚ|Y•ÞÝÿg©?Yõ Ô¶Ã½¤S	éï`2èKe’Jo…ìÕi„y´)Äi0¿;ÞÍ)ÕáFÊÿ+!´á@»Ü.W¤r*h¥ã‰§Œ#èa3bç&nbsp;i³²÷ÁÆÏÐ"+´žÈ¹í™ÁråìplK¡‡£„ª—;6Û¯¬ädŠcÕðíÊ©ƒéÐ¥G�úR©:tæ|Ü&gt;ñÉÄ0t]Ÿ{J8x´·{2”!nì Þ6ßØß'-½:'Þ8ßMâ½—]ž´æJg¸…ˆh&lt;õRžm~;©y1¦�åê1P+x‹$¦­Ö‡¶«†ãIŠ…Q~UBµöÏRŠ¢/A"pâðK!ääB7Í&nbsp;cM€ãjO4_ÅÞ*&nbsp;“ø{Ë&amp;/Ü¼muÕ…¯Glþ2ö}e´õ,�;é£zw¾¼)$Š$×lú£\œîËÓ¨Nª]sïíg]p!i²l=P¥¨I‡\Ë½üÁ—BðÐ#ùòN8¯®uço¼1ñé¦»x¡t(a&amp;üóJÒ]2&gt;˜N¡@—–0Œå`•3ÂÖÏW’˜og#š�-‘àìðd-T‰­R*ÇbÐjÙ ¬ö¨bS
Ž^FêfdÓÂ&gt;·”ÊÔõóÊIŒ½Ý�5,H.�Íväë�œêÈŠL5,ÔTu]Õ&nbsp;í�”Ùï£šý¾´ïíŠ.®W˜F›ïœ9Ó˜.´9�ë[²ËwÂhÇNšéU‡‰@q2Ÿé|àÓê`üè4¤\Òõ—«hGˆ.ŽU¾q²¡S´ÏÂ«
ï^L("
[›ŠÜXÕg8b¥U'%Rr²¬†ñwkWä/Ñ	éÓ~lX
DÀÄ`Áíç#¸ÍBVêˆ—Ž[öƒÑwï”öw¥hs/D'Ø¯ypŠ¡âðÂƒ°äwp†r£1Â›Û¨Ì÷f;v'4üŸMsÉöKÕ�¢»b·IF1Î¥‡¯×Kñ›¦^š Ê
(èt3dßpe·	àÒÜk£$^J”nã=Ý¾�ö½MÉœÑá‡›˜ó-s~búÍ¢Í;¦¥ÍûUÈGP=ÌcB|ˆw×±ßÖô[³ý€`'žÑ†�Ø]*)‹ñB2c]WÁ¸1·•º¥ˆ6AîIÿYlz¤A@±C"3’
ôÇ¤ƒ;]©yÝÿáYëÛÓ©-«¡ú$ÚK¬²b5ÚÛÎÂg€u6Ý&nbsp;Ù¢Ò¿&amp;'¶œ|0Å#ã�`ó1Œõ„ü«à+%C�¯-QøÀùðCóyQÅï„kä¯?›ÓÉvk“b™YÈ2‹³¡RÈ·�xFþžØ&gt;K½´#te]+»Ä†'C½)£¬T¡BÄDrŠC·¶Œ¢³lN"òÌýÐv–âKHå'û@!�eGc§�©$!LÍ„8¶=Ð&nbsp;6ïXvšùJ¿”í	–�–ÉñesÀgvRAr©køTmlŠÂžiÜ65Ã0ðÏæ™=ïêNÙ°‘öcxF„OÀ ¸3ÒÎO¬ÁTÝ‹Ò{RúùH[å–/mpV&amp;t‡Ïx“QQ…0Î=ÙýBqèµG\&amp;Z=Ð•&nbsp;U’VOtº<a”o�È_vŸØ¹`3�t´,5bpfŸr1˜þÑ“ª7w·‘ûíÉ¯mÜyöy@2�ü�—i*a^Ür+cîÙh=6ŠkÎûaÐ³5º�Ù™Ò¾²>—þX�!¶0v8‚$&amp;'@¬=ß©ÒeÕG¥`2ª”ON·9(‡�5™&nbsp;˜‡¶-û5­VÐé;5ê¥TifE³8ý"©¦`¤¬ŒVß6‚æÓùØ{^išÏ€�hØS�(ø7Z³”6º²�6_B
”‡ixþkV[‚^Zs¦�(Ëp«\¼6-'&gt;
ýM[Kö±œj¸ƒ€b8ý’ƒ®×“Võ÷„ÿcWµ«†”M$§Šâ$ÚM/°˜®¥þO
«á
Ö+îâ$šÌQ+ÓFlg·kgÞ�5â•(Þ`Í�’ýhÒ &amp;3iAEb+À•u�[´c]
Ó6¤ùVØV¯»‹rMÕKu£&lt;ñÉ³&lt;[â‡Pxp�'£ƒ1ç�q:šõvžˆáªk�á¾ã`hè½f÷†±d³•Ä©Þ§sÝ
âLJ.í
š�œXöRÍI”8uÖlv5…rmò®LJì0Œ˜Iš—%)‰"ùõNj?†Ñ¯BI8Jµª/Âí['%ì&nbsp;0lhÂÊüVpÆ©eï»K×O­VãÂÜ†)‚ªw¼gÜ@6öLnžî™x`çvmî–ú&nbsp;K,LJ†×BHQ#¡yŽ,I!Ä€ª�Ã4ÿ©!#&lt;Œ°5çßî„/¸ª÷ŒôX£Íc#¬my°Â",qêÅ	ÅÑdôAWú�‘¦@P4k;¾TÅ$Ÿ²@Iê;çö7óòEã´K)¹WkºâÈ@]FV34ÑÇ¶X›W2”¼µ9¥¬R“;	…E2ô$Nëa{j
¡„‘·À¤Vº|F4k˜•…þ"¼•²¤$OÌ}&gt;[Í&gt;´ãá¨ì©WßÖå‹ZÕq2;­4uC¨³¦2L3H¶¿,8÷‚w¨¼VZÏù"mYu«)Þ
}=?ˆoâ&lt;öòXŸÉÔÐØn¨ØžïB
e‰Xä%&lt;á|ï²w( {×hHª!B,"¨±¿²;éâ0È&gt;™‘ðãt&nbsp;j;ÀŸ².³aKV&lt;Ñ¿ÓF�XÕ�×û`ÆºýÔ¹:â‹	 –;|f²Tü�&gt;ª?†)[bª\ºœô*¢ûÉ‹2.‚7%Ø
Y^½=NîV	qL.a›8ÚEÞ(šw·²&gt;,@§¦„�S5»«˜ìÙ3®
y¤t—»ôü¤Çç©š³Û&lt;×<yù§õhnvbªgb³âó ­ü÷d,iful«í<÷¤gÍ''Ð!Êâi…‚;af¢8á4="" Ûxæ^éx{—ŒbÙÀµÀhzzlØ£¨mß³{–ÈÃum¦…ÛaßàµˆóÁ‘•€2Òqï8w¥Ððîu]vÓenc¢$&9œëÊq«¦$!Š·a¥(9jŒ0ÔÖå”•­òd×Ýð+ëÐ½q$�ˆdê8i¬j‰Õc="">oðmÍ‚ƒÁy~!å†e¼¦^YËFkf½[²®0œ-SëQ¶KIì—W\Šƒ{!§ûƒ^|½}gÓ®¡ÞªX•0Vó	¢ä¡kÐ²­ë©N–³�Dsº™»N*…ÒR†(�Þ®™Ó3C|þpaõ(±÷À\NÁ,íô
�¢Ú0�¦Ôœ/Öº›¨ûÌþgÍ/ŒŽ›øs@§zùjð6�Õ«&amp;æçÛŒßàÂ&lt;—BT¢Ià¨¿ÜíÖ6Z1?¢Ån°çã–‰çêÌo	/mÌ8¸ç§OŠx‘³Î#…Ûƒ*&lt;`‰&gt;ÂUµ†#IþÒy%ùldPxÅÈLëÛFút‰7ìhµf6…TâG#nÈ§@?�X±’4Œ»üÊ)ÿí¸š~šŸŠÈˆÖS²vR&amp;#b~8zÑfrØ¨êÜ“Ìêã	{M÷â`rãþÙ‡&nbsp;Ü Çž…J
žbßnƒ
´…Å£Z1¨&nbsp;~4’ÁJ!ðh´š#ym­Û0‹f%ºŸB"”.
¢´^ˆºÅP¥vtš›-ž3
¤AþE,{2EÇ
¤I¢&gt;¹IóÀ¹W&amp;é�´ôB'Ùt3À$òéX1"Åh½üëKÑÊÔ{6AùnŠPÊæ&nbsp;U¢ Äå
Eü®z™SopqŠ_ä-%"‘ê½žŸV\Íé þpëÇZô4Ò±š��¾‹E�-"›HP¥rùÃ(ùòKþGWÓrôu·vMŠ¶ÄóHD
UšÁ(©ð)â--çŸQprÎÆJ`Í6%ÜYY®7&gt;÷]²cí(F÷=A¶Û’]Ü­+ËNH"û	}úmF°s�Òüà�J§¤ÌwXóš¡AôÏÁ©hÛ«vö2�˜úêË€SSŸóúàx�ª¢0§¼ðÎˆV2'×�Ï¤…X—t¼qG?å»’ÜŽífæ4•ˆ,`Nªùº�ÚÍº*6˜?%}
1\¾…Yê²w»ÉŽ¾š¥XnaGý…åLóÏY#Tßësá"Ý‰!¦…ÿTì­ÈÀ%Ñ"�(þ5ÔôÉgR~vI£
?ªr�Ø_âQ‡wT9~æ/ÞR×®ïâe5Œv›q¨\&gt;ÚiŽ|·Ìkãº’¡‡µGª–Q±'“L­MÝ·B­ø*§5|£"Jœ›‘–”¤ ¸€ÐÊ5Žw
©&lt;‡šá”½_;Ä4ô£û(Á
Z¾–ÈxÍr.g´|Bá*±Ö/~âWžõîÔ&lt;ËŸFD-6±²’&gt;¼—nÈÊŒÃýHKZyÐ’ÚeB_5‰?ïi9þ²*ŠväÄ.:»%gÑ¬%8ÆàÐN‚“�Ä�ÑŽPðÓQO¤†-9ýëIœ8qô™O†1Ì^\’Š³ÉºŽý:ëâgù¯[²[ú¡ûœ‚Ño;¡àJ«	íªéÛCÍ¶Sßä3ÆùÞ�èÓ„›ùY±’#Ò¼õòKñTDH8.PÛ­zÂäû¶Ö÷i’Ç.ÒF7»
"ÏwŸ¬þïe‹�dJýéžÅAH°fa‘Ú³0�üõ‹�¬c`Ó9~¦Zö¹&lt;ÄÕ•\	Uˆ�Ïææ×Ùä)‚_|Jz²{DCç?L¹Üw…¶xRËN„ûÀ
4‡£kOÑî»|o2Ã\”ôUG®?Ùe®—J	‰ÐØäË@öµ»�Hê2=Â¼•Øv-æ’�Fæ#Ü®Iò³ÇÌÆ¶Ûó§–üªÜÈ‹òŒÀ…ò¿„iÙºá]|@‡6‹ïÖ.ÖµÈÆq~ù
gâ/Å±~ç3GAR�&nbsp;ÜÉó¿ó&gt;'ù2gHÑž¼6H
g4ð¯/©msQ«_à­�®:íÄŸRü&nbsp;/–}'
»&lt;£ë¯`(ZÈ¼4ÎÜw0ù.ÛÛ]˜Û4Ê¢môU�õ?Qàù¾Î&amp;Aè¹~ˆüöíÛ‹ñßÞñOjl”¹
endstream
endobj
183 0 obj
&lt;&lt; /Filter /FlateDecode /Length1 1421 /Length2 6662 /Length3 0 /Length 7636 &gt;&gt;
stream
xÚ�uT”m×.-Ž 
ÒCwwJJKwè03À303tw
ˆ4ˆÒ"RJŠ - ¢´€t‡ÿ¨ïû½ÿû�³Ö9ëYkžç¾vÜ{ßûºîá`10P† ì¡÷p´€ˆ&nbsp;°,PUÏØR(,,&amp;(,,
àà0�¡]¡Á3(CÀeÿ—ƒ*
Bc05ã§‡€µ=]�"b@IY)Yaa&nbsp;¨°°ÌßŽ¤,P
äƒõ�Ú8àPE¸û"aŽNhÌ6¹Á&lt;@)þßá@e7(Á�z ´Ô
³#ä
4F€aP´ï¿RpË;¡Ñî²BBÞÞÞ‚ 7” é¨ÈÃô†¡�€FPé…5¼rƒþéLÀ4q‚¡þàÆ´7	bW
Ga"&lt;á(ˆÙh¬¥Ôw‡Âÿ8ëþqàþu6@A‘ÿ¤û+úW"üw0F¸¹ƒà¾0¸#Ðæ
êßÓDû&nbsp;ù� 8ä—#È…ÀÄƒ¼@0W�=Æáwå à=eC Óà_í¡ÀH˜;%ˆ‚¹þjQèWÌ)«Ã!ª77(�üªO
†„‚1Çî+ôg².p„7Üÿ¯…qøÕÄÓ]Èóð„j©ýå‚�ÿ`ŽP4PBXFRR\õB}ÀNB¿Ò›øºCE~Á˜ýÝî@LÐ@˜óø£@^P é	
ôÿß†¯""@ŒÚCapÀ?Ù10ÔáÏ3|$Ìh-ŒážPø×óŸ/[½ ¸«ï?î¿ç+d¦fn&nbsp;lÂ÷§ãÿØTT&gt;@1a&nbsp;€Œ„PDDZ(%%üwì¯2„ÿ‰Õ‚; €2ªÅÓß{ýEî¿ÄÁüw®ûk¡@îHn#,!ÆüˆüSýwÈÿ�á¿²ü¿HþßÝótuýmæþmÿ?Ì 7˜«ï_Òz¢1ÐC`dÿoWsèÑêA!0O·ÿ¶j¡A!(Ã1dÿƒÃP÷`&gt;Pˆ
vúC™?¸é/©¹ÂàP
öënÁD	ÿ—
£/°æþ@axùÛÅÈçßûªÃÁÈ/�‰JHAH$È Œ¡“(fÞþ"AB&nbsp;&gt;¿™„#Ð˜ ¦Ç@&nbsp;	ø5V1&nbsp;…é†rÁLÁé—ñ7."*‚€1Çñ&amp;ú7±wý
ÿ«°'‰ÑäoÊ`
ý{ýû€B}&nbsp;`À§)X.Â¹&gt;¢í´V™Þ[`åÁ—¯±É–½1hÎéçþNº7²5Æ=TBªîfÌ”E�MÐóþ8ö±y›º�®ù�¥Þ»¤²+�?pvðÖÊAÐ9ŽHeÄ.‘í&lt;6d$yh�gÍõÄ|ê2RªKl­zµèªµf%³	t“²¤Ó£)çulâÄ[²°E¿·´!M$GZgzšh™Ž\áÄÃOTÚhÏ|—¬³ÞÏÏ¿ËíëÍÛ/Ðé®‘¬Š¢&nbsp;¥ôÓfÉLé§¥zœ«ÝŒÂâT‘8V-KKf¥}áÅ¹€êíþd-Óòf
¢Í½ñhçñá½æ{,xJôÖ3ãÚ�@ûÝÞúyý›.¡ä"mÛ1ùà”óùí
î&amp;‚%@‡dbÀ€¸^QbÛÁ¢¸ƒÃcÅ~UBâ|:¹Ù¡ŸÖ·ÎFíÊŽ�%¶àrÙŠ�ªÏ‰9±~ÊÝ'^&gt;uG$µQñw�,&nbsp;&amp;Q5dœô­AÓ•Ÿ:�ë†h	nûÊY	Þ?ôiª K7‹Ö•w÷pc8{'¬wC
ñ|Y¿ÅvŸˆmÎ1c­&amp;âéJÀÙŒClºD¨wË\ÕœéDÂ�K*ß¦ô­·s&gt;_ùÐWb£�Ðâ5ïœó"±yâ2=øcðVºBŸÐ×	V«Fž‚4‰ëj¦Ù6ÜÀn»‹iÁw¿û�vÏÜŠx²�ô«ÙÜñÀ~ $ž“'n™&gt;å[x‘@òá�èZÉ2~qV”ÃôU~w�o°c—2yIG4·
Aî}´ß#œúÇ%ÇE}Âc@¿oš]N8$˜9ìõ…VÇá‡9%‹ö*ÊKÿÂ©=ÿZóù�ê`Ý—�Ì
'ô
Q+%?ß›ðá
ÃK&amp;"ÙrÐd‰ŸÎ–¨’pÔ7ÉË¬=wá/òáûf£NÚ82â¹Ä�WQ8Y¸î)œ)Ü[nq«R¹Ÿð­ÃÜXìR7Á�qÖ0Ö[³ÝûBªgÖ4™.¯î"ïí‚’ãÓX×Ð¶w’e…«ùå¼õä´o›2Û6†ÁÉÌø¹(Í@ùÂá{N)E½b¶r­N
ÞG×IÌ�õöGgnÙ±~%1.Ã¤tEâ²_SiÈŽ¥F¢$�	ÃvT$Û©'ch‡/?á…Ê_x|exKÇ•xBñÕ-¸&gt;ÄUÊWØ‘½á{¨œ¦é;Á3×2^Žtañq¾k.oKiG1"[»‡“DÃd!ïéÝ5ï²û†7„ôˆ1;^B�y[ÉL²’Ùz?³;oÈD~Tí1cVýbœ(,è„¼ã¬%A�pU	y–þŽ
ßHrö¤Çm2{9&nbsp;€¦2Fjã�Èß�µs:Ñ“8�k&nbsp;À�l&amp;%/ŸÐXÓ¦Ì„¯µši,õa“‡hÏ’ÕÉ{¯…îÁery�õBPxÖò·ïÜé±µEo&nbsp;
qó
=ÏüŸj–?!Ê'¯üô”¥tÍ½b-B¯œ@Âôƒ'?–b,¬ÛÚ1ÔTÉiH*hÓ²x¬s†eXb$‰¼eõ9Ží²pÓ¯ïÜ¤û�E&gt;š×—ÚVe&amp;ÿ¥ÏÅ&nbsp;p©S®»UÇ‡Sæ&lt;ð!á\IicQÒ×2¯›š!Ù"¼I8o“‘+WÑ\a_qíÚ%§FÌ‡;ÓÈ·‰m,Ýö,�î¾ðmÍfÑJ@8òSÈ¶tæŽì§~–5Õ[ãjâÈkÐ5m/{pB:ƒ±ªti³o–ƒóîO4%¬[…ù�‰XŸÈ‰/‡ºÛ­7Ýßq¿(÷PQ’“pÍÄžÎQq=Š^ø�Lf‘rÖóÈ¥1&nbsp;hUÏ0­Ï&nbsp;1dcöª¶G�^&lt;¦Ó.‰¤}½e,3~QágÇ¼ÕÏÃ8EpwlQ@¦öz;ÏWïhvkæ•�|³Ç&amp;d5§’Œ1€W½
ŸŸ=Å»Ä]ZâMú¶Bs¯ÒUGzŒî†-{¤z¡ÛGw¥ÞBß&gt;É69`o�&amp;¯<k^n¢»r,jÔ³]áðgt¤è(Í´|fû1ª$ö 7‡Â<÷Õ`ÿ—}µçeÓüœõn,}ŸÂËbffé¶="~ÚëðôM:ÄÊK/ðƒ¤Ì''{ÓUÒ|NÃÆðôÔD%Ç.–‹è|kY¯dó&quot;‡Y¢ÈXÕ–€rnlûÙ]\X­Yæª�_'ã¨ËP&nbsp;Uy°²ÆÁR<M‹óç³Ðå­åÅ¨" £ãpæšêÝwn»œiõõ}¶›ÑÅpóð°;ýàÂåœm·%y¯~n?Þ8v›®="">¾.&gt;™«³Ü{FÎÞ730W�·¯F­èýÉè2´?Ä*ô:�rEºgé­ªtN–ç4�õ¹ºäI¶WÞ]¥ŽmA§yì�s’—Z(úRb·^i“7:=ì&lt;)‹ã]áÜ×·,wñ½•¾uwˆ9Y÷˜¦öµ´ßd‹ê&lt;Œ8&amp;J¶g5x:X[éÖ?Ž™G6?üG&amp;ÈP^è
cÇõ.¹{çGm,µ2&nbsp;�ý¼¬2‘¬J¿‹&nbsp;Ii
¬8ž¬.z{nJ+Çö¥Ñr'ß»änÝLÙ›"¨›ôú0s&nbsp;îüî¥ƒ¼Iêyö½T<h®f-×±!ég›.o�égÏæÉž� {@½s"vay;="" *‚ŸÈ™–x‰xŒºÙeÞä-‰�}vù¢:c¶ËÂój|¹¢võÍ+¿¦{¬="" s­¬Ô="" ¸ÆÌÄ¾»ÃŒo:ø³f¯ù÷l#ÂèÓc="">p/á)Šx0qŠº1Äí»S‹jQ3ê\å«pÍyk„÷ÑE?lF¦j¥­R&amp;�Z_ü(?ÐuþÈ½þÃ‡Àž
Z�nÐ5æÖÇ½Sßš!m{j‹–Î]e1e�ÞˆyB¨R\„#ñ£î:K´;¹‰ŽHë{Ž6A*Ü¼êdì•Ò,ˆô½l©gÕSÚOßªCñ×Ÿ€¥R£|gè(fN’³@‹†éúÏ¸™â¯Iò}Žã]Þz@è
¥ÎÀ^{€ÒÇ4‡„¶mþ¯RªäWGˆ×i7u,F™¥î‘ÛÓq…µÁ“`¿œOþäÝæDÖ²/ÂŒJXÒ´…ûpÜÁ"à§*=f:¸f,9„&amp;U]«®á=‘–/‹IµpÈ.âc\_~÷å�½9ñrL;ô34
Ò¶)©:ºÀ�RmurþUWfâ�­¥‡âó`ZŸ<f¡¼´õ‚ ýÏÒ¹5ål¬ŠÙßÎc*Ë�êõc¸(îpêÑxæ]¾‡˜_v7±Ðçm�*ý="" g‰Á="" êób+µèèðÔxÚòhýåÙl�ÿ-ŒaÑ(³ÇrdÙˆfò“'îÏoï‰†¦="" k6};eç¼[ÜŠoé|h¹¦="" Àk:–«yø3·ª2jœÄr‘azùå±¤Ùæ§ãtzlÕùÒtya#æo‘��o(n="©=UÕ!Ý#Õ?L¸GÕ3Ï%—7�n$R/ˆÍWê&quot;Ø¬¹Z(—ÞÚ?JRXÇ6V,wŸ" s_*Šc"o™�l{ÀÆk-÷ôÛÚ®æÉùkÇªËb“ó"¦Àå±íhß¡×edü‹”dùoñÍíxz‹…s="" y†ˆááè²�h©úy×wÂoj¥­ãÃÁ@¥çÂøi!å&á7ˆÒªj¥“¯pd�g‰£Ëmsõjórgãpê="">épÝfù&gt;õp?¤óL/#Úf­¹P±ñˆNèíu›ƒÆ‘ÓôRGLr]žaH~"1yÕõg¿"ÅËÁ—Ãš�)ÙEÇÖð0‚'­¸æ
ýxsU…A(UÁñ,¥ZÓó¶:=Š¡�|Ï¾D;7¬:b©b5¹Ã­æ¢m¾”úÕÚ!pFš%ÆäM;¾ÑŒU�É3Ù˜bþð®aUÄ:gM©ïý3bX7¬GÕ�rRíœpø·/µ&nbsp;…	œä,pöÐ«3E[ïÝŽv¬UKœÝj~q3®åSWì‘0{´ïKpHh%HÃ×RYÁ#¦Ì‚0OJÇ$æîW»Oú'ÜËâ¢ÌÓp÷=õz}Ø¢P“¯ŸôZ®¤Ž¹]Œ¤Wù®2êÛ*ÝÔhºPEwË»’ÌFz#Ÿ�H¥j²Auð%ót#&nbsp;Ÿb'“H‰óUüØucÊ&lt;5Lã2—àï._›/Ö�¾¨›lËïz](¨&lt;ƒÏ"KY8Q«ò„¯²¾žäzÞ�˜8à1&nbsp;:¶\/¶ÏyUÝÿ³BÃø"qü›þ…</f¡¼´õ‚></h®f-×±!ég›.o�égïæéž�></k^n¢»r,jô³]áðgt¤è(í´|fû1ª$ö></yù§õhnvbªgb³âó></a”o�è_vÿø¹`3�t´,5bpfÿr1˜þñ“ª7w·‘ûíé¯müyöy@2�ü�—i*a^ür+cîùh=6škîûað³5º�ù™ò¾²></o¨n€çøð0¨ç¾xýi±.üo-öt[lœ¹‹;—ùôáq~µmxt7œøl¹í¸5];¼&h^ã%s?õðëóà%ntþ2ì}5¼=a®_z></ulš²-ûbhag·=©óñ~yv%«þy.í»1ÿ÷™êy–}í6¶o«“t_»º¼æ#´à¼dòé¨úu÷></mƒa8:,#è-�‘<�sˆ></wtó€></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://damonmccoy.com/papers/ecrime2019.pdf">https://damonmccoy.com/papers/ecrime2019.pdf</a></em></p>]]>
            </description>
            <link>https://damonmccoy.com/papers/ecrime2019.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030329</guid>
            <pubDate>Mon, 09 Nov 2020 00:23:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[GitHub Whoami via SSH]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25030301">thread link</a>) | @menduz
<br/>
November 8, 2020 | https://menduz.com/posts/2020.11.08 | <a href="https://web.archive.org/web/*/https://menduz.com/posts/2020.11.08">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>There is a very easy way to know know which Github user is associated with your SSH key.<!--more--> 
Executing <code>ssh -T <a href="https://menduz.com/cdn-cgi/l/email-protection" data-cfemail="0d6a64794d6a647965786f236e6260">[email&nbsp;protected]</a></code>, you will receive a greeting with your username.</p>

<div><div><pre><code>❭ ssh <span>-T</span> <a href="https://menduz.com/cdn-cgi/l/email-protection" data-cfemail="5b3c322f1b3c322f332e3975383436">[email&nbsp;protected]</a>
Hi menduz! Youve successfully authenticated, but GitHub does not provide shell access.
</code></pre></div></div>

<p>To extract the username we can run <code>sed</code> over the response.</p>



<h2 id="importing-gpg-keys">Importing GPG keys</h2>

<p>I use my YubiKey to store both my GPG and SSH. But having the SSH working out of the box is way easier than GPG, the last requires the machine to know the public key before it can be used. And it can be either downloaded from a key server, plain .asc files, or like in my case: download it from my Github profile.</p>

<p>To do so, I leverage the <code>https://github.com/{username}.gpg</code> function.</p>

<p>Since we now know the username associated with the SSH in the YubiKey (the previous step), we can get the GPG keys like this:</p>

<div><div><pre><code>❭ <span># this step may require you to touch the YubiKey</span>
❭ <span>username</span><span>=</span><span>$(</span>ssh <span>-T</span> <a href="https://menduz.com/cdn-cgi/l/email-protection" data-cfemail="80e7e9f4c0e7e9f4e8f5e2aee3efed">[email&nbsp;protected]</a> 2&gt;&amp;1 | <span>sed</span> <span>'s/^Hi //'</span> | <span>sed</span> <span>'s/\! .*//'</span><span>)</span>
❭ curl <span>--silent</span> <span>"https://github.com/</span><span>${</span><span>username</span><span>}</span><span>.gpg"</span> | gpg <span>--import</span>
</code></pre></div></div>

<p>Check it works running <code>gpg --card-status</code> and search for the email in the “General key info” section, it must match your GPG’s. If it doesn’t show up, make sure you are importing the same keys present in the card.</p>

<div><div><pre><code>❭ gpg <span>--card-status</span> | <span>grep</span> <span>"General key info"</span>
General key info..: pub  ed25519/3ABC123401923E0A 2020-11-08 Agustin Mendez &lt;<a href="https://menduz.com/cdn-cgi/l/email-protection" data-cfemail="ceb7a1bbbc8eaba3afa7a2e0ada1a3">[email&nbsp;protected]</a>&gt;
</code></pre></div></div>

<h2 id="setting-up-git">Setting up Git</h2>

<p>Now that we already have our GPG and SSH working, we must configure Git to use the GPG and the mail.</p>

<p>To do so, the email address from the GPG will be used (which is a requirement for Github).</p>

<div><div><pre><code>
<span># Read email from the --card-status</span>
<span>CARD_MAIL</span><span>=</span><span>$(</span>gpg <span>--card-status</span> | <span>grep</span> <span>-Po</span> <span>--color</span><span>=</span>never <span>"(?&lt;=&lt;).*(?=&gt;)"</span><span>)</span>

<span>if</span> <span>[[</span> <span>$?</span> <span>==</span> 0 <span>]]</span><span>;</span> <span>then
  </span><span>echo</span> <span>"&gt; Using mail: </span><span>${</span><span>CARD_MAIL</span><span>}</span><span>"</span>
  <span>CARD_NAME</span><span>=</span><span>$(</span>gpg <span>--card-status</span> | <span>grep</span> <span>-Po</span> <span>--color</span><span>=</span>never <span>"(?&lt;=[0-9]{4}-[0-9]{2}-[0-9]{2} ).*(?= &lt;</span><span>${</span><span>CARD_MAIL</span><span>}</span><span>)"</span><span>)</span>

  <span>if</span> <span>[[</span> <span>$?</span> <span>!=</span> 0 <span>]]</span><span>;</span> <span>then
    </span><span>echo</span> <span>"&gt; ! Cannot find CARD_NAME."</span>
    <span>echo</span> <span>"&gt; FAILED!"</span>
  <span>else
    </span><span>echo</span> <span>"&gt; Using name: </span><span>${</span><span>CARD_NAME</span><span>}</span><span>"</span>
    <span>KEY_ID</span><span>=</span><span>$(</span>gpg <span>--keyid-format</span> none <span>--list-key</span> <span>"</span><span>${</span><span>CARD_MAIL</span><span>}</span><span>"</span> | <span>grep</span> <span>-Po</span> <span>"[A-F0-9]{40}"</span><span>)</span>
    <span>if</span> <span>[[</span> <span>$?</span> <span>==</span> 0 <span>]]</span><span>;</span> <span>then
      </span><span>echo</span> <span>"&gt; Using key:  </span><span>${</span><span>KEY_ID</span><span>}</span><span>"</span>
      git config <span>--global</span> user.name <span>"</span><span>${</span><span>CARD_NAME</span><span>}</span><span>"</span>
      git config <span>--global</span> user.email <span>"</span><span>${</span><span>CARD_MAIL</span><span>}</span><span>"</span>
      git config <span>--global</span> commit.gpgsign <span>true
      </span>git config <span>--global</span> user.signingkey <span>"</span><span>${</span><span>KEY_ID</span><span>}</span><span>"</span>
      <span># git config --global url."ssh://<a href="https://menduz.com/cdn-cgi/l/email-protection" data-cfemail="690e001d290e001d011c0b470a0604">[email&nbsp;protected]</a>/".insteadOf "https://github.com/"</span>

      <span>echo</span> <span>"&gt; SUCCESS!"</span>
    <span>else
      </span><span>echo</span> <span>"&gt; ! Cannot find KEY_ID"</span>
      <span>echo</span> <span>"&gt; FAILED!"</span>
    <span>fi
  fi
else
  </span><span>echo</span> <span>"&gt; ! No known yubikey was detected."</span>
  <span>echo</span> <span>"&gt; FAILED!"</span>
<span>fi</span>
</code></pre></div></div>



<p>Did you know you are sending your identities every time you connect to an ssh server?</p>

<p>Be careful to not leak your keys to every place you want to connect to.</p>

<p>To do so, create an identity for each site you want to connect to and add the following lines to your <code>~/.ssh/config</code> file:</p>

<div><div><pre><code>IdentitiesOnly <span>yes

</span>Host github.com
  IdentityFile ~/.ssh/id_rsa_yubikey.pub
</code></pre></div></div>

<p>To get the public key from your SSH in the YubiKey run:</p>

<div><div><pre><code>ssh-add <span>-L</span> | <span>grep</span> <span>"cardno"</span> <span>&gt;</span> ~/.ssh/id_rsa_yubikey.pub
</code></pre></div></div>

<p>That is part of my https://menduz.com/bootstrap.sh script, used every time I set up a new machine or when I think that my machine was somehow compromised.</p>
 
  </div></div>]]>
            </description>
            <link>https://menduz.com/posts/2020.11.08</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030301</guid>
            <pubDate>Mon, 09 Nov 2020 00:17:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Howard Marks Memos [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25030208">thread link</a>) | @tosh
<br/>
November 8, 2020 | https://austenallred.com/assets/all-howard-marks-memos.pdf | <a href="https://web.archive.org/web/*/https://austenallred.com/assets/all-howard-marks-memos.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://austenallred.com/assets/all-howard-marks-memos.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030208</guid>
            <pubDate>Sun, 08 Nov 2020 23:56:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Future of File Utilities: Encryption, Hashing and Compression in the Browser]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 4 (<a href="https://news.ycombinator.com/item?id=25030125">thread link</a>) | @securemonkey
<br/>
November 8, 2020 | https://blog.secure-monkey.com/the-future-of-file-utilities-encryption-hashing-and-compression-in-the-browser/ | <a href="https://web.archive.org/web/*/https://blog.secure-monkey.com/the-future-of-file-utilities-encryption-hashing-and-compression-in-the-browser/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://blog.secure-monkey.com/the-future-of-file-utilities-encryption-hashing-and-compression-in-the-browser/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030125</guid>
            <pubDate>Sun, 08 Nov 2020 23:40:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Onboarding is not a one way street]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25030054">thread link</a>) | @bored_hacker
<br/>
November 8, 2020 | https://boredhacking.com/onboarding-is-not-a-one-way-street/ | <a href="https://web.archive.org/web/*/https://boredhacking.com/onboarding-is-not-a-one-way-street/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><header><h3><a href="https://boredhacking.com/">Bored Hacking</a></h3></header><main><p>November 08, 2020<!-- --> | <b>4<!-- --> min read</b></p><div><p>Onboarding can be a very hectic and stressful time when you are starting a new job. New coworkers to meet, new codebases to learn, new languages to learn, new technologies, new processes, and the list can go on and on. Good onboarding materials and processes can go a long way when you’re first starting out. However, it shouldn’t just be a one way street. Onboarding should be a collaborative and incremental process that every new hire(and employee) actively contributes to. Things change from onboarding class to onboarding class, especially in a fast moving startup. It is therefore not only up to the existing members but also the new hires to contribute to the onboarding experience and documentation to keep it up to date and make it better overtime. Here are some recommendations for making onboarding not only productive for you but also better for future new hires.</p>
<h2>Take Your Time to Onboard</h2>
<p>Onboarding isn’t a race to the finish line. It can feel like you need to onboard as quickly as possible and be productive right away. However, you have to remember that Rome wasn’t built in a day and this time is important to build a good foundation for the rest of your time at the company. You don’t want to jump in before you’re ready and struggle to keep up. Take your time to properly onboard, and learn as much as you can during this time. Your team will be understanding that you are onboarding and shouldn’t feel like you need to be productive from day 1. Although there is generally a set time period for new hires to still be onboarding, the first month or two generally depending on your company. Take full advantage of your expected onboarding period. When you run into something you don’t know or haven’t seen before, take the time to really dig into it and learn it. Do your own research or ask teammates about it and make sure you fully understand it. Once again, this will pay off in the future and help you compound your knowledge as you continue to learn and work at the company.</p>
<h2>Write and Update Documentation</h2>
<p>As you are onboarding hopefully there is already some documentation on how to at the very least setup local development. Depending on your company, you may also have documentation on team specific information, common tasks, etc. Find out what your company uses to store documentation(Google Drive, Github, Notion, Confluence, Dropbox Paper, etc.) and look through it for even more documents. As you go through this documentation, if there’s anything that doesn’t make sense or is outdated make sure to update the documentation. And if you realize there isn’t any documentation for something you encountered then you should write it! Anything you do while onboarding that didn’t have clear directions or documentation, feel free to write it up yourself. Even once you are done onboarding, you should continue this. Every little improvement will benefit someone else down the road and make them productive quicker, multiplying your effect and increasing overall productivity.</p>
<h2>Take Notes During Onboarding</h2>
<p>Take notes of things that went well during your onboarding and things that could have gone better. These will be important for improving the situation for the next hire(s). Take these notes to your manager and talk through the good and bad. This will help the organization understand what they are doing well and what they can do to make it better for the next hire on your team. Your manager will want to make sure they are doing everything they can to help you onboard better and improve the situation for the future.</p>
<h2>Ask Questions</h2>
<p>Asking questions and getting good answers is incredibly important during onboarding. You may get blocked often while onboarding because there are a lot of unknowns and that’s okay. But don’t wait too long to unblock yourself. One big thing to remember is, you should never hesitate to reach out to someone because they seem too busy. They may be busy but they will normally make the time to help onboard new people. Although not all onboarding buddies may be okay with constant interruptions, it is reasonable that you should try and limit their context shifting. Therefore, setup time to talk with your onboarding buddy or teammate. Setting up time can help limit this but also ensure you get your questions answered. At first this should probably be a set time daily for the first week, but as you continue to onboard it can be less often.</p>
<h2>Good Onboarding and Helping Out New Hires is a 10X Situation</h2>
<p>The 10x engineer is often talked about in tech and seen as a unicorn of sorts. For those not familiar, it was commonly thought that a 10x engineer performed 10 times as efficiently as other engineers, performing the work of 10 engineers. However, I don’t think that a 10x engineer should only be more efficient but I think 10x engineers are engineers who help 10X the learning of other engineers as well. A 10x engineer helps their whole team be more efficient by teaching and mentoring them. This applies directly to onboarding and new hires. The faster new hires onboard the faster they can be productive. Spreading information to more people makes everyone better and creates a cycle of spreading information, which can lead not only to a 10x situation but a 100x situation where multiple people within your org are increasing their knowledge and productivity by 10x. The real “unicorn” engineers are the ones who lift up the whole organization or team, and onboarding is a key component of that. Everyone should realize the importance of a good onboarding period for new hires and how it can help the organization as a whole.</p></div><hr><ul><li><a rel="prev" href="https://boredhacking.com/starting-a-new-elixir-project/">Starting a new Elixir Project</a></li><li></li></ul></main></div></div>]]>
            </description>
            <link>https://boredhacking.com/onboarding-is-not-a-one-way-street/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030054</guid>
            <pubDate>Sun, 08 Nov 2020 23:26:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[CS 2150: Program and Data Representation]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25029751">thread link</a>) | @swatson741
<br/>
November 8, 2020 | https://aaronbloomfield.github.io/pdr/readme.html | <a href="https://web.archive.org/web/*/https://aaronbloomfield.github.io/pdr/readme.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

<p><a href="#introduction">Introduction</a> | <a href="#contents">Repository contents</a> | <a href="#contributing">Contributing to this repository</a> | <a href="#description">Course description</a> | <a href="#markdown">Markdown</a> | <a href="#sourcecode">Source code</a> | <a href="#license">License</a></p>
<h2 id="introduction"><a name="introduction"></a>Introduction</h2>
<p>This repository contains the materials for the course entitled “CS 2150: Program and Data Representation” in the <a href="http://www.cs.virginia.edu/">Computer Science Department</a> at the <a href="http://www.virginia.edu/">University of Virginia</a>. It contains all of the slides, labs, exams, etc., used throughout the course. The course description is <a href="#description">below</a>. The github repository for this course is at <a href="https://github.com/uva-cs/pdr">https://github.com/uva-cs/pdr</a>. It can be viewed online at <a href="http://uva-cs.github.io/pdr/">http://uva-cs.github.io/pdr/</a>.</p>
<p>Students <em>currently</em> in the course should view the <a href="https://aaronbloomfield.github.io/pdr/uva/index.html">uva/index.html</a> (<a href="https://aaronbloomfield.github.io/pdr/uva/index.md">md</a>) file in the <strong>cloned</strong> repository (i.e., don’t try to view it on github.com); current students may also want to view the <a href="https://aaronbloomfield.github.io/pdr/uva/daily-announcements.html#/">daily announcements</a>. Note that many of the course materials are modified right before they are needed – for example, this repository will be updated right before the semester starts.</p>
<p>Students who were previously in the course may want to view the current version, or you can view the version from your semester. All semester versions are tagged with an end-of-semester tag of the form “year-semester”. For example, the spring 2014 semester was tagged as <code>2014-spring</code>. To obtain a specific tag, you can enter <code>git checkout tags/2014-spring</code> in an already cloned repository.</p>
<p>The primary authors of this repository are <a href="http://www.cs.virginia.edu/~mrf8t">Mark Floryan</a> (<a href="mailto:mrf8t@cs.virginia.edu">mrf8t@cs.virginia.edu</a>), <a href="http://www.cs.virginia.edu/~nn4pj">Rich Nguyen</a> (<a href="mailto:nn4pj@virginia.edu">nn4pj@virginia.edu</a>), and <a href="http://www.cs.virginia.edu/~asb">Aaron Bloomfield</a> (<a href="mailto:aaron@virginia.edu">aaron@virginia.edu</a>). Many students and faculty have worked on this course material over the years.</p>
<h2 id="repository-contents"><a name="contents"></a>Repository Contents</h2>
<p><strong>Note that the links below will not work correctly if you are viewing this online at github.com – you will need to clone (download) the repository first</strong></p>
<ul>
<li><a href="https://aaronbloomfield.github.io/pdr/book/index.html">book</a> (<a href="https://aaronbloomfield.github.io/pdr/book/index.md">md</a>): the beginnings of a textbook to be used for this course. It is written using LaTeX.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/docs/index.html">docs</a> (<a href="https://aaronbloomfield.github.io/pdr/docs/index.md">md</a>): a series of useful documents that are not labs or tutorials.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/uva/index.html">uva</a> (<a href="https://aaronbloomfield.github.io/pdr/uva/index.md">md</a>): the materials that are specific to CS 2150 as taught at the University of Virginia, such as daily announcements, due dates, etc.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/exams/index.html">exams</a> (<a href="https://aaronbloomfield.github.io/pdr/exams/index.md">md</a>): past exams for the course; there are two midterms and a final for each semester.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/ibcm/ibcm.html">ibcm</a> (<a href="https://aaronbloomfield.github.io/pdr/ibcm/ibcm.md">md</a>): the files necessary for the IBCM module on machine language, which is taught about two thirds of the way into the course.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/labs/index.html">labs</a> (<a href="https://aaronbloomfield.github.io/pdr/labs/index.md">md</a>): the labs are the main assignments in the course, and each lab is split into pre-lab, in-lab, and post-lab parts. There are 11 full labs, with a partial 12th lab that is an optional component of the course. The labs are written using <a href="http://daringfireball.net/projects/markdown/">markdown</a>, and the rendered HTML version of each lab is also committed to this repository.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/slides/index.html">slides</a> (<a href="https://aaronbloomfield.github.io/pdr/slides/index.md">md</a>): Contains the slides used in the course. The slides use <a href="https://github.com/hakimel/reveal.js/">reveal.js</a>, an HTML presentation framework.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/tutorials/index.html">tutorials</a> (<a href="https://aaronbloomfield.github.io/pdr/tutorials/index.md">md</a>): the tutorials that are used as part of the lab assignments, these are primarily Linux tutorials.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/utils/index.html">utils</a> (<a href="https://aaronbloomfield.github.io/pdr/utils/index.md">md</a>): various utilities for this repository</li>
</ul>
<h2 id="contributing-to-this-repository"><a name="contributing"></a>Contributing to this Repository</h2>
<p>Updates to the repository are restricted to approved individuals only, to prevent anybody from messing with the slides right before a lecture. However, others can still contribute to this repository – to do so, take the following steps:</p>
<ol type="1">
<li>Create a github account, if you do not have one</li>
<li>Fork this repository: you can click on the “Fork” link in the upper right, or just click <a href="https://github.com/uva-cs/pdr/fork">here</a></li>
<li>Clone your forked repository on to your local machine</li>
<li>Make any changes you want to your forked version</li>
<li>Run <code>make</code> - note that you will need <a href="http://johnmacfarlane.net/pandoc/">pandoc</a>, <a href="http://astyle.sourceforge.net/">astyle</a>, and <a href="http://www.gnu.org/software/src-highlite/source-highlight.html">source-highlight</a> installed</li>
<li>Commit and push your changes back to your forked repository</li>
<li>Create a pull request, following the instructions <a href="https://help.github.com/articles/creating-a-pull-request">here</a></li>
</ol>
<p>At that point, I will receive a notice that a change has been submitted, and I’ll look at it and hopefully accept it into the main repository.</p>
<p>When you want to bring in the updates from the main pdr github repository into your forked repository, you will need to follow the instructions <a href="https://help.github.com/articles/syncing-a-fork">here</a>.</p>
<h2 id="course-description"><a name="description"></a>Course Description</h2>
<p>This course is a second-year course for computer science majors. It is the primary data structures course in the <a href="http://www.virginia.edu/">University of Virginia</a>’s <a href="http://www.cs.virginia.edu/">computer science</a> curriculum. Unlike many other data structure courses at other institutions, it is intended as the <em>third</em> course in sequence, meaning that students are expected to have taken two semesters of Java (or equivalent, although some of the examples are specifically from Java). The course focuses on how programs and data are represented from the high level down to the low level. For programs, we examine (from high to low): abstract data types, Java code, C++ code, C code, assembly (x86) code, and a customized machine language. For data, we examine (also from high to low): abstract data types, objects, primitive types, and how numbers are encoded (both floats (IEEE 754) and integers (two’s complement)). About two-thirds of this course is programming using C++. The remainder of this course uses other languages, including (in decreasing order): x86 assembly, IBCM (a machine language), C, Objective C, and shell scripting.</p>
<p>The <a href="http://www.abet.org/">ABET</a> course objects are as follows:</p>
<ul>
<li>Understand program representation from the high-level programming language perspective down to the underlying machine level representation, including: number representation, operations, conditionals, and control structures</li>
<li>Be able to implement basic and advanced abstract data types in C++ including: linked lists, stacks, queues, hash tables, trees, and graphs</li>
<li>Be able to evaluate asymptotic time and space complexity analysis of programs and data structure implementations using Big-O, Big-Omega, and Big-Theta notation and assess the suitability of a data structure for a particular problem</li>
<li>Understand the basic program execution model and the underlying computer hardware and software (fetch-execute cycle, memory hierarchy, operating system, compiler)</li>
<li>Be able to implement basic program control and data structures in an assembly language (loops, conditionals, subroutines and parameter passing modes, arrays)</li>
</ul>
<h2 id="markdown"><a name="markdown"></a>Markdown</h2>
<p>The majority of the content in this repository was created using <a href="http://daringfireball.net/projects/markdown/">Markdown</a>. Unfortunately, the only standardized Markdown is very old (2004), and has limited support for many HTML features, such as tables. A simple conversion script in a Makefile is in the <a href="https://aaronbloomfield.github.io/pdr/utils/index.html">utils</a> (<a href="https://aaronbloomfield.github.io/pdr/tutorials/index.md">md</a>) directory, which uses <a href="http://johnmacfarlane.net/pandoc/">pandoc</a>. Assuming pandoc is installed, run <code>make markdown</code> in the root repo directory to re-create all the .html files from their associated .md files.</p>
<p>For all the Markdown files in this repository, both the original (.md) file and the HTML version (.html) are added to the repository, so that people who do not have Markdown installed can still view the contents of this repository.</p>
<p>Note that Github supports an enhanced version of Markdown, called <a href="https://help.github.com/articles/github-flavored-markdown">Github Flavored Markdown</a>, or GFM. This mostly pertains to this README file. In an effort to ensure compatibility with other Markdown programs (such as the one described here and what reveal.js uses), GFM specific features are generally avoided. One example is the use of anchors in this document – the HTML tags are included instead of using GFM’s version.</p>
<h2 id="source-code"><a name="sourcecode"></a>Source code</h2>
<p>All source code is formatted via <a href="http://astyle.sourceforge.net/">astyle</a> and then highlighted via <a href="http://www.gnu.org/software/src-highlite/source-highlight.html">source-highlight</a>. Both the original file (foo.cpp) and the highlighted version (foo.cpp.html) are included in the repository. All links to source code will like to the .html, with a “(<a href="https://aaronbloomfield.github.io/pdr/README.md">src</a>)” after it to link to the original source code. Utility scripts are provided to convert all the files in the <a href="https://aaronbloomfield.github.io/pdr/utils/index.html">utils</a> (<a href="https://aaronbloomfield.github.io/pdr/tutorials/index.md">md</a>) directory. <code>make format</code> and <code>make highlight</code> can also be run to invoke the scripts.</p>
<h2 id="license"><a name="license"></a>License</h2>
<p>The material in this repository is released under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a> (CC BY-SA).</p>
<p>Copyright (c) 2017-2018 by Mark Floryan Copyright (c) 2013-2017 by Aaron Bloomfield.</p>
<p>Some parts of this repository are taken, with permission, from other sources. The full details are in the <a href="https://aaronbloomfield.github.io/pdr/license.html">License</a> (<a href="https://aaronbloomfield.github.io/pdr/license.md">md</a>) file. In particular, some parts of this repository that were obtained elsewhere can not be used for commercial purposes.</p>


</div>]]>
            </description>
            <link>https://aaronbloomfield.github.io/pdr/readme.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029751</guid>
            <pubDate>Sun, 08 Nov 2020 22:36:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Restoring a 37 Year-Old IBM F Mechanical Keyboard]]>
            </title>
            <description>
<![CDATA[
Score 143 | Comments 75 (<a href="https://news.ycombinator.com/item?id=25029571">thread link</a>) | @opsdisk
<br/>
November 8, 2020 | https://blog.opsdisk.com/restoring-a-37-year-old-ibm-model-f-mechanical-keyboard.html | <a href="https://web.archive.org/web/*/https://blog.opsdisk.com/restoring-a-37-year-old-ibm-model-f-mechanical-keyboard.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
                <p>I wanted to share my journey from start to finish to restore a 1983 IBM Model F XT mechanical keyboard to it's former glory.  It includes the steps, mistakes, and additional hardware required to make it functional with a modern computer.  This blog post is dedicated to my dad for teaching me about computers.</p>
<p><img alt="original.jpg" src="https://blog.opsdisk.com/images/keyboard/original.jpg"></p>

<p>A few months ago, my dad asked if I was interested in taking ownership of his <a href="https://en.wikipedia.org/wiki/IBM_Personal_Computer">IBM model 5150 PC</a>.  Without hesitation, I said "yes!".  The first project I wanted to tackle was to restore the
<a href="https://en.wikipedia.org/wiki/Model_F_keyboard">1983 Model F XT mechanical keyboard</a> keyboard.  In a sentimental and
journeyman/apprentice-sense, it felt like my dad was passing on the tools of his craft to continue the family line of
computer work and I couldn't pass that up.</p>
<p><img alt="datestamp.jpg" src="https://blog.opsdisk.com/images/keyboard/datestamp.jpg"></p>

<p>For those of you unfamiliar with the IBM Model F, <a href="https://www.modelfkeyboards.com/">modelfkeyboards.com</a> summarizes it
nicely:</p>
<blockquote>
<p>The IBM Model F keyboards not only used the best switches, the materials used in their production (well over 5lbs of
steel and other metals) means they will be working as good as new when it’s time to pass it on to your grandchildren.
The problem...they just aren't made that way any more.  The IBM Model F was discontinued in the 1980's.  If you do
find a Model F, it will be some combination of dirty, broken and/or expensive, requiring hours of work to get it
working again!</p>
</blockquote>
<p>This tank of a keyboard weighs in at over 6 pounds, sounds like <a href="https://youtu.be/XTVeSCqYSmE?t=7">this</a>, and at the
time of production, retailed for $300-400 in 1982 ($800-1000 dollars adjusted in today's dollars!) according to this
<a href="https://www.youtube.com/watch?v=y9Jds326gks">review</a>.</p>
<p>The restoration did take a few hours and fortunately none of it involved a soldering iron or replacing any of the
electrical or physical components because it was already in great functioning shape...a true testament to the design.
Even cooler, this blog post was typed up using the restored Model F keyboard!</p>

<p>Before beginning the project, I discovered <a href="https://www.clickykeyboards.com/">ClickyKeyboards</a>, a site "Specializing in
the restoration and collection of model M keyboards", the successor to the Model F.  On ClickyKeyboards, there is a
section dedicated to the adapters and converters that may be required to make older 5-pin DIN plug keyboards compatible
with modern USB ports.  One of the companies mentioned on ClickyKeyboards is
<a href="https://www.hagstromelectronics.com/">Hagstrom Electronics</a>, which sells keyboard encoders and protocol converters.
Without looking too closely at the other products, I quickly purchased the
<a href="https://www.hagstromelectronics.com/keyboard-encoder-ke18-xtat-ps2-shp.html">KE18-XTAT-PS/2</a> which "converts an XT
keyboard into a PS/2 protocol keyboard" for $55.  I was ecstatic that there was at least something to get me into PS/2
land, because at that point I knew I could easily find a PS/2 to USB converter.</p>
<p>After the KE18-XTAT-PS/2 arrived, I quickly fired up an ancient box (yes, that is Windows 2000!) with a PS/2 input on
the motherboard.  I tested all the keys to ensure they still worked and they did!  With confirmation that the keyboard
still worked, it was time to start the restoration.</p>
<p><img alt="win2000.jpg" src="https://blog.opsdisk.com/images/keyboard/win2000.jpg"></p>

<p><img alt="original2.jpg" src="https://blog.opsdisk.com/images/keyboard/original2.jpg"></p>
<p>The first step was to remove the metal casing and see what the state of the board was underneath the keys.  There was
years worth of debris, coffee stains, and gunk that had to be removed.  In addition, there were a few spots with
corrosion on the board that needed to be addressed.</p>
<p><img alt="preclean.jpg" src="https://blog.opsdisk.com/images/keyboard/preclean.jpg"></p>
<p>Once the casing was off, I utilized compressed air to clean out the key bunkers and get rid of any loose nastiness.
Next, I tried using rubbing alcohol and Q-tips to try and remove some of the stickier stuff, but that didn't
really work.</p>

<p>I borrowed a rotary tool to buff out the corrosion.  I used one of the provided bits that had soft plastic tentacles to
try and buff out the corroded spots as gently as possible.  There are likely better and more appropriate bits, but it
did the job.  The rotary tool's power and RPMs were a bit overkill even on the lowest setting.  Ideally, I would have
used one with fewer minimum RPMs and a more precise bit to get all the spots.</p>
<p><img alt="buffing_tool_and_bit.jpg" src="https://blog.opsdisk.com/images/keyboard/buffing_tool_and_bit.jpg"></p>
<p><img alt="bit.jpg" src="https://blog.opsdisk.com/images/keyboard/bit.jpg"></p>
<p>I was using the rotary tool under a bright overhead desk lamp, and with the way the light was reflecting, I didn't
notice it was taking off the black finish and revealing the silver metal base.  At that point, I just decided to buff
off the finish where I could to make it look more uniform.  The silver metal is visible when the keys and cover are back
on it, but it looks fine.  Unfortunately, with the rotary head and bit size, I wasn't able to buff every last square
inch of the board, but I knew it'd be covered so I wasn't too concerned with it looking perfect.</p>
<p><img alt="allbuffed.jpg" src="https://blog.opsdisk.com/images/keyboard/allbuffed.jpg"></p>

<p><img alt="postbuffing.jpg" src="https://blog.opsdisk.com/images/keyboard/postbuffing.jpg"></p>
<p>There was still some residue stuck to the board that I wanted to remove.  I started out using a small eye glass
screwdriver to scrape it off, but it took some of the black finish off as well and didn't look that nice.  At that point,
a chemical pivot was required and I reached for the <a href="https://googone.com/">Goo Gone</a>, which I should have done from the
beginning.  The Goo Gone and a bit of elbow grease with Q-Tips did the trick in removing the stubborn gunk on the board.</p>
<p><img alt="postgoogone.jpg" src="https://blog.opsdisk.com/images/keyboard/postgoogone.jpg"></p>

<p>With the board in good shape, it was time to tackle the actual keys.  I first gave them a good wipe-down using desk
cleaning wipes, which removed most of the discoloration and stains.  However, they still didn't look as good as they
could, and I read that just soaking them in a bowl of dish soap and water for a few hours can do wonders...and it did.
They keys look brand new.</p>
<p><img alt="keybath.jpg" src="https://blog.opsdisk.com/images/keyboard/keybath.jpg"></p>
<p>One mistake I made after washing them was to not let them completely dry on the inside.  I placed the keys back on the
board too soon, and some water leaked onto a few of the springs causing a small amount of rusting (slightly visible in
the post-Goo Gone image).  I gave them another soap soak and used the compressed air to really get out the water.  I
also let them air dry for 1-2 days before putting them back on the board.</p>

<p>After successfully testing the actual keyboard and the restoration almost complete, it was time to search for a PS/2 to
USB cable.  On a whim, I was back on the Hagstrom Electronic site and noticed they already had a small box, the
<a href="https://www.hagstromelectronics.com/ke-xtusb-keyboard-encoder-shp.html">KE-XTUSB</a>, that converted the XT signal to USB,
and it was the same price.  They graciously allowed me to return the KE18-XTAT-PS/2 in exchange for the KE-XTUSB which
was the same price.  A few days later the KE-XTUSB arrived in the mail and I eagerly connected it to my current computer.</p>
<p><img alt="xtusb.jpg" src="https://blog.opsdisk.com/images/keyboard/xtusb.jpg"></p>

<p>Everything worked beautifully except for two keys.  The "s" key's spring mechanism would either not detect a key press
or would be stuck in the depressed state blasting "sssssssssssssssssssssssssssssssssssss" across the screen.  With a
bit of finagling, I got the key cap placed correctly so now it works like a champ.  The other key that has issues is the
accountant's "+" near the 10 key pad.  The spring had come off somehow and I super glued it back, but something still
isn't right and it fails to register key strokes.  Not a huge loss since "+" can be achieved with another key
combination.</p>
<p>Overall, I'm really impressed with how it looks and, when comparing it to <a href="https://www.youtube.com/watch?v=E2bAhxK76hc">this</a>
unboxing video of a never before opened Model F, it looks about the same!</p>
<p>If you have any questions or comments, hit me up on Twitter <a href="https://twitter.com/opsdisk">@opsdisk</a>.</p>
<p><img alt="final.jpg" src="https://blog.opsdisk.com/images/keyboard/final.jpg"></p>
            </section></div>]]>
            </description>
            <link>https://blog.opsdisk.com/restoring-a-37-year-old-ibm-model-f-mechanical-keyboard.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029571</guid>
            <pubDate>Sun, 08 Nov 2020 22:09:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The traffic of some Apple processes isn’t shown in Little Snitch 5]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25029535">thread link</a>) | @bangonkeyboard
<br/>
November 8, 2020 | https://www.obdev.at/support/littlesnitch/245914647368270 | <a href="https://web.archive.org/web/*/https://www.obdev.at/support/littlesnitch/245914647368270">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://www.obdev.at/support/littlesnitch/245914647368270</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029535</guid>
            <pubDate>Sun, 08 Nov 2020 22:06:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Papers We Love Conference: Mini Edition]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25029531">thread link</a>) | @blopeur
<br/>
November 8, 2020 | https://paperswelove.org/2020/video/pwlconf-mini/ | <a href="https://web.archive.org/web/*/https://paperswelove.org/2020/video/pwlconf-mini/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main" role="main" itemprop="mainContentOfPage">
                <article itemscope="" itemtype="http://schema.org/Article">
        
        
        <section itemprop="articleBody">
        <h3 id="november-18-2020---1745-est-on-twitchhttpswwwtwitchtvpaperswelove">November 18, 2020 - 17:45 EST on <a href="https://www.twitch.tv/paperswelove">Twitch</a></h3>

<p><a href="https://connect.clickandpledge.com/w/Form/a9f96acc-aa05-4c52-a9b4-e12ab505abdf"><img src="https://www.usenix.org/sites/all/themes/custom/cotija/images/logo.svg"></a>Like many non-profit organizations (ourselves included) <a href="https://www.usenix.org/">USENIX</a> has struggled during the COVID-19 pandemic. Conferences made up a large part of their revenue streams, and their cancellation has deprived USENIX of operating funds. As a <a href="https://www.usenix.org/publications">major source of research papers</a> in Computer Science and Networking we'd like to help anyway we can. <strong>So we're going to try something new to help raise donations for USENIX: a streaming mini-conference.</strong></p>

<h3 id="schedule">Schedule</h3>

<ul>
  <li><strong>17:45 EST</strong> Panel led by <a href="https://blog.acolyer.org/">Adrian Colyer</a>, with guests <a href="https://www.cc.gatech.edu/home/ada/">Ada Gavrilovska</a>, <a href="">Joe Hellerstein</a>, <a href="https://drkp.net/">Dan R. K. Ports</a>, <a href="https://www.justinesherry.com/">Justine Sherry</a> and <a href="">Hakim Weatherspoon</a>.</li>
  <li><strong>18:35 EST</strong> <a href="https://research.google/people/jeff/">Jeff Dean</a> - The Rise of Cloud Computing Systems</li>
  <li><strong>19:30 EST</strong> <a href="https://irenezhang.net/">Irene Zhang</a> - The <a href="https://github.com/demikernel/demikernel">Demikernel</a> and the Future of Kernel-Bypass Systems</li>
</ul>

<p>We'll <a href="https://www.twitch.tv/paperswelove">stream live on Twitch</a>, aftwerwards we'll post the video to our <a href="https://www.youtube.com/user/PapersWeLove">YouTube channel</a> once we've done some editing.</p>

<p><strong>This is a fund raiser</strong> so we're asking everyone who attends to <a href="https://connect.clickandpledge.com/w/Form/a9f96acc-aa05-4c52-a9b4-e12ab505abdf">make a donation to USENIX</a>. Almost every local chapter of Papers We Love has presented a paper from USENIX over the years, so this is our chance to give back.</p>



<p>Keep an eye on this page and our <a href="https://twitter.com/papers_we_love/">Twitter feed</a> for updates to the schedule and speaker list.</p>

<h3 id="speakers">Speakers</h3>

<hr>

<p><strong>Jeff Dean</strong></p>

<p><em>Abstract</em>: In this talk, I'll highlight some of the developments in cloud computing systems over the past two decades.  I'll also describe why machine learning systems have dramatically changed some of the kinds of computer systems we want to build.</p>

<p><em>Bio</em>: Jeff Dean joined Google in 1999 and is currently a Google Senior Fellow and leads Google Research and Google Health, which focus on basic computer science and AI research and their use in important problem domains. He has worked on various computer systems including Google's search and advertising systems, MapReduce, BigTable, Spanner, and open-source software such as TensorFlow, protocol buffers, and LevelDB.</p>

<p>Jeff has a Ph.D. in Computer Science from the University of Washington and a B.S. in Computer Science &amp; Economics from the University of Minnesota. He was awarded the 2012 ACM Prize in Computing, and is a member of the U.S. National Academy of Engineering and the American Academy of Arts and Sciences, and a Fellow of the ACM</p>

<p>Links: <a href="https://research.google/people/jeff/">Site</a> / <a href="https://scholar.google.com/citations?user=NMS69lQAAAAJ">Publications</a></p>

<hr>

<p><strong>Irene Y. Zhang</strong></p>

<p><em>Abstract</em>: This talk presents the <a href="https://github.com/demikernel/demikernel">Demikernel</a> [<a href="http://irenezhang.net//papers/demikernel-hotos19.pdf">Paper</a>], a new OS architecture for kernel-bypass I/O devices.  Demikernel hides device complexity and heterogeneity by defining a new high-level, kernel-bypass I/O API and implementing it using different user-level, library OSes for each device type I will discuss the challenges in designing kernel-bypass library OSes and the future research directions in kernel-bypass for datacenter applications.</p>

<p><em>Bio</em>: My research focuses on datacenter operating systems and distributed systems.</p>

<p>I completed my PhD in 2017 at the University of Washington, where I was advised by Hank Levy and Arvind Krishnamurthy. My thesis is on distributed systems for applications that span mobile devices and cloud servers. Before my PhD, I received my S.B. and M.Eng. from MIT and worked for 3 years in the virtual machine monitor group at VMware.</p>

<p>I was born in Beijing, China but spent most of my time growing up in Columbus, Indiana. My husband and I like to cook, travel and occasionally do computer science together.</p>

<p>Links: <a href="https://irenezhang.net/">Site</a> / <a href="https://irenezhang.net/publications.html">Publications</a></p>

<hr>

<p><strong>Adrian Colyer</strong> - Panel Moderator</p>

<p><em>Bio</em>: I publish <a href="https://blog.acolyer.org/">The Morning Paper</a>: a short summary every weekday of an important, influential, topical or otherwise interesting paper in the field of computer science.</p>

<p>I’m a Venture Partner with Accel in London, where it’s my job to help find and build great technology companies across Europe and Israel.  If you’re working on an interesting technology-related business I’d love to hear from you: you can reach me at acolyer at accel dot com. Prior to joining Accel I spent over twenty years in technical roles, including CTO roles at Pivotal, VMware, and SpringSource.</p>

<p>Links: <a href="https://blog.acolyer.org/">Site</a></p>

<hr>

<p><strong>Ada Gavrilovska</strong> - Panelist</p>

<p><em>Bio</em>: Ada Gavrilovska is an associate professor in the School of Computer Science at Georgia Tech. She directs the Kernel research group, focused on performance, scalability and efficiency problems across the systems software stack, including operating, distributed, and high-performance computing systems. Gavrilovska's research is supported by the National Science Foundation, the US Department of Energy, industry support from Cisco, Facebook, HPE, Intel, Intercontinental Exchange, LexisNexis, Samsung, VMware, and others, and the Applications Driving Architectures (ADA) Research Center, a JUMP Center co-sponsored by the Semiconductor Research Corporation and DARPA. She served as the program co-chair of the USENIX Annual Technical Conference in 2020.</p>

<p>Links: <a href="https://www.cc.gatech.edu/~ada/">Site</a> / <a href="https://dblp.org/pid/76/3229.html">Publications</a></p>

<hr>

<p><strong>Joe Hellerstein</strong> - Panelist</p>

<p><em>Bio</em>: Joseph M. Hellerstein is the Jim Gray Professor of Computer Science at the University of California, Berkeley, whose work focuses on data-centric systems and the way they drive computing. He is an <a href="http://fellows.acm.org/fellow_citation.cfm?id=4354833&amp;srt=year&amp;year=2009">ACM Fellow</a>, an <a href="http://www.sloan.org/fellowships">Alfred P. Sloan Research Fellow</a> and the recipient of three <a href="http://www.sigmod.org/sigmod-awards/sigmod-awards#time">ACM-SIGMOD "Test of Time"</a> awards for his research. Fortune Magazine has included him in their list of 50 <a href="https://archive.fortune.com/galleries/2010/technology/1007/gallery.smartest_people_tech.fortune/27.html">smartest people in technology</a>, and MIT's Technology Review magazine included his work on their <a href="http://www.technologyreview.com/news/418545/tr10-cloud-programming/">TR10</a> list of the 10 technologies "most likely to change our world".</p>

<p>Hellerstein is the co-founder and Chief Strategy Officer of <a href="http://trifacta.com/">Trifacta</a>, a software vendor providing intelligent interactive solutions to the messy problem of wrangling data. He has served on the technical advisory boards of a number of computing and Internet companies including <a href="http://www.dellemc.com/">Dell EMC</a>, <a href="http://www.surveymonkey.com/">SurveyMonkey</a>, <a href="http://www.captricity.com/">Captricity</a>, and <a href="http://www.datometry.com/">Datometry</a>, and previously served as the Director of <a href="https://en.wikipedia.org/wiki/Intel_Research_Lablets">Intel Research, Berkeley</a>.</p>

<p>Links: <a href="https://dsf.berkeley.edu/jmh/index.html">Site</a> / <a href="https://dsf.berkeley.edu/jmh/publications.html">Publications</a></p>

<hr>

<p><strong>Dan R. K. Ports</strong> - Panelist</p>

<p>I am a researcher in the Systems Research Group at Microsoft Research.</p>

<p>My research focuses on distributed systems – using a combination of new algorithms and systems techniques to build practical systems that are faster, more reliable, easier to program, and more secure.</p>

<p>I take a broad view of the systems field: besides distributed systems, I've worked in operating systems, networking, databases, architecture, and security. I believe that looking across the entire systems stack yields interesting opportunities at the intersection of these areas.</p>

<p>Most of my work these days involves rethinking how distributed systems should be built for the datacenter environment. I lead the Prometheus project at MSR, which asks how we can use new reconfigurable devices, such as programmable dataplane switches and smart NICs, to support advanced systems applications. The key idea is to co-design distributed systems with new network primitives.</p>

<p>Before joining MSR, I was on the faculty in CSE at the University of Washington. I still advise a few excellent students over there.
An increasingly long time ago (i.e., 2012), I was a student at MIT, where I was (approximately) Barbara Liskov's last Ph.D. graduate. Even before that, I was an undergraduate at MIT.</p>

<p>Links: <a href="https://drkp.net/">Site</a> / <a href="https://drkp.net/publications.html">Publications</a></p>

<hr>

<p><strong>Justine Sherry</strong> - Panelist</p>

<p><em>Bio</em>: Justine Sherry is an assistant professor at Carnegie Mellon University. Her interests are in computer networking; her work includes middleboxes, networked systems, measurement, cloud computing, and congestion control. Dr. Sherry received her PhD (2016) and MS (2012) from UC Berkeley, and her BS and BA (2010) from the University of Washington. She is a recipient of the SIGCOMM doctoral dissertation award, the David J. Sakrison prize, paper awards at USENIX NSDI and ACM SIGCOMM, and an NSF Graduate Research Fellowship. Most importantly, she is always on the lookout for a great cappuccino.</p>

<p>Links: <a href="https://www.justinesherry.com/">Site</a> / <a href="https://www.justinesherry.com/papers.html">Publications</a></p>

<hr>

<p><strong>Hakim Weatherspoon</strong> - Panelist</p>

<p><em>Bio</em>: I received my PhD in 2006 from the <a href="http://www.cs.berkeley.edu/">University of California, Berkeley</a>, in the area of secure and fault-tolerant distributed wide-area storage systems (e.g. <a href="http://antiquity.sourceforge.net/">Antiquity</a>, <a href="http://oceanstore.cs.berkeley.edu/">OceanStore</a>, etc.). I received a B.S. in Computer Engineering from the University of Washington in 1999.</p>

<p>Links: <a href="https://www.cs.cornell.edu/~hweather/index.php">Site</a> / <a href="https://www.cs.cornell.edu/~hweather/publications.php">Publications</a></p>


        </section>
        
    </article>

        </div></div>]]>
            </description>
            <link>https://paperswelove.org/2020/video/pwlconf-mini/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029531</guid>
            <pubDate>Sun, 08 Nov 2020 22:06:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Toroidal Isolation Power Transformers]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25029498">thread link</a>) | @peter_d_sherman
<br/>
November 8, 2020 | https://www.toruspower.com/toroidal-transformers/ | <a href="https://web.archive.org/web/*/https://www.toruspower.com/toroidal-transformers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main-content"> <article id="post-2391" class="page"> <div> <div itemscope="" itemtype="http://schema.org/Product"><div id="et-boc"> <div> <div>  <div> <div> <div> <p><span><img src="https://www.toruspower.com/wp-content/uploads/2015/02/home33image.jpg" alt="" title="" srcset="https://www.toruspower.com/wp-content/uploads/2015/02/home33image.jpg 1600w, https://www.toruspower.com/wp-content/uploads/2015/02/home33image-300x113.jpg 300w, https://www.toruspower.com/wp-content/uploads/2015/02/home33image-1024x384.jpg 1024w, https://www.toruspower.com/wp-content/uploads/2015/02/home33image-1080x405.jpg 1080w" sizes="(max-width: 1600px) 100vw, 1600px"></span> </p> </div> </div> </div> <div> <div> <div> <div> <div><h4>High Efficiency</h4> <div id="attachment_3320"><p><img aria-describedby="caption-attachment-3320" loading="lazy" src="https://www.toruspower.com/wp-content/uploads/2015/06/HighEfficiency3.jpg" alt="HighEfficiency3" width="450" height="279" srcset="https://www.toruspower.com/wp-content/uploads/2015/06/HighEfficiency3.jpg 450w, https://www.toruspower.com/wp-content/uploads/2015/06/HighEfficiency3-300x186.jpg 300w" sizes="(max-width: 450px) 100vw, 450px"></p><p id="caption-attachment-3320">Typical efficiency figures for toroidal transformers are 95% or better.</p></div> <p>The key to all toroidal transformer advantages is its efficiency. And the key to its efficiency is the core. The core is a continuous strip of grain oriented silicon steel, wound like a clock spring, under tension. It is annealed to relax the molecular structure which ensures that all grains are in the magnetic direction (unlike the old standard EI, with about 40% in the wrong direction).</p> <p>There is no air gap, resulting in a stacking factor of 97.5% of its weight. Since all the windings are symmetrically spread over the entire round, gapless core, a higher flux density is possible (toroidal transformers operate at flux densities of 16 to 18 kilogauss, while traditional EI transformers operate at 12 to 14 kilogauss).</p> <p>The magnetic flux is in the same direction as the grain oriented silicon steel core, thus achieving very high electrical efficiencies. Typical efficiency figures for toroidal transformers are 95% (e.g. 18kVA at 98% efficiency).</p> </div> </div> </div> </div> </div> <div> <div> <div> <div> <div><h4>Low Mechanical Hum</h4> <div id="attachment_3318"><p><img aria-describedby="caption-attachment-3318" loading="lazy" src="https://www.toruspower.com/wp-content/uploads/2015/06/Lowmechhum-300x186.jpg" alt="Lowmechhum" width="300" height="186" srcset="https://www.toruspower.com/wp-content/uploads/2015/06/Lowmechhum-300x186.jpg 300w, https://www.toruspower.com/wp-content/uploads/2015/06/Lowmechhum.jpg 450w" sizes="(max-width: 300px) 100vw, 300px"></p><p id="caption-attachment-3318">Compared to EI transformers, toroids are silent.</p></div> <p>Toroidal transformers are also acoustically quieter than EI transformers. The absence of an air gap typically provides an 8:1 reduction of acoustic noise. In addition the windings tightly envelope the entire core, effectively reducing magnetostriction – the main source of the familiar mechanical hum found in standard vertically laminated EI transformers. Compared to EI transformers, toroids are silent.</p></div> </div> <div> <div><h4>Small Size – Saves Real Estate</h4> <div id="attachment_3319"><p><a href="https://www.toruspower.com/wp-content/uploads/2015/06/Smallsize.jpg"><img aria-describedby="caption-attachment-3319" loading="lazy" src="https://www.toruspower.com/wp-content/uploads/2015/06/Smallsize-300x186.jpg" alt="Smallsize" width="300" height="186" srcset="https://www.toruspower.com/wp-content/uploads/2015/06/Smallsize-300x186.jpg 300w, https://www.toruspower.com/wp-content/uploads/2015/06/Smallsize.jpg 450w" sizes="(max-width: 300px) 100vw, 300px"></a></p><p id="caption-attachment-3319">Toroids are smaller and lighter than standard transformers.</p></div> <p>This efficiency yields the most easily identifiable feature of the toroid, its size benefit. Toroidal transformers are about half the size and weight of standard transformers.</p></div> </div> </div> <div> <div> <div><h4>Low Off-Load Losses</h4> <div id="attachment_3316"><p><img aria-describedby="caption-attachment-3316" loading="lazy" src="https://www.toruspower.com/wp-content/uploads/2015/06/Lowoffloadloss-300x186.jpg" alt="Lowoffloadloss" width="300" height="186" srcset="https://www.toruspower.com/wp-content/uploads/2015/06/Lowoffloadloss-300x186.jpg 300w, https://www.toruspower.com/wp-content/uploads/2015/06/Lowoffloadloss.jpg 450w" sizes="(max-width: 300px) 100vw, 300px"></p><p id="caption-attachment-3316">Toroids require about 1/16 the excitation power of conventional transformers.</p></div> <p>It takes a lot less energy to maintain the magnetic field in a toroidal core. This is known as excitation or quiescent power. Toroids require about 1/16 the excitation power of conventional transformers. That’s 1/16 the electricity required by EI transformers in standby mode. This can translate into big savings in large transformer applications such as industrial controls.</p></div> </div> <div> <div><h4>Low Stray Magnetic Fields</h4> <div id="attachment_3317"><p><img aria-describedby="caption-attachment-3317" loading="lazy" src="https://www.toruspower.com/wp-content/uploads/2015/06/Lowmagfield-300x186.jpg" alt="Lowmagfield" width="300" height="186" srcset="https://www.toruspower.com/wp-content/uploads/2015/06/Lowmagfield-300x186.jpg 300w, https://www.toruspower.com/wp-content/uploads/2015/06/Lowmagfield.jpg 450w" sizes="(max-width: 300px) 100vw, 300px"></p><p id="caption-attachment-3317">Toroidal transformers radiate about 1/10 the magnetic field of EI transformers.</p></div> <p>Another type of noise is magnetic. Toroidal transformers radiate about 1/10 the magnetic field of EI transformers; this is, again, because of the inherent efficiency and unique construction. The windings which cover the core act as a shield. The magnetic field is contained doing what it should, transforming energy from primary to secondary. This may eliminate the need for special shielding and makes toroidal transformers especially suitable for applications in sensitive electronic equipment, such as: low level amplifiers, medical equipment, and near CRTs.</p></div> </div> </div> </div> </div> </div> </div> </div> <p><span>Product Name:<span itemprop="name"> Toroidal Isolation Transformers </span></span><span>Product Brand:<span itemprop="brand"> Torus Power </span></span><span>Product Description: <span itemprop="description"> The key to all toroidal transformer advantages is its efficiency. And the key to its efficiency is the core. </span></span><span>Product Image:<a itemprop="image" src="https://www.toruspower.com/wp-content/uploads/2018/10/Lo_Torus-Power_POWERBLOCK_PB10_angle.jpg"> https://www.toruspower.com/wp-content/uploads/2018/10/Lo_Torus-Power_POWERBLOCK_PB10_angle.jpg </a></span><span itemprop="offers" itemscope="" itemtype="http://schema.org/Offer"> <span>Product Price:<span itemprop="price"> Various </span></span><span>Price Currency: <span itemprop="priceCurrency"> $CA and $US </span></span><span>Name Of The Seller:<span itemprop="seller"> Torus Power </span></span> </span></p></div> </div> </article> </div></div>]]>
            </description>
            <link>https://www.toruspower.com/toroidal-transformers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029498</guid>
            <pubDate>Sun, 08 Nov 2020 22:01:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I Took a Break from Ham Radio and Why I Came Back]]>
            </title>
            <description>
<![CDATA[
Score 17 | Comments 8 (<a href="https://news.ycombinator.com/item?id=25029459">thread link</a>) | @parsecs
<br/>
November 8, 2020 | https://www.kj7nzl.net/blog/3-reasons-why-i-took-a-break-from-ham-radio-why-i-came-back/ | <a href="https://web.archive.org/web/*/https://www.kj7nzl.net/blog/3-reasons-why-i-took-a-break-from-ham-radio-why-i-came-back/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section id="content-pane">
  <div>
    
    <p>About three months ago I unconsciously or perhaps consciously lost interest in ham radio. This isn’t the first time this has happened to me, in fact this was what lead me to allow my license to lapse the first time in 2016. If it weren’t for a new coworker of mine showing interest in ham radio again after a twenty-five year absent; I presumably wouldn’t be writing this post now. His excitement about all of the recent advances that have been made since he was first license reminded me of the things I missed about ham radio. This forced me to reflect on the reasons that I stepped away from the hobby for nearly three months. After some time I narrowed it down to three primary reasons. Without further ado I present to you my three reasons I took a break from ham radio.</p>
<h2 id="1-2020-amsat-board-of-directors-election">1. 2020 AMSAT Board of Directors Election</h2>
<p>I love satellite operations! It was my primary reason for returning to the hobby earlier this year. Shortly after becoming licensed I joined AMSAT to support amateur radio in space. Little did I know there was an ongoing dispute among two newly elected board members and the rest of the board of directors. Apparently the dispute was over the access of to AMSAT’s finical records with the lengthy ordeal playing out like some pollical scandal with lawyers and secret audio recordings. The board of directors went as far as <a href="https://www.amsat.org/amsat-leadership-explains-2018-2020-legal-expenses/">releasing a statement</a> that was full of contempt for the two new directors. In turn Patrick Stoddard WD9EWK, one of the individuals at that center of this controversy <a href="http://amsat.wd9ewk.net/">released his own statement</a> on the events that transpired. All of this drama culminated with those who sided with Mr. Stoddard attempting replace three of the existing board members with there of their own candidates. Ultimately they were unsuccessful in their coup attempt, and I’ve exhausted all interest in being a part of AMSAT.</p>
<figure>
    <img src="https://www.kj7nzl.net/img/satellites/amsat-fm-0001.webp" alt="KJ7NZL working AO-92."> <figcaption>
            <p>KJ7NZL working AO-92</p>
        </figcaption>
</figure>

<h2 id="2-limited-radio-budget--limited-operating-options">2. Limited Radio Budget = Limited Operating Options</h2>
<p>Whether you want to admit it or not, ham radio is an expensive hobby. Sure you can purchase a Baofeng handheld for thirty-five dollars, but to really take advantage of all your license privileges you need to shell out some cold hard cash for either an HF rig or an all mode VHF/UHF radio. Initially, I wanted to get on the air as quickly and cheaply as possible to try my hand at working a few of the FM satellites, as a result I purchased an Arrow Antenna II and the Wouxun KG-UV8D Plus. To make a long story short, the Wouxun radio was garbage. Don’t believe me <a href="https://www.kj7nzl.net/blog/wouxun-kg-uv8d-plus-fm-satellites/">check out my review of the Wouxun KG-UV8D Plus</a>. After some time of fooling around with that dumb thing, I decided to purchase a Yaesu FT3D and I’m glad I did. On a side note, I should absolutely do a review of that thing since I’ve had it for about three months now and I’ve explored most everything the radio has to offer. With the KG-UV8D I was limited in how I could operate. With just supporting FM I really only could use the thing for satellites outside of using it to connect to any of the local repeaters in the area. On the other hand the Yaesu FT3D allowed me the ability to expore APRS and C4FM AKA System Fussion. This however came at the cost of three times the price of the KG-UV8D. All told, even after selling my KG-UV8D, I am about $600 into the hobby with an Arrow Antenna II, Yaesu FT3D, and Comet Dual Band HT antenna. Even with this setup I’m only able play around with FM satellites, APRS, and simplex/repeaters. I would have loved exploring some of the other areas of the hobby when I jumped back in the spring of this year, but discretionary funds were spread out among different things at the time.</p>
<figure>
    <img src="https://www.kj7nzl.net/img/aprs/aprs-001.webp" alt="KJ7NZL's Trip to Antelope Island State Park"> <figcaption>
            <p>KJ7NZL’s Trip to Antelope Island State Park</p>
        </figcaption>
</figure>

<h2 id="3-life-just-got-in-the-way">3. Life Just Got in the Way</h2>
<p>This year 2020 has been an unusual year for me. Between a global pandemic and finishing some of my basement, I’ve been very busy. As a result I’ve had little free time. What free time I’ve possessed has been divided between multiple hobbies with amateur radio taking a back seat most of the time. It’s my own fault really since I set out earlier this year with the goal of exclusively working FM satellites. I initial assumed it would be effortless to make time through out my day for a quick ten to fifteen minute pass. It made sense at the time; take a brief break, make a couple of contacts, and back to what I was doing before hand. But it turns out that FM satellite QSOs are hard work This in turn lead me to get frustrated very quickly and wish I could operate at my own pace whenever I felt like it. Just so you know, rarely do two FM satellite passes line up back to back when you want them two giving you a thirty to forty minute window of time in which to have fun. Looking back on my journey into ham radio, both the first time and second time, I should have just gone the HF route from the get go. Since I didn’t, I feel like I set myself up for disappointment. With very little motivation and other competing priorities it’s no wonder why I stepped away from ham radio as long I did.</p>
<figure>
    <img src="https://www.kj7nzl.net/img/radios/yaesu-ftdx-3000.webp" alt="Yaesu FTDX 3000"> <figcaption>
            <p>Yaesu FTDX 3000</p>
        </figcaption>
</figure>

<h3 id="moving-forward">Moving Forward</h3>
<p>So what does the future hold for me now that I’m interested in ham radio again? Well, Santa is coming to town, and he’s going to have a new Yaesu FTDX 3000 wrapped up under three for me. I’ll finally be able to dive head first into the world of HF. I have some ambitious plans too. I’d like to explore some of the lesser used digital modes like Hellschreiber, Olivia, and Contestia. I’ll still jump on the FT8 bandwagon, but ultimately I want to explore all the digital modes. I also want to <a href="https://www.kj7nzl.net/blog/learning-morse-code-series/">learn morse code</a>, which I’ve unsuccessfully started and begin working som CW. The Yaesu FTDX 3000 contains some exceptionally attractive features for CW operations that I really want to take advantage of. Another area that I’m going to focus on coming up soon will be building a MMDVM hotspot. Sure I could merely purchase a prebuilt one, but what the fun in that? All in all I think the rest of this year and next year is shaping up to be a excellent time for me to get back into ham radio.</p>

  </div>
</section></div>]]>
            </description>
            <link>https://www.kj7nzl.net/blog/3-reasons-why-i-took-a-break-from-ham-radio-why-i-came-back/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029459</guid>
            <pubDate>Sun, 08 Nov 2020 21:56:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Getting Started with VMware ESXi on ARM with a Raspberry Pi]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25029404">thread link</a>) | @todsacerdoti
<br/>
November 8, 2020 | https://www.servethehome.com/getting-started-with-vmware-esxi-on-arm-with-a-raspberry-pi/ | <a href="https://web.archive.org/web/*/https://www.servethehome.com/getting-started-with-vmware-esxi-on-arm-with-a-raspberry-pi/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <!-- image --><div><figure><a href="https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735.jpg" data-caption="Raspberry Pi"><img width="696" height="522" src="https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-696x522.jpg" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-696x522.jpg 696w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-400x300.jpg 400w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-800x600.jpg 800w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-1068x801.jpg 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-560x420.jpg 560w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-80x60.jpg 80w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-265x198.jpg 265w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735.jpg 1163w" sizes="(max-width: 696px) 100vw, 696px" alt="Raspberry Pi" title="Raspberry Pi"></a><figcaption>Raspberry Pi</figcaption></figure></div>
            <!-- content --><p>Last month VMWare released what they have called <a href="https://blogs.vmware.com/vsphere/2020/10/announcing-the-esxi-arm-fling.html">ESXi Arm-fling.</a> This new release allows you to install the same ESXi you know and love on an ARM processor. VMWare has certified a few systems for datacenter use. They also have certified it for the Raspberry Pi 4, but only for what they call “Far Edge”.</p>
<p>Today we are going to perform the installation on a Raspberry Pi 8GB and do some testing. While we generally feel <a href="https://www.servethehome.com/introducing-project-tinyminimicro-home-lab-revolution/">ProjectTinyMiniMicro</a> may be a better option for some, there are many enthusiasts who sing the praises of the Raspberry Pi. The Pi has exceptional power efficiency, scalability, and a small footprint.<span id="more-48249"></span></p>
<figure id="attachment_48251" aria-describedby="caption-attachment-48251"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151215-1-800x670.jpg" alt="Raspberry Pi 4" width="696" height="583" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151215-1-800x670.jpg 800w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151215-1-358x300.jpg 358w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151215-1-696x583.jpg 696w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151215-1-502x420.jpg 502w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151215-1.jpg 1014w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48251">Raspberry Pi 4 CanaKit and a LovePI PoE HAT</figcaption></figure>
<p>Before we continue, it is worth noting that this installation is a little bit different than other Raspberry Pi installations. We will need a total of <strong>three</strong> different pieces of storage to complete this installation. You need a microSD card for the firmware, but in this guide that is ALL the microSD card will be used for. Then you will need a USB thumb drive to act as your VMWare installer. Finally, you will need a place to install VMWare to. While it is possible to install it to your microSD card, that is not officially supported. Instead, you want to look at a USB based solution or a network solution such as PXE or iSCSI.</p>
<h2>Preparing our Pi for ESXi</h2>
<p>You will need to grab the<a href="https://www.raspberrypi.org/downloads/noobs/"> NOOBS </a>image and&nbsp;burn it to your microSD card if you did not buy a kit with it preinstalled as we had. To do so you can utilize the <a href="https://www.raspberrypi.org/downloads/">Raspberry Pi Imager</a>.</p>
<p>When the NOOBs installer boots up, select the Raspberry Pi OS Lite (32-bit) option. You do not need the full desktop version. We are only using the OS to update the EEPROM and get some other updates out of the way.</p>
<figure id="attachment_48252" aria-describedby="caption-attachment-48252"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/NOOBS-800x501.png" alt="N" width="696" height="436" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/NOOBS-800x501.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/NOOBS-400x251.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/NOOBS-696x436.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/NOOBS-670x420.png 670w, https://www.servethehome.com/wp-content/uploads/2020/11/NOOBS.png 921w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48252">NOOBS Selecting Raspberry Pi OS 32-bit</figcaption></figure>
<p>When the installation is completed, run the following commands:</p>
<pre><code>sudo rpi-eeprom-update -a
sudo reboot</code></pre>
<p>After you have completed the EEPROM update, we need to now update the firmware and switch to the community UEFI firmware. First, start by going to the Raspberry Pi <a href="https://github.com/raspberrypi/firmware">Github page</a> and download the latest firmware. Next, go to pftf’s UEFI <a href="https://github.com/pftf/RPi4">Github page</a>, and download it as well.</p>
<p>Safely shutdown your Pi and take the microSD card out. Next, plug your microSD card into a computer and prepare to update the files on it. To get started, we need to format the RECOVERY partition and rename it UEFI:</p>
<figure id="attachment_48259" aria-describedby="caption-attachment-48259"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/format.png" alt="Format" width="251" height="460" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/format.png 251w, https://www.servethehome.com/wp-content/uploads/2020/11/format-164x300.png 164w, https://www.servethehome.com/wp-content/uploads/2020/11/format-229x420.png 229w" sizes="(max-width: 251px) 100vw, 251px"><figcaption id="caption-attachment-48259">SD CARD Format for UEFI</figcaption></figure>
<p>Next, drag and drop the new files from the <strong>boot folder in the firmware-master</strong> onto the <strong>SD card</strong>. Start by going to the firmware-master folder and selecting everything. Once completed, you must remove the four files starting with the name “kernel”:</p>
<figure id="attachment_48260" aria-describedby="caption-attachment-48260"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/kernel.png" alt="Kernel" width="605" height="87" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/kernel.png 605w, https://www.servethehome.com/wp-content/uploads/2020/11/kernel-400x58.png 400w" sizes="(max-width: 605px) 100vw, 605px"><figcaption id="caption-attachment-48260">VMware ESXi Raspberry Pi Kernel in firmware-master</figcaption></figure>
<p>Next, do the same thing for the files from the <strong>UEFI firmware</strong>. Make sure you <strong>replace/overwrite</strong>&nbsp;the files with the updated ones.</p>
<figure id="attachment_48255" aria-describedby="caption-attachment-48255"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/UEFI-800x347.png" alt="UEFI" width="696" height="302" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/UEFI-800x347.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/UEFI-400x173.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/UEFI-1536x666.png 1536w, https://www.servethehome.com/wp-content/uploads/2020/11/UEFI-696x302.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/UEFI-1068x463.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/UEFI-969x420.png 969w, https://www.servethehome.com/wp-content/uploads/2020/11/UEFI.png 1829w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48255">VMware ESXi Raspberry Pi UEFI Setup</figcaption></figure>
<p>Now, open the config.txt file in the UEFI drive. We need to modify it by adding a line</p>
<pre><code>gpu_mem=32</code></pre>
<p>You can now put the SD card back into your RPI.</p>
<h2>Setting up ESXi for ARM</h2>
<p>To get started you will need to navigate to <a href="https://flings.vmware.com/esxi-arm-edition#summary">VMware’s page</a> for ESXi for ARM. Once there you will need to create an account and download the ISO. You will need to burn this ISO to a separate USB thumb drive. To burn it to the drive, you can utilize <a href="https://rufus.ie/">Rufus</a>.</p>
<figure id="attachment_48258" aria-describedby="caption-attachment-48258"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/esxi.png" alt="Esxi" width="352" height="485" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/esxi.png 419w, https://www.servethehome.com/wp-content/uploads/2020/11/esxi-218x300.png 218w, https://www.servethehome.com/wp-content/uploads/2020/11/esxi-305x420.png 305w" sizes="(max-width: 352px) 100vw, 352px"><figcaption id="caption-attachment-48258">Rufus Esxi for Raspberry Pi</figcaption></figure>
<p>After you have burned the ISO to your thumb drive, you need to plug it into your Pi and turn it on. You will see a new UEFI boot menu, press escape, and get into the UEFI to make any changes.</p>
<p>Click on&nbsp;<strong>Device Manager&nbsp;</strong>and then click <strong>Limit Ram to 3GB</strong> and change it to <strong>Disabled</strong>.</p>
<figure id="attachment_48262" aria-describedby="caption-attachment-48262"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Unlimited-RAM-800x557.png" alt="Unlimited RAM" width="696" height="485" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Unlimited-RAM-800x557.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Unlimited-RAM-400x279.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Unlimited-RAM-696x485.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Unlimited-RAM-603x420.png 603w, https://www.servethehome.com/wp-content/uploads/2020/11/Unlimited-RAM-100x70.png 100w, https://www.servethehome.com/wp-content/uploads/2020/11/Unlimited-RAM.png 1034w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48262">Unlimited RAM</figcaption></figure>
<p>Press <strong>F10</strong> and save the change you made. Exit the UEFI and press <strong>Enter</strong> to boot to the USB drive. The system will then boot into the ESXi installer. One option for installing ESXi to an SSD, using something like a <a href="https://www.amazon.com/StarTech-com-SATA-USB-Cable-USB3S2SAT3CB/dp/B00HJZJI84/ref=sr_1_1_sspa?dchild=1&amp;keywords=usb+to+sata+startech&amp;qid=1604788276&amp;sr=8-1-spons&amp;psc=1&amp;spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUEySDJEUE9JUUgxQzlQJmVuY3J5cHRlZElkPUEwNTM2MTQ1RDA0VFA3V0lJUUZYJmVuY3J5cHRlZEFkSWQ9QTA3MzMzOTgzUzAwNTNCQkxWSk9XJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==">Startech USB3.0 to SATA adapter</a>. Another option is to install it to another USB thumb drive and use iSCSI storage for your VMs.</p>
<figure id="attachment_48263" aria-describedby="caption-attachment-48263"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/SSD-800x600.jpg" alt="SSD" width="696" height="522" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/SSD-800x600.jpg 800w, https://www.servethehome.com/wp-content/uploads/2020/11/SSD-400x300.jpg 400w, https://www.servethehome.com/wp-content/uploads/2020/11/SSD-696x522.jpg 696w, https://www.servethehome.com/wp-content/uploads/2020/11/SSD-560x420.jpg 560w, https://www.servethehome.com/wp-content/uploads/2020/11/SSD-80x60.jpg 80w, https://www.servethehome.com/wp-content/uploads/2020/11/SSD-265x198.jpg 265w, https://www.servethehome.com/wp-content/uploads/2020/11/SSD.jpg 1000w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48263">Intel DC S3500 SATA SSD with StarTech USB adapter</figcaption></figure>
<p>Select your disk, it has to be one other than the USB installer drive or the microSD card. Once you have selected your disk, the installer will format it and destructively remove all data. You need only to assign a password and installation will begin. When you are done, simply remove the installation media and reboot.</p>
<h2>Getting Started in VMware ESXi on the Pi</h2>
<p>To get started, open a web browser on another computer and point it to the IP of your Pi. Once there, you should see the familiar ESXi home page. For more information on setting up ESXi, please see our <a href="https://www.servethehome.com/building-a-lab-part-3-configuring-vmware-esxi-and-truenas-core/">Building a Lab series</a>.</p>
<figure id="attachment_48264" aria-describedby="caption-attachment-48264"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/esxi-home-800x450.png" alt="Esxi Home" width="696" height="392" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/esxi-home-800x450.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/esxi-home-400x225.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/esxi-home-696x392.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/esxi-home-1068x601.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/esxi-home-746x420.png 746w, https://www.servethehome.com/wp-content/uploads/2020/11/esxi-home.png 1276w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48264">VMware Esxi Home with VMware ESXi on Arm Fling Label</figcaption></figure>
<p>The first step we should do is to add our NTP servers. Go to&nbsp;<strong>Manage&nbsp;</strong>then to <strong>System</strong><strong>&nbsp;</strong>and finally to <strong>Time and Date. </strong>Add some NTP servers, then click on&nbsp;<strong>Services&nbsp;</strong>and start the <strong>ntpd</strong> service.</p>
<figure id="attachment_48265" aria-describedby="caption-attachment-48265"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/time-and-date-800x399.png" alt="Time And Date" width="696" height="347" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/time-and-date-800x399.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/time-and-date-400x200.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/time-and-date-696x347.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/time-and-date-842x420.png 842w, https://www.servethehome.com/wp-content/uploads/2020/11/time-and-date.png 1034w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48265">VMware ESXi on Arm Fling Time And Date</figcaption></figure>
<p>If you wanted to add iSCSI storage you can. For my testing, I added a 1TiB iSCSI LUN from my production TrueNAS Core box.</p>
<figure id="attachment_48267" aria-describedby="caption-attachment-48267"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/iscsi-800x256.png" alt="Iscsi" width="696" height="223" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/iscsi-800x256.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/iscsi-400x128.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/iscsi-696x223.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/iscsi-1068x342.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/iscsi.png 1281w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48267">VMware ESXi on Arm Fling iSCSI</figcaption></figure>
<p>Additionally, you can add it to a vCenter just like a normal ESXi host. You simply right-click on a datacenter, press <strong>A</strong><strong>dd Host,&nbsp;</strong>type your IP address, credentials and select a license.</p>
<figure id="attachment_48266" aria-describedby="caption-attachment-48266"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/vsphere-800x402.png" alt="Vsphere" width="696" height="350" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/vsphere-800x402.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/vsphere-400x201.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/vsphere-696x349.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/vsphere-1068x536.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/vsphere-837x420.png 837w, https://www.servethehome.com/wp-content/uploads/2020/11/vsphere.png 1281w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48266">VMware ESXi on Arm Fling vSphere</figcaption></figure>
<p>At this point, you are basically ready to get going with the VMware ESXi on Arm Fling using your Raspberry Pi.</p>
<h2>A Word on Some Limitations</h2>
<p>There are some limitations to this setup, however. As one example, ESXi on Arm cannot run X86 compatible operating systems. As a result, you will not be able to install Windows in the traditional sense. At this time Windows 10 for ARM is not supported. You will be able to install operating systems compiled for Arm, however. As a result, we see Ubuntu Server being a more popular guest OS than Windows Server.</p>
<p>Using ESXi on a Raspberry Pi in conjunction with something like Kubernetes makes for an interesting solution, however. This gives us tinkerers the ability to build an entire three node cluster on a single Raspberry Pi. While I certainly would not suggest doing this in production, it is a fantastic opportunity for learning and development.</p>
<h2>Final Words</h2>
<p>As Patrick stated in his piece <a href="https://www.servethehome.com/of-bbq-and-virtualization-large-nodes/">of BBQ and Virtualization</a>, large nodes make sense. With the advent of ESXi on Arm, we get to look at the other side of that picture. If you need a highly available solution for a lightweight application, you can use a cluster of Raspberry Pis to build a vSAN capable of obtaining that goal. Alternatively, if you are new to this world and just trying to learn, having ESXi available gives you access to what is used in the enterprise.</p>
<p>One of the coolest possibilities for this platform is for the Raspberry Pi to become a witness node for a cluster in vCenter. If you have two production x86 servers and are planning to build a vSAN, a RaspberryPi can be added as a witness node. This will effectively save you a ton of money by not having to buy a full-blown third server. Using a small node like this in the future can lower both up-front as well as operating costs over time due to the low power consumption. A note of warning, however. That is not an officially supported topology by VMWare, but it is certainly interesting! For those using other virtualization setups such as standard KVM on Linux clusters, this is a great use case as well.</p>
<figure id="attachment_47443" aria-describedby="caption-attachment-47443"><a href="https://www.servethehome.com/nvidia-bluefield-2-and-bluefield-2x-dpu-offerings-launched/nvidia-dpu-roadmap/" rel="attachment wp-att-47443"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap.jpg" alt="NVIDIA DPU Roadmap" width="1721" height="911" srcset="https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap.jpg 1721w, https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap-400x212.jpg 400w, https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap-800x423.jpg 800w, https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap-1536x813.jpg 1536w, https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap-696x368.jpg 696w, https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap-1068x565.jpg 1068w, https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap-793x420.jpg 793w" sizes="(max-width: 1721px) 100vw, 1721px"></a><figcaption id="caption-attachment-47443"><a href="https://www.servethehome.com/nvidia-shows-dpu-roadmap-combining-arm-cores-gpu-and-networking/">NVIDIA DPU</a> Roadmap</figcaption></figure>
<p>With Apple switching to Arm, NVIDIA looking to <a href="https://www.servethehome.com/nvidia-to-acquire-arm-in-major-shift/">acquire</a> Arm, and <a href="https://www.servethehome.com/an-arm-opportunity-with-cloud-service-providers/2/">Cloud Providers</a> looking more at Arm, VMWare has made a strong move here. To be clear, it needed to do this as it was falling behind in the edge. Likewise, VMware is many years behind Amazon AWS in the <a href="https://www.servethehome.com/what-is-a-dpu-a-data-processing-unit-quick-primer/">DPU</a> architecture but is catching up with <a href="https://www.servethehome.com/vmware-project-monterey-esxi-on-arm-on-dpu/">VMware Project Monterey ESXi on Arm on DPU</a>. It may be harder to buy a DPU today to work with ESXi on Arm, but it is relatively low-cost to simply get a Raspberry Pi. I am personally excited to see what happens next.</p>
        </div></div>]]>
            </description>
            <link>https://www.servethehome.com/getting-started-with-vmware-esxi-on-arm-with-a-raspberry-pi/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029404</guid>
            <pubDate>Sun, 08 Nov 2020 21:49:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Looping Techniques]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25029363">thread link</a>) | @janprincek
<br/>
November 8, 2020 | https://www.pythonstacks.com/blog/looping-techniques/ | <a href="https://web.archive.org/web/*/https://www.pythonstacks.com/blog/looping-techniques/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <p><span>In this short tutorial, you will learn how to use loops more effectively in different situations.</span><br>
&nbsp;</p>

<h3><span>Looping over two lists at the same time</span><p>

<span>There are some moments in coding, where you would be required to <span>iterate/loop over two lists</span> or sequences at the <span>same time.</span></span></p></h3>

<p><span>This can be achieved using the <code>zip()</code> function.</span></p>

<pre><code>colors = ["red", "blue", "white"]
types = ["warm", "cool", "neutral"]

for c, t in zip(colors, types):
    print(c + " is " + t)</code></pre>

<p><span><strong>Program Output:</strong></span></p>

<pre><code>red is warm
blue is cool
white is neutral</code></pre>





<h3><span>The <span>enumerate()</span> function.</span></h3>

<p><span>When looping through a sequence (lists, tuple, string, etc), the <span>position index</span> and <span>corresponding value </span>can be retrieved at the <span>same time</span> using the <code>enumerate()</code> function.</span></p>

<pre><code>colors = ["orange", "brown", "indigo", "black"]

for i, v in enumerate(colors):
    print(i , v)</code></pre>

<p><span><strong>Program Output:</strong></span></p>

<pre><code>0 orange
1 brown
2 indigo
3 black</code></pre>



<p><span>The <code>enumerate()</code> function returns individual elements in the list with their <span>indexes</span>.</span></p>





<h3><span>Looping through a dictionary with the<span> items()</span> method</span><span>.</span><br>
&nbsp;</h3>

<p><span>We can get both the keys and the corresponding values when looping over a dictionary by using the <code>items()</code> method.</span></p>

<pre><code>grades = {'Ana': 'B', 'John':'A+', 'Denise':"A", "katy": 'A'}

for name, g in grades.items():
    print(name + " had " + g)</code></pre>

<p><span><strong>Program Output:</strong></span></p>

<pre><code>Ana had B
John had A+
Denise had A
katy had A</code></pre>





<h3><span>Reverse looping.</span><br>
&nbsp;</h3>

<p><span>To loop over a range of numbers in reverse, first, specify the range and then call the <code>reversed()</code> function.</span></p>

<pre><code>for r in reversed(range(7)):
    print(r)</code></pre>

<p><span><strong>Program Output:</strong></span></p>

<pre><code>6
5
4
3
2
1
0</code></pre>



<p><em><span>The same analogy applies to looping over a list in reverse:</span></em></p>

<pre><code>colors = ["blue", "red", "black", "yellow"]

for i in reversed(colors):
    print(i)</code></pre>

<p><span><strong>Program Output:</strong></span></p>

<pre><code>yellow
black
red
blue</code></pre>








        </div></div>]]>
            </description>
            <link>https://www.pythonstacks.com/blog/looping-techniques/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029363</guid>
            <pubDate>Sun, 08 Nov 2020 21:44:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[More Changes to Oil's Syntax]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25029243">thread link</a>) | @todsacerdoti
<br/>
November 8, 2020 | http://www.oilshell.org/blog/2020/11/more-syntax.html | <a href="https://web.archive.org/web/*/http://www.oilshell.org/blog/2020/11/more-syntax.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
  <!-- INSERT LATCH HTML -->
<p><a href="http://www.oilshell.org/blog/">blog</a> | <a href="http://www.oilshell.org/">oilshell.org</a></p>

<p>
  2020-11-08
</p>
<p>The recent <a href="http://www.oilshell.org/release/0.8.3/">0.8.3</a> and <a href="http://www.oilshell.org/release/0.8.4/">0.8.4</a> releases were
so big that it's taking <strong>five blog posts</strong> to describe them!</p>
<ol>
<li><a href="http://www.oilshell.org/blog/2020/10/big-changes.html">Big Changes to the Oil Language</a>: I published this
post about the expression language a couple weeks ago.</li>
<li><em>More Changes to Oil's Syntax</em>.  <strong>This post</strong> describes more syntactic
changes, including enhancements to shell builtins.</li>
<li><a href="http://www.oilshell.org/blog/2020/11/proposed-syntax.html">Proposed Changes to Oil's Syntax</a>.  New constructs I
left syntactic space for.  We cleaned up a lot, but there are still more
warts in shell.</li>
<li><a href="http://www.oilshell.org/blog/2020/11/runtime-semantics.html">Changes to Shell Runtime Semantics</a>.  I overhauled
shell options, variable scope, and <code>proc</code>.</li>
<li><em>The Shell Programmer's Guide to <code>errexit</code></em>.  About error handling in shell
and Oil.</li>
</ol>
<p>(And that doesn't count <a href="http://www.oilshell.org/blog/2020/11/metrics.html">yesterday's metrics post</a>, which isn't
essential.)</p>
<p>A quick story that motivates these changes: A few months ago, I wrote the first
draft of <a href="http://www.oilshell.org/release/0.8.4/doc/idioms.html">Oil Language Idioms</a>.  That
led to more TODOs than expected, and to more docs, like <a href="http://www.oilshell.org/release/0.8.4/doc/shell-idioms.html">Shell Language
Idioms</a>.</p>
<p>Since then, I've been knocking off the TODOs.  So we're basically doing
<strong>documentation-driven development</strong>.</p>
<p>The purpose of this post is to <strong>get feedback</strong> about the Oil language.  That
said, I also want to spend time on <a href="http://www.oilshell.org/release/0.8.4/doc/">official
documentation</a>, so I may breeze through this quickly.
Please send feedback <a href="https://old.reddit.com/r/oilshell/comments/jqj86j/more_changes_to_oils_syntax/?">in the comments</a>, on <a href="http://www.oilshell.org/cross-ref.html?tag=Zulip#Zulip">Zulip</a>, or
on Github.</p>
 
<a name="special-variables-and-functions-_status-_match"></a>
<h2>Special Variables and Functions: <code>_status</code>, <code>_match()</code></h2>
<p>Shell has special variables like <code>$?</code> and <code>${BASH_REMATCH[@]}</code>.  They are
<strong>implicitly mutated</strong> by the interpreter.</p>
<p>Oil supports them, but I decided that we need a more consistent style with less
punctuation and CAPS.</p>
<p>A <strong>leading underscore</strong> gives these variables their own namespace, and lower
case makes them easier to type.</p>
<ul>
<li>Related to <a href="http://www.oilshell.org/blog/2020/10/osh-features.html#reliable-error-handling">the errexit
overhaul</a>:
<ul>
<li><code>_status</code> is a synonym for <code>$?</code>, and it's useful in expression mode.</li>
<li><code>@_pipeline_status</code> and <code>@_process_sub_status</code> for the result of process
concurrency constructs.</li>
</ul>
</li>
<li>Regex API:
<ul>
<li><code>_match()</code> is now implemented.  The call <code>_match(x)</code> is like <code>m.group(x)</code>
in Python, except it's not a method on a match object.</li>
<li><code>_start()</code> and <code>_end()</code> are planned.</li>
</ul>
</li>
<li>Design decision: If we ever get <a href="http://www.oilshell.org/blog/tags.html?tag=awk#awk">Awk</a>-like functionality,
<ul>
<li>We'll use  <code>_line</code>, <code>_field(1)</code>, and <code>_filename</code> ...</li>
<li>Instead of <code>$0</code>, <code>$1</code>, and <code>FILENAME</code></li>
<li><a href="http://www.oilshell.org/cross-ref.html?tag=QTSV#QTSV">QTSV</a> columns can also be named, eliminating the need for
<code>_field()</code>.</li>
</ul>
</li>
</ul>
<a name="stricter-syntax"></a>
<h2>Stricter Syntax</h2>
<a name="enforced-parse_backslash-in-unquoted-words"></a>
<h3>Enforced <code>parse_backslash</code> in Unquoted Words</h3>
<p>This is like the <a href="http://www.oilshell.org/blog/2020/10/big-changes.html#tightened-up-string-literals">string literal
changes</a> mentioned in the
last post.</p>
<p>See these nice tables!  <a href="https://github.com/oilshell/oil/issues/860">https://github.com/oilshell/oil/issues/860</a></p>
<p>Summary:</p>
<ul>
<li>Oil only has <strong>two meanings</strong> of <code>\n</code>: it's either a newline, or literally
<code>\n</code>, depending on the quotes.</li>
<li>Shell actually has <strong>three</strong>, as <code>echo \n</code> prints just <code>n</code>!  This is now
disallowed in Oil: use <code>echo n</code> or <code>echo \\n</code> instead.</li>
</ul>
<a name="syntax-error-for-seq-3trailing"></a>
<h3>Syntax Error For <code>@(seq 3)trailing</code></h3>
<p>All constructs beginning with a <code>@</code> sigil must occupy a whole word.  There's no
implicit joining as with bash:</p>
<pre><code><span>$</span> <span>echo x"$@"y </span>  
</code></pre>
<p>The <code>@</code> constructs are:</p>
<pre><code><span>$</span> <span>echo @myarray</span>
<span>$</span> <span>echo @(split command sub)</span>
<span>$</span> <span>echo @array_func(x, y) @glob(pat) @split(s)</span>
</code></pre>
<a name="keywords-and-operators"></a>
<h2>Keywords and Operators</h2>
<a name="the-and-_-pseudo-assignment-keywords"></a>
<h3>The <code>=</code> and <code>_</code> "Pseudo-Assignment" Keywords</h3>
<p>This was done in a previous release, but deserves mention here.  Shell
assignments take expressions on the RHS:</p>
<pre><code>var x = 42 + f(x)
</code></pre>
<p>You can pretty-print an expression like this, which is useful in the REPL:</p>
<pre><code>= 42 + f(x)    
</code></pre>
<p>You can also <strong>ignore</strong> the result of an expression:</p>
<pre><code>_ 42 + f(x)    
_ = 42 + f(x)  
</code></pre>
<p>This is useful for functions with side effects:</p>
<pre><code>_ mylist.append(x)
_ mylist.extend(['str', var])
</code></pre>
<p>However we also have a shell style:</p>
<pre><code>push :mylist str $var
</code></pre>
<p>I don't expect <code>_</code> to be used that often in real code.  Functions usually
return values and are used like <code>echo $len(x)</code>.</p>
<a name="removed-pass-based-on-your-feedback"></a>
<h3>Removed <code>pass</code> Based On Your Feedback</h3>
<p>The <code>pass</code> keyword was intended for left-to-right function calls as in
<a href="http://www.oilshell.org/cross-ref.html?tag=dplyr#dplyr">dplyr</a>.</p>
<p>However, many people mentioned that it conflicted with existing programs.  And
we can use the <code>_</code> keyword instead.</p>
<p>So I removed it.  I listened to your feedback!</p>
<a name="the-operator-for-approximate-equality"></a>
<h3>The <code>~==</code> Operator for Approximate Equality</h3>
<p>Oil has typed data, so this operator will help us be as convenient as <a href="http://www.oilshell.org/blog/tags.html?tag=awk#awk">Awk</a>,
while avoiding the pitfalls of JavaScript's <code>==</code>:</p>
<pre><code>var mystr = '42'  

if (x == 42) {    
  echo 'yes'    
}

if (x ~== 42) {   
  echo 'yes'
}
</code></pre>
<p>We might also use this operator for approximate floating point comparisons.  I
could use help on this!</p>
<a name="doc-comments-like-are-now-recognized"></a>
<h3>Doc Comments Like <code>###</code> Are Now Recognized</h3>
<p>The parser now recognizes doc comments and attached them to the AST.  It's the
first line after an opening <code>{</code> with <code>###</code>.</p>
<pre><code>proc restart(pid) {
   

   kill $pid
}
</code></pre>
<p>It also works for shell-style functions:</p>
<pre><code>f() {
   

   kill $1
}
</code></pre>
<p>We can use this for autocompletion and more.  Feedback is welcome.</p>
<a name="many-builtin-commands-enhanced"></a>
<h2>Many Builtin Commands Enhanced</h2>
<a name="repr-renamed-to-pp-pretty-print"></a>
<h3><code>repr</code> Renamed to <code>pp</code> (pretty print)</h3>
<p>(1) <code>pp cell</code> pretty-prints cells, which are the locations of variables.</p>
<p>Cells have flags like <code>-x</code> (export).  This builtin is very useful for debugging
shell programs!</p>
<pre><code><span>osh$</span> <span>export FOO=bar</span>
<span>osh$</span> <span>pp cell FOO</span>
FOO = (cell exported:T readonly:F nameref:F val:(value.Str s:bar))
</code></pre>
<p>(This format isn't stable yet.  See <a href="https://github.com/oilshell/oil/issues/817">issue 817</a>).</p>
<p>(2) <code>pp proc</code> shows doc comments.  It prints a table, which means it's the
first usage of <a href="http://www.oilshell.org/cross-ref.html?tag=QTSV#QTSV">QTSV</a> in Oil!</p>
<pre><code><span>osh$</span> <span>pp proc</span>
proc_name       doc_comment
f       'doc \' comment with " quotes'
g       ''
</code></pre>
<p>Now we need a <code>QTSV_PAGER</code>, i.e. something like <code>less</code> for tables.  <a href="https://lobste.rs/s/zvallq/pretty_csv_viewing_on_command_line">I recently
learned</a> that we
can do a quick and dirty job with <code>column</code>.</p>
<a name="added-long-flags-shopt-set-test-dir"></a>
<h3>Added Long Flags: <code>shopt --set</code>, <code>test --dir</code></h3>
<ul>
<li>We now have <code>shopt --set</code> and <code>--unset</code> instead of <code>shopt -s</code> and <code>-u</code>.</li>
<li>And <code>test --dir</code> <code>--exists</code>, etc. instead of <code>test -d</code>, <code>-e</code>, etc.</li>
</ul>
<p>Other enhancements to <code>shopt</code> and <code>test</code>:</p>
<ul>
<li><code>shopt -p</code> respects option groups.  For example, <code>shopt -p oil:all</code> prints
the current value of all Oil options.
<ul>
<li>We may want <code>shopt --print</code> to use a clearer format.</li>
</ul>
</li>
<li>Implemented <code>simple_test_builtin</code>, which enforces that <code>test</code>  accepts 2 or 3
arguments, and isn't spelled <code>[</code>.  This is on in <code>oil:all</code>.
<ul>
<li>See <a href="http://www.oilshell.org/blog/2017/08/31.html">Problems With the test Builtin: What Does -a
Mean?</a></li>
</ul>
</li>
</ul>
<p>The idiom for turning on Oil is now:</p>
<pre><code>shopt --set oil:basic  
shopt --set oil:all    
</code></pre>
<p>Or you can use <code>bin/oil</code> to turn on <code>oil:all</code>.</p>
<a name="structured-io-read-line-write-qsn"></a>
<h3>Structured I/O: <code>read --line</code>, <code>write --qsn</code></h3>
<p>These changes were discussed in <a href="http://www.oilshell.org/blog/2020/10/osh-features.html">Four Features That Justify a New Unix
Shell</a>.  We want to remove the need for ad hoc parsing
and splitting.</p>
<p>So we have preliminary <a href="http://www.oilshell.org/cross-ref.html?tag=QSN#QSN">QSN</a> support, but we still need more
<a href="http://www.oilshell.org/cross-ref.html?tag=QTSV#QTSV">QTSV</a> support.</p>
<p>Another idea for a primitive:</p>
<ul>
<li><code>read -qsn-cells :var1 :var2</code> should read multiple cells on a line.  It
should split by tabs, and decode each field.  On the other hand,
<a href="http://www.oilshell.org/cross-ref.html?tag=QTSV#QTSV">QTSV</a> has typed columns (integers, floats, and booleans).</li>
</ul>
<a name="added-block-arguments-shopt-fork-forkwait"></a>
<h3>Added Block Arguments: <code>shopt</code>, <code>fork</code>, <code>forkwait</code></h3>
<p>This came directly out of <a href="http://www.oilshell.org/release/0.8.4/doc/idioms.html">Oil Language
Idioms</a>.  Oil has a more consistent syntax:</p>
<pre><code>sleep 2 &amp;             
fork { sleep 2 }      

( sleep 2 )           
forkwait { sleep 2 }  
</code></pre>
<p>This allows us to use <code>&amp;</code> for redirects, and <code>()</code> for expressions.</p>
<p>We use the same block syntax to save and restore state:</p>
<pre><code>shopt --unset errexit {
  step1
  echo $?

  step2
  echo $?
}
</code></pre>
<p>This was enabled by a pleasant refactoring of the "option stack", which was
<a href="https://oilshell.zulipchat.com/#narrow/stream/208950-zephyr-asdl/topic/ASDL.2Fstatic.20typing.20success">aided by static
typing</a>.
This stack is now used consistently for many purposes:</p>
<ol>
<li><code>shopt</code> blocks</li>
<li>The broken POSIX <code>errexit</code> semantics.  Error handling is disabled in the
constructs <code>if / while / until / &amp;&amp;   ||   !</code>.  We unfortunately have to
implement this.</li>
<li>The <code>run</code> builtin, which undoes this bad behavior by re-enabling <code>errexit</code>.</li>
<li>The <code>strict_errexit</code> option, which detects code that would <strong>lose errors</strong>.</li>
<li>Disabling <a href="http://www.oilshell.org/cross-ref.html?tag=dynamic-scope#dynamic-scope">dynamic scope</a> in procs, which I describe in
an upcoming post.  The option stack follows the call stack.</li>
</ol>
<a name="conclusion"></a>
<h2>Conclusion</h2>
<p>This was post 2 of 5 that explains the Oil 0.8.3 and 0.8.4 release.  I wrote
down what I think Oil's idioms should be, and then I implemented them!</p>
<p>Please try <a href="http://www.oilshell.org/release/0.8.4/">Oil 0.8.4</a> and <a href="https://github.com/oilshell/oil/wiki/Where-To-Send-Feedback">let me
know</a> what
happens!</p>
<a name="appendix-issues-closed-in-083"></a>
<h2>Appendix: Issues Closed in 0.8.3</h2>
<p>These issues were closed for <a href="http://www.oilshell.org/release/0.8.3/">Oil 0.8.3</a>, and I discuss most
of them in this series of posts.</p>
<table>
<tbody><tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/835">#835</a>
  </td>
  <td>
    Make expression language compatible with Python
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/826">#826</a>
  </td>
  <td>
    clarify QSN use cases
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/775">#775</a>
  </td>
  <td>
    errexit not disabled where it should be
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/735">#735</a>
  </td>
  <td>
    remove 'pass' builtin
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/713">#713</a>
  </td>
  <td>
    long flags for shopt builtin, e.g. --set and --unset
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/711">#711</a>
  </td>
  <td>
    Oil should have a slurp builtin
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/582">#582</a>
  </td>
  <td>
    QSN serialization format: parser,  printer, builtin
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/501">#501</a>
  </td>
  <td>
    shopt should respect set -o options
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/476">#476</a>
  </td>
  <td>
    consider a different definition of strict_errexit
  </td>
</tr>
</tbody></table>




</div>]]>
            </description>
            <link>http://www.oilshell.org/blog/2020/11/more-syntax.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029243</guid>
            <pubDate>Sun, 08 Nov 2020 21:29:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Demystifying malloc]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028746">thread link</a>) | @riverg
<br/>
November 8, 2020 | https://river.codes/demystifying-malloc/ | <a href="https://web.archive.org/web/*/https://river.codes/demystifying-malloc/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content"> <article itemscope="" itemtype="https://schema.org/BlogPosting">  <div itemprop="articleBody"> <p>It feels wrong to use a tool without knowing fully how it works. As programmers it is hard to accept that there is only so much that can fit into our noggins at once, but looking at code benchmarks or stack traces to see that some large amount of time is spent in some low-level C code always makes me wonder what is really going on in there. Maybe you’re like me and occasionally start to use the ‘Go to definition’ IDE feature on standard libraries, and after a second or two of searching your window fills with scary underscores and <code>#DEFINE</code>s of things you didn’t know existed, you think maybe this thing was auto-generated and no human would bother writing this header-file-hell. Unsatisfied you go back to whatever you were working on, no closer to understanding what’s <em>really</em> going on down there.</p> <p>For me I was always perplexed by <code>malloc</code>. It’s a simple function, you ask for memory and it gives it to you. But <em>how</em> could a C function do that? What on Earth does it mean to <em>allocate</em> memory? Isn’t it all there, sitting on the bus, just <strong>waiting</strong> for us to issue some good ‘ol <code>MOV</code> instructions? Even worse, it is used <em>everywhere</em>. Even if you aren’t using C there’s a good chance you’re using <code>malloc</code>, every time you create a new object in a language implemented in C, like <code>cpython</code> for instance. It isn’t the only way to acquire memory, but it sure is a popular one.</p> <p>So let’s take a look at <code>malloc</code>, it can’t be that complicated right? Here’s a simple <code>malloc</code>:</p> <div><div><pre><code><span>void</span><span>*</span> <span>malloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>{</span>
  <span>return</span> <span>sbrk</span><span>(</span><span>size</span><span>);</span>
<span>}</span>
</code></pre></div></div> <p>From this you may be able to figure out what <code>sbrk</code> does. It gives us a chunk of memory. Specifically it allocates program heap memory of a given size and returns a pointer to it. What exactly is “it” that is being pointed to? Let’s start printing stuff and find out.</p> <div><div><pre><code><span>void</span><span>*</span> <span>a</span> <span>=</span> <span>sbrk</span><span>(</span><span>0</span><span>);</span>
<span>void</span><span>*</span> <span>b</span> <span>=</span> <span>sbrk</span><span>(</span><span>100</span><span>);</span>
<span>void</span><span>*</span> <span>c</span> <span>=</span> <span>sbrk</span><span>(</span><span>0</span><span>);</span>
<span>printf</span><span>(</span><span>"[a: %p]</span><span>\n</span><span>[b: %p]</span><span>\n</span><span>[c: %p]</span><span>\n</span><span>"</span><span>,</span> <span>a</span><span>,</span> <span>b</span><span>,</span> <span>c</span><span>);</span>
</code></pre></div></div> <div><div><pre><code>&gt; [a: 0x1065b4064]
  [b: 0x1065b4064]
  [c: 0x1065b40c8]
</code></pre></div></div> <p>Looks like <code>b</code> is the same as <code>a</code> and <code>c</code> is <code>b + 100</code> bytes. So <code>sbrk</code> returns a pointer to whatever the last ‘tip’ of the program heap is, and if we give it a size in bytes it’ll move that ‘tip’ up. Meaning if we <code>sbrk</code> ourselves that 100 byte chunk we can do whatever we want with it knowing that the next time we <code>sbrk</code> ourselves some more memory, it’ll be 100 bytes farther along.</p> <p>This may not be too satisfying, we’ve replaced one magical function with another. However, in this case <code>sbrk</code> is a system call. It’s going to jump the CPU over to some assembly to execute (the instruction set implemented by your CPU is very likely to have a set of functions for interfacing with memory), at least now we’re talking to the kernel instead of wondering what’s going in the the C standard library.</p> <p>So that’s <code>malloc</code>, simple right? Well judging from the length of this article you can probably deduce otherwise. There’s more here, and for two reasons:</p> <ol> <li><code>sbrk</code> is absolutely ancient and super-deprecated. In fact if you run these snippets on macOS you’re going to get tons of warnings (but hey, it still works!). It doesn’t work with virtual memory and it isn’t thread-safe. However, its API is very simple to use and <code>malloc</code> at one point in time very likely was implemented using <code>sbrk</code>.</li> <li>This implementation of <code>malloc</code> is incorrect. The first reason why, which you may be able to guess, is that <code>sbrk</code> can fail. Memory is a finite resource.</li> </ol> <p>According to <code>man sbrk</code>, the call can return -1 if it fails, but <code>malloc</code> is supposed to return <code>NULL</code>. This is fixed easily enough.</p> <div><div><pre><code><span>void</span><span>*</span> <span>malloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>{</span>
  <span>void</span><span>*</span> <span>chunk_start</span> <span>=</span> <span>sbrk</span><span>(</span><span>size</span><span>);</span>
  <span>return</span> <span>chunk_start</span> <span>==</span> <span>(</span><span>void</span><span>*</span><span>)</span><span>-</span><span>1</span> <span>?</span> <span>NULL</span> <span>:</span> <span>chunk_start</span><span>;</span>
<span>}</span>
</code></pre></div></div> <p>One more thing. <code>malloc(0)</code> has special behavior, it needs to return <code>NULL</code> as well, otherwise you’d be able to get a pointer back from <code>malloc</code> that you didn’t actually allocate, and that would be weird.</p> <div><div><pre><code><span>void</span><span>*</span> <span>malloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>size</span> <span>==</span> <span>0</span><span>)</span> <span>return</span> <span>NULL</span><span>;</span>
  <span>void</span><span>*</span> <span>chunk_start</span> <span>=</span> <span>sbrk</span><span>(</span><span>size</span><span>);</span>
  <span>return</span> <span>chunk_start</span> <span>==</span> <span>(</span><span>void</span><span>*</span><span>)</span><span>-</span><span>1</span> <span>?</span> <span>NULL</span> <span>:</span> <span>chunk_start</span><span>;</span>
<span>}</span>
</code></pre></div></div> <p>Great, now the user can actually know whether their memory request was fulfilled. We’re still missing something though, the result of our <code>malloc</code> doesn’t work with <code>free</code>. What <em>is</em> <code>free</code> exactly?</p> <p>According to <code>man free</code>, <code>free</code> will remove the allocation (“free”ing the space) from an input pointer that was previously returned from malloc. Now <code>sbrk</code> has a feature where when a negative input is passed to it, it will move the tip of the heap <em>down</em> instead of up. So effectively it allows us to push and pop from the program heap, because the heap is a stack and computer terminology is silly.</p> <p>Unfortunately this isn’t enough for us. Imagine a user does the following:</p> <div><div><pre><code><span>void</span><span>*</span> <span>a</span> <span>=</span> <span>malloc</span><span>(</span><span>500</span><span>);</span>
<span>// We can now call sbrk(-500) to free a.</span>
<span>void</span><span>*</span> <span>b</span> <span>=</span> <span>malloc</span><span>(</span><span>1000</span><span>);</span>
<span>// But how do we free a from here?</span>
</code></pre></div></div> <p>This is the age-old problem of trying to delete something from the middle of the stack. We could pop everything off of the stack until we reach the memory we’re trying to delete (storing it somewhere else, we could even use the disk), then pop the item to delete, then push everything else back onto the stack. This would be miserably slow. We could also abandon the stack mentality and just use <code>memcpy</code> to copy over the old bytes. This would also be very slow, usually we expect <code>free</code> to take an insignificant amount of time to complete. In either of these cases, we’ve created a new problem: when shifting all of the old memory to utilize the newly free’d space, all of the pointers in the program refering to that old memory would be invalidated.</p> <p>It looks like we’re going to have take matters into our own hands. Maybe in the future we’ll have more memory than we know what to do with and never free anything. Until then we’ll need to do something clever. We have one thing going for us though, <code>malloc</code> always returns a pointer to <em>contiguous</em> memory. If we have a single “hole” of <code>free</code>d memory in the heap large enough to use somewhere, we can use it. It’s simply a matter, then, of us keeping track of the allocated chunks (and the “holes” created by <code>free</code>ing those chunks) ourselves.</p> <p>So maybe it was wrong to say earlier that <code>sbrk</code> “allocates” memory for us, because now it seems like it just moves some pointer and tells us where it used to be. The actual “allocation” part of <code>malloc</code> is something we’ll have to implement. Let’s set up a general outline of what we want to accomplish.</p> <div><div><pre><code><span>typedef</span> <span>struct</span> <span>chunk</span> <span>{</span>
  <span>size_t</span> <span>size</span><span>;</span>  <span>// size of user-accessible memory.</span>
  <span>struct</span> <span>chunk</span><span>*</span> <span>next</span><span>;</span>
  <span>bool</span> <span>allocated</span><span>;</span>
<span>}</span> <span>Chunk</span><span>;</span>

<span>Chunk</span><span>*</span> <span>heap</span><span>;</span>

<span>void</span><span>*</span> <span>chunk_data</span><span>(</span><span>Chunk</span><span>*</span> <span>chunk</span><span>)</span> <span>{</span>
  <span>// Adding 1 to a Chunk* will get us to the part of memory</span>
  <span>// directly after the fields.</span>
  <span>return</span> <span>chunk</span> <span>?</span> <span>chunk</span><span>+</span><span>1</span> <span>:</span> <span>NULL</span><span>;</span>
<span>}</span>

<span>// Returns the Chunk corresponding to the chunk's data.</span>
<span>Chunk</span><span>*</span> <span>chunk_metadata</span><span>(</span><span>void</span><span>*</span> <span>ptr</span><span>)</span> <span>{</span>
  <span>return</span> <span>ptr</span> <span>?</span> <span>(</span><span>Chunk</span><span>*</span><span>)</span><span>ptr</span><span>-</span><span>1</span> <span>:</span> <span>NULL</span><span>;</span>
<span>}</span>

 <span>// TODO: Do the hard part.</span>
<span>Chunk</span><span>*</span> <span>find_or_reserve_chunk</span><span>(</span><span>size_t</span> <span>size</span><span>);</span>

<span>void</span><span>*</span> <span>malloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>size</span> <span>==</span> <span>0</span><span>)</span> <span>return</span> <span>NULL</span><span>;</span>
  <span>return</span> <span>chunk_data</span><span>(</span><span>find_or_reserve_chunk</span><span>(</span><span>size</span><span>));</span>
<span>}</span>

<span>void</span> <span>free</span><span>(</span><span>void</span><span>*</span> <span>ptr</span><span>)</span> <span>{</span>
  <span>Chunk</span><span>*</span> <span>chunk</span> <span>=</span> <span>chunk_metadata</span><span>(</span><span>ptr</span><span>);</span>
  <span>if</span> <span>(</span><span>!</span><span>chunk</span><span>)</span> <span>return</span><span>;</span>
  <span>chunk</span><span>-&gt;</span><span>allocated</span> <span>=</span> <span>false</span><span>;</span>
<span>}</span>
</code></pre></div></div> <p>We want some way to model the heap, so a linked list sounds simple enough. We could extend it to create an actual stack, but it isn’t really needed here. Normally we’d implement a linked list using <code>malloc</code>, but we’re implementing <code>malloc</code> so we can’t really use that, can we? So let’s just sneak our data structure in with the user data. The general idea is that for every chunk of memory allocated by <code>malloc</code>, the we store a few bytes (specifically <code>sizeof(Chunk)</code>) of metadata about that chunk right beforehand. We could store a pointer in Chunk to the actual user memory, but since the memory is contiguous we can easily compute where the metadata ends and the user data begins. <code>free</code>ing then becomes super easy, we can just get the metadata and mark that it’s no longer allocated. The hard part is using that information.</p> <div><div><pre><code><span>// Allocates a new chunk right after 'prev'.</span>
<span>Chunk</span><span>*</span> <span>allocate_chunk</span><span>(</span><span>Chunk</span><span>*</span> <span>prev</span><span>,</span> <span>size_t</span> <span>size</span><span>);</span>

<span>Chunk</span><span>*</span> <span>find_or_reserve_chunk</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>{</span>
  <span>Chunk</span><span>*</span> <span>prev_chunk</span> <span>=</span> <span>NULL</span><span>;</span>

  <span>// Initialize the heap if necessary.</span>
  <span>if</span> <span>(</span><span>!</span><span>heap</span><span>)</span> <span>{</span>
    <span>heap</span> <span>=</span> <span>allocate_chunk</span><span>(</span><span>prev_chunk</span><span>,</span> <span>size</span><span>);</span>
    <span>return</span> <span>chunk_data</span><span>(</span><span>heap</span><span>);</span>
  <span>}</span>

  <span>// Scan the heap for holes large enough for the chunk we want.</span>
  <span>Chunk</span><span>*</span> <span>chunk</span> <span>=</span> <span>heap</span><span>;</span>
  <span>while</span> <span>(</span><span>chunk</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>!</span><span>chunk</span><span>-&gt;</span><span>allocated</span> <span>&amp;&amp;</span> <span>chunk</span><span>-&gt;</span><span>size</span> <span>&gt;=</span> <span>size</span><span>)</span> <span>break</span><span>;</span>
    <span>prev_chunk</span> <span>=</span> <span>chunk</span><span>;</span>
    <span>chunk</span> <span>=</span> <span>chunk</span><span>-&gt;</span><span>next</span><span>;</span>
  <span>}</span>

  <span>if</span> <span>(</span><span>chunk</span><span>)</span> <span>{</span>
    <span>chunk</span><span>-&gt;</span><span>allocated</span> <span>=</span> <span>true</span><span>;</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>chunk</span> <span>=</span> <span>allocate_chunk</span><span>(</span><span>prev_chunk</span><span>,</span> <span>size</span><span>);</span>
  <span>}</span>

  <span>return</span> <span>chunk</span><span>;</span>
<span>}</span>
</code></pre></div></div> <p>From here you can devise faster ways of doing this. The memory-speed tradeoff here is real, you can avoid scanning the heap every time by reserving a portion of the start of the heap for a hash map storing holes by size requirement, but then of course you’ve got less of the heap for the user. You also need to consider the size of this portion. You won’t be able to grow it, since then you’d be invalidating user pointers.</p> <p>If speed is less of a concern, you’d want to scan the entire heap rather than stopping at the first available hole. With the above implementation the user would often get back chunks of memory where the allocation is actually greater than they requested. If you scan the entire heap, you can look for the smallest hole that fits the requirements. Another way of doing this is to terminate the chunk to always fit the requested size, creating a new chunk for the leftover data. You’d have to make sure whatever leftover has enough room for the metadata fields.</p> <p>This is why the data stored in the pointer returned from <code>malloc</code> is uninitiliazed, it may have been a chunk from some previously requested memory.</p> <p>The last bit is where we actually build up the heap model:</p> <div><div><pre><code><span>Chunk</span><span>*</span> <span>allocate_chunk</span><span>(</span><span>Chunk</span><span>*</span> <span>prev</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> <span>{</span>
  <span>Chunk</span><span>*</span> <span>chunk</span> <span>=</span> <span>(</span><span>Chunk</span><span>*</span><span>)</span><span>sbrk</span><span>(</span><span>size</span> <span>+</span> <span>sizeof</span><span>(</span><span>Chunk</span><span>));</span>
  <span>if</span> <span>(</span><span>chunk</span> <span>==</span> <span>(</span><span>Chunk</span><span>*</span><span>)</span><span>-</span><span>1</span><span>)</span> <span>{</span>
    <span>return</span> <span>NULL</span><span>;</span>
  <span>}</span>

  <span>if</span> <span>(</span><span>prev</span><span>)</span> <span>{</span>
    <span>prev</span><span>-&gt;</span><span>next</span> <span>=</span> <span>chunk</span><span>;</span>
  <span>}</span>

  <span>chunk</span><span>-&gt;</span><span>size</span> <span>=</span> <span>size</span><span>;</span>
  <span>chunk</span><span>-&gt;</span><span>allocated</span> <span>=</span> <span>true</span><span>;</span>
  <span>chunk</span><span>-&gt;</span><span>next</span> <span>=</span> <span>NULL</span><span>;</span>
  <span>return</span> <span>chunk</span><span>;</span>
<span>}</span>
</code></pre></div></div> <p>We take in …</p></div></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://river.codes/demystifying-malloc/">https://river.codes/demystifying-malloc/</a></em></p>]]>
            </description>
            <link>https://river.codes/demystifying-malloc/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028746</guid>
            <pubDate>Sun, 08 Nov 2020 20:34:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Autodesk File: Bits of History, Words of Experience]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028729">thread link</a>) | @ingve
<br/>
November 8, 2020 | https://www.fourmilab.ch/autofile/ | <a href="https://web.archive.org/web/*/https://www.fourmilab.ch/autofile/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
<img src="https://www.fourmilab.ch/autofile/e5/figures/aiflag.png" width="189" height="179" alt="Autodesk flag">

<h3><i>Bits of History, Words of Experience</i></h3>
<address>
Edited by<br>
<a href="https://www.fourmilab.ch/" target="_top">John Walker</a>
</address>
<p>
<span face="Helvetica, Arial, sans-serif">Fifth Edition, 2017</span>
</p>


<p>
<em>The Autodesk File</em> chronicles the history of Autodesk, Inc.
and its principal product, AutoCAD,
through contemporary documents edited and annotated
by Autodesk founder and former CEO <a href="https://www.fourmilab.ch/" target="_top">John Walker</a>.
The book traces the company from the first glimmer of an idea in the
minds of the founders, through start-up, initial public stock
offering, and growth from a loose confederation of moonlighting
individuals to a leader in the industry of computer aided design.
The book is available in several different editions, suited
for on- or off-line reading with various tools.  Click on
the titles of the section describing the edition you prefer
to view it or download to your computer.
</p>

<h2><a href="https://www.fourmilab.ch/autofile/e5/">Fifth Edition</a></h2>

<p>
<a href="https://www.fourmilab.ch/autofile/e5/"><img src="https://www.fourmilab.ch/autofile/figures/af5_screenshot.png" width="256" height="252" alt="The Autodesk File: Fifth Edition"></a>
The Fifth Edition (2017) of <cite>The Autodesk File</cite> was prepared to
commemorate the thirty-fifth anniversary of the founding of Autodesk
in 1982.  Except for correction of a few typographical errors, the
content is identical to that of the 1994 fourth edition, but the
book has been entirely reformatted and updated to contemporary
Web standards.  The typography uses Unicode text entities, and should
be much easier on the eye.  Each chapter is now a single document,
instead of being broken into sections and subsections, and easier
to read without incessant clicking on navigation buttons.  All of
the AutoCAD sample drawings used as illustrations have been
re-made from their original PostScript plot files with higher
resolution.  The pop-up windows for footnotes (which were irritating
and ran afoul of some browser pop-up blockers) have been replaced by
<img src="https://www.fourmilab.ch/autofile/e5/i/footnote.png" width="16" height="16" alt="[Footnote]">
icons which display the footnote when clicked.  Cross-references
are indicated by an
<img src="https://www.fourmilab.ch/autofile/e5/i/xref.png" width="16" height="16" alt="[Ref]">
icon which navigates to the cited page when clicked.  A navigation
bar at the left provides instant access to all chapters, and
highlights the current chapter regardless of how you arrived there.
The <a href="https://www.fourmilab.ch/autofile/e5/">Fifth Edition</a> is compatible with
most modern desktop browsers.  The Safari browser on iOS mobile devices
(iPad, iPhone) has a serious flaw in scrolling text within a
window which has remained uncorrected for years.  On these devices,
you can read the
<a href="https://www.fourmilab.ch/autofile/e5/indexi.html">iOS work-around edition</a>,
which contains a device-specific fix for the problem.
<br>
</p>

<h2><a href="https://www.fourmilab.ch/autofile/www/autoframe.html">Fourth Edition with Frames</a></h2>

<p>
For older browsers which which support frames, this
edition allows navigation through the book with
a panel which lets you click chapter titles and go
directly to that chapter.  If, in addition, your browser
supports JavaScript, simply moving the mouse over a
footnote icon, like this one:
<img src="https://www.fourmilab.ch/autofile/www/i/foot.gif" width="15" height="15" alt="[Footnote]">
will pop up a window containing the footnote.  Moving the
mouse over other footnotes displays them in the auxiliary window.
Browsers without JavaScript (or users who have disabled
JavaScript in their browsers) may display footnotes in
the main document window by clicking the footnote icon,
then use their browser's “Back” button to return to the
main text.  Users with more modern browers will find the
<a href="https://www.fourmilab.ch/autofile/e5/">Fifth Edition</a> easier to read and
navigate.
</p>

<h2><a href="https://www.fourmilab.ch/autofile/www/autofile.html">No-Frame Web Edition</a></h2>

<p>
<a href="https://www.fourmilab.ch/autofile/www/autofile.html"><img src="https://www.fourmilab.ch/autofile/www/figures/any_browser.gif" width="88" height="31" alt="Works with Any Browser"></a>
Users with browsers which do not support frames, or those
who prefer a more linear presentation in a single
window, may access a no-frame edition of <cite>The Autodesk
File</cite> with identical content to the frame-based book.
The no-frame edition includes the pop-up footnotes present
in the frame edition, but since few browsers which lack
frames are likely to support JavaScript, you can simply click
on the footnote icon to display it, then use the
“Back” button or keystroke to return to the text containing
the footnote.
<br>
</p>

<h2><a href="https://www.fourmilab.ch/autofile/afpdf.zip" name="offline">Acrobat PDF Edition</a></h2>

<p>
<a href="https://www.fourmilab.ch/autofile/afpdf.zip"><img src="https://www.fourmilab.ch/autofile/www/figures/afpdf.gif" width="226" height="234" alt="Acrobat PDF Screen"></a>
If you prefer to read the book off-line, you can
<a href="https://www.fourmilab.ch/autofile/afpdf.zip">download a PDF edition</a> (5.8 Mb, ZIP compressed)
which you
can read with the Adobe Acrobat Reader utility,
available for most personal computers and Unix workstations,
which may be
<a href="http://www.adobe.com/acrobat/readstep.html" target="Autofile_Aux">downloaded
free of charge</a> directly from the
<a href="http://www.adobe.com/" target="Autofile_Aux">Adobe Systems</a> Web site.
The Acrobat PDF edition preserves all the formatting of
the original book, some of which was lost in creating
the Web editions, and permits point-and-click navigation
among chapters and to follow cross-references in the text.
</p>

<p>
Adobe is one of the most consistently irritating companies
on Earth with which to do business.  I'd like to give you
a nice button for downloading your own copy of Acrobat Reader,
but they won't let me use the image without “registering”
and “licensing” it, which I'm certainly not going to do
in order to promote their product and its file format.
<br>
</p>

<h2><a href="https://www.fourmilab.ch/autofile/afps.zip">PostScript Edition</a></h2>

<p>
<cite>The Autodesk File</cite> was originally typeset
using <a href="http://www.tug.org/" target="Autofile_Aux">TeX</a> with the
LaTeX macro package.  Camera-ready copy was generated
from PostScript created by the <tt>dvips</tt>
utility.  The PostScript edition is a single monolithic
file, more than 16 megabytes, containing the
entire book as originally typeset.  You can read it on-line
with a PostScript viewing program such as
<a href="https://www.ghostscript.com/" target="Autofile_Aux">GhostScript</a>
(which is free), or print it on any PostScript-compatible
printer.  <strong>Before sending this
file to a printer, consider that the book is almost
900 pages long!</strong>  This is a <em>big</em> print
job, which will consume lots of paper, toner, and,
potentially, good will of any colleagues with whom
you share the printer.  The PostScript edition may be
downloaded as either a <a href="https://www.fourmilab.ch/autofile/afps.zip">ZIPped archive</a>
or a <a href="https://www.fourmilab.ch/autofile/autofile.ps.gz"><b>gzip</b> compressed PostScript
file</a>; both are 4.9 Mb in length and uncompress to a 16
Mb PostScript file.
</p>

<hr>
<address>
by <a href="https://www.fourmilab.ch/" target="_top">John Walker</a>
</address>

<h3><a href="https://www.fourmilab.ch/autofile/images/">Autodesk Vintage Image Gallery</a></h3>
<h3><a href="https://www.fourmilab.ch/autofile/images/premises/">Autodesk Premises Over the Years</a></h3>
<h3><a href="https://www.fourmilab.ch/">Fourmilab Home Page</a></h3>



</div>]]>
            </description>
            <link>https://www.fourmilab.ch/autofile/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028729</guid>
            <pubDate>Sun, 08 Nov 2020 20:32:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Adding OpenStreetMaps to Matplotlib]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028727">thread link</a>) | @jhrabb
<br/>
November 8, 2020 | https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/ | <a href="https://web.archive.org/web/*/https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><header><p>Adding context to thousands of dots</p><p> published:&nbsp; <time datetime="2020-10-21T00:00:00+00:00"> 21 October 2020 </time></p></header><p>Adding a map to your visuals is a great way to quickly understand the geographic information you're trying to investigate. Thankfully there are quite a few packages and libraries (like <a href="https://geopandas.org/">geopandas</a>, <a href="https://scitools.org.uk/cartopy/docs/latest/">cartopy</a>, <a href="https://github.com/rossant/smopy">smopy</a>, <a href="https://github.com/python-visualization/folium">folium</a>, <a href="https://github.com/MatthewDaws/TileMapBase">tilemapbase</a>, or <a href="https://ipyleaflet.readthedocs.io/en/latest/">ipyleaflet</a>) that can make creating these visuals fairly straightforward and easy in your jupyter notebooks or whatever stack you're using.</p><p>For this essay though, I'll walk through the process of adding a base-map from OpenStreetMap to you're matplotlib visuals without using any of these libraries. In the end, we'll have a visual much like this (very messy) scatter-plot of buses as they service route 16 in New Orleans.</p><img alt="A plot of the ~466,000 position reports for buses servicing route 16 as they work their way up and down South Claiborne Avenue, between South Carrollton Avenue and Harrah's near the French Quarter." src="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/route-16-plot.png"><div id="how-it-works"><h2>How it Works</h2><p>The ability to add our base-map to our <a href="https://matplotlib.org/">matplotlib</a> visuals relies on matplotlib's <a href="https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.imshow.html">imshow() function</a>, which internally uses the <a href="https://python-pillow.org/">Pillow library</a> to display images, or any other two dimensional scalar data we want (like <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html">numpy arrays</a>).</p><p>For example, if we download <a href="https://bryanbrattlof.com/pages/hi/profile.png">my self-portrait</a>, we can add the image to a plot using code like this:</p><div><pre><span></span><span>from</span> <span>matplotlib</span> <span>import</span> <span>pyplot</span> <span>as</span> <span>plt</span>

<span>img</span> <span>=</span> <span>plt</span><span>.</span><span>imread</span><span>(</span><span>'path/to/my/self-portrait.png'</span><span>)</span>
<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>img</span><span>)</span>

<span>plt</span><span>.</span><span>show</span><span>()</span>
</pre></div><p>Resulting in a matplotlib visual that looks like this:</p><p><img alt="A matplotlib plot of a self portrait (stick figure drawing) of Bryan Brattlof" src="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/self-portrait-plot.png"></p></div><div id="creating-the-map"><h2>Creating the Map</h2><p>The easiest way to generate the base-map for <code>plt.imshow()</code> is to use a mapping service. These mapping services use enormous amounts of raw computing power to take the <a href="https://wiki.openstreetmap.org/wiki/Planet.osm">terabytes of map data</a> and render a map for us. Today there are quite a few services available online. The one I enjoy working with (and the one we will be using in this essay) is a free, community maintained service called <a href="https://www.openstreetmap.org/about">OpenStreetMap</a>.</p></div><div id="tile-servers"><h2>Tile Servers</h2><p>To make updating and sharing their work easier, OpenStreetMap (and virtually all other mapping services) have split their maps into billions of tiny (256 pixel) sections, called tiles, that we can download individually from their tile servers.</p><p>OpenStreetMap has <a href="https://wiki.openstreetmap.org/wiki/Tile_servers">quite a few tile servers</a> that style or prioritize different map features with some having a slightly different <a href="https://en.wikipedia.org/wiki/API">API</a> to request tiles. For example the <a href="http://maps.stamen.com/toner/#12/29.9722/-90.1167">Stamen Toner Map</a> that I used in the first visual and prefer for it's simple color pallet.</p><p>For this essay though, we'll use the default tile server's API to request tiles:</p><div><pre><span></span>URL = "https://tile.openstreetmap.org/{z}/{x}/{y}.png".format
</pre></div><p>This string formatting function will replace the <code>{z}</code>, <code>{x}</code>, and <code>{y}</code> with the tile coordinates and zoom level of the tile we want to download, where:</p><ul><li><code>{z}</code> is the "zoom" level ranging from 0 to 18. Zoom 0 being the most "zoomed out" and needs only one tile to depict <a href="https://tile.openstreetmap.org/0/0/0.png">the entire world</a> at that level.</li><li><code>{x}</code> is the number of tiles from the left most tile of the map.</li><li>and <code>{y}</code> is the number of tiles from the top most tile of the map.</li></ul><p>Both <code>{x}</code> and <code>{y}</code> depend on the zoom level <code>{z}</code> we've chosen, with larger zoom levels requiring more tiles to render the map. To understand how to calculate which tiles we need for our data-set, we'll need to understand how mapping projections work.</p></div><div id="map-projections"><h2>Map Projections</h2><p>Without going too deep into mapping projections, OpenStreetMap (along with many other mapping services) needed a way to convert <span>(<i>lat</i>, <i>lon</i>)</span> coordinates into planer <span>(<i>x</i>, <i>y</i>)</span> coordinates which work with their maps. Sadly there is no perfect way to do this.</p><p>Google (and everyone else eventually) settled on a variant of the <a href="https://en.wikipedia.org/wiki/Mercator_projection">Mercator Projection</a> called the <a href="https://en.wikipedia.org/wiki/Web_Mercator_projection">Web Mercator Projection</a> which simplifies the conversion by assuming the earth is a perfect sphere (it's not). This can (and does) lead to confusion in the final visuals and why many official bodies refuse to accept this standard.</p><p>The advantage of assuming the earth is a perfect sphere is that the equation to convert our GPS coordinates into Web Mercator coordinates is fairly straightforward. The <a href="https://wiki.openstreetmap.org/wiki/Main_Page">OpenStreetMap Wiki</a> has the algorithm available in <a href="https://wiki.openstreetmap.org/wiki/Mercator">multiple programming languages</a>. Here is the one for Python:</p><div><pre><span></span><span>import</span> <span>math</span>
<span>TILE_SIZE</span> <span>=</span> <span>256</span>

<span>def</span> <span>point_to_pixels</span><span>(</span><span>lon</span><span>,</span> <span>lat</span><span>,</span> <span>zoom</span><span>):</span>
    <span>"""convert gps coordinates to web mercator"""</span>
    <span>r</span> <span>=</span> <span>math</span><span>.</span><span>pow</span><span>(</span><span>2</span><span>,</span> <span>zoom</span><span>)</span> <span>*</span> <span>TILE_SIZE</span>
    <span>lat</span> <span>=</span> <span>math</span><span>.</span><span>radians</span><span>(</span><span>lat</span><span>)</span>

    <span>x</span> <span>=</span> <span>int</span><span>((</span><span>lon</span> <span>+</span> <span>180.0</span><span>)</span> <span>/</span> <span>360.0</span> <span>*</span> <span>r</span><span>)</span>
    <span>y</span> <span>=</span> <span>int</span><span>((</span><span>1.0</span> <span>-</span> <span>math</span><span>.</span><span>log</span><span>(</span><span>math</span><span>.</span><span>tan</span><span>(</span><span>lat</span><span>)</span> <span>+</span> <span>(</span><span>1.0</span> <span>/</span> <span>math</span><span>.</span><span>cos</span><span>(</span><span>lat</span><span>)))</span> <span>/</span> <span>math</span><span>.</span><span>pi</span><span>)</span> <span>/</span> <span>2.0</span> <span>*</span> <span>r</span><span>)</span>

    <span>return</span> <span>x</span><span>,</span> <span>y</span>
</pre></div></div><div id="downloading-a-tile"><h2>Downloading A Tile</h2><p>Now we can use the <code>point_to_pixels()</code> function to calculate the number of pixels from the top-left corner of the OSM map from the GPS coordinates in our data-set at any <code>zoom</code> level, for example the French Quarter of New Orleans:</p><div><pre><span></span><span>zoom</span> <span>=</span> <span>16</span>
<span>x</span><span>,</span> <span>y</span> <span>=</span> <span>point_to_pixels</span><span>(</span><span>-</span><span>90.064279</span><span>,</span> <span>29.95863</span><span>,</span> <span>zoom</span><span>)</span>
</pre></div><p>Dividing the number of pixels by <code>TILE_SIZE</code> will then give us the <code>{x}</code> and <code>{y}</code> that we need for the <code>URL()</code> function we created <a href="#tile-servers">a few sections ago</a> for the OpenStreetMap API.</p><div><pre><span></span><span>x_tiles</span><span>,</span> <span>y_tiles</span> <span>=</span> <span>int</span><span>(</span><span>x</span> <span>/</span> <span>TILE_SIZE</span><span>),</span> <span>int</span><span>(</span><span>y</span> <span>/</span> <span>TILE_SIZE</span><span>)</span>
</pre></div><p>That we can then use, along with the <a href="https://requests.readthedocs.io/en/master/">requests</a> and <a href="https://python-pillow.org/">Pillow</a> libraries, to download a tile from the OpenStreetMap tile servers:</p><div><pre><span></span><span>from</span> <span>io</span> <span>import</span> <span>BytesIO</span>
<span>from</span> <span>PIL</span> <span>import</span> <span>Image</span>
<span>import</span> <span>requests</span>

<span># format the url</span>
<span>url</span> <span>=</span> <span>URL</span><span>(</span><span>x</span><span>=</span><span>x_tiles</span><span>,</span> <span>y</span><span>=</span><span>y_tiles</span><span>,</span> <span>z</span><span>=</span><span>zoom</span><span>)</span>

<span># make the request</span>
<span>with</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>url</span><span>)</span> <span>as</span> <span>resp</span><span>:</span>
    <span>img</span> <span>=</span> <span>Image</span><span>.</span><span>open</span><span>(</span><span>BytesIO</span><span>(</span><span>resp</span><span>.</span><span>content</span><span>))</span>

<span># plot the tile</span>
<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>img</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</pre></div><p>Producing a tile of Jackson Square in the French Quarter of New Orleans:</p><p><img alt="the tile" src="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/french-quarter-plot.png"></p></div><div id="stitching-tiles-together"><h2>Stitching Tiles Together</h2><p>To download all the tiles needed for our visual, we'll need to calculate the limits of the data we'll be using in our visual. There are many ways we can do this, all of them are valid. For simplicity though, I'll calculate the <span><i>min</i></span> and <span><i>max</i></span> of both the <code>lat</code> and <code>lon</code> columns in my <a href="https://pandas.pydata.org/">pandas</a> DataFrame:</p><div><pre><span></span><span>top</span><span>,</span> <span>bot</span> <span>=</span> <span>df</span><span>.</span><span>lat</span><span>.</span><span>max</span><span>(),</span> <span>df</span><span>.</span><span>lat</span><span>.</span><span>min</span><span>()</span>
<span>lef</span><span>,</span> <span>rgt</span> <span>=</span> <span>df</span><span>.</span><span>lon</span><span>.</span><span>min</span><span>(),</span> <span>df</span><span>.</span><span>lon</span><span>.</span><span>max</span><span>()</span>
</pre></div><p>This gives us a bounding box (in GPS coordinates) that encompasses our entire data-set.</p><p>Next, just like we did in <a href="#downloading-a-tile">the last section</a>, we'll use the <code>point_to_pixels()</code> function to convert our GPS coordinates into Web Mercator coordinates.</p><div><pre><span></span><span>zoom</span> <span>=</span> <span>13</span>
<span>x0</span><span>,</span> <span>y0</span> <span>=</span> <span>point_to_pixels</span><span>(</span><span>lef</span><span>,</span> <span>top</span><span>,</span> <span>zoom</span><span>)</span>
<span>x1</span><span>,</span> <span>y1</span> <span>=</span> <span>point_to_pixels</span><span>(</span><span>rgt</span><span>,</span> <span>bot</span><span>,</span> <span>zoom</span><span>)</span>
</pre></div><p>That we can then divide by <code>TILE_SIZE</code> to calculate the minimum and maximum number of tiles we'll need to download for both the <code>{x}</code> and <code>{y}</code> arguments for the API:</p><div><pre><span></span><span>x0_tile</span><span>,</span> <span>y0_tile</span> <span>=</span> <span>int</span><span>(</span><span>x0</span> <span>/</span> <span>TILE_SIZE</span><span>),</span> <span>int</span><span>(</span><span>y0</span> <span>/</span> <span>TILE_SIZE</span><span>)</span>
<span>x1_tile</span><span>,</span> <span>y1_tile</span> <span>=</span> <span>math</span><span>.</span><span>ceil</span><span>(</span><span>x1</span> <span>/</span> <span>TILE_SIZE</span><span>),</span> <span>math</span><span>.</span><span>ceil</span><span>(</span><span>y1</span> <span>/</span> <span>TILE_SIZE</span><span>)</span>
</pre></div><p>As a precaution, we'll add an <code>assert</code> statement to limit the number of tiles we can download and save us from the embarrassment of accidentally burdening OpenStreetMap tile servers.</p><div><pre><span></span><span>assert</span> <span>(</span><span>x1_tile</span> <span>-</span> <span>x0_tile</span><span>)</span> <span>*</span> <span>(</span><span>y1_tile</span> <span>-</span> <span>y0_tile</span><span>)</span> <span>&lt;</span> <span>50</span><span>,</span> <span>"That's too many tiles!"</span>
</pre></div><p>Now that we've calculated which tiles we need to download from OpenStreetMap, we can use the built-in <a href="https://docs.python.org/3/library/itertools.html">itertools</a><code>product()</code> function to loop through every tile, downloading and saving the tiles to a single large pillow image using <a href="https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.paste">Pillow's paste() function</a>:</p><div><pre><span></span><span>from</span> <span>itertools</span> <span>import</span> <span>product</span>

<span># full size image we'll add tiles to</span>
<span>img</span> <span>=</span> <span>Image</span><span>.</span><span>new</span><span>(</span><span>'RGB'</span><span>,</span> <span>(</span>
    <span>(</span><span>x1_tile</span> <span>-</span> <span>x0_tile</span><span>)</span> <span>*</span> <span>TILE_SIZE</span><span>,</span>
    <span>(</span><span>y1_tile</span> <span>-</span> <span>y0_tile</span><span>)</span> <span>*</span> <span>TILE_SIZE</span><span>))</span>

<span># loop through every tile inside our bounded box</span>
<span>for</span> <span>x_tile</span><span>,</span> <span>y_tile</span> <span>in</span> <span>product</span><span>(</span><span>range</span><span>(</span><span>x0_tile</span><span>,</span> <span>x1_tile</span><span>),</span> <span>range</span><span>(</span><span>y0_tile</span><span>,</span> <span>y1_tile</span><span>)):</span>
    <span>with</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>URL</span><span>(</span><span>x</span><span>=</span><span>x_tile</span><span>,</span> <span>y</span><span>=</span><span>y_tile</span><span>,</span> <span>z</span><span>=</span><span>zoom</span><span>))</span> <span>as</span> <span>resp</span><span>:</span>
        <span>tile_img</span> <span>=</span> <span>Image</span><span>.</span><span>open</span><span>(</span><span>BytesIO</span><span>(</span><span>resp</span><span>.</span><span>content</span><span>))</span>

    <span># add each tile to the full size image</span>
    <span>img</span><span>.</span><span>paste</span><span>(</span>
        <span>im</span><span>=</span><span>tile_img</span><span>,</span>
        <span>box</span><span>=</span><span>((</span><span>x_tile</span> <span>-</span> <span>x0_tile</span><span>)</span> <span>*</span> <span>TILE_SIZE</span><span>,</span> <span>(</span><span>y_tile</span> <span>-</span> <span>y0_tile</span><span>)</span> <span>*</span> <span>TILE_SIZE</span><span>))</span>

<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>img</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</pre></div><p>Resulting in a plot like this:</p><p><img alt="A plot of New Orleans using the script we just developed to stitch multiple tiles together into one continuous map that we can place under our scatter plot in the next section." src="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/the-basemap.png"></p><p>The eagle-eyed among us will notice the image is too large for the visual we want to create. This is because of the <code>math.ceil()</code> and <code>int()</code> functions we used to round the pixel coordinates into <code>{x}</code> and <code>{y}</code> tiles we used above. To get our image back to size we'll need to crop out the fractions of tiles not inside our bounding box.</p></div><div id="cropping-the-basemap"><h2>Cropping the Basemap</h2><p>To help my human-eyed brethren, I added some lines to our previous graphic to help understand what's going on. Essentially some fraction of each tile we've downloaded (outlined in black) will be used in our final visual (outlined in red) that we calculated in <a href="#stitching-tiles-together">the last section</a>. Our goal for this section is to trim the fraction of tiles outside of our red square.</p><p><img alt="A plot of New Orleans with black lines outlining each tile we downloaded from the tile servers overlaid with a red line representing the section of the map we wish to keep after we crop the image." src="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/basemap-cropping-lines.png"></p><p>To curtail our oversize image, we'll use pillow's <a href="https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.crop">Image.crop()</a> function, which takes a tuple <code>(left, top, right, bottom)</code> measured in pixels from the top left corner to crop our image.</p><p><a href="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/StitchingTilesTogether">From our work above</a>, we know the pixel coordinates of the red square is defined as <code>x0, y0</code> and <code>x1, y1</code>. We can then multiply the tile coordinates <code>x0_tile, y0_tile</code> by <code>TILE_SIZE</code> to find the pixel coordinates for the top-left corner of the current (oversize) basemap:</p><div><pre><span></span><span>x</span><span>,</span> <span>y</span> <span>=</span> <span>x0_tile</span> <span>*</span> <span>TILE_SIZE</span><span>,</span> <span>y0_tile</span> <span>*</span> <span>TILE_SIZE</span>
</pre></div><p>It is a simple process of subtracting the edges of our red square from the pixel coordinates we just calculated for our oversize image to crop it to our desired size:</p><div><pre><span></span><span>img</span> <span>=</span> <span>img</span><span>.</span><span>crop</span><span>((</span>
    <span>int</span><span>(</span><span>x</span> <span>-</span> <span>x0</span><span>),</span>  <span># left</span>
    <span>int</span><span>(</span><span>y</span> <span>-</span> <span>y0</span><span>),</span>  <span># top</span>
    <span>int</span><span>(</span><span>x</span> <span>-</span> <span>x1</span><span>),</span>  <span># right</span>
    <span>int</span><span>(</span><span>y</span> <span>-</span> <span>y1</span><span>)))</span> <span># bottom</span>

<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>img</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</pre></div><p>Resulting in our final (properly sized) basemap for our visual:</p><p><img alt="A, now properly sized, plot of New Orleans using the cropping script we just developed to resize our basemap to the proper size for our visual." src="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/basemap-cropped.png"></p></div><div id="plotting-the-data"><h2>Plotting The Data</h2><p>Finally, with our basemap created, we can plot our data just like any other visual with some key exceptions. We can start by setting a <a href="https://matplotlib.org/3.3.1/api/_as_gen/matplotlib.pyplot.subplot.html">matplotlib subplots()</a> and a <a href="https://matplotlib.org/3.3.1/api/_as_gen/matplotlib.axes.Axes.scatter.html">scatter()</a> plot for the <code>lat</code> and <code>lon</code> columns in our pandas DataFrames:</p><div><pre><span></span><span>fig</span><span>,</span> <span>ax</span> <span>=</span> <span>plt</span><span>.</span><span>subplots</span><span>()</span>
<span>ax</span><span>.</span><span>scatter</span><span>(</span><span>df</span><span>.</span><span>lon</span><span>,</span> <span>df</span><span>.</span><span>lat</span><span>,</span> <span>alpha</span><span>=</span><span>0.1</span><span>,</span> <span>c</span><span>=</span><span>'red'</span><span>,</span> <span>s</span><span>=</span><span>1</span><span>)</span>
</pre></div><p>Then we'll add an extra argument to the <code>imshow()</code> function to properly locate our image in the final visual. The <code>extent</code> argument is used to move a image to a <a href="https://matplotlib.org/3.3.1/tutorials/intermediate/imshow_extent.html">particular region in dataspace</a>.</p><div><pre><span></span><span>ax</span><span>.</span><span>imshow</span><span>(</span><span>img</span><span>,</span> <span>extent</span><span>=</span><span>(</span><span>lef</span><span>,</span> <span>rgt</span><span>,</span> <span>bot</span><span>,</span> <span>top</span><span>))</span>
</pre></div><p>Next, we'll lock down the <span><i>x</i></span> and <span><i>y</i></span> axes to the limits we defined <a href="#stitching-tiles-together">a few sections ago</a> by using the <code>set_ylim()</code> and <code>set_xlim()</code> functions.</p><div><pre><span></span><span>ax</span><span>.</span><span>set_ylim</span><span>(</span><span>bot</span><span>,</span> <span>top</span><span>)</span>
<span>ax</span><span>.</span><span>set_xlim</span><span>(</span><span>lef</span><span>,</span> <span>rgt</span><span>)</span>
</pre></div><p>All of this work will produce a simple graphic with a (gorgeous) basemap of buses servicing New Orleans' Route 16.</p><p><img alt="The final visual we've been working to depicting the roughly 400,000 position reports of buses as they service route 16 of New Orleans." src="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/final-visual.png"></p></div></div></div>]]>
            </description>
            <link>https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028727</guid>
            <pubDate>Sun, 08 Nov 2020 20:32:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[TDD kata with serverless services in AWS]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028650">thread link</a>) | @gchinis
<br/>
November 8, 2020 | https://blog.gchinis.com/posts/tdd-aws-serverless-services/ | <a href="https://web.archive.org/web/*/https://blog.gchinis.com/posts/tdd-aws-serverless-services/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Last week, I’ve re-read “Test Driven Development: By Example” by Kent Beck.
I was amazed by the simplicity of his process, consisting of small pragmatic steps.
So, I decided to put the process to the test in an unfamiliar domain.</p>
<h3 id="the-kata">The kata</h3>
<p>In this kata, I am going to develop a serverless service in AWS using a Lambda and the API Gateway.
I chose this task because, on the one hand, it will contain a fair amount of infrastructure code, which is considered hard to test.
On the other hand, because I wanted a task that was more abstract and closer to a business requirement in contrast to a technical requirement like ‘<em>deploy an AWS HTTP API Gateway</em>’.</p>
<p>My goal is to understand whether TDD for infrastructure is possible and what are the trade-offs.</p>
<h3 id="the-tdd-process">The TDD process</h3>
<p>I am going to be following the TDD process, as described in the book, as close as possible.
I will attempt to follow the Red-Green-Refactor cycle.</p>
<ul>
<li>Red: Start by writing a failing test. It may not even compile at that time.</li>
<li>Green: Make the test pass, committing any sin necessary in the process.</li>
<li>Refactor: Eliminate any duplication introduced to make the tests green.</li>
</ul>
<p>One important piece of the process is having a <strong>To-Do</strong> list.
It is going to help me keep track of what is left to do and help me discover what to work on next.</p>
<div><pre><code data-lang="text">To-Do List
==========
* Publish sales API</code></pre></div>
<p>What is new for me is an appreciation of TDD as a process to manage my uncertainty and fear, rather than a process to write tests.
You can read more about this in my previous blog post, <a href="https://blog.gchinis.com/posts/the-trade-offs-of-tdd/">here</a>.</p>
<h3 id="tech-stack">Tech stack</h3>
<p>Being cognizant of the uncertainty, I decided to use as much as possible familiar tools.</p>
<ul>
<li>Terraform for infrastructure provisioning.</li>
<li>Javascript for the Lambda.</li>
<li>Jest as a test runner.</li>
</ul>
<p>There are probably tools better than terraform when it comes to testability.
What I want to demonstrate is that the TDD process is independent of the tools you have to work with.</p>
<p>There are also aspects of this task that I am not familiar with:</p>
<ul>
<li>Using the API Gateway</li>
<li>Using TDD in an infrastructure heavy task</li>
</ul>
<p>I will have to be careful to tackle them in small increments, so that I don’t get overwhelmed.</p>
<h2 id="putting-terraform-under-test-harness">Putting Terraform under test harness</h2>
<p>‘<em>Publish sales API</em>’ is a very big task to do in one step.
So, let’s look for a more achievable intermediate step to start with.
If I can have a test which applies a minimal terraform configuration, I will at least know that I can put terraform under test harness, which is a prerequisite to the TDD cycle.</p>
<div><pre><code data-lang="text">To-Do List
==========
* Publish sales API
<span>* Run terraform in test</span></code></pre></div>
<p><strong>Test snippet</strong></p>
<p>The test uses the simplest approach I can think of to drive terraform.
It does what I would do in the command line.</p>
<div><pre><code data-lang="javascript"><span>const</span> {<span>execSync</span>} <span>=</span> <span>require</span>(<span>'child_process'</span>);
<span>const</span> <span>https</span> <span>=</span> <span>require</span>(<span>'https'</span>);
<span>const</span> <span>AWS</span> <span>=</span> <span>require</span>(<span>'aws-sdk'</span>)

<span>describe</span>(<span>'serverless'</span>, () =&gt; {
  <span>test</span>(<span>'run terraform'</span>, () =&gt; {
    <span>const</span> <span>modulePath</span> <span>=</span> <span>'./src'</span>;
    <span>execSync</span>(<span>'terraform init'</span>, {<span>cwd</span><span>:</span> <span>modulePath</span>});
    <span>const</span> <span>applyResp</span> <span>=</span> <span>execSync</span>(<span>'terraform apply -auto-approve'</span>, {<span>cwd</span><span>:</span> <span>modulePath</span>});
    <span>expect</span>(<span>applyResp</span>.<span>toString</span>()).<span>toContain</span>(<span>'Apply complete!'</span>)
  })
})
</code></pre></div><p><strong>Code snippet</strong></p>
<div><pre><code data-lang="terraform"><span>provider</span> <span>"aws"</span> {
  <span>region</span> = <span>"eu-central-1"</span>
}
</code></pre></div><p>Writing this test was hard work but making it green required only 3 lines of boilerplate code!
Still, it is an important milestone.
It demonstrates that it is possible to use jest to drive terraform to <em>act</em> and then also <em>assert</em> on the outcome of the operation.</p>
<div><pre><code data-lang="text">To-Do List
==========
* Publish sales API
<span>☑️ Run terraform in test</span></code></pre></div>
<h2 id="deploying-the-lambda">Deploying the Lambda</h2>
<p>Even with terraform under test harness, deploying the sales API in one step, is still too big a task.
What I find especially challenging about the task ahead, is doing all the infrastructure automation in one go, especially since I am unfamiliar with the API Gateway.</p>
<p>I think a smaller task, that I feel comfortable to undertake, is to deploy the AWS Lambda with my application code and make sure I can invoke it using the aws-sdk.
Let’s update the To-Do List with our next steps.</p>
<div><pre><code data-lang="text">To-Do List
==========
* Publish sales API
☑️ Run terraform in test
<span>* Deploy the sales Lambda</span></code></pre></div>
<h4 id="test-snippet">Test snippet</h4>
<p>I’ve refactor the test code from before, and I’ve extracted a <code>beforeAll</code> block where the
terraform related code now lives.</p>
<p>The test itself is using the aws-sdk to invoke a Lambda function and asserts that the function returned the expected values.
The name of the function is coming from terraform.</p>
<div><pre><code data-lang="javascript"><span>const</span> {<span>execSync</span>} <span>=</span> <span>require</span>(<span>'child_process'</span>);
<span>const</span> <span>https</span> <span>=</span> <span>require</span>(<span>'https'</span>);
<span>const</span> <span>AWS</span> <span>=</span> <span>require</span>(<span>'aws-sdk'</span>)

<span>describe</span>(<span>'lambda'</span>, () =&gt; {
    <span>jest</span>.<span>setTimeout</span>(<span>20000</span>)
    <span>let</span> <span>lambda_name</span>;

    <span>beforeAll</span>(() =&gt; {
        <span>execSync</span>(<span>'terraform init'</span>, {<span>cwd</span><span>:</span> <span>'./src'</span>});
        <span>const</span> <span>modulePath</span> <span>=</span> <span>'./src'</span>;
        <span>const</span> <span>applyResp</span> <span>=</span> <span>execSync</span>(<span>'terraform apply -auto-approve'</span>, {<span>cwd</span><span>:</span> <span>modulePath</span>});
        <span>expect</span>(<span>applyResp</span>.<span>toString</span>()).<span>toContain</span>(<span>'Apply complete!'</span>)

        <span>const</span> <span>resp</span> <span>=</span> <span>JSON</span>.<span>parse</span>(<span>execSync</span>(<span>'terraform output -json'</span>, {<span>cwd</span><span>:</span> <span>modulePath</span>}));
        <span>lambda_name</span> <span>=</span> <span>resp</span>.<span>lambda_name</span>.<span>value</span>
    })

    <span>test</span>(<span>'have a lambda'</span>, <span>async</span> () =&gt; {
        <span>const</span> <span>lambda</span> <span>=</span> <span>new</span> <span>AWS</span>.<span>Lambda</span>({<span>apiVersion</span><span>:</span> <span>'2015-03-31'</span>, <span>region</span><span>:</span> <span>'eu-central-1'</span>});
        <span>const</span> <span>resp</span> <span>=</span> <span>await</span> <span>lambda</span>.<span>invoke</span>({
            <span>FunctionName</span><span>:</span> <span>lambda_name</span>,
        }).<span>promise</span>()

        <span>expect</span>(<span>resp</span>.<span>StatusCode</span>).<span>toBe</span>(<span>200</span>)
        <span>expect</span>(<span>resp</span>.<span>Payload</span>).<span>toContain</span>(<span>'{ sales: [] }'</span>)
    })
})
</code></pre></div><h4 id="code-snippet">Code snippet</h4>
<div><pre><code data-lang="terraform"><span>data</span> <span>"archive_file"</span> <span>"example"</span> {
  <span>type</span> = <span>"zip"</span>
  <span>source_file</span> = <span>"</span><span>${</span><span>path</span>.module<span>}</span><span>/example/index.js"</span>
  <span>output_path</span> = <span>"</span><span>${</span><span>path</span>.module<span>}</span><span>/files/example.zip"</span>
}
<span>
</span><span>resource</span> <span>"aws_lambda_function"</span> <span>"example"</span> {
  <span>function_name</span> = <span>"serverless_example"</span>
  <span>handler</span> = index.<span>handler</span>
  <span>role</span> = <span>aws_iam_role</span>.<span>lambda_exec</span>.<span>arn</span>
  <span>runtime</span> = <span>"nodejs12.x"</span>

  <span>filename</span> = data.<span>archive_file</span>.<span>example</span>.<span>output_path</span>
  <span>source_code_hash</span> = filebase64sha256(data.<span>archive_file</span>.<span>example</span>.<span>output_path</span>)
  <span>reserved_concurrent_executions</span> = <span>1</span>
  <span>timeout</span> = <span>10</span>
  <span>publish</span> = <span>true</span>
}
<span>
</span><span>data</span> <span>aws_iam_policy_document</span> <span>"lambda_exec"</span> {
  <span>statement</span> {
    <span>actions</span> = [<span>"sts:AssumeRole"</span>]
    <span>principals</span> {
      <span>identifiers</span> = [<span>"lambda.amazonaws.com"</span>]
      <span>type</span> = <span>"Service"</span>
    }
    <span>effect</span> = <span>"Allow"</span>
  }
}
<span>
</span><span>resource</span> <span>"aws_iam_role"</span> <span>"lambda_exec"</span> {
  <span>name</span> = <span>"serverless_example_lambda"</span>
  <span>assume_role_policy</span> = data.<span>aws_iam_policy_document</span>.<span>lambda_exec</span>.<span>json</span>
}
<span>
</span><span>output</span> <span>"lambda_name"</span> {
  <span>value</span> = <span>aws_lambda_function</span>.<span>example</span>.<span>function_name</span>
}
</code></pre></div><p>There was a bit more code, I had to write to make this test green.
Fortunately, I was able to use jest, and work through the failures one by one until my Lambda was properly deployed.</p>
<p>I had to make the <code>lambda_name</code> an output of terraform to have it available in the test.</p>
<p>I don’t provide the JS code of the Lambda. I don’t think there is any educational value in it.</p>
<div><pre><code data-lang="text">To-Do List
==========
* Publish sales API
☑️ Run terraform in test
<span>☑️ Deploy the sales Lambda</span></code></pre></div>
<h2 id="publishing-the-sales-api">Publishing the sales API</h2>
<p>Now, I think I can go back and tackle the original task.</p>
<div><pre><code data-lang="text">To-Do List
==========
<span>* Publish sales API
</span>☑️ Run terraform in test
☑️ Deploy the sales Lambda</code></pre></div>
<h4 id="test-snippet-1">Test snippet</h4>
<div><pre><code data-lang="Javascript"><span>const</span> {<span>execSync</span>} <span>=</span> <span>require</span>(<span>'child_process'</span>);
<span>const</span> <span>https</span> <span>=</span> <span>require</span>(<span>'https'</span>);
<span>const</span> <span>AWS</span> <span>=</span> <span>require</span>(<span>'aws-sdk'</span>)

<span>describe</span>(<span>'simple http api'</span>, () =&gt; {
    <span>jest</span>.<span>setTimeout</span>(<span>20000</span>)
    <span>let</span> <span>tf_output</span> <span>=</span> {};

    <span>beforeAll</span>(() =&gt; {
        <span>execSync</span>(<span>'terraform init'</span>, {<span>cwd</span><span>:</span> <span>'./src'</span>});
        <span>const</span> <span>applyResp</span> <span>=</span> <span>execSync</span>(<span>'terraform apply -auto-approve'</span>, {<span>cwd</span><span>:</span> <span>'./src'</span>});
        <span>expect</span>(<span>applyResp</span>.<span>toString</span>()).<span>toContain</span>(<span>'Apply complete!'</span>)
        <span>tf_output</span> <span>=</span> <span>JSON</span>.<span>parse</span>(<span>execSync</span>(<span>'terraform output -json'</span>, {<span>cwd</span><span>:</span> <span>'./src'</span>}));
    })

    <span>test</span>(<span>'API'</span>, <span>async</span> () =&gt; {
        <span>const</span> <span>apigatewayv2</span> <span>=</span> <span>new</span> <span>AWS</span>.<span>ApiGatewayV2</span>({<span>apiVersion</span><span>:</span> <span>'2018-11-29'</span>, <span>region</span><span>:</span> <span>'eu-central-1'</span>});
        <span>const</span> <span>resp</span> <span>=</span> <span>await</span> <span>apigatewayv2</span>.<span>getApi</span>({
            <span>ApiId</span><span>:</span> <span>tf_output</span>.<span>simple_http_api</span>.<span>value</span>.<span>id</span>
        }).<span>promise</span>();

        <span>expect</span>(<span>resp</span>.<span>ApiEndpoint</span>).<span>toBeTruthy</span>()
        <span>expect</span>(<span>resp</span>.<span>ApiId</span>).<span>toEqual</span>(<span>tf_output</span>.<span>simple_http_api</span>.<span>value</span>.<span>id</span>)
    })

    <span>test</span>(<span>'Get response from API'</span>, (<span>done</span>) =&gt; {
        <span>const</span> <span>req</span> <span>=</span> <span>https</span>.<span>request</span>(
            <span>tf_output</span>.<span>simple_http_api</span>.<span>value</span>.<span>api_endpoint</span>,
            (<span>res</span>) =&gt; {
                <span>let</span> <span>data</span> <span>=</span> <span>''</span>
                <span>res</span>.<span>setEncoding</span>(<span>'utf8'</span>);
                <span>res</span>.<span>on</span>(<span>'data'</span>, (<span>chunk</span>) =&gt; {
                    <span>data</span> <span>+=</span> <span>chunk</span>
                });
                <span>res</span>.<span>on</span>(<span>'end'</span>, () =&gt; {
                    <span>expect</span>(<span>res</span>.<span>statusCode</span>).<span>toBe</span>(<span>200</span>)
                    <span>expect</span>(<span>data</span>).<span>toContain</span>(<span>'{sales: []}'</span>)
                    <span>done</span>()
                });
            });
        <span>req</span>.<span>on</span>(<span>'error'</span>, (<span>e</span>) =&gt; {
            <span>console</span>.<span>error</span>(<span>e</span>);
            <span>done</span>(<span>e</span>)
        });
        <span>req</span>.<span>end</span>();
    })
})
</code></pre></div><p>Those two tests demonstrate two different approaches to write assertions.</p>
<ul>
<li>
<p>The first one uses the aws-sdk to inspect whether the necessary resource has been created.</p>
</li>
<li>
<p>The second one uses a completely outside-in approach without any knowledge of the infrastructure.
It makes an HTTP request to the endpoint, demonstrating that our API is published and working.</p>
</li>
</ul>
<p>Both tests depend on output from terraform.</p>
<h4 id="code-snippet-1">Code snippet</h4>
<div><pre><code data-lang="terraform"><span>resource</span> <span>"aws_apigatewayv2_api"</span> <span>"example"</span> {
  <span>name</span> = <span>"simple_http_example"</span>
  <span>protocol_type</span> = <span>"HTTP"</span>
  <span>target</span> = <span>aws_lambda_function</span>.<span>example</span>.<span>arn</span>
}
<span>
</span><span>resource</span> <span>"aws_lambda_permission"</span> <span>"apigw"</span> {
  <span>action</span> = <span>"lambda:InvokeFunction"</span>
  <span>function_name</span> = <span>aws_lambda_function</span>.<span>example</span>.<span>function_name</span>
  <span>principal</span> = <span>"apigateway.amazonaws.com"</span>
  <span>source_arn</span> = <span>"</span><span>${</span><span>aws_apigatewayv2_api</span>.<span>example</span>.<span>execution_arn</span><span>}</span><span>/*/*"</span>
}
<span>
</span><span>output</span> <span>"simple_http_api"</span> {
  <span>value</span> = <span>aws_apigatewayv2_api</span>.<span>example</span>
}
</code></pre></div><p>With that, our main task is done!</p>
<div><pre><code data-lang="text">To-Do List
==========
<span>☑️ Publish sales API
</span>☑️ Run terraform in test
☑️ Deploy the sales lambda</code></pre></div>
<h2 id="conclusion">Conclusion</h2>
<p>All in all, I wrote 3 tests which take around 15 seconds to run including the <code>terraform apply</code>.</p>
<p>This is about one order of magnitude slower than what I am used to, when I write tests for classic applications.
Still, it is one of the fastest feedback cycles I’ve experienced doing infrastructure.</p>
<p>I hope I demonstrated that a TDD approach is a viable approach for developing infrastructure code.
Of course, with dedicated tooling it gets easier to write tests.
However, you can use simple tools to start …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.gchinis.com/posts/tdd-aws-serverless-services/">https://blog.gchinis.com/posts/tdd-aws-serverless-services/</a></em></p>]]>
            </description>
            <link>https://blog.gchinis.com/posts/tdd-aws-serverless-services/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028650</guid>
            <pubDate>Sun, 08 Nov 2020 20:23:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Trustpilot and Difficult Incentive Problems]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028645">thread link</a>) | @lharries
<br/>
November 8, 2020 | https://harries.co/trustpilot/ | <a href="https://web.archive.org/web/*/https://harries.co/trustpilot/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><blockquote>
<p>💡 TL;DR Trustpilot and Glassdoor are PR agencies for companies. Not impartial review platforms for customers. This creates a challenging incentive problem.</p>
</blockquote>
<p>Trustpilot enables customers to review companies and Glassdoor enables employees to review companies. They have similar and fascinating business models. Let’s dive into Trustpilot.</p>
<p><strong>How does Trustpilot work?</strong></p>
<p>Let’s imagine you are setting up an online physiotherapy clinic called StiffNecks.com. To build your reputation as a clinic you create a Trustpilot page. The first client comes along, has a great experience, and you send them an email to the Trustpilot page asking them to leave a review. The client had a great experience and writes a lovely review of StiffNecks.com rating it 5/5 stars. BOOM your Trustpilot page now has a 3.5/5 “TrustScore”. Wait, what?!</p>
<p>With that 3.5/5 rating, TrustPilot now has you hostage. They use a <a href="https://support.trustpilot.com/hc/en-us/articles/201748946-TrustScore-explained-How-is-the-TrustScore-calculated-">Bayesian average</a> to calculate the TrustScore which takes into account how old the reviews are, the frequency you collect reviews, and they start you with a lovely 7 reviews of 3.5 for good measure. In other words, they control what your reputation is. And they charge businesses for this service with incentives misaligned from the business and definitely the customer.</p>
<p>Let’s jump back to StiffNecks.com. You now have anxiety from the yellow 3.5/5 score staring at you and so with each new customer, you do your best to push them towards Trustpilot. Eventually, (with about 10x5 star reviews) you get past the dreaded 4/5 threshold beyond which no customer dares to buy. But with each new review, it signals to customers that this is a valid reflection of your business’s reputation. And did I forget that you get penalised if the frequency of collecting reviews decreases. No stopping now. A few days later you get a call from Trustpilot agent offering the paid service.</p>
<p><strong>Where do the incentives go wrong?</strong></p>
<p>The immediate incentive is for Trustpilot to charge businesses more. Pretty easy to do when they directly control a business’s reputation. How can Trustpilot charge more? By either threatening to reduce their reputation or helping them to improve their score.</p>
<p>How do you help a company improve their score? At the moment Trustpilot uses widgets to show reviews and tools to collect reviews. But… there’s also strong incentives for Trustpilot to curate/remove negative reviews. Something that is already happening at Glassdoor (<a href="https://news.ycombinator.com/item?id=24789865">anecdotal evidence here</a>).</p>
<p>What does this mean for the customer? Believing in Trustpilot as a measure of reputation is a crucial part of their flywheel. But there’s much stronger incentives to be on the side of helping paying businesses have an artificially high score.</p>
<p><strong>How can Trustpilot remedy this?</strong></p>
<p>I think Trustpilot is an awesome service for consumers but the incentive problem is tough… For Trustpilot to truly fix the incentive problem they would need to be directly aligned with the reviewer. But the obvious business models such as a paywalling reviews would strongly interfere with their flywheel.</p>
<p>It’s possible to continue charging businesses. But Trustpilot would need to play the long game - not fudging the reviews at any cost. Instead, continuing to be a forcing function for businesses to treat their customers better, and thus improve their score. But, moderating reviews is hard because customer experience is subjective and Trustpilot does not have proper validation for who has and has not been a customer. Solutions to the verification problem seem feasible, such as the Facebook retargeting mechanism of sharing a hashed list of their customer emails.</p>
<p><strong>Appendix: The Trustpilot flywheel</strong></p>
<p>More companies email customers to leave reviews on Trustpilot → More customers leave reviews → More customers trust and use Trustpilot → More companies sign up to Trustpilot → More companies email customers to leave reviews on Trustpilot → …</p></section></div>]]>
            </description>
            <link>https://harries.co/trustpilot/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028645</guid>
            <pubDate>Sun, 08 Nov 2020 20:23:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Live stream – Zig 0.7.0 Release Party]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028620">thread link</a>) | @todsacerdoti
<br/>
November 8, 2020 | https://www.twitch.tv/kristoff_it | <a href="https://web.archive.org/web/*/https://www.twitch.tv/kristoff_it">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.twitch.tv/kristoff_it</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028620</guid>
            <pubDate>Sun, 08 Nov 2020 20:19:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Find the Overlap Area]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028526">thread link</a>) | @daybrush
<br/>
November 8, 2020 | https://daybrush.com/overlap-area/ | <a href="https://web.archive.org/web/*/https://daybrush.com/overlap-area/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div class="page">
        <div>
            <div>
                
                <div>
                    
                    <p>0</p>
                </div>
                <div>
                    
                    <p>0</p>
                </div>
                <div>
                    
                    <p>0</p>
                </div>
                <svg>
                    <path d=""></path>
                </svg>
            </div>
            
            <p>
                <a href="https://www.npmjs.com/package/overlap-area" target="_blank">
                    <img src="https://img.shields.io/npm/v/overlap-area.svg?style=flat-square&amp;color=007acc&amp;label=version" alt="npm version"></a>
                <a href="https://github.com/daybrush/overlap-area" target="_blank">
                    <img src="https://img.shields.io/github/stars/daybrush/overlap-area.svg?color=42b883&amp;style=flat-square"></a>
                <a href="https://github.com/daybrush/overlap-area" target="_blank">
                    <img src="https://img.shields.io/badge/language-typescript-blue.svg?style=flat-square">
                </a>
                <a href="https://github.com/daybrush/overlap-area/blob/master/LICENSE" target="_blank">
                    <img src="https://img.shields.io/github/license/daybrush/overlap-area.svg?style=flat-square&amp;label=license&amp;color=08CE5D">
                </a>
            </p>
            <p>Find the Overlap Area.</p>
            <div>
                <p><a href="https://github.com/daybrush/overlap-area" target="_blank">Download</a>
                <a href="https://daybrush.com/selecto/release/latest/doc" target="_blank">API</a>
                <a href="https://daybrush.com/selecto/storybook/?path=/story/selecto--select-accurately" target="_blank">Use with Selecto</a>
            </p></div>
        </div>

    </div></div>]]>
            </description>
            <link>https://daybrush.com/overlap-area/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028526</guid>
            <pubDate>Sun, 08 Nov 2020 20:09:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[From Spring Boot to Micronaut]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028483">thread link</a>) | @nfrankel
<br/>
November 8, 2020 | https://blog.frankel.ch/spring-to-micronaut/ | <a href="https://web.archive.org/web/*/https://blog.frankel.ch/spring-to-micronaut/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main"> <div> <article itemscope="" itemtype="http://schema.org/BlogPosting"> <meta itemprop="mainEntityOfPage" content="//spring-to-micronaut/"> <meta itemprop="description" content="">  <figure itemscope="" itemprop="image" itemtype="http://schema.org/ImageObject"> <meta itemprop="url" content="https://blog.frankel.ch/assets/resources/spring-to-micronaut/micronaut.svg"> </figure> <section> <div itemprop="articleBody"> <p>In the last couple of years, I’ve been playing a bit with a generation of tools in the Java world, namely <a href="https://micronaut.io/" target="_blank" rel="noopener">Micronaut</a>, <a href="https://quarkus.io/" target="_blank" rel="noopener">Quarkus</a> and <a href="https://www.graalvm.org/" target="_blank" rel="noopener">GraalVM</a>. While I’m a Spring Boot fan since its beginning, I believe this quite an eye-opening opportunity. In this post, I’d like to see how easy, or how hard, it is to port a simple Spring Boot application to Micronaut.</p> <div> <h2 id="setting-up-the-context">Setting up the context</h2> <div> <p>The <abbr title="Java Virtual Machine">JVM</abbr> is an great piece of technology. Modern versions compile the running <em>bytecode</em> to native code, depending on the existing workload. For this reason, JVM applications are on par with - or even winning over - native executables regarding to runtime performance.</p> <p>JVM applications have a warm-up time during which they don’t perform well. The loading of classes at runtime doesn’t help. Frameworks such as Spring and Jakarta EE have been making use of classpath scanning and reflection, which make startup time even longer. For long-running processes, such as traditional application servers, this is not an issue.</p> <p>In the context of containers, it is. Because one handles containers as cattle and not pets, the platform <em>e.g.</em> Kubernetes kills pods and schedules new ones at regular intervals. The longer the startup time, the less relevant the JVM becomes. It becomes even worse in Serverless environments that need to auto-scale the number of pods quickly.</p> <p>To hop on the bandwagon, Oracle offers <a href="https://github.com/oracle/graal/tree/master/substratevm" target="_blank" rel="noopener">SubstrateVM</a>. A subcomponent of GraalVM, SubstrateVM, allows transforming JVM bytecode into a native executable. To do that, SubstrateVM compiles the bytecode <abbr title="Ahead-Of-Time">AOT</abbr>. For that reason, you need to explicitly feed it information that is available on the JVM at runtime. It’s the case of reflection for example. Note that some JVM features are not ported to GraalVM. Moreover, the AOT compilation is a time-consuming process.</p> <p>The result is that on one hand, we have the JVM and all its features leveraged by frameworks; on the other hand, we have native executables that require fine-tuned manual configuration and a massive amount of build time.</p> <p>A new generation of frameworks has spawned that aims to find a middle ground <em>i.e.</em> Micronaut and Quarkus. They both aim to generate bytecode AOT. Note that this AOT is different from the one mentioned above. Instead of using reflection at runtime, which is expensive, both frameworks generate extra classes at build time. This also allows us to avoid classpath scanning at startup time. In short, the idea is about making as much code as possible available at build time.</p> </div> </div> <div> <h2 id="the-sample-application">The sample application</h2> <div> <p>I want the sample application to migrate to be simple enough so I can migrate it by myself but not to the point of being trivial. It consists of the following:</p> <ul><li><span>A controller layer implemented by Spring MVC</span></li><li><span>A repository layer implemented by Spring Data JPA</span></li><li><span>A JPA entity</span></li><li><span>Schema generation and data insertion at startup via Spring Boot</span></li><li><span>The Spring Boot actuator, with the <code>health</code> and <code>beans</code> endpoints enabled and accessible without authentication</span></li></ul> <p>The application is written in Kotlin. I’ll be using H2 as the database to make the whole setup less complex.</p> <p>In general, I try to approach migrations in a step-by-step way. To do that, Micronaut offers a dedicated Micronaut-Spring dependency. I must admit I didn’t manage to make it work the way I wanted. Thus, I did a big-bang migration. The rest of this post will focus on different places for the migration.</p> </div> </div> <div> <h2 id="common-changes">Common changes</h2> <div> <p>The first change is to replace the parent POM.</p> <div> <p>pom.xml</p> <div> <pre><code data-lang="xml"><span>&lt;parent&gt;</span>
  <span>&lt;groupId&gt;</span>org.springframework.boot<span>&lt;/groupId&gt;</span>
  <span>&lt;artifactId&gt;</span>spring-boot-starter-parent<span>&lt;/artifactId&gt;</span>
  <span>&lt;version&gt;</span>2.3.5.RELEASE<span>&lt;/version&gt;</span>
  <span>&lt;relativePath/&gt;</span> <span>&lt;!-- lookup parent from repository --&gt;</span>
<span>&lt;/parent&gt;</span>

<span>&lt;parent&gt;</span>
    <span>&lt;groupId&gt;</span>io.micronaut<span>&lt;/groupId&gt;</span>
    <span>&lt;artifactId&gt;</span>micronaut-parent<span>&lt;/artifactId&gt;</span>
    <span>&lt;version&gt;</span>2.1.3<span>&lt;/version&gt;</span>
<span>&lt;/parent&gt;</span></code></pre> </div> </div> <p>Because Micronaut generates bytecode at build-time, we need to add an annotation processor during the compilation. Thus, the close second step is to configure that in the POM.</p> <div> <p>pom.xml</p> <div> <pre><code data-lang="xml"><span>&lt;plugin&gt;</span>
  <span>&lt;groupId&gt;</span>org.jetbrains.kotlin<span>&lt;/groupId&gt;</span>
  <span>&lt;artifactId&gt;</span>kotlin-maven-plugin<span>&lt;/artifactId&gt;</span>
  <span>&lt;version&gt;</span>${kotlin.version}<span>&lt;/version&gt;</span>
  ...
  <span>&lt;executions&gt;</span>
    <span>&lt;execution&gt;</span>
      <span>&lt;id&gt;</span>kapt<span>&lt;/id&gt;</span>
      <span>&lt;goals&gt;</span>
        <span>&lt;goal&gt;</span>kapt<span>&lt;/goal&gt;</span>
      <span>&lt;/goals&gt;</span>
      <span>&lt;configuration&gt;</span>
        <span>&lt;annotationProcessorPaths&gt;</span>
          <span>&lt;annotationProcessorPath&gt;</span>
            <span>&lt;groupId&gt;</span>io.micronaut<span>&lt;/groupId&gt;</span>
            <span>&lt;artifactId&gt;</span>micronaut-inject-java<span>&lt;/artifactId&gt;</span>        <i data-value="1"></i><b>(1)</b>
            <span>&lt;version&gt;</span>${micronaut.version}<span>&lt;/version&gt;</span>
          <span>&lt;/annotationProcessorPath&gt;</span>
          <span>&lt;annotationProcessorPath&gt;</span>
            <span>&lt;groupId&gt;</span>io.micronaut.data<span>&lt;/groupId&gt;</span>
            <span>&lt;artifactId&gt;</span>micronaut-data-processor<span>&lt;/artifactId&gt;</span>     <i data-value="2"></i><b>(2)</b>
            <span>&lt;version&gt;</span>${micronaut.data.version}<span>&lt;/version&gt;</span>
          <span>&lt;/annotationProcessorPath&gt;</span>
        <span>&lt;/annotationProcessorPaths&gt;</span>
      <span>&lt;/configuration&gt;</span>
    <span>&lt;/execution&gt;</span>
    ...
  <span>&lt;/executions&gt;</span>
  ...
<span>&lt;/plugin&gt;</span></code></pre> </div> </div> <div> <table> <tbody><tr> <td><i data-value="1"></i><b>1</b></td> <td>Handle dependency injection</td> </tr> <tr> <td><i data-value="2"></i><b>2</b></td> <td>Handle persistence-related classes</td> </tr> </tbody></table> </div> <p>You can check those extra classes by looking at the <code>target/classes</code> folder. For example, the sample application displays the following:</p> <div> <div> <pre>$Person$Introspection$$0.class                     PersonRepository$Intercepted$$proxy0.class
$Person$Introspection$$1.class                     PersonRepository$Intercepted$$proxy1.class
$Person$Introspection$$2.class                     PersonRepository$Intercepted$$proxy10.clas
$Person$Introspection$$3.class                     PersonRepository$Intercepted$$proxy2.class
$Person$Introspection.class                        PersonRepository$Intercepted$$proxy3.class
$Person$IntrospectionRef.class                     PersonRepository$Intercepted$$proxy4.class
$PersonControllerDefinition$$exec1.class            PersonRepository$Intercepted$$proxy5.class
$PersonControllerDefinition$$exec2.class            PersonRepository$Intercepted$$proxy6.class
$PersonControllerDefinition.class                   PersonRepository$Intercepted$$proxy7.class
$PersonControllerDefinitionClass.class              PersonRepository$Intercepted$$proxy8.class
$PersonRepository$InterceptedDefinition.class       PersonRepository$Intercepted$$proxy9.class
$PersonRepository$InterceptedDefinitionClass.class  PersonRepository$Intercepted.class
Person.class                                       PersonRepository.class
PersonController.class                             SpringToMicronautApplicationKt.class</pre> </div> </div> <p>Micronaut creates classes that contain <code>Introspection</code> and <code>Intercepted</code> via <code>kapt</code>.</p> <p>To start the application, Spring Boot refers to a class.</p> <div> <div> <pre><code data-lang="kotlin"><span>@SpringBootApplication</span>
<span>class</span> <span>SpringToMicronautApplication</span>

<span>fun</span> <span>main</span><span>(</span><span>args</span><span>:</span> <span>Array</span><span>&lt;</span><span>String</span><span>&gt;)</span> <span>{</span>
  <span>runApplication</span><span>&lt;</span><span>SpringToMicronautApplication</span><span>&gt;(*</span><span>args</span><span>)</span>
<span>}</span></code></pre> </div> </div> <p>Micronaut allows us to just use the standard <code>main</code> function.</p> <div> <div> <pre><code data-lang="kotlin"><span>fun</span> <span>main</span><span>(</span><span>args</span><span>:</span> <span>Array</span><span>&lt;</span><span>String</span><span>&gt;)</span> <span>{</span>
  <span>build</span><span>()</span>
    <span>.</span><span>args</span><span>(*</span><span>args</span><span>)</span>
    <span>.</span><span>packages</span><span>(</span><span>"ch.frankel.springtomicronaut"</span><span>)</span>
    <span>.</span><span>start</span><span>()</span>
<span>}</span></code></pre> </div> </div> <p>The Spring Boot plugin can find the <code>main</code> function "automagically". In Micronaut, the current version requires you to set it explicitly in the POM:</p> <div> <p>pom.xml</p> <div> <pre><code data-lang="xml"><span>&lt;properties&gt;</span>
  ...
  <span>&lt;exec.mainClass&gt;</span>ch.frankel.s2m.SpringToMicronautApplicationKt<span>&lt;/exec.mainClass&gt;</span>
<span>&lt;/properties&gt;</span></code></pre> </div> </div> </div> </div> <div> <h2 id="migrating-the-web-layer">Migrating the web layer</h2> <div> <p>Migrating to the web layer requires:</p> <ol><li><span>To replace Spring Boot starters with the relevant Micronaut dependencies</span></li><li><span>To replace Spring Boot’s annotations with Micronaut’s</span></li></ol> <p>To make an application a webapp, Micronaut mandates to add an embedded server dependency. Tomcat, Jetty, and Undertow are available. Since Spring Boot’s default is Tomcat, let’s use Tomcat:</p> <div> <p>pom.xml</p> <div> <pre><code data-lang="xml"><span>&lt;dependency&gt;</span>
  <span>&lt;groupId&gt;</span>io.micronaut.servlet<span>&lt;/groupId&gt;</span>
  <span>&lt;artifactId&gt;</span>micronaut-http-server-tomcat<span>&lt;/artifactId&gt;</span>
  <span>&lt;scope&gt;</span>runtime<span>&lt;/scope&gt;</span>
<span>&lt;/dependency&gt;</span></code></pre> </div> </div> <p>Spring’s and Micronaut’s annotations map pretty much one to one. To use Micronaut is just a matter of using the annotations of one package instead of the other. The difference is that Spring offers the ability to serialize to JSON by using a specialized <code>Controller</code> annotation, <code>@RestController</code>. Micronaut does not and requires to set a property on the <code>Controller</code> annotation.</p> <table> <colgroup> <col> <col> </colgroup> <thead> <tr> <th>Spring</th> <th>Micronaut</th> </tr> </thead> <tbody> <tr> <td><p><code>o.s.w.b.a.RestController</code></p></td> <td><p><code>i.m.h.a.Controller(produces = [TEXT_JSON])</code></p></td> </tr> <tr> <td><p><code>o.s.w.b.a.GetMapping</code></p></td> <td><p><code>i.m.h.a.Get</code></p></td> </tr> <tr> <td><p><code>o.s.w.b.a.PathVariable</code></p></td> <td><p><code>i.m.h.a.PathVariable</code></p></td> </tr> </tbody> <tfoot> <tr> <td colspan="2"><div><ul><li><span><code>o.s.w.b.a</code> = <code>org.springframework.web.bind.annotation</code></span></li><li><span><code>i.m.h.a</code> = <code>io.micronaut.http.annotation</code></span></li></ul></div></td> </tr> </tfoot> </table> </div> </div> <div> <h2 id="migrating-the-data-access-layer">Migrating the data access layer</h2> <div> <p>To migrate to the data access layer, one must:</p> <ol><li><span>Use Micronaut’s dependencies instead of Spring Boot’s</span></li><li><span>Replace Micronaut’s Spring Boot’s <code>Repository</code> with Micronaut’s</span></li><li><span>Create the schema and load the initial data with Micronaut</span></li></ol> <p>To create a data source and a connection pool, Spring Boot needs a Spring Data starter and a relevant driver. Micronaut demands three different parts:</p> <ol><li><span>A data access dependency</span></li><li><span>A driver dependency</span></li><li><span>A connection pool dependency</span></li></ol> <div> <p>pom.xml</p> <div> <pre><code data-lang="xml"><span>&lt;dependency&gt;</span>
  <span>&lt;groupId&gt;</span>org.springframework.boot<span>&lt;/groupId&gt;</span>
  <span>&lt;artifactId&gt;</span>spring-boot-starter-data-jpa<span>&lt;/artifactId&gt;</span>
<span>&lt;/dependency&gt;</span>

<span>&lt;dependency&gt;</span>
  <span>&lt;groupId&gt;</span>io.micronaut.data<span>&lt;/groupId&gt;</span>
  <span>&lt;artifactId&gt;</span>micronaut-data-hibernate-jpa<span>&lt;/artifactId&gt;</span>
  <span>&lt;version&gt;</span>${micronaut.data.version}<span>&lt;/version&gt;</span>
<span>&lt;/dependency&gt;</span>
<span>&lt;dependency&gt;</span>
  <span>&lt;groupId&gt;</span>io.micronaut.sql<span>&lt;/groupId&gt;</span>
  <span>&lt;artifactId&gt;</span>micronaut-jdbc-hikari<span>&lt;/artifactId&gt;</span>
<span>&lt;/dependency&gt;</span></code></pre> </div> </div> <p>Note that if you forget the connection pool, you’ll run into this error at runtime:</p> <div> <div> <pre>No backing RepositoryOperations configured for repository. Check your configuration and try again</pre> </div> </div> <p>Spring Data JPA generates repositories' implementation at runtime. Micronaut Data generates them at build time. For the developer, the main difference is that the repository interface must be annotated with Micronaut’s <code>@Repository</code>.</p> <div> <div> <pre><code data-lang="kotlin"><span>@Repository</span>
<span>interface</span> <span>PersonRepository</span> <span>:</span> <span>CrudRepository</span><span>&lt;</span><span>Person</span><span>,</span> <span>Long</span><span>&gt;</span></code></pre> </div> </div> <p>One needs to configure Micronaut to scan for repositories and entities:</p> <div> <p>application.yml</p> <div> <pre><code data-lang="yaml"><span>jpa.default</span><span>:</span>
  <span>packages-to-scan</span><span>:</span>
    <span>-</span> <span>'</span><span>ch.frankel.springtomicronaut'</span></code></pre> </div> </div> <p>To create the schema, you can configure Spring Boot in two different ways: either rely on Hibernate’s schema creation or provide a <code>create.sql</code> file at the …</p></div></div></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.frankel.ch/spring-to-micronaut/">https://blog.frankel.ch/spring-to-micronaut/</a></em></p>]]>
            </description>
            <link>https://blog.frankel.ch/spring-to-micronaut/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028483</guid>
            <pubDate>Sun, 08 Nov 2020 20:03:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Tiny CI System]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25028450">thread link</a>) | @todsacerdoti
<br/>
November 8, 2020 | https://www.0chris.com/tiny-ci-system.html | <a href="https://web.archive.org/web/*/https://www.0chris.com/tiny-ci-system.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        

<p>2020-11-08</p>
<p>This is a little demonstration of how little you need to host your own git
repositories and have a modest <a href="https://en.wikipedia.org/wiki/Continuous_integration">Continuous Integration</a>
system for them. All you need is a unixy
server you can ssh into, but arguably you can try this out locally as well.
We will use Redis at one point to queue tasks, but
strictly speaking this can be achieved without additional software. To keep things
simple this will only work with one repository, since this is only describing a
pattern.</p>
<p>The source code to all of that follows below can be found <a href="https://git.sr.ht/%7Estchris/tiny-ci">here</a>.</p>
<h2>Hosting bare git repositories</h2>
<p>Assuming you can ssh into a server and create a directory, this is all you need
to create a shareable git repository:</p>
<pre><code>$ git init --bare
</code></pre>
<p>Ideally you are using a distinct user for it (named <code>git</code>) and have it set to
use <code>git-shell</code> as its default shell. By convention bare repositories are stored
in directories which end in <code>.git</code>. You can now clone this repository from your
machine with:</p>
<pre><code>$ git clone ssh://git@host.example.com/~git/repo.git
</code></pre>
<h2>post-receive hooks</h2>
<p>A <a href="https://git-scm.com/docs/githooks#post-receive">post-receive hook</a> is an executable which can do some work as soon as something new was pushed to the repository. We will use an executable shell script which needs to go inside the <code>hooks</code> directory of the (bare) repository on the server side.</p>
<p>Now the most trivial thing to do would be to do the actual work in here, but this would block the <code>git push</code> on the client side, so we just want to enqueue a new job, return a handle and exit. If what you do takes only a short amount of time, you can stop here. Alternatively you can use this repository for deployments only, by defining it as a separate remote. But the goal here is to have tests run on every push, so we will split the job creation from the actual run.</p>
<p>This is where Redis comes into play for the job queueing. We will assume redis is installed and running and we will use redis-cli to access it from the script. We will use two data structures: a list of jobs waiting to be executed, referenced by a UUID we will generate and a hash where we can store the git revision and the state associated to a given job, as well as its output.</p>
<p>Note that git is passing three arguments to the script via stdin: the old revision before the push, the new revision and the current ref.</p>
<pre><code>#!/bin/bash
while read -r _ newrev ref
do
	id=$(uuid)
	echo "Starting CI job $id"
	redis-cli hset "$id" rev "$newrev" &gt;/dev/null
	redis-cli hset "$id" ref "$ref" &gt;/dev/null
	redis-cli lpush jobs "$id" &gt;/dev/null
done
</code></pre>
<h2>Defining build jobs</h2>
<p>By convention our system will run whatever is in an executable script named <code>ci.sh</code>. The drawback is that this only works with trusted systems and access to the repository needs to be guarded to prevent random code execution. The big advantage is that we don't need to come up with a job definition DSL or cumbersome file format.</p>
<p>Our convention will also be that the script will be passed one argument: the name of the git ref, so we can decide what to do based on the branch we are on.</p>
<p>Let's just put this into a file named <code>ci.sh</code>:</p>
<pre><code>#!/usr/bin/env bash

# the git ref gets passed in as the only argument
ref="$1"

# pretend we're running tests
echo "running tests"

# only deploy if we're on the main branch
[[ "$ref" == "refs/heads/main" ]] &amp;&amp; echo "Deploying"
</code></pre>
<h2>The build runner</h2>
<p>Now that jobs are queued the last piece missing is a job runner. We will make use of Redis' <a href="https://redis.io/commands/blpop">BLPOP command</a> to block until the jobs list has a new job for us. That job id will give us the revision we need to check out and will allow us to write back the output and status of the job.</p>
<p>Note that, as discussed, this assumes a repository called <code>test</code> is already checked out right next to the script.</p>
<p>tiny-ci.sh</p>
<pre><code>#!/usr/bin/env bash

# ./runner.sh is supposed to run on the server where your git repository lives

# the logic in here will run in an infinite loop:
# * (block and) wait for a job
# * run it
while :
do

# Announce that we're waiting
echo "Job runner waiting"

# We are using https://redis.io/commands/blpop to block until we have a new
# message on the "jobs" list. We use `tail` to get the last line because the
# output of BLPOP is of the form "list-that-got-an-element\nelement"
jobid=$(redis-cli blpop jobs 0 | tail -n 1)

# The message we received will have the job uuid
echo "Running job $jobid"

# Get the git revision we're supposed to check out
rev=$(redis-cli hget "${jobid}" "rev")
echo Checking out revision "$rev"

# Get the git ref
ref=$(redis-cli hget "${jobid}" "ref")

# Prepare the repository (hardcoded path) by getting that commit
cd test || exit; git fetch &amp;&amp; git reset --hard "$rev";

# Actually runs the job and saves the output
if ! output=$(./ci.sh "$ref" 2&gt;&amp;1);
then
    status="failed";
else
    status="success";
fi;

# Update the result status
redis-cli hset "${jobid}" "status" $status;

# Update the job output
redis-cli hset "${jobid}" "output" "$output";

echo "Job ${jobid} done"

done
</code></pre>
<h2>Running it</h2>
<p>Summing up:</p>
<ul>
<li>there's a bare git repository somewhere, called <code>test.git</code></li>
<li>we can clone the empty repo (or create a new one and add the respective remote)</li>
<li>on the server hosting the git repository we clone <code>test.git</code> into <code>test</code> and place <code>tiny-ci.sh</code> next to it</li>
<li>we run builds by starting <code>tiny-ci.sh</code> on the server hosting the repository</li>
</ul>
<p>Now if we <code>git push</code> a new commit to the <code>main</code> branch with the <code>ci.sh</code> file from above, the output will return the job id</p>
<pre><code>Enumerating objects: 5, done.
...
remote: Starting CI job dab82634-21cc-11eb-b3b3-9b8767dff47c
</code></pre>
<h2>Checking build status</h2>
<p>Knowing a job uuid, the easiest way to get the status
of a build is by using the <code>--csv</code> style output of the <a href="https://redis.io/commands/hgetall">HGETALL</a> command of redis.</p>
<pre><code>$ ssh example.com redis-cli --csv hgetall $JOB_UUID
"rev","f0706ea18a22031f84619b1161c8fbdb0dcd6850","ref","refs/heads/master","status","success","output","running tests\nDeploying"
</code></pre>
<h2>Possible further improvements</h2>
<ul>
<li>
<p><strong>multi-repo support</strong></p>
<p>This would mean changes to the <code>post-receive</code> hook to put jobs in a list named <code>job-${REPONAME}</code> and then have the worker also react based on that. Notice how <code>redis-cli blpop</code> takes several lists to watch and will also return the name of the list.</p>
</li>
<li>
<p><strong>job cleanup</strong></p>
<p>Creating a key for every job pollutes the redis database unnecesarily. Enqueuing the job could be done via <a href="https://redis.io/commands/setex">SETEX</a> so that the keys go away after one hour / one day / one week. The purpose of Redis here is short term storage and not long-term archival of job results</p>
</li>
<li>
<p><strong>more workers</strong></p>
<p>Scaling to multiple workers on the same machine would need different working folders (and some process isolation depending on the tasks run in there). Scaling to multiple machines would need access to a central redis instance for job distribution.</p>
</li>
<li>
<p><strong>worker isolation / sandboxing</strong></p>
<p>For more complex tasks some kind of process and file-system isolation is necessary. The worker could spin up VMs or Docker containers. The build system used on <a href="https://builds.sr.ht/">builds.sr.ht</a> for instance uses a <a href="https://man.sr.ht/builds.sr.ht/installation.md#security-model">Docker container run as an unprivileged user in a KVM qemu machine</a>.</p>
</li>
<li>
<p><strong>timestamps</strong></p>
<p>For convenience you would definitely want timestamps for every operation. This also allows to list queries like "the last five jobs" or to do maintenance on job results based on their time.</p>
</li>
<li>
<p><strong>notifications</strong></p>
<p>Any CI system will have some form of notifications and the simplest form would be to do something in the script, right at the end. But this covers only the success case, so a better approach would be to create a notification queue and have a notification worker react on that.</p>
</li>
</ul>
<p><a href="https://lobste.rs/s/fbc6wl/tiny_ci_system">Discuss on lobste.rs</a></p>


<ul>
  
</ul>

    </div></div>]]>
            </description>
            <link>https://www.0chris.com/tiny-ci-system.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028450</guid>
            <pubDate>Sun, 08 Nov 2020 19:59:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Data of 20 Million Big Basket Users Up for Sale on Dark Net]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25028437">thread link</a>) | @notRobot
<br/>
November 8, 2020 | https://cyspyindia.com/article/data-of-two-crore-big-basket-users-up-for-sale-on-dark-net/ | <a href="https://web.archive.org/web/*/https://cyspyindia.com/article/data-of-two-crore-big-basket-users-up-for-sale-on-dark-net/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                <h3>
                    Data Of Two Crore Big Basket Users Up For Sale On Dark Net
                </h3>


                

                

                

                

                
                <p>
                    <img src="https://cyspyindia.com/media/resources/155218ae437051499cb91d03046e64df.jpg">
                    <small>
                        A sample of the leaked data. Picture Courtesy: Cyble
                    </small>
                </p>
                
                <p>Indian food and grocery chain Big Basket was reported to have fallen prey to a data breach, with details of two crore of its customers reportedly being put up for sale on the dark web.&nbsp;</p><p>The retail giant known for online sale and delivery of groceries was founded in 2011 and offers its services across the country. Online grocery shopping became all the more popular in India ever since the country was placed under lockdown in light of the COVID 19 pandemic.&nbsp;</p><p>According to researchers at Cyble, the data breach was discovered during a routine sweep of the dark web on Friday. Cyble, in an official update on their website, stated that the data was being offered for sale for USD 40,000. Based on the data examined so far, Cyble believes that it was hacked from Big Basket servers in mid-October this year.&nbsp;</p><p>“The leak contains a database portion, with the table name ‘member_member’. The size of the SQL file is 15 GB, containing close to 20 Million user data. More specifically, this includes names, email IDs, password hashes, contact numbers (mobile + phone), addresses, date of birth, location, and IP addresses of login, among many others,” Cyble update said.&nbsp;</p><p>Cyble founder Beenu Arora told CySpy India that inquiries were still underway regarding the threat actor who was offering the data for sale, and that all the details regarding the incident had been shared with Big Basket.&nbsp;</p><p>CySpy India has reached out to Big Basket for comment and their response will be added to this article as soon as it is received.</p><p>Cyble has acquired the data and published it on its indexing website - AmiBreached - where users who suspect that their details might be part of the leaked data can check for the same.&nbsp;</p>
            </div></div>]]>
            </description>
            <link>https://cyspyindia.com/article/data-of-two-crore-big-basket-users-up-for-sale-on-dark-net/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028437</guid>
            <pubDate>Sun, 08 Nov 2020 19:57:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Teardown Design Notes]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25028432">thread link</a>) | @agluszak
<br/>
November 8, 2020 | http://blog.tuxedolabs.com/2020/11/05/teardown-design-notes.html | <a href="https://web.archive.org/web/*/http://blog.tuxedolabs.com/2020/11/05/teardown-design-notes.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Teardown started as a technology experiment and it’s one of those games where gameplay was designed to fit the technology, rather than the other way around. It’s not the first time I’ve been involved in such projects (Sprinkle, Smash Hit), and probably not the last, but Teardown was by far the most frustrating experience yet.</p>

<p>The idea of a fully destructible environment is compelling for the player but a nightmare for the game designer. Walls can no longer be used as obstacles, key objects that the player might need to complete an objective can break and the designer is no longer in control over a players path through the game, potentially breaking the intended progression. Not to mention all the technical hurdles a fully destructible environment implies when it comes to physics, lighting, scripting, etc, but more on that is a future blog post.</p>

<p>Destruction is often used in games as a decorative special effect, but for Teardown the intention was always to use destruction as the key element in gameplay and with a limited amount of action, allowing the player to do detailed precision work rather than total mayhem.</p>

<p><a href="http://blog.tuxedolabs.com/assets/2020-11-05-teardown.png"><img src="http://blog.tuxedolabs.com/assets/2020-11-05-teardown.png" alt=""></a></p>

<p>After nearly a full year of experimentation and many failed prototypes, the idea of a two-phase heist setting was born. It’s compatible with all the limitations (or lack thereof) that a fully destructible environment impose, while still offering an interesting challenge. It allows the player to move around freely in a fully accessible environment, carefully planning the heist and creating shortcuts using destruction, vehicles and objects from the environment in a creative way. The player chooses when, and I think it’s important that this is the players decision, to go into action mode and try out the created path.</p>


<p>Allowing the player to destroy everything has a huge impact on level design. Since any wall can be torn down, the only true obstacles at our disposal are elevation, distance, water and unbreakable objects. We could use unbreakable objects more, but it would make the environment harder to read and imply a failure to deliver on the promise of a fully destructible environment. Therefore unbreakable objects are only used for rock formations and the ground you’re standing on.</p>

<p>The relatively small level size started as a technical limitation, but I don’t think the game would benefit from larger levels even if it was technically possible. Villa Gordon is currently the largest level in the game, and it can already be a bit tedious to walk around during the preparation phase. Personally I think the game shines in a more compact and cluttered environment like Hollowrock Island, with some verticality to allow for more interesting shortcuts.</p>

<p>The only place we found the level size to be a limitation was the end chase on Frustrum level. We originally anticipated it to be twice as long, but due to a 3D texture size limitation on AMD graphics cards, we had to restrict it to 400 meters. We could have made it twice as long using a U-shaped level, but we also wanted to keep the level straight to have the goal direction consistently aligned with the sun.</p>


<p>Nobody likes a timer, and in previous iterations of the game idea there was no timer. Since the game offers so much player freedom, the only viable option to impose any form of challenge would be resource limitations, and for a sandbox game where destruction plays a central role, adding restrictive resource limitations just doesn’t make the game fun. The goal with the alarm timer has always been to offer a challenge even with a generous amount of tools and resources. While I can agree that a timer is usually a bad idea in game design, I’m really happy the way it turned out in Teardown.</p>

<p>Along the way we’ve mixed up the timed missions with other types where the challenge comes more from moving heavy objects, demolishing buildings or putting out fires, but I’m not convinced that alone could support a whole game. In several missions there are alarmed targets attached to something heavy, allowing it to be moved around to some extent, which I think is a good mix, letting the player choose whether to tinker with the environment or just make a run for it.</p>

<p><a href="http://blog.tuxedolabs.com/assets/2020-11-05-chopper.png"><img src="http://blog.tuxedolabs.com/assets/2020-11-05-chopper.png" alt=""></a></p>

<p>A popular suggestion has been to have the security chopper chase the player after arriving to the scene instead of 	instant failure, but as a general solution I don’t think it’s a good idea. It would introduce an element of randomness that would discourage the strategic thinking and careful planning that this game is all about, in favor of just replaying the mission until reaching the escape vehicle before dying. So instead we added a separate mission type that still allows the player to make preparations, but the chopper shows up shortly after clearing the first target, effectively replacing the timer with an enemy. I think both mission types work well, but that doesn’t necessarily mean it’s a good idea to combine them.</p>


<p>Quicksave can be a sensitive topic in game design. For linear games it’s often a tough decision whether to offer quicksave at any time or save progression only at certain times or locations. Some players refuse to use a generous quicksave feature, as it could be considered cheating.</p>

<p>This is something I think turned out particularly well in Teardown - allowing just one save slot, freely available at any time during preparation, but disabled as soon as the alarm goes off. It encourages player experimentation during the preparation phase, but since there is only one slot, it must still be used wisely. Even with quicksave available, a major change of plans often requires a full restart anyway due to resource limitations, vehicle condition or broken objects.</p>

<p>Trying out a route and then go back to the planning phase for improvement is a key part of the core loop and so intrinsic to the game that we actually enforce it in the third mission to communicate that this is the intended way to play the game.</p>


<p>Since any mission can be played in an infinte number of ways there is already natural incentive for replayability, but there are a couple of things in the game specifically designed to increase replayability. Most missions have optional targets that will increase the score. These optional targets are often placed in strategic locations that break up the most efficient path of the required targets, encouraging the player to use a different strategy and/or starting location.</p>

<p>New tools and upgrades introduced later in the game make all earlier missions easier to complete. It gives a natural incentive to go back and replay missions with better tools, clearing more optional targets, which increases score and gives even better tools, forming an outer game loop that can be quite rewarding. Admittedly, for this to have a strong impact on the game, there would need to be more optional targets. However, introducing a lot of optional targets early in the game can be quite overwhelming, so the whole thing might need to be redisigned a bit to work as intended.</p>


<p>Let’s be honest - no one plays Teardown for the story, but I think it serves an important role to frame the missions and as an incentive for progression. It was an early decision to deliver the story in the form of one way e-mail communication and I’m quite happy the way it turned out. Since the player can go back and read old e-mails, it’s possible to catch up on the story when coming back after taking a break from the game. This is something I miss in a lot of other games - the ability to recap the story when coming back to them.</p>

<p>The reason e-mails cannot be replied to is part of the bigger goal of making the player fully anonymous. The main character in Teardown is intentionally lacking name, age, gender and personality traits to fully leave that up the players imagination.</p>

<p><a href="http://blog.tuxedolabs.com/assets/2020-11-05-mail.png"><img src="http://blog.tuxedolabs.com/assets/2020-11-05-mail.png" alt=""></a></p>

<p>The story is also told through the environments, how they progress, descriptions of objects in them, themed valuables and last but not least the television. There’s a lot of room for improvements here. I originally envisioned much more environmental changes when coming back to the same environment (also involving procedural changes based on the players actions) but for several reasons we had to cut back on that.</p>

<p>Missions are kept separate from the e-mails on the Missions tab to give the player an overview of available missions for a particular location. This is to further incentivice replayability and make it clear where improvements are possible to increase score and rank.</p>


<p>Whether to have sandbox levels directly accessible or tied to campaign progression has been a long internal discussion. Knowing that a lot of people would want to play Teardown just for the sandbox experience, it may seem a bit inconsiderate to enforce a complete playthrough to make everything accessible. On the other hand, keeping all environments and tools available in sandbox mode from the beginning would ruin the experience for campaign players.</p>

<p>The route we chose was to keep them locked, but introduce new environments and tools relatively early in the campaign. The first three environments can be unlocked after completing just five missions while the fourth one requires a bit more work. While not suiting everyone, I think it turned out quite well, and I hope more people play and enjoy the campaign because of this decision.</p>

<p>Tool upgrades also carry over to the sandbox mode, which gives a stronger incentive to scavenge valuables and upgrade tools in the campaign.</p>


  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>http://blog.tuxedolabs.com/2020/11/05/teardown-design-notes.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028432</guid>
            <pubDate>Sun, 08 Nov 2020 19:57:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[About iSH’s pending removal from the App Store]]>
            </title>
            <description>
<![CDATA[
Score 692 | Comments 454 (<a href="https://news.ycombinator.com/item?id=25028252">thread link</a>) | @tbodt
<br/>
November 8, 2020 | https://ish.app/app-store-removal | <a href="https://web.archive.org/web/*/https://ish.app/app-store-removal">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
        

<p><strong><span>Update</span></strong>: We got a call this evening from someone who runs App Review. They apologized for the experience we had, then told us they've accepted our appeal and won't be removing iSH from the store tomorrow. We'll stay in contact with them to work out details. Thanks everyone for your support!
</p>


<p>On Monday, October 26th, just four days after we launched iSH on the App Store, we received a call from Apple informing us that they had found our app noncompliant with section 2.5.2 of the App Store Review Guidelines and that they would remove the app from sale if we did not submit a satisfactory update within two weeks. Despite our best efforts, we do not believe we will be able to bring iSH into compliance by tomorrow, the conclusion of this 14 day period, and we expect iSH to no longer be available to download from the App Store after that time. We are working our hardest to get iSH back on the App Store as soon as possible and hope for your understanding and support as we navigate our next steps in this process.</p>

<p>Thanks for using iSH!<br>
Theodore Dubois, Saagar Jha &amp; Martin Persson</p>

<h2 id="why-is-ish-being-removed">Why is iSH being removed?</h2>
<p>Apple believes iSH is not compliant with section 2.5.2 of the App Store Review Guidelines, which governs applications which download and run executable code. Specifically, they believe that iSH “is not self-contained and has remote package updating functionality”, and suggest that we should “remove the remote network activity functionality which could allow for remote code importing into the app, such as wget or curl, or other remote network commands”. Additional communication with Apple has indicated that they believe that iSH is a security concern if we allow any sort of code importing by the user.</p>

<p><strong>We believe iSH is fully compliant with the App Store Review Guidelines.</strong> <a href="https://saagarjha.com/blog/2020/11/08/fixing-section-2-5-2/">Saagar has written</a> a more detailed analysis of why we believe this rejection is incorrect, how we believe Apple has misinterpreted and misapplied this rule to our app, and describe how 2.5.2’s poor wording coupled with the review team being unable to review functionality of scripting applications leads to mistaken classifications like these. At a high level, Apple has selectively targeted iSH using section 2.5.2 without fullying understanding our application, their own guidelines, or the consequences of what they are asking and how they affect the App Store ecosystem as a whole. <strong>Consistent enforcement of Apple’s incorrect interpretation would require the removal of all scripting apps, including many of the most popular applications in the App Store and some of Apple’s own applications.</strong></p>

<h2 id="what-have-we-done-to-get-ish-back-on-the-app-store">What have we done to get iSH back on the App Store?</h2>
<p>We’ve been working for the last two weeks to try to keep iSH available without interruptions. We have drafted numerous appeals, requests for clarifications, rule modifications, and explanatory emails. We’ve been on the phone with Apple for hours. Unfortunately, even with this we have been unable to resolve the issue, and the process has been significantly more stressful than we would have liked it to be. Theodore, the primary iSH liaison to Apple, <a href="https://tbodt.com/2020/11/08/app-review-experiences.html">has written about</a> how this process should be improved.</p>

<p>Our first interaction with the App Store review team actually dates back to May, not October: we wanted to know what Apple thought of iSH, since we weren’t sure how the rules would be enforced for it. Of course, iSH complies with the letter of the guidelines, but review found it to violate 2.5.2 because it could download Linux executable code. The problem appeared that apk lets you install packages, so we decided to remove it and work on other features to make the app more useful in its absence. We submitted this updated build in October and this was what is currently on the App Store.</p>

<p>After our build was flagged for noncompliance, we went through the usual review process: we first asked for clarification, and then after we realized that the rule was being misapplied we submitted a rule change request and of course appealed the decision as well. As the deadline approached we sent off an email to Phil Schiller as well detailing our situation. Unfortunately none of this led anywhere, which brings up to our current situation today.</p>

<h2 id="does-this-mean-i-cant-use-ish-anymore">Does this mean I can’t use iSH anymore?</h2>
<p>No, not at all. However, it will mean that you will no longer be able to get iSH from the App Store, which is something which we would still like to be able to provide. The App Store remains the easiest and most popular method of software distribution on iOS, and we’re working hard to save iSH’s listing because we think the app should have a permanent spot there for users who prefer this method of distribution.</p>

<p>Removal of iSH’s listing on the App Store should not affect your use of iSH if you download the app before it is removed. We have not received any compliance messages from Apple regarding <a href="https://testflight.apple.com/join/97i7KM8O">our TestFlight beta</a>, so we plan to continue offering prerelease versions of iSH there for up to 10,000 beta testers.</p>

<p>Precompiled builds of iSH (distributed as IPA files) will <a href="https://github.com/ish-app/ish/releases">remain available on GitHub</a> for <a href="https://ish.app/altstore">installation through AltStore</a> and for jailbroken users. Advanced users are welcome to <a href="https://github.com/ish-app/ish#build-for-ios">build iSH</a> themselves—it’s free and open source and always will be!</p>

<p><strong><span>Update</span>: <a href="https://twitter.com/a_Shell_iOS/status/1325526061099196416">a-Shell has mentioned</a> that they have received a similar rejection notice. Apple may be running extra review for scripting apps.</strong></p>

        <hr>
        <p><a href="https://ish.app/">Return home</a> | 2020-11-08</p>
    

</div>]]>
            </description>
            <link>https://ish.app/app-store-removal</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028252</guid>
            <pubDate>Sun, 08 Nov 2020 19:33:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[CVE-2020-5387: Prevent a Dell XPS 9370 from booting]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25028250">thread link</a>) | @Sayrus
<br/>
November 8, 2020 | https://sayr.us/exploit/dell-exploit/ | <a href="https://web.archive.org/web/*/https://sayr.us/exploit/dell-exploit/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>In August 2019, I’ve discovered a bug in how the XPS Bios handles a bootable
device. This bug allows an attacker with write permission to a bootable device
or physical access to the computer to prevent it from booting.</p>

<h2 id="the-bug">The bug</h2>

<blockquote>
  <p>The vulnerability is available <a href="https://www.dell.com/support/article/en-us/sln322626/dsa-2020-209-dell-xps-13-9370-improper-exception-handling-vulnerability">on Dell’s website</a>
or by its <a href="https://nvd.nist.gov/vuln/detail/CVE-2020-5387">CVE Identifier</a>.</p>
</blockquote>

<p>The Dell XPS reads and drops file on the primary disk during the booting and
BIOS upgrade process. Namely, it drops:</p>
<ol>
  <li><code>/boot/EFI/Dell/Bios/Recovery/BIOS_PRE.rcv</code></li>
  <li><code>/boot/EFI/Dell/Bios/Recovery/BIOS_CUR.RCV</code></li>
</ol>

<p>These files are used by a process called BIOS Recovery Tool. On the XPS 9370,
<a href="https://www.dell.com/support/article/en-us/sln300716/how-to-recover-the-bios-on-a-dell-computer-or-tablet">BIOS Recovery 3</a>
is used.</p>

<blockquote>
  <p><code>BIOS_PRE.rcv</code> is used for F2/F12 recovery.
<code>BIOS_CUR.rcv</code> is used only during the process of flash update failures. After the successful FW update <code>BIOS_CUR.rcv</code> is always copied to <code>BIOS_PRE.rcv</code>.</p>
</blockquote>

<p>When <code>BIOS_PRE.rcv</code> is corrupted, the BIOS crashes. Triggering this bug prevents
the laptop from accessing the boot menu and accessing the BIOS. The only way to
recover from the exploit is to unplug the exploited boot device. If the exploit is
done on the internal disk, this means that you need to disassemble the laptop.</p>

<p>When “Bios Recovery from Hard Drive” is disabled, recovery (and the exploit) only works from USB stick / USB hard drive.</p>

<h2 id="a-tale-of-failure">A tale of failure</h2>

<p>This writeup is not your usual exploit writeup. I am not an experienced
attacker, especially not against a blackbox such as my laptop’s BIOS. So how did
I end up finding it?</p>

<p>It all started while I was benchmarking my laptop’s internal drive using <code>fio</code>. I
simply wrote <code>randrw</code> instead of <code>randread</code> and thus overwrote a part of my
own hard drive. This part mostly included my boot partition and the two aforementioned BIOS
files.</p>

<p>Luckily, I already had a live Arch Linux USB key to repair everything. My plan was
simple: reboot, fix, reboot. That doesn’t sound so hard!</p>

<p><img src="https://sayr.us/assets/images/20minutesadventure.jpg" alt="Reboot and fix. Twenty minutes adventure."></p>

<p>After rebooting, I could not access a single BIOS Menu. Everything either froze
on a black screen, or on “Preparing one-time boot menu”. If I was unable to boot
anything or access the BIOS setup, I was not going to be able to fix my problem.</p>

<p>At that time, I didn’t know I found a bug in the BIOS, only that I somehow
fucked up so hard that my laptop was bricked. So I contacted the Dell support,
because I should not be able to brick my laptop like this! They agreed that it
was not supposed to happen and offered me a replacement.</p>

<h3 id="backups-and-a-surprise">Backups and a surprise</h3>

<p>Before sending back my laptop, I wished to recover what I was working on as it
had yet to be pushed. So I took the NVMe out of the laptop and put it in another
machine to dump a disk image. Right after removing the NVMe, I could access the
Bios and even boot on my USB stick.</p>

<p>Was my NVMe broken? From another computer, I could access any file on it. The
boot partition was indeed corrupted but the content of the LUKS partition was
sane.</p>

<p>Well then, if it’s not hardware, is it something on the drive? I dumped a full
disk image on an external HDD, tried to boot it on my laptop and <strong>boom</strong>. I
just reproduced the bug I had earlier on another drive.</p>

<p>I couldn’t miss the chance to play with what I had found so I kept a full disk
image as a backup of my data and another as a way to trigger the bug.</p>

<h3 id="finding-the-culprit">Finding the culprit</h3>

<p>Now that I had a disk image that triggered the bug, it was time to investigate.
At that point, the only information I had were:</p>
<ul>
  <li>I wrote random bytes on my disk, including in the partitions table, the boot
partition, and part of my encrypted disk</li>
  <li>Something on the disk is definitely messing up with the BIOS</li>
  <li>Duplicating the disk image and plugging it in as a USB Disk triggers the bug</li>
</ul>

<p>I assumed that bytes inside my encrypted partition were not the cause as the
BIOS had no reason to read them and they were valid when read from another
computer. Hopefully, it is not broken hardware either.</p>

<h4 id="partitions-table">Partitions table</h4>

<p>My first guess was that my partitions table was invalid and something broke
while enumerating tables. For instance, I might have been listing a partition
offset outside of the disk size, or an invalid size.</p>

<p>To verify that, I created a new partition table from scratch and copied the boot
partition content (files) and tried to boot.</p>

<p>Spoiler alert: <strong>Still broken.</strong></p>

<p>Even better, I did not copy my encrypted partition so the bug was definitely
triggered by something inside the boot partition!</p>

<h4 id="boot-partition">Boot partition</h4>

<p>Obviously, there was something in my boot partition that triggered the bug. But
why would the BIOS read the content of the boot partition? Silly me, I did not
know that BIOS recovery from hard drive was a thing. As far as I knew, it might
have been coming from anything: an invalid folder name, a corrupted file or even
a hard-link to an invalid INode…</p>

<p><em>Obviously not an invalid hard-link as I just did copy the files and there were
no hard-links on this new drive…</em></p>

<p>I was lacking the experience in this kind of bug so the easiest way for me was
to proceed by elimination:</p>
<ul>
  <li><strong>Step 1:</strong> Delete a folder</li>
  <li><strong>Step 2:</strong> Try to boot</li>
  <li>If it boots, restore only this folder, and apply Step 1 to the content of this
folder</li>
  <li>If it does not boot, simply go back to Step 1</li>
  <li><strong>Repeat</strong> until there is only a single file or folder remaining that triggers
the bug</li>
</ul>

<p>I deleted files using this method until I found out that deleting the
<code>/boot/EFI/Dell/Bios/Recovery</code> folder fixed the issue.</p>

<p><strong>Fixed !</strong></p>

<p>At this point, I notified Dell about the bug and sent them the two files that
were triggering the bug. The BIOS being closed source, it would be time
consuming to find:</p>
<ul>
  <li>What the format of <code>BIOS_PRE.rcv</code> and <code>BIOS_CUR.rcv</code> was (looking at
Dell’s documentation, <em>it might just be a Windows executable</em>)</li>
  <li>What field in that file triggers the bug</li>
</ul>

<p>I might have been able to <code>hexdiff</code> a working file with the broken one, to
highlight the differences. However, that would not have given me a better
understanding of the issue. From there, Dell took over and we now have access to
the result of the investigation: <strong>Improper Exception Handling</strong>.</p>

<h2 id="timeline">Timeline</h2>

<ul>
  <li>12 August 2019: Message sent to secure@dell.com without the PoC</li>
  <li>13 August 2019: Answer from Dell asking for more details</li>
  <li>11 September 2019: Message sent to secure@dell.com with steps to reproduce,
including a small disk image and the broken files.</li>
  <li>12 September 2019: Answer from Dell with the ticket number</li>
  <li>26 September 2019: Dell confirmed the problem exists</li>
  <li>January 2020: Dell plans to try to release a patch in April</li>
  <li>April 2020: Dell plans to publish the patch end of May</li>
  <li>May 2020: Dell plans to publish the patch beginning of July</li>
  <li>19 August 2020: Bios 1.13.1 released with the patch</li>
  <li>29 September 2020: Dell notifies me a Security Advisory was published</li>
  <li>16 October 2020: Dell updates the CVE score as the CVE does not require physical access, only high privileges</li>
</ul>

<h2 id="final-words">Final words</h2>

<p>The story might have begun as a scary <em>do your backups</em> story, but it ends without
any data-loss and with a reported vulnerability. Overall, it was a lot of fun and
a fine addition to my collection.</p>

<p>Perhaps next time we’ll talk about how to destroy the displayed image on my
monitor using only a JPEG file or making a video input detected as HDR by
squeezing the DisplayPort cable.</p>

  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://sayr.us/exploit/dell-exploit/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028250</guid>
            <pubDate>Sun, 08 Nov 2020 19:33:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Korn Shell Operators]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027977">thread link</a>) | @pazyp
<br/>
November 8, 2020 | http://pazikas.com/2020/11/08/korn-shell-opertators/ | <a href="https://web.archive.org/web/*/http://pazikas.com/2020/11/08/korn-shell-opertators/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>I write $SHELL scripts most days, I am always looking these two tables up on other sites rather than commit them to memory,  so why not just post it here so I know where to find it forever more. We mostly use ksh over bash as ksh is on every system regardless of age of that host.</p><div><table><tbody><tr><td><strong>Item</strong></td><td><strong>Description</strong></td></tr><tr><td><strong>-a</strong> <em>File</em></td><td>True, if the specified file is a symbolic link that points to another file that does exist.</td></tr><tr><td><strong>-b</strong> <em>File</em></td><td>True, if the specified file exists and is a block special file.</td></tr><tr><td><strong>-c</strong> <em>File</em></td><td>True, if the specified file exists and is a character special file.</td></tr><tr><td><strong>-d</strong> <em>File</em></td><td>True, if the specified file exists and is a directory.</td></tr><tr><td><strong>-e</strong> <em>File</em></td><td>True, if the specified file exists.</td></tr><tr><td><strong>-f</strong> <em>File</em></td><td>True, if the specified file exists and is an ordinary file.</td></tr><tr><td><strong>-g</strong> <em>File</em></td><td>True, if the specified file exists and its <strong>setgid</strong> bit is set.</td></tr><tr><td><strong>-h</strong> <em>File</em></td><td>True, if the specified file exists and is a symbolic link.</td></tr><tr><td><strong>-k</strong> <em>File</em></td><td>True, if the specified file exists and its sticky bit is set.</td></tr><tr><td><strong>-n</strong> <em>String</em></td><td>True, if the length of the specified string is nonzero.</td></tr><tr><td><strong>-o</strong> <em>Option</em></td><td>True, if the specified option is on.</td></tr><tr><td><strong>-p</strong> <em>File</em></td><td>True, if the specified file exists and is a FIFO special file or a pipe.</td></tr><tr><td><strong>-r</strong> <em>File</em></td><td>True, if the specified file exists and is readable by the current process.</td></tr><tr><td><strong>-s</strong> <em>File</em></td><td>True, if the specified file exists and has a size greater than 0.</td></tr><tr><td><strong>-t</strong> <em>FileDescriptor</em></td><td>True, if specified file descriptor number is open and associated with a terminal device.</td></tr><tr><td><strong>-u</strong> <em>File</em></td><td>True, if the specified file exists and its <strong>setuid</strong> bit is set.</td></tr><tr><td><strong>-w</strong> <em>File</em></td><td>True, if the specified file exists and the write bit is on. However, the file will not be writable on a read-only file system even if this test indicates true.</td></tr><tr><td><strong>-x</strong> <em>File</em></td><td>True, if the specified file exists and the <strong>execute</strong> flag is on. If the specified file exists and is a directory, then the current process has permission to search in the directory.</td></tr><tr><td><strong>-z</strong> <em>String</em></td><td>True, if length of the specified string is 0.</td></tr><tr><td><strong>-L</strong> <em>File</em></td><td>True, if the specified file exists and is a symbolic link.</td></tr><tr><td><strong>-O</strong> <em>File</em></td><td>True, if the specified file exists and is owned by the effective user ID of this process.</td></tr><tr><td><strong>-G</strong> <em>File</em></td><td>True, if the specified file exists and its group matches the effective group ID of this process.</td></tr><tr><td><strong>-S</strong> <em>File</em></td><td>True, if the specified file exists and is a socket.</td></tr><tr><td><em>File1</em><strong> -nt</strong> <em>File2</em></td><td>True, if File1 exists and is newer than File2.</td></tr><tr><td><em>File1</em><strong> -ot</strong> <em>File2</em></td><td>True, if File1 exists and is older than File2.</td></tr><tr><td><em>File1</em><strong> -ef</strong> <em>File2</em></td><td>True, if File1 and File2 exist and refer to the same file.</td></tr><tr><td><em>String1</em><strong> =</strong> <em>String2</em></td><td>True, if String1 is equal to String2.</td></tr><tr><td><em>String1</em><strong> !=</strong> <em>String2</em></td><td>True, if String1 is not equal to String2.</td></tr><tr><td><em>String</em><strong> =</strong> <em>Pattern</em></td><td>True, if the specified string matches the specified pattern.</td></tr><tr><td><em>String</em><strong> !=</strong> <em>Pattern</em></td><td>True, if the specified string does not match the specified pattern.</td></tr><tr><td><em>String1</em><strong> &lt;</strong> <em>String2</em></td><td>True, if String1 comes before String2 based on the ASCII value of their characters.</td></tr><tr><td><em>String1</em><strong> &gt;</strong> <em>String2</em></td><td>True, if String1 comes after String2 based on the ASCII value of their characters.</td></tr><tr><td><em>Expression1</em><strong> -eq</strong> <em>Expression2</em></td><td>True, if Expression1 is equal to Expression2.</td></tr><tr><td><em>Expression1</em><strong> -ne</strong> <em>Expression2</em></td><td>True, if Expression1 is not equal to Expression2.</td></tr><tr><td><em>Expression1</em><strong> -lt</strong> <em>Expression2</em></td><td>True, if Expression1 is less than Expression2.</td></tr><tr><td><em>Expression1</em><strong> -gt</strong> <em>Expression2</em></td><td>True, if Expression1 is greater than Expression2.</td></tr><tr><td><em>Expression1</em><strong> -le</strong> <em>Expression2</em></td><td><br>True, if Expression1 is less than or equal to Expression2.</td></tr><tr><td><em>Expression1</em><strong> -ge</strong> <em>Expression2</em></td><td>True, if Expression1 is greater than or equal to Expression2.</td></tr></tbody></table></div></div>]]>
            </description>
            <link>http://pazikas.com/2020/11/08/korn-shell-opertators/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027977</guid>
            <pubDate>Sun, 08 Nov 2020 18:58:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[We Pay for Software]]>
            </title>
            <description>
<![CDATA[
Score 22 | Comments 5 (<a href="https://news.ycombinator.com/item?id=25027907">thread link</a>) | @aberoham
<br/>
November 8, 2020 | https://adamwiggins.com/making-computers-better/pay | <a href="https://web.archive.org/web/*/https://adamwiggins.com/making-computers-better/pay">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    <h3>how we pay for software</h3>

    <p>Quality software costs money to make.</p>

    <p>The tech industry has backed itself into a corner in the last couple decades by implying to users that software doesn’t cost much. Google led the charge here, by providing a series of useful and well-made applications like Gmail, Google Maps, and its core search product for free. They aren’t free, of course: users ”pay“ with their attention and personal data for advertisements.</p>

    <p>Paying with attention and personal data is probably a good trade for search and maps, maybe less so for email. But it’s not a model that fits professional and creative tools.</p>

    <p>Here are three proven models for paying for software:</p>

    <ul>
      <li><p><strong>Ad-supported software.</strong> Only really appropriate for pure-consumer products that are attention-oriented to begin with (e.g. social media). Requires huge scale since <a href="https://www.quora.com/What-is-Twitters-ARPU-average-revenue-per-user">each user is only worth a few bucks</a>. Because the user is the product rather than the customer, the user’s needs are not the priority of the business.</p></li>
      <li><p><strong>Software bundled with hardware or content.</strong> End users seem to be fine with paying directly for hardware (smartphone, laptop) or content (Netflix, Spotify). In these examples the software is a huge part of what you’re paying for—a Mac without macOS is just an overpriced PC, and Netflix without its excellent browsing and playback software is just cloud storage media stocked with MP4 files. Yet few people think of themselves as paying for macOS or Netflix’s app.</p></li>
    </ul>

    <figure>
      <img src="https://adamwiggins.com/making-computers-better/spotify-pricing.png" alt="pricing page for Spotify">

      <figcaption>
        <svg width="50" height="10" viewBox="0 0 50 10" fill="none" xmlns="http://www.w3.org/2000/svg">
          <rect width="50" height="10" rx="2.5" fill="#298070"></rect>
        </svg>

        <p><a href="https://www.spotify.com/us/premium/">Spotify’s pricing</a> demonstrates the common price range for individually-purchased subscriptions: $5/month on the low end, $15/month on the high end.</p>
      </figcaption>
    </figure>

    <ul>
      <li><p><strong>B2B SaaS.</strong> ”Business-to-business software-as-a-service“ is a mouthful, but represents a a beautiful, pure, and simple payment model for software. A business has a problem; a software creator offers a web-based product that solves the problem; the business purchases directly from the creator, and continues to pay as long as they have the problem and the product is good. Interests are aligned, and the no-middleman nature of the web means software creators have a direct relationship with their customers. Everybody wins. That’s why there’s so many success stories here: Slack, GitHub, Mailchimp, Atlassian, Zoom, Notion, and Carta just to name a few.</p></li>
    </ul>

    <p>I seek a world with a great diversity of lovingly-crafted software for niche audiences. But niche is not possible with ad-supported which requires huge scale to work, and hardware and content licensing have massive up-front capital costs that imply financing models not compatible with small-scale.</p>

    <p>Happily, B2B SaaS is having its <a href="https://microconf.com/">revolution for small independents</a> right now. So what’s the equivalent of that for pro creator tools like <a href="http://www.telestream.net/screenflow/overview.htm">video editing</a> or <a href="https://www.literatureandlatte.com/scrivener/overview">writing</a> or <a href="https://www.devontechnologies.com/apps/devonthink">knowledge management</a>? Can we have something as beautiful and pure as B2B SaaS but for individuals as well as businesses, and for desktop, mobile, and other native apps?</p>

    <p>And indeed there does seem to be movement toward subscription-priced prosumer products in the $5 –&nbsp;$15/month range. See
      established players like
      <a href="https://www.dropbox.com/individual/plans-comparison">Dropbox</a>,
      <a href="https://evernote.com/compare-plans">Evernote</a>,
      <a href="https://ulysses.app/get/">Ulysses</a> and
      <a href="https://www.omnigroup.com/omnifocus/buy/">OmniFocus</a>;
      plus up-and-comers like
      <a href="https://hey.com/pricing/">HEY</a>,
      <a href="https://roamresearch.com/">Roam</a>,
      <a href="https://www.descript.com/pricing">Descript</a>, and my own venture <a href="https://museapp.com/">Muse</a>.
    </p>

    <figure>
      <img src="https://adamwiggins.com/making-computers-better/calendly-pricing.png" alt="pricing for Calendly">

      <figcaption>
        <svg width="50" height="10" viewBox="0 0 50 10" fill="none" xmlns="http://www.w3.org/2000/svg">
          <rect width="50" height="10" rx="2.5" fill="#298070"></rect>
        </svg>

        <p><a href="https://calendly.com/pages/pricing">Calendly’s pricing</a> is similar to other prosumer products.</p>
      </figcaption>
    </figure>

    <p>As a <strike>user</strike> customer, I like the idea of finding a few great tools that empower my work and paying for them directly. That gives me confidence the product will serve my needs long-term, because I’m the customer.</p>

    <p>And this doesn’t mean a bigger budget for my tech product purchases overall: I can subtract these subscriptions from the money I previously spent on a new phone and laptop every two years, then buy new hardware a little less often. Right now I think great software is more important than hardware advances, much as I love shiny new devices.</p>

    <p>As a software creator, I love the idea of serving a niche of customers who are deeply connected with the product I’m making and get lots of value from it. It has that pure, direct, and personal relationship we see in B2B SaaS, but maybe even more initimate because these are personal tools.</p>

    <p>Ok sounds good—but now the next problem with a direct-payment model. If we’re going to make software subscriptions work, we as creators have to address the sharp edges and <a href="https://www.matthewball.vc/all/misnomers">subscription fatigue</a> people may experience maintaining a stack of software subscriptions.</p>

    <p>Individuals are cautious about recurring billing due to poorly-designed—even <a href="https://help.nytimes.com/hc/en-us/articles/360003499613-Cancel-your-subscription">borderline scammy</a>—subscription buying, renewal, and canceling experiences. The reference worst-case scenario are gym memberships or mobile phone contracts: being trapped in a recurring payment that no longer fits your needs, and being forced to navigate a customer service maze to try to downgrade or cancel.</p>

    <p>So how can we give customers a great experience with subscriptions? A few proposals:</p>

    <ul>
      <li><p>Treat the <strong>buying experience as part of your product</strong>. Make it easy for prospective customers to see what they will get; making buying easy and fun; show them love afterward, not just a ”you’re all set“ dialog and nothing else.</p></li>
    </ul>

    <figure>
      <img src="https://adamwiggins.com/making-computers-better/gumroad-pay.png" alt="paying for a product on Gumroad">

      <figcaption>
        <svg width="50" height="10" viewBox="0 0 50 10" fill="none" xmlns="http://www.w3.org/2000/svg">
          <rect width="50" height="10" rx="2.5" fill="#298070"></rect>
        </svg>

        <p><a href="https://gumroad.com/">Gumroad</a> offers <a href="https://gumroad.com/l/NlhpD">a great buying experience</a> with a minimum of fuss.</p>
      </figcaption>
    </figure>

    <ul>
      <li><p>Make it <strong>easy to cancel</strong> at any moment. Canceling should be as smooth as purchasing initially. For bonus points, give a refund for any unused portion of purchased time.</p></li>
      <li><p>For longer-cycle payments (e.g. annual), give <strong>heads-up notifications before payment</strong> so that users are not surprised. Direct them on how they can cancel with just a couple of clicks. You shouldn’t want any money from someone who is no longer getting value from your product.</p></li>
    </ul>

    <figure>
      <img src="https://adamwiggins.com/making-computers-better/hey-cancel.png" alt="canceling your subscription on HEY">

      <figcaption>
        <svg width="50" height="10" viewBox="0 0 50 10" fill="none" xmlns="http://www.w3.org/2000/svg">
          <rect width="50" height="10" rx="2.5" fill="#298070"></rect>
        </svg>

        <p>HEY makes canceling easy: just two clicks. The confirmation dialog is clear on exactly what happens next.</p>
      </figcaption>
    </figure>

    <ul>
     <li><p>Don’t lock users out of their data, even if their trial runs out or their subscription ends. Make it easy to <strong>get at their data and export</strong> for use in other places.</p></li>
      <li><p>Don’t treat lapsed or canceled subscriptions as strangers or defectors. Instead, <strong>consider them alumni and offer them your respect</strong> and maybe even some extra perks. They’ve been customers in the past, so be thankful for that. Remember they might return as a customer in the future or refer other potential customers.</p></li>
    </ul>

    <figure>
      <img src="https://adamwiggins.com/making-computers-better/slack-credit.png" alt="a credit for inactive users on Slack">

      <figcaption>
        <svg width="50" height="10" viewBox="0 0 50 10" fill="none" xmlns="http://www.w3.org/2000/svg">
          <rect width="50" height="10" rx="2.5" fill="#298070"></rect>
        </svg>

        <p>Slack delights and surprises with their <a href="https://slack.com/intl/en-de/help/articles/218915077-Fair-Billing-Policy">fair billing policy</a>.</p>
      </figcaption>
    </figure>

    <p>Beyond prosumer subscriptions, I’m happy to see products experimenting with models appropriate to their domain, like the ski app <a href="https://twitter.com/parrots/status/1314192830500347906">Slopes offering</a> packs of day passes; time-based notes app <a href="https://medium.com/@drewmccormack/cash-cow-revisited-4c2185e2b3d8">Agenda</a> with feature unlock based on subscription time windows; and <a href="https://www.komoot.com/product">Komoot</a> with hiking region budles for sale.</p>

    <p>One last one I find especially promising is the half-crowdfunding, half-early-purchase model that’s seen success with <a href="https://store.steampowered.com/earlyaccessfaq/?snr=1_200_200_Early+Access">video games</a>, <a href="https://www.studiobinder.com/blog/kickstarter-film-projects/">films</a>, and <a href="https://www.futurefonts.xyz/about">typefaces</a>. This blurs the distinctions between customer, investor, and donor. When you buy a Steam Early Access game you’re saying not just ”I want to purchase this product“ but also ”I believe in this product and the team behind it; I want it to exist; and I’m willing to risk a bit of my money to make that happen.“ And maybe: ”I like getting to give early feedback and influence the outcome of the finished product.“ <a href="https://wikimediafoundation.org/support/">Wikipedia with its pure patronage model</a> and <a href="https://www.are.na/roadmap">Are.na with its earnings transparency</a> are variations on this theme.</p>

    <p><strong>The world we have:</strong> Almost all software is paid for by (1) ads and selling personal data (2) bundling software with hardware or content or (3) business-purchased subscriptions. As a result, the type of software we have is limited to what will fit in those boxes.</p>

    <p><strong>The world I want:</strong> A wide diversity of funding and payment models for pure software products. Prosumer subscriptions are commonplace; users feel great about buying tools that matter to them; and they get a great experience when buying and canceling. Crowdfunding, patronage, and many other models flourish and individual products can choose the model that best fits them and their customers.</p>
  </article></div>]]>
            </description>
            <link>https://adamwiggins.com/making-computers-better/pay</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027907</guid>
            <pubDate>Sun, 08 Nov 2020 18:51:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[It’s OK to Not Care About Politics]]>
            </title>
            <description>
<![CDATA[
Score 35 | Comments 49 (<a href="https://news.ycombinator.com/item?id=25027906">thread link</a>) | @freediver
<br/>
November 8, 2020 | https://www.meta-nomad.net/its-ok-to-not-care-about-politics/ | <a href="https://web.archive.org/web/*/https://www.meta-nomad.net/its-ok-to-not-care-about-politics/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1182">

	<!-- .entry-header -->

	
	<div>

		<p><span>During recent research into the life of Machiavelli something began to become quite clear to me. We weren’t always, universally, socially, communally or even personally, political. That is to say, it’s only recently that it’s become commonplace to declare oneself as left, right, Republican, Democrat, Labour, Conservative, Centrist, Reactionary, Socialist, Red, Blue, x-pilled, y-pilled etc. In terms of human history this way of </span><span><em><span>being</span></em></span><span> – as a political-being, or even as homo-politicus – is </span><span><em><span>extremely</span></em></span><span> new. The very idea of a left/right split/spectrum comes from where people sat during the French Revolution, when members of the National Assembly divided themselves into those in support of the king (right) and supporters of the Revolution (left). Arguably this is one and only time that the idea of a left/right spectrum has ever made sense. Since then </span><span><em><span>both</span></em></span><span> ‘directions’ signal virtue to various camps and striate one into relatively specific ways of thinking. The year we’re roughly talking about here is 1789, that’s round that all up and say – for clarity’s sake – we’ve been political ‘beings’ for just over 200 years. Once again, humans in their current evolutionary iteration have been around for 200,000 years. So we’ve had this political chip on our shoulders for roughly 0.1% of our entire lifetime. Of course, you could argue that for a large amount of that time we haven’t exactly had the infrastructure to allow for what we now commonly understand as politics or political economy, but we </span><span><em><span>have</span></em></span><span> had that for a few thousand years at least, so even going by that metric, the notion of a political-being or of a political-human is still quite new. </span></p>
<p><span>It seems to me the reason for the original (non) position, wherein man wasn’t apolitical, nor anti-political, but simply detached from the political, wasn’t due to some oppression (though some would argue otherwise)[1], nor was it really to do with any ignorance; it was largely because in relation to man’s daily life, the specific political on-goings didn’t matter to him. I would argue that this is still true, we’re just all caught up in status and popularity games. </span></p>
<p><span>The very idea that within contemporary (Western) society one could be ‘detached’ from politics seems absurd, that’s how tight of a grasp it has on our lives. A grasp which is ever-tightened by the popular rhetoric surrounding politics. Society in general seems to unconsciously believe that they now have some kind of duty to </span><span><em><span>be</span></em></span><span> political, they must be in a certain camp, they must have certain opinions on various matters, and most of all, they must </span><span><em><span>care</span></em></span><span> in a specifically political way. I’m here to say that this way of thinking and being is complete bullshit, and it slowly leads one to misery and submission. There are a lot of factors as to why someone might feel compelled to constantly </span><span><em><span>be</span></em></span><span> political, largely emanating from one’s perpetual attachment to media. The two most heinous forms of media are – of course – social and mainstream. Primarily because, once you actually begin to think about what these terms actually mean, like most things in modernity, they no longer make any sense whatsoever. Let’s begin with ‘social media’.</span></p>
<p><span>We all apparently ‘know’ what social media is, which is another way of saying we understand it. I’ll admit, I don’t really understand social media, and I never have. The basic reasons as to why it’s so popular are of course clear, on average humans quite like attention, they quite like having a say and they quite like boasting about their lives. However, I would ask this? If it wasn’t </span><span><em><span>for</span></em></span><span> social media, and its invasive societally pressuring structures, would you actually </span><span><em><span>want</span></em></span><span> to express certain opinions? Would you even have them? Would you have even thought about them? Maybe you would, maybe you wouldn’t, be honest with yourself. If no one was looking, and you had no proof anyone </span><span><em><span>had</span></em></span><span> looked, would you expend energy on the various political and social tasks you do? Ok, so this then begs the question, why the hell </span><span><em><span>do</span></em></span><span> we want to express these opinions? Well, for that you need a mainstream current which tells you the correct, conventional and confirmative way to </span><span><em><span>be.</span></em></span><span> Enter the mainstream media. Such an idea of a ‘mainstream’ is already idiotic. There can’t be such a thing because we all live in different areas of the world, within different cultures, within different families, with different values, within different contexts, and so, the job the mainstream media then is to subsume all of these alternative ways of being and differing value systems into one relatively homogenous lump, which is then there’s to mold as they wish. I’d insert here Ted Kaczynski’s ‘critique’ of ‘multiculturalism’, though it’s less a critique and more of a deconstruction. Kaczynski’s point is that there isn’t really any such thing as ‘multiculturalism’ as it’s sold to us. The overt idea is that multiple diverse cultures live amongst one another, learn from each other and share their cultures for the betterment of all. Kaczynski makes it clear that this is </span><span><em><span>not</span></em></span><span> what happens within contemporary multiculturalism, all that really happens is that every culture is subsumed into the exact same culture of middle-class consumerist aspiration, and perhaps allowed to retain any cultural aesthetic which might be deemed profitable by their new culture of consumerist aspiration. The exact same thing happens with mainstream media. One begins with a variety of views, opinions, values, outlooks, perspectives and contexts which have been grown organically, from their local surroundings and upbringing, these are then pushed through the conformity thresher of mainstream media, cherry-picked for their applicability for submission, and what’s left are deemed dangerous, archaic, bad, fascist, radical, silly, absurd, weird, not-normal, odd or perhaps just too common-sensical for them to remain.</span></p>
<p><span>Now, the </span><span><em><span>exact</span></em></span><span> same process happens with the idea of a ‘political-human’ with a few minor alterations. Much like homo-criminalis, or homo-economicus, once the suffix is assumed </span><span><em><span>a priori</span></em></span><span> as a </span><span><em><span>way</span></em></span><span> of being – man </span><span><em><span>can</span></em></span><span> be a criminal, or man </span><span><em><span>can</span></em></span><span> be economical. There’s no longer such a thing as a man detached entirely from criminality or the economy, there is only a man who is </span><span><em><span>not</span></em></span><span> a criminal, or a man who acts within the economy in a </span><span><em><span>different</span></em></span><span> way than what is preferred. The exact same thing happens with political man. Once a political-outlook, a political-perspective or a political-reality is assumed as the given reality, everything is then filtered through politics in some manner. Then there is no longer such a thing as a entirely unpolitical man, only a man who is deemed ignorant of politics, someone who is seen as turning a blind eye or as simply too lazy to investigate that which they </span><span><em><span>should</span></em></span><span> be. The language here is the problem. Foucault makes this point clear with homo-criminalis and homo-economicus, once the ontology is taken as a given, no one is </span><span><em><span>not</span></em></span><span> of it, but simply seen as not part of a certain section of it. Men are not men, they are either criminals or </span><span><em><span>not-</span></em></span><span>criminals, we are not ourselves we are either economizing or </span><span><em><span>not-</span></em></span><span>economizing, either way, we’re still tethered to a way of being we had no say in.</span></p>
<p><span>Well I’m here to say that this is complete and utter crap. If you want to go get involved in politics, then be my guest, but do NOT assume that just because I don’t care about a certain topic, opinion or perspective that I am immediately the antagonist of that position. There is a difference between a hostile apathy, in which one truly doesn’t care about the plight of others and a detachment within one simply is not involved. Of course, any </span><span><em><span>involved</span></em></span><span> are going to disagree. ‘It’s your duty!’ they will cry. ‘Do you not care about the world!’ they will shriek. ‘How can you just do nothing?’ they will plead. Actually, I am doing something, I’m not expending my energy on a status game which largely exists to inflate various egos and create jobs. Lest we forget that politicians are </span><span><em><span>workers</span></em></span><span>, to be a politician is a </span><span><em><span>job</span></em></span><span>, and by the looks of it, quite a cushy one at that. </span></p>
<p><span>Being </span><span><em><span>detached</span></em></span><span> from politics isn’t </span><span><em><span>not caring</span></em></span><span> about those things you left behind, in fact, it’s arguably the opposite. As soon as a charitable organization, a communal effort or a group event becomes politicized, I am instantly skeptical of its agenda, why? Well, because since when did helping others, loving thy neighbor or creating something helpful </span><span><em><span>have</span></em></span><span> to be seen through a political lens. Call me a soppy-sod, but buying a homeless person some food, donating to a local charity or helping out in a local event isn’t – and doesn’t have to be – a specifically political move or motivation, and if it is, you’re doing so to cater to your own narcissism. What </span><span><em><span>are</span></em></span><span> these acts then? Well, they are what they are. You help someone because they need help, you do something because it needs doing, you create because something needs creating; once sincere acts are filtered through the malicious gauze of politics they are usually lost entirely, abused into a self-congratulatory mutation. </span></p>
<p><span>Ok, maybe you’re with me, but you’re starting to think…’Ok, so what do I…</span><span><em><span>do?</span></em></span><span>‘ Isn’t that the point? Up until now, for many people, each and every act they undertook was done primarily from a political position as opposed to the multitude of other (healthier) perspectives that exist. What do you do? Do what you’d like and what you understand to be right. </span></p>
<p><span>“Ah yes Meta, but if we ‘do nothing’ as you propose, wont we be simply bolstering support for whichever party is in the running to win?” You’re still thinking politically, why does it </span><span><em><span>actually</span></em></span><span> matter to you? If I support X I’ve entered into a system which is so unfathomably corrupt, confused and rife with personality that I will never </span><span><em><span>truly</span></em></span><span> know what it is my vote is doing. It is NOT an apathy, an ignorance or a superiority. It is a detachment. It is one unclipping themselves from a perspective they never asked for in the first place. The years upon years spent drooling …</span></p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.meta-nomad.net/its-ok-to-not-care-about-politics/">https://www.meta-nomad.net/its-ok-to-not-care-about-politics/</a></em></p>]]>
            </description>
            <link>https://www.meta-nomad.net/its-ok-to-not-care-about-politics/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027906</guid>
            <pubDate>Sun, 08 Nov 2020 18:51:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building a command line tool for literate programming]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027878">thread link</a>) | @kaunta
<br/>
November 8, 2020 | https://johnlekberg.com/blog/2020-11-08-cli-litprog.html | <a href="https://web.archive.org/web/*/https://johnlekberg.com/blog/2020-11-08-cli-litprog.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a href="https://johnlekberg.com/blog.html">Return to Blog
</a></p><p>By John Lekberg on November 08, 2020.
</p><hr>
<p>This week's Python blog post is about building a command line tool for <a href="https://en.wikipedia.org/wiki/Literate_programming">literate programming</a>.
You will learn:</p>
<ul>
<li>How to parse <a href="https://en.wikipedia.org/wiki/XML">XML</a> using Python's <a href="https://docs.python.org/3/library/xml.etree.elementtree.html">xml.etree.ElementTree</a> module.</li>
<li>How to traverse <a href="https://en.wikipedia.org/wiki/XML">XML</a> by using <a href="https://en.wikipedia.org/wiki/XPath">XPath</a> queries.</li>
<li>How to use <a href="https://en.wikipedia.org/wiki/Regular_expression">regular expressions</a> to parse macro definitions.</li>
<li>How to use string methods to normalize text.</li>
</ul>
<p><a href="https://en.wikipedia.org/wiki/Literate_programming">Literate programming</a> is a programming paradigm created by <a href="https://en.wikipedia.org/wiki/Donald_Knuth">Donald Knuth</a>.
Dr. Knuth, Professor Emeritus at Stanford University, is better known as the
creator of <a href="https://en.wikipedia.org/wiki/TeX">TeX</a> (see also <a href="https://en.wikipedia.org/wiki/LaTeX">LaTeX</a>) and the book series <a href="https://en.wikipedia.org/wiki/The_Art_of_Computer_Programming">"The Art of
Computer Programming"</a>.</p>
<p>The goal of literate programming is to allow programs to be displayed in a
narrative order -- as opposed to being limited by the constraints of a compiler
or interpreter.
Literate programming tools allow writers to use <a href="https://en.wikipedia.org/wiki/Macro_(computer_science)">macros</a> to abstract
and reorder source code to best suit the narrative.</p>
<p>There are two things to do with a literate program:</p>
<ul>
<li>You can <strong>tangle</strong> the literate program into source code -- which can then be
compiled and executed.</li>
<li>Or, you can <strong>weave</strong> the literate program into documentation -- which can be
read.</li>
</ul>

<blockquote>
<p><strong>litprog</strong></p>
</blockquote>
<pre><code>#!/usr/bin/env python3

import pathlib
import re
import sys
import xml.etree.ElementTree as ET


def MAIN():
    import argparse

    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(
        dest="mode",
        help="Weave or tangle source XML. (Required.)",
    )

    parser_weave = subparsers.add_parser(
        "weave",
        description="Weave source XML into HTML output.",
    )
    parser_weave.add_argument(
        "file", help="CommonMark XML file"
    )

    parser_tangle = subparsers.add_parser(
        "tangle",
        description="Tangle source XML into code output.",
    )
    parser_tangle.add_argument(
        "file", help="CommonMark XML file"
    )
    parser_tangle.add_argument(
        "target", help="Root macro name"
    )

    args = parser.parse_args()
    if args.mode is None:
        parser.print_help()
        sys.exit(1)
    elif args.mode == "weave":
        xml_input = pathlib.Path(args.file).read_text()
        html_output = litprog_weave(xml_input)
        print(html_output)
    elif args.mode == "tangle":
        xml_input = pathlib.Path(args.file).read_text()
        code_output = litprog_tangle(
            xml_input, root_target=args.target
        )
        print(code_output)
    else:
        raise RuntimeError(f"Unrecognized mode {mode!r}.")


macro_definition_re = re.compile(r"""(?x)
    &lt;&lt;
    (?P&lt;name&gt;[^\n&gt;]+)
    &gt;&gt;=
""")

macro_block_re = re.compile(r"""(?mx)
    ^
    (?P&lt;indent&gt;[ \t]*)
    &lt;&lt;
    (?P&lt;name&gt;[^\n&gt;]+)
    &gt;&gt;
    (?:[ \t]*)
    $
""")

macro_inline_re = re.compile(r"""(?x)
    &lt;&lt;
    (?P&lt;name&gt;[^\n&gt;]+)
    &gt;&gt;
""")


def normal_spacing(text):
    """Normalize the white space in a string."""
    return " ".join(text.split())


def litprog_weave(source_xml):
    """Weave CommonMark XML into HTML.

    source_xml -- str.
    """
    document = ET.fromstring(source_xml)
    namespace = "http://commonmark.org/xml/1.0"

    def elem_like(ppath):
        """Iterate over elements that match an XPath
        query like ".//{namespace}ppath".
        """
        return document.iterfind(
            ".//{%s}%s" % (namespace, ppath)
        )

    # NOTE: I only included the tag changes that I needed.
    # For the full Document Type Definition (DTD), see:
    # &gt; https://github.com/commonmark/commonmark-spec/blob/master/CommonMark.dtd

    # Easy tag changes.

    tag_changes = [
        ("paragraph", "p"),
        ("thematic_break", "hr"),
        ("text", "span"),
        ("code", "code"),
    ]
    for old_tag, new_tag in tag_changes:
        for elem in elem_like(old_tag):
            elem.tag = new_tag
            elem.attrib.clear()

    # Headings.

    for elem in elem_like("heading"):
        level = elem.attrib["level"]
        elem.tag = f"h{level}"
        elem.attrib.clear()

    # Code blocks.

    for elem in elem_like("code_block"):
        info = elem.attrib.get("info", "")
        code = elem.text

        elem.tag = "figure"
        elem.clear()

        match_macro = macro_definition_re.search(info)
        if match_macro:
            caption = ET.Element("figcaption")
            elem.append(caption)
            caption.text = f"Â« {normal_spacing(match_macro['name'])} Â»:"
            code = macro_inline_re.sub(
                lambda m: f"Â«{normal_spacing(m['name'])}Â»",
                code,
            )

        tag_pre = ET.Element("pre")
        elem.append(tag_pre)
        tag_code = ET.Element("code")
        tag_pre.append(tag_code)
        tag_code.text = code

    document.tag = "div"

    return ET.tostring(document).decode()


def litprog_tangle(source_xml, *, root_target):
    """Tangle CommonMark XML into source code.

    source_xml -- str.
    target -- str. Name of macro to tangle. e.g. "MAIN".
    """
    document = ET.fromstring(source_xml)
    namespace = "http://commonmark.org/xml/1.0"

    def elem_like(ppath):
        """Iterate over elements that match an XPath
        query like ".//{namespace}ppath".
        """
        return document.iterfind(
            ".//{%s}%s" % (namespace, ppath)
        )

    body = {}

    for elem in elem_like("code_block"):
        info = elem.attrib.get("info", "")

        match_macro = macro_definition_re.search(info)
        if match_macro:
            target = normal_spacing(
                match_macro["name"]
            ).casefold()
            body[target] = elem.text

    def expand(target):
        """Recursively expand a macro.

        target -- str. Macro name.
        """
        original_target = target
        target = normal_spacing(original_target).casefold()

        text = body[target]

        def expand_block(match):
            text = expand(match["name"]).rstrip()
            text = re.sub(r"(?m)^", match["indent"], text)
            return text

        def expand_inline(match):
            text = expand(match["name"]).strip()
            assert len(text.splitlines()) &lt;= 1
            return text

        text = macro_block_re.sub(expand_block, text)
        text = macro_inline_re.sub(expand_inline, text)

        return text

    return expand(root_target)


if __name__ == "__main__":
    MAIN()
</code></pre>
<blockquote>
<pre><code>$ litprog -h
</code></pre>
</blockquote>
<pre><code>usage: litprog [-h] {weave,tangle} ...

positional arguments:
  {weave,tangle}  Weave or tangle source XML. (Required.)

optional arguments:
  -h, --help      show this help message and exit
</code></pre>
<blockquote>
<pre><code>$ litprog weave -h
</code></pre>
</blockquote>
<pre><code>usage: litprog weave [-h] file

Weave source XML into HTML output.

positional arguments:
  file        CommonMark XML file

optional arguments:
  -h, --help  show this help message and exit
</code></pre>
<blockquote>
<pre><code>$ litprog tangle -h
</code></pre>
</blockquote>
<pre><code>usage: litprog tangle [-h] file target

Tangle source XML into code output.

positional arguments:
  file        CommonMark XML file
  target      Root macro name

optional arguments:
  -h, --help  show this help message and exit
</code></pre>

<p>Here is a sample literate program for <a href="https://en.wikipedia.org/wiki/Topological_sorting#Kahn's_algorithm">Khan's Algorithm</a>:</p>
<blockquote>
<p><strong>sample.markdown</strong></p>
</blockquote>
<pre><code># Topological Sort with Khan's Algorithm

``` &lt;&lt;MAIN&gt;&gt;=
&lt;&lt;imports&gt;&gt;

def khans_algorithm(*, V, E):
    &lt;&lt; init graph &gt;&gt;
    &lt;&lt; init topological order &gt;&gt;
    &lt;&lt; init source nodes &gt;&gt;
    
    while &lt;&lt; source nodes exist &gt;&gt;:
        &lt;&lt; take source node &gt;&gt;
        &lt;&lt; add node to topological order &gt;&gt;
        &lt;&lt; add neighboring source nodes &gt;&gt;
    
    if &lt;&lt; edges remain &gt;&gt;:
        raise &lt;&lt; cycle error &gt;&gt;
    else:
        return &lt;&lt; topological order &gt;&gt;
```

I initialize the graph indices from vertices `V` and edges `E`:

``` &lt;&lt; init graph &gt;&gt;=
E_idx0 = defaultdict(set)
E_idx1 = defaultdict(set)

def index(n, m):
    E_idx0[n].add(m)
    E_idx1[m].add(n)

def deindex(n, m):
    E_idx0[n].remove(m)
    E_idx1[m].remove(n)

def indegree(n):
    return len(E_idx1[n])

def outdegree(n):
    return len(E_idx0[n])

def is_source(n):
    return indegree(n) == 0

for n, m in E:
    index(n, m)
```

I use the `indegree` and `outdegree` helper functions to determine
whether any edges remain:

``` &lt;&lt; edges remain &gt;&gt;=
any((indegree(n) &gt; 0) or (outdegree(n) &gt; 0) for n in V)
```

I represent the topological order as a list:

``` &lt;&lt; topological order &gt;&gt;=
L
```

``` &lt;&lt; init topological order &gt;&gt;=
&lt;&lt; topological order &gt;&gt; = []
```

``` &lt;&lt; add node to topological order &gt;&gt;=
&lt;&lt; topological order &gt;&gt;.append(n)
```

I represent the source nodes as a set:

``` &lt;&lt; source nodes &gt;&gt;=
S
```

``` &lt;&lt; init source nodes &gt;&gt;=
&lt;&lt; source nodes &gt;&gt; = set(filter(is_source, V))
```

``` &lt;&lt; take source node &gt;&gt;=
n = &lt;&lt; source nodes &gt;&gt;.pop()
```

``` &lt;&lt; source nodes exist &gt;&gt;=
len(&lt;&lt; source nodes &gt;&gt;) &gt; 0
```

Given a node, I use the graph indices to find the
neighboring source nodes:

``` &lt;&lt; neighbors &gt;&gt;=
tuple(E_idx0[n])
```

``` &lt;&lt; add neighboring source nodes &gt;&gt;=
for m in &lt;&lt; neighbors &gt;&gt;:
    deindex(n, m)
    if is_source(m):
        &lt;&lt; source nodes &gt;&gt;.add(m)
```

If the graph contains a cycle, I will raise an exception:

``` &lt;&lt; cycle error &gt;&gt;=
RuntimeError("Graph contains a cycle.")
```

* * *

Here are the required imports:

``` &lt;&lt; imports &gt;&gt;=
from collections import defaultdict
```
</code></pre>
<p>To write literate programs, I use <a href="https://commonmark.org/">CommonMark</a>, a style of <a href="https://en.wikipedia.org/wiki/Markdown">markdown</a>.
I turn this <a href="https://en.wikipedia.org/wiki/Markdown">markdown</a> file into <a href="https://en.wikipedia.org/wiki/XML">XML</a> using the <code>cmark</code> command line tool:</p>
<pre><code>$ cmark --to xml sample.markdown &gt; sample.xml
</code></pre>
<p>To support macro definitions, I use the "info strings" on fenced code blocks. E.g.</p>
<blockquote>
<pre><code>$ echo '
``` hello world
a 
b
```
' | cmark --to xml
</code></pre>
</blockquote>
<pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!DOCTYPE document SYSTEM "CommonMark.dtd"&gt;
&lt;document xmlns="http://commonmark.org/xml/1.0"&gt;
  &lt;code_block info="hello world" xml:space="preserve"&gt;a
b
&lt;/code_block&gt;
&lt;/document&gt;
</code></pre>

<blockquote>
<pre><code>$ litprog weave sample.xml
</code></pre>
</blockquote>
<pre><code>&lt;div xmlns:ns0="http://commonmark.org/xml/1.0"&gt;
  &lt;h1&gt;
    &lt;span&gt;Topological Sort with Khan's Algorithm&lt;/span&gt;
  &lt;/h1&gt;
  &lt;figure&gt;&lt;figcaption&gt;&amp;#171; MAIN &amp;#187;:&lt;/figcaption&gt;&lt;pre&gt;&lt;code&gt;&amp;#171;imports&amp;#187;

def khans_algorithm(*, V, E):
    &amp;#171;init graph&amp;#187;
    &amp;#171;init topological order&amp;…</code></pre></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://johnlekberg.com/blog/2020-11-08-cli-litprog.html">https://johnlekberg.com/blog/2020-11-08-cli-litprog.html</a></em></p>]]>
            </description>
            <link>https://johnlekberg.com/blog/2020-11-08-cli-litprog.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027878</guid>
            <pubDate>Sun, 08 Nov 2020 18:47:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Remote Work: Tips on how to successfully work remotely in 2020]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027569">thread link</a>) | @martin_crd
<br/>
November 8, 2020 | https://remoteworkers.net/blog/7-tips-how-successfully-work-remotely-2020 | <a href="https://web.archive.org/web/*/https://remoteworkers.net/blog/7-tips-how-successfully-work-remotely-2020">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>It’s no secret that this year has been a testing year on all fronts of our waking lives, leading us to ‘get out of our comfort zones’ and learn to adapt in vastly changing times. Subsequently, most employees have had to work remotely to some capacity (if not fully remotely), and we’ve all had to learn how to improve our work-life balance while finding new ways to still be as efficient as before in our professional lives.</p><p>Needless to say, today you do not need to be tech-savvy to adjust to the digital world. The key to surviving and succeeding in this world is to make sure you’re prepared with the necessary online tools, mindset, and time management skills.</p><p>Below is a list of tips to help you thrive in a digital workforce:</p><h2>1. Make sure you have a strong internet connection</h2><p>This may seem like a no-brainer, but possibly the first thing you need to make sure you have when working remotely is a strong internet connection. This will help ensure you’re always accessible to your colleagues, therefore able to carry out your daily tasks and duties without interruption. Depending on your household and the number of devices you have connected to your WIFI, you may need to upgrade your internet package so you have a fast and reliable connection.</p><p>Just as in the traditional work environment, it’s important to collaborate with your co-workers and management, and poor internet connection may hinder that. Luckily, most service providers provide affordable internet bundle packages that will cater to your needs without breaking your bank balance.</p><h2>2. Invest in good office furniture and computer equipment</h2><p>Following some basic, yet essential ergonomics tips can improve your efficiency, reduce fatigue and strain to your back and neck, and facilitate proper posture. That’s why it’s important to invest in comfortable and adjustable furniture just as much as it is to have reliable computer equipment and updated software to complete your home office setup.</p><p>When working an eight-hour day or more, maintaining proper posture while sitting is crucial, not only for immediate benefits but for long-term damage you may cause to your back if you do not adopt the correct seating posture with adequate lower back support. Adjustable chairs and tables are a must-have to help ease pressure and body discomfort while working.</p><p>What often goes overlooked is the importance of taking care of your eyes. Protect your eyes from digital eye strain with an anti-blue light screen protector for your laptop or display monitor and you’ll disrupt the chances of sleep deprivation, fatigue, and risk of macular degeneration.</p><h2>3. Have a dedicated workspace</h2><p>Apart from helping you curb unnecessary distractions from the TV, social media, or dare I say: your kids, having a dedicated workspace is necessary to help improve productivity and creativity. Usually, when you go to the office it’s easier to get into ‘work-mode’ since you’re in an environment that induces productivity, but when working from home, you have to put in a little more effort to stay focused and motivated. Avoid working from your bed, the couch, or anywhere else that may lead to inattentiveness and interruptions to the work at hand.</p><p>Working remotely is a benefit that requires both accountability and responsibility as an employee, so creating an environment that promotes efficiency<strong></strong>goes a long way. Even if you don’t have the luxury of having an actual office at home you can still dedicate a certain ‘work-zone’ (preferably close to a window so you can enjoy the benefits of fresh air and natural light) in a quiet part of your home.</p><h2>4. Take breaks</h2><p>Don’t be hesitant to take breaks during the day. Taking regular short breaks will help keep your productivity at a high as well as help you feel constantly recharged and engaged. These could include short breaks to meditate for 5-10 minutes, having coffee, stretching, or simply just walking around to increase your blood circulation. Incorporate the 20-20-20 rule into your schedule: Take a 20-second break every 20 minutes by looking at things at least 20 feet away.</p><p>You can set reminders on your phone to help you remember to get up and walk. You can even get creative and strategic in your approach and place certain equipment (like the printer for instance) in another room to necessitate getting up and walking.</p><h2>5. Keep learning and improving your skills.</h2><p>Continuously upgrading your knowledge and skills will not only help keep you relevant in your respective field but will also help you map out your career progression. In an ever-changing job market and surplus in demand for remote work, you need to be marketable to progress. The more skilled and experienced you are, the wider your opportunities and growth prospects.</p><p>There are amazing free online learning materials you can take advantage of, or find reasonably affordable online courses. Even reading books on self-improvement, motivation, and success will help to keep you driven and inspired.</p><h2>6. Engage with colleagues and maintain positive professional relationships</h2><p>One of the downsides of working remotely is that you lose the benefits of building rapport with colleagues and your managers, which can result in life-long and beneficial professional connections.</p><p>You spend 8 hours of the day working, so it’s important to have healthy relations with the people you work with so that you feel level-headed and motivated to work. Something as simple as a good morning message to using professional and friendly jargon to communicate will help facilitate solid and positive relationships.</p><p>Be proactive and participate actively in meetings, after-work activities (provided you’re protecting yourself and others by social distancing and wearing a mask), and any other engagement activities offered. Be mindful to respect each other's diversities but also know when to not distract your co-workers as you must always maintain a professional demeanor. This will help to build trust, mutual respect, and open communication amongst each other, and bring about teamwork.</p><h2>7. Disconnect and know when to ‘call it a day’</h2><p>Time is valuable, and so are you! In order to be the best employee you can be, you need to learn to take care of yourself and your health, including your mental health. In a cut-throat work environment where we’re constantly being graded on our performance, it’s easy to feel overwhelmed and work to an excess.</p><p>Follow a strict work schedule but know when it’s time to ‘switch off’ and relax. If you find yourself unable to disconnect, practice managing your time better throughout the day so you’re able to finish your work on time. This will help reduce stress and improve the quality of your sleep.</p><p>Working overtime is necessary in certain cases, but downfall into the trap where you don’t give yourself (and your family and loved ones) the time to live and enjoy your life. Remember, the most productive employees are the ones who are well-rested and feel motivated to work.</p></div></div>]]>
            </description>
            <link>https://remoteworkers.net/blog/7-tips-how-successfully-work-remotely-2020</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027569</guid>
            <pubDate>Sun, 08 Nov 2020 18:13:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Spring boot vs. quarkus vs. micranaut vs. helidon vs. vertx]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027554">thread link</a>) | @ozkanpakdil
<br/>
November 8, 2020 | https://ozkanpakdil.github.io/microservicetests/2020-10-31-microservice-framework-test-11.html | <a href="https://web.archive.org/web/*/https://ozkanpakdil.github.io/microservicetests/2020-10-31-microservice-framework-test-11.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
      <p>Here is total package generation times for separate modules,</p>

<figure><pre><code data-lang="bash"><span>[</span>INFO] eclipse-microprofile-kumuluz-test 1.0-SNAPSHOT ..... SUCCESS <span>[</span> 33.217 s]
<span>[</span>INFO] helidon-quickstart-se 1.0-SNAPSHOT ................. SUCCESS <span>[</span> 30.383 s]
<span>[</span>INFO] micronaut-demo 0.1 ................................. SUCCESS <span>[</span> 31.055 s]
<span>[</span>INFO] quarkus-demo 1.0.0-SNAPSHOT ........................ SUCCESS <span>[</span> 38.910 s]
<span>[</span>INFO] springboot-demo 0.0.1-SNAPSHOT ..................... SUCCESS <span>[</span> 11.680 s]
<span>[</span>INFO] vertx-demo 1.0.0-SNAPSHOT .......................... SUCCESS <span>[</span>  4.269 s]</code></pre></figure>

<p>Size of created packages:</p>

<table>
  <thead>
    <tr>
      <th>Size in MB</th>
      <th>Name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>21M</td>
      <td>eclipse-microprofile-kumuluz-test/target/eclipse-microprofile-kumuluz-test.jar</td>
    </tr>
    <tr>
      <td>7.0M</td>
      <td>helidon-se-netty/target/helidon-quickstart-se.jar</td>
    </tr>
    <tr>
      <td>14M</td>
      <td>micronaut/target/micronaut-demo-0.1.jar</td>
    </tr>
    <tr>
      <td>14M</td>
      <td>quarkus/target/quarkus-demo-1.0.0-SNAPSHOT-runner.jar</td>
    </tr>
    <tr>
      <td>18M</td>
      <td>spring-boot/target/springboot-demo-0.0.1-SNAPSHOT.jar</td>
    </tr>
    <tr>
      <td>6.8M</td>
      <td>vertx/target/vertx-demo-1.0.0-SNAPSHOT-fat.jar</td>
    </tr>
  </tbody>
</table>

<p>:: Spring Boot :: (v2.3.5.RELEASE) Started DemoApplication in 2.503 seconds (JVM running for 3.16)</p>

<figure><pre><code data-lang="bash"><span>----</span> Global Information <span>--------------------------------------------------------</span>
<span>&gt;</span> request count                                       2000 <span>(</span><span>OK</span><span>=</span>2000   <span>KO</span><span>=</span>0     <span>)</span>
<span>&gt;</span> min response <span>time                                      </span>0 <span>(</span><span>OK</span><span>=</span>0      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> max response <span>time                                    </span>339 <span>(</span><span>OK</span><span>=</span>339    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean response <span>time                                    </span>39 <span>(</span><span>OK</span><span>=</span>39     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> std deviation                                         62 <span>(</span><span>OK</span><span>=</span>62     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>50th percentile                          2 <span>(</span><span>OK</span><span>=</span>2      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>75th percentile                         61 <span>(</span><span>OK</span><span>=</span>61     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>95th percentile                        178 <span>(</span><span>OK</span><span>=</span>178    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>99th percentile                        249 <span>(</span><span>OK</span><span>=</span>249    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean requests/sec                                    400 <span>(</span><span>OK</span><span>=</span>400    <span>KO</span><span>=</span>-     <span>)</span></code></pre></figure>

<p>powered by Quarkus 1.9.1.Final) started in 1.098s. Listening on: http://0.0.0.0:8080</p>

<figure><pre><code data-lang="bash"><span>----</span> Global Information <span>--------------------------------------------------------</span>
<span>&gt;</span> request count                                       2000 <span>(</span><span>OK</span><span>=</span>2000   <span>KO</span><span>=</span>0     <span>)</span>
<span>&gt;</span> min response <span>time                                      </span>0 <span>(</span><span>OK</span><span>=</span>0      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> max response <span>time                                    </span>430 <span>(</span><span>OK</span><span>=</span>430    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean response <span>time                                    </span>33 <span>(</span><span>OK</span><span>=</span>33     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> std deviation                                         61 <span>(</span><span>OK</span><span>=</span>61     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>50th percentile                          2 <span>(</span><span>OK</span><span>=</span>2      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>75th percentile                         40 <span>(</span><span>OK</span><span>=</span>40     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>95th percentile                        169 <span>(</span><span>OK</span><span>=</span>169    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>99th percentile                        260 <span>(</span><span>OK</span><span>=</span>260    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean requests/sec                                    400 <span>(</span><span>OK</span><span>=</span>400    <span>KO</span><span>=</span>-     <span>)</span></code></pre></figure>

<p>micronaut version:2.0.1 Startup completed in 1163ms. Server Running: http://localhost:8080</p>

<figure><pre><code data-lang="bash"><span>----</span> Global Information <span>--------------------------------------------------------</span>
<span>&gt;</span> request count                                       2000 <span>(</span><span>OK</span><span>=</span>2000   <span>KO</span><span>=</span>0     <span>)</span>
<span>&gt;</span> min response <span>time                                      </span>0 <span>(</span><span>OK</span><span>=</span>0      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> max response <span>time                                    </span>266 <span>(</span><span>OK</span><span>=</span>266    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean response <span>time                                    </span>34 <span>(</span><span>OK</span><span>=</span>34     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> std deviation                                         58 <span>(</span><span>OK</span><span>=</span>58     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>50th percentile                          2 <span>(</span><span>OK</span><span>=</span>2      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>75th percentile                         45 <span>(</span><span>OK</span><span>=</span>45     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>95th percentile                        179 <span>(</span><span>OK</span><span>=</span>179    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>99th percentile                        228 <span>(</span><span>OK</span><span>=</span>228    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean requests/sec                                    400 <span>(</span><span>OK</span><span>=</span>400    <span>KO</span><span>=</span>-     <span>)</span></code></pre></figure>

<p>vertx version:3.9.4</p>

<figure><pre><code data-lang="bash"><span>----</span> Global Information <span>--------------------------------------------------------</span>
<span>&gt;</span> request count                                       2000 <span>(</span><span>OK</span><span>=</span>2000   <span>KO</span><span>=</span>0     <span>)</span>
<span>&gt;</span> min response <span>time                                      </span>0 <span>(</span><span>OK</span><span>=</span>0      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> max response <span>time                                    </span>190 <span>(</span><span>OK</span><span>=</span>190    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean response <span>time                                    </span>19 <span>(</span><span>OK</span><span>=</span>19     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> std deviation                                         41 <span>(</span><span>OK</span><span>=</span>41     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>50th percentile                          1 <span>(</span><span>OK</span><span>=</span>1      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>75th percentile                          5 <span>(</span><span>OK</span><span>=</span>5      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>95th percentile                        125 <span>(</span><span>OK</span><span>=</span>125    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>99th percentile                        172 <span>(</span><span>OK</span><span>=</span>172    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean requests/sec                                    400 <span>(</span><span>OK</span><span>=</span>400    <span>KO</span><span>=</span>-     <span>)</span></code></pre></figure>

<p>kumuluz version:3.11.0 Server – Started @5032ms</p>

<figure><pre><code data-lang="bash"><span>----</span> Global Information <span>--------------------------------------------------------</span>
<span>&gt;</span> request count                                       2000 <span>(</span><span>OK</span><span>=</span>2000   <span>KO</span><span>=</span>0     <span>)</span>
<span>&gt;</span> min response <span>time                                      </span>0 <span>(</span><span>OK</span><span>=</span>0      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> max response <span>time                                    </span>343 <span>(</span><span>OK</span><span>=</span>343    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean response <span>time                                    </span>48 <span>(</span><span>OK</span><span>=</span>48     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> std deviation                                         77 <span>(</span><span>OK</span><span>=</span>77     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>50th percentile                          3 <span>(</span><span>OK</span><span>=</span>3      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>75th percentile                         75 <span>(</span><span>OK</span><span>=</span>75     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>95th percentile                        223 <span>(</span><span>OK</span><span>=</span>223    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>99th percentile                        292 <span>(</span><span>OK</span><span>=</span>292    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean requests/sec                                    400 <span>(</span><span>OK</span><span>=</span>400    <span>KO</span><span>=</span>-     <span>)</span></code></pre></figure>

<p>Helidon SE 2.1.0 features: [Config, Health, Metrics, WebServer]</p>

<figure><pre><code data-lang="bash"><span>----</span> Global Information <span>--------------------------------------------------------</span>
<span>&gt;</span> request count                                       2000 <span>(</span><span>OK</span><span>=</span>2000   <span>KO</span><span>=</span>0     <span>)</span>
<span>&gt;</span> min response <span>time                                      </span>0 <span>(</span><span>OK</span><span>=</span>0      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> max response <span>time                                    </span>472 <span>(</span><span>OK</span><span>=</span>472    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean response <span>time                                    </span>76 <span>(</span><span>OK</span><span>=</span>76     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> std deviation                                        108 <span>(</span><span>OK</span><span>=</span>108    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>50th percentile                          7 <span>(</span><span>OK</span><span>=</span>7      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>75th percentile                        123 <span>(</span><span>OK</span><span>=</span>123    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>95th percentile                        331 <span>(</span><span>OK</span><span>=</span>331    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>99th percentile                        419 <span>(</span><span>OK</span><span>=</span>419    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean requests/sec                                    400 <span>(</span><span>OK</span><span>=</span>400    <span>KO</span><span>=</span>-     <span>)</span></code></pre></figure>


      


    </article></div>]]>
            </description>
            <link>https://ozkanpakdil.github.io/microservicetests/2020-10-31-microservice-framework-test-11.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027554</guid>
            <pubDate>Sun, 08 Nov 2020 18:12:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Securing Crypto Keys: Ancient History Provides Clues]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027511">thread link</a>) | @npguy
<br/>
November 8, 2020 | http://doublespend.io/2020/10/29/ancient-history-provides-clues-for-securing-crypto-keys/ | <a href="https://web.archive.org/web/*/http://doublespend.io/2020/10/29/ancient-history-provides-clues-for-securing-crypto-keys/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

			
<article id="post-548">
	
				<p><a href="https://doublespend.io/wp-content/uploads/2020/10/ancient.jpg"><img width="800" height="445" src="https://doublespend.io/wp-content/uploads/2020/10/ancient-800x445.jpg" alt="" loading="lazy" srcset="https://doublespend.io/wp-content/uploads/2020/10/ancient.jpg 800w, https://doublespend.io/wp-content/uploads/2020/10/ancient-300x167.jpg 300w, https://doublespend.io/wp-content/uploads/2020/10/ancient-768x427.jpg 768w" sizes="(max-width: 800px) 100vw, 800px"></a>
								</p>
			
	<div>

		
		

		
		<div>
			
<p>Multiple well-funded teams in crypto are studying ancient human history to design new methods for securing the cryptographic keys for holding cryptocurrencies. </p>



<p>“Material that we have interpreted from ancient rocks prove that the methods we use today are already very close to the solution. Some of these techniques are hundreds of thousands of years old, so this is a significant undertaking” opined John Keymaker, Head of Research at GoldenKeys, a startup focused on innovative methods to secure crypto.</p>
		</div>

	</div>

	</article>

		</div></div>]]>
            </description>
            <link>http://doublespend.io/2020/10/29/ancient-history-provides-clues-for-securing-crypto-keys/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027511</guid>
            <pubDate>Sun, 08 Nov 2020 18:07:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[When “Progress” Is Backwards]]>
            </title>
            <description>
<![CDATA[
Score 29 | Comments 26 (<a href="https://news.ycombinator.com/item?id=25027462">thread link</a>) | @pmarin
<br/>
November 8, 2020 | https://sabotage-linux.github.io/blog/8 | <a href="https://web.archive.org/web/*/https://sabotage-linux.github.io/blog/8">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2>When "progress" is backwards</h2>

<p>20 Oct 2020 15:58 UTC</p>
<p>Lately I see many developments in the linux FOSS world that sell themselves as progress, but are actually hugely annoying and counter-productive.</p>

<p>Counter-productive to a point where they actually cause major regressions, costs, and as in the case of GTK+3 ruin user experience and the possibility that we'll ever enjoy "The year of the Linux desktop".</p>

<h2>Showcase 1: GTK+3</h2>

<p>GTK+2 used to be <em>the</em> GUI toolkit for Linux desktop applications.
It is highly customizable, reasonably lightweight and programmable from C, which means almost any scripting language can interface to it too.</p>

<p>Rather than improving the existing toolkit code in a backwards-compatible manner, <a href="https://www.freedesktop.org/">its developers</a> decided to introduce many breaking API changes which require a major porting effort to make an existing codebase compatible with the successor GTK+3, and keeping support for GTK+2 while supporting GTK+3 at the same time typically involves a lot of #ifdef clutter in the source base which not many developers are willing to maintain.</p>

<p>Additionally GTK+3 made away with a lot of user-customizable themeing options, effectively rendering useless most of the existing themes that took considerable developer effort for their creation.
Here's a <a href="https://ubuntu-mate.community/t/gtk3-regressions-from-a-gtk2-perspective/19511">list of issues</a> users are complaining about.</p>

<p>Due to the effort required to port a GTK+2 application to use GTK+3, many finished GUI application projects will never be ported due to lack of manpower, lost interest of the main developer or his untimely demise.
An example of such a program is the excellent audio editor <a href="http://www.metadecks.org/software/sweep/">sweep</a> which has seen its last release in 2008.
With Linux distros removing support for GTK+2, these apps are basically lost in the void of time.</p>

<p>The other option for distros is to keep both the (unmaintained) GTK+2 and GTK+3 in their repositories so GTK+2-only apps can still be used, however that causes the user of these apps to require basically the double amount of disk and RAM space as both toolkits need to live next to each other. Also this will only work as long as there are no breaking changes in the Glib library which both toolkits are built upon.</p>

<p>Even worse, due to the irritation the GTK+3 move caused to developers, many switched to QT4 or QT5, which requires use of C++, so a typical linux distro now has a mix of GTK+2, GTK+3, GTK+4, QT4 and QT5 applications, where each toolkit consumes considerable resources.</p>

<p>Microsoft (TM) knows better and sees backwards compatibility as the holy grail and underlying root cause of its success and market position. Any 25 year old Win32 GUI application from the Win95 era still works without issues on the latest Windows (TM) release. They even still support 16bit MS-DOS apps using some built-in emulator.</p>

<p>From MS' perspective, the freedesktop.org decision makers played into their hands when they decided to make GTK+3 a completely different beast.
Of course, we are <a href="https://en.wikipedia.org/wiki/Hanlon%27s_razor">taught to never believe in malice but in stupidity</a>, so it is unthinkable that there was actually a real conspiracy and monetary compensations behind this move.
Otherwise we would be conspiracy theorist nuts, right ?</p>

<h2>Showcase 2: python3</h2>

<p>Python is a hugely successful programming/scripting language used by probably millions of programmers.</p>

<p>Whereas python2 development has been very stable for many years, python3 changes at the blink of an eye. It's not uncommon to find that after an update of python3 to the next release, existing code no longer works as expected.</p>

<p>Many developers such as myself prefer to use a stable development environment over one that is as volatile as python3.</p>

<p>With the decision to <a href="https://mail.python.org/archives/list/python-announce-list@python.org/thread/OFCIETIXLX34X7FVK5B5WPZH22HXV342/">EOL python2</a> thousands of py2-based applications will experience the same fate as GTK+2 applications without maintainer: they will be rendered obsolete and disappear from the distro repositories. This may happen quicker than one would expect, as python by default provides bindings to the system's OpenSSL library, which has a history of making backwards-incompatible changes. At the very least, once the web agrees on a new TLS standard, python2 will be rendered completely useless.</p>

<p>Porting python2 to python3 isn't usually as involving as GTK+2 to GTK+3, but due to the dynamic nature of python the syntax checker can't catch all code issues automatically so many issues will be experienced at runtime in cornercases, causing the ported application to throw a backtrace and stopping execution, which can have grave consequences.</p>

<p>Many companies have <a href="https://www.techrepublic.com/article/jpmorgans-athena-has-35-million-lines-of-python-code-and-wont-be-updated-to-python-3-in-time/">millions of line of code still in python2</a> and will have to produce quite some sweat and expenses to make it compatible to python3.</p>

<h2>Showcase 3: ip vs ifconfig</h2>

<p>Once one had learned his handful of ifconfig and route commands to configure a Linux' box network connections, one could comfortably manage this aspect across all distros. Not any longer, someone had the glorious idea to declare ifconfig and friends obsolete and provide a new, more "powerful" tool to do its job: <code>ip</code>.</p>

<p>The command for bringing up a network device is now <code>ip link set dev eth1 up</code> vs the older <code>ifconfig eth1 up</code>. Does this really look like progress?
Worst, the documentation of the tool is non-intuitive so one basically has to google for examples that show the translation from one command to the other.</p>

<p>The same critics apply to <code>iw</code> vs <code>iwconfig</code>.</p>

<h2>Showcase 4: ethernet adapter renaming by systemd/udev</h2>

<p>Latest systemd-based distros come up with network interface names such as <code>enx78e7d1ea46da</code> or <code>vethb817d6a</code>, instead of the traditional <code>eth0</code>.
The interface names assigned by default on Ubuntu 20 are so long a regular human can't even remember them, any configuration attempt requires one to copy/paste the name from <code>ip a</code> output.
Yet almost every distro goes along with this <a href="https://www.freedesktop.org/wiki/Software/systemd/PredictableNetworkInterfaceNames/">Poettering/freedesktop.org-dictated</a> nonsense.</p>

<h2>Showcase 5: CMake, meson, and $BUILDSYSTEMOFTHEDAY</h2>

<p>While the traditional buildsystem used on UNIX, <code>autoconf</code>, has its warts, it was designed in such a way that only the application developer required the full set of tools, whereas the consumer requires only a POSIX compatible shell environment and a <code>make</code> program.</p>

<p>More "modern" build systems like <code>cmake</code> and <code>meson</code> don't give a damn about the dependencies a user has to install, in fact according to <a href="https://kevstras.com/programming/2017/12/18/meson.html">this</a>, <code>meson</code> authors claimed it to be one of their goals to force users to have a bleeding edge version of python3 installed so it can be universally assumed as a given.</p>

<p><code>CMake</code> is written in C++, consists of 70+ MB of extracted sources and requires an impressive amount of time to build from source. Built with debug information, it takes up 434 MB of my harddisk space as of version 3.9.3.
It's primary raison-d'etre is its support for the Microsoft (TM) Visual Studio (R) (TM) solution files, so Windows (TM) people can compile stuff from source with a few clicks.</p>

<p>The two of them have in common that they threw over board the well-known user interface to configure and make and invented their own NIH solution, which requires the user to learn yet another way to build his applications.</p>

<p>Both of these build systems seem to have either acquired a cult following just like systemd, or someone is paying trolls to show up on github with pull requests to replace GNU autoconf with either of those, for example <a href="https://github.com/containers/crun/issues/495">1</a> <a href="https://github.com/karelzak/util-linux/pull/968">2</a> .
Interestingly also, GNOME, which is tightly connected to freedesktop.org, has made it one of its goals to <a href="https://wiki.gnome.org/Initiatives/GnomeGoals/MesonPorting">switch all components to meson</a>.
Their porting effort involves almost every key component in the Linux desktop stack, including cairo, pango, fontconfig, freetype, and dozens of others. What might be the agenda behind this effort?</p>

<h2>Conclusion</h2>

<p>We live in an era where in the FOSS world one constantly has to relearn things, switch to new, supposedly "better", but more bloated solutions, and is generally left with the impression that someone is pulling the rug from below one's feet.
Many of the key changes in this area have been rammed through by a small set of decision makers, often closely related to Red Hat/Gnome/freedesktop.org.
We're buying this "progress" at a high cost, and one can't avoid asking oneself whether there's more to the story than meets the eye.
Never forget, Red Hat and Microsoft (TM) are <a href="https://www.redhat.com/en/partners/microsoft">partners</a> and might even have the same shareholders.</p>
</div></div>]]>
            </description>
            <link>https://sabotage-linux.github.io/blog/8</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027462</guid>
            <pubDate>Sun, 08 Nov 2020 18:02:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: React Frontload V2 – Simple full-stack data loading for React]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027368">thread link</a>) | @davnicwil
<br/>
November 8, 2020 | https://davnicwil.com/react-frontload | <a href="https://web.archive.org/web/*/https://davnicwil.com/react-frontload">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-reactroot="" data-reactid="1" data-react-checksum="1067123220"><div data-reactid="6"><div data-reactid="7"><p><img src="https://davnicwil.com/image/react-frontload-logo.png" width="100" height="100" data-reactid="8"></p></div><p data-reactid="10"><h2 data-reactid="11">Simple full-stack data loading for React</h2></p><p><a href="https://www.npmjs.com/package/react-frontload" data-reactid="13"><img src="https://img.shields.io/npm/v/react-frontload?style=social&amp;logo=npm" data-reactid="14"></a><a href="https://github.com/davnicwil/react-frontload" data-reactid="15"><img src="https://img.shields.io/github/stars/davnicwil/react-frontload?style=social" data-reactid="16"></a><a href="https://www.npmjs.com/package/react-frontload" data-reactid="17"><img src="https://img.shields.io/npm/dm/react-frontload?style=social" data-reactid="18"></a><a href="https://twitter.com/davnicwil" data-reactid="19"><img src="https://img.shields.io/twitter/url?label=made%20by%20%40davnicwil&amp;style=social&amp;url=https%3A%2F%2Fdavnicwil.com" data-reactid="20"></a></p></div><p data-reactid="21">React Frontload is a library to load and manage data inline in React components that works on both client and server.</p><div data-reactid="22"><ul data-reactid="23"><li data-reactid="24"><span data-reactid="25">Load data with a hook which works on client and server</span></li><li data-reactid="26"><span data-reactid="27">Data is managed in component state - no need for Redux / MobX</span></li><li data-reactid="28"><span data-reactid="29">Written in TypeScript, typing is easy as everything's inline</span></li><li data-reactid="30"><span data-reactid="31">Less than 3.5KB Gzipped, zero dependencies</span></li></ul></div><div data-reactid="32"><div data-reactid="33"><!-- react-text: 34 --><p>v2 has just shipped! </p><!-- /react-text --><p><a href="https://davnicwil.com/v2" data-reactid="35">See here</a></p><!-- react-text: 36 --><p> for the motivation for v2 and comparison with v1</p><!-- /react-text --></div></div><div data-reactid="37"><p>Install</p><p>npm install react-frontload</p></div><div id="problem" data-reactid="53"><h3 data-reactid="54">What problem does this solve?</h3><p><a href="#problem" data-reactid="55">#</a></p></div><p data-reactid="56">React provides no built-in way to do data loading - it's left for you to implement. Doing this is tricky in a React app that uses server side rendering (SSR) because client and server rendering work quite differently: Client render is async so data can be loaded inside components when they render, but server render is completely synchronous - the data must be loaded before render happens.</p><p data-reactid="57">Data loading is, of course, async. The client component-centric data loading pattern is nice, but it's incompatible with synchronous server render. React simply has no mechanism to wait for data to load when components render on SSR. There's a further problem too: React also provides no built-in way to hydrate data loaded during SSR into client state on first render. This is also up to you to implement.</p><p data-reactid="58">So, full stack data loading in React is a tricky problem. A couple of solutions have emerged:</p><ol data-reactid="59"><li data-reactid="60"><p data-reactid="61">Load data at the route level, instead of the component level, then pass data to all components under the route. Works on SSR since the route is known in the request, so data can be loaded before render begins. Can be implemented by piecing together router and state manager libraries.</p></li><li data-reactid="62"><p data-reactid="63">Use a framework that wraps React and abstracts the problem away, perhaps by providing a framework-specific async data loading function for components that works on SSR, and also takes care of hydrating state on the client.</p></li></ol><p data-reactid="64">React Frontload aims to provide a third way - the component-centric data loading pattern available full stack, but without having to buy into a whole framework just to get this feature. It's just a small library that solves this one problem, and can be used in any React stack.</p><p data-reactid="68">Here's an example of loading data into a component with React Frontload</p><p><span>const</span> <span>Component</span> <span>=</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> <span>{</span> data<span>,</span> frontloadMeta <span>}</span> <span>=</span> <span>useFrontload</span><span>(</span><span>'my-component'</span><span>,</span> <span>async</span> <span>(</span><span><span>{</span> api <span>}</span></span><span>)</span> <span>=&gt;</span> <span>(</span><span>{</span>
    stuff<span>:</span> <span>await</span> api<span>.</span><span>getStuff</span><span>(</span><span>)</span>
  <span>}</span><span>)</span><span>)</span>

  <span>if</span> <span>(</span>frontloadMeta<span>.</span>pending<span>)</span> <span>return</span> <span>&lt;</span>div<span>&gt;</span>loading<span>&lt;</span><span>/</span>div<span>&gt;</span>
  <span>if</span> <span>(</span>frontloadMeta<span>.</span>error<span>)</span>   <span>return</span> <span>&lt;</span>div<span>&gt;</span>error<span>&lt;</span><span>/</span>div<span>&gt;</span>

  <span>return</span> <span>&lt;</span>div<span>&gt;</span><span>{</span>data<span>.</span>stuff<span>}</span><span>&lt;</span><span>/</span>div<span>&gt;</span>
<span>}</span>
</p><p data-reactid="70"><!-- react-text: 71 -->Here we have a <!-- /react-text --><span data-reactid="72">Component</span><!-- react-text: 73 --> that needs to load <!-- /react-text --><span data-reactid="74">stuff</span><!-- react-text: 75 --> from an API, with the usual loading state whilst it loads and some sort of error state if loading fails for some reason.<!-- /react-text --></p><p data-reactid="76"><!-- react-text: 77 -->With React Frontload, we do this by passing an async data loading function to the <!-- /react-text --><span data-reactid="78">useFrontload</span><!-- react-text: 79 --> hook. The hook loads the return value of the function into<!-- /react-text --><!-- react-text: 80 --> <!-- /react-text --><span data-reactid="81">data</span><!-- react-text: 82 -->, and gives us<!-- /react-text --><!-- react-text: 83 --> <!-- /react-text --><span data-reactid="84">frontloadMetadata</span><!-- react-text: 85 --> out the box so we can see when it's still <!-- /react-text --><span data-reactid="86">pending</span><!-- react-text: 87 --> or if an <!-- /react-text --><span data-reactid="88">error</span><!-- react-text: 89 --> is thrown when running the function.<!-- /react-text --></p><p data-reactid="90"><!-- react-text: 91 -->That's it - we're done in those few lines of code. That's the power of doing data loading inline in a component. And the best part here is that this just works on the server. If we render<!-- /react-text --><!-- react-text: 92 --> <!-- /react-text --><span data-reactid="93">Component</span><!-- react-text: 94 --> in a route, any route,<!-- /react-text --><!-- react-text: 95 --> <!-- /react-text --><span data-reactid="96">stuff</span><!-- react-text: 97 --> will load.<!-- /react-text --></p><p data-reactid="98"><!-- react-text: 99 -->To emphasise the ease and lack of plumbing involved in making changes, let's have <!-- /react-text --><span data-reactid="100">Component</span><!-- react-text: 101 --> load some more stuff:<!-- /react-text --></p><p><span>const</span> <span>Component</span> <span>=</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> <span>{</span> data<span>,</span> frontloadMeta <span>}</span> <span>=</span> <span>useFrontload</span><span>(</span><span>'my-component'</span><span>,</span> <span>async</span> <span>(</span><span><span>{</span> api <span>}</span></span><span>)</span> <span>=&gt;</span> <span>(</span><span>{</span>
    stuff<span>:</span> <span>await</span> api<span>.</span><span>getStuff</span><span>(</span><span>)</span><span>,</span>
    moreStuff<span>:</span> <span>await</span> api<span>.</span><span>getMoreStuff</span><span>(</span><span>)</span> 
  <span>}</span><span>)</span><span>)</span>

  <span>if</span> <span>(</span>frontloadMeta<span>.</span>pending<span>)</span> <span>return</span> <span>&lt;</span>div<span>&gt;</span>loading<span>&lt;</span><span>/</span>div<span>&gt;</span>
  <span>if</span> <span>(</span>frontloadMeta<span>.</span>error<span>)</span>   <span>return</span> <span>&lt;</span>div<span>&gt;</span>error<span>&lt;</span><span>/</span>div<span>&gt;</span>

  <span>return</span> <span>&lt;</span>div<span>&gt;</span><span>{</span>data<span>.</span>stuff<span>}</span> and <span>{</span>data<span>.</span>moreStuff<span>}</span><span>&lt;</span><span>/</span>div<span>&gt;</span> 
<span>}</span>
</p><p data-reactid="103"><!-- react-text: 104 -->It's literally that simple - just add whatever you need to<!-- /react-text --><!-- react-text: 105 --> <!-- /react-text --><span data-reactid="106">useFrontload</span><!-- react-text: 107 --> and use it. Remember that this is all automatically typed. If the value returned by the<!-- /react-text --><!-- react-text: 108 --> <!-- /react-text --><span data-reactid="109">getMoreStuff</span><!-- react-text: 110 --> api call is a<!-- /react-text --><span data-reactid="111">string</span><!-- react-text: 112 -->,<!-- /react-text --><!-- react-text: 113 --> <!-- /react-text --><span data-reactid="114">data.moreStuff</span><!-- react-text: 115 --> has the string type, and you'll get errors if you try to use it as a number.<!-- /react-text --></p><p data-reactid="116"><!-- react-text: 117 -->You may have noticed that the above code loads data less efficiently than it could. <!-- /react-text --><span data-reactid="118">api.getStuff()</span><!-- react-text: 119 --> and<!-- /react-text --><!-- react-text: 120 --> <!-- /react-text --><span data-reactid="121">api.getMoreStuff()</span><!-- react-text: 122 --> are called in serial, when they could probably be called in parallel. Since it's just Javascript, though, we can change this:<!-- /react-text --></p><p><span>const</span> <span>{</span> data<span>,</span> frontloadMeta <span>}</span> <span>=</span> <span>useFrontload</span><span>(</span><span>'my-component'</span><span>,</span> <span>async</span> <span>(</span><span><span>{</span> api <span>}</span></span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> <span>[</span>stuff<span>,</span> moreStuff<span>]</span> <span>=</span> Promise<span>.</span><span>all</span><span>(</span><span>[</span>
    api<span>.</span><span>getStuff</span><span>(</span><span>)</span><span>,</span>
    api<span>.</span><span>getMoreStuff</span><span>(</span><span>)</span>
  <span>]</span><span>)</span>

  <span>return</span> <span>{</span> stuff<span>,</span> moreStuff <span>}</span>
<span>}</span><span>)</span>
</p><p data-reactid="124">In fact as data loaders get more complex, you can use any combination of serial and parallel that you need. It's just Javascript - you have the full power of the language without any abstractions or misdirection on top.</p><p data-reactid="125"><!-- react-text: 126 -->There is one more piece to this - what about updating data? Since React Frontload uses React component state to hold<!-- /react-text --><!-- react-text: 127 --> <!-- /react-text --><span data-reactid="128">data</span><!-- react-text: 129 -->, updating it is just a case of updating that state. React Frontload provides another function, called<!-- /react-text --><!-- react-text: 130 --> <!-- /react-text --><span data-reactid="131">setData</span><!-- react-text: 132 -->, for this:<!-- /react-text --></p><p><span>const</span> <span>Component</span> <span>=</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> <span>{</span> data<span>,</span> setData<span>,</span> frontloadMeta <span>}</span> <span>=</span> <span>useFrontload</span><span>(</span><span>'my-component'</span><span>,</span> <span>async</span> <span>(</span><span><span>{</span> api <span>}</span></span><span>)</span> <span>=&gt;</span> <span>(</span><span>{</span>
    stuff<span>:</span> <span>await</span> api<span>.</span><span>getStuff</span><span>(</span><span>)</span>
  <span>}</span><span>)</span><span>)</span>

  <span>if</span> <span>(</span>frontloadMeta<span>.</span>pending<span>)</span> <span>return</span> <span>&lt;</span>div<span>&gt;</span>loading<span>&lt;</span><span>/</span>div<span>&gt;</span>
  <span>if</span> <span>(</span>frontloadMeta<span>.</span>error<span>)</span>   <span>return</span> <span>&lt;</span>div<span>&gt;</span>error<span>&lt;</span><span>/</span>div<span>&gt;</span>

  <span>const</span> <span>updateStuff</span> <span>=</span> <span>async</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
    <span>try</span> <span>{</span>
      <span>const</span> updatedStuff <span>=</span> <span>await</span> <span>updateStuff</span><span>(</span><span>'new value'</span><span>)</span> 
      <span>setData</span><span>(</span><span>data</span> <span>=&gt;</span> <span>(</span><span>{</span> <span>...</span>data<span>,</span> stuff<span>:</span> updatedStuff <span>}</span><span>)</span><span>)</span> 
    <span>}</span> <span>catch</span> <span>{</span>
      
    <span>}</span>
  <span>}</span>

  <span>return</span> <span>(</span>
    <span>&lt;</span><span>&gt;</span>
      <span>&lt;</span>div<span>&gt;</span><span>{</span>data<span>.</span>stuff<span>}</span><span>&lt;</span><span>/</span>div<span>&gt;</span>
      <span>&lt;</span>button onClick<span>=</span><span>{</span>updateStuff<span>}</span><span>&gt;</span>update<span>&lt;</span><span>/</span>button<span>&gt;</span>
    <span>&lt;</span><span>&gt;</span>
  <span>)</span>
</p><p data-reactid="134"><!-- react-text: 135 -->Again, this is all just inline in the component with zero plumbing. If you're coming from Redux, you can think of<!-- /react-text --><!-- react-text: 136 --> <!-- /react-text --><span data-reactid="137">setData</span><!-- react-text: 138 --> a little bit like a mini reducer. It takes the existing value of<!-- /react-text --><!-- react-text: 139 --> <!-- /react-text --><span data-reactid="140">data</span><!-- react-text: 141 --> as an argument, and you merge updates into it to return an updated value of<!-- /react-text --><!-- react-text: 142 --> <!-- /react-text --><span data-reactid="143">data</span><!-- react-text: 144 -->.<!-- /react-text --></p><p data-reactid="145">And that's it - full stack data loading and management inline in your React components.</p><p data-reactid="149"><!-- react-text: 150 -->The <!-- /react-text --><span data-reactid="151">useEffect</span><!-- react-text: 152 --> hook seen in the example above is the core of React Frontload - the code you'll actually work with in your components - but there is also a small amount of one-time setup code to write to get it to work.<!-- /react-text --></p><p data-reactid="153"><!-- react-text: 154 -->Essentially this is setting up wrappers around your server render logic, and your React application on both server and client, to make the<!-- /react-text --><span data-reactid="155">useFrontload</span><!-- react-text: 156 --> hook work, and also enable hydration of state loaded on server render to the client.<!-- /react-text --></p><p data-reactid="157">App Provider</p><p data-reactid="158">Wrap your app in the React Frontload provider</p><p><span>import</span> <span>{</span> FrontloadProvider <span>}</span> <span>from</span> <span>'react-frontload'</span>

<span>const</span> <span>App</span> <span>=</span> <span>(</span><span><span>{</span> frontloadState <span>}</span></span><span>)</span> <span>=&gt;</span> <span>(</span>
  <span>&lt;</span>FrontloadProvider initialState<span>=</span><span>{</span>frontloadState<span>}</span><span>&gt;</span>
    <span>&lt;</span>Content<span>&gt;</span><span>...</span><span>&lt;</span><span>/</span>Content<span>&gt;</span>
  <span>&lt;</span><span>/</span>FrontloadProvider<span>&gt;</span>
<span>)</span>
</p><p data-reactid="160">Server render</p><p data-reactid="161"><!-- react-text: 162 -->On server render, you need to wrap your existing synchronous server render code with<!-- /react-text --><!-- react-text: 163 --> <!-- /react-text --><span data-reactid="164">reactFrontloadServerRender</span><!-- react-text: 165 -->.<!-- /react-text --></p><p data-reactid="166"><!-- react-text: 167 -->You can think of this as the polyfill that allows React Frontload to load data asynchronously on server render. It just uses regular React server rendering under the hood, and its output is exactly the same. Read more about this <!-- /react-text --><a href="#how-it-works" data-reactid="168">here</a><!-- react-text: 169 -->.<!-- /react-text --></p><p><span>import</span> <span>{</span> renderToString <span>}</span> <span>from</span> <span>'react-dom/server'</span>
<span>import</span> <span>{</span> createFrontloadState<span>,</span> frontloadServerRender <span>}</span> <span>from</span> <span>'react-frontload'</span>
<span>import</span> serverApi <span>from</span> <span>'./serverApi'</span>

app<span>.</span><span>get</span><span>(</span><span>'*'</span><span>,</span> <span>async</span> <span>(</span><span>req<span>,</span> res</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>...</span>

  
  
  <span>const</span> frontloadState <span>=</span> createFrontloadState<span>.</span><span>server</span><span>(</span><span>{</span>
    
    
    
    context<span>:</span> <span>{</span> api<span>:</span> serverApi <span>}</span>
  <span>}</span><span>)</span>

  <span>try</span> <span>{</span>
    
    <span>const</span> <span>{</span> rendered<span>,</span> data <span>}</span> <span>=</span> <span>await</span> <span>frontloadServerRender</span><span>(</span><span>{</span>
      frontloadState<span>,</span>
      <span>render</span><span>:</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>renderToString</span><span>(</span><span>&lt;</span>App frontloadState<span>=</span><span>{</span>frontloadState<span>}</span> <span>/</span><span>&gt;</span><span>)</span>
    <span>}</span><span>)</span>

    res<span>.</span><span>send</span><span>(</span><span><span>`</span><span>
      &lt;html&gt;
        ...
        &lt;!-- server rendered markup --&gt;
        </span><span><span>${</span>rendered<span>}</span></span><span>

        &lt;!-- loaded data (to be hydrated on client) --&gt;
        &lt;script&gt;window._frontloadData=</span><span><span>${</span><span>toSanitizedJSON</span><span>(</span>data<span>)</span><span>}</span></span><span>&lt;/script&gt;
        ...
      &lt;/html&gt;
    </span><span>`</span></span><span>)</span>
  <span>}</span> <span>catch</span> <span>(</span>err<span>)</span> <span>{</span>
    
  <span>}</span>
<span>}</span><span>)</span>
</p><p data-reactid="171"><!-- react-text: 172 -->The output of<!-- /react-text --><!-- react-text: 173 --> <!-- /react-text --><span data-reactid="174">reactFrontloadServerRender</span><!-- react-text: 175 --> contains a <!-- /react-text --><span data-reactid="176">rendered</span><!-- react-text: 177 --> string, which is just the server render output to inject into your HTML template as usual.<!-- /react-text --></p><p data-reactid="178"><!-- react-text: 179 -->It also contains a <!-- /react-text --><span data-reactid="180">data</span><!-- react-text: 181 --> object, which you should serialize into your HTML as sanitised JSON.<!-- /react-text --><!-- react-text: 182 --> <!-- /react-text --><span data-reactid="183">data</span><!-- react-text: 184 --> contains all the data loaded for the current view across all components, and the purpose of this is to hydrate this data into React Frontload on the client (using<!-- /react-text --><!-- react-text: 185 --> <!-- /react-text --><span data-reactid="186">initialState</span><!-- react-text: 187 -->) so that it does not have to be reloaded on first render. This pattern is essentially the same as the one you see with other state managers such as Redux.<!-- /react-text --></p><p data-reactid="188">Client render</p><p data-reactid="189">The last remaining step is client integration, which is rather simpler. Just initialise a React Frontload state object on the client using your serialized data from server render, and pass it to the provider.</p><p><span>import</span> clientApi <span>from</span> <span>'./clientApi'</span>

<span>const</span> frontloadState <span>=</span> createFrontloadState<span>.</span><span>client</span><span>(</span><span>{</span>
  
  
  context<span>:</span> <span>{</span> api<span>:</span> clientApi <span>}</span><span>,</span>

  
  serverRenderedData<span>:</span> window<span>.</span>_frontloadData
<span>}</span><span>)</span>
<span>...</span>
ReactDOM<span>.</span><span>render</span><span>(</span><span>&lt;</span>App frontloadState<span>=</span><span>{</span>frontloadState<span>}</span> <span>/</span><span>&gt;</span><span>,</span> <span>...</span><span>)</span>
</p><p data-reactid="194">For most usecases, you don't need to care about this.</p><p data-reactid="195">That said, all abstractions are leaky at some point, and it's always useful to understand how things work under the hood so that when they behave unexpectedly, you can figure out why.</p><p data-reactid="196">The mechanism used to polyfill async on server render is deliberately very simple. As shown in the code above, React Frontload wraps ordinary synchronous server render code with an async function.</p><p data-reactid="197"><!-- react-text: 198 -->It works by running that synchronous function, and collecting the promises encountered on each render of a<!-- /react-text --><!-- react-text: 199 --> <!-- /react-text --><span data-reactid="200">useFrontload</span><!-- react-text: 201 --> hook. After the render, the collected promises are then awaited, which loads the data in those functions the same as it would be on the client. Now, React Frontload runs the server render again - this time injecting the data loaded from the previous run into each component ahead of the render. In this new render round, if no new<!-- /react-text --><!-- react-text: 202 --> <!-- /react-text --><span data-reactid="203">useFrontload</span><!-- react-text: 204 --> hooks are encountered (i.e. if there are no nested components with a<!-- /react-text --><!-- react-text: 205 --> <!-- /react-text --><span data-reactid="206">useFrontload</span><!-- react-text: 207 --> hook), then the output of this render is returned as the final output. If nested<!-- /react-text --><!-- react-text: 208 --> <!-- /react-text --><span data-reactid="209">useFrontload</span><!-- react-text: 210 --> hooks<!-- /react-text --><!-- react-text: 211 --> <!-- /react-text --><span data-reactid="212">are</span><!-- react-text: 213 --> found, the process repeats.<!-- /react-text --></p><ul id="api-useFrontload" data-reactid="225"><li data-reactid="226"><span data-reactid="227">useFrontload</span><a href="#api-useFrontload" data-reactid="228">#</a></li></ul><p data-reactid="229"><span data-reactid="230">useFrontload</span><!-- react-text: 231 --> is the React Frontload hook.<!-- /react-text --></p><p><span>import</span> <span>{</span> useFrontload <span>}</span> <span>from</span> <span>'react-frontload'</span>

<span>useFrontload</span><span>(</span>
  …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://davnicwil.com/react-frontload">https://davnicwil.com/react-frontload</a></em></p>]]>
            </description>
            <link>https://davnicwil.com/react-frontload</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027368</guid>
            <pubDate>Sun, 08 Nov 2020 17:46:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Winning Without Competing]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027249">thread link</a>) | @jdcampolargo
<br/>
November 8, 2020 | https://www.juandavidcampolargo.com/blog/winning-competion | <a href="https://web.archive.org/web/*/https://www.juandavidcampolargo.com/blog/winning-competion">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-405d26435b59c98dcb8b"><div><p>It was Monday at 9:58 PM, and I finished a CAD assignment with the help of my TA. Nick is a great dude, not only because of how helpful and knowledgeable he is but because of his willingness to explain the fundamentals of CAD and engineering until one deeply understands it.&nbsp;</p><p>It’s become a routine when I finish the assignments, we talk about life, economics, psychology, and anything that comes our way. In our conversation, the topic of competition came up.&nbsp;</p><p>He tells me when he applied to colleges, he avoided the “famous” and “prestigious” places from the East Coast because of the overfocus on competition, and the lack of focus on what you’re genuinely curious about.&nbsp;</p><p>He says, “Students don’t do what they like or what they want to learn more about, rather they tend to do what they’re good at because the school, parents, friends, etc tell them to.”</p><p>As soon as he said that, sparks started happening in my head, and I was ready to share my response about the dangers of competition.&nbsp;</p><p>Competition is great [1], yet I avoid it as much as possible. Competing with others can lead you to an unhealthy path where you end up doing what you thought would make you a winner, and not what would make you enjoy what you do.&nbsp;</p><p>Nick tells me that often being the best and competing with others can often lead to putting other people down. In zero-sum games, if I win, that means you have to lose. There are plenty of cases where it's positive-sum, where no one wins at someone else’s expense.&nbsp;</p><p>I avoid competition [2] because if you want what other people want 1) it becomes harder to get what you want, and 2) even if you win or get what you wanted, you may realize that was not what you wanted.&nbsp;</p><p>It doesn’t have to be this way, as there are many alternatives to thrive without putting other people down and doing things you want to do by following your curiosity and interests.&nbsp;</p><p>How?</p><p><strong>You uniquely define what you do in a way that no one else is doing it.&nbsp;</strong></p><p>If you are passionate about entrepreneurship, you can’t call yourself an “entrepreneur.” You need to go deeper. For instance, if you are an entrepreneur, define the areas, the people who you work with, where you work, how you work, and why you work.&nbsp;</p><p>You might be an entrepreneur focused on the well-being of the environment and human life in the southern parts of the country who is trying to implement Algae-based biofuel on tractors. That’s specific and you can go even deeper like the states, the brand of the tractors, and the type of algae.&nbsp;</p><p>Or if you like writing, you can’t call yourself a “writer.” See within and start unburying. You could write about how the sophisticated Roman city of Pompeii affected how we think about building and designing cities in the 20th and 21st centuries.&nbsp;</p><p>I know you may feel uncomfortable being that specific because you may think: 1) You’re passionate about more topics, 2) You may lose opportunities, 3) You may not like being that specific.&nbsp;</p><p>Frankly, that’s how I feel too. However, I can define what I do in such a unique way so I can see where to go or what to do. I don’t have to follow it and can always change it. In my case, I didn’t even worry about it, and I followed what interested me and what seemed curious.&nbsp;</p><p>I’ve realized that following your genuine curiosity is the best way to avoid competition and win (whatever that means for you).&nbsp;</p><p>I followed my curiosity and I can look back at the essays. Although they may seem like different topics, they all have one purpose: making my readers more optimistic, ambitious, and curious.&nbsp;</p><p>Define what you do uniquely so you have no competition. That way, you’ll win without winning and without other people down.&nbsp;</p><p>And that was only ten minutes of one of my chats with Nick. Our conversations get more interesting every week. If you’d like to hear more about them, join hundreds of people in the <a href="http://juandavidcampolargo.com/newsletter" target="_blank"><strong><em>Weekly Memos</em></strong></a><strong><em>.</em></strong></p><p><strong>Notes</strong></p><p>[1] I’m not talking about “Market Competition.” I’m referring to competition at a personal and an individual level.</p><p>[2] I don’t just avoid competition for the sake of avoiding it. But if I find myself competing for no reason, it could be a sign that I’m not thinking for myself. </p></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1604669904171_5110"><p>If you’re into interesting ideas (like the one you just read), sign up for my <a href="https://juandavidcampolargo.com/newsletter" target="_blank">Weekly Memos</a>, and I’ll send you new essays right when they come out. Better than having to check the site!</p></div></div>]]>
            </description>
            <link>https://www.juandavidcampolargo.com/blog/winning-competion</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027249</guid>
            <pubDate>Sun, 08 Nov 2020 17:30:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Illustrated GPT-2 (Visualizing Transformer Language Models)]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027205">thread link</a>) | @bjourne
<br/>
November 8, 2020 | http://jalammar.github.io/illustrated-gpt2/ | <a href="https://web.archive.org/web/*/http://jalammar.github.io/illustrated-gpt2/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p><span>Discussions:
<a href="https://news.ycombinator.com/item?id=20677411">Hacker News (64 points, 3 comments)</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/cp8prq/p_the_illustrated_gpt2_visualizing_transformer/">Reddit r/MachineLearning (219 points, 18 comments)</a>
</span></p>

<p><span>Translations: <a href="https://habr.com/ru/post/490842/">Russian</a></span></p>

<p><img src="http://jalammar.github.io/images/gpt2/openAI-GPT-2-3.png">
  <br>
</p>

<p>This year, we saw a dazzling application of machine learning. <a href="https://openai.com/blog/better-language-models/">The OpenAI GPT-2</a> exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.</p>

<p>My goal here is to also supplement my earlier post, <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.</p>

<!--more-->

<div>

  <p><strong>Contents</strong></p>

  <ul>
    <li><strong><a href="#part-1-got-and-language-modeling">Part 1: GPT2 And Language Modeling</a></strong>
      <ul>
        <li>What is a Language Model</li>
        <li>Transformers for Language Modeling</li>
        <li>One Difference From BERT</li>
        <li>The Evolution of The Transformer Block</li>
        <li>Crash Course in Brain Surgery: Looking Inside GPT-2</li>
        <li>A Deeper Look Inside</li>
        <li>End of part #1: The GPT-2, Ladies and Gentlemen</li>
      </ul>
    </li>
    <li><strong><a href="#part-2-illustrated-self-attention">Part 2: The Illustrated Self-Attention</a></strong>
      <ul>
        <li>Self-Attention (without masking)</li>
        <li>1- Create Query, Key, and Value Vectors</li>
        <li>2- Score</li>
        <li>3- Sum</li>
        <li>The Illustrated Masked Self-Attention</li>
        <li>GPT-2 Masked Self-Attention</li>
        <li>Beyond Language modeling</li>
        <li>You’ve Made it!</li>
      </ul>
    </li>
    <li><strong><a href="#part-3-beyond-language-modeling">Part 3: Beyond Language Modeling</a></strong>
      <ul>
        <li>Machine Translation</li>
        <li>Summarization</li>
        <li>Transfer Learning</li>
        <li>Music Generation</li>
      </ul>
    </li>
  </ul>

</div>

<h2 id="part-1-gpt2-and-language-modeling-">Part #1: GPT2 And Language Modeling <a href="#part-1-got-and-language-modeling" name="part-1-got-and-language-modeling">#</a></h2>

<p>So what exactly is a language model?</p>

<h3 id="what-is-a-language-model">What is a Language Model</h3>
<p>In <a href="http://jalammar.github.io/illustrated-word2vec/">The Illustrated Word2vec</a>, we’ve looked at what a language model is – basically a machine learning model that is able to look at part of a sentence and predict the next word. The most famous language models are smartphone keyboards that suggest the next word based on what you’ve currently typed.</p>

<p><img src="http://jalammar.github.io/images/word2vec/swiftkey-keyboard.png">
  <br>
</p>

<p>In this sense, we can say that the GPT-2 is basically the next word prediction feature of a keyboard app, but one that is much larger and more sophisticated than what your phone has. The GPT-2 was trained on a massive 40GB dataset called WebText that the OpenAI researchers crawled from the internet as part of the research effort. To compare in terms of storage size, the keyboard app I use, SwiftKey, takes up 78MBs of space. The smallest variant of the trained GPT-2, takes up 500MBs of storage to store all of its parameters. The largest GPT-2 variant is 13 times the size so it could take up more than 6.5 GBs of storage space.</p>

<p><img src="http://jalammar.github.io/images/gpt2/gpt2-sizes.png">
  <br>
</p>

<p>One great way to experiment with GPT-2 is using the <a href="https://gpt2.apps.allenai.org/?text=Joel%20is">AllenAI GPT-2 Explorer</a>. It uses GPT-2 to display ten possible predictions for the next word (alongside their probability score). You can select a word then see the next list of predictions to continue writing the passage.</p>

<h3 id="transformers-for-language-modeling">Transformers for Language Modeling</h3>

<p>As we’ve seen in The <a href="http://jalammar.github.io/illustrated-transformer/">Illustrated Transformer</a>, the original transformer model is made up of an encoder and decoder – each is a stack of what we can call transformer blocks. That architecture was appropriate because the model tackled machine translation  – a problem where encoder-decoder architectures have been successful in the past.</p>

<p><img src="http://jalammar.github.io/images/xlnet/transformer-encoder-decoder.png">
  <br>
</p>

<p>A lot of the subsequent research work saw the architecture shed either the encoder or decoder, and use just one stack of transformer blocks – stacking them up as high as practically possible, feeding them massive amounts of training text, and throwing vast amounts of compute at them (hundreds of thousands of dollars to train some of these language models, likely millions in the case of <a href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/">AlphaStar</a>).</p>

<p><img src="http://jalammar.github.io/images/gpt2/gpt-2-transformer-xl-bert-3.png">
  <br>
</p>

<p>How high can we stack up these blocks? It turns out that’s one of the main distinguishing factors between the different GPT2 model sizes:</p>

<p><img src="http://jalammar.github.io/images/gpt2/gpt2-sizes-hyperparameters-3.png">
  <br>
</p>

<h3 id="one-difference-from-bert">One Difference From BERT</h3>
<blockquote>
<strong>First Law of Robotics</strong><br>
A robot may not injure a human being or, through inaction, allow a human being to come to harm.
</blockquote>

<p>The GPT-2 is built using transformer decoder blocks. BERT, on the other hand, uses transformer encoder blocks. We will examine the difference in a following section. But one key difference between the two is that GPT2, like traditional language models, outputs one token at a time. Let’s for example prompt a well-trained GPT-2 to recite the first law of robotics:</p>

<p><img src="http://jalammar.github.io/images/xlnet/gpt-2-output.gif">
  <br>
</p>

<p>The way these models actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step. This is an idea called “auto-regression”. This is one of the ideas that <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">made RNNs unreasonably effective</a>.</p>

<p><img src="http://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif">
  <br>
</p>

<p>The GPT2, and some later models like TransformerXL and XLNet are auto-regressive in nature. BERT is not. That is a trade off. In losing auto-regression, BERT gained the ability to incorporate the context on both sides of a word to gain better results. XLNet brings back autoregression while finding an alternative way to incorporate the context on both sides.</p>

<h3 id="the-evolution-of-the-transformer-block">The Evolution of the Transformer Block</h3>

<p>The <a href="https://arxiv.org/abs/1706.03762">initial transformer paper</a> introduced two types of transformer blocks:</p>

<h4 id="the-encoder-block">The Encoder Block</h4>

<p>First is the encoder block:</p>

<p><img src="http://jalammar.github.io/images/xlnet/transformer-encoder-block-2.png">
  <br>
  An encoder block from the original transformer paper can take inputs up until a certain max sequence length (e.g. 512 tokens). It's okay if an input sequence is shorter than this limit, we can just pad the rest of the sequence.
</p>

<h4 id="the-decoder-block">The Decoder Block</h4>
<p>Second, there’s the decoder block which has a small architectural variation from the encoder block – a layer to allow it to pay attention to specific segments from the encoder:</p>

<p><img src="http://jalammar.github.io/images/xlnet/transformer-decoder-block-2.png">
  <br>
</p>

<p>One key difference in the self-attention layer here, is that it masks future tokens – not by changing the word to [mask] like BERT, but by interfering in the self-attention calculation blocking information from tokens that are to the right of the position being calculated.</p>

<p>If, for example, we’re to highlight the path of position #4, we can see that it is only allowed to attend to the present and previous tokens:</p>

<p><img src="http://jalammar.github.io/images/xlnet/transformer-decoder-block-self-attention-2.png">
  <br>
</p>

<p>It’s important that the distinction between self-attention (what BERT uses) and masked self-attention (what GPT-2 uses) is clear. A normal self-attention block allows a position to peak at tokens to its right. Masked self-attention prevents that from happening:</p>

<p><img src="http://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png">
  <br>
</p>

<h4 id="the-decoder-only-block">The Decoder-Only Block</h4>
<p>Subsequent to the original paper, <a href="https://arxiv.org/pdf/1801.10198.pdf">Generating Wikipedia by Summarizing Long Sequences</a> proposed another arrangement of the transformer block that is capable of doing language modeling. This model threw away the Transformer encoder. For that reason, let’s call the model the “Transformer-Decoder”. This early transformer-based language model was made up of a stack of six transformer decoder blocks:</p>

<p><img src="http://jalammar.github.io/images/xlnet/transformer-decoder-intro.png">
  <br>
  The decoder blocks are identical. I have expanded the first one so you can see its self-attention layer is the masked variant. Notice that the model now can address up to 4,000 tokens in a certain segment -- a massive upgrade from the 512 in the original transformer.
</p>

<p>These blocks were very similar to the original decoder blocks, except they did away with that second self-attention layer. A similar architecture was examined in <a href="https://arxiv.org/pdf/1808.04444.pdf">Character-Level Language Modeling with Deeper Self-Attention</a> to create a language model that predicts one letter/character at a time.</p>

<p>The OpenAI GPT-2 model uses these decoder-only blocks.</p>

<h3 id="crash-course-in-brain-surgery-looking-inside-gpt-2">Crash Course in Brain Surgery: Looking Inside GPT-2</h3>

<blockquote>
  <p>Look inside and you will see,
The words are cutting deep inside my brain.
Thunder burning, quickly burning,
Knife of words is driving me insane, insane yeah.
~<strong><a href="https://en.wikipedia.org/wiki/Budgie_(band)">Budgie</a></strong></p>

</blockquote>

<p>Let’s lay a trained GPT-2 on our surgery table and look at how it works.</p>

<p><img src="http://jalammar.github.io/images/gpt2/gpt-2-layers-2.png">
  <br>
  The GPT-2 can process 1024 tokens. Each token flows through all the decoder blocks along its own path.
</p>

<p>The simplest way to run a trained GPT-2 is to allow it to ramble on its own (which is technically called <em>generating unconditional samples</em>) – alternatively, we can give it a prompt to have it speak about a certain topic (a.k.a generating <em>interactive conditional samples</em>). In the rambling case, we can simply hand it the start token and have it start generating words (the trained model uses <code>&lt;|endoftext|&gt;</code> as its start token. Let’s call it <code>&lt;s&gt;</code> instead).</p>

<div>
  <p><img src="http://jalammar.github.io/images/gpt2/gpt2-simple-output-2.gif"></p>
</div>

<p>The model only has one input token, so that path would be the only active one. The token is processed successively through all the layers, then a vector is produced along that path. That vector can be scored against the model’s vocabulary (all the words the model knows, 50,000 words in the case of GPT-2). In this case we selected the token with the highest probability, ‘the’. But we can certainly mix things up – you know how if you keep clicking the suggested word in your keyboard app, it sometimes can stuck in repetitive loops where the only way out is if you click the second or third suggested word. The same can happen here. GPT-2 has a parameter called top-k that we can use to have the model consider sampling words other than the top word (which is the case when top-k = 1).</p>

<p>In the next step, we add the output from the first step to our input sequence, and have the model make its next prediction:</p>

<p><img src="http://jalammar.github.io/images/gpt2/gpt-2-simple-output-3.gif">
  <br>
</p>

<p>Notice that the second path is the only that’s active in this calculation. Each layer of GPT-2 has retained its own interpretation of the first token and will use it in processing the second token (we’ll get into more detail about this in the following section about self-attention). GPT-2 does not re-interpret the first token in light of the second token.</p>

<h3 id="a-deeper-look-inside">A Deeper Look Inside</h3>

<h4 id="input-encoding">Input Encoding</h4>
<p>Let’s look at more details to get to know the model more intimately. Let’s start from the input. As in other NLP models we’ve discussed before, the model looks up the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://jalammar.github.io/illustrated-gpt2/">http://jalammar.github.io/illustrated-gpt2/</a></em></p>]]>
            </description>
            <link>http://jalammar.github.io/illustrated-gpt2/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027205</guid>
            <pubDate>Sun, 08 Nov 2020 17:23:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Connected data: Using BigQuery to analyse user behaviour in response to webhooks]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25027030">thread link</a>) | @lawrjone
<br/>
November 8, 2020 | https://blog.lawrencejones.dev/connected-data/ | <a href="https://web.archive.org/web/*/https://blog.lawrencejones.dev/connected-data/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
<p>Of all the changes I’ve seen over GoCardless’ adolescence, one of the most
impactful has been collecting all our data into a single place.</p>

<p>Thanks to an extraordinary effort across many teams, we went from all our data
being inside a single Postgres database to all our data sources- Postgres,
MySQL, Salesforce, Slack, even our HR platform- being loaded into datasets
inside <a href="https://cloud.google.com/bigquery">Google BigQuery</a>.</p>

<p>For those who haven’t worked with a setup like this, it’s difficult to overstate
how transformational it can be. Tools like BigQuery can analyse terabytes of
data in seconds, making complex analysis a casual exercise.</p>

<p>I think this is really cool, and I’m going to try convincing you by sharing an
example where 2hrs of analysis helped prioritise 2-4 weeks of engineering work.
We’ll end up with query that joins across two very different datasources, which
you can preview at this post’s <a href="https://gist.github.com/lawrencejones/5850c75ecdcbb77492c9e37d11076643">sample Gist</a>.</p>



<p>As some background, GoCardless <a href="https://developer.gocardless.com/api-reference/#appendix-webhooks">sends webhooks</a> to
integrators, informing them whenever something happens to their account.</p>

<p>An example of a payment created event might look like this:</p>

<div><div><pre><code><span>{</span><span>
  </span><span>"id"</span><span>:</span><span> </span><span>"EV001X2XMQXG73"</span><span>,</span><span>
  </span><span>"resource_type"</span><span>:</span><span> </span><span>"payments"</span><span>,</span><span>
  </span><span>"action"</span><span>:</span><span> </span><span>"submitted"</span><span>,</span><span>
  </span><span>"links"</span><span>:</span><span> </span><span>{</span><span>
    </span><span>"payment"</span><span>:</span><span> </span><span>"PM000ESVVBV1R3"</span><span>,</span><span>
    </span><span>"organisation"</span><span>:</span><span> </span><span>"OR00002CC2G5CG"</span><span>
  </span><span>},</span><span>
  </span><span>"details"</span><span>:</span><span> </span><span>{</span><span>
    </span><span>"origin"</span><span>:</span><span> </span><span>"gocardless"</span><span>,</span><span>
    </span><span>"cause"</span><span>:</span><span> </span><span>"payment_submitted"</span><span>,</span><span>
    </span><span>"bank_account_id"</span><span>:</span><span> </span><span>"BA00006XZ7SRNV"</span><span>,</span><span>
    </span><span>"description"</span><span>:</span><span> </span><span>"Payment submitted to the banks. As a result, it can no longer be cancelled."</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div></div>

<p>You’ll notice that the event speaks only of the payment ID (<code>PM000ESVVBV1R3</code>)
rather than providing a copy of the payment inline. This omission is deliberate,
intended to encourage integrators to reach back to the <a href="https://developer.gocardless.com/api-reference/">GoCardless
API</a> to fetch the most up-to-date version of a resource instead
of using potentially stale data.</p>

<p>But this means GoCardless does the work to serialize and send the webhook, just
for the receiver to come back the GoCardless API and request the full version of
these resources:</p>

<figure>
  <img src="https://blog.lawrencejones.dev/assets/images/connected-data-sequence-diagram.png" alt="API integrator making requests in response to webhooks">
  <figcaption>
    API integrator making requests in response to webhooks
  </figcaption>
</figure>

<p>In questioning optimisations, we wondered whether we should <strong>fully serialize
resources into the webhook payload,</strong> instead of referencing them by ID,
allowing an integrator to avoid calling back into our API.</p>

<p>Before we ever consider this, though, we need to know how many API requests
could be attributed to this behaviour. Without the data, we could be vastly
overestimating the number of integrators who make requests in this manner.</p>



<p>Our goal is to understand if these webhook callback requests consume a
significant amount of our API capacity. If the cost is negligible, then it makes
more sense to prioritise work to optimise our API than to remove this workload.</p>

<p>We really want to understand the <strong>percentage of API work (seconds spent
responding) that goes to webhook callback requests</strong>.</p>

<p>Unfortunately, we have no way of knowing if a request made by an integrator is
in response to receiving a webhook. We have no telemetry–nor can I imagine what
this would look like–that could tell us “I’m <code>GET’ing /payments/PM123</code> because I
just saw <code>PM123</code> in a webhook”.</p>

<p>While we can’t be certain, we can make some assumptions about user behaviour
that can help us guess a relationship. You’ll find examples of these assumptions
in a load of useful metrics, such as the Google GSuite team’s definition of user
uptime: <a href="https://blog.acolyer.org/2020/02/26/meaningful-availability/">Meaningful
Availability</a>.</p>

<p>Because we encourage integrators to use webhooks over polling, it’s reasonable
to assume any request for a specific resource immediately after creation is in
response to a webhook. After all, how would you know the ID unless you’d been
told?</p>

<p>With this, we can say:</p>

<blockquote>
  <p>Requests are considered webhook related if they access a specific resource,
and were made within 3 minutes of sending a webhook that referenced the same
resource</p>
</blockquote>

<p>Now we explore our data to see how we can use this.</p>



<p>The thrust of this article is that a connected dataset is worth more than the
sum of its parts. By exporting such a variety of data sources in BigQuery, we
can combine them in ways that aren’t practical to predict in advance.</p>

<p>We’ll combine two such sources to answer our question: the <code>webhooks</code> table of
our relational Postgres database, and HTTP request logs.</p>

<h2 id="webhooks"><code>webhooks</code></h2>

<p>As a developer configuring webhooks, it’s really useful to see what is being
sent. Often you screw up the endpoint or your receiver is failing for some
reason, problems that become clear when you can see things from the senders
perspective.</p>

<p>For this reason, we have a view in the GoCardless dashboard that shows exactly
this:</p>

<figure>
  <img src="https://blog.lawrencejones.dev/assets/images/connected-data-webhooks.png" alt="GoCardless dashboard showing a list of sent webhooks">
  <figcaption>
    Developer page in the GoCardless dashboard, showing a log of sent webhooks
  </figcaption>
</figure>

<p>To provide this view, we store webhook requests and responses in a <code>webhooks</code>
table of the database that powers our API. Each <code>webhooks</code> row has a
<code>request_body</code> field, which is a string containing the JSON body of the request,
in which we’ll find our resource IDs.</p>

<div><div><pre><code>psql=&gt; select id, url, request_body from webhooks limit 1;
-[ RECORD 1]+-----------------------------------------------------------
id           | WB0001J9R226EM
url          | https://webhook.site/f345c389-a70a-48e1-8548-9bf2efea5fbc
request_body | {
                 "events": [{
                   "id": "EV001X3873HCX9",
                   "links": {
                     "payment": "PM000EWZC0KAJX"
                   },
                   ...,
                 }]
               }
</code></pre></div></div>

<p>This Postgres database belongs to our monolithic payments service, an
application which powers a significant part of our product. Unsurprisingly, this
was one of the first data sources exported to BigQuery, so we can easily use
this table for our analysis.</p>

<h2 id="merchant_activity"><code>merchant_activity</code></h2>

<p>To analyse API requests, we’d normally reach for HTTP logs from our servers.
GoCardless uses Elasticsearch to store application logs, which are collected
from the <code>STDOUT</code> of processes.</p>

<p>Elasticsearch encourages you to think about your logging schema- if you don’t,
you tend to lose logs. As much pain as this caused, our push for standardising
log schemas meant writing a tool that could <strong>intercept the application logs</strong>
and <strong>push them into BigQuery was an easy win</strong>.</p>

<p>The application log we’re interested in is the <code>merchant_activity</code> event, which
you can think of as a plain HTTP request log-line with additional information
added after authentication. We’re interested in GET requests to specific
resources, something like <code>GET /payments/PM123</code>. For these requests, we log a
<code>resource_id</code> field specifying the resource accessed, looking like this:</p>

<div><div><pre><code><span>{</span><span>
  </span><span>"_log_timestamp"</span><span>:</span><span> </span><span>"2020-10-30 16:21:14.818 UTC"</span><span>,</span><span>
  </span><span>"method"</span><span>:</span><span> </span><span>"GET"</span><span>,</span><span>
  </span><span>"handler"</span><span>:</span><span> </span><span>"Routes::Payments::Show"</span><span>,</span><span>
  </span><span>"duration"</span><span>:</span><span> </span><span>"0.024530284"</span><span>,</span><span>
  </span><span>"resource_id"</span><span>:</span><span> </span><span>"PM000EWZC0KAJX"</span><span>
</span><span>}</span><span>
</span></code></pre></div></div>



<p>Now we have our data, we can begin crunching it. If it’s easier, you can see the
<a href="https://gist.github.com/lawrencejones/5850c75ecdcbb77492c9e37d11076643#file-query-sql">full BigQuery query here</a>, and I’ll explain piece-by-piece
below.</p>

<p>We’re aiming to produce a dataset of:</p>

<ul>
  <li><strong>timestamp,</strong> group requests by the minute (eg. <code>2020-10-29 00:01:00 UTC</code>)</li>
  <li><strong>duration,</strong> sum of all request durations in seconds (eg. 436.4s)</li>
  <li><strong>webhook_work_duration,</strong> sum of all webhook callback request durations (eg. 5.95s)</li>
  <li><strong>work_ratio,</strong> fraction of work for webhook callbacks (1.36%)</li>
</ul>

<p>First, we need to extract the resource IDs from our <code>webhooks</code> table where the
JSON request body is stored as a string in the <code>request_body</code> field.
Manipulating large blobs of JSON isn’t that SQL friendly, but thankfully
BigQuery supports Javascript functions (<a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions#javascript-udf-structure">BigQuery: Standard SQL user-defined
functions</a>) which can much more flexibly use
this JSON.</p>

<p>We’ll define a function <code>extractIDs</code>, which can pull all the references resource
IDs from the <code>request_body</code> field:</p>

<div><div><pre><code><span>/*
extractIDs takes the webhook request payload, which is structured like so:
{
  "events": [
    { "links": { "&lt;resource-name&gt;": "RES123", ... } },
    ...,
  ]
}

And extracts the unique set of resource IDs referenced by the webhook.
*/</span>
<span>CREATE</span> <span>TEMP</span> <span>FUNCTION</span>
  <span>extractIDs</span><span>(</span><span>json</span> <span>STRING</span><span>)</span>
  <span>RETURNS</span> <span>ARRAY</span><span>&lt;</span><span>string</span><span>&gt;</span>
  <span>LANGUAGE</span> <span>js</span> <span>AS</span> <span>"</span><span>""</span><span>
  return [
    ... new Set(
      JSON.parse(json)["</span><span>events</span><span>"]
        .map(e=&gt;Object.values(e["</span><span>links</span><span>"]))
        .flat()
    )
  ];
</span><span>""</span><span>"</span><span>;</span>
</code></pre></div></div>

<p>Once defined, we can use this function natively inside of any standard SQL
query. We’re going to want to create a CTE called <code>webhook_resources</code>, which
takes each row from <code>webhooks</code> and generates a new row per resource ID inside
the <code>webhooks.request_body</code>.</p>

<div><div><pre><code><span>WITH</span>
  <span>-- Expand each webhook into a row per resource ID from the request_body</span>
  <span>-- payload. Use the Javascript extractIDs function to parse all the events,</span>
  <span>-- then UNNEST the parsed array into separate rows.</span>
  <span>webhook_resources</span> <span>AS</span> <span>(</span>
  <span>SELECT</span>
    <span>id</span><span>,</span>
    <span>created_at</span><span>,</span>
    <span>resource_id</span>
  <span>FROM</span> <span>(</span>
    <span>SELECT</span>
      <span>id</span><span>,</span>
      <span>created_at</span><span>,</span>
      <span>extractIDs</span><span>(</span><span>request_body</span><span>)</span> <span>AS</span> <span>resource_ids</span>
    <span>FROM</span>
      <span>webhooks</span> <span>)</span> <span>AS</span> <span>webhook_resources</span>
    <span>-- UNNEST transforms the array of resources IDs into a row per resource ID,</span>
    <span>-- while the CROSS JOIN finds every pairing from the original webhook (1) to</span>
    <span>-- every resource ID (n).</span>
    <span>CROSS</span> <span>JOIN</span>
    <span>UNNEST</span><span>(</span><span>resource_ids</span><span>)</span> <span>AS</span> <span>resource_id</span>
  <span>),</span>
</code></pre></div></div>

<p>That gets us the webhook resources, and this is where we’re about to leverage
the <strong>connected data concept</strong> to join our Postgres data (<code>webhooks</code>) onto
application logs (<code>merchant_activity</code>).</p>

<p>We’ll join the two sources on a condition that expresses our previous
assumption: that requests are webhook related if they happen within 3m of
sending a webhook that referenced the request resource.</p>

<div><div><pre><code>  <span>-- Join each HTTP request log onto webhook_resources. Set the</span>
  <span>-- webhook_work_duration field to be the duration of the request, if we can</span>
  <span>-- find a webhook that was sent within 3m either side of the request.</span>
  <span>requests</span> <span>AS</span> <span>(</span>
  <span>SELECT</span>
    <span>merchant_activity</span><span>.</span><span>_log_timestamp</span> <span>AS</span> <span>timestamp</span><span>,</span>
    <span>merchant_activity</span><span>.</span><span>handler</span><span>,</span>
    <span>merchant_activity</span><span>.</span><span>resource_id</span><span>,</span>
    <span>merchant_activity</span><span>.</span><span>duration</span><span>,</span>
    <span>-- Set any non-webhook related requests webhook work to be 0.0</span>
    <span>(</span>
      <span>CASE</span> <span>webhook_resources</span><span>.</span><span>resource_id</span> <span>IS</span> <span>NULL</span>
      <span>WHEN</span> <span>TRUE</span> <span>THEN</span> <span>0</span><span>.</span><span>0</span>
      <span>ELSE</span> <span>merchant_activity</span><span>.</span><span>duration</span></code></pre></div></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.lawrencejones.dev/connected-data/">https://blog.lawrencejones.dev/connected-data/</a></em></p>]]>
            </description>
            <link>https://blog.lawrencejones.dev/connected-data/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027030</guid>
            <pubDate>Sun, 08 Nov 2020 16:58:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Interview with Paul Biggar (video interview on moving Dark from OCaml to F#)]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026998">thread link</a>) | @DanielBMarkham
<br/>
November 8, 2020 | https://danielbmarkham.locals.com/post/210794/interview-paul-biggar | <a href="https://web.archive.org/web/*/https://danielbmarkham.locals.com/post/210794/interview-paul-biggar">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div>
        <header>
            <p><a href="https://locals.com/" target="_blank" title="locals.com">
                    <img src="https://danielbmarkham.locals.com/images/locals-logo-transparent.svg" alt="locals logo">
                </a>
            </p>
            <div>
                <div>
                    <p><a href="https://locals.com/" target="_blank" title="locals.com">
                        <img src="https://danielbmarkham.locals.com/images/locals-logo-transparent.svg" alt="locals logo">
                    </a></p>
                    
                </div>
            </div>
            <div>
                <div>
                    <p><a href="https://locals.com/" target="_blank" title="locals.com">
                        <img src="https://danielbmarkham.locals.com/images/locals-logo-transparent.svg" alt="locals logo">
                    </a></p>
                    
                </div>
            </div>
        </header>
        <section>
            
            <div>
                <div>
                    <div>
                        <p><span>
                            Shared post from
                        </span></p><p>
                            DanielBMarkham <br>
                            Community
                        </p>
                    </div>
                    <p><img src="https://cdn.locals.com/images/avatars/92269/92269_xsgujuxuwvuchvt_thumb.jpeg" alt="community logo">
                    </p>
                </div>
                <article>
                    

<div data-id="210794" data-author="DanielBMarkham">
    

<div>
    <div>
        <p><a href="https://danielbmarkham.locals.com/member/DanielBMarkham">
                <span>
                    <span>
                        <img src="https://danielbmarkham.locals.com/images/creator-label.svg?v=0.01" alt="" title="">                    </span>
                </span>
        </a></p>
    </div>
    
</div>


    <div>
        <div>
                    <p>
                Interview: Paul Biggar            </p>
                    <p>Paul's doing some really cool stuff with F# and FaaS.</p>
                    </div>

            </div>


    
    



    <ul>
            </ul>

        
    
    


    
</div>



                    
                </article>
                <div><p>
                    Connect with DanielBMarkham and other
                    members of DanielBMarkham community.

                    </p>
                </div>
            </div>
        </section>
            <section>
            <h5>
                See what else the community is up to...
            </h5>
            <div>
                

<div data-id="168073" data-author="DanielBMarkham">
    

<div>
    <div>
        <p><a href="https://danielbmarkham.locals.com/member/DanielBMarkham">
                <span>
                    <span>
                        <img src="https://danielbmarkham.locals.com/images/creator-label.svg?v=0.01" alt="" title="">                    </span>
                </span>
        </a></p>
    </div>
    
</div>


    <div>
        <div>
                    <p>
                Welcome!            </p>
                    <p>Happy to have you here!</p>
                    </div>

            </div>


    
    



    <ul>
            </ul>

        
    
    


    
</div>




<div data-id="195928" data-author="DanielBMarkham">
    

<div>
    <div>
        <p><a href="https://danielbmarkham.locals.com/member/DanielBMarkham">
                <span>
                    <span>
                        <img src="https://danielbmarkham.locals.com/images/creator-label.svg?v=0.01" alt="" title="">                    </span>
                </span>
        </a></p>
    </div>
    
</div>


    <div>
        <div>
                    <p>
                The Portal            </p>
                    <p>Cool podcast. It's for thinking people who have time to explore new ideas in depth</p>
                    </div>

            </div>


    
    



    <ul>
            </ul>

        
    
    


    
</div>




<div data-id="226146" data-author="DanielBMarkham">
    

<div>
    <div>
        <p><a href="https://danielbmarkham.locals.com/member/DanielBMarkham">
                <span>
                    <span>
                        <img src="https://danielbmarkham.locals.com/images/creator-label.svg?v=0.01" alt="" title="">                    </span>
                </span>
        </a></p>
    </div>
    
</div>


    <div>
        <div>
                    <p>
                Random (history): Veterans' (Armsistice) Day            </p>
                    <div>
                 <p>102 years ago on 11 November 1918 the Armistice with Germany was signed ending the Great War. All was quiet on the Western front. The guns of August had led to the mechanized slaughter of millions. The armistice did not end the conflict; it only delayed the resolution another 30 years or so. </p>
<p>On this day we remember all veterans of wars who go out and do the dirty work for the rest of us. It's the old men who always start wars. It's the rich, lazy and indolent who refuse to fight them. It's the veterans who respond to the call.</p>
<p>We remember you. Thank you.</p>
<p>Here's an apt poem from the earlier Crimean War by Alfred, Lord Tennyson.</p>
<p>--</p>
<p>I<br>
Half a league, half a league,<br>
Half a league onward,<br>
All in the valley of Death<br>
Rode the six hundred.<br>
“Forward, the Light Brigade!<br>
Charge for the guns!” he said.<br>
Into the valley of Death<br>
Rode the six hundred.</p>
<p>II<br>
“Forward, the Light Brigade!”<br>
Was there a man dismayed?<br>
Not though the soldier knew<br>
Someone had ...</p>            </div>
                    </div>

            </div>


    
    



    <ul>
            </ul>

        
    
    


    
</div>


            </div>
            
            
        </section>
            <foolter>
            <div>
                <p><a href="https://apps.apple.com/us/app/locals-com/id1511032007">
                        <img src="https://danielbmarkham.locals.com/images/app-store-badge.png" alt="app store">
                    </a>
                                                    <a href="https://play.google.com/store/apps/details?id=com.locals.localsapp">
                        <img src="https://danielbmarkham.locals.com/images/google-play-badge.png" alt="google store">
                    </a>
                                </p>
            </div>
            
        </foolter>
    </div>
</div></div>]]>
            </description>
            <link>https://danielbmarkham.locals.com/post/210794/interview-paul-biggar</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026998</guid>
            <pubDate>Sun, 08 Nov 2020 16:55:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why Ownership Is Important for Great Product Managers]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026963">thread link</a>) | @justanotherpm
<br/>
November 8, 2020 | https://blog.justanotherpm.com/why-is-ownership-such-an-important-quality-for-great-product-managers-5-reasons/ | <a href="https://web.archive.org/web/*/https://blog.justanotherpm.com/why-is-ownership-such-an-important-quality-for-great-product-managers-5-reasons/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            
<div>
    <main>
            <article>
    
    <div>
            <p>Close your eyes and think about these situations:</p><p>Situation one: Imagine you are on your way to work on a Monday morning. You take the regular metro (or subway or bus), which is extremely crowded. Five minutes after you get in, you notice that an older woman loses her balance and falls. She gets hurt and is crying in pain. By the looks, it might be a severe injury.</p><p>What would you do?</p><p>Before you read on, please take a second to visualize what you would do for the older woman.</p><p>Situation two- It is the same as above. The only difference is that the injured person is not an older stranger, but one of your parents.</p><p>What would you do now? Would it be any different from the first situation?</p><p>I believe that a lot of people would help the older woman from the first situation to a certain extent. Maybe they take her to the closest hospital, or they call the emergency services, or they call someone from her phone. And the situation would end there.</p><p>In the second case, most would probably take their parent to the hospital, take the day off from work, and stay with their parent until they get the necessary treatment and recover fully.</p><p>As expected, the level of <em>ownership</em> is much higher in the second case compared to the first.</p><p>In the first instance, the person worked towards a short term goal or an instant output - get the injured person immediate help. In the second case, the person fought for a longer-term goal and an <em>outcome - </em>help the parent get <em>whatever</em> they need to recover.</p><p>The above set of actions sound apparent, because in one situation, the wounded is a stranger, and in the second, they are a parent. But that is not the point. The point is that the same person can operate at different levels of ownership depending on how much they care about the outcome.</p><p>Great product managers genuinely care about the problems they solve and the solutions they deliver. Great product managers also have an exceptionally high level of ownership.</p><p>Let us understand the advantages of being a high ownership PM:</p><ol><li><strong>Confidence and trust. </strong>Direct manager, peers, and seniors will have very high confidence in those who consistently own and deliver results. They will be the most coveted team member or leader for critical and complex tasks.</li><li><strong>Better decision making. </strong>High ownership individuals have the "do whatever it takes" attitude. Getting things done requires excellent decision making. PMs who are ownership driven create systems and processes to enable and improve their decisions.</li><li><strong>Bigger and stronger network.</strong> Most business-critical decisions require alignment across multiple leaders and teams. High ownership PMs build and leverage relationships to expedite the process of making difficult decisions.</li><li><strong>Create a positive and happy culture in the organization. </strong>Great PMs are self-motivated and deliver positive results, which builds a success-driven culture - something that every employer desires.</li><li><strong>Create a meaningful and visible impact. </strong>As long as PMs are working on the right tasks - which great PMs always do - their work will create a positive impact on their team and the organization.</li></ol><p>At this stage, you might be asking yourself - "How do I become (more) ownership driven?" And, we have the answer to that question, which I will share in the next post.</p><p>For now, I leave you with one of Gary W. Keller's quote, that summarizes the importance of <em>ownership</em>.</p><blockquote>Taking complete ownership of your outcomes by holding no one but yourself responsible for them is the most powerful thing you can do to drive your success.</blockquote>
    </div>
        
</article>                    
                </main>
</div>
        </div></div>]]>
            </description>
            <link>https://blog.justanotherpm.com/why-is-ownership-such-an-important-quality-for-great-product-managers-5-reasons/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026963</guid>
            <pubDate>Sun, 08 Nov 2020 16:51:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How active noise cancellation for automotive works]]>
            </title>
            <description>
<![CDATA[
Score 59 | Comments 68 (<a href="https://news.ycombinator.com/item?id=25026841">thread link</a>) | @giuliomagnifico
<br/>
November 8, 2020 | https://www.silentium.com/advanced-broad-band-active-noise-cancellation-now-available-in-cars/ | <a href="https://web.archive.org/web/*/https://www.silentium.com/advanced-broad-band-active-noise-cancellation-now-available-in-cars/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<div id="mk-page-id-6588">
					<div itemprop="mainEntityOfPage">
							
	<article id="6588" itemscope="itemscope" itemprop="blogPost" itemtype="http://schema.org/BlogPosting">

	<div><p><a title="Advanced, broad-band active noise cancellation now available in cars" href="https://www.silentium.com/wp-content/uploads/2020/11/silentium-anc-system.jpg">&nbsp;</a><img alt="Advanced, broad-band active noise cancellation now available in cars" title="Advanced, broad-band active noise cancellation now available in cars" src="https://www.silentium.com/wp-content/uploads/bfi_thumb/dummy-transparent-oj3yxgsoeorp9werpevcwqrlmj5p4y57n6vjhiqmq0.png" data-mk-image-src-set="{&quot;default&quot;:&quot;https://www.silentium.com/wp-content/uploads/2020/11/silentium-anc-system.jpg&quot;,&quot;2x&quot;:&quot;&quot;,&quot;mobile&quot;:&quot;&quot;,&quot;responsive&quot;:&quot;true&quot;}" width="800" height="500" itemprop="image"></p></div>				
	





<div itemprop="mainEntityOfPage">
	<p>Silentium has introduced advanced, broad-band active road noise cancellation to the auto industry for the first time.</p>
<p>After several years in development, Jaguar and Land Rover are the first carmakers to integrate Silentium’s ‘Active Acoustics’ software in three of their new vehicles, meaning the technology is now available for car buyers to experience. Active road noise cancellation removes 90% of unwanted noise across a broad band of frequencies – from 20Hz up to 1kHz – providing a quieter and more refined experience for occupants, and therefore preventing driver fatigue.</p>
<p>In addition to wellbeing benefits, Silentium’s Active Acoustics technology offers vehicle manufacturers a way to reduce their reliance on costly passive noise damping and insulation materials, and reduce vehicle weight – an increasingly important R&amp;D factor as the industry enters a new era of electro-mobility.</p>
<p>Anthony Manias, Head of Automotive at Silentium, commented: “<em>Active Acoustics will change the way car manufacturers reduce, cancel and enhance sound inside their vehicles, and how customers perceive and interact with these sounds. Silentium has proven that it can make broadband in-car noise cancellation work – now the duty is on carmakers to adopt the technology and ensure their customers can enjoy the benefits.”</em></p>
<p><img src="https://www.silentium.com/wp-content/uploads/2020/11/graphic-silentium1.jpg" alt="" width="1000" height="756" srcset="https://www.silentium.com/wp-content/uploads/2020/11/graphic-silentium1.jpg 1000w, https://www.silentium.com/wp-content/uploads/2020/11/graphic-silentium1-300x227.jpg 300w, https://www.silentium.com/wp-content/uploads/2020/11/graphic-silentium1-768x581.jpg 768w" sizes="(max-width: 1000px) 100vw, 1000px"></p>
<p><strong>How active noise cancellation works</strong></p>
<p>Silentium’s industry-first technology is similar to that found in a pair of high-end noise-cancelling headphones, but more advanced as it manipulates a larger amount of air. Up to six strategically positioned accelerometers on a vehicle’s chassis monitor unwanted road noise and send a signal to an on-board control unit with Silentium’s software, which plays an equivalent anti-noise signal through the vehicle’s speaker system. The pressure waves from both the unwanted exterior noise and manufactured anti-noise reach occupants’ eardrums at exactly the same time and cancel each other out.</p>
<p>Silentium’s Active Acoustics software can reduce, cancel and enhance sound inside any vehicle, improving occupant comfort, safety and wellbeing, and creating a more enjoyable environment for all.</p>
<p><iframe title="Silentium Active Acoustics on road noise" width="1140" height="641" src="https://www.youtube.com/embed/5x9NEpfRZuc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p>
<p><iframe title="Silentium Active Acoustics with road noise and music" width="1140" height="641" src="https://www.youtube.com/embed/uDzSkaWBD7E?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p>
</div>



</article>

							
											</div>
										
				</div>
			</div></div>]]>
            </description>
            <link>https://www.silentium.com/advanced-broad-band-active-noise-cancellation-now-available-in-cars/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026841</guid>
            <pubDate>Sun, 08 Nov 2020 16:35:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Introduction to scraping in Python using BeautifulSoup and Requests]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026820">thread link</a>) | @ahmedbesbes
<br/>
November 8, 2020 | https://www.ahmedbesbes.com/case-studies/introduction-to-scraping | <a href="https://web.archive.org/web/*/https://www.ahmedbesbes.com/case-studies/introduction-to-scraping">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section><section><div><div><iframe title="//www.youtube.com/embed/7Odi2_u-yDk/?modestbranding=1&amp;showinfo=0&amp;autohide=1&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope" allowfullscreen=""></iframe></div></div></section><section><section><article><p>A lot of people at different levels of an organization may need to collect external data from the internet for various
reasons: analyzing the competition, aggregating news feeds to follow trends in particular markets, or collecting daily
stock prices to build predictive models…</p>
<p>Whether you’re a data scientist or a business analyst, you may be in this situation from time to time and ask yourself
this ever-lasting question: <strong>How can I possibly extract this website’s data to conduct market analysis?</strong></p>
<p>One possible free way to extract website data and structure it is scraping. In this post, you’ll learn about data
scraping and how to easily build your first scraper in python.</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/fa92b/overview.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="scraping workflow" title="scraping workflow" src="https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/b9e4f/overview.png" srcset="https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/cf440/overview.png 148w,
https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/d2d38/overview.png 295w,
https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/b9e4f/overview.png 590w,
https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/f9b6a/overview.png 885w,
https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/2d849/overview.png 1180w,
https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/fa92b/overview.png 1400w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>scraping workflow</figcaption>
  </figure>
<h3 id="1---What-is-data-scraping-🧹"><a href="#1---What-is-data-scraping-%F0%9F%A7%B9" aria-label="1   What is data scraping 🧹 permalink"></a>1 - What is data scraping? 🧹</h3>
<p>Let me spare you long definitions.</p>
<p>Broadly speaking, data scraping is the process of extracting a website’s data programmatically and structuring it
according to one’s needs. Many companies are using data scraping to gather external data and support their business
operations: this is currently a common practice in multiple fields.</p>
<h4 id="What-do-I-need-to-know-to-learn-data-scraping-in-python"><a href="#What-do-I-need-to-know-to-learn-data-scraping-in-python" aria-label="What do I need to know to learn data scraping in python permalink"></a>What do I need to know to learn data scraping in python?</h4>
<p>Not much. To build small scrapers, you’ll have to be a little bit familiar with Python and HTML syntaxes.</p>
<p>To build scalable and industrial scrapers, you’ll need to know one or two frameworks such as
<a href="https://scrapy.org/">Scrapy</a> or <a href="https://www.selenium.dev/">Selenium</a>.</p>
<h3 id="2---Build-your-first-scraper-in-Python"><a href="#2---Build-your-first-scraper-in-Python" aria-label="2   Build your first scraper in Python permalink"></a>2 - Build your first scraper in Python</h3>
<h4 id="Setup-your-environment"><a href="#Setup-your-environment" aria-label="Setup your environment permalink"></a>Setup your environment</h4>
<p>Let’s learn how to turn a website into structured data! To do this, you’ll first need to install the following
libraries:</p>
<ul>
<li><strong><a href="https://requests.readthedocs.io/en/master/">requests</a></strong>: to simulate HTTP requests like GET and POST. We’ll mainly
use it to access the source page of any given website.</li>
<li><strong><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a></strong>: to parse HTML and XML data very easily</li>
<li><strong><a href="https://lxml.de/">lxml</a></strong>: to increase the parsing speed of XML files</li>
<li><strong><a href="https://pandas.pydata.org/">pandas</a></strong>: to structure the data in dataframes and export it in the format of your
choice (JSON, Excel, CSV, etc.)</li>
</ul>
<p>If you’re using <a href="https://www.anaconda.com/">Anaconda</a>, you should be good to go: all these packages are already
installed. Otherwise, you should run the following commands:</p>
<div data-language="shell"><pre><code>pip <span>install</span> requests
pip <span>install</span> beautifulsoup4
pip <span>install</span> lxml
pip <span>install</span> pandas</code></pre></div>
<p>To make people easily follow along with my video tutorial, I also used a jupyter notebook to make the process
interactive.</p>
<h4 id="What-website-and-data-are-we-going-to-scrape"><a href="#What-website-and-data-are-we-going-to-scrape" aria-label="What website and data are we going to scrape permalink"></a>What website and data are we going to scrape?</h4>
<p>One friend of mine asked me if I could help him scrape this
<a href="https://www.premiumbeautynews.com/fr/marches-tendances/">website</a>. So I decided to do it in a tutorial.</p>
<p>This website is called <strong>Premium Beauty News</strong>. It publishes recent trends in the beauty market. If you look at the
front page, you’ll see that the articles that we want to scrape are organized in a grid.</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/9e57d/headlines.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="article headlines" title="article headlines" src="https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/b9e4f/headlines.png" srcset="https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/cf440/headlines.png 148w,
https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/d2d38/headlines.png 295w,
https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/b9e4f/headlines.png 590w,
https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/f9b6a/headlines.png 885w,
https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/2d849/headlines.png 1180w,
https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/9e57d/headlines.png 1246w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>article headlines</figcaption>
  </figure>
<p>Over multiple pages:</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/57f9cb769fec6b677d700d6e14505fce/a8200/pages.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="pagination" title="pagination" src="https://www.ahmedbesbes.com/static/57f9cb769fec6b677d700d6e14505fce/b9e4f/pages.png" srcset="https://www.ahmedbesbes.com/static/57f9cb769fec6b677d700d6e14505fce/cf440/pages.png 148w,
https://www.ahmedbesbes.com/static/57f9cb769fec6b677d700d6e14505fce/d2d38/pages.png 295w,
https://www.ahmedbesbes.com/static/57f9cb769fec6b677d700d6e14505fce/b9e4f/pages.png 590w,
https://www.ahmedbesbes.com/static/57f9cb769fec6b677d700d6e14505fce/a8200/pages.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>pagination</figcaption>
  </figure>
<p>Of course, we won’t extract the header of each article appearing on these pages only. We’ll go inside each post and grab
everything we need:</p>
<p>the <strong>title</strong>, the <strong>date</strong>, the <strong>abstract</strong>:</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/a8c15d8b0c86256d24facdba8e4dcdef/a8200/metadata.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="metadata" title="metadata" src="https://www.ahmedbesbes.com/static/a8c15d8b0c86256d24facdba8e4dcdef/b9e4f/metadata.png" srcset="https://www.ahmedbesbes.com/static/a8c15d8b0c86256d24facdba8e4dcdef/cf440/metadata.png 148w,
https://www.ahmedbesbes.com/static/a8c15d8b0c86256d24facdba8e4dcdef/d2d38/metadata.png 295w,
https://www.ahmedbesbes.com/static/a8c15d8b0c86256d24facdba8e4dcdef/b9e4f/metadata.png 590w,
https://www.ahmedbesbes.com/static/a8c15d8b0c86256d24facdba8e4dcdef/a8200/metadata.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>metadata</figcaption>
  </figure>
<p>And of course the remaining <strong>full content</strong> of the post.</p>
<h3 id="Show-me-the-code"><a href="#Show-me-the-code" aria-label="Show me the code permalink"></a>Show me the code!</h3>
<p>Because that’s why you’re here, right?</p>
<p>Basic imports first:</p>
<div data-language="python"><pre><code><span>import</span> requests
<span>from</span> bs4 <span>import</span> BeautifulSoup
<span>import</span> pandas <span>as</span> pd
<span>from</span> tqdm <span>import</span> tqdm_notebook</code></pre></div>
<p>I usually define a function to parse the content of each page given its URL. This function will be called multiple
times. Let’s call it <strong>parse_url</strong>:</p>
<div data-language="python"><pre><code><span>def</span> <span>parse_url</span><span>(</span>url<span>)</span><span>:</span>
    response <span>=</span> requests<span>.</span>get<span>(</span>url<span>)</span>
    content <span>=</span> response<span>.</span>content
    parsed_response <span>=</span> BeautifulSoup<span>(</span>content<span>,</span> <span>"lxml"</span><span>)</span>
    <span>return</span> parsed_response</code></pre></div>
<h4 id="Extracting-each-post-data-and-metadata"><a href="#Extracting-each-post-data-and-metadata" aria-label="Extracting each post data and metadata permalink"></a>Extracting each post data and metadata</h4>
<p>I will first start by defining a function that extracts the data of each post (the title, date, abstract, etc) given its
URL. Then we’ll later call this function inside a for loop that goes over all the pages.</p>
<p>To build our scraper, we first have to understand the underlying HTML logic and structure of the page. Let’s start by
extracting the title of the post.</p>
<p>By inspecting this element on Chrome inspector:</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/4fbd9fabd279423673553a46b7b6776d/a8200/title.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="title of the post" title="title of the post" src="https://www.ahmedbesbes.com/static/4fbd9fabd279423673553a46b7b6776d/b9e4f/title.png" srcset="https://www.ahmedbesbes.com/static/4fbd9fabd279423673553a46b7b6776d/cf440/title.png 148w,
https://www.ahmedbesbes.com/static/4fbd9fabd279423673553a46b7b6776d/d2d38/title.png 295w,
https://www.ahmedbesbes.com/static/4fbd9fabd279423673553a46b7b6776d/b9e4f/title.png 590w,
https://www.ahmedbesbes.com/static/4fbd9fabd279423673553a46b7b6776d/a8200/title.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>title of the post</figcaption>
  </figure>
<p>we notice that the title appears inside an <strong>h1</strong> of the class <strong>“article-title”</strong>. After extracting the content of the
page using BeautifulSoup, extracting the title can be done using the <strong>find method</strong>.</p>
<div data-language="python"><pre><code>title <span>=</span> soup_post<span>.</span>find<span>(</span><span>"h1"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"article-title"</span><span>}</span><span>)</span><span>.</span>text</code></pre></div>
<p>Let’s now look at the date:</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/e0c41610dab1b93cf1c5893af9e5393a/a8200/date.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="date of the post" title="date of the post" src="https://www.ahmedbesbes.com/static/e0c41610dab1b93cf1c5893af9e5393a/b9e4f/date.png" srcset="https://www.ahmedbesbes.com/static/e0c41610dab1b93cf1c5893af9e5393a/cf440/date.png 148w,
https://www.ahmedbesbes.com/static/e0c41610dab1b93cf1c5893af9e5393a/d2d38/date.png 295w,
https://www.ahmedbesbes.com/static/e0c41610dab1b93cf1c5893af9e5393a/b9e4f/date.png 590w,
https://www.ahmedbesbes.com/static/e0c41610dab1b93cf1c5893af9e5393a/a8200/date.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>date of the post</figcaption>
  </figure>
<p>The date appears inside a <strong>span</strong>, which itself appears inside a <strong>header</strong> of the class <strong>“row sub-header”</strong>.
Translating this into code is quite easy using BeautifulSoup:</p>
<div data-language="python"><pre><code>datetime <span>=</span> soup_post<span>.</span>find<span>(</span><span>"header"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"row sub-  header"</span><span>}</span><span>)</span><span>.</span>find<span>(</span><span>"span"</span><span>)</span><span>[</span><span>"datetime"</span><span>]</span></code></pre></div>
<p>Regarding the abstract:</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/0ba4d9f58e026e44a5c254ff17d74cc3/a8200/abstract.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="abstract of the post" title="abstract of the post" src="https://www.ahmedbesbes.com/static/0ba4d9f58e026e44a5c254ff17d74cc3/b9e4f/abstract.png" srcset="https://www.ahmedbesbes.com/static/0ba4d9f58e026e44a5c254ff17d74cc3/cf440/abstract.png 148w,
https://www.ahmedbesbes.com/static/0ba4d9f58e026e44a5c254ff17d74cc3/d2d38/abstract.png 295w,
https://www.ahmedbesbes.com/static/0ba4d9f58e026e44a5c254ff17d74cc3/b9e4f/abstract.png 590w,
https://www.ahmedbesbes.com/static/0ba4d9f58e026e44a5c254ff17d74cc3/a8200/abstract.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>abstract of the post</figcaption>
  </figure>
<p>it looks like it’s contained in an <strong>h2</strong> tag of the class “article-intro”.</p>
<div data-language="python"><pre><code>abstract <span>=</span> soup_post<span>.</span>find<span>(</span><span>"h2"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"article-intro"</span><span>}</span><span>)</span><span>.</span>text</code></pre></div>
<p>Now, what about the full content of the post? It’s actually pretty easy to extract. This content is spread over multiple
paragraphs <strong>(p tags)</strong> inside a <strong>div</strong> of the class <strong>“article-text”.</strong></p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/20d71ac723a723d44610cd1b335313d9/a8200/content.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="content of the post" title="content of the post" src="https://www.ahmedbesbes.com/static/20d71ac723a723d44610cd1b335313d9/b9e4f/content.png" srcset="https://www.ahmedbesbes.com/static/20d71ac723a723d44610cd1b335313d9/cf440/content.png 148w,
https://www.ahmedbesbes.com/static/20d71ac723a723d44610cd1b335313d9/d2d38/content.png 295w,
https://www.ahmedbesbes.com/static/20d71ac723a723d44610cd1b335313d9/b9e4f/content.png 590w,
https://www.ahmedbesbes.com/static/20d71ac723a723d44610cd1b335313d9/a8200/content.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>content of the post</figcaption>
  </figure>
<p>Instead of going through each individual <strong>p tag</strong>, extracting its text, and then concatenating all the texts,
BeautifulSoup can extract the full text in one way like this:</p>
<div data-language="python"><pre><code>content <span>=</span> soup_post<span>.</span>find<span>(</span><span>"div"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"article-text"</span><span>}</span><span>)</span><span>.</span>text</code></pre></div>
<p>Let’s package everything in a single function:</p>
<div data-language="python"><pre><code><span>def</span> <span>extract_post_data</span><span>(</span>post_url<span>)</span><span>:</span>
    soup_post <span>=</span> parse_url<span>(</span>post_url<span>)</span>

    title <span>=</span> soup_post<span>.</span>find<span>(</span><span>"h1"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"article-title"</span><span>}</span><span>)</span><span>.</span>text
    datetime <span>=</span> soup_post<span>.</span>find<span>(</span><span>"header"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"row sub-header"</span><span>}</span><span>)</span><span>.</span>find<span>(</span><span>"span"</span><span>)</span><span>[</span><span>"datetime"</span><span>]</span>
    abstract <span>=</span> soup_post<span>.</span>find<span>(</span><span>"h2"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"article-intro"</span><span>}</span><span>)</span><span>.</span>text
    content <span>=</span> soup_post<span>.</span>find<span>(</span><span>"div"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"article-text"</span><span>}</span><span>)</span><span>.</span>text

    data <span>=</span> <span>{</span>
        <span>"title"</span><span>:</span> title<span>,</span>
        <span>"datetime"</span><span>:</span> datetime<span>,</span>
        <span>"abstract"</span><span>:</span> abstract<span>,</span>
        <span>"content"</span><span>:</span> content<span>,</span>
        <span>"url"</span><span>:</span> post_url
    <span>}</span>

    <span>return</span> data</code></pre></div>

<p>If we inspect the source of the home page, where articles are shown with their headlines,</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/c68ac8be504552b4e8a8910a9aa9808f/a8200/posts.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="the posts on the home page" title="the posts on the home page" src="https://www.ahmedbesbes.com/static/c68ac8be504552b4e8a8910a9aa9808f/b9e4f/posts.png" srcset="https://www.ahmedbesbes.com/static/c68ac8be504552b4e8a8910a9aa9808f/cf440/posts.png 148w,
https://www.ahmedbesbes.com/static/c68ac8be504552b4e8a8910a9aa9808f/d2d38/posts.png 295w,
https://www.ahmedbesbes.com/static/c68ac8be504552b4e8a8910a9aa9808f/b9e4f/posts.png 590w,
https://www.ahmedbesbes.com/static/c68ac8be504552b4e8a8910a9aa9808f/a8200/posts.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>the posts on the home page</figcaption>
  </figure>
<p>we’ll see that each of the ten posts appearing in the grid is inside a div of class <strong>“post-style1 col-md-6”</strong> which is
itself inside a <strong>section</strong> of class <strong>“content”</strong>.</p>
<p>Extracting posts per page is therefore quite easy:</p>
<div data-language="python"><pre><code>url <span>=</span> <span>"https://www.premiumbeautynews.com/fr/marches-tendances/"</span>
soup <span>=</span> parse_url<span>(</span>url<span>)</span>
section <span>=</span> soup<span>.</span>find<span>(</span><span>"section"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"content"</span><span>}</span><span>)</span>
posts <span>=</span> section<span>.</span>findAll<span>(</span><span>"div"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"post-style1 col-md-6"</span><span>}</span><span>)</span></code></pre></div>
<p>Then, for each individual post, we can extract the URL which appears inside an <strong>“a tag”</strong> that is itself inside an
<strong>h4</strong>. We’ll use this URL to call our previously defined function <strong>extract<em>post</em>data</strong>.</p>
<div data-language="python"><pre><code>uri <span>=</span> post<span>.</span>find<span>(</span><span>"h4"</span><span>)</span><span>.</span>find<span>(</span><span>"a"</span><span>)</span><span>[</span><span>"href"</span><span>]</span></code></pre></div>
<h4 id="Paginating"><a href="#Paginating" aria-label="Paginating permalink"></a>Paginating</h4>
<p>Once the posts are extracted on a given page, you may want to go to the next page and repeat the same operation.</p>
<p>If you look at the pagination, you’ll notice a “next button”:</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/40c0c97d51ffade267a82984590006ba/f1dec/pagination.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="pagination" title="pagination" src="https://www.ahmedbesbes.com/static/40c0c97d51ffade267a82984590006ba/b9e4f/pagination.png" srcset="https://www.ahmedbesbes.com/static/40c0c97d51ffade267a82984590006ba/cf440/pagination.png 148w,
https://www.ahmedbesbes.com/static/40c0c97d51ffade267a82984590006ba/d2d38/pagination.png 295w,
https://www.ahmedbesbes.com/static/40c0c97d51ffade267a82984590006ba/b9e4f/pagination.png 590w,
https://www.ahmedbesbes.com/static/40c0c97d51ffade267a82984590006ba/f1dec/pagination.png 608w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>pagination</figcaption>
  </figure>
<p>This button becomes inactive once you reach the last page. Put differently, while the next button is active, you have to
tell the scraper to grab the posts of the current page, move to the next page and repeat the operation. When the button
becomes inactive, the process should stop.</p>
<p>Wrapping up this logic, this translates into the following code:</p>
<div data-language="python"><pre><code>next_button <span>=</span> <span>""</span>
posts_data <span>=</span> <span>[</span><span>]</span>
count <span>=</span> <span>1</span>
base_url <span>=</span> <span>'https://www.premiumbeautynews.com/'</span>

<span>while</span> next_button <span>is</span> <span>not</span> <span>None</span><span>:</span>
    <span>print</span><span>(</span><span><span>f"page number : </span><span><span>{</span>count<span>}</span></span><span>"</span></span><span>)</span>

    soup <span>=</span> parse_url<span>(</span>url<span>)</span>
    section <span>=</span> soup<span>.</span>find<span>(</span><span>"section"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"content"</span><span>}</span><span>)</span>
    posts <span>=</span> section<span>.</span>findAll<span>(</span><span>"div"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"post-style1 col-md-6"</span><span>}</span><span>)</span>

    <span>for</span> post <span>in</span> tqdm_notebook<span>(</span>posts<span>,</span> leave<span>=</span><span>False</span><span>)</span><span>:</span>
        uri <span>=</span> post<span>.</span>find<span>(</span><span>"h4"</span><span>)</span><span>.</span>find<span>(</span><span>"a"</span><span>)</span><span>[</span><span>"href"</span><span>]</span>
        post_url <span>=</span> base_url <span>+</span> uri
        data <span>=</span> extract_post_data<span>(</span>post_url<span>)</span>
        posts_data<span>.</span>append<span>(</span>data<span>)</span>

    next_button <span>=</span> soup<span>.</span>find<span>(</span><span>"p"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"pagination"</span><span>}</span><span>)</span><span>.</span>find<span>(</span><span>"span"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"next"</span><span>}</span><span>)</span>
    <span>if</span> next_button <span>is</span> <span>not</span> <span>None</span><span>:</span>
        url <span>=</span> base_url <span>+</span> next_button<span>.</span>find<span>(</span><span>"a"</span><span>)</span><span>[</span><span>"href"</span><span>]</span>
        count <span>+=</span> <span>1</span></code></pre></div>
<p>Once this loop completes, you’ll have all the data inside posts_data, which you can turn it into a beautiful dataframe
and export to CSV or Excel file.</p>
<div data-language="python"><pre><code>df <span>=</span> pd<span>.</span>DataFrame<span>(</span>posts_data<span>)</span>
df<span>.</span>head<span>(</span><span>)</span></code></pre></div>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/18c1607d3c11bbb201e37ec88fdd4e23/a8200/dataframe.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="dataframe" title="dataframe" src="https://www.ahmedbesbes.com/static/18c1607d3c11bbb201e37ec88fdd4e23/b9e4f/dataframe.png" srcset="https://www.ahmedbesbes.com/static/18c1607d3c11bbb201e37ec88fdd4e23/cf440/dataframe.png 148w,
https://www.ahmedbesbes.com/static/18c1607d3c11bbb201e37ec88fdd4e23/d2d38/dataframe.png 295w,
https://www.ahmedbesbes.com/static/18c1607d3c11bbb201e37ec88fdd4e23/b9e4f/dataframe.png 590w,
https://www.ahmedbesbes.com/static/18c1607d3c11bbb201e37ec88fdd4e23/a8200/dataframe.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>dataframe</figcaption>
  </figure>
<p>Thanks for reading for staying till the end! If you’re interested in the video recording of this tutorial, here is the
link:</p>
<h3 id="Where-to-go-from-here"><a href="#Where-to-go-from-here" aria-label="Where to go from here permalink"></a>Where to go from here?</h3>
<p>You just learned how to build your first scraper, congratulations!</p>
<p>Now if you want to improve it you may think about these next steps:</p>
<ul>
<li>Multiprocessing and multithreading the execution of the script</li>
<li>Scheduling the run of the scraper over periods of time, to automate data scraping</li>
<li>Handle error — scrapers are hard to maintain over time because the source code may change</li>
<li>Deploy a database or an s3 bucket to store the scraped items</li>
</ul>
<p>This may get you busy for a while! Happy data scraping!</p></article></section><section></section></section></section></div></div>]]>
            </description>
            <link>https://www.ahmedbesbes.com/case-studies/introduction-to-scraping</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026820</guid>
            <pubDate>Sun, 08 Nov 2020 16:32:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Using Alfred to open your GitHub repositories in the browser]]>
            </title>
            <description>
<![CDATA[
Score 80 | Comments 31 (<a href="https://news.ycombinator.com/item?id=25026757">thread link</a>) | @mmazzarolo
<br/>
November 8, 2020 | https://mmazzarolo.com/blog/2020-09-28-alfred-github-repos/ | <a href="https://web.archive.org/web/*/https://mmazzarolo.com/blog/2020-09-28-alfred-github-repos/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p>One thing I do multiple times a day is browsing my company’s GitHub organization repositories.<br>
My process for opening these repositories in the browser is:</p>
<ul>
<li>Open Chrome</li>
<li>Press <kbd>Command</kbd> + <kbd>L</kbd> to focus the address bar</li>
<li>Start typing the GitHub repository name</li>
<li>Look for the page suggestion and click on it</li>
</ul>
<p>This flow works well for repositories that I have starred as bookmarks or that I browsed recently…<br>
…But because I’m a total sucker for Alfred, today I wasted almost an hour moving this process into an Alfred workflow.</p>
<p><span>
      <span></span>
  <img alt="alfred" title="alfred" src="https://mmazzarolo.com/static/f6b79b275f646159f864ae32c3d83508/fcda8/alfred.png" srcset="https://mmazzarolo.com/static/f6b79b275f646159f864ae32c3d83508/12f09/alfred.png 148w,
https://mmazzarolo.com/static/f6b79b275f646159f864ae32c3d83508/e4a3f/alfred.png 295w,
https://mmazzarolo.com/static/f6b79b275f646159f864ae32c3d83508/fcda8/alfred.png 590w,
https://mmazzarolo.com/static/f6b79b275f646159f864ae32c3d83508/efc66/alfred.png 885w,
https://mmazzarolo.com/static/f6b79b275f646159f864ae32c3d83508/c83ae/alfred.png 1180w,
https://mmazzarolo.com/static/f6b79b275f646159f864ae32c3d83508/7ef4c/alfred.png 1748w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
    </span></p>
<h2>The Action Plan</h2>
<p>Knowing that:</p>
<ul>
<li>The entire repository list is huge and doesn’t change often</li>
<li>I wanted the Alfred repository search result to be instant</li>
</ul>
<p>Making an API request to filter the repositories each time I invoke the Alfred workflow wasn’t an option.</p>
<p>So I decided to 1) download the entire repository list into a JSON file, 2) transform it into an Alred-compatible format, and 3) use the <a href="https://github.com/deanishe/alfred-fuzzy" target="_blank" rel="nofollow noopener noreferrer">Alfred’s fuzzy search helper</a> to filter the results.</p>
<h2>Creating the GitHub repository list</h2>
<p>First of all, I <a href="https://github.com/settings/tokens/new" target="_blank" rel="nofollow noopener noreferrer">created a GitHub API token with a scope to access the repositories list</a>.</p>
<p><span>
      <span></span>
  <img alt="github repo access" title="github repo access" src="https://mmazzarolo.com/static/5debf06855fa8eef2379c163d8d95943/fcda8/github-repo-access.png" srcset="https://mmazzarolo.com/static/5debf06855fa8eef2379c163d8d95943/12f09/github-repo-access.png 148w,
https://mmazzarolo.com/static/5debf06855fa8eef2379c163d8d95943/e4a3f/github-repo-access.png 295w,
https://mmazzarolo.com/static/5debf06855fa8eef2379c163d8d95943/fcda8/github-repo-access.png 590w,
https://mmazzarolo.com/static/5debf06855fa8eef2379c163d8d95943/efc66/github-repo-access.png 885w,
https://mmazzarolo.com/static/5debf06855fa8eef2379c163d8d95943/69476/github-repo-access.png 926w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
    </span></p>
<blockquote>
<p>TIL: Selecting all the “repo” sub-scopes is not the same as selecting the entire scope — which is needed to use the API token to get the list of all the private repositories.</p>
</blockquote>
<p>The GitHub API pagination has a limit of 200 items per page. I needed to fetch way more than 200 repositories, so I modified <a href="https://gist.github.com/mbohun/b161521b2440b9f08b59" target="_blank" rel="nofollow noopener noreferrer">this cool (but outdated) bash script</a> to fetch all of them in a single command and print them out to the console:</p>
<div data-language="bash"><pre><code><span>#!/bin/bash</span>

<span>if</span> <span>[</span> <span>${<span>#</span>@}</span> -lt <span>2</span> <span>]</span><span>;</span> <span>then</span>
    <span>echo</span> <span>"usage: <span>$0</span> [your github credentials as 'user:token'] [REST expression]"</span>
    <span>exit</span> <span>1</span><span>;</span>
<span>fi</span>

<span>GITHUB_CREDENTIALS</span><span>=</span><span>$1</span>
<span>GITHUB_API_REST</span><span>=</span><span>$2</span>

<span>GITHUB_API_HEADER_ACCEPT</span><span>=</span><span>"Accept: application/vnd.github.v3+json"</span>

<span>temp</span><span>=</span><span><span>`</span><span>basename</span> $0<span>`</span></span>
<span>TMPFILE</span><span>=</span><span><span>`</span>mktemp /tmp/$<span>{</span>temp<span>}</span>.XXXXXX<span>`</span></span> <span>||</span> <span>exit</span> <span>1</span>

<span>function</span> <span>rest_call</span> <span>{</span>
    <span>curl</span> -s -u <span>$GITHUB_CREDENTIALS</span> <span>$1</span> -H <span>"<span>${GITHUB_API_HEADER_ACCEPT}</span>"</span> <span>&gt;&gt;</span> <span>$TMPFILE</span>
<span>}</span>


<span>last_page</span><span>=</span><span><span>`</span><span>curl</span> -s -I -u $GITHUB_CREDENTIALS <span>"https://api.github.com<span>${GITHUB_API_REST}</span>?per_page=200"</span> -H <span>"<span>${GITHUB_API_HEADER_ACCEPT}</span>"</span> <span>|</span> <span>grep</span> <span>'^Link:'</span> <span>|</span> <span>sed</span> -e <span>'s/^Link:.*page=//g'</span> -e <span>'s/&gt;.*$//g'</span><span>`</span></span>


<span>if</span> <span>[</span> -z <span>"<span>$last_page</span>"</span> <span>]</span><span>;</span> <span>then</span>
    
    rest_call <span>"https://api.github.com<span>${GITHUB_API_REST}</span>?per_page=200"</span>
<span>else</span>
    
    <span>for</span> <span>p</span> <span>in</span> <span><span>`</span><span>seq</span> <span>1</span> $last_page<span>`</span></span><span>;</span> <span>do</span>
        rest_call <span>"https://api.github.com<span>${GITHUB_API_REST}</span>?per_page=200&amp;page=<span>$p</span>"</span>
    <span>done</span>
<span>fi</span>

<span>cat</span> <span>$TMPFILE</span></code></pre></div>
<p>I named the script <code>githubapi-get.sh</code> and used it this way:</p>
<div data-language="bash"><pre><code>
githubapi-get.sh <span>"{USERNAME}:{TOKEN}"</span> <span>"/orgs/{ORGANIZATION}/repos"</span> <span>&gt;</span> ~/my-company-repos.txt</code></pre></div>
<p>FYI, you can also run it this way to get all the repositories you have access to (both on your personal account and on other organization accounts):</p>
<div data-language="bash"><pre><code>
githubapi-get.sh <span>"{USERNAME}:{TOKEN}"</span> <span>"/user/repos"</span> <span>&gt;</span> ~/my-github-repos.txt</code></pre></div>
<h2>Making the repository list compatible with Alfred</h2>
<p>To populate the Alfred list filter, for each repo I extracted the following information:</p>
<ul>
<li><code>uid</code>: Unique identifier for the item which allows Alfred to learn about this item for subsequent sorting and ordering of the user’s actioned results. I used the repository ID (<code>id</code>).</li>
<li><code>arg</code>: The argument which is passed through the workflow to open it in the browser. I used the repository URL (<code>html_url</code>).</li>
<li><code>title</code>: The title displayed in the result row. I used the repository name (<code>name</code>).</li>
<li><code>subtitle</code>: The subtitle displayed in the result row. I used the repository description (<code>description</code>).</li>
</ul>
<p>Using the following <code>jq</code> script:</p>
<div data-language="bash"><pre><code><span>cat</span> ~/my-company-repos.txt <span>|</span> jq -s <span>'.[] | map({ arg: .html_url, uid: .id, title: .name, subtitle: .description }) | { items: . }'</span> <span>&gt;</span> ~/my-company-repos.json</code></pre></div>
<p>The <code>jq</code> script generates a <code>~/my-company-repos.json</code> file compatible with the <a href="https://www.alfredapp.com/help/workflows/inputs/script-filter/json/" target="_blank" rel="nofollow noopener noreferrer">Alfred Script Filter JSON format</a>.</p>
<h2>Creating the Alfred workflow</h2>
<p>The Alfred standard script filtering doesn’t have a good fuzzy search option — which I really wanted given the huge amount of repositories.</p>
<p>As a workaround, I used <a href="https://github.com/deanishe/alfred-fuzzy" target="_blank" rel="nofollow noopener noreferrer">alfred-fuzzy</a>, a Python helper script for Alfred that replaces the “Alfred filters results” option with fuzzy search.</p>
<p>Here’s what I did, step by step:</p>
<ol>
<li>Create a new empty Alfred workflow</li>
<li>Right-click on the created workflow ⭢ “Open in Finder”</li>
<li>In the workflow directory, copy and paste both <a href="https://raw.githubusercontent.com/deanishe/alfred-fuzzy/master/fuzzy.py" target="_blank" rel="nofollow noopener noreferrer">fuzzy.py</a> and <code>my-company-repos.json</code>.</li>
<li>In the workflow, create the following “Script Filter”:
<span>
      <span></span>
  <img alt="workflow 0" title="workflow 0" src="https://mmazzarolo.com/static/79ea20fa5f1577ca4cb07a0fd9329861/fcda8/workflow-0.png" srcset="https://mmazzarolo.com/static/79ea20fa5f1577ca4cb07a0fd9329861/12f09/workflow-0.png 148w,
https://mmazzarolo.com/static/79ea20fa5f1577ca4cb07a0fd9329861/e4a3f/workflow-0.png 295w,
https://mmazzarolo.com/static/79ea20fa5f1577ca4cb07a0fd9329861/fcda8/workflow-0.png 590w,
https://mmazzarolo.com/static/79ea20fa5f1577ca4cb07a0fd9329861/efc66/workflow-0.png 885w,
https://mmazzarolo.com/static/79ea20fa5f1577ca4cb07a0fd9329861/c83ae/workflow-0.png 1180w,
https://mmazzarolo.com/static/79ea20fa5f1577ca4cb07a0fd9329861/2b608/workflow-0.png 1540w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
    </span></li>
<li>In the workflow, create the following “Open URL” action:
<span>
      <span></span>
  <img alt="workflow 1" title="workflow 1" src="https://mmazzarolo.com/static/5f157cd5f9d5e2e8f928074cc6d37708/fcda8/workflow-1.png" srcset="https://mmazzarolo.com/static/5f157cd5f9d5e2e8f928074cc6d37708/12f09/workflow-1.png 148w,
https://mmazzarolo.com/static/5f157cd5f9d5e2e8f928074cc6d37708/e4a3f/workflow-1.png 295w,
https://mmazzarolo.com/static/5f157cd5f9d5e2e8f928074cc6d37708/fcda8/workflow-1.png 590w,
https://mmazzarolo.com/static/5f157cd5f9d5e2e8f928074cc6d37708/efc66/workflow-1.png 885w,
https://mmazzarolo.com/static/5f157cd5f9d5e2e8f928074cc6d37708/c83ae/workflow-1.png 1180w,
https://mmazzarolo.com/static/5f157cd5f9d5e2e8f928074cc6d37708/7960f/workflow-1.png 1274w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
    </span></li>
<li>In the workflow, connect the “Script Filter” to the “Open URL” action:
<span>
      <span></span>
  <img alt="workflow 2" title="workflow 2" src="https://mmazzarolo.com/static/ca9a91ea4b4aed993050fc92f0780f4d/fcda8/workflow-2.png" srcset="https://mmazzarolo.com/static/ca9a91ea4b4aed993050fc92f0780f4d/12f09/workflow-2.png 148w,
https://mmazzarolo.com/static/ca9a91ea4b4aed993050fc92f0780f4d/e4a3f/workflow-2.png 295w,
https://mmazzarolo.com/static/ca9a91ea4b4aed993050fc92f0780f4d/fcda8/workflow-2.png 590w,
https://mmazzarolo.com/static/ca9a91ea4b4aed993050fc92f0780f4d/8ae3e/workflow-2.png 756w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
    </span></li>
</ol>
<p>That’s it.
I can now invoke the workflow using the keyword set in the “Script Filter” action and fuzzy search the repo I’m interested in.</p>
<p>Alfred is also smart enough to keep track of my workflow usage, surfacing the most clicked results to the top of the list 💥</p>
<blockquote>
<p>Yes, the workflow can be improved in several ways (e.g.: auto-update the repository list after n days)… but I’m happy enough with the current result for now.</p>
</blockquote></section></div>]]>
            </description>
            <link>https://mmazzarolo.com/blog/2020-09-28-alfred-github-repos/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026757</guid>
            <pubDate>Sun, 08 Nov 2020 16:24:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Mind Management, Not Time Management Interview]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026678">thread link</a>) | @ozres1
<br/>
November 8, 2020 | https://ruizhidong.com/writing-routines-daily-rituals-productivity-and-generating-creative-insights-with-david-kadavy/ | <a href="https://web.archive.org/web/*/https://ruizhidong.com/writing-routines-daily-rituals-productivity-and-generating-creative-insights-with-david-kadavy/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div id="content" role="main"><div id="post-separate" class="page">
<figure><p><span><iframe width="900" height="507" src="https://www.youtube.com/embed/VswNjU3G-ZU?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en-US&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span></p></figure><p>I talk to David Kadavy about his new book, <a href="https://amzn.to/354K48w" target="_blank" rel="noopener">Mind Management, Not Time Management</a>.</p><p>David is an author, podcaster, and self-publishing coach. He believes one of the biggest challenges we face in the age of AI is the ability for humans to tap into their innate creativity.</p><p>In this conversation, we talk about structuring the four stages of creativity according to your mental state, how to generate creative insights and ideas, note-taking and creating an ideal working environment, especially for those that are currently working from home.</p><p><strong>Find David online: </strong></p><p><a href="https://kadavy.net/" target="_blank" rel="noopener">David’s Blog</a><br><a href="https://twitter.com/kadavy" target="_blank" rel="noopener">Twitter</a></p><p><strong>Books Mentioned:</strong></p><p><a href="https://amzn.to/3k5NR9N" target="_blank" rel="noopener">Getting Things Done<br></a><a href="https://amzn.to/3k5qrBh" target="_blank" rel="noopener">Black Swan</a><br><a href="https://amzn.to/32lo2g3" target="_blank" rel="noopener">Understanding Media</a></p><p><strong>Further Resources:</strong></p><p>0:12<br>So, what got you started on writing this book about mind management?</p><p>0:19<br>Yeah, the new book is my management, not time management. And the way that I got started on was writing my first book, about 10 years ago, almost exactly, I got a book deal to write my first book. I was not a writer, I hated writing as a kid did not think of myself as a writer. But I’d written some blog posts, and got a book deal.</p><p>0:45<br>And I</p><p>0:47<br>quickly realized that nothing that I had learned about productivity, had prepared me to write a book, I had been a productivity enthusiast, I love to getting things done, and was always trying different tools. But when it came time to write a book, I realized it was just banging my head against the wall for 12 hours a day. And just to get like, 15 minutes of writing that that came easily. And, you know, I tried to clear away my whole schedule, I fired clients, I kind of a lot of my social life, I outsource things, you know, my meal preparation, household chores, cleaning the house, all this stuff. I just cleared out my whole schedule. And I just still couldn’t really get the book, yet it progress going on the book. And I even had a friend say, you just look at the data on your contract and break it down. It was like 250 words a day. Okay, cool. I put that on the calendar and everything. No, it doesn’t work that way. You can’t just like, sit down and write 250 words a day, you have to have the thoughts behind those 250 words, first, you have to like get that all organized. And I did suffer through writing that book. But when the smoke cleared on that process, I wanted to figure out how I had managed to accomplish this, I had started to develop patterns and routines to make that sort of moment of flow happen to make those insights happen. And I started to dig into the neuroscience and the psychology of creativity. And I started to see patterns there. And so I wrote a blog post in 2012, called mind management, not time management. And that prompted Dan Ariely, the behavioral scientist to reach out to me and he was working on an app called timeful. And we collaborated on that app, I was the design advisor to the team, helping them integrate sort of a mind management philosophy into that app. And then an app sold to Google. And I all along been experimenting. I’ve even redesigned my entire life around my creative output. I moved to Columbia five years ago, where I now live. And so during all that time, I’ve really been trying to tweak this system to have a consistent way to manage creative energy toward having those moments where you have the ideas come easily to you. And so I think I’ve I think I’ve got it. So the book just came out. And I have like a far more healthy approach to creativity now, thanks to the things that I’ve learned.</p><p>3:52<br>Or how do you think the being creative engine has helped you? And what about creativity is so important in today’s world?</p><p>4:06<br>Well, I mean, as far as right helping me, I think that I had sort of settled into unhealthy patterns of getting creative work done, I wanted to create my own thing. But I it just became, it just became very unsustainable, which is working around the clock, damaging relationships, of just being obsessed with trying to create things and trying to deal with the day to day world along the way. And so being able to have the confidence that I have a system to manage the energy to manage my project, so they go forward consistently, is just great for my mental health in general, because Take, for example, something like writing that book, it was just, it was just this huge single to do item that was never done. And I didn’t know or have a confidence that I would arrive at the point where I would get it done. But now I have a better perspective of how the creative process works, how my energy works, along with that creative process, how the cycles in the world are there that you can harness to propel your creative projects forward. And so it’s just day to day a lot more relaxing, to be me than it was before. And I think that my work improves as a result of it. Now, as far as why creativity is so important, this wasn’t the reason that I became a creator. But, you know, in this age of AI, where if you can teach a human how to do something, or you can automate it, or you can outsource it, and it will soon be automated, you know, Kai Fuli, who I was, who was a AI expert, work for Google projected 40 to 50% of jobs being replaced by AI in the next 10 to 20 years or so. And a lot of people think it is kind of scary, in a lot of people think that is scary. And you know, there’s a whole other conversation about whether that should happen. But it’s happening there, it appears to be happening. And it’s actually very freeing for us as humans because it means that well now we can go back to being creative. And in fact, if you want to have an edge as a human, you need to be creative. So you can sit down and you can type in 50,000 nonsense words in a day or so. And but you your computer could generate 50,000 nonsense words in you know, instantly fashion you could blink. So you know what the where the value is in is in writing that novel or 50 the thoughts behind the words not in the words themselves?</p><p>7:23<br>Have you seen the stuff on GPT three, and they they’ve produced content that has apparently like full people? Well, some of the some of the content produced by GPT three ranks like on the top page on the first page of Hacker News.</p><p>7:40<br>Yeah, I haven’t seen that level of GPG three I have seen some experiments from sage and l Shane who is messing around with, with ay ay ay. Ay ay ay ay weirdness, calm and, and it was, you know, it was not that impressive, actually, like, how many how many eyes does a horse have? And it just like kept saying for? Like, I don’t know, I think that who knows, I guess I could be wrong like the way that some people were writing and cranking out formulaic say, mystery novels or something I could see AI doing that at some point, but I think there’s always going to be a place for becoming you because there’s only one you and and, and finding a way to present that to the world. So I could be wrong. But I defer to Kai Fuli on this, he doesn’t seem to think that that’s gonna be happening</p><p>8:40<br>terribly soon, if ever.</p><p>8:45<br>So I think that, I guess that</p><p>8:50<br>in terms of the the last thing to be automated by AI would be harnessing that creativity. A lot of the followers on, on this, on this channel are into, into writing and note taking, how do you think that? What What’s something that? How do you think they can benefit from changing, changing their perspective on focusing only on, you know, being busy and focusing on like managing the time and getting as much as possible out of time, as opposed to focusing on mind management and on creative energy management?</p><p>9:37<br>Yeah, so one of the things I’m a note taker, as well, I’ve sort of built my own little zettelkasten. And I think one of the things that makes it work is a lot of the similar to what I’ve found in writing this book, which is that you don’t arrive at great insights or you don’t arrive at learning All at once, you know, taking notes makes, it just reduces the cognitive load for thinking about, say, what is this book about you just like write one little note, another note, another note, another note. And then eventually, you can connect those things. And, you know, make a summary or you can check for new project ideas from different places and make new ideas. But this is kind of like one of the things that I’ve discovered. In dissecting creativity, there is this sort of framework for understanding the stages of creativity, I’m calling it in the book, the four stages of creativity is based upon a speech by a German scientists from the late 1800s, which was then picked up by a guy named Herman Bell, von Helmholtz. And then there was a social scientist named Graham Wallace, who called the four stages of control, and which I just adapted to the four stages of creativity, and their prep your preparation, incubation, illumination, and verification. So preparation is the collecting of information. incubation is the sort of pulling away from the problem space. And there’s all sorts of subconscious incubation, that happens, that makes things easier to later Connect for new ideas. illumination is that moment of having an insight. And then verification is preparing something to verify that it’s ready to be shipped or that it actually is a valid idea. And so this is something that happens when you’re you’re taking notes, as you’re doing the preparation, you’re just collecting these short, little notes. And that is, as you’re doing that you’re exercising those connections in your mind. And so as you sleep, or as you do other things, your brain is sort of testing out connections, it’s any sort of bad dead end connections that you came up with along the way. Are, those are falling away, it’s a it’s called fixation forgetting. And then later on, when you do sit down to say, write the summary of the book that you just took a bunch of notes on it, suddenly, it’s way, way easier. Because that stuff has sunk into your mind. We very easily forget, when we sit down and say, Oh, I’m gonna try to write a summary of this book, or I’m gonna try to write an article or a blog post. And …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ruizhidong.com/writing-routines-daily-rituals-productivity-and-generating-creative-insights-with-david-kadavy/">https://ruizhidong.com/writing-routines-daily-rituals-productivity-and-generating-creative-insights-with-david-kadavy/</a></em></p>]]>
            </description>
            <link>https://ruizhidong.com/writing-routines-daily-rituals-productivity-and-generating-creative-insights-with-david-kadavy/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026678</guid>
            <pubDate>Sun, 08 Nov 2020 16:14:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I made a website to guess tomorrow’s Covid-19 case count]]>
            </title>
            <description>
<![CDATA[
Score 22 | Comments 7 (<a href="https://news.ycombinator.com/item?id=25026666">thread link</a>) | @nathell
<br/>
November 8, 2020 | http://blog.danieljanus.pl/2020/11/08/coronalotto/ | <a href="https://web.archive.org/web/*/http://blog.danieljanus.pl/2020/11/08/coronalotto/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2>Before</h2><p>It seems so obvious in hindsight. Here in Poland, people have been guessing it ever since the pandemic breakout: in private conversations, in random threads on social media, in comments under governmental information outlets. It seemed a&nbsp;matter of time before someone came up with something like this. In fact, on one Sunday evening in October, I&nbsp;found myself flabbergasted that apparently no one yet has.</p><p>I&nbsp;doled out $4 for a&nbsp;domain, <a href="http://koronalotek.pl/">koronalotek.pl</a> (can be translated as “coronalotto” or “coronalottery” – occurrences of the name on Twitter date back at least as far as April), and fired up a&nbsp;REPL. A&nbsp;few hours and 250 Clojure LOCs later, the site was up.</p><p>I&nbsp;wanted it to&nbsp;be as simple as possible. A&nbsp;form with two fields: “your name” and “how many cases tomorrow?” A&nbsp;top-ten list of today’s winners, sorted by the absolute difference between the guess and the actual number of cases, as <a href="https://twitter.com/mz_gov_pl">reported daily on Twitter</a> by the Polish Ministry of Health. The official number, prominently displayed. And that’s all.</p><p><img src="http://blog.danieljanus.pl/img/blog/koronalotek.png"></p><p>On 17 October, I&nbsp;posted the link on my Facebook and Twitter feeds, and waited. The stream of guesses started to&nbsp;trickle in.</p><h2>After</h2><p>It never grew to&nbsp;be more than a&nbsp;stream, but it hasn’t gone completely unnoticed either.</p><p><img src="http://blog.danieljanus.pl/img/blog/koronalotek-g1.png"></p><p>The above plot shows daily number of accepted guesses (i.e., those that were used to&nbsp;generate the next day’s winners) over time – a&nbsp;metric of popularity. Each day’s number means guesses cast in the 24 hours up until 10:30 (Warsaw time) on that day, which is when the official numbers are published by the Ministry of Health.</p><p>I’ve been filtering out automated submissions, as well as excess manual submissions by the same IP that seemed to&nbsp;skew the results too much – I’ve arbitrarily set the “excess” threshold at 10. The missing datapoint for 19 October is not a&nbsp;zero, but a&nbsp;N/A: I’ve lost that datapoint due to&nbsp;a&nbsp;glitch. More on this below.</p><p>The interest peaked on October 23, with more than a&nbsp;thousand guesses for that day (I&nbsp;think it was reposted by someone with a&nbsp;significant outreach back then), and has been slowly declining since.</p><p>I&nbsp;have privately received some feedback. One person has pointed out that they found the site distasteful and that making fun of pandemic tragedies made them uncomfortable. (I&nbsp;empathise; for me it’s not so much making fun as it is a&nbsp;coping mechanism—a&nbsp;way to&nbsp;put distance between my thoughts and the difficult times we’re in and to&nbsp;keep fears at bay.) Some people, however, have thanked me for making them smile when they guessed more or less correctly.</p><p>Back to&nbsp;data. Being a&nbsp;data junkie, I&nbsp;looked at what I&nbsp;had been collecting. First things first: how accurate is the collective predictive power of the guessers?</p><p><img src="http://blog.danieljanus.pl/img/blog/koronalotek-g2.png"></p><p>Quite accurate, in fact! Data for this plot has only been slightly preprocessed, by filtering out “unreasonable” guesses that don’t fall within the range <code>[100; 50000]</code>.</p><p>People have over- and underguesstimated the number of new cases, but not by much. There were only a&nbsp;few occasions where the actual case count didn’t fall within one standard deviation of the mean of guesses (represented by the whiskers around blue bars on the plot). Granted, the daily standard deviation tends to&nbsp;be large (on the order of a&nbsp;few thousand), but still, I’m impressed. A&nbsp;paper on estimating the growth of pandemic based on coronalottery results coming soon to&nbsp;a&nbsp;journal near you! ;-)</p><p>Just for the heck of it, I’ve also been looking at individual votes. Specifically, names. Here’s a&nbsp;snapshot of unique guessers’ names sorted by decreasing length, on 23 October. (NSFW warning: expletives ahead!)</p><p><img src="http://blog.danieljanus.pl/img/blog/koronalotek-names.jpg"></p><p>Let me translate a&nbsp;few of these for those of you who don’t speak Polish:</p><p>1 is “Sasin has fucked over 70 million zlotys for elections that didn’t take place and was never held responsible.” This alludes to&nbsp;the <a href="https://notesfrompoland.com/2020/05/27/70-million-zloty-bill-for-polands-abandoned-presidential-election/">ghost election in Poland</a> from May. This news had gone memetic, going so far as Minister Sasin’s name being ironically used as a&nbsp;dimensionless unit of 70 million (think Avogadro’s number). You’ll discover the same theme in #2, #3, #5, and others.</p><p>6 is “CT {Constitutional Tribunal}, you focking botch, stop repressing my abortion”. Just a&nbsp;day before, the Polish constitutional court (whose current legality is <a href="https://en.wikipedia.org/wiki/Constitutional_Tribunal_(Poland)#2015%E2%80%93present:_Polish_Constitutional_Court_crisis">disputed at best</a>) has <a href="https://notesfrompoland.com/2020/10/22/constitutional-court-ruling-ends-almost-all-legal-abortion-in-poland/">decreed a&nbsp;ban on almost all legal abortion</a> in Poland, giving rise to&nbsp;<a href="https://edition.cnn.com/2020/10/31/europe/poland-abortion-protests-scli-intl/index.html">the biggest street protests in decades</a>.</p><p>Not all is political: 4 is “Why study for the exam if we’re not gonna survive until November anyway?”. I&nbsp;hope whoever wrote this is alive and well.</p><p>Corollary? Give people a&nbsp;text field, and they’ll use it to&nbsp;express themselves: politically or otherwise.</p><p>In fact, I&nbsp;have taken the liberty of chiming in. Shortly after, I&nbsp;altered the thank-you page (which used to&nbsp;just say “thanks for guessing”) to&nbsp;proudly display one of the emblems of the Women’s Strike, along with a&nbsp;link to&nbsp;a&nbsp;<a href="https://zrzutka.pl/kasa-na-aborcyjny-dream-team-55g5gx">crowdfounding campaign</a> for an NGO that supports women needing abortion.</p><p><img src="http://blog.danieljanus.pl/img/blog/koronalotek-thanks.jpg"></p><h2>Inside out</h2><p>I’m not much of a&nbsp;DevOps person, so I&nbsp;deployed it the quick and dirty way, not caring about scalability or performance. The maxim “make it as simple as possible” permeates the setup.</p><p>I&nbsp;just started a&nbsp;REPL within a&nbsp;<code>screen</code> session on the tiny Scaleway C1 server that also hosts this blog and some of my other personal stuff. I&nbsp;launched a&nbsp;Jetty server within it, and set up a&nbsp;nginx proxy. And that’s pretty much it. I&nbsp;liberally tinker with the app’s state in “production,” evaluating all kinds of expressions when I&nbsp;feel like it.</p><p>Code changes are deployed by <code>git pull</code>ing new developments and doing <code>(require 'koronalotek.core :reload)</code> in the REPL.</p><p>Someone tried a&nbsp;SQL injection attack. This is doomed to&nbsp;fail because there’s no SQL involved. In fact, there’s no database at all. The entire state is kept in an in-memory atom and periodically synced out to&nbsp;an EDN file. In addition, state is reset and archived daily at the time of announcing winners. (I’ve added the archiving after forgetting it on one occasion – hence the lack of data for 19 October.)</p><p>I&nbsp;also don’t yet have a&nbsp;mechanism of automatically pulling in the Ministry of Health’s data. Every morning, I&nbsp;spend two minutes checking if there’s excess automatic votes, removing them if any, and then filling in the blanks:</p><pre><code>(new-data! #inst "2020-11-08T10:30+01:00" 24785)
</code></pre><p>For all the violations of good practices in this setup, it has worked out surprisingly well so far. I’ve resorted to&nbsp;removing automated votes a&nbsp;handful of times, and blacklisting IPs of voting bots in the nginx setup twice, but otherwise it’s been a&nbsp;low-maintenance toy. People seem to&nbsp;be willing to&nbsp;have fun, and I’m just not interfering.</p><h2>Takeaways</h2><ol><li>You should call on your country’s authorities to&nbsp;exert pressure on the Polish government to&nbsp;respect women’s choices and stop actively repressing them.</li><li>Give people a&nbsp;text field, and they’ll use it to&nbsp;express themselves.</li><li>Release early, release often.</li></ol></div></div>]]>
            </description>
            <link>http://blog.danieljanus.pl/2020/11/08/coronalotto/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026666</guid>
            <pubDate>Sun, 08 Nov 2020 16:13:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The long term costs of object storage in the public cloud]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026660">thread link</a>) | @jtsymonds
<br/>
November 8, 2020 | https://blog.min.io/the-long-term-costs-of-storage-in-the-cloud/ | <a href="https://web.archive.org/web/*/https://blog.min.io/the-long-term-costs-of-storage-in-the-cloud/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
                    <p>The promise/allure of the public cloud is based on the concept that it is elastic. One can, with little effort, scale up workloads and, if desired, scale down those same workloads. We have <a href="https://blog.min.io/repatriation_wave/">written on this subject before</a> - from the perspective of what workloads to consider as you evaluate what to take to the public cloud and what to repatriate to the private cloud. <br></p><p>We have not, however, taken a hard look at the costs associated with the two options. Our position can be related as an analogy. The public cloud is like a nice hotel. Plenty of amenities, secure, spacious etc. It is priced like a nice hotel too. As a result, people don’t live in nice hotels - they stay there for a period of time to achieve a certain objective (business trip, vacation) because it gets too expensive otherwise. <br></p><div><p>We wanted to quantify this objectively so we rolled up our sleeves, made some reasonable assumptions and wrote this post. Here we go:</p><p>If we assume that we will be using object storage in the cloud and want to understand the costs of deploying a PB of data (200TB active, 800TB inactive, with a Read:Write ratio of 80:20) we can compute what this storage would cost on various public clouds.</p></div><p>AWS S3 storage tiers costs are as follows (See <a href="https://aws.amazon.com/s3/pricing/">https://aws.amazon.com/s3/pricing/</a>). This should not be a surprise as prices have not changed in the last four years (but costs have :).<br></p><ul><li><strong>S3 Standard</strong> storage the first 50TB costs $0.023 per month per GB, the next 450TB costs $0.22 per month per GB, and all storage over 500TB costs $0.021 per month per GB. So 1PB of S3 Standard storage would cost $1.2K/month for the 1st 50TB, $9.9K/month for the next 450 TB and $10.3K/month for the next 500TB or <strong>$21.6K/month for 1PB of data</strong>.</li><li><strong>S3 Intelligent Tiering </strong>storage costs the same as S3 Standard for frequently active data and $0.0125 for infrequently accessed data plus an $0.0025 per 1000 Objects fee for management. So assuming 200TB:800TB active:inactive the active data would cost $1.2K/month for 50TB and $3.3K/month for the 150TB and $10.0K for the inactive 800TB. To that, we must add object management costs. It’s not unusual to see PB data warehouse with billions of objects/files. So if we assume 1B objects (average ~1MB/object) in our PB of data object management costs would be $2.5K per month. Totaling all that together we have an overall <strong>storage and object management costs of</strong> ~<strong>$17.0K per month for our 1PB of data</strong>. <br></li></ul><p>It is true one could do a blend of standard S3 plus S3 Glacier or S3 Glacier Deep Archive, but that doesn’t really get to the apples to apples comparison we are seeking. So for the purposes of this we will consider the first two. <br></p><p>Although Azure and GCP don’t have exactly equivalent tiers of storage, if we just focus on S3 Standard equivalents then Azure Blob and GCP cloud storage costs are:</p><ul><li>Azure (Hot) Blob storage <a href="https://azure.microsoft.com/en-us/pricing/details/storage/blobs/">costs</a> a flat $0.0184 per GB per month or $18.4K per 1PB per month, or about 85% of AWS S3 standard costs.</li><li>GCP us-central cloud storage costs are $0.020 per GB per month or $20K for 1PB per month or about 93% of what AWS S3 standard costs. <br></li></ul><p>There are some additional costs that are associated with per 1,000 S3 operations. But we estimate in the scheme of things they don’t add more than $300/month to the above costs. As such we will ignore these costs here.</p><h3 id="what-if-we-wanted-to-copy-the-data-out-of-the-cloud">What if we wanted to copy the data out of the cloud?</h3><p>Of course none of the above cloud storage costs takes into consideration any egress charges which for AWS and GCP are $0.09 to $0.05/GB per month and $0.12/GB, respectively. So if you wanted to move the active 200TB of your 1PB of data out of the (AWS or GCP) cloud each month you would need to add it would cost you another ~$14K/month (on average) with AWS S3 and $24K/month with GCP. <br></p><p>For Azure they don’t appear to have a standard egress charge but rather charge per operation and bandwidth used. We would guess (although we haven’t verified this) that the costs would be comparable to AWS S3 standard egress charges. <br></p><p>Of course there would be no direct charge to move the 1PB of data center data around. You would incur bandwidth costs depending on where you moved it. But the server costs are already accounted for in the costs above. <br></p><h3 id="the-private-cloud-onprem-equivalents-software">The Private Cloud/OnPrem Equivalents: Software</h3><p>First off, you need equivalent software. MinIO offers that (and more) with its S3 compatible, feature rich object storage suite. It is literally a private cloud, drop-in equivalent for AWS. While MinIO is open source, it does offer two tiers for the MinIO Subscription Network. SUBNET, as we call it, combines a commercial license with 24/7/365 direct-to-engineer support, security and resilience audits and other diagnostic technologies that effectively insure production deployments of our software. <br></p><p>The Standard tier is priced at .01 per GB per month and the Enterprise tier is priced at .02 per GB per month.<br></p><p>For our PB of data, that equates to 10K and 20K per month respectively. Needless to say, there is the open source option as well, which would be appropriate if your data was not mission critical. That cost is zero.</p><p>There are no egress costs. There are no per object management costs. <br></p><p>For the purposes of this exercise let’s choose the middle - the Standard plan at <strong>$10K per month for software costs.</strong></p><h3 id="the-private-cloud-onprem-equivalents-hardware">The Private Cloud/OnPrem Equivalents: Hardware</h3><p>While MinIO can run on a range of hardware from Raspberry Pis to IBM Power, we wanted to target dense JBODs for the purposes of our analysis. <br></p><p>It just so happens that through our partnership with Seagate, we have publishable pricing for a 1TB configuration of the Exos AP 2U12 (Dual AP) with two Intel Silver Xeon CPUs. It has</p><p>60 drives at 16 TB per drive delivering .96 PB raw capacity and .72 actual capacity. This assumes erasure coding factor of .75. The price for that hardware is a very reasonable $70K. Let’s choose a three year amortization schedule on that hardware to determine a monthly per GB cost. At 36 months our monthly cost for the hardware works out<strong> to $1,510 per month or $0.0015 GB/month</strong>. <br></p><p>Interested in more? Jump over to the <a href="https://min.io/product/reference-hardware#pricing-calculator">Reference Hardware page to play with the calculator</a> to see other capacities. <br></p><h3 id="don-t-forget-the-data-center-costs">Don’t Forget the Data Center Costs</h3><p>For a data center deployment, we would need to add administration, rack, space, power and cooling costs. We could go into great detail here to determine each of these costs but in general according to the US Chamber of Commerce <a href="https://www.uschamber.com/sites/default/files/ctec_datacenterrpt_lowres.pdf">data center space</a> can be had for about $1305/NRSF (net rentable square feet) in CapEx and another $112/NRSF ( ~8.6% of CapEx) in OpEx or a total of ~$1.4K/NRSF annual cost or an extra $116 per month. <br></p><h3 id="totaling-up-the-private-cloud">Totaling Up the Private Cloud</h3><p>The cost to run your own, 1 PB private cloud, with state of the art hardware, 24/7 direct-to-engineer support, panic button access and annual performance reviews is <strong>$11,510 per month</strong>. Let’s circle back and compare that to what we calculated above.</p><h3 id="summarizing-the-one-year-costs">Summarizing the One Year Costs</h3><p>At $11,510 MinIO and Seagate represent the best economics - by a considerable amount. The combination is 47% less expensive per month than standard S3 and 33% less than S3 with Intelligent Tiering. <br></p><p>These calculations ignore AWS egress costs - which would make MinIO and Seagate less than half as expensive as Standard S3. <br></p><p>MinIO and Seagate are also 38% less than Azure’s comparable option and 43% less than Google Cloud Platform’s comparable offering. <br></p><p>The breakeven point for building your own private cloud vs. S3 Standard comes in at seven months. Thought of another way - for the price of 1PB on S3 Standard you could have almost 1.5 PBs of MinIO and Seagate.</p><p><br>The overall point is not to compete on price. </p><p>We think it is a <a href="https://blog.min.io/the-new-metrics-of-object-storage/">poor metric</a> when thinking about object storage. What we seek to detail here, is that, over time, you will get better performance, superior security, more control and additional flexibility by going on prem - without sacrificing anything on cost (indeed you gain economic advantage). That is why many innovative enterprises are engaged in large scale repatriation strategies, because they realize the cloud is here to stay and they have a choice about what cloud that is - and they are picking the private cloud.</p><p>Feel free to disagree with us. You can reach out at hello@min.io. </p><p>Feel free also to put us to the test. &nbsp;You can contact a Seagate rep from the <a href="https://min.io/product/reference-hardware#pricing-calculator">calculator</a> and <a href="https://min.io/download#/macos">download our software here</a>. If you need a little help, join the 9,700 members of the Slack channel. </p>

                                    </section></div>]]>
            </description>
            <link>https://blog.min.io/the-long-term-costs-of-storage-in-the-cloud/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026660</guid>
            <pubDate>Sun, 08 Nov 2020 16:12:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Introduction to Makefiles]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25026656">thread link</a>) | @ingve
<br/>
November 8, 2020 | https://xs-labs.com/en/blog/2020/11/07/introduction-to-makefiles/ | <a href="https://web.archive.org/web/*/https://xs-labs.com/en/blog/2020/11/07/introduction-to-makefiles/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<h2>Introduction to Makefiles</h2><p>
    I often use Makefiles in some of my projects.<br>
    I really like the flexibility it gives, and I often find myself writing a Makefile instead of a simple shell script to automatize tasks.
</p>
<p>
    So here's a little crash course.<br>
    I'll obviously only cover the basics, but I hope this will give you a good idea on how you could improve your workflows using Makefiles.
</p>
<h3>About Make</h3>
<p>
    Make was developed in 1976 mainly as a build automation tool, to produce executable files or libraries from source code.
</p>
<p>
    While it excels as a build system, it can also be used for a lot of different things.<br>
    If you do write shell scripts to automatize certain tasks, you'll be able to use Makefiles instead.<br>
    As we're going to see, a Makefile can have several advantages over a regular shell script.
</p>
<p>
    This tutorial will only be focused on the GNU version of Make, as it's the most widely used and the most powerful.
</p>
<h3>Basics</h3>
<p>
    First of all, when you invoke the <code>make</code> command, it will look for a file named <code>Makefile</code> in the current working directory.<br>
    This is the default, but note that a specific Makefile can be used with the <code>-f</code> flag, followed by the file name or path.
</p>
<p>
    If such a file is found, it will by default execute the <code>all</code> target.
</p>
<p>
    <strong>Make is target-based system.</strong><br>
    Your Makefile can specify multiple targets, and targets may be executed individually when invoking <code>make</code>. But more on this later.
</p>
<p>
    <strong>For now, we'll just start by creating a basic <em>hello world</em> example.</strong>
</p>
<h4>hello, world</h4>
<p>
    In some directory, create a file called <code>Makefile</code> with the following content:
</p>
<pre>all:
    
    echo "hello, world"
</pre>
<p>
    <code>all</code> is the target name. Target definitions are followed by a colon sign.<br>
    As mentioned earlier, <code>make</code> will by default look for a target called <code>all</code>. So this is our main entry point.
</p>
<p>
    Inside the target, you'll simply execute shell commands.  
    Here, we print the <em>hello, world</em> string, using the shell's builtin <code>echo</code> command.
</p>
<p>
    <strong>Note that target commands need to be indented with at least a single tab.</strong><br>
    <strong>While spaces can be used elsewhere for indentation, tabulation is mandatory inside a target.</strong>
</p>
<p>
    Now from a command prompt, <code>cd</code> to that directory and type <code>make</code>.
</p>
<p>
    <code>make</code> will read the <code>Makefile</code>, and execute the <code>all</code> target, giving the following output:
</p>
<pre>echo "hello, world"
hello, world
</pre>
<p>    
    As you can see, <code>make</code> will first print the full command, before printing any output.<br>
    This can be disabled by using an <code>@</code> sign before the command:
</p>
<pre>all:
    
    @echo "hello, world"
</pre>
<p>
    Now the output is simply:
</p>
<pre>hello, world
</pre>
<h4>Additional targets</h4>
<p>
    You can define as many targets as you want.<br>
    For instance:
</p>
<pre>all:
    
    @echo "hello, world"

foo:
    
    @echo "hello, foo"

bar:
    
    @echo "hello, bar"
</pre>
<p>
    While invoking <code>make</code> will still only execute the <code>all</code> target, the <code>foo</code> or <code>bar</code> targets can be executed individually by specifying their names:
</p>
<pre>$ make
hello, world

$ make foo
hello, foo

$ make bar
hello, bar
</pre>
<h4>Target dependencies</h4>
<p>
    A target may depend on another target, or on multiple other targets.<br>
    This is called a <strong>prerequisite</strong>.
</p>
<p>
    Prerequisites follows the target name:
</p>
<pre>foo: bar
    
    @echo "hello, foo"
</pre>
<p>
    Here, the <code>foo</code> target depends on <code>bar</code>. This means that when <code>foo</code> is about to be executed, <code>bar</code> will be executed first.
</p>
<p>
    Multiple prerequisites are simply separated by a space:
</p>
<pre>foo: bar all
    
    @echo "hello, foo"
</pre>
<p>
    Here, upon executing <code>foo</code>, <code>make</code> will start by executing <code>bar</code>, then <code>all</code>, and finally <code>foo</code>.
</p>
<p>
    And obviously, chaining works too:
</p>
<pre>all: foo
    
    @echo "hello, world"

foo: bar
    
    @echo "hello, foo"

bar:
    
    @echo "hello, bar"
</pre>
<p>
    <code>all</code> depends on <code>foo</code>, which depends on <code>bar</code>. So when invoking <code>make</code>, you'll get the following output:
</p>
<pre>hello, bar
hello, foo
hello, world
</pre>
<p>    
    And you can also manually execute <code>foo</code> by typing <code>make foo</code>, which will give:
</p>
<pre>hello, bar
hello, foo
</pre>
<h4>Error handling</h4>
<p>
    <strong>A very nice thing about <code>make</code> is that it does error handling for you.</strong><br>
    If a command returns a <strong>non-zero</strong> exit status, <code>make</code> will report the error and <strong>abort execution</strong>.
</p>
<p>
    This means that if a command fails inside some target (which may be a prerequisite of another target), the whole execution will stop.<br>
    So you don't have to do any manual error checking, as you would/should do with a shell script.
</p>
<p>
    For instance:
</p>
<pre>all: foo
    
    @echo "hello, world"

foo: bar
    
    @echo "hello, foo"

bar:
    
    @echo "Executing false"
    @false
    @echo "hello, bar"
</pre>
<p>
    Note that in the <code>bar</code> target, we execute the shell's <code>false</code> command, which always returns a non-zero exit status.<br>
    Now if we invoke <code>main</code>, we'll get the following output:
</p>
<pre>Executing false
make: *** [bar] Error 1
</pre>
<p>
    <code>make</code> will execute <code>all</code>, which needs to execute <code>foo</code>, which needs to execute <code>bar</code>.  
    <code>bar</code> will print the first message, and then execute the <code>false</code> command.
</p>
<p>
    As it returns a non-zero exit status, this is detected as an error, and execution is stopped.<br>
    The remaining message in <code>bar</code> will not be printed, and the <code>foo</code> and <code>all</code> targets won't be executed.
</p>
<h4>Debugging</h4>
<p>
    Also note that you can obtain detailed informations about how <code>make</code> reads your Makefile using the <code>--debug</code> flag.<br>
    With the previous example:
</p>
<pre>$ make --debug
Reading makefiles...
Updating goal targets....
    File `all' does not exist.
        File `foo' does not exist.
            File `bar' does not exist.
        Must remake target `bar'.
Executing false
make: *** [bar] Error 1
</pre>
<h4>Variables</h4>
<p>
    You can also define variables inside your Makefile.<br>
    Variables are defined outside targets, and can be referred to with a <code>$</code> sign and parenthesis:
</p>
<pre>HELLO := hello, world

all:

    @echo "$(HELLO)"
</pre>
<p>
    Variables may also be overridden when invoking <code>make</code>, giving extra flexibility.<br>
    For instance, with the example above:
</p>
<pre>$ make HELLO="This is a test"
This is a test
</pre>
<p>
    We'll cover more about variables later.
</p>
<h3>Real life example - Build system</h3>
<p>
    <strong>Now that we have covered the basics, let's take a more useful example.</strong>
</p>
<p>
    We'll create a simple build system for the C programming language.<br>
    The goal is to compile C source files, and to produce an executable.
</p>
<p>
    We'll start by a very simple build system, and work on it step by step to achieve a more generic one.
</p>
<h4>Project structure</h4>
<p>
    Here's the basic project structure:
</p>
<ul>
    <li><code>build</code> (directory)</li>
    <li><code>Makefile</code></li>
    <li>
        <code>source</code> (directory)
        <ul>
            <li><code>main.c</code></li>
        </ul>
    </li>
</ul>

<p>
    We have a <code>build</code> directory for the final executable and temporary files, the Makefile, and a <code>source</code> directory with a single <code>main.c</code> file.
</p>
<p>
The <code>main.c</code> file is a basic <em>hello world</em> program:
</p>
<pre>#include &lt;stdio.h&gt;

int main( void )
{
    printf( "hello, world\n" );
    
    return 0;
}
</pre>
<h4>Producing a simple executable</h4>
<p>
    We'll start with a very simple <code>Makefile</code> that invokes the <code>clang</code> C compiler.<br>
    You can obviously replace it with <code>gcc</code> if you want:
</p>
<pre>all:
    
    @clang -Wall -Werror source/main.c -o build/main
</pre>
<p>
    When invoking <code>make</code>, it will compile the <code>source/main.c</code> and produce an executable in <code>build/main</code>.<br>
    Dead simple.
</p>
<h4>Compiling multiple files</h4>
<p>
    Now let's say we want to compile multiple C files to produce the executable.
</p>
<p>
    We'll first create a function named <code>hello</code> in the <code>source/hello.c</code> file:
</p>
<pre>#include &lt;stdio.h&gt;
#include "hello.h"

void hello( void )
{
    printf( "hello, world\n" );
}
</pre>
<p>
    And we'll also add the corresponding header in <code>source/hello.h</code> with the function prototype:
</p>
<pre>#ifndef HELLO_H
#define HELLO_H

void hello( void );

#endif
</pre>
<p>
    Our <code>main.c</code> file will then call the <code>hello</code> function:
</p>
<pre>#include "hello.h"

int main( void )
{
    hello();
    
    return 0;
}
</pre>
<p>
    Now the <code>Makefile</code> could simply be:
</p>
<pre>all:

    @clang -Wall -Werror source/hello.c source/main.c -o build/main
</pre>
<p>
    However, this is not really flexible, and this is usually not how individual files are compiled.<br>
    Instead, we'll produce an <strong>object file</strong> for each C source file, and <strong>link them together</strong> to produce the final executable:
</p>
<pre>all:

    @clang -Wall -Werror -c source/hello.c -o build/hello.o
    @clang -Wall -Werror -c source/main.c -o build/main.o
    @clang -Wall -Werror build/hello.o build/main.o -o build/main
</pre>
<p>
    Note the additional <code>-c</code> flag, needed to tell the compiler to produce an unlinked object file, instead of an executable.
</p>
<p>
    But we obviously want the compilation to happen in separate targets, so we'll create a specific target for each C source file.<br>
    The <code>all</code> target will depend on these, and be responsible for linking the executable:
</p>
<pre>all: main hello
    
    @clang -Wall -Werror build/hello.o build/main.o -o build/main
    
main:
    
    @clang -Wall -Werror -c source/main.c -o build/main.o
    
hello:
    
    @clang -Wall -Werror -c source/hello.c -o build/hello.o
</pre>
<p>
    Also notice that the compiler flags (<code>-Wall -Werror</code>) are now repeated in each target.<br>
    Time to create a variable:
</p>
<pre>CFLAGS := -Wall -Werror

all: main hello
    
    @clang $(CFLAGS) build/hello.o build/main.o -o build/main
    
main:
    
    @clang $(CFLAGS) -c source/main.c -o build/main.o
    
hello:
    
    @clang $(CFLAGS) -c source/hello.c -o build/hello.o
</pre>
<p>
    This is obviously better, and it also mean we can now override the compiler flags when invoking <code>make</code>:
</p>
<pre>$ make CFLAGS=-Weverything
</pre>
<p>
    It might also be a good idea to create a variable for the compiler itself:
</p>
<pre>CC     := clang
CFLAGS := -Wall -Werror

all: main hello
    
    @$(CC) $(CFLAGS) build/hello.o build/main.o -o build/main
    
main:
    
    @$(CC) $(CFLAGS) -c source/main.c -o build/main.o
    
hello:
    
    @$(CC) $(CFLAGS) -c source/hello.c -o build/hello.o
</pre>
<p>
    So if you want to use <code>gcc</code> instead of <code>clang</code>, you can simply use:
</p>
<pre>$ make CC=gcc
</pre>
<p>
    And we should also add some output:
</p>
<pre>    
CC     := clang
CFLAGS := -Wall -Werror

all: main hello
    
    @echo "Linking executable"
    @$(CC) $(CFLAGS) build/hello.o build/main.o -o build/main
    
main:
    
    @echo "Compiling main.c"
    @$(CC) $(CFLAGS) -c source/main.c …</pre></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://xs-labs.com/en/blog/2020/11/07/introduction-to-makefiles/">https://xs-labs.com/en/blog/2020/11/07/introduction-to-makefiles/</a></em></p>]]>
            </description>
            <link>https://xs-labs.com/en/blog/2020/11/07/introduction-to-makefiles/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026656</guid>
            <pubDate>Sun, 08 Nov 2020 16:12:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Emulator Framework – EMF]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026602">thread link</a>) | @lsferreira42
<br/>
November 8, 2020 | http://em.ulat.es/info.php | <a href="https://web.archive.org/web/*/http://em.ulat.es/info.php">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                    <p>
                        The Emulator Framework was conceived, designed, and implemented by Steven Goodwin, between 2011 and 2020.
                    </p>
                    <p>
                        It is a system to build emulators, assemblers, disassemblers, and assorted tools
                        using a single description language.
                    </p>
                    <p>
                        The code is too much of a 'work in progress' to release, at the moment, but will
                        find its way onto Github at some point. But you can see the resultant examples on 
                        these pages. It truly does work!
                    </p>
                    <p>
                        Contact: MarquisdeGeek@gmail.com
                    </p>
                   <!-- INCLUDE:COPYRIGHT -->
                </div></div>]]>
            </description>
            <link>http://em.ulat.es/info.php</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026602</guid>
            <pubDate>Sun, 08 Nov 2020 16:04:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Problem Solving Techniques]]>
            </title>
            <description>
<![CDATA[
Score 183 | Comments 23 (<a href="https://news.ycombinator.com/item?id=25026561">thread link</a>) | @denvaar
<br/>
November 8, 2020 | https://denvaar.github.io/articles/problem_solving_example.html | <a href="https://web.archive.org/web/*/https://denvaar.github.io/articles/problem_solving_example.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>âœ�ï¸� Going meta - Working through a programming problem to understand problem solving techniques.</p><div>
        <p>The technical skills of computer programming fall under two broad categories, in my opinion.</p>
<p>The first category includes things like learning language syntax, constructs, and patterns. I would summarize it as the ability to connect and utilize the myriad, "tools of the trade" -- languages, frameworks, APIs, libraries -- to create software. There's usually tutorials for these things.</p>
<p>The second category includes things that are a little bit harder to put your finger on, but can probably be best described as problem solving. It's the ability to analyze, troubleshoot, debug, or solve a problem. It's the ability to reason with abstract ideas and turn them into code.</p>
<p>There is for sure some overlap between these two categories, but this is how I like to think about it.</p>
<p>It's hard to be specific about what it takes to be good at problem solving. I believe that's because we all have a slightly different perception of the world around us. Everyone learns differently. There must, however, be at least some techniques that I might learn from you, and vice versa.</p>
<p>Problems like the one I am about to share can be great tools for learning about problem solving. This one's from Project Euler. My intention is not to simply spoil the answer. I want to share my process of figuring it out, with the hope of being able to pinpoint some specific strategies that can be used to solve all kinds of problems.</p>
<h2 id="thechallenge">The challenge</h2>
<p>You can find the original problem statement <a href="https://projecteuler.net/problem=79">here</a>. It's short, so take a moment to read through.</p>
<p><img src="https://denvaar.github.io/assets/secret_number_0.png" alt=""></p>
<p>The challenge is to determine what the shortest possible secret number is. Here's an excerpt from the keylogger file.</p>
<pre><code>...
160
689
716
731
736
...
</code></pre>
<h2 id="firststeps">First steps</h2>
<p>I began by sitting down with pencil and paper to write some numbers from the list. Writing helps me begin to think about the problem.</p>
<p>I thought about what these numbers in the list could tell me about the secret number. Maybe I could find a way to at least figure out its length. There are about fifty numbers in the list, so maybe that number is somehow correlated to the secret number.</p>
<p>These were a few of the questions going through my head. If the answers seem super obvious to you, then congratulations, you might be smarter than me. What's most important during the first steps is that you ask questions.</p>
<p>I realized pretty quickly that no, the length of the list I was given didn't tell me much, but you've got to start somewhere. Next, my thoughts turned to the fact that each number was three digits long.</p>
<p><em>What if I was given a list of two-digit numbers, or even a list of just one-digit numbers? What could that tell me about the length of the secret number? Is there something special about three-digit numbers in particular?</em> These were valuable questions to ask, because it helped me to simplify the problem.</p>
<p>I made a list of one-digit numbers and tried to think about how I might be able to solve the same problem with that instead.</p>
<pre><code>6
2
1
0
</code></pre>
<p>Given a list like this as clues, I could say for certain that the secret number has at least a 6,2,1, and 0, but I also loose essential information about the problem: The order that the numbers appear in. A list of one-digit numbers is too ambiguous. The secret number could be <code>6210</code>, <code>2601</code>, or any other combination, and I would have no way of knowing which one is correct.</p>
<p>A list of two-digit numbers might be nice. It's less to think about, yet is still able to convey the needed information. From this point forward, I decided to think about the list as two-digit numbers, rather than three-digit numbers.</p>
<h2 id="possibleoptionmaintainasortedarray">Possible option: Maintain a sorted array</h2>
<p>At this point, I still wasn't sure how to solve the problem on paper, so I decided to try and work backwards. I wrote down a random number and then picked a few pairs of digits from it to try and reconstruct my own version of the problem. I decided to go through each number in the list and write down the digits as if they were being inserted into some kind of array.</p>
<p><img src="https://denvaar.github.io/assets/secret_number_1.png" alt=""></p>
<p>I realized an approach like this would not be very practical because it still leaves room for ambiguity.</p>
<p>In the example above, its clear that 5 is before 2, but if I continue to add the digits from the next number, I can't tell if the 1 should come before or after the 5. It's the same problem for the 8: I know it should come before the 2, but the data says nothing about if it should be before or after the 1, or the 5.</p>
<p>Even if I had another data point to disambiguate the clues -- <code>51</code>, for example -- sure, it would tell me that the 1 is in the correct spot between 5 and 2, but I already get the feeling that trying to write code to account for switching numbers around in an array is not going to be practical, and that there is probably an easier way.</p>
<p>Writing it out this way helped me realize that the help of some sort of data structure would be useful for solving this problem.</p>
<p>So now the question is, <em>what kind of data structure could help model this problem?</em> To help with this decision, I thought about what data is actually provided in the problem. Using the example above, the list of numbers reads as:</p>
<ul>
<li>5 "comes before" 2</li>
<li>1 "comes before" 2</li>
<li>8 "comes before" 2</li>
</ul>
<p>Each number in the list provides helpful clues, but the problem is that it's difficult to keep track of how each clue ultimately fits together. I tried to think of some kind of data structure that would be able to represent each clue individually, but also as a whole.</p>
<p>A directed graph seems to be a pretty natural fit to represent this information. Each node could be a digit, and the edges between nodes could represent the relationship between them.</p>
<p><img src="https://denvaar.github.io/assets/secret_number_2.png" alt=""></p>
<p>I felt good about using a graph to solve the problem, but there were still some questions that I had to figure out.</p>
<ul>
<li>How would I know when the graph has enough information to to be able to get the secret number? In other words, how can I know when my answer is conclusive?</li>
<li>How could the graph be read or interpreted programmatically to produce the secret number?</li>
<li>What if a secret number had more than one of the same digit? Would that ruin my approach?</li>
</ul>
<h2 id="howtoknowwhentheanswerisconclusive">How to know when the answer is conclusive</h2>
<p>To help answer this question I used the same technique of creating a simplified version of the problem and working backwards. Pretend now that 157 is the secret number. How many edges between the nodes would it take to definitively say that 1 comes before both 5 and 7, and 5 comes before 7? The answer is three edges for this particular graph.</p>
<p><img src="https://denvaar.github.io/assets/secret_number_3.png" alt=""></p>
<p>In this example, the order is known when number of edges are equal to the number of nodes. <em>Is it that simple? Can we know what the secret number is if the number of edges are equal to the number of nodes?</em></p>
<p>In this example, yes, but it doesn't hold true for the general case. By creating more examples, I start to find a relationship between the number of nodes, and the number of edges. Have a look at what four and five-digit secret numbers look like as a graph:</p>
<p><img src="https://denvaar.github.io/assets/secret_number_4.png" alt=""></p>
<p><img src="https://denvaar.github.io/assets/secret_number_5.png" alt=""></p>
<p>After drawing a few of these, I could begin to see a pattern emerge. As the number of nodes increase, the number of edges increase like, <code>1, 3, 6, 10, 15, 21, 28, ..</code>.</p>
<p>It can be helpful to look for patterns, because it means that there's an equation which can be used to represent some aspect of the problem. Here the pattern showed me what condition to use in order to know when my answer could be considered conclusive. This is the equation that represents that pattern, where n is the number of nodes in the graph.</p>
<p><img src="https://denvaar.github.io/assets/secret_number_6.png"></p>
<h2 id="readingthegraphtofindtheanswer">Reading the graph to find the answer</h2>
<p>A hand-drawn graph helps to visualize the approach of solving this problem, but I knew that I would also need to keep in mind how the graph could be represented with code. Specifically, how to programmatically traverse the graph to produce a result.</p>
<p>After staring at the examples for a bit longer, I realized an obvious and helpful property about the graph.</p>
<p>The nodes with the most outward edges come before those with less outward edges. Additionally, the number of edges for each node differ by exactly one. This means that the first digit of the secret number should have the most outward edges, while the last digit would not have any outward edges.</p>
<p>This property made logical sense to me, and was something that could be easily translated into code.</p>
<h2 id="duplicatedigits">Duplicate digits</h2>
<p>A secret number with more than one of the same digit could cause problems with my approach. This was something that worried me as I was working, because it was not clear how to know which two nodes to put the edge between. For example, take a look at 1030 as the secret number, and imagine the digits given in the following order:</p>
<p><img src="https://denvaar.github.io/assets/secret_number_7.png" alt=""></p>
<p>There are multiple ways to draw the graph because there are two 0's. There should still be only one "correct" way. Creating a correct graph might depend on the order in which the digits are given. I might need to think of some way to backtrack and re-connect nodes in order to end up with the correct graph.</p>
<p>The correct and incorrect graphs can be compared to understand how exactly they differ. The incorrect graph has a circular dependency: 3 comes before both the orange and blue 0's, but then the blue 0 comes before 3, which is contradictory.</p>
<p>Another difference is that the correct version is the only one that satisfies the property mentioned above, where the number of each node's edges differ by exactly one. This property should always be true for any secret number modeled with the graph.</p>
<p><img src="https://denvaar.github.io/assets/secret_number_8.png" alt=""></p>
<p>At this point, I decided to put the question of duplicate digits on hold. It looked like this would break the approach that I had planned to use. I think its possible to figure out, but it was unclear if this use case needed to be supported at all.</p>
<p>My plan was now to turn my ideas into code to see if it would produce the correct answer.</p>
<h2 id="translateideastocode">Translate ideas to code</h2>
<p>This part went by pretty quickly because I had formed a good understanding of the problem, as well as an approach for how to solve it. I picked Python for no particular reason, …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://denvaar.github.io/articles/problem_solving_example.html">https://denvaar.github.io/articles/problem_solving_example.html</a></em></p>]]>
            </description>
            <link>https://denvaar.github.io/articles/problem_solving_example.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026561</guid>
            <pubDate>Sun, 08 Nov 2020 15:57:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why the Average Customer Lifetime Value is not enough]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026559">thread link</a>) | @dthread3
<br/>
November 8, 2020 | https://www.revenueforesight.com/blog | <a href="https://web.archive.org/web/*/https://www.revenueforesight.com/blog">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div>
          <div><div>
	<table id="blogTable">
	<tbody><tr>
	    <td>
	        	<div id="272954475637336152-blog"> 
		<div id="wsite-content">	<div id="blog-post-378967267713861712">
	
	
		
	
		
	
		<div>
				<p><span size="7" color="#5040ae">W</span>hen you think of the demand curve for your business, you imagine a general downward sloping curve.&nbsp; Generally, economic theory would dictate that as you decrease the price, the quantity sold might increase.&nbsp; However, decreasing the price might not be the revenue maximizing action, because things would depend on what economists call the elasticity of the demand.&nbsp; The elasticity of demand is the percentage change in goods demanded for a percentage change in price.&nbsp; If the demand is highly elastic, then lowering prices will raise revenue, but if the demand is highly inelastic, then lowering prices will lower revenue.<br></p>  <p>We see a similar dynamic play out in the Customer's Lifetime Value.&nbsp; Suppose that you were to give a discount to a particular group of customers.&nbsp; Would that raise customer lifetime values so that total revenue is higher, or would it lower customer lifetime values so that total revenue is lower?&nbsp; This is a question that is played out over and over again in the relationship between the customers and the seller.<br></p>  <p>Consider the case with your "best" customer, in terms of generating the highest amount of revenue over their lifetime for your business.&nbsp; If you gave your "best" customer a discount, would that generate more revenue for you over their lifetime for your business, or less?&nbsp; Paradoxically, the answer is that it would most likely generate less revenue.&nbsp; The reason is because at the current prices, their demand is satiated.&nbsp; If you gave them a discount, it would not increase consumption enough to cover the discount.<br></p>  <p>Consider the case of your "worst" customer, in terms of generating the lowest amount of revenue over their lifetime for your business.&nbsp; These are the people who will most likely churn.&nbsp; If you gave these people a discount would they generate more revenue over their lifetime to cover the discount and have a gain, or would they generate less?&nbsp; Unsurprisingly, they would most likely still churn and use your discount, so your business would lose revenue.<br></p>  <p>However, there is a particular class of customers that you currently have, which would most likely grow their consumption given the right incentives.&nbsp; They are the ones that need to be introduced into to new products, or features, or even to help their own businesses grow so that they could consume more.&nbsp; They have the potential to grow into your "best" customers.&nbsp; They are however a very select group that is hard to identify.<br></p>  <p>With our AI software that forecasts future lifetime values we can identify this group easily and help you avoid incentivizing the wrong groups.&nbsp; We are able to see what levels of incentives are necessary to induce changes in behavior and the resulting gains or losses.<br></p>  <div><div> <p><a> <img src="https://www.revenueforesight.com/uploads/1/3/4/4/134486339/clv-presentation3-pic_orig.jpg" alt="Picture"> </a></p> </div></div>  <p>Because we are able to forecast down to individual future purchase values for each customer, you can target only those customers that bring you a net gain.&nbsp; We can even automate this feature for you. &nbsp; Contact us for a demo.<br></p>

		</div>
	
	
			

	
		
	
		
	</div>	<div id="blog-post-895881066272963342">
	
	
		
	
		
	
		<div>
				<p><strong><span size="6" color="#5040ae"><em>W</em></span></strong>e previously covered average lifetime value by looking at actual past purchase lifetimes.&nbsp; There is however an alternative formulation that is widely used that doesn't look necessarily at averaging each individual lifetime value, but instead just uses the averages in customer behavior.&nbsp; Let us elaborate with each formula.<br></p>  <h2><span size="4">The Average Order Size</span><br></h2>  <p>The Average Order size is given by&nbsp; AO = Total Revenue / Number of Orders. This treats all orders from everyone the same, as if they came from each individual equally.<br></p>  <h2><span size="4">The Average Frequency Of Orders</span></h2>  <p>The Average Frequency of Orders is given by AF = Number of Customers / Number of Purchases.&nbsp; This doesn't measure true frequency because you don't know the actual time between purchases.<br></p>  <h2><span size="4">The Average Lifetime</span></h2>  <p>The Average Lifetime is given by AL = sum of Customer Lifetimes / Number of Customers.&nbsp; However, because you can't necessarily wait 20 years to see the total customer lifetimes, it is sometimes suggested to use 1 / churn rate percentage to estimate this value.<br></p>  <h2><span size="4">The Average Customer Lifetime Value</span></h2>  <p>The Average Customer Lifetime Value is then given by AvgCLV = AO*AF*AL.&nbsp; It is just the previous formulas multiplied with each other to get the "average" in lifetime value.<br></p>  <h2><span size="4">Correlated Values and Problems with using the "<em>Average</em>"</span><br></h2>  <p>The above formula, while simple, has a few glaring flaws that are not fixable.&nbsp; Customers don't behave like the average, and the "average" lifetime value will be terribly misleading.&nbsp; When you multiple the averages together, you assume that each factor AO, AF,&nbsp; and AL are statistically independent.&nbsp;&nbsp; They are not.&nbsp; When the customer is a high lifetime value customer, the Average Order sizes are larger, the Average Frequency is greater, and the Average Lifetime is greater, for example.&nbsp; When the customer is a low lifetime value customer, the Average Order sizes are smaller, the Average Frequency is less, and the Average Lifetime is shorter, for example.<br></p>  <p>Let's work through an example where we can see the effect of correlation on the "average" lifetime value.&nbsp; For simplicity, assume the correlation is perfect, which won't be too far from the actual case.&nbsp; These numbers come from a Starbucks case study.&nbsp; For Starbucks, the average frequency is AF = 4.2 visits per week.&nbsp; The average order size is AO = $4.05 per visit.&nbsp; The estimated lifetime is 20 years, so AL = 52 weeks * 20 years.&nbsp; This gives the Average Customer Lifetime Value as<br></p>  <p>AvgCLV = $4.05*4.2*52*20 = $17,690.40<br></p>  <p>We can look at the correlation effect as equivalent to adding an extra term inside the formula for Average Customer Lifetime Value</p>  <p>AvgCLV = (AO+x)(AF+x)(AL+x)</p>  <p>When a customer is high lifetime value customer, x is added to all the base values.&nbsp; When a customer is a low lifetime value customer, you can think of subtracting x from all the base values.&nbsp; Now let us look at what happens to the Starbucks customer when they are a high value lifetime customer by assuming it raises all the values by 20%.<br></p>  <p>HighAvgCLV = ($4.05*1.2)*(4.2*1.2)*(52*20*1.2) = $ 30,569.01<br></p>  <p>This value is almost twice the baseline. And let's look similarly at what happens when the Starbucks customer is a low value lifetime customer by lowering all the values by 20%.<br></p>  <p>LowAvgCLV = ($4.05*0.8)*(4.2*0.8)*(52*20*0.8) = $9,057.48<br></p>  <p>This value is almost half the baseline value.&nbsp; High and Low value customers might not be a problem with there were an equal number of both types, but if you recall the L-Shaped distribution from the previous post on "Why The Average Customer Lifetime Value is Not Enough", the majority of customers come from the low value of the distribution with a long tail, since roughly 80% of revenue is generated by only 20% of the customers.<br></p>  <div><div> <p><a> <img src="https://www.revenueforesight.com/uploads/1/3/4/4/134486339/clv-presentation-4-001_orig.png" alt="Picture"> </a></p> </div></div>  <p>The real-world effect of correlation on the terms in the "average" lifetime value AvgCLV = AO*AF*AL, is to make the calculation highly biased and misleading by making the L-shaped distribution have greater extremes in the tails and making it more L-shaped.&nbsp;&nbsp; When the true lifetime value of customers affects the profitability of your business, you cannot depend on the "average" formulas. Given that there is a cost to acquiring customers (through ads and incentives), then knowing the true lifetime values is a key piece of information that you need for your business.<br></p>  <p>We can help you with discovering what the true lifetime value is of each and every customer in your business, because we have AI software that forecasts what each individual customer's lifetime value will be, and what their stream of future purchase values will be, with high accuracy, early in their life.&nbsp; Contact us for a demo.<br></p>

		</div>
	
	
			

	
		
	
		
	</div>	<div id="blog-post-898714202450837069">
	
	
		
	
		
	
		<div>
				<div><p><span size="7" color="#5040ae"><em><strong>A</strong></em></span> lot of e-commerce advice sites will suggest you look at customer lifetime value by their average.&nbsp; The average value however will be very misleading and may cause you to make terrible decisions on acquiring customers.</p><p>Let's examine why the average lifetime value is highly misleading.&nbsp; Below is a graph of customer lifetime purchase values ordered by customer.&nbsp; It is a familiar L-shaped distribution with long tails.</p></div>  <div><div> <p><a> <img src="https://www.revenueforesight.com/uploads/1/3/4/4/134486339/clv-presentation2-pic_orig.jpg" alt="Picture"> </a></p> </div></div>  <p>This distribution becomes more L-shaped as more customers are added.&nbsp; It never becomes the shape of a normal Gaussian distribution that everyone is familiar with, which is a symmetric bell-shaped distribution with 2 sides.<br></p>  <h2><span size="5">The 80/20 rule</span><br></h2>  <p>There is a rule of thumb for business that roughly 80% of the revenues are driven by 20% of the customers. (In actually it is closer to 73-76%).&nbsp; Let's see what kind of implications such an extreme distribution has on the average customer lifetime value.</p>  <p>Let's begin with some round numbers.&nbsp; Say $80 million of lifetime revenue is generated by 20 customers.&nbsp; The rest of the 80 customers only generate $20 million of lifetime revenue.&nbsp;&nbsp; This makes the average lifetime revenue generated to be $4m*20% + $0.25m*80% = $1 million lifetime revenue on average per customer (out of 100 customers and $100m revenue).&nbsp; From the high revenue generating group the average is $4 million per customer.&nbsp; From the low revenue generating group the average is $0.25 million per customer. &nbsp;<br></p>  <p>Now for the sake of simplicity, assume that the Cost of Acquisition of&nbsp; each Customer (<strong>CAC</strong>) is $1 million, or close to it because you are basing decisions on the average lifetime revenue generated by each customer.&nbsp;&nbsp; Then for 20 customers, you are profitable by $3 million, but for 80 customers you are losing $0.75million, each.<br></p>  <p>As you scale your business, it is more likely that you will add customers who are unprofitable.&nbsp;&nbsp; The tail of the L-shaped distribution becomes more extreme, and what you thought was the average lifetime revenue of $1 million 6 months ago, is now only $0.5 million on average.&nbsp; You will not know this is changing because it takes a while to realize that new customer lifetime values are lower than before.&nbsp; This could be disastrous if you kept the $1 million per Customer Acquisition Cost.&nbsp; …</p></div></div></div></div></td></tr></tbody></table></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.revenueforesight.com/blog">https://www.revenueforesight.com/blog</a></em></p>]]>
            </description>
            <link>https://www.revenueforesight.com/blog</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026559</guid>
            <pubDate>Sun, 08 Nov 2020 15:57:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Making a programming language using Rust]]>
            </title>
            <description>
<![CDATA[
Score 195 | Comments 25 (<a href="https://news.ycombinator.com/item?id=25026419">thread link</a>) | @azhenley
<br/>
November 8, 2020 | https://arzg.github.io/lang/ | <a href="https://web.archive.org/web/*/https://arzg.github.io/lang/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>A series about making a programming language called <a href="https://github.com/arzg/eldiro">Eldiro</a> using the <a href="https://rust-lang.org/">Rust</a> programming language.</p><p><a href="https://arzg.github.io/lang/9/">Part Nine: Function Calls</a></p><p><a href="https://arzg.github.io/lang/8/">Part Eight: Function Definitions</a></p><p><a href="https://arzg.github.io/lang/7/">Part Seven: A REPL</a></p><p><a href="https://arzg.github.io/lang/6/">Part Six: Blocks</a></p><p><a href="https://arzg.github.io/lang/5/">Part Five: Binding Usages</a></p><p><a href="https://arzg.github.io/lang/4/">Part Four: Backtracking</a></p><p><a href="https://arzg.github.io/lang/3/">Part Three: Defining Variables</a></p><p><a href="https://arzg.github.io/lang/2/">Part Two: Whitespace Support</a></p><p><a href="https://arzg.github.io/lang/1/">Part One: A Basic Parser</a></p><p><a href="https://arzg.github.io/lang/0/">Part Zero: Getting set up</a></p></div></div>]]>
            </description>
            <link>https://arzg.github.io/lang/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026419</guid>
            <pubDate>Sun, 08 Nov 2020 15:41:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Google Cloud IAM for Security Teams]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026227">thread link</a>) | @gbrindisi
<br/>
November 8, 2020 | https://cloudberry.engineering/article/google-cloud-iam-security-guide/ | <a href="https://web.archive.org/web/*/https://cloudberry.engineering/article/google-cloud-iam-security-guide/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
    <div>
        <div>
            <p>Identity and Access Management (IAM) is an important piece of the cloud puzzle and it’s usually a source of headaches from a security point of view. Let’s try to give some pointers from a blue team perspective.</p>

<p>If you are a security team that just inherited a bunch of Google Cloud Platform (GCP) accounts, this guide is for you.</p>

<h2 id="identities-and-roles">Identities and Roles</h2>

<p>IAM revolves around the concept of <strong>identity</strong>: an (authenticated) entity to which authorization grants are applied.</p>

<h3 id="members">Members</h3>

<p>In Google Cloud, identities are called <strong>members</strong> and are the following:</p>

<ul>
<li>A <strong>User</strong>: defined by a Google Account</li>
<li>A <strong>Group</strong>: defined by a Google Group</li>
<li>A <strong>domain</strong>: a super group representing all accounts under a gsuite/workspace domain</li>
<li>A <strong>Service Account</strong>: an account for an application rather than an end user</li>
</ul>

<p>To make things spicier, there are also two special members that you are going to hate and you want to make sure they will <em>never be used</em> (unless valid business reasons):</p>

<ul>
<li><strong>allAuthenticatedUsers</strong>: a special group comprised of everyone on earth with a google account (yes)</li>
<li><strong>allUser</strong>: anyone on the internet, authenticated or not</li>
</ul>


<div><p>
Pro Tip:</p>

<ul>
<li>scan your IAM policies and make sure that <code>allAuthenticatedUsers</code> and <code>allUsers</code> are never used</li>
<li>or even better, set up an organizational policy to only allow members from your gsuite/workspace domain</li>
</ul>
</div>


<h3 id="roles">Roles</h3>

<p>You assign (bind) a Role to a Member to grant that identity access to a resource. An example role is  <code>resourcemanager.projectCreator</code>:</p>

<pre><code>$ gcloud iam roles list
…
---
description: Access to create new GCP projects.
etag: AA==
name: roles/resourcemanager.projectCreator
stage: GA
title: Project Creator
---
…
</code></pre>

<p>Roles are a set of <em>permissions</em> grouped together, each one representing a fine grained operation. You can’t assign permissions directly to members.</p>

<p>An example permission would be <code>resource manager.projects.create</code>:</p>

<pre><code>$ gcloud iam roles describe roles/resourcemanager.projectCreator
description: Access to create new GCP projects.
etag: AA==
includedPermissions:
- resourcemanager.organizations.get
- resourcemanager.projects.create
name: roles/resourcemanager.projectCreator
stage: GA
title: Project Creator
</code></pre>

<p>Usually, every cloud service will come with a dedicated set of Roles (<a href="https://cloudberry.engineering/article/stricter-access-control-to-gcr/">not for Google Container Registry</a>).</p>

<h3 id="custom-roles">Custom Roles</h3>

<p>As a rule of thumb <strong>stick to standard roles</strong>, but if you have to bind a role to a member in a high level policy you might want to <a href="https://cloud.google.com/iam/docs/creating-custom-roles">use a custom role</a>. Custom Roles allows you to group together the specific set of permissions you need.</p>

<p>They are helpful to maintain least privilege because a role bind on a high level policy (like the Organization one) will affect way more resources.</p>


<div><p>
Pro Tip:</p>

<ul>
<li>Use standard roles when the number of affected resources is limited</li>
<li>Use custom roles when the authorization grant affect too many resources</li>
</ul>
</div>


<h3 id="primitive-roles">Primitive Roles</h3>

<p>There are a bunch of roles you should be wary of: <em>primitive roles</em>. These are <code>Owner</code>, <code>Editor</code> and <code>Viewer</code>. When they are bind on the Project IAM policy they translate to admin, write and read access to everything inside that project.</p>

<p>You want to be wary for two reasons:</p>

<ol>
<li>The set of permissions change over time: when Google release new cloud services, permissions for such services are included in the primitive roles. This means <strong>you have an authorization grant that change over time and you have no control over</strong>. No bueno.</li>
<li><strong>They are a bridge to <a href="https://cloud.google.com/storage/docs/access-control/lists">Access Control List</a></strong> (ACL). ACL are the legacy authorization system for some storage services such as Buckets and Bigquery. In such resources you can assign ACL grants to whoever is bind to a primitive role in the Project. This relationship <strong>adds complexity when trying to understand the <a href="https://cloudberry.engineering/article/lateral-movement-cloud/">blast radius</a></strong> of a member.</li>
</ol>

<p>If you can’t escape using a primitive role, bind them to members that you will use only in “break the glass” scenarios. In practice, you don’t want a team doing their day to day operations as <code>Owners</code>.</p>


<div><p>
Pro Tip:</p>

<ul>
<li>Avoid primitive roles</li>
<li>Do not use Access Control Lists (ACL)</li>
</ul>
</div>


<h2 id="iam-policies-and-where-to-find-them">IAM Policies (and where to find them)</h2>

<p>Roles are bind to members in an IAM Policy. Policies are organized in hierarchical layers (from top to bottom):</p>

<ul>
<li><strong>Organization</strong></li>
<li><strong>Folder</strong></li>
<li><strong>Project</strong></li>
<li>Specific Resources.</li>
</ul>

<p>Bindings will be inherited from top to bottom.</p>

<p>So if you assign <code>Storage Admin</code> to a service account in the Organization IAM Policy, the same grant will be applied to everything down (don’t do that).</p>

<p>You obviously want to be extra careful when binding roles high in the hierarchy as the authorization grant will be quite large. That’s why it’s a good idea to use custom roles to shrink it to just the permissions you need.</p>

<p>Some specific cloud resources, such as Buckets, have their own IAM policy. These are easy to overlook because they don’t have a consistent place in the google cloud admin interface.</p>

<p>The best way to <strong>get visibility over all IAM policies</strong> is to <a href="https://cloud.google.com/asset-inventory/">create a Cloud Asset Inventory (CAI) dump</a>. CAI is, in my opinion, <strong>the most useful thing in GCP</strong>. It’s an API that will generate a json (or bigquery) dump of all the resources and IAM Policies you are currently running.</p>

<p>The best thing you can do on day one is to set up periodical CAI dumps, and then build your detections on top of them.</p>

<h3 id="maintain-the-principle-of-least-privilege">Maintain the principle of least privilege</h3>

<p>The second best thing is to <a href="https://cloud.google.com/iam/docs/recommender-overview">use the IAM Recommender</a>: a service that monitors how role bindings are actually used to make sure you are not over granting them.</p>

<p>Another helpful trick to know is that you can <a href="https://cloud.google.com/iam/docs/conditions-overview">attach conditions to role bindings</a>. For example you can set an expiration time, or you can scope down the binding to affect only certain resources matching a pattern.</p>


<div><p>
Pro Tip:</p>

<ul>
<li>The higher in the IAM hierarchy, the wider the scope of the authorization grant: use sporadically, be a gatekeeper.</li>
<li>Set up Cloud Asset Inventory to not miss anything</li>
<li>Use the IAM Recommender</li>
<li>Use IAM Conditions</li>
</ul>
</div>


<h2 id="notes-on-service-accounts">Notes on Service Accounts</h2>

<p>Service accounts can be of two types: google managed and user managed.</p>

<p>A user managed service account is one you manually create. To use it, you need to create a private key and embed it into your application. A service account can have multiple keys.</p>

<p><strong>Service Account keys lifecycle is your responsibility</strong>: they never really expires, are hard to audit (you don’t see which key has been used from the audit logs). From day one you should start thinking how to keep track of them.</p>

<p>Personally I am a fan of wrapping keys creation into an internal service for your developers to use, but I understand it’s not always possible.</p>

<p>The best alternative is to <strong>use google managed service accounts</strong>. For example each project will come with a default service account that is used by Google Compute Engine (GCE) services (if the GCE API is enabled).</p>

<p>This means that virtual machines will transparently be identified by that service account, and they are authorized to request short-lived authorization tokens from the internal metadata service. You can configure them to use a service account of your choice and <strong>no keys are involved</strong>.</p>


<div><p>
Pro tip:</p>

<ul>
<li>Become a gatekeeper for provisioning Service Accounts keys</li>
<li>Use the identity of the virtual machines whenever possible</li>
<li>Are you running Google Kubernetes Engine (GKE)? Take a look at <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">Workload Identity</a></li>
<li>Follow <a href="https://cloudberry.engineering/article/google-cloud-service-accounts-security-best-practices/">service accounts best practices</a></li>
</ul>
</div>


<h2 id="conclusion">Conclusion</h2>

<p>To recap:</p>

<ul>
<li>scan your IAM policies and make sure that <code>allAuthenticatedUsers</code> and <code>allUsers</code> are never used</li>
<li>or even better, set up an organizational policy to only allow members from your gsuite/workspace domain</li>
<li>Use standard roles when the number of affected resources is limited</li>
<li>Use custom roles when the authorization grant affect too many resources</li>
<li>Avoid primitive roles</li>
<li>Do not use Access Control Lists (ACL)</li>
<li>The higher in the IAM hierarchy, the wider the scope of the authorization grant: use sporadically, be a gatekeeper.</li>
<li>Set up Cloud Asset Inventory to not miss anything</li>
<li>Adhere to the least privilege principle with the IAM Recommender and IAM Conditions</li>
<li>Become a gatekeeper for provisioning Service Accounts keys</li>
<li>Use the identity of the virtual machines whenever possible</li>
<li>Are you running Google Kubernetes Engine (GKE)? Take a look at <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">Workload Identity</a></li>
<li>Follow <a href="https://cloudberry.engineering/article/google-cloud-service-accounts-security-best-practices/">service accounts best practices</a></li>
</ul>

<p>There is a lot more to say about IAM governance and security best practices. This article’s purpose is to give a high level overview of the main security considerations.</p>

<p>If you are looking for specific advices, <a href="mailto:hello@cloudberry.engineering">let me know</a>. I might have some.</p>

<p>Finally, I highly recommend this <a href="https://www.youtube.com/watch?v=YGT_AmCA-eA&amp;feature=youtu.be">fantastic talk by Kat Traxler</a> about primitive roles and IAM quirks in GCP.</p>
        </div>
        
    </div>
</section></div>]]>
            </description>
            <link>https://cloudberry.engineering/article/google-cloud-iam-security-guide/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026227</guid>
            <pubDate>Sun, 08 Nov 2020 15:13:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[South Ayrshire Golf club owner loses 2020 presidential election]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25026202">thread link</a>) | @gnufx
<br/>
November 8, 2020 | https://www.ayrshiredailynews.co.uk/post/south-ayrshire-golf-club-owner-loses-2020-presidential-election | <a href="https://web.archive.org/web/*/https://www.ayrshiredailynews.co.uk/post/south-ayrshire-golf-club-owner-loses-2020-presidential-election">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-hook="post-description"><article><div><div data-rce-version="8.3.0"><div dir="ltr"><div><p id="viewer-8j2hn"><span>Donald Trump, a South Ayrshire golf club owner has lost the 2020 presidential election to Joe Biden, after running again to be re-elected for a second presidential term.</span></p><div id="viewer-2nsni"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img" aria-label=""><p><img data-pin-url="https://www.ayrshiredailynews.co.uk/post/south-ayrshire-golf-club-owner-loses-2020-presidential-election" data-pin-media="https://static.wixstatic.com/media/bc51f5_8bc1fa53ac3f4237b0037c9490c6fb37~mv2.jpg/v1/fit/w_1000%2Ch_637%2Cal_c%2Cq_80/file.png" src="https://static.wixstatic.com/media/bc51f5_8bc1fa53ac3f4237b0037c9490c6fb37~mv2.jpg/v1/fit/w_300,h_300,al_c,q_5/file.jpg" alt=""></p></div><p><span dir="auto">Pic - Northwest Public Broadcasting </span></p></div></div></div><p id="viewer-1nsvc"><span>Donald Trump, currently the 45th president who also owns the Trump Turnberry golf course in South Ayrshire, Scotland has lost the 2020 presidential race to Joe Biden It’s been confirmed.</span></p><p id="viewer-42nml"><span><span>Biden is projected to win Pennsylvania, Arizona &amp; Georgia taking him well over the 270 needed to win the US election and the win for Joe was projected by CNN earlier this afternoon (UK Time).</span></span></p><p id="viewer-b9m6j"><span>Donald who is currently the 45th US president has now lost to Joe Biden who is projected at 290 vs Trumps projected 214 with Joe Biden now set to become the 46th US president.</span></p><p id="viewer-85of6"><span>Donald Trump has however vowed to contest the election in the Supreme Court after saying the election was a fraud, and rigged by the Democrats after loosing his big majorities in states like Pennsylvania &amp; Georgia to postal votes which have been counted over the past several days. </span></p><p id="viewer-3fp2h"><span>Trump has also repeated his unfounded claims that poll watchers were not allowed into ballot counting rooms to observe.&nbsp;</span></p><p id="viewer-1r2l8"><span>Election officials have however said that both Republican and Democrat poll watchers were able to watch the process, and have rubbished claims of unfairness.&nbsp;</span></p><blockquote id="viewer-csnc2"><span><strong><em>Donald Trump tweeted the following earlier after hearing Biden was projected to win the election:</em></strong></span></blockquote><p id="viewer-52uur"><span><span>"THE OBSERVERS WERE NOT ALLOWED INTO THE COUNTING ROOMS. I WON THE ELECTION, GOT 71,000,000 LEGAL VOTES. BAD THINGS HAPPENED WHICH OUR OBSERVERS WERE NOT ALLOWED TO SEE. NEVER HAPPENED BEFORE. MILLIONS OF MAIL-IN BALLOTS WERE SENT TO PEOPLE WHO NEVER ASKED FOR THEM!"</span></span></p><p id="viewer-6b0kl"><span><span>"71,000,000 Legal Votes. The most EVER for a sitting President!"</span></span></p><blockquote id="viewer-dk8em"><span><span><strong>Donald Trump also issued the following statement this afternoon:</strong></span></span></blockquote><p id="viewer-ef707"><span>"Beginning Monday, our campaign will start prosecuting our case in court to ensure election laws are fully upheld and the rightful winner is seated," he added.</span></p><p id="viewer-7m3av"><span>"I will not rest until the American people have the honest vote count they deserve and that democracy demands."</span></p><blockquote id="viewer-162q3"><span><span><strong>Joe Biden also issued statement this afternoon after finding out he had won the US Election:</strong></span></span></blockquote><p id="viewer-ao2b6"><span>"America, I’m honored that you have chosen me to lead our great country.</span></p><p id="viewer-85u1p"><span>The work ahead of us will be hard, but I promise you this: I will be a President for all Americans — whether you voted for me or not."</span></p><p id="viewer-5akrg"><span>"I will keep the faith that you have placed in me."</span></p><p id="viewer-ceo50"><span><span>Joe Biden is also expected to give a speech in the early hours of tomorrow morning at around 1AM UK Time.</span></span></p><p id="viewer-cfvr0"><span><span><em><strong>PM Boris Johnson also congratulated Joe Biden on his win</strong></em></span></span></p><div id="viewer-bq7va"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img"><p><img data-pin-url="https://www.ayrshiredailynews.co.uk/post/south-ayrshire-golf-club-owner-loses-2020-presidential-election" data-pin-media="https://static.wixstatic.com/media/bc51f5_3155158b89b94139b5a3131dfeb4df65~mv2.jpg/v1/fit/w_680%2Ch_383%2Cal_c%2Cq_80/file.png" src="https://static.wixstatic.com/media/bc51f5_3155158b89b94139b5a3131dfeb4df65~mv2.jpg/v1/fit/w_300,h_300,al_c,q_5/file.jpg"></p></div></div></div></div></div></div></div></div></article></div></div>]]>
            </description>
            <link>https://www.ayrshiredailynews.co.uk/post/south-ayrshire-golf-club-owner-loses-2020-presidential-election</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026202</guid>
            <pubDate>Sun, 08 Nov 2020 15:09:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[BGP Lego Bricks]]>
            </title>
            <description>
<![CDATA[
Score 86 | Comments 6 (<a href="https://news.ycombinator.com/item?id=25026189">thread link</a>) | @bitcynth
<br/>
November 8, 2020 | https://blog.cynthia.re/post/bgp-lego | <a href="https://web.archive.org/web/*/https://blog.cynthia.re/post/bgp-lego">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<h3>Nov 8 2020</h3>


<p>Since I was 7 years old, I have wanted a LEGO Mindstorms robot. I will admit that in the later years it was mainly that I wanted to prove to myself that I got somewhere as it seemed so infinitely expensive back when I was younger…</p>

<p><a href="https://twitter.com/bitcynth/status/1319956429252513792" title="Tweet about it"><img src="https://blog.cynthia.re/asset/g1ZkG7sMAD" alt="LEGO Robot Inventor"></a></p>

<p>Well I finally got one!</p>

<p>I knew this thing had some capability to run <a href="https://micropython.org/">MicroPython</a> since that was mentioned in the product description quite predominantly, which sounded like a pretty hackable platform.</p>

<p>However, shortly after opening the box, I realized that this wasn’t at all just an update to the <a href="https://www.lego.com/en-us/product/lego-mindstorms-ev3-31313">Mindstorms EV3</a>. Apparently the Robot Inventor was a slightly more expensive thing with less features and using a LiPo battery instead of AA cells.</p>

<p>Luckily I ordered it via the store’s website which means that by law I can return it within 14 days for any reason, and it allows me to open the box for inspection.</p>

<p>So after convincing some stubborn store employee that this is not a consumable product (which would remove my right to open the box), I got it returned and ordered the EV3.</p>

<p>Then two days later I finally had the thing which I have wanted for so long.</p>

<p>While I was waiting for the EV3, I was looking around at how it did MicroPython as it had this paragraph on the downloads page.</p>

<pre><code>You can now use your EV3 Brick to unleash the power of Python programming using MicroPython. Simply install the EV3 MicroPython image onto any micro SD card and boot up your EV3 Brick from it to start programming straight away. 
</code></pre>

<p>After just a few minutes of searching I found <a href="https://www.ev3dev.org/">ev3dev</a> which is an open source community project that allows you to run Debian on it!</p>

<p>It appears to be really well made and apparently good enough for LEGO to provide <a href="https://education.lego.com/en-us/support/mindstorms-ev3/python-for-ev3">official builds</a> of it, which is what they use for MicroPython support.</p>

<p>So I just went ahead and downloaded and flashed the disk image to a microSD card with <code>dd</code>.
I then plugged in the microSD card into the EV3 controller and turned it on…</p>

<p><img src="https://blog.cynthia.re/asset/rC3JqPiBNA" alt="EV3 kernel dmesg"></p>

<p>To my surprise, after applying power I was pleased to see kernel dmesg output scrolling out on the LCD of the EV3 on the first try!</p>

<pre><code>robot@ev3dev:~$ uname -a
Linux ev3dev 4.14.117-ev3dev-2.3.5-ev3 #1 PREEMPT Sat Mar 7 12:54:39 CST 2020 armv5tejl GNU/Linux

robot@ev3dev:~$ cat /etc/os-release 
PRETTY_NAME="ev3dev-stretch"
NAME="ev3dev-stretch"
ID=ev3dev
ID_LIKE=debian
HOME_URL="http://www.ev3dev.org"
SUPPORT_URL="http://www.ev3dev.org/support"
BUG_REPORT_URL="https://github.com/ev3dev/ev3dev/issues"
</code></pre>

<p>After playing around with it for about two minutes, I realized something… this thing runs Debian with a pretty damn complete package repository (it has ~64000 packages and normal amd64 Debian has ~66000 packages), can this thing run BIRD?</p>

<p>I assumed that if the ARM926EJ-S CPU can run Debian and stuff it can probably run a very minimal BIRD config, however I wasn’t so sure about the 56MB of RAM.</p>

<p>But well you won’t know if you don’t try :p</p>

<p>So I just went ahead and typed <code>sudo apt install bird</code> and pressed enter…</p>

<pre><code>robot@ev3dev:~$ sudo apt install bird
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Suggested packages:
  bird-doc
</code></pre>

<p>(this was actually painfully slow as this thing doesn’t have a powerful CPU in the slightest)</p>

<p>As soon as I saw <code>bird-doc</code> I realized that yes it did at least have BIRD in the repo so that was a good first step.</p>

<p>I am only installing BIRD 1.6.3 here as it will probably use less resources than BIRD 2 if I only run the IPv6 daemon and disable the IPv4 daemon.</p>

<p>I then wrote a <a href="https://blog.cynthia.re/asset/gv3GvwzBGH">quick little bird config</a> to let the EV3 (AS202314) announce 2a0d:1a45:666::/48 to my home router (AS210089), and the home router will then deal with the rest.</p>

<p>I applied that and wrote the other end of the config on my home router and applied that too.</p>

<p>Then I went to the <a href="http://lg.ring.nlnog.net/">NLNOG RING Looking Glass</a> and I saw the /48 show up as announced by AS202314!</p>

<p><img src="https://blog.cynthia.re/asset/KVCU9bynjB" alt="NLNOG RING Looking Glass"></p>

<p>I then sent it to a friend who pointed out only about half of his sources could see the route, and well turns out I forgot to give AS202314 an <a href="https://en.wikipedia.org/wiki/Resource_Public_Key_Infrastructure">RPKI ROA</a> for the prefix, so I added that.</p>

<p>Then just for demo purposes I installed nginx which worked surprisingly well, so then I had a website I could access at http://[2a0d:1a45:666::]/ <sup><a href="#fn1">[1]</a></sup>.</p>

<p>This was surprisingly easy to get working. Hats off to the <a href="https://www.ev3dev.org/">ev3dev</a> people for making this Debian derivative for the EV3.</p>

<p>While having the EV3 sitting there announcing a /48 of IPv6 is not useful in itself, this shows off how flexible the platform is. The fact that I can do this silly thing means that I can do much cooler things that don’t involve BGP on this, like running web servers and other things.</p>

<hr>

<p>If this was to your liking then maybe you will find my other posts interesting, and you can also find my smaller projects and ramblings on twitter: <a href="https://twitter.com/bitcynth">@bitcynth</a>.</p>

<p>Thank you to <a href="https://twitter.com/Benjojo12">Ben Cox</a> and <a href="https://www.mollymiller.net/">Molly Miller</a> for the help in editing this blog post.</p>

<p><sup><a name="fn1">[1]</a></sup>: no longer online</p>

</div></div>]]>
            </description>
            <link>https://blog.cynthia.re/post/bgp-lego</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026189</guid>
            <pubDate>Sun, 08 Nov 2020 15:06:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Writing for Reasons]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026099">thread link</a>) | @sulami
<br/>
November 8, 2020 | https://blog.sulami.xyz/posts/writing-for-reasons/ | <a href="https://web.archive.org/web/*/https://blog.sulami.xyz/posts/writing-for-reasons/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
            <p>This year, I have been writing more than even before over. In this article, I would like to discuss some of the reasons for writing and provide some thoughts on each.</p>
<h2 id="writing-to-remember">Writing to Remember</h2>
<p>This is probably the most obvious reason to write for a lot of people. Having written down a piece of information, you can come back later and recall it. Historical context can be invaluable for decision making, and often covers information that is not readily available anymore.</p>
<p>The key here is being able to find notes later on. Paper-based ones can be sorted by topic or chronologically, digital ones can be searched for.<span><label for="sn-1"></label><span>As an aside, I find paper notebooks really clumsy in this regard. They make a decent “staging area” to quickly capture information, but are terrible to find anything in unless you take care to maintain an index. Index cards can at least be reordered instead.</span></span> Formats can be useful here too, for example by supporting embedded code blocks or graphics.</p>
<h2 id="writing-to-solve-problems">Writing to Solve Problems</h2>
<p>Early this year, before the pandemic hit Europe, I saw Paulus Esterhazy’s talk <em><a href="https://www.youtube.com/watch?v=T7-2DW-KDV4&amp;t=1429s">Angels Singing: Writing for Programmers</a></em> at <a href="https://clojured.de/">clojureD</a>. It contained this great quote of Milton Friedman:</p>
<blockquote>
<p>If you cannot state a proposition clearly and unambiguously, you do not understand it.</p>
</blockquote>
<p>In <a href="https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/HammockDrivenDev.md">another talk</a>, Rich Hickey explained his notion of using notes as an extension of his working memory:</p>
<blockquote>
<p>So we have a problem, in general, because we’re just being asked to write software that’s more and more complex as time goes by. And we know there’s a 7 +/- 2 sort of working memory limit and as smart as any of us are, we all suffer from the same limit but the problems that we are called upon to solve are much bigger than that. So what do we do if we can’t think the whole thing in our head at the same time? How can we work on a problem with more than nine components. What I’m going to recommend is that you write all the bits down.</p>
<p>[…]</p>
<p>But if we look at the 7 +/- 2 thing, we could say we can juggle seven to nine balls but if you can imagine having an assistant who every now and then can take one of those out and put a different color in and you can juggle balls of 20 different colors at the same time as long as there are only nine in the air at any one point in time. And that’s what you’re doing, you’re going to sort of look around at all these pieces and shift arbitrary shapes of seven into your head at different points in time.</p>
</blockquote>
<p>Writing everything down allows digging deep into details and going off on tangents, and then returning to other aspects. As an added bonus, these notes can be useful in the future as well, if archived properly. I found <a href="https://orgmode.org/features.html">org-mode</a> outlines incredibly powerful for this purpose, with their foldable, tree-like structure that allows nesting sub-problems.</p>
<h2 id="writing-to-make-decisions">Writing to Make Decisions</h2>
<p>Writing is invaluable for decision making. Not only does it aid the decision process (see above), it also allows returning to a decision later and reviewing it.</p>
<p><a href="https://github.com/joelparkerhenderson/architecture_decision_record">Architecture decision records (ADRs)</a> are a tool established just for this purpose. The exact formats vary, and the details do not matter too much, but here are a few key points I consider essential:</p>
<ul>
<li>The motivation for the decision</li>
<li>The constraints involved</li>
<li>The alternatives to consider and their respective tradeoffs</li>
</ul>
<p>All of these are useful in several ways: they force you to acknowledge the components of the decision, make it simple to get an opinion on the matter from someone else, and also allow you to review the (potentially bad) decision later on.</p>
<p>There is one more point: the conclusion. This is easy to forget, because once a conclusion is reached, no one wants to spend time writing it down. But if you do not write it down, the document does not tell the whole story if reviewed in the future.</p>
<h2 id="writing-to-develop-ideas">Writing to Develop Ideas</h2>
<p>This year I have seen a lot of people writing about Sönke Ahrens’ <a href="https://takesmartnotes.com/"><em>How to Take Smart Notes</em></a>, which is about taking notes as a means to develop long form writing. It popularised the idea of the <em>Zettelkasten</em>, a physical or virtual box of notes which reference each other to build an information network.</p>
<p>While I found the book quite interesting, I would not recommend it to everyone due to the significant organisation overhead involved.<span><label for="sn-2"></label><span>I think researchers and writers can gain a lot from this method, but others not so much. Of course, if you want to read the book, feel free to do so. It is an interesting read, and I wouldn’t call it overrated by any means.</span></span></p>
<p>That being said, I believe that if you have a digital system which can provide automatic back-links to avoid the exponentially growing amount of manual maintenance required, there is little harm in linking notes.<span><label for="sn-3"></label><span><a href="https://roamresearch.com/">Roam</a>, <a href="https://notion.so/">Notion</a>, <a href="https://obsidian.md/">Obsidian</a>, and <a href="https://github.com/Kungsgeten/org-brain">many others</a> can do so.</span></span> At the very least it will make it easier to find a note, and maybe it can aid the thinking process by exposing previously unseen connections between concepts.</p>
<h2 id="writing-to-communicate">Writing to Communicate</h2>
<p>This very article was written expressively to communicate information, and as such required some extra work for it to be effective.</p>
<p>The most important factor when writing for communication is the target audience. It dictates the format to use, and which prior knowledge can be assumed. Maximising information density by being as concise as possible is important to avoid wasting the reader’s time.</p>
<p>As an added difficulty, when writing something to be published you need to get it right the first time, there is no channel for discussing follow-up questions. The old adage in writing is “writing is rewriting”, and I very much believe that to be true in this case. Write an outline, then a first draft, then keep reading and revising it until it is just right. Maybe show it to someone you trust for feedback.</p>
<p>I personally also like to leave a draft and come back a few weeks later. This way I always have a few drafts for new articles ready for revision, until I feel that one is ready for publishing.</p>
                </section></div>]]>
            </description>
            <link>https://blog.sulami.xyz/posts/writing-for-reasons/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026099</guid>
            <pubDate>Sun, 08 Nov 2020 14:49:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The downfall of command and control data leadership]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026076">thread link</a>) | @RobinL
<br/>
November 8, 2020 | https://www.robinlinacre.com/command_control/ | <a href="https://web.archive.org/web/*/https://www.robinlinacre.com/command_control/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Originally posted: <!-- -->2020-11-07<!-- -->. <!-- -->View source code for this page<!-- --> <a href="https://github.com/RobinL/robinl.github.io/blob/dev/src/mdx/command_control.mdx">here</a>.</p><p>The phenomenal success of data driven businesses like Google and Amazon has led to increased recognition of the value of data, and a corresponding desire for new investment.</p><p>As a result, seemingly every big organisation has a new data platform that’s always just around the corner — a platform which will finally impose order on the organisation’s data and allow it to realise its data driven ambitions.  This vision of a step change in data capability is closely linked to a ‘command and control’ style of data leadership whereby solutions are designed and agreed by a small group of experts.</p><p>These platforms - if they arrive at all - never seem to deliver the step change that is promised.  Why is this such a common problem, and how can organisations realise more value from their data without falling into the ‘big bang platform’ trap?</p><h2>A Dangerous Narrative</h2><p>A root cause of later problems is the tension between an appealing narrative and a deliverable plan.  The politics of organisations tend to push towards an ambitious vision and a compelling narrative - and since senior decision makers rarely have deep experience of delivering data platforms, they can struggle to offer effective scrutiny.</p><p>The narrative usually centres around fixing the data mess once and for all by creating a new, beautifully curated data platform that contains a single version of truth for all the organisation’s data.  I think this is a compelling story because it appeals to a basic human instinct for order - capitalising on the same feeling we get when browsing the Ikea catalogue.</p><p>Unfortunately, this vision is usually undeliverable.</p><h2>Why it fails</h2><h3>The benefits are too vague</h3><p>The ultimate goal is not to deliver a platform but to enable the organisation to derive more value from its data.  Too much weight is often placed on the role of the platform in unlocking value, and this distracts from detailed scrutiny of how value will be realised.  For example, if operational efficiencies are expected, how much are these worth, and how does the platform help?  Could these benefits be delivered with current infrastructure?  It’s too often assumed that a better data platform will inevitably result in business value without enough focus on specific business problems and quantifiable benefits.</p><p>An example that illustrates the problem is the idea of the single version of the truth, which is often a key selling point of a new platform, and a big part of the ‘cleaning up the mess’ narrative.  The promise is that this will reduce complexity and duplication of work and eliminate inconsistency.  The narrative is compelling because it’s usually easy to find examples of different parts of the organisation using mutually inconsistent data.</p><p>Whilst opportunities for consolidation usually do exist, the benefits of the single version of truth are usually oversold, and many of the blockers are not due to the lack of a new platform.</p><p>There are legitimate reasons for holding different measures of the same thing.  A high profile example is the challenge of <a href="https://www.cebm.net/covid-19/public-health-england-death-data-revised/">defining the number of deaths from COVID</a>: various different definitions may be appropriate depending on the use of the statistics.</p><p>These competing user needs create tensions between the the vision and the delivery of value.  The vision promises to deliver simplicity, but this simplicity can only come at the cost of reducing the value of data to some customers.</p><h3>The delivery approach is flawed</h3><p>The vision of the ‘big bang’ platform implies some type of top-down design - a style of data leadership in which a small group of experts decide what’s best.  This could include decisions about what data to store, how to organise the data, how things should be measured and the tools available to users.</p><p>There may be wide consultation, but the assumption is that a few common solutions and patterns are used by everyone.  This simplification is an important part of the vision.</p><p>This fails to account for the complexity of real-world data.  Deep expertise about the organisation’s data generally sits relatively low down the hierarchy, with many details not written down.  It can take many months of working with a data source to understand its thorniest challenges.</p><p>This information is not something that can be distilled into a few interviews; and more generally the overwhelming complexity of big organisations’ data makes it extremely difficult to corral into a single new architecture.</p><p>Faced with the high complexity of existing systems, it's also too easy to assume that this is the result of a lack of expertise or tooling. Whilst this may be part of the story, the reality is a lot more nuanced, involving issues like underlying data quality, user needs, staff skills, or organisational culture that a new platform cannot usually solve.</p><p>As a result, whilst a top down design may superficially accommodate each dataset, it is unlikely to meet real-world user needs.  Information simply cannot flow fast enough between planners and implementers to make it work - and attempts to gather enough information can result in suffocating governance requirements.</p><h2>The role of data leaders</h2><p>If centrally designed platforms and a command-and-control style of data leadership don’t work, what does?</p><p>In a nutshell, the recipe for success is to have small delivery teams working on specific problems with well-defined business value. Data leaders should explicitly recognise that the organisation’s data problems are too complex for a small group of people to understand and solve. Teams should therefore be empowered by a flexible range of tools, and are given the freedom to find innovative solutions themselves. This provides a sustainable model for the delivery of real business value and continuous improvement.</p><p>There is still a huge role for data leaders in this delivery model, but the focus is more about creating the right conditions for success rather than designing solutions.  The key elements are as follows:</p><ul><li><p><strong>Understanding value and prioritising work.    </strong>This means managing the portfolio of work to make sure it’s focussed on areas that have tangible business value and have a realistic prospect of success.  It also means being explicit about the tradeoffs between delivery speed and perfection and defending these choices to stakeholders.</p></li><li><p><strong>Empowering teams to solve problems. </strong>  A data platform is important, but its role should be to give flexibility to users and remove barriers to data flow, not to provide cookie-cutter solutions to all problems.  The platform should empower users to try a range of different tools and approaches to help them find the best fit for their problem.  This is a critical driver of continuous innovation in a world where data tooling is rapidly evolving.</p></li><li><p><strong>Coordinate teams and drive adoption of good ideas.  </strong>Whilst individual teams should be given considerable flexibility, there is an important role for data leadership in coordinating teams and promoting information flow between them.  This is a delicate balance.  It involves encouraging ground rules and principles to emerge, and promoting (and occasionally enforcing) best practice, but falls short of designing and imposing solutions. Solutions themselves will generally originate in delivery teams, with the role of data leaders to recognise quality and encourage adoption amongst other teams.</p><p>The purpose of ground rules is to encourage transparency and re-use without significantly harming flexibility.  For example, one ground rule may be that there should be no data without metadata, and that metadata should be held in a consistent, open-source, machine readable format.  This enables coordination between teams whilst imposing very little constraint on how individual teams solve their problems.</p><p>Another principle may be of transparency and reproducibility, with a ground rule that all code should be stored in the same version control system.  This allows teams to easily see what each other are doing.</p><p>If this approach is successful, it should be very rare to need to force teams to use particular tools or implement principles. This should only happen after significant reflection on why adoption has not occurred.  Enforcement should only be used where there is ample proof of a tool or rule working in similar contexts, and a clear explanation for why the existing approach is harmful.</p></li></ul><h2>Does it work?</h2><p>Having experienced both styles of leadership,  I have been able to observe the difference in outcomes.  I am lucky to currently work on a delivery team where we are empowered to find our own solutions, and what I see is a huge amount of innovation, leading to a rapid, sustained improvement in data capability, and highly motivated staff who love their work.</p></div></div>]]>
            </description>
            <link>https://www.robinlinacre.com/command_control/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026076</guid>
            <pubDate>Sun, 08 Nov 2020 14:45:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[macOS file diff app, Kaleidoscope, acquired by Letter Opener GmbH]]>
            </title>
            <description>
<![CDATA[
Score 84 | Comments 66 (<a href="https://news.ycombinator.com/item?id=25026070">thread link</a>) | @dplgk
<br/>
November 8, 2020 | https://kaleidoscope.app/release-notes | <a href="https://web.archive.org/web/*/https://kaleidoscope.app/release-notes">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="macos"><h3>Kaleidoscope 2.3.4<p>November 6 2020</p></h3><p>build 1444 (Direct)</p><ul><li>Fixed some potential bugs hiding behind warnings.</li><li>Fixed missing update functionality of version 2.3.2.</li><li><strong>Note:</strong> If you installed version 2.3.2 please download manually the latest version.</li></ul><h3>Kaleidoscope 2.3.3<p>October 24 2020</p></h3><p>build 1443.1 (Mac App Store)</p><ul><li>Update contact information to support@kaleidoscope.app, don't be shy and say hi!</li><li>Fixed newsletter sign-up in the Help &gt; Getting Started window.</li></ul><h3>Change of ownership<p>October 9 2020</p></h3><ul><li>Letter Opener GmbH acquired Kaleidoscope from Hypergiant LLC.</li></ul><h3>Kaleidoscope 2.3.2<p>April 17 2020</p></h3><p>build 1442 (Direct) / 1442.1 (Mac App Store)</p><ul><li><strong>Note:</strong> build 1442 (Direct) was pulled because it is missing update functionality. If you installed version 2.3.2 please download manually the latest version.</li><li>Fixed bug in integration window</li></ul><h3>Kaleidoscope 2.3.1<p>April 7 2020</p></h3><p>build 1441 (Direct) / 1441.2 (Mac App Store)</p><ul><li>Fixed a crash caused by opening a zero-length text file</li><li>Improved scrolling performance when using Find to search for text</li><li>Minor bug fixes</li></ul><h3>Kaleidoscope 2.3<p>February 21 2020</p></h3><p>build 1438 (Direct)</p><ul><li>Improved macOS Catalina compatibility</li><li>Notarized builds for improved security</li><li>Fixed blurred scrollbar endcap issue on Retina displays</li><li>Updated crash analytics package</li><li>Minor bug fixes</li></ul><h3>Kaleidoscope 2.2.2<p>November 7 2017</p></h3><p>build 1376 (Direct) / 1376.01 (Mac App Store)</p><ul><li>Bug Fixes.</li></ul><h3>Kaleidoscope 2.2.1<p>August 9 2017</p></h3><p>build 1158 (Direct) / 1158.01 (Mac App Store)</p><ul><li><strong>Note:</strong> Kaleidoscope now requires macOS 10.12 or above.</li><ul><li>Fixed crashes related to future macOS releases.</li><li>Fixed an issue where the user is unnecessarily prompted to update ksdiff.</li><li>Updated documentation.</li><li>Added analytics to help our developers improve future releases.</li></ul></ul><h3>Kaleidoscope 2.2<p>May 3 2017</p></h3><p>build 439 (Direct) / 439.01 (Mac App Store)</p><ul><li><strong>New:</strong> Added support for recent macOS updates.</li><ul><li>Overhauled the interface to better reflect the contemporary Mac environment.</li><li>Added stability with multiple under-the-hood improvements.</li><li>Modernized the codebase to make future work more manageable.</li><li>Fixed various issues related to macOS Sierra.</li></ul></ul><h3>Kaleidoscope 2.1.1<p>June 9 2015</p></h3><p>build 219 (Direct)</p><ul><li><strong>Note:</strong> We are working to get version 2.1.1 into the Mac App Store. For now, please <a href="http://www.kaleidoscopeapp.com/download">download</a> the direct sale version. Your purchase will carry over.</li><li>Fixed a couple issues with our Bazaar integration instructions.</li><li>⌘-D now triggers the Don't Resolve button when dismissing a merge warning.</li><li>Improved automatic graphics switching support (Early 2011 or newer MacBooks Pro): Kaleidoscope will now only use the discrete GPU when necessary.</li><li>Updated our mechanism for purchasing a Kaleidoscope registration.</li></ul><h3>Kaleidoscope 2.1<p>April 30 2014</p></h3><p>build 134 (Direct) / 133.01 (Mac App Store)</p><ul><li><strong>New Feature:</strong> Added support for ignoring whitespace (leading, trailing and line ending) in text comparisons.</li><li><strong>New Feature:</strong> Added an indicator to display remaining unresolved conflicts in a merge document.</li><li><strong>Text Scope</strong><ul><li>Added dropdown menus on either side of Choose Left/Choose Right buttons to make “Choose Both” options more discoverable.</li><li>Added better tooltips for the “Copy to” buttons when in Unified view.</li><li>Fixed various issues with Dark Theme which made text difficult to read.</li><li>Fixed issue where selecting different text scope views on one window could affect copy right/left buttons on other windows.</li><li>Fixed issue where holding option to modify the behavior of copy right/left buttons on one window could affect other windows.</li><li>Fixed issue that could prevent Kaleidoscope from picking up changes made to a document open in more than one window.</li><li>Fixed issue that could prevent Kaleidoscope from picking up changes made to documents externally, especially on the MAS build.</li></ul></li><li><strong>Folder Scope</strong><ul><li>Fixed issue where sometimes Folder Scope copies would not show up correctly after the copy had taken place.</li><li>Fixed issue where Folder Scope would not pick up external additions of empty files or directories.</li><li>Fixed issue that caused the app to reject dragging of folders to the dock icon.</li></ul></li><li><strong>Integration</strong><ul><li>Fixed issue that caused git integration to fail on 10.9 Mavericks.</li><li>Fixed issue where ksdiff was sometimes not able to connect to Kaleidoscope after reboots with window restoration enabled.</li><li>Fixed issue that where Kaleidoscope would not allow quitting when choosing “Review Conflicts” on a modified document.</li></ul></li><li><strong>General Improvements</strong><ul><li>Updated Automator actions to categorize correctly in Automator.</li><li>Added support for copy/paste shortcuts in the crash reporter window.</li><li>Kaleidoscope now avoids saving files without changes.</li><li>Kaleidoscope will now disallow edits to files that can be read but not written to (e.g. docx files).</li><li>Kaleidoscope now better remembers size and position of your windows.</li><li>Fixed issue that stopped the comparison windows from minimizing when double clicking their title bar.</li><li>Fixed issue where the path bar area could fail to update correctly when switching tabs.</li><li>Fixed issue where clicking the dock icon would not restore minimized documents.</li><li>Fixed issue where dragging a group of files that were already open in Kaleidoscope could cause issues resulting in not all new files being added.</li><li>Fixed issue that made it possible for the comparison window to grow vertically offscreen on 10.9 leaving you with a window you could not reposition afterwards.</li><li>Fixed issue that made it impossible to bring up the open dialog by clicking on an empty tab when fullscreen in 10.9.</li><li>Fixed issue where sometimes full-screen windows would not be full-screen.</li><li>Fixed small visual issues with the Ignored Files dialog window.</li><li>Fixed documentation issues with ksdiff help.</li><li>Improved Help Documentation.</li><li>Various performance and stability fixes.</li></ul></li></ul><h3>Kaleidoscope 2.0.2<p>October 23 2013</p></h3><p>build 116</p><ul><li>Improved compatibility with OS X 10.9 Mavericks</li><li>Improved stability</li></ul><h3>Kaleidoscope 2.0.1<p>February 19 2013</p></h3><p>build 114</p><ul><li><strong>Text Scope</strong><ul><li>Tweaked the visual appearance of the change count stepper in Text Scope.</li><li>Fixed the "Reset Selection" menu item in Text Scope to enable and disable properly.</li><li>The Save menu is now properly disabled when comparing text snippets.</li><li>Fixed a bug where the summary text in document titles and tabs might not properly update.</li><li>The Resolved document in Three Way Blocks now has better alignment with similar content in A and B.</li><li>Kaleidoscope can now properly diff .textClipping documents.</li></ul></li><li><strong>Folder Scope</strong><ul><li>User-defined system date formats will now be properly used.</li><li>Fixed a bug that prevented Folder Scope from having the correct keyboard focus by default.</li></ul></li><li><strong>Image Scope</strong><ul><li>Kaleidoscope now handles different color spaces more reliably in Image Scope.</li><li>Kaleidoscope now properly accounts for camera orientation when displaying images in Image Scope.</li></ul></li><li><strong>Changesets</strong><ul><li>Improved keyboard navigation in changesets.</li><li>Unsaved files will now be properly marked as dirty in changesets.</li><li>Changesets now properly select the list of files on the left when opening, allowing you to quickly review changes.</li></ul></li><li><strong>General Improvements</strong><ul><li>Direct Sale fulfillment emails will now properly activate Kaleidoscope for users with diacritics in their names.</li><li>Kaleidoscope will no longer move itself to ~/Applications if that folder exists. It will now move to /Applications in all cases.</li><li>Fixed a bug that caused temporary licenses to expire one day earlier than they should have.</li><li>Fixed a bug that caused the corner radii of windows in Full Screen to not match.</li><li>Fixed an issue that sometimes led to poor vertical alignment in the File Shelf.</li><li>Dragging files to Kaleidoscope will properly open to a comparison document and will no longer leave the launch window open in the background.</li><li>Fixed a bug that caused accessing files from the Recents list to sometimes stop working.</li><li>Improved the messaging if Kaleidoscope is unable to open a document that was previously available via AFP.</li></ul></li></ul><h3>Kaleidoscope 2.0<p>January 17, 2013</p></h3><p>build 107</p><ul><li>Resets trial period for users whose trial period expired during beta</li></ul><h3>Kaleidoscope 2.0<p>January 17, 2013</p></h3><p>build 104</p><ul><li><strong>New Feature:</strong> Added support for merging text documents using a Two-Way interface in Text Scope.</li><li><strong>New Feature:</strong> Added version control integration for merging and resolving conflicts using a Three-Way interface in Text Scope.</li><li><strong>New Feature:</strong> Folder Scope — now you can spot the differences between folders and copy files and folders between them. Double click any row to open a new comparison and look at any pair of files or folders more closely.</li><li><strong>New Feature:</strong> Kaleidoscope Snippets and Services</li><ul><li>You can now drag text and images directly into the Kaleidoscope window, or the Kaleidoscope dock icon, to create Snippets. This lets you quickly compare content without having to save and name files. Try dragging images or text directly from Safari or an email message!</li><li>Kaleidoscope now includes OS X System Services to make you more productive. They are enabled by default, but you can manually turn them On or Off in the Keyboard section of System Preferences. You can also set global keyboard shortcuts for them in the Keyboard pane of System Preferences if you want to get to these even faster.</li><li>Open in Kaleidoscope: Right click on any files or folders in Finder, and compare them in a single Kaleidoscope tab. This is the easiest way to compare folders!</li><li>Text and Image Compare: Right click on text or images and send them directly to Kaleidoscope as Snippets. Try this by selecting and right clicking on any text in TextEdit, then select “Compare Text in Kaleidoscope” from the Services menu.</li></ul><li><strong>New Feature:</strong> Clipboard Support</li><ul><li>You can use the new "Edit -&gt; Paste as File" and "File -&gt; New from Clipboard" menu items to compare directly from the Clipboard. This works similarly to the drag and drop Snippets functionality. Use this to quickly create a new comparison document or to add existing text or images to an open document.</li></ul><li><strong>New Feature:</strong> Kaleidoscope now supports resolving merge conflicts for images.</li><li><strong>New Feature:</strong> Added support for Full Screen on Lion and Mountain Lion.</li><li>Full support for Macs with Retina displays.</li><li>Substantially updated and modernized user interface.</li><li>Added support for sending arbitrary changesets and partial changsets with ksdiff.</li><li>Added support for arbitrary merges and diffs using ksdiff.</li><li>Integration with third-party tools now requires installation of the ksdiff command-line tool from the Integration …</li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://kaleidoscope.app/release-notes">https://kaleidoscope.app/release-notes</a></em></p>]]>
            </description>
            <link>https://kaleidoscope.app/release-notes</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026070</guid>
            <pubDate>Sun, 08 Nov 2020 14:44:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Collect Log for SIEM?]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026039">thread link</a>) | @gunal2
<br/>
November 8, 2020 | https://letsdefend.io/blog/how-to-collect-log-for-siem/?q=hackernews | <a href="https://web.archive.org/web/*/https://letsdefend.io/blog/how-to-collect-log-for-siem/?q=hackernews">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<div>

			
<h2>Log Collection</h2>



<p>It contains a basic log, time, source system and a message. For example, when we look at the content of the “/var/log/auth.log” file on an Ubuntu server, we can see the source, time and message information.</p>



<figure><img src="https://letsdefend.io/blog/wp-content/uploads/2020/11/log.png" alt="" srcset="https://letsdefend.io/blog/wp-content/uploads/2020/11/log.png 685w, https://letsdefend.io/blog/wp-content/uploads/2020/11/log-300x22.png 300w, https://letsdefend.io/blog/wp-content/uploads/2020/11/log-330x24.png 330w" sizes="(max-width: 685px) 100vw, 685px"></figure>



<p>Logs are generally collected in the following 2 ways:</p>



<ul><li>Log Agents</li><li>Agentless</li></ul>



<p>We created online lab for investigation SIEM alerts. If you are interested, you can try for free on <a href="https://letsdefend.io/" target="_blank" aria-label="undefined (opens in a new tab)" rel="noreferrer noopener">letsdefend.io</a></p>



<h3>Log Agents</h3>



<p>In order to implement this method, a log agent software is required. Agents often have parsing, log rotation, buffering, log integrity, encryption, conversion features. In other words, this agent software can take action on the logs it collects before forwarding them to the target.</p>



<p>For example, with the agent software, we can divide a log with “username: LetsDefend; account: Administrator” into 2 parts and forward it as:</p>



<ul><li>message1 = “username: LetsDefend”&nbsp;</li><li>message2 = “account: Administrator”</li></ul>



<p><strong>Syslog</strong></p>



<p>It is a very popular network protocol for log transfers. It can work with both UDP and TCP, and can optionally be encrypted with TLS. Some devices that support syslog: Switch, Router, IDS, Firewall, Linux, Mac, Windows devices can become syslog supported with additional software.</p>



<p>If you want to forward your log with Syslog, you will need to parsing in syslog format.</p>



<p>Syslog Format:</p>



<p>Timestamp – Source Device – Facility – Severity – Message Number – Message Text</p>



<figure><img src="https://letsdefend.io/blog/wp-content/uploads/2020/11/09fig02.gif" alt=""><figcaption><em>https://flylib.com/books/1/297/1/html/2/images/1587051583/graphics/09fig02.gif</em></figcaption></figure>



<p>Also, the maximum packet size that can be sent with Syslog UDP is 1024 bytes. For TCP it is 4096 bytes.</p>



<p><strong>3. Party Agents</strong></p>



<p>Most SIEM products have their own agent software. 3rd party agents have more capabilities than syslog because of the features they support. Some agents:</p>



<p><strong>Splunk: </strong>universal forwarder</p>



<p><strong>ArcSight</strong>: ArcSight Connectors</p>



<p>These agents are easy to integrate into SIEM and have parsing features.</p>



<p><strong>Open Source Agents</strong></p>



<p>They are generally agents that provide basic needs comfortably. However, it may not be as effective as the agent of the SIEM product itself. (Ease of installation, integration, additional features etc.)</p>



<p>Popular open source agents:</p>



<ul><li><a aria-label="undefined (opens in a new tab)" href="https://www.elastic.co/beats/" target="_blank" rel="noreferrer noopener">Beats </a></li><li><a aria-label="undefined (opens in a new tab)" href="https://nxlog.co/" target="_blank" rel="noreferrer noopener">NXLog </a></li></ul>



<h3>Agentless</h3>



<p>Agentless log sending process is sometimes preferred as there is no installation and update cost. Usually, logs are sent by connecting to the target with SSH or WMI.</p>



<p>For this method, the username and password of the log server are required, therefore there is a risk of the password being stolen.</p>



<p>Easier to prepare and manage than the agent method. However, it has limited capabilities and credentials are wrapped in the network.</p>



<h3>Manual Collection</h3>



<p>Sometimes there are logs that you cannot collect with existing agent software. For example, if you cannot read the logs of a cloud-based application with the agent, you may need to write your own script</p>



<h3>Summary</h3>



<p>As you can see, there are various ways to collect logs. These are agents and agentless. In cases where the agents on the market are not sufficient, you should write your own scripts.</p>





		</div><!-- .entry-content -->

	</div></div>]]>
            </description>
            <link>https://letsdefend.io/blog/how-to-collect-log-for-siem/?q=hackernews</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026039</guid>
            <pubDate>Sun, 08 Nov 2020 14:36:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The myriad meanings of pwd in Unix systems]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025830">thread link</a>) | @qmacro
<br/>
November 8, 2020 | https://qmacro.org/2020/11/08/the-meaning-of-pwd-in-unix-systems/ | <a href="https://web.archive.org/web/*/https://qmacro.org/2020/11/08/the-meaning-of-pwd-in-unix-systems/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><em>Last week I ran a poll on Twitter to see what people considered with respect to the meaning of ‘pwd’ in Unix and Linux systems. The results were varied, for perhaps good reason.</em></p>

<p>At the end of Oct 2020 I ran a <a href="https://twitter.com/qmacro/status/1322567992551624705">brief poll on Twitter</a>, on which 82 people voted. Here’s that poll, and the results. They’re quite mixed, which at first might seem surprising. But there are reasons for that, as we’ll find out.</p>

<p><img src="https://qmacro.org/content/images/2020/11/twitter-poll-pwd.png" alt="Poll on Twitter: &quot;Fun Saturday afternoon shell poll. In Unix (and Linux), what do you think the P in $PWD (or pwd) stand for?&quot;"></p>

<p><strong>Print working directory</strong></p>

<p>The most popular option was “print working directory”. At first sight it seems logical: “print out the current working directory, i.e. where I am right now”. Moreover, the description in various versions of the manual for <code>pwd</code> help to drive home that notion. Typically we see sentences like “<a href="https://linux.die.net/man/1/pwd">print name of current/working directory</a>” or “<a href="https://www.mankier.com/1/pwd">print the current directory</a>”.</p>

<p>But there are lots of commands that print stuff, and are described in that way too. Take the <code>id</code> command. Here’s what one man page says: “<a href="https://man7.org/linux/man-pages/man1/id.1.html">print real and effective user and group IDs</a>”. There’s “print” again. But the command isn’t <code>pid</code>, it’s <code>id</code>. When you think about it, many, many commands in Unix send information to STDOUT, i.e. to the terminal. That’s sort of the point of many of them.</p>

<p>This time arguably only superficially definitive, it would seem, the Wikipedia entry states, on the <a href="https://en.wikipedia.org/wiki/Pwd">page for <code>pwd</code></a>: “the pwd command (print working directory) writes the full pathname of the current working directory to the standard output”. As if to underline the hopeful authority of this statement, there are five (!) footnotes that supposedly link to resources that back this up.</p>

<p>Unfortunately, the first footnote points to a Wayback Machine copy of the <a href="https://web.archive.org/web/20050520231659/http://cm.bell-labs.com/7thEdMan/v7vol1.pdf">UNIX PROGRAMMERS MANUAL - Seventh Edition, Volume 1 - January, 1979</a>, wherein there are actually zero references to <code>pwd</code> being short for “print working directory”:</p>

<p><img src="https://qmacro.org/content/images/2020/11/programmers-manual-pwd.png" alt="excerpt from UNIX PROGRAMMERS MANUAL on pwd"></p>

<p>I don’t know about you, but this historic document carries more weight for me than other sources I’ve come across, and it only serves here to undermine the credibility of the Wikipedia entry.</p>

<p>The rest of the footnote links seem dubious at best, except for the one pointing to the <a href="https://www.gnu.org/software/coreutils/manual/coreutils.html#pwd-invocation">GNU Coreutils manual on pwd</a> which has it as “print working directory”. But everything else I’ve seen so far makes me think that this is a misunderstanding that has spread for obvious and innocent reasons. In addition, the one footnote in the Wikipedia page that is not used to back this claim up is a pointer to <a href="https://pubs.opengroup.org/onlinepubs/9699919799/utilities/pwd.html">The Open Group Base Specifications Issue 7, 2018 edition’s information on pwd</a>, which almost seems like it’s actually avoiding using the word “print” at all: “return working directory name” … “The pwd utility shall write to standard output an absolute pathname of the current working directory, which does not contain the filenames dot or dot-dot.”. Very specific, very not-print.</p>

<p>So I’m thinking that “print working directory” isn’t what <code>pwd</code> stands for. In fact, “print working directory” may be common to some man pages, but on this macOS machine, with its <a href="https://en.wikipedia.org/wiki/Berkeley_Software_Distribution">BSD</a> heritage, we have, instead: “pwd – return working directory name”. Moreover, it goes on to say “The pwd utility writes the absolute pathname of the current working directory to the standard output”.</p>

<p><strong>Pathname of working directory</strong></p>

<p>So perhaps it really is “pathname of working directory”. That would, at least to me, make more sense. Not only does it eschew the redundancy of “print”, it also is more specific about the output - if I’m in <code>/home/dja/</code> for example, then invoking pwd will tell me that, i.e. where I am, including the whole path, and not just <code>dja</code>:</p>



<p><strong>Process working directory</strong></p>

<p>As for the other options, I do favour “process working directory”, mostly because it makes a lot of sense to me; every process in Unix has the concept of a current working directory, and that’s exactly what I’m asking for when I’m in my shell process and enter <code>pwd</code> - there’s a part in the video <a href="https://youtu.be/hgFBRZmwpSM?t=165">Unix terminals and shells</a> that explains this very well.</p>

<p>I’d love to be able to point to some old Unix sources that definitively explain the answer, but unfortunately that search has come up with very little - the <code>pwd</code> source in both the <a href="https://minnie.tuhs.org/cgi-bin/utree.pl?file=V5/usr/source/s2/pwd.c">5th</a> and <a href="https://github.com/yisooan/unix-v6/blob/master/source/s2/pwd.c">6th</a> Editions of Unix shed no light on this whatsoever.</p>

<p><strong>Present working directory</strong></p>

<p>What about “present working directory”? Well, that option seems to have legs, in the form of the Korn shell. While <a href="https://northstar-www.dartmouth.edu/doc/solaris-forte/ipe-help/dbx/dbx88cc.html">one source</a> implies that the answer might well be “pathname of current working directory”, in that <code>pwd</code> just emits the value of the <code>$PWD</code> environment variable (and a variable called “print working directory” makes no sense at all) … it would seem that in ksh-land, at least, “present working directory” is what <code>pwd</code> represents. Take, for example, the <a href="https://osr507doc.xinuos.com/en/man/html.C/ksh.C.html">ksh man page</a> which states “PWD - The present working directory set by the cd command”.</p>

<p>There’s a ton of discussion, both direct and indirect, on this very question. Take for example these two entries in the Unix &amp; Linux Stack Exchange forum: <a href="https://unix.stackexchange.com/questions/399026/etymology-of-pwd">Etymology of $PWD</a> and <a href="https://unix.stackexchange.com/questions/174990/what-is-pwd-vs-current-working-directory">What is $PWD? (vs current working directory)</a>. Of course, perhaps the definitive answer will never be found, as computing history is nothing if not varied and prone to forking.</p>

<p><strong>Multics and print_wdir</strong></p>

<p>Talking of history, we could go further back to pre-Unix roots, in the form of Multics, which indirectly gave rise to Unix (originally “Unics”). In the <a href="https://multicians.org/multics-commands.html">list of Multics Commands</a>, we see, nestled amongst other similarly named commands, something that jumps out at us:</p>

<div><div><pre><code>print_mail (pm)	display mail in a mailbox
print_messages (pm)	display interactive messages in a mailbox
print_motd (pmotd)	display message of the day (source)
print_proc_auth (ppa)	display process's sensitivity level and compartments
print_request_types (prt)	display list of I/O daemon request types
print_search_paths (psp)	display search paths
print_search_rules (psr)	display ready messages
print_wdir (pwd)	display working directory
</code></pre></div></div>

<p>There’s <code>pwd</code>, and in fact, just like its sibling <code>pmotd</code>, for example, which is short for <code>print_motd</code>, it’s short for <code>print_wdir</code>. Now, given the context of the original poll being set to Unix and Linux, perhaps we must discount this information. But as someone who is fascinated with Unix history in general - how can I?</p>

<p>I guess there are few things to conclude. The history is rich and diverse, and maybe we’ll never know for sure. Perhaps, in fact, the answer will depend on whom we ask. In the grand scheme of things, it doesn’t really matter … but to those who delight in minutiae, it’s a fun topic worth exploring.</p>

  </div>

</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://qmacro.org/2020/11/08/the-meaning-of-pwd-in-unix-systems/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025830</guid>
            <pubDate>Sun, 08 Nov 2020 13:56:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Can we talk about failure?]]>
            </title>
            <description>
<![CDATA[
Score 18 | Comments 13 (<a href="https://news.ycombinator.com/item?id=25025819">thread link</a>) | @mqsley
<br/>
November 8, 2020 | https://www.mquinn.online/blog/can-we-talk-about-failure | <a href="https://web.archive.org/web/*/https://www.mquinn.online/blog/can-we-talk-about-failure">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>For the longest time felt like I didn’t fail at things. I was brought up with the mindset that if something is doable, it cannot be hard. A mindset which compels you to find harder and harder challenges.</p><p>That all changed when Flex, the Techstars company I co-founded didn’t work out. It broke me.&nbsp;</p></div><div><p>What surprised me was that the network, “give first” and all of the opportunity that comes with it is great when you’re on the up but awkward, uncomfortable and at times “crickets” on the down.</p><p>Part of this I now realise is because when you’re in failure mode, you’re insufferable to everyone around you.</p><p>You carry this huge weight on your shoulders that you put on others every time you meet like a wounded animal, limping around looking to be put down.</p><p>I’m not sure if everyone has gone through this, but for me I suffered through thoughts like:<br>Will I be ever successful?<br>Will I ever be happy?<br>Will I ever have another good idea?<br>Was my idea good?<br>Can I have good Ideas?<br>Can I be good at anything?</p><p>I came to learn these can be called cognitive distortions, and the best way to stop them is to write them down and use reason and logic to disprove them.</p><p>Everyone just tells you its okay, put in their perspective, you’ll work it out - This whole experience was incredibly frustrating, how could a non founder possibly relate to the level of aspiration I had taken away from me. What is a well meaning attempt at helping often comes across as people shrugging you off.</p><p>There was a limit to what my coach could do because I convinced myself he hadn’t experienced it. How could he possibly understand what was left behind after this huge emotional debt bubble deflated.</p><p>I sought out others who’s companies had died, or who had been ousted from their own boards but it was hard to match with other failures because it was either a sufferfest of two people feeling sorry for themselves or one of you values success more than the other and you judge the others’ failure.</p><p>I didn't grieve, I tried to re-inflate the bubble inside me.</p><p>I didn’t get angry, I just wanted to get better, I just wanted to feel better without putting in the work.</p><p>I turned to reading, thinking that with over a century of modern business practices surely titans have recovered from this. Reading a gazillion business biographies, literature I now know to pop science schlop, did not help. In fact it made it worse.&nbsp;</p><p>I remember reading Lean In and it slid me into depression. How did Sheryl Sandberg career magically line up at the end? Was I on the right track? How do I get on track? Where is the track? What is the track?</p><p>I would openly talk about imposter syndrome to explain why I approached work and decisions the way I did, and came to learn that:</p><ul><li>Outside of tech many people have never heard of the concept<br></li><li>People who had, did not understand it<br></li><li>Many people thought imposter syndrome was in fact the Dunning Kruger effect and thus I was admitting to them I was a fraud.&nbsp;</li></ul><p>For me, the voices in my head eventually did die off. Normalcy came about a year later when I realised I was talking about and writing about my experiences from a perspective of wisdom rather than re-hashing scar tissue.</p><p>Serendipitously after this point, I was gifted the book - <a href="https://www.amazon.com/When-Smart-People-Fail-Rebuilding/dp/0140178112" target="_blank">When Smart People Fail </a>by another Techstars founder whose grandmother had gifted her the book after she had experienced a set back. A set back which ultimately led to the founding of her Techstars company.</p><p>Where was this book when I failed? While a subjectively unnecessary read, I read it anyway. In my personal opinion, it was probably the best thing I’ve ever read on dealing with personal failure.</p><p>I think it would have saved me months of anguish had I read it earlier, and while I truly love helping entrepreneurs and makers - my goto advice now, when someone has experienced failure and is looking for guidance is - read this book.</p><p>I think Techstars should gift this to every founder that fails, whether they leave their company, are forced out or go insolvent.</p><p>Between stories of failure, to understanding the core stages of grief that one must go through - there is something in the book for everyone.&nbsp;</p></div><div><p>Key things the book taught me:</p><ul><li>Failure is subjective to the victim</li><li>Failure has stages of grief that you must go through - if you skip any, you will experience prolonged pain. I can first hand attest to this.</li><li>If we treat success as a binary outcome, everything we sacrifice in the pursuit of success is meaningless when we fail.</li></ul><p>The third point is something I embraced over the year that followed. Clearly others think I have experience/advice/wisdom/ideas to share so there is value there and it made it all much less painful. If success is binary, those of us who don’t make it, lose many years of our lives for nothing.</p><p>This is a stance I fully reject and I thank Jen and the book for really teaching me this.</p><p>In full realisation of this final point - <a href="https://www.mquinn.online/failures.html">I’m committing to my failures in public.</a> We’ll see how long I can keep it up for, but for every project that didn’t hit the goal, the ideas that crash landed and the products that went unloved I’m keeping a record because these failures are not a waste of life, they are the stepping stones that when looking back in 20 years will all make sense.</p><p>Show me someone without failure and I’ll show you someone without learning.&nbsp;</p></div><div><p>Thanks to everyone that listened to me moan for a year about my failures.</p><p>A special thanks to fellow Techstars founder Sarah Tuneberg who gave me perspective through the journey, <span>Techstars founder&nbsp;</span>Jen Saxton who gifted me “When smart people fail” and my wife for supporting me while I put myself back together.&nbsp;</p></div></div>]]>
            </description>
            <link>https://www.mquinn.online/blog/can-we-talk-about-failure</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025819</guid>
            <pubDate>Sun, 08 Nov 2020 13:54:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[IHP: Live Reloading Haskell Code, How It Works]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025774">thread link</a>) | @_query
<br/>
November 8, 2020 | https://ihp.digitallyinduced.com/blog/2020-08-10-ihp-live-reloading.html | <a href="https://web.archive.org/web/*/https://ihp.digitallyinduced.com/blog/2020-08-10-ihp-live-reloading.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <ul>
                <li>
                    <strong>Starting an IHP app</strong>

                    <p>
                        Let's first take a look at what happens when you start your IHP app by running <code>./start</code>.
                    </p>

                    <p>
                        The <a href="https://github.com/digitallyinduced/ihp-boilerplate/blob/master/start" target="_blank"><code>./start</code></a> script in your project is basically just a small wrapper that makes sure that all dependencies that are managed by nix are available and then calls the <code>RunDevServer</code> binary. <a href="https://github.com/digitallyinduced/ihp/blob/master/exe/IHP/IDE/DevServer.hs" target="_blank">This binary is part of IHP</a> and does the actual application startup as well as managing the postgres server.
                    </p>

                    <p>
                        You can think of <code>RunDevServer</code> as a small process manager. When <code>RunDevServer</code> is started, it will directly start all processes required for your application to run:
                        </p><ul>
                            <li>the web-based IDE</li>
                            <li>a status server on port 8000 showing the compiler status and possible type errors</li>
                            <li>a websocket server used for communicating file changes for live reloading</li>
                            <li>a ghci (the standard haskell REPL) where it directly loads your app</li>
                            <li>a postgres server bound to unix socket</li>
                            <li>a file watcher to track haskell and css file changes</li>
                        </ul>
                    

                    <p>
                        All these processes are started in parallel for fast performance and synchronised in the main event loop of the dev server. Made possible thanks to haskells great concurrency capabilities.
                    </p>

                    <p>
                        The most important process is the ghci process with your app. Instead of fully recompiling your app on every file change, IHP loads your app in the repl and then refreshes only the changed files. 
                    </p>

                    <p>
                        At first ghci needs a couple of seconds to load all haskell files of your application. While loading your application the status server is serving all http requests on localhost:8000. The status server will also show all type errors in case ghci failed to load your app. When the ghci finished loading, the status server is stopped and your application is started on localhost:8000.
                    </p>

                </li>

                <li>
                    <strong>Haskell File Changes</strong>

                    <p>
                        <iframe width="100%" height="500" src="https://www.youtube.com/embed/nTjjDo57B8g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
                    </p>

                    <p>
                        Once your application is started, the dev server mainly deals with file changes. Using a file watcher the dev server is notified about any changes to your haskell files in your project. When a haskell file is changed the app process running in the ghci process is stopped and a refresh (<code>:r</code>) is triggered. The refresh is usually very fast. Once completed the app server will be started again.
                    </p>

                    <p>
                        Once the haskell app has started, open browser pages will be notified using a websocket connection. The open pages will fetch the current page via ajax and will then update the dom using <a href="https://github.com/patrick-steele-idem/morphdom" target="_blank">a diff and patch approach</a>. So only dom nodes that have actually changed will be touched during the live reloading. This approach will keep existing page state like scroll position or text typed into a form field. It allows for a very fast and productive feedback cycle.
                    </p>
                </li>

                <li>
                    <strong>CSS File Changes</strong>

                    <blockquote><p lang="en" dir="ltr">Did you know: <a href="https://twitter.com/hashtag/ihp?src=hash&amp;ref_src=twsrc%5Etfw">#ihp</a> also supports live reloading of CSS. <a href="https://t.co/uRCCr3gkHz">pic.twitter.com/uRCCr3gkHz</a></p>— digitally induced (@digitallyinduce) <a href="https://twitter.com/digitallyinduce/status/1277583045919375363?ref_src=twsrc%5Etfw">June 29, 2020</a></blockquote> 
                    

                    <p>
                        IHP also supports live reloading of CSS files. Once IHP sees a file change to your CSS files in the <code>static</code> directory it will notify all open browser tabs using its websocket connection. Once notified in the browser IHP will look for any <code>&lt;link rel="stylesheet"&gt;</code> and <a href="https://github.com/digitallyinduced/ihp/blob/master/lib/IHP/static/livereload.js#L68" target="_blank">will reload the css file using a cache buster</a>.
                    </p>

                    <p>
                        This is only possible because nix allows us to pin down the set of package definitions to a specific git commit of the nix package registry.
                    </p>
                </li>

                <li>
                    <strong>Type Errors</strong>

                    <p>
                        Sometimes you make a change which will stop your application from compiling. In these error cases the status server jumps in and starts listening on localhost:8000. The status server will then display the error message so you can quickly fix it:
                    </p>

                    <img src="https://ihp.digitallyinduced.com/releases/v07082020.png" alt="Example of a status server error message">

                    <p>
                        Additionally open browser tabs will be notified about this and will refresh. This way the error is instantly visible to you.
                    </p>
                </li>

                <li>
                    <strong>Batteries-included</strong>

                    <p>
                        While the above steps are technically complicated, when doing actual development you will not see much of this complexity.  Lots of time have been spent to find the best approach and smoothing out all the edge cases. The whole process is very much inspired by PHP where you just make file changes and it works. The live reloading really <q>just works</q>.
                    </p>

                    <p>
                        The dev server is the heart of IHP and makes the dev process extremely productive. You get all the benefits of type-safety with the development speed you previously only got with scripting languages. If you haven't already it's time to try it out! 🚀
                    </p>
                </li>
            </ul>


        </div><p>Feel free to share this post on Twitter, Reddit, Hacker News or anywhere else on the internet :)</p></div>]]>
            </description>
            <link>https://ihp.digitallyinduced.com/blog/2020-08-10-ihp-live-reloading.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025774</guid>
            <pubDate>Sun, 08 Nov 2020 13:46:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Jaccard Similarity Coefficient and MinHash]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025683">thread link</a>) | @arpitbbhayani
<br/>
November 8, 2020 | https://arpitbhayani.me/blogs/jaccard-minhash | <a href="https://web.archive.org/web/*/https://arpitbhayani.me/blogs/jaccard-minhash">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>Set similarity measure finds its application spanning the Computer Science spectrum; some applications being - user segmentation, finding near-duplicate webpages/documents, clustering, recommendation generation, sequence alignment, and many more. In this essay, we take a detailed look into a set-similarity measure called - Jaccard's Similarity Coefficient and how its computation can be optimized using a neat technique called MinHash.</p>

<p>Jaccard Similarity Coefficient quantifies how similar two <em>finite</em> sets really are and is defined as the size of their intersection divided by the size of their union. This similarity measure is very intuitive and we can clearly see that it is a real-valued measure bounded in the interval <code>[0, 1]</code>.</p>
<p><img src="https://user-images.githubusercontent.com/4745789/98461673-302d7180-21d4-11eb-9722-41f473c1fe84.png" alt="https://user-images.githubusercontent.com/4745789/98461673-302d7180-21d4-11eb-9722-41f473c1fe84.png"></p>
<p>The coefficient is <code>0</code> when the two sets are mutually exclusive (disjoint) and it is <code>1</code> when the sets are equal. Below we see the one-line python function that computes this similarity measure.</p>
<pre><code><span><span>def</span> <span>similarity_jaccard</span><span>(a: set, b: set)</span> -&gt; float:</span>
    <span>return</span> len(a.intersection(b)) / len(a.union(b))
</code></pre>
<h2>Jaccard Similarity Coefficient as Probability</h2>
<p>Jaccard Coefficient can also be interpreted as the probability that an element picked at random from the universal set <code>U</code> is present in both sets <code>A</code> and <code>B</code>.</p>
<p><img src="https://user-images.githubusercontent.com/4745789/98462221-8dc3bd00-21d8-11eb-95bf-5a9267e88b97.png" alt="https://user-images.githubusercontent.com/4745789/98462221-8dc3bd00-21d8-11eb-95bf-5a9267e88b97.png"></p>
<p>Another analogy for this probability is the chances of throwing a dart and it hitting the intersection. Thus we see how we can transform the Jaccard Similarity Coefficient into a simple probability statement. This will come in very handy when we try to optimize the computation at scale.</p>
<h2>Problem at Scale</h2>
<p>Computing Jaccard Similarity Coefficient is very simple, all we require is a union operation and an intersection operation on the participating sets. But these computations go haywire when things run at scale.</p>
<p>Computing set similarity is usually a subproblem fitting in a bigger picture, for example, near-duplicate detection which finds near-duplicate articles across millions of documents. When we tokenize the documents and apply raw Jaccard Similarity Coefficient for every two combinations of documents we find that the computation will take <a href="https://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/">years</a>.</p>
<p>Instead of finding the true value for this coefficient, we can rely on an approximation if we can get a considerable speedup and this is where a technique called MinHash fits well.</p>

<p>MinHash algorithm gives us a fast approximation to the Jaccard Similarity Coefficient between any two finite sets. Instead of computing the unions and the intersections every single time, this method once creates <em>MinHash Signature</em> for each set and use it to approximate the coefficient.</p>
<h2>Computing single MinHash</h2>
<p>MinHash <code>h</code> of the set <code>S</code> is the index of the first element, from a permuted Universal Set, that is present in the set <code>S</code>. But since permutation is a computation heavy operation especially for large sets we use a hashing/mapping function that typically reorders the elements using simple math operation. One such hashing function is</p>
<p><img src="https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png" alt="https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png"></p>
<p>If <code>u</code> is the total number of elements in the Universal Set <code>U</code> then <code>a</code> and <code>b</code> are the random integers less than <code>u</code> and <code>c</code> is the prime number slightly higher than <code>u</code>.  A sample permute function could be</p>
<pre><code><span><span>def</span> <span>permute_fn</span><span>(x: int)</span> -&gt; int:</span>
    <span>return</span> (<span>23</span> * x + <span>67</span>) % <span>199</span>
</code></pre>
<p>Now that we have defined permutation as a simple mathematical operation that spits out the new row index, we can find MinHash of a set as the element that has the minimum new row number. Hence we can define the MinHash function as</p>
<pre><code><span><span>def</span> <span>minhash</span><span>(s: set)</span> -&gt; int:</span>
    <span>return</span> min([permute_fn(e) <span>for</span> e <span>in</span> s])
</code></pre>
<h2>A surprising property of MinHash</h2>
<p>MinHash has a surprising property, according to which, the probability that the MinHash of random permutation produces the same value for the two sets equals the Jaccard Similarity Coefficient of those sets.</p>
<p><img src="https://user-images.githubusercontent.com/4745789/98463732-8229c380-21e3-11eb-9b26-04ec08bc8753.png" alt="https://user-images.githubusercontent.com/4745789/98463732-8229c380-21e3-11eb-9b26-04ec08bc8753.png"></p>
<p>The above equality holds true because the probability of MinHash of two sets to be the same is the number of elements present in both the sets divided by the total number of elements in both the sets combined; which in fact is the definition of Jaccard Similarity Coefficient.</p>
<p>Hence to approximate Similarity Coefficient using MinHash all we have to do is find the Probability of MinHash of two sets to be the same, and this is where the MinHash Signature comes in to play.</p>
<h2>MinHash Signature</h2>
<p>MinHash Signature of a set <code>S</code> is a collection of <code>k</code> MinHash values corresponding to <code>k</code> different MinHash functions. The size <code>k</code> depends on the error tolerance, keeping it higher leads to more accurate approximations.</p>
<pre><code><span><span>def</span> <span>minhash_signature</span><span>(s: set)</span>:</span>
    <span>return</span> [minhash(s) <span>for</span> minhash <span>in</span> minhash_fns]
</code></pre>
<blockquote>
<p>MinHash functions usually differ in the permutation parameters i.e. coefficients <code>a</code>, <code>b</code> and <code>c</code>.</p>
</blockquote>
<p>Now in order to compute <code>Pr[h(A) = h(B)]</code> we have to compare the MinHash Signature of the participating sets <code>A</code> and <code>B</code> and find how many values in their signatures match; dividing this number by the number of hash functions <code>k</code> will give the required probability and in turn an approximation of Jaccard Similarity Coefficient.</p>
<pre><code><span><span>def</span> <span>similarity_minhash</span><span>(a: set, b: set)</span> -&gt; float:</span>
    sign_a = minhash_signature(a)
    sign_b = minhash_signature(b)
    <span>return</span> sum([<span>1</span> <span>for</span> a, b <span>in</span> zip(sign_a, sign_b) <span>if</span> a == b]) / len(sign_a)
</code></pre>
<blockquote>
<p>MinHash Signature could well be computed just once per set.</p>
</blockquote>
<p>Thus to compute set similarity, we need not perform heavy computation like Union and Intersection and that too across millions of sets at scale, rather we can simply compare <code>k</code> items of in their signatures and get a fairly good estimate of it.</p>

<p>In order to find how close the estimate is we compute the Jaccard Similarity Coefficient and its approximate using MinHash on two disjoint sets having equal cardinality. One of the sets will undergo a transition where one element of it will be replaced with one element of the other set. So with time, the sets will go from disjoint to being equal.</p>
<p><img src="https://user-images.githubusercontent.com/4745789/98465023-860e1380-21ec-11eb-8813-7cb6920bc1fd.png" alt="https://user-images.githubusercontent.com/4745789/98465023-860e1380-21ec-11eb-8813-7cb6920bc1fd.png"></p>
<p>The illustration above shows the two plots and we can clearly see that the MinHash technique provides a fairly good estimate of Jaccard Similarity Coefficient with much fewer computations.</p>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard Index</a></li>
<li><a href="https://en.wikipedia.org/wiki/MinHash">MinHash Wikipedia</a></li>
<li><a href="https://www.researchgate.net/profile/Ekkachai_Naenudorn/publication/317248581_Using_of_Jaccard_Coefficient_for_Keywords_Similarity/links/592e560ba6fdcc89e759c6d0/Using-of-Jaccard-Coefficient-for-Keywords-Similarity.pdf">Using of Jaccard Coefficient for Keywords Similarity</a></li>
<li><a href="https://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/">MinHash Tutorial with Python Code</a></li>
</ul>
</div></div><section><div><div><p><img src="https://arpitbhayani.me/static/img/arpit.jpg"></p>  <h2>
              500+ Signups
            </h2> <p>
              If you like what you read subscribe you can always subscribe to
              my newsletter and get the post delivered straight to your inbox.
              I write
              <a href="https://arpitbhayani.me/blogs">essays</a> on various
              engineering topics and share it through my weekly
              <a href="https://arpitbhayani.me/newsletter">newsletter</a> 👇
            </p> <br> </div></div></section></div>]]>
            </description>
            <link>https://arpitbhayani.me/blogs/jaccard-minhash</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025683</guid>
            <pubDate>Sun, 08 Nov 2020 13:32:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Artificial and Machine Learning in Finance]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025512">thread link</a>) | @tosky
<br/>
November 8, 2020 | https://blog.swizel.co/artificial-and-machine-learning-in-finance-ckgns119e03dfncs11pereysu | <a href="https://web.archive.org/web/*/https://blog.swizel.co/artificial-and-machine-learning-in-finance-ckgns119e03dfncs11pereysu">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1603549835455/fClS2ugSU.jpeg?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div itemprop="text"><p>The year is 1992. In the United States, the cold war has been declared officially over and a young Bill Clinton is set to become the 42nd president of America. The space shuttle Atlantis takes off from Cape Canaveral with meteorological instruments to study global warming as the resources tied up in the costly cold war are freed for nobler pursuits. In South Africa, white citizens vote for political reforms to end apartheid, the struggle of the past two years is coming to an end. On the British Isles, the most famous speculative attack in history has been carried out on the British Pound by a group of investors masterminded by Hungarian-American financier George Soros. On ‘Black Wednesday’ 16 September 1992, investors sell massive amounts of British Pounds, expecting a devaluation- a drop in the price of the pound against other currencies. The Bank of England buys £4 billion in order to keep the demand for the pound high, but by the next day the value of the pound has fallen by more than 10%. The Bank reports a loss of £3.3 billion, a third of which George Soros gets to keep.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1603548855412/aABMN3Nsu.jpeg?auto=format&amp;q=60" alt="https___specials-images.forbesimg.com_imageserve_5f4e72bdd82a882a3012a595_0x0.jpg_background=000000&amp;cropX1=886&amp;cropX2=3035&amp;cropY1=515&amp;cropY2=2664.jpg"></p>
<p>Fast forward to 2001. The September 11 attacks have just taken place prompting the formation of four new security agencies and a war on terror that would last for over a decade.
What do these two events have in common? Two things. Firstly, they are both ‘black swan’ events. ‘Black swan’ is a termed coined by Lebanese-American statistician Nicholas Nassim Taleb to describe events that have low probabilities of happening and cannot be predicted. They usually have major effects and are often wrongly explained away by historians and analysts who have the benefit of hindsight. In other words, events that seem obvious when viewed from a perspective further in time, but are unpredictable by observers in the present. Examples include; World war 1, the dissolution of the Soviet Union, the housing crisis of 2008, and Donald Trump’s election.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1603548997001/tSn4UkG9K.jpeg?auto=format&amp;q=60" alt="donald-trump-portrait-with-silhouette-style_23-2147952267.jpg"></p>
<p>More interestingly, both of these events may not have happened today. This due to the advances being made in the prediction of rare events in the field of Machine learning. Machine learning is a branch of the Artificial intelligence discipline that automates analytical model building. A model is a simple structure that captures all the features of more complicated real-world situations.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1603549165479/1VwKMKpzR.jpeg?auto=format&amp;q=60" alt="file-20190409-2931-2n3fgx.jpg"></p>
<p>Machine learning is currently being considered in predicting future cyber-terrorist attacks, and is already being used to anticipate sudden changes in stock prices here, and here.</p>
<p>While Machine learning already has so many applications today in almost every sphere of human life; </p>
<ol>
<li>Technology (virtual assistants and self-driving cars) </li>
<li>Banking (fraud detection)</li>
<li>Marketing (adaptive online ads)</li>
</ol>
<p>A number of criticisms have yet to be addressed by proponents of machine learning in black swan event prediction such as;</p>
<ol>
<li><strong>Delusional turkeys:</strong> Algorithms predict events based on past data, which is well… from the past. This has been likened to Inductive reasoning, a generalization of which is the anecdotal turkey predicting that the farmer will not kill it, based on past data which holds up until thanksgiving.</li>
<li><strong>Fighting fire with fire:</strong> Other critics have pointed out that the stock market is very complex and responds to the actions which members take. This means that algorithms that predict what humans do right now, will eventually have to predict what other algorithms will do as more traders resort to Machine learning. Eventually those algorithms will also have to predict what other algorithms predict that other algorithms will do. The same reasoning applies to terrorist attacks. What happens when the terrorists resort to machine learning?</li>
<li><strong>The enemy within:</strong> Other critics have yet opined that machines are incapable of predicting black swan events and instead are most likely to cause one like in the 2010 flash crash. This carries greater implications in predicting terrorist attacks. Implications such as strained diplomatic relations or even war.</li>
</ol>
<p>In conclusion, while the pros and cons of machine learning have not yet been fully measured, we can only say two things with the exact minimum degree of certainty with which you might be comfortable, that machine learning will predict black swan events or that it will itself be absolved as a factor in the black hole that is the quest for the prediction of black swan events. But whatever the case, like with google maps, there is no wrong turn and there certainly is no going back now.</p>
</div></div></section></div></div>]]>
            </description>
            <link>https://blog.swizel.co/artificial-and-machine-learning-in-finance-ckgns119e03dfncs11pereysu</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025512</guid>
            <pubDate>Sun, 08 Nov 2020 12:59:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Digitalizing Education in Nigeria]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025504">thread link</a>) | @tosky
<br/>
November 8, 2020 | https://blog.swizel.co/digitalizing-education-in-nigeria-ckeyoypr2003xtrs13taddlkg | <a href="https://web.archive.org/web/*/https://blog.swizel.co/digitalizing-education-in-nigeria-ckeyoypr2003xtrs13taddlkg">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1599856000240/tnHMHxQgo.jpeg?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div itemprop="text"><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1599855429914/8tPIXBOLW.jpeg?auto=format&amp;q=60" alt="Education.jpg"></p>
<p><strong>T</strong>he state of education in Nigeria is dismal, especially in rural areas. Access to good education is scarce, and where it isn’t, it is ridiculously expensive. This is due to the enormous amount resources that go into setting up a school. Infrastructure, Licensing, Staff payment, Environmental maintenance are some of the major culprits, and rightly so.</p>
<p>In a nation ravaged by poverty, there must be ways to give access to quality education beyond the confines of the classroom, or school. <strong>This is where Digitalization comes in</strong>. </p>
<p>In this ever digital world where you have computers more and more becoming prerequisite for learning, it is wise to harness the power these devices wield and channel them towards creating a system that can cater for education with or without the classroom.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1599855268846/MJ5Xm3i3I.jpeg?auto=format&amp;q=60" alt="LAPTOP.jpg"></p>
<p><strong>Taking Over</strong></p>
<p>Technology keeps on advancing and making life easier. People make use of their computers and cellular devices more often than not these days and this is drastically improving the way we accomplish tasks, keep records, and even deliver lecturers all over the world.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1599855424798/_UELdA0cg.jpeg?auto=format&amp;q=60" alt="book.jpg"></p>
<p>Technology has made it possible for students in rural areas to gain access to standard education from anywhere in the world, up to certificate level. The opportunities are endless.</p>
<p><strong>A step further</strong></p>
<p>A step further would be the harnessing of technology to suit both academic and administrative needs and processes. This is what the Project Appman brings to the table.
We envision such a time when every school would be capable of offering virtual options for every course, such that students who cannot afford to go to school, can have school meet them halfway through a simple device.
We envision a time when field trips will be turned to webinars, so that students who cannot afford to go on trips, can traverse within the screens of their devices and gain relevant experience equal to that gotten on the field. 
We envision a time when self learning can be encouraged, practiced and rewarded. A time when we can achieve equitable access to learning material and content to every individual.</p>
<p>A step further is the <strong>Project Appman</strong></p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1599855205798/fJ0lShSiA.jpeg?auto=format&amp;q=60" alt="APPMAN.jpg"></p>
</div></div></section></div></div>]]>
            </description>
            <link>https://blog.swizel.co/digitalizing-education-in-nigeria-ckeyoypr2003xtrs13taddlkg</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025504</guid>
            <pubDate>Sun, 08 Nov 2020 12:58:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Machine Learning News (Like HN, but for ML)]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025501">thread link</a>) | @rerapp
<br/>
November 8, 2020 | https://mln.dev/top/1 | <a href="https://web.archive.org/web/*/https://mln.dev/top/1">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://mln.dev/top/1</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025501</guid>
            <pubDate>Sun, 08 Nov 2020 12:57:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Priming People to Take Risk]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025464">thread link</a>) | @sheefrex
<br/>
November 8, 2020 | https://www.themetasophist.com/daily-notes/priming-people-to-take-risk | <a href="https://web.archive.org/web/*/https://www.themetasophist.com/daily-notes/priming-people-to-take-risk">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-e3d84c4e3b6158abd6f5"><div><div><p>A key contributor to stagnation could be a growing reluctance to take risk. All creation involves risk, because when embarking on the journey you rarely know the result. As Paul Graham said, <a href="http://paulgraham.com/early.html">you might fear creating something "lame"</a>.</p><p>As society develops, it could become less attractive to take risk. For example, if you are musically inclined, learning and performing the works of Beethoven, Mozart, Chopin, and Wagner may deliver more instant rewards then creating your own musical forms. There is a large audience for the renowned, little for the unknown. In addition, someone like J.S. Bach could focus more on writing music, because he didn't have any of those four composers to listen to, and couldn't be paid for performing their works.</p><p>Similarly, as wealth grows, more people are employed to manage it. In a society with little wealth, financiers are few as there are fewer assets to trade. The mathematicians and engineers who become quants in our current age would actually have done maths and engineering in a poorer one.</p><p>These are two particular instances of a general problem whereby established paths are less risky and therefore more attractive than novel ones.</p><p>But without risk, no one will come up with new paradigms of knowledge and organisation. We need people to take risk to ward off stagnation. How can we encourage people to do so?</p><p>A potential answer to this question emerged in a <a href="https://jimruttshow.blubrry.net/the-jim-rutt-show-transcripts/transcript-of-episode-87-joscha-bach-on-theories-of-consciousness/">conversation between Jim Rutt and Joscha Bach</a>. Rutt cited a psychological experiment <a href="https://pdfs.semanticscholar.org/ad1e/6dac677b7793836585405076e63839e99b22.pdf" title="https://pdfs.semanticscholar.org/ad1e/6dac677b7793836585405076e63839e99b22.pdf">(Dutton and Aron, 1974)</a> whereby those who just walked across a dangerous bridge were more likely to arrange a date with a member of the opposite sex immediately afterwards than those who walked over a safer bridge. The implication in the discussion is that doing something risky raises your willingness to take more risk.</p><p>In fact, Dutton and Arron state that this behaviour probably arises from what is known as the "Misattribution of Arousal". Essentially, going over a dangerous bridge induces physiological arousal, which the participant mistakes for romantic arousal. <a href="https://www.researchgate.net/publication/232518962_Arousal_and_attraction_A_response-facilitation_alternative_to_misattribution_and_negative-reinforcement_models" title="https://www.researchgate.net/publication/232518962_Arousal_and_attraction_A_response-facilitation_alternative_to_misattribution_and_negative-reinforcement_models">According to Allen et al.</a>, this effect even holds when someone is aware that the main source of arousal is non-romantic in nature.</p><p>Music can also induce physiological arousal: <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0183531" title="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0183531">another study</a> finds that women, but not men, were more likely to rate a man as attractive after listening to music. Interestingly, the study notes that "high-arousing, complex music yielded the largest effects". If excitation-transfer theory holds, the same dynamic would probably apply to exercise.</p><p>This may be interesting if one wants to raise the number of couples in society. But is there a link to risk-taking in areas such as business, politics and academia? There is some evidence that becoming aware of the neutral nature of arousal can boost confidence, and therefore also risk-taking, at least in the short-term. For example, <a href="https://journals.sagepub.com/doi/abs/10.1177/0146167298245008">one study says that arousal is just part of "gearing up" to do a task</a>, and that this feeling is often wrongly attributed to diminished confidence. From the abstract (my emphasis):</p></div><blockquote><p>"People's confidence that they will do well tends to diminish as the "moment of truth" draws near. We propose that this phenomenon stems in part from individuals using their pre-task arousal as a cue to their level of confidence. Arousal that is part and parcel of "gearing up" to perform a task may be misattributed to diminished confidence... <strong>Participants in two experiments who were encouraged to misattribute their arousal to a neutral source ("subliminal noise") expressed greater confidence in their ability </strong>than did participants not able to do so"</p></blockquote><div><p>In this abstract, we have found what we are looking for, and it's slightly better than our starting point as it is much easier to attribute arousal to a normal part of doing a task than it is to find a dangerous bridge to walk across. In practice, this would be useful on a short-term horizon. For example, if you are an entrepreneur about to approach a potential customer or pitch to investors, you should attribute any nervousness to just part of doing a novel task rather than interpreting it as a sign you are about to do the coming task poorly. That could increase your confidence and reduce the probability you will abandon the effort.</p><p>Framing nervousness in this way may help people to take risk. But of course, taking a risk does not mean that it will always be successful.</p></div></div></div></div>]]>
            </description>
            <link>https://www.themetasophist.com/daily-notes/priming-people-to-take-risk</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025464</guid>
            <pubDate>Sun, 08 Nov 2020 12:49:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How Python bytecode is executed]]>
            </title>
            <description>
<![CDATA[
Score 164 | Comments 16 (<a href="https://news.ycombinator.com/item?id=25025451">thread link</a>) | @r4victor
<br/>
November 8, 2020 | https://tenthousandmeters.com/blog/python-behind-the-scenes-4-how-python-bytecode-is-executed/ | <a href="https://web.archive.org/web/*/https://tenthousandmeters.com/blog/python-behind-the-scenes-4-how-python-bytecode-is-executed/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<!-- /.post-info -->      <p>We started this series with <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-1-how-the-cpython-vm-works/">an overview of the CPython VM</a>. We learned that to run a Python program, CPython first compiles it to bytecode, and we studied how the compiler works in <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-2-how-the-cpython-compiler-works/">part two</a>. <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-3-stepping-through-the-cpython-source-code/">Last time</a> we stepped through the CPython source code starting with the <code>main()</code> function until we reached the evaluation loop, a place where Python bytecode gets executed. The main reason why we spent time studying these things was to prepare for the discussion that we start today. The goal of this discussion is to understand how CPython does what we tell it to do, that is, how it executes the bytecode to which the code we write compiles.</p>
<p><strong>Note</strong>: In this post I'm referring to CPython 3.9. Some implementation details will certainly change as CPython evolves. I'll try to keep track of important changes and add update notes.</p>
<h3>Starting point</h3>
<p>Let's briefly recall what we learned in the previous parts. We tell CPython what to do by writing Python code. The CPython VM, however, understands only Python bytecode. This is the job of the compiler to translate Python code to bytecode. The compiler stores bytecode in a code object, which is a structure that fully describes what a code block, like a module or a function, does. To execute a code object, CPython first creates a state of execution for it called a frame object. Then it passes a frame object to a frame evaluation function to perform the actual computation. The default frame evaluation function is <code>_PyEval_EvalFrameDefault()</code> defined in <a href="https://github.com/python/cpython/blob/3.9/Python/ceval.c#L889">Python/ceval.c</a>. This function implements the core of the CPython VM. Namely, it implements the logic for the execution of Python bytecode. So, this function is what we're going to study today.</p>
<p>To understand how <code>_PyEval_EvalFrameDefault()</code> works, it is crucial to have an idea of what its input, a frame object, is. A frame object is a Python object defined by the following C struct:</p>
<div><pre><span></span><span>// typedef struct _frame PyFrameObject; in other place</span>
<span>struct</span> <span>_frame</span> <span>{</span>
    <span>PyObject_VAR_HEAD</span>
    <span>struct</span> <span>_frame</span> <span>*</span><span>f_back</span><span>;</span>      <span>/* previous frame, or NULL */</span>
    <span>PyCodeObject</span> <span>*</span><span>f_code</span><span>;</span>       <span>/* code segment */</span>
    <span>PyObject</span> <span>*</span><span>f_builtins</span><span>;</span>       <span>/* builtin symbol table (PyDictObject) */</span>
    <span>PyObject</span> <span>*</span><span>f_globals</span><span>;</span>        <span>/* global symbol table (PyDictObject) */</span>
    <span>PyObject</span> <span>*</span><span>f_locals</span><span>;</span>         <span>/* local symbol table (any mapping) */</span>
    <span>PyObject</span> <span>**</span><span>f_valuestack</span><span>;</span>    <span>/* points after the last local */</span>
    <span>/* Next free slot in f_valuestack.  Frame creation sets to f_valuestack.</span>
<span>       Frame evaluation usually NULLs it, but a frame that yields sets it</span>
<span>       to the current stack top. */</span>
    <span>PyObject</span> <span>**</span><span>f_stacktop</span><span>;</span>
    <span>PyObject</span> <span>*</span><span>f_trace</span><span>;</span>          <span>/* Trace function */</span>
    <span>char</span> <span>f_trace_lines</span><span>;</span>         <span>/* Emit per-line trace events? */</span>
    <span>char</span> <span>f_trace_opcodes</span><span>;</span>       <span>/* Emit per-opcode trace events? */</span>

    <span>/* Borrowed reference to a generator, or NULL */</span>
    <span>PyObject</span> <span>*</span><span>f_gen</span><span>;</span>

    <span>int</span> <span>f_lasti</span><span>;</span>                <span>/* Last instruction if called */</span>
    <span>int</span> <span>f_lineno</span><span>;</span>               <span>/* Current line number */</span>
    <span>int</span> <span>f_iblock</span><span>;</span>               <span>/* index in f_blockstack */</span>
    <span>char</span> <span>f_executing</span><span>;</span>           <span>/* whether the frame is still executing */</span>
    <span>PyTryBlock</span> <span>f_blockstack</span><span>[</span><span>CO_MAXBLOCKS</span><span>];</span> <span>/* for try and loop blocks */</span>
    <span>PyObject</span> <span>*</span><span>f_localsplus</span><span>[</span><span>1</span><span>];</span>  <span>/* locals+stack, dynamically sized */</span>
<span>};</span>
</pre></div>


<p>The <code>f_code</code> field of a frame object points to a code object. A code object is also a Python object. Here's its definition:</p>
<div><pre><span></span><span>struct</span> <span>PyCodeObject</span> <span>{</span>
    <span>PyObject_HEAD</span>
    <span>int</span> <span>co_argcount</span><span>;</span>            <span>/* #arguments, except *args */</span>
    <span>int</span> <span>co_posonlyargcount</span><span>;</span>     <span>/* #positional only arguments */</span>
    <span>int</span> <span>co_kwonlyargcount</span><span>;</span>      <span>/* #keyword only arguments */</span>
    <span>int</span> <span>co_nlocals</span><span>;</span>             <span>/* #local variables */</span>
    <span>int</span> <span>co_stacksize</span><span>;</span>           <span>/* #entries needed for evaluation stack */</span>
    <span>int</span> <span>co_flags</span><span>;</span>               <span>/* CO_..., see below */</span>
    <span>int</span> <span>co_firstlineno</span><span>;</span>         <span>/* first source line number */</span>
    <span>PyObject</span> <span>*</span><span>co_code</span><span>;</span>          <span>/* instruction opcodes */</span>
    <span>PyObject</span> <span>*</span><span>co_consts</span><span>;</span>        <span>/* list (constants used) */</span>
    <span>PyObject</span> <span>*</span><span>co_names</span><span>;</span>         <span>/* list of strings (names used) */</span>
    <span>PyObject</span> <span>*</span><span>co_varnames</span><span>;</span>      <span>/* tuple of strings (local variable names) */</span>
    <span>PyObject</span> <span>*</span><span>co_freevars</span><span>;</span>      <span>/* tuple of strings (free variable names) */</span>
    <span>PyObject</span> <span>*</span><span>co_cellvars</span><span>;</span>      <span>/* tuple of strings (cell variable names) */</span>
    <span>/* The rest aren't used in either hash or comparisons, except for co_name,</span>
<span>       used in both. This is done to preserve the name and line number</span>
<span>       for tracebacks and debuggers; otherwise, constant de-duplication</span>
<span>       would collapse identical functions/lambdas defined on different lines.</span>
<span>    */</span>
    <span>Py_ssize_t</span> <span>*</span><span>co_cell2arg</span><span>;</span>    <span>/* Maps cell vars which are arguments. */</span>
    <span>PyObject</span> <span>*</span><span>co_filename</span><span>;</span>      <span>/* unicode (where it was loaded from) */</span>
    <span>PyObject</span> <span>*</span><span>co_name</span><span>;</span>          <span>/* unicode (name, for reference) */</span>
    <span>PyObject</span> <span>*</span><span>co_lnotab</span><span>;</span>        <span>/* string (encoding addr&lt;-&gt;lineno mapping) See</span>
<span>                                   Objects/lnotab_notes.txt for details. */</span>
    <span>void</span> <span>*</span><span>co_zombieframe</span><span>;</span>       <span>/* for optimization only (see frameobject.c) */</span>
    <span>PyObject</span> <span>*</span><span>co_weakreflist</span><span>;</span>   <span>/* to support weakrefs to code objects */</span>
    <span>/* Scratch space for extra data relating to the code object.</span>
<span>       Type is a void* to keep the format private in codeobject.c to force</span>
<span>       people to go through the proper APIs. */</span>
    <span>void</span> <span>*</span><span>co_extra</span><span>;</span>

    <span>/* Per opcodes just-in-time cache</span>
<span>     *</span>
<span>     * To reduce cache size, we use indirect mapping from opcode index to</span>
<span>     * cache object:</span>
<span>     *   cache = co_opcache[co_opcache_map[next_instr - first_instr] - 1]</span>
<span>     */</span>

    <span>// co_opcache_map is indexed by (next_instr - first_instr).</span>
    <span>//  * 0 means there is no cache for this opcode.</span>
    <span>//  * n &gt; 0 means there is cache in co_opcache[n-1].</span>
    <span>unsigned</span> <span>char</span> <span>*</span><span>co_opcache_map</span><span>;</span>
    <span>_PyOpcache</span> <span>*</span><span>co_opcache</span><span>;</span>
    <span>int</span> <span>co_opcache_flag</span><span>;</span>  <span>// used to determine when create a cache.</span>
    <span>unsigned</span> <span>char</span> <span>co_opcache_size</span><span>;</span>  <span>// length of co_opcache.</span>
<span>};</span>
</pre></div>


<p>The most important field of a code object is <code>co_code</code>. It's a pointer to a Python bytes object representing the bytecode. The bytecode is a sequence of two-byte instructions: one byte for an opcode and one byte for an argument.</p>
<p>Don't worry if some members of the above structures are still a mystery to you. We'll see what they are used for as we move forward in our attempt to understand how the CPython VM executes the bytecode.</p>
<h3>Overview of the evaluation loop</h3>
<p>The problem of executing Python bytecode may seem a no-brainer to you. Indeed, all the VM has to do is to iterate over the instructions and to act according to them. And this is what essentially <code>_PyEval_EvalFrameDefault()</code> does. It contains an infinite <code>for (;;)</code> loop that we refer to as the evaluation loop. Inside that loop there is a giant <code>switch</code> statement over all possible opcodes. Each opcode has a corresponding <code>case</code> block containing the code for executing that opcode. The bytecode is represented by an array of 16-bit unsigned integers, one integer per instruction. The VM keeps track of the next instruction to be executed using the <code>next_instr</code> variable, which is a pointer to the array of instructions. At the start of each iteration of the evaluation loop, the VM calculates the next opcode and its argument by taking the least significant and the most significant byte of the next instruction respectively and increments <code>next_instr</code>. The <code>_PyEval_EvalFrameDefault()</code> function is nearly 3000 lines long, but its essence can be captured by the following simplified version:</p>
<div><pre><span></span><span>PyObject</span><span>*</span>
<span>_PyEval_EvalFrameDefault</span><span>(</span><span>PyThreadState</span> <span>*</span><span>tstate</span><span>,</span> <span>PyFrameObject</span> <span>*</span><span>f</span><span>,</span> <span>int</span> <span>throwflag</span><span>)</span>
<span>{</span>
    <span>// ... declarations and initialization of local variables</span>
    <span>// ... macros definitions</span>
    <span>// ... call depth handling</span>
    <span>// ... code for tracing and profiling</span>

    <span>for</span> <span>(;;)</span> <span>{</span>
        <span>// ... check if the bytecode execution must be suspended,</span>
        <span>// e.g. other thread requested the GIL</span>

        <span>// NEXTOPARG() macro</span>
        <span>_Py_CODEUNIT</span> <span>word</span> <span>=</span> <span>*</span><span>next_instr</span><span>;</span> <span>// _Py_CODEUNIT is a typedef for uint16_t</span>
        <span>opcode</span> <span>=</span> <span>_Py_OPCODE</span><span>(</span><span>word</span><span>);</span>
        <span>oparg</span> <span>=</span> <span>_Py_OPARG</span><span>(</span><span>word</span><span>);</span>
        <span>next_instr</span><span>++</span><span>;</span>

        <span>switch</span> <span>(</span><span>opcode</span><span>)</span> <span>{</span>
            <span>case</span> <span>TARGET</span><span>(</span><span>NOP</span><span>)</span> <span>{</span>
                <span>FAST_DISPATCH</span><span>();</span> <span>// more on this later</span>
            <span>}</span>

            <span>case</span> <span>TARGET</span><span>(</span><span>LOAD_FAST</span><span>)</span> <span>{</span>
                <span>// ... code for loading local variable</span>
            <span>}</span>

            <span>// ... 117 more cases for every possible opcode</span>
        <span>}</span>

        <span>// ... error handling</span>
    <span>}</span>

    <span>// ... termination</span>
<span>}</span>
</pre></div>


<p>To get a more realistic picture, let's discuss some of the omitted pieces in more detail.</p>
<h4>reasons to suspend the loop</h4>
<p>From time to time, the currently running thread stops executing the bytecode to do something else or to do nothing. This can happen due to one of the four reasons:</p>
<ul>
<li>There are signals to handle. When you register a function as a signal handler using <a href="https://docs.python.org/3/library/signal.html#signal.signal"><code>signal.signal()</code></a>, CPython stores this function in the array of handlers. The function that will actually be called when a thread receives a signal is <code>signal_handler()</code> (it's passed to the <a href="https://www.man7.org/linux/man-pages/man2/sigaction.2.html"><code>sigaction()</code></a> library function on Unix-like systems). When called, <code>signal_handler()</code> sets a boolean variable telling that the function in the array of handlers corresponding to the received signal has to be called. Periodically, the main thread of the main interpreter calls the tripped handlers.</li>
<li>There are pending calls to call. Pending calls is a mechanism that allows to schedule a function to be executed from the main thread. This mechanism is exposed by the Python/C API via the <a href="https://docs.python.org/3/c-api/init.html#c.Py_AddPendingCall"><code>Py_AddPendingCall()</code></a> function.</li>
<li>The asynchronous exception is raised. The asynchronous exception is an exception set in one thread from another. This can be done using the <a href="https://docs.python.org/3/c-api/init.html#c.PyThreadState_SetAsyncExc"><code>PyThreadState_SetAsyncExc()</code></a> function provided by the Python/C API.</li>
<li>The currently running thread is requested to drop the GIL. When it sees such a request, it drops the GIL and waits until it acquires the GIL again.</li>
</ul>
<p>CPython has indicators for each of these events. The variable indicating that there are handlers to call is a member of <code>runtime-&gt;ceval</code>, which is a <code>_ceval_runtime_state</code> struct:</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-4-how-python-bytecode-is-executed/">https://tenthousandmeters.com/blog/python-behind-the-scenes-4-how-python-bytecode-is-executed/</a></em></p>]]>
            </description>
            <link>https://tenthousandmeters.com/blog/python-behind-the-scenes-4-how-python-bytecode-is-executed/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025451</guid>
            <pubDate>Sun, 08 Nov 2020 12:48:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An Immutable Blog Application in PostgreSQL]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25025415">thread link</a>) | @todsacerdoti
<br/>
November 8, 2020 | https://kevinmahoney.co.uk/articles/immutable-data/ | <a href="https://web.archive.org/web/*/https://kevinmahoney.co.uk/articles/immutable-data/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting" id="database-design-immutable-data">
  <div itemprop="articleBody">
    <p><time itemprop="datePublished">27 June 2015</time></p>

<p>In this article I’ll be talking about the advantages and disadvantages
of immutable data in databases. As a demonstration I’ll walk through a simple blog
application in PostgreSQL. I highly recommend this approach in many
cases, but the standard ‘caveat emptor’ disclaimer applies. Be aware
of the trade-offs you are making.</p>

<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#advantages">Advantages</a></li>
  <li><a href="#drawbacks">Drawbacks</a></li>
  <li><a href="#an-immutable-blog-application-in-postgresql">An Immutable Blog Application in PostgreSQL</a></li>
  <li><a href="#performance-improvements">Performance Improvements</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>In a traditional blog application, a blog post may be defined as
follows:</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>TABLE</span> <span>blog</span><span>.</span><span>article</span> <span>(</span>
       <span>slug</span> <span>TEXT</span> <span>PRIMARY</span> <span>KEY</span><span>,</span>
       <span>title</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span><span>,</span>
       <span>content</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span>
<span>);</span></code></pre></figure>

<p>What happens when the content or title of the post changes? An
<code>UPDATE</code> is performed. This destructively mutates your database,
i.e. information has been lost - it is no longer known what the blog
post contained prior to its edit.</p>

<p>The idea behind the approach detailed here is that the current state
of the blog post is not recorded, instead there is an immutable log of
events for the entire history of the application. The current
application state is a function of these events. No information is
lost.</p>

<p>This approach will only ever <code>INSERT</code> into the database. It will never
<code>UPDATE</code>. The blog post table will now look more like this:</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>SEQUENCE</span> <span>logblog</span><span>.</span><span>revision_id_seq</span><span>;</span>
<span>CREATE</span> <span>TABLE</span> <span>logblog</span><span>.</span><span>article_revision</span> <span>(</span>
       <span>revision_id</span> <span>INTEGER</span> <span>PRIMARY</span> <span>KEY</span> <span>DEFAULT</span> <span>nextval</span><span>(</span><span>'logblog.revision_id_seq'</span><span>),</span>
       <span>timestamp</span> <span>TIMESTAMP</span> <span>NOT</span> <span>NULL</span> <span>DEFAULT</span> <span>now</span><span>(),</span>
       <span>slug</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span><span>,</span>
       <span>title</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span><span>,</span>
       <span>content</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span>
<span>);</span></code></pre></figure>

<p>Note the slug is no longer unique. An article is updated by
inserting a new <code>article_revision</code> with an existing slug.</p>

<p>There are some existing databases based around this concept, including
<a href="http://datomic.com/">Datomic</a> and <a href="https://geteventstore.com/">EventStore</a>, but in this article I’ll be focusing on how to
do this in PostgreSQL.</p>

<h2 id="advantages">Advantages</h2>

<p>Why would you want to do this?</p>

<p>Immutable data in databases has all the same advantages as
immutable data in general purpose languages. It is usually much
easier to reason about than mutable state. See:
<a href="https://en.wikipedia.org/wiki/Referential_transparency_(computer_science)">Referential Transparency</a>.</p>

<p>This paragraph from the Datomic website explains other advantages of this
approach quite well:</p>

<blockquote>
  <p>How can data be immutable? Don’t facts change? They don’t, in fact,
when you incorporate time in the data. For instance, when Obama became
president, it didn’t mean that Bush was never president. As long as
who is president isn’t stored in a single (logical) place, there’s no
reason a database system couldn’t retain both facts
simultaneously. While many queries might be interested in the
‘current’ facts, others might be interested in, e.g. what the product
catalog looked like last month compared to this month. Incorporating
time in data allows the past to be retained (or not), and supports
point-in-time queries. Many real world systems have to retain all
changes, and struggle mightily to efficiently provide the ‘latest’
view in a traditional database. This all happens automatically in
Datomic. Datomic is a database of facts, not places.</p>
</blockquote>

<p>Immutable facts are great for auditing and debugging. It’s
tremendously helpful to be able to see the state of you application at
any point in time, and step through the state changes one by one. It’s
especially invaluable when working with financial data.</p>

<h2 id="drawbacks">Drawbacks</h2>

<p>There are some trade-offs to this approach: space usage, complexity
and performance.</p>

<p>Of course, storing every change to the state instead of mutating data
will require more persistent storage space. If you’re only inserting
data your storage requirements can only ever grow.</p>

<p>In PostgreSQL, naive read queries that reduce events down into the
current application state will often be more complicated and
have worse performance; However, this can compensated for with some
additional effort.
See the <a href="#performance-improvements">Performance Improvements</a>
section for more details.</p>

<h2 id="an-immutable-blog-application-in-postgresql">An Immutable Blog Application in PostgreSQL</h2>

<p>This code is available as a <a href="https://gist.github.com/KMahoney/dcc12d3ff6a49c11cdc9">gist</a>.</p>

<p>This application should be able to:</p>

<ul>
  <li>Privately write and edit blog posts</li>
  <li>Publish revisions of posts for public viewing</li>
  <li>Delete posts</li>
  <li>Add or remove tags to posts</li>
</ul>

<h3 id="schema">Schema</h3>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>SCHEMA</span> <span>logblog</span><span>;</span></code></pre></figure>

<h3 id="article-table">Article Table</h3>

<p>A table representing the set of article slugs. Foreign keys can
reference this to maintain integrity. PostgreSQL does
not allow referencing the <code>slug</code> field in the <code>article_revision</code> table
as it is not unique in that table.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>TABLE</span> <span>logblog</span><span>.</span><span>article</span> <span>(</span>
       <span>slug</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span> <span>PRIMARY</span> <span>KEY</span> <span>CHECK</span><span>(</span><span>slug</span> <span>SIMILAR</span> <span>TO</span> <span>'[-a-z]+'</span><span>)</span>
<span>);</span></code></pre></figure>

<h3 id="revision-table">Revision Table</h3>

<p>Next is an immutable log of article revisions. New articles can be
created by atomically inserting the slug into <code>article</code> and the
revision into <code>article_revision</code>. Articles can be updated by simply
inserting a new revision with an existing <code>article</code> slug.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>SEQUENCE</span> <span>logblog</span><span>.</span><span>revision_id_seq</span><span>;</span>
<span>CREATE</span> <span>TABLE</span> <span>logblog</span><span>.</span><span>article_revision</span> <span>(</span>
       <span>revision_id</span> <span>INTEGER</span> <span>PRIMARY</span> <span>KEY</span> <span>DEFAULT</span> <span>nextval</span><span>(</span><span>'logblog.revision_id_seq'</span><span>),</span>
       <span>timestamp</span> <span>TIMESTAMP</span> <span>NOT</span> <span>NULL</span> <span>DEFAULT</span> <span>now</span><span>(),</span>
       <span>slug</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span> <span>REFERENCES</span> <span>logblog</span><span>.</span><span>article</span><span>,</span>
       <span>title</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span><span>,</span>
       <span>content</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span>
<span>);</span></code></pre></figure>

<h3 id="publishing">Publishing</h3>

<p>Now for an immutable log of article publish events. The public will be
able to see the last published version of an article. This means it is
possible to publish an earlier revision to ‘rollback’ an article.</p>

<p>Note the timestamp when an article was published or deleted is
distinct from when the revision was created. Readers are probably more
interested in when an article was first published than when it was
first drafted.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>TABLE</span> <span>logblog</span><span>.</span><span>article_publish</span> <span>(</span>
       <span>timestamp</span> <span>TIMESTAMP</span> <span>NOT</span> <span>NULL</span> <span>DEFAULT</span> <span>now</span><span>(),</span>
       <span>revision_id</span> <span>INTEGER</span> <span>REFERENCES</span> <span>logblog</span><span>.</span><span>article_revision</span>
<span>);</span></code></pre></figure>

<h3 id="deleting">Deleting</h3>

<p>An immutable log of article deletion events. Articles are only
considered deleted when the deletion timestamp is later than any
publish actions. This means articles can be ‘undeleted’ by
re-publishing them. No data is ever truly removed.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>TABLE</span> <span>logblog</span><span>.</span><span>article_deletion</span> <span>(</span>
       <span>timestamp</span> <span>TIMESTAMP</span> <span>NOT</span> <span>NULL</span> <span>DEFAULT</span> <span>now</span><span>(),</span>
       <span>slug</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span> <span>REFERENCES</span> <span>logblog</span><span>.</span><span>article</span>
<span>);</span></code></pre></figure>

<h3 id="tagging">Tagging</h3>

<p>An immutable log of tag events. It’s awkward to create a tag table
with a set of unique tag names like we do with articles, so instead we
just record tag events. This is a bit lazy as it doesn’t enforce
consistency with removed tags (i.e. you can remove a non-existing
tag).</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>TYPE</span> <span>logblog</span><span>.</span><span>tag_event_type</span> <span>AS</span> <span>ENUM</span> <span>(</span><span>'add'</span><span>,</span> <span>'remove'</span><span>);</span>
<span>CREATE</span> <span>TABLE</span> <span>logblog</span><span>.</span><span>tag_event</span> <span>(</span>
       <span>timestamp</span> <span>TIMESTAMP</span> <span>NOT</span> <span>NULL</span> <span>DEFAULT</span> <span>now</span><span>(),</span>
       <span>slug</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span> <span>REFERENCES</span> <span>logblog</span><span>.</span><span>article</span><span>,</span>
       <span>event</span> <span>logblog</span><span>.</span><span>tag_event_type</span> <span>NOT</span> <span>NULL</span><span>,</span>
       <span>tag</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span>
<span>);</span></code></pre></figure>

<h3 id="building-views">Building Views</h3>

<p>Querying this data can get quite complicated, so it is a good idea to
break it down with views that show the current state of the
application. They make heavy use of <code>DISTINCT ON</code> to find the latest
state of each component.</p>

<p>This view is the latest deletion date for an article (if applicable)</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>VIEW</span> <span>logblog</span><span>.</span><span>last_deleted_view</span> <span>AS</span>
     <span>SELECT</span> <span>DISTINCT</span> <span>ON</span> <span>(</span><span>slug</span><span>)</span> <span>timestamp</span> <span>AS</span> <span>deleted_on</span><span>,</span> <span>slug</span>
     <span>FROM</span> <span>logblog</span><span>.</span><span>article_deletion</span>
     <span>ORDER</span> <span>BY</span> <span>slug</span><span>,</span> <span>timestamp</span> <span>DESC</span><span>;</span></code></pre></figure>

<p>We will want to show users the latest published content of an article.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>VIEW</span> <span>logblog</span><span>.</span><span>last_published_view</span> <span>AS</span>
     <span>SELECT</span> <span>DISTINCT</span> <span>ON</span> <span>(</span><span>rev</span><span>.</span><span>slug</span><span>)</span>
            <span>rev</span><span>.</span><span>revision_id</span><span>,</span>
            <span>pub</span><span>.</span><span>timestamp</span> <span>AS</span> <span>last_updated_on</span><span>,</span>
            <span>rev</span><span>.</span><span>slug</span><span>,</span>
            <span>rev</span><span>.</span><span>title</span><span>,</span>
            <span>rev</span><span>.</span><span>content</span>
     <span>FROM</span> <span>logblog</span><span>.</span><span>article_publish</span> <span>AS</span> <span>pub</span>
     <span>INNER</span> <span>JOIN</span> <span>logblog</span><span>.</span><span>article_revision</span> <span>AS</span> <span>rev</span> <span>ON</span> <span>rev</span><span>.</span><span>revision_id</span> <span>=</span> <span>pub</span><span>.</span><span>revision_id</span>
     <span>ORDER</span> <span>BY</span> <span>rev</span><span>.</span><span>slug</span><span>,</span> <span>timestamp</span> <span>DESC</span><span>;</span></code></pre></figure>

<p>Another piece of useful information is when an article was first
published. This is the date you usually show on an article. The last
published timestamp shows when an article was last updated.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>VIEW</span> <span>logblog</span><span>.</span><span>first_published_view</span> <span>AS</span>
     <span>SELECT</span> <span>DISTINCT</span> <span>ON</span> <span>(</span><span>rev</span><span>.</span><span>slug</span><span>)</span>
            <span>rev</span><span>.</span><span>revision_id</span><span>,</span>
            <span>pub</span><span>.</span><span>timestamp</span> <span>AS</span> <span>first_published_on</span>
     <span>FROM</span> <span>logblog</span><span>.</span><span>article_publish</span> <span>AS</span> <span>pub</span>
     <span>INNER</span> <span>JOIN</span> <span>logblog</span><span>.</span><span>article_revision</span> <span>AS</span> <span>rev</span> <span>ON</span> <span>rev</span><span>.</span><span>revision_id</span> <span>=</span> <span>pub</span><span>.</span><span>revision_id</span>
     <span>ORDER</span> <span>BY</span> <span>rev</span><span>.</span><span>slug</span><span>,</span> <span>timestamp</span><span>;</span></code></pre></figure>

<p>Aggregate the tags as a PostgreSQL array for convenience.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>VIEW</span> <span>logblog</span><span>.</span><span>article_tag_view</span> <span>AS</span>
       <span>WITH</span> <span>last_tag_event</span> <span>AS</span>
         <span>(</span><span>SELECT</span> <span>DISTINCT</span> <span>ON</span> <span>(</span><span>slug</span><span>,</span> <span>tag</span><span>)</span> <span>*</span>
          <span>FROM</span> <span>logblog</span><span>.</span><span>tag_event</span>
          <span>ORDER</span> <span>BY</span> <span>slug</span><span>,</span> <span>tag</span><span>,</span> <span>timestamp</span> <span>DESC</span><span>)</span>
       <span>SELECT</span> <span>slug</span><span>,</span> <span>array_agg</span><span>(</span><span>tag</span><span>)</span> <span>AS</span> <span>tags</span>
       <span>FROM</span> <span>last_tag_event</span>
       <span>WHERE</span> <span>event</span> <span>=</span> <span>'add'</span>
       <span>GROUP</span> <span>BY</span> <span>slug</span><span>;</span></code></pre></figure>

<h3 id="the-publics-view">The Public’s View</h3>

<p>Here the previous views are used as building blocks to create a public
article view. Note that articles that have a deletion date later than
the last published date are not shown.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>VIEW</span> <span>logblog</span><span>.</span><span>public_article_view</span> <span>AS</span>
       <span>SELECT</span> <span>last_pub</span><span>.</span><span>slug</span><span>,</span>
              <span>first_pub</span><span>.</span><span>first_published_on</span><span>,</span>
              <span>last_pub</span><span>.</span><span>last_updated_on</span><span>,</span>
              <span>last_pub</span><span>.</span><span>title</span><span>,</span>
              <span>last_pub</span><span>.</span><span>content</span><span>,</span>
              <span>COALESCE</span><span>(</span><span>tags</span><span>.</span><span>tags</span><span>,</span> <span>'{}'</span><span>::</span><span>TEXT</span><span>[])</span> <span>AS</span> <span>tags</span>
       <span>FROM</span> <span>logblog</span><span>.</span><span>last_published_view</span> <span>AS</span> <span>last_pub</span>
       <span>LEFT</span> <span>JOIN</span> <span>logblog</span><span>.</span><span>last_deleted_view</span> <span>AS</span> <span>del</span>
            <span>ON</span> <span>del</span><span>.</span><span>slug</span> <span>=</span> <span>latest_pub</span><span>.</span><span>slug</span>
       <span>LEFT</span> <span>JOIN</span> <span>logblog</span><span>.</span><span>first_published_view</span> <span>AS</span> <span>first_pub</span>
            <span>ON</span> <span>first_pub</span><span>.</span><span>slug</span> <span>=</span> <span>latest_pub</span><span>.</span><span>slug</span>
       <span>LEFT</span> <span>JOIN</span> <span>logblog</span><span>.</span><span>article_tag_view</span> <span>AS</span> <span>tags</span>
            <span>ON</span> <span>tags</span><span>.</span><span>slug</span> <span>=</span> <span>last_pub</span><span>.</span><span>slug</span>
       <span>WHERE</span> <span>NOT</span> <span>COALESCE</span><span>(</span><span>del</span><span>.</span><span>timestamp</span> <span>&gt;</span> <span>last_pub</span><span>.</span><span>timestamp</span><span>,</span> <span>false</span><span>)</span>
       <span>ORDER</span> <span>BY</span> <span>first_pub</span><span>.</span><span>timestamp</span><span>;</span></code></pre></figure>

<h3 id="the-life-of-a-blog-post">The Life of a Blog Post</h3>

<p>To finish, a fun query to show the entire history of an article.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>VIEW</span> <span>logblog</span><span>.</span><span>article_history_view</span> <span>AS</span>
       <span>WITH</span>
        <span>revision_events</span> <span>AS</span>
        <span>(</span><span>SELECT</span> <span>timestamp</span><span>,</span>
                <span>slug</span><span>,</span>
                <span>(</span><span>'Created article revision '</span> <span>||</span> <span>revision_id</span><span>)::</span><span>TEXT</span> <span>AS</span> <span>event</span>
         <span>FROM</span> <span>logblog</span><span>.</span><span>article_revision</span><span>),</span>
        <span>publish_events</span> <span>AS</span>
        <span>(</span><span>SELECT</span> <span>pub</span><span>.</span><span>timestamp</span><span>,</span>
                <span>rev</span><span>.</span><span>slug</span><span>,</span>
                <span>(</span><span>'Published revision '</span> <span>||</span> <span>pub</span><span>.</span><span>revision_id</span><span>)::</span><span>TEXT</span> <span>AS</span> <span>event</span>
         <span>FROM</span> <span>logblog</span><span>.</span><span>article_publish</span> <span>AS</span> <span>pub</span>
         <span>INNER</span> <span>JOIN</span> <span>logblog</span><span>.</span><span>article_revision</span> <span>AS</span> <span>rev</span>
   …</code></pre></figure></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://kevinmahoney.co.uk/articles/immutable-data/">https://kevinmahoney.co.uk/articles/immutable-data/</a></em></p>]]>
            </description>
            <link>https://kevinmahoney.co.uk/articles/immutable-data/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025415</guid>
            <pubDate>Sun, 08 Nov 2020 12:39:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What Is Software Architecture]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025368">thread link</a>) | @DevTalker
<br/>
November 8, 2020 | https://ddimitrov.dev/2020/11/08/what-is-software-architecture/ | <a href="https://web.archive.org/web/*/https://ddimitrov.dev/2020/11/08/what-is-software-architecture/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>


			
<p>In this series of posts, I will introduce you to the most popular software architecture types. But before we delve into the most common variants, let us talk about software architecture itself.</p>



<h2><strong>What is software architecture?</strong></h2>



<p>It is tough to write simply for such a complex topic as software architecture but let me try my best.</p>



<p>A software system is made up of individual elements. Elements could be databases, application servers, message brokers, load balancers, etc.</p>



<p>Each element has its properties and characteristics. Between these elements, there are relationships.</p>



<p>The way we structure these elements and their relationships, we call software architecture.</p>



<p>By defining such a structure, we aim to gain some benefits. Quality attributes define and describe these benefits, and it is essential to remember that they are not business logic related.</p>



<p>Software architecture is the skeleton of your application. Once you start implementation, it is tough to change it, so the right decision for architecture is crucial in the early phase of the project.</p>



<h2><strong>Quality attributes</strong></h2>



<p>Though quality attributes are not functionality requirements, they mainly arise from the functional requirements.&nbsp;</p>



<p>By defining requirements for a software product, the business aims to enhance some competitive advantages.</p>



<p>In the best-case scenario, quality attributes gathering should come from those competitive advantages.</p>



<p>A simple example. Back in the days, I worked in a company that wanted to develop a new retail system. The business analysis showed that most of our main competitor’s customers do not like their system because it was always online. The user couldn’t even check product stocks in case of an internet outage.</p>



<p>For our new product to be competitive in the market, we had to aim for a competitive advantage. Our system had to work offline during internet outages and sync all recorded operations when the internet is back.</p>



<p>We, developers, translated that requirement into the <strong>availability</strong> quality attribute.</p>



<p>Sometimes quality attributes don’t come from requirements. Some quality attributes are enforced by constraints. Constraints could be different, from the dev team’s experience level to government regulations.</p>



<p>And last but not least, quality attributes should be measurable and testable.</p>



<h3><strong>Some of the most popular quality attributes:</strong></h3>



<h4>Availability</h4>



<p>It defines to what level the system is available to perform its tasks. Availability expands on the notion of reliability. Systems with high availability tend to be resilient to faults and errors (well, to most of them). High availability systems implement methods for fault detection and recovery.</p>



<h4>Modifiability</h4>



<p>Modifiability defines how well a system can adapt to changes. Technologies change, business requirements change. High modifiability software should be able to embrace new changes by reducing change costs and risks.</p>



<h4>Interoperability</h4>



<p>No system can live in total isolation. Interoperability quality attribute measures to what degree a software system can “communicate” with other software systems. Systems with high interoperability tend to have very well defined interfaces and implement widely accepted standards and communication protocols.</p>



<h4>Security</h4>



<p>The security of a system could be viewed from three different angles. &nbsp;</p>



<p>Confidentiality â€“ is the data well secured from users who don’t have the right to access it.</p>



<p>Availability â€“ is the data available when needed.</p>



<p>Integrity â€“ is the data well secured, so tampering and deletion are not possible.</p>



<h4>Performance</h4>



<p>Well, performance is all about time. This quality attribute aims to put an upper execution time limit to the system’s tasks (no matter if the user or internal mechanisms initiate them).</p>



<h4>Testability</h4>



<p>Testability defines to what degree a system can be <a href="https://ddimitrov.dev/2020/10/24/how-to-write-a-good-unit-tests/">tested</a>. How well business and technical requirements can be simulated, observed, and analyzed. High testability systems provide a high level of isolation and abstraction of its modules.</p>



<h4>Usability</h4>



<p>Usability is about how easy it is for the user to perform tasks in the system.</p>



<p>Sound simple but believe me, usability is a real pain. Usability is even bigger pain when we talk about systems with very complex business logic and processes.</p>



<p>Every user comes with his own previous experience and opinions. That’s why usability is often based on compromises for the greater good ðŸ˜Š.</p>



<p>For more comprehensive list you can check <a href="https://en.wikipedia.org/wiki/List_of_system_quality_attributes" target="_blank" rel="noreferrer noopener nofollow">this</a>.</p>



<h2><strong>What is the difference between software architecture and software design?</strong></h2>



<p>While software architecture aims to define the structure and relationships of the different system elements (considering all the present constraints), the software design focuses on their specific implementation.</p>



<p>If a software architecture specifies that we will have microservices with async internal communication, the software design will define how this will be implemented.</p>



<p>Do we use RabitMQ or Mass Transit, or something other? What is the format of our messages? How will the data models reflect the bounded context we have defined? The software design answers all these questions.</p>



<h2><strong>Architecting for performance (example)</strong></h2>



<p>Let us see how a quality attribute requirement affects system architecture.</p>



<p>We have a potential client who wants a very fast (performance quality attribute) reporting system.</p>



<p>The client should make decisions fast so that the reporting system’s speed is a competitive advantage for his business.</p>



<p>Currently, the client has two different systems. He queries them when he needs some data (using their operational databases, which degrade performance additionally) and combines the extracted data in an ugly excel report.</p>



<p>Back in the days, that approach worked perfectly for him, but now, this is becoming a bottleneck when his business is much bigger.</p>



<p>Our company “Drink beer and code Ltd.” receives invite to solve the problem.</p>



<p>After a complete analysis of the case, the developers come with the solution. According to the research, they think that a report’s average execution time could be lowered to 1500 ms (yeahâ€¦).</p>



<p>At this stage, dev team will not change current systems. Either way, they work well to meet the customer’s needs, and he doesn’t want to change them.</p>



<p>The Dev team plans to deploy a new SQL Server. The server will have a new analytics database and two replicated copies of the other systems’ operational databases.</p>



<p>Operational database replication is event-based.</p>



<p>A worker service extracts the data from the replicated databases, transforms it appropriately for the analytics database, and loads it into the analytics database.</p>



<p>The analytics database design is structured and optimized for the format of the needed reports. <a href="https://ddimitrov.dev/2020/10/04/optimizing-sql-queries-sometimes-two-queries-are-better-than-one/">Querying</a> the database will be very fast.</p>



<p>Finally, we have a reporting application reading the data over the analytics database and visualizing it in beautiful tables and charts.</p>



<h2>Summary</h2>



<p>Even with that oversimplistic example, we can see that we have introduced new elements and relationships between them to meet the desired quality attribute metric.</p>



<p>We also faced the constraint that the old systems enforced over our technical decision.</p>



<p>But in the end, we achieved the desired performance and that enhanced our client’s competitive advantage.</p>
				
		</div></div>]]>
            </description>
            <link>https://ddimitrov.dev/2020/11/08/what-is-software-architecture/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025368</guid>
            <pubDate>Sun, 08 Nov 2020 12:32:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Clues to identify a destructive leader]]>
            </title>
            <description>
<![CDATA[
Score 255 | Comments 113 (<a href="https://news.ycombinator.com/item?id=25025363">thread link</a>) | @BossingAround
<br/>
November 8, 2020 | https://articles.tilt365.com/identify-destructive-leadership-patterns/ | <a href="https://web.archive.org/web/*/https://articles.tilt365.com/identify-destructive-leadership-patterns/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://articles.tilt365.com/content/images/size/w300/2020/11/sharks-in-the-water-1.png 300w,
                            https://articles.tilt365.com/content/images/size/w600/2020/11/sharks-in-the-water-1.png 600w,
                            https://articles.tilt365.com/content/images/size/w1000/2020/11/sharks-in-the-water-1.png 1000w,
                            https://articles.tilt365.com/content/images/size/w2000/2020/11/sharks-in-the-water-1.png 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://articles.tilt365.com/content/images/size/w2000/2020/11/sharks-in-the-water-1.png" alt="4 Clues to Identify a Destructive Leader">
            </figure>

            <section>
                <div>
                    <p>In many aspects of our lives, we rely on those in positions of power to lead us. The role of leaders becomes especially salient in times of uncertainty. Throughout your life, you’ve probably seen several ways leaders can respond to challenging and ambiguous situations. A transformational leader can see the opportunities in turmoil and inspire people to follow them to a better future. On the other hand, an incompetent leader will leave you to deal with everything alone. In the worst-case scenario, a destructive leader will see the potential for self-enhancement and exploit others to maximize their gain. </p><figure><img src="https://articles.tilt365.com/content/images/2020/11/four-patterns-of-destructive-leadership.png" alt="" srcset="https://articles.tilt365.com/content/images/size/w600/2020/11/four-patterns-of-destructive-leadership.png 600w, https://articles.tilt365.com/content/images/size/w1000/2020/11/four-patterns-of-destructive-leadership.png 1000w, https://articles.tilt365.com/content/images/2020/11/four-patterns-of-destructive-leadership.png 1069w" sizes="(min-width: 720px) 720px"><figcaption>4 Clues to Identify a Destructive Leader</figcaption></figure><h3 id="people-leave-managers-not-companies-">People leave managers, not companies.</h3><p>Incompetent and destructive leaders both create negative consequences, but the distinction between the two is essential. An incompetent leader may lack the compelling charisma to engage others to follow, but we wouldn’t call someone lacking charisma actively destructive. A <a href="https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199755615.001.0001/oxfordhb-9780199755615-e-014">destructive leader</a> intentionally and systematically behaves in a way that violates the organization’s members and stakeholders’ best interests. The extent of damage that a destructive leader can cause often goes unnoticed until it is too late (e.g., Enron), but there are clues you can look for to help you identify destructive leaders earlier. These clues reflect behaviors and attitudes that can reveal the destructive nature of an otherwise seemingly competent leader.</p><h3 id="what-causes-destructive-leadership-patterns">What causes destructive leadership patterns?</h3><p>Everyone has some characteristics that are annoying to someone who prefers a different way of behaving and working. These characteristics are inherited in our DNA and influenced by the environment in which we grew up. The genetic influence was passed down from generation to generation and is a product of evolution that helped your ancestors survive the environments they encountered. And they can help you too, as long as they don’t become distorted from early experiences of chronic fear. </p><p>In destructive leadership, typical behavior patterns become distorted into extremes powered by the brain’s more primitive parts under such circumstances. When this happens, we are in fear-mode, and a blend of fear reactions becomes our norm. These distorted patterns become habitual, and our responses to others become unhealthy. For example, suppose we experienced excessive criticism in our early development. In that case, our ego will record a perception of being diminished by important caregivers and sense that our very survival depends on not being criticized. In this case, we may learn to strive excessively for superiority to alleviate the fear of feeling inferior in our assessment of ourselves.</p><p>While a reactive pattern like this helped us survive the precise environment we were born into, in a global world, that same pattern may not. The brain’s brilliant design constructs a set of behaviors that will ensure our survival in whatever we experience in the development years to maturity. But if we move to an entirely new environment as an adult, the patterns that served us before may not translate into helpful practices somewhere else. </p><h3 id="the-reason-anxiety-and-stress-are-at-an-all-time-high-">The reason anxiety and stress are at an all-time high.</h3><p>In the last century, this phenomenon has become even more complicated. With the advancement of technology comes exposure to human systems all over the world. Transportation enables moving to any part of the globe in a day or two. As we move about rapidly from one culture to another, we find ourselves unable to understand why we are perceived positively in one environment and the opposite in another. No wonder anxiety and stress are at an all-time high, especially for those who interact in global companies. Adaptation to other cultures becomes a necessity, making self-awareness and emotional intelligence some of the most critical skill sets of our time. </p><h3 id="first-look-for-the-underlying-intention-">First, look for the underlying intention.</h3><p>Because all human systems are imperfect, and most parents do the best they can do, many of us have traces of fear behaviors that can drive others nuts. There are a few prototypical examples of these types of behaviors, including, but not limited to </p><ul><li>The constant worrier who is always second-guessing themselves,</li><li>The storyteller who seems to live in an ideal world no one else can relate to,</li><li>The dominant driver who wants everything to go their way,</li><li>The prideful judge who doesn’t realize they can’t possibly know everything there is to know about everything. </li></ul><p>When we’re talking about destructive leadership, we’re not merely referring to annoying habits unless they have become very extreme or painfully frequent. Destructive leaders have little interest in how they are perceived, so they are rarely interested in how they could improve. </p><h3 id="destructive-leaders-are-single-mindedly-self-interested-">Destructive leaders are single-mindedly self-interested.</h3><p>Generally, healthy leaders may have annoying habits, but when you look beneath the surface, two things are different: </p><ol><li>They adopt a mindset that conveys they care about their impact on others and are willing to listen, learn, and exert the choice and character to change.</li><li>They hold a positive intention toward others and work for the good of the mission and the enterprise they serve. </li></ol><p>On the other hand, destructive leaders are either:</p><ol><li>Un-coachable because they adopt a rigid mindset that conveys they don’t care about their impact on others and will use their authority to manipulate others to bend to their will. They imply, “I am who I am, so deal with it.”</li><li>They have a hidden ulterior motive for wanting and using power to serve themselves at the expense of others, the mission, or the enterprise. </li></ol><p>So, as we lay out the four clues of destructive leaders, keep two things in mind. Do you have a hunch that they are well-intended, and are they willing to work on themselves? If the answer to both is affirmative, they are probably not destructive, but just like you and me, they are trying to serve their company and grow as best they can. In these circumstances, we can mind our own business and work on our self-improvement plan rather than thinking about how annoying they are. After all, it’s temporary unless they are genuinely destructive. And then you must do everything in your power to remove them from your company, or they could take the whole thing down. </p><h3 id="clue-1-behaviors-or-words-that-imply-i-m-kind-of-a-big-deal-">Clue 1: Behaviors or words that imply “I’m kind of a big deal!”</h3><p><strong>Excessive fabrication and exaggeration that is nowhere near the truth. </strong></p><p>This destructive pattern arises from an inner identity that seeks to resolve a childhood dilemma about not getting enough attention from caregivers, so they do not feel special. This dilemma results in insatiable attention-seeking and an inflated need to be special, unique, or novel. The inner fear is feeling trapped in the loneliness or sadness of not being “special” enough to those who matter. Because they perceive being attention-deprived or unworthy, they feel shame, and it becomes so painful, they rebel from authority figures whom they believe could not be trusted. Instead of following appropriately, they become rebellious and provocative, conning others to go along with their fantastic plans. </p><h3 id="clues-you-will-notice-">Clues you will notice: </h3><ul><li>Exaggeration of the truth to the point of fantasy</li><li>Unapologetic self-promoting and self-aggrandizing</li><li>Excessive talking to dominate others</li><li>Pontification and fabrication of elaborate stories</li><li>Disrespect for authority figures</li><li>Disregard for rules that are contrary to their aims</li><li>Automatically dismiss ideas from others</li><li>An insatiable need to be the center of attention</li><li>Terminally individualistic, unique, or novel</li><li>External image is unusually extreme in some way</li></ul><h3 id="results-in-a-chaotic-climate">Results in a Chaotic Climate</h3><p>These destructive leaders are challenging to work with because they demand attention but don’t want the restrictions that come with being front and center. When they are in the limelight, it can be exceedingly uncomfortable because they also unconsciously fail to believe they are impressive enough. So they attract attention, initiate excessive activity, and then thwart the attention this draws. This pattern makes them unpredictable, so the shadow of the climate they create around them is chaotic and confusing for others. </p><h3 id="clue-2-behaviors-or-words-that-imply-none-of-this-is-my-fault-">Clue 2: Behaviors or words that imply “None of this is my fault!”</h3><p><strong>Excessive conflict-avoidance by deflecting personal responsibility. </strong></p><p>This destructive pattern arises from an inner identity that seeks to resolve a childhood dilemma about not getting enough approval and acceptance. This dilemma results in insatiable approval-seeking and an excessive need to be liked by everyone, even strangers. The inner fear is to be rejected or ridiculed by others as unlovable by those who matter. Because they perceive being unacceptable to others, they feel powerless and unworthy of care. This experience can become painful and they are unable to communicate or ask others for what they need because they don’t feel they deserve it. They defer to those in authority roles and suffer quietly in dependence, hopelessness, and seen as “needy” for any shred of approval. </p><h3 id="clues-you-will-notice--1">Clues you will notice:</h3><ul><li>Complaining, blaming, gossiping about others</li><li>Disgruntled resentment of those in authority</li><li>Giving up their power and being dependent on others</li><li>Come across as “needy” and draining</li><li>Asking others to decide and then resenting it</li><li>Avoiding leadership to avoid culpability later</li><li>Unconsciously inviting others to dominate them</li><li>Blaming others for being the “bully”</li><li>Desire to be the “nice” or “good” one</li><li>An insatiable need to be liked, accepted, included</li></ul><h3 id="results-in-a-conflict-averse-climate">Results in a Conflict Averse Climate </h3><p>These destructive leaders are challenging to work with because they put extreme energy into taking care of or helping others with an unstated expectation that there will be a reward in return. For example, they take care of someone with the expectation that the other person will take the responsibility of making decisions for them. However, they may not tell the other person this expectation. There is an unconscious …</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://articles.tilt365.com/identify-destructive-leadership-patterns/">https://articles.tilt365.com/identify-destructive-leadership-patterns/</a></em></p>]]>
            </description>
            <link>https://articles.tilt365.com/identify-destructive-leadership-patterns/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025363</guid>
            <pubDate>Sun, 08 Nov 2020 12:30:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Clojure: Going Faster Than TensorFlow on the GPU (GTX 1080Ti)]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025262">thread link</a>) | @tosh
<br/>
November 8, 2020 | https://dragan.rocks/articles/20/Going-faster-than-Tensorflow-on-GPU-with-Clojure | <a href="https://web.archive.org/web/*/https://dragan.rocks/articles/20/Going-faster-than-Tensorflow-on-GPU-with-Clojure">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
      
You can <a href="https://www.patreon.com/posts/22476035">adopt a pet function!</a>
Support my work <a href="https://patreon.com/draganrocks">on my Patreon page</a>, and access my <a href="https://www.patreon.com/posts/im-ditching-and-22476348">dedicated discussion server</a>. Can't afford to <a href="https://patreon.com/draganrocks">donate</a>? Ask for a free invite.
<p>November 2, 2020</p>
<p>
    Please share: .
</p>

<p>
    <a href="https://aiprobook.com/">New books are available for subscription.</a>
    </p><p><a href="https://aiprobook.com/deep-learning-for-programmers">
            <img src="http://aiprobook.com/img/dlfp-cover.png">
        </a>
        <a href="https://aiprobook.com/numerical-linear-algebra-for-programmers">
            <img src="http://aiprobook.com/img/lafp-cover.png">
        </a>
    </p>


<p>
A few weeks ago I've shown you how simple Clojure's
<a href="https://github.com/uncomplicate/deep-diamond">Deep Diamond</a>() is, even compared to Keras. I've also mentioned
that it's superfast. Here's how fast it is on the GPU!
</p>

<div id="outline-container-orgc0a2ae3">
<h2 id="orgc0a2ae3">TL;DR Much faster than Keras+TensorFlow on the GPU, too!</h2>
<div id="text-orgc0a2ae3">
<p>
In the <a href="https://dragan.rocks/articles/20/Going-faster-than-TensorFlow-with-Clojure">previous article</a>, we have only compared the libraries on the CPU.
Deep Diamond was considerably faster: 368 seconds vs 509 seconds. Most readers were intrigued,
but, being skeptical as they should be, they complained that CPU performance doesn't matter
anyway, since everybody uses GPU for training convolution networks;
let's do the GPU comparison then.
</p>

<p>
Both Deep Diamond, and Keras with TensorFlow, use <a href="https://developer.nvidia.com/cudnn">Nvidia's cuDNN</a> low level performance
library under the hood, and any difference is due to the higher-level implementation.
</p>

<p>
Deep Diamond completes this training in <b>21</b> seconds while Keras + TensorFlow takes <b>35</b> seconds.
The gap even increased in favor of Deep Diamond! Now the ratio is <b>1.67</b>, in place of 1.38 on the CPU.
</p>
</div>
</div>

<div id="outline-container-org0601d75">
<h2 id="org0601d75">Keras CNN in Python</h2>
<div id="text-org0601d75">
<p>
I repeat the relevant model code for reference. We're
interested in the running time of <code>model.fit</code>, with minimal verbosity,
for 12 epochs. I'm using Nvidia's GTX 1080Ti GPU. Keras code is taken from official Keras examples.
</p>

<div>
<pre>model = Sequential<span>()</span>
model.add<span>(</span>Conv2D<span>(</span>32, kernel_size=<span>(</span>3, 3<span>)</span>,
                 activation='relu',
                 input_shape=<span>(</span>28, 28, 1<span>)</span><span>)</span><span>)</span>
model.add<span>(</span>Conv2D<span>(</span>64, <span>(</span>3, 3<span>)</span>, activation='relu'<span>)</span><span>)</span>
model.add<span>(</span>MaxPooling2D<span>(</span>pool_size=<span>(</span>2, 2<span>)</span><span>)</span><span>)</span>
model.add<span>(</span>Dropout<span>(</span>0.25<span>)</span><span>)</span>
model.add<span>(</span>Flatten<span>()</span><span>)</span>
model.add<span>(</span>Dense<span>(</span>128, activation='relu'<span>)</span><span>)</span>
model.add<span>(</span>Dropout<span>(</span>0.5<span>)</span><span>)</span>
model.add<span>(</span>Dense<span>(</span>num_classes, activation='softmax'<span>)</span><span>)</span>

model.compile<span>(</span>loss=keras.losses.categorical_crossentropy,
              optimizer=Adam<span>(</span>learning_rate=0.01<span>)</span>,
              metrics=<span>[</span>'accuracy'<span>]</span><span>)</span>

s = time.time_ns<span>()</span>
model.fit<span>(</span>x_train, y_train,
          batch_size=128,
          verbose=2,
          epochs=12<span>)</span>
e = time.time_ns<span>()</span>
print<span>(</span><span>(</span>e-s<span>)</span>/<span>(</span>10**9<span>)</span>, <span>" seconds"</span><span>)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org7c1e24e">
<h2 id="org7c1e24e">Deep Diamond CNN in Clojure</h2>
<div id="text-org7c1e24e">
<p>
In Clojure, we're measuring the runtime of the <code>train</code> function.
</p>

<div>
<pre><span>(</span><span>defonce</span> <span>net-bp</span>
  <span>(</span>network <span>(</span>desc <span>[</span>128 1 28 28<span>]</span> <span>:float</span> <span>:nchw</span><span>)</span>
           <span>[</span><span>(</span>convo <span>[</span>32<span>]</span> <span>[</span>3 3<span>]</span> <span>:relu</span><span>)</span>
            <span>(</span>convo <span>[</span>64<span>]</span> <span>[</span>3 3<span>]</span> <span>:relu</span><span>)</span>
            <span>(</span>pooling <span>[</span>2 2<span>]</span> <span>:max</span><span>)</span>
            <span>(</span>dropout<span>)</span>
            <span>(</span>dense <span>[</span>128<span>]</span> <span>:relu</span><span>)</span>
            <span>(</span>dropout<span>)</span>
            <span>(</span>dense <span>[</span>10<span>]</span> <span>:softmax</span><span>)</span><span>]</span><span>)</span><span>)</span>

<span>(</span><span>defonce</span> <span>net</span> <span>(</span>init! <span>(</span>net-bp <span>:adam</span><span>)</span><span>)</span><span>)</span>

<span>(</span>time <span>(</span>train net train-images y-train <span>:crossentropy</span> 12 <span>[]</span><span>)</span><span>)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf9c6799">
<h2 id="orgf9c6799">The books</h2>
<div id="text-orgf9c6799">
<p>
The book <a href="https://aiprobook.com/deep-learning-for-programmers/">Deep Learning for Programmers: An Interactive Tutorial with
CUDA, OpenCL, DNNL, Java, and Clojure</a> teaches the nuts and bolts of neural networks and deep learning
by showing you how Deep Diamond is built, <b>from scratch</b>, in interactive sessions. Each line of code
can be executed and the results inspected in the plain Clojure REPL. The best way to master something is to build
it yourself!
</p>

<p>
It' simple. But fast and powerful!
</p>

<p>
Please subscribe, read the drafts, get the full book soon, and support my work on this free open source library.
</p>
</div>
</div>


    </article></div>]]>
            </description>
            <link>https://dragan.rocks/articles/20/Going-faster-than-Tensorflow-on-GPU-with-Clojure</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025262</guid>
            <pubDate>Sun, 08 Nov 2020 12:14:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Writing Terraform, but with TypeScript]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025186">thread link</a>) | @juliankrispel
<br/>
November 8, 2020 | https://jkrsp.com/writing-terraform-with-typescript/ | <a href="https://web.archive.org/web/*/https://jkrsp.com/writing-terraform-with-typescript/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p>You may or may not have heard about the release of the <a href="https://github.com/hashicorp/terraform-cdk">terraform cdk</a> (short for cloud development kit). It’s HashiCorps answer to the aws cdk. In the words of the projects readme:</p>
<blockquote>
<p>CDK (Cloud Development Kit) for Terraform allows developers to use familiar programming languages to define cloud infrastructure and provision it through HashiCorp Terraform.</p>
</blockquote>
<p>Let’s try this out shall we?</p>
<p>To get the full developer experience, make sure you have <a href="https://github.com/Microsoft/TypeScript/wiki/TypeScript-Editor-Support">typescript support installed for your IDE</a></p>
<p><span>
      <a href="https://jkrsp.com/static/006a51697ab63c90c57788793394fd25/00172/cover.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="autocomplete-terraform" title="autocomplete-terraform" src="https://jkrsp.com/static/006a51697ab63c90c57788793394fd25/fcda8/cover.png" srcset="https://jkrsp.com/static/006a51697ab63c90c57788793394fd25/12f09/cover.png 148w,
https://jkrsp.com/static/006a51697ab63c90c57788793394fd25/e4a3f/cover.png 295w,
https://jkrsp.com/static/006a51697ab63c90c57788793394fd25/fcda8/cover.png 590w,
https://jkrsp.com/static/006a51697ab63c90c57788793394fd25/efc66/cover.png 885w,
https://jkrsp.com/static/006a51697ab63c90c57788793394fd25/00172/cover.png 1044w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<h3>Generating the boilerplate</h3>
<p>Let’s open our terminal and install install cdktf-cli:</p>

<p>Next we’ll initialize the project</p>
<div data-language="bash"><pre><code><span>mkdir</span> hello-cdktf
<span>cd</span> hello-cdktf
cdktf init --template<span>=</span><span>"typescript"</span> --local</code></pre></div>
<p>Answer the two configuration questions and the project boilerplate will be generated.</p>
<p>Now we should see a <code>main.ts</code> file with the following contents in our folder:</p>
<div data-language="ts"><pre><code><span>import</span> <span>{</span> Construct <span>}</span> <span>from</span> <span>'constructs'</span><span>;</span>
<span>import</span> <span>{</span> App<span>,</span> TerraformStack <span>}</span> <span>from</span> <span>'cdktf'</span><span>;</span>

<span>class</span> <span>MyStack</span> <span>extends</span> <span>TerraformStack</span> <span>{</span>
  <span>constructor</span><span>(</span>scope<span>:</span> Construct<span>,</span> name<span>:</span> <span>string</span><span>)</span> <span>{</span>
    <span>super</span><span>(</span>scope<span>,</span> name<span>)</span><span>;</span>

    

  <span>}</span>
<span>}</span>

<span>const</span> app <span>=</span> <span>new</span> <span>App</span><span>(</span><span>)</span><span>;</span>
<span>new</span> <span>MyStack</span><span>(</span>app<span>,</span> <span>'hello-cdktf2'</span><span>)</span><span>;</span>
app<span>.</span><span>synth</span><span>(</span><span>)</span><span>;</span></code></pre></div>
<h3>Adding a provider package and importing modules from it</h3>
<p>After generation finishes you’ll see a message in the console listing instructions of what to do next. To add the prebuilt aws provider (which also gives us all the modules and types that we might want).</p>
<div data-language="bash"><pre><code><span>npm</span> <span>install</span> -a @cdktf/provider-aws</code></pre></div>
<p>Now you can import modules from <code>@cdktf/provider-aws</code> such as <code>AwsProvider</code> and others. We’ll go for the <code>AwsProvider</code>, <code>LambdaFunction</code> and <code>IamRole</code>. Add this at the top of your file:</p>
<div data-language="ts"><pre><code><span>import</span> <span>{</span> AwsProvider<span>,</span> LambdaFunction<span>,</span> IamRole <span>}</span> <span>from</span> <span>'@cdktf/provider-aws'</span><span>;</span></code></pre></div>
<p>and then create an AwsProvider in your stack:</p>
<div data-language="ts"><pre><code><span>new</span> <span>AwsProvider</span><span>(</span><span>this</span><span>,</span> <span>'aws'</span><span>,</span> <span>{</span>
  region<span>:</span> <span>'eu-west-2'</span>
<span>}</span><span>)</span></code></pre></div>
<h3>Adding a lambda function to our stack</h3>
<p>To create a lambda we need to define an IAM role at first. Boring, but made easier by autocomplete of cours. Anyway here’s the default policy:</p>
<div data-language="ts"><pre><code><span>const</span> roleForLambda <span>=</span> <span>new</span> <span>IamRole</span><span>(</span><span>this</span><span>,</span> <span>'iam-role-for-lambda'</span><span>,</span> <span>{</span>
  name<span>:</span> <span>'iam-role-for-lambda'</span><span>,</span>
  assumeRolePolicy<span>:</span> <span>JSON</span><span>.</span><span>stringify</span><span>(</span><span>{</span>
    <span>"Version"</span><span>:</span> <span>"2012-10-17"</span><span>,</span>
    <span>"Statement"</span><span>:</span> <span>[</span>
      <span>{</span>
        <span>"Action"</span><span>:</span> <span>"sts:AssumeRole"</span><span>,</span>
        <span>"Principal"</span><span>:</span> <span>{</span>
          <span>"Service"</span><span>:</span> <span>"lambda.amazonaws.com"</span>
        <span>}</span><span>,</span>
        <span>"Effect"</span><span>:</span> <span>"Allow"</span>
      <span>}</span>
    <span>]</span>
  <span>}</span><span>)</span>
<span>}</span><span>)</span></code></pre></div>
<p>Now we can add a lambda function to the stack like this:</p>
<div data-language="typescript"><pre><code><span>new</span> <span>LambdaFunction</span><span>(</span><span>this</span><span>,</span> <span>'hello-world'</span><span>,</span> <span>{</span>
  filename<span>:</span> process<span>.</span><span>cwd</span><span>(</span><span>)</span> <span>+</span> <span>'hello-world.zip'</span>
  functionName<span>:</span> <span>'hello-world'</span><span>,</span>
  handler<span>:</span> <span>'index.handler'</span><span>,</span>
  runtime<span>:</span> <span>'nodejs12.x'</span><span>,</span>
  role<span>:</span> roleForLambda<span>.</span>arn<span>,</span>
<span>}</span><span>)</span></code></pre></div>
<p>You will need to zip your lambda function - which is usually a separate step before running terraform. For example sake, let’s say you have a file in your project named <code>hello-world.js</code>:</p>
<div data-language="js"><pre><code><span>export</span> <span>const</span> <span>handler</span> <span>=</span> <span>async</span> <span>function</span> <span>(</span><span>)</span> <span>{</span>
  <span>return</span> <span>{</span> hello<span>:</span> world <span>}</span>
<span>}</span></code></pre></div>
<p>Then zip your lambda <code>zip -r lambda.zip hello-world.js</code></p>
<h3>Deploying your stack</h3>
<p>Before you deploy don’t forget need to <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html">have your aws credentials in your path</a>.</p>
<p>Now that you have everything ready you can deploy your stack with <code>cdktf deploy</code>. This command will display an execution plan and ask you if you want to deploy. Press the <code>Y</code> and <code>Enter</code> key to deploy.</p>
<p>Any errors at this stage should be farely self-explanatory. If they don’t make sense, google the error message - other people have likely run into the same problem.</p>
<hr>
<p>If you’re a terraform user and you’ve used the <code>lambda_function</code> module before, you’ll notice that the configuration is exactly the same.</p>
<p>Ultimately, when you run <code>cdktf synth</code> cdktf compiles your javascript/typescript modules into terraforms alternative <a href="https://www.terraform.io/docs/configuration/syntax-json.html"><code>JSON</code> configuration syntax</a>.</p>
<p>This is an extremely powerful feature of terraform’s design since it can be a compile target not just for javascript and typescript, but any kind of language. The open source community could add it’s own language compilers.</p>
<p>Why not write one in rust? 😅</p></section></div>]]>
            </description>
            <link>https://jkrsp.com/writing-terraform-with-typescript/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025186</guid>
            <pubDate>Sun, 08 Nov 2020 12:00:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Serious Engine – Multiplayer Explained]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025148">thread link</a>) | @sklopec
<br/>
November 8, 2020 | https://staniks.github.io/articles/serious-engine-networking-analysis.html | <a href="https://web.archive.org/web/*/https://staniks.github.io/articles/serious-engine-networking-analysis.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
<p><img src="https://staniks.github.io/img/articles/serious-engine/banner.jpg" alt="Banner" title="Banner"></p>

<p>Croteam released the <a href="https://github.com/Croteam-official/Serious-Engine">Serious Engine 1 source code</a> under GNU GPL v2 in 2016, and I've wanted to check it out for quite a while now. My observations here are based on reading and debugging this particular codebase and not reverse-engineering the classics released on GOG and Steam. Keep in mind that comments in code snippets have been replaced to provide more context. Also, some of my conclusions here may be wrong, so you can send me a message over at <a href="https://twitter.com/Sklopec">@Sklopec</a> if you feel like something needs correction.</p>

<blockquote>
  <p><strong>NOTE:</strong> <em>This isn't an in-depth technical analysis, but an overview with more focus on the concepts rather than the implementation. I have skipped over a lot of things for the sake of simplicity. Also, the following sections assume you have at least a vague idea of how Serious Sam looks and plays.</em></p>
</blockquote>







<ul>
<li><strong>2020-11-05</strong> - published.</li>
</ul>



<hr>

<p><strong>Serious Sam</strong> was built from the ground up as a multiplayer game. In a way, it's multiplayer even when you're playing the singleplayer campaign. While this idea may seem unusual at first, it's really just a clever way of abstraction. Let's explore how it works.</p>

<p>Serious Engine supports:</p>

<ul>
<li>singleplayer - offline campaign</li>
<li>multiplayer - online, LAN or local co-op and various game modes
<ul>
<li>supports multiple players on the same client via split-screen!</li>
</ul></li>
<li>demo recording and playback</li>
</ul>

<p>Let's look at the demo functionality first. Serious Engine allows recording and reproduction of gameplay clips or <em>demos</em>. Both multiplayer and singleplayer game sessions can be recorded. In order to record a game, the most naive solution would be to persist the game state of every tick into a file.</p>

<p>However, such an approach has a problem - demo files would be ridiculously large.</p>

<p>Instead, Serious Engine records the entire game state at the beginning of the recording, and then, each tick, records something called <strong>game stream blocks</strong>. For now, think of these as messages which describe events in the game. They can be of these types:</p>

<pre><code>MSG_SEQ_ALLACTIONS,      // Player actions. See below.
MSG_SEQ_ADDPLAYER,       // Add a new player to the game.
MSG_SEQ_REMPLAYER,       // Remove a player from the game.
MSG_SEQ_PAUSE,           // Pause or unpause the game.
MSG_SEQ_CHARACTERCHANGE, // Change an aspect of player's character.
</code></pre>

<p>It's not important that you understand these context of these message types right now - we'll get to that later. For now, let's focus on message type <code>MSG_SEQ_ALLACTIONS</code>, because this is key to understanding how the whole thing works. This particular message type is processed in <code>CSessionState::ProcessGameTick</code>:</p>

<pre><code>FOREACHINSTATICARRAY(ses_apltPlayers, CPlayerTarget, itplt) {
    if (itplt-&gt;IsActive()) {
        // Extract action from message passed as parameter.
        CPlayerAction paAction;
        nmMessage&gt;&gt;paAction;

        // Apply the action to the CPlayerTarget.
        itplt-&gt;ApplyActionPacket(paAction);
    }
}
</code></pre>

<p>The engine deserializes several <code>CPlayerAction</code> objects from the message, one for each active player (since multiplayer games can be recorded as well), and applies these packets. Let's take a look at the <code>CPlayerAction</code> class to see what these packets actually are.</p>

<pre><code>class ENGINE_API CPlayerAction {
public:
    FLOAT3D pa_vTranslation;
    ANGLE3D pa_aRotation;
    ANGLE3D pa_aViewRotation;
    ULONG   pa_ulButtons;
    __int64 pa_llCreated;

    // ...
}
</code></pre>

<p><code>CPlayerAction</code> describes the player's state:</p>

<ul>
<li>player character velocity in world-space (<code>pa_vTranslation</code>)</li>
<li>player character rotation in world-space (<code>pa_aRotation</code>)</li>
<li>player view rotation in world-space (<code>pa_aViewRotation</code>)</li>
<li>buttons currently held down (<code>pa_ulButtons</code>, application defined, independent of control mapping scheme)</li>
<li>timestamp in milliseconds (<code>pa_llCreated</code>, from <a href="https://en.wikipedia.org/wiki/Time_Stamp_Counter">TSC</a>)</li>
</ul>

<p>These messages are generated by the Engine each tick during gameplay as the player interacts with the game (presses buttons, moves the mouse and/or thumbsticks). The messages are continuously serialized during recording and written into the demo file.</p>

<p>So how does reproduction work? The idea is simple - the Engine assumes everything in the game is completely predictable, and the players are the only ones with the power to change things. So in order to record the demo, the Engine only needs to record the entire game state once, and then only record the actions players perform each tick. In order to perform playback, the Engine deserializes the initial game state from the demo file, and then deserializes and applies player actions each tick as if the player was playing the game.</p>

<p>Neat, isn't it?</p>

<p>There's a caveat, though - <strong>this means the Engine's game model has to be completely deterministic.</strong> And it is. You can see an example of this if you peek into the <code>CEntity</code> implementation:</p>

<pre><code>ULONG CEntity::IRnd(void)
{
    return ((_pNetwork-&gt;ga_sesSessionState.Rnd()&gt;&gt;(31-16))&amp;0xFFFF);
}
</code></pre>

<p>where <code>CSessionState::Rnd()</code> is a pseudo-random number generator whose seed is part of the game state and is therefore initialized during game state deserialization:</p>

<pre><code>void CSessionState::Read_t(CTStream *pstr)  // throw char *
{
    // ...
    (*pstr)&gt;&gt;ses_ulRandomSeed;
    // ...
</code></pre>

<p>This makes sure the Engine is able to reproduce the exact same scenario every time we play the demo. If we would, say, use a truly random number generator for some game logic, or even a pseudo-random generator with differing seed, we would get different results every time - the famous <strong>desynchronization</strong>.</p>

<h2>Floating Point Determinism <a name="floating-point-determinism"></a></h2>

<p>There's also the matter of potential desynchronization due to floating point numbers. However, since Serious Sam on PC was originally released on Windows only, they could get away with using one compiler for everything, thus eliminating any sync issues that would emerge due to differences in C runtime library, like different implementations of trigonometry functions.</p>

<p>Similar issues can also arise due to differences in FPU precision. For example, the renderers are DLLs, and different clients might use different renderers. Renderers call various APIs (OpenGL, DirectX), and function calls in some of them might set FPU precision to different than expected. Serious Engine seems to have that covered as well. You can see precision guards like these, sprinkled around:</p>

<pre><code>CSetFPUPrecision FPUPrecision(FPT_24BIT);
</code></pre>

<p>Upon this object's construction, <code>_control87</code> function (MSVC specific) is used to cache the current FPU precision, then apply the new one. Once the object goes out of scope, the cached FPU precision is restored.</p>

<p>In theory, problems like these could also occur due to rounding control, but I haven't seen it explicitly set anywhere in the engine. There's this assert though, but this is just a query.</p>

<pre><code>ASSERT((_controlfp(0, 0)&amp;_MCW_RC)==_RC_NEAR);
</code></pre>

<p>Maybe it just wasn't that big of a deal - perhaps the rounding differences would be small enough not to accumulate significantly over the relatively short time that a session lasts, and thus, not produce any noticeable desynchronization.</p>

<h2>Tick vs. Frame <a name="tick-vs-frame"></a></h2>

<p>Notice how I use the word <strong>tick</strong> instead of <strong>frame</strong>. This is because the game logic tickrate is decoupled from the rendering framerate. Rendering framerate varies depending on the hardware and settings, but seems to be capped at <strong>500 frames per second</strong> internally. However, the game logic rate is constant and limited to <strong>20 ticks per second</strong>. But why do we see smooth movement and animation?</p>

<p><strong>Interpolation</strong>. Serious Engine interpolates between the current and the previous game tick based on time passed between. Try opening the in-game console (<code>~</code> key) and typing this to see how the game looks and feels without interpolation:</p>

<pre><code>/net_bLerping=0
</code></pre>

<p>It's kind of like playing a modern console exclusive. So how does Serious Engine smooth this out?</p>

<p>Animations and movement are interpolated with simple linear interpolation (lerp):</p>

<pre><code>interpolated_state = old_state + (new_state - old_state) * factor;
</code></pre>

<p>where <code>factor</code> is a floating point value in range <code>[0.0f, 1.0f]</code>. The factor in a particular moment in time is calculated as follows (pseudocode):</p>

<pre><code>// Time is in seconds.
float real_delta = time_since_session_started;
float tick_delta = time_of_last_tick - time_of_first_tick;

// 20 FPS logic framerate.
static constexpr float tick_quantum = 1 / 20.0f;

float factor2 = 1.0f - (tick_delta - real_delta) / tick_quantum;
</code></pre>

<p>Or illustrated, if don't mind my terrible handwriting...</p>

<p><img src="https://staniks.github.io/img/articles/serious-engine/frametick.png" alt="Ticks explained" title="Ticks explained"></p>

<p>You can also see the implementation in <code>CSessionState::SetLerpFactor</code>. You will notice there are two interpolation (<code>Lerp</code>) factors - one is for predicted movement, and one is for non-predicted. For now, don't worry about predicted movement - we'll get to prediction and explain how it works later.</p>

<p>Now that we've covered the basic concept of the demo recording and reproduction, think about this: instead of recording the course of the game into a file to be reproduced later, we could send it over the network to be reproduced in real time as we play the game with another person. That is the basic idea of Serious Engine multiplayer.</p>



<hr>

<p>Unfortunately, the internet is a much more complicated environment than a file on your disk drive. Serious Sam is a fast paced game, and making things work fast over the internet is somewhat tricky, especially if you consider the fact that Serious Sam came out in the early 2000s, when a noticeable amount of people were still using 56k modems.</p>

<p>As you may have already guessed, Serious Sam employs a multiplayer model in which every player runs their own simulation and merely receives instructions on what the players have done, much like the demo system. If you glance at the code, you might see function names like <code>CNetworkLibrary::StartPeerToPeer_t</code>, but this is somewhat misleading - Serious Sam's networking isn't really peer to peer, even though the logic is processed akin to the old lockstep multiplayer games.</p>

<p>Serious Engine's networking model is actually client-server.</p>

<p>The basic idea is that, for a single multiplayer session, there is a single server, and the clients connect to it. The server receives messages from clients, …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://staniks.github.io/articles/serious-engine-networking-analysis.html">https://staniks.github.io/articles/serious-engine-networking-analysis.html</a></em></p>]]>
            </description>
            <link>https://staniks.github.io/articles/serious-engine-networking-analysis.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025148</guid>
            <pubDate>Sun, 08 Nov 2020 11:54:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Traefik: Canary deployments with weighted load balancing]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024861">thread link</a>) | @kiyanwang
<br/>
November 8, 2020 | https://iximiuz.com/en/posts/traefik-canary-deployments-with-weighted-load-balancing/ | <a href="https://web.archive.org/web/*/https://iximiuz.com/en/posts/traefik-canary-deployments-with-weighted-load-balancing/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a name="cut"></a>
<a href="https://containo.us/traefik/">Traefik</a> is <del>The Cloud Native Edge Router</del> yet another reverse proxy and load balancer. Omitting all the cloud-native buzzwords, what really makes Traefik different from Nginx, HAProxy, and alike is the automatic and dynamic configurability it provides out of the box. And the most prominent part of it is probably its ability to do automatic service discovery. If you put Traefik in front of <a href="https://docs.traefik.io/providers/overview/#supported-providers">Docker, Kubernetes, or even an old-fashioned VM/bare-metal deployment</a> and show it how to fetch the information about the running services, it'll automagically expose them to the outside world. If you follow some conventions of course...<a name="eofcut"></a></p>
<h2 id="weighted-load-balancing">Weighted load balancing</h2>
<p>If you have a fairly small deployment, up to a single-digit number of machines, and for some reason, you cannot jump into the clouds and enjoy the <a href="https://aws.amazon.com/fargate/">serverless containers</a>, combining Docker and Traefik is an ideal choice. For deployments of such scale using a full-fledged orchestrator like Kubernetes or Mesos would be overkill due to the resource requirements and the inherent complexity of the orchestrator itself. But the fact that we are going to stick with the poor man's solution doesn't mean that we don't want to benefit from the modern development best practices.</p>
<p>So, for simplicity, imagine, we have just one machine. There is a Docker daemon running on it and a <code>traefik</code> container listening on the host's port <em>80</em> (or <em>443</em>, whatever). And we want to deploy our service on that machine. However, we would also like to release the new versions safely by applying the <a href="https://martinfowler.com/bliki/CanaryRelease.html">canary deployment technique</a>:</p>
<p><img src="https://iximiuz.com/traefik-canary-deployments-with-weighted-load-balancing/05-traefik-canary-single-box.png" width="80%">
</p>

<p>Thus, we need to get Traefik to do the weighted load balancing between the Docker containers of the same service. If we could solve the load balancing problem on a single machine, we would simply scale it out to the rest of the fleet:</p>
<p><img src="https://iximiuz.com/traefik-canary-deployments-with-weighted-load-balancing/07-traefik-canary-many-boxes.png" width="80%">
</p>

<p>If every instance of the <em>traefik</em> proxy gets more or less the same number of requests we could achieve the desired share of the canary requests across the whole fleet.</p>
<h2 id="not-invented-here-traefik-v1-vs-traefik-v2">Not invented here (Traefik v1 vs Traefik v2)</h2>
<p>All that proxy kind of software architecturally looks more or less the same. There is always:</p>
<ul>
<li>a front end component dealing with the incoming requests from clients;</li>
<li>an intermediary pipeline dealing with requests transformations;</li>
<li>a back end component dealing with the outgoing requests to upstream services.</li>
</ul>
<p>Every service proxy calls these parts in its own way (<em>entrypoint</em>, <em>server</em>, <em>virtual host</em>, <em>listener</em>, <em>filter</em>, <em>middleware</em>, <em>upstream</em>, <em>endpoint</em>, etc) but Traefik folks went even further...</p>
<p>Historically, Traefik was using <code>entrypoint -&gt; frontend -&gt; backend</code> model:</p>


<p>However, <a href="https://containo.us/blog/back-to-traefik-2-0-2f9aa17be305/">in 2019 the new Traefik major version has been announced</a> bringing a breaking configuration change and a refined approach:</p>


<p>So, in Traefik 2 instead of <em>frontends</em> and <em>backends</em>, we now have <em>routers</em> and <em>services</em>. And there is also an explicit layer of <em>middleware</em> components dealing with extra request transformations. Well, makes perfect sense! But if the v1 documentation basically <a href="https://docs.traefik.io/v1.7/basics/">starts from the architectural overview</a> making the further reading much simpler, in the case of v2 you need to dig down to the <a href="https://docs.traefik.io/routing/overview/">Routing</a> or <a href="https://docs.traefik.io/middlewares/overview/">Middleware</a> concepts to get the first decent diagrams (even though I found all the preceding illustrations very entertaining).</p>
<p>For the newcomers trying to configure Traefik following blog posts on the Internet (well, how doesn't?), beware - as of Q3 2020 most of the articles show the Traefik v1 examples. Config snippets from those articles will simply not work with the Traefik v2 release (often silently). There is a <a href="https://docs.traefik.io/migration/v1-to-v2/">migration page</a> in the official documentation, although IMO it lacks visual representation of the change.</p>
<h2 id="weighted-load-balancing-with-traefik-1">Weighted load balancing with Traefik 1</h2>
<p>Apparently, it <del>is</del> was super simple. First, run the <code>traefik:v1.7</code> container with Docker provider:</p>
<pre><code>docker run -d --rm \
  --name traefik-v1.7 \
  -p 9999:80 \
  -v /var/run/docker.sock:/var/run/docker.sock \
  traefik:v1.7 \
    --docker \
    --docker.exposedbydefault=false</code></pre>
<p>And since it's a v1, we'd need to think in terms of <em>frontends</em> and <em>backends</em>. Apparently, every container would become a server of a particular <em>backend</em>. Conveniently, <a href="https://docs.traefik.io/v1.7/configuration/backends/docker/">the weight of the server could be assigned</a> using <code>traefik.weight</code> label:</p>
<pre><code># Run the current app version (weight 40)
docker run -d --rm --name app_normal \
  --label "traefik.enable=true" \
  --label "traefik.backend=app_weighted" \
  --label "traefik.frontend.rule=Host:example.local" \
  --label "traefik.weight=40" \
  nginx:1.19.1

# Run the contender version (weight 10)
docker run -d --rm --name app_canary \
  --label "traefik.enable=true" \
  --label "traefik.backend=app_weighted" \
  --label "traefik.frontend.rule=Host:example.local" \
  --label "traefik.weight=10" \
  nginx:1.19.2</code></pre>
<p>Send some traffic, just to make sure that it works:</p>
<pre><code>for i in {1..100}; do curl -s -o /dev/null -D - -H Host:example.local localhost:9999 | grep Server; done | sort | uniq -c

&gt;  80 Server: nginx/1.19.1
&gt;  20 Server: nginx/1.19.2</code></pre>
<p>Perfect, 20 out of 100 requests have been served by the canary release container. And if we don't need the canary at some point in time, we can simply stop the container:</p>
<pre><code>docker stop app_canary</code></pre>
<p>Now, if you repeat the traffic probe, 100% of the requests will be served by the <code>app_normal</code> container:</p>
<pre><code>for i in {1..100}; do curl -s -o /dev/null -D - -H Host:example.local localhost:9999 | grep Server; done | sort | uniq -c

&gt;  100 Server: nginx/1.19.1</code></pre>
<p>Easy-peasy, right?</p>
<h2 id="weighted-load-balancing-with-traefik-2">Weighted load balancing with Traefik 2</h2>
<p>And that's where things start getting more complicated... After thoroughly studying the v2 docs, I could not find the <code>weight</code> directive anymore. The closest thing I was able to find was the <a href="https://docs.traefik.io/routing/services/#weighted-round-robin-service">Weighted Round Robin Service</a> (WRR):</p>
<blockquote>
<p>The WRR is able to load balance the requests between multiple services based on weights.</p>
</blockquote>
<p>But there is a couple of limitations with it:</p>
<blockquote>
<ul>
<li>This strategy is only available to load balance between <strong>services</strong> and not between <strong>servers</strong>.</li>
<li>This strategy can be defined currently with the <strong>File</strong> or <strong>IngressRoute</strong> providers.</li>
</ul>
</blockquote>
<p>I.e. no Docker provider support and no direct weight assignment to servers (i.e. containers).</p>
<p>Well, let's try to be creative. Excluding IngressRoute provider (sounds like a Kubernetes thing), we basically have only one option to define WRR service - the File provider. What if we combine it with the Docker provider?</p>
<p>First, define a WRR service in the file:</p>
<pre><code>cat &lt;&lt; "EOF" &gt; file_provider.yml
---
http:
  routers:
    router0:
      service: app_weighted
      rule: "Host(`example.local`)"
  services:
    app_weighted:
      weighted:
        services:
          - name: app_normal@docker  # I'm not defined yet
            weight: 40
          - name: app_canary@docker  # Neither do it
            weight: 10
EOF</code></pre>
<p>Notice, that we haven't defined any <strong>servers</strong> (i.e. containers) there. Instead, we defined an <code>app_weighted</code> service in terms of its sub-services - <code>app_normal</code> and <code>app_canary</code> (there is <code>@docker</code> suffix to say that these services are expected to be defined by the Docker provider).</p>
<p>Let's start the <code>traefik:v2.2</code> container with the Docker and file providers:</p>
<pre><code>docker run -d --rm --name traefik-v2.2 \
  -p 9999:80 \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v `pwd`:/etc/traefik_providers \
  traefik:v2.2 \
    --providers.docker \
    --providers.docker.exposedbydefault=false \
    --providers.file.filename=/etc/traefik_providers/file_provider.yml</code></pre>
<p>Now, it's time to launch the application containers. Since it's the v2, we need to think in terms of <em>routers</em> and <em>services</em> while <a href="https://docs.traefik.io/reference/dynamic-configuration/docker/">configuring</a> the container labels:</p>
<pre><code># Run the current app version (weight 40)
docker run -d --rm --name app_normal_01 \
  --label "traefik.enable=true" \
  --label "traefik.http.services.app_normal.loadbalancer.server.port=80" \
  --label "traefik.http.routers.app_normal_01.entrypoints=traefik" \
  nginx:1.19.1

# Run the contender version (weight 10)
docker run -d --rm --name app_canary_01 \
  --label "traefik.enable=true" \
  --label "traefik.http.services.app_canary.loadbalancer.server.port=80" \
  --label "traefik.http.routers.app_canary_01.entrypoints=traefik" \
  nginx:1.19.2</code></pre>
<p>Let's try to understand the reasoning behind these labels. In general, launching a container means creating a single-server service. If we don't ask otherwise, Traefik 2 implicitly creates such a service using the container's name (replacing <code>_</code> with <code>-</code> for some reasons). On top of that, it adds a routing rule <code>Host(`&lt;container-name-goes-here&gt;`)</code>.</p>
<p>But in our case, we don't want to have arbitrary services for our containers. Instead, we know exactly the name of the service for the normal app containers (<code>app_normal</code>) and the name of the service for the canary app containers (<code>app_canary</code>). Thus, we need to somehow bind the containers (i.e. servers) to the desired services. And a somewhat <em>hacky</em> way of doing that is by using <code>traefik.http.services.&lt;service-name&gt;.loadbalancer.server.port=80</code> label. We don't really need to specify the port here because Traefik would figure it out by itself. But doing so allows us to introduce the <code>app_normal</code> and <code>app_canary</code> services and put the containers in there.</p>
<p>For the second label, remember the default routing rule <code>Host(`&lt;container-name-goes-here&gt;`)</code> that gets assigned to every container automatically? To avoid these containers being accidentally exposed to the outside world, we use the label <code>traefik.http.routers.&lt;stub&gt;.entrypoints=traefik</code>. It's just another hack, binding the containers to the internal <code>entrypoint</code> called <code>traefik</code>. This entrypoint is used for the Traefik's admin API and dashboard and should not be exposed publicly in production environments.</p>
<p>Finally, let's send some traffic, just to make sure that it works:</p>
<pre><code>for i in {1..100}; do curl -s -o /dev/null -D - -H Host:example.local localhost:9999 | grep Server; done | sort | uniq -c

&gt;  80 Server: nginx/1.19.1
&gt;  20 Server: nginx/1.19.2</code></pre>
<p>Great! But what if we need to stop the canary containers? If we just do it right away, the <code>app_weighted@file</code> service will stop functioning due to the disappeared <code>app_canary</code> service. Likely, even the File is <a href="https://docs.traefik.io/reference/dynamic-configuration/file/">a dynamic …</a></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://iximiuz.com/en/posts/traefik-canary-deployments-with-weighted-load-balancing/">https://iximiuz.com/en/posts/traefik-canary-deployments-with-weighted-load-balancing/</a></em></p>]]>
            </description>
            <link>https://iximiuz.com/en/posts/traefik-canary-deployments-with-weighted-load-balancing/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024861</guid>
            <pubDate>Sun, 08 Nov 2020 10:57:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Contract Testing for Node.js Microservices with Pact]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024836">thread link</a>) | @kiyanwang
<br/>
November 8, 2020 | https://codersociety.com/blog/articles/contract-testing-pact | <a href="https://web.archive.org/web/*/https://codersociety.com/blog/articles/contract-testing-pact">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Contract testing helps ensure the compatibility of microservices and decouples the development and deployment processes of software teams. In this article, you'll learn more about contract testing and how to use Pact to verify and ensure your Node.js microservices' API compatibility.</p><div><div><h2 id="ensuring-api-compatibility-in-distributed-systems"><span>Ensuring API compatibility in distributed systems</span></h2><p>The use of microservices is growing in popularity for good reasons. They allow software teams to develop, deploy, and scale software independently to deliver business value faster. Large software projects are broken down into smaller modules, which are easier to understand and maintain. While the internal functionality of each microservice is getting simpler, the complexity in a microservice architecture is moved to the communication layer and often requires the integration between services.</p><p>However, in microservice architectures, you often find service to service communication, leading to increased complexity in the communication layer and the need to integrate other services.</p><p><img src="https://cdn.codersociety.com/uploads/amazon-netflix-services.png">
<em>Figure 1: Distributed systems at Amazon and Netflix</em></p><p>Traditional <a href="https://martinfowler.com/articles/microservice-testing/#testing-integration-introduction">integration testing</a> has proven to be a suitable tool to verify the compatibility of components in a distributed system. However, as the number of services increases, maintaining a fully integrated test environment can become complex, slow, and difficult to coordinate. The increased use of resources can also become a problem, for example when starting up a full system locally or during continuous integration (CI). Contract testing aims to address these challenges – let's find out how.</p><h2 id="what-is-contract-testing"><span>What is contract testing?</span></h2><p>Contract testing is a technique for checking and ensuring the interoperability of software applications in isolation and enables teams to deploy their microservices independently of one another. Contracts are used to define the interactions between API consumers and providers. The two participants must meet the requirements set out in these contracts, such as endpoint definitions and request and response structures.</p><p><img src="https://cdn.codersociety.com/uploads/contract-testing.png">
<em>Figure 2: A contract that defines a HTTP GET interaction</em></p><h2 id="what-is-consumer-driven-contract-testing"><span>What is consumer-driven contract testing?</span></h2><p>Consumer-driven contract testing allows developers to start implementing the consumer (API client) even though the provider (API) isn’t yet available. For this, the consumer writes the contract for the API provider using <a href="https://martinfowler.com/bliki/TestDouble.html">test doubles</a> (also known as API mocks or stubs). Thanks to these test doubles, teams can decouple the implementation and testing of consumer and provider applications so that they’re not dependent on each other. Once the provider has verified its structure against the contract requirements, new consumer versions can be deployed with confidence knowing that the systems are compatible.</p><p><img src="https://cdn.codersociety.com/uploads/consumer-driven-contract-testing.png">
<em>Figure 3: Consumer-driven contract testing</em></p><h2 id="what-is-pact"><span>What is Pact?</span></h2><p><a href="https://pact.io/">Pact</a> is a code-first consumer-driven contract testing tool. Consumer contracts, also called Pacts, are defined in code and are generated after successfully running the consumer tests. The Pact files use JSON format and are used to spin up a Pact Mock Service to test and verify the compatibility of the provider API.</p><p>The tool also offers the so-called Pact Mock Provider, with which developers can implement and test the consumer using a mocked API. This, in turn, accelerates development time, as teams don't have to wait for the provider to be available.</p><p><img src="https://cdn.codersociety.com/uploads/pact-overview.png">
<em>Figure 4: Pact overview</em></p><p>Pact was initially designed for request/response interactions and supports both REST and GraphQL APIs, as well as many different <a href="https://docs.pact.io/implementation_guides">programming languages</a>. For Providers written in languages that don't have native Pact support, you can still use the generic <a href="https://github.com/pact-foundation/pact-provider-verifier">Pact Provider Verification tool</a>.</p><h2 id="try-out-pact"><span>Try out Pact</span></h2><p>Why don't we test things ourselves and see how consumer-driven contract testing with Pact actually works? For this, we use <a href="https://github.com/pact-foundation/pact-js">Pact JS</a>, the Pact library for JavaScript, and Node.js. We've already created a <a href="https://github.com/coder-society/contract-testing-nodejs-pact">sample repository</a> containing an order API, which returns a list of orders. Let’s start by cloning the project and installing the dependencies:</p><div><div><div><div><pre><p><span>$ </span><span>git</span><span> clone https://github.com/coder-society/contract-testing-nodejs-pact.git</span></p><p><span>$ </span><span>cd</span><span> contract-testing-nodejs-pact</span></p><p><span>$ </span><span>npm</span><span> </span><span>install</span></p></pre></div></div></div></div><h2 id="writing-a-pact-consumer-test"><span>Writing a Pact consumer test</span></h2><p>We created a file called <code>consumer.spec.js</code> to define the expectedinteractions between our order API client (consumer) and the order API itself (provider). We expect the following interactions:</p><ul><li>HTTP GET request against path <code>/orders</code> which returns a list of orders.</li><li>The order response matches a defined structure. For this we use <a href="https://github.com/pact-foundation/pact-js#matching">Pact’s Matchers</a>.</li></ul><div><div><div><div><pre><p><span>const</span><span> assert </span><span>=</span><span> </span><span>require</span><span>(</span><span>'assert'</span><span>)</span><span></span></p><p><span></span><span>const</span><span> </span><span>{</span><span> </span><span>Pact</span><span>,</span><span> </span><span>Matchers</span><span> </span><span>}</span><span> </span><span>=</span><span> </span><span>require</span><span>(</span><span>'@pact-foundation/pact'</span><span>)</span><span></span></p><p><span></span><span>const</span><span> </span><span>{</span><span> fetchOrders </span><span>}</span><span> </span><span>=</span><span> </span><span>require</span><span>(</span><span>'./consumer'</span><span>)</span><span></span></p><p><span></span><span>const</span><span> </span><span>{</span><span> eachLike </span><span>}</span><span> </span><span>=</span><span> </span><span>Matchers</span><span></span></p><p><span></span><span>describe</span><span>(</span><span>'Pact with Order API'</span><span>,</span><span> </span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>const</span><span> provider </span><span>=</span><span> </span><span>new</span><span> </span><span>Pact</span><span>(</span><span>{</span><span></span></p><p><span>    port</span><span>:</span><span> </span><span>8080</span><span>,</span><span></span></p><p><span>    consumer</span><span>:</span><span> </span><span>'OrderClient'</span><span>,</span><span></span></p><p><span>    provider</span><span>:</span><span> </span><span>'OrderApi'</span><span>,</span><span></span></p><p><span>  </span><span>}</span><span>)</span><span></span></p><p><span>  </span><span>before</span><span>(</span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> provider</span><span>.</span><span>setup</span><span>(</span><span>)</span><span>)</span><span></span></p><p><span>  </span><span>after</span><span>(</span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> provider</span><span>.</span><span>finalize</span><span>(</span><span>)</span><span>)</span><span></span></p><p><span>  </span><span>describe</span><span>(</span><span>'when a call to the API is made'</span><span>,</span><span> </span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>before</span><span>(</span><span>async</span><span> </span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>      </span><span>return</span><span> provider</span><span>.</span><span>addInteraction</span><span>(</span><span>{</span><span></span></p><p><span>        state</span><span>:</span><span> </span><span>'there are orders'</span><span>,</span><span></span></p><p><span>        uponReceiving</span><span>:</span><span> </span><span>'a request for orders'</span><span>,</span><span></span></p><p><span>        withRequest</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>          path</span><span>:</span><span> </span><span>'/orders'</span><span>,</span><span></span></p><p><span>          method</span><span>:</span><span> </span><span>'GET'</span><span>,</span><span></span></p><p><span>        </span><span>}</span><span>,</span><span></span></p><p><span>        willRespondWith</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>          body</span><span>:</span><span> </span><span>eachLike</span><span>(</span><span>{</span><span></span></p><p><span>            id</span><span>:</span><span> </span><span>1</span><span>,</span><span></span></p><p><span>            items</span><span>:</span><span> </span><span>eachLike</span><span>(</span><span>{</span><span></span></p><p><span>              name</span><span>:</span><span> </span><span>'burger'</span><span>,</span><span></span></p><p><span>              quantity</span><span>:</span><span> </span><span>2</span><span>,</span><span></span></p><p><span>              value</span><span>:</span><span> </span><span>100</span><span>,</span><span></span></p><p><span>            </span><span>}</span><span>)</span><span>,</span><span></span></p><p><span>          </span><span>}</span><span>)</span><span>,</span><span></span></p><p><span>          status</span><span>:</span><span> </span><span>200</span><span>,</span><span></span></p><p><span>        </span><span>}</span><span>,</span><span></span></p><p><span>      </span><span>}</span><span>)</span><span></span></p><p><span>    </span><span>}</span><span>)</span><span></span></p><p><span>    </span><span>it</span><span>(</span><span>'will receive the list of current orders'</span><span>,</span><span> </span><span>async</span><span> </span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>      </span><span>const</span><span> result </span><span>=</span><span> </span><span>await</span><span> </span><span>fetchOrders</span><span>(</span><span>)</span><span></span></p><p><span>      assert</span><span>.</span><span>ok</span><span>(</span><span>result</span><span>.</span><span>length</span><span>)</span><span></span></p><p><span>    </span><span>}</span><span>)</span><span></span></p><p><span>  </span><span>}</span><span>)</span><span></span></p><p><span></span><span>}</span><span>)</span></p></pre></div></div></div></div><p>Run the Pact consumer tests using the following command:</p><div><div><div><div><pre><p><span>$ </span><span>npm</span><span> run test:consumer</span></p><p><span></span><span>&gt;</span><span> contract-testing-nodejs-pact@1.0.0 test:consumer /Users/kentarowakayama/CODE/contract-testing-nodejs-pact</span></p><p><span></span><span>&gt;</span><span> mocha consumer.spec.js</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:22:44.144Z</span><span>]</span><span>  INFO: pact-node@10.11.0/7575 on coder.local: </span></p><p><span>    Creating Pact Server with options: </span></p><p><span>    </span><span>{</span><span>"consumer"</span><span>:</span><span>"OrderClient"</span><span>,</span><span>"cors"</span><span>:false,</span><span>"dir"</span><span>:</span><span>"/Users/kentarowakayama/CODE/contract-testing-nodejs-pact/pacts"</span><span>,</span><span>"host"</span><span>:</span><span>"127.0.0.1"</span><span>,</span><span>"log"</span><span>:</span><span>"/Users/kentarowakayama/CODE/contract-testing-nodejs-pact/logs/pact.log"</span><span>,</span><span>"pactFileWriteMode"</span><span>:</span><span>"overwrite"</span><span>,</span><span>"port"</span><span>:8080,</span><span>"provider"</span><span>:</span><span>"OrderApi"</span><span>,</span><span>"spec"</span><span>:2,</span><span>"ssl"</span><span>:false</span><span>}</span><span></span></p><p><span>  Pact with Order API</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:22:45.204Z</span><span>]</span><span>  INFO: pact@9.13.0/7575 on coder.local: </span></p><p><span>    Setting up Pact with Consumer </span><span>"OrderClient"</span><span> and Provider </span><span>"OrderApi"</span><span></span></p><p><span>        using mock </span><span>service</span><span> on Port: </span><span>"8080"</span><span></span></p><p><span>    when a call to the API is made</span></p><p><span></span><span>[</span><span>{</span><span>"id"</span><span>:1,</span><span>"items"</span><span>:</span><span>[</span><span>{</span><span>"name"</span><span>:</span><span>"burger"</span><span>,</span><span>"quantity"</span><span>:2,</span><span>"value"</span><span>:100</span><span>}</span><span>]</span><span>}</span><span>]</span><span></span></p><p><span>      ✓ will receive the list of current orders</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:22:45.231Z</span><span>]</span><span>  INFO: pact@9.13.0/7575 on coder.local: Pact File Written</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:22:45.231Z</span><span>]</span><span>  INFO: pact-node@10.11.0/7575 on coder.local: Removing Pact process with PID: </span><span>7576</span><span></span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:22:45.234Z</span><span>]</span><span>  INFO: pact-node@10.11.0/7575 on coder.local: </span></p><p><span>    Deleting Pact Server with options: </span></p><p><span>    </span><span>{</span><span>"consumer"</span><span>:</span><span>"OrderClient"</span><span>,</span><span>"cors"</span><span>:false,</span><span>"dir"</span><span>:</span><span>"/Users/kentarowakayama/CODE/contract-testing-nodejs-pact/pacts"</span><span>,</span><span>"host"</span><span>:</span><span>"127.0.0.1"</span><span>,</span><span>"log"</span><span>:</span><span>"/Users/kentarowakayama/CODE/contract-testing-nodejs-pact/logs/pact.log"</span><span>,</span><span>"pactFileWriteMode"</span><span>:</span><span>"overwrite"</span><span>,</span><span>"port"</span><span>:8080,</span><span>"provider"</span><span>:</span><span>"OrderApi"</span><span>,</span><span>"spec"</span><span>:2,</span><span>"ssl"</span><span>:false</span><span>}</span><span></span></p><p><span>  </span><span>1</span><span> passing </span><span>(</span><span>1s</span><span>)</span></p></pre></div></div></div></div><p>The consumer tests generate a Pact contract file named "orderclient-orderapi.json" in the "pacts" folder, which looks like this:</p><div><div><div><div><pre><p><span>{</span><span></span></p><p><span>  </span><span>"consumer"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>"name"</span><span>:</span><span> </span><span>"OrderClient"</span><span></span></p><p><span>  </span><span>}</span><span>,</span><span></span></p><p><span>  </span><span>"provider"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>"name"</span><span>:</span><span> </span><span>"OrderApi"</span><span></span></p><p><span>  </span><span>}</span><span>,</span><span></span></p><p><span>  </span><span>"interactions"</span><span>:</span><span> </span><span>[</span><span></span></p><p><span>    </span><span>{</span><span></span></p><p><span>      </span><span>"description"</span><span>:</span><span> </span><span>"a request for orders"</span><span>,</span><span></span></p><p><span>      </span><span>"providerState"</span><span>:</span><span> </span><span>"there are orders"</span><span>,</span><span></span></p><p><span>      </span><span>"request"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>        </span><span>"method"</span><span>:</span><span> </span><span>"GET"</span><span>,</span><span></span></p><p><span>        </span><span>"path"</span><span>:</span><span> </span><span>"/orders"</span><span></span></p><p><span>      </span><span>}</span><span>,</span><span></span></p><p><span>      </span><span>"response"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>        </span><span>"status"</span><span>:</span><span> </span><span>200</span><span>,</span><span></span></p><p><span>        </span><span>"headers"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>        </span><span>}</span><span>,</span><span></span></p><p><span>        </span><span>"body"</span><span>:</span><span> </span><span>[</span><span></span></p><p><span>          </span><span>{</span><span></span></p><p><span>            </span><span>"id"</span><span>:</span><span> </span><span>1</span><span>,</span><span></span></p><p><span>            </span><span>"items"</span><span>:</span><span> </span><span>[</span><span></span></p><p><span>              </span><span>{</span><span></span></p><p><span>                </span><span>"name"</span><span>:</span><span> </span><span>"burger"</span><span>,</span><span></span></p><p><span>                </span><span>"quantity"</span><span>:</span><span> </span><span>2</span><span>,</span><span></span></p><p><span>                </span><span>"value"</span><span>:</span><span> </span><span>100</span><span></span></p><p><span>              </span><span>}</span><span></span></p><p><span>            </span><span>]</span><span></span></p><p><span>          </span><span>}</span><span></span></p><p><span>        </span><span>]</span><span>,</span><span></span></p><p><span>        </span><span>"matchingRules"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>          </span><span>"$.body"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>            </span><span>"min"</span><span>:</span><span> </span><span>1</span><span></span></p><p><span>          </span><span>}</span><span>,</span><span></span></p><p><span>          </span><span>"$.body[*].*"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>            </span><span>"match"</span><span>:</span><span> </span><span>"type"</span><span></span></p><p><span>          </span><span>}</span><span>,</span><span></span></p><p><span>          </span><span>"$.body[*].items"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>            </span><span>"min"</span><span>:</span><span> </span><span>1</span><span></span></p><p><span>          </span><span>}</span><span>,</span><span></span></p><p><span>          </span><span>"$.body[*].items[*].*"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>            </span><span>"match"</span><span>:</span><span> </span><span>"type"</span><span></span></p><p><span>          </span><span>}</span><span></span></p><p><span>        </span><span>}</span><span></span></p><p><span>      </span><span>}</span><span></span></p><p><span>    </span><span>}</span><span></span></p><p><span>  </span><span>]</span><span>,</span><span></span></p><p><span>  </span><span>"metadata"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>"pactSpecification"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>      </span><span>"version"</span><span>:</span><span> </span><span>"2.0.0"</span><span></span></p><p><span>    </span><span>}</span><span></span></p><p><span>  </span><span>}</span><span></span></p><p><span></span><span>}</span></p></pre></div></div></div></div><h2 id="verifying-the-consumer-pact-against-the-api-provider"><span>Verifying the consumer pact against the API provider</span></h2><p>We can now use the generated Pact contract file to verify our order API. To do so, run the following command:</p><div><div><div><div><pre><p><span>$ </span><span>npm</span><span> run test:provider</span></p><p><span></span><span>&gt;</span><span> contract-testing-nodejs-pact@1.0.0 test:provider /Users/kentarowakayama/CODE/contract-testing-nodejs-pact</span></p><p><span></span><span>&gt;</span><span> node verify-provider.js</span></p><p><span>Server is running on http://localhost:8080</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:21:15.038Z</span><span>]</span><span>  INFO: pact@9.13.0/7077 on coder.local: Verifying provider</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:21:15.050Z</span><span>]</span><span>  INFO: pact-node@10.11.0/7077 on coder.local: Verifying Pacts.</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:21:15.054Z</span><span>]</span><span>  INFO: pact-node@10.11.0/7077 on coder.local: Verifying Pact Files</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:21:16.343Z</span><span>]</span><span>  WARN: pact@9.13.0/7077 on coder.local: No state handler found </span><span>for</span><span> </span><span>"there are orders"</span><span>, ignoring</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:21:16.423Z</span><span>]</span><span>  INFO: pact-node@10.11.0/7077 on coder.local: Pact Verification succeeded.</span></p></pre></div></div></div></div><p>The code to verify the provider can be found in <a href="https://github.com/coder-society/contract-testing-nodejs-pact/blob/master/verify-provider.js">verify-pact.js</a> and looks like this:</p><div><div><div><div><pre><p><span>const</span><span> path </span><span>=</span><span> </span><span>require</span><span>(</span><span>'path'</span><span>)</span><span></span></p><p><span></span><span>const</span><span> </span><span>{</span><span> </span><span>Verifier</span><span> </span><span>}</span><span> </span><span>=</span><span> </span><span>require</span><span>(</span><span>'@pact-foundation/pact'</span><span>)</span><span></span></p><p><span></span><span>const</span><span> </span><span>{</span><span> startServer </span><span>}</span><span> </span><span>=</span><span> </span><span>require</span><span>(</span><span>'./provider'</span><span>)</span><span></span></p><p><span></span><span>startServer</span><span>(</span><span>8080</span><span>,</span><span> </span><span>async</span><span> </span><span>(</span><span>server</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>console</span><span>.</span><span>log</span><span>(</span><span>'Server is running on http://localhost:8080'</span><span>)</span><span></span></p><p><span>  </span><span>try</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>await</span><span> </span><span>new</span><span> </span><span>Verifier</span><span>(</span><span>{</span><span></span></p><p><span>      providerBaseUrl</span><span>:</span><span> </span><span>'http://localhost:8080'</span><span>,</span><span></span></p><p><span>      pactUrls</span><span>:</span><span> </span><span>[</span><span>path</span><span>.</span><span>resolve</span><span>(</span><span>__dirname</span><span>,</span><span> </span><span>'./pacts/orderclient-orderapi.json'</span><span>)</span><span>]</span><span>,</span><span></span></p><p><span>    </span><span>}</span><span>)</span><span>.</span><span>verifyProvider</span><span>(</span><span>)</span><span></span></p><p><span>  </span><span>}</span><span> </span><span>catch</span><span> </span><span>(</span><span>error</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>console</span><span>.</span><span>error</span><span>(</span><span>'Error: '</span><span> </span><span>+</span><span> error</span><span>.</span><span>message</span><span>)</span><span></span></p><p><span>    process</span><span>.</span><span>exit</span><span>(</span><span>1</span><span>)</span><span></span></p><p><span>  </span><span>}</span><span></span></p><p><span>  server</span><span>.</span><span>close</span><span>(</span><span>)</span><span></span></p><p><span></span><span>}</span><span>)</span></p></pre></div></div></div></div><p>This starts the API server and runs the Pact Verifier. After successful verification, we know that the order API and the client are compatible and can be deployed with confidence.</p><h2 id="wrapping-up"><span>Wrapping up</span></h2><p>By now you should have a good understanding of contract testing and how consumer-driven contract testing works. You also learned about Pact and how to use it to ensure the …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://codersociety.com/blog/articles/contract-testing-pact">https://codersociety.com/blog/articles/contract-testing-pact</a></em></p>]]>
            </description>
            <link>https://codersociety.com/blog/articles/contract-testing-pact</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024836</guid>
            <pubDate>Sun, 08 Nov 2020 10:51:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Digital sound processing tutorial]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024835">thread link</a>) | @cunidev
<br/>
November 8, 2020 | http://yehar.com/blog/?p=121 | <a href="https://web.archive.org/web/*/http://yehar.com/blog/?p=121">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<p>In 1998, I had some extra time while others were reading for final exams of the senior high school, and got into digital signal processing. I wrote as I learned, and here is the result. It is not entirely accurate in places but may serve as a nice tutorial into the world of audio DSP. Previously this document was called Yehar's digital sound processing tutorial for the braindead, but I have kinda grown out of  my scene identity over the years. Enjoy the ASCII art!</p>
<p>Chapters:</p>
<ul>
<li><a href="#dspstuff_1">Foreword</a></li>
<li><a href="#dspstuff_2">About sampled sound</a></li>
<li><a href="#dspstuff_3">Adding two sine waves together</a></li>
<li><a href="#dspstuff_4">What's a filter?</a></li>
<li><a href="#dspstuff_5">Filter types: FIR and IIR</a></li>
<li><a href="#dspstuff_6">Interpolation of sampled sound</a></li>
<li><a href="#dspstuff_7">About filter design</a></li>
<li><a href="#dspstuff_8">Pole-zero IIR filter design</a></li>
<li><a href="#dspstuff_9">Some pole-zero IIR filters</a></li>
<li><a href="#dspstuff_10">Windowed FIR filter design</a></li>
<li><a href="#dspstuff_11">Filter implementation</a></li>
<li><a href="#dspstuff_12">Positive and negative frequencies</a></li>
<li><a href="#dspstuff_13">Frequency shifting</a></li>
<li><a href="#dspstuff_14">Nature of sound and music</a></li>
<li><a href="#dspstuff_15">Flanger </a></li>
<li><a href="#dspstuff_16">Wavetable synthesis</a></li>
</ul>
<p>Bonus chapters:</p>
<ul>
<li><a href="#dspstuff_17">Shuffling IIR equations</a></li>
<li><a href="#dspstuff_18">A collection of IIR filters</a></li>
<li><a href="#dspstuff_19">A collection of FIR filters</a></li>
</ul>
<h2><a name="dspstuff_1">Foreword</a></h2>
<p>This is written for the audio digital signal processing enthusiasts (as the title suggests ;) and others who need practical information on the subject. If you don't have this as a "linear reading experience" and encounter difficulties, check if there's something to help you out in the previous chapters.</p>
<p>In filter frequency response plots, linear frequency and magnitude scales are used. Page changes are designed for 60+ lines/page printers.</p>
<p>Chapter "Shuffling IIR equations" is written by my big brother Kalle. And, thanks to Timo Tossavainen for sharing his DSP knowledge!</p>
<p>Copy and use this text freely.</p>
<p>by Olli Niemitalo, o@iki.fi</p>
<h2><a name="dspstuff_2">About sampled sound</a></h2>
<p>Note that "sample" can mean (1) a sampled sound or (2) a samplepoint!</p>
<p>Sampled sound data is a pile of samples, amplitude values taken from the actual sound wave. Sampling rate is the frequency of the "shots". For example, if the frequency is 44100, 44100 samples have been taken in one second.</p>
<p>Here's an example of sampling:</p>
<pre>                                           _0---0_
                                         _/       --0__
        0_                __0---0__    _0              -0---0___
          \_          _-0-         -0--                         0-__
        +---0==-+--=0---+---+---+---+---+---+---+---+---+---+---+---0==-+---+
               -0--                                                    -0_
                                                                          --0
                                            &lt;---&gt;
                                        1/Samplerate</pre>
<p>The original sound is the curve, and "0"s are the sampled points. The horizontal straight line is the zero level.</p>
<p>A sampled sound can only represent frequencies up to half the samplerate. This is called the Nyquist frequency. An easy proof: You need to have stored at least two samplepoints per wave cycle, the top and the bottom of the wave to be able to reconstruct it later on:</p>
<pre>        0\     /0\     /0\     /0\     /0\     /0\     /0\     /0\     /0\
          |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
        +-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+
          |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
           \0/     \0/     \0/     \0/     \0/     \0/     \0/     \0/     \0</pre>
<p>If you try to include above Nyquist frequencies in your sampled sound, all you get is extra distortion as they appear as lower frequencies.</p>
<h2><a name="dspstuff_3">Adding two sine waves together</a></h2>
<p>A Sound consists of frequency components. They all look exactly like sine waves, but they have different frequencies, phases and amplitudes. Let's look at a single frequency:</p>
<pre>                | __        __        __        __      |
                |/  \      /  \      /  \      /  \     |
                /----\----/----\----/----\----/----\----/
                |     \__/      \__/      \__/      \__/|
                |                                       |</pre>
<p>Now, we take the same frequency from another sound and notice that it has the same amplitude, but the opposite (rotated 180 degrees) phase.</p>
<pre>                |      __        __        __        __ |
                |     /  \      /  \      /  \      /  \|
                \----/----\----/----\----/----\----/----\
                |\__/      \__/      \__/      \__/     |
                |                                       |</pre>
<p>Merging two signals is done simply by adding them together. If we do the same with these two sine waves, the result will be:</p>
<pre>                |                                       |
                |                                       |
                +=======================================+
                |                                       |
                |                                       |</pre>
<p>It gets silent. If we think of other cases, where the phase difference is less than 180 degrees, we get sine waves that all have different amplitudes and phases, but the same frequency.</p>
<p>Here's the way to calculate the phase and the amplitude of the resulting sinewave... Convert the amplitude and phase into one complex number, where angle is the phase, and absolute value the amplitude.</p>
<pre>        amplitude*e^(i*phase) = amplitude*cos(phase)+i*amplitude*sin(phase)</pre>
<p>If you do this to both of the sinewaves, you can add them together as complex numbers.</p>
<p>Example:</p>
<pre>        (Wave A) amplitude 1, phase 0, (Wave B) amplitude 1, phase 90 degrees

                   _---|---_         _---0---_         _---|---_ 0
                  /    |    \       /    |    \       /    |    \
                 |     |     |     |     |     |     |     |     |
                -|-----+-----0- + -|-----+-----|- = -|-----+-----|-
                 |     |     |     |     |     |     |     |     |
                  \_   |   _/       \_   |   _/       \_   |   _/
                    ---|---           ---|---           ---|---</pre>
<p>As you see, the phase of the new sine wave is 45 degrees and the amplitude sqrt(1^2+1^2) = sqrt(2) = about 1.4</p>
<p>It is very important that you understand this, because in many cases, it is more practical to present the amplitude and the phase of a frequency as a complex number.</p>
<p>When adding two sampled sounds together, you may actually wipe out some frequencies, those that had opposite phases and equal amplitudes. The average amplitude of the resulting sound is (for independent originals) sqrt(a^2+b^2) where a and b are the amplitudes of the original signals.</p>
<h2><a name="dspstuff_4">What's a filter?</a></h2>
<p>The main use of a filter is to scale the amplitudes of the frequency components in a sound. For example, a "lowpass filter" mutes all frequency components above the "cutoff frequency", in other words, multiplies the amplitudes by 0. It lets through all the frequencies below the cutoff frequency unattenuated.</p>
<h3>Magnitude</h3>
<p>If you investigate the behaviour of a lowpass filter by driving various sinewaves of different frequencies through it, and measure the amplifications, you get the "magnitude frequency response". Here's a plot of the magnitude frequency response curve of a lowpass filter:</p>
<pre>                1-+----------------------_                              |
                  |                       \                             |
                  |       Audible          |          Inaudible         |
                  |                        |                            |
                  |                        |                            |
                  |                         \_                          |
                0-+---------------------------===========================
                  |                        |                            |
                 0Hz                Cutoff frequency                   max</pre>
<p>Frequency is on the "-" axis and amplification on the "|" axis. As you see, the amplification (= scaling) of the frequencies below the cutoff frequency is 1. So, their amplitudes are not affected in any way. But the amplitudes of frequencies above the cutoff frequency get multiplied by zero so they vanish.</p>
<p>Filters never add any new frequency components to the sound. They can only scale the amplitudes of already existing frequencies. For example, if you have a completely quiet sample, you can't get any sound out of it by filtering. Also, if you have a sine wave sample and filter it, the result will still be the same sine wave, only maybe with different amplitude and phase - no other frequencies can appear.</p>
<h3>Phase</h3>
<p>Professionals never get tired of reminding us how important it is not to forget the phase. The frequency components in a sound have their amplitudes and... phases. If we take a sine wave and a cosine wave, we see that they look alike, but they have a phase difference of pi/2, one fourth of a full cycle. Also, when you play them, they sound alike. But, try wearing a headset and play the sinewave on the left channel and the cosine wave on the right channel. Now you hear the difference!</p>
<p>Phase itself doesn't contain important information for us so it's not heard, but the phase difference, of a frequency, between the two ears can be used in estimating the position of the origin of the sound so it's heard.</p>
<p>Filters have a magnitude frequency response, but they also have a phase frequency response. Here's an example curve that could be from a lowpass filter:</p>
<pre>              +pi-+-----------------------------------------------------+
                  |                    ___                              |
                  |      _________-----   \                             |
                0-+======------------------|---------====================
                  |                         \___-----                   |
                  |                                                     |
              -pi-+-----------------------------------------------------+
                  |                        |                            |
                 0Hz                Cutoff frequency                   max</pre>
<p>If you filter a sound, the values from the phase frequency response …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://yehar.com/blog/?p=121">http://yehar.com/blog/?p=121</a></em></p>]]>
            </description>
            <link>http://yehar.com/blog/?p=121</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024835</guid>
            <pubDate>Sun, 08 Nov 2020 10:51:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[“Only a Woman If You Have Breasts? That's Nonsense”]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25024715">thread link</a>) | @Tomte
<br/>
November 8, 2020 | https://www.spiegel.de/international/zeitgeist/a-personal-experience-with-breast-cancer-only-a-woman-if-you-have-breasts-that-s-nonsense-a-88da903d-2aa5-428b-99d6-a4fc4ce28a68 | <a href="https://web.archive.org/web/*/https://www.spiegel.de/international/zeitgeist/a-personal-experience-with-breast-cancer-only-a-woman-if-you-have-breasts-that-s-nonsense-a-88da903d-2aa5-428b-99d6-a4fc4ce28a68">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-article-el="body">
<section data-app-hidden="true">


</section>
<section>
<div>
<section>
<p><em>NOTE TO OUR READERS: Journalist Hajo Schumacher, who conducted this interview, and former politician Silvana Koch-Mehrin have known each other for many years. It is on the basis of this mutual trust that Koch-Mehrin responded to the questions that many of our readers found to be impertinent and unsettling. We apologize for not having been clear about their rapport from the very beginning.</em></p>
</section>


<section>
<p><strong>DER SPIEGEL:</strong> Ms. Koch-Mehrin, you were diagnosed with breast cancer last fall. How are you doing today?</p><p><strong>Koch-Mehrin:</strong> I'm doing well, thank you. I feel healthy. And the prognoses look good; it will likely stay that way.</p><p><strong>DER SPIEGEL:</strong> What physical and psychological effects did your experience leave you with.</p><p><strong>Koch-Mehrin:</strong> So much has changed that "leave" is the wrong word. Cancer diagnosis, mastectomy, chemo and radiation therapy: It is an existential experience. Very little is like it was before, both physically and psychologically. But it's not worse, just different.</p><p><strong>DER SPIEGEL:</strong> Can you give us an example?</p>
</section>


<section>
<p><strong>Koch-Mehrin:</strong> The side effects I experienced from chemo included a runny and bloody nose, my eyes would tear up and I had to constantly clear my throat. Picking up and holding small things became difficult and I experienced a loss of feeling in my fingertips. My hair, eyebrows and eyelashes fell out. "Before," I took those things, and the ability to do things, for granted. Now, I see it as a gift.</p><p><strong>DER SPIEGEL:</strong> Describe the moment when you received your diagnosis.</p><p><strong>Koch-Mehrin:</strong> I can precisely recall every detail - the vase on the table, the pictures and books on the shelf. I was in the United States for work and was sitting in a friend's office when my doctor called me: cancer. It was a shock because I felt so healthy. I thought it was just my normal annual checkup. Simply uttering the sentence "I have cancer" sounded to me like "I am dying." Still, even after the telephone call, I went to all of the meetings on my schedule. I was like a robot, which is probably a protective mechanism. The doctor's optimism helped me a great deal in getting over the shock. I can only urge all women, even if you're under 50, to please get screened regularly. The stage at which cancer is discovered makes a huge difference.</p>
</section>

<section>
<p><strong>DER SPIEGEL:</strong> Is breast cancer more taboo than other varieties of cancer?</p><p><strong>Koch-Mehrin:</strong> When people learn from somebody that he or she has cancer, they often fall silent. They don't know what to say. And how should they? "You'll be fine" sounds rather desperate and doesn't help the person who is sick. We live in a society that suppresses things: We still ignore illness and death far too often. Because of that, many women remain silent about their breast cancer diagnosis. On the one hand, they do so because they themselves are shaken up, but also because they want to spare others from having to talk about something they simply can't talk about.</p>
</section>

<section>
<p><strong>DER SPIEGEL:</strong> You were long seen as a kind of superwoman during your political career with the liberal Free Democrats (FDP), combining career and family, with never a hair out of place. You were also fond of playing that role. How has breast cancer changed your feeling of femininity?</p><p><strong>Koch-Mehrin:</strong> Do I hear the male perspective seeping through? Long hair on the head and no hair on the legs? Only a woman if you have breasts? That's nonsense. Take a look at the self-confident women at #goingflat or #onebreastpride. That is a kind of femininity that has nothing to do with conventions. I admire it.</p><p><strong>DER SPIEGEL:</strong> You haven't given an interview for nine years. Are you considering a return to politics?</p><p><strong>Koch-Mehrin:</strong> If I wanted to return to politics, I certainly wouldn't do so with an interview. My focus is elsewhere entirely. I decided to do this interview to address the silence surrounding cancer and to encourage openness in dealing with serious illnesses.</p><p><strong>DER SPIEGEL:</strong> What did you find particularly helpful during the most difficult times?</p><p><strong>Koch-Mehrin:</strong> I was overwhelmed by the readiness to help, the warmth and the empathy I experienced from family and friends, but also from people whom I hardly knew. My husband and brother shaved their hair off in solidarity. Particularly in moments of despondence, I found it encouraging to hear about how open other women were about their illness.</p><p><strong>DER SPIEGEL:</strong> How great is the danger of remission?</p><p><strong>Koch-Mehrin:</strong> There isn't a single day when I don't think of cancer. The statistical improbability of remission is certainly reassuring, but every checkup sets me off on an emotional rollercoaster.</p><p><strong>DER SPIEGEL:</strong> How did you tell your daughters about your diagnosis?</p><p><strong>Koch-Mehrin:</strong> I was open and honest with them because I wanted them to continue trusting me.</p><p><strong>DER SPIEGEL:</strong> What were the most difficult moments?</p>
</section>
<blockquote>

<div>
<p>"I wanted to be strong for my children."</p>
</div>

</blockquote>
<section>
<p><strong>Koch-Mehrin:</strong> The first and last chemotherapy sessions were extremely challenging. At the first one, I was so afraid of the side effects that I almost went crazy. By the last session, my mental strength was almost completely used up. I didn't want to do it anymore. I was only able to make it through because I wanted to be strong for my children.</p><p><strong>DER SPIEGEL:</strong> What aspects of the experience did you underestimate?</p><p><strong>Koch-Mehrin:</strong> That it is a marathon. The treatment goes on for so long, it was almost nine months for me, and it is difficult. And afterwards, it still isn't over. A small example: fingernails take months to grow back normally. And I will be facing regular follow-up appointments for the next 10 years. Plus, some side effects are permanent.</p><p><strong>DER SPIEGEL:</strong> What were the most important lessons you learned?</p><p><strong>Koch-Mehrin:</strong> For the first time in my life, I really became acquainted with fear. And I had to learn how to deal with it. It's easy for me to say, but it's not easy to do.</p><p><strong>DER SPIEGEL:</strong> Whether you wanted to or not, you fulfilled the classic stereotypes of the blond during your political career. What was it like to be bald and have to wear a wig?</p><p><strong>Koch-Mehrin:</strong> I missed these kinds of questions so much …</p><p><strong>DER SPIEGEL:</strong> … it is my pleasure.</p><p><strong>Koch-Mehrin:</strong> Promoting clichés is apparently still part of the standard repertoire of journalism. The virologist Sandra Ciesek is a token woman at the side of Christian Drosten, of course. In the Reykjavik Index, which measures the influence stereotypes have on opinions, Germany doesn't do particularly well. Such questions, which subconsciously strengthen prejudices, contribute to that.</p><p><strong>DER SPIEGEL:</strong> Fine, but you were pretty good at the blond game. You knew very well that in a male-dominated party such as the FDP, you got a lot of attention and that you wouldn't have risen through the ranks so quickly if you had been a man. That is exactly why those who were envious of you abandoned you in 2011 and were happy to see you fall. Did your illness change any of your political positions?</p><p><strong>Koch-Mehrin:</strong> Everyone should have access to first-class health care, irrespective of how much money they have. COVID-19 has made us all realize how vulnerable we are.</p><p><strong>DER SPIEGEL:</strong> You sound like the Social Democratic health expert Karl Lauterbach.</p><p><strong>Koch-Mehrin:</strong> Take a look at what (Free Democrat) Daniel Bahr proposed back when he was health minister: a legally mandated health-care system rooted in liberalism. It was an intelligent and affordable program that put the focus on the patient.</p><p><strong>DER SPIEGEL:</strong> Do you miss the FDP?</p><p><strong>Koch-Mehrin:</strong> Some fellow party members became real friends of mine, and I've stayed in touch with a lot of them, even if most are no longer active in politics. I am still convinced that Germany needs a strong liberal party. But I am no longer engaged in party politics. Today, I work with and for women politicians around the world in a strictly non-partisan manner. I am head of Women Political Leaders, a foundation which aims at increasing both the number and the influence of women in politics.</p><p><strong>DER SPIEGEL:</strong> Will you vote for the FDP in the general election next year?</p><p><strong>Koch-Mehrin:</strong> I will always vote for a liberal party.</p><p><strong>DER SPIEGEL:</strong> Many cancer patients look for reasons why they fell ill, whether it be stress or an unhealthy lifestyle. What explanation did you come up with?</p>
</section>
<blockquote>

<div>
<p>"The most important thing is that the women in question feel good about the decision they make, and not whether others like it or not."</p>
</div>

</blockquote>
<section>
<p><strong>Koch-Mehrin:</strong> It is pretty much impossible to not search for potential causes. There have been huge advances in treatments and cures for breast cancer, but the "why" question has never been adequately answered. The pill is a blessing, because it provides women with self-determination. But I'm not the only one furious at the fact that even 50 years after the pill's introduction, hormones are the focus of contraception, even though the correlation with breast cancer is clear and proven. The focus must be more squarely on women's health.</p><p><strong>DER SPIEGEL:</strong> The Reykjavik Global Forum, which is organized by your foundation every year, recognizes initiatives that are important for women. This year, the prize has been awarded to the pink ribbon, the symbol for the fight against breast cancer. Why?</p><p><strong>Koch-Mehrin:</strong> The pink ribbon, which was introduced exactly 30 years ago, is now a global symbol of the fight against breast cancer. Because women - and now also men – have joined together around the world for the fight, it has been a huge success story. Many women are now able to survive breast cancer because it has been possible to destigmatize the illness, collect donations, finance research and improve treatments. That is what we are recognizing. Prize recipients are the American Nancy Brinker, the first to use the pink ribbon back in the 1990s, and Vigdís Finnbogadóttir of Iceland, a breast cancer survivor and activist in addition to being the first female head of state in the world.</p><p><strong>DER SPIEGEL:</strong> The Berlin-based music journalist Anja Caspary is extremely open about her double mastectomy, not shying away from wearing tight tops in public, which confuses some. What do you think about her confrontational strategy?</p><p><strong>Koch-Me…</strong></p></section></div></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.spiegel.de/international/zeitgeist/a-personal-experience-with-breast-cancer-only-a-woman-if-you-have-breasts-that-s-nonsense-a-88da903d-2aa5-428b-99d6-a4fc4ce28a68">https://www.spiegel.de/international/zeitgeist/a-personal-experience-with-breast-cancer-only-a-woman-if-you-have-breasts-that-s-nonsense-a-88da903d-2aa5-428b-99d6-a4fc4ce28a68</a></em></p>]]>
            </description>
            <link>https://www.spiegel.de/international/zeitgeist/a-personal-experience-with-breast-cancer-only-a-woman-if-you-have-breasts-that-s-nonsense-a-88da903d-2aa5-428b-99d6-a4fc4ce28a68</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024715</guid>
            <pubDate>Sun, 08 Nov 2020 10:31:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[One Week of NixOS]]>
            </title>
            <description>
<![CDATA[
Score 244 | Comments 168 (<a href="https://news.ycombinator.com/item?id=25024639">thread link</a>) | @jaemoe
<br/>
November 8, 2020 | https://jae.moe/blog/2020/11/one-week-of-nixos/ | <a href="https://web.archive.org/web/*/https://jae.moe/blog/2020/11/one-week-of-nixos/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <h4>One week of NixOS</h4><p>
            Reading time: 4 minutes.
            </p><p>As you may have seen I on Mastodon, I am testing NixOS for over a week now and here is a few comments about how the distro works and what doesn’t work for me.</p>










<p>First, everything is NixOS is declared in a configuration file located at <code>/etc/nixos/configuration.nix</code> where you declare packages you want to install, services to start, udev rules, ECT, you get it.</p>
<p>This is the first very weird thing as you don’t really install packages with the CLI directly (even if you can), instead you modify the config file and rebuild the system with <code>nixos-rebuild switch</code>.</p>
<p>NixOS is made in such a way that everything aims to be reproducible. An example: if you want to get the exact same setup as I have, you can just take <a href="https://forge.tedomum.net/jae/nixos-configs">my configs repo</a>, clone it on your NixOS installation, run <code>nixos-rebuild switch</code> and voilà, you will have the exact same programs as me installed in the same way with the same version (roughly).</p>
<p>In this distro, adding a user is really easy and goes like this:</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span></code></pre></td>
<td>
<pre><code data-lang="nixos"><span>{</span> <span>config</span><span>,</span> <span>pkgs</span><span>,</span> <span>...</span> <span>}:</span>

<span>{</span>

    <span>users</span><span>.</span><span>groups</span><span>.</span><span>plugdev</span> <span>=</span> <span>{};</span>

    <span>users</span><span>.</span><span>users</span><span>.</span><span>jae</span> <span>=</span> <span>{</span>
        <span>isNormalUser</span> <span>=</span> <span>true</span><span>;</span>
        <span>extraGroups</span> <span>=</span> <span>[</span> <span>"wheel"</span> <span>"docker"</span> <span>"adbusers"</span> <span>"plugdev"</span> <span>];</span>
        <span>shell</span> <span>=</span> <span>pkgs</span><span>.</span><span>zsh</span><span>;</span>
        <span>packages</span> <span>=</span> <span>with</span> <span>pkgs</span><span>;</span> <span>[</span>
            <span># Games</span>
            <span>minetest</span> <span>stepmania</span> <span>lutris-free</span> <span>pcsx2</span>
            <span># Misc audio / video / image</span>
            <span>pulseeffects</span> <span>ffmpeg-full</span> <span>obs-studio</span> <span>inkscape</span> <span>krita</span>
            <span># Useful software</span>
            <span>mumble</span> <span>qbittorrent</span> <span>libreoffice</span> <span>ledger-live-desktop</span>
            <span># Dev</span>
            <span>jetbrains</span><span>.</span><span>idea-community</span> <span>lazygit</span> <span>gnome3</span><span>.</span><span>zenity</span> <span>insomnia</span> <span>jetbrains</span><span>.</span><span>rider</span> <span>mono</span> <span>msbuild</span> <span>dotnet-sdk_3</span> <span>ganttproject-bin</span> <span>kubectx</span>
            <span># SDR</span>
            <span>rtl-sdr</span> <span>gqrx</span> <span>gpredict</span> <span>noaa-apt</span> <span>welle-io</span>
        <span>];</span>
    <span>};</span>
    <span>users</span><span>.</span><span>extraGroups</span><span>.</span><span>vboxusers</span><span>.</span><span>members</span> <span>=</span> <span>[</span> <span>"jae"</span> <span>];</span>
<span>}</span></code></pre></td></tr></tbody></table>
</div>
</div>
<p>Let’s see what everything does!</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span>1
</span></code></pre></td>
<td>
<pre><code data-lang="nixos"> <span>users</span><span>.</span><span>groups</span><span>.</span><span>plugdev</span> <span>=</span> <span>{};</span></code></pre></td></tr></tbody></table>
</div>
</div><p>
Creates a group named <code>plugdev</code>, don’t pay attention to it, it is just a test for the Ledger Live application.</p>
<p>
Tells the os that the current user is a normal one. It will create a home folder and set the default shell.</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span>1
</span></code></pre></td>
<td>
<pre><code data-lang="nixos"> <span>extraGroups</span> <span>=</span> <span>[</span> <span>"wheel"</span> <span>"docker"</span> <span>"adbusers"</span> <span>"plugdev"</span> <span>];</span></code></pre></td></tr></tbody></table>
</div>
</div><p>
Here, we are setting the groups the user is in to grant special permissions.</p>
<p>
As you may have guessed it, we are setting the default user shell to ZSH.</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span></code></pre></td>
<td>
<pre><code data-lang="nixos"> <span>packages</span> <span>=</span> <span>with</span> <span>pkgs</span><span>;</span> <span>[</span>
            <span># Games</span>
            <span>minetest</span> <span>stepmania</span> <span>lutris-free</span> <span>pcsx2</span>
            <span># Misc audio / video / image</span>
            <span>pulseeffects</span> <span>ffmpeg-full</span> <span>obs-studio</span> <span>inkscape</span> <span>krita</span>
            <span># Useful software</span>
            <span>mumble</span> <span>qbittorrent</span> <span>libreoffice</span> <span>ledger-live-desktop</span>
            <span># Dev</span>
            <span>jetbrains</span><span>.</span><span>idea-community</span> <span>lazygit</span> <span>gnome3</span><span>.</span><span>zenity</span> <span>insomnia</span> <span>jetbrains</span><span>.</span><span>rider</span> <span>mono</span> <span>msbuild</span> <span>dotnet-sdk_3</span> <span>ganttproject-bin</span> <span>kubectx</span>
            <span># SDR</span>
            <span>rtl-sdr</span> <span>gqrx</span> <span>gpredict</span> <span>noaa-apt</span> <span>welle-io</span>
        <span>];</span></code></pre></td></tr></tbody></table>
</div>
</div><p>
There, we are installing per-user packages because yes, NixOS supports that, any user can have its own packages that others users can’t access.</p>
<blockquote>
<p>Correction from <em>hvdijk</em> on Hacker News, “<em>Other users can access those packages if they want to. Those packages won’t show up in other users' $PATH, so other users will not be affected by them, but they could see what’s in /nix/store if they wanted to. This matters when you’re thinking of putting private data (such as an encryption key) in a package: it’s vital that you don’t do that on a multi-user system.</em>”</p>
</blockquote>
<p>Configuring NixOS for a daily use is at the end very easy (although I am getting some trouble to get Ledger Live working; which is the biggest problem I’ve had so far).</p>
<p>Now, let’s talk about where I got some trouble.
As you may know it, I am a dev and every day I need to compile, test, run and so on.
NixOS gave me some trouble to only run some programs from source such as <a href="https://element.io/">Element Desktop</a> or <a href="https://img.tedomum.net/">TeDomum IMG</a> as the way the system is built, lots of directories are read-only and programs can’t be installed globally through NPM or PIP.
I ended up using Docker to build the apps (even if it took a bit more time).</p>
<p>Needless to say, almost every other project worked.
If you want NodeJS to start a project for instance, you can just do <code>nix-shell -p nodejs</code> and here you go, a shell with nodejs installed, ready to do what you want.</p>
<p>At the end, NixOS brings very interesting concepts such as a really great reproducibility but new users can feel lost as its way to work is really different from conventional Linux distributions.
I’ll give NixOS more time and write a follow-up in some time to see how everything went.</p>
<p>If you want to give it a shot, the <a href="https://nixos.org/">official NixOS website awaits you</a>!</p>
<p>That’s all for today,
I’ll see you next time!
If you like my content, <a href="https://jae.moe/blog/index.xml">don’t forget to subscribe through RSS</a>!</p>

            
                <p><a href="https://news.ycombinator.com/item?id=25024639">Talk about it on Hacker News!</a>
            
        </p></div></div>]]>
            </description>
            <link>https://jae.moe/blog/2020/11/one-week-of-nixos/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024639</guid>
            <pubDate>Sun, 08 Nov 2020 10:17:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[More Free Design Resources for Developers]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024623">thread link</a>) | @moeminm
<br/>
November 8, 2020 | https://blog.moeminmamdouh.com/20-more-free-design-resources-for-developers | <a href="https://web.archive.org/web/*/https://blog.moeminmamdouh.com/20-more-free-design-resources-for-developers">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1604830238180/sI-dunVYp.png?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div><div><div><p>Subscribe to <!-- -->my<!-- --> newsletter and never miss <!-- -->my<!-- --> upcoming articles</p></div></div></div><div itemprop="text"><p>In continuation of the previous  <a target="_blank" href="https://blog.moeminmamdouh.com/20-free-design-resources-for-developers">post</a>, here are 30+ more free design resources for developers.</p>

<ul>
<li><p><a target="_blank" href="https://mixkit.co/?ref=blog.moeminmamdouh.com">Mixkit</a>: Awesome Stock Video Clips, Stock Music, Sound Effects &amp; Video Templates. All available for free!</p>
</li>
<li><p><a target="_blank" href="https://www.pexels.com/?ref=blog.moeminmamdouh.com">Pexels</a>: Free stock photos you can use everywhere. ✓ Free for commercial use ✓ No attribution required.</p>
</li>
<li><p><a target="_blank" href="https://unsplash.com/?ref=blog.moeminmamdouh.com">Unsplash</a>: The internet’s source of freely-usable images. Powered by creators everywhere.</p>
</li>
<li><p><a target="_blank" href="https://burst.shopify.com/?ref=blog.moeminmamdouh.com">Burst</a>: Browse thousands of beautiful copyright-free images. All our pictures are free to download for personal and commercial use, no attribution required.</p>
</li>
<li><p><a target="_blank" href="https://coverr.co/?ref=blog.moeminmamdouh.com">Coverr</a>: Beautiful Free Stock Video Footage.</p>
</li>
</ul>

<ul>
<li><p><a target="_blank" href="https://www.typewolf.com/?ref=blog.moeminmamdouh.com">Typewolf</a>: Typewolf helps designers and developers choose the perfect font combination for their next design project.</p>
</li>
<li><p><a target="_blank" href="https://fonts.google.com/?ref=blog.moeminmamdouh.com">Google Fonts</a>: Making the web more beautiful, fast, and open through great typography.</p>
</li>
<li><p><a target="_blank" href="https://fontsarena.com/?ref=blog.moeminmamdouh.com">FontsArena</a>: Free fonts, free alternatives to premium fonts, done-for-you-research articles. Open startup.</p>
</li>
<li><p><a target="_blank" href="https://www.speakhuman.today/?ref=blog.moeminmamdouh.com">Speak Human</a>: Generate human centric microcopy for all purposes.</p>
</li>
<li><p><a target="_blank" href="https://www.fontfabric.com/free-fonts/?ref=blog.moeminmamdouh.com">Font Foundry</a>: A digital type foundry crafting retail fonts and custom typography for various brands. Sharing a passion for premium typefaces, calligraphy and lettering.</p>
</li>
</ul>

<ul>
<li><p><a target="_blank" href="https://www.amazon.com/dp/0321965515">Don't Make Me Think</a></p>
</li>
<li><p><a target="_blank" href="https://www.amazon.com/Hooked-How-Build-Habit-Forming-Products/dp/1591847788/">Hooked: How to Build Habit-Forming Products</a></p>
</li>
<li><p><a target="_blank" href="https://www.amazon.com/dp/0465050654">The Design of Everyday Things</a></p>
</li>
<li><p><a target="_blank" href="https://www.amazon.com/Things-Designer-People-Voices-Matter/dp/0321767535/">100 Things Every Designer Needs to Know About People</a></p>
</li>
</ul>

<ul>
<li><p><a target="_blank" href="https://www.checklist.design/?ref=blog.moeminmamdouh.com">Checklist Design</a>: A collection of the best design practices.</p>
</li>
<li><p><a target="_blank" href="https://icons8.com/upscaler/?ref=blog.moeminmamdouh.com">AI Image Upscaler</a>: Enhance image resolution with AI.</p>
</li>
<li><p><a target="_blank" href="https://www.glyphy.io/?ref=blog.moeminmamdouh.com">Glyphy</a>: Copy and paste glyphs with ease.</p>
</li>
</ul>

<ul>
<li><p><a target="_blank" href="https://2.flexiple.com/scale/all-illustrations?ref=blog.moeminmamdouh.com">Scale</a>[One new high-quality, open-source illustration each day. Use our color-picker to adapt the illustrations to your brand identity!)</p>
</li>
<li><p><a target="_blank" href="https://formito.com/tools/favicon/?ref=blog.moeminmamdouh.com">Free Favicon Maker</a>: Generate favicons and watch a live preview in your tab.</p>
</li>
<li><p><a target="_blank" href="https://www.colorsandfonts.com/?ref=blog.moeminmamdouh.com">Colors and Fonts</a>: Find colors and typography combinations ready to copy paste in one click.</p>
</li>
</ul>
<p>As always, if you have any resources you'd like me to post in the next issue, do let me know and I'll have it included if it's suitable.</p>
<p>See you next time!</p>
</div></div></section></div></div>]]>
            </description>
            <link>https://blog.moeminmamdouh.com/20-more-free-design-resources-for-developers</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024623</guid>
            <pubDate>Sun, 08 Nov 2020 10:15:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pro Rata and User Centric Distribution Models: A Comparative Study (2017)]]>
            </title>
            <description>
<![CDATA[
Score 83 | Comments 86 (<a href="https://news.ycombinator.com/item?id=25024552">thread link</a>) | @iamacyborg
<br/>
November 8, 2020 | http://www.digitalmedia.fi/wp-content/uploads/2018/02/UC_report_final_171213.pdf | <a href="https://web.archive.org/web/*/http://www.digitalmedia.fi/wp-content/uploads/2018/02/UC_report_final_171213.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://www.digitalmedia.fi/wp-content/uploads/2018/02/UC_report_final_171213.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024552</guid>
            <pubDate>Sun, 08 Nov 2020 10:01:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Protecting TimeMachine Backups from Itself]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024547">thread link</a>) | @gingerlime
<br/>
November 8, 2020 | https://blog.gingerlime.com/2020/going-down-the-time-machine-rabbit-hole/ | <a href="https://web.archive.org/web/*/https://blog.gingerlime.com/2020/going-down-the-time-machine-rabbit-hole/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-2650">
		<!-- .entry-header -->

	
	<div>
		
<h2>Going down the time machine rabbit hole…</h2>



<p>I love the fact that MacOS comes with TimeMachine built-in, and I also really appreciate its simplicity. It makes backups easy and accessible even for non-technical people. It gets messy though if you also want to have real offsite backups however.</p>



<p>TimeMachine works great with a USB external HD, but things get tricky over the network.</p>



<p>I own a small Synology NAS, and I managed to mount a TimeMachine volume and get it to backup to that volume. The problem started when the volume size started to grow. I could set a quota on the volume, but for some strange reason, when the quota is reached, TimeMachine just started failing without a clear reason. There’s no way to tell TimeMachine to only keep X versions, or keep disk storage below a certain threshold. It’s <em>supposed</em> to prune backups automatically, but seems to fail with my network volume.</p>



<h2>tmutil to the rescue</h2>



<p>After a bit of digging, I discovered a useful command-line utility that helps managing TimeMachine. <code>tmutil</code> lets you list your backups and also delete specific backups. I could manually easily prune the oldest backup of TimeMachine using <code>tmutil listbackups | head -1 | xargs -I {} sudo tmutil delete "{}"</code>.</p>



<p>How difficult could it be to automate this script so it runs regularly and keeps X copies?</p>



<h2>The script</h2>



<p>The script itself was pretty simple. Please don’t bash my bash skills, but I hope the code is clear and seems to do the job</p>


<pre title="">#!/bin/bash

# keeps backups for up to 7 days
KEEP=7

function timestamp() {
    date -jf '%F-%H%M%S' "$1" '+%s'
}

LAST_BACKUP=$(/usr/bin/tmutil listbackups | /usr/bin/tail -n1)
LAST_BACKUP_DATE=$(basename "$LAST_BACKUP")
LAST_TIMESTAMP=$(timestamp $LAST_BACKUP_DATE)

OLDEST_BACKUP=$(/usr/bin/tmutil listbackups | /usr/bin/head -n1)
OLDEST_BACKUP_DATE=$(basename "$OLDEST_BACKUP")
OLDEST_TIMESTAMP=$(timestamp $OLDEST_BACKUP_DATE)

DIFF=$(( ($LAST_TIMESTAMP - $OLDEST_TIMESTAMP) / (24*3600) ))

while [[ $DIFF &gt; $KEEP ]]; do
    echo "cleaning"
    sudo tmutil delete "$OLDEST_BACKUP"
    OLDEST_BACKUP=$(/usr/bin/tmutil listbackups | /usr/bin/head -n1)    
    OLDEST_BACKUP_DATE=$(basename "$OLDEST_BACKUP")
    OLDEST_TIMESTAMP=$(timestamp $OLDEST_BACKUP_DATE)
    DIFF=$(( ($LAST_TIMESTAMP - $OLDEST_TIMESTAMP) / (24*3600) ))
done
</pre>


<p>Easy-peasy, right? There were two problems left to solve:</p>



<ol><li>run this script on schedule, let’s say once a day or once a week</li><li>make sure we can run this as root, so we don’t need to use <code>sudo</code> (or avoid the password prompt if we do run sudo)</li></ol>



<h2>How difficult can it be?</h2>



<p>Running scheduled jobs on MacOS is quite a bit different from Linux. You could still somehow use a cron job, but it’s not recommended, might be deprecated, and generally not the “right way” to do things. You are supposed to create a <code>launchd</code> script… Ok, so without digging too deep, here’s a simple launchd file for running a command every 24 hours. You need to place this file under <code>~/Library/LaunchAgents/</code> and give it a name like <code>org.whatever.script-name.plist</code></p>


<pre title="">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd"&gt;
&lt;plist version="1.0"&gt;
&lt;dict&gt;
    &lt;key&gt;Label&lt;/key&gt;
    &lt;!-- The label should be the same as the filename without the extension --&gt;
    &lt;string&gt;org.whatever.script-name&lt;/string&gt;
    &lt;!-- Specify how to run your program here --&gt;
    &lt;key&gt;ProgramArguments&lt;/key&gt;
    &lt;array&gt;
        &lt;string&gt;/path/to/code/timemachine-cleanup.sh&lt;/string&gt;
    &lt;/array&gt;
    &lt;!-- Run every 24 hours --&gt;
    &lt;key&gt;StartInterval&lt;/key&gt;
    &lt;integer&gt;86400&lt;/integer&gt;&lt;!-- seconds --&gt;
&lt;/dict&gt;
&lt;/plist&gt;
</pre>


<p>Then you would need to run this command, so the launchd script gets scheduled</p>



<p><code>launchctl load org.whatever.script-name.plist</code></p>



<p>note: if you need to stop your script from running, you can use the same command, but with <code>unload</code> instead to unload it.</p>



<h2>First problem: Run as root</h2>



<p>The first problem is that this job now runs as your own user id. This is fixable if you move the file to the global /Library/LaunchAgents, make the owner of the file root and use sudo to load it.  It runs as root, but tmutil now fails. Why? because some tools/operations like tmutil require something called Full Disk Access (FDA). </p>



<h2>Second problem: Full Disk Access</h2>



<p>Here’s an interesting post explaining <a href="https://eclecticlight.co/2020/02/15/why-privileged-commands-may-never-be-allowed/" target="_blank" rel="noreferrer noopener">why some privileged commands are not allowed on MacOS</a>, and a few <a href="https://apple.stackexchange.com/questions/338213/how-to-run-a-launchagent-that-runs-a-script-which-causes-failures-because-of-sys" target="_blank" rel="noreferrer noopener">problems/workarounds being suggested </a>on StackOverflow’s Ask Different.</p>



<p>You can get those commands to run from your terminal, if you add your terminal to the list of allowed apps with Full Disk Access</p>



<figure><img loading="lazy" width="853" height="758" src="https://blog.gingerlime.com/assets/CleanShot-2020-11-08-at-10.15.10.png" alt="" srcset="https://blog.gingerlime.com/assets/CleanShot-2020-11-08-at-10.15.10.png 853w, https://blog.gingerlime.com/assets/CleanShot-2020-11-08-at-10.15.10-300x267.png 300w, https://blog.gingerlime.com/assets/CleanShot-2020-11-08-at-10.15.10-768x682.png 768w" sizes="(max-width: 706px) 89vw, (max-width: 767px) 82vw, 740px"></figure>



<p>The problem however, is that for some strange reason, you cannot simply add your script to the list. Or you could add it, but it will still be blocked. I have no clue why.</p>



<h2>Workaround: wrap your script as an app</h2>



<p>You could use Automator.app or Script Editor to do that. You can then add this wrapper app to give it Full Disk Access permissions. That works, but then there’s no way to make it run as root. So we’re back to the first problem again… :-/</p>



<h2>Workaround #2: sudoers with NOPASSWD</h2>



<p>So we won’t run it as root, but use sudo. Then we’re prompted for a password, which we tried to avoid for a scheduled job.</p>



<p>We can however add any specific command to be allowed to run with sudo without a password. We’ll run <code>sudo visudo</code> and then edit the file to add this line:</p>



<p><code>myuser ALL=(ALL) NOPASSWD: /usr/bin/tmutil</code></p>



<p>If you want to be even more secure, you can include the sha256 hash of tmutil. First get the hash by running <code>sha256sum /usr/bin/tmutil</code> and then adding this sha to your sudoers file. Mine looks like this</p>



<p><code>myuser ALL=(ALL) NOPASSWD: sha256:57a753bd2bef425205684630a676765913e1adca7ec0a9d73c205e4da32488c6 /usr/bin/tmutil</code></p>



<h2>Alternatives to app wrapper</h2>



<p>One thing I noticed with the wrapper app is that when it launches, it’s visible on my desktop. It could even grab my input for a moment, which is slightly annoying. It also felt weird to have a whole app just to wrap a script to give it Full Disk Access.</p>



<p>One alternative mentioned on the <a href="https://apple.stackexchange.com/questions/338213/how-to-run-a-launchagent-that-runs-a-script-which-causes-failures-because-of-sys" target="_blank" rel="noreferrer noopener">linked StackOverflow page</a> above is to compile a binary app to wrap your shell script. I didn’t try it, since I am not using any compiled languages regularly.</p>



<p>Another thing I did try and will probably end up using is launching the script via the terminal. How? I already have passwordless (public key) SSH remote login access set up on my Mac. So I simply modified the launchd script slightly so it launches my script via ssh</p>


<pre title="">&lt;key&gt;ProgramArguments&lt;/key&gt;
&lt;array&gt;
    &lt;string&gt;ssh&lt;/string&gt;
    &lt;string&gt;myuser@myhost.local&lt;/string&gt;
    &lt;string&gt;/path/to/code/timemachine-cleanup.sh&lt;/string&gt;
&lt;/array&gt;
</pre>


<p>Since terminal is already set with FDA, this seems to work :)</p>



<p>It’s a bit of an awkward workaround to ssh to your own host with your own user in order to run a script, but it’s not that much different from compiling a binary to wrap your script, or wrapping it in an “app”.</p>



<h2>Why TimeMachine?</h2>



<p>I guess some people might rightfully question why I’m using TimeMachine and not a more flexible backup tool for Mac. I did consider it and tested a couple of options like CarbonCopyCloner and others. As far as I could tell, none of the commercial backup tools can create a full machine image on a remote network storage like TimeMachine does. If you know of something that does this, please let me know!</p>



<p>… and for those backup freaks out there: No. the Synology volume isn’t my only backup copy. I then also run restic to clone the TimeMachine volume to Backblaze B2 for offsite safekeeping.</p>
	</div><!-- .entry-content -->

	 <!-- .entry-footer -->
</article></div>]]>
            </description>
            <link>https://blog.gingerlime.com/2020/going-down-the-time-machine-rabbit-hole/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024547</guid>
            <pubDate>Sun, 08 Nov 2020 10:01:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pongmechanik]]>
            </title>
            <description>
<![CDATA[
Score 33 | Comments 7 (<a href="https://news.ycombinator.com/item?id=25024499">thread link</a>) | @remix2000
<br/>
November 8, 2020 | http://cyberniklas.de/pongmechanik/indexen.html | <a href="https://web.archive.org/web/*/http://cyberniklas.de/pongmechanik/indexen.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://cyberniklas.de/pongmechanik/indexen.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024499</guid>
            <pubDate>Sun, 08 Nov 2020 09:52:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[PostgreSQL Configuration for Humans]]>
            </title>
            <description>
<![CDATA[
Score 256 | Comments 40 (<a href="https://news.ycombinator.com/item?id=25024224">thread link</a>) | @sharjeelsayed
<br/>
November 8, 2020 | https://postgresqlco.nf/en/doc/param/ | <a href="https://web.archive.org/web/*/https://postgresqlco.nf/en/doc/param/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <div>
    <div id="welcome">
      
      <div lang="en">
        <h4><p>Your postgresql.conf documentation and recommendations.</p><p>Our mission is to help you tune and optimize your PostgreSQL configuration.</p><p>With around 270 configuration parameters in <span>postgresql.conf</span>, plus all the knobs in pg_hba.conf, it is definitely a difficult task!</p><p>How many parameters do you tune? 1? 8? 32? Ever tuned more than 64? We aim to make PostgreSQL configuration possible for HUMANS.</p></h4>
      </div>
    </div>
  </div>
</div></div>]]>
            </description>
            <link>https://postgresqlco.nf/en/doc/param/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024224</guid>
            <pubDate>Sun, 08 Nov 2020 08:52:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I built the product hunt launch video gallery using Gatsby JavaScript]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024206">thread link</a>) | @iliashad
<br/>
November 8, 2020 | https://iliashaddad.com/blog/how-i-built-the-product-hunt-launch-video-gallery-using-gatsby-js-google-sheets-and-product-hunt-api | <a href="https://web.archive.org/web/*/https://iliashaddad.com/blog/how-i-built-the-product-hunt-launch-video-gallery-using-gatsby-js-google-sheets-and-product-hunt-api">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><main><div><article><div><p>In this tutorial, I'll be sharing how I built the Product Hunt launch video gallery which use Product Hunt API, store the data in Google Sheets and display it using Gatsby JS. You can check out the website from <a href="https://product-hunt-launch-video.netlify.app/">this link</a></p><p>What I'll be covering today:</p><ul><li><p>Use Netlify functions to query data from Product Hunt API and store it in Google Sheets</p></li><li><p>Query data from Google Sheets and display it on Gatsby website</p></li><li><p>Download remote images to a local folder to utilize the power of Gatsby Images</p></li><li><p>Send GET requests using IFTTT to get fresh product launch video</p></li></ul><p>Let's do it.</p><h3>Use Netlify functions to query data from Product Hunt API and store it in Google Sheets</h3><ul><li>Install Gatsby CLI using Yarn</li></ul><pre data-language="" data-index="0"><code><span><span></span></span>
<span><span>yarn global add gatsby-cli</span></span>
<span><span></span></span></code></pre><p>or NPM</p><pre data-language="" data-index="1"><code><span><span></span></span>
<span><span>npm i gatsby-cli -g</span></span>
<span><span></span></span></code></pre><ul><li>Install Netlify Dev CLI to test Netlify functions locally using Yarn</li></ul><p>yarn global add netlify-cli</p><p>or NPM</p><pre data-language="" data-index="2"><code><span><span></span></span>
<span><span>npm i netlify-cli -g</span></span>
<span><span></span></span></code></pre><ul><li>Create new gatsby website using CLI</li></ul><pre data-language="" data-index="3"><code><span><span></span></span>
<span><span>gatsby new product-hunt-launch-video https://github.com/oddstronaut/gatsby-starter-tailwind</span></span>
<span><span></span></span></code></pre><ul><li>Run the Gatsby website</li></ul><pre data-language="" data-index="4"><code><span><span></span></span>
<span><span>cd product-hunt-launch-video &amp;&amp; gatsby develop</span></span>
<span><span></span></span></code></pre><ul><li>Create functions folder inside the root folder</li></ul><pre data-language="" data-index="5"><code><span><span></span></span>
<span><span>cd src &amp;&amp; mkdir functions</span></span>
<span><span></span></span></code></pre><ul><li>Create product-hunt.js ﬁle inside functions folder</li></ul><pre data-language="" data-index="6"><code><span><span></span></span>
<span><span>touch product-hunt.js</span></span>
<span><span></span></span></code></pre><ul><li>In the root directory, install node-fetch to fetch data from GraphQl API, dotenv to load</li></ul><p>env variables</p><pre data-language="" data-index="7"><code><span><span></span></span>
<span><span>yarn add node-fetch dotenv</span></span>
<span><span></span></span></code></pre><ul><li>We need to initialize a new netlify project</li></ul><pre data-language="" data-index="8"><code><span><span></span></span>
<span><span>netlify init</span></span>
<span><span></span></span></code></pre><p>and choose "No, I will connect this directory with GitHub ﬁrst" and follow the instructions</p><p>mentioned the command line</p><ul><li>Create netlify.toml ﬁle to conﬁg the netlﬁy website</li></ul><pre data-language="" data-index="9"><code><span><span></span></span>
<span><span>[build]</span></span>
<span><span></span></span>
<span><span>command = "yarn run build"</span></span>
<span><span></span></span>
<span><span>functions = "functions" # netlify dev uses this directory to scaffold a</span></span>
<span><span></span></span>
<span><span>publish = "public"</span></span>
<span><span></span></span></code></pre><ul><li>Create env ﬁle to hold Product Hunt API Key</li></ul><pre data-language="" data-index="10"><code><span><span></span></span>
<span><span>PH_ACCESS_TOKEN=</span></span>
<span><span></span></span></code></pre><p>In order to get your Product Hunt API Key, you need to login with your Product Hunt account</p><p>and check this <a href="https://api.producthunt.com/v2/oauth/applications">wesbite</a><a href="https://api.producthunt.com/v2/oauth/applications"> </a>and create new application and when you create a new application.</p><p>you'll have a Developer Access Token which never expires and we'll use this token in env ﬁle</p><p>In the product-hunt.js, we'll create the function to consume Product Hunt API</p><pre data-language="" data-index="11"><code><span><span></span></span>
<span><span>require("dotenv").config()</span></span>
<span><span></span></span>
<span><span>const fetch = require("node-fetch")</span></span>
<span><span></span></span>
<span><span>exports.handler = function (event, context, callback) {</span></span>
<span><span></span></span>
<span><span></span></span>
<span><span> const requestBody = {</span></span>
<span><span>    query: `</span></span>
<span><span>        {</span></span>
<span><span>            posts(order:RANKING) {</span></span>
<span><span>            </span></span>
<span><span>              edges {</span></span>
<span><span>                node {</span></span>
<span><span>                  name</span></span>
<span><span>                  url</span></span>
<span><span>                  topics {</span></span>
<span><span>                    edges {</span></span>
<span><span>                      node {</span></span>
<span><span>                         name</span></span>
<span><span>                      }</span></span>
<span><span>                    }</span></span>
<span><span>                  }</span></span>
<span><span>                  votesCount</span></span>
<span><span>                  media {</span></span>
<span><span>                    videoUrl</span></span>
<span><span>                    url</span></span>
<span><span>                  }</span></span>
<span><span>                  tagline</span></span>
<span><span>                </span></span>
<span><span>                  createdAt</span></span>
<span><span>                </span></span>
<span><span>                  </span></span>
<span><span>                }</span></span>
<span><span>              }</span></span>
<span><span>              </span></span>
<span><span>            }</span></span>
<span><span>          }</span></span>
<span><span>            `,</span></span>
<span><span>  };</span></span>
<span><span></span></span>
<span><span>fetch("https://api.producthunt.com/v2/api/graphql", {</span></span>
<span><span></span></span>
<span><span>method: "POST",</span></span>
<span><span></span></span>
<span><span>headers: {</span></span>
<span><span></span></span>
<span><span>authorization: `Bearer ${process.env.PH\_ACCESS\_TOKEN}`,</span></span>
<span><span></span></span>
<span><span>"Content-type": "Application/JSON",</span></span>
<span><span></span></span>
<span><span>},</span></span>
<span><span></span></span>
<span><span>body: JSON.stringify(requestBody),</span></span>
<span><span></span></span>
<span><span>})</span></span>
<span><span></span></span>
<span><span>.then(res =&gt; res.json())</span></span>
<span><span></span></span>
<span><span></span></span>
<span><span></span></span>
<span><span></span></span>
<span><span></span></span>
<span><span>.then(({ data }) =&gt; {</span></span>
<span><span></span></span>
<span><span>callback(null, {</span></span>
<span><span></span></span>
<span><span>statusCode: 200,</span></span>
<span><span></span></span>
<span><span>body: JSON.stringify({</span></span>
<span><span></span></span>
<span><span>message: "Success",</span></span>
<span><span></span></span>
<span><span>data: data.posts.edges,</span></span>
<span><span></span></span>
<span><span>}),</span></span>
<span><span></span></span>
<span><span>})</span></span>
<span><span></span></span>
<span><span>})</span></span>
<span><span></span></span>
<span><span>.catch(err =&gt; console.log(err))</span></span>
<span><span></span></span>
<span><span>}</span></span></code></pre><p>You need to run this script and check http://localhost:8888/.netlify/functions/product-hunt</p><p>to send a GET request to this netlify function and then send a POST request to Product</p><p>Hunt GraphQl API</p><pre data-language="" data-index="12"><code><span><span></span></span>
<span><span>netlify dev</span></span>
<span><span></span></span></code></pre><ul><li><p>We need to filter the product that had a launch video </p><pre data-language="" data-index="13"><code><span><span>    if (data) {</span></span>
<span><span>       const filterData = data.posts.edges.filter(el =&gt; {</span></span>
<span><span>      </span></span>
<span><span>         return el.node.media.map(el =&gt; el.videoUrl)[0] !== null</span></span>
<span><span>       })</span></span>
<span><span></span></span>
<span><span>       callback(null, {</span></span>
<span><span>         statusCode: 200,</span></span>
<span><span>         body: JSON.stringify({</span></span>
<span><span>           message: "Success",</span></span>
<span><span>           data: filterData,</span></span>
<span><span>         }),</span></span>
<span><span>       })</span></span>
<span><span>     }</span></span>
<span><span></span></span></code></pre><p>In this function, We checked if data is defined, filter posts data, map each product media array and return the videoUrl value, then we check if the first array item isn't null because the launch video is the first item in the media array </p></li></ul><p>Now, our code will look like this</p><pre data-language="" data-index="14"><code><span><span></span></span>
<span><span>require("dotenv").config()</span></span>
<span><span></span></span>
<span><span>const fetch = require("node-fetch")</span></span>
<span><span></span></span>
<span><span>exports.handler = function (event, context, callback) {</span></span>
<span><span>  const date = new Date(event.queryStringParameters.date).toISOString();</span></span>
<span><span></span></span>
<span><span>  const requestBody = {</span></span>
<span><span>    query: `</span></span>
<span><span>        {</span></span>
<span><span>            posts(order:RANKING,  postedBefore: ${date}) {</span></span>
<span><span>            </span></span>
<span><span>              edges {</span></span>
<span><span>                node {</span></span>
<span><span>                  name</span></span>
<span><span>                  url</span></span>
<span><span>                  topics {</span></span>
<span><span>                    edges {</span></span>
<span><span>                      node {</span></span>
<span><span>                         name</span></span>
<span><span>                      }</span></span>
<span><span>                    }</span></span>
<span><span>                  }</span></span>
<span><span>                  votesCount</span></span>
<span><span>                  media {</span></span>
<span><span>                    videoUrl</span></span>
<span><span>                    url</span></span>
<span><span>                  }</span></span>
<span><span>                  tagline</span></span>
<span><span>                </span></span>
<span><span>                  createdAt</span></span>
<span><span>                </span></span>
<span><span>                  </span></span>
<span><span>                }</span></span>
<span><span>              }</span></span>
<span><span>              </span></span>
<span><span>            }</span></span>
<span><span>          }</span></span>
<span><span>            `,</span></span>
<span><span>  }</span></span>
<span><span>  fetch("https://api.producthunt.com/v2/api/graphql", {</span></span>
<span><span>    method: "POST",</span></span>
<span><span>    headers: {</span></span>
<span><span>      authorization: `Bearer ${process.env.PH_ACCESS_TOKEN}`,</span></span>
<span><span>      "Content-type": "Application/JSON",</span></span>
<span><span>    },</span></span>
<span><span>    body: JSON.stringify(requestBody),</span></span>
<span><span>  })</span></span>
<span><span>    .then(res =&gt; res.json())</span></span>
<span><span>    .then(({ data }) =&gt; {</span></span>
<span><span>      if (data) {</span></span>
<span><span>        const filterData = data.posts.edges.filter(el =&gt; {</span></span>
<span><span>          return el.node.media.map(el =&gt; el.videoUrl)[0] !== null</span></span>
<span><span>        })</span></span>
<span><span></span></span>
<span><span>        callback(null, {</span></span>
<span><span>          statusCode: 200,</span></span>
<span><span>          body: JSON.stringify({</span></span>
<span><span>            message: "Success",</span></span>
<span><span>            data: filterData,</span></span>
<span><span>          }),</span></span>
<span><span>        })</span></span>
<span><span>      }</span></span>
<span><span>    })</span></span>
<span><span>    .catch(err =&gt; console.log(err))</span></span>
<span><span>}</span></span>
<span><span></span></span>
<span><span></span></span></code></pre><p>We're on the halfway to ﬁnish the netlify function</p><ul><li><p>You need create new Google spreadsheets at <a href="https://docs.google.com/spreadsheets/">Goo</a><a href="https://docs.google.com/spreadsheets/">g</a><a href="https://docs.google.com/spreadsheets/">le</a><a href="https://docs.google.com/spreadsheets/"> </a><a href="https://docs.google.com/spreadsheets/">Sheets</a></p></li><li><p>You need to get Google Sheets API credentials to be able to read data from sheets</p></li><li><p>Go to the Google APIs Console.</p></li><li><p>Create a new project.</p></li><li><p>Click Enable API. Search for and enable the Google Sheet API.</p></li><li><p>Create a new service account then, create a new API key and download the JSON file</p></li></ul><ul><li>Install Google Sheets Node JS SDK to add Product Hunt data to it</li></ul><pre data-language="" data-index="15"><code><span><span></span></span>
<span><span>yarn add google-spreadsheet util &amp;&amp; netlify dev</span></span>
<span><span></span></span></code></pre><ul><li>Access your Sheets using the Node JS SDK</li></ul><pre data-language="" data-index="16"><code><span><span></span></span>
<span><span> const acessSepreadSheet = async () =&gt; {</span></span>
<span><span>    const doc = new GoogleSpreadsheet(</span></span>
<span><span>      "YOUR GOOGLE SHEET ID"</span></span>
<span><span>    )</span></span>
<span><span></span></span>
<span><span>    await doc.useServiceAccountAuth({</span></span>
<span><span>      client_email: process.env.GOOGLE_SERVICE_ACCOUNT_EMAIL,</span></span>
<span><span>      private_key: process.env.GOOGLE_PRIVATE_KEY,</span></span>
<span><span>    })</span></span>
<span><span>    const info = await doc.loadInfo() // loads document properties and worksheets</span></span>
<span><span>    console.log(doc.title)</span></span>
<span><span>  }</span></span>
<span><span></span></span>
<span><span></span></span></code></pre><p>This function will access the Google Sheet and return the sheet title</p><p>Now, we need this row to add new product data like mentioned the screenshots below.</p><p><img src="https://dev-to-uploads.s3.amazonaws.com/i/1bbw0f9z3qphtjzwdlua.png" alt="Alt Text"></p><p>We need to write a function to add a new row </p><pre data-language="" data-index="17"><code><span><span></span></span>
<span><span> const accessSpreadSheet = async ({</span></span>
<span><span>    productName,</span></span>
<span><span>    topic,</span></span>
<span><span>    votesCount,</span></span>
<span><span>    videoUrl,</span></span>
<span><span>    featuredImage,</span></span>
<span><span>    url,</span></span>
<span><span>    created_at,</span></span>
<span><span>    description,</span></span>
<span><span>  }) =&gt; {</span></span>
<span><span>    const doc = new GoogleSpreadsheet(</span></span>
<span><span>      "YOUR SHEET ID"</span></span>
<span><span>    )</span></span>
<span><span></span></span>
<span><span>    // use service account creds</span></span>
<span><span>    await doc.useServiceAccountAuth({</span></span>
<span><span>      client_email: process.env.GOOGLE_SERVICE_ACCOUNT_EMAIL,</span></span>
<span><span>      private_key: process.env.GOOGLE_PRIVATE_KEY,</span></span>
<span><span>    })</span></span>
<span><span>    await doc.loadInfo() // loads document properties and worksheets</span></span>
<span><span></span></span>
<span><span>    const sheet = doc.sheetsByIndex[0] // or use doc.sheetsById[id]</span></span>
<span><span></span></span>
<span><span>    const row = {</span></span>
<span><span>      productName,</span></span>
<span><span>      topic,</span></span>
<span><span>      votesCount,</span></span>
<span><span>      videoUrl,</span></span>
<span><span>      featuredImage,</span></span>
<span><span>      url,</span></span>
<span><span>      created_at,</span></span>
<span><span>      description,</span></span>
<span><span>    }</span></span>
<span><span></span></span>
<span><span>    await sheet.addRow(row)</span></span>
<span><span>  }</span></span></code></pre><p>and the final code will look like this</p><pre data-language="" data-index="18"><code><span><span></span></span>
<span><span>require("dotenv").config()</span></span>
<span><span></span></span>
<span><span>const fetch = require("node-fetch")</span></span>
<span><span>const { GoogleSpreadsheet } = require("google-spreadsheet")</span></span>
<span><span>exports.handler = function (event, context, callback) {</span></span>
<span><span>  const date = new Date(event.queryStringParameters.date).toISOString();</span></span>
<span><span></span></span>
<span><span>  const accessSpreadSheet = async ({</span></span>
<span><span>    productName,</span></span>
<span><span>    topic,</span></span>
<span><span>    votesCount,</span></span>
<span><span>    videoUrl,</span></span>
<span><span>    featuredImage,</span></span>
<span><span>    url,</span></span>
<span><span>    created_at,</span></span>
<span><span>    description,</span></span>
<span><span>  }) =&gt; {</span></span>
<span><span>    const doc = new GoogleSpreadsheet(</span></span>
<span><span>      "YOUR SHEET ID"</span></span>
<span><span>    )</span></span>
<span><span></span></span>
<span><span>    // use service account creds</span></span>
<span><span>    await doc.useServiceAccountAuth({</span></span>
<span><span>      client_email: process.env.GOOGLE_SERVICE_ACCOUNT_EMAIL,</span></span>
<span><span>      private_key: process.env.GOOGLE_PRIVATE_KEY,</span></span>
<span><span>    })</span></span>
<span><span>    await doc.loadInfo() // loads document properties and worksheets</span></span>
<span><span></span></span>
<span><span>    const sheet = doc.sheetsByIndex[0] // or use doc.sheetsById[id]</span></span>
<span><span></span></span>
<span><span>    const row = {</span></span>
<span><span>      productName,</span></span>
<span><span>      topic,</span></span>
<span><span>      votesCount,</span></span>
<span><span>      videoUrl,</span></span>
<span><span>      featuredImage,</span></span>
<span><span>      url,</span></span>
<span><span>      created_at,</span></span>
<span><span>      description,</span></span>
<span><span>    }</span></span>
<span><span></span></span>
<span><span>    await sheet.addRow(row)</span></span>
<span><span>  }</span></span>
<span><span>  const requestBody = {</span></span>
<span><span>    query: `</span></span>
<span><span>        {</span></span>
<span><span>            posts(order:RANKING,  postedBefore: ${date}) {</span></span>
<span><span>            </span></span>
<span><span>              edges {</span></span>
<span><span>                node {</span></span>
<span><span>                  name</span></span>
<span><span>                  url</span></span>
<span><span>                  topics {</span></span>
<span><span>                    edges {</span></span>
<span><span>                      node {</span></span>
<span><span>                         name</span></span>
<span><span>                      }</span></span>
<span><span>                    }</span></span>
<span><span>                  }</span></span>
<span><span>                  votesCount</span></span>
<span><span>                  media {</span></span>
<span><span>                    videoUrl</span></span>
<span><span>                    url</span></span>
<span><span>                  }</span></span>
<span><span>                  tagline</span></span>
<span><span>                </span></span>
<span><span>                  createdAt</span></span>
<span><span>                </span></span>
<span><span>                  </span></span>
<span><span>                }</span></span>
<span><span>              }</span></span>
<span><span>              </span></span>
<span><span>            }</span></span>
<span><span>          }</span></span>
<span><span>            `,</span></span>
<span><span>  }</span></span>
<span><span>  fetch("https://api.producthunt.com/v2/api/graphql", {</span></span>
<span><span>    method: "POST",</span></span>
<span><span>    headers: {</span></span>
<span><span>      authorization: `Bearer ${process.env.PH_ACCESS_TOKEN}`,</span></span>
<span><span>      "Content-type": "Application/JSON",</span></span>
<span><span>    },</span></span>
<span><span>    body: JSON.stringify(requestBody),</span></span>
<span><span>  })</span></span>
<span><span>    .then(res =&gt; res.json())</span></span>
<span><span>    .then(async ({ data, status }) =&gt; {</span></span>
<span><span>      if (data) {</span></span>
<span><span>        const filterData = data.posts.edges.filter(el =&gt; {</span></span>
<span><span>          return el.node.media.map(el =&gt; el.videoUrl)[0] !== null</span></span>
<span><span>        })</span></span>
<span><span></span></span>
<span><span>        callback(null, {</span></span>
<span><span>          statusCode: 200,</span></span>
<span><span>          body: JSON.stringify({</span></span>
<span><span>            …</span></span></code></pre></div></article></div></main></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://iliashaddad.com/blog/how-i-built-the-product-hunt-launch-video-gallery-using-gatsby-js-google-sheets-and-product-hunt-api">https://iliashaddad.com/blog/how-i-built-the-product-hunt-launch-video-gallery-using-gatsby-js-google-sheets-and-product-hunt-api</a></em></p>]]>
            </description>
            <link>https://iliashaddad.com/blog/how-i-built-the-product-hunt-launch-video-gallery-using-gatsby-js-google-sheets-and-product-hunt-api</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024206</guid>
            <pubDate>Sun, 08 Nov 2020 08:47:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to build a Gatsby website with Google Sheets]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024204">thread link</a>) | @iliashad
<br/>
November 8, 2020 | https://iliashaddad.com/blog/how-to-build-a-gatsby-website-with-google-sheets | <a href="https://web.archive.org/web/*/https://iliashaddad.com/blog/how-to-build-a-gatsby-website-with-google-sheets">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><main><div><article><div><p><span>
      <span></span>
  <img alt="Photo by Arian Darvishi on Unsplash" title="Photo by Arian Darvishi on Unsplash" src="https://iliashaddad.com/static/7b43ce90104c55d06ec59e66f30bb906/0a251/how-to-build-a-gatsby-website-with-google-sheets-0.jpg" srcset="https://iliashaddad.com/static/7b43ce90104c55d06ec59e66f30bb906/bce2d/how-to-build-a-gatsby-website-with-google-sheets-0.jpg 250w,https://iliashaddad.com/static/7b43ce90104c55d06ec59e66f30bb906/953fe/how-to-build-a-gatsby-website-with-google-sheets-0.jpg 500w,https://iliashaddad.com/static/7b43ce90104c55d06ec59e66f30bb906/0a251/how-to-build-a-gatsby-website-with-google-sheets-0.jpg 1000w,https://iliashaddad.com/static/7b43ce90104c55d06ec59e66f30bb906/e3932/how-to-build-a-gatsby-website-with-google-sheets-0.jpg 1500w,https://iliashaddad.com/static/7b43ce90104c55d06ec59e66f30bb906/451a4/how-to-build-a-gatsby-website-with-google-sheets-0.jpg 2000w,https://iliashaddad.com/static/7b43ce90104c55d06ec59e66f30bb906/af240/how-to-build-a-gatsby-website-with-google-sheets-0.jpg 2600w" sizes="(max-width: 1000px) 100vw, 1000px" loading="lazy">
    </span>
Photo by  <a href="https://unsplash.com/@arianismmm?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Arian Darvishi</a> on&nbsp;<a href="https://unsplash.com/s/photos/learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></p><p>Recently I created a Gatsby website that uses Google sheets as Database and. It took me some time to get it right but the process was not that tough and once I completed it I thought I would share how to build similar one with basic functionality</p><h3>What we will be doing&nbsp;today?</h3><ul><li>Setup new Gatsby Project</li><li>Install and Config Gatsby Google Sheets Source Plugin</li><li>Fetch remote images and make them ready to use in Gatbsy Image</li><li>Query and Display the data from Google Sheets</li></ul><h3><strong>Setup New Gatsby&nbsp;Project</strong></h3><ul><li>Ensure you have the latest <strong>LTS</strong> version of Node installed (&gt;= 10.16.0). <code>node --version</code></li><li><a href="https://yarnpkg.com/en/docs/install">Install</a> the Yarn package manager.</li><li>Ensure you have the latest version of Yarn installed (&gt;= 1.0.2). <code>yarn --version</code></li></ul><p>First, install gatsby-cli globally</p><pre data-language="javascript" data-index="0"><code><span><span></span></span>
<span><span><span> </span><span>yarn</span><span> </span><span>global</span><span> </span><span>add</span><span> </span><span>gatsby</span><span>-</span><span>cli</span></span></span>
<span><span><span> </span></span></span></code></pre><p>Second, Create a new Gatsby website</p><pre data-language="javascript" data-index="1"><code><span><span></span></span>
<span><span><span>gatsby</span><span> </span><span>new</span><span> </span><span>gatsby</span><span>-</span><span>google</span><span>-</span><span>sheets</span><span>-</span><span>starter</span><span> </span></span></span>
<span><span></span></span></code></pre><p>Finally, run the Gatsby website</p><pre data-language="javascript" data-index="2"><code><span><span></span></span>
<span><span><span>gatsby</span><span>-</span><span>google</span><span>-</span><span>sheets</span><span>-</span><span>starte</span><span> </span><span>&amp;&amp;</span><span> </span><span>yarn</span><span> </span><span>develop</span></span></span>
<span><span></span></span></code></pre><h3><strong>Install and Config Gatsby Google Sheets Source&nbsp;Plugin</strong></h3><p>First, you need to get Google Sheets API credentials to be able to read data from sheets</p><ol><li>Go to the <a href="https://console.developers.google.com/">Google APIs Console</a>.</li><li>Create a new project.</li><li>Click <code>Enable API</code>. Search for and enable the Google Drive API.</li><li><code>Create credentials</code> for a <code>Web Server</code> to access <code>Application Data</code>.</li><li>Name the service account and grant it a <code>Project</code> Role of <code>Editor</code>.</li><li>Download the JSON file.</li><li>Copy the JSON file to your code directory and rename it to <code>secret.json</code></li></ol><p>Second, you need to install the gatsby source google sheets</p><pre data-language="javascript" data-index="3"><code><span><span></span></span>
<span><span><span> </span><span>yarn</span><span> </span><span>add</span><span> </span><span>gatsby</span><span>-</span><span>source</span><span>-</span><span>google</span><span>-</span><span>sheets</span></span></span>
<span><span></span></span></code></pre><p>Third, add the plugin in gatsby-config.js</p><pre data-language="javascript" data-index="4"><code><span><span></span></span>
<span><span><span>spreadsheetId</span><span>=</span><span> </span><span>// the id is after the [https://docs.google.com/spreadsheets/d/](https://docs.google.com/spreadsheets/d/)YOUR ID/edit#gid=0</span></span></span>
<span><span></span></span></code></pre><h3>Fetch remote images and make them ready to use in Gatbsy&nbsp;Image</h3><p>Note: I’ll use this G<a href="https://docs.google.com/spreadsheets/d/1L0aW6utYrcfd7xwYp1cIUkv8As4cFx1ECb0phF9-CEE/edit#gid=0">oogle Sheet</a> as a demo</p><p>First, install gatsby-source-filesystem</p><pre data-language="javascript" data-index="5"><code><span><span></span></span>
<span><span><span> </span><span>yarn</span><span> </span><span>add</span><span> </span><span>gatsby</span><span>-</span><span>source</span><span>-</span><span>filesystem</span><span> </span></span></span>
<span><span><span> </span></span></span></code></pre><p>Second, add this code to gatsby-node.js</p><p>Finally, replace node.featuredimage with the field where you have remote image URL</p><h3>Query and Display the data from Google&nbsp;Sheets</h3><p>First, change pages/index.js file</p><p>Second, run the gatsby website</p><pre data-language="javascript" data-index="6"><code><span><span></span></span>
<span><span><span>gatsby</span><span> </span><span>develop</span></span></span>
<span><span></span></span></code></pre><p>Voila, You have a Gatsby website powered by Google Sheets</p><p><span>
      <a href="https://iliashaddad.com/static/68f08f86dafba2bff6ae3f6a63fedfd0/78d47/1__TG1lwHGnlbim9PAbTBL42w.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Gatsby Website Powered By Google&nbsp;Sheets" title="Gatsby Website Powered By Google&nbsp;Sheets" src="https://iliashaddad.com/static/68f08f86dafba2bff6ae3f6a63fedfd0/78d47/1__TG1lwHGnlbim9PAbTBL42w.png" srcset="https://iliashaddad.com/static/68f08f86dafba2bff6ae3f6a63fedfd0/86700/1__TG1lwHGnlbim9PAbTBL42w.png 250w,https://iliashaddad.com/static/68f08f86dafba2bff6ae3f6a63fedfd0/0eb09/1__TG1lwHGnlbim9PAbTBL42w.png 500w,https://iliashaddad.com/static/68f08f86dafba2bff6ae3f6a63fedfd0/78d47/1__TG1lwHGnlbim9PAbTBL42w.png 800w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy">
  </a>
    </span>
Gatsby Website Powered By Google&nbsp;Sheets</p></div></article></div></main></div></div>]]>
            </description>
            <link>https://iliashaddad.com/blog/how-to-build-a-gatsby-website-with-google-sheets</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024204</guid>
            <pubDate>Sun, 08 Nov 2020 08:47:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Migrating My Blog to Zola]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024170">thread link</a>) | @todsacerdoti
<br/>
November 8, 2020 | https://mrkaran.dev/posts/migrating-to-zola/ | <a href="https://web.archive.org/web/*/https://mrkaran.dev/posts/migrating-to-zola/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>

<section>
    
    <article>
        <p>I've been writing on this blog for about 2 years now. This has been the longest I've stuck on to the same <em>technology stack</em> for my blog. I've previously jumped from a Jekyll based static site to a Medium blog before finally settling for <a href="https://gohugo.io/">Hugo</a>.</p>
<p>I've been using Hugo since 2018 but I don't recall as to <em>why</em> I went ahead with it. Maybe it was increasingly popular at that time and everyone touted Hugo as <em>the</em> solution to Static Site Generator (referred to as SSG from here on). There are 1000s of SSGs and at least a dozen of websites which lists all the SSGs out there. This is crazy by any standards. Hugo started as a generic blog generator but over the years it has become a <em>website generator</em>. It's no longer aimed at people who just want to have a small little static website/blog but supports all the use cases for people building full-fledged static websites. IMHO these two goals are overarching however this has resulted in a simple project to become incredibly complex over time.</p>
<h3 id="tipping-point">Tipping Point</h3>
<p>Anyway, so I wanted to change the look of the homepage on my website so I decided to look at Hugo's documentation. Hugo's documentation is great for someone who knows what exactly are they looking for. The documentation is so huge that you simply cannot grok it in one evening. I had zero ideas on how to customise the damn homepage of my blog and after spending hours buried in the documentation I was able to kind of figure the solution but it was unintuitive, to say the least. Apparently, to override any template from the theme, you have to mirror the directory structure of the theme in your root directory. Which meant, I needed to look at the source code of the theme, figure out the project structure, copy-paste all the folder names and put my override of <code>index.html</code> there. Which, BTW <strong>magically</strong> overrides it. This whole magic thing is BS and I am being strongly opinionated here.</p>
<p>There is more than one way to do something in Hugo. Different theme authors use different styles, which makes the whole thing even more complex. It also means for my customisations to work across themes, well you guessed it right: <strong>it's impossible</strong>.</p>
<p>Recently I discovered that I was unable to preview my Hugo website locally without internet because I had a Twitter <a href="https://gohugo.io/content-management/shortcodes/#tweet">shortcode</a> in one of my blog post (which makes an API call to Twitter to render a nice card preview). The site completely failed to render instead of just logging a warning. Bollocks.</p>
<p>The tipping point for me, however, was when the theme I was using stopped working with the latest version of Hugo at that point. So, picture this -- You make dozens of custom changes and then one update just <em>breaks</em> your website. Now not only you have to fix your shit but the theme you were using, you've to make upstream changes to the theme or maintain your own fork. And no, this is not a one-off experience. Hugo upgrades are a joke, they are known to break very very often.</p>
<p>I was done at this point. I didn't want to deal with this BS of continuously fighting the generator for my blog.</p>
<h3 id="a-fresh-change">A fresh change</h3>
<p>Being a practitioner of <a href="https://projects.csail.mit.edu/gsb/old-archive/gsb-archive/gsb2000-02-11.html">Yak Shaving</a>, I discussed the idea of a "tinyhugo" with <a href="https://nadh.in/">Kailash</a> and <a href="https://www.saratchandra.in/">Sarat</a>. We'd arrived at a spec and I started writing some code to pander to my NIH syndrome.</p>
<p>However, I was still not convinced that a simpler solution doesn't exist. I spent countless hours exploring other alternatives. I'd used <a href="https://www.getlektor.com/">Lektor</a>, <a href="https://blog.getpelican.com/">Pelican</a>, <a href="https://www.11ty.dev/">Eleventy</a> before finally stumbling upon <a href="https://www.getzola.org/">Zola</a> from HN/Lobster discussions. I've got to say, the landing page gave a <em>fresh</em> feeling - one that I've not seen with any other alternatives. In fact quite opposite to the Eleventy landing page which looks like an over-engineered piece of software to generate websites (Not hating on it, there might be use cases for it, but the JS tooling and dependency system is something that I would not want to touch with a 10ft pole).</p>
<p>Zola's primary appeal to me was that like Hugo it's extremely fast and comes as a single binary no dependency package. I looked at the docs the first impression was they are concise enough to get a basic idea. Zola is strongly opinionated, even to the extent of dictating a project structure and sometimes filenames too. I actually preferred this over the <em>magic</em> Hugo does. In less than 2 hours I was able to port the home page of my blog (and tweak it to my liking) in Zola. I decided to abandon my own <code>tinyhugo</code> attempt because for the very fact Zola fits my needs very well.</p>
<p>The thing that I really loved about Zola is how it enforces a separation between <a href="https://www.getzola.org/documentation/content/section/">Section</a> and <a href="https://www.getzola.org/documentation/content/page/">Pages</a>. The section represents a "collection" of posts. So a <em>blog</em> can be a section, and I can have another section called "Book Reviews". I could easily tell Zola where to look for the templates by specifying the same in <code>content/book_reviews/_index.md</code>. I don't have to read Hugo docs or do <em>Google-fu</em> to figure this out, it's right there in the docs and very apparent.</p>
<p>For the record, I still don't know how to customise different templates for different sections in Hugo, but I couldn't care less.</p>
<h3 id="migration">Migration</h3>
<p>The migration was pretty straightforward -- I had to copy the <code>content folder</code>s of my blog (which are just a bunch of <code>.md</code> fikes) and replace <code>YAML</code> frontmatter to <code>TOML</code>. There were a few variable changes that I needed to do manually but since they were a manageable 20-25 posts, I did it by hand. I could potentially automate but then rabbit deep in the rabbit hole of Yak Shaving. The good part was that I was able to retain the same URL structure for my new blog because the URL scheme was based on the file paths.</p>
<p>I spent some time porting <a href="https://github.com/knadh/hugo-ink">hugo-ink</a> to Zola and did minor CSS tweaks to it. Zola uses the Terra language for templating and it's much more pleasing to eyes than the Go Template syntax. Zola comes with pretty neat features like Search, RSS/Atom Feeds, Syntax Highlighting and SASS-&gt;CSS Processors.</p>
<p>What took me time however was to figure out how to get <code>opengraph</code> tags in each page. Hugo provides nifty <a href="https://github.com/gohugoio/hugo/blob/master/tpl/tplimpl/embedded/templates/opengraph.html">template</a> for this use case but Zola is pretty barebones like that. People who care a lot about SEO need to spend some extra efforts here.</p>
<h3 id="future">Future</h3>
<p>Zola is still a pretty new kid on the block but the author shares the same frustration about Hugo:</p>
<blockquote>
<p>it personally drives me insane, to the point of writing my own template engine and static site generator. Yes, this is a bit biased. -- <a href="https://github.com/getzola/zola#-explanations">Source</a></p>
</blockquote>
<p>This also reflects in the issues/PRs I've seen for Zola and the author is opinionated about not adding features which would make Zola complicated. Overall I am very happy with the switch and it was long due. I feel more confident in tweaking certain sections of my website. I plan to open-source the current theme in the next few days.</p>
<p>You can read the <a href="https://git.mrkaran.dev/karan/website">Source Code</a> of this website if you'd like to explore how this website is built.</p>
<p>Fin!</p>

    </article>
</section>
</article></div>]]>
            </description>
            <link>https://mrkaran.dev/posts/migrating-to-zola/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024170</guid>
            <pubDate>Sun, 08 Nov 2020 08:39:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Crypto Paper: Man Continues Reading After First Math Equation]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25023856">thread link</a>) | @npguy
<br/>
November 7, 2020 | https://doublespend.io/2020/10/28/man-continues-reading-crypto-white-paper-after-first-mathematical-equation/ | <a href="https://web.archive.org/web/*/https://doublespend.io/2020/10/28/man-continues-reading-crypto-white-paper-after-first-mathematical-equation/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
<p>DoubleSpend has obtained exclusive video footage of a man continuing to read a white paper after the occurrence of the first mathematical equation in the paper. The video footage has been analyzed with WebGazeAnalyzer, a system for capturing and analyzing web reading behavior using eye gaze to confirm that this was not a case of the viewer moving off the page or just leaving the desk. </p>



<p>More details on this to follow soon.</p>
<div><p><a href="https://twitter.com/share?url=https://doublespend.io/2020/10/28/man-continues-reading-crypto-white-paper-after-first-mathematical-equation/&amp;text=Man%20Continues%20Reading%20Crypto%20White%20Paper%20After%20First%20Math%20Equation" title="Share on Twitter" target="_blank" rel="nofollow noopener noreferrer" data-postid="453" data-social-network="Twitter" data-social-action="Tweet" data-social-target="https://doublespend.io/2020/10/28/man-continues-reading-crypto-white-paper-after-first-mathematical-equation/"><span><span><svg version="1.1" xmlns="http://www.w3.org/2000/svg" width="29.71875" height="32" viewBox="0 0 951 1024"><path d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"></path></svg></span><span>Tweet</span></span><span>0</span></a></p></div>		</div></div>]]>
            </description>
            <link>https://doublespend.io/2020/10/28/man-continues-reading-crypto-white-paper-after-first-mathematical-equation/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25023856</guid>
            <pubDate>Sun, 08 Nov 2020 07:29:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Embed Gravity programming language into your code]]>
            </title>
            <description>
<![CDATA[
Score 35 | Comments 8 (<a href="https://news.ycombinator.com/item?id=25023836">thread link</a>) | @creolabs
<br/>
November 7, 2020 | https://marcobambini.github.io/gravity/#/embedding | <a href="https://web.archive.org/web/*/https://marcobambini.github.io/gravity/#/embedding">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://marcobambini.github.io/gravity/#/embedding</link>
            <guid isPermaLink="false">hacker-news-small-sites-25023836</guid>
            <pubDate>Sun, 08 Nov 2020 07:25:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Parsing the Infamous Japanese Postal CSV]]>
            </title>
            <description>
<![CDATA[
Score 165 | Comments 40 (<a href="https://news.ycombinator.com/item?id=25023673">thread link</a>) | @polm23
<br/>
November 7, 2020 | https://www.dampfkraft.com/posuto.html | <a href="https://web.archive.org/web/*/https://www.dampfkraft.com/posuto.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://www.dampfkraft.com/posuto.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25023673</guid>
            <pubDate>Sun, 08 Nov 2020 06:52:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I wrote JavaScript to avoid JavaScript]]>
            </title>
            <description>
<![CDATA[
Score 71 | Comments 58 (<a href="https://news.ycombinator.com/item?id=25023594">thread link</a>) | @asaaki
<br/>
November 7, 2020 | https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/ | <a href="https://web.archive.org/web/*/https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Web technologies have come so far, that you realize: not everything needs to be done in JavaScript nowadays anymore.</p><blockquote><p><em>»Life is really simple, but we insist on making it complicated.« — Confucius</em></p></blockquote><p>My initial headline would have been:</p><p><em>How I wrote more JavaScript in the backend to eliminate JavaScript in the frontend.</em></p><p>But that's already a mouthful and also would have revealed too much and you might not have clicked my slightly clickbait-y title, right?</p><p>So here is my Public Service Announcement:</p><p> 📣 <strong>This site does not use any frontend JavaScript.<sup>*</sup></strong></p><p><em><small>*) There are only tiny exceptions on 2 pages, but for a good reason. More on that later.</small></em></p><p>First of all I am not against JavaScript (<span>JS</span>) at all. If you're building a web <strong>application</strong>, then this is not only totally fine but most likely a core requirement.</p><p>But I do have my pet peeve with <span>JS</span> for plain websites and blogs. Currently there is still such a strong draught in the static site generator world, telling us all our sites should be some kind of React or other frontend framework based project (looking at you, Gatsby, Next, Nuxt, VuePress, …). That you need to have that plentyful of code running in the browser of your visitors to have a smooth and <em>feels like a native app</em> user experience. That a site should be a <em>Single Page Application (SPA).</em> I can tell you, a plain HTML+CSS website does it really well, too. Surprise!</p><p>While on one hand the browser vendors add more and more <a href="https://developer.mozilla.org/en-US/docs/Web/API">Web APIs</a>, we also got a lot of improvement in the HTML and CSS area. Usually there is no big hype train around them, unless you are very enthusiastic and live in that niche.</p><p>Take a look at <a href="https://caniuse.com/">caniuse.com</a> to get an idea what is possible today and what might come tomorrow. <em>Did you know that HTML5 is still iterated on and we're moving towards <a href="https://www.w3.org/TR/html53/">version 5.3</a>?</em> On the other hand »HTML 5« is also used as an umbrella term for a <a href="https://spec.whatwg.org/" title="WHATWG Standards">wide variety of standards</a>. Also for CSS the story got very interesting: while CSS until 2.1 was a single specification, since CSS 3 there is a whole potpourri of recommendations and drafts. The <a href="https://wiki.csswg.org/spec">wiki of the CSS Working Group</a> might be a good starting point for further discovery.</p><p>But I want to give you some more practical examples and an experience report:</p><h2 id="sticky-navigation-bar">Sticky navigation bar</h2><p>This is something you can observe here on this blog:</p><video autoplay="" loop="" muted="" playsinline=""><source src="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/pos-sticky.hvec.mp4" type="video/mp4; codecs=hvc1"><source src="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/pos-sticky.hvec.mp4" type="video/mp4; codecs=hevc"><source src="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/pos-sticky.h264.mp4" type="video/mp4; codecs=avc1"><source src="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/pos-sticky.webm" type="video/webm; codecs=vp9"></video><p>The key ingredient is the CSS <code>position: sticky</code> <a href="https://caniuse.com/css-sticky">🛈</a>. Even though most of them are labeled as <em>partial support,</em> this property value can be used in most scenarios except in some table related cases. If you want a sticky menu after scrolling and use only elements like <code>div</code> everything is just fine. I could throw away all the code for that after I realized that none of the common and modern browsers had any blocking issues. So I did. The only real latecomers were the web view components, no big deal for me here.</p><h3 id="before">Before</h3><h4 id="javascript">JavaScript</h4><pre><code><span>const </span><span>navbar </span><span>= </span><span>document</span><span>.querySelector(</span><span>'.navbar'</span><span>);
</span><span>let </span><span>sticky </span><span>= </span><span>navbar.offsetTop;
</span><span>const </span><span>navbarScroll </span><span>= </span><span>() </span><span>=&gt; </span><span>{
  </span><span>if </span><span>(</span><span>window</span><span>.pageYOffset </span><span>&gt;= </span><span>sticky) {
    navbar.classList.add(</span><span>'sticky'</span><span>)
  } </span><span>else </span><span>{
    navbar.classList.remove(</span><span>'sticky'</span><span>);
  }
};

</span><span>window</span><span>.onscroll </span><span>= </span><span>navbarScroll;
</span></code></pre><h4 id="stylesheet">Stylesheet</h4><pre><code><span>.navbar </span><span>{
  </span><span>position</span><span>: </span><span>relative</span><span>;
}
</span><span>.sticky </span><span>{
  </span><span>position</span><span>: </span><span>fixed</span><span>;
  </span><span>top</span><span>: </span><span>0</span><span>;
  </span><span>left</span><span>: </span><span>0</span><span>;
}
</span></code></pre><h3 id="after">After</h3><h4 id="javascript-1">JavaScript</h4><pre><code><span>// nope
</span></code></pre><h4 id="stylesheet-1">Stylesheet</h4><pre><code><span>.navbar </span><span>{
  </span><span>position</span><span>: </span><span>sticky</span><span>;
  </span><span>top</span><span>: </span><span>0</span><span>; </span><span>/* it does not reposition right away,
             but determines at which point it sticks */
</span><span>}
</span></code></pre><h3 id="resolution">Resolution</h3><p>The workaround with <span>JS</span> is no more. Yay!</p><p>Also notice how little code is actually needed now? Two CSS properties and the job is done.</p><hr><h2 id="service-workers">Service Workers</h2><p>Also in 2018 I played with <a href="https://markentier.tech/posts/2018/04/progressive-web-app/">Progressive Web Apps (PWA)</a>. The whole blog was one. A few days ago I teared down all of it. At the core of PWAs sit <a href="https://serviceworke.rs/">Service Workers (SW)</a>, though you can use SW also without building an app. And that's what I was aiming for, but in the end my home-grown dynamic cache solution was more annoying to me than helpful for anyone else. Every time I updated anything here, I had to wait and/or force refresh to see the result. I'm sure some people probably see visual inconsistencies due to a still running service worker in their browser. If you do, try to force clear all data for this website.</p><p>Long story short: if you do not build a web <strong>app</strong>, you most likely do not need service workers. So yet another thing down from the <span>JS</span> list.</p><p>No <em>before/after</em> comparison here, but several precious kilobytes of JavaScript shaved off by removing them.</p><hr><h2 id="sqip-svg-lqip">SQIP (SVG LQIP)</h2><p><em>Woa, what are all these random acronyms here?</em> Don't worry, the simple answer is:</p><p>If you have images and they are not very small in file size, you maybe want to provide a temporary placeholder with very low resolution and quality. This is pretty useful for slow internet connections; living here in Germany I know how difficult this situation can be. That thing called internet is still very Neuland to us. 🤦</p><p>Anyway, <code>SQIP</code> can be translated with »<code>SVG</code>-based <code>LQIP</code>.«</p><p><code>SVG</code> are Scalable Vector Graphics, an image format I really love a lot, my logo is done with it (<a href="https://markentier.tech/posts/2018/05/minimalism-focus-clean-redesign/">I wrote about it a while ago</a>).</p><p>LQIP finally stands for <em>»Low Quality Image Placeholders«</em> and is based on an algorithm to find primitive shapes to describe the source image. Basically try to find only a few triangles, rectangles, circles, ellipsis, and other low poly shapes. It is also an art form in its own, you can enjoy some <a href="https://github.com/fogleman/primitive#static-animation">nice examples there</a>. The advantage of SVG is that it is made to encode such figures in very few characters of human readable text, so a less complex image for a placeholder can be written in one kilobyte or less.</p><p>Compared to the original high resolution image which can easily weigh half a megabyte and more this is great. You can reserve the space in your page and very early in the loading process display some visual hint that there will be a proper picture soon. Especially for types which do not support progessive loading (as JPEG can) using SQIP/LQIP placeholders makes a lot of sense.</p><p>In this scenario at first it was not really about saving frontend <span>JS</span>, more about saving it on the backend site and replacing it with something else. Unfortunately in between some code creeped into the frontend anyway.</p><p>But what happened that this beautiful technique fell out of favor with me?</p><h3 id="picture"><code>&lt;picture&gt;</code></h3><p>Enter another interesting HTML tag combo: <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/picture"><code>&lt;picture&gt;</code></a> with <code>&lt;source&gt;</code>.</p><p>So one reason to use small low quality placeholders is because before such tags became a thing we solely relied on a single <code>&lt;img&gt;</code> and some trickery with CSS (and sometimes aided by sprinkles of JavaScript). I tried to avoid <span>JS</span> completely, but of course I had to use some styling hacks eventually.</p><p>The essence of it was some style attached to the image in question:</p><pre><code><span>&lt;</span><span>img </span><span>src</span><span>=</span><span>"highres-and-heavy.png"
     </span><span>style</span><span>=</span><span>"</span><span>background-size</span><span>: </span><span>cover</span><span>;
            </span><span>background-image</span><span>: </span><span>url</span><span>(</span><span>'data:image/svg+xml;base64,PHN2…'</span><span>);</span><span>"</span><span>&gt;
</span><span>&lt;!-- Usually in some post processing all style attributes were collected
     into a &lt;style&gt; tag or CSS file. --&gt;
</span></code></pre><p>The JavaScript entered this scenery at one point: after I used images with transparency. Sadly with this background image workaround you would've seen the low quality placeholder through the transparent parts, and this was extremly ugly to be honest. I could not stand it and deployed some snippet to trigger a background removal once the actual image was loaded:</p><pre><code><span>// remove the background image styling, so transparent images won't have
// strange SQIP artefacts shining through
</span><span>document</span><span>.querySelectorAll(
  </span><span>"img[loading=lazy][class]:not(.thumbnail):not(.loaded)"
</span><span>).forEach((</span><span>img</span><span>) </span><span>=&gt; </span><span>{
  </span><span>img</span><span>.</span><span>onload </span><span>= </span><span>(</span><span>_event</span><span>) </span><span>=&gt; </span><span>img.className </span><span>= </span><span>"loaded"</span><span>;
});
</span><span>document</span><span>.querySelectorAll(
  </span><span>"img[loading=lazy].thumbnail:not(.loaded)"
</span><span>).forEach((</span><span>img</span><span>) </span><span>=&gt; </span><span>{
  </span><span>img</span><span>.</span><span>onload </span><span>= </span><span>(</span><span>_event</span><span>) </span><span>=&gt; </span><span>img.className </span><span>= </span><span>"thumbnail loaded"</span><span>;
});
</span></code></pre><p>Theoretically it would have been tolerable, but I noticed some strange behaviour once I started wrapping my images into <code>picture</code> tags.</p><p>Let's <a href="https://en.wiktionary.org/wiki/yak_shaving">shave the yak</a> a bit further to understand why.</p><h4 id="webp-and-avif">WEBP and AVIF</h4><p><em>Come on, more acronyms?</em> I'm sorry, the web is a place with a lot of them.</p><p>All you need to know for now is that both of them are pretty modern image formats with quite good (lossy) compression rates while keeping a respectable quality. <a href="https://caniuse.com/webp"><code>WEBP</code></a> has been around for some time and most of the browsers do support it. <a href="https://caniuse.com/avif"><code>AVIF</code></a> is extremly new and right now only Chrome since version 85 and Opera 71 can display them. Firefox has a configuration flag, maybe they will enable it by default pretty soon.</p><p>So the current situation is that I have my original image (PNG or JPEG in most cases), a WEBP version, an AVIF version, and the SQIP placeholder. How do I deal with it? Back to our <code>&lt;picture&gt;</code> tag:</p><pre><code><span>&lt;</span><span>picture</span><span>&gt;
  &lt;</span><span>source </span><span>srcset</span><span>=</span><span>"./cover.avif" </span><span>type</span><span>=</span><span>"image/avif"</span><span>&gt;
  &lt;</span><span>source </span><span>srcset</span><span>=</span><span>"./cover.webp" </span><span>type</span><span>=</span><span>"image/webp"</span><span>&gt;
  &lt;</span><span>img </span><span>src</span><span>=</span><span>"./cover.png"
       </span><span>style</span><span>=</span><span>"</span><span>/* SQIP data: see example above */</span><span>"</span><span>&gt;
&lt;/</span><span>picture</span><span>&gt;
</span></code></pre><p>You can also use source sets for different view sizes based on media queries, but in my case I'm mainly concerned about supporting different image formats. My idea is to prioritize the formats with the smallest file size first, and given the compression ratios the order is usually: AVIF, WEBP, PNG/JPG. Not in every case will this be true; WEBP does not always have better savings then a decently compressed JPEG for example. AVIF has not disappointed so far, but sadly a part of my visitors will not see the effect yet.</p><p>What did not really happen anymore was a display of the placeholder before the final image was loaded. I experimented for quite some time until I realized that I do not want to spent more energy any further.</p><p><picture><source srcset="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/sizes-png-webp-avif.w.avif" type="image/avif"><source srcset="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/sizes-png-webp-avif.w.webp" type="image/webp"><img src="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/sizes-png-webp-avif.w.png" alt="Size comparison of an example image; PNG original (22 KB), WEBP (14 KB, 37% saved), AVIF (7.3 KB, 67% saved)" width="1024" height="120" loading="lazy"></picture></p><p>I made a <em>risk-return tradeoff</em> compromise and got rid of SQIP altogether. For the growing number of AVIF support the images are sometimes significantly smaller which makes it acceptable to allow for some display delay anyway.</p><p>In the following screenshot the JPEG was the source photo. The PNG was created for some transparency stuff; of course, for photos this format does not really make a lot of sense in general. Sadly also WEBP fails to compete in this scenario. That's why I have to make this picture group generation a bit smarter soon to reorder based on the actual file …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/">https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/</a></em></p>]]>
            </description>
            <link>https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25023594</guid>
            <pubDate>Sun, 08 Nov 2020 06:30:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An Allegory on Whiteboard Interviews]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25023574">thread link</a>) | @mntonyc
<br/>
November 7, 2020 | http://www.unlimitednovelty.com/2011/12/can-you-solve-this-problem-for-me-on.html | <a href="https://web.archive.org/web/*/http://www.unlimitednovelty.com/2011/12/can-you-solve-this-problem-for-me-on.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-body-2047521258973142246" itemprop="description articleBody"><p>
Jim is a great chef. He's too modest to say that about himself, but he's worked either as head chef or&nbsp;assistant&nbsp;head chef at a number of restaurants. Everywhere he's worked he's been dependent and reliable, prepared great food, worked well with the other chefs, and is generally a fun guy to have in the kitchen. Unfortunately, due to the poor economy and some bad decisions by management, Jim's restaurant is about to close, so Jim is out of work and looking for a new job.</p>
<p>
There's a new restaurant opening, a fancy place with many well-to-do investors. In Jim's world, chefs are hard to find, so Jim assumes he's a shoo-in for the job. Jim arrives at the interview at a Mexican restaurant, which feels like a great fit for Jim because Mexican food is his specialty. Jim calls up the restaurant on the phone and chats with the manager about a chef position, and the manager likes what he hears enough to schedule a job interview for Jim.</p>

<p>
Jim arrives at the interview and talks to the manager a bit. Things seem to be going well, Jim is in his element at a Mexican restaurant. The initial meeting goes well: Jim talks his job history, how much he cares about having a fresh house salsa, and how good his Baja sauce is. "Look up the Yelp reviews of my Baja sauce!" remarks Jim. "It's the #1 reason people came to the last restaurant I worked at." The manager smiles and nods, and informs Jim he looks great on paper, however the remainder of the interview will be conducted by all of the other chefs in the kitchen. "Awesome!" Jim thinks, "I have a rapport with other chefs. This should go smoothly."</p>

<p>
The first chef walks in, sits down at the table, and coldly stares at Jim's resume. "Can you write down a&nbsp;recipe for me?" he asks Jim, "There's a whiteboard over there, can you write down your preferred recipe for crème brûlée?"</p>

<p>
Jim is a bit dumbfounded, both by the request and being asked to demonstrate his cooking ability on a whiteboard. "I'm sorry," he says, "I don't know how to make&nbsp;crème brûlée. I thought this was a Mexican restaurant. Would you like to know my favorite recipe for Flan?"</p>

<p>
"No, that won't do," the assistant chef says. "Please write down how you would prepare&nbsp;crème brûlée"</p>

<p>
Jim is a bit taken aback, first because he's a specialist in Mexican food, and second because instead of being asked to cook, he's being asked to write stuff on a whiteboard. "I honestly don't know how to make&nbsp;crème brûlée," Jim says. "Perhaps you could let me google the recipe and I could actually try to prepare it for you, instead of just demonstrating a rote ability to memorize recipes and write them down on a whiteboard."</p>

<p>
"No, that won't do," says the interviewer, who jots down "lack of confectionary skills" in his notes. "Can you at least attempt to write down how you would prepare&nbsp;crème brûlée?"</p>
<p>
Jim feels embarrassed   and lost. He's being asked to do something he would never have to do in a professional capacity, and worse, rather than actually doing it, he's being asked to describe how he would do it on a whiteboard. Perhaps this is a test of Jim's ability to think on his feet, but given the position he's being asked to interview for and the question he's been presented, it's certainly an unfair one. Jim picks up the black marker and thinks hard about what the possible ingredients of&nbsp;crème brûlée would be.</p>
<p>
"Well," says Jim, "I'll need cream." Jim pulls the cap off the marker and attempts to write "1. Cream", however the marker is dry and the whiteboard is on wheels that roll back when Jim attempts to write. Jim only succeeds in making a long, barely perceptible mark on the whiteboard. Having made a messy mark on the whitebard, Jim looks for an eraser but there isn't one.</p>

<p>
"Yes," says the interviewer sarcastically, rolling his eyes, "<i>obviously</i> you need cream for&nbsp;crème brûlée. Try a different marker." Jim picks up the red marker and tries to write with that to the same result, it's dried out and won't work. Frustrated, Jim puts it down and tries the green marker, which works fine, however the board swivels vertically as he tries to write. Jim grabs the board in the upper right corner and finally manages to jot down "1. Cream"</p>

<p>
"Okay, we have the most obvious ingredient down," says the assistant chef. "Can you think of any other ingredients that would go into&nbsp;crème brûlée?"</p>

<p>
"Sugar," says Jim. The assistant chef nods, and Jim writes down sugar. "What else?"</p>

<p>
"Milk," says Jim, and he begins to write it down before he comes to the realization that the cream and milk are redundant. Jim doesn't often cook with cream. The interviewer shakes his head in exasperation   and pinches the bridge of his nose as Jim looks dumbfounded. "It's not milk brûlée," he says. Unfortunately, there's no eraser, so Jim tries to erase "3. Milk" with his hand, smearing green ink all over the board and his hand before asking "do you have an eraser?" The interviewer looks around unenthusiastically before shrugging no. Jim continues smearing the marker's ink across the surface of the board with his fingertips in a desperate attempt to compensate for the absence of an eraser.</p>

<p>
"Can you think of any other ingredients that might go in&nbsp;crème&nbsp;brûlée?" asks the interviewer, clearly bored.</p>

<p>
"Eggs?" asks Jim. The interviewer nods. Jim writes down "eggs". "What else?" the interviewer asks. Jim stares at what he's written down: cream, sugar, eggs. "Well," says Jim, "I assume some kind of flavoring. Chocolate perhaps?"</p>

<p>
"Wrong," says the interviewer. "Please write vanilla." Jim looks confused for a second and jots down vanilla as asked. The interviewer jots down "trouble with basic recipes" before asking "What other ingredients can you think of?"</p>

<p>
Jim stares at the ingredients so far: cream, sugar, eggs, vanilla. "Perhaps some water?" Jim guesses. The interviewer nods, and Jim writes down water. "Now, what are you missing?" asks the interviewer.</p>

<p>
Jim stares at the list: cream, sugar, eggs, vanilla, water. Those seem like they should be the basic ingredients, and the interviewer rejected additional flavoring that wasn't vanilla. Jim is stupefied... he can't think of anything else. Taking a stab in the dark, Jim suggests "Salt?"</p>

<p>
The assistant chef does a facepalm and sighs, before looking up at Jim and stating the obvious solution: "the units. Your recipe is lacking units." The ambiguity of the interviewer's question has caught Jim off guard, especially when he professed no idea of what the recipe was to being with, and worse, he has absolutely no idea what the units should be. He stares at the whiteboard for awhile before asking "how much&nbsp;crème&nbsp;brûlée are we making?"</p>

<p>
"That's up to you," says the interviewer, "how many servings would you like to prepare?"</p>

<p>
Jim has absolutely no clue. He's not a confectioner, but he doesn't want to completely bomb the interview, so he ventures a guess. "I'd like to prepare 2 servings. Let's try a cup of cream, a teaspoon of vanilla, two tablespoons of sugar, 4 eggs, and a cup of water."</p>

<p>
"Those aren't the right proportions," say the interviewer. "You should use a quart of cream, two quarts water, a teaspoon of vanilla extract, a cup of sugar, and six eggs to produce six servings. Let's move on to the recipe. Can you write it down on the whiteboard for me?"</p>

<p>
Now Jim is completely lost. The ingredients of a recipe he has no clue about are something he can guess at, but how is he supposed to guess the recipe itself? He takes his best shot.</p>

<p>
"Break the eggs into a bowl and whisk them with the cream and sugar," guesses Jim.</p>

<p>
"Wrong," says the interviewer.</p>

<p>
"Whisk them with the cream and vanilla?" asks Jim.</p>

<p>
"Still wrong," says the interviewer, "but you were closer the first time."</p>

<p>
"Do you want me to keep guessing?" asks Jim. The interviewer sighs, writes down "completely incompetent", stands up, and says "Thank you for your time. I'll go get the next person."</p>

<p>
Jim stands by the whiteboard and feels confused and out of place. He wonders what&nbsp;crème&nbsp;brûlée has to do with preparing Mexican food. He sits down at the table and googles for&nbsp;crème&nbsp;brûlée on his phone, quickly scanning over the recipe and thinking "that doesn't look too hard at all, I could probably make a great&nbsp;crème&nbsp;brûlée if I had a little practice." The recipe for&nbsp;crème&nbsp;brûlée is in fact quite similar to Flan, and Jim can make great Flan, but unfortunately, the interviewer won't even know as he hasn't asked Jim to cook anything.&nbsp;The next interviewer comes into the room.</p>

<p>
He sits down at the table and scans over Jim's résumé, making a few grunts after scrutinizing various items. "You didn't go to&nbsp;culinary&nbsp;school?"</p>

<div><p>
"No," says Jim, "but I've loved cooking since I was a little kid. I used to cook dinner with my mom every night. I've been working professionally as a chef all my life, and I can prepare great food. Why don't you just take me to the kitchen and let me show you?"</p><p>

"That won't be necessary," says the interviewer. "Now, can you please write on the whiteboard how you would prepare a cheese danish?" Unfortunately, Jim is not a pastry chef either.</p><p>
. &nbsp; . &nbsp; .</p>
<p>
The manager has returned to conclude the interview. "Well Jim," he says, "we've discussed the issue, and we don't think you'd be a good fit here."</p><p>

At this point Jim is entirely expecting this response. Jim is most comfortable in a kitchen, preparing food hands on. He feels out of place trying to explain the theoretical act of preparing food with a whiteboard.&nbsp;Jim loves food so much that whenever he went out for a smoke break with his fellow chefs, he continued to talk about food even when they were on break. Unfortunately, during the interview he didn't get the opportunity to discuss food in this sort of context. Instead he was asked only pointed questions about food items he didn't know how to prepare.</p><p>

"I see," says Jim. "Can I ask you one question before I go?"</p><p>

"Okay," says the manager.</p><p>

"Throughout this interview," Jim asked, "I was asked about preparing confections and pastries, but not once …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.unlimitednovelty.com/2011/12/can-you-solve-this-problem-for-me-on.html">http://www.unlimitednovelty.com/2011/12/can-you-solve-this-problem-for-me-on.html</a></em></p>]]>
            </description>
            <link>http://www.unlimitednovelty.com/2011/12/can-you-solve-this-problem-for-me-on.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25023574</guid>
            <pubDate>Sun, 08 Nov 2020 06:25:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I’m Building a Personal Platform Why You Should Too]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25023515">thread link</a>) | @dannyeei
<br/>
November 7, 2020 | http://dasit.xyz/index.php/2020/10/15/building-a-personal-platform/ | <a href="https://web.archive.org/web/*/http://dasit.xyz/index.php/2020/10/15/building-a-personal-platform/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-content" role="main">

	
<article id="post-47">

	
<!-- .entry-header -->

	<div>

		<div>

			
<p>Recently I’ve got into the idea of building a personal platform for myself. By that I mean a system which makes it easy to distribute new things I’m working on and a way to grow the group of people who could be interested in what I’m working on.</p>



<h2>How I’m going about building a platform</h2>



<ol><li>Leveraging what I already have<ol><li>I have a <a href="https://www.youtube.com/channel/UCjRhOxof_h24gemf8If3Klw?view_as=subscriber">YouTube channel</a> with a following</li><li>I have a network from my personal and professional life</li></ol></li><li>More content<ol><li>This blog</li><li>“Computer Security: Attacking and defending web apps” course on Udemy (because my most successful content on YouTube is about Cyber Security and I’ve had a lot of demand for making a video on how to write shell code)</li><li><a href="https://scenario95.com/">Scenario95</a></li></ol></li><li>Creating a pipeline which I use for all new content<ol><li>Send an email to followers of my platform</li><li>Sharing on several platforms including: Facebook, HackerNews, Reddit, Twitter, and LinkedIn</li></ol></li></ol>



<h2>Why I’m doing this?</h2>



<p>When coming up with new ideas I’ve noticed that I often base them around what I already have access to and natural advantages I’ve got. For example when on a course topic for Udemy, I referred to the YouTube video which I released 4 years ago… and I’ve realised that since finishing university I haven’t done a good job of continuing to create things like this.</p>







<p>This is something I’m new to and and still learning the ropes so if you’ve got any advice please leave it in the comments below. </p>



<p>If you want to follow my adventure then subscribe to my platform! </p>

		</div><!-- .entry-content -->

	</div><!-- .post-inner -->

	<!-- .section-inner -->

	
	<!-- .pagination-single -->

	
		<!-- .comments-wrapper -->

		
</article><!-- .post -->

</div></div>]]>
            </description>
            <link>http://dasit.xyz/index.php/2020/10/15/building-a-personal-platform/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25023515</guid>
            <pubDate>Sun, 08 Nov 2020 06:14:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Cheapest Online Documentation Repository for Startups]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25023085">thread link</a>) | @timothy-quinn
<br/>
November 7, 2020 | https://blog.congruentlabs.co/the-cheapest-online-documentation-repository-for-startups/ | <a href="https://web.archive.org/web/*/https://blog.congruentlabs.co/the-cheapest-online-documentation-repository-for-startups/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://blog.congruentlabs.co/content/images/size/w300/2020/11/Screenshot-2020-11-08-151315.png 300w,
                            https://blog.congruentlabs.co/content/images/size/w600/2020/11/Screenshot-2020-11-08-151315.png 600w,
                            https://blog.congruentlabs.co/content/images/size/w1000/2020/11/Screenshot-2020-11-08-151315.png 1000w,
                            https://blog.congruentlabs.co/content/images/size/w2000/2020/11/Screenshot-2020-11-08-151315.png 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://blog.congruentlabs.co/content/images/size/w2000/2020/11/Screenshot-2020-11-08-151315.png" alt="The Cheapest Online Documentation Repository for Startups">
            </figure>

            <section>
                <div>
                    <p>I had a problem. I wanted public documentation with open collaboration, but I couldn't afford most of the products out there. As a startup we need to keep lean, and adding a paid documentation/knowledge base service is an expense I don't want to have to try to recover.</p><p>So we're moving all of our product documentation into public Github repositories, as it's the cheapest option with it being free 😉</p><p>The first repository is here, for our Signata MFA product: <a href="https://github.com/cl-tim/signata-mfa-docs">https://github.com/cl-tim/signata-mfa-docs</a></p><h3 id="why-github">Why Github?</h3><p>As much as I find it frustrating to write documentation in markdown (writing is easy, dealing with screenshots is clunky), the benefits of having a publicly accessible repository for documentation far outweighs my frustration.</p><p>I want to make sure documentation isn't sitting behind paywalls or needing a sign up. In fact, forcing people to sign up to read it has created a ton of accounts in our system that don't actually get used, which is an inconvenience for both parties.</p><p>The best part of putting it into Github is now there's the ability to collaborate - if you find a problem, or ambiguous statement, or you want something added, you can now directly interact with the content to submit pull requests or open issues.</p><p>We've also put a link to the documentation onto the Signata MFA website in a few places, which you'll also notice has undergone some rebranding:</p><figure><img src="https://blog.congruentlabs.co/content/images/2020/11/image.png" alt="" srcset="https://blog.congruentlabs.co/content/images/size/w600/2020/11/image.png 600w, https://blog.congruentlabs.co/content/images/size/w1000/2020/11/image.png 1000w, https://blog.congruentlabs.co/content/images/2020/11/image.png 1458w" sizes="(min-width: 720px) 720px"></figure><p>It's actually quite expensive to run an online knowledgebase in most instances. <a href="https://readthedocs.org/">Read the Docs</a> would've been the ideal choice, but it's only free for open source projects, and at this stage we don't yet have plans to open source our products. I could've spun up a self-hosted service like Dokuwiki, but that would be an additional $10/month of operating costs, as well as the administrative overhead for keeping it up to date.</p><p>An alternative we thought about was to use publicly accessible Google Docs, but they lose the ability to be indexed by search engines for people to actually find these guides. There's not much use to public documentation if it's not actually discoverable by the public.</p><h3 id="signata-mfa-presentation">Signata MFA Presentation</h3><p>I've also thrown a presentation that we put together about Signata MFA into the repository as well. <a href="https://github.com/cl-tim/signata-mfa-docs/blob/main/signata-mfa-overview-presentation.pdf">You can find it here</a> - it'll give you a quick overview of the product, it's capabilities, it's pricing, and how to get in contact with us.</p><p>Have you looked at Signata MFA? <a href="https://mfa.signata.net/">You can check it out by clicking here</a>. You can try it for free for a month, no credit card required.</p>
                </div>
            </section>

            
            
        </article>
    </div>
</div></div>]]>
            </description>
            <link>https://blog.congruentlabs.co/the-cheapest-online-documentation-repository-for-startups/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25023085</guid>
            <pubDate>Sun, 08 Nov 2020 04:45:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Undemocratic Republic: The Tyranny of the Senate]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25022747">thread link</a>) | @krasney
<br/>
November 7, 2020 | https://ni.chol.as/posts/senate-reform/ | <a href="https://web.archive.org/web/*/https://ni.chol.as/posts/senate-reform/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>“It may happen that this majority of States is a small minority of the people of America; and two thirds of the people of America could not long be persuaded, upon the credit of artificial distinctions and syllogistic subtleties, to submit their interests to the management and disposal of one third.” Alexander Hamilton, <a href="https://www.newyorker.com/news/hendrik-hertzberg/alexander-hamilton-speaks-out-iii-two-senators-per-state-regardless-of-population" target="_blank" rel="nofollow noopener noreferrer">Federalist 22</a>, December 14, 1787</p>
<p>“Legislators represent people, not trees or acres. Legislators are elected by voters, not farms or cities or economic interests. … And, if a State should provide that the votes of citizens in one part of the State should be given two times, or five times, or 10 times the weight of votes of citizens in another part of the State, it could hardly be contended that the right to vote of those residing in the disfavored areas had not been effectively diluted.” Reynold v Sims</p>
<p>The US Senate has incredible legislative and “advise and consent” powers. It passes bills, but also approves treaties, confirms Cabinet secretaries, Supreme Court justices, judges, and other officials.</p>
<p>It’s 2020, and:</p>
<ul>
<li>~33% of the population lives in just 4 states—California, Texas, Florida, and New York—and is represented by just 8 Senators.</li>
<li>~50% of the population is represented by just 18 Senators</li>
<li>~70% of the population is represented by just ~1/3 of the Senators</li>
<li>
<p>10% of the population is distributed across the 19 least populated states, and control 38 Senators.</p>
<p><span>
      <a href="https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/8affb/Senate-Population.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
        <source srcset="https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/8ac56/Senate-Population.webp 240w,
https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/d3be9/Senate-Population.webp 480w,
https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/e46b2/Senate-Population.webp 960w,
https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/a1214/Senate-Population.webp 1254w" sizes="(max-width: 960px) 100vw, 960px" type="image/webp">
        <source srcset="https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/8ff5a/Senate-Population.png 240w,
https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/e85cb/Senate-Population.png 480w,
https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/d9199/Senate-Population.png 960w,
https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/8affb/Senate-Population.png 1254w" sizes="(max-width: 960px) 100vw, 960px" type="image/png">
        <img src="https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/d9199/Senate-Population.png" alt="/media/Senate-Population.png" title="/media/Senate-Population.png" loading="lazy">
      </picture>
  </a>
    </span></p>
</li>
</ul>
<p>In 1790, the gap between the smallest and largest states was about 10x. Today, the most populous state, California, is about 70x more populous than the least populous state, Wyoming. A resident of California, New York, Florida, or Texas has no effective impact on the Senate impact, and a resident of a smaller state Wyoming, Vermont, Alaska, the Dakotas, Delaware, Rhode Island, or Maine has a disproportionate impact on the Senate.</p>
<p>While the Electoral College favors smaller states as well, it is still mostly driven by membership in the House of Representatives, which is proportionate to population. That’s unfair—the popular vote loser became president in 2000 and 2016— but it’s far less unfair than the even-more-undemocratic Senate.</p>
<p><strong>The Senate has become the defining slow-motion crisis of American democracy.</strong></p>
<ul>
<li><strong>The US Senate could be the <a href="https://nymag.com/intelligencer/2020/08/senate-washington-dc-puerto-rico-statehood-filibuster-obama-biden-racist.html" target="_blank" rel="nofollow noopener noreferrer">most structurally racist institution</a>.</strong> In the 1960s, Black people from the South <a href="https://onlinelibrary.wiley.com/doi/abs/10.3162/036298006X201869" target="_blank" rel="nofollow noopener noreferrer">migrated to populous Northern states</a>, eroding their political representation in the Senate. Similarly, recent decades have seen <a href="https://onlinelibrary.wiley.com/doi/abs/10.3162/036298006X201869" target="_blank" rel="nofollow noopener noreferrer">growing Latinx populations in urban areas</a>, primarily located in populous states. As a result, <a href="https://www.nytimes.com/2018/10/14/opinion/dc-puerto-rico-statehood-senate.html" target="_blank" rel="nofollow noopener noreferrer">David Leonhardt found</a> that whites have 0.35 Senators per million people, Blacks have 0.26, Asian-Americans 0.25, and Latinos just 0.19.</li>
<li><strong>Partisanship and rule changes have caused the problem to boil over.</strong> According to <a href="https://govtrackinsider.com/the-senate-has-never-been-as-un-democratic-as-it-was-in-2017-2018-and-minority-rule-could-801e1046af28" target="_blank" rel="nofollow noopener noreferrer">a 2018 analysis</a>, the Senate has never been as undemocratic as it was in 2017-2018. According to this analysis, “in 2017, for the first time, the Senate’s decisions were often made by a coalition of states representing less than half of the country’s population. The median share of senators supporting passed bills, confirmed judges and agency leaders, and other matters dropped to 58% (the lowest since 1930), with those senators representing just 49.5% of the U.S. population (the lowest ever)!”</li>
</ul>
<p>It’s time for us to revisit the Connecticut Compromise, the Devil’s Bargain that allowed for the adoption of the Constitution at the expense of a democratic Senate.</p>
<h2 id="whats-the-role-of-the-senate"><a href="#whats-the-role-of-the-senate" aria-label="whats the role of the senate permalink"></a>What’s the Role of the Senate?</h2>
<p>The Senate is a smaller, more deliberative body than the House of Representatives. You have to be 30 years old to run for the Senate, and in 1790, you would have had an average life expectancy of about <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2885717/" target="_blank" rel="nofollow noopener noreferrer">45 years</a>. Indeed, “Senator” comes from the Latin word “senex,” which means “senior” or “old man.”</p>
<p>Senators run on staggered 6-year terms, not the 2-year term of the House. That means that their terms exceed presidential terms and—unlike representatives—they don’t have to run in every election. While there are 435 representatives in the House, there are only 100 senators. Unlike members of the House, Senate representatives were meant to be appointed by the state legislatures, a process that changed after the 17th amendment in 1913. This meant that the Senate was meant to be a kind of U.N.-but-with-teeth for 13 (and now 50) coequal sovereigns.</p>
<p>These features—insulation from short-term political pressure and a more selective and deliberative group of representatives—are unrelated to the two-per-state design of the US Senate, and don’t benefit from it.</p>
<h2 id="the-connecticut-compromise-was-pragmatic-but-not-wise"><a href="#the-connecticut-compromise-was-pragmatic-but-not-wise" aria-label="the connecticut compromise was pragmatic but not wise permalink"></a>The Connecticut Compromise was Pragmatic, but Not Wise</h2>
<p>Some people defend the Connecticut Compromise as an enduring legacy of founder wisdom. But the Connecticut Compromise wasn’t meant to be wise, it was meant to be pragmatic. </p>
<p>Seven weeks into the Constitutional Convention of 1787, the founders were at an impasse.</p>
<p>Many large states felt that representation should be proportional to population in Congress. As larger states would be contributing more to the treasury and defense, they felt that should have more say.</p>
<p>Smaller states like Delaware and Rhode Island—and representatives from certain larger but slower-growing states like New York (except for Alexander Hamilton, of course)—weren’t having it, and had leverage. These states enjoyed authority and autonomy under the Articles of Confederation. Proportionate representation—plus anticipation of a rapidly growing South and West—would erode their political power.</p>
<p>Connecticut delegates Roger Sherman and Oliver Ellsworth, with the support of Benjamin Franklin, struck a compromise: a bicameral (two-chambered) legislature, with one chamber, the House of Representatives, allocating seats in proportion to population, and the other chamber, the Senate, allocated evenly by state. Benjamin Franklin proposed that the House originate all revenue matters.</p>
<p>Alexander Hamilton hated it. Writing in Federalist 22:</p>
<blockquote>
<p>Its operation contradicts the fundamental maxim of republican government, which requires that the sense of the majority should prevail. Sophistry may reply, that sovereigns are equal, and that a majority of the votes of the States will be a majority of confederated America. But this kind of logical legerdemain will never counteract the plain suggestions of justice and common-sense.</p>
</blockquote>
<p>Madison, in Federalist 62, didn’t want to defend it:</p>
<blockquote>
<p>The equality of representation in the Senate is another point, which, being evidently the result of compromise between the opposite pretensions of the large and the small States, does not call for much discussion.</p>
</blockquote>
<p>The decision to allocate Senate seats was a compromise needed to ratify the Constitution and satisfice an existing power structure—nothing more.</p>
<h2 id="this-is-the-hardest-thing-to-change-about-the-constitution"><a href="#this-is-the-hardest-thing-to-change-about-the-constitution" aria-label="this is the hardest thing to change about the constitution permalink"></a>This is the Hardest Thing to Change about the Constitution</h2>
<p>One possible mechanism of reform is amending the Constitution. Suppose you wanted to change the Senate to allocate seats in proportion to population. Not so fast—Article V of the Constitution writes that:</p>
<blockquote>
<p><strong>The Congress, whenever two thirds of both Houses shall deem it necessary, shall propose Amendments to this Constitution,</strong> or, on the Application of the Legislatures of two thirds of the several States, shall call a Convention for proposing Amendments, which, in either Case, shall be valid to all Intents and Purposes, as Part of this Constitution, when ratified by the Legislatures of three fourths of the several States, or by Conventions in three fourths thereof, as the one or the other Mode of Ratification may be proposed by the Congress; Provided that no Amendment which may be made prior to the Year One thousand eight hundred and eight shall in any Manner affect the first and fourth Clauses in the Ninth Section of the first Article; <strong>and that no State, without its Consent, shall be deprived of its equal Suffrage in the Senate.</strong></p>
</blockquote>
<p>Article V articulates a state right—equal suffrage in the Senate—that cannot be deprived through the Constitutional change process. Put differently: you can change anything in the Constitution (after 1808) through amendments or a convention, but you cannot change this aspect of the Connecticut Compromise.</p>
<p>In practice, even Justice Scalia <a href="https://www.newyorker.com/news/hendrik-hertzberg/the-weirdest-sentence-in-the-u-s-constitution" target="_blank" rel="nofollow noopener noreferrer">once remarked</a> that he didn’t see how the Supreme Court could declare a properly proposed and ratified Constitutional amendment unconstitutional. It is unclear how a future Supreme Court might view a dispute around this kind of Constitutional amendment, although any well-written amendment would begin by changing Article V.</p>
<p>Notwithstanding this speed bump, passing constitutional amendments requires the consent of 3/4 of the states, meaning that the 12 least populous states could block any Senate reform amendment.</p>
<h2 id="finding-a-way-out"><a href="#finding-a-way-out" aria-label="finding a way out permalink"></a>Finding a Way Out</h2>
<h2 id="what-does-a-good-solution-look-like"><a href="#what-does-a-good-solution-look-like" aria-label="what does a good solution look like permalink"></a>What does a good solution look like?</h2>
<p>An improvement to the representation problem has the following qualities:</p>
<ul>
<li>It is legal. In this sense, legal means that it would survive a challenge in the Supreme Court in which the justices were approved by a partisan Senate.</li>
<li>It preserves the “deliberative” aspects of the Senate.</li>
<li>It reduces population inequality in the Senate.</li>
<li>It is politically tractable.</li>
</ul>
<p>A better solution to this problem has these additional properties:</p>
<ul>
<li>New statehood, or reorganized statehood, is incentive-compatible. That means that citizens of prospective new states (Puerto Rico, for instance) should have the opportunity to form a state and be admitted to the Senate without disproportionately diluting power.</li>
<li>Senate proportionality is robust to population shifts. If Wyoming became a booming population center, then Wyoming should have more Senate representation.</li>
<li>It decreases wasted votes and reduces the <a href="https://www.brennancenter.org/sites/default/files/legal-work/How_the_Efficiency_Gap_Standard_Works.pdf" target="_blank" rel="nofollow noopener noreferrer">efficiency gap</a>. Conservatives in California and liberals in Texas have important interests, and they deserve a say.</li>
</ul>
<h3 id="state-swaps"><a href="#state-swaps" aria-label="state swaps permalink"></a>State Swaps</h3>
<p>The Constitution didn’t anticipate political parties, and Washington’s farewell address decried them as “likely in the course of time and things, to become potent engines, by which cunning, ambitious, and unprincipled men will …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ni.chol.as/posts/senate-reform/">https://ni.chol.as/posts/senate-reform/</a></em></p>]]>
            </description>
            <link>https://ni.chol.as/posts/senate-reform/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25022747</guid>
            <pubDate>Sun, 08 Nov 2020 03:50:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[QR Codes Aren't Magic]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25022738">thread link</a>) | @allending
<br/>
November 7, 2020 | https://blog.snappymob.com/qr-codes-arent-actually-magic | <a href="https://web.archive.org/web/*/https://blog.snappymob.com/qr-codes-arent-actually-magic">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p>Especially after COVID-19 began plaguing the planet, QR codes have been a big part of our daily lives. In this day and age, it’s quite impossible for one to have never seen a QR code, or used one. Even before the pandemic, QR codes had been around us for a while, but many of us — having grown up in the Digital Age — simply accept new technology into our lives without wondering too much about how they work. For most of us, QR codes just look like... square pixelated versions of the alien heptapod symbols in Arrival (2016), that somehow, magically (and really quickly) bring us to a different screen when we scan them.</p>
<!--more-->
<p>So what are these cryptic codes? Are they magic? Clearly they’re not, but how do they work? Before we take you in for the ride, let’s get the basics down.&nbsp;</p>
<p>QR stands for <em>Quick Response</em>, and a QR code is pretty much a barcode (which has been around since the 1950s) except two-dimensional. Because it’s 2D, it can contain more data than a barcode. It can encode over 7000 characters, which is a vast improvement from the standard barcode with a limited capacity of about 20 alphanumeric characters.</p>
<h2>How did they come about?&nbsp;</h2>
<p>In Japan circa 1990s, Denso Wave Incorporated was contacted by manufacturing sites and asked if it was possible to come up with a faster barcode scanning system. This presented a problem as barcodes had a limited capacity, which made the scanning process time consuming for workers no matter how efficient the scanner was. To tackle this, Denso Wave began developing a compact code that can contain more data, including Kanji and Kana characters. The launch of the QR code was later announced in 1994.</p>
<h2>What are they used for?</h2>
<p>The QR code was first used by the auto industry to track parts and products shipped around the globe. Then, gradually, other industries all over the world began joining in. Today, QR codes are used by nearly every physical and digital establishment for quick check-ins, displaying geolocation or contact info, redirecting customers to their websites, etc.</p>
<h2>How do they work?</h2>
<p>Grasping how a QR code works would require understanding the functions of its different parts. With the help of the labelled diagram, we hope to make this palatable for the layman. Let’s break it down.</p>
<p><img src="https://blog.snappymob.com/hs-fs/hubfs/QR%20Structure.png?width=715&amp;name=QR%20Structure.png" alt="QR Structure, modules, separators, alignment, timing, format info, version, labels, black and white" width="715" srcset="https://blog.snappymob.com/hs-fs/hubfs/QR%20Structure.png?width=358&amp;name=QR%20Structure.png 358w, https://blog.snappymob.com/hs-fs/hubfs/QR%20Structure.png?width=715&amp;name=QR%20Structure.png 715w, https://blog.snappymob.com/hs-fs/hubfs/QR%20Structure.png?width=1073&amp;name=QR%20Structure.png 1073w, https://blog.snappymob.com/hs-fs/hubfs/QR%20Structure.png?width=1430&amp;name=QR%20Structure.png 1430w, https://blog.snappymob.com/hs-fs/hubfs/QR%20Structure.png?width=1788&amp;name=QR%20Structure.png 1788w, https://blog.snappymob.com/hs-fs/hubfs/QR%20Structure.png?width=2145&amp;name=QR%20Structure.png 2145w" sizes="(max-width: 715px) 100vw, 715px"></p>
<p>(Image Source: techspot.com)</p>
<p>The tiny squares in a QR code are called <em><strong>modules</strong></em> - black squares would be considered foreground modules, and white ones would be background modules. They don’t always have to be black and white, though, they could be in color too. Most QR codes are in black and white only so decoder softwares easily distinguish the contrast between the background and foreground. If you want your QR code to be colorful, you just have to make sure that the contrast is retained when it is in grayscale / black and white.</p>
<p>The bigger the code, the more rows and columns of modules it will contain. Yup, just like humankind, QR codes come in different shapes, sizes and colors (although there are standards to follow).</p>
<p>There are 40 preset sizes (or, <em><strong>versions</strong></em>) to choose from. Version 1 being the smallest type with 21 rows and 21 columns, and Version 40 being the largest with 177 rows and 177 columns. When a generator produces a code for you, it will determine the suitable QR code version number for you depending on the amount of data you are encoding.</p>
<p>The 3 big squares on 3 corners of the QR code are <em><strong>finder patterns</strong></em>, which help your device camera determine the boundaries of the code and its correct orientation. This is so if the code is rotated, or upside-down, decoders would still be able to read it. These finder patterns are surrounded by a single-module spacing called <em><strong>separators</strong></em> which help decoders separate the finder patterns from the code data.</p>
<p>Connecting each finder pattern are <em><strong>timing patterns</strong></em> that alternate between black and white. These lines tell your decoder software how big the data cells are within the code.</p>
<p>The smaller square (that is distinctively not a finder pattern) on the fourth corner is an <em><strong>alignment pattern</strong></em> that helps the decoder prevent image distortions. Level 1 QR codes (the smallest size of QR codes) do not contain alignment patterns. The bigger the code, however, the more alignment patterns are added.&nbsp;</p>
<p>The 15 bits beside the separators are <em><strong>format and version strings</strong></em> which contain information on the code’s error correction level and the chosen mask pattern for the particular code. Hold on… information on the what and what?</p>
<p><em><strong>Error correction</strong></em> code makes sure that the code is still readable if up to 30% of the code is corrupt. Designers or generators of a code can decide how many levels of error correction they want to include in the code. The higher the level of error correction, the higher percentage of corruption a decoder can read past. However, the higher the level of error correction, the more space it takes up on the QR code, leaving less space for data. In other words, the more space the error correction code takes up, the lower the max numerical characters a QR code can fit. How much error correction a QR code needs probably will depend on where it will be displayed. E.g. A QR code on a paper flyer would need higher error correction than one on a laminated poster placed indoors, and a digital QR code could do with none because it is unlikely to be damaged.</p>
<p>The code is also overlaid by a <em><strong>mask pattern</strong></em>, inverting and retaining certain data areas to “mix it up” / conceal it. Decoder softwares detect the mask type from the format information and demask the QR code before reading the rest of the data. Of course this happens in a matter of milliseconds.</p>
<p><em><strong>Encoded characters </strong></em>- The black and white modules read in a fixed zigzaggy direction (as shown in the image), starting from the bottom right corner, gives the decoder a sequence of data bits that goes something like 001010111…&nbsp;&nbsp;&nbsp;</p>
<p><img src="https://blog.snappymob.com/hs-fs/hubfs/Google%20Drive%20Integration/Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png?width=447&amp;name=Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png" alt="QR code, zigzag sequence, binary sequence, pattern" width="447" srcset="https://blog.snappymob.com/hs-fs/hubfs/Google%20Drive%20Integration/Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png?width=224&amp;name=Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png 224w, https://blog.snappymob.com/hs-fs/hubfs/Google%20Drive%20Integration/Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png?width=447&amp;name=Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png 447w, https://blog.snappymob.com/hs-fs/hubfs/Google%20Drive%20Integration/Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png?width=671&amp;name=Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png 671w, https://blog.snappymob.com/hs-fs/hubfs/Google%20Drive%20Integration/Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png?width=894&amp;name=Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png 894w, https://blog.snappymob.com/hs-fs/hubfs/Google%20Drive%20Integration/Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png?width=1118&amp;name=Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png 1118w, https://blog.snappymob.com/hs-fs/hubfs/Google%20Drive%20Integration/Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png?width=1341&amp;name=Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png 1341w" sizes="(max-width: 447px) 100vw, 447px"><br>(Image Source: nayuki.io)</p>
<p>Different binary sequences represent different codewords, which include symbols, lowercase letters, uppercase letters, etc. For example, the binary sequence 01110111 represents the letter ‘w’, 00100001 represents the exclamation point ‘!’, and 00100000 represents a space.</p>
<h2><strong>How can you make a QR code?</strong></h2>
<p><span><img src="https://lh3.googleusercontent.com/Mi8ate-UiC0ZpKEoRFn8EUpe5bZDS5O6ggTSK96wPR1OTHx_zoKu_dbS5NvJDpPRsDAt9qdHXkQdfo1sm8Jun4Id0dpS3JjSnmOO51mXv6NT1ql7rb5P0DJc01k-kuw9RYw5vMF2" alt="QR code, generated QR code, snappymob website" width="297"></span></p>
<p>Here’s a QR code generated with <a href="http://www.qr-code-generator.com/"><span>www.qr-code-generator.com</span></a>. Do the patterns make a lot more sense to you now? If you can’t answer in confidence, that’s okay. You’re not going to have to make one by hand, ever. (Unless you want to.)&nbsp;</p>
<p>There are plenty of offline and online QR code generators free for use. All you have to do is key in what you want to encode in the QR code, be it a link to a coupon redemption page, a YouTube video, your social media sites, your contact details, or simply a website homepage.&nbsp;</p>
<p>Most free QR code generators, though, require you to sign up or subscribe to a plan to gain access to more design options and features, such as error correction levels and insights tracking.&nbsp;</p>
<h2><strong>How can QR codes help you?</strong></h2>
<p>Whether you own a business or simply have a web page to share, a QR code could help people reach you easily and quickly. In case you were looking for ideas, here are some of the ways in which using a QR code could greatly benefit you:</p>
<h4><strong>Webpages and Location</strong></h4>
<p>Your customers can get to your website, social media platforms, or map location in a matter of seconds. They don’t have to manually type your website URL, usernames, or addresses into their browsers or maps. They simply need to whip out their phones and scan the QR code to be redirected to you.</p>
<h4><strong>Payment</strong></h4>
<p>Payment is also made easy with QR codes. Most stores enable QR codes for quick payment via e-wallets so customers don’t have to fumble through their wallets for cash and keep others waiting. Some good examples would be GrabPay or ShopeePay, which are enabled at many on and offline merchants in Malaysia.</p>
<p><img src="https://blog.snappymob.com/hs-fs/hubfs/grabpayshopeepay.jpg?width=648&amp;name=grabpayshopeepay.jpg" alt="grabpay, shopeepay, qr code, merchant, scan to pay, app" width="648" srcset="https://blog.snappymob.com/hs-fs/hubfs/grabpayshopeepay.jpg?width=324&amp;name=grabpayshopeepay.jpg 324w, https://blog.snappymob.com/hs-fs/hubfs/grabpayshopeepay.jpg?width=648&amp;name=grabpayshopeepay.jpg 648w, https://blog.snappymob.com/hs-fs/hubfs/grabpayshopeepay.jpg?width=972&amp;name=grabpayshopeepay.jpg 972w, https://blog.snappymob.com/hs-fs/hubfs/grabpayshopeepay.jpg?width=1296&amp;name=grabpayshopeepay.jpg 1296w, https://blog.snappymob.com/hs-fs/hubfs/grabpayshopeepay.jpg?width=1620&amp;name=grabpayshopeepay.jpg 1620w, https://blog.snappymob.com/hs-fs/hubfs/grabpayshopeepay.jpg?width=1944&amp;name=grabpayshopeepay.jpg 1944w" sizes="(max-width: 648px) 100vw, 648px"><br>(Image Source: help.shopee.com.my, grab.com)</p>
<h4><strong>Email and Direct Messaging</strong></h4>
<p>You can embed URL links that lead straight to a ‘compose email’ or ‘compose direct message’ page (for instance, <a href="mailto:hello@snappymob.com">mailto:hello@snappymob.com</a>) into your QR code. For email, this saves your customers up to 10 seconds because it helps them skip the process of opening their email app, tapping on compose email, and typing your email address manually into the recipient bar. For social media, this saves them the effort of opening the app, typing your username into the search bar, and tapping on the message button. This might literally be “a matter of seconds” which seems small, but it keeps people who are interested in your services or products, well, interested. Making things quicker and easier for your patrons is always a good move.</p></span></p><p><label>app insights</label></p>
        
      </div></div>]]>
            </description>
            <link>https://blog.snappymob.com/qr-codes-arent-actually-magic</link>
            <guid isPermaLink="false">hacker-news-small-sites-25022738</guid>
            <pubDate>Sun, 08 Nov 2020 03:48:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why you should stop using Google Alerts]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25022723">thread link</a>) | @fstopmick
<br/>
November 7, 2020 | https://www.karma.fm/p/0SZfbtW/why-you-should-stop-using-google-alerts | <a href="https://web.archive.org/web/*/https://www.karma.fm/p/0SZfbtW/why-you-should-stop-using-google-alerts">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="report-contribution-modal" tabindex="-1" role="dialog">
    <div role="document">
        <div>
            

            <div>
                <div>
                    <p>
                        Report a violation of our TBD:
                    </p>

                    <dl>
                        <dt><label for="exclude-links">Dead Link:</label></dt>
                        <dd></dd>
                        <dt><label for="exclude-links">Jerk Vibes:</label></dt>
                        <dd></dd>
                        <dt><label for="exclude-links">Copyright Infringement:</label></dt>
                        <dd></dd>
                        <dt><label for="exclude-links">PII:</label></dt>
                        <dd></dd>
                        <dt><label for="exclude-links">Other:</label></dt>
                        <dd></dd>
                    </dl>

                    </div>
            </div>
        </div>
    </div>
</div></div>]]>
            </description>
            <link>https://www.karma.fm/p/0SZfbtW/why-you-should-stop-using-google-alerts</link>
            <guid isPermaLink="false">hacker-news-small-sites-25022723</guid>
            <pubDate>Sun, 08 Nov 2020 03:45:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Stack Videos Horizontally, Vertically, in a Grid With FFmpeg]]>
            </title>
            <description>
<![CDATA[
Score 218 | Comments 47 (<a href="https://news.ycombinator.com/item?id=25022665">thread link</a>) | @rrao84
<br/>
November 7, 2020 | https://ottverse.com/stack-videos-horizontally-vertically-grid-with-ffmpeg/ | <a href="https://web.archive.org/web/*/https://ottverse.com/stack-videos-horizontally-vertically-grid-with-ffmpeg/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<figure>
<amp-img src="https://i0.wp.com/ottverse.com/wp-content/uploads/2020/11/stacking-videos.png?resize=678%2C381&amp;ssl=1" alt="stack videos using ffmpeg" title="stacking-videos" width="678" height="381" layout="intrinsic" i-amphtml-layout="intrinsic"><img src="https://i0.wp.com/ottverse.com/wp-content/uploads/2020/11/stacking-videos.png?resize=678%2C381&amp;ssl=1" alt="stack videos using ffmpeg" title="stacking-videos" width="678" height="381" data-old-src="data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9JzM4MScgd2lkdGg9JzY3OCcgeG1sbnM9J2h0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnJyB2ZXJzaW9uPScxLjEnLz4="></amp-img>
</figure>


<p>Often times, when you want to compare two videos side-by-side or you want to create an effect during post-processing, you might want to stack videos together. It can get expensive if you end up buying a tool to do this, but, guess what? </p>



<p><strong>FFmpeg offers a variety of tools to help stack videos together – horizontally, vertically, or in a grid fashion. In this tutorial, let’s learn about FFmpeg’s <code>hstack</code> and <code>vstack</code> filters for stacking videos. </strong></p>



<hr data-amp-original-style="height:50px">




<h2><span id="How_to_Stack_Videos_Horizontally_using_FFmpeg"></span><strong>How to Stack Videos Horizontally using FFmpeg?</strong><span></span></h2>



<p>“Horizontally stacking videos” refers to placing videos side-by-side (one on the left and the other on the right). </p>



<p>Before you do this, there are a couple of points that you need to consider. </p>



<ol><li>The videos that you want to stack need to have the same height. </li><li> The videos need to have the same pixel format. </li></ol>



<p>The command line is shown below where we try and stack two <code>mp4</code> videos. </p>



<pre><code>ffmpeg -i input0.mp4 -i input1.mp4&nbsp;-filter_complex hstack=inputs=2 horizontal-stacked-output.mp4</code></pre>



<p>The <code>hstack</code> filter has a simple format. You need to specify the number of inputs and it parses that from the beginning portion of the commandline. The order of stacking follows the order of inputs. </p>



<p>Here is a screenshot of what it looks like. </p>



<figure><amp-img src="https://lh6.googleusercontent.com/4FwbqByqDpDpOGTNvpSWSOh6RVE9yzesrQyAyGvaOF1ZjVsycKOlQUjrTmdJRFJWblHjC9gF6zMds0s5w2Yy2w6CVXXme-H-zF8nYoJ496khN0aHXHaWbnwEe41BYmu6c2mdoQb8" alt="stack videos using ffmpeg" width="1600" height="423" layout="intrinsic" i-amphtml-layout="intrinsic"><img src="https://lh6.googleusercontent.com/4FwbqByqDpDpOGTNvpSWSOh6RVE9yzesrQyAyGvaOF1ZjVsycKOlQUjrTmdJRFJWblHjC9gF6zMds0s5w2Yy2w6CVXXme-H-zF8nYoJ496khN0aHXHaWbnwEe41BYmu6c2mdoQb8" alt="stack videos using ffmpeg" width="1600" height="423" data-old-src="data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9JzQyMycgd2lkdGg9JzE2MDAnIHhtbG5zPSdodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZycgdmVyc2lvbj0nMS4xJy8+"></amp-img></figure>



<p>And, here is a video! </p>











<p>Here is another use case. Companies or teams working on video compression often like to compare videos side-by-side in the lab or showcase their work in conferences. FFmpeg’s horizontal stacking is an easy way to do this and achieve a very good result. </p>



<p>Below are two videos encoded at different video quality settings and stacked horizontally. Comparison made simple, right? <em>(note: Vimeo’s choise of bitrate might mess with the comparison, but, when done offline (downloaded), the <code>hstack</code> filter makes comparisons easy!)</em></p>







<hr data-amp-original-style="height:50px">



<h2><span id="Stacking_Videos_Vertically_using_FFmpeg"></span><strong>Stacking Videos Vertically using FFmpeg</strong><span></span></h2>



<p>“Vertically stacked videos” results in placing videos one below the other. Unlike in horizontal stacking, inputs need to be having the same width. The command is as shown.&nbsp;</p>



<p>For vertical stacking, we need to use the <code>vstack</code> filter whose syntax is similar to the <code>hstack</code> filter we used in the previous horizontal stacking example.</p>



<pre><code>ffmpeg -i input0.mp4 -i input1.mp4&nbsp;-filter_complex vstack=inputs=2 vertical-stack-output.mp4</code></pre>



<figure><amp-img src="https://lh5.googleusercontent.com/qtyxioWFR54pqwZhX7jgX6HkSG7gCw840GdK78HGHHP7X3sv3fr3Pou9hOMFu2O-e6O7nmCith3U6CM1SpDanhwmIFIBzZUQyaG4T0BJaF9QCdMIPrmMmH0qc9WTK5f-5Nf4-92y" alt="stack videos using ffmpeg" width="1248" height="1400" layout="intrinsic" i-amphtml-layout="intrinsic"><img src="https://lh5.googleusercontent.com/qtyxioWFR54pqwZhX7jgX6HkSG7gCw840GdK78HGHHP7X3sv3fr3Pou9hOMFu2O-e6O7nmCith3U6CM1SpDanhwmIFIBzZUQyaG4T0BJaF9QCdMIPrmMmH0qc9WTK5f-5Nf4-92y" alt="stack videos using ffmpeg" width="1248" height="1400" data-old-src="data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9JzE0MDAnIHdpZHRoPScxMjQ4JyB4bWxucz0naHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmcnIHZlcnNpb249JzEuMScvPg=="></amp-img></figure>



<p>Both functions pretty much use the same commands with a simple distinction, the <a href="https://ffmpeg.org/ffmpeg-filters.html#hstack" target="_blank" rel="noopener"><code>hstack</code></a> and the <a href="https://ffmpeg.org/ffmpeg-filters.html#vstack" target="_blank" rel="noopener"><code>vstack</code></a> under the <code>-filter_complex</code> argument.&nbsp;</p>



<p>Here’s a video of stacking two videos vertically using FFmpeg. </p>







<hr data-amp-original-style="height:50px">



<h2><span id="Stacking_Videos_of_Different_Lengths"></span><strong>Stacking Videos of Different Lengths</strong><span></span></h2>



<p>Well, there’s a really nifty ability for both of these to prioritize the length of the shortest video. And as luck would have it the parameter is named <code>shortest</code>, and it’s applicable to both the horizontal and vertical stacking filters. Using <code>shortest=1</code> ensures the shortest length is used. </p>



<p>For example – </p>



<pre><code>ffmpeg -i input0.mp4 -i input1.mp4 -filter_complex hstack=inputs=2:shortest=1 shortest-output.mp4</code></pre>



<p>As a <b>side note</b>, if you run into an error that claims frames are being duplicated, the easiest workaround is to slip the <code>vsync 2</code> parameter into your command, and it worked like a charm.</p>



<h3><span id="Stacking_Videos_of_Different_Lengths_Without_the_shortest_parameter"></span>Stacking Videos of Different Lengths Without the <code>shortest</code> parameter<span></span></h3>



<p>To test what happens in this situation, let’s stack two videos vertically – a 10 second clip and an 18 second clip. You’ll see that the shorter clip just stops after it completes, but the output video continues till the longest of the input clips complete.  </p>















<p>If you want to truncate the clips to the length of the shortest clip, then you need to use the <code>shortest=1</code> parameter. Let’s look at that in the next section.</p>



<h3><span id="Stacking_Videos_of_Different_Lengths_With_the_shortest=1_parameter"></span>Stacking Videos of Different Lengths With the <code>shortest=1</code> parameter<span></span></h3>



<p>In this example, we use the <code>shortest=1</code> command-line parameter and as you can see, the length of the final video is truncated to the length of the shortest of the inputs. </p>







<hr data-amp-original-style="height:50px">



<h2><span id="2%C3%972_Grid_of_Videos_using_FFmpeg"></span><strong><strong>2×2 Grid of Videos using FFmpeg</strong></strong><span></span></h2>



<p>We can achieve a 2×2 grid of videos using a combination of the <code>hstack</code> and <code>vstack</code> filters. Let’s start by looking at the command-line and then break it down. It’s actually pretty simple! </p>



<pre><code>ffmpeg \
-i input0.mp4 -i input1.mp4 -i input2.mp4 -i input3.mp4 \
-filter_complex \
"[0:v][1:v]hstack=inputs=2[top]; \
[2:v][3:v]hstack=inputs=2[bottom]; \
[top][bottom]vstack=inputs=2[v]" \
-map "[v]" \
finalOutput.mp4</code></pre>



<p>What’s happening here?</p>



<ul><li>firstly, you need to provide 4 input videos with the same height and width</li><li>next, you stack the first two videos horizontally and call it “top” i.e. <code>[0:v][1:v]hstack=inputs=2[top]</code></li><li>then, you you stack the next two videos horizontally and call it “bottom” i.e. <code>[2:v][3:v]hstack=inputs=2[bottom]</code></li><li>then, you stack <code>top</code> and <code>bottom</code> vertically to create a 2×2 grid. — <code>[top][bottom]vstack=inputs=2[v]</code></li><li>then using the <code>map</code> command, we can extract and push the video track to the output container. </li></ul>



<p>Here is what the video looks like. </p>







<hr data-amp-original-style="height:50px">



<h2><span id="3%C3%972_Grid_of_Videos_using_FFmpeg"></span><strong>3×2 Grid of Videos using FFmpeg</strong><span></span></h2>



<p>Along the same lines, here is a 3×2 grid of videos using <code>hstack</code> and <code>vstack</code> filters. </p>



<pre><code>ffmpeg \
-i input0.mp4 -i input1.mp4 \
-i input2.mp4 -i input3.mp4 \
-i input4.mp4 -i input5.mp4 \
-filter_complex \
"[0:v][1:v][2:v]hstack=inputs=3[top];\
[3:v][4:v][5:v]hstack=inputs=3[bottom];\
[top][bottom]vstack=inputs=2[v]" \
-map "[v]" \
finalOutput.mp4</code></pre>







<hr data-amp-original-style="height:50px">



<h2><span id="Conclusion"></span>Conclusion<span></span></h2>



<p>That’s it folks. Now you know how to stack videos together horizontally, vertically, and in a grid. This is very useful in comparing videos and also creating fun effects along the way! </p>



<p data-amp-original-style="background-color:#f8da89">If you enjoyed this post, do check out the rest of<a href="https://ottverse.com/category/ffmpeg/"> <strong>OTTVerse’s FFmpeg tutorials</strong></a> to learn more about this amazing media editing and compression software!  </p>

	</div></div>]]>
            </description>
            <link>https://ottverse.com/stack-videos-horizontally-vertically-grid-with-ffmpeg/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25022665</guid>
            <pubDate>Sun, 08 Nov 2020 03:36:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Deploy Azure Function Apps with Powershell]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25022651">thread link</a>) | @davideguida
<br/>
November 7, 2020 | https://www.davideguida.com/how-to-deploy-azure-function-apps-with-powershell/ | <a href="https://web.archive.org/web/*/https://www.davideguida.com/how-to-deploy-azure-function-apps-with-powershell/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Hi All! Today I want to show a quick’n’dirty way to easily deploy your projects to Azure using Powershell.</p><p>I’ve been working a lot recently with Azure Functions and Web Apps. And of course, each time I’m confident with my code, I want to see it deployed on the Cloud.</p><p>Of course in an ideal world, we all would have a nice CI/CD pipeline, potentially on <a href="https://azure.microsoft.com/en-us/services/devops/?WT.mc_id=DOP-MVP-5003878" target="_blank" rel="noreferrer noopener">Azure DevOps</a>. It might happen, however, that for one reason or another, you can only get up to CI, without being able to deploy.</p><p>So the only option you have is to manually handle deployments, most likely from your local machine. But what happens if you have to deploy it to multiple destinations?</p><p>In my case, for example, I had to deploy a Function App and a Web App to multiple client subscriptions. Of course, you can always do this <a href="https://docs.microsoft.com/en-us/visualstudio/deployment/quickstart-deploy-to-azure?view=vs-2019&amp;WT.mc_id=DOP-MVP-5003878" target="_blank" rel="noreferrer noopener">directly from Visual Studio</a>, but it still feels like a lot of manual work.</p><h4>What if instead you can have a very nice script that handles all the grunt work for you?</h4><p>Moreover, you could potentially reuse it when you finally manage to get to the Continuous Deployment part.</p><p>So, the first step is to create the <a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/artifacts/artifacts-overview?view=azure-devops&amp;WT.mc_id=DOP-MVP-5003878" target="_blank" rel="noreferrer noopener">Release Artifact</a>. I am assuming, of course, that you’ve ran already <a href="https://www.davideguida.com/testing-azure-functions-on-azure-devops-part-1-setup/" target="_blank" rel="noreferrer noopener">your Tests</a> and everything went fine.</p><p>My weapon of choice for these scripts today, will be Powershell:</p><pre data-enlighter-language="powershell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">function publish{
    param(
        $projectName        
    )

    $projectPath="src/$($projectName)/$($projectName).csproj"
    $publishDestPath="publish/" + [guid]::NewGuid().ToString()

    log "publishing project '$($projectName)' in folder '$($publishDestPath)' ..." 
    dotnet publish $projectPath -c Release -o $publishDestPath

    $zipArchiveFullPath="$($publishDestPath).Zip"
    log "creating zip archive '$($zipArchiveFullPath)'"
    $compress = @{
        Path = $publishDestPath + "/*"
        CompressionLevel = "Fastest"
        DestinationPath = $zipArchiveFullPath
    }
    Compress-Archive @compress

    log "cleaning up ..."
    Remove-Item -path "$($publishDestPath)" -recurse

    return $zipArchiveFullPath
}</pre><p>Here I’m building a temporary path using a GUID and calling <em><a href="https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-publish?WT.mc_id=DOP-MVP-5003878" target="_blank" rel="noreferrer noopener">dotnet publish</a> </em>to compile the Project and output the binaries to it. Then we generate a Zip archive and get rid of the publish folder.</p><p>The <em>log </em>function is just a simple wrapper over <em>Write-Host</em>, I just added some fancy colors to highlight the text:</p><pre data-enlighter-language="powershell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">function log{
    param(
        $text
    )

    write-host $text -ForegroundColor Yellow -BackgroundColor DarkGreen
}</pre><p>Now that we have our Artifact, the next step is to deploy it to Azure. If you, like me, are working with Azure Functions, this is the script for you:</p><pre data-enlighter-language="powershell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">function deploy{
    param(
        $zipArchiveFullPath,
        $subscription,
        $resourceGroup,        
        $appName
    )    

    log "deploying '$($appName)' to Resource Group '$($resourceGroup)' in Subscription '$($subscription)' from zip '$($zipArchiveFullPath)' ..."
    az functionapp deployment source config-zip -g "$($resourceGroup)" -n "$($appName)" --src "$($zipArchiveFullPath)" --subscription "$($subscription)"   
}</pre><p>It simply takes the full path to the zip archive we produced before and the name of the destination Azure Subscription, Resource Group and Application. Easy peasy.</p><p>Now, I’ve found particularly handy to set some basic application settings, right after the deployment. For this, I keep a simple JSON file with key/value pairs and deploy it using this script:</p><pre data-enlighter-language="powershell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">function setConfig{
    param(
        $subscription,
        $resourceGroup,        
        $appName,
        $configPath
    )
    log "updating application config..."
    az functionapp config appsettings set --name "$($appName)" --resource-group "$($resourceGroup)" --subscription "$($subscription)" --settings @$configPath
}</pre><p>The config file can be something like this:</p><pre data-enlighter-language="json" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">{
  "FUNCTIONS_WORKER_RUNTIME": "dotnet",  
  "ASPNETCORE_ENVIRONMENT": "DEV",
  "Foo": "bar"
}
</pre><p>The last step is to put everything together and call it. I would suggest to create a separate script with all the previous functions. We can use it as a “library” and if we’re lucky enough, it won’t even change much when we move to CD.</p><p>For our <em>local</em> deployment script we will instead need two more helper function. The first one will take care of the Artifact:</p><pre data-enlighter-language="powershell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">function createArtifact {
    param(
        $appName
    )
    $zipPath = publish $appName
    if ($zipPath -is [array]) {
        $zipPath = $zipPath[$zipPath.Length - 1]
    }
    return $zipPath
}</pre><p>We can’t unfortunately call directly the <em>publish </em>function because seems that the output from the <em>dotnet publish</em> command will mess a bit with the return value. So we’ll need to do some magic tricks, but not that much.</p><p>Then we can send the artifact to the cloud:</p><pre data-enlighter-language="powershell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">function deployInstance {
    param(      
        $zipPath,  
        $subscription,
        $resourceGroup,        
        $appName,
        $configPath
    )

    deploy $zipPath $subscription $resourceGroup $appName

    if(![string]::IsNullOrEmpty($configPath)){
        setConfig $subscription $resourceGroup $appName $configPath
    }
}</pre><p>If you remember, at the top of the post I said that we might have to deploy the same artifact to multiple destination. Now that we have everything in place, all we have to do is just put the pieces together:</p><pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">$zipPath = createArtifact "MyAwesomeProject" 
deployInstance $zipPath "MyFirstSubscription" "MyFirstResourceGroup" "MyAwesomeProject1" "DEV.settings.json"
deployInstance $zipPath "MySecondSubscription" "MySecondResourceGroup" "MyAwesomeProject2" "DEV.settings.json"
deployInstance $zipPath "MyThirdSubscription" "MyThirdResourceGroup" "MyAwesomeProject3" "DEV.settings.json"</pre><p>…and so on and so forth. I think you got the idea.</p></div></div>]]>
            </description>
            <link>https://www.davideguida.com/how-to-deploy-azure-function-apps-with-powershell/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25022651</guid>
            <pubDate>Sun, 08 Nov 2020 03:32:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Everything you wanted to know about drone light shows]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25022274">thread link</a>) | @zuhayeer
<br/>
November 7, 2020 | https://verge.aero/everything-about-drone-light-shows/ | <a href="https://web.archive.org/web/*/https://verge.aero/everything-about-drone-light-shows/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text">
	
<figure><img loading="lazy" width="2000" height="1126" src="https://verge.aero/wp-content/uploads/2020/05/1_jWBjNhkYW18BwHjpukUUzQ.jpeg" alt="Philadelphia Drone Light Show" srcset="https://verge.aero/wp-content/uploads/2020/05/1_jWBjNhkYW18BwHjpukUUzQ.jpeg 2000w, https://verge.aero/wp-content/uploads/2020/05/1_jWBjNhkYW18BwHjpukUUzQ-300x169.jpeg 300w, https://verge.aero/wp-content/uploads/2020/05/1_jWBjNhkYW18BwHjpukUUzQ-768x432.jpeg 768w, https://verge.aero/wp-content/uploads/2020/05/1_jWBjNhkYW18BwHjpukUUzQ-1536x865.jpeg 1536w" sizes="(max-width: 2000px) 100vw, 2000px"><figcaption>Verge Aero flying over Franklin field for Philadelphia Drone Light Show</figcaption></figure>



<p>“Amazing”, “Beautiful”, “Incredible”, “Unreal”, “Brought tears to my eyes”.</p>



<p>These are all comments that repeatedly appeared in response to a viral Facebook video of a drone light show that <a href="https://verge.aero/">Verge Aero</a> flew to show gratitude for healthcare and essential workers in Philadelphia. The response was overwhelming, and these words demonstrate that when properly executed, drone light shows are a mesmerizing and powerful experience. Since then, we’ve received many questions about our technology, and we’d like to share our responses with a wider audience.</p>



<iframe src="https://player.vimeo.com/video/415494003" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""></iframe>



<h2><strong>What is a drone light show, exactly?</strong></h2>



<p>Drone light shows are performed by illuminated, synchronized, and choreographed groups of drones that arrange themselves into various aerial formations. Almost any image can be recreated in the sky by a computer program that turns graphics into flight commands and communicates them to the drones.</p>



<p>In recent years, drone shows have migrated from the university laboratory to being deployed at scale on prominent events around the world. We were originally inspired by this 2012 <a href="https://www.ted.com/talks/vijay_kumar_robots_that_fly_and_cooperate">TED video</a> featuring the University of Pennsylvania’s Dean of Engineering, Vijay Kumar, demonstrating drone fleets doing all kinds of stunning maneuvers. Later, pioneering work was done in Europe by the <a href="https://www.spaxels.at/">Spaxels Research Initiative</a>, <a href="https://collmot.com/">Collmot</a>, and <a href="http://www.veritystudios.com/">Verity</a>.<a href="https://www.intel.com/"> Intel</a> has done the most to popularize the concept, flying drone shows on big events such as the Super Bowl halftime show and the Winter Olympics.</p>



<figure><img loading="lazy" width="1000" height="562" src="https://verge.aero/wp-content/uploads/2020/05/1_xIWK_MTjYpP_xFAK9zP8gg.jpeg" alt="Philadelphia Drone Light Show LOVE" srcset="https://verge.aero/wp-content/uploads/2020/05/1_xIWK_MTjYpP_xFAK9zP8gg.jpeg 1000w, https://verge.aero/wp-content/uploads/2020/05/1_xIWK_MTjYpP_xFAK9zP8gg-300x169.jpeg 300w, https://verge.aero/wp-content/uploads/2020/05/1_xIWK_MTjYpP_xFAK9zP8gg-768x432.jpeg 768w" sizes="(max-width: 1000px) 100vw, 1000px"><figcaption>First Responders filming LOVE during Philadelphia Drone Light Show</figcaption></figure>



<h2><strong>How do drone shows work?</strong></h2>



<p>Let’s clear up one thing first: drone light shows are <strong>not </strong>powered by <a href="https://en.wikipedia.org/wiki/Skynet_%28Terminator%29">Skynet</a>, the artificial intelligence network depicted in <em>The Terminator</em>! Drones used in shows are not self-aware, can’t think for themselves, and make no real-time decisions. Instead, like obedient servants, they follow specific commands sent to them and can’t deviate!</p>



<p>The process for creating a show is quite straightforward. First, the design team creates a storyboard timeline showing the desired images and effects. These looks are then animated in a specialized piece of software that translates them into synchronized flight paths for each drone, and usually a soundtrack is created to accompany the show. Complete shows are sent to the drones via radio signal from a ground control station operated by a pilot. When the pilot is satisfied that everything is safe and ready to go, the show starts, and the drones take off to draw the storyboard in the sky.</p>



<figure><img loading="lazy" width="960" height="638" src="https://verge.aero/wp-content/uploads/2020/03/Mazatlan-2.jpg" alt="verge aero flying a drone light show in Mazatlan for Carnaval de Mazatlan" srcset="https://verge.aero/wp-content/uploads/2020/03/Mazatlan-2.jpg 960w, https://verge.aero/wp-content/uploads/2020/03/Mazatlan-2-300x199.jpg 300w, https://verge.aero/wp-content/uploads/2020/03/Mazatlan-2-768x510.jpg 768w" sizes="(max-width: 960px) 100vw, 960px"><figcaption>Verge Aero flies a show in Mazatlan for Carnaval 2020</figcaption></figure>



<p>Creating a system that can be flown safely and repeatedly requires a lot of clever engineering work. Verge Aero’s drones and software were designed by our engineers specifically for performing shows. Our custom drones are missing some things normally found on drones, like cameras, and include unique features, such as a blindingly bright LED light source.</p>



<p>Verge Aero’s design software lets users select graphics and special effects and place them in a timeline, similar to those found in video editing software. This software calculates the flight paths of each drone to guarantee they don’t collide in the air, and generates a full 3D rendering of the show to ensure it looks exactly as intended. Every drone is sent a unique program and the ground control station monitors each drone over a local, encrypted network for maximum safety.</p>



<p>The flight crew uses a detailed dashboard display on the ground station to prepare drones for flight and continuously monitor status. The drones themselves carry multiple radios operating simultaneously, away from busy WiFi frequencies, to ensure communications are maintained even in busy and noisy radio environments.</p>



<p>Shows are flown by certified pilots, experts in relevant aviation subject matter, including regulations and weather. Prior to every show, checklists are used to make sure everything is in order: drones are fully operational, batteries are charged, and the flight area is clear. Once these checks are complete, the pilot presses <strong>GO</strong> and the drones take off on their mission!</p>



<figure><img loading="lazy" width="2700" height="1800" src="https://verge.aero/wp-content/uploads/2020/03/FIG2019-2700x1800.jpg" alt="guitar drone light show at Festival Internacional del Globo" srcset="https://verge.aero/wp-content/uploads/2020/03/FIG2019-2700x1800.jpg 2700w, https://verge.aero/wp-content/uploads/2020/03/FIG2019-300x200.jpg 300w, https://verge.aero/wp-content/uploads/2020/03/FIG2019-768x512.jpg 768w, https://verge.aero/wp-content/uploads/2020/03/FIG2019-1536x1024.jpg 1536w, https://verge.aero/wp-content/uploads/2020/03/FIG2019-2048x1365.jpg 2048w" sizes="(max-width: 2700px) 100vw, 2700px"><figcaption>Guitar in drone light show at Festival Internacional del Globo</figcaption></figure>



<h2><strong>Will drones replace fireworks?</strong></h2>



<p>Fireworks shows are increasingly criticized for their negative environmental impact—they are noisy, polluting, and wasteful. Concerns are regularly raised around their impact on sensitive wildlife populations, as well as military veterans experiencing PTSD. What’s more, in many locations, fireworks displays have been banned altogether, due to the increased risk of wildfires.</p>



<p>These and other factors have led many people to consider replacing fireworks displays with alternatives—and drone light shows are perfectly positioned to fill the gap.</p>



<div><figure><img loading="lazy" src="https://verge.aero/wp-content/uploads/2020/05/1_tWamVl5u_EH8BWg5Nmm3ZQ.jpeg" alt="Philadelphia Drone Light Show Stethoscope" width="286" height="391" srcset="https://verge.aero/wp-content/uploads/2020/05/1_tWamVl5u_EH8BWg5Nmm3ZQ.jpeg 1000w, https://verge.aero/wp-content/uploads/2020/05/1_tWamVl5u_EH8BWg5Nmm3ZQ-220x300.jpeg 220w, https://verge.aero/wp-content/uploads/2020/05/1_tWamVl5u_EH8BWg5Nmm3ZQ-768x1049.jpeg 768w" sizes="(max-width: 286px) 100vw, 286px"><figcaption>Verge Aero flies a stethoscope for frontline workers at the Hospital of the University of Pennsylvania</figcaption></figure></div>



<p>In many cases, drone shows have been used successfully as a great complement to fireworks displays. However, as a new and exciting form of entertainment, drones have much more to offer. They are capable of a far greater range of effects than fireworks, and their capacity for sophisticated choreography gives them vastly more potential for storytelling in the sky. Drones can also be deployed in more constrained environments where fireworks would never be allowed.</p>



<p>When you think about it, fireworks actually have quite a limited repertoire. Usually, a handful of effects are repeated over and over again, just in varying combinations, sizes, colors and intensities. Why settle for this when you can have dynamic, repositionable 3D pixels capable of generating virtually unlimited imagery? Drones offer far more creative options than fireworks. In fact, it’s easy to envision that as drone shows become more commonplace, people will one day look back on fireworks displays as being quite mundane in comparison.</p>



<p>Regardless, the transition from fireworks to drones will happen gradually. Fireworks shows will be with us for some time to come, if for no other reason than that they are fairly inexpensive to stage. But as drone shows become more affordable, we can expect to see events using them more frequently.</p>



<h2><strong>Why haven’t I seen more shows?</strong></h2>



<p>Even Wal-Mart now sells drones, so why aren’t we seeing more light shows? The problem is that successful show execution requires different technologies that are only now maturing. Innovations usually take time to disseminate, and drone shows are no different. A number of factors have limited the uptake of drone shows before now:</p>



<ul><li>High cost</li><li>Need for regulatory approval</li><li>Expensive and limited insurance options</li><li>Labor intensive operations</li><li>Lack of efficient show design tools</li><li>Safety requirements</li></ul>



<p>The use of specialized drones with high-precision avionics drives high cost. Labor intensive operations also contribute, whether it’s wrangling rudimentary control software or preparing finicky drones for flight.</p>



<p>High operating expenses are possibly acceptable for the Super Bowl or the Olympics, but are not viable for most events. Nonetheless, things are changing, and Verge Aero’s innovations are helping to make drone light shows more accessible to a far wider range of budgets.&nbsp;</p>



<figure><img loading="lazy" width="1400" height="399" src="https://verge.aero/wp-content/uploads/2020/05/1_GwmlzLjRyU-BpqmpEpLzhA.jpeg" alt="Verge Aero X1 drones ready for flight" srcset="https://verge.aero/wp-content/uploads/2020/05/1_GwmlzLjRyU-BpqmpEpLzhA.jpeg 1400w, https://verge.aero/wp-content/uploads/2020/05/1_GwmlzLjRyU-BpqmpEpLzhA-300x86.jpeg 300w, https://verge.aero/wp-content/uploads/2020/05/1_GwmlzLjRyU-BpqmpEpLzhA-768x219.jpeg 768w" sizes="(max-width: 1400px) 100vw, 1400px"><figcaption>Verge Aero X1 drones ready for flight</figcaption></figure>



<h2><strong>How much do drone light shows cost?</strong></h2>



<p>Like many new technologies, drone light shows were extremely expensive at first, and they’re still more expensive than desired. But as with computers and flat panel TVs, prices are decreasing over time. Drone light shows will go mainstream as the technology matures. The tools developed by Verge Aero already make it possible to do more for less!</p>



<p>The main driver of cost is the number of drones being flown. It clearly costs a lot more to put on a 500-drone show than a 50-drone show. In addition to the drones themselves, labor, freight, and logistics all add expense.</p>



<p>A small show that’s easy to deploy costs as little as $20,000. But larger and more complicated shows can easily cost many times this number. Here’s a list of factors that impact price:</p>



<ul><li>Drone quantity</li><li>Show design complexity</li><li>Show coordination and rehearsal time</li><li>Airspace authorization and regulatory compliance</li><li>Shipping and logistics</li><li>Crew travel and accommodation</li></ul>



<h2><strong>What is Verge Aero doing differently?</strong></h2>



<p>To make drone shows more affordable, we introduced a few innovations to streamline the process and reduce the amount of work required.</p>



<figure><img loading="lazy" width="1200" height="550" src="https://verge.aero/wp-content/uploads/2020/03/swarm_550.jpg" alt="drone swarm for PNAU All of Us" srcset="https://verge.aero/wp-content/uploads/2020/03/swarm_550.jpg 1200w, https://verge.aero/wp-content/uploads/2020/03/swarm_550-300x138.jpg 300w, https://verge.aero/wp-content/uploads/2020/03/swarm_550-768x352.jpg 768w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>Drones swarming for PNAU’s music video All of Us</figcaption></figure>



<h3>Simplified Show Design</h3>



<p>Historically, drone light shows were manually created using a series of applications including animation software like<a href="https://www.blender.org/"> Blender</a>—which was an extremely tedious and limiting process. Even worse, the programmer had to check that vertices in Blender <strong>never </strong>overlapped during animation, because that would mean drones colliding in the air. The output then needed to be transferred to at least one more program to prepare for flight. All this is an extremely unsafe process because it’s prone to human error.&nbsp;</p>



<p>A far better way is to use a unified software application with an easy-to-use interface, like the Verge Aero Design Studio, which automatically handles the anti-collision calculations. Cues are created in the built-in animation software and easily manipulated. Once the design process is complete, the software removes the possibility of human error by deconflicting flight paths, identifying potential issues or mistakes in the show design, and simulating the performance via a rendering pipeline—all without any human intervention.</p>



<p>This software allows designers to generate content with a few key presses, instead of through the laborious processes of using animation tools or writing code. Effects such as complex flocking, as seen in this<a href="https://youtu.be/7DQ4covPEyE"> All of Us music video by PNAU</a>, are automatically created by an effects generator in the Verge Aero Design Studio. Effects created by one designer can easily be …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://verge.aero/everything-about-drone-light-shows/">https://verge.aero/everything-about-drone-light-shows/</a></em></p>]]>
            </description>
            <link>https://verge.aero/everything-about-drone-light-shows/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25022274</guid>
            <pubDate>Sun, 08 Nov 2020 02:34:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Remote Work: How the alternative has become the new norm]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25021471">thread link</a>) | @martin_crd
<br/>
November 7, 2020 | https://remoteworkers.net/blog/remote-work-how-alternative-has-become-new-norm | <a href="https://web.archive.org/web/*/https://remoteworkers.net/blog/remote-work-how-alternative-has-become-new-norm">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>The debate on the impact telework has on employee productivity &amp; morale is one that has slowly aged with the improvement of technology, but never so important to discuss than in the current global pandemic state the world is faced with.</p><p>For decades the working class has enjoyed, and even thrived in the conventional 9-to-5 office workday with very little contention. In fact, most of the well-known productivity techniques are centered around the premise that employees can only maximize team cohesion &amp; communication when working in a controlled work environment like the office.</p><p>But with the rise of the digital age and need to travel, the world has started to realize the value and need for remote work and set in place internal measures to promote alternative work methods that benefit the workforce.<br>
&nbsp;</p><h2>Reimagining the 9-to-5 workday could improve productivity</h2><p>The contemporary 9-to-5, eight-hour workday was imagined by American labor unions in the 1800s and normalized by Henry Ford in the 1920s. Most employees today simply accept this narrative because it’s what they have gotten so acquainted with. But despite the growing popularity in the demand for remote and flexible work, the need for a change in work culture has been met with much skepticism from managers. A recent study conducted by Remote Workplace Study explores the prime obstacles to implementing a flexible workplace policy:&nbsp;</p><ul><li>Company’s long-standing resistance to change &nbsp;</li><li>Privacy</li><li>Lack of understanding about the benefits of remote work</li><li>Fear of how it will impact the overall company culture</li><li>Technology requirements</li><li>Data security</li></ul><p>Understandably, managers have reservations about changing traditional work culture. Most concerns center around losing control of assessing team cohesiveness and potential reduction in employee productivity &amp; focus. But despite this, change is inevitable. In fact, most studies confirm an increase in productivity, working “smarter” and willingness to work over-time when employees are given the flexibility to choose when and how to work.&nbsp;</p><h2>Millennials are driving the demand for flexible and remote work&nbsp;</h2><p>As of today, millennials (those born between 1981 and 1996) account for almost 50% of the global workforce and are expected to grow to 75% by 2025. This means that the need for companies to adopt flexible and remote work policies is more compelling today than ever before in order to attract and retain a younger, skilled workforce.&nbsp;</p><h2>There are equal benefits to jumping on the remote-work bandwagon for both companies and employees:</h2><p>There are equal benefits to jumping on the remote-work bandwagon for both companies and employees:</p><h3>For Companies</h3><ul><li>Cost savings (office &amp; overhead costs)</li><li>Increase in productivity</li><li>Employee retention &amp; reduced turnover</li><li>Profitability (companies save avg. $11, 000 &nbsp;per part-time telecommuter)</li><li>Environmental benefits (less commuting to work reduces carbon footprint)</li><li>Improved inclusivity and diversity (easier reach and accessibility to skilled talent from different demographics, races, and people with disabilities from all over the world)</li></ul><h3>For Employees</h3><ul><li>Improved work-life balance</li><li>Flexible schedule</li><li>No commute (time and cost savings)</li><li>Location independence (work from home/travel while working)</li><li>More time to spend with family</li><li>Expense savings (transportation, gas, &amp; freedom &nbsp;to live in low-cost cities) &nbsp;</li></ul><h2>The world’s largest firms have endorsed remote working policies in the wake of Covid19</h2><p>It’s no secret that the novel Coronavirus pandemic has played an integral role in the sudden surge of remote work policies implemented across companies worldwide. Big corporations such as Twitter, Facebook, Google, Amazon, &amp; Microsoft have adopted such changes with Twitter and Square CEO Jack Dorsey announcing that his employees can work remotely indefinitely.&nbsp;</p><blockquote><p>“... the work-from-home revolution is shaping the future of the workplace.”</p></blockquote><p>Improved inclusivity and diversity (easier reach and accessibility to skilled talent from different demographics, races, and people with disabilities from all over the world)</p><p>In closing, albeit reservations from conservative and outdated work practices, the future of the work environment is remote and flexible work. In the age of the internet, connecting with skilled professionals has become easier and accessible. Thanks to online professional platforms like remoteworkers.net, companies have access to a wider pool of highly experienced talent and job-seekers can find their ideal remote jobs. Companies would be wise to evolve with the growth in remote work trends in order to become more competitive &amp; improve productivity.</p><h2>Looking for a remote job?</h2><p>Thanks to the internet, connecting skilled professionals with remote-friendly companies has never been easier. Whether you’re an experienced professional looking for your dream remote job or a remote-friendly company looking to hire the best talent, join <a href="https://remoteworkers.net/signup">Remote Workers</a>&nbsp;today!&nbsp;</p></div></div>]]>
            </description>
            <link>https://remoteworkers.net/blog/remote-work-how-alternative-has-become-new-norm</link>
            <guid isPermaLink="false">hacker-news-small-sites-25021471</guid>
            <pubDate>Sun, 08 Nov 2020 00:36:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Ideal Foundation for a General Purpose Serverless Platform]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25021393">thread link</a>) | @thedataexchange
<br/>
November 7, 2020 | https://www.anyscale.com/blog/the-ideal-foundation-for-a-general-purpose-serverless-platform | <a href="https://web.archive.org/web/*/https://www.anyscale.com/blog/the-ideal-foundation-for-a-general-purpose-serverless-platform">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><h4>Why Ray is poised to play a central role in future serverless offerings</h4><p>The history of computing infrastructure is one of steady improvements over extended periods. Cloud computing brought several capabilities that we now take for granted including the elimination of upfront investments in hardware, the ability to pay for compute resources on an as-needed basis, and elasticity. The move to the cloud has <a href="https://www.networkworld.com/article/3512885/enterprises-now-spend-more-on-cloud-infrastructure-services-than-on-premises-data-center-gear.html"><u>accelerated in recent years</u></a> and many companies now use a mix of on-premise infrastructure alongside <a href="https://gradientflow.com/one-simple-chart-most-companies-use-multiple-cloud-providers/"><u>multiple cloud providers</u></a>.</p><p>In 2015, AWS introduced Lambda, a service that offered <i>cloud functions</i> and introduced the concept of <i>serverless computing</i>. Serverless and cloud functions differ from traditional cloud computing in two critical ways: (1) serverless abstracts away infrastructure (scaling, provisioning servers) freeing developers to focus on writing programs, (2) serverless provides finer-grained increments for billing (sub-second, compared to traditional cloud computing which uses minutes).</p><p>Since the introduction of AWS Lambda, interest in serverless and cloud functions <a href="https://trends.google.com/trends/explore/TIMESERIES/1597100400?hl=en-US&amp;tz=420&amp;date=2015-01-01+2020-08-10&amp;geo=US&amp;q=serverless&amp;sni=3"><u>has grown</u></a> and all major cloud providers now have similar offerings. <a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-3.pdf"><u>Experts predict</u></a> that serverless will continue to grow in importance and more applications will use serverless computing platforms in the years ahead. But we are still in the early stages of serverless computing. A widely read&nbsp; 2019 paper (<a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-3.pdf"><u>“A Berkeley View on Serverless Computing”</u></a>) described&nbsp; “challenges and research opportunities that need to be addressed for serverless computing to fulfill its promise”.</p><p>In this post we examine some limitations of current cloud functions (also referred to as <i>FaaS</i> or <i>serverless</i>). We note that the distributed computing framework <a href="https://github.com/ray-project/ray"><u>Ray</u></a> addresses many of these challenges and argue that Ray is the right foundation for a <a href="https://www.youtube.com/watch?v=vzMXTpdJSuk"><u>general purpose serverless framework</u></a>.</p><h2>Serverless: Special Purpose vs General Purpose</h2><p>There have been multiple efforts to expand the scope of serverless. Projects like <a href="http://ex.camera/nsdi17/"><u>ExCamera</u></a>, <a href="http://pywren.io/"><u>PyWren</u></a>, <a href="https://www.serverless.com/blog/using-tensorflow-serverless-framework-deep-learning-image-recognition"><u>TF-on-serverless</u></a>, and <a href="https://www.qubole.com/blog/spark-on-aws-lambda/"><u>Spark-on-Lambda</u></a> try to run more complex workloads on FaaS platforms like Lambda. Several industry platforms provide serverless-like capabilities and user experiences, including <a href="https://aws.amazon.com/rds/aurora/serverless/"><u>Amazon Aurora</u></a>, <a href="https://databricks.com/blog/2017/06/07/databricks-serverless-next-generation-resource-management-for-apache-spark.html"><u>Databricks</u></a>, <a href="https://cloud.google.com/serverless/whitepaper"><u>BigQuery</u></a>, and others. However, these offerings generally target <i>specific workloads</i> and do not aim to run arbitrary applications. We define the characteristics of such a framework (Ray) in the following table, along with key tradeoffs:</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/5bhF7EF9VdFyQtqqqgvz45/c446dcf421ea3c288a6dd087ddba469b/blog_grid.png" alt="blog table"></p></div></div><h2>Why Ray for General Purpose Serverless?</h2><p><b>Ray hides servers</b></p><p>Like existing FaaS platforms, Ray abstracts away servers from the applications. With version 1.0, users do not need to specify the cluster size or the type of instances when launching an application. Instead, they only need to provide Ray with the set of available resources (e.g., a list of instance types in AWS or a list of nodes and their capabilities in an on-prem cluster) and Ray will automatically pick the instance types and dynamically scale up and down to match the application demands.</p><p><b>Ray supports stateful applications</b></p><p>Serverless platforms scale and provision storage and compute separately, thus computations on serverless platforms are stateless. Cloud functions do not store previous transactions or knowledge. The typical flow is to run a computation (a function call), write results to a storage service, and if needed, another function can take that output and use it.&nbsp;&nbsp;</p><p>The <a href="https://en.wikipedia.org/wiki/Actor_model#Fundamental_concepts"><u>Actor model</u></a> is a powerful programming paradigm that focuses on the semantics of message passing, and works seamlessly when local or remote. Ray supports <a href="https://en.wikipedia.org/wiki/Actor_model"><u>actors</u></a>, which are stateful workers (or services). Having a serverless platform that can support stateful computations dramatically increases the number of possible applications. Stateful operators allow Ray to efficiently support streaming computations, <a href="https://anyscale.com/blog/heres-what-you-need-to-look-for-in-a-model-server-to-build-ml-powered-services"><u>machine learning model serving</u></a>, and web applications. In fact there are already companies who use Ray in applications that combine both streaming and machine learning. For example, Ant Group “built a multi-paradigm fusion engine on top of Ray that combines streaming, graph processing, and machine learning in a single system to perform real-time fraud detection and online promotion.”</p><p><b>Ray supports direct communication between tasks</b></p><p>Almost every distributed application, such as streaming or data processing, exchanges data between tasks. Unfortunately, existing serverless platforms do not allow direct communication between functions. The only way two functions can communicate with each other is via a cloud storage system like S3. Unfortunately, this is slow and expensive.&nbsp;</p><p>In contrast, Ray enables arbitrary tasks and actors to communicate with each other. This enables applications to efficiently exchange data and implement arbitrary communication patterns.</p><p><b>Ray lets developers access hardware accelerators</b></p><p>Developers know what type of hardware resources best serve their applications so giving them the ability to control resources is an important feature of any compute platform. FaaS services currently let developers specify execution time limits and memory sizes. But in some applications - notably machine learning model training - developers need to access specific accelerators (GPUs, TPUs, etc.). At this time FaaS providers do not offer this level of control over resources.&nbsp;</p><p>A serverless platform built on top of Ray would have no limitations in terms of specifying resources. Developers who use Ray can already describe the hardware resources they need (number of CPUs, GPUs, TPUs, and other hardware accelerators). Future versions of Ray will even allow developers to specify the precise type of chip they prefer (e.g., “two V100 GPUs”).</p><p><b>Ray provides an open source API</b></p><p>There is no “standard API” for writing serverless applications. We believe that Ray is a strong candidate for such an open, serverless API. Ray allows developers to write general programs, not just functions. It combines support for both stateless and stateful applications and access to widely used programming languages (Python and Java, with more to follow). In fact, Ray is usually described as a distributed computing platform that can be used to scale applications with minimal effort.</p><p><br><b>Ray supports fine-grained coordination and control, which can lead to better performance</b></p><p>There are many applications where control over data locality and scheduling are critical. This includes the broad range of applications that need to distribute and share data across compute nodes. Current serverless offerings are a poor fit for this class of applications. For example, a developer who uses serverless to perform a gradient computation in machine learning will not have control over where data and cloud functions are located. Another example comes from reinforcement learning, where developers will want to use the same compute node for training policies and performing rollouts.</p><p>Ray overcomes these limitations by providing data locality and scheduling control. A developer who uses Ray can specify which actors should run on the same machine.</p><p><br><b>Ray has no runtime limits</b></p><p>Current offerings (<a href="https://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/"><u>Lambda</u></a>, <a href="https://cloud.google.com/functions/docs/concepts/exec#:~:text=Function%20execution%20time%20is%20limited,period%20up%20to%209%20minutes."><u>Cloud Functions</u></a>, <a href="https://build5nines.com/azure-functions-extend-execution-timeout-past-5-minutes/#:~:text=One%20of%20the%20biggest%20benefits,originally%20set%20to%205%20minutes."><u>Azure Functions</u></a>) have execution times that are capped at around 15 minutes or less. This limits the types of applications that a FaaS platform can support. In contrast, since Ray runs on top of existing clouds or clusters, there are no time limits.</p><h2>But what about costs?</h2><p>One of the most popular aspects of serverless is that users are billed based on usage. Serverless platforms use accounting units that are measured in milliseconds, compared to traditional cloud computing which uses minutes.</p><p>Since cloud providers charge at the level of an instance, as a software layer, (open source) Ray by itself cannot instantiate and release compute resources quickly enough to deliver fine-grained accounting units. For example, if you have an actor running on a server with 32 cores (and you only have one actor running on it), your cloud provider will likely charge you for 32 cores. Ray does not address this discrepancy between utilization and cost. Getting to fine-grained accounting units would require that you build <a href="https://www.anyscale.com/product"><u>a serverless platform on top of Ray</u></a>.</p><p>With that said, Ray can automatically allocate new instances and shutdown existing instances in minutes. Thus, Ray can already provide serverless functionality for coarse grained jobs that run for say 30 minutes. Examples that might fall under the realm of coarse grained jobs include&nbsp; streaming, model training, and model serving jobs.&nbsp;</p><p>Finally, Ray also has the potential to provide substantial cost savings for coarse-grain workloads. The cost of using cloud functions can be 4x-5x higher than a cloud computing instance operating at 100% utilization. Indeed, at the time of writing, a 3GB RAM AWS Lambda <a href="https://aws.amazon.com/lambda/pricing/"><u>costs $0.0000048958 per 100ms</u></a> or $0.1762488/hour. In comparison, a t3.medium instance with 4GB RAM <a href="https://aws.amazon.com/ec2/pricing/on-demand/"><u>costs just $0.0416/hour</u></a> which is 4.2x cheaper, and when considering per GB RAM pricing, it is 5.65x cheaper. This is an underestimate because it does not include the per-request costs.</p><h2><b>Summary</b></h2><p>As far back as 2016, <a href="https://youtu.be/wYCLbLrEoqs?t=95"><u>experts were already noting</u></a> how serverless tools enable teams to quickly build extremely functional and scalable applications. Since then a growing number of developers are turning to serverless technologies to build applications. But for serverless to live up to its promise, current offerings (cloud functions) need to address the limitations listed in this post.</p><p>Companies are still in the early stages of exploring serverless technologies. We believe that Ray is the ideal foundation for a general purpose serverless framework. With the rise of AI and other data intensive applications, Ray is poised to play a central role in future serverless offerings.</p></div></div></div>]]>
            </description>
            <link>https://www.anyscale.com/blog/the-ideal-foundation-for-a-general-purpose-serverless-platform</link>
            <guid isPermaLink="false">hacker-news-small-sites-25021393</guid>
            <pubDate>Sun, 08 Nov 2020 00:28:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why do things go right? – Safety Differently]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25021366">thread link</a>) | @absolute100
<br/>
November 7, 2020 | https://safetydifferently.com/why-do-things-go-right/ | <a href="https://web.archive.org/web/*/https://safetydifferently.com/why-do-things-go-right/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
<div id="attachment_3805"><p><img aria-describedby="caption-attachment-3805" src="http://www.safetydifferently.com/wp-content/uploads/2018/09/josue-isai-ramos-figueroa-741921-unsplash-1-300x218.jpg" alt="" width="300" height="218"></p><p id="caption-attachment-3805">Photo by Josue Isai Ramos Figueroa on Unsplash</p></div>
<p>In his 2014 <em>Safety I and Safety II: The past and future of safety management</em>, Erik Hollnagel makes the argument that we should not (just) try to stop things from going wrong. Instead, we need to understand why most things go right, and then ensure that as much as possible indeed goes right. It seems so obvious. Yet it is light years away from how most organizations ‘do’ safety today, with their focus on low numbers of lagging indicators, incidents and injuries.</p>
<p>That said, many organizations have now begun to recognize the severe organizational deficiencies, cultural problems and ethical headaches that lag indicators create for them. Most will be familiar with the following sorts of things:</p>
<ul>
<li>Numbers games and the hiding or renaming of injuries and incidents;</li>
<li>Counterproductive and credibility-straining sloganeering (‘Zero Harm!’);</li>
<li>Short-termism (driven by quarterly figures);</li>
<li>Creative case management and a lack of compassion for those who do get hurt in the course of work (think of the cynical use of ‘suitable duties’ to keep someone off the injury stats or lost time books);</li>
<li>The misdirection of accountability through sanctioning, dismissal or exclusion of those who have been hurt in the past (just cancel the contractor’s access card, for instance);</li>
<li>The statistical insignificance of any change in typical lagging indicators (‘We went from 3 lost time injuries last year to 1 this year! And we worked a total of 5 million person-hours!’ …<em>Uhmm, right</em>);</li>
<li>Organizational learning disabilities and cultures of risk secrecy;</li>
<li>Worker cynicism, mistrust and disenchantment;</li>
<li>Cases of outright management fraud that have got managers dismissed or even in jail.</li>
</ul>
<p>Erik’s insight is the hinge on which the transition to Safety Differently turns. Let’s not stare ourselves silly at the lowest possible injury and incident numbers, with the ridiculous and counterproductive Siren Song of ‘Zero Harm’ (See Sheratt &amp; Dainty, 2017, for how ‘Zero’ is linked to more fatalities and serious injuries)as the ultimate paradise we want to reach. Deming said it long before anybody in the Resilience community said it: get rid of targets, slogans and exhortations. They get in the way of allowing your people to produce quality work (Deming, 1982). So instead, let’s learn why things go right and find out what we can do to make it even more so. Safety is not about the <em>absence&nbsp;</em>of negatives; it is about the <em>presence&nbsp;</em>of capacities. The field of Resilience Engineering, formally founded at a meeting in Söderköping in Sweden over a decade ago (Dekker, 2006)was of course driven by this logic. I witnessed Erik and others forcefully making this very point, from many different angles, for a week in a room full of peers and stakeholders.</p>
<p>One way to illustrate this point, as Erik indeed does (and others now do as well) is by way of a Gaussian, or normal curve. The curve shows that the number of the things that go wrong (the left side of the curve) is tiny. On the right side of the curve are the heroic, unexpected surprises (a Hudson River landing, for instance) that fall far outside what people would normally experience or have to deal with. In between, the huge bulbous middle of the figure, sits the quotidian or daily creation of success. This is where good outcomes are made, despite the organizational, operational and financial obstacles, despite the rules, the bureaucracy, the common frustrations and obstacles. This is where work can be hard, but is still successful. The way to make even further progress on safety, suggests this figure, is not by trying to make the red part of things that go wrong even smaller, but by understanding what accounts for the big middle part where things go right. And then enhancing the capacities that make it so. That way, we don’t make the red part smaller by making the red part smaller. We make the red part smaller by making the white part bigger. Research by René Amalberti tells us that it is indeed likely that this is <em>the&nbsp;</em>way to make progress on safety in already safe systems (Amalberti, 2001, 2006, 2013). In those systems, we have milked the recipes for how to prevent things from going wrong to the maximum already. We have many layers of protection in place. We have rules to the point of overregulation. We monitor, record, investigate and standardize the designs people work with. Ever more things targeted at the red part are not going to make it any smaller. The complexity of the system won’t let us. And in fact, the more we do to make that part smaller with what we already know (more rules, more limits on people, more technology and barriers) may in fact contribute to novel pathways to breakdown, accidents and failures (Dekker, 2011).</p>
<div id="attachment_3799"><p><img aria-describedby="caption-attachment-3799" src="http://www.safetydifferently.com/wp-content/uploads/2018/09/Picture1.png" alt="" width="939" height="633" srcset="https://safetydifferently.com/wp-content/uploads/2018/09/Picture1.png 939w, https://safetydifferently.com/wp-content/uploads/2018/09/Picture1-768x518.png 768w" sizes="(max-width: 939px) 100vw, 939px"></p><p id="caption-attachment-3799">The way to make the red part (unwanted outcomes) on the left smaller is not by making it impossible for things to go wrong (as we’ve done almost everything in that regard already). We make the red part smaller by making the white part bigger: focusing on why things go right and enhancing the capacities that make it so. Figure by Kelvin Genn.</p></div>
<p><em>Shifting the paradigm</em></p>
<p>The question that most organizations yearn to have answered, though, is this: what is going to take the place of their long-held and easily communicated total recordable injury frequency rate? As Thomas Kuhn (1970)pointed out, people are unwilling to relinquish a paradigm—despite all its faults—if there is no plausible, viable alternative to take its place.</p>
<p>A few years back, I was working, together with some students, with a large health authority which employed some 25,000 people. The patient safety statistics were dire, if typical: one in thirteen of the patients who walked (or were carried) through the doors to receive care were hurt in the process of receiving that care. 1 in 13, or 7%. These numbers weren’t unique, of course. They were also problematic. Because what exactly is ‘nosocomial harm,’ harm that originates in a hospital? What is ‘medical error’ and when is it putatively responsible for the adverse event that happened to the patient? Indeed, when exactly does the patient become that ‘one’ out of thirteen? These are important (and huge) epistemological and ontological questions. I have vociferously commented on them before (and I didn’t exactly make friends in the field, for example by claiming how safe gun owners are in comparison to doctors) (Dekker, 2007).</p>
<p>But it’s not the point here. When we asked the health authority what they typically found in the one case that went wrong—the one that turned into an ‘adverse event,’ the one that inflicted harm on the patient—here is what they came up with. After all, they had plenty of data to go on: one out of thirteen in a large healthcare system can add up to a sizable number of patients per day. So in the patterns that all this data yielded, they consistently found:</p>
<ul>
<li>Workarounds</li>
<li>Shortcuts</li>
<li>Violations</li>
<li>Guidelines not followed</li>
<li>Errors and miscalculations</li>
<li>Unfindable people or medical instruments</li>
<li>Unreliable measurements</li>
<li>User-unfriendly technologies</li>
<li>Organizational frustrations</li>
<li>Supervisory shortcomings</li>
</ul>
<p>It seemed a pretty intuitive and straightforward list. It was also a list that firmly belonged to a particular era in our evolving understanding of safety: that of the person as the weakest link, of the ‘human factor’ as a set of mental and moral deficiencies that only great systems and stringent supervision can meaningfully guard against. In that sort of logic, we’ve got great systems and solid procedures—it’s just those people who are unreliable or non-compliant:</p>
<ul>
<li>People are the problem to control</li>
<li>We need to find out what people did wrong</li>
<li>We write or enforce more rules</li>
<li>We tell everyone to try harder</li>
<li>We get rid of bad apples</li>
</ul>
<p>Many organizational strategies, to the extent that you can call them that, were indeed organized around these very premises. Poster campaigns that reminded people of particular risks they needed to be aware of, for instance. Or strict surveillance and compliance monitoring with respect to certain ‘zero-tolerance’ or ‘red-rule’ activities (e.g. hand hygiene, drug administration protocols). Or a ‘just culture’ process that got those lower on the medical competence hierarchy more frequently ‘just-cultured’ (code for suspended, demoted, dismissed, fired) than those with more power in the system. Or some miserably measly attention to supervisor leadership training.</p>
<p>We were of course interested to know the extent to which these investments in reducing the ‘one in thirteen’ had paid off. They hadn’t. The health authority was still stuck at one in thirteen.</p>
<p><em>What would Erik do?</em></p>
<p>This is when we asked the Erik Hollnagel question. We asked: “What about the other twelve? Do you even know why they go right? Have you ever asked yourself that question?” The answer we got was “no.” All the resources that the health authority had were directed toward investigating and understanding the ones that went wrong. There was organizational, reputational and political pressure to do so, for sure. And the resources to investigate the instances of harm were too meager to begin with. So this is all they could do. We then offered to do it for them. And so, in an acutely unscientific but highly opportunistic way, we spent time in the hospitals of the authority to find out what happened when things went well, when there was no evidence of adverse events or patient harm.</p>
<p>When we got back together after a period of weeks, we compared notes. At first we couldn’t believe it, thinking that what we had found was just a fluke, an irregular and rare irritant in data that should otherwise have been telling us something quite different. But it turned out that everybody had found that in the twelve cases that go right, that don’t result in an adverse event or patient harm, there were:</p>
<ul>
<li>Workarounds</li>
<li>Shortcuts</li>
<li>Violations</li>
<li>Guidelines not followed</li>
<li>Errors and miscalculations</li>
<li>Unfindable people or medical instruments</li>
<li>Unreliable measurements</li>
<li>User-unfriendly technologies</li>
<li>Organizational frustrations</li>
<li>Supervisory …</li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://safetydifferently.com/why-do-things-go-right/">https://safetydifferently.com/why-do-things-go-right/</a></em></p>]]>
            </description>
            <link>https://safetydifferently.com/why-do-things-go-right/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25021366</guid>
            <pubDate>Sun, 08 Nov 2020 00:25:41 GMT</pubDate>
        </item>
    </channel>
</rss>
