<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 5]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 5. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Sat, 14 Nov 2020 08:22:13 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-5.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Sat, 14 Nov 2020 08:22:13 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[You don't need a blockchain solution for your next project]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 5 (<a href="https://news.ycombinator.com/item?id=25067702">thread link</a>) | @shrmv
<br/>
November 12, 2020 | https://exyte.com/blog/why-you-dont-need-a-blockchain-solution-for-your-next-project | <a href="https://web.archive.org/web/*/https://exyte.com/blog/why-you-dont-need-a-blockchain-solution-for-your-next-project">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                    <p>Blockchain is one of the hottest technology fields of the last decade, together with machine learning and big data. According to the <a rel="nofollow" href="https://www2.deloitte.com/content/dam/insights/us/articles/6608_2020-global-blockchain-survey/DI_CIR%202020%20global%20blockchain%20survey.pdf">Deloitte 2020 Global Blockchain Survey</a>, businesses worldwide find blockchain an integral part of organizational innovation. </p>
<p>Properly implemented, a blockchain solution can make parts of your business transparent or enable different actors to cooperate quickly and trustlessly. </p>
<p>But today, we wonâ€™t examine blockchainâ€™s strengths. Instead, we will look at why itâ€™s not the perfect choice for all your software projects.  </p>
<h2>Blockchain is more expensive</h2>
<p>From our experience, infrastructure and maintenance costs for a blockchain solution are typically 10-15 times higher compared to an ordinary server with a database running on AWS or a custom AWS/GCP/Azure solution.</p>
<p>Even when using economically efficient Azure solutions, it can cost up to ten times more when compared to a centralized DB with similar functionality running on AWS.</p>
<p>Therefore, itâ€™s necessary to seriously weigh the costs of running your solution vs. the need for transparency and distribution that blockchain offers. </p>
<h2>Blockchain scales worse</h2>
<p>Performance and scalability are the two major bottlenecks for any blockchain project. </p>
<p>From our experience working for large enterprise companies such as Bayer AG, Delta, PwC, and others, blockchain solutions that work fine for hundreds and thousands of users degrade in performance quite quickly when they need to serve more than ten thousand users.</p>
<p>Scaling blockchain, on the other hand, usually requires a complete rework of the core. Sometimes, there might even be no viable solution. So, despite some ambitious claims by blockchain companies, scaling is still an immature topic in the blockchain world.</p>
<h2>Blockchain is about transparency, not privacy</h2>
<p>Privacy is an issue. It is challenging to maintain privacy on the blockchain. In contrast to regular solutions, existing blockchain solutions are often all about transparency rather than privacy.</p>
<p>Solutions that provide the required level of privacy typically have to compromise on performance through zero-knowledge proofs or other kinds of encryption, which is costly.</p>
<p>Another problem with privacy is that most of the time, blockchain is permissionless or has very primitive permissions levels. Therefore, solutions for permissions are built on top of it as an additional middle layer, which, of course, adds performance overhead, degrades scalability, adds implementation and execution costs, etc.
Conclusion</p>
<p>There are three significant challenges that blockchain projects face: performance, scalability, and privacy. While there are multiple benefits for using blockchain, such as transparency and the ability to operate trustlessly, they need to be weighed against the downsides. </p>
<p>We want to share our knowledge with you about potential problems not to talk you away from blockchain, but to make sure the choice is clear and the right one for your project. If you still arenâ€™t sure and would like to have a 30-minute consultation with an expert in the field, youâ€™re welcome to <a href="https://exyte.com/contacts">contact us</a>. </p>
                </div></div>]]>
            </description>
            <link>https://exyte.com/blog/why-you-dont-need-a-blockchain-solution-for-your-next-project</link>
            <guid isPermaLink="false">hacker-news-small-sites-25067702</guid>
            <pubDate>Thu, 12 Nov 2020 08:25:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pyxell – a programming language that combines Python's elegance with C++'s speed]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25066058">thread link</a>) | @masijo
<br/>
November 11, 2020 | https://www.pyxell.org/docs/manual.html | <a href="https://web.archive.org/web/*/https://www.pyxell.org/docs/manual.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> <p>Pyxell [<em>pixel</em>] is a multi-paradigm, statically typed programming language, compiled to machine code via C++.</p> <p>This manual should let you quickly learn all the details to start programming in Pyxell.
It is assumed that you already know some programming language (preferably Python), since basic programming concepts are not explained here.</p> <p>You are encouraged to run the code snippets and experiment with them for yourself.
To run Pyxell code, go to the <a href="https://www.pyxell.org/docs/playground.html">Playground</a>,
clone the repository and follow the instructions on <a href="https://github.com/adamsol/Pyxell#requirements" target="_blank" rel="noopener noreferrer">Github<span> <span>(opens new window)</span></span></a>,
or download Windows binaries from the <a href="https://github.com/adamsol/Pyxell/releases" target="_blank" rel="noopener noreferrer">Releases<span> <span>(opens new window)</span></span></a>.</p> <h2 id="hello-world"><a href="#hello-world">#</a> Hello, world!</h2> <p>If you can run the following code and see the message on the screen, you are ready to start.</p> <h2 id="variables-and-types"><a href="#variables-and-types">#</a> Variables and types</h2> <h3 id="variable-declaration"><a href="#variable-declaration">#</a> Variable declaration</h3> <p>Pyxell is statically typed. Variables have types assigned during compilation.
In most cases, type of an expression is automatically inferred and the value can be directly assigned to a variable.</p> <p>In other cases, when the type cannot be inferred or you want to declare a variable without initializing it, you can set the type explicitly.
When not directly initialized, variable is automatically initialized with the default value for a given type.
You can find the list of all available types and their default values in the <a href="https://www.pyxell.org/docs/specification.html#types">Specification</a>.</p> <p>Variable name must start with a letter, but may also contain digits, underscores, and apostrophes.
Once a variable has been created, its type cannot be changed.</p> <h3 id="type-coercion"><a href="#type-coercion">#</a> Type coercion</h3> <p>Values of some types can be automatically converted to more general types: <code>Int -&gt; Rat -&gt; Float</code> or <code>Char -&gt; String</code>.</p> <p>The coercion doesn't work in the other direction.</p> <h2 id="arithmetic-and-logic"><a href="#arithmetic-and-logic">#</a> Arithmetic and logic</h2> <h3 id="numbers"><a href="#numbers">#</a> Numbers</h3> <p>Standard integers in Pyxell have 64 bits of precision and range from <code>-2^63</code> to <code>2^63-1</code>.
Binary, octal, and hexadecimal literals are supported.</p> <p>Rational numbers have unlimited precision.
They can be written either as integers with <code>r</code> suffix, or non-integers in decimal form.
They are also created as the result of division or exponentiation
(to obtain an integer from a division or exponentiation, use lossy <code>//</code> and <code>^^</code> operators).</p> <p>You can retrieve the numerator and denominator from a rational number.</p> <p>Floating-point numbers have 64 bits of precision and follow the IEEE 754 standard.
They can be written with <code>f</code> suffix or in scientific notation.</p> <p>Underscores can be additionally used in all numeric literals, to enhance readability of long numbers.</p> <h3 id="boolean-values"><a href="#boolean-values">#</a> Boolean values</h3> <p>There are two boolean values: <code>true</code> and <code>false</code>.
Logical negation and short-circuiting conjunction and disjunction operators are available.</p> <p>Boolean values are most often obtained in the result of comparisons.
When comparison operators are chained, they behave as if connected with <code>and</code> operator.</p> <h2 id="characters-and-strings"><a href="#characters-and-strings">#</a> Characters and strings</h2> <h3 id="characters"><a href="#characters">#</a> Characters</h3> <p>Characters are written in single quotes.</p> <p>You can get character's ASCII code, as well as obtain character corresponding to a given integer.</p> <p>You can also perform some arithmetic operations on characters.</p> <h3 id="strings"><a href="#strings">#</a> Strings</h3> <p>Strings are immutable sequences of characters. They are written in double quotes.</p> <p>You can access string's length, as well as its individual characters. Negative indexing and slicing is also supported, like in Python.</p> <p>Strings can be concatenated with <code>+</code> operator and repeated with <code>*</code> operator.</p> <p>You can construct formatted strings using interpolation syntax.</p> <h2 id="control-flow"><a href="#control-flow">#</a> Control flow</h2> <p>Pyxell uses indentation-based syntax, similar to Python's. Only spaces are allowed (tab character will cause a syntax error).
Rather than <code>:</code> character, Pyxell uses <code>do</code> keyword for indicating beginning of a block, and <code>def</code> keyword for function and class definitions.
The scope of a variable declared in a block is limited to that block.</p> <h3 id="if-statement"><a href="#if-statement">#</a> <code>if</code> statement</h3> <p>The first branch whose condition evaluates to <code>true</code> is executed.</p> <h3 id="while-loop"><a href="#while-loop">#</a> <code>while</code> loop</h3> <p>The loop runs while the condition is satisfied.</p> <h3 id="until-loop"><a href="#until-loop">#</a> <code>until</code> loop</h3> <p>This loop is similar to <code>while</code> loop, but it is always executed at least once and runs until the condition is satisfied.</p> <h3 id="for-loop"><a href="#for-loop">#</a> <code>for</code> loop</h3> <p>It can be used to loop over ranges (of numbers or characters) and other iterables.
Range can be inclusive (<code>a..b</code>), exclusive (<code>a...b</code>), or infinite (<code>a...</code>).</p> <p>You can optionally provide a step value, which can be either positive or negative (it is 1 by default).</p> <p>It is possible to loop over multiple iterables at once and provide custom step values to any of them.</p> <h3 id="continue-and-break"><a href="#continue-and-break">#</a> <code>continue</code> and <code>break</code></h3> <p>Inside loops you can use statements to exit the current iteration or the whole loop.</p> <h2 id="containers"><a href="#containers">#</a> Containers</h2> <h3 id="arrays"><a href="#arrays">#</a> Arrays</h3> <p>Arrays are similar to strings, but they are mutable and can have elements of any type.</p> <p>Containers have reference semantics, so they are not implicitly copied when variables are passed.
Mutation of one instance is reflected in all other instances of the same container.</p> <p>When an empty array is used, its type must be explicitly given.</p> <p>Arrays can be concatenated, repeated, and compared using standard operators.</p> <p>You can use array comprehension, as well as range literals and spread operator with optional step.</p> <p>For type safety, containers in Pyxell are invariant, which means they cannot be implicitly converted to another type,
even if types of the elements match (see <a href="https://stackoverflow.com/q/2745265" target="_blank" rel="noopener noreferrer">here<span> <span>(opens new window)</span></span></a> for a broader explanation).
However, container literals can be automatically converted.</p> <h3 id="sets"><a href="#sets">#</a> Sets</h3> <p>Sets contain no duplicates and do not preserve order of elements.</p> <p>Empty set can be created like an empty array.</p> <p>To check if an element is in the set, use <code>in</code> operator.</p> <p>There exist operators for set union, difference, and intersection.</p> <p>Like with arrays, you can use comprehensions, ranges, and spread syntax to create sets.</p> <p>Containers are not hashable, so they cannot be stored in sets.</p> <h3 id="dictionaries"><a href="#dictionaries">#</a> Dictionaries</h3> <p>Dictionaries are hash maps. Like sets, they do not preserve order of elements.
They work similarly to <code>defaultdict</code> in Python: if a key is not present, the default value for a given type is automatically created in the dictionary.</p> <p>Empty dictionary literal has an additional colon.</p> <p>Use <code>in</code> operator for checking if the dictionary contains a given key.</p> <p>Dictionaries can be merged with <code>+</code> operator. In the case of repeated keys, the second value wins.</p> <p>Dictionary comprehension works similarly to array and set comprehension.</p> <p>When iterated over, dictionaries produce pairs of key and value.</p> <p>Spread operator for dictionaries consists of an extra colon.</p> <h2 id="nullable-types"><a href="#nullable-types">#</a> Nullable types</h2> <p>To accept <code>null</code> value, variable's type must be explicitly marked as nullable.</p> <p>You can either directly check if a value is <code>null</code>, or use special coalescing and conditional operators.</p> <p>There is also an operator to directly retrieve the value, for cases when you are sure it is not null.</p> <h2 id="tuples"><a href="#tuples">#</a> Tuples</h2> <p>Two or more values separated with a comma (outside of a container, function call, and print statement) form a tuple.</p> <p>Values can be retrieved using alphabetical properties or tuple destructuring (unneeded part can be discarded with an underscore).</p> <p>Tuples are mutable, but they have value semantics, so they are hashable and can be passed around as if they were immutable.</p> <h2 id="functions"><a href="#functions">#</a> Functions</h2> <h3 id="function-definition-and-call"><a href="#function-definition-and-call">#</a> Function definition and call</h3> <p>Basic definition of a function consists of its name, list of arguments, return type, and body.</p> <p>When a function does not return anything, the return type may be written as <code>Void</code> or may be omitted completely.</p> <p>You can provide default values for optional arguments.
The expressions will be evaluated every time the function is called (if they are needed), so mutable container literals can be safely used.</p> <p>Arguments can be also passed in any order using their names.</p> <p>Variadic functions are supported too. This is just a syntactic sugar for passing an array.
Ranges and spread syntax can be used when such a function is called.</p> <p>Functions can be stored in variables, passed to other functions as arguments, etc.
However, when a function is converted to a variable, all information about its arguments except for their types is lost.</p> <h3 id="generic-functions"><a href="#generic-functions">#</a> Generic functions</h3> <p>Generic functions are standard functions with additional type variables, which can be used just like normal types.
They are compiled independently for each combination of types they are called with.</p> <p>Function declaration may contain default values for generic arguments, and the body may contain any code dependent on the real types.
Errors will be reported when the function cannot be compiled with given types.</p> <p>When a type name is used more than once, the compiler will try to unify types of the arguments, following the coercion rules.</p> <h3 id="lambda-functions"><a href="#lambda-functions">#</a> Lambda functions</h3> <p>Lambda is a simpler version of generic function, where all arguments, as well as the return type, are generic.</p> <p>You can use placeholder syntax to write even more concise functions. Each underscore corresponds to one argument.</p> <p>Placeholder resolving doesn't run through function calls by default (placeholders inside a function call form their own functions for corresponding arguments).
To create a partial function, add <code>@</code> character.</p> <p>Lambdas can also be multi-line, so that you can define normal functions without any type annotations.</p> <p>Note that lambda functions currently work only with arguments of known types.
You cannot pass a lambda function to another lambda function.
In the case of functional arguments, it's better to use the full generic definition instead.</p> <h3 id="generators"><a href="#generators">#</a> Generators</h3> <p>Generator is a function producing a sequence of values that can be iterated over without storing it in memory.
To create a generator, add an asterisk symbol to the function definition.</p> <p>Lambdas can be generators as well.</p> <p>Note that generators are currently supported only with Clang.</p> <h2 id="classes"><a href="#classes">#</a> Classes</h2> <h3 id="class-definition-and-object-construction"><a href="#class-definition-and-object-construction">#</a> Class definition and object construction</h3> <p>Definition of a class consists of its name and list of fields.
Each field may have an explicit default value; if not provided, it will be the default value for a given type.</p> <p>Every class has a default constructor function that accepts field values in the order of definition, or as named arguments.
Fields not directly initialized will receive their default values.</p> <p>Remember that class objects must always be explicitly constructed before use (they have no valid default …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.pyxell.org/docs/manual.html">https://www.pyxell.org/docs/manual.html</a></em></p>]]>
            </description>
            <link>https://www.pyxell.org/docs/manual.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25066058</guid>
            <pubDate>Thu, 12 Nov 2020 03:25:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Booting a macOS Apple Silicon Kernel in QEMU]]>
            </title>
            <description>
<![CDATA[
Score 201 | Comments 52 (<a href="https://news.ycombinator.com/item?id=25064593">thread link</a>) | @empyrical
<br/>
November 11, 2020 | https://worthdoingbadly.com/xnuqemu3/ | <a href="https://web.archive.org/web/*/https://worthdoingbadly.com/xnuqemu3/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>I booted the arm64e kernel of macOS 11.0.1 beta 1 kernel in QEMU up to launchd. It’s completely useless, but may be interesting if you’re wondering how an Apple Silicon Mac will boot.</p>

<h2 id="howto">Howto</h2>

<p>This is similar to my previous guide on running <a href="https://worthdoingbadly.com/xnuqemu2/">iOS kernel in QEMU</a>:</p>

<ul>
  <li>install macOS 11.0.1 beta 1 (20B5012D)</li>
  <li>run <a href="https://github.com/zhuowei/XNUQEMUScripts/blob/macos1101b1/macos11/build_arm64e_kcache.sh"><code>build_arm64e_kcache.sh</code></a> to create an Apple Silicon Boot Kext Collection</li>
  <li>build the modified QEMU:
    <div><div><pre><code>git clone https://github.com/zhuowei/qemu
cd qemu
git checkout a12z-macos
mkdir build
cd build
../configure --target-list=aarch64-softmmu
make
</code></pre></div>    </div>
  </li>
  <li>create a modified device tree by running <a href="https://github.com/zhuowei/XNUQEMUScripts/blob/macos1101b1/FourthTry/DTRewriter.java">DTRewriter</a> on <a href="https://updates.cdn-apple.com/2020SummerSeed/fullrestores/001-30235/6D8C0CA3-5952-4FD8-AEB3-4B4CADB626BC/iPad8,11,iPad8,12_14.0_18A5332f_Restore.ipsw">iPad Pro firmware</a>:
    <div><div><pre><code>python3 extractfilefromim4p.py Firmware/all_flash/DeviceTree.j421ap.im4p DeviceTree_iPad_Pro_iOS_14.0_b3.devicetree
java DTRewriter DeviceTree_iPad_Pro_iOS_14.0_b3.devicetree DeviceTree_iPad_Pro_iOS_14.0_b3_Modified.dtb
</code></pre></div>    </div>
  </li>
  <li>run QEMU:
    <div><div><pre><code>./aarch64-softmmu/qemu-system-aarch64 -M virt -cpu max \
  -kernel /path/to/bootcache-arm64e \
  -dtb /path/to/DeviceTree_iPad_Pro_iOS_14.0_b3_Modified.dtb  \
  -monitor stdio -m 6G -s -S -d unimp,mmu \
  -serial file:/dev/stdout -serial file:/dev/stdout -serial file:/dev/stdout \
  -append "-noprogress cs_enforcement_disable=1 amfi_get_out_of_my_way=1 nvram-log=1 debug=0x8 kextlog=0xffff io=0xfff serial=0x7 cpus=1 rd=md0 apcie=0xffffffff" \
  -initrd /path/to/ios14.0b3/ramdisk.dmg $@
</code></pre></div>    </div>
  </li>
  <li>run gdb with <a href="https://github.com/zhuowei/XNUQEMUScripts/blob/macos1101b1/macos11/bootit.gdbscript">this script</a>:
    <div><div><pre><code>~/Library/Android/sdk/ndk/21.0.6113669/prebuilt/darwin-x86_64/bin/gdb \
-D /any/emptydir \
-x bootit.gdbscript
</code></pre></div>    </div>
    
  </li>
</ul>

<p>And the macOS kernel will <a href="https://gist.github.com/zhuowei/5aa668e76f387374cd56848313aa2197">boot into launchd</a>.</p>

<h2 id="is-this-useful">Is this useful?</h2>

<p><img src="https://worthdoingbadly.com/assets/blog/xnuqemu3/wwdc2018_no.jpg" alt="&quot;No.&quot; - Craig Federighi, WWDC 2018"></p>

<p>No:</p>

<ul>
  <li>Absolutely nothing is supported: literally only the kernel and the serial port works, not even the userspace since there’s no disk driver</li>
  <li>Userspace is instead borrowed from iOS 14 b3</li>
  <li>This will never boot anything close to graphical macOS UI</li>
  <li>Most importantly, even if I ever managed to fully boot the macOS kernel, emulating macOS is useless anyways.</li>
</ul>

<p>There are only three reasons I can think of for emulating macOS: security research, software development without a real Apple Silicon machine, and Hackintoshing. This approach will help with none of these:</p>

<ul>
  <li>Emulating iOS is useful for security research when jailbreak is not available. Apple Silicon Macs already support kernel debugging.</li>
  <li>Not useful for software dev: QEMU’s CPU emulation doesn’t support Apple Silicon-specific features, such as Rosetta’s memory ordering or the APRR JIT.</li>
  <li>as for Hackintosh: macOS uses CPU instructions that aren’t available yet on non-Apple ARM CPUs, so you can’t have hardware accelerated virtualization, only very slow emulation. Besides, Hackintoshes are often built when Apple’s own hardware isn’t fast enough; in this case, Apple’s ARM processors are already some of the fastest in the industry.</li>
</ul>

<p>I researched this this not because it’ll be practical, but only to understand how an Apple Silicon Mac works. This will never be a <a href="https://www.youtube.com/watch?v=1AtE54HpXBM">Time Train</a>: only a <a href="https://youtu.be/UswpJh6Zvd8?t=119">science experiment</a>.</p>

<h2 id="what-i-did">What I did</h2>

<h2 id="create-kext-collection">Create kext collection</h2>

<p>On iOS, the kernel and its Kexts are packed together into a bootable file called the <strong>Kernel Cache</strong>.</p>

<p>macOS 11 uses an evolved version of this format, called the <strong>Boot Kext Collection</strong>.</p>

<p>Like the iOS kernelcache, it contains all Kexts required for booting, so the bootloader only needs to load it into memory and jump into it.</p>

<p>To create a boot kext collection, macOS 11 introduces the <a href="https://developer.apple.com/documentation/kernel/installing_a_custom_kernel_extension?language=objc"><code>kmutil</code></a> tool.</p>

<p>Here’s <a href="https://github.com/zhuowei/XNUQEMUScripts/blob/macos1101b1/macos11/build_arm64e_kcache.sh">my script</a> to get <code>kmutil</code> to generate an arm64e kext collection.</p>

<p>It manually excludes some kexts because they cause kmutil to error out. Most are because they depend on ACPI, which is not available on Apple Silicon. I made a <a href="https://github.com/zhuowei/XNUQEMUScripts/blob/macos1101b1/macos11/printexcludekexts.sh">script</a> to detect them.</p>

<p>Debugging <code>kmutil</code> failures on macOS 11 beta 3 was easy because it dumped out the entire NSError message. However, on macOS 11.0.1 beta, Apple decided to hide the full error message and only print an error code. I had to disable SIP and put a breakpoint on <code>swift_errorRetain</code> to get at the underlying error.</p>

<p>Once the <code>build_arm64e_kcache.sh</code> runs, a Boot Kext Collection is created at <code>~/kcache_out/bootcache-arm64e</code>, which can be booted in QEMU.</p>

<h2 id="disassembling-the-boot-kext-collection">Disassembling the boot kext collection</h2>

<p>For debugging, I also had to disassemble the newly created Boot Kext Collection in Ghidra.</p>

<p>Unfortunately, Ghidra isn’t updated for macOS 11 and will refuse to load the file, first giving an error about XML DOCTYPE, then - once that’s worked around - an <a href="https://github.com/NationalSecurityAgency/ghidra/issues/2192">IOException</a> from the invalid <code>ntools</code> value in the <code>LC_BUILD_VERSION</code> load command.</p>

<p>I created a <a href="https://github.com/zhuowei/XNUQEMUScripts/blob/macos1101b1/macos11/patch_boot_kext_collection_for_ghidra.py">script</a> to fix up the kext collection so that Ghidra can load it.</p>

<p>Note that this is still not perfect - Ghidra still doesn’t fixup pointers or read symbols.</p>

<p>To get method names, I also disassembled the raw kernel file (<code>/System/Library/Kernels/kernel.release.t8020</code>) for cross reference. Note that the raw kernel is based at a different address - you can either rebase it in Ghidra, or just be careful to convert addresses.</p>

<h2 id="disable-pac">Disable PAC</h2>

<p>I already had a <a href="https://worthdoingbadly.com/xnuqemu2/">modified QEMU to boot an iOS kernel</a> (which has inspired others, such as <a href="https://alephsecurity.com/">Aleph Security</a>, to build much better open-source iOS emulation platforms)</p>

<p>In early 2019 I updated my modified QEMU to work with PAC for the iPhone Xs/Xr.</p>

<p>QEMU by that time already supported PAC instructions; however, Apple <a href="https://googleprojectzero.blogspot.com/2019/02/examining-pointer-authentication-on.html">modified</a> the crypto algorithm when implementing PAC, so the kernel fails to boot.</p>

<p>I decided to instead <a href="https://github.com/zhuowei/qemu/commit/16613b67ad15a902791109077ebfb1091f1873aa">turn PAC instructions</a> into no-ops, since I don’t know how Apple’s algorithm worked. This also makes it easier to debug the kernel.</p>

<p>Since the macOS DTK used an A12z processor, the modified QEMU just worked.</p>

<h2 id="device-tree">Device tree</h2>

<p>Like iOS, macOS on Apple Silicon uses a <strong>device tree</strong> to describe hardware to the kernel, and to pass boot arguments.</p>

<p>macOS 11.0.1 beta’s installer doesn’t contain a device tree for the DTK: I suspect it would be in the .ipsw files, which are not publically available. Instead, I borrowed the iPad Pro’s device tree from iOS 14 beta 3.</p>

<p>Like the <a href="https://worthdoingbadly.com/xnuqemu2/">iOS in QEMU experiments</a>, I first disabled every piece of hardware in the device tree except the serial port.</p>

<p>Unlike iOS, macOS expects some more information in the device tree:</p>
<ul>
  <li>ram size (since Macs have upgradeable RAM)</li>
  <li>nvram, otherwise panics with a null pointer while reading nonce-seed. (I copied nvram from <a href="https://gist.github.com/bazad/1faef1a6fe396b820a43170b43e38be1">bazad’s dump of an iPhone device tree</a>.)</li>
  <li>AMCC (KTRR) register positions</li>
  <li>System Integrity Protection status</li>
</ul>

<p>I rewrote my <a href="https://github.com/zhuowei/XNUQEMUScripts/blob/macos1101b1/FourthTry/DTRewriter.java">device tree editor</a> to allow populating these extra params.</p>

<h2 id="up-to-launchd">Up to launchd</h2>

<p>I can’t actually boot a macOS root filesystem as I don’t have an emulated hard disk.</p>

<p>I don’t have a recovery ramdisk either: that would likely only be included in the DTK IPSW, which is not public.</p>

<p>Instead, I decided to boot with an iOS ramdisk to test the kernel, and disable signature checking using a GDB breakpoint.</p>

<p>I also couldn’t get the trustcache (list of executables trusted by the kernel) to load. I tried following <a href="https://alephsecurity.com/2019/06/25/xnu-qemu-arm64-2/">Aleph Security’s guide</a>, but macOS 11 is more strict than iOS 12 and needs it below the kernel; I couldn’t figure out the correct memory address.</p>

<h2 id="why-it-took-so-long">Why it took so long</h2>

<p>I actually had this blog post ready since <a href="https://gist.github.com/zhuowei/27816d39f234468cf2956479c0dea7ad">August 9th</a>, but I spent an extra 3 months trying to fix issues, since I really wanted to at least get to a shell!</p>

<p>Unfortunately:</p>
<ul>
  <li>debugging why drivers wasn’t loading was hard</li>
  <li>I couldn’t disable signature checking properly</li>
  <li>I wanted to wait until Apple released an A14 kernel instead of the DTK’s A12, so that we can look at how virtualization works, but they never did</li>
</ul>

<p>It’s now November 9th and Apple’s holding their press conference tomorrow: so it’s <a href="https://www.youtube.com/watch?v=9tAbhrDUrqM">now or never</a>.</p>

<h2 id="whats-left">What’s left</h2>

<p>I’m probably not going to be working further on this, but here’s what one can do to make this an actual useful research platform:</p>

<ul>
  <li>Figure out why half the drivers aren’t loading at all</li>
  <li>Write basic drivers/emulations:
    <ul>
      <li>probably emulate AIC in QEMU (based on Project Sandcastle’s Linux driver) since a custom interrupt controller Kext would be hard to write</li>
      <li>port Apple’s old <a href="https://opensource.apple.com/source/AppleMacRiscPCI/AppleMacRiscPCI-3.4/AppleMacRiscPCI.cpp.auto.html">PowerPC PCIE</a> drivers, since it’s too hard to emulate the Apple Silicon PCIE controller. This will allow us to connect a virtual hard drive.</li>
    </ul>
  </li>
  <li>Switch to the A14 kernel when Apple releases Apple Silicon Macs, so we can test virtualization</li>
</ul>

<h2 id="what-i-learned">What I learned</h2>

<ul>
  <li>How to modify QEMU to disable PAC</li>
  <li>How iBoot on Apple Silicon passes boot options in the device tree</li>
  <li>How to generate an Apple Silicon kernel cache without an Apple Silicon Mac</li>
  <li>How to fight <code>kmutil</code> for the real error message</li>
  <li>Never procrastinate on a blog post for three months</li>
</ul>

  </div>
</article>

      </div>
      
    </div></div>]]>
            </description>
            <link>https://worthdoingbadly.com/xnuqemu3/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25064593</guid>
            <pubDate>Thu, 12 Nov 2020 00:09:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Software engineering photonics and color science]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25062325">thread link</a>) | @pete314
<br/>
November 11, 2020 | https://www.softcolorsoftware.com/resources/software-engineering-photonics-and-color-science/ | <a href="https://web.archive.org/web/*/https://www.softcolorsoftware.com/resources/software-engineering-photonics-and-color-science/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>


<hr><h2>Software engineering photonics and color science</h2>

<p>

<img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science.png">

</p>
<p>Hi everybody, I am Petri Piirainen, a co-founder and chief technology officer of SoftColor company. Welcome to this video lecture about SoftColor's fifteen-year software engineering photonics and color science story. </p>

<p>This lecture is part of the University of Eastern Finland's photonics applications course and lecture series. </p>

<p>Since 2005 we have made photo editing automation software. Our photonics journey is slightly different from traditional optics-focused companies, which you have met during this lecture series. </p>

<p>During this lecture, I will tell you what we have learned about developing and selling photonics applications. </p>

<hr><h2>My history with photonics and software engineering </h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_2.png"></p><p>I have an MSc degree in computer science (from the University of Eastern Finland). I studied computer science in a digital signal processing program, and then I had mandatory applied mathematics and physics as minor studies. With physics studies, there was a lot of photonics and color science courses. </p>

<p>I started by software business and engineering career during high school in the 90s.  In 2005 we founded SoftColor Oy, and since that, we have developed faster, easier, and better photo editing automation software.</p>


<hr><h2>Photonics applications: What have we learned?</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_3.png"></p><p>First, I would like to talk a little bit about photonics applications, the beauty and the beast of engineering photonics applications. We have learned that developing useful photonics applications is very hard and requires a lot of engineering and math knowledge. It is challenging because photonics applications (software or hardware) usually have to quickly process tons of data and calculations. </p>

<hr><h2>Technical elements of useful photonics application</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_4.png"></p><p>My favorite thing with photonics is that all photonics applications contain four types of engineering. </p>
<ul>
<li>Physics</li>
<li>Mathematics</li>
<li>Electrical engineering</li>
<li>Computer science </li>

</ul>

<p>But there is also one thing, which is fascinating.  All photonics applications require a lot of arts too.</p>

<ul>
<li>Image quality</li>
<li>Industrial design</li>
<li>User experience</li>
<li>User interaction</li>
<li>User interfaces</li>

</ul>

<p>This mixture of arts and engineering is my main topic for you today.</p>

<hr><h2>Topics</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_5.png"></p><p>My topics today are:</p>

<ul>
<li>A short history and introduction to our company</li>
<li>What we do and how products work</li>
<li>How have we mixed photonics with software engineering</li>
<li>What we learned about to make useful photonics applications</li>
<li>And there will be a bonus "homework" for  you</li>

</ul>


<hr><h2>Story of SoftColor Oy</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_6.png"></p><p>Let's have a quick look at our products and technology. </p>

<hr><h2>What we do</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_7.png"></p><p>We make faster, easier, and better photo edition automation software. Our software runs on Windows PCs and servers.</p>

<hr><h2>SoftColor Oy</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_8.png"></p><p>We founded SoftColor in 2005, so our company is now a teenager. </p>

<p>We have three products: Automata Server, Automata Pro, and PhotoEQ.</p>

<p>We are located in Joensuu, Finland. </p>

<hr><h2>Our business</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_9.png"></p><p>We sell our software on the internet. And all our products are free to try before buying.  Our customers are:</p>

<ul>
<li>Printing industry</li>
<li>Photographers</li>
<li>360 photography</li>
<li>Newspapers</li>
<li>Ad agencies</li>
<li>Repro</li>
<li>Real estate</li>
<li>Car retail</li>
<li>Photo editors</li>
<li>Office workers</li>
<li>Developers</li>

</ul>

<p>Our customers are from the English speaking world. But we have a lof customers from Germany and Spain too.</p>

<hr><h2>Our research and development</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_10.png"></p><p>Our research and development work is to make our photo editing automation software faster, more comfortable, and better to use.</p>

<hr><h2>SoftColor engine </h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_11.png"></p><p>Our applications use the SoftColor engine. It is the brains and heart of our software. </p>

<p>SoftColor engine does all photo editing automation tasks, color correction, image editing, and color management. </p>

<p>To get this automation working. We have combined computer vision, color science, computer graphics, digital signal processing, and machine learning techniques into one packet. </p>

<p>This combination of different engineering tools has made our photo editing automation to work very well.</p>

<hr><h2>How does our color and tone correction work?</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_12.png"></p><p>Let's check how our photo enhancement automation works, with good or bad photos. </p>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_13.png"></p><p>We can fix white balance, exposure, and contrast problems automatically. Results are very natural and good looking.</p>

<p>Our correction works with challenging photos too.</p>

<p>Our white balance correction will you very natural results.</p>

<p>It works with all kinds of photos and cameras.</p>

<p>You will never lose any shots. We can fix them.</p>

<hr><h2>SoftColor and photonics</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_14.png"></p><p>Let's talk a little bit about photonics. </p>

<hr><h2>We are processing colors, not pixels.</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_15.png"></p><p>To get photo editing automation working better. We have learned computers to process colors, not pixels. </p>

<hr><h2>A good photo is a combination of art and science.</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_16.png"></p><p>The most challenging part of photo editing automation is to get results that make our customers happy. The problem is that the excellent photo is a combination of art and science. There is eighty percent of art in the superb picture and only twenty percent of engineering.  </p>

<p>For this problem, we managed to create an excellent solution.</p>

<hr><h2>Traditional image editing and photonics</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_17.png"></p><p>There is photonics behind every camera, display, and photo-editing algorithms. </p>

<hr><h2>SoftColor engine and photonics</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_18.png"></p><p>We have changed to traditional photo editing. We built our engine to take colors first.  This solution has helped us to make better photo enhancement automation.</p>

<hr><h2>We have made a color correction automation that has tools for science and art.</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_19.png"></p><p>Our applications offer tools to our customers to combine art and science with photo editing automation. </p>

<p>Science part:</p>

<ul>
<li>Layer based processing</li>
<li>Statistical analysis </li>
<li>Metadata analysis</li>
<li>Machine vision</li>
<li>Machine learning</li>

</ul>

<p>and the arts:</p>

<ul>
<li>Color grading for mass photo processing</li>
<li>Selective color adjustments </li>
<li>No workflow limitations</li>

</ul>

<hr><h2>SoftColor engine benefits</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_20.png"></p><p>We are processing the colors, not pixels. This approach gives three significant benefits:</p>

<ul>
<li>More accurate automatic correction</li>
<li>Batch color grading for photos</li>
<li>Easier and accurate customization</li>

</ul>


<hr><h2>SoftColor engine technical details</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_21.png"></p><p>Let's take a look inside our engine.</p>

<hr><h2>Layer based processing</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_22.png"></p><p>We use layers based processing, which means that all correction and image processing tools are separate layers. </p>

<p>You will full control of how each step works. For automatic color and tone correction, we have six layers. </p>

<ul>
<li>Rich dynamics enhancer</li>
<li>Luminosity enhancer</li>
<li>White balance </li>
<li>Natural color temperature</li>
<li>Exposure and contrast</li>
<li>Color grading</li>

</ul>


<hr><h2>Spectral illumination estimation technology for better color correction</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_23.png"></p><p>To get better automatic results for all kinds of photos. We have developed spectral illumination estimation technology. </p>

<hr><h2>Same parameters for human and computer</h2>
<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_24.png"></p><p>We use spectral illumination detection to create the same parameters for the user and the computer. </p>

<p>We calculate these parameters from the original image by using:</p>

<ul>
<li>Spectral  illumination estimation from RGB image</li>
<li>Metadata analysis EXIF &amp; camera data</li>
<li>Machine learning for estimated data</li>

</ul>

<p>From this data, we generate parameters for automatic correction.</p>

<p>When users change parameters, they will alter the same settings as our automatic correction uses.</p>

<p>The solution is possible by mixing spectral illumination data with computer graphics techniques.</p>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_25.png"></p><hr><h2>What we have learned about photonics applications during the last fifteen years</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_26.png"></p><p>There six things which we have learned. Which are the requirements for good photonics software or hardware applications.</p>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_27.png"></p><ol>
<li>Software is the glue for photonics applications</li>
<li>Optical engineering</li>
<li>Software engineering</li>
<li>Electrical engineering</li>
<li>User experience engineering</li>

</ol>

<p>These are things and skills which are required. But there is always one challenge, battery, and CPU limitations.  It is the reason why we all need to tune our algorithms, software, and hardware better every day.</p>

<hr><h2>Bonus "homework." Useful resources for your entrepreneur career</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_28.png"></p><p>There has been a lot of questions about how to get started with the photonics business.  Here are two great resources to read or watch.</p>


<hr><h2>To watch "Halt and Catch Fire" tv-series.</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_29.png"></p><p>Halt and Catch Fire is an excellent and very realistic business world tv-series.  The tv-series follows some players in the 80s technological revolution that lead to an information society. It is quite an unknown series, but now it is available in streaming services. </p>



<p>You will learn a lot about the hardware and software business. </p>

<p>The tv-series covers the following useful topics:</p>

<ul>
<li>Venture capital</li>
<li>Bootstrapping</li>
<li>Human resources</li>
<li>Risk management </li>
<li>Work/life balance</li>
<li>Legal stuff (due diligence, intellectual property rights, revenge engineering process) </li>
<li>Fortune 500 vs. startup life </li>

</ul>

<p>And there is a lot of 80s and 90s retro computing and nostalgia too.</p>

<p>
<a href="https://en.wikipedia.org/wiki/Halt_and_Catch_Fire_(TV_series)" target="_blank">Halt and Catch Fire in Wikipedia</a>
</p>


<hr><h2>To read "Masters of Doom" book.</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_30.png"></p><p>Masters of Doom book tells the story of ID software. The makers of Wolf3D, Doom, and Quake games. </p>

<p>There are fascinating stories about how small teams can change the world. For the photonics industry, there is interesting how ID software took the latest research topics from computer graphics and science. And how they utilized them to make their games better.</p>

<p>This book is also available as an audiobook.</p>

<p>
<a href="https://en.wikipedia.org/wiki/Masters_of_Doom" target="_blank">Masters of Doom book in Wikipedia</a>
</p>


<hr><h2>Summary </h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_31.png"></p><p>That was our 15 years story with software engineering, photonics, and color science. I hope that you have learned something new for photonics career or business.</p>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_32.png"></p><p>Here is a summary of the main topics:</p>

<ul>
<li>A good photo is a combination of art and science</li>

</ul>

<ul>
<li>Software is the glue for photonics applications.</li>

</ul>

<ul>
<li>Photonics is a mixture of science, engineering, and arts.</li>

</ul>

<hr><h2>Feedback and comments</h2>

<p><img src="https://www.softcolorsoftware.com/images/Software_engineering_photonics_and_color_science_33.png"></p><p>
It would be great to hear your feedback about this lecture! <a href="https://www.softcolorsoftware.com/contact/">
</a></p><a href="https://www.softcolorsoftware.com/contact/">
</a><center><a href="https://www.softcolorsoftware.com/contact/">
Just drop us a message.</a>
</center>


</div></div>]]>
            </description>
            <link>https://www.softcolorsoftware.com/resources/software-engineering-photonics-and-color-science/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25062325</guid>
            <pubDate>Wed, 11 Nov 2020 20:02:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Notion Timeline View]]>
            </title>
            <description>
<![CDATA[
Score 287 | Comments 147 (<a href="https://news.ycombinator.com/item?id=25061781">thread link</a>) | @AlphaWeaver
<br/>
November 11, 2020 | https://www.notion.so/guides/timeline-view-unlocks-high-output-planning-for-your-team | <a href="https://web.archive.org/web/*/https://www.notion.so/guides/timeline-view-unlocks-high-output-planning-for-your-team">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.notion.so/guides/timeline-view-unlocks-high-output-planning-for-your-team</link>
            <guid isPermaLink="false">hacker-news-small-sites-25061781</guid>
            <pubDate>Wed, 11 Nov 2020 19:15:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Source Code Is Not the Whole Story: Understanding Software Through APIs]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25061745">thread link</a>) | @jeanyang
<br/>
November 11, 2020 | https://www.akitasoftware.com/blog/2020/11/10/kbgnmpu9bdi6qsgkkxo1amqptellna | <a href="https://web.archive.org/web/*/https://www.akitasoftware.com/blog/2020/11/10/kbgnmpu9bdi6qsgkkxo1amqptellna">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-0b8680fe602586cc73eb"><div><p>👋 hunters!</p><p>After putting out an early iteration a few months ago, we’re excited to officially launch our private beta—and to make our docs public for the first time!</p><p>And we'd like to give a big thank-you to all the users and friends of Akita who got us here. 😊<br></p><h2><strong>🚗 How we got here</strong></h2><p>In 2018, I was a <a href="http://jeanyang.com/">CS professor at Carnegie Mellon University</a>. When Cambridge Analytica hit, I started asking friends in industry how they knew what data their apps were sending around. It turned out that there was no good solution for understanding interactions across APIs—not just for privacy and security but also for reliability and diagnostics. When I realized that this was the exact problem I’d been teeing up to solve, I took leave from my job, sold my furniture, and drove across the country to start Akita.</p><p>There is now a small team of us working on Akita, coming from places like Twilio and Amazon. Our team’s experience building in service-oriented environments made everyone especially excited to build a product that would help developers move faster together.</p><h2><strong>🔎 Source code isn’t the whole story.</strong></h2><p>At Akita, we believe the way to understand your software is through your APIs. Our solution builds dynamic models of API behavior to automatically:</p><p>✅Catch breaking changes on every pull request<br>✅Generate specs for any API<br>✅Update API specs on every pull request<br>✅Discover and document endpoints</p><h2>🏗 <strong>How we built it</strong></h2><p>From the beginning of Akita, I knew where I wanted us to go: automatically map out the graph of API interactions. This would be key to improving reliability, diagnostics, and security in modern web apps.</p></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1605055841399_25008"><div><p>What we didn’t know was exactly how we would get there. Should we integrate with a tracing library? Should we build a proxy?</p><p>After <a href="https://blog.sigplan.org/2020/10/27/whats-the-role-of-developer-experience-in-programming-languages-research/">talking with dozens of developers and engineering leaders</a>, we came up with three main requirements. First, whatever we built needed to work with any tech stack. Second, people needed to be able to integrate our solution in minutes. Finally, we wanted something that could run in production without adding overhead or exposing sensitive data. These were not easy requirements to balance!</p><p>After over a year of building, we’re super excited to launch a solution that requires no code changes, no proxies, works with any language, and integrates in just minutes. Akita works by watching API traffic, analyzing, and sanitizing the traffic locally to share only metadata back to our cloud. This means you can Akita deploy Akita anywhere: your laptop, CI/CD, or production—without having to worry about us seeing your data. We are really proud of our approach and believe it is the future of cloud observability.</p><p>And for those who are curious, our tech stack is Go, typed Python, and React. 😊</p><h2><strong>💖 Let’s make software development better</strong></h2><p>We believe that Akita can help anybody with a web app who wants to move quickly without losing customers. We understand that we have a long way to go to achieve the vision—and that the only way to get there is by getting feedback early and often from people like you.</p><p>We’d love to have you <a href="https://www.akitasoftware.com/get-invite?utm_campaign=2020_private_launch&amp;utm_medium=blog&amp;utm_source=2020_11_11_producthunt">try out the private beta</a>! We’ll also be checking the comments on <a href="https://www.producthunt.com/posts/akita-private-beta">ProductHunt</a> for feedback and questions. We look forward to hearing from you.</p><p>Onward ⚡️,<br>Jean Yang (<a href="https://www.linkedin.com/in/jean-yang-96575030/">LinkedIn</a>; <a href="https://twitter.com/jeanqasaur">Twitter</a>)<br>Founder and CEO, Akita Software</p><p>P.S. Thank you to the veterans out there!<br>P.P.S. Follow our updates and tell us what you think on Twitter <a href="https://twitter.com/AkitaSoftware">@AkitaSoftware</a>!</p></div></div></div>]]>
            </description>
            <link>https://www.akitasoftware.com/blog/2020/11/10/kbgnmpu9bdi6qsgkkxo1amqptellna</link>
            <guid isPermaLink="false">hacker-news-small-sites-25061745</guid>
            <pubDate>Wed, 11 Nov 2020 19:12:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AndroWish – run desktop Tcl/Tk programs almost unaltered on the Android Platform]]>
            </title>
            <description>
<![CDATA[
Score 59 | Comments 16 (<a href="https://news.ycombinator.com/item?id=25061292">thread link</a>) | @jakobdabo
<br/>
November 11, 2020 | https://www.androwish.org/index.html/home | <a href="https://web.archive.org/web/*/https://www.androwish.org/index.html/home">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<tbody><tr><td>
<p>Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language, suitable for a very wide range of uses, including web and desktop applications, networking, administration, testing and many more. Open source and business-friendly, Tcl is a mature yet evolving language that is truly cross platform, easily deployed and highly extensible.

</p><p>Tk is a graphical user interface toolkit that takes developing desktop applications to a higher level than conventional approaches. Tk is the standard GUI not only for Tcl, but for many other dynamic languages, and can produce rich, native applications that run unchanged across Windows, Mac OS X, Linux and more. 

</p><p>AndroWish allows to run desktop Tcl and Tk programs almost unaltered on the Android Platform while it opens the door to script a rich feature set of a mobile platform. Its sibling <a href="https://www.androwish.org/index.html/wiki?name=undroidwish">undroidwish</a> uses the same code base and  offers a similar feature set on various desktop and embedded platforms.

</p></td><td>
<img width="160px" height="160px" src="data:image/png;base64,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">
</td></tr>
</tbody></div><p>The current AndroWish-debug.apk can be downloaded <a href="https://www.androwish.org/download/AndroWish-c48f047f5b-debug.apk">here</a> (about 36 MByte, requires "install from unknown sources" in Android settings). Prehistoric versions are <a href="http://www.ch-werner.de/sdltk/AndroWish">still available here</a>.


</p></div>]]>
            </description>
            <link>https://www.androwish.org/index.html/home</link>
            <guid isPermaLink="false">hacker-news-small-sites-25061292</guid>
            <pubDate>Wed, 11 Nov 2020 18:36:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Music-Related Copyright Claims and Twitch]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25061259">thread link</a>) | @haunter
<br/>
November 11, 2020 | https://blog.twitch.tv/en/2020/11/11/music-related-copyright-claims-and-twitch/ | <a href="https://web.archive.org/web/*/https://blog.twitch.tv/en/2020/11/11/music-related-copyright-claims-and-twitch/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-i18n="ba2fc6a46b864e1cc0b2afadb1eff0cf-content">
    <p>Creators, we hear you. Your frustration and confusion with recent music-related copyright issues is completely justified. Things can–and should–be better for creators than they have been recently, and this post outlines our next steps to get there. Moving forward, we’ll be more transparent with what’s happening and what tools and resources we’re building to help.</p>

<p>Copyright law and the DMCA are not small or simple topics, so this won’t be a brief post. We’ll do our best to keep the legalese to a minimum, though there’s bound to be technical terms here and there.&nbsp;</p>

<h4 id="dmca-and-twitch"><strong>DMCA and Twitch</strong></h4>

<p>First off, a quick review of what DMCA actually is. The Digital Millennium Copyright Act (“DMCA”) is a set of US laws that allows you to create and share content on digital service providers like Twitch. We comply with the DMCA and similar laws worldwide. Part of complying means that when a copyright holder thinks a streamer has used their content without permission, we have a process in place for them to be able to request the content be taken down.</p>

<p>When we receive a DMCA notification, we process the notification in accordance with our <a href="https://www.twitch.tv/p/legal/dmca-guidelines/">DMCA Guidelines</a>. This includes removing the content, sharing the details with the channel owner, and tracking the allegation.&nbsp;</p>

<p>DMCA takedown notifications can affect your ability to stream because we, as part of our efforts to comply with the DMCA and similar global laws, issue and track copyright strikes and ban the accounts of those who repeatedly infringe the copyrights of others.&nbsp;</p>

<p>This policy is important because we respect the rights of all creators, including those who create or record music, as well as the rights of those who own and control copyrights. As a company that is built around a community of people who create content, we take allegations of copyright infringement seriously.&nbsp;</p>

<h4 id="recent-dmca-notifications"><strong>Recent DMCA notifications</strong></h4>

<p>How did we get to this moment? Until May of this year, streamers received <strong>fewer than 50 music-related DMCA notifications each year</strong> on Twitch. Beginning in May, however, representatives for the major record labels started sending <strong>thousands of DMCA notifications each week</strong> that targeted creators’ archives, mostly for snippets of tracks in years-old Clips. We continue to receive large batches of notifications, and we don’t expect that to slow down.&nbsp;</p>

<p>This means two things: 1) if you play recorded music on your stream, you need to stop doing that and 2) if you haven’t already, you should review your historical VODs and Clips that may have music in them and delete any archives that might.&nbsp;</p>

<p>We were as surprised by this sudden avalanche of notifications as many of you were. We also realized that we needed to provide streamers with more educational programs and content management tools to help you deal with this unprecedented number of notifications coming in all at once. So, while we continued to remove content targeted by these notifications as required by the DMCA, we understood VODs and Clips from years ago may not necessarily reflect your current approach to music. Therefore, we also paused the processing of strikes associated with these batched notifications in order to give you the tools, information, and time that you would need to deal with them.</p>

<p>We have analyzed the notifications we received during that period from the end of May through the middle of October. What we found is that more than 99% of the notifications were for tracks that streamers were playing in the background of their stream.&nbsp;</p>

<p>The point of the DMCA is to strike a balance between the interests of rights holders (the major record labels in this case) and creators. Because of this, we were compelled to delete the VODs and Clips that were identified in the notifications. This showed our commitment to upholding our obligations under the DMCA, while affording us the opportunity to sort out the best way to handle issuing strikes in these circumstances. Under these extraordinary circumstances, we recognized creators should have a reasonable chance to understand that content created in the past was being targeted as allegedly infringing and be given an opportunity to change their approach to music use before they got hit with strikes.</p>

<p>This led to the current situation, which is understandably frustrating and worrying for many of you. Given the circumstances, the warning email many of you received didn’t include all the information that you’d typically get in a DMCA notification (normally, when we receive a DMCA notification against your channel, we send you an email that includes information about the allegedly infringed work, who the claimant is, how the claimant can be contacted, and possible penalties under our repeat infringer policy, so that you can make an informed decision about whether to submit a counter notification or seek a retraction). We hear your feedback about how frustratingly little information we provided, and we should have made that warning email a lot more informative and helpful.</p>

<p>Over the last several months, we have done our best to manage this situation on behalf of both rights holders and creators. One of the mistakes we made was not building adequate tools to allow creators to manage their own VOD and Clip libraries. You’re rightly upset that the only option we provided was a mass deletion tool for Clips, and that we only gave you three-days notice to use this tool. We could have developed more sophisticated, user-friendly tools awhile ago. That we didn’t is on us. And we could have provided creators with a longer time period to address their VOD and Clip libraries – that was a miss as well. We’re truly sorry for these mistakes, and we’ll do better.</p>

<h4 id="how-to-avoid-dmca-notifications"><strong>How to avoid DMCA notifications&nbsp;</strong></h4>

<p>One important question we’ve heard from you is: how can I stream safely and confidently on Twitch without having to worry about getting DMCA notifications from music use?</p>

<p>Most importantly, <strong>don’t play recorded music in your stream</strong> unless you own all rights in the music, or you have the permission of the necessary rights holder(s). Doing this is the best protection for your streams going forward. If you’re unsure whether you own all the rights, it’s pretty likely you don’t. If you want to include recorded music in your stream, use a fully licensed alternative like Soundtrack by Twitch, or other rights cleared music libraries such as Soundstripe, Monstercat Gold, Chillhop, Epidemic Sound, and NCS.</p>

<p>While we haven’t received more than a handful of DMCA notifications targeting in-game music, if you’re playing games with recorded music in them, we recommend you review their End User License Agreements (that wall of text at the beginning of a game) to see how the terms cover streaming with that music. One way to do this is to search for a game’s official EULA online and then do a ctrl+f (Command+f on Mac) search for words like “stream,” “licensed,” and “music” to point you toward the correct sections. If you’re unsure about the rights, some games allow you to turn off music when streaming, or you can mute the game audio yourself. If neither of those apply, consider turning off VODs and Clips.&nbsp;</p>

<p>For your stream archives (VODs and Clips), right now your only options, if you think they contain unauthorized music, is to either go through them one by one, or, for Clips, use the “delete all” tool we’ve provided. We understand both of these options have downsides, and we’re working to provide you more and better options as soon as possible. These things will take time to get right, and new challenges may appear in the future. Regardless, we’re committing here and now to investing in building better tools and keeping you posted on our progress.</p>

<h4 id="new-products-and-tools"><strong>New products and tools</strong></h4>

<p>Ever since the influx of DMCA notifications began, we have been working on building new (and improving existing) tools to help creators (such as the Clips mass deletion tool). This work is still happening. Many of these changes won’t be visible to the community, but we’re focused on three areas where we heard you need more support from us:</p>

<p>First, you don’t have enough control over the recorded content on your channel. We have made improvements to enable you to mass delete Clips, but in addition, we will (1) expand the use of technology to detect copyrighted audio, and (2) give you more granular ways to manage your archive instead of just a “delete all” option.</p>

<p>Second, we’ll make it easier for you to control what audio from your live streams will show up in your recorded content. Soundtrack by Twitch has some of this technology built into it, and we’ll work to make it available for everyone regardless of whether you want to use Soundtrack, for which we’ve cleared all necessary rights, or music from others that provide rights-cleared music.</p>

<p>Third, we need to give you the ability to actually review your allegedly infringing content when you receive a DMCA notification, in addition to the details already provided in our takedown notifications - that is, information about what copyrighted work was allegedly infringed, who the claimant is, and how the claimant can be contacted. We also need to help you more easily file counter notifications if you believe you have the rights to use the content–for example, because you’ve secured a license, believe the use is a fair use,&nbsp; the claimant does not control the rights, or believe you have the right to use the music without permission.</p>

<p>Some of you have asked why we don’t have a license covering any and all uses of recorded music. We are actively speaking with the major record labels about potential approaches to additional licenses that would be appropriate for the Twitch service. That said, the current constructs for licenses that the record labels have with other services (which typically take a cut of revenue from creators for payment to record labels) make less sense for Twitch. The vast majority of our creators don’t have recorded music as a part of their streams, and the revenue implications to creators of …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.twitch.tv/en/2020/11/11/music-related-copyright-claims-and-twitch/">https://blog.twitch.tv/en/2020/11/11/music-related-copyright-claims-and-twitch/</a></em></p>]]>
            </description>
            <link>https://blog.twitch.tv/en/2020/11/11/music-related-copyright-claims-and-twitch/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25061259</guid>
            <pubDate>Wed, 11 Nov 2020 18:33:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Raspberry Pi Homelab with Kubernetes]]>
            </title>
            <description>
<![CDATA[
Score 106 | Comments 47 (<a href="https://news.ycombinator.com/item?id=25061097">thread link</a>) | @amitpm
<br/>
November 11, 2020 | https://amithm.ca/2020/10/kubernetes-raspberrypi-homelab/ | <a href="https://web.archive.org/web/*/https://amithm.ca/2020/10/kubernetes-raspberrypi-homelab/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody"><p>I’ve been running <a href="https://pi-hole.net/">Pi-Hole</a> on a Raspberry Pi 3b wired into my wifi router for most of last year and its been great. So when the new Raspberry Pi 4 came out, I picked one up. It sits on my desk, mostly for easy access to its USB ports, which allows me to hook it up to some of my esp32 devkits and push micropython code onto them. The pi4 has been a great general purpose development environment.</p><p>Recently, I’ve been wanting to write some trivial web endpoints for “internal” dashboards and such for the house. Plus, its a great excuse to learn Golang. In this day and age, clearly a dockerized golang dev environment is the way to go. Have I truly built something, if my dev environment isn’t dockerized?</p><p>So we’re agreed that dockerizing my dev environment is the way to go. Surely if my dev environment is dockerized, how much more should my app deployments use containers? Nothing less will do. But now I need a way to deploy and orchestrate said containers? I know! I should run a kubernetes cluster across my two Pi’s! Might as well run the Pi-hole on it as well, how hard can it be?</p><p>So that is what I spent the better part of last week figuring out.</p><figure><img src="https://imgs.xkcd.com/comics/automation.png" alt="Mandatory xkcd"><figcaption><center>Mandatory xkcd</center></figcaption></figure><p>This blog post walks through what I did, and how I did it, It’s purpose is two-fold -</p><ol><li>It is a map to allow me to retrace my steps if I need to</li><li>Perhaps it may prove of (dubious) use to you.</li></ol><p>So, both my Pi’s run Ubuntu server. I decided I should start from scratch, and flashed the latest ubuntu server image onto the SD cards for both Pi’s. Being a very optimistic person by nature, I expected to have Pi-hole back up and running on this new Kubernetes cluster within a day, and a day of unfiltered ads was a small price to pay for the experience. Alas, it was close to a week before I had Pi-Hole working on my network again, but yay! you get to learn from my experience!</p><p>I didn’t have much of an understanding of Kubernetes components going into this project - but hey, that’s what these projects are meant to give you, and boy, did it. So fret not if you don’t understand some of these terms, the kubernetes documentation pages are great!</p><p>None of this work is original. I cobbled together guides and walkthroughts from various sources to get to this frankenstein’s monster of a post that you see here. You can find links to the sources I used at the end of this page.</p><p>The first step to this journey involves making sure you have the required packages on all your machines. In my case, this was two machines - the Pi4 (called Terminus) and the Pi3b (called Trantor). You need <code>docker</code>, <code>kubelet</code>, <code>kubeadm</code> and <code>kubectl</code>. You want this installed on all your nodes. Terminus will be my master node, Trantor will be my worker. Asimov fans may protest that the Second Foundation was on Trantor after all, but let’s go with this for now. Setting static IPs on the master and workers on your cluster also helps, but I won’t cover that here.</p><p>Update apt repos and packages.</p><div><pre><code data-lang="bash">sudo apt-get update
sudo apt-get upgrade
</code></pre></div><p>Install Docker using the Convenience script. Yes, shame on you for blindly running a script you downloaded from the internet.</p><div><pre><code data-lang="bash">curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
</code></pre></div><p>Let’s make sure our non-root user can use Docker.</p><div><pre><code data-lang="bash">sudo usermod -aG docker $USER
</code></pre></div><p>Now there’s some additional setup that needs to be done in order to get Kubernetes to work on the Raspberry Pi - specifically enabling <code>cgroups</code>. You can do this by editing the file <code>/boot/firmware/cmdline.txt</code> and adding the following options at the end.</p><pre><code>cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1
</code></pre><p>You’ll need to reboot the Pi after this.</p><p>Add the K8s apt repo.</p><div><pre><code data-lang="bash">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

cat <span>&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
</span><span>deb https://apt.kubernetes.io/ kubernetes-xenial main
</span><span>EOF</span>
</code></pre></div><p>You’ll notice we’re using <code>kubernetes-xenial</code> which was the latest release at the time of writing this. Update this to the latest release available if you need to.</p><p>Let’s install our main K8s helpers. We’ll also make sure they’re excluded freom any system upgrades. As the kubernetes <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">documentation</a> says, “<code>kubeadm</code> and <code>kubectl</code> require special attention to upgrade.”</p><div><pre><code data-lang="bash">sudo apt update <span>&amp;&amp;</span> sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
</code></pre></div><figure><img src="https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png" width="200" height="200"></figure><p>Create the cluster by running the following commands on the master node only. Pay special attention to the <code>--pod-network-cidr</code> parameter. You’ll need this CIDR range later on when setting up Flannel.</p><div><pre><code data-lang="bash"><span># Create the bootstrap token</span>
TOKEN<span>=</span><span>$(</span>sudo kubeadm token generate<span>)</span>
sudo kubeadm init --token<span>=</span><span>${</span>TOKEN<span>}</span> --pod-network-cidr<span>=</span>10.10.0.0/16
</code></pre></div><p>Congratulations. You are now the proud owner of a bare-metal kubernetes cluster (with one node). Admire the output, and consider running the commands they ask you to. For example, you’ll need a config file in <code>$HOME/.kube/config</code> if you want <code>kubectl</code> to work without too much hassle. Also make special note of the <code>kubeadm join</code> command as well, you’ll need to run that on your worker nodes.</p><p>These are the commands that the output from the previous step suggest you to run. Run this on the master node, in case that isn’t clear.</p><div><pre><code data-lang="bash">mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown <span>$(</span>id -u<span>)</span>:<span>$(</span>id -g<span>)</span> $HOME/.kube/config
</code></pre></div><p>Go run the <code>kubeadm join</code> commands on all the worker nodes you’d like to dedicate to this cluster. I’ll wait.</p><p>Going through this guide, you’ll quickly become familiar with the command <code>kubectl apply</code>. This command “applies a configuration to a resource” in kubernetes parlance and is typically provided a YAML “manifest” file as parameter.</p><p>So now we have a cluster, but technically Kubernetes doesn’t know how to handle networking between any pods that are scheduled on this cluster - atleast, that’s what I’ve understood. This is why you need an addon like Flannel to handle this for you. You can find a full list of Networking and Network Policy Addons <a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/">here</a>. But in case it isn’t clear yet, we’ll use Flannel.</p><figure><img src="https://raw.githubusercontent.com/coreos/flannel/master/logos/flannel-horizontal-color.png"></figure><p>If you’ve specified a <code>pod-network-cidr</code> parameter when creating your cluster, you’ll need to edit the Flannel manifest with this CIDR before you apply it to the cluster.</p><p>Let’s download the default flannel manifest</p><div><pre><code data-lang="bash">curl https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml --output kube-flannel-updated.yml
</code></pre></div><p>Open it the file up in your favourite editor, and find the key <code>net-conf.json</code>. Update the CIDR given there with the right CIDR for your cluster. Once done, apply the manifest like so.</p><div><pre><code data-lang="bash">kubectl apply -f ./kube-flannel-updated.yml
</code></pre></div><p>To check if this worked, run the following command to get all pods running on your cluster.</p><p>You should see <code>core-dns</code> and <code>kube-flannel</code> pods running like so. I have two pods for each because I have two nodes in my cluster.</p><div><pre><code data-lang="bash">NAMESPACE              NAME                                          READY   STATUS    RESTARTS   AGE
kube-system            coredns-f9fd979d6-h9m47                       1/1     Running   <span>1</span>          3d2h
kube-system            coredns-f9fd979d6-m5jrd                       1/1     Running   <span>1</span>          3d2h
kube-system            kube-flannel-ds-2ngxd                         1/1     Running   <span>1</span>          3d2h
kube-system            kube-flannel-ds-kqflv                         1/1     Running   <span>1</span>          3d2h
</code></pre></div><p>Namespaces are used to isolate pods and services running on the same cluster. My data engineer brain thinks of the cluster as a database and namespaces as schemas, but I could be mistaken and maybe should be thinking of the cluster as a single database install, and the namespaces as individual databases. Or maybe, this is entirely the wrong abstraction to bring in. Scratch all of this, let’s move on.</p><p>We now have a cluster, that knows how to handle pod networking. Let’s run something on it! How about the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">Kubernetes dashboard</a>, so that you have something pretty to show your non-technically inclined significant other as the output of your hard work?</p><figure><img src="https://raw.githubusercontent.com/kubernetes/dashboard/master/docs/images/dashboard-ui.png" alt="Behold! The fruits of your labour!"><figcaption><center>Behold! The fruits of your labour!</center></figcaption></figure><p>We’ll create a namespace to hold everything related to the Kubernetes Dashboard. I’m calling the namespace - <code>kubernetes-dashboard</code>. Very imaginative, no?</p><div><pre><code data-lang="bash">kubectl create namespace kubernetes-dashboard
</code></pre></div><p>We’ll now download the manifest file for Kubernetes dashboard, because we need to make some changes.</p><div><pre><code data-lang="bash">curl https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.4/aio/deploy/recommended.yaml --output kubernetes-dashboard.yaml
</code></pre></div><p>I spent a few days trying to figure out why the manifest did not work out of the box, it kept failing when trying to pull the docker image. I worked around this by doing two things -</p><ol><li>Ran <code>docker pull kubernetesui/dashboard:v2.0.0</code> to cache a local copy of the docker image.</li><li>Commented out the <code>imagePullPolicy: Always</code> in the manifest file under the <code>kubernetes-dashboard</code> deployment block.</li></ol><p>For the more K8s experienced among you, you may be wondering why I did not try using the Helm chart - I did. Kubernetes-dashboard needs to run two services - <code>dashboard-metrics-scraper</code> and <code>kubernetes-dashboard</code>. The Helm chart only seemed to bring up <code>kubernetes-dashboard</code>. I’m sure I must be doing something wrong, but at this point my patience was wearing thin and I just wanted to get on with it.</p><p>Ok, so now we have an edited manifest, let’s apply it.</p><div><pre><code data-lang="bash">kubectl apply -f kubernetes-dashboard.yaml
</code></pre></div><p>It takes a little bit of time for the dashboard to come up. You can amuse yourself by looking at the pods as they spin up as follows -</p><div><pre><code data-lang="bash">watch kubectl get pods -n kubernetes-dashboard
</code></pre></div><p>You can get details on a specific pod by running -</p><div><pre><code data-lang="bash">kubectl describe pod &lt;pod_name&gt; -n kubernetes-dashboard
</code></pre></div><p>You can also tail logs on a specific pod by running -</p><div><pre><code data-lang="bash">kubectl -n kubernetes-dashboard logs &lt;pod_name&gt; -f
</code></pre></div><p>Once you see the dashbaord services up and running, let’s figure out how we actually get access to the dashboard UI.</p><p>We’ll assume that you haven’t configured kubectl on your local machine and are instead, running all these commands from your (headless) raspberry pi.</p><p>Run <code>kubectl proxy</code> first. This exposes the cluster API server over HTTP to the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://amithm.ca/2020/10/kubernetes-raspberrypi-homelab/">https://amithm.ca/2020/10/kubernetes-raspberrypi-homelab/</a></em></p>]]>
            </description>
            <link>https://amithm.ca/2020/10/kubernetes-raspberrypi-homelab/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25061097</guid>
            <pubDate>Wed, 11 Nov 2020 18:21:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Overview of 3D meshing methods using open source tools]]>
            </title>
            <description>
<![CDATA[
Score 27 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25060942">thread link</a>) | @alibabaSX
<br/>
November 11, 2020 | https://www.sesamx.io/blog/3d_mesh_with_free_tools/ | <a href="https://web.archive.org/web/*/https://www.sesamx.io/blog/3d_mesh_with_free_tools/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
  <div>
    <div>
      <article role="main">
        

<p>The Internet is full of beautiful 3D mesh examples, but it is difficult to get clues
about how they were constructed. In fact, <strong>creating a good 3D mesh can be very
painful without the right tools or training</strong>. Furthermore, most of commercial
pre-processing software have been improving and propose powerful 3D meshing algorithm,
capable of building hybrid or full hexahedron mesh. <strong>The aim of
this article is to give an overview of the constraints involved when building
a 3D mesh for structural finite element, as well as exposing various meshing
methods relying on free and open source tools</strong>.</p>

<h2 id="introduction">Introduction</h2>

<p>Before we start, let’s give some background information about 3D mesh
construction. Usually a 3D mesh can be composed of 4 types of elements:</p>

<ul>
<li><p>tetrahedron (4 corners),</p></li>

<li><p>wedge (6 corners),</p></li>

<li><p>hexahedron (8 corners),</p></li>

<li><p>and rarely pyramids (5 corners).</p></li>
</ul>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/3d_mesh_element_types.png" alt="3D mesh element types"></p>

<p><strong>The goal is to get a mesh with the lowest number of
degrees of freedom (“dofs”), while maintaining a good representative capability</strong>.
We may be
tempted to race for tetrahedrons, which have only 4 nodes (for a linear
element). But reality is more involved: <strong>certain types of elements behave better
than others</strong>. Without entering to much into the details, we can provide some hints
about this:</p>

<ul>
<li><p><strong>4-node tetrahedron (linear element) must be avoided as mush as possible</strong>. It
behaves poorly and a lot of them are needed to get meaningful results. If you
have no other choice, try to convert them to 10-node tetrahedron which is much
better.</p></li>

<li><p>if you want to stay with linear elements, <strong>you must aim towards hexahedron</strong>. Most
finite element software (you can guess that SesamX is part of them) propose an
improved version of the hexahedron element. That makes it a better choice than
the other elements.</p></li>

<li><p><strong>6-node wedge (linear element)</strong> is better than 4-node tetrahedron but worse than
8-node hexahedron. <strong>It is ok to use them, but go for hexahedron wherever possible</strong>.</p></li>

<li><p>I have not tested the pyramid enough to give relevant advice. Nevertheless,
from the fact that this element is seldom used in 3D mesh, this article will not
shed light on it.</p></li>
</ul>

<p>It is easy to build a full tetrahedron mesh using an automatic mesher
(and it is widespread among various software). On the contrary, <strong>full hexahedron or
hybrid automatic meshers are more involved and harder to find</strong> (you can find them
among commercial solutions but almost not among free ones).</p>

<p><strong>However, using only free and open source tools, we are still able to build
quality 3D meshes</strong>.</p>

<p>The remaining of <strong>this article exhibits 4 methods to build 3D meshes using Salome or
Gmsh</strong>. The goal is not to enter into every detail about the options used, but to
give an overview of how 3D meshes can be built. Whatever the tool we use, changing
the element order is usually a trivial task (either linear or quadratic).
Therefore I will not detail it here. Instead, <strong>I will focus on controlling the
element shapes while meshing</strong>.</p>

<p>For each method, I provide a step by step guide with illustrative screenshots. And, when
appropriate, I also provide the final result file that you can edit and modify
on your own.</p>

<h2 id="geometry-used">Geometry used</h2>

<p>I will use the following con rod to showcase how to build each mesh.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/con_rod_geometry.PNG" alt="Con rod geometry"></p>

<p>One important feature to mention here, is that <strong>this con rod geometry is made of
a compound of 3 solids</strong> (this reason will make sense when talking about the hybrid
mesh generation method) corresponding to each color on the image above.</p>

<p>You can find the corresponding step file
<a href="https://www.sesamx.io/blog/files/008_3d_mesh_with_free_tools/con_rod_to_mesh.step">here</a>.</p>

<h2 id="full-tetrahedron-automatic-meshing-method">Full tetrahedron automatic meshing method</h2>

<p>As mentioned before, <strong>it is pretty straightforward to get a full tetrahedron<br>
mesh</strong>. To build this mesh, we use <strong>Salome</strong>.</p>

<p>First, we go to the geometry module and import the step file. The Salome tree
should look like this:</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/salome_import_tree.PNG" alt="Salome import tree"></p>

<p>Then we have to explode the compound geometry into its 3 sub-solids and create a
partition from these solids. <strong>This step is necessary to ensure that Salome will merge
coincident nodes from each solid faces</strong>. The result tree is then:</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/salome_partition_tree.PNG" alt="Salome partition"></p>

<p>Next we go to the mesh module, and create a new mesh on the partition. Under
algorithm we can select “NETGEN 1D-2D-3D” and under hypothesis “NETGEN 3D
Simple Parameters”. Finally we have to input the edge size that our elements should
have.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/tetrahedron_mesh_parameters.png" alt="Salome tetrahedron mesh parameters"></p>

<p>Eventually we have to right click on the mesh and hit “Compute”. The mesh should
look like this:</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/tetrahedron_mesh.PNG" alt="Salome tetrahedron mesh"></p>

<p>As you can see, the mesh is made of tetrahedron but also triangles and edges
elements. To get rid of the 2D and 1D elements, the first step is to
click on the mesh and select “Create Group”. A panel appear and we can create one
group containing all the 2D elements (as shown on the picture below) and
similarly for the 1D elements.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/salome_create_group.png" alt="Salome create group"></p>

<p>Next to delete these elements, we have to right
click on each group and select “Delete Group with Content”. <strong>And we get
the following full tetrahedron mesh</strong>.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/final_tetrahedron_mesh.PNG" alt="Salome final tetrahedron mesh"></p>

<p>Finally, <strong>to check that the mesh does not contain any duplicated nodes</strong> we have to
select the mesh and use “Controls / Node Controls / Double Nodes”.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/full_tet_double_nodes.PNG" alt="Tetrahedron duplicated nodes check"></p>

<p>If you want to manipulate this mesh, you can find the corresponding Salome database
<a href="https://www.sesamx.io/blog/files/008_3d_mesh_with_free_tools/full_tet_con_rod.hdf">here</a>.</p>

<h2 id="full-hexahedron-automatic-meshing-method">Full hexahedron automatic meshing method</h2>

<p>Next come the full hexahedron mesh. To build this mesh, we use <strong>Gmsh</strong>.</p>

<p>First, <strong>it is necessary to create a volume physical group containing the 3
solids of the model. It ensure afterwards that the mesh export will, in fact,
export only the 3D elements.</strong></p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/gmsh_volume_physical_group.PNG" alt="Gmsh volume physical group"></p>

<p>Next we go to “Tools / Options” then “Mesh / General” to select the meshing
parameters. We can
choose whatever makes it for the 2D algorithm, 3D algorithm and 2D recombination
algorithm. These parameters influence how the mesh is built, feel free to change
them to notice the difference in the mesh. As a first guess, we can
stay with “Delaunay” and “Blossom”. However, <strong>make sure to select “All Hexas” as the
“Subdivision algorithm” so that the volumes will be filled with hexahedron only.</strong></p>

<p>Finally under “Min/Max element size” we can fix the element size.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/gmsh_meshing_parameters.PNG" alt="Gmsh meshing parameters"></p>

<p>Eventually, we have to go back to the Gmsh tree and click “3D” under “Mesh” to
build the mesh.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/gmsh_mesh_creation.png" alt="Gmsh mesh creation"></p>

<p>We can check the mesh content under “Tools / Statistics”. As you can see, the mesh
is made of 1D, 2D and 3D-hexahedron elements. <strong>Because we have created a physical
group for the 3 volumes, only the 3D mesh will be exported.</strong></p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/final_hexaedron_mesh.PNG" alt="Gmsh mesh creation"></p>

<p>If you have troubles visualizing the 3D elements, you can adjust the visibility
parameters in the “Mesh” options window under the “Visibility” tab.</p>

<p>Unfortunately, there is a trap here. <strong>The mesh obtained has duplicated nodes</strong>
at the interfaces between the 3 solids. To visualize them, we can export the mesh
as a .med file, import it in Salome, and use the “double nodes” tool mentioned
previously.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/full_hexahedron_duplicated_nodes.PNG" alt="Hexahedron duplicated nodes"></p>

<p>Finally, to solve this issue, we have to use the “Merge Nodes” tool under
“Modification / Transformation”.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/full_hexahedron_remove_duplicated.PNG" alt="Hexahedron remove duplicated nodes"></p>

<h2 id="hybrid-meshing-method">Hybrid meshing method</h2>

<p>Next <strong>I am showcasing how to build an efficient hybrid mesh with Salome.</strong> This
method is my favorite because <strong>it leads to a well structured mesh, which can
capture more efficiently the details of the geometry</strong> (if you have a close look
to the automatic tetrahedron and hexadreon meshes, you can see that the fillet
is not always “well captured” for instance). The drawback of this method is that it does not
lead to a full hexahedron mesh but an hybrid mesh made with a majority of
hexahedrons, and a minority of wedges used to fill gaps.</p>

<p><strong>The 3D mesh will be built first by meshing 2D surfaces and then by extruding
them.</strong> To make this process workable, the geometry has been split into 3 solids
beforehand. Each of these solids can then be meshed as an extrusion of the surface
meshes.</p>

<table>
<thead>
<tr>
<th><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/hybride_mesh_solid_1.PNG" alt="Solid 1"></th>
<th><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/hybride_mesh_solid_2.PNG" alt="Solid 2"></th>
<th><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/hybride_mesh_solid_3.PNG" alt="Solid 3"></th>
</tr>
</thead>

<tbody>
<tr>
<td><center><em>Solid 1</em></center></td>
<td><center><em>Solid 2</em></center></td>
<td><center><em>Solid 3</em></center></td>
</tr>
</tbody>
</table>

<p>As explained for the full-tetrahedron mesh, <strong>we first need to explode the
compound geometry and build a partition.</strong></p>

<p>Then, in order to build the 2D meshes on the surfaces and the 3D extrusion meshes,
we need to extract (using explode) the relevant geometries from this partition:</p>

<ul>
<li>the 3 solids geometries,</li>
<li>the top face of solid 1 (red face on solid 1 image), that will drive the
3D mesh on solid 1,</li>
<li>the “fillet face” of solid 2 and 3 (red face on solid 3 image) that will drive
the 3D mesh on solid 2 and solid 3.</li>
</ul>

<p>The Salome tree should look like this:</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/salome_partition_tree_hybrid.PNG" alt="Salome tree for hybrid mesh"></p>

<p>Next, we go to the mesh module. The meshing process is the following:</p>

<ul>
<li><p>Create a mesh object and assign default 3D mesh parameters to the whole
partition,</p></li>

<li><p>Create 2 sub-meshes for the 2 surfaces to mesh,</p></li>

<li><p>Create 3 sub-meshes for the 3 solids to mesh.</p></li>
</ul>

<p><strong>The default 3D meshing parameters will not be used while computing the mesh,
because the sub-meshes definition will cover the whole partition.</strong> Nevertheless, Salome
still requires these default parameters.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/tetrahedron_mesh_parameters.png" alt="Salome import tree"></p>

<p>To create the faces sub-meshes, we have to right click on the mesh and
select “Create Sub-mesh”. We then need to select one of the faces and choose the
“NETGEN 1D-2D”
algorithm with “NETGEN 2D Simple Parameters”. Then we can input the element size
and <strong>make sure to check “Quad-dominated” (to avoid at most triangles)</strong>.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/face_submesh_parameters.png" alt="Salome face submesh parameters"></p>

<p>To compute the sub-mesh, we have to right click on it and select “Compute
Sub-mesh”. And we repeat these operations for the second face.</p>

<p><strong>Creating the 3D sub-meshes is similar.</strong> Once we have selected the solid to mesh,
we choose “Extrusion 3D” as the meshing algorithm and no hypothesis needs to be associated.
However, we have to provide the 1D algorithm and hypothesis to define
how the mesh extrusion should behave.</p>

<p>We select “Wire Discretisation” as the 1D algorithm, and the previous local length
used for the 2D sub-meshes as the hypothesis.</p>

<table>
<thead>
<tr>
<th><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/hybrid_3d_submesh_parameters.png" alt="Hybrid 3D submesh parameters"></th>
<th><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/hybrid_1d_submesh_parameters.png" alt="Hybrid 1D submesh parameters"></th>
</tr>
</thead>

<tbody>
<tr>
<td><center><em>3D submesh parameters</em></center></td>
<td><center><em>1D submesh parameters</em></center></td>
</tr>
</tbody>
</table>

<p>We repeat this for the 2 other solids. The mesh tree should look like this:</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/hybrid_mesh_tree.PNG" alt="Salome hybrid mesh tree"></p>

<p><strong>Before computing the mesh, we need to tell Salome in which order the sub-meshes
should be computed.</strong> To avoid meshing conflict while extruding, it is best in
our case, to fully mesh solid 1 before meshing the driving surface of solid 2
and solid 3. We have to right click on the mesh and select “Change sub-mesh Priority”.
The meshing order should be the following:</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/hybrid_mesh_order.PNG" alt="Salome hybrid mesh tree"></p>

<p>After …</p></article></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.sesamx.io/blog/3d_mesh_with_free_tools/">https://www.sesamx.io/blog/3d_mesh_with_free_tools/</a></em></p>]]>
            </description>
            <link>https://www.sesamx.io/blog/3d_mesh_with_free_tools/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25060942</guid>
            <pubDate>Wed, 11 Nov 2020 18:07:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fixing leaky logs: how to find a bug and ensure it never returns]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25060456">thread link</a>) | @pabloest
<br/>
November 11, 2020 | https://r2c.dev/blog/2020/fixing-leaky-logs-how-to-find-a-bug-and-ensure-it-never-returns/ | <a href="https://web.archive.org/web/*/https://r2c.dev/blog/2020/fixing-leaky-logs-how-to-find-a-bug-and-ensure-it-never-returns/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><div><div><div><blockquote>
<p><strong>TL;DR</strong> I lay out a case for moving security enforcement into the hands
of developers. I show how I and another developer at r2c successfully identified data
leakage in our logs, fixed the issue, and prevented it from happening in the
future. We did this <em>in a matter of hours, without assistance from our AppSec team</em>.</p>
</blockquote>
<h2>Introduction</h2>
<p>As a developer and engineering manager, I’ve become obsessed with finding ways
to rapidly solve security issues across the engineering organization without ever
needing to fully involve our security team.</p>
<p>Why is this important? I see multiple benefits:</p>
<ul>
<li>Fixing security issues is <em>fast</em>. So fast, in fact, that we can solve them in
minutes after identifying them, without security issues languishing for days
or weeks. In previous roles, I've seen internally known security issues lie
open with only obscurity protecting my organization from fallout.</li>
<li>When developers can solve security issues easily themselves, it frees the
security team to focus on “big picture” security. I want security engineers
to be thinking how to choose frameworks, set up tools, help with secure
architecture, and build defense-in-depth—not finding my last XSS mistake.</li>
</ul>
<p>I call this concept “self-service DevSec”.</p>
<p>In the rest of this blog post, I'll walk through a security bug we encountered
during the day-to-day course of regular development work. I'll discuss how we
discovered the issue, and how, within just a few hours, fixed the security
issue, and used Semgrep to prevent the bug from reoccurring.</p>
<p>Here's the story:</p>
<h2>Story</h2>
<p>Last month, I was debugging a Flask web-app authentication workflow with
Clara McCreery, another engineer at r2c. Like many engineers faced with
a confusing debugging problem, one of our first steps was to throw the web-app
into debug logging.</p>
<p>Specifically, we wanted to know what was going on with our database operations,
so we set our ORM (in this case, we use SQLAlchemy) into INFO-level logging with:</p>
<div data-language="py"><pre><code>logging<span>.</span>getLogger<span>(</span><span>"sqlalchemy.engine.base.Engine"</span><span>)</span><span>.</span>setLevel<span>(</span>logging<span>.</span>INFO<span>)</span></code></pre></div>
<p>This configures SQLAlchemy to log all SQL statements, together with passed
parameters. Let's look at some of the output we saw:</p>
<div data-language="shell-session"><pre><code><span>INFO:werkzeug:127.0.0.1 - - [25/Sep/2020 11:50:01] "POST /api/auth/authenticate HTTP/1.1" 200 -
INFO:sqlalchemy.engine.base.Engine:BEGIN (implicit)
INFO:sqlalchemy.engine.base.Engine:SELECT token.id AS token_id, token.token AS token_token, token.name AS token_name
FROM token
WHERE token.token = %(token_1)s
 LIMIT %(param_1)s
</span><span>INFO<span>:</span><span>sqlalchemy.engine.base.Engine:{'token_1': </span></span><span><span>$</span><span>2a<span>$10</span><span>$KVsyW1jjKn</span>.pvkVi3w9Rn.1mwnZFd7F2SFveGDG8flIhbe.MoJH4G, <span>'param_1'</span><span>:</span> <span>1</span><span>}</span></span></span></code></pre></div>
<p>...Uh-oh.</p>
<p>We definitely shouldn’t be logging tokens (even if they're securely hashed).
(In this example the actual token value has been changed to protect
the innocent.)</p>
<h2>Let’s make a plan</h2>
<p>At this point we’ve identified a security issue, and we want to stomp it out
while preserving our ability to inspect logs. Our plan:</p>
<ol>
<li>Mitigate the immediate security issue.</li>
<li>Find a permanent solution to the problem that’s future proof. A permanent
solution means a baked-in change to our systems. Ideally this solution is
automated and seamless across our entire organization.</li>
<li>Add a mechanism to enforce our solution’s use organization-wide.</li>
</ol>
<p>In the rest of this post, I’ll walk you through how we addressed each step.
Notably, we were able to accomplish this entire flow in a couple hours, without
engaging the security team at all.</p>
<h3>1. Mitigation</h3>
<p>Mitigation here was fairly straightforward, as we already knew the root cause
of our problem. We can quickly revert our logging change. Then we can do
a quick audit of our logs to ensure that only development test tokens were
leaked.</p>
<h3>2. The permanent solution</h3>
<p>How do we prevent SQLAlchemy from logging sensitive data?</p>
<p><em>A valiant attempt</em></p>
<p>Step 1 was to read the docs. A quick web search of “sqlalchemy hide parameters
in engine logging” linked us to the SQLAlchemy <a href="https://docs.sqlalchemy.org/en/13/core/engines.html" target="_blank" rel="noopener">Engine
documentation</a>. A detailed
read later, we found the <code>hide_parameters</code> flag, which prevents the logging
framework from emitting <em>any</em> parameters in logs or exceptions.</p>
<p>While this certainly would prevent our security issue, it was too blunt of a hammer
for us: we wanted to know (for example) database IDs, and the like, for debugging.</p>
<p><em>The real solution</em></p>
<p>We then inspected the relevant SQLAlchemy <a href="https://github.com/sqlalchemy/sqlalchemy/tree/master/lib/sqlalchemy" target="_blank" rel="noopener">source
code</a>. The
relevant code is in <code>sqlalchemy/engine/base.py</code>:</p>
<div data-language="py"><pre><code>    <span>if</span> self<span>.</span>_echo<span>:</span>
        self<span>.</span>engine<span>.</span>logger<span>.</span>info<span>(</span>statement<span>)</span>
        <span>if</span> <span>not</span> self<span>.</span>engine<span>.</span>hide_parameters<span>:</span>
            self<span>.</span>engine<span>.</span>logger<span>.</span>info<span>(</span>
                <span>"%r"</span><span>,</span>
                sql_util<span>.</span>_repr_params<span>(</span>
                    parameters<span>,</span> batches<span>=</span><span>10</span><span>,</span> ismulti<span>=</span>context<span>.</span>executemany
                <span>)</span><span>,</span>
            <span>)</span></code></pre></div>
<p><code>sql_util._repr_params</code>, in turn, runs:</p>
<div data-language="py"><pre><code><span>def</span> <span>_repr_params</span><span>(</span>self<span>,</span> params<span>,</span> typ<span>)</span><span>:</span>
    trunc <span>=</span> self<span>.</span>trunc
    <span>if</span> typ <span>is</span> self<span>.</span>_DICT<span>:</span>
        <span>return</span> <span>"{%s}"</span> <span>%</span> <span>(</span>
            <span>", "</span><span>.</span>join<span>(</span>
                <span>"%r: %s"</span> <span>%</span> <span>(</span>key<span>,</span> trunc<span>(</span>value<span>)</span><span>)</span>
                <span>for</span> key<span>,</span> value <span>in</span> params<span>.</span>items<span>(</span><span>)</span>
            <span>)</span>
        <span>)</span>
    <span>.</span><span>.</span><span>.</span></code></pre></div>
<p>Investigating <code>trunc</code>, we found that it converts the parameter value by
truncating the parameter’s <code>repr</code> to a maximum number of characters.</p>
<p>This meant that we should override the <code>repr</code> method of the parameter object to
prevent sensitive logging.</p>
<p>At this point, like good engineers, we took the lazy route: stand on your
peers’ shoulders. I found <a href="https://github.com/sqlalchemy/sqlalchemy/issues/4806" target="_blank" rel="noopener">this GitHub
issue</a>, where <a href="https://techspot.zzzeek.org/" target="_blank" rel="noopener">Mike
Bayer</a> had already posted a nice solution.</p>
<p>Some shameless copying later (and adding some types to make <code>mypy</code> happy), we
had <a href="https://gist.github.com/nbrahms/2fee940f4d87f09ffc3823be5a334cf3" target="_blank" rel="noopener">this
Gist</a>. The
key code is:</p>
<div data-language="py"><pre><code><span>class</span> <span>ObfuscatedString</span><span>(</span>types<span>.</span>TypeDecorator<span>)</span><span>:</span>
    <span>"""
    String column type for use with SQLAlchemy models whose
    content should not appear in logs or exceptions
    """</span>

    impl <span>=</span> types<span>.</span>String

    <span>class</span> <span>Repr</span><span>(</span><span>str</span><span>)</span><span>:</span>
        <span>def</span> <span>__repr__</span><span>(</span>self<span>)</span> <span>-</span><span>&gt;</span> <span>str</span><span>:</span>
            <span>return</span> <span>"********"</span>

    <span>def</span> <span>process_bind_param</span><span>(</span>self<span>,</span> value<span>:</span> Optional<span>[</span><span>str</span><span>]</span><span>,</span> dialect<span>:</span> Any<span>)</span> <span>-</span><span>&gt;</span> Optional<span>[</span>Repr<span>]</span><span>:</span>
        <span>return</span> self<span>.</span>Repr<span>(</span>value<span>)</span> <span>if</span> value <span>else</span> <span>None</span>

    <span>def</span> <span>process_result_value</span><span>(</span>
        self<span>,</span> value<span>:</span> Optional<span>[</span>Repr<span>]</span><span>,</span> dialect<span>:</span> Any
    <span>)</span> <span>-</span><span>&gt;</span> Optional<span>[</span><span>str</span><span>]</span><span>:</span>
        <span>return</span> <span>str</span><span>(</span>value<span>)</span> <span>if</span> value <span>else</span> <span>None</span>


<span>setattr</span><span>(</span>db<span>,</span> <span>"ObfuscatedString"</span><span>,</span> ObfuscatedString<span>)</span></code></pre></div>
<p>What does this code accomplish? It replaces our original <code>str</code> parameters
with a new <code>ObfuscatedString.Repr</code> parameter. When logged (or when emitted
into an exception message), the string is replaced by our <code>********</code>
obfuscation sentinel. Since the parameter is still bound as a raw string (via
<code>impl = types.String</code>), the correct value is still inserted and selected from
the database.</p>
<p>To use this new column type, we set our <code>token</code>’s column type:</p>
<div data-language="py"><pre><code><span>class</span> <span>Token</span><span>(</span>db<span>.</span>Model<span>)</span><span>:</span>
    <span>.</span><span>.</span><span>.</span>
    token <span>=</span> db<span>.</span>Column<span>(</span>db<span>.</span>ObfuscatedString<span>,</span> <span>.</span><span>.</span><span>.</span><span>)</span>
    <span>.</span><span>.</span><span>.</span></code></pre></div>
<p>We then re-enabled INFO logging, and checked that we were properly obfuscating
text:</p>
<div data-language="shell-session"><pre><code><span>INFO:werkzeug:127.0.0.1 - - [25/Sep/2020 13:48:55] "GET /api/agent/deployments/1/policies HTTP/1.1" 200 -
INFO:sqlalchemy.engine.base.Engine:BEGIN (implicit)
INFO:sqlalchemy.engine.base.Engine:SELECT token.id AS token_id, token.token AS token_token, token.name AS token_name
FROM token
WHERE token.token = %(token_1)s
 LIMIT %(param_1)s
INFO:sqlalchemy.engine.base.Engine:{'token_1': ********, 'param_1': 1}</span></code></pre></div>
<p>For completeness, we also validated in our development database console that the
correct values were stored and retrieved.</p>
<p>Great success! 🚢 Ship it.</p>
<h3>3. Enforcement</h3>
<p>It was tempting to rest on our laurels here. We had solved our security issue for
the time being, and we could get back to debugging our original auth issue.</p>
<p>But we wanted to guarantee that <em>we would never see this issue again</em>. How would we do
this?</p>
<p>Here are some ideas that I’m sure we’ve all encountered before:</p>
<ol>
<li>Block all commits to SQLAlchemy models on security review!</li>
<li>Host a yearly security training for all devs, including the pitfalls of
logging sensitive data!</li>
<li>Audit logs weekly!</li>
<li>File an issue with your SAST provider, demanding they add checks to catch
sensitively logged data!</li>
</ol>
<p>If there is a central take-away from this blog post, it is this: these are not
ideal solutions:</p>
<ol>
<li>Blocking commits introduces needless friction into the development
process, slows development velocity, and needlessly distracts the security
team.</li>
<li>Security trainings are an important component to a security program, and
necessary to keep developers aware of evolving security threats, but humans
have fallible memory, and we can forget things we've heard months or even
days in the past.</li>
<li>Regular audits, like blocking commits, introduce a heavy workload on an
almost certainly overloaded security team.</li>
<li>Your SAST provider will certainly welcome your suggestion, but you will be
beholden to their software release cycle, and may not see checks be
available for months; furthermore, if your issue is domain-specific, it may
not even make sense for a check to be implemented within a generalist product.</li>
</ol>
<p>Fortunately, Semgrep gave us a simple solution here: Define an
<em>invariant</em> in your code, and <em>enforce</em> it using a Semgrep scan on every CI
run.</p>
<p>At r2c, we use GitHub Actions to run Semgrep on every merge request. We define
what checks Semgrep should run using <em>a managed policy</em>, a list of rules and
notification settings managed by <a href="https://semgrep.dev/" target="_blank" rel="noopener">semgrep.dev</a>.</p>
<p>To guarantee our code against future issues, I went to
<a href="https://semgrep.dev/editor" target="_blank" rel="noopener">semgrep.dev/editor</a> and wrote <a href="https://semgrep.dev/s/nbrahms:obfuscate-sensitive-string-columns-2" target="_blank" rel="noopener">a quick rule</a>
to detect potential insecurely logged SQLAlchemy columns.</p>
<p>Here's the rule definition in Semgrep's YAML definition language:</p>
<div data-language="yaml"><pre><code><span>rules</span><span>:</span>
<span>-</span> <span>id</span><span>:</span> obfuscate<span>-</span>sensitive<span>-</span>string<span>-</span>columns
  <span>patterns</span><span>:</span>
    <span>-</span> <span>pattern</span><span>:</span> <span>|</span><span>
        $COLUMN = db.Column(db.String, ...)</span>
    <span>-</span> <span>metavariable-regex</span><span>:</span>
        <span>metavariable</span><span>:</span> $COLUMN
        <span>regex</span><span>:</span> <span>'.*(?&lt;![A-Za-z])(token|key|email|secret)(?![A-RT-Za-rt-z]).*'</span>
  <span>message</span><span>:</span> <span>|</span><span>
    '$COLUMN' may expose sensitive information in logs and exceptions. Use
    'db.ObfuscatedString' instead of 'db.String'.</span>
  <span>severity</span><span>:</span> WARNING</code></pre></div>
<p>What does this rule do? Let’s break it down:</p>
<ul>
<li><code>id</code>: We give our rule a concise descriptive ID for easy reference by any developer who
sees it pop up in their editor or CI output.</li>
<li>
<p><code>patterns</code>: This is composed of two parts:</p>
<ul>
<li><code>pattern</code>: This expression tells Semgrep to …</li></ul></li></ul></div></div></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://r2c.dev/blog/2020/fixing-leaky-logs-how-to-find-a-bug-and-ensure-it-never-returns/">https://r2c.dev/blog/2020/fixing-leaky-logs-how-to-find-a-bug-and-ensure-it-never-returns/</a></em></p>]]>
            </description>
            <link>https://r2c.dev/blog/2020/fixing-leaky-logs-how-to-find-a-bug-and-ensure-it-never-returns/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25060456</guid>
            <pubDate>Wed, 11 Nov 2020 17:29:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Doomsday prepping – Disaster planning for less crazy folk (2016)]]>
            </title>
            <description>
<![CDATA[
Score 168 | Comments 111 (<a href="https://news.ycombinator.com/item?id=25060418">thread link</a>) | @VBprogrammer
<br/>
November 11, 2020 | https://lcamtuf.coredump.cx/prep/ | <a href="https://web.archive.org/web/*/https://lcamtuf.coredump.cx/prep/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

 

<a name="1"></a>
<h2>1. Introduction <span>[<a href="#1">link</a>]</span></h2>

<p>
The prepper culture begs to be taken with a grain of salt. In a sense, it has
all the makings of a doomsday cult: a tribe of unkempt misfits who hoard gold
bullion, study herbalism, and preach about the imminent collapse of our society.
</p>

<p>
Today, we see such worries as absurd. It's not that life-altering disasters are
rare: every year, we hear about millions of people displaced by wildfires, earthquakes,
hurricanes, or floods. Heck, not a decade goes by without at least one first-class
democracy lapsing into armed conflict or fiscal disarray. But having grown up in a period
of unprecedented prosperity and calm, we take our way of life for granted - and find
it difficult to believe that an episode of bad weather or a currency crisis could
upend our lives for good.
</p>

<p>
I suspect that we dismiss such hazards not only because they seem surreal, but also because
worrying about them makes us feel helpless and lost. What's more, we follow the same instincts
to tune out far more pedestrian and avoidable risks; for example, 
most of us don't plan ahead for losing a job, for dealing with a week-long water outage, or
for surviving the night if our home goes up in smoke.
</p>

<p>
For many, the singular strategy for dealing with such dangers is to pray for the
government to bail us out. But no matter if our elected officials prefer to school us with
passages from
<a href="https://smile.amazon.com/dp/0156334607">Milton
Friedman</a> or from
<a href="https://smile.amazon.com/dp/0486477487" title="I'm sorry... I'm really sorry!">Thomas
Piketty</a>, the hard truth is that no state can provide a robust safety net for all
of life's likely contingencies; in most places, government-run social programs are severely deficient in funding, in
efficiency, and in scope. Large-scale disasters pit us against even worse odds; from New Orleans in 2005 to
Fukushima in 2011, there are countless stories of people left behind due to political dysfunction, poorly
allocated resources, or lost paperwork.
</p>

<p>
And so, the purpose of this guide is to combat the mindset of learned helplessness by
promoting simple, level-headed, personal preparedness techniques that are easy to
implement, don't cost much, and will probably help you cope with whatever life throws your way.
</p>

<p>
Oh, one thing: in contrast to most other docs of its kind, this page an
unadulterated labor of love; there are no affiliate links, paid product placements, or ads anywhere in the guide.
</p>

<a name="2"></a>
<h2>2. Mapping out the unknown <span>[<a href="#2">link</a>]</span></h2>

<p>
Effective preparedness can be simple, but it has to be rooted in an honest and
systematic review of the risks you are likely to face. Plenty of excited newcomers begin
by shopping for ballistic vests and night vision goggles; they would be better
served by grabbing a fire extinguisher, some bottled water, and then putting the rest of
their money in a rainy-day fund.
</p>

<p>
To maintain sanity while trying to enumerate risks, I found that it's best to focus on
broad outcomes instead of trying to track down every single way for things to go south.
Say, it should not matter if you are laid off because of a downsizing, because
your new boss hates you, or because they finally catch you stealing paperclips. The
outcome is the same: you are out of a job and urgently need a way to pay your bills.
</p>

<p>
Another insidious distraction is the desire to immediately figure out how to respond to all the scenarios
we end up dreaming of. Let's save that for later; by prematurely focusing on the second half of the
problem, we may end up glossing over some of the less tractable scenarios - or make
haphazard assumptions that will cloud our judgment in other ways.
</p>

<p>
I also found that to come up with a rational threat model, we need to think of "risk" as a product of
both the probability and the consequences of a given event. By that metric, stubbed toes
and zombie outbreaks are equally uninteresting; one of them has nearly zero significance,
the other, nearly zero odds.
</p>

<p>
What else? Ah, right: the final piece of advice I have is to keep things uncomplicated. There are
popular doomsday predictions that deal with cutting-edge particle physics, god-like computer
hackers, vast government conspiracies, or extraterrestrial messages hidden in pop songs. I suppose
we can't <i>really</i> rule that stuff out, but historical data suggests that there's a lot more
merit in worrying about falling off a ladder or getting hit by a car.
</p>

<p>
All right! With these caveats in mind, let's go over some canonical scenarios that are worth thinking about.
</p>

<a name="2.1"></a>
<h3>2.1. Problem space #1: Small-scale events <span>[<a href="#2.1">link</a>]</span></h3>

<p>
It's always fun to speculate about solar flares and supervolcanoes; it's far more mind-numbing to
seriously evaluate the consequences of backed up sewage or burst water mains. But in reality,
such unglamorous, small-scale incidents are far more likely to disrupt and reshape our
lives.
</p>

<p>
Broadly speaking, disastrous outcomes of such humdrum contingencies can be divided into
several groups:
</p>

<ul>

<li>
<p>
<b>Insolvency.</b>
  If a person over the age of 40 tells you that they have never lost a job, they are
  pretty lucky (or lying). Yet, the risk is seldom taken seriously; many middle-class,
  single-income families would be in deep trouble if it ever took them more than 2-3 months
  to find a new, equally well-paying gig.
</p>

</li><li>
<p>
<b>Disrupted access to water, food, energy, or transportation.</b>
  Substantial and prolonged outages happen everywhere; many of us will experience
  at least one at some point in our lives. A week without electricity may be just
  inconvenient and scary, especially in a high-rise or
  in a seedy neighborhood; but even a single hot day without potable water is really
  bad news.
</p>

</li><li>
<p>
<b>Loss of shelter.</b>
  Every year, there are over 350,000 house fires in the United States. Such accidents
  usually aren't deadly - but if you are unlucky, they can leave you stranded in the middle
  of the night in your PJs, with no documents or credit cards in hand.
</p>

</li><li>
<p>
<b>Unintentional injury.</b>
  Largely preventable and predictable incidents - such as falls, vehicle collisions, and poisonings -
  account for some 40 million ER visits annually. And lest you say people are simply too
  quick to rush to the hospital, said incidents also result in about 100,000 US
  deaths every year.
</p>

</li><li>
<p>
<b>Intentionally inflicted harm.</b>
  Violent crime is essentially <i>normal</i> almost everywhere in the world.
  In the US in the 90s, your lifetime likelihood of victimization was
  estimated to be around 80%; the odds of suffering criminal injury hovered at 40%. More recent
  research is hard to come by - but rest assured, life-threatening encounters remain a very real risk.
</p>

</li><li>
<p>
<b>Debilitating illness or death.</b>
  It's going to get you; maybe next week, maybe in 50 years. We can't really predict the day,
  but we can understand and meaningfully manage the impact it will have on those who depend on us -
  say, our stay-at-home partners or young kids.
</p>

</li></ul>

<p>
All in all, the risks discussed in this section have three defining characteristics: they are relatively
likely to happen; are strikingly easy to mitigate (we'll get into that soon); and tend to be so
unglamorous that they seldom make the cut in any "serious" guide to emergency preparedness.
</p>

<a name="2.2"></a>
<h3>2.2. Problem space #2: Mass calamities <span>[<a href="#2.2">link</a>]</span></h3>

<p>
If an errant backhoe took out the utilities for your block, you would probably head to the
grocery store to pick up bottled water (and use their restrooms, too). But if a
once-in-a-century storm damaged major roads and left half the city without running water, your
options wouldn't be as clear-cut.
</p>


<p>
That's why we have to look at larger-scale emergencies through somewhat different lens, taking into
account their likely magnitude, duration, and the nature of the forces at play. Some of the
plausible scenarios to think about include:
</p>

<ul>

<li>
<p>
<b>Natural disasters.</b>
  Common examples include floods, hurricanes, earthquakes, wildfires, and heatwaves. In some
  regions, such events are very rare; in others, they are almost guaranteed every decade or two.
</p>

</li><li>
<p>
<b>Industrial accidents.</b>
  Many people live in the proximity of heavy industries - say, refineries, freight railroads, or power plants.
  Depending on the type of industrial facilities nearby, you may want to evaluate the potential
  consequences of upwind and upstream explosions or chemical spills.
</p>

</li><li>
<p>
<b>Social unrest.</b>
  Riots are a distinct risk in many urban and suburban areas around the world. When angry mobs
  take it to the streets, widespread arson and violent crime are not unheard of, sometimes going
  on for days or weeks.
</p>

</li><li>
<p>
<b>Economic crises.</b>
  All highly developed countries go through cyclic recessions and periods of high unemployment;
  the US had about ten big ones in the past 100 years alone. Sometimes, such events
  are accompanied by bank runs and collapses of financial institutions; other times,
  they involve hyperinflation, product rationing, and currency controls.
</p>

</li><li>
<p>
<b>Pandemic.</b>
  It's been a while since the highly developed world experienced a devastating outbreak, but it
  may be premature to flat out dismiss the risk. In 1918, an unusual strain of flu managed to kill 75
  million people. Few years later, a mysterious sleeping sickness - probably also of viral origin -
  swept the globe, crippling millions, some for life. We aren't necessarily better prepared
  for similar events today.
</p>

</li><li>
<p>
<b>Terrorism or conventional war.</b>
  We think we would see it coming - but history shows that such events tend to catch nations
  off guard. These phenomena are noteworthy not only because of their immediate death toll,
  which can be relatively low - but because of the far-reaching and long-term socioeconomic
  disruption they can cause.
</p>

</li></ul>

<p>
Most of us will probably not get tangled up in a large-scale disaster of any sort, but it
is only wise to hedge your bets. There are countless examples to demonstrate that such events
happen often and can strike close to home - say:
</p>

<ul>

<li>
<p>
The EU debt crisis, from 2009 onward. A series of events that led to staggering unemployment rates
in Greece, deposit confiscations in Cyprus, and uncertain prospects for the entire
eurozone.
</p></li></ul></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://lcamtuf.coredump.cx/prep/">https://lcamtuf.coredump.cx/prep/</a></em></p>]]>
            </description>
            <link>https://lcamtuf.coredump.cx/prep/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25060418</guid>
            <pubDate>Wed, 11 Nov 2020 17:26:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Will Futhark Work on Apple Silicon?]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25060172">thread link</a>) | @Athas
<br/>
November 11, 2020 | https://futhark-lang.org/blog/2020-11-11-will-futhark-work-on-apple-silicon.html | <a href="https://web.archive.org/web/*/https://futhark-lang.org/blog/2020-11-11-will-futhark-work-on-apple-silicon.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
      

<p>
    Posted on November 11, 2020
    
        by Troels Henriksen
    
</p>

<p>Apple is coming out with computers that are basically ARM64, <a href="https://en.wikipedia.org/wiki/Think_different">but with a different ABI than existing ARM64 for some reason</a>. They call the architecture <a href="https://en.wikipedia.org/wiki/Mac_transition_to_Apple_Silicon">Apple Silicon</a>, which is a wonderful term that will undoubtedly never become dated or insufficiently precise.</p>
<p>Anyway, you see various posts such <a href="https://developer.r-project.org/Blog/public/2020/11/02/will-r-work-on-apple-silicon/">Will R Work on Apple Silicon?</a> where language developers answer whether their language will work on these new machines. Since these machines will support transparent emulation of x86, the simple answer is <em>yes</em>. Apple’s emulation was quite good during the PPC-to-x86 transition, so this is trustworthy. Of course, emulation is never going to be as fast as native compilation. For R, the problem is that they depend on some Fortran code, and there is <a href="https://developer.apple.com/forums/thread/651476">not yet a Fortran compiler available for Apple Silicon</a>.</p>
<p>Well, I can confirm that Futhark depends on absolutely no Fortran. Futhark compiles to C (or Python), and does not care about the specific target architecture. Therefore, Futhark programs should run fine on Apple Silicon. The bigger problem is that <a href="https://www.extremetech.com/computing/270902-apple-defends-killing-opengl-opencl-as-developers-threaten-revolt">Apple has deprecated OpenCL</a>, and <a href="https://www.provideocoalition.com/officially-official-nvidia-drops-cuda-support-for-macos/">does not support CUDA at all</a> due to being grumpy with NVIDIA, so there may eventually be no way to run Futhark <em>on a GPU</em> on macOS. It is unlikely that we will find the time to add a backend for Apple’s <a href="https://developer.apple.com/metal/">proprietary Metal API</a> that is supported <em>absolutely nowhere else</em>, but it’s possible that we’ll finish Futhark’s embryonic Vulkan backend. While macOS does not come bundled with Vulkan, <a href="https://github.com/KhronosGroup/MoltenVK">even Apple probably cannot hold back this eruption</a>.</p>
<p>And of course, the Futhark multi-core backend should run well on any Unix-like system.</p>


    </div></div>]]>
            </description>
            <link>https://futhark-lang.org/blog/2020-11-11-will-futhark-work-on-apple-silicon.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25060172</guid>
            <pubDate>Wed, 11 Nov 2020 17:04:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Xilinx-Samsung SmartSSD Computational Storage Drive Launched]]>
            </title>
            <description>
<![CDATA[
Score 104 | Comments 74 (<a href="https://news.ycombinator.com/item?id=25059946">thread link</a>) | @blopeur
<br/>
November 11, 2020 | https://www.servethehome.com/xilinx-samsung-smartssd-computational-storage-drive-launched/ | <a href="https://web.archive.org/web/*/https://www.servethehome.com/xilinx-samsung-smartssd-computational-storage-drive-launched/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <!-- image --><div><figure><a href="https://www.servethehome.com/wp-content/uploads/2020/11/smartssd-pr-1120x560-1.jpg" data-caption="Smartssd Pr 1120x560"><img width="696" height="461" src="https://www.servethehome.com/wp-content/uploads/2020/11/smartssd-pr-1120x560-1-696x461.jpg" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/smartssd-pr-1120x560-1-696x461.jpg 696w, https://www.servethehome.com/wp-content/uploads/2020/11/smartssd-pr-1120x560-1-400x265.jpg 400w, https://www.servethehome.com/wp-content/uploads/2020/11/smartssd-pr-1120x560-1-634x420.jpg 634w, https://www.servethehome.com/wp-content/uploads/2020/11/smartssd-pr-1120x560-1.jpg 800w" sizes="(max-width: 696px) 100vw, 696px" alt="Smartssd Pr 1120x560" title="Smartssd Pr 1120x560"></a><figcaption>Smartssd Pr 1120x560</figcaption></figure></div>
            <!-- content --><p>Computational storage is a small but growing segment of the market. To address this, the Samsung SmartSSD is being launched with a Xilinx Kintex FPGA inside to bring computational storage capabilities in a standard form factor. In this article, we are going to discuss how Xilinx and Samsung are delivering a computational storage platform.<span id="more-48283"></span></p>
<h2>Xilinx-Samsung SmartSSD Background</h2>
<p>First, why computational storage. One of the big drivers is that moving data, at high speeds, across systems can use a lot of power and consumes bandwidth. With computational storage, data can be processed without bringing it back to the main CPU.</p>
<figure id="attachment_48289" aria-describedby="caption-attachment-48289"><a href="https://www.servethehome.com/?attachment_id=48289" rel="attachment wp-att-48289"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Demand.png" alt="Xilinx SmartSSD Computational Storage Demand" width="1511" height="825" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Demand.png 1511w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Demand-400x218.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Demand-800x437.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Demand-696x380.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Demand-1068x583.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Demand-769x420.png 769w" sizes="(max-width: 1511px) 100vw, 1511px"></a><figcaption id="caption-attachment-48289">Xilinx SmartSSD Computational Storage Demand</figcaption></figure>
<p>Part of the other driver here is that Xilinx sees computational storage as becoming mainstream, projected to be 5% of the market in only a few years. For its part, Xilinx is covering a number of different types of accelerators aside form the Samsung SmartSSD including those from Pliops, ScaleFlux, and BittWare.</p>
<figure id="attachment_48288" aria-describedby="caption-attachment-48288"><a href="https://www.servethehome.com/?attachment_id=48288" rel="attachment wp-att-48288"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Becoming-Mainstream.png" alt="Xilinx SmartSSD Computational Storage Becoming Mainstream" width="1481" height="781" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Becoming-Mainstream.png 1481w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Becoming-Mainstream-400x211.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Becoming-Mainstream-800x422.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Becoming-Mainstream-696x367.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Becoming-Mainstream-1068x563.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Becoming-Mainstream-796x420.png 796w" sizes="(max-width: 1481px) 100vw, 1481px"></a><figcaption id="caption-attachment-48288">Xilinx SmartSSD Computational Storage Becoming Mainstream</figcaption></figure>
<p>The basic Samsung SmartSSD has two main sets of components. One is basically a 4TB Samsung V-NAND SSD. This includes a NAND controller, and we are told DRAM for the controller to use as well. The second part of the solution is a Xilinx Kintex FPGA with its own 4GB of memory.</p>
<figure id="attachment_48285" aria-describedby="caption-attachment-48285"><a href="https://www.servethehome.com/?attachment_id=48285" rel="attachment wp-att-48285"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components.png" alt="Samsung Xilinx SmartSSD Internal Components" width="1263" height="783" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components.png 1263w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components-400x248.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components-800x496.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components-696x431.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components-1068x662.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components-677x420.png 677w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components-356x220.png 356w" sizes="(max-width: 1263px) 100vw, 1263px"></a><figcaption id="caption-attachment-48285">Samsung Xilinx SmartSSD Internal Components</figcaption></figure>
<p>The basic flow is that commands can be issued to either the SSD or the FPGA portion of the drive and processing can occur at the FPGA instead of going back to the host system.</p>
<figure id="attachment_48286" aria-describedby="caption-attachment-48286"><a href="https://www.servethehome.com/?attachment_id=48286" rel="attachment wp-att-48286"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Operation.png" alt="Samsung Xilinx SmartSSD Internal Operation" width="1456" height="836" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Operation.png 1456w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Operation-400x230.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Operation-800x459.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Operation-696x400.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Operation-1068x613.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Operation-731x420.png 731w" sizes="(max-width: 1456px) 100vw, 1456px"></a><figcaption id="caption-attachment-48286">Samsung Xilinx SmartSSD Internal Operation</figcaption></figure>
<p>We are going to show an example later but a common question will be how are these programmed. One can use a standard storage stack or the OpenCL stack for computational storage aspects.</p>
<figure id="attachment_48291" aria-describedby="caption-attachment-48291"><a href="https://www.servethehome.com/?attachment_id=48291" rel="attachment wp-att-48291"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack.png" alt="Xilinx SmartSSD IP Runtime Stack" width="1470" height="723" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack.png 1470w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack-400x197.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack-800x393.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack-696x342.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack-1068x525.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack-854x420.png 854w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack-324x160.png 324w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack-533x261.png 533w" sizes="(max-width: 1470px) 100vw, 1470px"></a><figcaption id="caption-attachment-48291">Xilinx SmartSSD IP Runtime Stack</figcaption></figure>
<p>As one would expect with a FPGA, there is a tie in with partner IP solutions as well as those that Xilinx and Samsung will have.</p>
<figure id="attachment_48290" aria-describedby="caption-attachment-48290"><a href="https://www.servethehome.com/?attachment_id=48290" rel="attachment wp-att-48290"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development.png" alt="Xilinx SmartSSD IP Development" width="1487" height="727" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development.png 1487w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development-400x196.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development-800x391.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development-696x340.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development-1068x522.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development-859x420.png 859w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development-533x261.png 533w" sizes="(max-width: 1487px) 100vw, 1487px"></a><figcaption id="caption-attachment-48290">Xilinx SmartSSD IP Development</figcaption></figure>
<p>The Xilinx Storage Services (XSS) are offloads available for the platform. These include compression and crypto offloads.</p>
<figure id="attachment_48292" aria-describedby="caption-attachment-48292"><a href="https://www.servethehome.com/?attachment_id=48292" rel="attachment wp-att-48292"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Xilinx-Storage-Services.png" alt="Xilinx SmartSSD IP Xilinx Storage Services" width="1531" height="786" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Xilinx-Storage-Services.png 1531w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Xilinx-Storage-Services-400x205.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Xilinx-Storage-Services-800x411.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Xilinx-Storage-Services-696x357.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Xilinx-Storage-Services-1068x548.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Xilinx-Storage-Services-818x420.png 818w" sizes="(max-width: 1531px) 100vw, 1531px"></a><figcaption id="caption-attachment-48292">Xilinx SmartSSD IP Xilinx Storage Services</figcaption></figure>
<p>Taking the compression in VDO as an example, the following slides have the basic flow:</p>
<figure id="attachment_48294" aria-describedby="caption-attachment-48294"><a href="https://www.servethehome.com/?attachment_id=48294" rel="attachment wp-att-48294"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-1.png" alt="Xilinx SmartSSD VDO 1" width="1379" height="780" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-1.png 1379w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-1-400x226.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-1-800x453.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-1-696x394.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-1-1068x604.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-1-743x420.png 743w" sizes="(max-width: 1379px) 100vw, 1379px"></a><figcaption id="caption-attachment-48294">Xilinx SmartSSD VDO 1</figcaption></figure>
<p>For reads, the FPGA is used to decompress data at the SmartSSD. By putting the compression on the SSD, Xilinx says it can get better compression ratios.</p>
<figure id="attachment_48295" aria-describedby="caption-attachment-48295"><a href="https://www.servethehome.com/?attachment_id=48295" rel="attachment wp-att-48295"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-2.png" alt="Xilinx SmartSSD VDO 2" width="1447" height="784" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-2.png 1447w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-2-400x217.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-2-800x433.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-2-696x377.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-2-1068x580.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-2-775x420.png 775w" sizes="(max-width: 1447px) 100vw, 1447px"></a><figcaption id="caption-attachment-48295">Xilinx SmartSSD VDO 2</figcaption></figure>
<p>In terms of examples, we wanted to highlight one from Lewis Rhodes Labs where they are doing NPUSearch using computational storage. Effectively here the SmartSSDs are being used to scale out the number of accelerators with the number of SSDs. An application can send requests to the storage, data can be evaluated at the drives, and only results passed back to the main system.</p>
<figure id="attachment_48293" aria-describedby="caption-attachment-48293"><a href="https://www.servethehome.com/?attachment_id=48293" rel="attachment wp-att-48293"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search.png" alt="Xilinx SmartSSD Lewis Rhodes Labs Search" width="1538" height="837" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search.png 1538w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search-400x218.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search-800x435.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search-1536x836.png 1536w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search-696x379.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search-1068x580.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search-772x420.png 772w" sizes="(max-width: 1538px) 100vw, 1538px"></a><figcaption id="caption-attachment-48293">Xilinx SmartSSD Lewis Rhodes Labs Search</figcaption></figure>
<p>Since many of our readers will have noticed this, we asked about the PCIe Gen3 and we were told that there is a roadmap to the future.</p>
<h2>Final Words</h2>
<p>For STH readers, an immediate question is going to be why computational storage? Part of this model is that accelerators are tied to storage. For accelerator companies, this is great. Many of our readers though are going to ask about why not use DPUs instead. If you missed it&nbsp;<a href="https://www.servethehome.com/what-is-a-dpu-a-data-processing-unit-quick-primer/">What is a DPU A Data Processing Unit Quick Primer</a> is a good resource there. We asked since if the only goal is offload, and the SmartSSD is in many ways two devices that are co-packaged, then it could make sense to offload to a bigger chip. We were told that it is less expensive to use a smaller accelerator on each drive than to scale to a larger accelerator. This is one area that we know there is a lot of momentum behind each model in the data center. It will be interesting to see which ultimately wins.</p>
        </div></div>]]>
            </description>
            <link>https://www.servethehome.com/xilinx-samsung-smartssd-computational-storage-drive-launched/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25059946</guid>
            <pubDate>Wed, 11 Nov 2020 16:46:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Miniselect: Practical and Generic Selection Algorithms]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25059942">thread link</a>) | @cristaloleg
<br/>
November 11, 2020 | https://danlark.org/2020/11/11/miniselect-practical-and-generic-selection-algorithms/ | <a href="https://web.archive.org/web/*/https://danlark.org/2020/11/11/miniselect-practical-and-generic-selection-algorithms/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-555">

	

	
	<div>
		
<p>Today I present a big effort from my side to publish <a href="https://github.com/danlark1/miniselect">miniselect</a> — generic C++ library to support multiple selection and partial sorting algorithms. It is already <a href="https://github.com/ClickHouse/ClickHouse/pull/16825">used</a> in <a href="https://clickhouse.tech/">ClickHouse</a> with huge performance benefits. Exact benchmarks and results will be later in this post and now let’s tell some stories about how it all arose. I publish this library under Boost License and any contributions are highly welcome.</p>



<h2>It all started with sorting</h2>



<p>While reading lots of articles, papers, and posts from Hacker News, I found it pretty funny each several months new “shiny”, “fastest”, “generic” sorting algorithms to come or remembered from old papers such as the recent paper on <a href="https://blog.acolyer.org/2020/10/19/the-case-for-a-learned-sorting-algorithm/">learned sorting</a>, <a href="https://sortingsearching.com/2020/06/06/kirkpatrick-reisch.html">Kirkpatrick-Reisch</a> sort or <a href="https://news.ycombinator.com/item?id=14661659">pdqsort</a>. It is that we are essentially 65+ years into writing sorting algorithms, and we still find improvements. Shouldn’t sorting items be a “solved” problem by now? Unfortunately, not. New hardware features come, we find that sorting numbers can be actually done faster than best comparison <img src="https://s0.wp.com/latex.php?latex=O%28n+%5Clog+n%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="O(n \log n)" title="O(n \log n)"> time complexity and we still find improvements in sorting algorithms like avoiding <a href="https://www.researchgate.net/publication/301614727_BlockQuicksort_How_Branch_Mispredictions_don't_affect_Quicksort">branches in partitions</a> and trying to find good pivots as pdqsort does. Also, there are many open questions in that area as “what is the minimum number of comparisons needed?”.</p>



<p>Huge competition is still going on in sorting algorithms and I believe we are not near the optimal sorting and learned sorting looks like the next step. But it uses the fundamental fact that no one expects sorting to be completed in a couple of passes and we can understand something about data during first array passes. We will understand why it matters later.</p>



<p>My favorite general sorting is <a href="https://github.com/orlp/pdqsort">pdqsort</a>, it proves to be currently the best general sorting algorithm and it shows a significant boost over all standard sorts that are provided in C++. It is also <a href="https://docs.rs/pdqsort/1.0.3/pdqsort/">used</a> in Rust.</p>



<h2>Selection and Partial Sorting</h2>



<p>Nearly a couple of months ago I started thinking about a slightly different approach when it comes to sorting — partial sorting algorithms. It means that you don’t need to sort all <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="n" title="n"> elements but only find <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="k" title="k"> smallest and sort them. For example, it is widely used in SQL queries when you do <code>ORDER BY LIMIT N</code> and <code>N</code> is often small, from 1-10 to ideally couple of thousands, bigger values still happen but rare. And, oh god, how little engineering and theoretical research has been done there compared to full sorting algorithms. In fact, the question of specifically finding <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="k" title="k">th order statistics when <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="k" title="k"> is small is open and no good solution is presented. Also, partial sorting is quite easy to obtain after that, you need to sort the first <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="k" title="k"> elements by some sorting algorithm to get optimal <img src="https://s0.wp.com/latex.php?latex=O%28n+%2B+k+%5Clog+k%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="O(n + k \log k)" title="O(n + k \log k)"> comparisons and we will look at only one example when it is not the case. Yes, there are a bunch of median algorithms that can be generalized to find the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="k" title="k">th smallest element. So, what are they? Yeah, you may know some of them but let’s revise, it is useful to know your enemies.</p>



<h3>QuickSelect</h3>



<p>This is almost the very first algorithm for finding the <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="k" title="k">th smallest element, just do like <a href="https://en.wikipedia.org/wiki/Quicksort">QuickSort</a> but don’t go recursively in two directions, that’s it. Pick middle or even random element and partition by this element, see in which of two parts <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="k" title="k"> is located, update the one of the borders, voila, after maximum of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="n" title="n"> partitions you will find <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="k" title="k">th smallest element. Good news that on average it takes <img src="https://s0.wp.com/latex.php?latex=O%28n%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="O(n)" title="O(n)"> comparisons if we pick random pivot. That is because if we define <img src="https://s0.wp.com/latex.php?latex=C%28n%2C+k%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="C(n, k)" title="C(n, k)"> is the expected number of comparisons for finding <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="k" title="k">th element in <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="n" title="n"> elements and <img src="https://s0.wp.com/latex.php?latex=C%28n%29+%3D+%5Cmax_%7B1%7D%5E%7Bn%7D+C%28n%2C+k%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="C(n) = \max_{1}^{n} C(n, k)" title="C(n) = \max_{1}^{n} C(n, k)">, then during one stage we do <img src="https://s0.wp.com/latex.php?latex=n+-+1&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="n - 1" title="n - 1"> comparisons and uniformly pick any pivot, then even if we pick the biggest part on each step</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+C%28n%29+%5Cleq+n+-+1+%2B+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bn%2F2%7D%5E%7Bn+-+1%7D+C%28i%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="\displaystyle C(n) \leq n - 1 + \frac{1}{n}\sum_{n/2}^{n - 1} C(i)" title="\displaystyle C(n) \leq n - 1 + \frac{1}{n}\sum_{n/2}^{n - 1} C(i)"></p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+C%28n%29+%5Cleq+%28n+-+1%29+%2B+%5Cmathrm%7Bavg%7D%28C%28n%2F2%29%2C+%5Cldots%2C+C%28n%29%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="\displaystyle C(n) \leq (n - 1) + \mathrm{avg}(C(n/2), \ldots, C(n))" title="\displaystyle C(n) \leq (n - 1) + \mathrm{avg}(C(n/2), \ldots, C(n))"></p>



<p>If assuming by induction that <img src="https://s0.wp.com/latex.php?latex=C%28i%29+%5Cleq+4i&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="C(i) \leq 4i" title="C(i) \leq 4i"> with an obvious induction base, we get</p>



<p><img src="https://s0.wp.com/latex.php?latex=C%28n%29+%5Cleq+%28n+-+1%29+%2B+%5Cmathrm%7Bavg%7D%28C%28n%2F2%29%2C+%5Cldots%2C+C%28n%29%29+%5Cleq+n+-+1+%2B+4%283n%2F4%29+%3C+4n&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="C(n) \leq (n - 1) + \mathrm{avg}(C(n/2), \ldots, C(n)) \leq n - 1 + 4(3n/4) < 4n" title="C(n) \leq (n - 1) + \mathrm{avg}(C(n/2), \ldots, C(n)) \leq n - 1 + 4(3n/4) < 4n"> </p>



<p>Bad news is that the worst case will still be <img src="https://s0.wp.com/latex.php?latex=O%28n%5E2%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="O(n^2)" title="O(n^2)"> if we are unfortunate and always pick the biggest element as a pivot, thus partitioning .</p>



<p>In that sense that algorithm provides lots of pivot “strategies” that are used nowadays, for example, picking pivot as a <img src="https://s0.wp.com/latex.php?latex=n%2F2&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="n/2" title="n/2"> element of the array or picking pivot from 3 random elements . Or do like <code>std::nth_element</code> from libcxx — choose the middle out out of <img src="https://s0.wp.com/latex.php?latex=A%5B0%5D%2C+A%5Bn%2F2%5D%2C+A%5Bn+-+1%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="A[0], A[n/2], A[n - 1]" title="A[0], A[n/2], A[n - 1]">.</p>



<p>I decided to visualize all algorithms I am going to talk about today, so quickselect with a median of 3 strategy on random input looks something like this:</p>



<figure><img data-attachment-id="579" data-permalink="https://danlark.org/nth-element-clang-2020-11-09_11-18-38/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/11/nth-element-clang-2020-11-09_11.18.38.gif" data-orig-size="1720,1034" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nth-element-clang-2020-11-09_11.18.38" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/11/nth-element-clang-2020-11-09_11.18.38.gif?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/11/nth-element-clang-2020-11-09_11.18.38.gif?w=750" src="https://danlarkorg.files.wordpress.com/2020/11/nth-element-clang-2020-11-09_11.18.38.gif?w=1024" alt=""><figcaption>nth_element in libcxx, median of 3 strategies</figcaption></figure>



<p>And random pivot out of 3 elements works similar</p>



<figure><img data-attachment-id="581" data-permalink="https://danlark.org/median-of-3-random-2020-11-09_11-06-02/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/11/median-of-3-random-2020-11-09_11.06.02.gif" data-orig-size="1720,1034" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="median-of-3-random-2020-11-09_11.06.02" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/11/median-of-3-random-2020-11-09_11.06.02.gif?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/11/median-of-3-random-2020-11-09_11.06.02.gif?w=750" src="https://danlarkorg.files.wordpress.com/2020/11/median-of-3-random-2020-11-09_11.06.02.gif?w=1024" alt=""><figcaption>Finding median in median of 3 random algorithm</figcaption></figure>



<p>For a strategy like <a href="https://github.com/llvm/llvm-project/blob/3ed89b51da38f081fedb57727076262abb81d149/libcxx/include/algorithm#L5159">libcxx</a> (C++ llvm standard library) does, there are quadratic counterexamples that are pretty easy to detect, such patterns also appear in real data. The counterexample looks like that:</p>



<figure><div>
<div id="gist106376435">
    <div>
      <div>
        

      </div>
      
    </div>
</div>

</div></figure>



<figure><img data-attachment-id="584" data-permalink="https://danlark.org/nth_element_clang_median_3_killer-2020-11-10_22-55-30/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/11/nth_element_clang_median_3_killer-2020-11-10_22.55.30.gif" data-orig-size="1720,1034" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nth_element_clang_median_3_killer-2020-11-10_22.55.30" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/11/nth_element_clang_median_3_killer-2020-11-10_22.55.30.gif?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/11/nth_element_clang_median_3_killer-2020-11-10_22.55.30.gif?w=750" src="https://danlarkorg.files.wordpress.com/2020/11/nth_element_clang_median_3_killer-2020-11-10_22.55.30.gif?w=1024" alt=""><figcaption>std::nth_element in libcxx for Medianof3Killer</figcaption></figure>



<p>This is definitely quadratic. By the way, this is perfectly ok with the C++ standard wording as it says:</p>



<figure><img data-attachment-id="587" data-permalink="https://danlark.org/2020-11-10-230126_795x63_scrot/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-230126_795x63_scrot.png" data-orig-size="795,63" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2020-11-10-230126_795x63_scrot" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-230126_795x63_scrot.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-230126_795x63_scrot.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-230126_795x63_scrot.png?w=795" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-230126_795x63_scrot.png 795w, https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-230126_795x63_scrot.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-230126_795x63_scrot.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-230126_795x63_scrot.png?w=768 768w" sizes="(max-width: 795px) 100vw, 795px"><figcaption><a href="https://eel.is/c++draft/alg.nth.element#5">https://eel.is/c++draft/alg.nth.element#5</a></figcaption></figure>



<h2>Median of Medians</h2>



<p>For a long time, computer scientists thought that it is impossible to find medians in worst-case linear time, however, Blum, Floyd, Pratt, Rivest, Tarjan came up with BFPRT algorithm or like sometimes it is called, median of medians algorithm.</p>



<p>Median of medians algorithm: Given array <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="A" title="A"> of size <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="n" title="n"> and integer <img src="https://s0.wp.com/latex.php?latex=k+%5Cleq+n&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="k \leq n" title="k \leq n">,</p>



<ol><li>Group the array into <img src="https://s0.wp.com/latex.php?latex=n%2F5&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="n/5" title="n/5"> groups of size 5 and find the median of each group. (For simplicity, we will ignore integrality issues.)</li><li>Recursively, find the true median of the medians. Call this <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="p" title="p">.</li><li>Use <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="p" title="p"> as a pivot to partition the array.</li><li>Recurse on the appropriate piece.</li></ol>



<p>When we find the median <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="p" title="p"> of <img src="https://s0.wp.com/latex.php?latex=g+%3D+n%2F5&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="g = n/5" title="g = n/5"> groups, at least <img src="https://s0.wp.com/latex.php?latex=g%2F2&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="g/2" title="g/2"> of them have at least 3 out of 5 elements that are smaller or equal than <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="p" title="p">, that said the biggest out of 2 partitioned chunks have size <img src="https://s0.wp.com/latex.php?latex=7n%2F10&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="7n/10" title="7n/10"> and we have the reccurence</p>



<p><img src="https://s0.wp.com/latex.php?latex=C%28n%29+%5Cleq+cn+%2B+C%28n%2F5%29+%2B+C%287n%2F10%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="C(n) \leq cn + C(n/5) + C(7n/10)" title="C(n) \leq cn + C(n/5) + C(7n/10)"></p>



<p>If we appropriately build the recurse tree we will see that</p>



<figure><img data-attachment-id="589" data-permalink="https://danlark.org/2020-11-10-232345_1330x458_scrot/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-232345_1330x458_scrot.png" data-orig-size="1330,458" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2020-11-10-232345_1330x458_scrot" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-232345_1330x458_scrot.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-232345_1330x458_scrot.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-232345_1330x458_scrot.png?w=1024" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-232345_1330x458_scrot.png?w=1024 1024w, https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-232345_1330x458_scrot.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-232345_1330x458_scrot.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-232345_1330x458_scrot.png?w=768 768w, https://danlarkorg.files.wordpress.com/2020/11/2020-11-10-232345_1330x458_scrot.png 1330w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>This is the geometric series with <img src="https://s0.wp.com/latex.php?latex=cn%281+%2B+9%2F10+%2B+%289%2F10%29%5E2+%2B+%289%2F10%29%5E3+%2B+%5Cldots+...%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="cn(1 + 9/10 + (9/10)^2 + (9/10)^3 + \ldots ...)" title="cn(1 + 9/10 + (9/10)^2 + (9/10)^3 + \ldots ...)"> which gives us the result <img src="https://s0.wp.com/latex.php?latex=C%28n%29+%5Cleq+10+c+n&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="C(n) \leq 10 c n" title="C(n) \leq 10 c n">.</p>



<p>Actually, this constant 10 is really big. For example, if we look a bit closer, <img src="https://s0.wp.com/latex.php?latex=c&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="c" title="c"> is at least 1 because we need to partition the array, then finding median out of 5 elements cannot be done in less than 6 comparisons (can be proven by only brute-forcing) and in 6 comparisons it can be done in the following way</p>



<ol><li>Use three comparisons and shuffle around the numbers so that <img src="https://s0.wp.com/latex.php?latex=a%5B1%5D+%3C+a%5B2%5D%2C+a%5B4%5D+%3C+a%5B5%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="a[1] < a[2], a[4] < a[5]" title="a[1] < a[2], a[4] < a[5]">, and <img src="https://s0.wp.com/latex.php?latex=a%5B1%5D+%3C+a%5B4%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="a[1] < a[4]" title="a[1] < a[4]">.</li><li>If <img src="https://s0.wp.com/latex.php?latex=a%5B3%5D+%3E+a%5B2%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="a[3] > a[2]" title="a[3] > a[2]">, then the problem is fairly easy. If <img src="https://s0.wp.com/latex.php?latex=a%5B2%5D+%3C+a%5B4%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="a[2] < a[4]" title="a[2] < a[4]">, the median value is the smaller of <img src="https://s0.wp.com/latex.php?latex=a%5B3%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="a[3]" title="a[3]"> and <img src="https://s0.wp.com/latex.php?latex=a%5B4%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="a[4]" title="a[4]">. If not, the median value is the smaller of <img src="https://s0.wp.com/latex.php?latex=a%5B2%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="a[2]" title="a[2]"> and <img src="https://s0.wp.com/latex.php?latex=a%5B5%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="a[5]" title="a[5]">.</li><li>So <img src="https://s0.wp.com/latex.php?latex=a%5B3%5D+%3C+a%5B2%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="a[3] < a[2]" title="a[3] < a[2]">. If <img src="https://s0.wp.com/latex.php?latex=a%5B3%5D+%3E+a%5B4%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="a[3] > a[4]" title="a[3] > a[4]">, then the solution is the smaller of <img src="https://s0.wp.com/latex.php?latex=a%5B3%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="a[3]" title="a[3]"> and <img src="https://s0.wp.com/latex.php?latex=a%5B5%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="a[5]" title="a[5]">. Otherwise, the solution is the smaller of <img src="https://s0.wp.com/latex.php?latex=a%5B2%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="a[2]" title="a[2]"> and <img src="https://s0.wp.com/latex.php?latex=a%5B4%5D&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="a[4]" title="a[4]">.</li></ol>



<p>So that maximum <img src="https://s0.wp.com/latex.php?latex=c&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="c" title="c"> can be <img src="https://s0.wp.com/latex.php?latex=11%2F5&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="11/5" title="11/5"> and it gives us the upper bound <img src="https://s0.wp.com/latex.php?latex=22n&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="22n" title="22n"> comparisons which looks like it can be achieved. Some other tricks can be done in place to achieve a bit lower constants like <img src="https://s0.wp.com/latex.php?latex=18n&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="18n" title="18n"> (for example, sorting arrays of 5 and comparing less afterwards). In practice, the constant is really big and you can see it from the following demonstration which was even fastened because it took quite a few seconds:</p>



<figure><img data-attachment-id="593" data-permalink="https://danlark.org/median-of-medians-2020-11-09_11-04-33/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/11/median-of-medians-2020-11-09_11.04.33.gif" data-orig-size="1720,1034" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="median-of-medians-2020-11-09_11.04.33" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/11/median-of-medians-2020-11-09_11.04.33.gif?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/11/median-of-medians-2020-11-09_11.04.33.gif?w=750" src="https://danlarkorg.files.wordpress.com/2020/11/median-of-medians-2020-11-09_11.04.33.gif?w=1024" alt=""><figcaption>Median of medians for random input</figcaption></figure>



<h2>HeapSelect</h2>



<p>Another approach to finding <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="k" title="k">th element is to create a <a href="https://en.wikipedia.org/wiki/Heap_(data_structure)">heap</a> on an array of size <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="k" title="k"> and push other <img src="https://s0.wp.com/latex.php?latex=n+-+k&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="n - k" title="n - k"> elements into this heap. C++ <code>std::partial_sort</code> works that way (with additional heap sorting of the first heap). It shows good results for very small <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="k" title="k"> and random/ascending arrays, however starts to significantly degrade with growing <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="k" title="k"> and becomes impractical. Best case <img src="https://s0.wp.com/latex.php?latex=O%28n%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="O(n)" title="O(n)">, worst <img src="https://s0.wp.com/latex.php?latex=O%28n+%5Clog+k%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="O(n \log k)" title="O(n \log k)">, average <img src="https://s0.wp.com/latex.php?latex=O%28n+%5Clog+k%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="O(n \log k)" title="O(n \log k)">.</p>



<figure><img data-attachment-id="596" data-permalink="https://danlark.org/partial_sort-2020-11-09_12-28-40/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/11/partial_sort-2020-11-09_12.28.40.gif" data-orig-size="1720,1034" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="partial_sort-2020-11-09_12.28.40" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/11/partial_sort-2020-11-09_12.28.40.gif?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/11/partial_sort-2020-11-09_12.28.40.gif?w=750" src="https://danlarkorg.files.wordpress.com/2020/11/partial_sort-2020-11-09_12.28.40.gif?w=1024" alt=""><figcaption>std::partial_sort, two stages, first HeapSelect then heap sort of the first half, accelerated for speed</figcaption></figure>



<h2>IntroSelect</h2>



<p>As the previous algorithm is not very much practical and QuickSelect is really good on average, in 1997 <a href="http://www.cs.rpi.edu/~musser/gp/introsort.ps">“Introspective Sorting and Selection Algorithms”</a>  from David Musser came out with a sorting algorithm called “IntroSelect”. </p>



<p>IntroSelect works by optimistically starting out with QuickSelect and only switching to MedianOfMedians if it recurses too many times without making sufficient progress. Simply limiting the recursion to constant depth is not good enough, since this would make the algorithm switch on all sufficiently large arrays. Musser discusses a couple of simple approaches:</p>







<p>This algorithm came into <a href="https://github.com/gcc-mirror/gcc/blob/e0af865ab9d9d5b6b3ac7fdde26cf9bbf635b6b4/libstdc%2B%2B-v3/include/bits/stl_algo.h#L4748">libstdcxx</a> and guess which strategy was chosen? Correct, none of them. Instead, they try <img src="https://s0.wp.com/latex.php?latex=2%5Clog+n&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="2\log n" title="2\log n"> QuickSelect steps and if not successful, fallback to HeapSelect algorithm. So, worst case <img src="https://s0.wp.com/latex.php?latex=O%28n+%5Clog+n%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="O(n \log n)" title="O(n \log n)">, average <img src="https://s0.wp.com/latex.php?latex=O%28n%29&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="O(n)" title="O(n)"></p>



<figure><img data-attachment-id="598" data-permalink="https://danlark.org/nth-element-gcc-2020-11-09_11-06-37/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/11/nth-element-gcc-2020-11-09_11.06.37.gif" data-orig-size="1720,1034" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nth-element-gcc-2020-11-09_11.06.37" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/11/nth-element-gcc-2020-11-09_11.06.37.gif?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/11/nth-element-gcc-2020-11-09_11.06.37.gif?w=750" src="https://danlarkorg.files.wordpress.com/2020/11/nth-element-gcc-2020-11-09_11.06.37.gif?w=1024" alt=""><figcaption>std::nth_element in libstdcxx, “IntroSelect”</figcaption></figure>



<h2>PDQSelect</h2>



<p>Now that most of the known algorithms come to an end 😈, we can start looking into something special and extraordinary. And the first one to look at is pdqselect which comes pretty straightforward from <a href="https://github.com/orlp/pdqsort">pdqsort</a>, the algorithm is basically QuickSelect but with some interesting ideas on how to choose an appropriate pivot:</p>



<ol><li>If there are <img src="https://s0.wp.com/latex.php?latex=n+%3C+24&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="n < 24" title="n < 24"> elements, use <a href="https://en.wikipedia.org/wiki/Insertion_sort">insertion sort</a> to partition or even sort them. As insertion sort is really fast for a small amount of elements, it is reasonable</li><li>If it is more, choose <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="p" title="p"> — pivot:<ol><li>If there are less or equal than 128 elements, choose pseudomedian (or “ninther”, or median of medians which are all them same) of the following 3 groups:<ol><li>begin, mid, end</li><li>begin + 1, mid – 1, end – 1</li><li>begin + 2, mid + 1, end – 2</li></ol></li><li>If there are more than 128 elements, choose median of 3 from begin, mid, end</li></ol></li><li>Partition the array by the chosen pivot with avoiding <a href="https://www.researchgate.net/publication/301614727_BlockQuicksort_How_Branch_Mispredictions_don't_affect_Quicksort">branches</a>:<ol><li>The partition is called bad if it splits less than <img src="https://s0.wp.com/latex.php?latex=1%2F8n&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="1/8n" title="1/8n"> elements</li><li>If the total number of bad partitions exceeds <img src="https://s0.wp.com/latex.php?latex=%5Clog+n&amp;bg=FFFFFF&amp;fg=181818&amp;s=0&amp;c=20201002" alt="\log n" title="\log n">, use <code>std::nth_element</code> or any other fallback algorithm and return</li><li>Otherwise, try to defeat some patterns in the partition by (sizes are l_size and r_size respectively):<ol><li>Swapping begin, begin + l_size / 4</li><li>Swapping p – 1 and p – l_size / 4</li><li>And if the number of elements is more than 128<ol><li>begin + 1, begin + l_size / 4 + 1</li><li>begin + 2, begin + l_size / 4 + 2</li><li>p – 2, p – l_size / 4 + 1</li><li>p – 3, p – l_size / 4 + 2</li></ol></li><li>Do the same with the right partition</li></ol></li></ol></li><li>Choose the right partition part and repeat like in QuickSelect</li></ol>



<figure><img data-attachment-id="605" data-permalink="https://danlark.org/pdqselect-2020-11-09_11-03-23/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/11/pdqselect-2020-11-09_11.03.23.gif" data-orig-size="1720,1034" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pdqselect-2020-11-09_11.03.23" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/11/pdqselect-2020-11-09_11.03.23.gif?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/11/pdqselect-2020-11-09_11.03.23.gif?w=750" src="https://danlarkorg.files.wordpress.com/2020/11/pdqselect-2020-11-09_11.03.23.gif?w=1024" alt=""><figcaption>pdqselect on random input</figcaption></figure>



<h2>Media…</h2></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://danlark.org/2020/11/11/miniselect-practical-and-generic-selection-algorithms/">https://danlark.org/2020/11/11/miniselect-practical-and-generic-selection-algorithms/</a></em></p>]]>
            </description>
            <link>https://danlark.org/2020/11/11/miniselect-practical-and-generic-selection-algorithms/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25059942</guid>
            <pubDate>Wed, 11 Nov 2020 16:46:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Is It Time to Modernize the PostgreSQL Core Team?]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25059852">thread link</a>) | @ahachete
<br/>
November 11, 2020 | https://postgresql.fund/blog/is-it-time-to-modernize-postgresql-core/ | <a href="https://web.archive.org/web/*/https://postgresql.fund/blog/is-it-time-to-modernize-postgresql-core/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
    <div>
        <div>
            <div>
                
                <div>
                    
<p>The PostgreSQL Community is large, diverse and global. There are users, enthusiasts, developers, contributors, advocates and commercial entities from around the world. All of them working in a loosely collaborative fashion to grow and make PostgreSQL succeed.</p>
<p>The Postgres Core Team is considered to be the steering committee for the Community. The definition of the group responsibilities can be <a href="https://www.postgresql.org/developer/core/">found here</a>. The core team members are listed on the <a href="https://www.postgresql.org/community/contributors/">Contributor Profiles</a> page.</p>
<p>On September 30th EnterpriseDB acquired 2ndQuadrant. At the time of the acquisition there were five members in Core; two of them were EnterpriseDB employees and another one a 2ndQuadrant employee. This meant that 60% of the Core members would be employed by EnterpriseDB. On October 20th, in an effort to diffuse concerns about a single commercial entity having majority control, the <a href="https://www.postgresql.org/about/news/statement-from-the-postgresql-core-team-on-the-edb-acquisition-of-2ndquadrant-2094/">Core Team announced</a> that this is an issue that they would be addressing:</p>
<p>“<em>There has long been an unwritten rule that there should be no more than 50% of the membership of the Core Team working for the same company</em>”</p>
<p>This rule was enacted back in the days of the <a href="https://www.postgresql.org/message-id/39181CCD.99531ADA@greatbridge.com">Great Bridge</a>. Core addressed the unwritten rule by appointing on November 2nd <a href="https://www.postgresql.org/about/news/new-postgresql-core-team-members-2103/">two new members: Andres Freund and Jonathan Katz</a>. This change in Core reduced the proportion of EnterpriseDB members to three out of seven. <strong>Fundación PostgreSQL</strong> would like to extend a very warm welcome to Andres and Jonathan. They are both well known and long time community contributors.</p>
<p>The addition of the new members allowed Core to be compliant with the 50% rule. However: was this organizational change the best choice? Was it the only change that could have been implemented? Could we have looked at the culture of our global community and used this opportunity to strengthen our ties?</p>
<p>Here are some facts about Core’s structure and membership:</p>
<ul>
<li><strong>Company influence</strong>:
<ul>
<li>Core has switched from having 40% of its members from a single company to now having 43% from a single company and 71% from two companies.</li>
<li>100% of the members are from only 4 companies.</li>
</ul>
</li>
<li><strong>Diversity</strong>:
<ul>
<li>100% of the current Core team members are white men.</li>
<li>All of the Core members are either US or European. No other region is represented.</li>
<li>All but one Core member work for US companies.</li>
</ul>
</li>
<li><strong>Democracy:</strong>
<ul>
<li>Core members are only appointed by existing Core members. In contrast, the “<a href="https://www.postgresql.org/community/recognition/#npos">Recognised Postgres Nonprofit Organisations</a>” (created and enforced by Core) has as a requirement that the “<em>board of directors MUST be elected by the membership</em>”. These rules were, in turn, created by Core itself.</li>
<li>Core members serve for an <em>unlimited</em> term. In contrast, the same Community recognition rules above also require that “<em>Lifetime directorships MUST NOT be allowed</em>”. Four of the current Core members <a href="https://web.archive.org/web/20051023004218/http://www.postgresql.org/developer/bios">have been serving in the Core team for more than 15 years</a>.</li>
</ul>
</li>
<li><strong>Transparency</strong>:
<ul>
<li>The election process, candidate selection, selection criteria, etc are all secret.</li>
<li>Core Team meeting minutes are secret.</li>
<li>Core team policies are enacted by declaration, without involvement of the global community.</li>
</ul>
</li>
</ul>
<p>Facts aside, there are some organizational concerns that may require some further analysis.</p>
<p>In the PostgreSQL distributed community, the <a href="https://www.postgresql.org/developer/core/">Core Team</a> acts as the <em>de facto</em> “central authority” for the project. The <a href="https://www.postgres.ca/">Postgres Association of Canada</a> (“CA”, in short), acts as its legal arm, holding assets (including intellectual property, like domain names and trademarks).</p>
<p>However, this presents an interesting dichotomy: Core makes decisions, but if these require a legal entity to be executed, they are executed by CA. Which has its own board of directors, that needs to approve them. What if they don’t? What if they don’t follow Core? Similarly, how is Core accountable, if it is not backed directly by a legal entity? Because of this, are there any potential liabilities faced directly by their members, as individuals? And what happens if CA’s Board goes haywire?</p>
<p>Other mature and successful open source projects, while distributed as Postgres and built from the contributions of people and organizations all around the world, are nowadays backed by clear and strong legal and organizational structure. Take for example the <a href="https://www.apache.org/foundation/">Apache Foundation</a>, or the <a href="https://www.fsf.org/working-together/fiscal-sponsorship">Free Software Foundation</a>. Or the <a href="https://www.cncf.io/">Cloud Native Computing Foundation (CNCF)</a>, which is a Charter of the Linux Foundation. Its structure <a href="https://www.cncf.io/blog/2019/12/06/cncf-toc-governance-structure-elections-2020/">has three main bodies</a>:</p>
<p>“<em>A <strong>Governing Board (GB)</strong> that is responsible for marketing, budget and other business oversight decisions for the CNCF, a <strong>Technical Oversight Committee (TOC)</strong> that is responsible for defining and maintaining the technical vision, and an <strong>End User Community (EUC)</strong> that is responsible for providing feedback from companies and startups to help improve the overall experience for the cloud native ecosystem</em>”</p>
<p>The <a href="https://www.cncf.io/people/governing-board/">Governing Board has currently 24 members</a>, and their <a href="https://www.cncf.io/about/governing-board-meeting-minutes/">meeting minutes are public</a> (they are not alone: MariaDB Foundation <a href="https://mariadb.org/bodminutes/2020-10-21/">is now publishing their board meetings too</a>); the <a href="https://www.cncf.io/people/technical-oversight-committee/">Technical Committee consists of 11 members and 77 contributors</a>; the <a href="https://docs.google.com/presentation/d/194SyKdHL7ws_DBOdbrXdowEJi54kIzDdDK_h-6Ag0uo/edit#slide=id.g9ffb40d42b_0_161">End User Community has more than 150 companies</a>; furthermore, there are dozens of <a href="https://www.cncf.io/people/ambassadors/">ambassadors</a>; and also dozens of <a href="https://www.cncf.io/people/staff/">staff</a> members. While possibly operating at a different scale than PostgreSQL, they all contribute, in different manners, to the steering, development and vision of the CNCF.</p>
<p>What do you think? <strong>Is PostgreSQL Core today what the PostgreSQL Community needs, or is it time to modernize its processes, structure and governance?</strong> If you think it is the latter, please leave your comments below. I hope this post serves as the starting point for a broader and constructive discussion that can serve as feedback to Core. Let’s ensure the best future for our beloved open source database!</p>

                </div>
                
                    
                
            </div>
        </div>
    </div>
</section></div>]]>
            </description>
            <link>https://postgresql.fund/blog/is-it-time-to-modernize-postgresql-core/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25059852</guid>
            <pubDate>Wed, 11 Nov 2020 16:39:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Are Package Registries Holding Open-Source Hostage?]]>
            </title>
            <description>
<![CDATA[
Score 88 | Comments 67 (<a href="https://news.ycombinator.com/item?id=25059755">thread link</a>) | @aviaviavi
<br/>
November 11, 2020 | https://about.scarf.sh/post/package-registries-and-open-source | <a href="https://web.archive.org/web/*/https://about.scarf.sh/post/package-registries-and-open-source">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><p>A few days ago, I received an email from Docker about a change I already knew was coming:<em>‍</em></p><blockquote><em>Docker will begin enforcing rate limits on container pulls for Anonymous and Free users.</em><br></blockquote><p>To many, this came as no surprise. For years, <a href="http://hub.docker.com/">Docker Hub</a> has offered free hosting of container images, which typically range in size from a few megabytes to many gigabytes. Docker workflows as a result use a <em>lot</em> of bandwidth, and that bandwidth costs money.<br></p><p>Should we OSS (open-source software)&nbsp;developers have to think about the business and financial models of the platforms we host our software on? In a perfect world, we wouldn't have to—but in the real world we very much do. The incentives between OSS maintainers and the registries they use are often misaligned.&nbsp;<br></p><p>Docker Hub, <a href="http://npmjs.com/">npm</a>, and other comparable registries are incentivized to create lock-in, even if it makes the product experience worse for their users and customers. This is especially true of the for-profit companies behind the registries, but we see similar issues from many of the not-for-profit registries. </p><p>Maintainers, on the other hand, are incentivized to choose the best product for their needs at the lowest cost, which depends on being able to switch providers when a better service comes on the market.&nbsp;<br></p><p>This situation is fundamentally at odds with the today's package management ecosystem, where immutability is paramount in order to achieve stability. We avoid breaking things at all costs, on principle, since OSS packages are the nuts and bolts of the software ecosystem, the internet, and thus society itself.<br></p><h6><strong>Mechanics of registry lock-in</strong><br></h6><p>The registry where you host your packages and containers might be free today, but if that changes later, as is the case now with Docker, you and your users might be stuck paying whatever price the vendor chooses to set. Your users might even agree to access your package without the rate limit, but<strong> </strong><em>you</em> will not be seeing any of that revenue. Access to your open-source software was effectively just sold for a profit, and you, the author of that software, were cut out of the transaction.<br></p><p>While you could in theory just host your software somewhere else, can you really do that without breaking things for your current users? If you maintain and distribute a popular Docker image, switching the package registry is likely difficult.<br></p><p>Currently, any image on Docker Hub is installable as:<br></p><div><pre><code>$ docker run org-name/image-name</code></pre></div><p><em>‍</em>If you decide later you actually want to move your container hosting somewhere else - let's say <a href="https://cloud.google.com/container-registry" target="_blank">Google Container Registry</a> for this example - the Docker client is reasonable and lets you pull down images by their URL:<br></p><div><pre><code>$ docker run gcr.io/org-name/image-name</code></pre></div><p><em>‍</em>The problem here is that once you've changed the URL to your images, all of your existing users will stop getting updates! Even worse, this can break builds or pipelines for your users whenever they hit the new rate limits, which are not under your control. </p><p>At the point where your container has a sizable user-base all going through one of the existing container registries, your lock-in is substantial. Moving platforms will be painful. The crux of the problem here is that <em>you</em> don't own the distribution channel. The registry is the&nbsp; first place the web traffic goes, and everything that happens after that is at <em>the registry vendor’s</em><strong> </strong>discretion and to their advantage, not yours.<br></p><h6><strong>Effects of registry lock-in</strong><br></h6><p>Some might respond: <em>"This still seems like more of a theoretical problem than a practical one."</em><br></p><p>There are several practical downstream effects of the misaligned incentive structures to open-source package hosting. One major effect of registry lock-in is that maintainers cannot access their usage data. The data that registries naturally collect from package downloads can be quite useful to maintainers in a myriad of ways, yet registries typically don't share anything beyond a download count. </p><p>Registries know where the downloads are coming from, the devices, the package versions, which other packages are installed alongside, and a whole lot more. Little to none that information is shared with maintainers. Thus, maintainers are effectively locked out from observing the usage traffic.&nbsp;<br></p><p>Why is this the case? It's not because developers don't ask for it (<a href="https://github.com/npm/npm/issues/279">https://github.com/npm/npm/issues/279</a>). It's because the registries have no incentive to do so. It would cost the registries money to build and maintain the features to provide this data. Some registries even claim that exposing this data publicly would incentivize maintainers to game the system. Meanwhile, the extreme levels of inertia in software distribution keep maintainers locked in. </p><p>The registries' demonstrated distrust of maintainers seems counterproductive in a space where there's opportunity to work together cooperatively. If registry incentives were aligned accordingly, a registry like npm, for instance, would be in a great position to empower maintainers to leverage their own distribution data to deliver the best software possible.<br></p><p>What makes npm’s particular scenario even worse is that they've made it so difficult to use a registry that is<em> not</em> npm. There's no way to pull a single package from an alternate registry without switching to that registry. Which makes it quite impossible to actually publish a widely used JavaScript package without putting it on npm.&nbsp;<br></p><p>Contrast this scenario to Docker: Docker Hub creates different tradeoffs that both help and harm OSS maintainers. They've loosened their grip on OSS maintainers by making it user-friendly to pull containers down from alternative registries besides Docker Hub. However, even if you switch away from Docker Hub, you're still jumping from one company to another. This is because, at the end of the day, the registries—not the maintainers—own the distribution URL. The power imbalance continues.<br></p><p>My argument is not intended to dismiss the efforts of the registries as a whole. Package registries serve an essential role in software distribution, and have collectively serviced billions upon billions of package downloads. They’ve made it easy for anyone in the world to interact with open-source, and as a result have helped push open-source forward. Astonishingly, they have, for most part, remained free to use! But as software continues to eat the world and the distribution of that software becomes more important, conflicting interests in this space become increasingly problematic.<br></p><h6><strong>Looking forward</strong><br></h6><p>How do we solve this? Ultimately, package registries need to align their incentives with those of maintainers. Registries should build products maintainers <em>want</em> to use rather than products they <em>have</em> to use.&nbsp;The entire OSS&nbsp;community can benefit. <br></p><p>Part of this means registries must be more intentional about giving maintainers back control over the distribution of their own software, even when it means the maintainers could take their packages elsewhere. As a community, we should be empowering maintainers to do their best work rather than constraining them to work within a specific&nbsp; platform or framework.&nbsp;<br></p><p>For the health of the open source ecosystem, it's critical to ensure that maintainers are not locked out from accessing the data about their own software distribution. Maintainers must be able to make data-informed decisions and treat distribution data as something that rightfully belongs to them, instead of just the registry providers.<br></p><p>Unfortunately, the current software distribution model works to cut maintainers out, making all downstream actions more difficult and strictly less informed. When we as a community decide to better align ourselves with open-source maintainers and build platforms to empower them, the result will be better software for the entire ecosystem.</p><p>Scarf is working on new tooling to address these problems, so stay tuned! Follow <a href="https://twitter.com/scarf_oss" target="_blank">@scarf_oss</a> on Twitter or subscribe below for periodic updates.<br></p></div></div></div></div>]]>
            </description>
            <link>https://about.scarf.sh/post/package-registries-and-open-source</link>
            <guid isPermaLink="false">hacker-news-small-sites-25059755</guid>
            <pubDate>Wed, 11 Nov 2020 16:32:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Productivity vs. Privacy]]>
            </title>
            <description>
<![CDATA[
Score 43 | Comments 14 (<a href="https://news.ycombinator.com/item?id=25059205">thread link</a>) | @jessems
<br/>
November 11, 2020 | https://jessems.com/productivity-vs-privacy | <a href="https://web.archive.org/web/*/https://jessems.com/productivity-vs-privacy">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><div><p>In recent years there's been a steady growth in privacy focused companies. Some examples that have reached large-scale adoption are <a href="https://protonmail.com/">ProtonMail</a>, <a href="https://signal.com/">Signal</a>, and <a href="https://duckduckgo.com/">DuckDuckGo</a>. These are companies that have put privacy front and center to their value proposition and can be considered <em>privacy-preserving products</em>. I've come to beleive a goal of preserving user privacy is often inherently in tension with the goal of advancing user productivity.</p><p>What these services have in common is that they promise their users a higher degree of privacy relative to their competitors. Instead of the usual encryption in transit (protection from eavesdroppers) and encryption at rest (protection against unauthorized users), services like Signal and ProtonMail enable their users to hide data from anyone except the intended recipient, which — crucially — includes the service providers themselves.</p><p>This category of encryption is known as end-to-end encryption (e2e) and has found adopters in anyone from principled libertarians to journalists and human rights activitists whose lives may depend on their conversations remaining unwiretapped.</p><h2>Early privacy-preserving software was too difficult to use</h2><p>The canonical implementation of e2e for email is known as Pretty Good Privacy (PGP) and its reference implementation is GPG. GPG never reached mass adoption and there seems to be a myriad of reasons for that. The most salient reason, however, seems to be that to this day, it continues to be difficult to use. As the founder of Signal, Moxie Marlinspike <a href="https://moxie.org/2015/02/24/gpg-and-me.html">explains</a>, the spirit behind GPG was the following:</p><blockquote><p>Instead of developing opinionated software with a simple interface, GPG was written to be as powerful and flexible as possible.</p></blockquote><p>Powerful, flexible software written by nerds, unfortunately also tends to be prohibitively complex for normal users. Combined with the fact that <a href="https://signal.org/blog/the-ecosystem-is-moving/">decentralized technology seems unable to quickly adapt to change</a>, the result has been a clunky solution that has, quite frankly, stayed clunky. With no feasible privacy-preserving alternative <!-- -->[1]<!-- -->, non-privacy preserving email providers became the norm.</p><h2>Surveillance capitalist companies will not encrypt your data, because they rely on being able to read it</h2><p>One such email provider, Gmail by Google, gained millions of users by offering a free plan. Their initial monetization strategy was scanning your emails and serving you personalized ads. Although they've stopped personalizing the ads, they're still scanning your email's contents to serve you a better experience across their services. Similarly, Facebook tracks what you do to shape your experience and keep you glued (they would say 'engaged') to their platform.</p><p>What unites platforms like Google and Facebook, is described by Professor Shoshana Zuboff as “<a href="https://en.wikipedia.org/wiki/Surveillance_capitalism">surveillance capitalism</a>”. The business model of surveillance capitalist companies is to harvest personal data about you to build a model that predicts your behavior. These prediction models are packaged and sold as advertisement opportunities to companies eager to buy your attention. You might be the user, but you're not the customer — the advertisers are.</p><p>It should come as no surprise then, that none of these platforms has shipped with end-to-end encryption by default. Doing so would go against the incentives that undergird their very business model. Their ability to predict your behavior, and sell ads based on those predictions, hinges on their ability to harvest your data.</p><h2>Data is also collected to improve the service</h2><p>A company like Google has other business models of course. Google Workspace, aimed at businesses, is a collection of collaboration and productivity tools. This ranges from Google Docs, to chat, to video conferencing, and more. By offering this as a paid service, Google exposes itself to a different incentive, one where the customer and the user are now one and the same.</p><p>Even if you're both the user and the customer, your data is still being harvested. This data might not feed into personalized ads (because that’s no longer the primary business model) but rather into improving your experience. But as a business user, when does your experience improve? And as a service provider, how do you know what improves the experience?</p><h2>Improvements are productivity gains</h2><p>There's an inclination to think of improvements as things that help you do the thing you want to do quicker, better and/or with less frustration. We can go one step further and borrow some of the thinking used in economics and treat productivity simply as the ratio between outputs (salaries and corporate profits) and inputs (hours worked). Productivity increases if inputs can be decreased (for equal outputs) or outputs can be increased (for equal inputs). What's more, we would expect this quantity to improve along with advances in technology.</p><p>How does technology lead to increases in productivity? One obvious way is by making us more efficient. If some new technology saves us time doing a certain task (decreased input), all other things being equal, we’ll end up seeing those gains reflected in our outputs.</p><h2>Productivity gains are discovered, not planned</h2><p>What exactly are the things that increase efficiency? Here's where it gets tricky. In the realm of knowledge work, we don't always know where the gains will come from — that is, before they are made. We are still discovering new ways in which we can be more productive and especially so in the domain of collaborative productivity. An illustrative example of how productivity gains are discovered comes from Kevin A. Kwok's description of Figma's road to success.</p><p>In "Why Figma Wins", <a href="https://kwokchain.com/2020/06/19/why-figma-wins/">Kwok details</a> how the product team discovered a way to enable more efficient collaboration in the design process. That this potential existed wasn't at all  obvious to even those within the scene. While Sketch had broken new ground with their vector based design tool geared towards product designers, Figma took it to another level by taking many of the same (dare I say revolutionary) UX patterns and offering them in a web-native, multiplayer web application.</p><blockquote><p>The core insight of Figma is that design is larger than just designers. Design is all of the conversations between designers and PMs about what to build. It is the mocks and prototypes and the feedback on them. It is the handoff of specs and assets to engineers and how easy it is for them to implement them.</p></blockquote><p>As Kevin explains, Figma brought together the disparate disciplines that are involved in a design process into a synced browser window for everybody. This helped democratize design and remove a lot of friction that had existed before.</p><p>Not only did Figma push the frontier of productivity into new territory, it wasn’t obvious beforehand what that territory would look like. The lesson is that productivity improvements are won through a process of <em>discovery</em>. Kevin explains:</p><blockquote><p>As disciplines evolve, they figure out the social norms needed to operate better, build tools that can be shared across the industry, and invent abstractions that allow offloading more and more of the workload. They learn how to collaborate better, not just with each other but with all the other functions as well.</p></blockquote><p>Although there's some inherent uncertainty about what the productivity gains will look like (and where to look for them), there's no uncertainty about whether they will be made at all. If one thing can be counted on, it's the tech industry's relentless march towards higher productivity. The big tech platforms know this and don't shy away from investing heavily in innovation (discovery) in that direction.</p><h2>Productivity gains are unlocked by harvesting data</h2><p>Although there is some inherent tension between preserving privacy vs. allowing for a multiplayer mode like Figma, we can find even stronger tensions when it comes to harvesting data in favor of productivity gains.</p><p>A search feature relies on indexing your data. A recommendation feature relies on mining your browsing history. An autocomplete feature relies on what you (or other users) typed before.</p><p>All these potential features which are made possible through harvesting your user data are not available to privacy-preserving products. The user data isn't readable to them — and that's the whole point.</p><p>This creates a trade-off from the user's perspective. Whatever your particular motivation might be, as soon as you opt for a privacy-preserving service you're opting for a service that is not able to read your data, and by extension, not able to harvest it. Because the harvesting of data is what is driving many of the improvements in productivity, in choosing to preserve user privacy, these services are forgoing their ability to provide additional gains in productivity.</p><p>Historically, as we saw with the origins of GPG, there has always been additional friction involved in replicating a workflow in a privacy-preserving manner. Although using e2e services such as Signal and ProtonMail has become nearly frictionless, they lack many features their non-privacy preserving counterparts offer.</p><h2>The productivity gap between privacy-preserving and non-preserving services</h2><p>If you compare the productivity gains between privacy-preserving and non-preserving products from the perspective of the user, it's hard not to arrive at the conclusion that there’s a gap between the two — and it appears to be growing.</p><p>There is perhaps no better example of a feature which hinges on the ability to read user data than search. Although ProtonMail is reminiscent of Gmail in many ways, one area where it falls short is the absence of any ability to  search the contents of your emails. Search only works if the provider of such functionality can scan and index your content. It works even better if the provider is able to harvest search queries and use those to build predictive models (e.g. autocomplete and smart suggestions). These are features which make Gmail users more productive but aren't available to ProtonMail users <!-- -->[3]<!-- -->.</p><p>The absence of search might not be a dealbreaker for a …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://jessems.com/productivity-vs-privacy">https://jessems.com/productivity-vs-privacy</a></em></p>]]>
            </description>
            <link>https://jessems.com/productivity-vs-privacy</link>
            <guid isPermaLink="false">hacker-news-small-sites-25059205</guid>
            <pubDate>Wed, 11 Nov 2020 15:37:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[You may not need Redis with Elixir]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25058805">thread link</a>) | @wojtekmach
<br/>
November 11, 2020 | https://dashbit.co/blog/you-may-not-need-redis-with-elixir | <a href="https://web.archive.org/web/*/https://dashbit.co/blog/you-may-not-need-redis-with-elixir">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <article>
    
<ul>
  <li>
    <i></i> José Valim
  </li>
  <li>
    <i></i> November 11th, 2020
  </li>
  <li>
    <i></i><a href="https://dashbit.co/blog/tags/redis">redis</a>, <a href="https://dashbit.co/blog/tags/phoenix">phoenix</a>, <a href="https://dashbit.co/blog/tags/pubsub">pubsub</a>, <a href="https://dashbit.co/blog/tags/processes">processes</a>
  </li>
</ul>
<p>
If you have participated in a discussion about Elixir, you may have heard “you may not need Redis with Elixir”. Given that Redis has many use cases, this sentence may confuse developers as they try to match Elixir’s different features against Redis’ capabilities. This article aims to explore different scenarios where the above is true, when they are not, and which trade-offs you may want to consider. We will discuss four cases:</p>
<ol>
  <li>
<a href="#post-pubsub">Distributed PubSub</a>  </li>
  <li>
<a href="#post-presence">Presence</a>  </li>
  <li>
<a href="#post-caching">Caching</a>  </li>
  <li>
<a href="#post-async">Asynchronous processing</a>  </li>
</ol>
<p>
Before we start, I want to emphasize we find Redis a fantastic piece of technology. This is not a critique of Redis but rather a discussion of the different options Elixir developers may have available.</p>
<h2 id="post-pubsub">
  Case #1: Distributed PubSub</h2>
<p>
The first scenario where you may not need Redis with Elixir is Distributed <a href="https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern">PubSub</a>. Throughout this section, we will consider PubSub systems to provide <strong>at-most-once delivery</strong>: they broadcast events to the currently available subscribers. If a subscriber is not around, they won’t receive the message later.</p>
<p>
For this reason, PubSub systems are often paired with databases to offer persistence. For example, every time someone sends a message in a chat application, the system can save the contents to the database and then broadcast it to all users. This means everyone connected at a given moment sees the update immediately, but disconnected users can catch up later.</p>
<p>
Imagine that you have multiple nodes, and you want to exchange messages between said nodes. In Elixir, thanks to the Erlang VM, which ships with distribution support, this can be as simple as:</p>
<pre><code><span>for</span><span> </span><span>node</span><span> </span><span>&lt;-</span><span> </span><span>Node</span><span>.</span><span>list</span><span data-group-id="8737372369-1">(</span><span data-group-id="8737372369-1">)</span><span> </span><span data-group-id="8737372369-2">do</span><span>
  </span><span>send</span><span data-group-id="8737372369-3">(</span><span data-group-id="8737372369-4">{</span><span>:known_name</span><span>,</span><span> </span><span>node</span><span data-group-id="8737372369-4">}</span><span>,</span><span> </span><span>:hello_world</span><span data-group-id="8737372369-3">)</span><span>
</span><span data-group-id="8737372369-2">end</span></code></pre>
<p>
In <a href="https://github.com/phoenixframework/phoenix_pubsub/blob/master/lib/phoenix/pubsub.ex">200LOC or less</a>, you can implement a PubSub system that broadcasts to all subscribers within the same node or anywhere else in a cluster, without bringing any third-party tools. At best, you will need <a href="https://github.com/bitwalker/libcluster">libcluster</a> - an Elixir library - to establish the connection between the nodes based on some strategy (K8s, AWS, DNS, etc.).</p>
<p>
In other words, PubSub pretty much ships out of the box with Elixir. Technologies without distribution would need to rely on Redis PubSub, PostgreSQL Notifications, or similar to achieve the same.</p>
<p>
Of course, the above assumes your infrastructure allows you to directly establish connections between nodes, which may not be possible in some PaaS, such as Heroku. In those cases, you can use any of the technologies above (Phoenix <a href="https://dashbit.co/blog/github.com/phoenixframework/phoenix_pubsub_redis/">has a Redis adapter</a> for its PubSub), or alternatively use platforms, such as <a href="https://www.gigalixir.com/">Gigalixir</a>, that make it trivial to setup a cluster.</p>
<h2 id="post-presence">
  Case #2: Presence</h2>
<p>
Presence is the ability to track who is connected in a cluster right now — the “who” may be users, phones, IoT devices, etc. For example, if Alice is connected to node A, she wants to see that Bob is also available, even if he has joined node B.</p>
<p>
Presence is one of the problems that are more complicated to implement than it sounds. For example, let’s consider implementing Presence by storing the connected entities in a database. However, what happens if a node crashes or leaves the cluster? Because the node crashed, all the users connected to it must be removed, but the node itself cannot do so. Therefore the other nodes need to detect those failure scenarios and act accordingly. But observing failures in a distributed system is also complicated: how do you differentiate between a temporarily unresponsive node from one that permanently failed?</p>
<p>
Another common approach to solve this problem is to frequently write to a database while users are connected. If you have seen no writes within a timeframe, you consider those users to be disconnected. However, such solutions have to choose between being write-intensive or inaccurate. For instance, let’s say that users become disconnected after 1 minute. This means that you need to write to the database every 1 minute for every user. If you have 10k users, that’s 167 writes per second, only to track that the users are connected. Meanwhile, the gap between a user leaving and having their status reflected in the UI is, in the worst-case scenario, also 1 minute. Any attempt at reducing the number of writes implies an increased gap.</p>
<p>
Given Elixir’s clustering support, we can once more implement <a href="https://hexdocs.pm/phoenix/Phoenix.Presence.html">Presence</a> without a need for third-party dependencies! We use a PubSub system to implement Presence, as we need to notify as users join and leave. Instead of relying on centralized storage, the nodes directly communicate and exchange information about who is around. This removes the need for frequent writes. When a user leaves, this is also reflected immediately.</p>
<p>
So while you can use Redis or another storage to provide Presence, Elixir can deliver a solution that is efficient and doesn’t require third-party tools.</p>
<h2 id="post-caching">
  Case #3: Caching</h2>
<p>
The solutions to previous cases were built on top of Erlang’s unique distribution capabilities. In the following sections, the distinguishing factor between needing Redis or not will be <strong>multi-core concurrency</strong>, so this discussion is more generally applicable. Therefore, when we say Elixir in this section, it will also apply to JVM, Go, and other environments. They will contrast to Ruby, Python, and Node.js, in which their primary runtimes do not provide adequate multi-core concurrency within a single Operating System process.</p>
<p>
Let’s start with the non-concurrent scenario. Consider you are building a web application in Ruby, Python, etc. To deploy it, you get two eight-core machines. In languages that do not provide satisfactory multi-core concurrency, a common option for deployment is to start 8 instances of your web application, one per core, on each node. Overall, you will have CxN instances, where C is the number of cores, and N is the number of nodes.</p>
<p>
Now consider a particular operation in this application that is expensive, and you want to cache its results. The easiest solution, regardless of your programming environment, is to cache it in memory. However, given we have 16 instances of this application, caching it in memory is suboptimal: we will have to perform this expensive operation at least 16 times, one for each instance. For this reason, it is widespread to use Redis, Memcached, or similar for caching in environments like Ruby, Python, etc. With Redis, you would cache it only once, and it will be shared across all instances. The trade-off is that we are replacing memory access by a network round-trip, and the latter is orders of magnitude more expensive.</p>
<p>
Now let’s consider environments with multi-core concurrency. In languages like Elixir, you start one instance per node, regardless of the number of cores, since the runtime will share memory and efficiently spread the work across all cores. When it comes to caching, keeping the cache in-memory is a much more affordable scenario, as you will only have to compute once per node. Therefore, you have the <em>option</em> to skip Redis or Memcached altogether and avoid network round-trip.</p>
<p>
Of course, this depends on how many nodes you are effectively running in production. Luckily, many companies report being able to <a href="https://dev.to/erlangsolutions/why-elixir-is-the-programming-language-you-should-learn-in-2020-5g00">run Elixir with an order of magnitude less nodes</a> than technologies they have migrated from.</p>
<p>
You can also choose a mixed approach and store the cache both in-memory and in Redis. First, you look up in memory and, if missing, you fallback to Redis. If unavailable in both, then you execute the operation and cache it in each. The critical part to highlight here is that multi-core environments give you more flexibility to tackle these problems while reducing resource utilization. In Elixir/Erlang, you can also keep the cache in memory and use PubSub to distribute it across nodes. You can see this last approach in action <a href="https://github.com/tompave/fun_with_flags">in the excellent FunWithFlags library</a>.</p>
<p>
Another trade-off to consider is that all in-memory cache will be gone once you deploy new nodes. Therefore, if you need data to persist across deployments, you will want to use Redis as a cache layer, as detailed above, or dump the cache in a storage, such as database, S3, or Redis, before each deployment.</p>
<h2 id="post-async">
  Case #4: Asynchronous processing</h2>
<p>
Another scenario you may not need Redis in Elixir is to perform asynchronous processing. Let’s continue the discussion from the previous case.</p>
<p>
In environments without or with limited multi-core concurrency, given each instance is assigned to one core, they are limited in their ability to handle requests concurrently. This has led to a common saying that “you should avoid blocking the main thread”. For example, imagine that your application has to deliver emails on sign up or generate some computationally expensive reports. While one of your 16 web instances is doing this, it cannot handle other incoming requests efficiently. For this reason, a common choice here is to move the work elsewhere, typically a background-job processing queue. First, you store the work to be done on Redis or similar. Then one of the 16 web instances (or more commonly a completely different set of workers) grabs it from the queue.</p>
<p>
In multi-core concurrent environments, requests can be handled concurrently regardless if they are doing CPU or IO work. Sending the email from the request itself won’t block other requests. Generating the report is not a problem, as requests can be served by other CPUs. These platforms typically get assigned as many requests as they can handle and they distribute the work over the machine resources. Even if you prefer to deliver emails outside of the request, in order to send an earlier response to users, you can spawn an asynchronous worker without a need to move the delivery to an external queue or to another machine. Once again, concurrency gives us a more straightforward option to tackle these scenarios.</p>
<p>
Note the Erlang VM takes care of multiplexing CPU and IO work without a need for developers to tag functions as async or similar. Workers in Erlang/Elixir are …</p></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dashbit.co/blog/you-may-not-need-redis-with-elixir">https://dashbit.co/blog/you-may-not-need-redis-with-elixir</a></em></p>]]>
            </description>
            <link>https://dashbit.co/blog/you-may-not-need-redis-with-elixir</link>
            <guid isPermaLink="false">hacker-news-small-sites-25058805</guid>
            <pubDate>Wed, 11 Nov 2020 14:48:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Systematically removing code]]>
            </title>
            <description>
<![CDATA[
Score 30 | Comments 44 (<a href="https://news.ycombinator.com/item?id=25058632">thread link</a>) | @jerodsanto
<br/>
November 11, 2020 | https://thepugautomatic.com/2020/11/systematically-removing-code/ | <a href="https://web.archive.org/web/*/https://thepugautomatic.com/2020/11/systematically-removing-code/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section><hgroup><p><span>Written November 8, 2020. <span>Tagged <a href="https://thepugautomatic.com/tag/methodology">Methodology</a>.</span></span></p></hgroup><div><p>It's easy to miss things when removing code, leaving behind unused methods, templates, CSS classes or translation keys. (Especially in a dynamic language like Ruby, without a compiler to help you spot dead code.)</p><p>I avoid this by removing code systematically, line by line, depth-first.</p><p>This is one of those things that seems obvious when you do it, but in my experience, many people do it haphazardly.</p><p>Say we wanted to remove the "item box" from this page:</p><p>page.html.erb</p><pre><code><span><span><span>&lt;</span>p</span><span>&gt;</span></span>Welcome to my page!<span><span><span>&lt;/</span>p</span><span>&gt;</span></span><p><span><span>&lt;%=</span> render<span>(</span><span>"item_box"</span><span>,</span> item<span>:</span> item<span>)</span> <span>%&gt;</span></span></p></code></pre><p>_item_box.html.erb</p><pre><code><span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>box box--fancy<span>"</span></span><span>&gt;</span></span><br>  <span><span><span>&lt;</span>h2</span><span>&gt;</span></span><span><span>&lt;%=</span> item<span>.</span>title <span>%&gt;</span></span><span><span><span>&lt;/</span>h2</span><span>&gt;</span></span><br>  <span><span>&lt;%=</span> format_description<span>(</span>item<span>.</span>description<span>)</span> <span>%&gt;</span></span><br>  <span><span><span>&lt;</span>p</span><span>&gt;</span></span><span><span>&lt;%=</span> <span>I18n</span><span>.</span>translate<span>(</span><span>"my.translation.key"</span><span>)</span> <span>%&gt;</span></span><br><span><span><span>&lt;/</span>div</span><span>&gt;</span></span></code></pre><p>So our end goal is to remove the <code>&lt;%= render("item_box", item: item) %&gt;</code> line.</p><p>First, we search the project to check that <code>_item_box.html.erb</code> isn't used somewhere else, or referenced in docs that we'll need to update. It isn't, so we're OK to remove it – but before we do that, we must go through it line by line.</p><p>The first line is <code>&lt;div class="box box--fancy"&gt;</code>. So we search the project for these two CSS classes, checking if they're in use somewhere else. If not, we remove them from the CSS files.</p><p>We go deeper if required – perhaps the CSS for <code>.box--fancy</code> uses a CSS variable. Then we check if that variable is in use elsewhere. <a href="https://thepugautomatic.com/2014/03/stacked-vim-searches-down-cold/">Stacked searches in Vim</a> are helpful here.</p><p>Once we've checked a line in the file, we delete that line. This helps us keep track of what we've already checked.</p><p>So after we've checked and removed that line, we're left with</p><p>_item_box.html.erb</p><pre><code>  <span><span><span>&lt;</span>h2</span><span>&gt;</span></span><span><span>&lt;%=</span> item<span>.</span>title <span>%&gt;</span></span><span><span><span>&lt;/</span>h2</span><span>&gt;</span></span><br>  <span><span>&lt;%=</span> format_description<span>(</span>item<span>.</span>description<span>)</span> <span>%&gt;</span></span><br>  <span><span><span>&lt;</span>p</span><span>&gt;</span></span><span><span>&lt;%=</span> <span>I18n</span><span>.</span>translate<span>(</span><span>"my.translation.key"</span><span>)</span> <span>%&gt;</span></span><br><span><span><span>&lt;/</span>div</span><span>&gt;</span></span></code></pre><p>And we continue this way, line by line. Is the <code>item.title</code> used elsewhere? If not, we should probably remove it, too. What about <code>format_description</code>, <code>item.description</code>, the <code>my.translation.key</code> translation key?</p><p>Again, we go deeper if required, not removing the <code>format_description</code> method until we've gone through <em>it</em> line by line.</p><p>When we've looked at every line in <code>_item_box.html.erb</code> and deleted them as we went, the file will be empty, and we can start popping the stack.</p><p>We remove the empty <code>_item_box.html.erb</code> file.</p><p>And we can finally remove the <code>&lt;%= render("item_box", item: item) %&gt;</code> line, fairly confident that we didn't leave dead code behind.</p><p>This probably sounds more tedious than it is. It tends to be quick work, and you can take shortcuts – removing a swathe of lines that don't reference anything else, or that only call methods that you know are used elsewhere.</p></div></section></div><div><p>Content and design © <a href="https://henrik.nyh.se/">Henrik Nyh</a> (<a href="https://twitter.com/henrik">@henrik</a>). Code is under a <a href="http://en.wikipedia.org/wiki/MIT_License">MIT License</a> unless otherwise stated.</p><p>Pug art by <a href="https://johannaost.com/">Johanna Öst</a>; other graphics are under a <a href="http://creativecommons.org/licenses/by/3.0/">CC BY License</a>.</p><p>Powered by <a href="https://11ty.dev/">Eleventy</a>.</p></div></div>]]>
            </description>
            <link>https://thepugautomatic.com/2020/11/systematically-removing-code/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25058632</guid>
            <pubDate>Wed, 11 Nov 2020 14:26:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Introduction to Linear Algebra for Applied Machine Learning with Python]]>
            </title>
            <description>
<![CDATA[
Score 380 | Comments 48 (<a href="https://news.ycombinator.com/item?id=25058619">thread link</a>) | @Anon84
<br/>
November 11, 2020 | https://pabloinsente.github.io/intro-linear-algebra | <a href="https://web.archive.org/web/*/https://pabloinsente.github.io/intro-linear-algebra">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <section id="main_content">
        
<!-- https://docs.mathjax.org/en/latest/configuration.html#local-config-files -->




<article>
  <h2>Introduction to Linear Algebra for Applied Machine Learning with Python</h2>
  <time datetime="2020-05-26T00:00:00+00:00">26 May 2020</time>
  

  

<p>Linear algebra is to machine learning as flour to bakery: <strong>every machine learning model is based in linear algebra, as every cake is based in flour</strong>. It is not the only ingredient, of course. Machine learning models need vector calculus, probability, and optimization, as cakes need sugar, eggs, and butter. Applied machine learning, like bakery, is essentially about combining these mathematical ingredients in clever ways to create useful (tasty?) models.</p>

<p>This document contains <strong>introductory level linear algebra notes for applied machine learning</strong>. It is meant as a reference rather than a comprehensive review. If you ever get confused by matrix multiplication, don’t remember what was the $L_2$ norm, or the conditions for linear independence, this can serve as a quick reference. It also a good introduction for people that don’t need a deep understanding of linear algebra, but still want to learn about the fundamentals to read about machine learning or to use pre-packaged machine learning solutions. Further, it is a good source for people that learned linear algebra a while ago and need a refresher.</p>

<p>These notes are based in a series of (mostly) freely available textbooks, video lectures, and classes I’ve read, watched and taken in the past. If you want to obtain a deeper understanding or to find exercises for each topic, you may want to consult those sources directly.</p>

<p><strong>Free resources</strong>:</p>

<ul>
  <li><strong>Mathematics for Machine Learning</strong> by Deisenroth, Faisal, and Ong. 1st Ed. <a href="https://mml-book.github.io/">Book link</a>.</li>
  <li><strong>Introduction to Applied Linear Algebra</strong> by Boyd and Vandenberghe. 1sr Ed. <a href="http://vmls-book.stanford.edu/">Book link</a></li>
  <li><strong>Linear Algebra Ch. in Deep Learning</strong> by Goodfellow, Bengio, and Courville. 1st Ed. <a href="https://www.deeplearningbook.org/contents/linear_algebra.html">Chapter link</a>.</li>
  <li><strong>Linear Algebra Ch. in Dive into Deep Learning</strong> by Zhang, Lipton, Li, And Smola. <a href="https://d2l.ai/chapter_preliminaries/linear-algebra.html">Chapter link</a>.</li>
  <li><strong>Prof. Pavel Grinfeld’s Linear Algebra Lectures</strong> at Lemma. <a href="https://www.lem.ma/books/AIApowDnjlDDQrp-uOZVow/landing">Videos link</a>.</li>
  <li><strong>Prof. Gilbert Strang’s Linear Algebra Lectures</strong> at MIT. <a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/">Videos link</a>.</li>
  <li><strong>Salman Khan’s Linear Algebra Lectures</strong> at Khan Academy. <a href="https://www.khanacademy.org/math/linear-algebra">Videos link</a>.</li>
  <li><strong>3blue1brown’s Linear Algebra Series</strong> at YouTube. <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Videos link</a>.</li>
</ul>

<p><strong>Not-free resources</strong>:</p>

<ul>
  <li><strong>Introduction to Linear Algebra</strong> by Gilbert Strang. 5th Ed. <a href="https://www.amazon.com/Introduction-Linear-Algebra-Gilbert-Strang/dp/0980232775">Book link</a>.</li>
  <li><strong>No Bullshit Guide to Linear Algebra</strong> by Ivan Savov. 2nd Ed. <a href="https://www.amazon.com/No-bullshit-guide-linear-algebra/dp/0992001021">Book Link</a>.</li>
</ul>

<p>I’ve consulted all these resources at one point or another. Pavel Grinfeld’s lectures are my absolute favorites. Salman Khan’s lectures are really good for absolute beginners (they are long though). The famous 3blue1brown series in linear algebra is delightful to watch and to get a solid high-level view of linear algebra.</p>

<p>If you have to pic one book, I’d pic <strong>Boyd’s and Vandenberghe’s Intro to applied linear algebra</strong>, as it is the most beginner friendly book on linear algebra I’ve encounter. Every aspect of the notation is clearly explained and pretty much all the key content for applied machine learning is covered. The Linear Algebra Chapter in Goodfellow et al is a nice and concise introduction, but it may require some previous exposure to linear algebra concepts. Deisenroth et all book is probably the best and most comprehensive source for linear algebra for machine learning I’ve found, although it assumes that you are good at reading math (and at math more generally). Savov’s book it’s also great for beginners but requires time to digest. Professor Strang lectures are great too but I won’t recommend it for absolute beginners.</p>

<p>I’ll do my best to keep notation consistent. Nevertheless, learning to adjust to changing or inconsistent notation is a useful skill, since most authors will use their own preferred notation, and everyone seems to think that its/his/her own notation is better.</p>

<p>To make everything more dynamic and practical, I’ll introduce bits of Python code to exemplify each mathematical operation (when possible) with <code>NumPy</code>, which is the facto standard package for scientific computing in Python.</p>

<p>Finally, keep in mind this is created by a non-mathematician for (mostly) non-mathematicians. I wrote this as if I were talking to myself or a dear friend, which explains why my writing is sometimes conversational and informal.</p>

<p>If you find any mistake in notes feel free to reach me out at pcaceres@wisc.edu and to https://pablocaceres.org/ so I can correct the issue.</p>



<p><strong>Note:</strong> <em>underlined sections</em> are the newest sections and/or corrected ones.</p>

<p><strong><a href="#preliminary-concepts">Preliminary concepts</a></strong>:</p>
<ul>
  <li><a href="#sets">Sets</a></li>
  <li><a href="#belonging-and-inclusion">Belonging and inclusion</a></li>
  <li><a href="#set-specification">Set specification</a></li>
  <li><a href="#ordered-pairs">Ordered pairs</a></li>
  <li><a href="#relations">Relations</a></li>
  <li><a href="#functions">Functions</a></li>
</ul>

<p><strong><a href="#vectors">Vectors</a></strong>:</p>
<ul>
  <li><a href="#types-of-vectors">Types of vectors</a>
    <ul>
      <li><a href="#geometric-vectors">Geometric vectors</a></li>
      <li><a href="#polynomials">Polynomials</a></li>
      <li><a href="#elements-of-r">Elements of R</a></li>
    </ul>
  </li>
  <li><a href="#zero-vector-unit-vector-and-sparse-vector">Zero vector, unit vector, and sparse vector</a></li>
  <li><a href="#vector-dimensions-and-coordinate-system">Vector dimensions and coordinate system</a></li>
  <li><a href="#basic-vector-operations">Basic vector operations</a>
    <ul>
      <li><a href="#vector-vector-addition">Vector-vector addition</a></li>
      <li><a href="#vector-scalar-multiplication">Vector-scalar multiplication</a></li>
      <li><a href="#linear-combinations-of-vectors">Linear combinations of vectors</a></li>
      <li><a href="#vector-vector-multiplication-dot-product">Vector-vector multiplication: dot product</a></li>
    </ul>
  </li>
  <li><a href="#vector-space-span-and-subspace">Vector space, span, and subspace</a>
    <ul>
      <li><a href="#vector-space">Vector space</a></li>
      <li><a href="#vector-span">Vector span</a></li>
      <li><a href="#vector-subspaces">Vector subspaces</a></li>
    </ul>
  </li>
  <li><a href="#linear-dependence-and-independence">Linear dependence and independence</a></li>
  <li><a href="#vector-null-space">Vector null space</a></li>
  <li><a href="#vector-norms">Vector norms</a>
    <ul>
      <li><a href="#euclidean-norm">Euclidean norm: $L_2$</a></li>
      <li><a href="#manhattan-norm">Manhattan norm: $L_1$</a></li>
      <li><a href="#max-norm">Max norm: $L_\infty$</a></li>
    </ul>
  </li>
  <li><a href="#vector-inner-product-length-and-distance">Vector inner product, length, and distance</a></li>
  <li><a href="#vector-angles-and-orthogonality">Vector angles and orthogonality</a></li>
  <li><a href="#systems-of-linear-equations">Systems of linear equations</a></li>
</ul>

<p><strong><a href="#matrices">Matrices</a></strong>:</p>

<ul>
  <li><a href="#basic-matrix-operations">Basic matrix operations</a>
    <ul>
      <li><a href="#matrix-matrix-addition">Matrix-matrix addition</a></li>
      <li><a href="#matrix-scalar-multiplication">Matrix-scalar multiplication</a></li>
      <li><a href="#matrix-vector-multiplication-dot-product">Matrix-vector multiplication: dot product</a></li>
      <li><a href="#matrix-matrix-multiplication">Matrix-matrix multiplication</a></li>
      <li><a href="#matrix-identity">Matrix identity</a></li>
      <li><a href="#matrix-inverse">Matrix inverse</a></li>
      <li><a href="#matrix-transpose">Matrix transpose</a></li>
      <li><a href="#hadamard-product">Hadamard product</a></li>
    </ul>
  </li>
  <li><a href="#special-matrices">Special matrices</a>
    <ul>
      <li><a href="#rectangular-matrix">Rectangular matrix</a></li>
      <li><a href="#square-matrix">Square matrix</a></li>
      <li><a href="#diagonal-matrix">Diagonal matrix</a></li>
      <li><a href="#upper-triangular-matrix">Upper triangular matrix</a></li>
      <li><a href="#lower-triangular-matrix">Lower triangular matrix</a></li>
      <li><a href="#symmetric-matrix">Symmetric matrix</a></li>
      <li><a href="#identity-matrix">Identity matrix</a></li>
      <li><a href="#scalar-matrix">Scalar matrix</a></li>
      <li><a href="#null-or-zero-matrix">Null or zero matrix</a></li>
      <li><a href="#echelon-matrix">Echelon matrix</a></li>
      <li><a href="#antidiagonal-matrix">Antidiagonal matrix</a></li>
      <li><a href="#design-matrix">Design matrix</a></li>
    </ul>
  </li>
  <li><a href="#matrices-as-systems-of-linear-equations">Matrices as systems of linear equations</a></li>
  <li><a href="#the-four-fundamental-matrix-subsapces">The four fundamental matrix subsapces</a>
    <ul>
      <li><a href="#the-column-space">The column space</a></li>
      <li><a href="#the-row-space">The row space</a></li>
      <li><a href="#the-null-space">The null space</a></li>
      <li><a href="#the-null-space-of-the-transpose">The null space of the transpose</a></li>
    </ul>
  </li>
  <li><a href="#solving-systems-of-linear-equations-with-matrices">Solving systems of linear equations with matrices</a>
    <ul>
      <li><a href="#gaussian-elimination">Gaussian Elimination</a></li>
      <li><a href="#gauss-jordan-elimination">Gauss-Jordan Elimination</a></li>
    </ul>
  </li>
  <li><a href="#matrix-basis-and-rank">Matrix basis and rank</a></li>
  <li><a href="#matrix-norm">Matrix norm</a></li>
</ul>

<p><strong><a href="#linear-and-affine-mappings">Linear and affine mappings</a></strong>:</p>

<ul>
  <li><a href="#linear-mappings">Linear mappings</a></li>
  <li><a href="#examples-of-linear-mappings">Examples of linear mappings</a>
    <ul>
      <li><a href="#negation-matrix">Negation matrix</a></li>
      <li><a href="#reversal-matrix">Reversal matrix</a></li>
    </ul>
  </li>
  <li><a href="#examples-of-nonlinear-mappings">Examples of nonlinear mappings</a>
    <ul>
      <li><a href="#norms">Norms</a></li>
      <li><a href="#translation">Translation</a></li>
    </ul>
  </li>
  <li><a href="#affine-mappings">Affine mappings</a>
    <ul>
      <li><a href="#affine-combination-of-vectors">Affine combination of vectors</a></li>
      <li><a href="#affine-span">Affine span</a></li>
      <li><a href="#affine-space-and-subspace">Affine space and subspace</a></li>
      <li><a href="#affine-mappings-using-the-augmented-matrix">Affine mappings using the augmented matrix</a></li>
    </ul>
  </li>
  <li><a href="#special-linear-mappings">Special linear mappings</a>
    <ul>
      <li><a href="#scaling">Scaling</a></li>
      <li><a href="#reflection">Reflection</a></li>
      <li><a href="#shear">Shear</a></li>
      <li><a href="#rotation">Rotation</a></li>
    </ul>
  </li>
  <li><a href="#projections">Projections</a>
    <ul>
      <li><a href="#projections-onto-lines">Projections onto lines</a></li>
      <li><a href="#projections-onto-general-subspaces">Projections onto general subspaces</a></li>
      <li><a href="#projections-as-approximate-solutions-to-systems-of-linear-equations">Projections as approximate solutions to systems of linear equations</a></li>
    </ul>
  </li>
</ul>

<p><strong><a href="#matrix-decompositions">Matrix decompositions</a></strong>:</p>
<ul>
  <li><a href="#lu-decomposition">LU decomposition</a>
    <ul>
      <li><a href="#elementary-matrices">Elementary matrices</a></li>
      <li><a href="#the-inverse-of-elementary-matrices">The inverse of elementary matrices</a></li>
      <li><a href="#lu-decomposition-as-gaussian-elimination">LU decomposition as Gaussian Elimination</a></li>
      <li><a href="#lu-decomposition-with-pivoting">LU decomposition with pivoting</a></li>
    </ul>
  </li>
  <li><a href="#qr-decomposition">QR decomposition</a>
    <ul>
      <li><a href="#orthonormal-basis">Orthonormal basis</a></li>
      <li><a href="#orthonormal-basis-transpose">Orthonormal basis transpose</a></li>
      <li><a href="#gram-schmidt-orthogonalization">Gram-Schmidt Orthogonalization </a></li>
      <li><a href="#qr-decomposition-as-gram-schmidt-orthogonalization">QR decomposition as Gram-Schmidt Orthogonalization</a></li>
    </ul>
  </li>
  <li><a href="#determinant">Determinant</a>
    <ul>
      <li><a href="#determinant-as-measures-of-volume">Determinant as measures of volume</a></li>
      <li><a href="#the-2-x-2-determinant">The 2X2 determinant</a></li>
      <li><a href="#the-n-x-n-determinant">The NXN determinant</a></li>
      <li><a href="#determinants-as-scaling-factors">Determinants as scaling factors</a></li>
      <li><a href="#the-importance-of-determinants">The importance of determinants</a></li>
    </ul>
  </li>
  <li><a href="#eigenthings">Eigenthings</a>
    <ul>
      <li><a href="#change-of-basis">Change of basis</a></li>
      <li><a href="#eigenvectors-eigenvalues-and-eigenspaces">Eigenvectors, Eigenvalues, and Eigenspaces</a></li>
      <li><a href="#trace-and-determinant-with-eigenvalues">Trace and determinant with eigenvalues</a></li>
      <li><a href="#eigendecomposition">Eigendecomposition</a></li>
      <li><a href="#eigenbasis-are-a-good-basis">Eigenbasis are a good basis</a></li>
      <li><a href="#geometric-interpretation-of-eigendecomposition">Geometric interpretation of Eigendecomposition</a></li>
      <li><a href="#the-problem-with-eigendecomposition">The problem with Eigendecomposition</a></li>
    </ul>
  </li>
  <li><a href="#singular-value-decomposition">Singular Value Decomposition</a>:
    <ul>
      <li><a href="#singular-value-decomposition-theorem">Singular Value Decomposition Theorem</a></li>
      <li><a href="#singular-value-decomposition-computation">Singular Value Decomposition computation</a></li>
      <li><a href="#geometric-interpretation-of-the-singular-value-decomposition">Geometric interpretation of the Singular Value Decomposition</a></li>
      <li><a href="#singular-value-decomposition-vs-eigendecomposition">Singular Value Decomposition vs Eigendecomposition</a></li>
    </ul>
  </li>
  <li><a href="#matrix-approximation">Matrix Approximation</a>:
    <ul>
      <li><a href="#best-rank-k-approximation-with-svd">Best rank-k approximation with SVD</a></li>
      <li><a href="#best-low-rank-approximation-as-a-minimization-problem">Best low-rank approximation as a minimization problem</a></li>
    </ul>
  </li>
</ul>

<p><strong><a href="#epilogue">Epilogue</a></strong></p>



<p>While writing about linear mappings, I realized the importance of having a basic understanding of a few concepts before approaching the study of linear algebra. If you are like me, you may not have formal mathematical training beyond high school. If so, I encourage you to read this section and spent some time wrapping your head around these concepts before going over the linear algebra content (otherwise, you might prefer to skip this part). I believe that reviewing these concepts is of great help to understand the <em>notation</em>, which in my experience is one of the main barriers to understand mathematics for nonmathematicians: we are <em>non</em>native speakers, so we are continuously building up our vocabulary. I’ll keep this section very short, as is not the focus of this mini-course.</p>

<p>For this section, my notes are based on readings of:</p>

<ul>
  <li><strong>Geometric transformations (Vol. 1)</strong> (1966) by Modenov &amp; Parkhomenko</li>
  <li><strong>Naive Set Theory</strong> (1960) by P.R. Halmos</li>
  <li><strong>Abstract Algebra: Theory and Applications</strong> (2016) by Judson &amp; Beeer. <a href="http://abstract.pugetsound.edu/download/aata-20160809.pdf">Book link</a></li>
</ul>

<h2 id="sets">Sets</h2>

<p>Sets are one of the most fundamental concepts in mathematics. They are so fundamental that they are not defined in terms of anything else. On the contrary, other branches of mathematics are defined in terms of sets, including linear algebra. Put simply, <strong>sets are well-defined collections of objects</strong>. Such objects are called <strong>elements or members</strong> of the set. The crew of a ship, a caravan of camels, and the LA Lakers roster, are all examples of sets. The captain of the ship, the first camel in the caravan, and LeBron James are all examples of “members” or “elements” of their corresponding sets. We denote a set with an upper case italic letter as $\textit{A}$. In the context of linear algebra, we say that a line is a set of points, and the set of all lines in the plane is a set of sets. Similarly, we can say that <em>vectors</em> are sets of points, and <em>matrices</em> sets of vectors.</p>

<h2 id="belonging-and-inclusion">Belonging and inclusion</h2>

<p>We build sets using the notion of <strong>belonging</strong>. We denote that $a$ <em>belongs</em> (or is an <em>element</em> or <em>member</em> of) to $\textit{A}$ with the Greek letter epsilon as:</p>



<p>Another important idea is <strong>inclusion</strong>, which allow us to build <em>subsets</em>. Consider sets $\textit{A}$ and $\textit{B}$. When every element of $\textit{A}$ is an element of $\textit{B}$, we say that $\textit{A}$ is a <em>subset</em> of $\textit{B}$, or that $\textit{B}$ <em>includes</em> $\textit{A}$. The notation is:</p>



<p>or</p>



<p>Belonging and inclusion are derived from <strong>axion of extension</strong>: <em>two sets are equal if and only if they have the same elements</em>. This axiom may sound trivially obvious but is necessary to make belonging and …</p></article></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pabloinsente.github.io/intro-linear-algebra">https://pabloinsente.github.io/intro-linear-algebra</a></em></p>]]>
            </description>
            <link>https://pabloinsente.github.io/intro-linear-algebra</link>
            <guid isPermaLink="false">hacker-news-small-sites-25058619</guid>
            <pubDate>Wed, 11 Nov 2020 14:24:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What mother never told you about VM service (1983) [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25058543">thread link</a>) | @fanf2
<br/>
November 11, 2020 | http://www.leeandmelindavarian.com/Melinda/tutorial.pdf | <a href="https://web.archive.org/web/*/http://www.leeandmelindavarian.com/Melinda/tutorial.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://www.leeandmelindavarian.com/Melinda/tutorial.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25058543</guid>
            <pubDate>Wed, 11 Nov 2020 14:15:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[$200k in sales from a $6k advertisement]]>
            </title>
            <description>
<![CDATA[
Score 371 | Comments 102 (<a href="https://news.ycombinator.com/item?id=25058363">thread link</a>) | @mildlyclassic
<br/>
November 11, 2020 | https://www.wifidabba.com/blog/200000-dollars-in-sales-from-one-daringfireball-ad | <a href="https://web.archive.org/web/*/https://www.wifidabba.com/blog/200000-dollars-in-sales-from-one-daringfireball-ad">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
      <div>
        <div>
          <div>


          
            <table>
                <thead>
                  <tr>
                    <th>
                      Metric
                    </th>
                    <th>
                      Count
                    </th>
                  </tr>
                </thead>
                <tbody>
                <tr data-href="#!">
                    <td>
                      Time period
                    </td>
                    <td>
                      Aug 17,2020 - Aug 24,2020
                    </td>
                  </tr>
                  <tr data-href="#!">
                    <td>
                      Visitors
                    </td>
                    <td>
                      7,200
                    </td>
                  </tr>
                  <tr data-href="#!">
                    <td>
                      Emails
                    </td>
                    <td>
                      45
                    </td>
                    
                  </tr>
                  <tr data-href="#!">
                    <td>
                      Video calls
                    </td>
                    <td>
                      30
                    </td>
                    
                  </tr>
                  <tr data-href="#!">
                    <td>
                      <strong>Units sold</strong>
                    </td>
                    <td>
                      <b>10</b>
                    </td>
                  </tr>   
                  <tr data-href="#!">
                    <td>
                      <strong>Unit price</strong>
                    </td>
                    <td>
                      <b>$20,000</b>
                    </td>
                  </tr>
                  <tr data-href="#!">
                    <td>
                      <strong>Total revenue from DF</strong>
                    </td>
                    <td>
                      <b>$200,000</b>
                    </td>
                  </tr>
                  
                </tbody>
              </table>
              
              
          <ul>
              <li>
                Build cheap broadband distribution technology.
              </li>
              <li>
                Prove the tech works by connecting 1M people in one city.
              </li>
              <li>
                Deploy across 1,000 cities in India
              </li>
            </ul>

            

            <p>
                Our goal at <a href="https://www.wifidabba.com/">Wifi Dabba</a> is to lower the cost of broadband access in India. We use lasers instead of underground fiber as our core network and commodity components to dramatically lower the cost of deploying a broadband network.
            
                We've been running a beta network in Bengaluru, India for the last 9 months serving thousands of live customers. We're now ready to deploy a city wide network and provide cheap internet access to a million people.
            </p>

            <iframe width="100%" height="450" src="https://www.youtube.com/embed/LwVWJXBNQg8?autoplay=1" srcdoc="<style>*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}</style><a href=https://www.youtube.com/embed/LwVWJXBNQg8?autoplay=1><img width='100%' style='min-height:250px;' src='https://img.youtube.com/vi/LwVWJXBNQg8/hqdefault.jpg' alt='Wifi Dabba overview'><span>▶</span></a>" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" title="Wifi Dabba overview"></iframe>

            
            <p>
              A core tenet of the Wifi Dabba network is distributed ownership. We believe that ownership of the internet should be in the hands of as many people as possible. If the cost of broadband tech drops, then more people can help pay for the cost of the network.
              And if you're one of the people paying for the distribution, we believe you should get revenue in return.
            </p>

            <p>
              We've divided the city of Bengaluru into 100 regions called PoPs. Anyone can buy a region and get a share in the revenue from those subscribers. 
            </p>

            <p>
              The Wifi Dabba franchise model:
            </p>

            <ul>
              <li>
                <strong>$20,000</strong> to purchase a 4sqkm. PoP.
              </li>
              <li>
                <strong>Minimum guaranteed revenue</strong> Paid quarterly with a 6 year rev share agreement.
              </li>
              <li>
                <strong>Fully managed service</strong> Be an absentee landlord
              </li>
            </ul>

            

            <p>
              <span>We've sold 40 as of the time of writing this.</span>
              <br>
              
              Wifi Dabba is insanely lucky for the amount of public support we have as a company. We regularly get phone calls, emails and even people dropping by our office just to tell us they like our service. Over the last 3 years we've received dozens of emails from people requesting franchises or other types of partnerships.
              We're incredibly humbled and thankful for this support on a daily basis.
            </p>

            
            <p>
              We believe there is a large group of people that care about the future of the internet and would be willing to put their money where their mouths are. As long as the price and the level of risk involved is reasonable.
              Our gut told us that this group would most likely be people that have seen success in the technology business as engineers, operators and entreprenuers.
            </p>

            
            <p>
              We've had our heads down over the last three years building and testing our network stack. Publicity or notariety has never been high on our list. 
              We've begun ramping up our social media efforts but it was clear that to kickstart our outreach, we had to do a little bit of advertising.
            </p>

            
            
              <p><img src="https://www.wifidabba.com/images/df-venn.png" height="300" width="330" alt="...">
                <br>
              </p>
            <p>
              <a href="https://www.daringfireball.net/">Daringfireball.net</a> is a great blog authored by <a href="https://en.wikipedia.org/wiki/John_Gruber">John Gruber</a> who is also the creator of <a href="https://daringfireball.net/projects/markdown/">Markdown</a>.
              DF was a natural choice for us as we've been readers of the blog for a long while and we knew from experience that DF readers would fit our target market rather well. Given the high quality of John's writing and insights into the industry, we felt that there would be a large pool of senior tech veterans that would be interested in Wifi Dabba among DF's audience.
            </p>

            <p>
                The sponsorship cost us $6,500 and ran for the week starting Aug 17, 2020 and we got:
                </p><ul>
                <li>
                  A display ad in the sidebar on every page of the site, all week long.
                </li>
                <li>
                  A post from the sponsor in the RSS feed at the start of the week. Us, the sponsor, got to address Daring Fireball’s most dedicated readers directly.
                </li>
                <li>
                  At the end of the week, John also posts an item thanking and linking to the feed sponsor.
                </li>
              </ul>
            

            <p>
                Stats about DF readership
                </p><ul>
                <li>
                Typical weekday web page views: 80,000–100,000.
                </li>
                <li>
                Estimated monthly web page views: 2.5 million.
                </li>
                <li>
                Estimated Daring Fireball RSS feed subscribers: Over 200,000.
                </li>
                <li>
                Twitter followers on the @daringfireball  account: Over 92,000.
                </li>
              </ul>
            
            

            
            <p>
                We created two variants of our message. Designed in bold colours to stand out against DF's dark theme. These creatives rotated randomly.
                We decided to focus on the technology because of the nature of the audience and hoped that the website did a good job of explaining the product.  
                </p><p><img src="https://www.wifidabba.com/images/df-ads.png" width="100%" alt="Buy internet POP">
                </p>
            

            
            <div>
                <p><strong>Click Ad -&gt; Browse site -&gt; Setup a call</strong></p><p>

                We expected visitors to click on the ad in DF and land on our homepage. Once on our site, we hoped that visitors would check out our videos as well as browse through a few pages.
                If they liked what they saw, we had a prominent buy button on the front page which led to a page to setup a video call.
            </p></div>

            <p>
                It's worth noting here that we knew going in that a large percentage of DF's audience would be using Ad-blockers. Nothing wrong with that, we use ad-blockers ourselves.
            </p>

            <p>
              Furthermore, we made a deliberate choice to add a high friction call to action and contact process. In order to purchase a PoP, a visitor would be directed to a calendar managed by calendly that would help them setup a call with someone from our team at a convenient time.
            </p>
            <p><a href="https://wifidabba.com/buy">
              <img width="100%" src="https://www.wifidabba.com/images/df-buy.png" alt="Setup Call">
            </a></p><p>
              The reason for this is that we knew DF would deliver a few hundred visitors a day to our site. We're a small team and our core focus is deploying the network, not necessarily sales and our goal is to sell the PoPs to people that are really excited a lot about our idea and show a high level of interest.
              The $20,000 price point of our product + the high friction of the contact process + users that are OK with ads = A high signal to noise ratio from DF visitors. 
              We'd love to hear any feedback on what you think about this.
            </p>

            
              

              <p>
                Our thesis turned out to be pretty spot on. Senior engineers from Google, Apple and a host of other technology companies purchased the PoPs.
                The actual sales process turned out to be fairly quick and straight forward. Most of the people that purchased the PoPs did so within a period of 48 hours of having the call.
              </p>

              



          </div>
        </div>
    </div>
    </section></div>]]>
            </description>
            <link>https://www.wifidabba.com/blog/200000-dollars-in-sales-from-one-daringfireball-ad</link>
            <guid isPermaLink="false">hacker-news-small-sites-25058363</guid>
            <pubDate>Wed, 11 Nov 2020 13:51:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Visualization of connections between politicians and orgs awarded gov contracts]]>
            </title>
            <description>
<![CDATA[
Score 34 | Comments 5 (<a href="https://news.ycombinator.com/item?id=25057931">thread link</a>) | @roxanneonhacker
<br/>
November 11, 2020 | https://sophieehill.shinyapps.io/my-little-crony/ | <a href="https://web.archive.org/web/*/https://sophieehill.shinyapps.io/my-little-crony/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <p>
          A visualization of the connections between
          <strong>Tory politicians</strong>
          and
          <strong>companies being awarded government contracts during the pandemic,</strong>
          based on reporting by
          <a href="https://www.opendemocracy.net/en/dark-money-investigations/">openDemocracy,</a>
          <a href="https://bylinetimes.com/">Byline Times,</a>
          and more.
        </p>
    </div></div>]]>
            </description>
            <link>https://sophieehill.shinyapps.io/my-little-crony/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25057931</guid>
            <pubDate>Wed, 11 Nov 2020 12:57:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Advanced System-on-Chip Design Lecture Notes]]>
            </title>
            <description>
<![CDATA[
Score 204 | Comments 15 (<a href="https://news.ycombinator.com/item?id=25057889">thread link</a>) | @allending
<br/>
November 11, 2020 | https://iis-people.ee.ethz.ch/~gmichi/asocd/lecturenotes/ | <a href="https://web.archive.org/web/*/https://iis-people.ee.ethz.ch/~gmichi/asocd/lecturenotes/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://iis-people.ee.ethz.ch/~gmichi/asocd/lecturenotes/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25057889</guid>
            <pubDate>Wed, 11 Nov 2020 12:51:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hardly Working with Cloudflare Workers]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25057709">thread link</a>) | @malthejorgensen
<br/>
November 11, 2020 | https://blog.notifly.io/2020/11/04/hardly-working-with-cloudflare-workers | <a href="https://web.archive.org/web/*/https://blog.notifly.io/2020/11/04/hardly-working-with-cloudflare-workers">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <!-- Possible titles:
Cloudflare workers are hard to work with
Working with Cloudflare Workers
Hardly working with Cloudflare Workers
-->

<p><em>Note: The team behind Notifly also runs <a href="https://www.eduflow.com/">Eduflow</a> and <a href="https://www.peergrade.io/">Peergrade</a>.</em></p>

<h2 id="introduction">Introduction</h2>

<p>This is the story of me trying to replace a simple NGINX reverse proxy (plus some basic redirects) with a Cloudflare Worker.</p>

<p>Our old landing page is a Wordpress blog hosted on WPEngine. Historically, this has always been set up behind an NGINX reverse proxy serving at <a href="http://peergrade.io/">peergrade.io</a> and <a href="http://www.peergrade.io/">www.peergrade.io</a>. The reverse proxy was needed for doing various redirects outside of Wordpress and doing some cookie trickery to redirect to <a href="http://app.peergrade.io/">app.peergrade.io</a> if the session cookie for the app was present.</p>

<p>The reverse proxy is hosted on DigitalOcean and is the only thing we have hosted there, so I wanted to get rid of it. We already use Cloudflare and so I thought “this would be a good test to try out Cloudflare Workers”. And less infrastructure is better, right?</p>

<h2 id="the-good-parts">The good parts</h2>

<p>Getting set up with <code>wrangler</code> – the CLI for Cloudflare Workers – was a breeze. It gives you a webpack setup out of the box which allowed me to install NPM packages and use them without any extra work on my part. I eventually downgraded to a setup without webpack (called “javascript” in <code>wrangler</code>) – since I ended up not needing any packages.</p>

<p>The vanilla Javascript setup allows you to “live edit” the worker at <code>https://dash.cloudflare.com/&lt;account-id&gt;/workers/edit/&lt;worker-slug&gt;</code></p>

<p>Here you can edit and run the updated script without saving and deploying the worker, allowing for a very fast and easy “edit-compile-run” loop.</p>

<p>Another cool thing is that you can change the URL in the small “browser” on the page to your liking – this is very useful for testing out proxies and other things that depend on the domain name or precise URL being sent to the worker. The debugger part of the UI is also incredibly useful but does have a tendency to disconnect from time to time.</p>

<h2 id="page-rules-vs-workers">Page Rules vs. Workers</h2>

<p>In a classic setup you’ll usually have a couple of redirects alongside your reverse proxy – and so do we. We use <a href="http://www.peergrade.io/">www.peergrade.io</a> as our canonical domain so we redirect peergrade.io to www.peergrade.io and we redirect http:// to https://.</p>

<p>This can be set up easily in Cloudflare by adding a couple of redirects in your Page Rules.</p>

<p>However, redirects from page rules are applied after any worker on the same URL. Since my worker’s default action is to reverse proxy, the redirect page rule will never be hit.</p>

<p>Annoyingly, this isn’t clearly described in the docs and you’ll have to find <a href="https://community.cloudflare.com/t/cf-workers-and-rate-limiting-firewall-rules-bot-management/132164/3">this forum post</a> from the official Cloudflare forum to know that.
The post notes that “<em>security-related ones will run before [workers]</em>” – but which ones are those? (All respect to Kenton Varda who wrote the post and is the main architect behind Cloudflare Workers. Cloudflare Workers <em>are</em> very very cool, but they are also a bit more quirky than I’d like at the moment)</p>

<p>In order to preserve these redirects, I’ll have to manually write them in the worker code (or relay the URLs that need to redirect to Cloudflare itself – which is basically the same amount of code).</p>

<!-- 
- An aside:

    Apparently *Always Use HTTPS* is such a "security-related" page rule, even though it's basically an http:// to https:// redirect. Cloudflare even admits to that [in the docs](https://support.cloudflare.com/hc/en-us/articles/204144518-SSL-FAQ#h_a61bfdef-08dd-40f8-8888-7edd8e40d156). 

    Cloudflare Page Rules allows you to set up multiple rules for a single URL-pattern, but then only allows you to use that pattern once. However, *Always Use HTTPS* is special and doesn't allow any other rules once it's used on a URL-pattern. This means if you want *Automatic HTTPS Rewrites* on top of *Always Use HTTPS* you have to specify 2 rules:

    1. www.peergrade.io – *Always Use HTTPS*
    2. [https://www.peergrade.io](https://www.peergrade.io) – *Automatic HTTPS Rewrites*
-->

<p>The same thing goes for cache rules. I had previously been using a page rule to aggressively cache static assets and user-uploaded content served from Wordpress. That now has to be written inside the worker as well.</p>

<p>Page rules have an internal ordering that you can set. Rules that match the given URL are executed in order – so that if two redirect rules match the URL, the first one in the ordering will be used. It would be <em>really nice</em> if workers could be added to the same list – that would mean I could put the redirects and cache rules before my worker and much more easily handle this scenario. <em>In principle</em> this would be easy if all the built-in page rules were reimplemented as workers, but there’s probably legacy behaviors and tie-ins to the rest of the stack that makes that impossible or at least non-trivial. (Still hoping for a future update on this 🤞🏻)</p>

<h2 id="the-reverse-proxy">The reverse proxy</h2>

<p>Back to the main task at hand – we’re implementing a simple reverse proxy and that happens to be <a href="https://developers.cloudflare.com/workers/examples/bulk-origin-proxy">one of the examples</a> in the Cloudflare Worker docs. However, getting it set up myself I quickly ran into issues with redirect loops and cases where my origin would redirect for seemingly no reason. To be fair, proxying can be tricky to get right since it’s hard to test properly before rollout, and on top of that you have DNS propagation and caching, which means there might be timing issues. But even with that, it seemed extra tricky with Cloudflare Workers.</p>

<p>On closer inspection, the example from the Cloudflare docs seems to defy reasoning. The incoming request in the example must have the header <code>Host: google.yourdomain.com</code> in order for it to match the Google entry in <code>ORIGINS</code>. I was able to confirm as much by inspecting the incoming request in the Cloudflare worker debugger. That incoming request is then relayed directly to <code>www.google.com</code>. Let’s try that ourselves:</p>

<div><div><pre><code>curl -H 'Host: google.yourdomain.com' https://www.google.com
</code></pre></div></div>

<p>The response we get is a 404 page (which makes sense since the host doesn’t match). However, the Cloudflare worker doesn’t get a 404 – it renders the familiar Google search frontpage. Something must be happening behind the scenes. That something is what I call “The Web Platform” part of Cloudflare Workers.</p>

<h2 id="the-web-platform">The Web Platform</h2>

<p>Cloudflare Workers uses Chrome’s V8 as its execution engine and this also sets the context in which your script is run.</p>

<p>The available API is a very small subset of <a href="https://platform.html5.org/">The Web Platform</a> (the Javascript API available in modern browsers) – specifically Ecmascript/Javascript itself, plus <code>Fetch</code>, <code>URL</code>, and <code>Blob</code>. I believe Cloudflare chose this API because it melds well with V8, but also because web devs will be familiar with those APIs. But how familiar are you <em>really</em> with <code>fetch</code>, <code>Request</code>, and <code>Response</code>? (all part of the <code>Fetch</code>-spec)<br>
I don’t think I actually knew the <code>Request</code> and <code>Response</code>-objects in any detail before using Cloudflare Workers – having gotten along just fine with variations of</p>

<div><div><pre><code>fetch('http://example.org', { options }).then((r) =&gt; r.json())
</code></pre></div></div>

<p>plus some error handling on top for many years.</p>

<p>When working with Workers what you’ll mostly be doing is to manipulate the incoming <code>Request</code>-object  and pass it on to <code>fetch</code>, or manipulate the outgoing <code>Response</code>-object and passing that on to Cloudflare’s handler. Have you ever manually created a <code>Request</code>-object in the browser? I haven’t. The reason this gets complicated is the fact that the spec for <code>fetch</code> itself is very “loose”. For example, <code>fetch</code> can take either a <code>Request</code>-object or a simple Javascript object that just looks a lot like a <code>Request</code>-object as its argument – and it not really clear what differences between the two are.
<code>fetch</code> also allows passing a <code>Request</code>-objects as both its first and second argument <code>fetch(Request(...), Request(...))</code> – good luck trying to figure out what that does!</p>

<p>If we go back to the example from the Cloudflare docs – what’s going on “behind the scenes” in our proxy example from earlier is that you can’t change the <code>Host</code>-header when doing a <code>fetch</code>. This makes a lot of security sense in the browser where <code>fetch</code> normally lives, but it’s quite normal behavior for a reverse proxy and actually something I was doing in my NGINX setup in order to have WPEngine respond with the right content. It’s not a behavior you’ve ever needed or thought about when using <code>fetch</code> in the browser.
The server is just a very different environment than the browser. The browser Javascript API is not built with server functionality in mind, and it ends up being a hamstring when working with Cloudflare Workers.</p>

<h2 id="maybe-maybe-maybe">Maybe, maybe, maybe…?</h2>

<p>A bunch of forum posts on community.cloudflare.com talk about this issue</p>

<ul>
  <li><strong>Only available on the Enterprise plan?</strong> <a href="https://community.cloudflare.com/t/override-host-header-using-workers/73434/2">This forum post</a> describes that setting the <code>Host</code> -header in workers is not possible. Followed up with <a href="https://community.cloudflare.com/t/override-host-header-using-workers/73434/5">a later answer</a> that it’s possible but only for Enterprise accounts.
 <a href="https://community.cloudflare.com/t/reverse-proxy-using-page-rules/47836/16">This other post</a> says the same.</li>
  <li><strong>Kenton Varda to the rescue</strong> In response to <a href="https://community.cloudflare.com/t/not-possible-to-override-the-host-header-on-workers-requests/13077/7">this post</a> Kenton Varda actually extends Cloudflare Workers with the <code>cf.resolveOverride</code>-flag on the <code>Request</code>-object,
which should allow at least part of the reverse proxy setup to work.
Unfortunately, to explain the new feature the post just links to the top-level URL of the documentation for Cloudflare Workers – which currently doesn’t
describe how  <code>cf.resolveOverride</code> works and how to use it.</li>
  <li><strong>The missing documentation</strong> <a href="https://community.cloudflare.com/t/different-hostname-with-same-origin-in-workers/16662/12">This older post</a> seemingly cites documentation that no longer exists! :(<br>
 I have been unable to find any meaningful documentation of <code>cf.resolveOverride</code> outside of the community forum, and I was unable to have it allow me to switch the <code>Host</code>-header.</li>
</ul>

<h2 id="the-final-nail-in-the-coffin">The final nail in the coffin</h2>

<p>For a brief moment I actually thought my setup was working, but it only “looked” like it was working due to the following sequence of events:</p>

<ul>
  <li>A request for <code>www.peergrade.io</code> would hit the worker</li>
  <li>The worker would then do a request to <code>peergrade.wpengine.com</code></li>
  <li>Wordpress/WPEngine would then respond with a redirect to <code>www.peergrade.io</code> since the <code>Host</code>-header is incorrect</li>
  <li>Cloudflare by default then follows that redirect and makes a new request to <code>www.peergrade.io</code>.
Since Cloudflare is the host of <code>www.peergrade.io</code> you’d think we’d hit infinite recursion here.
But magically, it doesn’t just enter the worker script again – it knows (somehow) it has to go further down the Cloudflare stack.
Since the DNS A-record in Cloudflare still had the IP of the DigitalOcean instance, that final fetch would simply fetch the page from the old proxy server which worked as it always had 🤦🏻</li>
</ul>

<!--
Another example of this "familiar but unfamiliar" API is when I was trying to inspect the session cookie: I had to do a base64 decode into a `Uint8Array` (in order to do a zlib decompression). The function available for decoding base64 is `atob` which you may know from the browser.

However, in order to get the actual binary data you'll have to do this Javascript incantation:

```jsx
const weirdstr = atob(cookiestr);
const bytearray = new Uint8Array(new ArrayBuffer(weirdstr.length));

for (let i = 0; i < weirdstr.length; i++) {
  bytearray[i] = weirdstr.charCodeAt(i);
}
```

Again, this isn't Cloudflare's fault per se, but they're inheriting a bad choice from The Web Platform where they could have done something else. That bad choice becomes accentuated by the fact that most workers need to implement something that is basically backend or proxy server behavior, which by now you can see The Web Platform really isn't set up for. 

Similarly, you'll inherit this weird quirk directly from the browser Javascript engine:

```jsx
console.log(btoa('汉字'))
// The above raises a DOMException in your Cloudflare Worker with the
// following message:
// "btoa() can only operate on characters in the Latin1 (ISO/IEC 8859-1) range."
```

Yes yes, there's some sense to this – Javascript strings are UTF-16 and that's why this example doesn't work. But take a look at Node.js where `btoa` and `atob` are not available – Node.js has a much better answer to many of these problems.

Lastly, since many things are iterables or DOM-objects, you won't get anything useful out of console logging `request.headers`, `request.headers.keys()`, `request.headers.values()`, `request.headers.entries()`. This wouldn't be a problem if the `request`-object was fully inspectable in the debugger but nothing shows up when you open up `request.headers`.
The solution to this is just `console.log([...request.headers])`.
-->

<h2 id="conclusion">Conclusion</h2>

<p>Overall, Cloudflare Workers are really cool and the tooling around them is pretty great. But I do feel like it was an unfortunate choice to adopt The Web Platform
instead of using parts of the Node standard library or a different, more server-oriented API. Lastly, while the documentation feels fairly complete and fleshed out – the fact
that the answers on the forum tell 2-3 different stories about whether it’s possible to change the <code>Host</code>-header means that it’s something that is …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.notifly.io/2020/11/04/hardly-working-with-cloudflare-workers">https://blog.notifly.io/2020/11/04/hardly-working-with-cloudflare-workers</a></em></p>]]>
            </description>
            <link>https://blog.notifly.io/2020/11/04/hardly-working-with-cloudflare-workers</link>
            <guid isPermaLink="false">hacker-news-small-sites-25057709</guid>
            <pubDate>Wed, 11 Nov 2020 12:15:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Write Unit Tests for Logging]]>
            </title>
            <description>
<![CDATA[
Score 54 | Comments 35 (<a href="https://news.ycombinator.com/item?id=25057372">thread link</a>) | @JanVanRyswyck
<br/>
November 11, 2020 | https://principal-it.eu/2020/11/unit-tests-for-logging/ | <a href="https://web.archive.org/web/*/https://principal-it.eu/2020/11/unit-tests-for-logging/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<div>
					<h2>
						How To Write Unit Tests For Logging
					</h2>
					<p><span>
						November 11, 2020
					</span>
				</p></div>

				
<p>Once in a while I get asked the question whether one should write <a href="https://principal-it.eu/2019/10/taxonomy-of-tests/">solitary tests</a> for 
logging functionality. My answer to this question is the typical consultant answer: “It depends”. In essence, logging 
is an infrastructure concern. The end result is log data that is being written to a resource which is external to 
an application. Usually the generated data ends up in a file, a database or it might even end up in a cloud service.</p>

<p>Because logging crosses the process boundary of an application, it is more useful to write 
<a href="https://principal-it.eu/2019/10/taxonomy-of-tests/">sociable tests</a> to verify this particular functionality. It doesn’t make sense to 
use solitary tests in this particular case.</p>

<p>That being said, there are situations where business requirements explicitly state that logging should be a part of the 
interface of an application. In this situation, the intent of logging should be expressed explicitly by the code which 
in turn should also be exercised by solitary tests. The excellent book 
<a href="https://bit.ly/tdd-goos2" target="blank" rel="noopener noreferrer nofollow">Growing Object Oriented Software 
Guided By Tests</a>, written by Steve Freeman and Nat Pryce, mentions that there are generally two separate types of 
logging:</p>

<ul>
  <li>Support logging</li>
  <li>Diagnostic logging</li>
</ul>

<p>A support log contains messages that are intended for those that perform operational activities. These messages are used 
to determine whether the system behaves correctly or not. The log level for these messages is usually of type <em>error</em> 
or <em>info</em>.</p>

<p>A diagnostic log on the other hand holds messages that are targeted towards software developers. These messages provide 
valuable insights into the details of a running system. The log level for these messages is usually of type <em>debug</em> or 
<em>trace</em>.</p>

<p>Given these two types of logging, the basic idea is that code which expresses the intent of support logging should be 
exercised by solitary tests. Code statements that initiate diagnostic logging are usually not covered by tests.</p>

<p>Let’s have a look at an example that demonstrates both support and diagnostic logging in action.</p>

<pre><code>public class ExpenseSheetController : Controller
{
    private readonly ICommandHandler&lt;CreateExpenseSheet&gt; _commandHandler;
    private readonly ISupportNotifier _supportNotifier;

    public ExpenseSheetController(ICommandHandler&lt;CreateExpenseSheet&gt; commandHandler,
                                  ISupportNotifier supportNotifier)
    {
        _commandHandler = commandHandler;
        _supportNotifier = supportNotifier;
    }
    
    [HttpPost]
    [ServiceFilter(typeof(PerformanceTracing))]
    public IActionResult Create(CreateExpenseSheetFormModel formModel)
    {
        try
        {
            var command = new CreateExpenseSheet(Guid.NewGuid(), formModel.EmployeeId);
            _commandHandler.Handle(command);
        }
        catch(Exception ex)
        {
            _supportNotifier.ErrorDuringExpenseSheetCreation(ex, formModel.EmployeeId);
            return BadRequest();
        }
        
        _supportNotifier.ExpenseSheetCreated(formModel.EmployeeId);
        return Ok();
    }
}
</code></pre>

<p>Here we have the implementation of a controller that can receive a request for creating a new expense sheet. Notice that 
the constructor of this controller class expects an instance of the <em>ISupportNotifier</em> interface. This dependency is 
being used by the implementation of the <em>Create</em> method for logging an error when an exception occurs. It is also used 
for logging when an expense sheet has been successfully created.</p>

<p>This is how the implementation of the <em>SupportNotifier</em> looks like.</p>

<pre><code>public class SupportNotifier : ISupportNotifier
{
    private readonly ILogger&lt;SupportNotifier&gt; _logger;

    public SupportNotifier(ILogger&lt;SupportNotifier&gt; logger)
    {
        _logger = logger;
    }
    
    public void ExpenseSheetCreated(Guid employeeId)
    {
        _logger.LogInformation("Expense sheet created for employee with ID '{employeeId}'.");
    }

    public void ErrorDuringExpenseSheetCreation(Exception ex, Guid employeeId)
    {
        _logger.LogError(ex, $"Unable to create a new expense sheet for employee with ID '{employeeId}'");
    }
}
</code></pre>

<p>This code demonstrates that support logging uses log levels <em>error</em> or <em>info</em> depending on the context. Verifying the
code of the <em>SupportNotifier</em> class itself can be done by using sociable tests. It’s not a good idea to write
solitary tests for the <em>SupportNotifier</em> class. This would imply that a test double should be used as an instance of 
<em>ILogger</em>. As we already touched on in a <a href="https://principal-it.eu/2020/05/test-double-heuristics/">previous blog post</a>, it’s much better to 
avoid using test doubles for types that you don’t own. In this particular case it would even be quite hard to do as 
the <em>Logxx</em> methods of <em>ILogger</em> are actually extension methods and not regular methods.</p>

<p>Let’s have a look at the tests for the <em>ExpenseSheetController</em>.</p>

<pre><code>[Specification]
public class When_handling_a_request_for_creating_a_new_expense_sheet
{
    [Establish]
    public void Context()
    {
        var commandHandler = Substitute.For&lt;ICommandHandler&lt;CreateExpenseSheet&gt;&gt;();
        _supportNotifier = Substitute.For&lt;ISupportNotifier&gt;();

        _sut = new ExpenseSheetController(commandHandler, _supportNotifier);
    }

    [Because]
    public void Of()
    {
        var formModel = new CreateExpenseSheetFormModel 
        { 
            EmployeeId = new Guid("94EDE8F3-9675-4DD7-A18F-E37B1F323699") 
        };

        _sut.Create(formModel);
    }
    
    [Observation]
    public void Then_it_should_notify_support()
    {
        _supportNotifier.Received()
            .ExpenseSheetCreated(new Guid("94EDE8F3-9675-4DD7-A18F-E37B1F323699"));
    }

    private ExpenseSheetController _sut;
    private ISupportNotifier _supportNotifier;
}

[Specification]
public class When_an_error_occurs_while_handling_a_request_for_creating_a_new_expense_sheet
{
    [Establish]
    public void Context()
    {
        _supportNotifier = Substitute.For&lt;ISupportNotifier&gt;();
        _exception = new InvalidOperationException("Meltdown");
        
        var commandHandler = Substitute.For&lt;ICommandHandler&lt;CreateExpenseSheet&gt;&gt;();
        commandHandler.WhenForAnyArgs(ch =&gt; ch.Handle(null))
            .Throw(_exception);
        
        _sut = new ExpenseSheetController(commandHandler, _supportNotifier);
    }
    
    [Because]
    public void Of()
    {
        var formModel = new CreateExpenseSheetFormModel 
        { 
            EmployeeId = new Guid("D1067157-5C73-4140-9D29-0FE5C1C4C2FB") 
        };

        _sut.Create(formModel);
    }
    
    [Observation]
    public void Then_it_should_notify_support_that_a_new_expense_sheet_has_been_created()
    {
        _supportNotifier.Received()
            .ErrorDuringExpenseSheetCreation(_exception, 
                new Guid("D1067157-5C73-4140-9D29-0FE5C1C4C2FB"));
    }
    
    private ExpenseSheetController _sut;
    private ISupportNotifier _supportNotifier;
    private Exception _exception;
}
</code></pre>

<p>These tests verify whether support logging occurs when an expense sheet has been created or when an exception gets 
raised. This way we express the intent of the operational requirements.</p>

<p>Notice that controller method has been decorated with a <em>ServiceFilter</em> attribute.</p>

<pre><code>[HttpPost]
[ServiceFilter(typeof(PerformanceTracing))]
public IActionResult Create(CreateExpenseSheetFormModel formModel)
{
    ...
}
</code></pre>

<p>By applying this attribute, the <em>PerformanceTracing</em> action filter is being registered to surround the execution of the 
controller method. Let’s have a look at the implementation of this action filter.</p>

<pre><code>public class PerformanceTracing : ActionFilterAttribute
{
    private readonly ILogger&lt;PerformanceTracing&gt; _logger;
    private readonly Stopwatch _stopWatch;

    public PerformanceTracing(ILogger&lt;PerformanceTracing&gt; logger)
    {
        _logger = logger;
        _stopWatch = new Stopwatch();
    }

    public override void OnActionExecuting(ActionExecutingContext context)
    {
        _stopWatch.Start();
    }

    public override void OnActionExecuted(ActionExecutedContext context)
    {
        _stopWatch.Stop();

        var controllerName = context.Controller.GetType().Name;
        var controllerActionName = context.ActionDescriptor.DisplayName;
        
        _logger.LogTrace($"Action '{controllerActionName}' of controller {controllerName} executed in " + 
            $"{_stopWatch.ElapsedMilliseconds} ms.");
    }
}
</code></pre>

<p>This implementation is a nice example of diagnostic logging. The action filter measures the execution time of a 
controller method and logs the result. Notice that we’re injecting the <em>ILogger</em> interface directly into the constructor.
By registering the <em>PerformanceTracing</em> action filter using the <em>ServiceFilter</em> attribute, we ensure that an instance 
of <em>ILogger</em> gets resolved and properly injected. We didn’t provide any tests for this implementation.</p>

<p>I think it’s useful to consider support logging and diagnostic logging as two separate concepts, even though they quite 
often use the same mechanisms under the hood.</p>


				<p>
						<em>
							If you and your team want to learn more about how to <u>write maintainable unit tests</u>
							and <u>get the most out of TDD practices</u>, make sure to have look at our
							<a href="https://principal-it.eu/training.html">trainings and workshops</a> or checkout
							the <a href="https://principal-it.eu/books.html">books section</a>. Feel free to reach
							out at <span>info@principal-it.be</span>.
						</em>
					</p>

				

				

				
			</div></div>]]>
            </description>
            <link>https://principal-it.eu/2020/11/unit-tests-for-logging/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25057372</guid>
            <pubDate>Wed, 11 Nov 2020 11:06:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Debugging the Kernel with QEMU]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25057309">thread link</a>) | @__rompy
<br/>
November 11, 2020 | https://blog.k3170makan.com/2020/11/linux-kernel-exploitation-0x0-debugging.html | <a href="https://web.archive.org/web/*/https://blog.k3170makan.com/2020/11/linux-kernel-exploitation-0x0-debugging.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-body-4480419177723293408">
<p>Hi folks, in this post I'm going to walk through how to setup the linux kernel for debugging. I will also demonstrate that the setup works by setting a break-point to a test driver I wrote myself. All the code will be available from my gitlab, all the links to my gitlab will be re-posted at the end.&nbsp;</p><p>The setup I describe here re-uses some parts of the syzkaller setup, and for good reason later on in the post series I will break into a tutorial for the syzkaller tool as well. So lets get on with it.</p><table><tbody><tr><td><a href="https://1.bp.blogspot.com/-u2vPJD5mvfQ/X6v9Jig2QOI/AAAAAAAAQFA/AeLKhrX1BXM8HY7pN694qJFyvWDilFojACLcBGAsYHQ/s1109/Screenshot%2Bfrom%2B2020-11-11%2B17-00-44.png" imageanchor="1"><img data-original-height="625" data-original-width="1109" height="360" src="https://1.bp.blogspot.com/-u2vPJD5mvfQ/X6v9Jig2QOI/AAAAAAAAQFA/AeLKhrX1BXM8HY7pN694qJFyvWDilFojACLcBGAsYHQ/w640-h360/Screenshot%2Bfrom%2B2020-11-11%2B17-00-44.png" width="640"></a></td></tr><tr><td>Screenshot of a successful debug session with full debug symbols for the kernel! We can even see the call to start_kernel and a frame before that as well!<br></td></tr></tbody></table><br>&nbsp;<h2>The Process</h2><p>Okay so we want to study kernel exploitation but given that the kernel isn't something totally accessible in userspace, its not as convenient to debug as userpace stuff, we need a bit of a run up before we can actually poke and prod the kernel to figure out how to write our exploits. So there's a number of important steps to how we get this done, here's what we're going to do:</p><ol><li>Build a kernel</li><li>Build an image</li><li>Launch the virtual machine&nbsp;</li><li>Attach and setup the debugger</li><li>Building, loading and debugging a test module <br></li></ol><p>We also need to be able to build our kernel because there may be build options that are important to configure in order to control exploit protection or include modules and functionality to the kernel when needed. <br></p><h2>Building a Kernel</h2><p>Okay so before we get going with launching our Qemu instances and debugging modules we need an environment. For convenience sake I'm working off of a fresh Ubuntu 18.04.5 LTS machine. I'll document the processes from fresh install to first successful kernel build.</p><p>To start we need to make sure we have everything we need to build a kernel:</p><p><span>$<b>sudo apt-get update</b></span></p><p><span>$<b>sudo apt-get upgrade </b><br></span></p><p><span>$<b>sudo apt-get install git fakeroot build-essential ncurses-dev xz-utils libssl-dev bc flex libelf-dev bison qemu-system-x86</b></span></p><p>Next we obviously need a kernel so lets download a brand new kernel:</p><p><span>$<b>wget https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.9.7.tar.xz</b><br>--2020-11-10 23:00:26--&nbsp; https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.9.7.tar.xz<br>Resolving cdn.kernel.org (cdn.kernel.org)... 151.101.225.176, 2a04:4e42:35::432<br>Connecting to cdn.kernel.org (cdn.kernel.org)|151.101.225.176|:443... connected.<br>HTTP request sent, awaiting response... 200 OK<br>Length: 115538096 (110M) [application/x-xz]<br>Saving to: ‘linux-5.9.7.tar.xz’<p>linux-5.9.7.tar.xz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 42%[=============&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]&nbsp; 46.79M&nbsp; 3.08MB/s&nbsp;&nbsp;&nbsp; eta 23s&nbsp;&nbsp; &nbsp;</p></span></p><p><span>... <br></span></p><p><span>$<b>tar -xf linux-5.9.7.tar.xz</b></span></p><p>We're just a couple steps from sending the final build commands, before we get to that lets make sure the kernel config is ready to rock. Because we're working on a Linux host we can simply swipe the .config for the virtual machine's Ubuntu kernel like so:</p><p><span>$<b>cp /boot/config-5.4.0-52-generic .config</b></span></p><p>We then need to select some options that make debugging and exploit dev a little easier. First thing we need is to merge some options for making the kernel easier to run in a virtual machine:</p><p><span>$<b>make kvmconfig</b></span></p><p><span>Using .config as base<br>Merging ./kernel/configs/kvm_guest.config<br>#<br># merged configuration written to .config (needs make)<br>#</span></p><p><span>...</span></p><p>Great, now we need to enable some options for debug symbols, kaslr and other awesome things. So open the <span>.config</span> somewhere in a text editor and make sure you either add or modify the file so these options are set:</p><p><span>CONFIG_KCOV=y<br>CONFIG_DEBUG_INFO=y<br>CONFIG_KASAN=y<br>CONFIG_KASAN_INLINE=y<br>CONFIG_CONFIGFS_FS=y<br>CONFIG_SECURITYFS=y </span><br><span><span># CONFIG_RANDOMIZE_BASE is not set<br></span></span></p><p>Cool now we need to make sure the config is ready to go for a build:</p><p><span>$<b>make savedefconfig</b></span></p><p><span>$<b>make -j4</b></span></p><p><span>&nbsp;...</span></p><p>Now you should grab some coffee, play a startcraft2 game because this may take a while. Okay so if your build worked you should have an object file in the following location:</p><p><span>[kernel_dir]/arch/x86_64/boot/bzImage</span>&nbsp;</p><h2>Build an image</h2><p>We're going to build an image for this kernel so we might as well plop a "image" directory in this folder:</p><p><span>$<b>mkdir [kernel_dir]/image/</b></span></p><p>Once you're kernel is build we need to start thinking about how to build a file system for this. Here I'm going to cheat and steal some tips from the syzkaller folks. We need to first download syzkaller, as follows:</p><p><span>$<b>git clone https://github.com/google/syzkaller.git</b></span></p><p><span>Cloning into 'syzkaller'...<br>remote: Enumerating objects: 1, done.<br>remote: Counting objects: 100% (1/1), done.<br>...<br></span></p><p>Move back to the kernel build and setup an image:</p><p><span>$<b>cd [kernel_dir]/image/</b></span></p><p><span>$<b>cp [syzkaller_dir]/tools/create_image.sh .</b></span></p><p>Okay so we can now create an image, all we need to do is simply invoke create_image.sh:</p><p><span>$<b>./create_image.sh&nbsp;</b></span></p><p><span>+ DIR=chroot<br>+ PREINSTALL_PKGS=openssh-server,curl,tar,gcc,libc6-dev,time,strace,sudo,less,psmisc,selinux-utils,policycoreutils,checkpolicy,selinux-policy-default,firmware-atheros,python,xrdp,g++,make,libtool,autoconf,nasm<br>+ '[' -z ']'<br>+ ADD_PACKAGE=make,sysbench,git,vim,tmux,usbutils,tcpdump</span></p><p><span>...</span><br></p><p>If that worked you should have the following in your folder:</p><p><span>$<b>ls</b>&nbsp;</span></p><p><span>chroot/</span></p><p><span>create-image.sh</span></p><p><span>stretch.id_rsa</span></p><p><span>stretch.id_rsa.pub</span></p><p><span>stretch.img</span><br></p><h2>Launch the virtual machine <br></h2><p>Now we can launch qemu with all the goodies in place:</p><p><span>qemu-system-x86_64 \<br>&nbsp; -kernel <b>../arch/boot/x86_64/bzImage</b> \<br>&nbsp; -append "console=ttyS0 root=/dev/sda earlyprintk=serial nokaslr"\<br>&nbsp; -hda <b>./stretch.img</b> \<br>&nbsp; -net user,hostfwd=tcp::10021-:22 -net nic \<br>&nbsp; -enable-kvm \<br>&nbsp; -nographic \<br>&nbsp; -m 2G \<br>&nbsp; -s \<br>&nbsp; -S \<br>&nbsp; -smp 2 \<br>&nbsp; -pidfile vm.pid \<br>&nbsp; 2&gt;&amp;1 | tee vm.log</span></p><p><span>...</span></p><p><br>The <span>-s</span> is a shorthand for <span>-gdb tcp::1234</span>, which means the gdbserver will be hosted at port 1234. -S tells qemu not to start the cpu automatically, this gives us a chance to set a breakpoint before the kernel starts executing. </p><p>So that's the image running smoothly, lets setup our debugging environment.</p><h2>Attach and setup the debugger<br></h2><p>We can then attach a gdb debugger to the qemu instance as follows. On another terminal, separate from the one running your qemu instance, start up gdb and issue the following commands:</p><p><span>$<b>cd [kernel_dir]/image/ </b><br></span></p><p><span>$<b>gdb ../vmlinux<br></b></span></p><p><span>Reading symbols from ../vmlinux...</span></p><p><span>(gdb)<b> target remote :1234<br></b></span></p><p><span>Remote debugging using :1234<br>0x000000000000fff0 in exception_stacks ()<br></span></p><p><span>(gdb) <b>c</b></span></p><p>We give the "c" command to continue execution. We can now set some of our own breakpoints. As part of the tutorial I've included a custom IOCTL driver and app code (code that invokes the ioctl from userspace), i thought this would be nifty since it shows full ability to develope and debug a driver, something crucial to hunting down modern bugs and exploit development. Anyway lets code and build our own module.</p><h2>Building, Loading and debugging a test module<br></h2><p>Okay so we need to make a test ioctl driver, so lets head over the to kernel source directory and make a new folder in the /driver/ subfolder:</p><p><span>$</span><b><span>cd&nbsp; [kernel_dir]/drivers/</span></b></p><p><span>$</span><b><span>mkdir debug_driver/</span></b></p><p><span>$</span><b><span>cd debug_driver/ <br></span></b></p><p><span>$</span><b><span>touch debug_driver.c</span></b></p><p><span>$</span><b><span>touch debug_driver_app.c</span></b></p><p><span>$</span><b><span>touch Makefile</span></b></p><p>The code for <span>debug_driver.c</span> and <span>debug_driver_app.c </span>as we well as the <span>Makefile</span> are available at this repo <a href="https://gitlab.com/k3170makan/linux-kernel-exploit-development">https://gitlab.com/k3170makan/linux-kernel-exploit-development</a>. All you need to do is download the repo and stick this in its own folder under <span>[kernel_dir]/drivers/</span>. To build the module the we need to set the "M" variable in the kernel make script:</p><p><span>$<b>cd [kernel_dir]; make -C . M=drivers/debug_driver/</b></span></p><p><span>make: Entering directory '/home/kh3m/Research/Kernel/debug_image/linux-5.5.3'<br>&nbsp; AR&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; drivers/debug_driver//built-in.a<br>&nbsp; CC [M]&nbsp; drivers/debug_driver//debug_driver.o</span></p><p><span>...</span></p><p>Now we need to get this module on our qemu host somehow, I do this the hard way, I'm sure there's all sorts of nifty ways to scp files onto the qemu host but I actually just re-create the image after copying the drivers to a folder to be baked into the start up filesystem. First we need to edit create-image.sh so it includes everything in a folder we specify, that way we can just dump stuff in the folder and run create-image.sh whenever we want those files on a live instance.</p><p>So before create-image.sh builds the disk image on line 129, stick this in there:</p><p>++ <span>sudo cp -r ./add/* $DIR/home/.</span><br></p><p>now we make a "add" folder and stick the kernel module and app code in there:</p><p><span>$<b> cd [kernel_dir]/image/</b></span></p><p><span>$ <b>mkdir add/</b></span></p><p><span>$ <b>cd add/</b></span></p><p><span>$ <b>cp ../../drivers/debug_driver/debug_driver.ko .</b><br></span></p><p><span>$ <b>cp ../../drivers/debug_driver/debug_driver_app.c .</b></span></p><p><span>$ <b>./create-image.sh</b> </span></p><p>Okay so we have a module, we have a symbol file debug_driver.ko, with stuff we need to set breakpoints. Lets load the module into the kernel, then check where it gets loaded before we actually set the breakpoint:</p><p><span>root@syzkaller:$ <b>cd /home/</b></span></p><p><span>root@syzkaller:$ insmod debug_driver.ko</span></p><p><span> [&nbsp;&nbsp; 32.792570] audit: type=1400 audit(1605058227.605:7): avc:&nbsp; denied&nbsp; { module_load } for&nbsp; pid=249 comm="insmod" path="/home/debug_driver.ko" dev="sda" ino=21253 scontext=system_u:system_r:kernel_t:s0 1<br>[&nbsp;&nbsp; 32.793766] debug_driver: loading out-of-tree module taints kernel.<br>[&nbsp;&nbsp; 32.800394] [debug_driver] loaded! <br>[&nbsp;&nbsp; 32.800826] [debug_driver] device registered successfully<br>[&nbsp;&nbsp; 32.802298] [debug_driver] device has been successfully created <b><br></b></span></p><p>Before we can debug it properly we need to know where it is loaded in kernel memory:</p><p><span>root@syzkaller:/home# <b>cat /proc/modules</b> <br>debug_driver 16384 0 - Live <b>0xffffffffa0000000</b> (O)</span></p><p>Okay lets now set our breakpoint and load the symbol file using the base address of the module:</p><div><p><span>&nbsp;(gdb) <b>add-symbol-file ../drivers/debug_driver/debug_driver.ko&nbsp; 0xffffffffa0000000</b><br>add symbol table from file "../drivers/debug_driver/debug_driver.ko" at<br>&nbsp;&nbsp; &nbsp;.text_addr = 0xffffffffa0000000<br>(y or n) <b>y</b><br>Reading symbols from ../drivers/debug_driver/debug_driver.ko...<br>(gdb) <b>break dev_read</b><br>Breakpoint 1 at <b>0xffffffffa0000010: file drivers/debug_driver//debug_driver.c</b>, line 81.<br>(gdb) c</span></p></div><p>Cool lets execute the driver program so we can trigger the code we want:</p><p><span>root@syzkaller:$ <b>gcc -o debug_driver_app.elf debug_driver_app.c<br></b></span></p><p><span><span>root@syzkaller:/home# <b>./debug_driver_app.elf </b><br>Usage: ./debug_driver_app.elf [message to write] [read length] <br></span></span></p><p><span><span>root@syzkaller:</span>$ <b>./debug_driver_app.elf "hello" 10</b></span></p><p><span>[&nbsp; 160.083320] [debug_driver] message successfully copied message =&gt; [hello]<br>[&nbsp; 160.083326] [debug_driver] buffer copied to message holder<br>[debug_driv…</span></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.k3170makan.com/2020/11/linux-kernel-exploitation-0x0-debugging.html">https://blog.k3170makan.com/2020/11/linux-kernel-exploitation-0x0-debugging.html</a></em></p>]]>
            </description>
            <link>https://blog.k3170makan.com/2020/11/linux-kernel-exploitation-0x0-debugging.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25057309</guid>
            <pubDate>Wed, 11 Nov 2020 10:53:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On Choosing Boring Tech]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25057240">thread link</a>) | @amzans
<br/>
November 11, 2020 | https://panelbear.com/blog/boring-tech/ | <a href="https://web.archive.org/web/*/https://panelbear.com/blog/boring-tech/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>Over the years I have observed that many engineers tend to attribute much of the success or failure of a company to the technical choices made. I know I’m often guilty of this too. And while it is often justified, I would argue that for the vast majority of startups out there, the choice of programming language, framework, or even database doesn’t matter that much. This seems especially true during the early stages.</p><h2>Through the engineering lens</h2><p>This perception is understandable, we as engineers tend to look at the world from a specific lens, and are often biased by what we know best. Our daily activities may include things such as debugging CI pipelines, implementing new features, pairing with colleagues, or migrating the always present legacy codebase. The environment that surrounds us makes it easy to believe that it all boils down to those things that we see and understand. It’s an illusion that makes us feel like we’re fully in control of what makes or breaks the product.</p><p>Don’t get me wrong, it can be a huge advantage for many companies to make their product 3x more efficient than competitors, or to have elegant, composable code. But you might be focusing on the wrong problems if nobody cares about the product you’re actually building, and sooner or later your business will hit this wall.</p><p>I’m not saying that software doesn’t matter. A solid foundation for your startup goes a long way. If investing in this allows you to build better features faster than your competitors, more power to you. But finding the right balance is highly dependent on what you’re trying to solve and the resources you have at hand. There’s no right or wrong way to do it, and as usual, it mainly comes down to tradeoffs.</p><p><img src="https://panelbear.com/static/img/blog/lenses.png" alt="Different lenses"></p><h2>Boring makes me happy</h2><p>I believe aiming for a healthy balance of risk vs reward when it comes to your technical choices is something to strive for. In particular, if it decreases the chances you get stuck on the wrong problems down the road.</p><p>This is why I have come to appreciate ideas such as <a href="https://mcfunley.com/choose-boring-technology">Choose Boring Technology</a>. This is often interpreted as “picking old technologies over newer ones”, but it doesn’t necessarily mean that. For me, this comes down to using proven technologies in which the ways it can fail are mostly known, but occasionally experimenting with different, possibly newer tools that might suit me better.</p><p>Maybe you want to gain more experience by using the latest framework or programming language, or you just want to have some fun. You do what makes you happy. But if you’re trying to make a decision to increase the odds that your product or business will succeed, it’s worth stepping back and considering your options.</p><p>For me, mainly choosing software that has been around for longer is not about it being boring or older, it’s about the fact that the ways in which it fails are better known. There are fewer unknowns for you to deal with and this maximizes your chances of actually shipping the project.</p><p>For example the other day I had an issue with my Django app, and a quick search led me to tens of answers to this problem in various forums and websites. It took me at most 10 minutes to get back on track and that was the end of this issue.</p><p>I experienced the exact opposite a few years ago with a popular, but not so battle-tested Scala library my team had been using for a while. We were probably among the first to encounter the issues we were facing, and it seemed nobody had walked down this path before. Maybe it sounds like a fun challenge or a great chance to contribute back to OSS (which I’m happy to), but once you solve it, do your customers really care about it? How many days, weeks, or even months are you willing to invest in such issues? In my case, I’d rather use that time to ship new features or improve the existing ones.</p><h2>Proven tech vs new tools</h2><p>I try to follow an 80/20 distribution when it comes to my choice of tools. This means my stack consists of about 80% software I already know well, but I do allow myself 20% of the stack to explore tech I have less experience with. The exact ratio is not what’s important here, it’s more the fact that you should lean towards using proven technologies.</p><p>This also resonates with how <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">Multi-armed bandits</a> work. You try to maximize your expected gain by taking advantage of what worked well in the past, while sometimes exploring new things to avoid missing out on a possible goldmine.</p><p><img src="https://panelbear.com/static/img/blog/bandits.png" alt="Balance new vs proven"></p><p>A more recent example of mine is <a href="https://panelbear.com/">Panelbear</a>, it started as an embarrassingly simple Django app with no charts, all metrics were rendered on a plain HTML table, and all data was stored on a SQLite database. Took literally a weekend to get it up and running including manually deploying to a $5/mo VM. Low risk and high reward for my needs at the time.</p><p>Fast forward and as I added more features and began handling more page views for various websites, I started to notice that the codebase could use some refactoring. It also became increasingly repetitive to do things like deploying to new instances, issuing SSL certs, and keeping the DNS records up to date in case the IP address of my instances changed.</p><p>As a second iteration, I upgraded to a docker-compose setup plus lots of glue code. But soon enough I found myself reinventing what other tools already do well. There are multiple ways to solve each of these pain points, but in my case, it came down to using a tool I am very familiar with from my full-time job: Kubernetes.</p><p>Yes, I am well aware Kubernetes is an absolute overkill for a lot of projects out there, and I could have gotten away with a more traditional solution. But it allowed me to simplify the operational aspects tremendously, and I feel comfortable working with it after having the pleasure of putting down multiple production fires for my employer over the years. That’s why I wouldn’t bindly recommend it to everyone. Do what you know best.
As an added benefit, it also made it trivial when I migrated from DigitalOcean to Linode, and most recently to AWS (each migration took mostly an evening of changing my Terraform files and deploying them - I’m being serious). But that’s for another post.</p><p>Another case in which it paid off once again, was when I wanted to experiment with using Clickhouse for data ingestion and the aggregation queries. It took me less than 10 minutes to write a basic deployment manifest and have it up and running. This included automated SSL certs, in-cluster service discovery, and unified logging/monitoring out of the box. It was a huge win since it allowed me to try things out faster than before.</p><p>Even better, I can deploy any container and operate it the exact same way as I deploy anything else on my cluster. Need more volume storage with zero downtime? It’s a simple manifest change, commit and deploy. Same thing when I needed Redis for caching, I was up and running in minutes, without increasing my costs or adding operational complexity.</p><h2>Focus on shipping</h2><p>My point is, I moved into these technologies as the pain with the previous solution was higher than dealing with the new tech. But more importantly, it helped me ship features even faster to my customers while reducing the operational overhead for me.</p><p>If I had started with the more advanced setup from day one, I might have lost all motivation before I would have had the first version of Panelbear. The key is to solve the problems that are getting between you and your goals, not potential issues you believe one day will be yours.</p><p>Hope you enjoyed this blog post. I plan on writing more about Panelbear’s tech stack, and lessons learned along the way. So stay tuned!</p></div></div></div>]]>
            </description>
            <link>https://panelbear.com/blog/boring-tech/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25057240</guid>
            <pubDate>Wed, 11 Nov 2020 10:37:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Option Hacking the Tektronix TDS 420A]]>
            </title>
            <description>
<![CDATA[
Score 55 | Comments 21 (<a href="https://news.ycombinator.com/item?id=25057162">thread link</a>) | @segfaultbuserr
<br/>
November 11, 2020 | https://tomverbeure.github.io/2020/07/11/Option-Hacking-the-Tektronix-TDS-420A.html | <a href="https://web.archive.org/web/*/https://tomverbeure.github.io/2020/07/11/Option-Hacking-the-Tektronix-TDS-420A.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><em>Previous installments in this series: <a href="https://tomverbeure.github.io/2020/06/27/In-the-Lab-Tektronix-TDS420A.html">In the Lab - Tektronix TDS 420A Oscilloscope</a>, 
 <a href="https://tomverbeure.github.io/2020/06/27/Tektronix-TDS420A-Remote-Control-over-GPIB.html">Tektronix TDS 420A Remote Control over GPIB</a>, 
 <a href="https://tomverbeure.github.io/2020/07/02/Extracting-the-Tektronix-TDS420A-Firmware.html">Extracting the Tektronix TDS 420A Firmware</a>, 
 <a href="https://tomverbeure.github.io/2020/07/03/TDS420A-Serial-Debug-Console-Symbol-Table-Ghidra.html">A Tektronix TDS 420A, a Serial Debug Console, a Symbol Table, and Ghidra</a></em></p>

<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#how-a-tds400-oscilloscope-manages-hardware-features" id="markdown-toc-how-a-tds400-oscilloscope-manages-hardware-features">How a TDS400 Oscilloscope Manages Hardware Features</a></li>
  <li><a href="#the-key-to-enabling-option-05---video-triggering" id="markdown-toc-the-key-to-enabling-option-05---video-triggering">The Key to Enabling Option 05 - Video Triggering</a></li>
  <li><a href="#the-key-to-enabling-option-2f---advanced-dsp-math" id="markdown-toc-the-key-to-enabling-option-2f---advanced-dsp-math">The Key to Enabling Option 2F - Advanced DSP Math</a></li>
  <li><a href="#options-05-and-2f-enabled" id="markdown-toc-options-05-and-2f-enabled">Options 05 and 2F Enabled!</a></li>
  <li><a href="#option-1m---120k-sample-points---a-different-story" id="markdown-toc-option-1m---120k-sample-points---a-different-story">Option 1M - 120K Sample Points - A Different Story</a></li>
  <li><a href="#in-search-of-the-missing-memory" id="markdown-toc-in-search-of-the-missing-memory">In Search of the Missing Memory</a></li>
  <li><a href="#success-at-last" id="markdown-toc-success-at-last">Success at Last!</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>



<p>I wrote <a href="https://tomverbeure.github.io/2020/06/27/In-the-Lab-Tektronix-TDS420A.html#the-tektronix-tds-420a-in-brief">earlier</a>
about the optional features of TDS 400 series of oscilloscopes:</p>

<ul>
  <li>Option 05: Video Trigger</li>
  <li>Option 13: RS-232/Centronics Hardcopy Interface</li>
  <li>Option 1F: File System/Floppy</li>
  <li>Option 2F: Advanced DSP Math</li>
  <li>Option 1M: 120k waveform sample points</li>
</ul>

<p>Most scopes, including mine, come with options 13 and 1F, but the remaining ones are less common.</p>

<p>The video triggering and advanced DSP math options are pure firmware functions, but even 
the 120k sample points option seemed like something that could be enabled with a software hack, since
the signal acquisition board has the 512KB of RAM available to store the data.</p>

<p>Here, I’ll describe how the TDS 400 series manages option enablement, and
how you can hack the scope into getting them to work.</p>



<p>Using Ghidra and the debug console, I figured out how the scope manages hardware
features: it has a function called <code>hwAccountantQuery</code> that has a single
parameter which I’ll call the ‘feature ID’.</p>

<p><code>hwAccountantQuery</code> will return an integer value for that feature ID. These values
can be boolean in nature (“Is a certain feature present or not”) or can be the
amount of DSP memory etc.</p>

<p>Here’s a very non-exhaustive list of codes that I’ve been able to identify:</p>

<div><div><pre><code>0x20d: number of scope channels
0x20f: size of acquisition RAM
0x216: ProbeD2MemSize
0x248: CPU clock period
0x255: InstrumentNameStringPtr
0x271: hwProbeSpecialDiagModeActive
0x2a0: hwProbeSpecialDiagLoopCount
0x2a1: hwProbeSpecialDaigSeqId
0x2b8: 30000 points -&gt; value when 1M option is not possible
0x2bf: TDS420A
0x2d2: RS232 Debug uart present
0x317: MathPak      -&gt; this is the advanced DSP math function
0x461: Floppy drive present
0x537: flashRomDateStringPtr
0x54c: TDS410A
0x560: TDS430A
0x700: hwProbeTvTrigPresent
</code></pre></div></div>

<p><code>hwAccountantQuery</code> calls <code>hwAccountantGetValue</code>. The first part of that function looks liks this:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/hwAccountantGetValue.png" alt="hwAccountantGetValue"></p>

<p>It’s a large <code>if-then-else</code> or <code>case</code> statement that calls a dedicated function for a particular
feature ID.</p>



<p>Did you see <code>_hwProbeTvTrigPresent()</code>? That’s the function that checks
if the video triggering feature should be enabled:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/hwProbeTvTrigPresent.png" alt="hwProbeTvTrigPresent"></p>

<p>And there we have it! To enable “Option 05 - Video Triggering”, all you need to do
is store a non-zero value in non-volatile RAM location 7!</p>

<p><em>This is not a shocking new discovery: plenty of online sources already mentioned this,
but it’s great to confirm it from first principles, by going to the source.</em></p>



<p>Internally, the Advanced Math DSP is called “MathPak”. Just like for video triggering, 
the <code>hwAccountGetValue</code> function issues a call to <code>hwProbeMathPakPresent()</code>:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/hwProbeMathPakPresent.png" alt="hwProbeMathPakPresent"></p>

<p>Option 2F simply relies on a non-zero value in NVRAM location 9!</p>



<p>It’s now just a matter of issuing the following 2 commands on the debug console:</p>

<div><div><pre><code>libManagerWordAtPut 0x50007, 1
libManagerWordAtPut 0x50009, 1
</code></pre></div></div>

<p>My scope booted up with this image:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/options_05_and_2f_enabled.jpg" alt="Options 05 and 2F enabled"></p>

<p>Success! I’m now the proud owner of a scope that supports an entirely obsolete video triggering
mode, and a FFT math option!</p>

<p>Video Triggering Menu:
<img src="https://tomverbeure.github.io/assets/tds420a/video_triggering_features.jpg" alt="Video triggering features"></p>

<p>Live FFT of a 1kHz square wave:
<img src="https://tomverbeure.github.io/assets/tds420a/fft.jpg" alt="FFT"></p>



<p>Unfortunately, the <code>case</code> statement is only a small part of the <code>hwAccountGetValue</code> function: most
feature checking functions are performed by looping through an array of structs that
have the feature ID and a function pointer to the checking function. It’s a bit harder to figure 
out in Ghidra, but we already know that the function names to enable options start with <code>hwProbe</code>.</p>

<p>With Ghidra, we can filter on this, and that gives the <code>hwProbe1MOption</code> and the 
<code>hwProbe1MPresent</code> functions.</p>

<p><code>hwProbe1MPresent</code> looks very familiar:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/hwProbe1MPresent.png" alt="hwProbe1MPresent"></p>

<p>Just like for the 05 and 2F options, we need to set a specific byte in the
NVRAM:</p>

<div><div><pre><code>libManagerWordAtPut 0x50006, 1
</code></pre></div></div>

<p><code>hwProbe1MOption</code> is a different story:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/hwProbe1MOption.png" alt="hwProbe1MOption"></p>

<p>When you run <code>hwProbe1MOption</code> on the command line, the function returns a 0.</p>

<p>Feature IDs 0x216 and 0x20f are also part of the array of structs. They call the functions
<code>hwProbeD2MemSize</code> and <code>hwProbeAcqMemSize</code> respectively.</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/hwProbeTable.png" alt="hwProbe table"></p>

<p><code>hwProbeD2MemSize</code> and <code>hwProbeAcqMemSize</code> both run a test to check the amount of RAM that 
is populated on the board.</p>

<p>When you run these query commands on the debug console, you get:</p>

<div><div><pre><code>hwAccountantQuery(0x216)    
262143
hwAccountantQuery(0x20f)    
131071
</code></pre></div></div>

<p>It’s now clear why option 1M doesn’t get enabled after changing the NVRAM value: 
feature ID 0x20f is fine (131071/0x1ffff is larger than 0x1fffe), but feature ID 0x216 is not 
(262143/0x3ffff is smaller than 0xffffe).</p>

<p>Whatever it is used for, the amount of “D2” memory in the scope is too small.</p>



<p>This finally gave me the crucial hint to start looking at other PCBs inside the scope and
try to find if there’s a place with empty footprints for RAM chips.</p>

<p>I call this the DSP PCB. Luckily, it’s a board that’s easy to remove from the chassis, without 
fragile flex cables or connectors.</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/dsp_pcb.jpg" alt="DSP PCB"></p>

<p>Look at those 6 beautiful, unused footprints!</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/ram_footprints_closeup.jpg" alt="RAM footprints closeup"></p>

<p>The RAM chips are M5M51008 with a 100ns speed rating, made by Mitsubishi LSI.</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/memory_datasheet.jpg" alt="Memory Datasheet"></p>

<p>Surprisingly, Digikey still carries these parts: they’re now made by Rochester
Electronics, and only available in 70ns or 55ns version, but faster is better,
so that shouldn’t be a problem.</p>

<p>They’re cheap too at just $2.56 a piece.</p>

<p>The only issue is a minimum order quantity of 100 parts. $256 for a feature
on a 25 years old $190 oscilloscope is a bit too much! Luckily, the parts
are available at various Chinese chip brokers: I was able to buy them at 
<a href="https://utsource.net/">UTSource</a> for just $1.81 a piece. Even when buying 10 
of them (for redundancy), shipping was the biggest part of the cost:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/memory_order.png" alt="Memory Order"></p>

<p><em>Once ordered, UTSource let me know that these parts were refurbished…</em></p>

<p>A few days later, the parts arrived at my front door, ready to be populated
on the DSP board:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/dsp_board_before_surgery.jpg" alt="DSP Board Before Surgery"></p>

<p>Note how I did not disconnect the battery that’s wired to the board: it’s used to
permanently provide power to those 4 RAMs chips on the left that are encased into 
some transparant polymer gu. Removing the battery will result in lost calibration
data (or so they say.)</p>

<p>I used a regular soldering iron instead of a hot air gun to attach the 6 RAMs:
there was enough solder on the pads and I’m most comfortable doing it that way.
Afterwards I Ohm’ed out most of the pins, and I’m glad I did because
there were some open connections.</p>

<p>The end result isn’t perfect, but it’s good enough:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/rams_populated.jpg" alt="RAMs Populated"></p>



<p>With the RAM populated, it’s time to power on the scope and check the result
of the enhancement surgery!</p>

<p>The scope bootup screen looks good:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/option_1m_enabled.jpg" alt="Option 1M enabled"></p>

<p>And this formerly grayed out 12000 points menu option is now available:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/120k_points.jpg" alt="120K Points"></p>

<p>Victory at Last!</p>



<p>The TDS 420A is an old oscilloscope, and even with those 3 new options enabled, it’s
far inferior to my Siglent 2304X or even my HP 54825A (Windows 95!) loaner.</p>

<p>120K sample points is obviously better than 30K, but it still pales in comparison
to the 140M sample points of the Siglent.</p>

<p>So what then was the point of this whole exercise?</p>

<p>I got a close up view of oscilloscope internals, I learned Ghidra from scratch and
applied it on a real, non-trival project, I added RAM to a 25 year old oscilloscope 
and it worked, I spent tons of late night hours decoding firmware, and 
I had an unreasonable amount of fun doing so.</p>

<p>I even started to appreciate the Tektronix user interface a little bit!</p>

<p>It was time well spent.</p>

<p>For now, the scope will remain on my bench while I start adding Tektronix support 
in glscopeclient. That was the whole point of acquiring the scope to being with!</p>

<p>And if it turns out that it’s really too limited for my use, I can always
sell it back on eBay, this time with 3 additional features enabled.</p>



<ul>
  <li>
    <p><a href="https://www.eevblog.com/forum/testgear/hacking-my-tds460a-to-have-options-1m2f/">Hacking my TDS460A to have options 1M/2F?</a></p>
  </li>
  <li>
    <p><a href="https://forum.tek.com/viewtopic.php?t=140268">TDS420 Options Possible?</a></p>
  </li>
  <li>
    <p><a href="http://videohifi17.rssing.com/chan-62314146/all_p49.html">Upgrade Tektronix: FFT analyzer</a></p>

    <p>Story about upgrading the CPU board from 8MB to 16MB on a TDS420 (not the 420A?) and then FFT in the
  NVRAM.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=iJt2O5zaLRE">Enabling FFT option in Tektronix TDS 540A oscilloscope</a></p>

    <p>Not very useful for 420A owners: enables FFT by copying NVRAM EEPROM.</p>
  </li>
  <li>
    <p><a href="https://www.eevblog.com/forum/testgear/tds420-with-lost-options/msg2032465/?PHPSESSID=021nnvu02ca549sh5le7s9r8i5#msg2032465">TDS420 with lost options</a></p>

    <p>Specific comment about how to enable options on the 420A over GPIB. I wasn’t able to get this to 
  work for some reason.</p>
  </li>
  <li>
    <p><a href="http://www.ko4bb.com/getsimple/index.php?id=enable-tds754d-options">Enable TDS754D Options using GPIB</a></p>

    <p>Another one about using GPIB.</p>
  </li>
</ul>


  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://tomverbeure.github.io/2020/07/11/Option-Hacking-the-Tektronix-TDS-420A.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25057162</guid>
            <pubDate>Wed, 11 Nov 2020 10:22:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Forming Professional Dev Team Habits]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25057078">thread link</a>) | @morchen
<br/>
November 11, 2020 | https://swimm.io/blog/2020-07-09-professionalism-by-forming-a-team-habit/ | <a href="https://web.archive.org/web/*/https://swimm.io/blog/2020-07-09-professionalism-by-forming-a-team-habit/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section data-v-f94c7cca=""> <!----> <div data-v-0cf44990="" data-v-f94c7cca=""><div><p>How does your workplace cultivate an environment of team development? How do you make sure your engineers have time for creative ideas and professional development, all the while meeting sprints, deadlines and deliverables? While branded photogenic team building activities are common in many tech companies — especially with rapidly growing teams, this strategy fails to help grow together as professionals and misses the aim in the long run.</p>
<p>For long, I’ve been toying with such questions, asking myself — how can companies keep “the fun bits”, but also cultivate purpose and a culture of self and team professional development. In this post you’ll find the pilot we’re launching at Swimm: the main goals of such a pilot and why we believe in it, as well as the execution strategy. We will follow-up and share our experiences in future posts.</p>
<h3><strong>The Why: Delivering Code Excellence</strong></h3>
<p><strong>Professionalism</strong>. First, we want to create a culture where all of our team members learn new things, on an ongoing basis. To become professionals, we need to keep learning all the time. To stay happy and challenged at work we need access to resources, and need our managers to invest in our development.</p>
<p><strong>Innovation</strong>. Second, reviewing together new topics and brainstorming on how they can be integrated into our products will provide new ideas and make us constantly rethink our current approaches.</p>
<p>While thinking about a way to achieve these goals, I was looking for best practice, study cases and models that worked well or gloriously failed in other places in the tech industry. Specifically, I was inspired by <a href="https://medium.com/@Idan.Bassuk/a-proven-methodology-for-becoming-an-a-i-expert-32d43887cb1e">this post</a> by Idan Bassuk from <strong>Aidoc</strong>. I contacted Idan and he was very kind to answer all my questions. I learned that they’ve been continuing their “Deep Snips” (where one of their team members learns a subject and presents it to the team) for the past 3 years, and that he still believes this method helps achieve its goals. I learned from their experience that many of their talks resulted in actual impact to their products. This gave me confidence that this method can indeed have a meaningful impact, and I was now more eager than before to put this to the test.</p>
<h3><strong>The How: Swimminars X 2 Weeks</strong></h3>
<p><strong>Swimminars.</strong> Every other week, one of our engineers will get to learn something new that they wish to dive deeper into and learn. It can be about anything at all, as long as it’s technological, and can be applied to our product(s), even if not in the foreseeable future. Then, the engineer will give a lecture, sharing their research and new knowledge with the team. After every session, we will also publish a blog post, summarising the lecture for our community or new hires to use if they wish.</p>
<p><strong>Technicalities</strong>. We plan to divide the session into two parts — the first will be technical, an in-depth overview of the relevant subject (will be covered on our blog posts). The second part will include holding internal discussions on the possible utilisation, adoption and impacts of the topic on our product(s). Are we already relying on some of this knowledge? Can it help us tackle a current or future issue? Perhaps we need to consider implementing it now?</p>
<p>During the two weeks of the engineer’s turn, (s)he gets as much time as needed to learn the subject and prepare the lecture. This will be prioritised over other tasks, and we assume it will take between one and two days. This is a huge commitment — with all the tasks that we have as a startup, every day is precious. Still, we decided that the impact we are hoping for is so valuable that it’s worth the price, and that we are willing to make the experiment.</p>
<h3><strong>Piloting: Managing Expectations</strong></h3>
<p><strong>Risks</strong>. Yet, as always, it’s easier said than done. Indeed this can go wrong in different ways — time management vs efficiency, getting to a high level of interesting presentations and useful technological insight, or getting every one’s voice heard on the team in a manner that compliments them. It’s a learning on the go activity. So we’re up for a team challenge.</p>
<p><strong>Upsides</strong>. For the duration of our team pilot, every other week, the entire dev team will get to learn something new while taking turns deepening knowledge, improving writing and presentation skills and becoming experts within the team on their Swimminar topics. This team exercise will provide each engineer individually and the team as whole, positive experiences of success. We hope.</p>
<p>I will be the first to give a Swimminar — specifically, on <strong>git internals</strong>. How it goes from there, only time will tell. We promise to report back on how this experiment is working for us. Stay tuned.</p>
<p><em>Swimm is a tool helping engineers contribute to any codebase faster and better with automatically generated hints and codebase insight.</em></p>
<p><em>Omer Rosenbaum, Swimm’s Chief Technology Officer. Cyber training expert and Founder of Checkpoint Security Academy. Author of <a href="https://data.cyber.org.il/networks/networks.pdf">Computer Networks (in Hebrew)</a>. Visit My <a href="https://www.youtube.com/watch?v=79jlgESHzKQ&amp;list=PL9lx0DXCC4BMS7dB7vsrKI5wzFyVIk2Kg">YouTube Channel</a>.</em></p>
</div></div> </section></div>]]>
            </description>
            <link>https://swimm.io/blog/2020-07-09-professionalism-by-forming-a-team-habit/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25057078</guid>
            <pubDate>Wed, 11 Nov 2020 10:04:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bdshemu: Bitdefender shellcode emulator]]>
            </title>
            <description>
<![CDATA[
Score 62 | Comments 23 (<a href="https://news.ycombinator.com/item?id=25057062">thread link</a>) | @mdontu
<br/>
November 11, 2020 | https://hvmi.github.io/blog/2020/11/11/bdshemu.html | <a href="https://web.archive.org/web/*/https://hvmi.github.io/blog/2020/11/11/bdshemu.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <h2 id="introduction">Introduction</h2>

<p>Detecting exploits is one of the major strengths of Hypervisor Memory Introspection (HVMI). The ability to monitor guest physical memory pages against different kinds of accesses, such as write or execute, allows HVMI to impose restrictions on critical memory regions: for example, stack or heap pages can be marked as being non-executable at the EPT level, so when an exploit manages to gain arbitrary code execution, the introspection logic would step in and block the execution of the shellcode.</p>

<p>In theory, intercepting execution attempts from memory regions such as the stack or the heap should be enough to prevent most of the exploits. Real life is often more complicated, and there are many cases where legit software uses techniques that may resemble on attack - Just In Time compilation (JIT) in browsers is one good example. In addition, an attacker may store its payload in other memory regions, outside the stack or the heap, so a method of discerning good code from bad code is useful.</p>

<p>We will talk in this blog post about the Bitdefender Shellcode Emulator, or <a href="https://github.com/bitdefender/bddisasm">bdshemu</a> for short. bdshemu is a library capable of emulating basic x86  instructions (in all modes - 16, 32 and 64 bit), while observing shellcode-like behavior. Legitimate code, such as JIT code, will look different compared to a traditional shellcode, so this is what bdshemu is trying to determine: whether the emulated code behaves like a shellcode or not.</p>

<h2 id="bdshemu-overview">bdshemu Overview</h2>

<p>bdshemu is a library written in C, and is part of the bddisasm project (and of course, it makes use of bddisasm for instruction decoding). The bdshemu library is built to emulate x86 code only, so it has no support for API calls. In fact, the emulation environment is highly restricted and stripped down, and there are only two memory regions available:</p>

<ul>
  <li>The page(s) containing the emulated code;</li>
  <li>The stack;</li>
</ul>

<p>Both of these memory regions are virtualized, meaning that they are in fact copies of the actual memory being emulated, so modifications made to them don’t affect the actual system state. Any access made by the emulated code outside of these two areas (which we will call the shellcode and the stack, respectively) will trigger immediate emulation termination. For example, an API call will automatically cause a branch outside the shellcode region, thus terminating emulation. However, in bdshemu, all we care about is instruction-level behavior of the code, which is enough to tell us whether the code is malicious or not.</p>

<p>While bdshemu provides the main infrastructure for detecting shellcodes inside a guest operating-system, it is worth noting that this is not the only way HVMI determines that execution of a certain page is malicious - two other important indicators are used:</p>

<ul>
  <li>The executed page is located on the stack - this is common with stack-based vulnerabilities;</li>
  <li>The stack is pivoted - when a page is first executed and the <code>RSP</code> register points outside the normal stack allocated for the thread;</li>
</ul>

<p>These two indicators are enough on their own to trigger an exploit detection. If these are not triggered, bdshemu is used to take a good look at the executed code, and decide if it should be blocked or not.</p>

<h2 id="bdshemu-architecture">bdshemu Architecture</h2>

<p>bdshemu is created as a standalone C library, and it only depends on bddisasm. Working with bdshemu is fairly simple, as just like bddisasm, it is a single-API library:</p>
<div><div><pre><code><span>SHEMU_STATUS</span>
<span>ShemuEmulate</span><span>(</span>
    <span>SHEMU_CONTEXT</span> <span>*</span><span>Context</span>
    <span>);</span>
</code></pre></div></div>

<p>The emulator expects a single <code>SHEMU_CONTEXT</code> argument, containing all the needed information in order to emulate the suspicious code. This context is split in two sections - input parameters and output parameters. The input parameters must be supplied by the caller, and they contain information such as the code to be emulated, or initial register values. The output parameters contain information such as what shellcode indicators bdshemu detected. All these fields are well documented in the source-code.</p>

<p>Initially, the context is filled in with the following main information (please note that emulation outcome may change depending on the value of the provided registers and stack):</p>

<ul>
  <li>Input registers, such as segments, general purpose registers, MMX and SSE registers; they can be left 0, if they are not known, or if they are irrelevant;</li>
  <li>Input code, which is the actual code to be emulated;</li>
  <li>Input stack, which can contain actual stack contents, or can be left 0;</li>
  <li>Environment info, such as mode (32 or 64 bit), or ring (0, 1, 2 or 3);</li>
  <li>Control parameters, such as minimum stack-string length, minimum NOP sled length or the maximum number of instructions that should be emulated;</li>
</ul>

<p>The main output parameter is the <code>Flags</code> field, which contains a list of shellcode indicators detected during the emulation. Generally, a non-zero value of this field strongly suggests that the emulate code is, in fact, a shellcode.</p>

<p>bdshemu is built as a plain, quick and simple x86 instruction emulator: since it only works with the shellcode itself and a small virtual stack, it doesn’t have to emulate any architectural specifics - interrupts or exceptions, descriptor tables, page-tables, etc. In addition, since we only deal with the shellcode and stack memory, bdshemu does not do memory access checks, since it doesn’t even allow accesses to other addresses. The only state apart from the registers that can be accessed is the shellcode itself and the stack, and both are copies of the actual memory contents - the system state is never modified during the emulation, only the provided <code>SHEMU_CONTEXT</code> is. This makes bdshemu extremely fast, simple, and lets us focus on its main purpose: detecting shellcodes.</p>

<p>As far as instruction support goes, bdshemu supports all the basic x86 instructions, such as branches, arithmetic, logic, shift, bit manipulation, multiplication/divison, stack access and data transfer instructions. In addition, it also has support for other instructions, such as some basic MMX or AVX instructions - <code>PUNPCKLBW</code> or <code>VPBROADCAST</code> are two good examples.</p>

<h2 id="bdshemu-detection-techniques">bdshemu Detection Techniques</h2>

<p>In order to determine whether an emulated piece of code behaves like a shellcode, there are several indicators bdshemu uses.</p>

<h3 id="nop-sled">NOP Sled</h3>

<p>This is the classic presentation of shellcodes; since the exact entry point of the shellcode when gaining code execution may be unknown, attackers usually prepend a long sequence of <code>NOP</code> instructions, encoding <code>0x90</code>. The parameters for the NOP-sled length can be controlled when calling the emulator, via the <code>NopThreshold</code> context field. The default value is <code>SHEMU_DEFAULT_NOP_THRESHOLD</code>, which is <code>75</code>, meaning that minimum 75% of all the emulated instruction must be <code>NOP</code>.</p>

<h3 id="rip-load">RIP Load</h3>

<p>Shellcodes are designed to work correctly no matter what address they’re loaded at. This means that the shellcode has to determine, dynamically, during runtime, the address it was loaded at, so absolute addressing can be replaced with some form of relative addressing. This is typically achieved by retrieving the value of the instruction pointer using well-known techniques:</p>

<ul>
  <li><code>CALL $+5/POP ebp</code> - executing these two instructions will result in the value of the instruction pointer being stored in the <code>ebp</code> register; data can then be accessed inside the shellcode using offsets relative to the <code>ebp</code> value;</li>
  <li><code>FNOP/FNSTENV [esp-0xc]/POP edi</code> - the first instruction is any FPU instruction (not necessarily <code>FNOP</code>), and the second instruction, <code>FNSTENV</code> saves the FPU environment on the stack; the third instruction will retrieve the <code>FPU Instruction Pointer</code> from <code>esp-0xc</code>, which is part of the FPU environment, and contains the address of the last FPU executed - in our case, <code>FNOP</code>; from there on, addressing relative to the <code>edi</code> can be used to access shellcode data;</li>
</ul>

<p>Internally, bdshemu keeps track of all the instances of the instruction pointer being saved on the stack. Later loading that instruction pointer from the stack in any way will result in triggering this detection. Due to the way bdshemu keeps track of the saved instruction pointers, it doesn’t matter when, where or how the shellcode attempts to load the RIP in a register and use it, bdshemu will always trigger a detection.</p>

<p>In 64 bit, RIP-relative addressing can be used directly, since the instruction encoding allows it. However, surprisingly, a large number of shellcodes still use a classic method of retrieving the instruction pointer (generally the <code>CALL/POP</code> technique), which is somehow weird, but it probably indicated that 32 bit shellcodes were ported to 64 bit with minimal modifications.</p>

<h3 id="write-self">Write Self</h3>

<p>Most often, shellcodes come in encoded or encrypted forms, in order to avoid certain bad characters (for example, <code>0x00</code> in a shellcode that should resemble a string may break the exploit) or to avoid detection by security technologies (for example, AV scanners). This means that during runtime, the shellcode must decode itself (usually in-place), by modifying its own contents, and then executing the plain-text code. Typical methods of decoding involve <code>XOR</code> or <code>ADD</code> based decryption algorithms.</p>

<p>Certainly, bdshemu follows this kind of behavior, and keeps track internally of each modified byte inside the shellcode. Whenever the suspected shellcode writes any portion of itself, and then it executes it, the self-write detection will be triggered.</p>

<h3 id="tib-access">TIB Access</h3>

<p>Once a shellcode has gained code execution, it needs to locate several functions inside various modules, in order to carry its actual payload (for example, downloading a file, or creating a process). On Windows, the most common way of doing this is by parsing the user-mode loader structures, in order to locate the addresses where the required modules were loaded, and then locate the needed functions inside these modules. The sequence of structures the shellcode will access is:</p>

<ol>
  <li>The Thread Environment Block (<code>TEB</code>), which is located at <code>fs:[0]</code> (32 bit thread) or <code>gs:[0]</code> (64 bit thread);</li>
  <li>The Process Environment Block (<code>PEB</code>), which is located at <code>TEB+0x30</code> (32 bit) or <code>TEB+0x60</code> (64 bit)</li></ol></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://hvmi.github.io/blog/2020/11/11/bdshemu.html">https://hvmi.github.io/blog/2020/11/11/bdshemu.html</a></em></p>]]>
            </description>
            <link>https://hvmi.github.io/blog/2020/11/11/bdshemu.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25057062</guid>
            <pubDate>Wed, 11 Nov 2020 10:01:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Cell Signaling Technologies – Detailed 3D model of human cells]]>
            </title>
            <description>
<![CDATA[
Score 161 | Comments 50 (<a href="https://news.ycombinator.com/item?id=25055908">thread link</a>) | @ozten
<br/>
November 10, 2020 | http://www.digizyme.com/cst_landscapes.html | <a href="https://web.archive.org/web/*/http://www.digizyme.com/cst_landscapes.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="u6389-bw">
     <div id="u6389"><!-- column -->
      <div id="u6389_align_to_page">
       <!-- m_editable region-id="editable-static-tag-U6264-BP_infinity" template="cst_landscapes.html" data-type="html" data-ice-options="disableImageResize,link,txtStyleTarget" -->
       <p><span id="u6264">Cell Signaling Technologies</span></p>
       <!-- /m_editable -->
       <!-- m_editable region-id="editable-static-tag-U6399-BP_infinity" template="cst_landscapes.html" data-type="html" data-ice-options="disableImageResize,link,txtStyleTarget" -->
       <p>Molecular Landscapes</p>
       <!-- /m_editable -->
       
       
       
      </div>
     </div>
    </div></div>]]>
            </description>
            <link>http://www.digizyme.com/cst_landscapes.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25055908</guid>
            <pubDate>Wed, 11 Nov 2020 06:02:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Eddie's Ink Chip Hack (2002)]]>
            </title>
            <description>
<![CDATA[
Score 20 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25054177">thread link</a>) | @userbinator
<br/>
November 10, 2020 | http://www.eddiem.com/photo/CIS/inkchip/chip.html | <a href="https://web.archive.org/web/*/http://www.eddiem.com/photo/CIS/inkchip/chip.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<tr>
<td>
<p lang="en-GB"><a href="http://www.eddiem.com/photo/CIS/cis.htm">My CIS page.</a></p></td>
<th>
<p lang="en-GB"><img alt="" src="http://www.eddiem.com/photo/CIS/inkchip/inkchip.JPG" name="Graphic2" width="401" height="400"></p></th>
<td>
<p lang="en-GB"><a href="http://www.eddiem.com/photo/printer/chipreset/resetchip.html">Part
				2 build your own reseter</a></p></td></tr>
<tr>
<td colspan="3">
<p lang="en-GB"><strong>What is a Intellidge ink chip.</strong><span size="5"><br>
</span>Epson fit
				small circuit boards to most of their ink cartridges. These
				record the amount of ink that is estimated to be in the
				cartridge. I read that the official epson line is that it is for
				the customers benefit and not an anti-refill device. Whether you
				believe this or not they are a bloody nuisance to anyone wanting
				to refill the cartridges or use bulk ink. It also stops people
				using old cartridges full of solvent for cleaning the heads.
				Another problem was early printer models didn't check if the
				cartridge had been changed while power was on. This was good if
				you wanted to trick the printer into copying a “full”
				chip to and empty one, however the reverse was also true and you
				could easily copy and “empty” one into your full
				one.<br>
They are just a small memory device holds 32 bytes of
				data, they do not measure real ink level and nor does the
				printer. The printer reads the chips on startup, estimates
				(sometimes badly) how much ink should have been used and writes
				this back at shutdown. They hold other data as well.<br>
So epson
				go to the trouble of fitting chips to cartridges and building all
				the extra sockets, wiring, electronics and software into the
				printer so you can use the computer to see the predicted level
				and it can stop you printing if it think you've used enough ink.
				High-end Canon's on the other hand make the inks tank clear so
				you can see and have optical sensor to detect emptiness. This
				make a lot more sense – unless you are making an
				anti-refill device that is. Canon almost got my business this
				time but nobody I could find has run pigment in them – too
				risky.<br>
To get around the chip problems someone usually end up
				producing read-only chip which always read full (for use with
				CIS) and chip reseters for those who want to refill. These are
				not available for 2100p at the time of writing as far as I can
				tell.<br>
Before ordering my 2100p I did my homework and it seemed
				fairly likely a chip reseter would become available at some point
				and read-only chips as well. I was also cocky enough to think I
				could crack it myself and I have. It didn't go quite as expected
				though.<br>
<strong>What do I want to do?</strong><br>
I want the easiest way
				to fool the printer into believing it has full cartridges present
				so I can build my CIS.<br>
<strong>What did I expect?</strong><br>
A logical
				interface for Intellidge is i2c (i squared c) or TWI (two wire
				interface). Then the chip could just be some standard i2c eeprom.
				The Intellidge have too many pads for this but I was hopeful.
				After that would could SPI or microwire – again this could
				use off the shelf parts. If the chips were micro-controllers then
				plain asynchronous serial would be my choice.<br>
<strong>I had a
				look.</strong><br>
To do this I use a AVR mega323 micro, I declined
				offers of logic analyzers being a homebrew type of guy. The 323
				has 2K of internal ram which is enough for some minimalist data
				logging. It was about $50AUS in parts ($30US) to make. I wired a
				cartridge to bring the signals out and took a quick look with a
				voltmeter.<br>
<strong>Nothing!</strong><br>
There was nothing there. I
				expected some power but no, the chips are only powered briefly
				when the are accessed. I used leds to get a rough idea what was
				what and hooked up the micro via resistors to give some degree of
				protection to the printer if I screwed up. The code in the micro
				was written is assembler and captured data sent via rs232 to my
				PC where I wrote a delphi program to display and process the
				data.</p></td></tr>
<tr>
<td colspan="3">
</td></tr>
<tr>
<td colspan="3">
<p lang="en-GB"><a href="http://www.eddiem.com/photo/CIS/inkchip/traces.html"><img alt="" src="http://www.eddiem.com/photo/CIS/inkchip/fastendsml.gif" name="Graphic5" width="615" height="213"></a><br>
<a href="http://www.eddiem.com/photo/CIS/inkchip/traces.html">Click
				here for more traces.</a></p>
<p lang="en-GB">This is the sort of thing I got. No
				protocol I ever seen. Obviously synchronous with bi-directional
				data, very short format. I was confused a little by how short it
				was - because I expect much better precision for the ink
				level.<br>
The traces seem to be.<br>
Top – some sort of sync
				line, this always goes low before the start of transmission.<br>
Next
				– power this goes low (off) between chip reads at printer
				startup but stays high during the shutdown – when data is
				written to the chip.<br>
Next – the clock, data is read of
				the rising edge and changed on the falling.<br>
Bottom –
				bi-directional data, the first 4 bits are always from printer to
				the chip, the rest depend on whether it is a read or write. LSB
				first (left).</p></td></tr>
<tr>
<td colspan="3">
<p lang="en-GB">Convert to binary and some patterns emerge.<br>
It
				was not real obvious how the chips were addressed or which bits
				encoded ink levels. Some more data when some ink had been used
				made it easier.</p>
<p lang="en-GB">Below is one chip being read at startup, there
				are 7 accesses one for each chip. Only 3 block have data –
				the other chips must be hooked to different data lines.</p>
<div lang="en-GB"><p><img alt="" src="http://www.eddiem.com/photo/CIS/inkchip/strtbinary.gif" name="Graphic6" width="818" height="45"></p></div></td></tr>
<tr>
<td colspan="3">
<p lang="en-GB">Below is the complete shutdown stream. Again we
				can only see 3 chips from here.</p>
<p lang="en-GB"><img alt="" src="http://www.eddiem.com/photo/CIS/inkchip/endbinary.gif" name="Graphic7" width="297" height="199"><br clear="left">
After
				printing a few bits near the beginning of the bit stream did
				change. It looks to me like the first 3 bits are the chip address
				the next is a write bit then the ink level, I get the feeling
				there aren't many bits used to encode it (later looks like 6).<br>
So
				– the top one shows 252 bits of data being read out of the
				chip.<br>
The first part of the shutdown shows just the ink level
				being read out, this is to check the same chip is there.<br>
The
				second part is the ink-level and some other stuff (printer serial
				number maybe) being written into the chip. Seeing I didn't use
				any ink the bit-stream is identical to the read except for bit 3
				– presumably the write bit.</p></td></tr>
<tr>
<td colspan="3">
<p lang="en-GB"><span size="5"><br>
Tuesday
				24 Sept 2002. I fooled the printer.</span></p></td></tr>
<tr>
<td colspan="3">
<p lang="en-GB"><img alt="" src="http://www.eddiem.com/photo/CIS/inkchip/spoofed.gif" name="Graphic8" width="381" height="363"><span size="3">The
				interesting thing about this screen grab is the black cartridge
				is really only two thirds full. I spoofed the printer by pulling
				the serial data line low during the time the ink level bits are
				being clocked out of the ink chip.<br>
This is means 6 bits
				starting at the 5'th bit in the stream.</span></p>
<p lang="en-GB"><span size="3">The first 3 bits appear to be the
				chip address, I guess the next is a read/write select. I used a
				AVR mega323 to detect the start of the serial transmission look
				for the address of chip1+read (black apparently) then pull data
				low for 6 clock edges. </span></p>
<p lang="en-GB"><span size="3">I'm sure I can reset 3 of the chips
				by tapping into chip1 signal. Reseting the rest will mean tapping
				into at least one more. </span></p>
<p lang="en-GB"><span size="3">The current set up is for
				experimentation only – it is not “the real thing”.</span></p>
<p lang="en-GB"><span size="3">Shorting the data to ground may be a
				bit drastic but it is only for a very brief time. I hoped the
				data line would be open collector but this doesn't seem to be the
				case.</span></p></td></tr></div></div>]]>
            </description>
            <link>http://www.eddiem.com/photo/CIS/inkchip/chip.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25054177</guid>
            <pubDate>Wed, 11 Nov 2020 00:41:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Analyzing Voting Systems]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25053892">thread link</a>) | @shihn
<br/>
November 10, 2020 | https://shihn.ca/posts/2020/voting-systems/ | <a href="https://web.archive.org/web/*/https://shihn.ca/posts/2020/voting-systems/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
      
<h2>Introduction</h2>
<p>We encounter voting in some form around us all the time. We rate our Uber drivers, they rate us back. We up-vote and down-vote posts and trolls on Reddit. We give stars to movies and restaurants. We vote on who gets kicked out of our favorite reality television show. We vote for Presidents.</p>
<p>All these voting systems seem a bit different from one another, but one thing that's definitely common among them — we will find ways to complain about them. The way a voting system is designed can make an <em>election</em> trivial or really complicated in nature. In fact, sometimes, the winner of an election may be determined by the rules of the voting system and not the intent of the voters (electoral college anyone?). In this post I try to explore the core of different voting systems and wonder if there is a perfect voting system.</p>
<p>Here I am going to use the word <em>election</em> to define an event or a goal that requires voting. An election doesn't have to be political in nature.</p>
<p><em>Note and Acknowledgement: This blog post is influenced by the chapter on voting systems in video games in the book Power-Up by Matthew Lane.</em></p>
<h2>Plurality Voting</h2>
<p>This is the simplest form of voting. Most political elections in the United States are done using this form of voting. It's quite simple — every voter casts a vote for their favorite candidate. The candidate with the most number of votes wins.</p>
<p>Let's look at an example that we will continue to use in this post. We ask 100 people to vote for their favorite flavor of ice cream. The candidates are <em>Vanilla</em>, <em>Chocolate</em>, and <em>Strawberry</em>. Here's the result:</p>
<table>
<thead>
<tr>
<th>Flavor</th>
<th>Votes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vanilla</td>
<td>45</td>
</tr>
<tr>
<td>Chocolate</td>
<td>40</td>
</tr>
<tr>
<td>Strawberry</td>
<td>15</td>
</tr>
</tbody>
</table>
<p><strong>Vanilla has won!</strong> Now if you stare at the numbers a bit, you will find some downsides in declaring Vanilla the winner in this election of the flavors.</p>
<p>An obvious one is that more votes were cast for a flavor that is not the winning flavor. You could also argue that no flavor should win because none of them reached a majority.</p>
<p>Here Strawberry is acting as a <strong>spoiler</strong> — similar to how third-party candidates in US elections can be considered spoilers. Maybe we should have a <strong><em>run-off election</em></strong> where only Vanilla and Chocolate are considered. Perhaps more people favor Chocolate over Vanilla when Strawberry is out of the picture. (The US state of Georgia has rules akin to this. In the 2020 elections for the senate seats in Georgia, none of the candidates achieved a majority. So run-off elections will be held in January of 2021 with the top two candidates).</p>
<p>The essence of the Plurality voting system is that it does not capture the full spectrum of voters' preferences. If someone voted for Strawberry, it does not tell us how they feel about Vanilla or Chocolate.</p>
<p>This system does not truly determine the <em>'will of the people'</em>, unless.... there are only two candidates. One of the candidates is guaranteed to receive a majority, barring a tie. So if it were truly a <em>two-party system</em> some of the flaws of this system do not matter any more.</p>
<h2>Ranked Choice Voting</h2>
<p>Since the Plurality based system does not capture the full spectrum of the voter's preferences, we should probably ask for more information from the voters. What if we asked the voters to rank all the candidates, rather than cast a ballot for their favorite?</p>
<p>Let's look at the example we've been working with. We asked the 100 people to rank the candidate flavors. Here's the result:</p>
<table>
<thead>
<tr>
<th>1st</th>
<th>2nd</th>
<th>3rd</th>
<th>Votes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vanilla</td>
<td>Strawberry</td>
<td>Chocolate</td>
<td>45</td>
</tr>
<tr>
<td>Strawberry</td>
<td>Chocolate</td>
<td>Vanilla</td>
<td>15</td>
</tr>
<tr>
<td>Chocolate</td>
<td>Strawberry</td>
<td>Vanilla</td>
<td>30</td>
</tr>
<tr>
<td>Chocolate</td>
<td>Vanilla</td>
<td>Strawberry</td>
<td>10</td>
</tr>
</tbody>
</table>
<p>All of the 45 people who voted for Vanilla had Strawberry as the second choice. All 15 people who voted for Strawberry, had Chocolate as their second choice. Of the 40 people who voted for Chocolate, 30 preferred Strawberry over Vanilla, and 10 preferred Vanilla. So, which flavor won? There are multiple ways to interpret this data. Let's look at a couple 👇</p>
<h2>Borda Count</h2>
<p>In this system for <code>n</code> candidates, each first-place vote receives <code>n</code> points. Second-place receives <code>n-1</code> points, and so on. The candidate with the most points wins.</p>
<p>Let's compute the points in our example. Vanilla received 45 first places, 10 second places, and 45 third places. So the score for Vanilla is <code>45n + 10(n-1) + 45(n-2)</code>. Here, <code>n</code> is <code>3</code>, giving Vanilla a score of <code>200</code>. Here's the final tally:</p>
<table>
<thead>
<tr>
<th>Flavor</th>
<th>Points</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vanilla</td>
<td>200</td>
</tr>
<tr>
<td>Chocolate</td>
<td>195</td>
</tr>
<tr>
<td>Strawberry</td>
<td>205</td>
</tr>
</tbody>
</table>
<p><strong>Strawberry has won!</strong> Strawberry, which had the fewest votes in the Plurality voting system, has the most points in the Borda ranking system. Totally ridiculous, isn't it? Well maybe, but maybe not. Strawberry did receive the fewest third-place votes. And 75% of the people had Strawberry as their second choice.  Perhaps Strawberry does deserve to win!</p>
<h2>Instant Runoff Voting</h2>
<p>Let's take a look at a different model of interpreting the ranked voting data. In an Instant Runoff, the candidate with the fewest first-place votes is eliminated, and its votes are distributed to the second choice. This is then repeated until we have one candidate left standing.</p>
<p>Some consider this model of iterative elimination a bit confusing and thereby not practical. But it's getting wide adoption, including in political elections (San Francisco and Oakland city elections, for example). It is also used to decide the winner of the Best Picture Academy Award.</p>
<p>Let's apply this to our current example.</p>
<table>
<thead>
<tr>
<th>Flavor</th>
<th>Votes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vanilla</td>
<td>45</td>
</tr>
<tr>
<td>Chocolate</td>
<td>40</td>
</tr>
<tr>
<td>Strawberry (eliminated)</td>
<td>15</td>
</tr>
</tbody>
</table>
<p>Strawberry is eliminated. Since all Strawberry voters preferred Chocolate over Vanilla, Chocolate gets Strawberry's 15 votes. Chocolate now has 55 votes, a majority. <strong>Chocolate has won!</strong></p>
<h2>Quick Recap</h2>
<p>We have discussed three systems so far, and in our example, we have had three different winners for the same election. You may decide subjectively that one of these systems may be better for the use case you have in mind, or you might think as I did at first: <strong>It's all pointless!</strong></p>
<h2>The Impossibility</h2>
<p>There is a concept in decision theory called the <strong><a href="https://en.wikipedia.org/wiki/Independence_of_irrelevant_alternatives">Independence of Irrelevant Alternatives (IIA)</a></strong> which states a voter's preference between two choices <code>x</code> and <code>y</code>, should not depend on any other choices.</p>
<p>This seems like a simple and a good rule to live by and our election systems should live by them as well. Sadly, all the systems we have looked at so far do not abide by this rule.</p>
<p>Let's look at the Plurality system - From the rankings we know that all of Strawberry voters prefer Chocolate over Vanilla. If the choice of Strawberry was not there, Chocolate would have won with 55 votes. But with Strawberry present, Vanilla wins with 45 votes.</p>
<p>For the Borda system, Chocolate is the spoiler. With Chocolate in the picture, Strawberry wins. Without Chocolate, Vanilla wins 55-45.</p>
<p>In the Instant Runoff, Chocolate wins when Vanilla is present but Strawberry wins 60-40 if Vanilla is not.</p>
<h3>Arrow's Impossibility Theorem</h3>
<p>In decision theory, here are some good things to have in an election or any voting system.</p>
<ul>
<li>Independence of Irrelevant Alternatives: which we have discussed and failed to account for so far.</li>
<li>Nondictatorship: Output should not be based on one individual, the wishes of multiple voters should be taken into consideration.</li>
<li>Pareto Efficiency (Unanimity): should have a notion of <a href="https://en.wikipedia.org/wiki/Pareto_efficiency">unanimity</a> — If every voter prefers candidate A over candidate B, candidate A should win.</li>
<li>Unrestricted Domain: Voting must account for all individual preferences.</li>
<li>Ordering:  Each individual should be able to order the choices in any way.</li>
</ul>
<p>All good rules, don't you think? Let's create the ultimate voting system! But here comes <a href="https://en.wikipedia.org/wiki/Kenneth_Arrow">Kenneth Arrow</a> to shatter our hopes.</p>
<p><strong><a href="https://en.wikipedia.org/wiki/Arrow's_impossibility_theorem">Arrow's Impossibility Theorem</a> states that in all cases where preferences are ranked, it is impossible to formulate a social ordering without violating one of these rules.</strong></p>
<p>In other words, any democracy that satisfies Unanimity and the Independence of Irrelevant Alternatives, must be a dictatorship! *<em>insert dramatic sound effects</em>*</p>
<p>So yeah, we will always find things to argue about in an election. 😒</p>
<h2>Dodging the Impossibility</h2>
<p>Since every system is flawed, is it the end of this essay? Unfortunately for you, I, like many of you, noticed this one clause in Arrow's impossibility theorem which provides a way for us to escape this gravity well.</p>
<p>The theorem assumes that we are dealing with a ranked choice voting system. Let's just not rank our candidates. 💡</p>
<p>Here I would remind you, that we're trying to look at voting systems in general, not just political elections.</p>
<p>We have implemented non rank based systems in Software numerous times. Think Netflix, Yelp, Reddit, Tinder. The key as you may have guessed is rating, and not ranking (Tinder being a more specific type of rating - approval voting, which I'll discuss later). A voting system based on rating is usually called <strong>Score Voting</strong>.</p>
<h2>Score Voting</h2>
<p>The idea behind score voting is that you give each candidate a score in one or many categories. This score is independent of the score the other candidates receive. Think Diving and Gymnastics in the Olympics. The judges rate each athlete based on form, routine, landings. One with the highest total score wins.</p>
<p>But is this system better? That's subjective but we know it lets us escape the impossibility mathematically, and yet conform to independence, unanimity and nondictatorship rules.</p>
<h2>Approval Voting</h2>
<p>There's a simpler form or Score Voting - Approval Voting. Think of it as a binary version of the score voting. Each person can give a candidate a score of <code>0</code> or <code>1</code>. In other words one can approve or disapprove any number of candidates.</p>
<p>This is similar to how people vote on dating apps like Tinder. They give prospects a score of <code>1</code> by swiping right, and a score of <code>0</code> by swiping left.</p>
<h2>Strategizing the Ranked Vote</h2>
<p>One key advantage for Score Voting and Approval Voting is that it never hurts to vote for your favorite candidate. It may seem obvious and trivial but it's not always satisfied by voting systems. For example, it is common in political elections for people to not vote for the third-party candidate even though the third-party candidate may be the voter's first …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://shihn.ca/posts/2020/voting-systems/">https://shihn.ca/posts/2020/voting-systems/</a></em></p>]]>
            </description>
            <link>https://shihn.ca/posts/2020/voting-systems/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25053892</guid>
            <pubDate>Wed, 11 Nov 2020 00:09:18 GMT</pubDate>
        </item>
    </channel>
</rss>
