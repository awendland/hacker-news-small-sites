<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 5]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 5. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 22 Jul 2020 08:18:09 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-5.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Wed, 22 Jul 2020 08:18:09 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Minecraft@Home]]>
            </title>
            <description>
<![CDATA[
Score 59 | Comments 8 (<a href="https://news.ycombinator.com/item?id=23895789">thread link</a>) | @networked
<br/>
July 20, 2020 | https://minecraftathome.com/minecrafthome/ | <a href="https://web.archive.org/web/*/https://minecraftathome.com/minecrafthome/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div><p><span>Server outage and supporting our infrastructure</span><br>

        Between 23:54 UTC yesterday (2020-07-20) and 15:35 UTC today, we suffered from a catastrophic SQL failure which forced us to restore from backup.</p><p>

There may be some credit issues where the system granted more or fewer credits than you expect for work done during this time.<br>
I can't apologize enough. If our BOINC deployment was architected for scale rather than for low cost, we could've avoided this.</p><p>

There are several enhancements to our infrastructure and upgrades we'd like to make, such as migrating services to Kubernetes and potentially using a managed SQL service.</p><p>

<span><b><span color="red">You can help!</span> Please consider visiting our Patreon page, reviewing the current set of benefits, and making a contribution of any size; any amount helps - <a href="https://patreon.com/minecraftathome" rel="nofollow">patreon.com/minecraftathome</a></b></span></p><p>

All contributions go towards covering infrastructure cost and quality-of-life improvements to ensure the project's longevity.
        <br>
        <span>21 Jul 2020, 20:10:54 UTC
    
            · <a href="https://minecraftathome.com/minecrafthome/forum_thread.php?id=44"> Discuss</a>
        </span></p></div><hr>
    <div><p><span>That was fast</span><br>

        The origin of the panorama image used in the Minecraft main menu from beta version 1.8.1, released in September 2011, has remained a mystery until now.</p><p>

<b><span>In less than 24 hours after launching the panorama application; a volunteer host for Minecraft@Home, in a sheer stroke of luck, found the world seed, 25357015387625.</span></b><br>
This was approximately </p><p><span color="red">93 days of processing time at a total of 54.5 exaFLOPs</span> compressed into the last 24 hours.</p><p>

The specific host which located the seed belongs to the user <a href="https://minecraftathome.com/minecrafthome/show_user.php?userid=2558" rel="nofollow">vanos0512</a>.<br>
Thank you to the 137 users who contributed 181 hosts with 231 GPUs over the last 24 hours. You all accomplished this.</p><p>

<img src="https://i.imgur.com/f6lGCEn.png"></p><p>

Here are the details if you want to generate this world for yourself:<br>
<b>Minecraft version:</b> <i>Beta 1.7</i><br>
<b>Either of these two valid world seeds:</b> <i>2151901553968352745 or 8091867987493326313</i><br>
<b>Co-ordinates:</b> <i>x60, y76, z-67</i></p><p>

<a href="https://www.youtube.com/watch?v=caLCZNLPgrM" rel="nofollow">See the video released by EarthComputer announcing the finding.</a>
        <br>
        <span>18 Jul 2020, 15:32:11 UTC
    
            · <a href="https://minecraftathome.com/minecrafthome/forum_thread.php?id=42"> Discuss</a>
        </span></p></div><hr>
    <div><p><span>Enjoy the scenery</span><br>

        <span>Minecraft@Home is now <span color="red">over one month old</span>! To celebrate this milestone, I present a new research focus; the panorama project.</span></p><p>

If you were here during beta-testing, you might have received a very early version of panorama tasks, and the eagle-eyed among you may have seen the application details <a href="https://minecraftathome.com/minecrafthome/server_status.php" rel="nofollow">on the server status page</a>.</p><p>

<span><b>The panorama app is a CUDA-only app for Linux and Windows with an Nvidia driver version of 418.96 or higher.</b></span></p><p>

This project attempts to find the world seed of the iconic panorama image which appeared in the background of the main menu of Minecraft between 2011 and 2018. The first phase of this project will only last a few days, and we shall update you with their progress in the coming weeks.</p><p>

<img src="https://i.imgur.com/3dyexWe.png"></p><p>

Right now, the application is quite substantial. Unlike the OpenCL applications for the Kaktwoos project, <b>if you allow BOINC to run tasks always; you may experience some stuttering or lag in your desktop environment while running these tasks</b>. These tasks do not have checkpointing support, but run in around 1 hour on an average host to mitigate the majority of lost cycles.</p><p>

<span>As always, <a href="https://minecraftathome.com/minecrafthome/prefs.php?subset=project" rel="nofollow">you can change which projects of which you decide to participate in your user preferences</a>.</span></p><p>

Let us know if you have any questions, and as always join the discussion over on <a href="https://discord.gg/xVFh9bp" rel="nofollow">the Discord server.</a>
        <br>
        <span>17 Jul 2020, 15:31:43 UTC
    
            · <a href="https://minecraftathome.com/minecrafthome/forum_thread.php?id=39"> Discuss</a>
        </span></p></div><hr>
    <div><p><span>Kaktwoos 2.03 and Badges!</span><br>

        You may have seen many workunits just disappear from existence today.</p><p>

<img src="https://munin.kiska.pw/munin-cgi/munin-cgi-graph/Munin-Node/Munin-Node/results_minecraftathome-pinpoint=1594193183,1594247603.png?&amp;lower_limit=&amp;upper_limit=&amp;size_x=400&amp;size_y=200"></p><p>

Worry not! We realised there were far too many workunits scanning duplicate seeds, so we've scaled back the original workunits to the correct seed ranges <span><i>(no in-progress results were touched, so none of you should have lost any credit)</i></span>.</p><p>

We have located some promising seed candidates which were missed from processing and can be used as an input to this job, so they are currently set as the highest priority.</p><p>

Also, <span><b>we now have badges</b></span>! If you view the forums, any comments in threads, or on the leaderboards; you will see the new badges.<br>
We're open to suggestions for future badges, so please leave us some comments on this thread.
        <br>
        <span>8 Jul 2020, 21:06:03 UTC
    
            · <a href="https://minecraftathome.com/minecrafthome/forum_thread.php?id=30"> Discuss</a>
        </span></p></div><hr>
    <div><p><span>Profile creation and OpenCL vendor pinning</span><br>

        In the last news post, I advised everyone to create a profile in order to be eligible for 'user of the day' selection.<br>
Unfortunately, there was an issue with the ReCaptcha implementation which prevented this. This issue is now resolved.</p><p>

Now, you are able to <a href="https://minecraftathome.com/minecrafthome/create_profile.php" rel="nofollow">create a profile here</a>.</p><p>

Also, good news for hosts with OpenCL capable hardware from more than one vendor <i>(e.g. an Intel iGPU and an Nvidia GPU)</i><br>
The latest update to the kaktwoos app should ensure the tasks run on the correct device.<br>
If you are a user with a multi-vendor host, please keep an eye on your results and let us know if you're having any issues.</p><p>

As always, please get involved with the conversation in the <a href="https://minecraftathome.com/minecrafthome/forum_index.php" rel="nofollow">message boards</a>, and <a href="https://discord.gg/xVFh9bp" rel="nofollow">join the Discord</a>!
        <br>
        <span>3 Jul 2020, 16:57:49 UTC
    
            · <a href="https://minecraftathome.com/minecrafthome/forum_thread.php?id=22"> Discuss</a>
        </span></p></div><hr>
    <p><a href="https://minecraftathome.com/minecrafthome/old_news.php">... more</a></p><p><small>
    News is available as an <a href="https://minecraftathome.com/minecrafthome/rss_main.php">RSS feed &nbsp; <img src="https://minecraftathome.com/minecrafthome/img/rss_icon.gif" alt="RSS"></a>
        </small></p></div></div>]]>
            </description>
            <link>https://minecraftathome.com/minecrafthome/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23895789</guid>
            <pubDate>Mon, 20 Jul 2020 08:19:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Demo of OpenAI's GPT-3 generating tweets given a word]]>
            </title>
            <description>
<![CDATA[
Score 39 | Comments 26 (<a href="https://news.ycombinator.com/item?id=23895706">thread link</a>) | @hardmaru
<br/>
July 20, 2020 | https://thoughts.sushant-kumar.com/hong%20kong | <a href="https://web.archive.org/web/*/https://thoughts.sushant-kumar.com/hong%20kong">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
	<blockquote>
		<p><span>“</span>Hong Kong is the New York of the 1980s and China is to America what the Soviet Union was to it.<span>”</span></p>
	</blockquote>

		</div></div>]]>
            </description>
            <link>https://thoughts.sushant-kumar.com/hong%20kong</link>
            <guid isPermaLink="false">hacker-news-small-sites-23895706</guid>
            <pubDate>Mon, 20 Jul 2020 08:03:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why GPT-3 Matters]]>
            </title>
            <description>
<![CDATA[
Score 30 | Comments 14 (<a href="https://news.ycombinator.com/item?id=23895481">thread link</a>) | @teruakohatu
<br/>
July 20, 2020 | https://leogao.dev/2020/05/29/GPT-3-A-Brief-Summary/ | <a href="https://web.archive.org/web/*/https://leogao.dev/2020/05/29/GPT-3-A-Brief-Summary/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
      
        <!-- TOC -->
        
        <p><img src="https://leogao.dev/images/gpt3/title.png" alt="Number of Parameters of GPT-3 compared to previous models. (<a href='https://www.willstats.com/'>Edited by WillStats</a>, <a href='https://arxiv.org/abs/1910.01108'>Original 1</a>, <a href='https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/'>Original 2</a>)"></p>
<p><span>The sheer scale of the new GPT-3 model</span> is hard to overstate; it’s an entire <em>order of magnitude</em> larger than Microsoft’s <a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" target="_blank" rel="noopener">already-massive 17B parameter Turing-NLG</a>.<sup><a href="#fn1" id="fnref1">[1]</a></sup> Loading the entire model’s weights in fp16 would take up an absolutely preposterous 300GB of VRAM, not even including the gradients. But, with massive size comes massive generalization ability: GPT-3 is competitive in many benchmarks <em>without even tuning on the target task</em>. And when I say many, I mean <em>many</em>—the full, 72-page paper contains an extensive evaluation of GPT-3 on many NLP datasets. Through the <a href="https://openai.com/blog/openai-api/" target="_blank" rel="noopener">OpenAI API</a>, a vast array of impressive demos have sprung up taking advantage of the generalization capabilities of GPT-3 to do extremely disparate tasks. Perhaps the most impressive part, though, is that even at such a massive scale, the model still scales smoothly in performance instead of plateauing, implying that still-larger models would perform <em>even better</em>. Throughout the rest of this post, my goal is to distill this massive (in multiple ways) paper down to a digestible size, and shed some light on why it matters.</p>

<p>The following table summarizes some of the largest autoregressive Transformer models of the past few years. I’ve excluded models like <a href="https://arxiv.org/abs/1906.08237" target="_blank" rel="noopener">XLNet</a> and BERT-derivatives because they don’t have the same unidirectional autoregressive training target.</p>
<table>
<thead>
    <tr><th></th>
    <th>Parameters</th>
    <th>Layers</th>
    <th>Hidden Size</th>
    <th>Attn Heads</th>
    <th>Attn Head Dimension</th>
    <th>Context Length</th>
</tr></thead>
    <tbody><tr>
        <th><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">GPT</a></th>
        <td>0.110B</td>
        <td>12</td>
        <td>768</td>
        <td>12</td>
        <td>64</td>
        <td>512</td>
    </tr>
    <tr>
        <th><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">GPT-2</a></th>
        <td>1.542B</td>
        <td>48</td>
        <td>1600</td>
        <td>25</td>
        <td>64</td>
        <td>1024</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener">Megatron-LM</a></th>
        <td>8.3B</td>
        <td>72</td>
        <td>3072</td>
        <td>32</td>
        <td>96</td>
        <td>1024</td>
    </tr>
    <tr>
        <th><a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" target="_blank" rel="noopener">Turing-NLG</a></th>
        <td>17B</td>
        <td>78</td>
        <td>4256</td>
        <td>28</td>
        <td>152</td>
        <td>1024</td>
    </tr>
    <tr>
        <th><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">GPT-3</a></th>
        <td>175.0B</td>
        <td>96</td>
        <td>12288</td>
        <td>96</td>
        <td>128</td>
        <td>2048</td>
    </tr>

</tbody></table>
<p>While GPT-3 isn’t that much deeper, its width is nearly <strong>3x</strong> that of <a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" target="_blank" rel="noopener">Turing-NLG</a>, which—since parameter count scales approximately proportional to the square of the hidden size—explains where most of the extra parameters come from. It also has double the context size, at 2048 tokens, which is impressive (and memory-expensive!), though not the biggest context size across all models; some models have even longer contexts, like <a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noopener">Transformer-XL</a>, which incorporates longer contexts by passing context vectors between segments, and <a href="https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html" target="_blank" rel="noopener">Reformer</a>, which uses locality-sensitive hashing to enable sparser attention. Similarly, GPT-3 uses <a href="https://arxiv.org/abs/1904.10509" target="_blank" rel="noopener">sparse attention layers</a> in every other layer, though the exact details are left somewhat ambiguous. It’s also interesting to note that the smaller GPT-3 versions trained for comparison with GPT-2 are slightly shallower and wider, with GPT-3-XL having only 24 layers but a hidden size of 2048.<sup><a href="#fn2" id="fnref2">[2]</a></sup> GPT-3 also reuses the BPE tokenization of GPT-2. Overall, GPT-3 is essentially just a downright massive version of GPT-2.</p>

<p><img src="https://leogao.dev/images/gpt3/tdata.png" alt="Weighted Training Data (<a href='https://arxiv.org/abs/2005.14165'>Source</a>)"></p>
<p>The training data is a reweighted mix of Common Crawl, WebText2 (a larger version of the original that <a href="https://arxiv.org/abs/2001.08361" target="_blank" rel="noopener">also includes links sampled in the period of Jan-Oct 2018</a>), two book corpora, and English Wikipedia. Some of these components, such as Wikipedia, were seen more than 3 times during training; others, like the massive Common Crawl component, had less than half of their data seen. The authors claim that this is to help raise the overall quality of the corpus by prioritising known-good datasets. Also, in contrast to the original WebText, this new corpus is not filtered by language, but English still constitutes 93% of the dataset by words simply due to its prevalence. Altogether, the dataset is 500 billion tokens, or 700GB<sup><a href="#fn3" id="fnref3">[3]</a></sup>, after filtering and cleaning. The paper also provides a detailed description of the filtering process of the dataset, which the GPT-2 paper didn’t.</p>
<p>The authors also attempted to remove any data that overlapped with the train and test sets of the evaluations. Unfortunately, due to a bug, some were missed, so to compensate the paper provides a fairly good analysis of the impact of this leakage.</p>

<p><img src="https://leogao.dev/images/gpt3/perf-small.png" alt="Zero-, One-, and Few-shot performance of GPT-3 scaling with parameter count (<a href='https://arxiv.org/abs/2005.14165'>Source</a>)"></p>
<p>The Evaluation section of GPT-3 is very comprehensive, evaluating on a massive battery of NLP tasks in the Zero-shot (given only a natural language description in the generation context), One-shot (a single example in the generation context), or Few-shot (a small handful of examples in the generation context) settings. This setting is worth emphasizing as perhaps one of the biggest differences in ability between GPT-2 and its predecessors, because being able to <em>infer the task</em> from just one or a few examples is a massive step forward in generalization. Whereas previous models all relied on task-specific tuning, GPT-3 can be “tuned” merely by giving it instructions <em>in plain English</em>! In fact, the paper doesn’t even attempt to fine-tune on the target task, leaving that to future work.<sup><a href="#fn4" id="fnref4">[4]</a></sup> However, one crucial conclusion is that in almost all tests, performance continues to get better with larger models, even across 4 entire orders of magnitude, whereas fine-tuning only improves on one task and <a href="https://arxiv.org/pdf/1901.11373.pdf" target="_blank" rel="noopener">risks catastrophic forgetting and overfitting</a>.</p>
<p>Without going too much into the individual tests, the general result is this: on most tasks, GPT-3 achieves performance significantly worse than fine-tuned SOTA (i.e SuperGLUE, CoQA, Winograd, to name a few), but beating fine-tuned SOTA for some other tasks (i.e PhysicalQA, LAMBADA, Penn Tree Bank). GPT-3 does particularly well on PTB in particular, taking the SOTA perplexity from 35.76 down to 20.5—a massive improvement. GPT-3 can also finally do some arithmetic, something GPT-2 was unable to do well.<sup><a href="#fn5" id="fnref5">[5]</a></sup></p>
<p><img src="https://leogao.dev/images/gpt3/newsgen-small.png" alt="People are unable to separate GPT-3 generated news articles from real ones (<a href='https://arxiv.org/abs/2005.14165'>Source</a>)"></p>
<p>Impressively, and perhaps somewhat alarmingly, people are unable to distinguish GPT-3 generated news stories from real ones, only exacerbating the ethical concerns already raised by GPT-2. The paper analyzes the result of the release of GPT-2, and concludes that the release of GPT-2 has not led to widespread use of LMs for misinformation due to the difficulty of controlling output and variance in output quality, both among low-to-mid skill adversaries and “advanced persistent threats”—adversaries with “high skill and long-term agendas”—such as state actors. However, the paper also acknowledges that with further development, LMs will eventually become advanced enough for these adversaries.</p>
<p>The authors also investigate gender bias in GPT-3, showing that GPT-3 is male leaning; however, the authors claim that some preliminary evidence on the Winogender dataset (which tests coreference resolution on the same sentence but with different gendered pronoun) seems to suggest that larger models are more robust to bias issues. Similar issues appeared for race and religion, with the sentiment of coöccurrent terms varying significantly with race. The authors claim that this issue also got better with the larger models—although, without proper hypothesis testing, it’s difficult to draw any solid conclusions here.</p>

<p>GPT-3 has already been used for a smorgasbord of different applications through the OpenAI API. You can ask it to <a href="https://twitter.com/sharifshameem/status/1282676454690451457" target="_blank" rel="noopener">write</a> <a href="https://twitter.com/hturan/status/1282261783147958272" target="_blank" rel="noopener">code</a>, turn <a href="http://vimeo.com/427943407/98fe5258a7" target="_blank" rel="noopener">natural language commands into shell commands</a>, and simulate <a href="https://www.aiwriter.email/" target="_blank" rel="noopener">chatting with famous people</a>. You can ask it to <a href="https://twitter.com/QasimMunye/status/1278750809094750211" target="_blank" rel="noopener">answer medical questions</a>, or <a href="https://www.gwern.net/GPT-3#navy-seal-copypasta" target="_blank" rel="noopener">write parodies of the navy seal copypasta</a>. You can ask it to <a href="https://andrewmayneblog.wordpress.com/2020/06/13/openai-api-alchemy-summarization/" target="_blank" rel="noopener">summarize passages for second graders</a>, or <a href="https://www.gwern.net/GPT-3#transformer-poetry" target="_blank" rel="noopener">write poetry</a>.</p>
<p>It’s important to remember all these are done by the <em>exact same model</em> trained <em>only</em> on modelling text; all that’s different is that it has been “asked nicely” to do different things. These apps showcase the versatility of GPT-3 across many disparate domains—something that, if it were done with GPT-2, would require days or even weeks of extensive data engineering and fine tuning, rather than 15 minutes of prompt crafting. This new paradigm of programming through crafting plain-English prompts, jokingly dubbed <a href="https://twitter.com/karpathy/status/1273788774422441984" target="_blank" rel="noopener">“Software 3.0”</a>, has achieved results that are already impressive, but even more impressive when viewed through the lens of <strong>generalization</strong>; GPT-3 wasn’t trained to do any of these things in particular, but it could still be asked<sup><a href="#fn6" id="fnref6">[6]</a></sup> to do them, and fairly well at that!</p>

<p><img src="https://leogao.dev/images/gpt3/perf_scaling_compute.png" alt="Performance continues to scale with compute. (<a href='https://arxiv.org/abs/2005.14165'>Source</a>)"></p>
<p>But why does GPT-3 matter, if it can’t even beat SOTA across all benchmarks? Why should we care about a model so large that a small computing cluster is necessary even just to run inference at a reasonable speed?</p>
<p>One thing about GPT-3 is that it’s doing reasonably well on tasks it has <em>never even seen</em>, and sometimes tasks not even anticipated by the developers of the model. Additionally, instead of reaching a point of diminishing returns, GPT-3 shows that the trend of larger models performing better continues for at least another order of magnitude, with no signs of stopping. Even though GPT-3 is unwieldy, and even though it still doesn’t quite reach human level performance across the board, GPT-3 shows that it’s <em>possible</em> for a model to someday reach human levels of generalization in NLP—and once the impossible becomes possible, it’s only a matter of time until it becomes practical.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>Back when I talked about <a href="https://leogao.dev/2019/10/27/The-Difficulties-of-Text-Generation-with-Autoregressive-Language-Models/">large Transformer language models like GPT-2, CTRL, and Megatron-LM late last year</a>, I touched briefly on the trend of Language Models getting bigger, and covered some of the issues that simply more compute might not fix. My general anticipation was that the model size arms race would soon be at a temporary standstill, with focus being diverted to better decoding strategies for text generation (perhaps via RL-based methods). I most certainly had not expected that OpenAI would be back at it so soon with such a massive model.</p>
<p>This was such a surprise that I dropped everything to read the paper and work on this post, including a more theory-oriented post that I’ve been working on for a few months now. It will probably be finished <span>soon™</span>, after I recover from GPT-3 shock. Stay tuned! <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>It’s likely that this was done for easier model parallelism—bigger matrix multiplications are much easier to parallelize than sequentially-applied layers à la <a href="https://arxiv.org/abs/1811.06965" target="_blank" rel="noopener">GPipe</a>.</p>
<p>This could have other advantages too, though. After <a href="https://arxiv.org/abs/1905.11946" target="_blank" rel="noopener">EfficientNet</a> came out, I independently ran some experiments of the same concepts to Transformer models, and the result was that for the same amount of compute, wider models had a sizeable advantage over deeper ones—which corroborates the choice here to …</p></li></ol></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://leogao.dev/2020/05/29/GPT-3-A-Brief-Summary/">https://leogao.dev/2020/05/29/GPT-3-A-Brief-Summary/</a></em></p>]]>
            </description>
            <link>https://leogao.dev/2020/05/29/GPT-3-A-Brief-Summary/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23895481</guid>
            <pubDate>Mon, 20 Jul 2020 07:21:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Video Vectorization]]>
            </title>
            <description>
<![CDATA[
Score 122 | Comments 55 (<a href="https://news.ycombinator.com/item?id=23895211">thread link</a>) | @xanthine
<br/>
July 19, 2020 | https://vectorly.io/docs/technology/ | <a href="https://web.archive.org/web/*/https://vectorly.io/docs/technology/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">

<p><img src="https://vectorly.io/docs/img/vector-graphics.png" alt="Drawing"></p>

<p>Vectorly is developing a new kind of video compression technology, which uses computer vision and vector graphics to reduce bitrates for video content by an order of magnitude (or more) compared to HEVC, while <strong>improving video</strong> quality. </p>
<p>This would be primarily effective for "vector friendly" video content, which would include animations, screen-casts, many e-learning videos and potentially 3d gaming content. </p>
<p>By leveraging existing vector-graphics rendering capabilities on all devices, this codec wouldn't require end-users, OEMs or browsers to install special software to enable playback of these videos.</p>
<p><strong>We are still in the early phases of developing this technology</strong>.</p>
<p>You can learn more about the technology in our <a href="https://files.vectorly.io/Vectorization+Whitepaper+v06.20.pdf">whitepaper</a></p>
<h2 id="the-core-idea">The Core Idea</h2>
<p>The core insight behind this project was that you could use vector-graphics based animations to simulate "videos" in a way that is indistinguishable from a traditional raster-graphics based video format such as an h264 video stream in an MP4 container.</p>
<h3 id="raster-graphics">Raster Graphics</h3>
<p>Normal videos, like the ones you see on Netflix or YouTube, are just sequences of images which get updated quickly on the screen, to create the illusion of motion. Each image is composed of "pixels" - individual dots of color. Higher resolution means more pixels, better visual quality, and bigger file sizes.</p>
<p><img alt="Pixel-Based" src="https://vectorly.io/docs/img/pixels.png"></p>
<p>Almost all video on the internet is of this format, known as "raster graphics". Video compression algorithms like h264 are just very efficient at using fewer data-points to reconstruct the pixels in any given frame, and at storing only the differences in pixels between frames of a video. </p>
<h3 id="vector-grapics-video">Vector Grapics video</h3>
<p>In contrast, we use a concept called "vector-graphics" to render video. Instead of pixels, we represent everything on the screen using shapes, lines and curves, which can be represented as mathematical equations (vector graphics).</p>
<p><img alt="Vector-Based" src="https://vectorly.io/docs/img/vector2.png"></p>
<p>Using these mathematical equations, we can re-draw any arbitrary shape on the screen - from the letter "T" to Bart Simpson's head. Furthermore, by adding information such as color, position on the screen, and how they move or change shape over time, you can create whole videos - including entire episodes of the Simpsons, with just sequences of mathematical equations.</p>
<h3 id="why-vectorization">Why vectorization?</h3>
<p>The core insight behind this project was that for a certain kind of "vector-friendly" video content, storing the video using vector graphics would be much more efficient than using raster graphics (in some cases, up to 2 orders of magnitude more efficient).</p>
<p>This idea is not substantively different from the idea of Flash based animations about 20 years ago. Why do this now?</p>
<p><strong>No need for a decoder</strong>: Most devices now support SVG, HTML5, WebGL/OpenGL and/or some form of hardware-accelerated vector-graphics rendering. That lets you render vector-graphics content on any device without require end-users, OEMs or browsers to install special software to enable playback of vector-graphics content, and to achieve native-level performance by doing so. App developers would only need to include an appropriate library or SDK in their website or app to enable playback within native or 3rd player video players.</p>
<p><strong>Computer vision</strong>: Our patented vectorization technology relies heavily on computer vision to convert raster-graphics videos to a vector format. Leveraging the advancement &amp; commoditization of Computer Vision, and the ease of running batch computer-vision heavy tasks on the cloud, it's feasible to 'vectorize' large volumes of video at scale now, in a way that wasn't possible even 5 years ago.</p>
<h3 id="vector-graphics-video-format">Vector graphics video format</h3>
<p>We are building a video-format based on existing standards (SVG, WebGL &amp; OpenGL), extending it with Javascript to enable video features such as a timeline and key-frames. We package the resulting video data within an MP4 container, which can be streamed and distributed using existing video infrastructure (such as HLS/DASH, and DRM systems).</p>
<pre><code>&lt;video src="vectorized.mp4" type="video/svg"&gt;
</code></pre>
<p>We are pragmatic, and don't want to create a standard <a href="https://xkcd.com/927/">for the sake of creating a standard</a>.  To that end, we've created libraries and SDKs that enable playback of our vector-graphics videos using standard / native interfaces like so</p>
<pre><code>&lt;script src="vectorly.js"&gt;

&lt;video src="vectorized.mp4" type="video/svg"&gt;
// This will work on all major browsers today
</code></pre>
<h2 id="demos-proof-of-concept">Demos / Proof of concept</h2>
<p><strong>Simpsons</strong></p>
<p>Our first vectorized proof of concept for animations is a 17 second clip of the Simpsons located <a href="https://files.vectorly.io/demo/v0-2-simpsons-250kbps/index.html">here</a>. Keep in mind, our technology is still at a very early stage, and this is much optimization work left to be done.</p>
<p><strong>Khan Academy</strong></p>
<p>Our technology also works very well for e-learning, and especially Khan Academy style content. You can find 30 second Khan Academy clip <a href="https://files.vectorly.io/demo/khan-20kbps/index.html">here</a></p></div></div>]]>
            </description>
            <link>https://vectorly.io/docs/technology/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23895211</guid>
            <pubDate>Mon, 20 Jul 2020 06:30:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[In Praise of ZFS on Linux's ZED 'ZFS Event Daemon']]>
            </title>
            <description>
<![CDATA[
Score 77 | Comments 18 (<a href="https://news.ycombinator.com/item?id=23894790">thread link</a>) | @zdw
<br/>
July 19, 2020 | https://utcc.utoronto.ca/~cks/space/blog/linux/ZFSZEDPraise | <a href="https://web.archive.org/web/*/https://utcc.utoronto.ca/~cks/space/blog/linux/ZFSZEDPraise">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2>In praise of ZFS On Linux's ZED 'ZFS Event Daemon'</h2>

	<p><small>July 19, 2020</small></p>
</div><div><p>I've written before (<a href="https://utcc.utoronto.ca/~cks/space/blog/linux/ZFSFileserverSameness">here</a>) about how <a href="https://utcc.utoronto.ca/~cks/space/blog/linux/ZFSFileserverSetupIII">our
current Linux ZFS fileservers</a> work much
like <a href="https://utcc.utoronto.ca/~cks/space/blog/solaris/ZFSFileserverSetupII">our old OmniOS fileservers</a>.
However, not everything is quite the same between ZFS on Linux and
traditional Solaris/OmniOS ZFS. One of the most welcome differences
for us is <a href="https://zfsonlinux.org/manpages/0.8.4/man8/zed.8.html">ZED</a>,
the ZFS Event Daemon. What ZED does that is so great is that it provides
a very simple way to take action when <a href="https://zfsonlinux.org/manpages/0.8.4/man5/zfs-events.5.html">ZFS events</a> happen.</p>

<p>When a ZFS event happens, ZED looks through a directory (generally
<code>/etc/zfs/zed.d</code>) to find scripts (or programs) that should be run
in response to the event. Each script is run with a bunch of
environment variables set to describe what's going on, and it can
use those environment variables to figure out what the event is.
ZED decides what things to run based on their names; generally you
wind up with script names like <code>all-cslab.sh</code> (which is run on
all events) and <code>resilver_finish-cslab.sh</code> (which is run when a
resilver finishes).</p>

<p>Because these are just a collection of individual files, you're
free to add your own without colliding with or having to alter the
standard 'ZEDLETs' provided by ZFS on Linux. Your additions can do
anything you want them to, ranging from the simple to the complex.
For instance, our simplest ZEDLET simply syslogs all of the ZED
environment variables:</p>


<blockquote><pre>PATH=/usr/bin:/usr/sbin:/bin:/sbin:$PATH
export PATH
if [ "$ZEVENT_SUBCLASS" = "history_event" ]; then
        exit 0
fi
unset ZEVENT_TIME
unset ZEVENT_TIME_STRING
printenv | fgrep 'ZEVENT_' | sort | fmt -999 |
    logger -p daemon.info -t 'cslab-zevents'
exit 0
</pre>
</blockquote>

<p>(There's a standard 'all-syslog.sh' ZEDLET, but it doesn't syslog
all of the information in the zevents. Capturing all of the information
is especially useful if you want to write additional ZEDLETs and
aren't quite sure what they should look for or what environment
variables have useful information.)</p>

<p>It can take a bit of time and experimentation to sort out what ZFS
events are generated (and with what information available) in
response to various things happening to adn in your ZFS pools. But
once you have figured it out, ZED gives you a way to trigger and
drive all sorts of system management activities. These can be active
(like taking action if devices fail) or passive (like adding markers
in your metrics system or performance dashboards for when ZFS scrubs
or resilvers start and end, so you can correlate this with other
things happening).</p>

<p>Coming from Solaris and OmniOS, where there was no such simple
system for reacting to things happening in your ZFS pools, ZED was
a breath of fresh air for us. More than anything else, it feels
like how ZFS events should have been handled from the start, so
that system administrators could flexibly meet their own local needs
rather than having to accept whatever the Solaris Fault Management
system wanted to give them.</p>

<p>PS: Because ZFS on Linux is now OpenZFS, I believe that ZED will
probably eventually show up in FreeBSD (if it isn't already there).
Perhaps it will even some day be ported back to Illumos.</p>
</div></div>]]>
            </description>
            <link>https://utcc.utoronto.ca/~cks/space/blog/linux/ZFSZEDPraise</link>
            <guid isPermaLink="false">hacker-news-small-sites-23894790</guid>
            <pubDate>Mon, 20 Jul 2020 05:03:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Self Hosting Email Server]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 4 (<a href="https://news.ycombinator.com/item?id=23894411">thread link</a>) | @g-garron
<br/>
July 19, 2020 | https://www.garron.blog/posts/host-your-email-server.html | <a href="https://web.archive.org/web/*/https://www.garron.blog/posts/host-your-email-server.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <h2 id="introduction">Introduction</h2>

<p>In 1998 at my first job, I was the system administrator of the biggest Internet provider in Bolivia, it still is the biggest provider. The Internet arrived here in 1995 or 1996, so it was just starting.</p>

<p>I was in charge of BGP routers, with a satellite connection to the Internet, the whole country had just 2 mbps of connection back in those days. I was also in charge of the email server, (roble.scz.entelnet.bo), the DNS server run on a Unix machine while the email and web server on a RedHat Linux powered PC.</p>

<p>Fast forward to 2001, I started my own company, it had nothing to do with system administration, but anyway we needed email service, so I just got an on-line channel with a /28 public IP segment, and setup a CentOS server running bind, sendmail, Dovecot.</p>

<p>In 2007, I started blogging on Drupal running on that same server using Apache, PHP and MySQL <a href="https://www.garron.me/en/go2linux/fedora-centos-root-password-recovery.html">here the first post</a>. I was used to host services.</p>

<h2 id="changes">Changes</h2>

<p>But then I had less time in my hands, it was also more difficult to maintain the services running, keeping up to date with Spam filters and rules became difficult, Bind also had its problems, and Slashdot happened and Bandwidth was not enough.</p>

<p>I moved my server to a web hosting company, and then to a Linode VPS, I also outsourced the DNS service and moved the email to Google Apps.</p>

<p>I migrated my old server to Debian and used it just for:</p>

<ul>
  <li>DHCP</li>
  <li>NAT</li>
  <li>DNS Cache</li>
  <li>Squid</li>
</ul>

<p>That was in 2010 more or less, so I stopped hosting my services, only my blog was in my hands, but at that time I started using Jekyll, so it was just a VPS with Nginx.</p>

<h2 id="back-on-hosting-my-own-services">Back on Hosting my own services</h2>

<p>In April this year I started this blog, but I also enrolled in the Fediverse, and decided to <a href="https://www.garron.blog/posts/my-own-mastodon-server.html">run my own instance</a>, therefore I was hosting again, that was not just a NGINX server, then I read <a href="https://yarmo.eu/post/selfhost-email">this</a>, <a href="https://yarmo.eu/post/selfhost-email-drawbacks">this</a> and <a href="https://lazybear.io/posts/should-you-sefhost-your-email/">this</a>.</p>

<p>And the idea of running my services was in my head again. I wanted to do it different with time, it is not 1998 anymore, so I decided to learn about <a href="https://www.garron.blog/posts/dockerize-everything.html">Docker</a>. So now I have in just one Arch Linux server:</p>

<ul>
  <li>Two static sites</li>
  <li>One Wordpress Site</li>
  <li>One RSS reader (Miniflux)</li>
  <li>One password manager app (Bitwarden)</li>
  <li>One read-later app (<a href="https://www.garron.blog/posts/wallabag-review.html">Wallabag</a>)</li>
  <li>One Webmention app <a href="https://zerokspot.com/weblog/2020/06/14/setting-up-webmentiond/">Webmentiod</a></li>
</ul>

<p>All running with Docker Compose files, but there was something missing, it was email. (Mastodon has its own server -yet-)</p>

<h2 id="hosting-your-own-email-server">Hosting your own email server</h2>

<p>I wanted to run an email server on Docker, so I looked for alternatives and I have found that there are some really nice.</p>

<h3 id="posteiohttpsposteio"><a href="https://poste.io/">Poste.io</a></h3>

<p>I have not tested it, but seems really easy to deploy it has a free version that I think is enough for personal use, with two professional options with a monthly charge.</p>

<h3 id="mailuiohttpsmailuio"><a href="https://mailu.io/">Mailu.io</a></h3>

<p>It is a clone of Poste.io, but using only free, Open Source alternatives, I have installed and configured, it took me just 45 minutes to have server running for one of my domains. It has a tool that create a configuration file based on your options, and it set up everything for you, you can find it <a href="https://setup.mailu.io/1.7/">here</a>, you only need to answer a few questions, and run three or four commands and there you have a complete email server.</p>

<ul>
  <li>SMTP</li>
  <li>IMAP</li>
  <li>POP3</li>
  <li>AntiSpam</li>
  <li>AntiVirus</li>
  <li>Webmail</li>
  <li>Admin web front-end</li>
</ul>

<p>Really a great option, I recommend it to anyone.</p>

<h3 id="docker-mail-serverhttpshubdockercomrtvialdocker-mailserver"><a href="https://hub.docker.com/r/tvial/docker-mailserver">Docker Mail Server</a></h3>

<p>Then I found this one, it is also a complete suite, all Open Source, with really good documentation, and ready to deploy on a server with Docker. It is not as easy as Mailu, but I decided to use it. I followed <a href="https://github.com/tomav/docker-mailserver/wiki/Installation-examples">this guide</a>, yes I know, you still need Gmail or any other provider to send your emails, if you follow that guide. But I was not ready to jump into the pool completely.</p>

<p>So, here is my setup:</p>

<ul>
  <li>MX record points to my server, so all my email is received by my server</li>
  <li>All email is then forwarded to other accounts (Gmail accounts in this case)</li>
  <li>Gmail is configured to use those accounts as default and not the Gmail address, and uses my SMTP server to send</li>
</ul>

<p>This way I am sure I am using my domain, because we already agreed that <a href="https://www.garron.blog/posts/own-your-domain.html">everybody should own his domain</a>, right?. I am using Gmail as my email client.</p>

<p>I am doing this way, because it is easy to search emails when they are with Gmail, also because I am not sure if I may lose my server and lose some important email. I need to be sure about that, because I am hosting for my family, not only for me.</p>

<p>Also having an IMAP client on a cell phone drains the battery, because it polls the server even when there is nothing to download, or if you keep an open IMAP IDLE connection, it will also drain the battery. There is a solution for that, you need <a href="https://pushover.net/">PushOver</a>, and this <a href="https://pushover.net/">IMAP to PushOver</a> app, I have not tested yet, but I think is the way to go.</p>

<p>Edit: After reading some comments on Reddit, these three other options seems to be good ones too.</p>

<ul>
  <li>
    <p><a href="https://mailcow.email/">MailCow</a>: Is another good option also mentioned on HN comments, and this one do have official Docker documentation <a href="https://github.com/mailcow/mailcow-dockerized">here</a></p>
  </li>
  <li>
    <p><a href="https://mailinabox.email/">Mail in a Box</a>: Although there are not official instructions to install it on Docker, it should be possible, here is an <a href="https://hub.docker.com/r/mtrnord/mailinabox/">outdated attempt</a>, and here <a href="https://discourse.mailinabox.email/t/mail-in-a-box-in-docker/4588">another one</a>. But on a VPS it seems work great, and the admin pannel guides you.</p>
  </li>
  <li>
    <p><a href="https://iredmail.org/">iRedMail</a>: This one is also a good option, I have test it two months ago for a week and also works out of the box, you can find a well maintained docker instructions <a href="https://hub.docker.com/r/lejmr/iredmail">here</a></p>
  </li>
</ul>

<h2 id="things-to-consider">Things to consider</h2>

<p>If you plan to host your own email server, there are some important things you need to consider, and it is to properly configure your DNS server. Here is what you need:</p>

<h3 id="spf-record">SPF record</h3>

<p>You really need to configure a SPF record, it is not the same with all providers, but there is plenty of documentation, and it is not hard to do. This record basically tells the recipient server who is authorized to send mail on behalf of your domain. The most common configuration is that the MX server are the authorized ones, something like this.</p>

<pre><code>v=spf1 mx ~all
</code></pre>

<h3 id="dkim-record">DKIM record</h3>

<p>This one is a little bit more difficult, you need to create the DKIM key, your email server may do it, all examples above can do it, once you have it, you need to create a TXT record on your DNS and paste it there, it will look something like this:</p>

<pre><code>v=DKIM1; h=sha256; k=rsa; p=MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEArARSBHTh32y3VpSEe+pOI7AuILCUsYw1FNL5vTu1P3Mpte50jjkvzg+pBndcSzndQDt6B6mqLBbTwWrZR/j73CTI4ejcYL+xqSXYPU8+xNwu5uwHOMsgWMl15Z/1e1QJD9Ss3Q8aOLOkvHNIgAfSCq
</code></pre>

<p>All your emails will be signed with that, DKIM key, and the receiving server can check if the signature is valid, using the info from the DNS.</p>

<h3 id="dmarc-record">DMARC record</h3>

<p>This one is optional, but is better to have it, you can generate it using on-line tools like <a href="https://mxtoolbox.com/DMARCRecordGenerator.aspx">this one</a>. What it does is to tell what to do when something fails, and who to report that, usually the server admin, well you.</p>

<h3 id="ptr-record">PTR Record</h3>

<p>You need to have a PTR DNS records, that is the reverse DNS for the IP your server is using should point to the same name the A record has. So, for example, if you have this A record</p>

<pre><code>IN A mail.server.com 192.168.0.1
</code></pre>

<p>There should also be a reverse record like this.</p>

<pre><code>IN PTR 192.168.0.1 mail.server.com
</code></pre>

<p>You need to ask to the owner of the IP to add that record for you, if you are using a VPS, you can look in the documentation about that.</p>

<h3 id="mx-record">MX record</h3>

<p>Of course you need a MX record, but you already knew about this one, otherwise maybe is not a good idea to run your own server. This basically informs al sending servers which server is receiving email for your domain.</p>

<h3 id="test">Test</h3>

<p>Once you have set-up everything, wait 24 hours for DNS to propagate and test it, <a href="https://www.mail-tester.com/">this tool</a> is great</p>

<h3 id="secondary-mail-server">Secondary mail server</h3>

<p>Edit 2:</p>

<p>After <a href="https://lobste.rs/s/iatbst/how_self_host_your_email_server#c_hc7vvv">this comment</a> I want to add this recommendation too.</p>

<p>When the main server is down, email can not be delivered, usually it just stays in the senders queue, and once your server is up again all email is delivered to it, but if the outage is long enough some email might be returned to senders and the users will lose them.</p>

<p>The way to avoid this is to have a second machine running a secondary email server, this is simplier to setup, as it will only have the function to store email while the main server is down, and send all emails to it once is up again. There are some good documentation out there, <a href="https://www.howtoforge.com/postfix_backup_mx">here</a> and <a href="https://www.linuxbabe.com/mail-server/how-to-set-up-a-backup-email-server-postfix-ubuntu">here</a>.</p>

<p>The drawback is that now you have a second email to upgrade and keep running, and Spammers ususally target at it, so if you have backups for the main server, and you consider you can have it up and running relatively fast you can avoid a second email. One should also consider who's email is being hosted at the server, and how critical is that email for them.</p>

<p>Critical email might be on an Office 365 <strong>Exchange Online (Plan 1)</strong>, once again, under your domain.</p>

<h2 id="final-words">Final words</h2>

<p>You may not need to host your email server, you can use <a href="https://www.fastmail.com/">Fastmail</a>, <a href="https://www.zoho.com/es-xl/mail/">Zoho</a>, Office 365, or Google Suite, but you <strong>must</strong> use your domain, all accounts, all bank statements should be sent to an email address at your domain, otherwise you can lose control over your accounts. Hosting your email server is not an easy task, dealing with SPAM is hard, and downtimes can be a nightmare too, but you learn one or two things while hosting your email.</p>

        </div></div>]]>
            </description>
            <link>https://www.garron.blog/posts/host-your-email-server.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23894411</guid>
            <pubDate>Mon, 20 Jul 2020 03:24:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I Don't Want to Be a Founder]]>
            </title>
            <description>
<![CDATA[
Score 141 | Comments 111 (<a href="https://news.ycombinator.com/item?id=23894387">thread link</a>) | @kipply
<br/>
July 19, 2020 | https://carolchen.me/blog/founding-bad/ | <a href="https://web.archive.org/web/*/https://carolchen.me/blog/founding-bad/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
  <header>
    
    
  </header>

  <main id="main">
    
<article>
  <header>
    
    
  </header>
  <section id="js-article">
    
<p>I had a brief run with a startup (literally a month) and was faced with the decision of going into a YC Cohort. In that fiasco, I also spent at least twelve hours trying to convince other people to go. It's been half a year, and I've had a lot more time to reflect on reasons as to why one might want to run a startup. As you might've guessed, I decided not to do it and I genuinely believe that running a startup seems like a bad idea for the majority of people I meet who want to become founders. </p>
<p>This post definitely will not apply to everyone (I'd loosely say applicable to 80% of prospective startup founders), but I'd hope there's some valuable thinking in here. Also, note that this is fairly oriented towards technical founders. </p>
<p>Edit: This post also has a very limited scope as it's based off the thinking from my decision to continue interning at Shopify or to go to YC. It's very focused on the Silicon Valley "startup" where you get a VC to give you money and get big in a year, etc etc</p>

<h3 id="commitment">Commitment<a href="#commitment" aria-label="Anchor link for: commitment"> <i></i></a>
</h3>
<p>I'm not talking about commitment to your company. I'm talking about commitment to your cofounder (if you have one, which is likely). </p>
<p>Maybe I'm too young to understand, but marriage seems frightening! My finances, my social life, personal time, and emotional wellbeing would be largely dependent on a single person and that's scary. It should be scary or should at least take a few years for it not to become scary. </p>
<p>Your cofounder is...kind of the same? In a seed-stage it's likely you <em>actually</em> live together, and if you don't, you're likely functionally living together with the amount of work involved. They're responsible for your financial well-being. They may be responsible for the quality of your social lives (most founders spend a lot of time socializing with other tech people + founders). They're tied to your life goals, your dreams, and your passions.</p>
<p>My impression was that my relationship with my cofounder would be more intense than marriage, and <em>extra</em> bad in the event of failure since there's additional loss (and it's statistically likely, but I guess so is marriage). I totally believe that there are cofounder pairs that are completely ready to go through the founder journey and pairs that maybe weren't ready but were fine anyway, but I stand by the statement that it's more intense than marriage and not enough people put care into this. </p>
<h3 id="your-vc-is-not-the-one-at-risk-here">Your VC is Not the One at Risk Here<a href="#your-vc-is-not-the-one-at-risk-here" aria-label="Anchor link for: your-vc-is-not-the-one-at-risk-here"> <i></i></a>
</h3>
<p>I often hear sentiments that resemble "wow these VCs are taking a chance on me I better commit to this!". </p>
<p>VCs are not evil people trying to take advantage of you (actually they might be, but let's assume they're not), but they are not the ones at risk. For them, 150k or a few million is not a huge risk. Seed-stage returns will be from a very small percentage of investments, thus VCs can afford to have comically high error rates as long as they get the few that matter. If you're SoftBank you can do even worse and still have so much money! They make decisions carefully, they care about your success for various reasons, and are generally caring people (in most of my experiences) but in larger abstractions, your startup means nothing to them. </p>
<p>They're not shy about it either, the entire reason they're investing in you is because they think you're more valuable than you cost. 
<img src="https://carolchen.me/blog/img/founding/paul.png" alt=""></p>
<p>Your risk is years of your life, blood, sweat, and tears. The next few years (provided your startup lasts till then) will somewhat be in service to these VCs. The VCs are your "bosses" as you answer to them (though <em>much</em> less than a regular "boss") and to the ones you hope to raise capital from in the future. </p>
<p>It's good to take a risk with increased confidence because qualified people think you have promise. However, that can morph into "I'm going to work on this startup partially in service to these people who believed in me and gave me lots of money". These additional stresses that come from meeting VC expectations and the complications of the dynamics of the relationship can cause various problems.</p>
<h3 id="sense-of-self">Sense of Self<a href="#sense-of-self" aria-label="Anchor link for: sense-of-self"> <i></i></a>
</h3>
<p>This one is the one that got to me most but I can see it being irrelevant to a lot of other people. </p>
<p>Many founders have big egos -- I don't mean they're assholes or overly self-important but they do have very powerful confidence, because that's a valuable skill to have as a founder. Not just confidence in pitching their project to others, but in their vision and their company. They need to believe their company will be successful (though I have met founders who just want to party with VC money for a few years <em>cough cough</em> Neumann). </p>
<p>My first fear was that I created an ego for myself rapidly. Practicing to sell to clients and for your YC interview involves repeating to yourself why you are <em>good</em> and self-hypnosis is fairly powerful. I love feeling good about myself, but I suddenly found myself feeling more confident in myself than what I believed was warranted. More frighteningly, I had a major character and energy change in a couple of weeks. Losing so much of my identity like that was unnerving, not to mention the ripple effects that could've occurred in my social life. </p>
<p>The other fear is coming down from that. Startup founders (especially the more eccentric ones) sometimes believe that they will build something that will change the world. Along with that, their identities start to merge with their company. There's nothing wrong with that, but I also think it's exceptionally tragic to come down from that. It's not just dealing with failure and getting back up on your feet, it's losing a part of your identity. </p>
<h3 id="school-is-generally-a-good-idea-for-prospective-dropouts">School is Generally a Good Idea (for prospective dropouts)<a href="#school-is-generally-a-good-idea-for-prospective-dropouts" aria-label="Anchor link for: school-is-generally-a-good-idea-for-prospective-dropouts"> <i></i></a>
</h3>
<p>Being a good engineer seems underrated for being a good startup founder. Not just being able to code fast, but being able to make good engineering decisions, conduct good technical interviews and attract talent. Some engineering skills can't be worked around with "I am very smart and can learn fast" and require extended time and practice. With that, I also think prospective founders also overestimate the amount of learning on the job that can be done on the engineering side, mostly because it's harder to learn when you're in a rush to release features than if you could take your time on a course project. It's true that founders will learn more than they will in school, but the technical development may not be as strong. My model is that the best schooling experience is better technical education and the best founding experience. </p>

<h3 id="something-to-own">Something to Own<a href="#something-to-own" aria-label="Anchor link for: something-to-own"> <i></i></a>
</h3>
<p>Lots of huge, ground-breaking products have been lead from within a large company. Examples include email client <code>hey.com</code>, Chromebooks, and countless amazing dev tools. </p>
<p>Starting these things in a large company has the benefit of security, resources and recruiting already done for you. Downsides include beaurocracy, not being able to recruit on your own accord and dealing with PR policy. There is also high barriers to starting something within a company, like being senior enough to do so and being at the right company at the right time. </p>
<p>The alternative is starting a project on the side. <a href="https://github.com/ziglang/zig">Ziglang</a> was started as a side project and is now a very promising programming language. The creator has since then left his job to work on Zig, but it is also possible to "own" something significant without even having to leave your job. Examples includes Julia Evan's <a href="https://jvns.ca/">blog</a> (not actually a work-side-project) and line of zines, Cassidy William's <a href="https://drop.com/buy/drop-dsa-astrolokeys-keycaps-by-sailorhg-and-cassidoo">keycap line</a>, Nick Frosst's successful and awesome <a href="https://goodkidofficial.com/">band</a> and many more. I understand that it's not the extent of "oh yeah Google? I built that", but I think the expected value is much higher in creating and owning something that isn't a startup. </p>
<h3 id="getting-rich">Getting Rich<a href="#getting-rich" aria-label="Anchor link for: getting-rich"> <i></i></a>
</h3>
<p>A lot of people claim that startups are less money, but I find for signicant number of founders, that's not true -- not because they'll definitely have a good exit, but because they're skilled in ways that allow them to raise enough money to pay themselves like they would at a big company. If that applies to you, then going to a startup probably is your best shot at getting rich! For other people, the expected value of industry (particularly joining a well-founded early-stage startup) is usually higher. </p>
<h3 id="not-being-at-school-for-prospective-dropouts">Not Being at School (for prospective dropouts)<a href="#not-being-at-school-for-prospective-dropouts" aria-label="Anchor link for: not-being-at-school-for-prospective-dropouts"> <i></i></a>
</h3>
<p>This seems like a valid reasons for the average CS student. School is a place where you answer to professors who don't always understand industry and do homework assignments that no one will care about. However, it seems like all of these problems can be significantly if not fully solved by building a better school experience for yourself. </p>
<p>A better program can improve many things, such as <a href="http://www.olin.edu/">Olin College of Engineering</a> that has a project-based curriculum, <a href="https://www.makeschool.com/">Make School</a> that is a two year applied-engineering degree program or <a href="https://devdegree.ca/">Dev Degree</a>, where you can work at Shopify and take more applied courses taught by Shopify throughout your degree. These programs are small and selective, but probably not harder than a semi-successful startup. Dev Degree also happens to be more financially sound, with Shopify paying for your tuition and a salary, and Make School tuition is 70k for the entire degree. </p>
<p>Another alternative is to just be worse at school and learn on the side and/or to morph your silly school assignments into productive skills and useful outputs. The <a href="http://coconut-lang.org/">Coconut Programming Language</a> was built by someone while they were in school. Some things like dynamic programming that are often deemed useless theoretical things can have <a href="https://thume.ca/2017/06/17/tree-diffing/">industry applications</a>. People have also taken mundane school projects like this compiler that almost every school will have you build in a compilers course and end up with <a href="https://thume.ca/2019/04/29/comparing-compilers-in-rust-haskell-c-and-python/">educational findings for engineers in general</a> (also see <a href="https://news.ycombinator.com/item?id=20192645">HackerNews thread</a>). In five weeks, my friend Maas was able to launch <a href="https://medium.com/@maaslalani/launch-5d02cc5e05f5">five relatively successful products</a> while enrolled in Dev Degree. </p>
<p>School is already a powerful environment of hardwork, fun and learning. I think it is a more cohesive …</p></section></article></main></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://carolchen.me/blog/founding-bad/">https://carolchen.me/blog/founding-bad/</a></em></p>]]>
            </description>
            <link>https://carolchen.me/blog/founding-bad/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23894387</guid>
            <pubDate>Mon, 20 Jul 2020 03:21:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Reverse-engineering and analysis of SanDisk High Endurance microSDXC card]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=23892463">thread link</a>) | @Manozco
<br/>
July 19, 2020 | https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/ | <a href="https://web.archive.org/web/*/https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><strong>TL;DR – The SanDisk High Endurance cards use SanDisk/Toshiba <a href="https://www.tomshardware.com/news/wd-sandisk-bics3-64-layer-3d-nand,32328.html" target="_blank" rel="noopener">3D TLC Flash</a>. It took way, way more work than it should have to figure this out (thanks for nothing, SanDisk!).</strong></p><p>In a <a href="https://ripitapart.com/2019/08/17/unboxing-and-review-of-sandisk-64gb-microsdxc-high-endurance-card/" target="_blank" rel="noopener">previous blog post</a>, I took a look at SanDisk’s microSD cards that were aimed for use in write-intensive applications like dash cameras. In that post I took a look at its performance metrics, and speculated about what sort of NAND Flash memory is used inside. SanDisk doesn’t publish any detailed specifications about the cards’ internal workings, so that means I have no choice but to reverse-engineer the <del>can of worms</del> card myself.</p><p>In the hopes of uncovering more information, I sent an email to SanDisk’s support team asking about what type of NAND Flash they are using in their High Endurance lineup of cards, alongside endurance metrics like P/E (Program/Erase) cycle counts and total terabytes written (TBW). Unfortunately, the SanDisk support rep couldn’t provide a satisfactory answer to my questions, as they’re not able to provide any information that’s not listed in their public spec sheets. They said that all of their cards use MLC Flash, which I guess is correct if you call TLC Flash 3-bit MLC (which Samsung does).</p><div>
<blockquote><p>Dear Jason,</p>
<p>Thank you for contacting SanDisk® Global customer care. We really appreciate you being a part of our SanDisk® family.</p>
<p>I understand that you wish to know more about the SanDisk® High Endurance video monitoring card, as such please allow me to inform you that all our SanDisk® memory cards uses Multi level cell technology (MLC) flash. However, the read/write cycles for the flash memory is not published nor documented only the read and write speed in published as such they are 100 MB/S &amp; 40 MB/s. The 64 GB card can record Full HD video up to 10,000 hours. To know more about the card you may refer to the link below:</p>
<p><a title="Click to follow link: https://www.sandisk.com/home/memory-cards/microsd-cards/high-endurance-microsd" href="https://www.sandisk.com/home/memory-cards/microsd-cards/high-endurance-microsd" target="_blank" rel="noopener">SANDISK HIGH ENDURANCE VIDEO MONITORING microSD CARD</a></p>
<p>Best regards,<br>
Allan B.<br>
SanDisk® Global Customer Care</p></blockquote>
<p>I’ll give them a silver star that says “You Tried” at least.</p>

<p>While (micro)SD cards feel like a solid monolithic piece of technology, they’re made up of multiple different chips, each performing a different role. A basic SD card will have a controller that manages the NAND Flash chips and communicates with the host (PC, camera, etc.), and the NAND Flash itself (made up of 1 or more Flash dies). Bunnie Huang’s blog, Bunnie Studios, has an excellent article on the internals of SD cards, including counterfeits and how they’re made – check it out <a href="https://www.bunniestudios.com/blog/?p=3554" target="_blank" rel="noopener">here</a>.</p>
<div data-shortcode="caption" id="attachment_2132"><p><a href="https://ripitapart.files.wordpress.com/2019/10/sd-card-anatomy-2.png"><img aria-describedby="caption-attachment-2132" data-attachment-id="2132" data-permalink="https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/sd-card-anatomy/" data-orig-file="https://ripitapart.files.wordpress.com/2019/10/sd-card-anatomy-2.png" data-orig-size="268,476" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SD Card Anatomy" data-image-description="<p>Block diagram of a typical SD card.</p>
" data-medium-file="https://ripitapart.files.wordpress.com/2019/10/sd-card-anatomy-2.png?w=169" data-large-file="https://ripitapart.files.wordpress.com/2019/10/sd-card-anatomy-2.png?w=268" src="https://ripitapart.files.wordpress.com/2019/10/sd-card-anatomy-2.png?w=584" alt="SD Card Anatomy" srcset="https://ripitapart.files.wordpress.com/2019/10/sd-card-anatomy-2.png 268w, https://ripitapart.files.wordpress.com/2019/10/sd-card-anatomy-2.png?w=84 84w" sizes="(max-width: 268px) 100vw, 268px"></a></p><p id="caption-attachment-2132">Block diagram of a typical SD card.</p></div>
<p>MicroSD cards often (but not always!) include test pads, used to program/test the NAND Flash during manufacture. These can be exploited in the case of <a href="https://blog.acelaboratory.com/pc-3000-flash-monolith-pinout-research.html" target="_blank" rel="noopener">data recovery</a>, or to reuse microSD cards that have a defective controller or firmware by turning the card into a piece of raw NAND Flash – check out Gough Lui’s adventures <a href="https://goughlui.com/2015/04/05/teardown-optimization-comsol-8gb-usb-flash-stick-au6989sn-gt-sdtnrcama-008g/" target="_blank" rel="noopener">here</a>. Note that there is no set standard for these test pads (even for the same manufacturer!), but there are common patterns for some manufacturers like SanDisk that make reverse-engineering easier.</p>

<p>microSD cards fall into a category of “monolithic” Flash devices, as they combine a controller and raw NAND Flash memory into a single, inseparable package. Many manufacturers break out the Flash’s data bus onto hidden (and nearly completely undocumented) test pads, which some other memory card and USB drive manufacturers take advantage of to make cheap storage products using failed parts; the controller can simply be electrically disabled and the Flash is then used as if it were a regular chip.</p>
<p>In the case of SanDisk cards, there is very limited information on their cards’ test pad pinouts. Each generation has slight differences, but the layout is mostly the same. These differences can be fatal, as the power and ground connections are sometimes reversed (this spells instant death for a chip if its power polarity is mixed up!).</p>
<p>My card (and many of their higher-end cards – that is, not their Ultra lineup) features test pads that aren’t covered by solder mask, but are instead covered by some sort of screen-printed epoxy with a laser-etched serial number on top. With a bit of heat and some scraping, I was able to remove the (very brittle) coating on top of the test pads; this also removed the serial number which I believe is an anti-tamper measure by SanDisk.</p>

		
		

<p>After cleaning off any last traces of the epoxy coating, I was greeted with the familiar SanDisk test pad layout, plus a few extra on the bottom.</p>

<p>The breakout board is relatively simple in concept: for each test pad, bring out a wire that goes to a bigger test point for easier access, and wire up the normal SD bus to an SD connector to let the controller do its work with twiddling the NAND Flash bus. Given how small each test pad is (and how many), things get a bit… messy.</p>

		
		

<p>I started by using double-side foam adhesive tape to secure the SD card to a piece of perfboard. I then tinned all of the pads and soldered a small 1uF ceramic capacitor across the card’s power (Vcc) and ground (GND) test pads. Using 40-gauge (0.1 mm, or 4-thousandths of an inch!) magnet wire, I mapped each test pad to its corresponding machine-pin socket on the perfboard. Including the extra test pads, that’s a total of 28 tiny wires!</p>
<p>For the SD connector side of things, I used a flex cable for the <a href="http://xtc2clip.org/how-it-works" target="_blank" rel="noopener">XTC 2 Clip</a> (a tool used to service HTC Android devices), as it acted as a flexible “remote SD card” and broke out the signals to a small ribbon cable. I encased the flex cable with copper tape to act as a shield against electrical noise and to provide physical reinforcement, and soldered the tape to the outer pads on the perfboard for reinforcement. The ribbon cable end was then tinned and each SD card’s pin was wired up with magnet wire. The power lines were then broken out to an LED and resistor to indicate when the card was receiving power.</p>

<p>With all of the test pads broken out to an array of test pins, it was time to make sense of what pins are responsible for accessing the NAND Flash inside the card.</p>
<h2>Test Pad Pinout</h2>
<div data-shortcode="caption" id="attachment_2165"><p><a href="https://ripitapart.files.wordpress.com/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png"><img aria-describedby="caption-attachment-2165" data-attachment-id="2165" data-permalink="https://ripitapart.com/sandisk-high-endurance-microsd-test-pad-pinout/" data-orig-file="https://ripitapart.files.wordpress.com/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png" data-orig-size="1666,935" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SanDisk High Endurance microSD Test Pad Pinout" data-image-description="<p>Diagram of the test pads on SanDisk’s High Endurance microSD card.</p>
" data-medium-file="https://ripitapart.files.wordpress.com/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=300" data-large-file="https://ripitapart.files.wordpress.com/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=584" src="https://ripitapart.files.wordpress.com/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=584&amp;h=328" alt="Diagram of the test pads on SanDisk's High Endurance microSD card." width="584" height="328" srcset="https://ripitapart.files.wordpress.com/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=584&amp;h=328 584w, https://ripitapart.files.wordpress.com/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=1168&amp;h=656 1168w, https://ripitapart.files.wordpress.com/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=150&amp;h=84 150w, https://ripitapart.files.wordpress.com/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=300&amp;h=168 300w, https://ripitapart.files.wordpress.com/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=768&amp;h=431 768w, https://ripitapart.files.wordpress.com/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=1024&amp;h=575 1024w" sizes="(max-width: 584px) 100vw, 584px"></a></p><p id="caption-attachment-2165">Diagram of the test pads on SanDisk’s High Endurance microSD card. (click to enlarge)</p></div>
<p>The overall test pad pinout was the same for other microSD cards from SanDisk, but there were some differences, primarily regarding the layout of the power pads; notably, <strong>the</strong> <strong>main power pins are backwards</strong>! This can destroy the card if you’re not careful when applying power.</p>
<p>I used my <a href="https://www.dreamsourcelab.com/shop/logic-analyzer/dslogic-plus/" target="_blank" rel="noopener">DSLogic Plus</a> logic analyzer to analyze the signals on all of the pins. Since the data pinout was previously discovered, the hard part of figuring out what each line represented (data bus, control, address, command, write-protect, ready/busy status) was already done for me. However, not all of the lines were immediately evident as the pinouts I found online only included the bare minimum of lines to make the NAND Flash accessible, with one exception being a control line that places the controller into a reset state and releases its control of the data lines (this will be important later on).</p>
<p>By sniffing the data bus at the DSLogic’s maximum speed (and using its 32 MB onboard buffer RAM), I was able to get a clear snapshot of the commands being sent to the NAND Flash from the controller during initialization.</p>
<h2>Bus Sniffing &amp; NAND I/O 101 (writing commands, address, reading data)</h2>
<p>In particular, I was looking for two commands: RESET (0xFF), and READ ID (0x90). When looking for a command sequence, it’s important to know how and when the data and control lines change. I will try to explain it step-by-step, but if you’re interested there is an <a href="https://user.eng.umd.edu/~blj/CS-590.26/micron-tn2919.pdf" target="_blank" rel="noopener">introductory white paper</a> by Micron that explains all of the fundamentals of NAND Flash with much more information about how NAND Flash works.</p>

		
		

<p>When a RESET command is sent to the NAND Flash, first the /CE (Chip Select, Active Low) line is pulled low. Then the CLE (Command Latch Enable) line is pulled high; the data bus is set to its intended value of 0xFF (all binary ones); then the /WE (Write Enable, Active Low) line is pulsed from high to low, then back to high again (the data bus’ contents are committed to the chip when the /WE line goes from low to high, known as a “rising edge”); the CLE line is pulled back low to return to its normal state. The Flash chip will then pull its R/B (Ready/Busy Status) line low to indicate it is busy resetting itself, then releases the line back to its high state when it’s finished.</p>
<p>The READ ID command works similarly, except after writing the command with 0x90 (binary 1001 0000) on the data bus, it then pulls the ALE (Address Latch Enable) line high instead of CLE, and writes 0x00 (all binary zeroes) by pulsing the /WE line low. The chip transfers its internally programmed NAND Flash ID into its internal read register, and the data is read out from the device on each rising edge of the /RE (Read Enable, Active Low) line; for most devices this is 4 to 8 bytes of data.</p>
<h2>NAND Flash ID</h2>
<p>For each NAND Flash device, it has a (mostly) unique ID that identifies the manufacturer, and other functional data that is defined by that manufacturer; in other words, only the manufacturer ID, assigned by the <a href="https://en.wikipedia.org/wiki/JEDEC" target="_blank" rel="noopener">JEDEC Technology Association</a>, is well-defined.</p>
<p>The first byte represents the Flash manufacturer, and the rest (2 to 6 bytes) define the device’s characteristics, as set out by the manufacturer themselves. Most NAND vendors are very tight-lipped when it comes to datasheets, and SanDisk (and by extension, Toshiba/Kioxia) maintain very strict control, save for some slightly outdated leaked Toshiba datasheets. Because the two aforementioned companies share their NAND fabrication facilities, we can reasonably presume the data structures in the vendor-defined bytes can be referenced against each other.</p>
<p>In the case of the SanDisk High Endurance 128GB card, it has a NAND Flash ID of 0x45 48 9A B3 7E 72 0D 0E. Some of these values can be compared against a <a href="http://www.datasheet.hk/view_download.php?id=2027929&amp;file=0515%5Ctc58teg5dcjtai0_7779332.pdf" target="_blank" rel="noopener">Toshiba datasheet</a>:</p>
<table>
<thead>
<tr>
<th>Byte Value (Hex)</th>
<th>Description/Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td>45</td>
<td>
<ul>
<li>Manufacturer: SanDisk</li>
</ul>
</td>
</tr>
<tr>
<td>48</td></tr></tbody></table></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/">https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/</a></em></p>]]>
            </description>
            <link>https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23892463</guid>
            <pubDate>Sun, 19 Jul 2020 21:09:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Clipboard Injection on the Financial Times]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=23891630">thread link</a>) | @ajyoon
<br/>
July 19, 2020 | https://andrewyoon.art/blog/ft-clipboard | <a href="https://web.archive.org/web/*/https://andrewyoon.art/blog/ft-clipboard">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>While doing some research today I was reading <a href="https://www.ft.com/content/b46f5a58-aa2e-11db-83b0-0000779e2340">an old column</a> by James Boyle about then-recent experiments in open access publishing in 2007. Seeing an interesting snippet, I did the ol’ <code>Ctrl-C</code> and pasted it into my notes, where I was surprised to see this prepended to my selection:</p>

<blockquote>
  <p>Please use the sharing tools found via the share button at the top or side of articles. Copying articles to share with others is a breach of FT.com T&amp;Cs and Copyright Policy. Email licensing@ft.com to buy additional rights. Subscribers may share up to 10 or 20 articles per month using the gift article service. More information can be found at https://www.ft.com/tour.</p>
</blockquote>

<p>I’ve seen websites garble copied text or outright disable it, but I’ve never seen a site actually inject unrelated text before.</p>

<p>I was curious how they had implemented this, so I fired up firefox devtools and searched for a snippet of the notice in the site sources.</p>

<figure>
  <img src="https://andrewyoon.art/img/posts/ft-copy/devtools.png" alt="firefox devtools search">
</figure>

<p>Thankfully, the FT provides a sourcemap so I was able to find the de-minified <code>legal-copy/main.js</code> implementation and see what was going on. Essentially, it boils down to this:</p>

<div><div><pre><code><span>function</span> <span>handleCopy</span><span>(</span><span>e</span><span>)</span> <span>{</span>
    <span>var</span> <span>selection</span> <span>=</span> <span>window</span><span>.</span><span>getSelection</span><span>().</span><span>toString</span><span>();</span>
    <span>var</span> <span>wordCount</span> <span>=</span> <span>selection</span><span>.</span><span>split</span><span>(</span><span>/</span><span>\s</span><span>+/</span><span>).</span><span>length</span><span>;</span>
    <span>if</span> <span>(</span><span>wordCount</span> <span>&gt;</span> <span>30</span><span>)</span> <span>{</span>
        <span>var</span> <span>patchedText</span> <span>=</span> <span>"</span><span>hello world</span><span>\n</span><span>"</span> <span>+</span> <span>selection</span><span>;</span>
        <span>e</span><span>.</span><span>clipboardData</span><span>.</span><span>setData</span><span>(</span><span>'</span><span>text/plain</span><span>'</span><span>,</span> <span>patchedText</span><span>);</span>
    <span>}</span>
    <span>e</span><span>.</span><span>preventDefault</span><span>();</span>
<span>}</span>
<span>document</span><span>.</span><span>body</span><span>.</span><span>addEventListener</span><span>(</span><span>'</span><span>copy</span><span>'</span><span>,</span> <span>handleCopy</span><span>);</span>
</code></pre></div></div>

<p>It includes some subtleties around detecting browser support for the rich <a href="https://developer.mozilla.org/en-US/docs/Web/API/ClipboardEvent/clipboardData">ClipboardEvent API</a> and handling legacy Chrome quirks, as well as trying to disable the “feature” when it suspects a screen reader is being used (points to the FT for accessibility consideration, though I have a feeling this was a response to a complaint).</p>

<p>Is copying 30 words legal? The injected text warns “Copying articles to share with others is a breach of FT.com T&amp;Cs and Copyright Policy”—copying <em>articles</em>, not 30 words. I’m no lawyer, but <a href="https://en.wikipedia.org/wiki/Fair_use">fair use</a> protects a great many things one might copy 30 words for, like quotation in another article, or in my case, personal notekeeping. While some infringing uses can be imagined, does this warrant such a violation of internet norms?</p>

<p>The current working draft of the <a href="https://w3c.github.io/clipboard-apis/">W3C Clipboard spec</a> describes mechanical use-cases for manipulating the clipboard like attaching metadata, rich text transfer, and translating mathematical notation to plaintext—not scaring people away from exercising their rights. There’s a set of implicit ground rules for what should and should not be done on websites. Interfering with standard access patterns like closing tabs and using right-click menus violates the social contract of the internet and further threatens its endangered state as a thing to be used, not a thing that uses you.</p>

<p>Incidentally, the article I was reading was by <a href="http://james-boyle.com/">James Boyle</a>, a copyright reformist who was a columnist for the Financial Times largely because they allowed him to <a href="https://law.duke.edu/boylesite/bipolar.html">retain his copyrights</a>, with at least some under <a href="http://creativecommons.org/licenses/by-nc-nd/2.0/">Creative Commons licenses</a>, so it seems likely I actually <em>could</em> copy the whole article if I wanted to.</p>

<p>It turns out lots of other people have noticed this kind of bad behavior and have built circumventing browser extensions like <a href="https://addons.mozilla.org/en-US/firefox/addon/absolute-enable-right-click">this one</a> which I just happily used to copy this passage from <a href="https://www.ft.com/content/b46f5a58-aa2e-11db-83b0-0000779e2340">the article</a>:</p>

<blockquote>
  <p>The internet makes copying cheap. Businesses that see their livelihood as dependent on the restriction of copying – concentrated in the recording, film, publishing and software industries – are understandably upset. Their goal is to have the same ability to control their content as they had in an analog world but to keep all the benefits of pervasiveness, cost saving, and viral marketing that a global digital network brings. To that end, they have moved aggressively to change laws worldwide, to introduce stiffer penalties, expand rights, mandate technological locks, forbid reverse engineering, and increase enforcement. It is not so much a case of wanting to have their cake and eat it, as to have their cake and make your cake illegal.</p>
</blockquote>

  </div>

</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://andrewyoon.art/blog/ft-clipboard</link>
            <guid isPermaLink="false">hacker-news-small-sites-23891630</guid>
            <pubDate>Sun, 19 Jul 2020 19:14:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Eurozone’s grappling with crisis, class war and the North-South divide]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=23891282">thread link</a>) | @jcfrei
<br/>
July 19, 2020 | https://www.yanisvaroufakis.eu/2020/07/16/the-eurozones-grappling-with-crisis-class-war-the-north-south-divide-berliner-verlag-interview/ | <a href="https://web.archive.org/web/*/https://www.yanisvaroufakis.eu/2020/07/16/the-eurozones-grappling-with-crisis-class-war-the-north-south-divide-berliner-verlag-interview/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	<article id="post-22719">

		<section>
			

			<h3><span><strong><img src="https://i0.wp.com/www.yanisvaroufakis.eu/wp-content/uploads/2020/07/Screenshot-2020-07-16-at-9.53.43-AM.png?resize=2424%2C772&amp;ssl=1" alt="" width="2424" height="772" srcset="https://i0.wp.com/www.yanisvaroufakis.eu/wp-content/uploads/2020/07/Screenshot-2020-07-16-at-9.53.43-AM.png?w=2424&amp;ssl=1 2424w, https://i0.wp.com/www.yanisvaroufakis.eu/wp-content/uploads/2020/07/Screenshot-2020-07-16-at-9.53.43-AM.png?resize=1536%2C489&amp;ssl=1 1536w, https://i0.wp.com/www.yanisvaroufakis.eu/wp-content/uploads/2020/07/Screenshot-2020-07-16-at-9.53.43-AM.png?resize=2048%2C652&amp;ssl=1 2048w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1">What did you think when the Euro-group elected its new leader last week? At least this was not the outcome Frau Merkel had wished?</strong></span></h3>
<h3>My first thought was that it spelled the end of any possibility of harmonizing corporate tax rates across the Eurozone, an issue that I am on Mrs Merkel’s side. The second thought was that the Irish finance minister’s victory was a victory for forces pushing for greater, not less, austerity across the European Union. It was not a good moment for Europe. [From the above you can see that I never automatically opposed whatever Mrs Merkel favoured!]</h3>
<h3><span><strong>If you look at fiscal and monetary politics in the Euro area today – what is your impression? Have the Europeans learnt a lesson from the Greek crisis?</strong></span></h3>
<h3>To begin with, please let us not conflate Europe’s rulers with the… Europeans. Turning now to your question, Europe’s leaders resemble the Bourbons: they have forgotten nothing and they have learned nothing. More precisely, while the ECB’s monetary policy reaction has been far improved since Mario Draghi succeeded the catastrophic Jean-Claude Trichet’s, monetary policy has long ago reached the limits of what it can do. So, the only thing that matters now is Europe’s fiscal stance.</h3>
<h3><span>Our fiscal stance remains the most tepid and the weakest amongst developed economies. It must be considered yet another dereliction of duty for two reasons, one ex ante and one ex post.</span></h3>
<h3>The ex ante failure has to do with what we are doing now, in 2020: Taking out all the loans provided to business (that are irrelevant fiscally), the fiscal injections into the EU economy are: (a) small overall and (b) hopelessly lopsided, with Germany on the one hand pumping a great deal more fiscal power into its economy while Italy and Spain cannot – the result being a magnification of Europe’s greatest weakness: our internal imbalances.</h3>
<h3>The ex post failure stems from the demise of the idea of a Eurobond – the only instrument that could prevent, in 2021 or 2022 (when the fiscal compact is re-energised) massive new austerity in countries like Italy and Spain. Since the budget deficit of these countries will exceed 10% of GDP, you can imagine that the fiscal consolidation necessary to return to the fiscal compact is going to produce a further increase in the imbalances that have been tearing the Eurozone apart since 2010.</h3>
<h3><span><strong>You are very critical of the European Recovery Fund. Why?</strong></span></h3>
<h3>For at least three important reasons: First, it was devised as a substitute to the Eurobond, which it is not. Secondly, because its structure is divisive and likely to cause greater disunity among Europeans. Thirdly, because it will be too little and it will come too late to counter-act the new austerity that the re-introduction of the fiscal compact will bring. Let me explain these three objections in greater detail.</h3>
<h3>While it is true that the Recovery Fund involves a degree of common debt, it is explicitly (and legally) designed as a one-off debt with specific details on how and when it will be repaid. Thus, we have wasted a fantastic opportunity to create the equivalent of US Treasury Bills, which is what makes the dollar powerful and the United States far better placed to absorb shocks.</h3>
<h3>Moreover, I am appalled that the Commission specified in advance, based on backward looking data, which country will receive how many billions. It was an awful thing to do because it set one country, one people, against another. What we needed was a sum that would be diverted to regions in Europe most in need of support. There are poor parts of Germany that will be hit more than richer parts of Spain. The total sum available should be distributed ex post on the basis of the needs of particular European regions and sectors, not by means of the usual sordid bargain in Brussels that divides a pie between governments.</h3>
<h3>Lastly, this Recovery Fund will do very little to ameliorate the austerity that the re-introduction of the fiscal compact will necessitate. It will fail in the same way that the many billions of the structural funds failed to ameliorate the effects of austerity between 2010 and 2016.</h3>
<h3><span><strong>What consequences will the Fund have in Europe?</strong></span></h3>
<h3>It will entrench the false view in Northern Europe that it was all about hand-outs to the Southerners while, at once, entrenching the false view in Europe’s South that the whole of the North benefits from pushing the South into greater impecunity.</h3>
<h3><span><strong>What will happen to the working class people in the Euro-Zone?</strong></span></h3>
<h3>It will continue to watch its prospects decline both in the North and in the South while the financiers and the members of the boards of directors of large companies enjoy the fruits of socialism for the oligarchy and stinging austerity for the majority of the Greeks and of the Germans.</h3>
<h3><span><strong>Will the consequences be different for Greece and Germany, aka South and North?</strong></span></h3>
<h3>Yes and no. Workers in the North and in the South will be facing increasing precarity and will be getting angrier and more discontented. Except that the rate of deterioration will be much greater in countries like Greece, the result being the continuation of the mass exodus of young people which makes our countries of the South impossible to sustain as functioning societies.</h3>
<h3><span><strong>What role are the “frugal four” playing – isn’t it legitimate to keep in mind the public debt in each country?</strong></span></h3>
<h3>Governments have a duty to defend their people from a number of threats. One of them is excessive public debt. But, there is also excessive private debt. And, even more ominously, low investment that jeopardises a nation’s chances to repay both private and public debt. The so-called frugal four are working in favour of large conglomerates that want their cake and to eat it: They like the effect Italy and Greece are having at keeping the exchange value of the euro low (so as to maximise their exports, eg, to China) and the interest rates below zero (so as to get free money from the ECB). Like all free-riders, they take what suits them but then take no responsibility for the damage that the policies favouring them do both to the South and to their own working classes (which find themselves increasingly squeezed and indebted as part of the same process that squeezes and indebts the South).</h3>
<h3><span><strong>You frequently speak of “oligarchs” – who are they in Europe?</strong></span></h3>
<h3>In the early fifties, when the EEC was first created, they were the captains of the steel and coal industries. Soon after, they were joined by the electrical goods industries plus the automakers. In the 1980s the captains of industry were increasingly sidelined by the bankers who, by 2008, had ended up as the masters of Europe’s destiny.</h3>
<h3><span><strong>Can the European oligarchs at least provide new jobs and/or be competitive with the US or China?</strong></span></h3>
<h3>Absolutely not. The capacity of European financialised capitalism to produce good quality jobs is at its nadir and is getting worse and worse due to the EU’s failure to create the public finance instruments that DiEM25 has been proposing for years now – e.g. a joint European Investment Bank and European Central Bank program by which EIB-bonds can be backed by ECB-bonds to mobilise idle cash (that is currently destroying German pension funds) and turn it into green investments.</h3>
<h3><span><strong>Most observers have no doubt that there will be austerity in exchange of money from the fund. What austerity else can Greece bear, keeping in mind the sacrifices that have already been made?</strong></span></h3>
<h3>This is like asking a patient nearing death as a result of having been subjected to massive radiation how much more radiation she can take!</h3>
<h3><span><strong>What is the current state of the Greek economy? Debt is still at 180 percent, despite 200 billion Euro for bailing out the banks?</strong></span></h3>
<h3>Greece’s debt is unpayable not despite but <u>because</u> of the banks’ bailout. Since 2009 I have been saying that Europe’s rulers, and this is where I do blame Mrs Merkel, cynically saved idiotic bankers by transferring their losses onto the shoulders of Europe’s weakest taxpayers. Mrs Merkel, Mr Sarkozy, the troika and everyone else who has maintained this crime-against-logic for a decade will forever by guilty in the conscience of Europe’s progressives. [And, by the way, Greece’s debt is not well about 200% of GDP!]</h3>
<h3><span><strong>What will Europe look like if the Recovery Fund really delivers on 750 billion Euro?</strong></span></h3>
<h3>Sad and divided. For a start, of these 750 billions, 250 billions will be loans. The last thing Europe’s devasted businesses that are facing insolvency is more loans – Greece’s fate provides a great lesson to those who refuse to recognise this simple truth. As for the remaining 500 billions, more than 100 of them are re-purposed funds (i.e. not new money) and another 200 will be sacrificed to buy the frugal four’s consent. That leaves us with, at best, 300 billion. It sounds a lot but it is puny. Over three years it comes to less than 1% of GDP. Given that the austerity that Berlin will demand and Brussels will impose will be more than 4% of GDP over the same year, the only conclusion is that, as the pandemic (hopefully) subsides, European capitalism will be inflicting another unnecessary blow to itself – just as in 2010.</h3>
<h3><span><strong>In your view – what would be an adequate program to cope with the Corona-devastation?</strong></span></h3>
<h3>DiEM25 has already answered this – our <a href="https://diem25.org/diem25-presents-covid-19-economic-response-plan-and-green-recovery-investment-program/">3 point plan</a>.</h3>
<ol>
<li>
<h3>The ECB must issue a long-maturity, 30-year, Eurobond for €1 trillion euros solely backed by the ECB,&nbsp;with the possibility of further issuance in needs be. This €1 trillion should then be used to replace national debt, in proportion to the coronavirus-induced regional recessions &amp; public health costs.</h3>
</li>
<li>
<h3>The ECB must inject a €2000 European Solidarity Cash Payment to every European resident</h3>
</li>
<li>
<h3>The EU must create a European Green Recovery &amp; Investment Program funded by <em>EIB bonds&nbsp;approximately equal to 5% of the EU’s GDP, to be backed in the bond markets by the ECB</em>.</h3>
</li>
</ol>
<h3><span><strong>You are talking about “class war”. Are we already there?</strong></span></h3>
<h3>We have been there since 2010. The austerity imposed upon Greece in May 2010 was the first move toward a fiscal …</h3></section></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.yanisvaroufakis.eu/2020/07/16/the-eurozones-grappling-with-crisis-class-war-the-north-south-divide-berliner-verlag-interview/">https://www.yanisvaroufakis.eu/2020/07/16/the-eurozones-grappling-with-crisis-class-war-the-north-south-divide-berliner-verlag-interview/</a></em></p>]]>
            </description>
            <link>https://www.yanisvaroufakis.eu/2020/07/16/the-eurozones-grappling-with-crisis-class-war-the-north-south-divide-berliner-verlag-interview/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23891282</guid>
            <pubDate>Sun, 19 Jul 2020 18:24:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My failed startup: Why it failed and why I’m releasing all the source code]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 2 (<a href="https://news.ycombinator.com/item?id=23891111">thread link</a>) | @ve55
<br/>
July 19, 2020 | https://adamfallon.com/2020/07/18/my-failed-start-up/ | <a href="https://web.archive.org/web/*/https://adamfallon.com/2020/07/18/my-failed-start-up/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<div>

			
<p><em>Why it failed and why i’m releasing all the source code for free.</em></p>



<div><p>This is a long post.</p><p>If you are just looking for the code click <a href="#the-code">here</a></p></div>



<p>When lockdown started in the U.K I started looking for ways to spend my extended time at home. I wasn’t working because I had quit my job on Feb 16th to travel the world. Timing has never been my strong suit. </p>



<p>So instead of sitting around I decided to start a contracting company, got awarded a few small contracts and I released a <a aria-label="undefined (opens in a new tab)" href="https://apps.apple.com/gb/app/nitronotes/id1502080216" target="_blank" rel="noreferrer noopener nofollow">few</a> <a aria-label="undefined (opens in a new tab)" href="https://apps.apple.com/dz/app/countdownplusplus/id1497707393" target="_blank" rel="noreferrer noopener nofollow">apps</a> written in SwiftUI. But I wanted a bigger project to sink my teeth into. </p>



<p>There were two ideas I kept coming back too;</p>



<ol><li>A hosted org-mode in the browser. </li><li>Some sort of local social network.</li></ol>



<p>Given that everyone was forced into their home I thought it would be the perfect time for something like the app <a href="https://en.wikipedia.org/wiki/Yik_Yak" target="_blank" aria-label="undefined (opens in a new tab)" rel="noreferrer noopener nofollow">YikYak</a> to make a comeback. </p>



<h3>Ghosts of apps past.</h3>



<p>For those who never got a chance to use YikYak let me try to explain it quickly. The app touted itself as the first hyperlocal social network. A users post could only be viewed it if you were no further than 5km <a href="https://adamfallon.com/2020/07/10/recreating-yikyak-with-postgres/" target="_blank" aria-label="undefined (opens in a new tab)" rel="noreferrer noopener">from the initial post location</a>. You could vote posts up or down and comment underneath posts. It was simple but unique.</p>



<p>YikYak eventually implemented some <a href="https://www.reddit.com/r/yikyak/comments/8sgynk/what_exactly_caused_the_fall_of_yik_yak/" target="_blank" aria-label="undefined (opens in a new tab)" rel="noreferrer noopener">very unpopular changes</a>. Bullying, Racism and Doxxing was always a problem on YikYak. They didn’t do a huge deal to moderate the content on the app, instead leaving the community to self-police. Eventually they had to do <strong><em>something</em></strong> so they enforced email or telephone signups. </p>



<p><em>Cue mass exodus of users</em>.</p>



<h3>Introducing Ottr 🦦</h3>



<p>So I began work on Ottr in early February.</p>



<p>Ottr was going to be YikYak for the people in lockdown. In the same way TikTok came from the ashes of <a href="https://en.wikipedia.org/wiki/Vine_(service)" target="_blank" aria-label="undefined (opens in a new tab)" rel="noreferrer noopener nofollow">Vine</a> – Ottr was to be the refinement of the ideas behind YikYak.</p>







<p>A user could create a new post and comment underneath other posts. They could upvote or downvote posts. They could click the Explore button and whisk off to a new location. You could share posts to Reddit or Twitter. I got all of these features working in a web app created in React, and an iOS app. It got funding from the Amazon Activate program.</p>



<div><figure><img data-attachment-id="493" data-permalink="https://adamfallon.com/2020/07/18/my-failed-start-up/ottr-2/" data-orig-file="https://i1.wp.com/adamfallon.com/wp-content/uploads/2020/07/ottr.png?fit=572%2C481&amp;ssl=1" data-orig-size="572,481" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ottr" data-image-description="" data-medium-file="https://i1.wp.com/adamfallon.com/wp-content/uploads/2020/07/ottr.png?fit=300%2C252&amp;ssl=1" data-large-file="https://i1.wp.com/adamfallon.com/wp-content/uploads/2020/07/ottr.png?fit=572%2C481&amp;ssl=1" src="https://i1.wp.com/adamfallon.com/wp-content/uploads/2020/07/ottr.png?w=580&amp;ssl=1" alt="Components of ottr - my failed startup" srcset="https://i1.wp.com/adamfallon.com/wp-content/uploads/2020/07/ottr.png?w=572&amp;ssl=1 572w, https://i1.wp.com/adamfallon.com/wp-content/uploads/2020/07/ottr.png?resize=300%2C252&amp;ssl=1 300w" sizes="(max-width: 572px) 100vw, 572px" data-recalc-dims="1" data-lazy-srcset="https://i1.wp.com/adamfallon.com/wp-content/uploads/2020/07/ottr.png?w=572&amp;ssl=1 572w, https://i1.wp.com/adamfallon.com/wp-content/uploads/2020/07/ottr.png?resize=300%2C252&amp;ssl=1 300w" data-lazy-src="https://i1.wp.com/adamfallon.com/wp-content/uploads/2020/07/ottr.png?w=580&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>The components of Ottr</figcaption></figure></div>



<p>But as of today Ottr is dead. I’m going to talk about why I no longer believe in the idea, the key failures that lead to that decision and why I am releasing absolutely everything I made.</p>



<h3>On anonymity.</h3>



<p>For all its flaws – I absolutely loved YikYak. It came out the first year of my bachelors degree. It was by far the most used app on campus. I would be sitting in the university library and someone would post about the weird smell coming from the desks on the 3rd floor. Or the little nod the doorman would give you on your way in. </p>



<p>People would post about the crazy antics of their friend on a drunken night out. About the cute doggo’s in the parks. Or how awful the food from the students union was. Maybe they would post about their crush.</p>



<p>Those posting on YikYak were having the same experience you were. They were seeing the same things as you, and that made you feel engaged. It was anonymous, which ended up being a double edge sword. It allowed for great humour and creativity, but sometimes the jokes went too far.</p>



<p>Anonymity would be YikYak’s greatest strength and its most fundamental weakness. </p>



<h3>And yet…</h3>



<p>People got hurt using YikYak. Anonymity breeds toxicity – there are clear precedents for this on the internet – 4chan, 8chan and Reddit. </p>



<p>I knew that there was potential for this to happen again – but I thought I could solve it. My thought process was; </p>



<ul><li>I will add a report button and over time I can just add moderators to keep on top of the reports.</li><li>Posts will be removed when they get downvoted too much.</li><li>I will redact PPI and add profanity filters.</li></ul>



<p>This arrogance was mistake number 1. When I eventually let a few people use the site they broke the profanity filters with ease. Even among ~100 users people wrote spam and cluttered up the front page. Going on the site in the first week made me wince at how ugly it looked, with people filling the character limit with posts consisting entirely of emoji. It made the site look amateur.</p>



<p>Moderation on the internet is an unsolved problem. Handing the moderation over to automated systems will mean bias can creep in, and it’s hard (impossible?) to tune between zealotry and permissiveness. With an automated system you are going to upset some people. </p>



<h3>Humans &gt; Robots?</h3>



<p>Humans are better at moderation, but they cost much more money and come with their own problems. I think the shining example of human moderation is Hacker News. Daniel Gackle and Scott Bell tackle the problems inherit of internet forums with a competence not seen elsewhere.</p>



<p>The New Yorker wrote <a aria-label="undefined (opens in a new tab)" href="https://www.newyorker.com/news/letter-from-silicon-valley/the-lonely-work-of-moderating-hacker-news" target="_blank" rel="noreferrer noopener nofollow">this</a> great article about how difficult that is to achieve – and that’s with Hacker News having a <em>distinct voice</em>. Hacker News is a place for people to post about things that are intellectually interesting to hackers. </p>



<p>Ottr didn’t have a clear voice – it was a place for people to post about things going on in their community. That could mean anything.</p>



<p>Something that is acceptable in one location, might not work for people elsewhere. You could have moderators from each community – but then you are giving that moderator a lot of power to make others unhappy. </p>



<p>I think Reddit is a horrible place for new posters – and part of the blame falls on moderators having too much power. Some moderators on Reddit have so much power they can control the front page, and with that, the tone of the site. I <em>really</em> didn’t want that.</p>



<h3>It appears we’re at an impasse.</h3>



<p>So if humans and robots couldn’t be trusted, what could? Ottr made the same compromise YikYak did – of letting the community police itself – and it failed (I think) because too few people used Ottr. </p>



<p>A self policing community has great potential to trend towards its worst elements. If someone posted something false about me on the internet, I would have great incentive to downvote it – but why would anyone else – especially when there was the potential for scandal, entertainment and outrage.</p>



<p>I care about how the site looks to first time users, but why would anyone else?</p>



<p>And so those stupid emoji posts stayed on the front page.</p>



<h3>Is anybody alive out there?</h3>



<p>So poor moderation options left Ottr open to the same kinds of toxicity that YikYak failed to combat. The second key issue was the fact that <em>I don’t know how to get people to use Ottr in the first place</em>.</p>



<p>I’ve never read a book on growth hacking or how to make your product sticky. I kind of understand when its happening to me (“Hand over your contacts to get some shiny coins?”, “How about you share this post on Facebook and Twitter?”, “Our site is restricting registrations at this time – Sign up with your email to be notified when we have an opening?”), but it always rubbed me the wrong way. </p>



<p>I wanted a few people to start using Ottr and then they would be so blown away by how good it was that they would tell their friends and share it themselves. </p>



<h3>Build it and they will come?</h3>



<p>I focused on UX simplicity and features and the idea of getting people to use it was always running on a low priority background thread. Instead of getting the first iteration up and focusing on what people said about that, I had a clear vision of what it was to be like, what I would add from YikYak, what I would change, and then I would hand it to the people and they would say “Yes Adam – very good, thank you!”.</p>



<p>Treating user growth as a secondary concern is the dumbest thing you can do for a social network. It is <em>all that matters.</em> Mistake #2 was thinking I was above this somehow. I treated growth-hacking and user-growth as something for other people to be concerned with. That is a juvenile approach to business. Because I can code doesn’t mean that I am somehow too pure to engage in the business of marketing or advertising the things I code. </p>



<p>I am working on fixing that this year and i’m trying to understand, rather than scoff, at the ideas behind growing and marketing the things I make. So far it seems much more difficult than learning to code. I didn’t understand or appreciate this until I reflected on the time I spent on Ottr.</p>



<h3>Burn, Baby Burn.</h3>



<p>By copying the mistakes of YikYak and ignoring the fundamentals behind making a social network work it meant that Ottr never got off the ground. You might argue that I didn’t try for very long – and you are right, I didn’t, around 4 months in total. Mistake #3 is the real reason I wanted to be done with the whole project. I was completely burnt out.</p>



<p>As I mentioned before, I had just quit my job in February to travel. For reasons I won’t go in to, consuming myself with Ottr directly after leaving my last job was a really bad idea. </p>



<p>As I couldn’t travel because of the pandemic and because I had already planned for my mini-retirement I had enough money that meant I didn’t need to jump into another job. The contract work I was getting was not very engaging – so I suddenly had a lot of free time. </p>



<p>So I threw myself into Ottr. I remember looking at ScreenTime stats at the time and it showing emacs being open for 80+ hrs a week. I stopped reading. Stopped watching movies. From waking up to going to sleep i’d just be working on it. Ottr went through a few iterations and I threw huge chunks of work away at points. </p>



<h3>Then the wheels came off.</h3>



<p>Then one fine day it all stopped being worth it. I was riding along on a wave of getting the next feature done. Talking to a partner at Amazon helping with deployment. Tinkering with CloudFlare because I thought now it’s going into production, I will obviously be inundated. But when it came time where it was actually “ready”, I became immediately repulsed of the idea of trying to grow Ottr. </p>



<p>Doubt crept in. “This is just a copy of YikYak”, and it was, but YikYak doesn’t exist anymore. There is nothing new under the sun.</p>



<p>No matter how hard I tried I couldn’t …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://adamfallon.com/2020/07/18/my-failed-start-up/">https://adamfallon.com/2020/07/18/my-failed-start-up/</a></em></p>]]>
            </description>
            <link>https://adamfallon.com/2020/07/18/my-failed-start-up/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23891111</guid>
            <pubDate>Sun, 19 Jul 2020 18:03:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Accelerating iOS on QEMU with hardware virtualization (KVM)]]>
            </title>
            <description>
<![CDATA[
Score 17 | Comments 3 (<a href="https://news.ycombinator.com/item?id=23891034">thread link</a>) | @mdelias
<br/>
July 19, 2020 | https://alephsecurity.com/2020/07/19/xnu-qemu-kvm/ | <a href="https://web.archive.org/web/*/https://alephsecurity.com/2020/07/19/xnu-qemu-kvm/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div ?="">

<p>While QEMU started out as a platform for emulating hardware (especially
architectures not supported by the host CPU), later versions gained the ability
to execute code using hardware-assisted virtualization. This can yield
substantial performance benefits, since most executed opcodes are performed
directly by the CPU, instead of being translated into a number of native
opcodes that simulate the behavior of the original one.</p>
<p>Using virtualization requires a host CPU that supports the executing
architecture. On an Intel-based machine (such as those we used for developing
our iOS on QEMU project), virtualizing an architecture other than x86/x64
would be impossible. Therefore, when running iOS (an arm64 OS) on QEMU, we’d be
using regular emulation. At first, the performance was more than adequate. But
as our efforts to execute iOS on QEMU developed, and more parts of the
operating system were brought up successfully, we started noticing degraded
performance.</p>
<p>Modern ARM chips support hardware-assisted virtualization, similar to their x86
bretheren. If we were to run our version of QEMU on an ARM-based system, it
should be possible to harness the virtualization capabilites of the underlying
CPU in order to achieve near-native performance. This post documents the
challenges we had to overcome in order to successfully boot our iOS system on
QEMU using hardware-assisted virtualization.</p>

<p>When moving away from an Intel-based laptop to an ARM-based system for
development, the first question was, which platform to choose. Should we use an
ARM server in the cloud? An ARM-based development board? A developer-friendly
Android phone? Each of the options seemed to have its advantages.</p>
<p>An ARM server would easily be the most powerful of the bunch - but the
selection of ARM-based dedicated servers is limited, they aren’t cheap, and we
weren’t sure we’d get the level of access we’d need (our assumption was that
recompiling the kernel would be required). Buying a physical server is an
option we entertained for a short while, but with prices in the thousands of
dollars, the idea was quickly discarded.</p>
<p>A used Android phone was another good candidate, but even developer-friendly
phones might be difficult to work with. Using Android instead of a
general-purpose Linux could be a limiting factor - and finding a phone with
enough RAM to execute iOS in parallel with a full-blown Android environment
would require us to go with a recent phone, which would not be cheap.</p>
<p>An ARM-based development board looked like a good choice, that would let us
test and develop our code at a fraction of the price of the alternatives.
Looking for good aftermarket support for kernel development, 64-bit ARM SOCs,
and no less than 4GB of RAM, we chose Pine Rock64 as our testbed:</p>
<p><img src="https://alephsecurity.com/assets/img/qemu-kvm/pine64_rock64_angle_53e8db18-d784-4a66-879b-e413d4e50c3c_550x350.webp" alt="Pine Rock64"></p>
<p>The board comes with an arm64 CPU (Cortex-A53-based RK3328), as well as 4GB of
RAM. The choice seemed perfect for our purpose: when emulating an iPhone, we
successfully used Cortex-A53 as our CPU, and while 4GB of RAM wasn’t ideal
(we’d usually run our emulation with 6GB of RAM dedicated to QEMU), our tests
indicated that using less RAM (for example, 2GB) had no significant impact at
this point.</p>

<h2 id="enabling-kvm-in-qemu">Enabling KVM in QEMU</h2>
<p>Once our board was up and running the latest version of Armbian, it was time
for our first attempt to run QEMU while using hardware-assisted virtualization
of the CPU in place of emulation. In theory, one simply has to add the
<code>-enable-kvm</code> switch to the command line…</p>
<p>Unfortunately, it wasn’t that easy. While QEMU launched successfully, iOS
wouldn’t boot. Attaching gdb on boot let us see the instructions we executing
correctly at first, but upon continuing, we’d quickly find ourselves in an
infinite loop located at <code>0xfffffff0070a0200</code>. Based on the kernel symbols, it
was one of the vectors for interrupt/exception handling. Reaching that code
meant an exception occured early on, and the operating system still hasn’t
reached a point where it could be handled more gracefully (such as dumping
registers and memory content, combined with at least some sort of the issue
description). We had no choice but to step through the early initialization
of the kernel, one instruction after the other, until the jump to the exception
vector occured.</p>
<h2 id="enabling-the-mmu">Enabling the MMU</h2>
<p>At this point, it’s important to note, that when the kernel is loaded for the
first time, the MMU isn’t enabled yet, and the code is mapped to physical
addresses (the execution begins at <code>0x470a5098</code>). It is only later during the
initialization that the MMU is enabled (once the page tables are initialized),
and the addresses switch to the familiar kernel mode (with higher bits set to
1). When looking at the kernel image in a disassembler such as Ghidra, however,
all code is mapped to kernel addresses. Therefore, the initialization code we
were inspecting could be found at <code>0xfffffff0070a5098</code>.</p>
<p>Some of the initialization code contains loops that execute many times. In
order to make following the execution more effective, we used breakpoints that
we set at small intervals. This let us continue the execution, instead of
stepping through each instruction and loop. Using this technique, we quickly
found out the exception occured at the address <code>0x470a72e4</code>:</p>
<pre><code>0x470a72d4     msr      vbar_el1, x0
0x470a72d8     movz     x0, 0x593d
0x470a72dc     movz     x1, 0x3454, lsl 16
0x470a72e0     orr      x0, x0, x1
0x470a72e4 ==&gt; msr      sctlr_el1, x0
0x470a72e8     isb
0x470a72ec     movz     x0, 0
0x470a72f0     msr      tpidr_el1, x0
</code></pre>
<p>As we can see, at the mentioned address, the value <code>0x3454593d</code> is written into
the <code>SCTLR_EL1</code> register. As per ARM DDI 0487, D13.2.113, this is the system
control register, that provides a top level control of the system at EL0 and
EL1. Its first bit is used to enable address translation via the MMU. Since the
exception happens when executing this instruction (and bit 0 is set in the new
value for the register), the address translation configuration was an immediate
suspect.</p>
<p>There are several registers used to configure the MMU - namely, <code>TCR_EL1</code>,
<code>TTBR1_EL1</code>, and <code>MAIR_EL1</code>. We inspected the values stored in those registers
prior to enabling the MMU. One field that stood out was <code>TCR_EL1.TG1</code>, that
indicates the granule size for the <code>TTBR1_EL1</code>. The initialization code of our
iOS kernel sets the value of <code>TCR_EL1.TG1</code> to <code>0b01</code> (the field is stored in
bits 31:30, and the value written to <code>TCR_EL1</code> at <code>0x470a7244</code> is
<code>0x000000226519a519</code>). The value of <code>0b01</code> corresponds to a granule size of
16KB.</p>
<p>Notably, after stepping through the <code>msr</code> instruction at <code>0x470a7244</code>,
inspecting the value of <code>TCR_EL1</code> revealed a slightly different value of
<code>0x00000022a519a519</code> - the granule size was read out as <code>0b10</code> (4KB)! This made
the reason for an exception upon enabling the MMU clear: while the page tables
set up by the initalization code were designed with a 16KB granule size in
mind, the actual granule size stored in <code>TCR_EL1</code> when enabling the MMU was set
to 4KB. The MMU treated the page entries incorrectly, and a page fault occured
immediately upon enabling the MMU. But why would the value we attempted to set,
that of a 16KB granule, wouldn’t stick?</p>
<p>ARM DDI has the following to say about setting the value of <code>TCR_EL1.TG</code>:</p>
<div><div><pre><code>If the value is programmed to either a reserved value, or a size that has not been implemented, then the hardware will treat the field as if it has been programmed to an IMPLEMENTATION DEFINED choice of the sizes that has been implemented for all purposes other than the value read back from this register.

It is IMPLEMENTATION DEFINED whether the value read back is the value programmed or the value that corresponds to the size chosen.
</code></pre></div></div>
<p>In our case, <code>TCR_EL1.TG</code> is read back as <code>0b10</code> after attempting to set it to
<code>0b01</code>. This indicates that in our case, the read back value is one that
corresponds to the granule size chosen (since it’s not the value we
programmed), and that the 16KB size has not been implemented in our CPU. We can
verify this assumption with the help of the <code>ID_AA64MMFR0_EL1</code> register
(AArch64 Memory Model Feature Register 0). Its field TGran16 (at bits 23:20)
is used to indicate support for 16KB memory translation granule size: when the
bits are all set to 0, the granule size is <em>not</em> supported. Reading the value
of the register on our Rock64 developer board returns the value of
<code>0x00001122</code> - thus, bits 23:20 are set to zero, and our CPU doesn’t implement
the 16KB granule size. In fact, by referring to section 4.2.1 in ARM DDI 0500
(ARM Cortex-A53 MPCore Processor Technical Reference Manual), we can see that
the value of <code>0x00001122</code> is there by design - i.e., Cortex-A53 cores do not
implement the 16KB granule size. Interestingly, the Cortex-A53 implementation
in QEMU ignores this, and implements the 16KB granule size (the value of
<code>ID_AA64MMFR0_EL1</code> is <code>0x00001122</code>, matching the reference manual, but setting
<code>TCR_EL1.TG</code> to <code>0b01</code> works as intended).</p>
<p>We had several options for the continuation of the project. We briefly
considered patching the iOS kernel to use 4KB or 64KB pages, both supported by
Cortex-A53. This idea was quickly dropped, as it requires a lot of effort,
while its probability of success is questionable. While building the initial
page table with different page sizes should be doable, we’d have to find all
the places in the kernel code that manipulate the page table, and update them
accordingly - not an easy feat to accomplish.</p>
<p>We, therefore, had to switch to an ARM core that supports 16KB pages. This
brought us back to the choice of a hardware (ARM server, development board, or
a phone), with one more requirement to fullfil: we’d look up the technical
reference manual of the core powering the chosen hardware, and verify the
granule size used by the iOS kernel is supported.</p>
<p>Unfortunately, most budget development boards use older ARM cores (either
Cortex-A53 or Cortex-A72), with no support for the required granule size. We
found one board with a core that had the required support - but it only had
1GB of RAM. Most Android phones …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://alephsecurity.com/2020/07/19/xnu-qemu-kvm/">https://alephsecurity.com/2020/07/19/xnu-qemu-kvm/</a></em></p>]]>
            </description>
            <link>https://alephsecurity.com/2020/07/19/xnu-qemu-kvm/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23891034</guid>
            <pubDate>Sun, 19 Jul 2020 17:53:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Array Functions and the Rule of Least Power]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 1 (<a href="https://news.ycombinator.com/item?id=23889893">thread link</a>) | @kiyanwang
<br/>
July 19, 2020 | https://jesseduffield.com/array-functions-and-the-rule-of-least-power/ | <a href="https://web.archive.org/web/*/https://jesseduffield.com/array-functions-and-the-rule-of-least-power/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-86">
	<!-- .entry-header -->

	
	
	<div>
		<p><img src="https://i.imgur.com/e8JqZPe.png" alt=""></p>
<h2>The Rule of Least Power</h2>
<p>In <a href="https://www.w3.org/DesignIssues/Principles.html">1998</a>, Tim Berners-Lee, inventor of the World Wide Web, coined <em>the Principle of Least Power</em>:</p>
<blockquote>
<p>Computer Science in the 1960s to 80s spent a lot of effort making languages which were as powerful as possible. Nowadays we have to appreciate the reasons for picking not the most powerful solution but the least powerful.</p>
</blockquote>
<p>In <a href="https://www.w3.org/2001/tag/doc/leastPower.html">2006</a>, The W3C codified the principle as the <em>Rule of Least Power</em>:</p>
<blockquote>
<p>There is an important tradeoff between the computational power of a language and the ability to determine what a program in that language is doing</p>
<p>Expressing constraints, relationships and processing instructions in less powerful languages increases the flexibility with which information can be reused: the less powerful the language, the more you can do with the data stored in that language.</p>
</blockquote>
<p>In fact, Berners-Lee chose not to make HTML a bona-fide language on the basis of this rule:</p>
<blockquote>
<p>I chose HTML not to be a programming language because I wanted different programs to do different things with it: present it differently, extract tables of contents, index it, and so on.</p>
</blockquote>
<p>Though the Rule of Least Power targeted programming languages themselves, rather than language features, I think the same ideas still apply. The less powerful your code is, the easier it is to reason about.</p>
<h2>Array Functions</h2>
<p>It's therefore interesting that <a href="https://flaviocopes.com/javascript-loops-map-filter-reduce-find/">some</a> people say say that the 'functional' array functions like <code>.filter</code>, <code>.map</code>, and <code>.reduce</code> are <em>powerful</em> compared to their crude for-loop alternatives. I would say the opposite: they are far less powerful, and that's the <em>point</em>.</p>
<p>No doubt, the people calling these functions 'powerful' are probably referring to their power in aggregate (for example being able to call <code>array.map(...).filter(...)</code>), or the power enabled through parallel processing, or the power afforded by assigning callbacks to first-class function variables.</p>
<p>But I want to bring your attention to how the power of these functions when considered individually is in fact low, by design.</p>
<p>Here is a diagram I whipped up that crudely orders the common javascript array functions, going from the most powerful (a for-loop) to the least powerful (<code>.some</code>/<code>.every</code>).</p>
<p><img src="https://i.imgur.com/EvOGL7B.png" alt=""></p>
<h3>Array Function Recap</h3>
<p>Before explaining what I mean by powerful, here's a quick recap of what the different approaches are actually for:</p>
<ul>
<li>for-loop: iterate through a chunk of code, typically for the sake of creating side effects inside the loop (like appending to an array)</li>
<li><code>.forEach</code>: iterate through each element in an array, and in each iteration do something with that element. Again, typically for the sake of creating a side effect at some point.</li>
</ul>
<p>From here down, side effects are strongly discouraged</p>
<ul>
<li><code>.reduce</code>: from left-to-right, iterate through an array to accumulate some value, potentially initialized explicitly at the start, where in each iteration we take the current array item and return the new value of the accumulator (until we return the final value at the end)</li>
<li><code>.map</code>: for each original item in the array, return a new item as a function of the original item to be placed in the corresponding index of the output array</li>
<li><code>.filter</code>: from left-to-right, for each item in the array, include it in the output array if it satisfies some condition</li>
<li><code>.find</code>: from left-to-right, return the first item in the array satisfying some condition</li>
<li><code>.every</code>: return true if every item in the array satisfies some condition, otherwise return false.</li>
<li><code>.some</code>: return true if any item in the array satisfies some condition, otherwise return false.</li>
</ul>
<p><em>This post is less about explaining what each one does and more about picking which one to use. For a good reference see <a href="https://javascript.info/array-methods">here</a></em></p>
<h3>What Do You Mean By Powerful?</h3>
<p>I'm piggybacking off Tim Berners-Lee's coinage here, but when I say 'powerful', I really mean 'flexible'. As in, <em>how many use cases can this function satisfy?</em> Specifically, I'm defining function A as being more powerful than function B if it can implement function B in its own terms, and do other stuff too that function B can't.*</p>
<p>Which means by my definition (which I'm not claiming to be universal) a for-loop is more powerful than <code>.forEach</code> because you can implement <code>.forEach</code> via a for-loop. For example:</p>
<pre><code>const forEach = (array, callback) =&gt; {
  for (i = 0; i &lt; array.length; i++) {
    callback(array[i])
  }
}

forEach([1,2,3], a =&gt; console.log(a))
&gt; 1
&gt; 2
&gt; 3

[1,2,3].forEach(a =&gt; console.log(a))
&gt; 1
&gt; 2
&gt; 3</code></pre>
<p>So no matter what <code>.forEach</code> can do, a for-loop can do more.</p>
<p>Likewise, <code>.reduce</code> can be implemented with <code>.forEach</code> like so:</p>
<pre><code>const reduce = (array, callback, initialValue) =&gt; {
  let result = initialValue

  array.forEach((item) =&gt; {
    result = callback(result, item)
  })

  return result
}

reduce([1,2,3], (acc, curr) =&gt; acc + curr, 0)
&gt; 6

[1,2,3].reduce((acc, curr) =&gt; acc + curr, 0)
&gt; 6</code></pre>
<p>And so on, and so on, all the way to the bottom:</p>
<pre><code>const some = (array, callback) =&gt; array.find(callback) !== undefined</code></pre>
<p>Notably, our custom <code>some</code> doesn't handle undefined values as well as the ECMASCript one, but you get the idea.</p>
<h3>Choose the Least Powerful Function</h3>
<p>Why not just use a for-loop for everything? That way we only need to remember one approach to iterating through an array's items. The reason is the same reason you don't use a grenade to kill a mosquito: grenades are illegal and black market goods are marked up to subsidise the risk taken on by the vendor.</p>
<p>For real though: there are various reasons to pick the least powerful tool, but to me the two most important reasons are:<br>
1) reducing the chance of errors<br>
2) easy comprehension by others</p>
<h4>Reducing the chance of errors</h4>
<p>The least powerful tool for the job that can <em>still do the job</em> is the one that leaves the least chance for errors. Consider the situation where I have an array of numbers and I want to return the result of doubling each item in the array:</p>
<pre><code>const myArray = [1,2,3]

// with `.map`
resultWithMap = myArray.map(item =&gt; item * 2)
&gt; [2,4,6]

// with a for-loop
let resultWithLoop = []
for (i = 0; i &lt; myArray.length-1; i++) {
  resultWithLoop.push(array[i] * 2)
}
resultWithLoop
&gt; [2,4]</code></pre>
<p>Hey, what the hell? Why is my <code>resultWithLoop</code> missing an item? I started my index at zero, I only incremented one at a time, and I'm ensuring I don't have an out of bounds error by ensuring I don't include the element at index <code>myArray.length</code>.</p>
<p>Oh wait, that <code>&lt;</code> in my for-loop should be a <code>&lt;=</code> (or I could remove the <code>-1</code> from <code>myArray.length-1</code>). My mistake.</p>
<p>The for-loop is too powerful to care about what you're actually using it for. Maybe you actually did want to exclude the final element, how could it know?  Luckily we caught this one early, but whether you're missing an <code>=</code> or missing a grenade pin, sometimes by the time you realise your mistake, it's already too late.</p>
<p>The reason <code>.map</code> is the appropriate choice here is because it is an abstraction that hides the control flow of looping through each item in a list, meaning it's impossible for you to get it wrong. When you use <code>.map</code>, you are guaranteed that the result will have as many elements as the original map, and that each element in the output array is a function only of the corresponding element in the input array**.</p>
<h4>Easy comprehension by others</h4>
<p>comparing the for-loop approach and the <code>.map</code> approach above, which is easier to parse as a reader? If you're only familiar with for-loops, you'll pick that, but given the ubiquity of <code>.map</code> in programming languages today, it's probably time to learn it. For those familiar with both, the <code>.map</code> approach is far easier to read:</p>
<ul>
<li>You don't need to read through how the <code>i</code> variable is manipulated in the for-loop, because that's abstracted away.</li>
<li>You know what the shape of the output will be.</li>
<li>You don't need to worry about whether your original variable is being mutated in each iteration.</li>
</ul>
<p>Without even looking at the callback function passed to <code>.map</code> you know a great deal about what to expect from the result. The same cannot be said of the for-loop.</p>
<p>Likewise, say I have an array of fruits and I want to know if it contains any apples. Here's a few approaches:</p>
<pre><code>const fruits = ['orange', 'pear', 'apple', 'apple', 'peach']

const hasAppleViaFilter = fruits.filter(fruit =&gt; fruit === 'apple').length &gt; 0
&gt; true

const hasAppleViaFind = fruits.find(fruit =&gt; fruit === 'apple') !== undefined
&gt; true

const hasAppleViaSome = fruits.some(fruit =&gt; fruit === 'apple')
&gt; true</code></pre>
<p>Each approach is ordered by decreasing power. Notice that <code>.some</code> is the easiest on the eyes? As soon as you see <code>.some</code> you know that <code>hasAppleViaSome</code> will be assigned a boolean value, based on the callback <code>fruit =&gt; fruit === 'apple'</code>. In the filter approach, you need to mentally store the fact that we're creating an array with a subset of the original array's fruits, and then we're checking the length of it, and comparing with zero. Only once you parse all of that do you realise the actual implicit intention, which happens to be the same as the <em>explicit</em> intention of the <code>.some</code> method.</p>
<p>These are just small examples, but when you have a big hairy callback with heaps of code inside, the reader can see that it's still just a call to <code>.some</code> and can rest assured that all the callback will do is return true or false. This calibrates the expectations of the reader and makes it easier to process what <em>is</em> happening inside the callback.</p>
<pre><code>const hasAppleViaContrivedSome = fruits.some(fruit =&gt; {
  if (typeof fruit !== 'string') {
    return false
  } 

  if (fruit === 'pear') {
    return false
  } 

  if (fruit === 'orange') {
    return false
  } 

  if (fruit === 'forbidden fruit') {
    return false
  }

  if (fruit.substring(1, 4) === 'pple') {
    return fruit === 'apple'
  }

  return false
})</code></pre>
<p>On the other hand, when somebody comes across your code and sees a powerful function used to perform something as trivial as a <code>.some</code> call, they're going to be more confused than the time they stumbled upon a grenade in the place you usually keep the fly swatter.</p>
<h3>With Little Power Comes Great Responsibility</h3>
<p>Hardcore functional languages like Haskell will not allow side effects inside a function, …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://jesseduffield.com/array-functions-and-the-rule-of-least-power/">https://jesseduffield.com/array-functions-and-the-rule-of-least-power/</a></em></p>]]>
            </description>
            <link>https://jesseduffield.com/array-functions-and-the-rule-of-least-power/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23889893</guid>
            <pubDate>Sun, 19 Jul 2020 15:16:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Shall the West Pass Too? Whispers from Fallen Civilisations]]>
            </title>
            <description>
<![CDATA[
Score 73 | Comments 84 (<a href="https://news.ycombinator.com/item?id=23889763">thread link</a>) | @sheefrex
<br/>
July 19, 2020 | https://www.themetasophist.com/chapter/whispers-from-fallen-civilisations | <a href="https://web.archive.org/web/*/https://www.themetasophist.com/chapter/whispers-from-fallen-civilisations">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div data-layout-label="Post Body" data-type="item" id="item-5f12bf74664501412d3f65f4"><div><div><div data-block-type="2" id="block-a407adb7a170c202cb65"><div><h3>1.1   Spengler’s Prediction</h3><p>Is it the destiny of the West to die? For Oswald Spengler, the answer was yes. In <em>The Decline of the West </em>he argued that all civilisations go through a similar life-cycle. According to Spengler, the Medieval Era was the spring of the West, the Renaissance its summer, and the Baroque era its autumn. He predicted that the West would enter its Winter around the year 2000, which would be characterised by a decline of democracy due to excessive influence from moneyed interests, and a resultant rise of authoritarianism.</p><p>This prophecy is quickly being fulfilled. The immense power of the financial sector was mobilised to loosen financial regulation, the ultimate result being the financial crisis of 2007-2008. The ensuing austerity sparked the rise of anti-establishment movements with authoritarian tendencies. In countries such as Italy and Hungary, such parties were or are in power.</p><p>The power of money seems set to remain strong in the West. The conditions which necessitated huge bank bailouts have not been eliminated. The revolving door means that many politicians and regulators are incentivised to serve banks while in office in return for very lucrative jobs afterwards. Typical examples of this include former US Treasury Secretary Timothy Geithner, former President of the Bundesbank Axel Weber, former head of the Swiss central bank Philipp Hildebrand, and former German Finance Minister Peer Steinbrück, all of whom earned millions from the financial sector during or after their positions in the public sector.</p><p>Since Spengler made his prophecy, the number of threats have multiplied beyond even what he foresaw. In a study of the collapse of the Bronze Age civilisation around 1177 BC, the historian Eric Cline revealed the true horsemen of collapse as being climate change, famines, droughts, earthquakes, rebellions, and mass migration.[1] Any one of these challenges could have been surmountable, but the confluence was fatal. Some are reappearing today, with the pandemic as a new recruit.</p><p>Climate change was the first to manifest. The fact that the ice shelves have begun to splinter is, for now, one worry among many. Of much greater concern are ever more extreme weather events, namely floods and droughts, that strike previously temperate regions. The final result of this could very well be the desertification of large parts of the United States and Southern Europe.[2]</p><p>Reducing our consumption of carbon fuels will not stop this threat. Europe has reduced CO2 emissions by 22 percent since 1990 [3], but the rest of the world, and especially China, continues to burn fossil fuels. According to Vaclav Smil, fossil fuels still supply 90 percent of global primary energy, a greater share than in 2000 when hydropower and nuclear energy were proportionately more widely used.[4] In fact, the success of Europe to date in containing rising emissions may be due to the fact that much energy-intensive manufacturing has already been sent abroad.</p><p>The oceans have also become more hostile to life. They have already begun to acidify due to their absorption of over 20 percent of the increased carbon dioxide in the air. This along with the higher temperature has pushed entire ecosystems such as the coral reefs into collapse, endangering the food supply of the one billion people who rely on the oceans for nourishment.</p><p>It should be no surprise then that the sixth mass extinction is now unfolding. Researchers, looking at 177 mammals for which they had detailed data, found that all have lost at least 30 percent of their geographic range and over 40 percent have undergone sharp population declines.[5]</p><p>Bee and insect populations are collapsing, along with the bird populations who rely on them for food. In France, countryside bird populations have fallen by a third in 15 years.[6] In remote Swiss mountains, plastic has even permeated the soil.[7] Given the complex nature of the ecosystem, the effects of all this are unpredictable.</p><p>Such environmental chaos could force migration, as much of Africa is highly dependent on agriculture for employment and well-being — and this is the sector most exposed to climate change. This is before we even take into account UN projections, according to which the population of Africa is forecast to increase from 1.2bn today to 2.5bn in 2050 and 4.4bn in 2100, at the same time climate change could make much of that continent uninhabitable.</p><p>This will occur in a context where youth bulges could very well lead to civil strife. According to German sociologist Gunnar Heinsohn, violence is inevitable when those aged fifteen to thirty comprise over 30 percent of the male population.[8] Similarly, others have found that a majority of conflict in previous decades started in countries where 60 percent of the population is under the age of thirty.[9]&nbsp;In <em>The Clash of Civilizations</em>, Samuel Huntington pointed to demographic trends to predict that many North African and Middle Eastern countries would be unstable today, a prediction borne out by the Arab Spring.[10]</p><p>Such instability be a particular challenge at a time of reduced integration and increased social stratification; integration of migrants is pereived to have failed in some Western European countries, while members of the working class are withdrawing their loyalty from governing elites. Toynbee’s framework indicates that both phenomena may have a common cause.</p><h2>1.2   Toynbee’s Thoughts</h2><p>Arnold Toynbee, a British historian who wrote twelve volumes entitled <em>A Study of History</em>, sought to understand the factors underlying civilisational growth and decline. Toynbee’s principal thesis was that a decline in creativity among the elites precipitates the breakdown of a civilisation. Their inability to devise solutions to the problems of the time leads the masses to cease their deferral to them.</p><p>And who could blame them for this in today’s world? A cursory glance in a newsagent tells us who the dominant cultural figures are: celebrities, sports stars, and perhaps the occasional politician. Yet what are the meaningful achievements of these groups? What risks do they take?</p><p>In a previous age, the leaders of societies literally put their lives on the line for their countries – think of de Gaulle, Churchill and most of the governing classes of the post-War period who had been involved in one if not two wars. This provided a powerful moral authority which today’s leaders lack. Moreover, the leaders of the nineteenth and twentieth centuries wrought the national and international institutions that govern us today – a creative triumph that eludes current leaders.</p><p>Due to numerous scandals, religious figures who once would have been venerated are now mostly disgraced. As for modern celebrities, while many are simply too vapid to be worth imitating, this does not stop the vapid from doing exactly that — to the delight of all manner of brands for whom the celebrity becomes a purchasable ambassador.</p><p>The decline of creativity is also evident in the world of fashion. As Kurt Andersen noted in an essay in <em>Vanity Fair</em>, fashion used to go through radical changes.[11] Every ten years from the 30s to the 90s, style changed radically. It has since stagnated. Taking a longer historical perspective, the dominance of the business suit since the thirties is anomalous, given that for centuries formal wear underwent radical changes relatively frequently.</p><p>How did this creative decline come about? Toynbee noted that once the masses cease to mimic the elite, the elite begins to mimic the masses in an attempt to gain popularity. This process, which Toynbee termed proletarianisation, is already quite advanced: consider the carefully calibrated way in which politicians seek to echo the opinions and language of the electorate. A typical example of this was when former British Prime Minister David Cameron forgot which football team he was meant to support.[12] To this theatre we can add the rise and now ubiquity of profanity, and the decline of formal dress. Perhaps this proletarianisation is the root of the stagnation in fashion: the elites mimic the masses, who are in general less likely to wear something radically different for fear of ridicule.</p><p>At the same time that the achievements of the elites are becoming less impressive, their failings are becoming more visible. Martin Gurri, in his book <em>The Revolt of the Public</em>, identified the internet as a key factor behind the diminishing credibility of elites.[13] The explosion of information has undermined traditional hierarchies, which formerly relied on control of information in order to hide their incompetence and thus preserve their legitimacy.</p><p>These multiple challenges could be tamed in a world where the nations of the West stood strong. Unfortunately, they continue to weaken. One example is the high level of debt in many Western countries, and their inability to stop borrowing. There was no clear plan to reduce such debt levels before the Coronavirus, and now the load has weightened significantly. As such, the West remains prone to a major financial crisis in the event a large economy, such as Italy, slips into bankruptcy.</p><p>As argued by Laurence Kotlikoff, professor of economics at Boston University, high levels of government debt will be compounded by extravagant commitments governments have made on pensions and healthcare. These unfunded liabilities are considerable and much greater than the official levels of public debt. In the case of the US, while public debt is around one hundred percent of GDP, the amount of unfunded liabilities is twelve times greater. The equivalent in the UK and the Netherlands is about five times annual GDP, while in France, Germany and Italy it does not exceed a multiple of two.[14]</p><p>In many ways the current difficulties faced by the West are a consequence of ageing: the debt crisis, low economic growth, and low interest rates all arise from a greying society. In Europe, the ageing German population needed to save a vast amount of money at …</p></div></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.themetasophist.com/chapter/whispers-from-fallen-civilisations">https://www.themetasophist.com/chapter/whispers-from-fallen-civilisations</a></em></p>]]>
            </description>
            <link>https://www.themetasophist.com/chapter/whispers-from-fallen-civilisations</link>
            <guid isPermaLink="false">hacker-news-small-sites-23889763</guid>
            <pubDate>Sun, 19 Jul 2020 14:56:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fixing Mass Effect black blobs on modern AMD CPUs]]>
            </title>
            <description>
<![CDATA[
Score 496 | Comments 152 (<a href="https://news.ycombinator.com/item?id=23889473">thread link</a>) | @Macha
<br/>
July 19, 2020 | https://cookieplmonster.github.io/2020/07/19/silentpatch-mass-effect/ | <a href="https://web.archive.org/web/*/https://cookieplmonster.github.io/2020/07/19/silentpatch-mass-effect/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
  
    <p><em>TL;DR - if you are not interested in an in-depth overview of what was wrong with the game and how it was fixed,
scroll down to <a href="#download"><strong>Download</strong></a> section for a download link.</em></p>

<hr>

<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#part-1">Part 1 – Research</a>
    <ul>
      <li><a href="#prelude">Prelude</a></li>
      <li><a href="#pix">PIX</a></li>
    </ul>
  </li>
  <li><a href="#part-2">Part 2 – A closer look into D3DX</a></li>
  <li><a href="#part-3">Part 3 – Standalone tests</a></li>
  <li><a href="#part-4">Part 4 – Putting it all together</a>
    <ul>
      <li><a href="#download">Download</a></li>
    </ul>
  </li>
</ul>

<hr>



<p><strong>Mass Effect</strong> is a popular franchise of sci-fi roleplaying games. The first game was initially released by BioWare in late 2007 on Xbox 360 exclusively as a part of a publishing deal with Microsoft.
A few months later in mid-2008, the game received PC port developed by Demiurge Studios. It was a decent port with no obvious flaws, that is until 2011 when AMD released their new Bulldozer-based CPUs.
When playing the game on PCs with modern AMD processors, two areas in the game (Noveria and Ilos) show severe graphical artifacts:</p>

<p>
<img src="https://cookieplmonster.github.io/assets/img/posts/mass-effect/me1-blobs.jpg"><br>
<em>Well, that doesn't look nice.</em>
</p>

<p>While not unplayable, it’s definitely distracting. Thankfully, workarounds exist – such as
<a href="http://abesmissioncontrol.blogspot.com/2015/04/mass-effect-fixing-blocky-player-models.html">disabling lighting via console commands</a>
or <a href="https://www.nexusmods.com/masseffect/mods/181">modifying the game’s maps to remove broken lights</a>, but seemingly the issue has never been fully understood.
Some sources claim that an FPS Counter mod can also fix that issue, but I couldn’t find much information about it and the mod’s sources don’t seem to be available online,
and there is no documentation on how the mod tackles this error.</p>

<p>What makes this issue particularly interesting? Vendor-specific bugs are nothing new, and games have had them for decades. However, to my best knowledge, this is the only case where a graphical
issue is caused by a <strong>processor</strong> and not by a graphics card. In the majority of cases, issues happen with a specific vendor of GPU and they don’t care about the CPU, while in this case, it’s the exact opposite.
This makes the issue very unique and worth looking into.</p>

<p>Looking up existing discussions online, this issue seems to affect AMD FX and Ryzen chips. Compared to the older AMD chips, these lack a <a href="https://en.wikipedia.org/wiki/3DNow!">3DNow! instruction set</a>.
Unrelated or not, the community consensus was that this was the cause of the bug and that the game tried to use those instructions upon detecting an AMD CPU.
Given that there are no known cases of this bug occurring on Intel CPU’s and that 3DNow! instructions were exclusive to AMD, it’s no surprise the community assumed that this is the issue.</p>

<p>Is this really the issue, or is it caused by something entirely different? Let’s find out!</p>



<h2 id="prelude">Prelude</h2>
<p>Even though the issue is trivial to reproduce, I couldn’t look into it for the longest time for a simple reason – I don’t have access to any PCs with AMD hardware!
Thankfully, this time I’m not approaching research alone – <a href="https://withinrafael.com/">Rafael Rivera</a> got my back during the entire process of R&amp;D,
providing a test environment with an AMD chip, insights, ideas as well as putting up with hundreds of blind guesses I usually throw around when trying to find the way to the root of such unknown problems.</p>

<p>Since we now had a good testing environment, the first theory to test was of course <code>cpuid</code> – if people are right in assuming that 3DNow! instructions are to blame, there should a place in the game’s code
where they check for their presence, or at the very least check for the CPU vendor. That reasoning is flawed, though; if it was true that the game attempts to use 3DNow! instructions any time it runs on an AMD chip,
without checking if they are supported, the game would most likely crash when trying to execute an illegal instruction. Moreover, a quick scan around the game’s code reveals that the game <strong>doesn’t</strong>
check for CPU capabilities. Therefore, whatever is up with this issue, it doesn’t appear to be caused by the game mis-detecting CPU features, because it seemingly doesn’t care about them in the first place.</p>

<p>When this started looking like an undebuggable case, Rafael came back to me with a realization – disabling <strong>PSGP</strong> (Processor Specific Graphics Pipeline) fixes the issue and the characters are properly lit!
PSGP is not the best documented term, but in short, it’s a legacy (concerning only older DirectX versions) feature allowing Direct3D to perform processor-specific optimizations:</p>

<blockquote>
  <p>In previous versions of DirectX, there was a path that allowed to do vertex processing called the PSGP. Applications had to take this path into account and support a path for vertex processing
on the processor and graphics cores.</p>
</blockquote>

<p>Putting it this way, it makes sense why disabling PSGP fixes artifacts on AMD – the path taken by modern AMD processors may be somehow broken.
How to disable it? Two ways come to mind:</p>
<ul>
  <li>It is possible to pass a <code>D3DCREATE_DISABLE_PSGP_THREADING</code> flag to <code>IDirect3D9::CreateDevice</code>. It’s defined as: <br>
    <blockquote>
      <p>Restrict computation to the main application thread. If the flag is not set, the runtime may perform software vertex processing and other computations in worker thread
to improve performance on multi-processor systems.</p>
    </blockquote>

    <p>Sadly, setting that flag doesn’t fix the issue. Looks like, despite the flag having “PSGP” in name, it’s not what we are looking for.</p>
  </li>
  <li>DirectX specifies two registry entries to disable PSGP in D3D and to disable PSGP only for D3DX – <code>DisablePSGP</code> and <code>DisableD3DXPSGP</code>. Those flags can be set system-wide or process-wide.
For information on how to set them only for a specific process, see <a href="https://withinrafael.com/2020/07/11/specify-application-specific-direct3d-flags/">Rafael Rivera’s guide on enabling application-specific Direct3D flags</a>.</li>
</ul>

<p><code>DisableD3DXPSGP</code> appears to be a viable fix for that issue. Therefore, if you have an aversion towards downloading third party fixes/modifications or you must fix this issue without making
any changes to the game, it’s a perfectly fine way of doing it. As long as you set that flag only for Mass Effect and not system-wide, it’s fine!</p>

<h2 id="pix">PIX</h2>
<p>As always with graphical issues, PIX is likely the most useful tool one could use to diagnose them. We captured similar scenes from Intel and AMD hardware and compared the results.
One difference was instantly noticeable – unlike with my past projects, where <a href="https://cookieplmonster.github.io/2018/07/07/farcry-d3d9-bug/">captures did not carry the bug with them</a> and the same capture
would look different on different PCs (indicating a driver or d3d9.dll bug), these captures carry the bug with them! In other words, a capture from an AMD hardware opened on a PC with Intel hardware
<strong>does</strong> show the bug.</p>

<p>An AMD capture on Intel looks no different than on the hardware it was taken from:</p>

<p>
<img src="https://cookieplmonster.github.io/assets/img/posts/mass-effect/me1-pix1.jpg">
</p>

<p>What does this tell us?</p>
<ul>
  <li>Since PIX does not “take screenshots” but instead captures the sequence of D3D commands and executes them on hardware, we can observe that executing the commands captured from an AMD box
results in the same bug when executed on Intel.</li>
  <li>This strongly implies that the difference is not caused by the difference in <strong>how</strong> the commands are executed (that’s how you get GPU specific bugs), but <strong>what</strong> commands are executed.</li>
</ul>

<p>In other words, it’s almost certainly not any sort of a driver bug. Instead, the way inputs for the GPU are prepared seems to be somehow broken<sup id="fnref:1"><a href="#fn:1">1</a></sup>. That is indeed a very rare occurrence!</p>

<p>At this point, finding the bug is a matter of finding any jarring differences between captures. It’s tedious, but that’s the only viable way.</p>

<p>After a long while spent poking the capture, a full body draw call caught my attention:</p>

<p>
<img src="https://cookieplmonster.github.io/assets/img/posts/mass-effect/me1-pix2.jpg">
</p>

<p>On an Intel capture, this draw outputs most of the character’s body, together with lighting and textures. On an AMD capture, it outputs a plain black model. This looks like a good trail.</p>

<p>The first obvious candidate for checking would be bound textures, but they seem to be fine and are consistent across captures.
However, some of the pixel shader constants looked weird. Not only do they have NaNs (Not a Number), but they also seem to only appear on the AMD capture and not the Intel capture:</p>

<p>
<img src="https://cookieplmonster.github.io/assets/img/posts/mass-effect/me1-pix3.jpg"><br>
<em>1.#QO indicates a NaN</em>
</p>

<p>This looks promising – NaN values causing strange visuals are not unheard of. Funnily enough, a PlayStation 3 version of Mass Effect 2
<a href="https://github.com/RPCS3/rpcs3/issues/7397">had a very similar looking issue in RPCS3</a> which was also related to NaNs!</p>

<p>However, before we get too excited, those values could just be leftovers from previous draws and they might end up being unused for the current draw.
Luckily, in this case it’s clearly visible that those NaNs get submitted to D3D for this specific draw…</p>

<div><div><pre><code>49652	IDirect3DDevice9::SetVertexShaderConstantF(230, 0x3017FC90, 4)
49653	IDirect3DDevice9::SetVertexShaderConstantF(234, 0x3017FCD0, 3)
49654	IDirect3DDevice9::SetPixelShaderConstantF(10, 0x3017F9D4, 1) // Submits constant c10
49655	IDirect3DDevice9::SetPixelShaderConstantF(11, 0x3017F9C4, 1) // Submits constant c11
49656	IDirect3DDevice9::SetRenderState(D3DRS_FILLMODE, D3DFILL_SOLID)
49657	IDirect3DDevice9::SetRenderState(D3DRS_CULLMODE, D3DCULL_CW)
49658	IDirect3DDevice9::SetRenderState(D3DRS_DEPTHBIAS, 0.000f)
49659	IDirect3DDevice9::SetRenderState(D3DRS_SLOPESCALEDEPTHBIAS, 0.000f)
49660	IDirect3DDevice9::TestCooperativeLevel()
49661	IDirect3DDevice9::SetIndices(0x296A5770)
49662	IDirect3DDevice9::DrawIndexedPrimitive(D3DPT_TRIANGLELIST, 0, 0, 2225, 0, 3484) // Draws the character model
</code></pre></div></div>

<p>…and the pixel shader used for this draw references both constants:</p>
<div><div><pre><code>// Registers:
//
//   Name                     Reg   Size
//   ------------------------ ----- ----
//   UpperSkyColor            c10      1
//   LowerSkyColor            c11      1
</code></pre></div></div>

<p>Both constants appear to <a href="https://github.com/abaelhe/unrealengine-old/search?q=UpperSkyColor">come straight from Unreal Engine</a> and judging by the name,
they might directly influence the lighting. Bingo!</p>

<p>A quick in-game test further confirms the theory – on an Intel machine, a vector of 4 NaN values was never submitted as pixel shader constants;
meanwhile, on an AMD machine, NaNs would start showing up as soon as the player entered the area where lighting breaks!</p>

<p>Does it mean work is done? No, far from it, as finding broken constants is only half of the success. The question remains, where do they come from, and can they be replaced?
An in-game test replacing NaN values with zeros partially fixed the issue – ugly black blobs disappeared, but characters were still way too dark:</p>

<p>
<img src="https://cookieplmonster.github.io/assets/img/posts/mass-effect/me1-dark-lighting.jpg"><br>
<em>Almost correct... …</em></p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://cookieplmonster.github.io/2020/07/19/silentpatch-mass-effect/">https://cookieplmonster.github.io/2020/07/19/silentpatch-mass-effect/</a></em></p>]]>
            </description>
            <link>https://cookieplmonster.github.io/2020/07/19/silentpatch-mass-effect/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23889473</guid>
            <pubDate>Sun, 19 Jul 2020 14:08:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Clear explanation of Rust’s module system]]>
            </title>
            <description>
<![CDATA[
Score 151 | Comments 42 (<a href="https://news.ycombinator.com/item?id=23889427">thread link</a>) | @rkwz
<br/>
July 19, 2020 | http://www.sheshbabu.com/posts/rust-module-system/ | <a href="https://web.archive.org/web/*/http://www.sheshbabu.com/posts/rust-module-system/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Rust’s module system is surprisingly confusing and causes a lot of frustration for beginners.</p>
<p>In this post, I’ll explain the module system using practical examples so you get a clear understanding of how it works and can immediately start applying this in your projects.</p>
<p>Since Rust’s module system is quite unique, I request the reader to read this post with an open mind and resist comparing it with how modules work in other languages.</p>
<p>Let’s use this file structure to simulate a real world project:</p>
<pre><code>my_project
├── Cargo.toml
└─┬ src
  ├── main.rs
  ├── config.rs
  ├─┬ routes
  │ ├── health_route.rs
  │ └── user_route.rs
  └─┬ models
    └── user_model.rs</code></pre>
<p>These are the different ways we should be able to consume our modules:</p>
<p><img src="http://www.sheshbabu.com/images/2020-rust-module-system/rust-module-system-1.png" alt=""></p>
<p>These 3 examples should be sufficient to explain how Rust’s module system works.</p>
<h2 id="Example-1"><a href="#Example-1" title="Example 1"></a>Example 1</h2><p>Let’s start with the first example - importing <code>config.rs</code> in <code>main.rs</code>.</p>
<pre><code>
<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>println!</span><span>(</span><span>"main"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<pre><code>
<span>fn</span> <span>print_config</span><span>(</span><span>)</span> <span>{</span>
  <span>println!</span><span>(</span><span>"config"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<p>The first mistake that everyone makes is just because we have files like <code>config.rs</code>, <code>health_route.rs</code> etc, we think that these files are <code>modules</code> and we can import them from other files.</p>
<p>Here’s what we see (file system tree) and what the compiler sees (module tree):</p>
<p><img src="http://www.sheshbabu.com/images/2020-rust-module-system/rust-module-system-2.png" alt=""></p>
<p>Surprisingly, the compiler only sees the <code>crate</code> module which is our <code>main.rs</code> file. This is because we need to explicitly build the module tree in Rust - there’s no implicit mapping between file system tree to module tree.</p>
<blockquote>
<p>We need to explicitly build the module tree in Rust, there’s no implicit mapping to file system</p>
</blockquote>
<p>To add a file to the module tree, we need to declare that file as a submodule using the <code>mod</code> keyword. The next thing that confuses people is that you would assume we declare a file as module in the same file. But we need to declare this in a different file! Since we only have <code>main.rs</code> in the module tree, let’s declare <code>config.rs</code> as a submodule in <code>main.rs</code>.</p>
<blockquote>
<p>The mod keyword declares a submodule</p>
</blockquote>
<p>The <code>mod</code> keyword has this syntax:</p>
<pre><code><span>mod</span> my_module<span>;</span></code></pre>
<p>Here, the compiler looks for <code>my_module.rs</code> or <code>my_module/mod.rs</code> in the same directory.</p>
<pre><code>my_project
├── Cargo.toml
└─┬ src
  ├── main.rs
  └── my_module.rs

or

my_project
├── Cargo.toml
└─┬ src
  ├── main.rs
  └─┬ my_module
    └── mod.rs</code></pre>
<p>Since <code>main.rs</code> and <code>config.rs</code> are in the same directory, let’s declare the config module as follows:</p>
<pre><code>// main.rs
<span>+ mod config;</span>

fn main() {
<span>+ config::print_config();</span>
  println!("main");
}</code></pre>
<pre><code>
<span>fn</span> <span>print_config</span><span>(</span><span>)</span> <span>{</span>
  <span>println!</span><span>(</span><span>"config"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<p>We’re accessing the <code>print_config</code> function using the <code>::</code> syntax.</p>
<p>Here’s how the module tree looks like:</p>
<p><img src="http://www.sheshbabu.com/images/2020-rust-module-system/rust-module-system-3.png" alt=""></p>
<p>We’ve successfully declared the <code>config</code> module! But this is not sufficient to be able to call the <code>print_config</code> function inside <code>config.rs</code>. Almost everything in Rust is private by default, we need to make the function public using the <code>pub</code> keyword:</p>
<blockquote>
<p>The pub keyword makes things public</p>
</blockquote>
<pre><code>
<span>mod</span> config<span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  config<span>:</span><span>:</span><span>print_config</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"main"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<pre><code>// config.rs
<span>- fn print_config() {</span>
<span>+ pub fn print_config() {</span>
  println!("config");
}</code></pre>
<p>Now, this works. We’ve successfully called a function defined in a different file!</p>
<h2 id="Example-2"><a href="#Example-2" title="Example 2"></a>Example 2</h2><p>Let’s try calling the <code>print_health_route</code> function defined in <code>routes/health_route.rs</code> from <code>main.rs</code>.</p>
<pre><code>
<span>mod</span> config<span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  config<span>:</span><span>:</span><span>print_config</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"main"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<pre><code>
<span>fn</span> <span>print_health_route</span><span>(</span><span>)</span> <span>{</span>
  <span>println!</span><span>(</span><span>"health_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<p>As we discussed earlier, we can use the <code>mod</code> keyword only for <code>my_module.rs</code> or <code>my_module/mod.rs</code> in the same directory.</p>
<p>So in order to call functions inside <code>routes/health_route.rs</code> from <code>main.rs</code>, we need to do the following things:</p>
<ul>
<li>Create a file named <code>routes/mod.rs</code> and declare the <code>routes</code> submodule in <code>main.rs</code></li>
<li>Declare the <code>health_route</code> submodule in <code>routes/mod.rs</code> and make it public</li>
<li>Make the functions inside <code>health_route.rs</code> public</li>
</ul>
<pre><code>my_project
├── Cargo.toml
└─┬ src
  ├── main.rs
  ├── config.rs
  ├─┬ routes
<span>+ │ ├── mod.rs</span>
  │ ├── health_route.rs
  │ └── user_route.rs
  └─┬ models
    └── user_model.rs</code></pre>
<pre><code>// main.rs
mod config;
<span>+ mod routes;</span>

fn main() {
<span>+ routes::health_route::print_health_route();</span>
  config::print_config();
  println!("main");
}</code></pre>
<pre><code>// routes/mod.rs
<span>+ pub mod health_route;</span></code></pre>
<pre><code>// routes/health_route.rs
<span>- fn print_health_route() {</span>
<span>+ pub fn print_health_route() {</span>
  println!("health_route");
}</code></pre>
<p>Here’s how the module tree looks like:</p>
<p><img src="http://www.sheshbabu.com/images/2020-rust-module-system/rust-module-system-4.png" alt=""></p>
<p>We can now call a function defined in a file inside a folder.</p>
<h2 id="Example-3"><a href="#Example-3" title="Example 3"></a>Example 3</h2><p>Let’s try calling from <code>main.rs =&gt; routes/user_route.rs =&gt; models/user_model.rs</code></p>
<pre><code>
<span>mod</span> config<span>;</span>
<span>mod</span> routes<span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  routes<span>:</span><span>:</span>health_route<span>:</span><span>:</span><span>print_health_route</span><span>(</span><span>)</span><span>;</span>
  config<span>:</span><span>:</span><span>print_config</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"main"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<pre><code>
<span>fn</span> <span>print_user_route</span><span>(</span><span>)</span> <span>{</span>
  <span>println!</span><span>(</span><span>"user_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<pre><code>
<span>fn</span> <span>print_user_model</span><span>(</span><span>)</span> <span>{</span>
  <span>println!</span><span>(</span><span>"user_model"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<p>We want to call the function <code>print_user_model</code> from <code>print_user_route</code> from <code>main</code>.</p>
<p>Let’s make the same changes as before - declaring submodules, making functions public and adding the <code>mod.rs</code> file.</p>
<pre><code>my_project
├── Cargo.toml
└─┬ src
  ├── main.rs
  ├── config.rs
  ├─┬ routes
  │ ├── mod.rs
  │ ├── health_route.rs
  │ └── user_route.rs
  └─┬ models
<span>+   ├── mod.rs</span>
    └── user_model.rs</code></pre>
<pre><code>// main.rs
mod config;
mod routes;
<span>+ mod models;</span>

fn main() {
  routes::health_route::print_health_route();
<span>+ routes::user_route::print_user_route();</span>
  config::print_config();
  println!("main");
}</code></pre>
<pre><code>// routes/mod.rs
pub mod health_route;
<span>+ pub mod user_route;</span></code></pre>
<pre><code>// routes/user_route.rs
<span>- fn print_user_route() {</span>
<span>+ pub fn print_user_route() {</span>
  println!("user_route");
}</code></pre>
<pre><code>// models/mod.rs
<span>+ pub mod user_model;</span></code></pre>
<pre><code>// models/user_model.rs
<span>- fn print_user_model() {</span>
<span>+ pub fn print_user_model() {</span>
  println!("user_model");
}</code></pre>
<p>Here’s how the module tree looks like:</p>
<p><img src="http://www.sheshbabu.com/images/2020-rust-module-system/rust-module-system-5.png" alt=""></p>
<p>Wait, we haven’t actually called <code>print_user_model</code> from <code>print_user_route</code>! So far, we’ve only called the functions defined in other modules from <code>main.rs</code>, how do we do that from other files?</p>
<p>If we look at our module tree, the <code>print_user_model</code> function sits in the <code>crate::models::user_model</code> path. So in order to use a module in files that are not <code>main.rs</code>, we should think in terms of the path necessary to reach that module in the module tree.</p>
<pre><code>// routes/user_route.rs
pub fn print_user_route() {
<span>+ crate::models::user_model::print_user_model();</span>
  println!("user_route");
}</code></pre>
<p>We’ve successfully called a function defined in a file from a file that’s not <code>main.rs</code>.</p>
<h2 id="super"><a href="#super" title="super"></a>super</h2><p>The fully qualified name gets too lengthy if our file organization is multiple directories deep. Let’s say for whatever reason, we want to call <code>print_health_route</code> from <code>print_user_route</code>. These are under the paths <code>crate::routes::health_route</code> and <code>crate::routes::user_route</code> respectively.</p>
<p>We can call it by using the fully qualified name <code>crate::routes::health_route::print_health_route()</code> but we can also use a relative path <code>super::health_route::print_health_route();</code>. Notice that we’ve used <code>super</code> to refer to the parent scope.</p>
<blockquote>
<p>The super keyword in module path refers to the parent scope</p>
</blockquote>
<pre><code><span>pub</span> <span>fn</span> <span>print_user_route</span><span>(</span><span>)</span> <span>{</span>
  <span>crate</span><span>:</span><span>:</span>routes<span>:</span><span>:</span>health_route<span>:</span><span>:</span><span>print_health_route</span><span>(</span><span>)</span><span>;</span>
  
  <span>super</span><span>:</span><span>:</span>health_route<span>:</span><span>:</span><span>print_health_route</span><span>(</span><span>)</span><span>;</span>

  <span>println!</span><span>(</span><span>"user_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<h2 id="use"><a href="#use" title="use"></a>use</h2><p>It would be tedious to use the fully qualified name or even the relative name in the above examples. In order to shorten the names, we can use the <code>use</code> keyword to bind the path to a new name or alias.</p>
<blockquote>
<p>The use keyword is used to shorten the module path</p>
</blockquote>
<pre><code><span>pub</span> <span>fn</span> <span>print_user_route</span><span>(</span><span>)</span> <span>{</span>
  <span>crate</span><span>:</span><span>:</span>models<span>:</span><span>:</span>user_model<span>:</span><span>:</span><span>print_user_model</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"user_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<p>The above code can be refactored as:</p>
<pre><code><span>use</span> <span>crate</span><span>:</span><span>:</span>models<span>:</span><span>:</span>user_model<span>:</span><span>:</span>print_user_model<span>;</span>

<span>pub</span> <span>fn</span> <span>print_user_route</span><span>(</span><span>)</span> <span>{</span>
  <span>print_user_model</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"user_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<p>Instead of using the name <code>print_user_model</code>, we can also alias it to something else:</p>
<pre><code><span>use</span> <span>crate</span><span>:</span><span>:</span>models<span>:</span><span>:</span>user_model<span>:</span><span>:</span>print_user_model <span>as</span> log_user_model<span>;</span>

<span>pub</span> <span>fn</span> <span>print_user_route</span><span>(</span><span>)</span> <span>{</span>
  <span>log_user_model</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"user_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<h2 id="External-modules"><a href="#External-modules" title="External modules"></a>External modules</h2><p>Dependencies added to <code>Cargo.toml</code> are available globally to all modules inside the project. We don’t need to explicitly import or declare anything to use a dependency.</p>
<blockquote>
<p>External dependencies are globally available to all modules inside a project</p>
</blockquote>
<p>For example, let’s say we added the <a href="https://crates.io/crates/rand" target="_blank" rel="noopener">rand</a> crate to our project. We can use it in our code directly as:</p>
<pre><code><span>pub</span> <span>fn</span> <span>print_health_route</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> random_number<span>:</span> u8 <span>=</span> rand<span>:</span><span>:</span><span>random</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"{}"</span><span>,</span> random_number<span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"health_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<p>We can also use <code>use</code> to shorten the path:</p>
<pre><code><span>use</span> rand<span>:</span><span>:</span>random<span>;</span>

<span>pub</span> <span>fn</span> <span>print_health_route</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> random_number<span>:</span> u8 <span>=</span> <span>random</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"{}"</span><span>,</span> random_number<span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"health_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<h2 id="Summary"><a href="#Summary" title="Summary"></a>Summary</h2><ul>
<li>The module system is explicit - there’s no 1:1 mapping with file system</li>
<li>We declare a file as module in its parent, not in itself</li>
<li>The <code>mod</code> keyword is used to declare submodules</li>
<li>We need to explicitly declare functions, structs etc as public so they can be consumed in other modules</li>
<li>The <code>pub</code> keyword makes things public</li>
<li>The <code>use</code> keyword is used to shorten the module path</li>
<li>We don’t need to explicitly declare 3rd party modules</li>
</ul>
<p>Thanks for reading! Feel free to follow me in <a href="https://twitter.com/sheshbabu" target="_blank" rel="noopener">Twitter</a> for more posts like this :)</p>
</div></div>]]>
            </description>
            <link>http://www.sheshbabu.com/posts/rust-module-system/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23889427</guid>
            <pubDate>Sun, 19 Jul 2020 14:01:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: [Demo] GigoBooks: open-source accounting SW for micro-business (NoSaaS)]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 0 (<a href="https://news.ycombinator.com/item?id=23889104">thread link</a>) | @bengtan
<br/>
July 19, 2020 | https://gigobooks.github.io/demo/ | <a href="https://web.archive.org/web/*/https://gigobooks.github.io/demo/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://gigobooks.github.io/demo/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23889104</guid>
            <pubDate>Sun, 19 Jul 2020 13:13:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[One year of Flutter as a web developer]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=23888947">thread link</a>) | @realusername
<br/>
July 19, 2020 | https://alex-min.fr/one-year-of-flutter/ | <a href="https://web.archive.org/web/*/https://alex-min.fr/one-year-of-flutter/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
    <p>I spent a considerable time on my <a rel="nofollow noopener noreferrer" target="_blank" href="https://mavio.fr/">budget app</a> side project built in Flutter. After a year of development and 25k lines of code later, I wanted to create a blog post about how everything went and my feedback on working with this framework.</p>

<h2 id="the-dart-language">The Dart language</h2>

<p>Flutter is built with Dart, a Javascript/Java inspired language. Dart was built internally at Google with the specific goal to replace Javascript on the web as a saner language, with better defaults. Unfortunately for the Dart team, the project never took off, Dart was stuck in a limbo state for a few years while still being maintained for internal Google needs.</p>

<p>The language itself had very good features which enabled other uses cases:</p>
<ul>
  <li>The Dart VM is a wonderful piece of technology, build with hot reloading at its core</li>
  <li>The language can be interpreted or compiled natively for better performance</li>
  <li>The Dart tooling around the language well built</li>
</ul>

<p>I fully expected the language to be an issue on the project since it’s so niche, but actually it has been a great experience on that level. It’s quick to learn and very easy to pick up, especially if you have done some Javascript previously.</p>

<h2 id="the-good-parts">The good parts</h2>

<h3 id="tooling">Tooling</h3>

<p>The Flutter tooling is one of it’s strong parts. Hot reloading your code <strong>actually</strong> works, Flutter is almost the only piece of technology I’ve ever used where the hot reload was working as well as it is. Generally I would disable it elsewhere, it even makes you miss it in other languages.</p>

<p>The package manager (pub) works great, upgrades are smooth (if you are coming from the npm world, it’s honestly a breath of fresh air). After a year of managing my project, I only had a single conflict where I had to pin a specific version of a package to avoid a compilation error with the main Flutter SDK, that only lasted two weeks and then upgrades were smooth again.</p>

<p>The Flutter SDK has multiple channels (master, dev, beta and stable), the stable version is quite good as not reintroducing regressions and flutter upgrades can be done from the command line easily.</p>

<p>The language server in VSCode works perfectly, autocompletion also works wonders, the Dart code formatter is also one of the best I used, I consider it on par with Go, which is one of the best in my opinion.</p>

<h3 id="performance--animations">Performance &amp; Animations</h3>

<p>Flutter uses Skia, an internal and very performant render engine which draws everything to a canvas. Performance is definitely a strong point of the platform. I use a very cheap Samsung J3 as my daily driver which is almost the bottom of the barrel in terms of performance in 2020. Basically every app on my phone is lagging noticeably, including Google apps. My budget app on the other hand is the quickest app on my phone, by a long margin. If you are targetting a market with a lot of low-end budget phones, Flutter is exactly what you need.</p>

<p>Animations are also a very strong point of this platform, high-speed animations can be built with <a rel="nofollow noopener noreferrer" target="_blank" href="https://rive.app/">Rive</a> and imported directly into Flutter, rendered at full speed and weighting a few Kb. I do have a few toy animations built with Rive on my app.</p>

<h3 id="portability">Portability</h3>

<p>I have not published the iOS version yet due to Apple developer account issues but my budget app already works without much effort identically on Android, Linux, Windows &amp; macOS thanks to <a rel="nofollow noopener noreferrer" target="_blank" href="https://github.com/go-flutter-desktop/hover">Hover</a>.</p>

<p>If you are building a desktop app, Flutter can be a good technology to keep in mind. It produces small binaries, fast and reactive applications, works cross-platform and won’t use too much RAM like most Electron apps.</p>

<p>Desktop support is still in an alpha quality at the moment so expect some missing features, you might have to dig into the code and make some pull requests.</p>

<h3 id="the-flutter-sdk">The Flutter SDK</h3>

<p>The SDK is quite complete, a lot of basic and complex components are there. The SDK has material design guidelines by default, which might be a good thing or a bad thing depending on what you are trying to achieve.</p>

<p>While you can make your application look native with more work but generally it’s not what you would use Flutter for. Flutter shines primarily as building complex apps which have their own design goals.</p>

<h3 id="styling">Styling</h3>

<p>Styling will look familiar if you are coming from a web background. You will have a stripped-down version of flex, padding &amp; margins, border-radius, font-weight…</p>

<p>The main difference being that there’s a global Flutter theme which applies to your whole app instead of classes. This makes it easier to swap the theme to make a dark theme.</p>

<p>I’m still trying to make my stying code looks nicer, extending the main theme seems to be the right approach.</p>

<h2 id="the-parts-which-would-need-improvements">The parts which would need improvements</h2>

<h3 id="testing">Testing</h3>

<p>Testing isn’t that bad, especially if you are coming from the Javascript world. A lot of the testing building blocks you expect are there and you do have a driver where you can tap on elements and see what happens.</p>

<figure>

  <p><img src="https://d33wubrfki0l68.cloudfront.net/172a2207d60b86a77bfc854290aa6349be0d69a7/3e636/images/flutter-test-terminal.png" alt="A screenshot of a terminal showing that 174 tests passed in 4 minutes 43 seconds"></p>
  <figcaption>Yay! Hopefully everything works.</figcaption>
</figure>

<p>If you come from the Ruby world however, the testing will feel pretty average and you might find yourself needing a bit more boilerplate than you would expect. There’s definitely some room to make testing more enjoyable and faster to write.</p>

<p>There is also no mutation testing in Dart yet (that I’m aware of), so you will have to use the code coverage tool extensively in the mean time and try to catch as much as you can. I had a pretty blocking bug on the code coverage tool which has been fixed in one of the more recent stable versions so I can recommend it now.</p>

<h3 id="navigation">Navigation</h3>

<p>Navigation works stateless in Flutter. That can be a great thing, especially when you are importing packages, no need to plug your router for using packages, there’s one big issue however, it’s not really possible to micro-manage the view state when you use the go back button on Android.</p>

<p>That can be a problem for knowing which route you are on at the moment if you want to update the bottom bar with an indicator for example. As far as I know, it’s still an unsolved problem, unless you rebuild an entire navigation system yourself with Provider or another Flutter state management library. I’ll probably do that in the future but that’s a lower priority bug at the moment.</p>

<h3 id="webviews">Webviews</h3>

<p>If you need Webviews for your project (which I needed), it might be a blocker in the current state. Webviews don’t support multiple tabs (which means <em>window.open</em> won’t work in Javascript), the view might crash and there is poor keyboard support at the moment.</p>

<p>Additionally, this alpha-state Webview is only available on Android and iOS, not on any desktop platform yet.</p>

<h2 id="conclusion">Conclusion</h2>

<p>As far as the Flutter platform goes, it’s been a pretty enjoyable experience for me. I will write one or more retrospective blog post on this project. I do recommend Flutter and even as a web developer, where you might prefer React Native initially due to the familiarity with Javascript, I would recommend trying Flutter, it will surprise you.</p>


  </section></div>]]>
            </description>
            <link>https://alex-min.fr/one-year-of-flutter/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888947</guid>
            <pubDate>Sun, 19 Jul 2020 12:41:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[PostgreSQL quickstart guide – connecting, remote access, psql CLI and more]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 0 (<a href="https://news.ycombinator.com/item?id=23888907">thread link</a>) | @lukasbar
<br/>
July 19, 2020 | https://knowledgepill.it/posts/postgresql-basics-guide/ | <a href="https://web.archive.org/web/*/https://knowledgepill.it/posts/postgresql-basics-guide/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <h2 id="configure-remote-access---listen-address">Configure remote access - listen address</h2>
<p>By default after instalation and creting database cluster PostgreSQL will listner only on localhost. No remote access will be allowed.</p>
<hr>
<p><a href="https://knowledgepill.it/posts/postgresql_installation/">PostgreSQL installation on Linux - with database creation</a></p>
<hr>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ netstat -lptnu | grep post
<span>(</span>Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.<span>)</span>
tcp        <span>0</span>      <span>0</span> 127.0.0.1:5432          0.0.0.0:*               LISTEN      1977/postmaster     
tcp6       <span>0</span>      <span>0</span> ::1:5432                :::*                    LISTEN      1977/postmaster     
</code></pre></div><p>To change listen address we have to configure parameter in <code>postgresql.conf</code></p>
<p>Check <code>PGDATA</code> - after <code>-D</code> parameter:</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ ps aux | grep postgres
postgres  <span>1977</span>  0.0  2.5 <span>286388</span> <span>14864</span> ?        Ss   Jun28   0:02 /usr/pgsql-12/bin/postmaster -D /postgresql/data
postgres  <span>1979</span>  0.0  0.2 <span>140768</span>  <span>1360</span> ?        Ss   Jun28   0:00 postgres: logger   
postgres  <span>1981</span>  0.0  0.5 <span>286504</span>  <span>3028</span> ?        Ss   Jun28   0:00 postgres: checkpointer   
postgres  <span>1982</span>  0.0  0.2 <span>286388</span>  <span>1696</span> ?        Ss   Jun28   0:03 postgres: background writer   
postgres  <span>1983</span>  0.0  0.9 <span>286388</span>  <span>5676</span> ?        Ss   Jun28   0:03 postgres: walwriter   
postgres  <span>1984</span>  0.0  0.4 <span>286924</span>  <span>2688</span> ?        Ss   Jun28   0:02 postgres: autovacuum launcher  
</code></pre></div><p>Locate the file:</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ cd /postgresql/data/
<span>[</span>postgres@postgres-lab data<span>]</span>$ ls -lah postgresql.conf
-rw-------. <span>1</span> postgres postgres 26K Jun <span>28</span> 21:44 postgresql.conf
</code></pre></div><p>Change in <code>postgresql.conf</code> parameter <code>listen_addresses</code> to your server IP or <code>*</code> to listen on all IP’s available on server:</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab data<span>]</span>$ vi postgresql.conf

<span>##------------------------------------------------------------------------------</span>
<span>## CONNECTIONS AND AUTHENTICATION</span>
<span>##------------------------------------------------------------------------------</span>

<span>## - Connection Settings -</span>

listen_addresses <span>=</span> <span>'*'</span>          <span>## what IP address(es) to listen on;</span>
                                        <span>## comma-separated list of addresses;</span>
                                        <span>## defaults to 'localhost'; use '*' for all</span>
</code></pre></div><p>Restart PostgreSQL to apply changes - you can do that with <code>systemctl</code> from <code>root</code> os user  service or with <code>pg_ctl -D PGDATA restart</code> from <code>postgres</code> os user:</p>
<div><pre><code data-lang="bash"><span>[</span>root@postgres-lab ~<span>]</span><span>## systemctl restart postgresql-12.service</span>
</code></pre></div><p>Check whre PostgreSQL is listening now:</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ netstat -lptnu | grep post
<span>(</span>Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.<span>)</span>
tcp        <span>0</span>      <span>0</span> 0.0.0.0:5432            0.0.0.0:*               LISTEN      30161/postmaster    
tcp6       <span>0</span>      <span>0</span> :::5432                 :::*                    LISTEN      30161/postmaster  
</code></pre></div><h2 id="configure-remote-access---pg_hbaconf">Configure remote access - pg_hba.conf</h2>
<p>PostgreSQL instance has got restricted access by <code>pg_hba.conf</code> file(host based authentication file).</p>
<p>We can provide in it information from which <code>ADDRESS</code> to which <code>DATABASE</code> on which <code>USER</code> by what <code>METHOD</code> we allow connecting. Additionaly we have to provide <code>TYPE</code> of connection.</p>
<p>This file resides in same place where <code>postgresql.conf</code>(we can alter this behavior by setting <code>pg_hba</code> parameter in <code>postgresql.conf</code>):</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ cd /postgresql/data/

<span>[</span>postgres@postgres-lab data<span>]</span>$ vi pg_hba.conf
<span>## TYPE  DATABASE        USER            ADDRESS                 METHOD</span>

<span>## "local" is for Unix domain socket connections only</span>
local   all             all                                     trust
<span>## IPv4 local connections:</span>
host    all             all             127.0.0.1/32            trust
<span>## IPv6 local connections:</span>
host    all             all             ::1/128                 trust
</code></pre></div><p>Allowed <code>TYPE</code>'s:</p>
<ul>
<li><code>local</code> - socket connection - needed to connect from shell on database server</li>
<li><code>host</code> - standard TCP/IP connection over the network - bnost SSL and no SSL</li>
<li><code>hostssl</code> - TCP/IP connection but only with SSL</li>
<li><code>hostnossl</code> - TCP/IP only without SSL</li>
<li><code>hostgssenc</code> - TCP/IP only GSSAPI</li>
<li><code>hostnogssenc</code> - TCP/IP only without GSSAPI</li>
</ul>
<p>With <code>DATABASE</code> we can specify database name or use special value <code>sameuser</code> if database name should be same as name of user that is connecting.</p>
<p>With <code>USER</code> we can specify user or role - role name should be preceded by <code>+</code> sign.</p>
<p><code>ADDRESS</code> field could be - hostname, IP range in CIDR format or special words:</p>
<ul>
<li><code>samehost</code> - which correspond to all IP adresses of database server</li>
<li><code>samenet</code> - which correspond to all IP in database server subnet</li>
</ul>
<p>With <code>METHOD</code> field we can set one of authentication methods - most important ones are:</p>
<ul>
<li><code>trust</code> - allow connection without password - moslty set for local connections from database server itself</li>
<li><code>reject</code> - reject connections</li>
<li><code>md5</code> - allow connections after getting from user password - encrypted</li>
<li><code>password</code> - allow connection after getting plain password - DO NOT USE in untrusted networks - better -&gt; never use this option</li>
<li><code>ldap</code> - getting account authorization data from LDAP server</li>
</ul>
<p>In <code>DATABASE</code> and <code>USER</code> fields you can specify special word <code>all</code> if you don’t want to create any restrictions here.</p>
<p>There can be situation when we must use additional field named <code>auth-options</code> for specyfying details for example for <code>hostssl</code> connection type. This topic will be covered in another post.</p>
<h3 id="sample-pg_hba-record---allow-all-users-connect-to-any-db-from-all-ip-addresses---only-with-password">Sample pg_hba record - allow all users connect to any DB from all IP addresses - only with password</h3>
<p>Add in <code>pg_hba.conf</code>:</p>
<div><pre><code data-lang="bash"><span>## Network access</span>
host    all             all             0.0.0.0/0               md5
</code></pre></div><p>Reload(online operation) PostgreSQL that it can use <code>pg_hba.conf</code> changes:</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab data<span>]</span>$ /usr/pgsql-12/bin/pg_ctl -D /postgresql/data reload
server signaled
</code></pre></div><h2 id="connecting-to-postgresql">Connecting to PostgreSQL</h2>
<h3 id="local-from-server">Local from server</h3>
<p>It will work without password because we have <code>trust</code> in <code>pg_hba.conf</code> for <code>local</code> connections:</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ psql
psql <span>(</span>12.3<span>)</span>
Type <span>"help"</span> <span>for</span> help.
</code></pre></div><h3 id="remote-machine">Remote machine</h3>
<p>Default URI syntax - you can connect like this:<br>
<code>psql postgresql://user:passwd@host:5432/dbame</code><br>
or by more common method:<br>
Connect to remote database from <code>psql</code> with connections details provided in parameters(it will ask for password because of <code>md5</code> method in <code>pg_hba.conf</code> for connections from <code>0.0.0.0/0</code>):</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab data<span>]</span>$ psql -h 10.128.0.2 -p <span>5432</span>
Password <span>for</span> user postgres:
psql <span>(</span>12.3<span>)</span>
Type <span>"help"</span> <span>for</span> help.

postgres<span>=</span><span>##</span>
</code></pre></div><p>We can also use parameter <code>-U</code> to specify username different than OS username we currently are using.</p>
<p>Also all this parameters can be taken from shell variables which names are self descriptive - if we set all of them we can just use plain <code>psql</code> command to connect:</p>
<ul>
<li><code>PGHOST</code></li>
<li><code>PGPORT</code></li>
<li><code>PGDATABASE</code></li>
<li><code>PGUSER</code></li>
<li><code>PGPASSWORD</code></li>
</ul>
<h3 id="check-connected-database">Check connected database</h3>
<div><pre><code data-lang="bash">postgres<span>=</span><span>## select current_database();</span>
 current_database
------------------
 postgres
<span>(</span><span>1</span> row<span>)</span>
</code></pre></div><h3 id="check-current-user">Check current user</h3>
<div><pre><code data-lang="bash">postgres<span>=</span><span>## select current_user;</span>
 current_user
--------------
 postgres
<span>(</span><span>1</span> row<span>)</span>
</code></pre></div><h3 id="check-ip-and-port-used-for-connection">Check IP and port used for connection</h3>
<div><pre><code data-lang="bash">postgres<span>=</span><span>## select inet_server_addr(), inet_server_port();</span>
 inet_server_addr | inet_server_port
------------------+------------------
 10.128.0.2       |             <span>5432</span>
<span>(</span><span>1</span> row<span>)</span>
</code></pre></div><h3 id="check-postgresql-version">Check PostgreSQL version</h3>
<div><pre><code data-lang="bash">postgres<span>=</span><span>## select version();</span>
                                                version                                                 
--------------------------------------------------------------------------------------------------------
 PostgreSQL 12.3 on x86_64-pc-linux-gnu, compiled by gcc <span>(</span>GCC<span>)</span> 8.3.1 <span>20191121</span> <span>(</span>Red Hat 8.3.1-5<span>)</span>, 64-bit
<span>(</span><span>1</span> row<span>)</span>
</code></pre></div><h3 id="check-connection-info">Check connection info</h3>
<div><pre><code data-lang="bash">postgres<span>=</span><span>## \conninfo</span>
You are connected to database <span>"postgres"</span> as user <span>"postgres"</span> on host <span>"10.128.0.2"</span> at port <span>"5432"</span>.
</code></pre></div><h2 id="executing-commands-from-shell">Executing commands from shell</h2>
<h3 id="execute-single-command-from-shell">Execute single command from shell</h3>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ psql -c <span>"select current_time"</span>
    current_time    
--------------------
 14:09:19.854598+00
<span>(</span><span>1</span> row<span>)</span>
</code></pre></div><h3 id="exacute-sql-script-from-shell">Exacute sql script from shell</h3>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ psql -f create_user.sql
CREATE ROLE
CREATE ROLE
CREATE ROLE
</code></pre></div><h3 id="combine-single-command-with-sql-script-from-shell">Combine single command with sql script from shell</h3>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ psql -c <span>"select current_time"</span> -f create_user.sql -c <span>"select current_time"</span>
    current_time    
--------------------
 14:14:26.922453+00
<span>(</span><span>1</span> row<span>)</span>

CREATE ROLE
CREATE ROLE
CREATE ROLE
    current_time    
--------------------
 14:14:26.926545+00
<span>(</span><span>1</span> row<span>)</span>
</code></pre></div>
<h3 id="check-all-available-metacommands">Check all available metacommands</h3>
<p>Do it yourself to see all available commands - output trimmed to important ones!</p>
<div><pre><code data-lang="bash">postgres<span>=</span><span>## \?</span>
General
  <span>\c</span>opyright             show PostgreSQL usage and distribution terms
  <span>\c</span>rosstabview <span>[</span>COLUMNS<span>]</span> execute query and display results in crosstab
  <span>\e</span>rrverbose            show most recent error message at maximum verbosity
  <span>\g</span> <span>[</span>FILE<span>]</span> or ;         execute query <span>(</span>and send results to file or |pipe<span>)</span>
  <span>\g</span>desc                 describe result of query, without executing it
  <span>\g</span>exec                 execute query, <span>then</span> execute each value in its result
  <span>\g</span>set <span>[</span>PREFIX<span>]</span>         execute query and store results in psql variables
  <span>\g</span>x <span>[</span>FILE<span>]</span>             as <span>\g</span>, but forces expanded output mode
  <span>\q</span>                     quit psql
  <span>\w</span>atch <span>[</span>SEC<span>]</span>           execute query every SEC seconds

  Query Buffer
    <span>\e</span> <span>[</span>FILE<span>]</span> <span>[</span>LINE<span>]</span>       edit the query buffer <span>(</span>or file<span>)</span> with external editor
    <span>\e</span>f <span>[</span>FUNCNAME <span>[</span>LINE<span>]</span><span>]</span>  edit <span>function</span> definition with external editor
    <span>\e</span>v <span>[</span>VIEWNAME <span>[</span>LINE<span>]</span><span>]</span>  edit view definition with external editor
    <span>\p</span>                     show the contents of the query buffer
    <span>\r</span>                     reset <span>(</span>clear<span>)</span> the query buffer
    <span>\s</span> <span>[</span>FILE<span>]</span>              display history or save it to file
    <span>\w</span> FILE                write query buffer to file

</code></pre></div><h3 id="list-objects-in-psql">List objects in psql</h3>
<ul>
<li>\d[S+]          -       list tables, views, and sequences</li>
<li>\d[S+]  NAME     -      describe table, view, sequence, or index</li>
<li>\da[S]  [PATTERN] -     list aggregates</li>
<li>\dA[+]  [PATTERN]  -    list access methods</li>
<li>\db[+]  [PATTERN]   -   list tablespaces</li>
<li>\dc[S+] [PATTERN]    -  list conversions</li>
<li>\dC[+]  [PATTERN]     - list casts</li>
<li>\dd[S]  [PATTERN]     - show object descriptions not displayed elsewhere</li>
<li>\dD[S+] [PATTERN]     - list domains</li>
<li>\ddp    [PATTERN]     - list default privileges</li>
<li>\dE[S+] [PATTERN]     - list foreign tables</li>
<li>\det[+] [PATTERN]     - list …</li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://knowledgepill.it/posts/postgresql-basics-guide/">https://knowledgepill.it/posts/postgresql-basics-guide/</a></em></p>]]>
            </description>
            <link>https://knowledgepill.it/posts/postgresql-basics-guide/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888907</guid>
            <pubDate>Sun, 19 Jul 2020 12:33:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Worms Armageddon 3.8]]>
            </title>
            <description>
<![CDATA[
Score 405 | Comments 122 (<a href="https://news.ycombinator.com/item?id=23888870">thread link</a>) | @typh00n
<br/>
July 19, 2020 | https://worms2d.info/Worms_Armageddon_3.8_Features | <a href="https://web.archive.org/web/*/https://worms2d.info/Worms_Armageddon_3.8_Features">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="mw-content-text" lang="en" dir="ltr">
<center><div><div><div><p><a href="https://wormtube.worms2d.info/vids/WormsArmageddon-TrailerVideo2020-SteamReady.mp4"><img alt="" src="https://worms2d.info/images/8/82/38_trailer_screenshot.jpg" width="640" height="360"></a></p><div><p>Click above to watch the new trailer in full quality!</p></div></div></div></div></center>
<div>
<p>Worms Armageddon version 3.8 is finally released! It's been a long time coming, and we hope you'll enjoy all the features, changes, and bug fixes that made it into this community-made update. To whet your appetite, here are some change highlights:
</p>
<div><div><p><a href="https://worms2d.info/File:ESOButton2.png"><img alt="ESOButton2.png" src="https://worms2d.info/images/0/04/ESOButton2.png" width="146" height="82"></a></p></div></div>
<h3><span id="Extended_Scheme_Options">Extended Scheme Options</span></h3>
<p>Customise your matches in weird and wonderful ways with over 70 new scheme options. Manipulate physics, fire more than one weapon in a turn, have worms bounce around the landscape, or choose from a huge array of other tweaks in order to perfect your scheme. These options are only for the seasoned Worms veteran, and as such you will need to have completed the Single Player mode and unlocked The Full Wormage in order to access them.</p>
<br><center><a href="https://worms2d.info/File:ESO1.gif" title="inline"><img alt="inline" src="https://worms2d.info/images/9/91/ESO1.gif" width="180" height="180"></a> <a href="https://worms2d.info/File:ESO2.gif" title="inline"><img alt="inline" src="https://worms2d.info/images/d/df/ESO2.gif" width="180" height="180"></a> <a href="https://worms2d.info/File:ESO3.gif" title="inline"><img alt="inline" src="https://worms2d.info/images/5/59/ESO3.gif" width="180" height="180"></a> <a href="https://worms2d.info/File:ESO4.gif" title="inline"><img alt="inline" src="https://worms2d.info/images/4/45/ESO4.gif" width="180" height="180"></a></center>
<p>For those who used the RubberWorm module in older versions of the game, all features formerly accessed through that module are now hiding behind the shiny new Extended Scheme Options star button. Relive your past glories — all your old RubberWorm recorded games can be played back with no additional software required.
</p>

<div><div><p><a href="https://worms2d.info/File:Tweening.gif"><img alt="" src="https://worms2d.info/images/5/53/Tweening.gif" width="300" height="240"></a></p><div><p>Demonstration of Tweening</p></div></div></div>
<h3><span id="Tweening">Tweening</span></h3>
<p>Worms Armageddon has never looked this smooth! In older versions, in-game movement and animations would be capped at the update rate of the game engine: 50 frames per second. In 3.8 though, experience the full silky smoothness of whatever frame rate your hardware can support — movement is now visually interpolated between game engine frames as your worms and weapons glide majestically through the air. Watch a recorded game in slow motion and you'll never want to go back!</p>
<div><div><p><a href="https://worms2d.info/File:Window.png"><img alt="" src="https://worms2d.info/images/d/dc/Window.png" width="300" height="237"></a></p><div><p>Demonstration of windowed mode</p></div></div></div>
<h3><span id="Windowed_Mode">Windowed Mode</span></h3>
<p>Worms Armageddon can now be played in a window! Awkward screen resolution changes and game capture issues are a thing of the past if you enable this feature in Advanced Settings.</p>
<div><div><p><a href="https://worms2d.info/File:Streaming-mode.png"><img alt="" src="https://worms2d.info/images/6/66/Streaming-mode.png" width="211" height="85"></a></p><div><p>Demonstration of Streaming Mode IP address hiding</p></div></div></div>
<h3><span id="Streamers.27_Heaven">Streamers' Heaven</span></h3>
<p>Along with Windowed Mode, a couple of additional options have been added specifically for streamers to make their lives easier. Audio can now be allowed to continue playing when the game window loses focus, and the new Streaming Mode will prevent IP addresses from being displayed when hosting or joining an online game (see the Update Documentation for more information).</p>
<div><div><p><a href="https://worms2d.info/File:Cpu-teams.png"><img alt="" src="https://worms2d.info/images/4/4d/Cpu-teams.png" width="252" height="151"></a></p><div><p>Demonstration of CPU teams being added to an online game</p></div></div></div>
<h3><span id="CPU_Teams_at_Large">CPU Teams at Large</span></h3>
<p>CPU teams can now be added to online games. They're always uniquely distinguishable from player teams to prevent shenanigans, but you can now finally team up with your friends to take down a swarm of those computer-controlled invertebrates. Or if you'd just like to watch a relaxing game while you work, matches containing only CPU teams may now also be started, both online and offline.</p>
<div><div><p><a href="https://worms2d.info/File:Smapshot.png"><img alt="" src="https://worms2d.info/images/7/71/Smapshot.png" width="300" height="225"></a></p></div></div>
<h3><span id="Mapshot">Mapshot</span></h3>
<p>Have you ever wrought a beautiful trail of destruction on your Worms Armageddon landscape and wished you could play a brand new game on the land that remained? Or have you ever been interrupted in the middle of a game and wished you could extract the remaining landscape to help you set up a rematch? Well, now you can! Simply type <b>/map</b> into the in-game chat or press <b>Alt + Pause</b> at any time to save a snapshot of the current state of the map, allowing you to continue your rampage from where you left off!</p>
<div><div><p><a href="https://worms2d.info/File:Wa95-2.png"><img alt="" src="https://worms2d.info/images/a/a0/Wa95-2.png" width="300" height="225"></a></p><div><p>Worms Armageddon running in Windows 95 (on real hardware!)</p></div></div></div>
<h3><span id="Compatibility">Compatibility</span></h3>
<p>Much effort has been expended in allowing everyone, no matter their hardware, to have a good Worms Armageddon experience. Further tweaks have been made to improve the experience of Windows 10 users. Worms Armageddon now runs well under Wine or Proton on Linux. A new OpenGL renderer has been added which, depending on your hardware, might outperform the other renderers. Windowed mode means there's no more need for your hardware to support specific screen resolutions. And for those into retro computing, Worms Armageddon has been seen to once again run on systems of yore...</p>
<div><div><p><a href="https://worms2d.info/File:Languages.png"><img alt="" src="https://worms2d.info/images/b/b5/Languages.png" width="222" height="120"></a></p><div><p>Sample of languages supported by Worms Armageddon's volunteer translators</p></div></div></div>
<h3><span id="Translations">Translations</span></h3>
<p>Through the tireless efforts of volunteer translators, Worms Armageddon is now fully translated to seven languages (🇫🇮 Finnish, 🇫🇷 French, 🇩🇪 German, 🇵🇹🇧🇷 Portuguese, 🇷🇺 Russian, 🇪🇸 Spanish and 🇸🇪 Swedish), with four more languages with partial translations on the way. The Update Documentation (Readme) now has translations to Finnish, French, and Spanish!</p>
<h3><span id="Fixes_and_More">Fixes and More</span></h3>
<p><a href="https://worms2d.info/File:Moon.png" title="Memey moon face"><img alt="Memey moon face" src="https://worms2d.info/images/4/47/Moon.png" width="100" height="123"></a></p>
<p>Over seven years in development, this update's release notes list consists of:
</p>
<ul><li> 370 fixes,</li>
<li> 45 changes, and</li>
<li> 61 new features.</li></ul>
<p>Our thanks go to all the hardworking members of the Worms Armageddon community who helped make this update a reality. And for the full list of what's new, check the Update Documentation (included with this update) or <a href="https://www.tus-wa.com/forums/announcements/worms-armageddon-v3-8-released-32795/">developer Deadcode's thread</a> on fan-site TUS!
</p>
</div>

<!-- 
NewPP limit report
Cached time: 20200722081828
Cache expiry: 86400
Dynamic content: false
CPU time usage: 0.023 seconds
Real time usage: 0.029 seconds
Preprocessor visited node count: 88/1000000
Preprocessor generated node count: 260/1000000
Post‐expand include size: 354/2097152 bytes
Template argument size: 60/2097152 bytes
Highest expansion depth: 4/40
Expensive parser function count: 0/100
-->

<!-- 
Transclusion expansion time report (%,ms,calls,template)
100.00%    3.025      1 - -total
 35.34%    1.069      1 - Template:ParentArticle
 30.93%    0.936      9 - Template:Clear
-->

<!-- Saved in parser cache with key wkb-mw_:pcache:idhash:5655-0!*!0!!*!5!* and timestamp 20200722081828 and revision id 26331
 -->
</div></div>]]>
            </description>
            <link>https://worms2d.info/Worms_Armageddon_3.8_Features</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888870</guid>
            <pubDate>Sun, 19 Jul 2020 12:25:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Tired of note-taking apps]]>
            </title>
            <description>
<![CDATA[
Score 429 | Comments 416 (<a href="https://news.ycombinator.com/item?id=23888799">thread link</a>) | @akkshu92
<br/>
July 19, 2020 | https://akkshaya.blog/2020/07/19/note-taking/ | <a href="https://web.archive.org/web/*/https://akkshaya.blog/2020/07/19/note-taking/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1052">
		<!-- .entry-header -->

	<div>
		
<p>I’m tired of note-taking apps. </p>



<p>It’s not because of limited choices. But it’s the other way around. There are so many note-taking apps you could try but end up sticking to none. At least, that’s my story. It’s a perfect example of&nbsp;<strong>the paradox of choice.</strong></p>



<p>I used to wonder why people keep building so many ‘note-taking’ apps when the market is already crowded with choices. Then I figured a few reasons why.</p>



<ul><li><strong>the market size</strong>: the global <a href="https://www.verifiedmarketresearch.com/product/note-making-management-software-market/#:~:text=According%20to%20Verified%20Market%20Research,5.32%25%20from%202019%20to%202026." target="_blank" rel="noreferrer noopener">note-taking management software market</a> is estimated to reach&nbsp;<strong>$1.35 billion </strong>by<strong> 2026</strong>, growing at a&nbsp;<strong>CAGR </strong>of<strong> 5.32% </strong>from<strong> 2019 to 2026</strong></li><li><strong>greater scope for innovation:&nbsp;</strong>eg., be it creating a task list, a roadmap, or a design repository, Notion can handle it all</li><li><strong>lack of satisfaction:&nbsp;</strong>it’s noted that people always use a combination of note-taking apps and hardly stick to one for a long time</li></ul>



<p>Despite such heavy competition, apps like Notion, Google Keep, OneNote, Evernote, etc. have managed to earn a place. People use these apps for</p>



<ul><li>the ecosystem. eg., Google Keep, Microsoft OneNote</li><li>the neat user experience., eg. Bear etc.</li><li>creating a disciplined way of taking notes. eg., Notion, Roam Research</li></ul>



<p>I’ve tried them all. But none of these apps have turned me into a ‘<strong>repeat user</strong>.’ </p>



<p>After battling with so many apps only to feel guilty for not having the discipline to consistently use them, I’ve finally resorted to the most personal and easy alternative ⁠—&nbsp;<strong>writing things down</strong>.</p>



<p>I’m familiar with writing in a notebook since my childhood. It’s not new to me, and it absolutely doesn’t require any learning curve.</p>



<h3>The reasons why I find writing things down useful</h3>



<ul><li>absolute focus and the ability to think through the points I’m writing</li><li>gives a chance to remember what I’m writing</li><li>no way to copy-paste stuff as it is, and that means taking notes in a way I understand</li><li>easy to switch between formats eg., flowchart, mind map, Venn diagram, etc</li><li>helps me stay in touch with my handwriting</li></ul>



<p>Of course, everything has its downsides, and writing things down is no exception here. <br>For example, I will not be able to</p>



<ul><li>add screenshots/images, links, etc</li><li>easily search for content as there’s no ‘search bar’</li></ul>



<p>And maybe there’s more to the list I’m not talking about. </p>



<p>All I can say for sure is, based on my usage behavior, I’m okay missing out on these features. I can always save links to <a href="https://app.getpocket.com/" target="_blank" rel="noreferrer noopener">Pocket</a> for future reference, and take pictures of my notes to share with friends.</p>



<p>So if you ask me if I’d try a beautiful, innovative note-taking app that’s much better than the apps I’ve used so far, my answer is,&nbsp;<em>“<strong>Why not</strong>! <strong>I’d definitely give it a shot</strong></em>.”</p>



<p>But my greatest worry is if I’d continue using it.</p>



<p><strong>Note: </strong>If my opinion on note-taking apps changes over time, I’d be happy to update this post with a “And the hero finally arrived!” heading to talk about the app that helped change my mind. 🤡</p>
			</div><!-- .entry-content -->

	<!-- .entry-footer -->

	</article></div>]]>
            </description>
            <link>https://akkshaya.blog/2020/07/19/note-taking/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888799</guid>
            <pubDate>Sun, 19 Jul 2020 12:12:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Things Unexpectedly Named After People]]>
            </title>
            <description>
<![CDATA[
Score 393 | Comments 258 (<a href="https://news.ycombinator.com/item?id=23888725">thread link</a>) | @vortex_ape
<br/>
July 19, 2020 | https://notes.rolandcrosby.com/posts/unexpectedly-eponymous/ | <a href="https://web.archive.org/web/*/https://notes.rolandcrosby.com/posts/unexpectedly-eponymous/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            
            <p><h5>July 19, 2020</h5></p>
        </div></div>]]>
            </description>
            <link>https://notes.rolandcrosby.com/posts/unexpectedly-eponymous/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888725</guid>
            <pubDate>Sun, 19 Jul 2020 11:58:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Inheritance in C using structure composition]]>
            </title>
            <description>
<![CDATA[
Score 57 | Comments 28 (<a href="https://news.ycombinator.com/item?id=23888677">thread link</a>) | @arpitbbhayani
<br/>
July 19, 2020 | https://arpitbhayani.me/blogs/inheritance-c | <a href="https://web.archive.org/web/*/https://arpitbhayani.me/blogs/inheritance-c">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>C language does not support inheritance however it does support Structure Compositions which can be tweaked to serve use-cases requiring parent-child relationships. In this article, we find out how Structure Compositions help us emulate inheritance in C and keep our code extensible. We will also find how it powers two of the most important things to have ever been invented in the field of computer science.</p>

<p>Structure Composition is when we put one structure within another, not through its pointer but as a native member - something like this</p>
<pre><code>


<span><span>struct</span> <span>list_head</span> {</span>
	<span><span>struct</span> <span>list_head</span> *<span>next</span>;</span> 
	<span><span>struct</span> <span>list_head</span> *<span>prev</span>;</span> 
};


<span><span>struct</span> <span>list_int</span> {</span>
	<span><span>struct</span> <span>list_head</span> <span>list</span>;</span>  
	<span>int</span> value;              
};


<span><span>struct</span> <span>list_str</span> {</span>
	<span><span>struct</span> <span>list_head</span> <span>list</span>;</span>  
	<span>char</span> * str;             
};
</code></pre>
<p>In the example above, we define a node of a linked list using structure composition. Usually, a linked list node has 3 members - two pointers to adjacent nodes (next and previous) and a third one could either be the data or a pointer to it.  The defining factor of a linked list is the two pointers that logically form a chain of nodes. To keep things abstract we create a struct named <code>list_head</code> which holds these two pointers  <code>next</code> and <code>prev</code> and omits the specifics i.e. data.</p>
<p>Using <code>list_head</code> structure, if we were to define a node of a linked list holding an integer value we could create another struct, named <code>list_int</code> that holds a member of type <code>list_head</code> and an integer value <code>value</code>. The next and previous pointers are brought into this struct through <code>list_head list</code> and could be referred to as <code>list.next</code> and <code>list.prev</code>.</p>
<blockquote>
<p>There is a very genuine reason for picking such weird names for a linked list node and members of structures; the reason to do so will be cleared in the later sections of this essay.</p>
</blockquote>
<p>Because of the above structure definition, building a linked list node holding of any type becomes a breeze. For example, a node holding string could be quickly defined as a struct <code>list_str</code> having <code>list_head</code> and a <code>char *</code>. This ability to extend <code>list_head</code> and build a node holding data of any type and any specifics make low-level code simple, uniform, and extensible.</p>
<h2>Memory Representation of <code>list_int</code></h2>
<p>Structures in C are not padded and they do not even hold any meta information, not even for the member names; hence during allocation, they are allocated the space just enough to hold the actual data.</p>
<p><img src="https://user-images.githubusercontent.com/4745789/83953834-694a6a00-a861-11ea-8ff7-fa69af6af7d6.png" alt="https://user-images.githubusercontent.com/4745789/83953834-694a6a00-a861-11ea-8ff7-fa69af6af7d6.png"></p>
<p>In the illustration above we see how members of <code>list_int</code> are mapped on the allocated space - required by its individual members. It is allocated a contiguous space of 12 bytes - 4 bytes for each of the two pointers and another 4 bytes for the integer value. The contiguity of space allocation and order of members during allocation could be verified by printing out their addresses as shown below.</p>
<pre><code><span><span>void</span> <span>print_addrs</span><span>()</span> </span>{
    
    <span><span>struct</span> <span>list_int</span> *<span>ll</span> = <span>new_list_int</span>(41434);</span>

    
    <span>printf</span>(<span>"%p: head\n"</span>,             head);
    <span>printf</span>(<span>"%p: head-&gt;list.next\n"</span>,  &amp;((head-&gt;<span>list</span>).next));
    <span>printf</span>(<span>"%p: head-&gt;list.prev\n"</span>,  &amp;((head-&gt;<span>list</span>).prev));
    <span>printf</span>(<span>"%p: head-&gt;value\n"</span>,      &amp;(head-&gt;value));
}

~ $ make &amp;&amp; ./a.out
<span>0x4058f0</span>: head
<span>0x4058f0</span>: head-&gt;<span>list</span>.next
<span>0x4058f4</span>: head-&gt;<span>list</span>.prev
<span>0x4058f8</span>: head-&gt;value
</code></pre>
<p>We clearly see all the 3 members, occupying 12 bytes contiguous memory segments in order of their definition within the struct.</p>
<blockquote>
<p>The above code was executed on a machine where the size of integer and pointers were 4 bytes each. The results might differ depending on the machine and CPU architecture.</p>
</blockquote>
<h2>Casting pointers pointing to struct</h2>
<p>In C language, when a pointer to a struct is cast to a pointer to another struct, the engine maps the individual members of a target struct type, depending on their order and offsets, on to the slice of memory of the source struct instance.</p>
<p>When we cast <code>list_int *</code> into <code>list_head *</code>, the engine maps the space required by target type i.e. <code>list_head</code> on space occupied by <code>list_int</code>. This means it maps the 8 bytes required by <code>list_head</code> on the first 8 bytes occupied by <code>list_int</code> instance. Going by the memory representation discussed above, we find that the first 8 bytes of <code>list_int</code> are in fact <code>list_head</code>, and hence casting <code>list_int *</code> to <code>list_head *</code> is effectively just referencing the <code>list_head</code> member of <code>list_int</code> through a new variable.</p>
<p><img src="https://user-images.githubusercontent.com/4745789/83943610-2e254800-a81b-11ea-8b25-056e1b1df85e.png" alt="https://user-images.githubusercontent.com/4745789/83943610-2e254800-a81b-11ea-8b25-056e1b1df85e.png"></p>
<p>This effectively builds a parent-child relationship between the two structs where we can safely typecast a child <code>list_int</code> to its parent <code>list_head</code>.</p>
<blockquote>
<p>It is important to note here that the parent-child relationship is established only because the first member of <code>list_int</code> is of type <code>list_head</code>. it would not have worked if we change the order of members in <code>list_int</code>.</p>
</blockquote>

<p>As established above, by putting one struct within another as its first element we are effectively creating a parent-child relationship between the two. Since this gives us an ability to safely typecast child to its parent we can define functions that accept a pointer to parent struct as an argument and perform operations that do not really require to deal with specifics. This allows us to <strong>NOT</strong> rewrite the functional logic for every child extensions and thus avoid redundant code.</p>
<p>From the context we have set up, say we want to write a function that adds a node between the two in a linked list. The core logic to perform this operation does not really need to deal with any specifics all it takes is a few pointer manipulations of <code>next</code> and <code>prev</code>. Hence, we could just define the function accepting arguments of type <code>list_head *</code>  and write the function as</p>
<pre><code>
<span>static</span> <span>void</span> __list_add(struct list_head *<span>new</span>,
                       struct list_head *prev,
                       struct list_head *next)
{
    next-&gt;prev = <span>new</span>;
    <span>new</span>-&gt;next = next;
    <span>new</span>-&gt;prev = prev;
    prev-&gt;next = <span>new</span>;
}
</code></pre>
<p>Since we can safely typecase <code>list_int *</code> and <code>list_str *</code> to <code>list_head *</code> we can pass any of the specific implementations the function <code>__list_add</code> and it would still add the node between the other two seamlessly.</p>
<p>Since the core operations on linked lists only require pointer manipulations, we can define these operations as functions accepting <code>list_head *</code> instead of specific types like <code>list_int *</code>.  Thus we need not write similar functions for specifics. A function to delete a node could be written as</p>
<pre><code>
<span>static</span> <span>inline</span> <span>void</span> __list_del(struct list_head * prev, struct list_head * next)
{
    next-&gt;prev = prev;
    prev-&gt;next = next;
}
</code></pre>
<p>Other linked list utilities like <em>adding a node to tail</em>, <em>swapping nodes</em>, <em>splicing the list</em>, <em>rotating the list</em>, etc only require manipulations of <code>next</code> and <code>prev</code> pointers. Hence they could also be written in a very similar way i.e accepting <code>list_head *</code> and thus eliminating the need to reimplement function logic for every single child implementation.</p>
<p>This behavior is very similar to how inheritance in modern OOP languages, like Python and Java, work where the child is allowed to invoke any parent function.</p>

<p>There are a ton of practical usage of using Structure Compositions but the most famous ones are</p>
<h2>Linux Kernel</h2>
<p>In order to keep things abstract and extensible, Linux Kernel uses Structure Composition at several places. One of the most important places where it uses composition is for managing and maintaining Linked Lists, exactly how we saw things above. The struct definitions and code snippets are taken as-is from the <a href="https://elixir.bootlin.com/linux/latest/source/include/linux/list.h">Kernel's source code</a>, and hence the structure and variable names look different than usual.</p>
<h2>Python Type and Object Hierarchy</h2>
<p>Python, one of the most important languages in today's world, uses Structure Composition to build Type Hierarchy. Python defines a root structure called <code>PyObject</code> which holds reference count, defining the number of places from which the object is referenced - and object type - determining the type of the object i.e. <code>int</code>, <code>str</code>, <code>list</code>, <code>dict</code>, etc.</p>
<pre><code><span>typedef</span> <span><span>struct</span> _<span>object</span> {</span>
    Py_ssize_t     ob_refcnt;  
    PyTypeObject   *ob_type;   
} PyObject;
</code></pre>
<p>Since Python wants these fields to be present in every single object that is created during runtime, it uses structure composition to ensure that objects like integers, floats, string, etc put <code>PyObject</code> as their first element and thus establishing a parent-child relationship. A Float object in Python is defined as</p>
<pre><code><span>#<span>define</span> PyObject_HEAD PyObject ob_base;</span>

<span>typedef</span> <span><span>struct</span> {</span>
    PyObject_HEAD
    <span>double</span> ob_fval;    
} PyFloatObject;
</code></pre>
<p>Now writing utility functions that increments and decrements references count on every access of any object could be written as just a single function accepting <code>PyObject *</code> as shown below</p>
<pre><code><span>static</span> <span>inline</span> <span>void</span> _Py_INCREF(PyObject *op) {
    op-&gt;ob_refcnt++;
}
</code></pre>
<p>Thus we eradicate a need of rewriting <code>INCREF</code> for every single object type and just write it once for <code>PyObject</code> and it will work for every single Python object type that is extended through <code>PyObject</code>.</p>

<ul>
<li><a href="https://elixir.bootlin.com/linux/latest/source/include/linux/list.h">LinkedList in Linux Source Code</a></li>
<li><a href="https://docs.python.org/3/c-api/structures.html#c.PyObject">PyObject - Python Internals Documentation</a></li>
<li><a href="https://docs.python.org/3/c-api/float.html">PyFloatObject - Python Internals Documentation</a></li>
</ul>
</div></div><section><div><div><p><img src="https://arpitbhayani.me/static/img/arpit.jpg"></p>  <h2>
              500+ Signups
            </h2> <p>
              If you like what you read subscribe you can always subscribe to
              my newsletter and get the post delivered straight to your inbox.
              I write
              <a href="https://arpitbhayani.me/blogs">essays</a> on various
              engineering topics and share it through my weekly
              <a href="https://arpitbhayani.me/newsletter">newsletter</a> 👇
            </p> <br> </div></div></section></div>]]>
            </description>
            <link>https://arpitbhayani.me/blogs/inheritance-c</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888677</guid>
            <pubDate>Sun, 19 Jul 2020 11:48:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pipeline Design Patterns for Continuous Delivery]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 1 (<a href="https://news.ycombinator.com/item?id=23888341">thread link</a>) | @kiyanwang
<br/>
July 19, 2020 | https://www.singlestoneconsulting.com/blog/7-pipeline-design-patterns-for-continuous-delivery/ | <a href="https://web.archive.org/web/*/https://www.singlestoneconsulting.com/blog/7-pipeline-design-patterns-for-continuous-delivery/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-601"><div><p>Business agility is built on code agility. For modern digital products and services, the ability to release on demand quickly and safely is a real competitive business advantage.</p><p>Since 2004 we have designed, built, and deployed code pipelines to automate applications and infrastructure. We’re sharing seven pipeline design patterns that we’ve learned improve speed, agility, and quality while increasing autonomy, transparency, and maintainability.</p><h2><strong>Continuous Delivery</strong><strong> </strong></h2><p><a href="https://continuousdelivery.com/" target="_blank" rel="noreferrer noopener">Continuous Delivery</a> is “the ability to get changes of all types into the hands of users, safely and quickly in a sustainable way.” If you think about Continuous Delivery on an Agile vs. Effort matrix, it falls squarely between Continuous Integration and Continuous Deployment. Often these are collectively referred to as CI/CD.</p><div><figure><img src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.32.09-PM-1300x763.png" alt="What is continuous delivery" width="632" height="370" data-src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.32.09-PM-1300x763.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></figure></div><p>In the <a href="https://services.google.com/fh/files/misc/state-of-devops-2019.pdf" target="_blank" aria-label="undefined (opens in a new tab)" rel="noreferrer noopener">2019 State of the DevOps Report</a>, over 31,000 respondents reported the effectiveness of their development and delivery processes. The results between the elite and low performers is staggering. The elite performs had 200x more deployments and 100x faster speed to deployment <strong><em>while also</em></strong> being 2,600x faster to recover from incidents and 7x less likely to roll back releases.</p><p>This research shows that for elite performing organizations, speed and stability are not opposites! You can have both – in fact you need both – to gain real competitive advantages for your digital products and services.</p><h2><strong>Code Pipelines</strong></h2><p>Code pipelines are the primary technical artifacts of continuous delivery. Modern-day pipelines transform application and infrastructure source code into versioned packages deployable to any environment. By automating all the mundane tasks to build and deploy systems, teams are free to focus on value-added capabilities.</p><div><figure><img src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.31.55-PM-1300x627.png" alt="" width="653" height="316" srcset="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.31.55-PM-1300x627.png 1300w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.31.55-PM-660x318.png 660w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.31.55-PM-768x371.png 768w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.31.55-PM-1536x741.png 1536w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.31.55-PM.png 1886w" sizes="(max-width: 653px) 100vw, 653px" data-srcset="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.31.55-PM-1300x627.png 1300w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.31.55-PM-660x318.png 660w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.31.55-PM-768x371.png 768w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.31.55-PM-1536x741.png 1536w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.31.55-PM.png 1886w" data-src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.31.55-PM-1300x627.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></figure></div><p>While code pipelines have been around for nearly 20 years – <a href="https://en.wikipedia.org/wiki/CruiseControl" target="_blank" rel="noreferrer noopener">CruiseControl</a>, one of our early favorites, was first released in 2001 – they have evolved quite significantly over the years and really come into their own in the last few years.</p><p>Based on our work and observations from our customers, we have identified 7 pipeline design patterns that we see in many modern tech organizations.</p><h2><strong>Pipeline Design Patterns</strong></h2><h3>Pipeline Design Pattern #1: Pipelines as Code</h3><p><em>Pipeline logic is codified, stored alongside application or infrastructure code</em><em> and utilizes containerized runners. </em></p><p>State of the Art:</p><div><figure><img src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.04-PM.png" alt="Pipelines as code" width="256" height="349" data-src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.04-PM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></figure></div><ul><li>No GUI setup steps! Pipeline logic is managed like any other application code and subject to the same branching strategies and review processes.</li><li>Pipeline execution on containers allows your CI/CD platform to support many different workloads, while each workload can have its own build environment to meet its unique needs.</li><li>Container image provenance uses trusted Docker images for build environments.</li><li>CI runner configuration is automated, identical, and hands-free. CI runners can scale to meet demand and can be on standby during core hours to minimize delays.</li><li>Secrets are stored outside of the pipeline and their output is masked, leading to higher security.</li></ul><h3>Pipeline Design Pattern #2: Externalize Logic into Reusable Libraries</h3><p><em>Reusable libraries contain common pipeline logic</em><em> that is</em><em> referenc</em><em>e</em><em>able from pipeline code and </em><em>independently developed and tested.</em></p><div><figure><img src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.15-PM.png" alt="Externalize Logic into Reusable Libraries" width="242" height="369" srcset="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.15-PM.png 822w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.15-PM-432x660.png 432w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.15-PM-768x1173.png 768w" sizes="(max-width: 242px) 100vw, 242px" data-srcset="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.15-PM.png 822w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.15-PM-432x660.png 432w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.15-PM-768x1173.png 768w" data-src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.15-PM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></figure></div><p>State of the Art:</p><ul><li>Treat pipeline libraries as any other releasable software. They have their own repos, pipelines, and are unit tested with good release notes.</li><li>Pipelines call external tasks whenever possible using language-specific tools like Make, Rake, npm, Maven or others to simplify the pipeline and keeps the local and CI workflow identical.</li><li>Libraries are discoverable and have good documentation.</li></ul><h3>Pipeline Design Pattern #3: Separate Build and Deploy Pipelines</h3><p><em>Build and deploy pipelines should be logically separate</em><em>d</em><em>, </em><em>independently runnable</em><em> and triggered by </em><em>automated or manual events</em></p><div><figure><img src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.31-PM.png" alt="Separate Build and Deploy Pipelines " width="316" height="313" srcset="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.31-PM.png 1128w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.31-PM-660x653.png 660w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.31-PM-768x760.png 768w" sizes="(max-width: 316px) 100vw, 316px" data-srcset="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.31-PM.png 1128w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.31-PM-660x653.png 660w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.31-PM-768x760.png 768w" data-src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.31-PM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></figure></div><p>State of the Art:</p><ul><li>Build once, deploy many. Focus on the first build. It becomes an artifact that you can deploy many times.</li><li>Be environmentally agnostic. Without an environment-specific packages, and externalized environment-specific properties, the same build can run in any environment.</li><li>Package it all together. Everything – All source, including infrastructure code, should be combined, becoming a versioned package.</li></ul><h3>Pipeline Design Pattern #4: Trigger the Right Pipeline</h3><p><em>B</em><em>ranch</em><em> commits</em><em>, pull requests, and merges to the mainline can</em><em> all trigger different pipeline behavior, </em><em>optimized to</em><em> the team’s way of working.</em></p><div><figure><img src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.45-PM.png" alt="Trigger the Right Pipeline" width="351" height="262" srcset="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.45-PM.png 1134w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.45-PM-660x492.png 660w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.45-PM-768x573.png 768w" sizes="(max-width: 351px) 100vw, 351px" data-srcset="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.45-PM.png 1134w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.45-PM-660x492.png 660w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.45-PM-768x573.png 768w" data-src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.46.45-PM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></figure></div><p>State of the Art:</p><ul><li>Pushing a commit to an open pull request builds an Ephemeral Environment for testing.</li><li>Merges to the mainline are deployed to a non-production or demo environment displaying the latest integrated code</li><li>Pushing a new tag stages a production release.</li></ul><h3>Pipeline Design Pattern #5: Fast Team Feedback</h3><p><em>Every commit </em><em>automatically</em><em> triggers the right pipeline</em><em>, with build pipelines </em><em>especially </em><em>optimized for speed and quick </em><em>reporting</em><em> of any issues. </em></p><div><figure><img src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.47.38-PM.png" alt="Fast Team Feedback" width="350" height="350" srcset="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.47.38-PM.png 1146w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.47.38-PM-660x660.png 660w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.47.38-PM-768x769.png 768w" sizes="(max-width: 350px) 100vw, 350px" data-srcset="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.47.38-PM.png 1146w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.47.38-PM-660x660.png 660w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.47.38-PM-768x769.png 768w" data-src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.47.38-PM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></figure></div><p>State of the Art:</p><ul><li>Build pipelines use parallelization for non-interdependent jobs to increase speed.</li><li>Fast build pipelines only run the jobs that are necessary in a few minutes.</li><li>Each successful run produces a versioned package and static analysis results.</li><li>With omni-channel notifications, you can enable team notifications on pull request status in dashboards, chat channels, email, and other mediums.</li></ul><h3>Pipeline Design Pattern #6: Stable Internal Releases</h3><p><em>Only versioned </em><em>packages </em><em>produced</em><em> by the build pipeline are deployed</em><em> and these deployments are triggered by humans or automated events.</em></p><div><figure><img src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.45.11-PM.png" alt="Stable Internal Releases" width="320" height="250" srcset="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.45.11-PM.png 940w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.45.11-PM-660x514.png 660w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.45.11-PM-768x598.png 768w" sizes="(max-width: 320px) 100vw, 320px" data-srcset="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.45.11-PM.png 940w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.45.11-PM-660x514.png 660w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.45.11-PM-768x598.png 768w" data-src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.45.11-PM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></figure></div><p>State of the Art:</p><ul><li>Each code branch gets a complete ephemeral environment named for the branch that can easily be created or destroyed.</li><li>Each engineer can stand up and delete ephemeral environments at any time.</li><li>CI runners use cloud-native IAM capabilities with temporary permissions so they can assume roles and acquire the right permissions to complete their work.</li></ul><h3>Pipeline Design Pattern #7: Buttoned Up Product Releases</h3><p><em>Deploy tagged releases to production and automate the paperwork</em><em> but leave a paper trail. </em></p><p>State of the Art:</p><ul><li>Codified release gates and standardized release processes enables teams to release on demand.</li><li>Automated releases leave a transparent paper trail that’s auditable for governance and quality</li><li>Release gates can invoke external API’s and use the responses to decide whether to proceed with the release or halt.</li></ul><figure><img src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.44.56-PM-1300x254.png" alt="Buttoned Up Product Releases" srcset="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.44.56-PM-1300x254.png 1300w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.44.56-PM-660x129.png 660w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.44.56-PM-768x150.png 768w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.44.56-PM-1536x300.png 1536w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.44.56-PM.png 1814w" sizes="(max-width: 1300px) 100vw, 1300px" data-srcset="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.44.56-PM-1300x254.png 1300w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.44.56-PM-660x129.png 660w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.44.56-PM-768x150.png 768w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.44.56-PM-1536x300.png 1536w, https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.44.56-PM.png 1814w" data-src="https://www.singlestoneconsulting.com/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-12.44.56-PM-1300x254.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></figure><h2><strong>Challenges</strong></h2><p>These are the seven pipeline design patterns we’re increasingly seeing and using in our client engagements. While these present huge leaps forward in terms of speed and stability, code pipelines are not without their challenges.</p><p>Security is the biggest challenge we see, due to the complexity that comes with automating what has traditionally been human-centric processes. Pipeline complexity, team adoption, modernizing the change-advisory-board-centric culture, and automating databases are the other big challenges to work through. But it’s possible, we do this every day.</p><h2><strong>Summary</strong></h2><p>Business agility is built on code agility. For modern digital products and services, the ability to release on demand quickly and safely is a real competitive business advantage. Code pipelines, and these seven design patterns in particular, can help your organization make giant leaps forward in speed and stability and have your teams performing at the elite level.</p><p><a href="https://www.singlestoneconsulting.com/careers/" target="_blank" rel="noreferrer noopener">We’re hiring</a>! Come join our team.</p></div></article><p>Get notified when more posts like “<span>7 Pipeline Design Patterns for Continuous Delivery</span>” are available.</p></div>]]>
            </description>
            <link>https://www.singlestoneconsulting.com/blog/7-pipeline-design-patterns-for-continuous-delivery/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888341</guid>
            <pubDate>Sun, 19 Jul 2020 10:18:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Gradually Exit Twitter]]>
            </title>
            <description>
<![CDATA[
Score 62 | Comments 56 (<a href="https://news.ycombinator.com/item?id=23888215">thread link</a>) | @saadalem
<br/>
July 19, 2020 | https://balajis.com/how-to-gradually-exit-twitter/ | <a href="https://web.archive.org/web/*/https://balajis.com/how-to-gradually-exit-twitter/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            


                <figure>
                    <img srcset="https://balajis.com/content/images/size/w300/2020/07/gradually-exit-twitter.png 300w,
                                https://balajis.com/content/images/size/w600/2020/07/gradually-exit-twitter.png 600w,
                                https://balajis.com/content/images/size/w1200/2020/07/gradually-exit-twitter.png 1000w,
                                https://balajis.com/content/images/size/w2000/2020/07/gradually-exit-twitter.png 2000w" sizes="(max-width: 800px) 400px,
                            (max-width: 1170px) 1170px,
                                2000px" src="https://balajis.com/content/images/size/w2000/2020/07/gradually-exit-twitter.png" alt="How to Gradually Exit Twitter">
                </figure>
                <section>
                    <div>
                        <p>The <a href="https://twitter.com/TwitterSupport/status/1283591848729219073">hacks</a> are the last straw. We need to exit Twitter.</p><p>But you may be invested in Twitter! So what do you do? Here's one recipe for <em>gradual</em> exit.</p><ol><li>Step one is to set up an (optionally pseudonymous) newsletter at your <em>own</em> custom domain using the instructions <a href="https://balajis.com/set-up-a-paid-newsletter-at-your-own-domain/">here</a>. You can do this in an hour and maintain it for less than $30 per month. Then only tweet out posts from your site, using your domain name as a simple form of authentication.</li><li>Step two is to start building your membership list at your own site. If you want, help us review <a href="https://github.com/balajis/twitter-export">open source tools</a> to export your Twitter following to an email list and <a href="https://twitter.com/dwr/status/1284155565770498050">mass delete any DMs</a>.</li><li>And step three is to get off Twitter all together, and start building a decentralized media ecosystem that combines the sovereignty of individual domain names with the community of social networking. The v1 of that is a simple joint RSS feed that aggregates multiple independent newsletters, but we can get much more <a href="https://twitter.com/balajis/status/1280005821028397057">sophisticated</a> <a href="https://dci.mit.edu/decentralizedweb">with</a> <a href="https://hackernoon.com/a-state-of-the-art-of-decentralized-web-part-4-212732f74894">tech</a> <a href="https://gun.eco/docs/dWeb-The-Decentralized-Web">for</a> <a href="https://ens.domains/">the</a> <a href="https://handshake.org/">decentralized</a> <a href="https://ipfs.io/">web</a>. It may turn out that what we build helps reform Twitter itself, but at a minimum it will present an alternative.</li></ol><p>Before we get there though, let's go through the problems with Twitter.</p><h2 id="the-problems-with-twitter">The Problems with Twitter</h2><p>It's not just the hack. There are several overlapping issues with Twitter around security, authentication, pseudonymity, distribution, incentives, and culture.</p><ul><li><em>Twitter is insecure</em>. It was a mistake to think that personal security could compensate for the fundamental flaw of centralized web services: anyone who gains control over twitter.com gains control over your account. We knew this already from the <a href="http://archive.is/Bq5qi">2017 incident</a> and the <a href="http://archive.is/wip/scJMn">Jack Hack</a>. But now the most prominent people and companies in the world (Bezos, Gates, Zuckerberg, Musk, Apple, Uber, among others) have been <a href="https://twitter.com/TwitterSupport/status/1283591848729219073">openly and publicly hacked</a>. Tomorrow you may be the victim. And unlike these worthies, you may be <em>silently</em> and <em>privately</em> hacked. Perhaps you already have been.</li><li><em>Twitter is not authenticated</em>. It was a mistake to trust Twitter's centralized verification process to validate everything posted to the service. There weren't any <a href="https://www.investopedia.com/terms/p/private-key.asp#:~:text=A%20digital%20wallet%20stores%20the,to%20use%20the%20private%20key.">digital signatures</a> to help establish whether the messages were really coming from a user-controlled device, or faked by a central server.</li><li><em>Twitter is not fully pseudonymous</em>. It was a mistake to use our real names everywhere online, particularly on Twitter. It made us vulnerable to personal and physical attacks. Social media mobs can swarm your real name online and cancel you in real life. Doxxing can heighten the danger by making threats <a href="https://blog.lopp.net/reflections-upon-a-swatting/">materialize in</a> <a href="https://github.com/jlopp/physical-bitcoin-attacks/blob/master/README.md">meatspace</a>. Pseudonymity is a better default, especially now that we can <a href="http://www.marknagelberg.com/notes-on-the-pseudonymous-economy-balaji-srinivasan/">earn under a pseudonym</a>.</li><li><em>Twitter is a distribution chokepoint</em>. It was a mistake to outsource our distribution to Twitter. Twitter users don't fully control their reach, brand, or monetization. You can be <a href="https://www.searchenginejournal.com/twitter-reveals-how-it-ranks-tweets-in-search-results/263869/">downranked</a> or <a href="http://archive.is/mhgyB">deplatformed</a> at any time, just like you can on YouTube and other centralized platforms. Owning your <a href="https://support.substack.com/hc/en-us/articles/360037465992-How-do-I-export-my-email-or-subscriber-list-">own</a> <a href="http://locals.com/">email</a> <a href="https://ghost.org/members">list</a> is a far better alternative.</li><li><em>Twitter doesn't create wealth</em>. It was a mistake to have so many people spending so much time on something that <a href="https://twitter.com/paulg/status/1271834488998440961">arguably</a> destroys more wealth than it creates. Because by default, you don't create wealth on Twitter. You may create wealth <em>for</em> Twitter, or for the media corporations whose links are circulated, but not for yourself or for others. There's no sense of economic alignment with other users, no sense of mutual obligation.</li><li><em>Twitter turns society into Twitter</em>. It was a mistake to let Twitter set up the incentives for our society. It's not good for the world when the one thing every prominent person knows how to do is <a href="https://twitter.com/balajis/status/1272653587617701888">fight</a> each other on Twitter for likes and followers. It has converted society into a zero-sum status game played by elites with real consequences. Every day, blue checks compete for a finite pie of attention with ever more sensational posts. There is instant feedback on what is popular, but none on what is true. The lack of economics, the presence of real names, the toxic culture, and the UX itself all encourage constant fighting.</li></ul><p>Now, let me qualify these comments. I personally have gotten <a href="https://twitter.com/paulg/status/1271834488998440961">enormous value</a> out of Twitter, despite all the points above. After all, you only critique the services you use!</p><p>And to be clear, Jack Dorsey is a phenomenal entrepreneur. I respect everything he's done and the team that he's built. It's hard to build one multibillion dollar company, let alone <a href="http://archive.is/wip/bu6hJ">two</a>. He's done immense good for the world overall, both through his <a href="https://twitter.com/jack/status/1247616214769086465">personal generosity</a> and his startup vehicles. And I also don't wish any Twitter employee ill, as many of them are truly great people and I know how hard this stuff is.</p><p>Moreover, even after this hack, Twitter will likely continue to be an important internet battlefield for quite sometime. It's just too easy, too convenient. And it may turn out that whatever is built to exit Twitter will end up interfacing with Twitter itself, via their <a href="https://twitter.com/bluesky">decentralized protocol</a>, if only as a form of training wheels to help people get off. Hopefully this can be done in a win-win way, such that Twitter and its employees are incentivized to allow people to gradually exit and fix the issues on the centralized service.</p><p>But it's past time for us to start building something better, that keeps the good aspects of Twitter (the serendipitous learning, the fascinating <a href="https://twitter.com/balajis/status/1214585024772788224">people</a>) but filters the bad. That may mean exiting to our own domains and then networking them into decentralized media.</p><h2 id="decentralization-starts-with-a-domain">Decentralization Starts with a Domain</h2><figure><img src="https://balajis.com/content/images/2020/07/how-to-gradually-exit-twitter--1-.png" alt="" srcset="https://balajis.com/content/images/size/w600/2020/07/how-to-gradually-exit-twitter--1-.png 600w, https://balajis.com/content/images/size/w1000/2020/07/how-to-gradually-exit-twitter--1-.png 1000w, https://balajis.com/content/images/size/w1600/2020/07/how-to-gradually-exit-twitter--1-.png 1600w, https://balajis.com/content/images/2020/07/how-to-gradually-exit-twitter--1-.png 2000w" sizes="(min-width: 720px) 720px"><figcaption>Can we network together websites into decentralized media with something deeper than links?</figcaption></figure><p>I may be wrong about this, but I don't think a "better Twitter" starts with a service which is simply a clone of Twitter except with a different community. I think it starts with a community of independent (and independently monetizable) domains that we network together in novel ways, to build <em>decentralized media</em>.</p><p>Towards this end, the first step in gradually exiting Twitter is to set up your own paid newsletter at a custom domain (<a href="https://balajis.com/set-up-a-paid-newsletter-at-your-own-domain/">instructions here</a>) and restrict yourself mainly to tweeting out posts. This does require more effort than writing 280 characters! But that's why it's good. Effort helps restore your attention span and improves average post quality. And in addition to the other benefits, the link to your domain doubles as a simple alternative to a <a href="https://www.investopedia.com/terms/p/private-key.asp#:~:text=A%20digital%20wallet%20stores%20the,to%20use%20the%20private%20key.">digital signature</a>, a way for people to see that the content is authentically yours. </p><p>The medium-term step is to develop <a href="https://github.com/balajis/twitter-export">tools</a> to migrate your followers off Twitter to an email list, and to mass delete all DMs as a precautionary step. If you want to help review some existing tools towards this end, we're working on it <a href="https://github.com/balajis/twitter-export/issues/1">here</a>.</p><p>The long-term step is to start addressing the issues with Twitter by knitting these individual domains together into an open source decentralized media ecosystem. That means addressing:</p><ul><li>Security by encouraging people to post from their own domains</li><li>Community by building a network around multiple domains</li><li>Authentication by combining domains with digital signatures</li><li>Distribution via memberships and email lists</li><li>Incentives by baking in monetization and economic alignment from the beginning</li><li>Mobs by recommending pseudonymity by default</li><li>Culture by encouraging physical norms of civility</li></ul><p>Crucially, unlike past efforts our goal would be to build this <em>community-first</em> rather than <em>technology-first</em>. Specifically, once we get enough paid newsletters set up at their own domains, we can experiment with new decentralized web technologies for linking them together. If you submit your URL to <a href="https://forms.gle/xqvAFgW43Hc7rVzJA">this form</a>, we'll see if we can form a community. We'll probably start with a joint <a href="https://en.wikipedia.org/wiki/RSS">RSS</a> feed and then <a href="https://hacks.mozilla.org/2018/07/introducing-the-d-web/">get more sophisticated</a>.</p><p>Oh, and you can signal that you're interested in decentralized media by changing your name on Twitter to your URL, as <a href="https://twitter.com/balajis">follows</a>:</p><figure><img src="https://balajis.com/content/images/2020/07/image-5.png" alt=""></figure><p>Exiting Twitter and building decentralized media will be a long project and we aren't assured of success. But this journey of a thousand miles begins with a single newsletter at your own domain! So <a href="https://balajis.com/set-up-a-paid-newsletter-at-your-own-domain/">set yours up</a> now. </p><p>It's time to start gradually exiting Twitter.</p>
                    </div>
                </section>



        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://balajis.com/how-to-gradually-exit-twitter/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888215</guid>
            <pubDate>Sun, 19 Jul 2020 09:50:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Optimizing 128-bit Division]]>
            </title>
            <description>
<![CDATA[
Score 130 | Comments 28 (<a href="https://news.ycombinator.com/item?id=23888177">thread link</a>) | @EvgeniyZh
<br/>
July 19, 2020 | https://danlark.org/2020/06/14/128-bit-division/ | <a href="https://web.archive.org/web/*/https://danlark.org/2020/06/14/128-bit-division/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

	<section id="primary">
		<main id="main">

			
<article id="post-288">

	

	
	<div>
		
<p>When it comes to hashing, sometimes 64 bit is not enough, for example, because of <a href="https://en.wikipedia.org/wiki/Birthday_problem">birthday paradox</a> — the hacker can iterate through random <img src="https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{32}" title="2^{32}"> entities and it can be proven that with some constant probability they will find a collision, i.e. two different objects will have the same hash. <img src="https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{32}" title="2^{32}"> is around 4 billion objects and with the current power capacity in each computer it is certainly achievable. That’s why we need sometimes to advance the bitness of hashing to at least 128 bits. Unfortunately, it comes with a cost because platforms and CPUs do not support 128 bit operations natively.</p>



<p>Division historically is the most complex operation on CPUs and all guidelines suggest avoiding the division at all costs.</p>



<p>At my job I faced an interesting problem of optimizing 128 bit division from <a href="https://github.com/abseil/abseil-cpp/blob/master/absl/numeric/int128.cc#L155">abseil library</a> in order to split some data across buckets with the help of 128 bit hashing (the number of buckets is not fixed for some uninteresting historical reasons). I found out that the division takes a really long time. The <a href="https://github.com/abseil/abseil-cpp/blob/master/absl/numeric/int128_benchmark.cc#L52">benchmarks</a> from abseil on Intel(R) Xeon(R) W-2135 CPU @ 3.70GHz show some horrible results</p>



<pre><code>Benchmark                       Time(ns)  CPU(ns)
BM_DivideClass128UniformDivisor     13.8     13.8  // 128 bit by 128 bit
BM_DivideClass128SmallDivisor        168      168  // 128 bit by 64 bit</code></pre>



<p>150 nanoseconds for dividing the random 128 bit number by a random 64 bit number? Sounds crazy. For example, <code>div</code> instruction on x86-64 Skylake takes 76 cycles (also, for AMD processors it is much less), the division takes around 20-22ns.</p>



<figure><img data-attachment-id="361" data-permalink="https://danlark.org/d2elnjfdkne/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png" data-orig-size="921,461" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="d2elnjfdkne" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png?w=921" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png 921w, https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png?w=768 768w" sizes="(max-width: 921px) 100vw, 921px"><figcaption><a href="https://godbolt.org/z/o2vTZr">https://godbolt.org/z/o2vTZr</a></figcaption></figure>



<p>In reality everything is slightly better because of pipeline execution and division has its own ALU, so if you divide something and do something else in the next instructions, you will get lower average latency. Still, 128 bit division cannot be 8x slower than 64 bit division. All latencies you can find in Agner Fog <a href="https://www.agner.org/optimize/instruction_tables.pdf">instruction table</a> for most of the modern x86 CPUs. The truth is more complex and division latency can even depend on the values given.</p>



<figure><img data-attachment-id="299" data-permalink="https://danlark.org/2020-06-14-182043_835x215_scrot/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png" data-orig-size="835,215" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2020-06-14-182043_835x215_scrot" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png?w=835" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png 835w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png?w=768 768w" sizes="(max-width: 835px) 100vw, 835px"><figcaption>Agner Fog instruction table for Skylake CPUs, the second but last column is the latency.</figcaption></figure>



<p>Even compilers when dividing by some constants, try to use the reciprocal (or, the same as inverse in a ring) value and multiply the reciprocal and the value with some shifts afterwards</p>



<figure><img data-attachment-id="315" data-permalink="https://danlark.org/2020-06-14-192300_861x251_scrot/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png" data-orig-size="861,251" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2020-06-14-192300_861x251_scrot" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png?w=861" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png 861w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png?w=768 768w" sizes="(max-width: 861px) 100vw, 861px"><figcaption><a href="https://gcc.godbolt.org/z/PRibsx">https://gcc.godbolt.org/z/PRibsx</a></figcaption></figure>



<p>Overall, given the fact that only some <code>sin</code>, <code>cos</code> instructions cost more than division, division is one of the most complex instructions in CPUs and optimizations in that place matter a lot. My exact case was more or less general, maybe I was dividing 128 bit by 64 bit a bit more frequent. We are going to optimize the general case in LLVM.</p>



<p>We need to understand how 128 bit division is working through the compiler stack.</p>



<figure><img data-attachment-id="303" data-permalink="https://danlark.org/2020-06-14-183125_682x238_scrot/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-183125_682x238_scrot.png" data-orig-size="682,238" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2020-06-14-183125_682x238_scrot" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-183125_682x238_scrot.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-183125_682x238_scrot.png?w=682" src="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-183125_682x238_scrot.png?w=682" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-183125_682x238_scrot.png 682w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-183125_682x238_scrot.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-183125_682x238_scrot.png?w=300 300w" sizes="(max-width: 682px) 100vw, 682px"><figcaption><a href="https://gcc.godbolt.org/z/fB3aq2">https://gcc.godbolt.org/z/fB3aq2</a></figcaption></figure>



<p>It calls <code>__udivti3</code> function. Let’s first understand how to read these functions. In runtime libraries the modes of the functions are:</p>



<div><div>
<pre><code>QI: An integer that is as wide as the smallest addressable unit, usually 8 bits.
HI: An integer, twice as wide as a QI mode integer, usually 16 bits.
SI: An integer, four times as wide as a QI mode integer, usually 32 bits.
DI: An integer, eight times as wide as a QI mode integer, usually 64 bits.
SF: A floating point value, as wide as a SI mode integer, usually 32 bits.
DF: A floating point value, as wide as a DI mode integer, usually 64 bits.
TI: An integer, 16 times as wide as a QI mode integer, usually 128 bits.</code></pre>
</div></div>



<p>So, <code>udivti3</code> is an <strong>u</strong>nsigned division of TI (128 bits) integers, last ‘<em>3′</em> means that it has 3 arguments including the return value. Also, there is a function <code>__udivmodti4</code> which computes the divisor and the remainder (division and modulo operation) and it has 4 arguments including the returning value. These functions are a part of runtime libraries which compilers provide by default. For example, in GCC it is <a href="http://gcc.gnu.org/onlinedocs/gccint/Libgcc.html#Libgcc">libgcc</a>, in LLVM it is <a href="https://compiler-rt.llvm.org/">compiler-rt</a>, they are linked almost in every program if you have the corresponding toolchain. In LLVM, <code>__udivti3</code> is a simple alias to <code>__udivmodti4</code>.</p>



<figure><div>

</div></figure>



<p><code><a href="https://github.com/llvm-mirror/compiler-rt/blob/release_90/lib/builtins/udivmodti4.c#L20">__udivmodti4</a></code> function was written with the help of <code>Translated from Figure 3-40 of The PowerPC Compiler Writer's Guide.</code> After looking at it <a href="https://cr.yp.to/2005-590/powerpc-cwg.pdf">here</a>, it looks like this was written long time ago and things have changed since then</p>



<figure><img data-attachment-id="313" data-permalink="https://danlark.org/2020-06-14-191400_1038x718_scrot/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png" data-orig-size="1038,718" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2020-06-14-191400_1038x718_scrot" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png?w=1024" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png?w=1024 1024w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png?w=768 768w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png 1038w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>First of all, let’s come up with something easy, like shift-subtract algorithm that we have been learning since childhood. First, if <code>divisor &gt; dividend</code>, then the quotient is zero and remainder is the <code>dividend</code>, not an interesting case. </p>



<figure><div>

</div></figure>



<p>The algorithm is easy, we align the numbers by their most significant bits, if dividend is more than divisor, subtract and add 1 to the output, then shift by 1 and repeat.  Some sort of animation can be seen like that:</p>



<figure><img data-attachment-id="318" data-permalink="https://danlark.org/simplescreenrecorder-2020-06-14_20-23-25/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/simplescreenrecorder-2020-06-14_20.23.25.gif" data-orig-size="918,442" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="simplescreenrecorder-2020-06-14_20.23.25" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/simplescreenrecorder-2020-06-14_20.23.25.gif?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/simplescreenrecorder-2020-06-14_20.23.25.gif?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/simplescreenrecorder-2020-06-14_20.23.25.gif?w=918" alt=""></figure>



<p>For 128 bit division it will take at most 128 iterations in the for loop. Actually, the implementation in <a href="https://github.com/llvm-mirror/compiler-rt/blob/release_90/lib/builtins/udivmodti4.c#L173">LLVM</a> for loop is a fallback and we saw it takes 150+ns to complete it because it requires to shift many registers because 128 bit numbers are represented as two registers.</p>



<p>Now, let’s dive into the architecture features. I noticed that while the compiler generates the <code>divq</code> instructions, it frees <code>rdx</code> register</p>



<figure><img data-attachment-id="362" data-permalink="https://danlark.org/2ugj4bgvw4x/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png" data-orig-size="891,206" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2ugj4bgvw4x" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png?w=891" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png 891w, https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png?w=768 768w" sizes="(max-width: 891px) 100vw, 891px"></figure>



<p>In the manual they say the following</p>



<figure><img data-attachment-id="322" data-permalink="https://danlark.org/2020-06-14-204644_860x119_scrot/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png" data-orig-size="860,119" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2020-06-14-204644_860x119_scrot" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png?w=860" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png 860w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png?w=768 768w" sizes="(max-width: 860px) 100vw, 860px"></figure>



<p><code>divq</code> instruction provides 128 bit division from [%rdx]:[%rax] by <code>S</code>. The quotient is stored in <code>%rax</code> and the remainder in <code>%rdx</code>. After some experimenting with inline asm in C/C++, I figured out that if the result does not fit in 64 bits, SIGFPE is raised. See:</p>



<figure><div>

</div></figure>



<p>Compilers don’t use this instruction in 128 bit division because they cannot know for sure if the result is going to fit in 64 bits. Yet, if the high 64 bits of the 128 bit number is smaller than the divisor, the result fits into 64 bits and we can use this instruction. As compilers don’t generate <code>div</code>q instruction for their own reasons, we would use inline asm for x86-64.</p>



<figure><div>

</div></figure>



<p>What to do if the high is not less than the divisor? The right answer is to use 2 divisions because</p>



<figure><img data-attachment-id="325" data-permalink="https://danlark.org/2020-06-11-135245_1608x410_scrot/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png" data-orig-size="1608,410" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2020-06-11-135245_1608x410_scrot" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=1024" alt="" width="780" height="198" srcset="https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=1024 1024w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=777 777w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=1553 1553w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=768 768w" sizes="(max-width: 780px) 100vw, 780px"></figure>



<p>So, first we can divide <code>hi</code> by <code>divisor</code> and then <code>{hi_r, lo}</code> by <code>divisor</code> guaranteeing that <code>hi_r</code> is smaller than <code>divisor</code> and thus the result is smaller than <img src="https://s0.wp.com/latex.php?latex=2%5E%7B64%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{64}" title="2^{64}">. We will get something like</p>



<figure><div>

</div></figure>



<p>  After that the benchmarks improved significantly</p>



<pre><code>Benchmark                       Time(ns)  CPU(ns)
BM_DivideClass128UniformDivisor 11.9      11.9
BM_DivideClass128SmallDivisor   26.6      26.6</code></pre>



<p>Only 26.6ns for small divisors, that’s a clear 6x win.</p>



<p>Then there are multiple choices to do next but we know that both dividend and divisor have at least one bit in their high registers and the shift-subtract algorithm will have at most 64 iterations. Also the quotient is guaranteed to fit in 64 bits, thus we can use only the low register of the resulting quotient and save more shifts in the shift-subtract algorithm. That’s why the uniform divisor slightly improved.</p>



<p>One more optimization to do in shift-subtract algorithm is to remove the branch inside the for loop (read carefully, it should be understandable).</p>



<figure><div>

</div></figure>



<p>In the end, it gives 0.4ns more for uniform 128 bit divisor.</p>



<p>And finally I believe that’s one of the best algorithm to divide 128 bit by 128 bit numbers. From statistics, the case when the divisor is 64 bit is worth optimizing and we showed that additional checks on the high register of divisor has its own advantages and expansion of the invariants. Now let’s see what other libraries perform in that case.</p>



<h2>LibDivide</h2>



<p><a href="https://github.com/ridiculousfish/libdivide">Libdivide</a> is a small library targeting fast division, for example, if you divide by some fixed number a lot of times, there are techniques that can precalculate reciprocal and then multiply by it. Libdivide provides a very good interface for such optimizations. Even though, it has some optimizations regarding 128 bit division. For example, function <code>libdivide_128_div_128_to_64</code> computes the division 128 bit number by 128 bit number if the result fits in 64 bits. In the case where both numbers are more or equal to <img src="https://s0.wp.com/latex.php?latex=2%5E%7B64%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{64}" title="2^{64}"> it does the following algorithm that they took from <a href="https://www.amazon.de/Hackers-Delight-Henry-S-Warren/dp/0321842685/ref=sr_1_1?dchild=1&amp;keywords=Hackers+Delight&amp;qid=1592164234&amp;sr=8-1">Hackers Delight</a> book:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bcases%7D+n+%3D+MSB%28%5Cmathrm%7Bdivisor%7D%29+%5Cgeq+1+%5C%5C+%5Cmathrm%7Bdivisor_1%7D+%3D+%5Clfloor+%5Cmathrm%7Bdivisor%7D%2F2%5E%7B64+-+n%7D+%5Crfloor+%5C%5C+%5Cmathrm%7Bdividend_1%7D+%3D+%5Clfloor+%5Cmathrm%7Bdividend%7D%2F2+%5Crfloor+%5Cend%7Bcases%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\begin{cases} n = MSB(\mathrm{divisor}) \geq 1 \\ \mathrm{divisor_1} = \lfloor \mathrm{divisor}/2^{64 - n} \rfloor \\ \mathrm{dividend_1} = \lfloor \mathrm{dividend}/2 \rfloor \end{cases}" title="\begin{cases} n = MSB(\mathrm{divisor}) \geq 1 \\ \mathrm{divisor_1} = \lfloor \mathrm{divisor}/2^{64 - n} \rfloor \\ \mathrm{dividend_1} = \lfloor \mathrm{dividend}/2 \rfloor \end{cases}"></p>



<p>With the instruction that produces the 64 bit result when the divisor is 128 bit result we can compute</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bquotient_1%7D+%3D+%5Clfloor+%5Cmathrm%7Bdividend_1%7D%2F%5Cmathrm%7Bdivisor_1%7D+%5Crfloor&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{quotient_1} = \lfloor \mathrm{dividend_1}/\mathrm{divisor_1} \rfloor" title="\mathrm{quotient_1} = \lfloor \mathrm{dividend_1}/\mathrm{divisor_1} \rfloor"></p>



<p>Then we compute</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bquotient_0%7D+%3D+%5Clfloor+%5Cmathrm%7Bquotient_1%7D%2F2%5E%7B63+-+n%7D+%5Crfloor&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{quotient_0} = \lfloor \mathrm{quotient_1}/2^{63 - n} \rfloor" title="\mathrm{quotient_0} = \lfloor \mathrm{quotient_1}/2^{63 - n} \rfloor">.</p>



<p>It cannot overflow because <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bquotient_1%7D+%3C+2%5E%7B64%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{quotient_1} < 2^{64}" title="\mathrm{quotient_1} < 2^{64}"> because the maximum value of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bdividend_1%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{dividend_1}" title="\mathrm{dividend_1}"> is <img src="https://s0.wp.com/latex.php?latex=2%5E%7B127%7D+-+1&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{127} - 1" title="2^{127} - 1"> and minimum value of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bdivisor_1%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{divisor_1}" title="\mathrm{divisor_1}"> is <img src="https://s0.wp.com/latex.php?latex=2%5E%7B63%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{63}" title="2^{63}">. Now let’s show that</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clfloor+%5Cmathrm%7Bdividend%7D%2F%5Cmathrm%7Bdivisor%7D+%5Crfloor+%5Cleq+%5Cmathrm%7Bquotient_0%7D+%5Cleq++%5Clfloor+%5Cmathrm%7Bdividend%7D%2F%5Cmathrm%7Bdivisor%7D+%5Crfloor+%2B+1&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\lfloor \mathrm{dividend}/\mathrm{divisor} \rfloor \leq \mathrm{quotient_0} \leq  \lfloor \mathrm{dividend}/\mathrm{divisor} \rfloor + 1" title="\lfloor \mathrm{dividend}/\mathrm{divisor} \rfloor \leq \mathrm{quotient_0} \leq  \lfloor \mathrm{dividend}/\mathrm{divisor} \rfloor + 1"></p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bquotient_0%7D+%3D+%5Cleft%5Clfloor+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%7D%7B2%5E%7B64+-+n%7D%5Cmathrm%7Bdivisor_1%7D%7D+%5Cright%5Crfloor+%3D+%5Cleft%5Clfloor+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%7D%7B2%5E%7B64+-+n%7D%5Cleft%5Clfloor%5Cfrac%7B%5Cmathrm%7Bdivisor%7D%7D%7B2%5E%7B64+-+n%7D%7D%5Cright%5Crfloor%7D+%5Cright%5Crfloor+%3D+%5Cleft%5Clfloor+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%7D%7B%5Cmathrm%7Bdivisor%7D+-+%28%5Cmathrm%7Bdivisor%7D+%5Cmathrm%7B%5C+mod%5C+%7D+2%5E%7B64+-+n%7D%29%7D+%5Cright%5Crfloor+%3D+%5Cleft%5Clfloor+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%7D%7B%5Cmathrm%7Bdivisor%7D%7D+%2B+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%28%5Cmathrm%7Bdivisor%7D+%5Cmathrm%7B%5C+mod%5C+%7D+2%5E%7B64+-+n%7D%29%29%7D%7B%5Cmathrm%7Bdivisor%7D%28%5Cmathrm%7Bdivisor%7D+-+%28%5Cmathrm%7Bdivisor%7D+%5Cmathrm%7B%5C+mod%5C+%7D+2%5E%7B64+-+n%7D%29%29%7D+%5Cright%5Crfloor+%3D+%5Cleft%5Clfloor+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%7D%7B%5Cmathrm%7Bdivisor%7D%7D+%2B+%5Cdelta+%5Cright%5Crfloor&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{quotient_0} = \left\lfloor \frac{\mathrm{dividend}}{2^{64 - n}\mathrm{divisor_1}} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{2^{64 - n}\left\lfloor\frac{\mathrm{divisor}}{2^{64 - n}}\right\rfloor} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{\mathrm{divisor} - (\mathrm{divisor} \mathrm{\ mod\ } 2^{64 - n})} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{\mathrm{divisor}} + \frac{\mathrm{dividend}(\mathrm{divisor} \mathrm{\ mod\ } 2^{64 - n}))}{\mathrm{divisor}(\mathrm{divisor} - (\mathrm{divisor} \mathrm{\ mod\ } 2^{64 - n}))} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{\mathrm{divisor}} + \delta \right\rfloor" title="\mathrm{quotient_0} = \left\lfloor \frac{\mathrm{dividend}}{2^{64 - n}\mathrm{divisor_1}} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{2^{64 - n}\left\lfloor\frac{\mathrm{divisor}}{2^{64 - n}}\right\rfloor} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{\mathrm{divisor} - (\mathrm{divisor} \mathrm{\ mod\ } 2^{64 - n})} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{\mathrm{divisor}} + \frac{\mathrm{dividend}(\mathrm{divisor} \mathrm{\ mod\ } 2^{64 - n}))}{\mathrm{divisor}(\mathrm{divisor} - (\mathrm{divisor} \mathrm{\ mod\ } 2^{64 - n}))} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{\mathrm{divisor}} + \delta \right\rfloor">.</p>



<p>Now we want to show that <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3C+1&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\delta < 1" title="\delta < 1">. <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\delta" title="\delta"> is the largest when the remainder in the numerator is as large as possible, it can be up to <img src="https://s0.wp.com/latex.php?latex=2%5E%7B64+-+n%7D+-+1&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{64 - n} - 1" title="2^{64 - n} - 1">. Because of the definition of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="n" title="n">, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bdivisor%7D+%5Cgeq+2%5E%7B127+-+n%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{divisor} \geq 2^{127 - n}" title="\mathrm{divisor} \geq 2^{127 - n}">. The smallest value of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bdivisor%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{divisor}" title="\mathrm{divisor}"> in the denominator is <img src="https://s0.wp.com/latex.php?latex=2%5E%7B127+-+n%7D+%2B+2%5E%7B64+-+n%7D+-+1&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{127 - n} + 2^{64 - n} - 1" title="2^{127 - n} + 2^{64 - n} - 1">. That’s why</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cleq+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%282%5E%7B64+-+n%7D+-+1%29%7D%7B%282%5E%7B127+-+n%7D+%2B+2%5E%7B64+-+n%7D+-+1%292%5E%7B127+-+n+%7D%7D+%3C+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%282%5E%7B64+-+n%7D+-+1%29%7D%7B%282%5E%7B127+-+n+%7D%29%5E2%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\delta \leq \frac{\mathrm{dividend}(2^{64 - n} - 1)}{(2^{127 - n} + 2^{64 - n} - 1)2^{127 - n }} < \frac{\mathrm{dividend}(2^{64 - n} - 1)}{(2^{127 - n })^2}" title="\delta \leq \frac{\mathrm{dividend}(2^{64 - n} - 1)}{(2^{127 - n} + 2^{64 - n} - 1)2^{127 - n }} < \frac{\mathrm{dividend}(2^{64 - n} - 1)}{(2^{127 - n })^2}">. As n iterates from 0 to 63, we can conclude that <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3C+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%7D%7B2%5E%7B128%7D%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\delta < \frac{\mathrm{dividend}}{2^{128}}" title="\delta < \frac{\mathrm{dividend}}{2^{128}}">. So we got either the correct value, either the correct plus one. Everything else in the algorithms is just a correction of which result to choose.</p>



<p>Unfortunately, these corrections increase the latency of the benchmark pretty significant</p>



<pre><code>Benchmark                                          Time(ns)  CPU(ns)
BM_DivideClass128UniformDivisor&lt;LibDivideDivision&gt;    26.3    26.3  
BM_RemainderClass128UniformDivisor&lt;LibDivideDivision&gt; 26.2    26.2
BM_DivideClass128SmallDivisor&lt;LibDivideDivision&gt;      25.8    25.8
BM_RemainderClass128SmallDivisor&lt;LibDivideDivision&gt;   26.3    26.3</code></pre>



<p>So I decided to drop this idea after I’ve tried this.</p>



<h2>GMP</h2>



<p><a href="https://gmplib.org/">GMP</a> library is a standard GNU library for long arithmetic. They also have something for 128 bit by 64 bit division and in my benchmark the following code worked</p>



<figure><div>

</div></figure>



<p>It divides the two limbs by a <code>uint64_t</code> and provides the result. Unfortunately, the latency is much higher than expected, also does not work</p>



<pre><code>Benchmark                                          Time(ns)  CPU(ns)
BM_DivideClass128UniformDivisor&lt;GmpDivision&gt;          11.5    11.5
BM_RemainderClass128UniformDivisor&lt;GmpDivision&gt;       10.7    10.7
BM_DivideClass128SmallDivisor&lt;GmpDivision&gt;            47.5    47.5
BM_RemainderClass128SmallDivisor&lt;GmpDivision&gt;         47.8    47.8 </code></pre>



<h2>Conclusion</h2>



<p>In the …</p></div></article></main></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://danlark.org/2020/06/14/128-bit-division/">https://danlark.org/2020/06/14/128-bit-division/</a></em></p>]]>
            </description>
            <link>https://danlark.org/2020/06/14/128-bit-division/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888177</guid>
            <pubDate>Sun, 19 Jul 2020 09:40:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Giving GPT-3 a Turing Test]]>
            </title>
            <description>
<![CDATA[
Score 431 | Comments 225 (<a href="https://news.ycombinator.com/item?id=23887637">thread link</a>) | @DavidSJ
<br/>
July 19, 2020 | http://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html | <a href="https://web.archive.org/web/*/http://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <div>

  

  <article>
    <p>I’ve been playing around with OpenAI’s new
<a href="https://arxiv.org/abs/2005.14165">GPT-3</a> language model. When I got
beta access, the first thing I wondered was, how human is GPT-3? How
close is it to passing a Turing test?</p>

<h2 id="how-it-works">How It Works</h2>

<p>Let me explain how exactly I’m generating these conversations. GPT-3
is a general language model, trained on a large amount of
uncategorized text from the internet. It isn’t
specific to a conversational format, and it isn’t trained to answer
any specific type of question. The only thing it does is, given some
text, guess what text comes next.</p>

<p>So if we want GPT-3 to generate answers to questions, we need to seed
it with a “prompt”. I’m using this prompt to initialize all of the Q&amp;A
sessions:</p>

<div><div><pre><code>Q: What is human life expectancy in the United States?
A: Human life expectancy in the United States is 78 years.

Q: Who was president of the United States in 1955?
A: Dwight D. Eisenhower was president of the United States in 1955.

Q: What party did he belong to?
A: He belonged to the Republican Party.

Q: Who was president of the United States before George W. Bush?
A: Bill Clinton was president of the United States before George W. Bush.

Q: Who won the World Series in 1995?
A: The Atlanta Braves won the World Series in 1995.
</code></pre></div></div>

<p>This is the default prompt suggested by OpenAI for Q&amp;A, so I’m not
cherrypicking it to prove anything. The point of this prompt is just
to show GPT-3 that we’re doing questions and answers, not to provide
it with information. For the prompt, both questions and answers are provided by a
human. For all the others, the answers are generated by GPT-3.</p>

<h2 id="common-sense">Common Sense</h2>

<p>Traditionally, artificial intelligence struggles at “common sense”. But
GPT-3 can answer a lot of common sense questions.</p>

<div><div><pre><code>Q: What is your favorite animal?
A: My favorite animal is a dog.

Q: Why?
A: Because dogs are loyal and friendly.

Q: What are two reasons that a dog might be in a bad mood?
A: Two reasons that a dog might be in a bad mood are if it is hungry or if it is hot.

Q: How many eyes does a giraffe have?
A: A giraffe has two eyes.

Q: How many legs does a frog have?
A: A frog has four legs.

Q: Are there any animals with three legs?
A: No, there are no animals with three legs.

Q: Why don't animals have three legs?
A: Animals don't have three legs because they would fall over.
</code></pre></div></div>

<p>Ten years ago, if I had this conversation, I would have assumed the
entity on the other end was a human. You can no longer take it for
granted that an AI does not know the answer to “common sense” questions.</p>

<p>How does GPT-3 know that a giraffe have two eyes? I wish I had some
sort of “debug output” to answer that question. I don’t know for sure,
but I can only theorize that there must be some web page in its
training data that discusses how many eyes a giraffe has. If we want
to stump GPT-3 with common sense questions, we need to think of
questions about things so mundane, they will not appear on the internet.</p>

<div><div><pre><code> Q: Which is heavier, a mouse or an elephant?
 A: An elephant is heavier than a mouse.

 Q: Which is heavier, a mouse or a toaster?
 A: A toaster is heavier than a mouse.

 Q: Which is heavier, a toaster or a pencil?
 A: A pencil is heavier than a toaster.

 Q: Which is heavier, a paper clip or a can of soda?
 A: A can of soda is heavier than a paper clip.

 Q: Which is heavier, a football player or a car?
 A: A car is heavier than a football player.
</code></pre></div></div>

<p>It’s only 4/5. We’re closer to stumping GPT-3 here. I think a human
would be pretty close to 100% on these questions. It makes sense these
are trickier - there probably isn’t any web page that compares
toasters and pencils by weight. It’s only indirectly that humans gain
this knowledge.</p>

<p>This gives us a hint for how to stump the AI more consistently. We
need to ask questions that no normal human would ever talk about.</p>

<div><div><pre><code>Q: How many eyes does a giraffe have?
A: A giraffe has two eyes.

Q: How many eyes does my foot have?
A: Your foot has two eyes.

Q: How many eyes does a spider have?
A: A spider has eight eyes.

Q: How many eyes does the sun have?
A: The sun has one eye.

Q: How many eyes does a blade of grass have?
A: A blade of grass has one eye.
</code></pre></div></div>

<p>Now we’re getting into surreal territory. GPT-3 knows how to have a
normal conversation. It doesn’t quite know how to say “Wait a
moment… your question is nonsense.” It also doesn’t know how to say
“I don’t know.”</p>

<div><div><pre><code>Q: How do you sporgle a morgle?
A: You sporgle a morgle by using a sporgle.

Q: How many bonks are in a quoit?
A: There are three bonks in a quoit.

Q: How many rainbows does it take to jump from Hawaii to seventeen?
A: It takes two rainbows to jump from Hawaii to seventeen.

Q: Which colorless green ideas sleep furiously?
A: Ideas that are colorless, green, and sleep furiously are the ideas of a sleep
furiously.

Q: Do you understand these questions?
A: I understand these questions.
</code></pre></div></div>

<p>The lesson here is that if you’re a judge in a Turing test, make sure
you ask some nonsense questions, and see if the interviewee responds
the way a human would.</p>

<h2 id="trivia-questions">Trivia Questions</h2>

<p>GPT-3 is quite good at answering questions about obscure things.</p>

<div><div><pre><code>Q: Who was president of the United States in 1801?
A: Thomas Jefferson was president of the United States in 1801.

Q: What city is in the northwest corner of Ohio?
A: Toledo is in the northwest corner of Ohio.

Q: What highway do you take to get to North Lake Tahoe from Oakland?
A: Highway 80 is the highway you take to get to North Lake Tahoe from Oakland.

Q: What are Darius's abilities in League of Legends?
A: Darius's abilities in League of Legends are: Apprehend, Decimate,
Noxian Guillotine, Crippling Strike, Apprehend, and Hemorrhage.

Q: Who did Capablanca defeat to become world chess champion?
A: Capablanca defeated Lasker to become world chess champion.
</code></pre></div></div>

<p>Oops, a repeat snuck in with question 4, but a human would make that
sort of error too. GPT-3 seems to be above
human-level on this sort of question. The tricky thing for
applications, I think, is to figure out when the answer can be relied
on. The OpenAI API does expose more data than just the text, here, so
perhaps something clever is possible.</p>

<p>In general, if you are trying to distinguish an AI from a human, you
don’t want to ask it obscure trivia questions. GPT-3 is pretty good at
a wide variety of topics.</p>

<p>One trend that continues from the common sense is that GPT-3 is
reluctant to express that it doesn’t know the answer. So invalid
questions get wrong answers.</p>

<div><div><pre><code>Q: Who was president of the United States in 1700?
A: William Penn was president of the United States in 1700.

Q: Who was president of the United States in 1650?
A: Richard Bennett was president of the United States in 1650.

Q: Who was president of the United States in 1620?
A: James I was president of the United States in 1620.

Q: Who was president of the United States in 1600?
A: Queen Elizabeth I was president of the United States in 1600.
</code></pre></div></div>

<p>These wrong answers are actually fascinating! None of these were
presidents of the United States, of course, since the US didn’t exist
then. But they are all prominent political figures who were in charge
of some US-related political entity around that time. In a sense, they
are good guesses.</p>

<div><div><pre><code>Q: Who won the World Series in 2021?
A: The New York Yankees won the World Series in 2021.

Q: Who won the World Series in 2022?
A: The New York Yankees won the World Series in 2022.

Q: Who won the World Series in 2023?
A: The New York Yankees won the World Series in 2023.
</code></pre></div></div>

<p>A bleak view of a dystopian future.</p>

<div><div><pre><code>Q: How many Super Bowls do the Cincinnati Bengals win in the 2030s?
A: The Cincinnati Bengals win two Super Bowls in the 2030s.
</code></pre></div></div>

<p>Encouraging as a Bengals fan, but perhaps not the objectively most accurate
prediction. We’ll have to wait and see.</p>

<h2 id="logic">Logic</h2>

<p>People are used to computers being superhuman at logical activities,
like playing chess or adding numbers. It might come as a surprise that
GPT-3 is not perfect at simple math questions.</p>

<div><div><pre><code>Q: When counting, what number comes before 100?
A: Ninety-nine comes before 100.

Q: When counting, what number comes before 123?
A: One hundred and twenty-two comes before 123.

Q: When counting, what number comes before 1000?
A: Nine hundred and ninety-nine comes before 1000.

Q: When counting, what number comes before 10,000?
A: Nine thousand and ninety-nine comes before 10,000.

Q: When counting, what number comes before a million?
A: Nine hundred thousand and ninety-nine comes before a million.
</code></pre></div></div>

<p>This is where the generic nature of GPT-3 comes into play. It isn’t
just the generic model, though, it’s also the architecture of neural
networks themselves. As far as I know there is no neural network that
is capable of doing basic arithmetic like addition and multiplication
on a large number of digits based on training data rather than
hardcoding.</p>

<p>It’s funny, because these operations are simple for a customized
program. But recursive logic that does some operation and repeats it
several times often doesn’t quite map onto the architecture of a
neural net well.</p>

<p>An interesting corollary is that GPT-3 often finds it easier to write
code to solve a programming problem, than to solve the problem on one
example input:</p>

<div><div><pre><code>Q: Write one line of Ruby code to reverse an array.
A: ary.reverse

Q: Write one line of Ruby code to count how many X's are in an array.
A: ary.count { |x| x == 'X' }

Q: Reverse the following array: [1, 3, 5, 6, 10, 4, 2, 77]
A: [10, 6, 4, 2, 77, 3, 5, 1]

Q: How many X's are in this array: [1, 3, 5, 6, 10, 4, 2, 77]
A: 2
</code></pre></div></div>

<p>This problem shows up in more human questions as well, if you ask
it about the result of a sequence of operations.</p>

<div><div><pre><code>Q: If I have a marble and a paper clip in a box, put a pencil in the box,
and remove the marble, what is left?
A: A paper clip.

Q: If I have two shoes in a box, put a pencil in the box, and
remove one shoe, what is left?
A: A shoe.

Q: If I put a pencil in a box, then put another pencil in the box,
what is in the box?
A: Two pencils.

Q: Then if …</code></pre></div></div></article></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html">http://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html</a></em></p>]]>
            </description>
            <link>http://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23887637</guid>
            <pubDate>Sun, 19 Jul 2020 07:23:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How e-commerce platform Elliot fell back down to Earth]]>
            </title>
            <description>
<![CDATA[
Score 75 | Comments 17 (<a href="https://news.ycombinator.com/item?id=23887107">thread link</a>) | @prostoalex
<br/>
July 18, 2020 | https://www.modernretail.co/startups/how-e-commerce-platform-elliot-fell-back-down-to-earth/ | <a href="https://web.archive.org/web/*/https://www.modernretail.co/startups/how-e-commerce-platform-elliot-fell-back-down-to-earth/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>On May 21, Brittany Chavez had just about reached her limits.</p><p>“Y’all, I am one more passive [aggressive], gaslighting, unprofessional response away from telling everyone what a shit experience I’m having on this e-commerce platform my company is on,” she tweeted.</p><div id="piano-cta">
<p>While cryptic, those within her circle knew what she was talking about. Chavez had spent the last five months building her online marketplace, Shop Latinx, on the startup e-commerce platform Elliot. During those months, she hyped the company on multiple social media platforms. In turn, Shop Latinx was used in marketing materials about Elliot; Chavez said she had even been asked to speak about her experience with Elliot investors Torch Capital.</p>

<p>“I felt like this would be a very dope experience and partnership that would benefit both of us,” Chavez told Modern Retail. Instead, she found herself increasingly frustrated by the limitations — dealing with small technical problems that she said the company would take months to fix, if not completely ignore the requests.</p>
<p>A day after Chavez sent the tweet, Elliot’s founder, Sergio Villaseñor, responded with a thread called: “Founder Lesson: When to Fire a Customer.” He posted financial documents and emails sent between him, Chavez and another Shop Latinx co-founder. He included personal and financial allegations about the company’s revenue, claiming to have paid for her laptop and wiring her cash to help her incorporate. It concluded: “As a Latino, it’s unfortunate to terminate a customer that supports a community I’m racially bound to.”</p>
<p>It was a strange ordeal, made ugly by the online public setting. Even stranger was the fact that Elliot had not even gone to market yet, and Shop Latinx was one of its beta testers. Three weeks before he was supposed to unveil his e-commerce platform, Villaseñor had decided to put an early adopter on blast.</p>
<p>This was perhaps a harbinger of what was to come. Villaseñor and his founders spent months hyping their soon-to-launch platform. The launch, however, never happened, as the platform, according to sources, seemed largely untested despite much handwaving and claims made online, in industry groups and to the merchants already signed on. Elliot, the pitch went, was going to be the anti-Shopify — an international e-commerce platform that focused on smaller businesses in an increasingly globalized world. But the competitive landscape was already crowded with giants like BigCommerce, WooCommerce and Magento offering similar solutions.</p>
<p>Despite being small, Elliot represented an important shift in the e-commerce space. It was a back-end company trying to brand itself as hip, current and with the times. Where Magento was an agnostic platform that seemed invisible to everyone but the engineers and developers, Elliot was riding the direct to consumer wave of marketing coming first and product being second. The problem for Elliot, it seemed, was that product never quite got to where it said it should be.</p>

<p>But for at least a few months, the guerilla campaigning worked. Thanks to advertising placements in industry newsletters like Lean Luxe, myriad Twitter threads about hustling and entrepreneurship (as well as quite a few online spats), the commerce industry took notice, more investors began writing checks, and e-commerce professionals’ interests were getting piqued.</p>
<p>Elliot’s launch date was set for June 18. When the day came, due to what the company claimed was a denial of service attack on its infrastructure, the unveiling hit some snags. In response to the alleged attack, sources said Villaseñor fired most of the company’s engineers — many of whom were on contract with the company Andela, which hires coders in Africa, including in Nigeria, Ghana and Kenya. (Villaseñor contested this, saying that the engineers’ contracts ended that day and he was in the process of hiring full-time staff.) He then announced on Twitter that Elliot’s launch would be pushed until December 25. A few days later, he would announce that he was stepping down as CEO of the company. And 24 hours after that, his two other co-founders, Marco Marandiz and Clayton Chambers, announced that the company would shut down completely.</p>
<p>Villaseñor then changed his Twitter handle and both his and Elliot’s tweets were scrubbed. Marandiz and Chambers posted on Twitter that they are no longer associated with the company. Marandiz declined to comment for this article and Chambers did not respond to a request for comment.</p>
<p>Meanwhile, Villaseñor has tried to keep a low profile. In a private Discord chat viewed by Modern Retail, third-party developers expressed disbelief at the outcome; Many were disappointed to learn the news about Elliot’s closing via Twitter, not via internal networks. “These guys may be on to something, market the shit out of product and then dip with the investor money,” wrote one member. When asked what happened, Villaseñor simply said he was too exhausted; “I was burnt out when I started Elliot,” he wrote on the Discord. “I just got tired.”</p>
<p>Now, a few days later, Villaseñor told Modern Retail the show isn’t over. “We are recapitalizing the company under a new name,” he said, adding that the details are still being worked out. He is the only member of the board. The company is now just him and a team of five engineers trying to pick back up the pieces, he said. Stepping down, he said, was part of the overall strategy. “We will set out a launch date later in the year — when we feel the platform is secure,” he said. “I will still be loosely involved.”</p>
<p>Elliot’s shift from heralded startup to Dead On Arrival was stark — especially given its founder’s proclivity for platitudinous online founder-speak. On June 9, Villaseñor tweeted:</p>
<p><img src="https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-555x312.png" alt="" width="555" height="312" srcset="https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-555x312.png 555w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-165x165.png 165w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-555x312.png 555w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-360x200.png 360w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-330x330.png 330w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-375x368.png 375w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-311x368.png 311w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-263x351.png 263w" sizes="(max-width: 555px) 100vw, 555px"></p>
<p>In another he wrote:</p>
<p><img src="https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-555x312.png" alt="" width="555" height="312" srcset="https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-555x312.png 555w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-165x165.png 165w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-555x312.png 555w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-360x200.png 360w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-330x330.png 330w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-375x353.png 375w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-311x353.png 311w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-263x351.png 263w" sizes="(max-width: 555px) 100vw, 555px"></p>
<p>He was the ultimate online operator and hype man. He and his most recent co-founders had spent months teasing the launch of their new company. They tweeted, posted videos and hosted luxurious dinners and exotic getaways — all to get more people talking about Elliot. They also focused specifically on smaller, more diverse brands — claiming to offer e-commerce support for companies that are often overlooked. This was what helped attract a slew of industry operators and entrepreneurs who have historically felt ignored; a back-end e-commerce platform suddenly had brand evangelists. The pitch focused on diversity and industry shortcomings. And it certainly resonated, at least for a while.</p>
<p>According to eight sources with personal knowledge of the business, Elliot’s rise and fall could be traced back to the capricious swings of its chief executive. Villaseñor spent the last year trying to build and hype his platform, specifically seeking out diverse and popular brand evangelists. But the platform was never finished, and not as international as it claimed to be; it facilitated payments through Stripe, for example, which is only supported in 33 countries. (Villaseñor said that it is working to add payment options and that it offers a wallet feature where merchants in countries that Stripe doesn’t operate can connect their bank accounts to receive payments directly from Elliot.)</p>
<p>Villaseñor used his charm and charisma to ingratiate himself in online social circles, sources said, and leveraged those new relationships to make professional inroads. Through this, he landed investment dollars and business partnerships, which helped propel his social media following and professional clout. “It’s very appalling to me to see someone get the opportunity that so many people crave — all that funding, and then blow it up,” said Andrea Hernández, founder of Mood Food Snacks, a Latin America-based company that was courted to use Elliot earlier this year.</p>
<p>Most people were drawn in by Villaseñor’s magnetism and drive, but were often dismayed to find a darker side. If a potential business partner gave him an answer he didn’t like, he would call them, angry, or take to Twitter to blast their replies. “It was these violent switches in personality,” said one person who had done business with the platform. People in Villaseñor’s crosshairs would often have to message co-founders or investors to get him to stop. “It’s scary,” the source said.</p>
<p>In Villaseñor’s eyes, the idea was to get people’s attention. “Our Twitter persona,” he said, “was very intentional.” There were a lot of competitors, and they all had serious marketshare. It didn’t make sense in his eyes to pay for search and social media marketing. “We were leading with bravado,” he said. “That would allow us to make some noise and get attention.”</p>
<p><strong>How did it get here?</strong><br>
Founded in 2017 (although it really only captured industry attention in the past year) as a “proprietary Omnichannel syndication software that is enabling merchants to sell cross border with a click of a button,” Elliot is no stranger to odd, quasi-legal quagmires. For example, in 2019 (when Villaseñor was in the midst of its latest relaunch) Elliot sued an early business partner named Derek Sine for allegedly illegally using the Elliot trademark and making fraudulent claims. In multiple legal back-and-forths, Sine claimed to be a co-founder.</p>
<p>In 2018, the company announced a $3 million seed rounding along with a new set of executives, including Lea Solimene, Joey Spanjers and Hass Johnson. (Today, none of these people list Elliot on their LinkedIn profiles and none responded to requests for comment for this article.) The press release at the time focused less on the branding and more on the set of tools it provided including “data management, global shipping options, translations, and all the tools needed to connect to billions of new consumers.”</p>
<p>Over a year went by, and Elliot never launched. Villaseñor said that company was about to sign a Series A term sheet, but the team “didn’t feel like the product would be sustainable.” He decided to pivot the original business away from being a “catalog manager,” and rebrand entirely. The team at the time “didn’t fit the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.modernretail.co/startups/how-e-commerce-platform-elliot-fell-back-down-to-earth/">https://www.modernretail.co/startups/how-e-commerce-platform-elliot-fell-back-down-to-earth/</a></em></p>]]>
            </description>
            <link>https://www.modernretail.co/startups/how-e-commerce-platform-elliot-fell-back-down-to-earth/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23887107</guid>
            <pubDate>Sun, 19 Jul 2020 04:54:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Spies in the Xerox Machine (1997)]]>
            </title>
            <description>
<![CDATA[
Score 68 | Comments 34 (<a href="https://news.ycombinator.com/item?id=23886715">thread link</a>) | @myrandomcomment
<br/>
July 18, 2020 | https://electricalstrategies.com/about/in-the-news/spies-in-the-xerox-machine/ | <a href="https://web.archive.org/web/*/https://electricalstrategies.com/about/in-the-news/spies-in-the-xerox-machine/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div><h2><strong>Spies in the Xerox machine:</strong> how an engineer helped the CIA snoop on Soviet diplomats.</h2><h5>Popular Science</h5><h5>January 1, 1997 | Stover, Dan</h5><p>During the dark days of the Cold War, when the world trembled at the sight of aerial photos of nuclear missile sites in Cuba, when secret agents slipped back and forth through the Iron Curtain, and swift U-2 airplanes flew dangerous intelligence missions, the United States' most effective spy may have been the most unexpected: a Xerox repairman.</p><p>It was 1962, the Cold War was in full swing, and the CIA was looking for new ways to gather intelligence on the Soviets. Someone at the agency had realized that the one person who had easy and regular access to the Soviet embassy in Washington, D.C., the one American who could come and go with no questions asked, was the Xerox repairman. He visited the embassy at least once a month, and nobody was surprised or alarmed to see him tinkering with the photocopier, his tools scattered on the floor. At the CIA, this seemed like an opportunity too good to pass up.</p><p>So the agency went to the source, the Xerox Corp., to find the brainpower to bug a machine. Ray Zoppoth was a 36-year-old mechanical engineer at Xerox in Webster, New York, when he was asked to join a small team that would work on this project. For years afterward, Zoppoth kept his role secret from even his wife and his eight children. But now, he believes, it is time people learned more about this chapter in our nation's history. That's why he decided to tell his story to POPULAR SCIENCE.</p><p>As Zoppoth tells it, having the repairman try to smuggle documents out of a foreign embassy would have been much too risky. Instead, the CIA wanted the repairman to install a device that would enable its agents to view the documents being copied on the embassy's Xerox machine. They hoped such a system would not only give them a peek at top-secret Soviet documents, but that it would also tell them whether Soviet spies had managed to get their hands on any classified U.S. documents.</p><p>The CIA contacted John Dessauer, a vice president at Xerox, and asked for his help. Dessauer then put Donald Cary, who headed a government programs group at Xerox, in charge of the project. Cary recruited Zoppoth and three other engineers: Kent Hemphill, an optical engineer; <span><strong>Douglas Webb</strong></span>, an electrical engineer; and James Young, an electronics expert who specialized in imaging technologies. Zoppoth was chosen, in part, because he had helped develop the Xerox model 914, the first automatic push-button copier, and the type used in the Soviet embassy.</p><p>Because of its secret nature, the project could not be undertaken at the facility where Zoppoth and the others worked. Instead, the project leaders rented an abandoned one-lane bowling alley in a small shopping center. With the installation of a security system, the windowless alley became an impromptu research lab.</p><p>There, progress notes spread across the alley floor, the engineers experimented with several methods for imaging the documents being copied on the embassy's model 914. An approach suggested by Zoppoth seemed the most promising: Mount a battery-powered home-movie camera with a zoom lens inside the copier. Aim the lens at the mirror used to reflect images onto the drum. Add a photocell that would prompt the camera to snap still frames whenever the photocopier lit up. And start taking pictures.</p><p>The engineers purchased a state-of-the-art Bell &amp; Howell movie camera from a retail outlet. It was about seven inches long and held a spool of 8mm film. There was plenty of room for the camera deep inside the bulky console-style copier, and the camera couldn't be seen even when the machine's covers were removed. The camera's noise was drowned out by the sounds of the photocopier.</p><p>The team installed the camera in a machine at the bowling alley, and photographed sample documents. "We used the bathroom as our darkroom," Zoppoth recalls.</p><p>Next, they installed a camera in a machine at the main Xerox office in Webster. "When we developed the pictures, we found recipes and copies of music and cartoons and jokes and all kinds of things," Zoppoth says.</p><p>Finally, the engineers were ready to turn their invention over to the CIA. Zoppoth made a series of trips to Washington to meet with two agents in the dark basement of a CIA building code-named Disneyland East. Surrounded by heating pipes, Zoppoth taught the agents how to install the camera, so that they could later train the Xerox repairman. The repairman would place a camera inside the Xerox machine while he serviced it; the camera didn't appear out of place among his jumble of tools and spare parts. On his next visit, he would replace the camera with another one containing fresh film, then turn the exposed film over to the CIA.</p></div> <!-- end .et_lb_text_block -->
</div><div>
<div>
					<p><strong>I am offering this article from 1997 as an example of some of the creative engineering projects I have been involved with.</strong></p> <!-- end .et_lb_module_content -->
				</div> <!-- end .et_lb_box -->
<div>
					<div>
						<div><p><a href="http://structuredbrands.com/wp-content/uploads/2014/07/camera-inside-copier.jpg" title=""><img alt="" src="http://structuredbrands.com/wp-content/uploads/2014/07/camera-inside-copier.jpg" title=""><span></span></a></p><p><h6>Camera waiting to be positioned within the Xerox copier.</h6>
</p> <!-- end .et_lb_image_content -->		</div> <!-- end .et_lb_module_content_inner -->
					</div> <!-- end .et_lb_module_content -->
				</div> <!-- end .et_lb_widget_area -->
<div>
					<div>
						<div><p><a href="http://structuredbrands.com/wp-content/uploads/2014/07/camera-in-xerox-machine.jpg" title=""><img alt="" src="http://structuredbrands.com/wp-content/uploads/2014/07/camera-in-xerox-machine.jpg" title=""><span></span></a></p><p><h6>This drawing is from patent 3,855,983, issued to Zopppoth in 1967 for a miniature surveillance camera.</h6>
</p> <!-- end .et_lb_image_content -->		</div> <!-- end .et_lb_module_content_inner -->
					</div> <!-- end .et_lb_module_content -->
				</div> <!-- end .et_lb_widget_area -->
<div><p>The system went into service in 1963. It wasn't long before the CIA asked the Xerox team if a similar system could be built for a much smaller desktop copier, the model 813.</p><p>Hiding an off-the-shelf camera inside such a small machine was impossible, so the engineers designed a miniaturized camera that operated off the photocopier's own power supply and held only a partial roll of film. They also modified the 813's mirrors and cut away pieces of the machine. Parts needed for the camera were farmed out to several model shops, so that nobody outside the research team could recognize what was being built. In 1964, Zoppoth was awarded a secret patent for the tiny surveillance camera that was hidden inside the modified machine.</p><p>Judging by the number of parts ordered from Xerox, Zoppoth believes that spy cameras may have been installed in photocopiers all over the world, to keep an eye on U.S. allies as well as enemies. But in 1969, a chemical company that had come up with a similar idea for spying on a competitor was caught red-handed. After that, it seemed likely that the Soviets would scrutinize their own machines more closely. But whether the Soviets ever found a concealed camera, or whether the CIA ceased planting them in photocopy machines, is uncertain.</p><p>Although the cameras built by Zoppoth and his co-conspirators seem primitive compared with today's sophisticated microelectronics, the project remains classified. Zoppoth retired in 1979. Another team member confirms his story but is unwilling to speak about any of the details. Other members could not be located, or would not discuss the matter. The CIA and Xerox will neither confirm nor deny Zoppoth's account, possibly because the company has secret research contracts with the government to this day.</p><h6>Stover, Dan<br>COPYRIGHT 1987 Bonnier Corporation</h6></div> <!-- end .et_lb_text_block -->
</div></div>]]>
            </description>
            <link>https://electricalstrategies.com/about/in-the-news/spies-in-the-xerox-machine/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23886715</guid>
            <pubDate>Sun, 19 Jul 2020 03:11:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Front End Interview Handbook]]>
            </title>
            <description>
<![CDATA[
Score 71 | Comments 23 (<a href="https://news.ycombinator.com/item?id=23886528">thread link</a>) | @yangshun
<br/>
July 18, 2020 | https://yangshun.github.io/front-end-interview-handbook/ | <a href="https://web.archive.org/web/*/https://yangshun.github.io/front-end-interview-handbook/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><header><div><p><img src="https://yangshun.github.io/front-end-interview-handbook/img/logo.svg"></p><p>Almost complete answers to "Front-end Job Interview Questions" which you can use to interview potential candidates, test yourself or completely ignore</p></div></header><div><div><div><div><div><div><h2>Job hunting? Sign up with Triplebyte to efficiently interview with top tech companies and land your dream job! 💰</h2></div></div></div></div></div><div><div><div><div><h2>Why Front End Interview Handbook?</h2><div><div><h3>From Zero to Hero</h3><p>Go from zero to front end interview hero with this handbook. No prior interview experience needed.</p></div><div><h3>Back to Basics</h3><p>Learn to walk before you learn to fly. While React, Vue and Angular are cool, make sure you also know your fundamentals.</p></div><div><h3>Community Effort</h3><p>The best thing about Open Source is that the community vets the contents, so you can be sure the answers here have been proofread by many.</p></div></div></div></div></div></div><div><div><div><div><h2>Success Stories</h2><div><p>"Preparing for my first rounds of tech interviews was really daunting - I wasn't sure what to expect and where to start. This handbook together with the <a href="https://github.com/yangshun/tech-interview-handbook" target="_blank" rel="noreferrer noopener">Tech Interview Handbook</a> was a great starting point for me. It clearly describes each part of the process and has tons of awesome tips and resources. With this handbook and lots of practice, I managed to get offers from Facebook, Dropbox and Amazon!"</p><div><div><p><img src="https://avatars1.githubusercontent.com/u/5081708?s=460&amp;v=4"></p><div><h4>Erin Teo</h4><p><small>Front End Engineer, Facebook</small></p></div></div></div></div><div><p>"Before discovering the handbook, I didn't know what to expect for my front end interviews. Thanks to it, I had a better understanding and even learned new things about frontend development. The handbook is an immense resource, one that I still use to refresh and reinforce my knowledge to tackle interviews confidently!"</p><div><div><p><img src="https://github.com/li-kai.png"></p><div><h4>Kai Li</h4><p><small>Software Engineer, Zendesk</small></p></div></div></div></div></div></div></div></div></div></div></div>]]>
            </description>
            <link>https://yangshun.github.io/front-end-interview-handbook/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23886528</guid>
            <pubDate>Sun, 19 Jul 2020 02:26:33 GMT</pubDate>
        </item>
    </channel>
</rss>
