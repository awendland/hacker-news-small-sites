<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 5]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 5. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 13 Jan 2021 09:04:56 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-5.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Wed, 13 Jan 2021 09:04:56 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Why Haskell is our first choice for building production software systems]]>
            </title>
            <description>
<![CDATA[
Score 35 | Comments 14 (<a href="https://news.ycombinator.com/item?id=25726588">thread link</a>) | @Albert_Camus
<br/>
January 11, 2021 | https://www.foxhound.systems/blog/why-haskell-for-production/ | <a href="https://web.archive.org/web/*/https://www.foxhound.systems/blog/why-haskell-for-production/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>Haskell is the first programming language we reach for when we build production software systems. This likely seems unusual to anyone who only has a passing familiarity with the language. Haskell has a reputation for being an advanced language with a steep learning curve. It is also often thought of as a research language with limited practical utility.</p>
<p>While Haskell does have a very large surface area, with many concepts and a syntax that will feel unfamiliar to programmers coming from most other languages, it is unrivaled in the combination of developer productivity, code maintainability, software reliability, and performance that it offers. In this post I will cover some of the defining features of Haskell that make it an excellent, industrial-strength language that is well-suited for building commercial software, and why it is usually the first tool we consider using for new projects.</p>
<!--more-->
<h3 id="haskell-has-a-strong-static-type-system-that-prevents-errors-and-reduces-cognitive-load">Haskell has a strong static type system that prevents errors and reduces cognitive load</h3>
<p>Haskell has a very powerful static type system which serves as a programmer aid that catches and prevents many errors before code ever even runs. Many programmers encounter statically typed languages like Java or C++ and find that the compiler feels like an annoyance. By contrast, Haskell’s static type system, in conjunction with compile-time type checking, acts as an invaluable pair-programming buddy that gives instantaneous feedback during development.</p>
<p>There’s a far smaller cognitive load that needs to be maintained when writing Haskell than when writing in languages like Python, JavaScript, or PHP. Many concerns can be completely offloaded to the compiler rather than needing to be remembered by the programmer. For example, when writing Haskell, there’s no need to preemptively ask questions like:</p>
<ul>
<li>Do I need to check whether this field is null?</li>
<li>What if fields are missing from the request payload?</li>
<li>Has this string already been decoded to an integer?</li>
<li>What if this string can’t be decoded to an integer?</li>
<li>Will this operator implicitly convert this integer to a string?</li>
<li>Are these two values comparable?</li>
</ul>
<p>This is not to say that these are questions that never need answering in Haskell; it’s to say that the compiler will throw an error when you need to address one of these issues. For example, it’s possible that a Haskell program needs to handle values that are sometimes not present, but instead of setting any value to <code>NULL</code>, a Haskell programmer must use a <code>Maybe</code> type, which indicates that the value may not be there, and the compiler forces the programmer to explicitly handle the <code>Nothing</code> value; the case where the value is not present.</p>
<p>Haskell’s static type system also leads to other benefits. Haskell code uses type signatures that precede its functions and describe the types of each parameter and return value. For example, a signature like <code>Int -&gt; Int -&gt; Bool</code> indicates that a function takes two integers and returns a boolean value. Since these type signatures are checked and enforced by the compiler, this allows a programmer reading Haskell code to look only at type signatures when getting a sense of what a certain piece of code does. For example, one would not use the type signature above when looking for a function that manipulates strings, decodes JSON, or queries a database.</p>
<p>Type signatures can even be used to search through the entire corpus of Haskell code for a relevant function. Using <a href="https://hoogle.haskell.org/" target="_blank" rel="noopener">Hoogle</a>, Haskell’s API search, we can search for a type signature based off of functionality we know that we need. For example, if we need to convert an <code>Int</code> to a <code>Float</code>, we can search Hoogle for <code>Int -&gt; Float</code> (<a href="https://hoogle.haskell.org/?hoogle=Int+-%3E+Float" target="_blank" rel="noopener">search results</a>), which will point us to the aptly named <code>int2Float</code> function.</p>
<p>Haskell also lets us create polymorphic type signatures through the use of type variables, represented by lowercase type names. For example, a signature of <code>a -&gt; b -&gt; a</code> tells us that that the function takes two parameters of two arbitrary types, and returns a value that whose type is the same as the first parameter. Suppose we want to check whether an element is in a list. We’re looking for a function that takes an item to search for, a list of items, and returns a boolean. We don’t care about the type of the item, so long as the search item and the items in the list are of the same type. So we can search Hoogle for <code>a -&gt; [a] -&gt; Bool</code> (<a href="https://hoogle.haskell.org/?hoogle=a%20-%3E%20%5Ba%5D%20-%3E%20Bool" target="_blank" rel="noopener">search results</a>), which will point us to the <code>elem</code> function. Parametric types are an extremely powerful feature in Haskell and are what enable writing reusable code.</p>
<h3 id="haskell-enables-writing-code-that-is-composable-testable-and-has-predictable-side-effects">Haskell enables writing code that is composable, testable, and has predictable side-effects</h3>
<p>In addition to being statically typed, Haskell is a pure functional programming language. This is one of Haskell’s defining features and what the language is well known for, even amongst programmers that have only heard of Haskell but never used it. Writing in a pure functional style has many benefits, and is conducive to a well-organized code base.</p>
<p>The word “pure” in “pure functional programming” is significant. Purity in this sense means that the code we write is pure, or free of side-effects. Another term that describes this is <a href="https://en.wikipedia.org/wiki/Referential_transparency" target="_blank" rel="noopener">referential transparency</a>, or the property where any expression (e.g.&nbsp;a function call with a given list of parameters) can be replaced with its return value without changing the functionality of the code. This is only possible when such pure functions do not have side effects, such as creating files on the host system, running database queries, or making HTTP requests. Haskell’s type system imposes this sort of purity.</p>
<p>So does being pure mean that Haskell programs cannot have side effects? Certainly not—but it does mean that effects are pushed to the edge of our system. Any functions that perform I/O actions (such as querying a database or receiving HTTP requests) must have a return type that captures this. This means that type signatures like the ones we saw in the previous section (e.g.&nbsp;<code>Int -&gt; Float</code> or <code>a -&gt; [a] -&gt; Bool</code>) are indicators that the corresponding functions do not produce side effects, since <code>Float</code> and <code>Bool</code> are just primitive return types. For a contrasting example that includes a side effect, a function signature of <code>FilePath -&gt; IO String</code> indicates that the function takes a file path and performs an I/O action that returns a string (which is exactly what the <code>readFile</code> function does).</p>
<p>Another feature of a pure functional programming paradigm is higher-order functions, which are functions that take functions as parameters. One of the most commonly used higher-order functions is <code>fmap</code>, which applies a function to each value in a container (such as a list). For example, we can apply a function named <code>square</code>, which takes an integer and returns that integer multiplied by itself, to a list of integers to turn it into a list of squared integers:</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>square ::</span> <span>Int</span> <span>-&gt;</span> <span>Int</span></span>
<span id="cb1-2">square x <span>=</span> x <span>*</span> x</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span>fmap</span> square [<span>1</span>,<span>2</span>,<span>3</span>,<span>4</span>,<span>5</span>] <span>-- returns [1,4,9,16,25]</span></span></code></pre></div>
<p>Code written in this style tends to be both composable and testable. This above example is trivial, but there are many applications of higher-order functions. For example, we can write a function like <code>renderPost</code> which takes a record of post data and returns the version of the post rendered in HTML. If we have a list of posts, we can run <code>fmap renderPost postList</code> to produce a list of rendered posts. Our <code>renderPost</code> function can be used in both the single case and the multi-post case without any changes, because composing it with <code>fmap</code> changes how we can apply it. We can also write tests for the <code>renderPost</code> function and compose it with <code>fmap</code> in our tests when validating the behavior for a list of posts.</p>
<h3 id="haskell-facilitates-rapid-development-worry-free-refactoring-and-excellent-maintainability">Haskell facilitates rapid development, worry-free refactoring, and excellent maintainability</h3>
<p>Through the combination of the aforementioned static types and pure functional style that Haskell has, developing software in Haskell tends to be very fast. One of the common development workflows we employ is relies on a tool called <a href="https://github.com/ndmitchell/ghcid" target="_blank" rel="noopener"><code>ghcid</code></a>, a simple command line tool that relies on the Haskell repl to automatically watch code for changes and incrementally recompile. This allows us to see any compiler errors in our code immediately after saving changes to a file. It’s not uncommon for us to open only a terminal with a text editor and <code>ghcid</code> while developing applications in Haskell.</p>
<p>While manually validating the results of our code is eventually necessary by refreshing a page in a browser or using a tool to validate a JSON endpoint, a lot of this can be deferred until the end of a programming session. Many of the would-be runtime errors a programmer would encounter when writing a web service in a language like Python or PHP are caught immediately and displayed as compiler errors by <code>ghcid</code>. This is a far cry from the need to switch to a browser window and refresh the page after making a change to some code, a development workflow that everyone who has worked on a web application is intimately familiar with.</p>
<p>Beyond the tight feedback loop during development, Haskell code is easy to refactor and modify. Like real world code written in any other language, such code written in Haskell is not write-only. It eventually will need to be maintained, updated, and extended, often by developers that are not the original authors of the code. With the aid of compile-time checking, many code refactors in Haskell become easy; a common refactoring workflow is to make a desired change in one location and then fix one compiler error at a time until the program compiles again. This is far easier than the equivalent changes in dynamically typed languages that offer no such assistance to the programmer.</p>
<p>Proponents of dynamically typed languages will often argue that automated tests supplant the need for compile-time type checking, and can help prevent errors as well. However, tests are not as powerful as type constraints. For tests to be effective, they must:</p>
<ol type="1">
<li>Actually be written, yet many real world code bases have limited testing</li>
<li>Make correct assertions</li>
<li>Be comprehensive (test a variety of inputs) …</li></ol></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.foxhound.systems/blog/why-haskell-for-production/">https://www.foxhound.systems/blog/why-haskell-for-production/</a></em></p>]]>
            </description>
            <link>https://www.foxhound.systems/blog/why-haskell-for-production/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25726588</guid>
            <pubDate>Mon, 11 Jan 2021 09:24:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Rebuilding the spellchecker, pt.2: Just look in the dictionary, they said]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25725583">thread link</a>) | @zverok
<br/>
January 10, 2021 | https://zverok.github.io/blog/2021-01-09-spellchecker-2.html | <a href="https://web.archive.org/web/*/https://zverok.github.io/blog/2021-01-09-spellchecker-2.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    <p><strong><em>This is the second part of the “Rebuilding the spellchecker” series, dedicated to the explanation of how the world’s most popular spellchecker Hunspell works.</em></strong></p>

<p><strong>Quick recap:</strong> <a href="https://zverok.github.io/blog/2021-01-05-spellchecker-1.html">In the first part</a>, I’ve described what Hunspell is; and why I decided to rewrite it in Python. It is an <strong>explanatory rewrite</strong> dedicated to uncovering the knowledge behind the Hunspell by “translating” it into a high-level language, with a lot of comments.</p>

<p>Now, let’s dive into how the stuff really works!</p>

<p>There are two main parts of word-by-word spellchecker algorithms:</p>

<ol>
  <li>Check if a word is correct: <strong>“lookup”</strong> part</li>
  <li>Propose the correction for incorrect words: <strong>“suggest”</strong> part</li>
</ol>

<blockquote>
  <p>Hunspell also implements several other algorithms to be useful as standalone software. It can extract plain text from numerous formats, like HTML or TeX, split it into words (tokenize), correctly handling punctuation—but at the end of the day, a word-by-word correctness check is applied. I excused myself from implementing “wrapper” algorithms: text extraction and tokenization are thoroughly investigated topics, and there are numerous libraries in any language solving it with decent speed and quality.</p>
</blockquote>

<p>Hunspell works on a word-by-word basis (no context is taken into account). Each word is just <strong>looked up</strong> in the <strong>dictionary</strong> loaded from the plaintext  <code>&lt;langname&gt;.dic</code> file in Hunspell-specific format. If it is not considered correct by dictionary lookup (which, as we’ll see soon, is more complex than “is it present in the dictionary”), several algorithms of <strong>suggest</strong> are applied sequentially, trying to find correct words similar to the given one.</p>

<h2 id="hunspells-lookup-algorithm-or-just-look-in-the-dictionary-they-said">Hunspell’s lookup algorithm, or, Just look in the dictionary, they said!</h2>

<p>When coming from English-only spellchecking, the developers tend to perceive the “lookup” part as trivial (e.g., the famous <a href="https://norvig.com/spell-correct.html">Peter Norvig’s article</a> starts from the assumption that only the correction—”suggest”—part deserves some explanations). But that’s not quite so.</p>

<p>The first and most straightforward idea<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup> for the lookup would be: we’ll just take the dictionary (presumably, the flat list of all correct words) and look for our candidate word in this list: if it is there, it is correct. End of story.</p>

<p>Now, Hunspell’s dictionaries exist for many languages and have a plaintext format, which, at the first sight is quite close to a plain word list—so, probably it would be easy to reuse them? Let’s take a look into the <a href="https://github.com/LibreOffice/dictionaries/blob/master/en/en_US.dic"><code>en_US.dic</code></a> in the LibreOffice dictionary repository. You’ll see a list of words in the following format:</p>

<div><div><pre><code>...
acetyl
acetylene/M
ache/DSMG
achene/MS
achievable/U
achieve/BLZGDRS
achievement/SM
...
</code></pre></div></div>

<p>The line <code>ache/DSMG</code> specifies the <em>stem</em> <code>ache</code>, having <code>D</code>, <code>S</code>, <code>M</code>, <code>G</code> <em>flags</em> associated with it. The meaning of flags is defined by <a href="https://github.com/LibreOffice/dictionaries/blob/master/en/en_US.aff"><code>en_US.aff</code></a> (called “affix file”, or just aff-file; every Hunspell dictionary is distributed as a pair of <code>.dic</code> and <code>.aff</code> files).</p>

<h3 id="affix-compression">Affix compression</h3>

<p>In this particular case, all four flags are associated with word <em>suffixes</em>. Here’s the definition of <code>D</code> suffix:</p>

<div><div><pre><code>SFX D Y 4                    # Suffix header: suffix (SFX), with flag D, combinable with prefixes (Y), 4 entries:
SFX D   0  d    e            # * if the stem ends is "e", strip nothing (0), and add "d"
SFX D   y  ied  [^aeiou]y    # * if the stem ends with "y", preceded by non-vowel, strip "y" and add "ied"
SFX D   0  ed   [^ey]        # * if the stem ends with not "e", and not "y", strip nothing, add "ed"
SFX D   0  ed   [aeiou]y     # * "y" with preceding vowel: strip nothing, add "ed"
</code></pre></div></div>

<p>For our <code>ache</code> stem, this definition says that form <code>ached</code> exists. In the similar fashion, <code>S</code> flag defines that word <code>aches</code> exists, <code>G</code> flag defines <code>aching</code>, <code>M</code> flag defines <code>ache's</code>. So, the <code>ache/DSMG</code> line in <code>.dic</code> file specifies 5 correct words: “ache”, “ached”, “aches”, “aching”, “ache’s”.</p>

<blockquote>
  <p>Note: the fact that flags are similar to the suffixes they define (<code>S</code> flag defines suffix <code>-s</code> and so on) is just a convention. It is rather a handy mnemonics that creators of <code>en_US</code> dictionary used, and there could be any other symbol.</p>
</blockquote>

<p>In the similar fashion, word prefixes might be defined (specified with flags in dic-file, and described with <code>PFX</code> directive in aff-file). Say, this definition:</p>



<p>…defines these forms: <em>advantage, advantage’s, advantaging, advantaged, advantages, disadvantage’s, disadvantaging, disadvantaged, disadvantages, disadvantage</em> (all combinations of four suffixes and a prefix “dis-“).</p>

<p>This technique of “packing” dictionaries is called <strong>affix compression</strong>, and its primary goal is to optimize dictionary size: on disk and in memory. It becomes extremely important for languages with rich inflection. For example, in English one stem might produce no more than ten forms, but in Ukrainian it could easily be dozens or even hundreds, so the amount of possible correct forms quickly grows to tens of millions. “Just a flat list of all known words” <em>might</em> become impractical (not on today’s top MacBook, probably, but still), and that’s where the affix compression comes in handy.</p>

<blockquote>
  <p>This is not the only possible approach to make dictionary storage more effective: for example, <a href="https://github.com/morfologik/morfologik-stemming">morfologik</a> (the default internal spellchecker of the most widely used open-source proofreading software <a href="https://languagetool.org/">LanguageTool</a>) codes <em>all</em> possible forms in binary files, using finite-state automata. And this approach is very efficient by speed and memory, but very hard for humans to edit and review, and thus, to keep dictionary up-to-date.</p>
</blockquote>

<p>Another benefit of splitting words into stems and affixes: when Hunspell’s user wants to add a new word to their personal dictionary, Hunspell allows to just specify “it is inflected the same way as (some other word)”, thus sparing the user of teaching the dictionary “‘monad’ is a word, and ‘monads’ too, as well as ‘monad’s’…”.</p>

<p>Note, though, that “suffixes” and “prefixes” specified in some language’s dictionary not necessarily correspond to <em>grammatical</em> suffixes/prefixes of the language. The splitting into stems and affixes is deduced automatically by dictionary authoring tools from flat word lists, so it is up to probabilities whether “common endings” deduced have any grammatical meaning. Actually, Hunspell’s format has a rich <a href="https://manpages.debian.org/experimental/libhunspell-dev/hunspell.5.en.html#Optional_data_fields">sub-language to specify grammatical information</a>, but of all LibreOffice and Firefox dictionaries I’ve checked, only a few (Latvian, Slovak, Galician, Breton) made use of this feature.</p>

<blockquote>
  <p>One important factor to mention is Hunspell’s limitation for the amount of suffixes/prefixes a word may have. Currently, the software understands no more than 2 suffixes and 2 prefixes in a word, which is lower than the common number of grammatical suffixes a lot of languages allow. Most of the dictionaries solve this by “linearizing” the word list: Ukrainian word “громадянство” (citizenship) grammatically consists of the stem “громад-“ and suffixes “-ян-“, “-ств-“, “-о” (the latter is called an ending in grammatically correct terms), but Hunspell’s Ukrainian dictionary includes the full word “громадянство”. For other languages, the suffix number limitation makes Hunspell totally unusable, so to spellcheck the Finnish, you need to install <a href="https://voikko.puimula.org/">Voikko</a> spellchecker.</p>
</blockquote>

<p><strong>Affix compression comes with a price</strong> paid in lookup algorithm complexity and performance. Instead of just a quick lookup through a hashtable or other lookup-optimized structure, we now have to:</p>

<ol>
  <li>Check if the whole word is in the list of stems. If yes, it is correct,
    <ul>
      <li>…unless it has a flag corresponding to the aff-file directive “this stem <em>requires</em> prefixes or suffixes”.</li>
    </ul>
  </li>
  <li>If no, check if the word has some of the known suffixes; and if so, whether the stem without one of those suffixes is in the stem list, <em>and</em> has a flag corresponding to this suffix.</li>
  <li>If no, check if one more suffix can be found in the stem (then we’ll have a stem and two suffixes, and need to check the compatibility of their flags).</li>
  <li>Repeat with prefixes (up to two), and with all possible suffix-prefix combination (taking into account whether suffix and prefix both have “cross-product” allowed).</li>
  <li>Consider that suffixes and prefixes can have flags of their own, specifying “suffixes with this flag might be attached after me”, or “if used, this suffix requires that at least one other affix would be present” and … many other things.</li>
  <li>And there are funny cases like <code>CIRCUMFIX</code> flag: if the suffix has it, this means that this suffix is only allowed in words having a prefix with the same flag.</li>
</ol>

<blockquote>
  <p>It is <em>still</em> a simplified description. To follow the algorithm in full, you can read Spylls docs starting from <a href="https://spylls.readthedocs.io/en/latest/hunspell/algo_lookup.html#spylls.hunspell.algo.lookup.Lookup.affix_forms"><code>Lookup.affix_forms</code></a>, follow the links to methods it invokes, and read inline comments under “Show code”.</p>
</blockquote>

<p>Performance-wise, for each word correctness check, there could be many lookups through known suffixes and prefixes lists, and quite a few dictionary lookups (as consecutive chopping off of suffixes and prefixes produces new stems we need to check).</p>

<p>And once you tackle the affixes problem, it is all uphill from there!</p>

<p><strong>Stay tuned for the next installment about the Hunspell lookup, where we’ll cover word compounding problem, and some other important edge cases of the word correctness check.</strong> Follow me <a href="https://twitter.com/zverok">on Twitter</a> or <a href="https://zverok.github.io/subscribe.html">subscribe to my mailing list</a> if you don’t want to miss the follow-up!</p>


  </article></div>]]>
            </description>
            <link>https://zverok.github.io/blog/2021-01-09-spellchecker-2.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25725583</guid>
            <pubDate>Mon, 11 Jan 2021 06:54:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Working Off-Grid Efficiently]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25723819">thread link</a>) | @zdw
<br/>
January 10, 2021 | https://100r.co/site/working_offgrid_efficiently.html | <a href="https://web.archive.org/web/*/https://100r.co/site/working_offgrid_efficiently.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div class="page">
        
        <ul>
            <li><a href="#intro">Introduction</a></li>
            <li><a href="#power">Power management</a></li>
            <li><a href="#internet">Internet</a></li>
            <li><a href="#data">Data storage</a></li>
            <li><a href="#software">Software</a></li>
            <li><a href="#hardware">Hardware</a></li>
            <li><a href="#conscientious">Conscientious living</a></li>
        </ul>
<h2 id="intro">Introduction</h2>

<img src="https://100r.co/media/blog/working/dworking5.jpg" loading="lazy">

<p>Our traveling studio has operated off-the-grid since 2016.</p>

<p>For the first 3 years we tested the limits of our space, and at first, it was difficult to create new things, as we had to make time to learn how to solve the underlying problems. Our boat was not just an office, it was also our house and transport. As for us, we were artists, but also had to be plumbers, deckhands, electricians, captains, janitors and accountants.</p>

<p>Our main problems as a studio were <b>internet scarcity</b>, <b>power management</b>, <b>data storage</b> as well as <b>hardware</b> and <b>software failures</b>. Overtime we found ways to balance work, pleasure and maintenance. Here are some of the lessons we learnt.</p>

<h2 id="power">Power management</h2>

<img src="https://100r.co/media/blog/working/dworking6.jpg" loading="lazy">

<p>Our work schedule is tied to the weather, as we depend on solar energy to power our computers. By looking at the forecast, we can determine when we will get the most work done: consecutive days of sun grant us enough power for video-editing, while overcast days are reserved for low-power work, like writing, coding and planning.</p>

<p>There are times when we must resort to secondary power sources, like our small generator or our engine's alternator, but we tend to prefer to <a href="https://www.youtube.com/watch?v=9ua_qxjbBTc">wait for the sun to return</a>. Waiting hasn’t affected our productivity, as we don't adhere to strict 8-hour workdays.</p>

<p>We tend to work only in the morning, leaving us time to pursue other interests in the afternoon. In our old life, we found that the 40-hour workweek ﻿<a href="https://www.raptitude.com/2010/07/your-lifestyle-has-already-been-designed/" target="_blank">kept free-time scarce</a>, resulting in us spending more for convenience, gratification and distraction.</p>

<p>Computers are generally power-sucking vampires. Choosing different softwares, operating systems, or working from machines with a lower draw (ARM) or even throttling the CPU, are some of the many things we do to lower our power requirements. The way that software is built has a substantial impact on the power consumption of a system, it is shocking how cpu-intensive modern programs can be.</p>

<p>Choosing software designed for low-end PCs is a good solution, it is also possible to throttle processes on your machine by using a ‘throttling controller’. The basic idea is like the throttle in a car, it allows to set the rate at which your system will operate and consume power. Another sure way to save battery is to limit multitasking. Disabling notifications, <a href="https://addons.mozilla.org/en-CA/firefox/addon/noscript/">scripts</a>, auto-playing videos or using internet browsers without opening multiple tabs at once are some of the many ways to achieve this.</p>

<p>Power consumption is also something to consider when choosing a computer. In evenings, if we need to work on light tasks, we switch to our low-power machines like the <a href="https://100r.co/site/raspberry_pi.html">Raspberry Pi</a>. To illustrate the difference in power draw, a Pi4 uses 2.85 W when idle, while a Macbook Pro uses 6-12 W. Raspberry Pis are backups to our main computers, as they are inexpensive and can run off small batteries.</p>

<h2 id="internet">Internet</h2>

<img src="https://100r.co/media/blog/working/dworking1.jpg" loading="lazy">

<p>Internet access is the woe of any working nomad. Internet is sometimes spotty, and data in some countries is slow, expensive, or limited to small blocks at a time. While circumnavigating the Pacific, we amassed sim cards, pocket WiFis, and have often used connections from businesses on land. Overtime, we found ways to lessen our dependence on internet, and to save on bandwidth.</p>

<p>With limited access, it is important to use online time wisely. Prior to connecting we make a list of tasks that we must do, such as pushing updates and making backups of our data online. It’s easy to get side-tracked on the internet, with websites designed to grab and keep our attention. When checking social media, we disable auto-playing videos and image previews to save bandwidth.</p>

<p>When we have a reliable internet connection, we gather copies of all the online material we will need. We keep offline versions of entire websites, writing guides, articles and even whole sections of Wikipedia. If we find ourselves without a connection, we can still solve our problems by using our offline mirrors. By the way, you can <a href="https://github.com/hundredrabbits/100r.co/archive/master.zip">download our entire website</a>.</p>

<p>We research our destinations ahead of time to make sure we’ll have a reliable connection when we need it. This means we’ll be spending less time in secluded areas, and more time in city centers near a cell tower or WiFi signal. With some planning it is possible to have both paradise and connectivity, we found such a place in <a href="https://100r.co/site/internet_in_paradise.html#internet">Huahine</a> in French Polynesia, and again in Fiji. Internet access will only get better as far-flung island nations gain purchasing power.</p>

<h2 id="data">Data storage</h2>

<img src="https://100r.co/media/blog/working/dworking2.jpg" loading="lazy">

<p>Hardware failure is common on boats due to the hostile environment. Saltwater is the kryptonite of electronics. This is why it is important to backup data often to avoid losing work. There are advantages and disadvantages with all methods of data storage, but I’ll outline the most useful ones for sailors:</p>

<p><b>Cloud storage:</b> For a fee, you can back up your data online and sync files from your desktop. This method doesn’t eliminate physical storage as data can’t be synced to the cloud without a connection. Offloading data storage to a centralized service is problematic in other ways, because services have rules and owners and processes which can complicate things. For instance, country politics have made it that Google restricts access to some of its business services in certain countries or regions, such as China, Crimea, Cuba, Iran, Sudan, and Syria. Whatever data you have stored with Google Drive, if traveling to any of these countries will not be accessible. As conflicts arise, more countries can end up on that list. We keep documents we don’t need regular access to on the cloud, with copies on hard disks.</p>

<p><b>Hard copies:</b> Paper is a stable and widely accessible material, unlike digital devices which are subject to breakages and obsolescence. There’s a good reason books and other documents from centuries ago are still readable today. We like to keep printed copies of websites and other online reference materials, such as grammar guides for writing, or language manuals for coding. Keeping data like this means we always have access and aren’t limited to our computer’s battery.</p>

<p><b>External hard drives:</b> A hard drive is the best balance of practical and reliable for storage. However, hard drives are rated for a limited number of read/write cycles, and can be expected to fail eventually. To prevent data loss due to HD failure, it’s a good idea to store the same data across multiple hard drives.</p>

<p><b>Offline databases:</b> Keeping an offline collection of websites on computers or HD ensures constant access, and reduces the energy associated with re-loading them repeatedly. It’s possible to save web pages with most browser by selecting File &gt; Save Page As. To access the page offline, click on the HTML file. Another option is to mirror entire web sites using <a href="https://www.gnu.org/software/wget/" target="_blank">command-line tools</a>. We keep offline databases full of notes on a variety of subjects to refer to when there’s no internet.</p>

<p>Keeping files on the cloud, on hard drives and hard copies gives our floating studio the redundancy required to ensure reliability.</p>

<h2 id="software">Software</h2>

<p>Software has a big impact on productivity, they need to be reliable and fast. Those that require heavy updates, that have a high CPU usage and that need frequent connectivity to function are problematic for working sailors.</p>

<p>Much of the software on the market is designed by people living on the grid with unlimited access to internet. Tools locking up at sea, asking for a connection to continue working don’t float on a boat. Adobe products are a good example, as they require an internet connection periodically for subscription validation. If away from big cities, you may open your computer in an atoll to find that you no longer have access to the tool you need to get things done. Choosing a tool that doesn’t require a subscription is <b>essential</b> for working nomads that don’t have a reliable connection.</p>

<p>In our first year, we struggled to download the frequent and mandatory 10GB software updates from Apple to release our software on their platform, while on slow Polynesian internet. Processor-intensive software or apps is a strain on limited power and bandwidth, but it doesn’t have to be that way. The way developers write them can affect the power consumption of the resulting product. Chat rooms and bare bones text editors aren’t supposed to be process-heavy, and yet the popular communication platform Slack requires <a href="https://josephg.com/blog/electron-is-flash-for-the-desktop/" target="_blank">outrageous amounts of ram and CPU</a> to function. This is because Slack is embedding the entirety of Google Chrome in their app. Making software this way is costly to off-grid users, or those on slow connections, but luckily there are <a href="https://github.com/mayfrost/guides/blob/master/ALTERNATIVES.md" target="_blank">many alternatives</a>.</p>

<p>Our computer batteries should not need to grow ever larger only to support these bloatwares, nor should we need to add extra solar to power them. Just as you would look at the nutritional content of food products at the grocery store, find out how much energy your apps are consuming.</p>

<h2 id="hardware">Hardware</h2>

<img src="https://100r.co/media/blog/working/working3.jpg" loading="lazy">

<p>Computers are subject to water intrusion and saltwater corrosion, but with some care they can survive in a normal marine environment. We solved most of the problems by cleaning external connections often, and by storing them in a sealed box with some desiccants after each use. The main issue with computers on boats, is that it is difficult to source parts when they break. To make matters worse, many modern machines have non-replaceable batteries, proprietary storage, and soldered-in RAM. The parts that fail the most are power connectors, external connections and batteries.</p>

<p>Leaving a port with spare parts is a good tactic, but leaving with backup PCs is even better. There are many good inexpensive computers on the market, like notebook processors (Pinebook, EeeBook) and single-board computers (Raspberry Pis, Pine64). We carry 3 extra Raspberry Pi computers as backups to our main laptops, as they are inexpensive and small. These computers run on lower voltage, which lower overall …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://100r.co/site/working_offgrid_efficiently.html">https://100r.co/site/working_offgrid_efficiently.html</a></em></p>]]>
            </description>
            <link>https://100r.co/site/working_offgrid_efficiently.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25723819</guid>
            <pubDate>Mon, 11 Jan 2021 02:58:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[TV Tuner History]]>
            </title>
            <description>
<![CDATA[
Score 62 | Comments 10 (<a href="https://news.ycombinator.com/item?id=25721804">thread link</a>) | @parsecs
<br/>
January 10, 2021 | https://www.maximus-randd.com/tv-tuner-history-pt5.html | <a href="https://web.archive.org/web/*/https://www.maximus-randd.com/tv-tuner-history-pt5.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.maximus-randd.com/tv-tuner-history-pt5.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25721804</guid>
            <pubDate>Sun, 10 Jan 2021 23:48:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Discovering and exploring mmap using Go]]>
            </title>
            <description>
<![CDATA[
Score 95 | Comments 6 (<a href="https://news.ycombinator.com/item?id=25720731">thread link</a>) | @brunoac
<br/>
January 10, 2021 | https://brunocalza.me/2021/01/10/discovering-and-exploring-mmap-using-go/ | <a href="https://web.archive.org/web/*/https://brunocalza.me/2021/01/10/discovering-and-exploring-mmap-using-go/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            


            <section>
                <div>
                    <p>Recently I've come to know the concept of <strong>memory-mapped files</strong> while watching a lecture of the course <a href="https://15445.courses.cs.cmu.edu/fall2019/">Intro to Database Systems</a> of <a href="https://twitter.com/andy_pavlo">Andy Pavlo</a> on database storage. One of the main problems a database storage engine has to solve is <strong>how to deal with data in disk that is bigger than the available memory</strong>. At a higher level, the main purpose of a disk-oriented storage engine is to manipulate data files in a disk. But if we assume that the data in the disk will eventually get bigger than the available memory, we cannot simply load the whole data file into memory, do the change, and write it back to disk.</p><p>This is not a new problem in Computer Science. When operational systems were being developed in the early 1960s, a similar problem was faced: <strong>how can we run programs stored in disk that are larger than the available memory?</strong> A solution to this problem was made by a group in Manchester, implemented on the <a href="https://en.wikipedia.org/wiki/Atlas_(computer)">Atlas Computer</a>, in 1961. It was called <em>virtual memory</em>. The <em>virtual memory</em> gives a running program the illusion that it has big enough memory, despite the fact that the computer does not have enough.</p><p>We are not going to go deep on how <em>virtual memory</em> works. Just have in mind that when a program is accessing memory it is accessing the <em>virtual memory</em>. And maybe the data the program is trying to access is not actually in memory, but it does not matter. The operational system will make pretend that it is by going to disk, and putting it there, and replace an old chunk of memory that is not going to be used.</p><p>So, one of the ways a database storage engine can solve the larger than memory problem is to make use of <em>virtual memory</em> and the concept of <strong>memory-mapped files</strong>.</p><p>In Linux, we can make this use by using the system call <a href="https://man7.org/linux/man-pages/man2/mmap.2.html">mmap</a> that lets you map a file, no matter how big, directly into memory. If your program needs to manipulate the file, all it needs is to manipulate the memory. The operating system handles the writes to disk for you.</p><p>In some occasions, programmers find this method more convenient than the usual system calls: <a href="https://man7.org/linux/man-pages/man2/open.2.html">open</a>, <a href="https://man7.org/linux/man-pages/man2/read.2.html">read</a>, <a href="https://man7.org/linux/man-pages/man2/write.2.html">write</a>, <a href="https://man7.org/linux/man-pages/man2/lseek.2.html">lseek</a> and <a href="https://man7.org/linux/man-pages/man2/close.2.html">close</a>.</p><h3 id="a-simple-demonstration">A simple demonstration</h3><p>Here is a small example of how you can take advantage of this in Go using the package <a href="https://github.com/edsrzf/mmap-go">mmap-go</a>:</p><pre><code>package main

import (
	"os"
	"fmt"
	"github.com/edsrzf/mmap-go"
)

func main() {
	f, _ := os.OpenFile("./file", os.O_RDWR, 0644)
	defer f.Close()
	
	mmap, _ := mmap.Map(f, mmap.RDWR, 0 )
	defer mmap.Unmap()
	fmt.Println(string(mmap))
	
	mmap[0] = 'X'
	mmap.Flush()
}</code></pre><figure><img src="https://asciinema.org/a/pRS8PvTRHksnCVQgSOWvPBF3a.svg" alt="asciicast"></figure><p>The beauty is that we could have a much bigger file, and the solution would still work. We would not have to worry about managing memory in order to avoid it filling up.</p><h3 id="detailing-mmap-capabilites">Detailing <em>mmap</em> capabilites</h3><p>We're going to explore more <em>mmap</em> functionalities from the point of view of the API provided by <a href="https://github.com/edsrzf/mmap-go">mmap-go</a>. There are probably more features that the <a href="https://godoc.org/golang.org/x/sys/unix#Mmap">native syscall</a> provides that this library does not implement.</p><h4 id="the-prot-argument">The <code>prot</code> argument</h4><p>Here is the <code>mmap.Map</code> signature</p><pre><code>func Map(f *os.File, prot, flags int) (MMap, error) 
</code></pre><p>Let's look at <code>prot</code> first. The <code>prot</code> argument lets you specify the protection levels of your mapping: <code>RDONLY</code>, <code>RDWR</code>, <code>EXEC</code> are the options provided for <code>mmap-go</code>. These levels are pretty straightforward, <code>RDONLY</code> means you can only read from the mapping, <code>RDWR</code> means you can also write, and <code>EXEC</code> means you can execute code on that mapping. &nbsp;Here is the description of <code>prot</code> from the Linux <code>man</code>:</p><pre><code>The prot argument describes the desired memory protection of the
mapping (and must not conflict with the open mode of the file).
It is either PROT_NONE or the bitwise OR of one or more of the
following flags:

PROT_EXEC
    Pages may be executed.

PROT_READ
    Pages may be read.

PROT_WRITE
    Pages may be written.

PROT_NONE
    Pages may not be accessed.
</code></pre><p>In the <a href="https://godoc.org/golang.org/x/sys/unix">unix package</a>, those flags are: <code>unix.PROT_EXEC</code>, <code>unix.PROT_READ</code>, <code>unix.PROT_WRITE</code> and <code>unix.PROT_NONE</code>.</p><h4 id="experimenting-with-prot_exec-flag">Experimenting with <code>PROT_EXEC</code> flag</h4><p>I've become intrigued by the <code>EXEC</code> flag and wanted to see an example of how that works. I've Google and could not find any example. So I tried a search in Github by <code>PROT_EXEC</code> and found a good example in <code>C</code>: <a href="https://github.com/onesmash/MMapExecDemo">MMapExecDemo</a>. I replicated this example in <code>Go</code> using <code>mmap-go</code>.</p><p>The first step was to create a function that I wanted to be put in memory by <code>mmap</code> allocation, compile it, and get its assembly opcodes.</p><p>I created the <code>inc</code> function in <code>inc.go</code> file</p><pre><code>package inc

func inc(n int) int {
	return n + 1
}

</code></pre><p>compiled it with <code>go tool compile -S -N inc.go</code>, then got its assembly by calling <code>go tool objdump -S inc.o</code>.</p><pre><code>func inc(n int) int {
  0x22b                 48c744241000000000      MOVQ $0x0, 0x10(SP)
        return n + 1
  0x234                 488b442408              MOVQ 0x8(SP), AX
  0x239                 48ffc0                  INCQ AX
  0x23c                 4889442410              MOVQ AX, 0x10(SP)
  0x241                 c3                      RET
</code></pre><p>With this, we can build represent our function in bytes on our code</p><pre><code>code := []byte{
        0x48, 0xc7, 0x44, 0x24, 0x10, 0x00, 0x00, 0x00, 0x00,
		0x48, 0x8b, 0x44, 0x24, 0x08,
		0x48, 0xff, 0xc0,
		0x48, 0x89, 0x44, 0x24, 0x10,
		0xc3,
}
</code></pre><p>We allocate our memory with <code>mmap</code>.</p><pre><code>memory, err := mmap.MapRegion(nil, len(code), mmap.EXEC|mmap.RDWR, mmap.ANON, 0)
if err != nil {
    panic(err)
}
</code></pre><p>In this call, we're using a more complete function called <code>MapRegion</code> that lets you specify how much memory you are allocating (<code>Map</code> allocates the size of the underlying file) and the offset of the file.</p><p>In the beginning, we said that the main purpose of <code>mmap</code> was to create a mapping between a file and memory. But in this call we are not indicating any file. <code>mmap</code> can be used just a regular memory allocater by setting <code>nil</code> to the <code>*os.File</code> argument and <code>mmap.ANON</code> to the <code>flags</code> argument. We will talk about more <code>mmap.ANON</code>. Since we are not mapping any file, the offset is <code>0</code>.</p><p>So we have memory allocated with the same size of our code <code>len(code)</code>. Since we set the flag <code>mmap.RDWR</code>, we can copy our <code>code</code> to <code>memory</code>.</p><pre><code>copy(memory, code)
</code></pre><p>We have the code of our <code>inc</code> function in memory. In order to execute it, we have to cast that memory address to a function with a signature that matches the signature of our compiled <code>inc</code>.</p><pre><code>memory_ptr := &amp;memory
ptr := unsafe.Pointer(&amp;memory_ptr)
inc := *(*func(int) int)(ptr)
</code></pre><p>When we call <code>inc</code>, we are executing the code we put in memory. That only works because of the flag <code>mmap.EXEC</code>. If that flag was not set, a <code>segmentation violation</code> would occur.</p><pre><code>fmt.Println(inc(10)) // Prints 11
</code></pre><p>I don't know if this is a real use case. I just wanted to see what it meant to execute code that you put in memory. And there are probably other ways of achieving the same with regular memory allocation and calls to <a href="https://man7.org/linux/man-pages/man2/mprotect.2.html">mprotect</a>.</p><p>One question that may come up is: but the code is already in the <code>code</code> variable, can't we just execute it? No, because the memory static allocated to <code>code</code> is not executable. Can we make it executable? I've tried to use <a href="https://man7.org/linux/man-pages/man2/mprotect.2.html">mprotect</a> on it but still got <code>segmentation violation</code>.</p><p>Here is the full working <a href="https://gist.github.com/brunoac/b9ff4ad46c27926e5e4f078133d0de79">gist</a>.</p><h4 id="the-flags-argument">The <code>flags</code> argument</h4><p>We can have many processes mapping the same memory region. This argument lets us decide about the visibility of the updates happening in the mapping. There are many flags, and you can check them out at <a href="https://man7.org/linux/man-pages/man2/mmap.2.html">mmap</a>. The important ones are <code>unix.MAP_SHARED</code>, <code>unix.MAP_PRIVATE</code> and <code>unix.MAP_ANON</code>.</p><p><code>MAP_SHARED</code> means that changes to the mapping are visible to all processes and will also occur at the underlying mapped file, although we cannot control when.</p><p><code>MAP_PRIVATE</code> means the changes are private and other processes will not see them. And also, they are not carried through to the underlying file.</p><p><code>MAP_ANON</code> means that there is not going to be a mapped file. It is useful for sub-processes communication with shared memory.</p><p>I've got confused about the <code>mmap-go</code> library implementation. It only provides the <code>mmap.ANON</code> flag, that we used in the above example. If you want your mapping to be private, you can set the <code>mmap.COPY</code> flag to the <code>prot</code> argument. Anyways, you can always use the flags provided by the <code>unix</code> package implementation.</p><h4 id="locking-and-flushing">Locking and flushing</h4><p>Two other nice methods, <code>Lock</code> and <code>Flush</code>, are provided by the API of <code>mmap-go</code>. The <code>Lock</code> method calls the <a href="https://man7.org/linux/man-pages/man2/mlock.2.html">mlock</a> system call that prevents the mapping to be paged out to disk. And the <code>Flush</code> method calls the <a href="https://man7.org/linux/man-pages/man2/msync.2.html">msync</a> system call that forces the data in memory to be written to disk. This is a good way to trying to have more control over how and when data is flushed to disk.</p><h3 id="wrapping-up">Wrapping up</h3><p>I felt kind of stupid of knowing about <code>mmap</code> after so long. I don't remember it being brought in my college class. For some reason, I felt amazed by it and its capabilities and decided to dig deeper. I like databases and I'm aiming to get a better grasp of them. This means that <code>mmap</code> cannot go unnoticed from my learning. For future posts, I'll try to bring about the benefits and drawbacks of using <code>mmap</code>, which projects use it, and what kind of problems it is suited for.</p><p>Even though the <code>mmap</code> can be used to solve that database problem we stated in the beginning, and many modern databases use it, <a href="https://twitter.com/andy_pavlo">Andy Pavlo</a> advocates against it and have three lecture on how to databases, that don't use <code>mmap</code>, manage data.</p><p>If you like this kind of content, follow me on <a href="https://twitter.com/brunocalza">twitter</a>. You may find more related stuff there.</p>
                </div>
            </section>



        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://brunocalza.me/2021/01/10/discovering-and-exploring-mmap-using-go/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25720731</guid>
            <pubDate>Sun, 10 Jan 2021 22:21:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Build a wall of testimonials with videos/tweets/text. Here is mine]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 8 (<a href="https://news.ycombinator.com/item?id=25720322">thread link</a>) | @damechen
<br/>
January 10, 2021 | https://testimonial.to/testimonial/all | <a href="https://web.archive.org/web/*/https://testimonial.to/testimonial/all">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://testimonial.to/testimonial/all</link>
            <guid isPermaLink="false">hacker-news-small-sites-25720322</guid>
            <pubDate>Sun, 10 Jan 2021 21:47:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Quick Overview of Julia language [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 139 | Comments 48 (<a href="https://news.ycombinator.com/item?id=25719454">thread link</a>) | @gurjeet
<br/>
January 10, 2021 | http://algorithmsbook.com/files/appendix-g.pdf | <a href="https://web.archive.org/web/*/http://algorithmsbook.com/files/appendix-g.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://algorithmsbook.com/files/appendix-g.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25719454</guid>
            <pubDate>Sun, 10 Jan 2021 20:38:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Cooking for Founders]]>
            </title>
            <description>
<![CDATA[
Score 75 | Comments 66 (<a href="https://news.ycombinator.com/item?id=25719188">thread link</a>) | @tylertringas
<br/>
January 10, 2021 | https://tylertringas.com/cooking-for-founders/ | <a href="https://web.archive.org/web/*/https://tylertringas.com/cooking-for-founders/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
									<p>With COVID-19 forcing many of us indoors and cooking more (yes, this post took a little longer to go live than I planned), there’s never been a better time to really learn how to cook. I grew up not learning much about how to cook and taught myself as an adult. Over the last 5-7 years I went from someone who could do the absolute bare minimum (boil pasta, cook chicken breast, etc) to genuinely quite a decent cook. I can easily whip up dinner for 4-6 friends without stressing, cook healthy dinners at home most nights of the week, run a barbecue for 12 people, and have a small quiver of fancy dishes to impress friends, family, and my wife from time to time. This post is mostly about what works for me, but I’m calling it Cooking For Founders because I think it will resonate with a lot of entrepreneurs who think like me.</p>
<p>The goal of this post is not to teach you how to cook but to provide fairly comprehensive, but also minimum viable, roadmap for going from a cooking noob to solid home chef.</p>
<h2>Why you should cook</h2>
<p>Until I was about 25 or so I really didn’t cook much. I lived in places like NYC and London where restaurants were always open and ubiquitous and especially in these cities, it’s a perfectly reasonable position to just not bother learning to cook well. But I want to make the case that even if you have world class restaurants and food delivery services on demand, you should learn to cook.</p>
<p><strong>Social: </strong>Home cooked meals are an awesome offer that people are very likely to take you up on and really appreciate. Cooking well is sexy and makes for an awesome date night. Dinner parties are fantastic well to meet new people and create a vibrant personal and professional network. Taking charge of a meal is a great way to bring your family together or impress your in-laws.</p>
<p><strong>Physical Health: </strong>Even if you aren’t bothering with any particular diet (low carb, paleo, etc), cooking at home is almost always going to be more nutritious than food from restaurants. Getting actually healthy food from restaurants/delivery is almost always expensive. Cooking at home is an affordable way to get great nutrition.</p>
<p><strong>Mental Health:&nbsp;</strong>This may be more specific to me, but I find cooking to be fantastic for my mental health. In my house I’m the one cooking about 90% of the time and I’m not into the mega meal prep strategies where you cook food for the whole week. So, most days, I’m cooking something fresh for dinner. The need to start cooking prep in time for a reasonable dinner puts a natural stopping point in my work day and then I get to switch to a very focused mono-tasking activity. This routine is, for me, a kind of meditation that separates the work day and let’s my brain process the events of the day.</p>
<h2>Meta-learning Tips for Learning to Cook</h2>
<p>Learning to cook is not exactly easy. There is an infinite amount of recipes, techniques, resources, diets, and on and on to consume. It can be overwhelming. Learning to cook is almost always laden with failures along the way. You’ll screw up some recipes, ruin some dishes, and get halfway through a complex recipe before realizing you’re missing some essential ingredient. Here are some lessons I’ve learned on how to learn.</p>
<p><strong>Find your YouTube &amp; TikTok muses</strong></p>
<p>There is an infinite amount of cooking content on the internet, but when you find a particular chef or channel that really speaks your language, subscribe and binge their entire backlog. Lots of channels out there will skip essential explanations, use overly exotic ingredients, or complex unnecessary techniques so when you find one that consistently speaks to you, lock it in. Some ones I like:</p>
<ul>
<li><a href="https://www.youtube.com/user/helenrennie">Helen Rennie (YouTube)</a></li>
<li><a href="https://www.youtube.com/user/foodwishes">Food Wishes (YouTube)</a></li>
<li><a href="https://www.youtube.com/user/SeriousEats">Serious Eats (YouTube)</a></li>
<li><a href="https://www.tiktok.com/@thatdudecancook?lang=en">@thatdudecancook (TikTok)</a></li>
<li><a href="https://www.tiktok.com/@sad_papi?lang=en">@sad_papi (TikTok)</a></li>
</ul>
<p><strong>Have a backup plan</strong></p>
<p>Learning to cook and feeding yourself can be two different things, especially when you are first starting out and failure rates are high. If you are going to try a new recipe for the first time on a busy week night that’s supposed to be your dinner that night (1) go for it! (2) have a frozen pizza or some other quick and easy back up plan ready in case you end up ruining the dish. It’s a really negative feedback loop to mess up a recipe and having to end up eating cereal for dinner, so have a backup plan.</p>
<p><strong>Read/watch the recipe several times well before cooking</strong></p>
<p>Read or watch your recipes <em>carefully,&nbsp;</em>several times, in preparation for trying a new recipe. It’s easy to miss, especially at first, that the recipe actually requires marinating over night, or needs buttermilk or some other ingredient you don’t typically have on hand. Don’t just plop open the recipe book at 7p and start with Step #1.</p>
<p><strong>Stick to a few core cookbooks</strong></p>
<p>Again, it’s easy to get overwhelmed by the millions of cookbooks out there. Like YouTube channels, I recommend finding a few comprehensive cookbooks that work for you and sticking to them for years until you get very confident with a wide variety of techniques. With cookbooks, Kindle will work but having the physical copy can also be really helpful (or honestly I usually get both). Here are some that I recommend:</p>
<ul>
<li><a href="https://amzn.to/3seCIbQ">How To Cook Everything, Mark Bittman</a></li>
<li><a href="https://amzn.to/3oyrCwn">Cooking For Geeks, Jeff Potter</a></li>
<li><a href="https://amzn.to/3sfvapE">Salt, Fat, Acid, Heat, Samin Nosrat</a></li>
<li><a href="https://amzn.to/38wzSre">The Four Hour Chef, Tim Ferriss</a></li>
</ul>
<h2>Essential Concepts</h2>
<p>The books and channels above all have great introductions to all of these concepts so I’m not going to try to actually cover them here, but I think it’s useful have a few simple concepts to check off as you read/watch through the first few.</p>
<p><strong>What heat does (chemistry)</strong></p>
<p>Make sure you pay attention to the sections on the basic chemistry of heat. The vast majority of cooking is just different ways applying heat to food and it’s critical to understand what heat is doing to different kinds of foods. For the most part heat is either (1) denaturing proteins or (2) producing a Maillard Reaction. Denaturing proteins is the slow gradual cooking process that turns eggs from runny to scrambled or steak from rare to well done. Different foods have different kinds of proteins which denature at different temperatures and in different ways. The Maillard Reaction is browning (mostly on on meats and vegetables) and happens at very high heat and low moisture environments. Read up on these carefully. Cooking for Geeks covers these the best in my opinion.</p>
<p><strong>Different kinds of heat transfer (physics)</strong></p>
<p>Similarly to understanding what heat does, it’s really important to have a basic grasp of the various methods of applying heat to food. Baking, broiling, roasting, sautéing, braising, searing, sous vide, boiling, and so on, are all just different methods for applying heat. Some, like baking, use convection where the air is heated up around the food, and others, like searing in a pan, use conduction where the heat is transferred directly surface to surface. A cast iron pan takes a very long time to heat up and stays hot for a long time, whereas the air in your oven can dissipate its heat quickly. Understanding these basic concepts will give you the architecture for understanding for why you should keep the oven door closed as much as possible, dry your meat before searing, and pre-heat your heavy pans for longer than your light ones.</p>
<p><strong>Keep it simple</strong></p>
<p>When you are first learning to cook I recommend avoiding complex recipes in favor of simple two or three-part meals where each component is cooked individually. The vast majority of our home-cooked meals involve cooking (a) a protein like fish or meat (b) a vegetable cooked simply, like roasted in olive oil and (c) a starch like rice, potatoes, simple pasta. This let’s you build a healthy meal with simple individual components, master the same techniques with repeat practice, and minimize the risk of blowing up the whole dish.</p>
<p><strong>Make it taste good</strong></p>
<p>This may seem obvious, but it’s important that the food you cook actually taste good. Home-cooked food is almost always healthier than restaurant food, so don’t try to learn to cook and cook the healthiest possible version of each dish. Most veggies taste better roasted in a generous amount of olive oil than they do steamed, so roast them! Baste your chicken in butter. Salt your food generously. Learning to cook by producing dishes that just aren’t that tasty is a very bad feedback loop, so do what you need to to make it taste good.</p>
<p><strong>Don’t cook everything evenly</strong></p>
<p>This is a little more specific but I feel like it needs to be specifically counter-programmed. For some dumb reason a lot people (including myself 10 years ago) got the notion that food needs to be cooked&nbsp;<em>evenly</em>. That it’s really important to constantly turn and shake and rotate your food so that it’s cooked the same all the around and through. This is a great way to make gross food. Stop touching and turning your food. Most dishes are better with a substantial amount of difference in how cooked different sides of the food are: steak with a crunchy sear and medium rare inside, roasted potatoes with a waxy crust and fluffy inside, carrots or asparagus charred on side are all much tastier and mostly produced by having uneven cooking.</p>
<p><strong>Baking is really hard</strong></p>
<p>Really. Baking is much harder and less forgiving than any other kind of cooking. If you’re just starting to get into cooking, don’t start with baking.</p>
<h2>Essential Gear</h2>
<p>Okay, obviously you can spend and absolute fortune and fill a kitchen with mountains of cooking gear. That’s part of the fun of getting into cooking, let’s be honest. But if all you’ve got is a crappy 10-piece Wal-mart cooking set you got as a wedding gift or a hodge podge you inherited from your roommates, and you need to build your kitchen from scratch, this is the minimum kit I think you need. <em>(Disclosure: most of these are Amazon affiliates links. Don’t click them if that’s a thing that will make you mad).</em></p>
<p><strong>Pots and Pans</strong></p>
<p>The essential workhorses of cooking. Different kinds of pots and pans provide really different value for money, so I’ll specifically recommend below which ones I think it makes sense to invest in something …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://tylertringas.com/cooking-for-founders/">https://tylertringas.com/cooking-for-founders/</a></em></p>]]>
            </description>
            <link>https://tylertringas.com/cooking-for-founders/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25719188</guid>
            <pubDate>Sun, 10 Jan 2021 20:17:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Vuejs rejects close to 75% of outside contributions]]>
            </title>
            <description>
<![CDATA[
Score 85 | Comments 85 (<a href="https://news.ycombinator.com/item?id=25719116">thread link</a>) | @gieksosz
<br/>
January 10, 2021 | https://merge-chance.info/target?repo=vuejs/vue | <a href="https://web.archive.org/web/*/https://merge-chance.info/target?repo=vuejs/vue">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    
    
    
    <p><a></a>
        <span> of the PRs made by outsiders (not owners/members) get merged.</span>
    </p>
    
    
    <p><a></a>
        <i> * Based on most recent <strong> 272 </strong> outsiders' PRs </i>
    </p>
    <p><a></a>
        <i> * PRs open but not merged within 90 days are also treated as rejected </i>
    </p>
    
    
    <p><a></a>
        <span>
            Copy Markdown below to your README.md to get a Merge-Chance badge
        </span>
    </p>
<div>
    <pre>        <code>
            ![Custom badge](https://img.shields.io/endpoint?url=https%3A%2F%2Fmerge-chance.info%2Fbadge%3Frepo%3Dvuejs/vue)
        </code>
    </pre>
</div>
<p><a></a>
    <span> Like this one</span>
    <img alt="Custom badge" src="https://img.shields.io/endpoint?url=https%3A%2F%2Fmerge-chance.info%2Fbadge%3Frepo%3Dvuejs/vue">
</p>
<br>

<br>
<hr>




</div>]]>
            </description>
            <link>https://merge-chance.info/target?repo=vuejs/vue</link>
            <guid isPermaLink="false">hacker-news-small-sites-25719116</guid>
            <pubDate>Sun, 10 Jan 2021 20:10:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The M1 MacBook Air is the best computer I've ever owned]]>
            </title>
            <description>
<![CDATA[
Score 214 | Comments 318 (<a href="https://news.ycombinator.com/item?id=25717727">thread link</a>) | @bouk
<br/>
January 10, 2021 | https://bou.ke/blog/macbouk-air/ | <a href="https://web.archive.org/web/*/https://bou.ke/blog/macbouk-air/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
  
  <p><span>Jan 2021</span></p>
  <p>I started a new job recently so I had the opportunity to get one of the new M1 MacBooks, I decided to go with the Air. The reviews have been very positive and I’m here to tell you: it is indeed an amazing device. The performance feels a lot better than my MacBook Pro 16”, which is only a year old and about 3x the price.</p>

<p>When I got the Mac I set out with the goal of avoiding Intel builds of software as much as possible and using native whenever possible unless it’s absolutely impossible.</p>

<h2 id="nix">Nix</h2>

<p>I have my <a href="https://github.com/bouk/b" target="_blank">whole system configuration</a> stored in <a href="https://bou.ke/blog/nix/">Nix</a>, which was the thing that I least expected to work and arm64 support is still a <a href="https://github.com/NixOS/nixpkgs/issues/95903" target="_blank">work in progress</a>. I could install the <code>x86_64</code> build of Nix and run it under Rosetta but wanted to avoid that, so I went back to my old pal;</p>

<h2 id="homebrew">Homebrew</h2>

<p>This was one of the first things I installed and got working, when I did it I had to install it into <code>/opt/homebrew</code> manually and install everything with the <code>--source</code> flag but… everything mostly worked? Lots of props to the Homebrew team for getting everything running so quickly, with some amazing <a href="https://github.com/Homebrew/brew/issues/7857" target="_blank">open-source project management</a> the community worked together very quickly to support most of the software that Homebrew offers. There’s still some software that doesn’t work—notably neovim. But I’m sure that will be fixed soon.</p>

<p>The installer now installs into <code>/opt/homebrew</code> by default and there’s prebuilt bottles of most packages, so the Homebrew experience is great.</p>

<h2 id="go">Go</h2>

<p>A lot of my work involves Go, and I depend on a lot of tools written in Go. I was happy to see that the <a href="https://blog.golang.org/ports" target="_blank">Go team was on top of it</a> and released the 1.16 beta quite quickly, which is what is installed right now when you do <code>brew install go</code>. I’ve had no issues with it and am enjoying some of the new features like <a href="https://github.com/golang/go/issues/41191" target="_blank">file embedding</a>. GoLand was also <a href="https://blog.jetbrains.com/go/2020/12/30/goland-2020-3-1-is-out/" target="_blank">updated</a> to support M1 pretty quickly.</p>

<h2 id="terraform">Terraform</h2>

<p>Terraform I had the most issues with since using it depends on a bunch of plugins, which are generally only available for x86_64. This won’t change until Go 1.16 has been released. So here I had to resort to building for x86_64, which is easy to do:</p>

<div><div><pre><code>curl -L 'https://github.com/hashicorp/terraform/archive/v0.14.4.tar.gz' | tar -xzf-
cd terraform-0.14.4/
GOARCH=amd64 go build -o ~/bin/terraform
</code></pre></div></div>

<p>And now Terraform will just use plugin built for Intel. I assume that most Terraform plugins will support arm64 very quickly after Go 1.16 is out.</p>

<h2 id="rust-and-universal-binaries">Rust and Universal Binaries</h2>

<p>I use <a href="https://github.com/alacritty/alacritty" target="_blank">Alacritty</a> as my terminal. It supported <code>arm64</code> pretty quickly but the current build for it doesn’t include it, so I <a href="https://github.com/alacritty/alacritty/pull/4683" target="_blank">made a PR</a> that will build a universal binary. Creating a universal binary for Rust is quite easy:</p>

<div><div><pre><code>rustup target add x86_64-apple-darwin aarch64-apple-darwin
cargo build <span>--release</span> <span>--target</span><span>=</span>x86_64-apple-darwin
cargo build <span>--release</span> <span>--target</span><span>=</span>aarch64-apple-darwin
lipo target/<span>{</span>x86_64,aarch64<span>}</span><span>-apple-darwin</span>/release/alacritty <span>-create</span> <span>-output</span> alacritty
</code></pre></div></div>

<p>Running <code>file alacritty</code> will now show you something like:</p>

<div><div><pre><code>alacritty: Mach-O universal binary with 2 architectures: [x86_64:Mach-O 64-bit executable x86_64] [arm64]
alacritty (for architecture x86_64):    Mach-O 64-bit executable x86_64
alacritty (for architecture arm64):     Mach-O 64-bit executable arm64
</code></pre></div></div>

<p>Now you can run it in either <code>x86_64</code> mode with <code>arch -x86_64 alacritty</code> or natively with <code>arch -arm64 alacritty</code> and the OS will automatically select the right binary.</p>

<p>Building a universal binary for a Go application is similarly easy:</p>

<div><div><pre><code>GOARCH=amd64 go build -o app_amd64 main.go
GOARCH=arm64 go build -o app_arm64 main.go
lipo app_{amd64,arm64} -create -output app
</code></pre></div></div>

<h2 id="windows-and-games">Windows and Games</h2>

<p>There’s a couple of <a href="https://en.wikipedia.org/wiki/Age_of_Mythology" target="_blank">games</a> I’ve been playing for a long time that I still need on any computer I get, but they’re 32-bit Windows-only. This was something that was surprisingly easy to get working:</p>

<ol>
  <li>Install the <a href="https://www.parallels.com/blogs/parallels-desktop-apple-silicon-mac/" target="_blank">Parallels Technical Preview</a></li>
  <li>Sign up for Windows Insider and download the <a href="https://www.microsoft.com/en-us/software-download/windowsinsiderpreviewARM64" target="_blank">Windows 10 on ARM Insider Preview</a></li>
  <li>Install it into Parallels</li>
  <li>Done</li>
</ol>

<p>Now you can download Steam and basically install anything and it will probably work. Microsoft really did some amazing magic in getting both 32-bit and 64-bit x86 programs running on <code>arm64</code>.</p>

<p><img src="https://bou.ke/images/windows-solitaire.png" alt="" loading="lazy"></p><h2 id="conclusion">Conclusion</h2>

<p>Somehow Apple has created the best PC in every category at once. It is even the best Windows PC, despite the multiple layers of emulation that are happening. The battery life is incredible, I haven’t experienced any slowdowns, I don’t hear any fans spin up (because there are none). It’s hard not to be excited about this.</p>

<p>If Apple chose to go down this path, they could dominate the server space if they took their magic chips, put it in a <a href="https://en.wikipedia.org/wiki/Rack_unit" target="_blank">rack unit</a> and made it easy to install Linux onto it. Intel is completely screwed unless they come up with something better, fast.</p>

  <hr>
  <p>You should follow me on <a href="https://twitter.com/BvdBijl">Twitter</a>!</p>
</div>
</div></div>]]>
            </description>
            <link>https://bou.ke/blog/macbouk-air/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25717727</guid>
            <pubDate>Sun, 10 Jan 2021 18:24:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Research-Based Methods for Learning Japanese]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25717393">thread link</a>) | @sova
<br/>
January 10, 2021 | https://japanesecomplete.com/articles/?p=1282 | <a href="https://web.archive.org/web/*/https://japanesecomplete.com/articles/?p=1282">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<h3><strong>Conclusions</strong> from the Academic Research</h3>



<p>We looked at research papers from the last 30 years to evaluate our set of teaching strategies undertaken in our Japanese language learning application called <a href="https://japanesecomplete.com/">Japanese Complete</a>.  We found that the scholarly and academic studies referred to confirm:</p>



<ol><li><strong>Kanji [logographs of mainland Asian origin] are categorized as the main impediment in acquisition of the Japanese language</strong> and anything to ease their acquisition from rote-memorization will help learners greatly.</li><li><strong>Computer aided learning of Japanese is superior to textbook-based</strong> learning <strong>when</strong> <strong>paired with timely and useful feedback</strong> for the learner, with politeness language, grammar, kanji, and more. [4, 5, 7, 12, 17]</li><li><strong>Kanji are processed by different parts of the brain compared to English letters and Hiragana mora,</strong> suggesting that placing special emphasis on learning Kanji as glyphs or <em>graphics with associated meanings <strong>first</strong></em> establishes a firm foundation upon which to then learn readings and pronunciations. [2, 6]</li><li>Kanji have <em>logographic</em> as well as <em>phonographic</em> value; <strong>cultural, etymological, and mnemonic strategies are not only more effective than rote-memorization strategies for learning, they also remove a lot of anxiety and perceived inscrutability of kanji, making kanji learning fun and intriguing</strong> instead of a chore.  The main techniques presented in these papers include component-analysis of kanji [what we call subkanji learning], and pairing kanji with graphical and mnemonic aides such as stories or imaginal scenes. [6, 7, 10, 11, 14, 15]</li><li>It is very likely that native speakers of languages featuring kanji create <em>some sort</em> of meaning-word associated with each kanji, whether able to give voice to it or not (“unconscious knowledge”), suggesting strongly that <strong>by teaching kanji meaning-words first one is creating a very helpful and direct shortcut to their successful assimilation.</strong> [6, 9, 10]</li><li>One paper suggests the use of an “orthographic gradient” where terms are first shown as color-coded Hiragana, and later instances are shown as color-coded kanji using the same coloration, allowing learners to make an implied association of equality.  <strong>This idea of “orthographic gradient” confirms the efficacy of our innovative approach “Kanji in English Context” where we t取ke kanji and pl置ce them in an 英ngl語sh c文nt脈xt [take, place, English, context] to expedite acquisition</strong> and increase familiarity with the plurality of [English] words kanji can represent. [11, 16]</li><li><strong>Acquisition of Japanese grammar and sequence continues to be the most challenging part of learning the language for 1st, 2nd, and 3rd year students,</strong> suggesting that the focus for the first several semesters of learning ought be grammar-focused.  Nouns are acquired at a linear rate no matter the level of grammar competency, and thus it would suggest that the learning of nouns can actually be greatly delayed without any negative impact on comprehension; rather, focus on grammar first would result in greatly improved comprehension when nouns are later added to one’s cogent grammar understanding. </li><li><strong>Very Accurate Japanese Pitch Accent</strong> can be trained and learned swiftly with the right learning materials (showing pitch contours on screen and having learners listen and repeat). [17]</li><li><strong>One’s native language shapes how one acquires second+ languages.</strong> [3, 13]</li></ol>



<figure><img src="http://jpc0.b-cdn.net/img/lake-onami.jpg" alt=""><figcaption>Lake Onami</figcaption></figure>



<h3><br><strong>Scholarly and Academic Works Cited</strong></h3>



<hr>



<p>[1] <em><strong>Phonological processing of Japanese Kanji and Chinese characters in bilingual Japanese : An fMRI study (2017)</strong></em></p>



<p><a href="https://ieeexplore.ieee.org/document/8015970" target="_blank" rel="noreferrer noopener">https://ieeexplore.ieee.org/document/8015970</a></p>



<p>“The result showed that our bilingual Japanese subjects have large overlaps in the neural substrates for phonological processing of both native and second language.  Our finding supports the idea that the neural systems of second language reading are shaped by native language.” </p>



<hr>



<p>[2] <em><strong>Japanese and English sentence reading comprehension and writing systems: An fMRI study of first and second language effects on brain activation. (2009)</strong></em></p>



<p><a href="https://pubmed.ncbi.nlm.nih.gov/19946611/" target="_blank" rel="noreferrer noopener">https://pubmed.ncbi.nlm.nih.gov/19946611/</a></p>



<p>Parts of the brain that are engaged when native Japanese readers process English, Hiragana, and Kanji:</p>



<figure><img src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/2782536/bin/nihms120575f4.jpg" alt=""><figcaption>Activation regions for native Japanese speakers decoding written language.</figcaption></figure>



<p>“Functional magnetic resonance imaging (fMRI) was used to compare brain activation from Japanese readers reading hiragana (syllabic) and kanji (logographic) sentences, and English as a second language (L2). Kanji showed more activation than hiragana in right-hemisphere occipito-temporal lobe areas associated with visuospatial processing; hiragana, in turn, showed more activation than kanji in areas of the brain associated with phonological processing. L1 results underscore the difference in visuospatial and phonological processing demands between the systems. Reading in English as compared to either of the Japanese systems showed more activation in inferior frontal gyrus, medial frontal gyrus, and angular gyrus. The additional activation in English in these areas may have been associated with an increased cognitive demand for phonological processing and verbal working memory.”</p>



<hr>



<p>[3] <em><strong>Comparative Spatial Semantics and Language Acquisition:Evidence from Danish, English, and Japanese (1994)</strong></em></p>



<p><a href="https://www.researchgate.net/profile/Chris-Sinha/publication/249234936_Comparative_Spatial_Semantics_and_Language_Acquisition_Evidence_from_Danish_English_and_Japanese/links/57da79a508ae72d72ea2b8b8/Comparative-Spatial-Semantics-and-Language-Acquisition-Evidence-from-Danish-English-and-Japanese.pdf" target="_blank" rel="noreferrer noopener">https://www.researchgate.net/profile/Chris-Sinha/publication/249234936_Comparative_Spatial_Semantics_and_Language_Acquisition_Evidence_from_Danish_English_and_Japanese/links/57da79a508ae72d72ea2b8b8/Comparative-Spatial-Semantics-and-Language-Acquisition-Evidence-from-Danish-English-and-Japanese.pdf</a></p>



<p>“Perhaps the most important finding is that in all three languages that we have analysed acquisition appears to take place in two phases, during the first of which the child gradually acquires six to eight simple forms corresponding to ‘basic’ spatial meanings encoded in the target language.”</p>



<p>“In the second phase of acquisition, the child’s productive repertoire and the frequency of its use increase in a way that is reminiscent of (though perhaps less dramatic than) the ‘vocabulary explosion’ in nominal usage. The extension of the repertoire takes different courses, depending on the structure of the target language. In all languages it can be expected that the repertoire within the form class which is dominant in expressing spatial relational meaning will continue to expand, and that this will remain the most frequently employed vehicle for the child’s expression of spatial relational meaning throughout and perhaps beyond the third year of life.”</p>



<p>“In languages, such as Japanese, in which spatial relational meaning is overtly distributed over different form classes, the second phase will involve both an increase in the range of types controlled in the dominant form class (in this case,verbs), and the extension of the acquisition process to the other form classes (in this case, particles and nouns). Both the slow pace of acquisition in the non-dominant forms classes, and the fact that the meanings initially expressed using nouns are cognate with those already expressed using verbs, suggest that, in Japanese too, the learning process continues to be governed in the second phase by a conservative strategy. It can perhaps be seen as follows: the child employs the already acquired meanings as clues for the establishment of new centres in new form classes for the repetition with respect to these new form classes of the radial strategy already successfully employed with respect to the dominant form class.”</p>



<hr>



<p>[4] <strong><em>Supporting the acquisition of Japanese polite expressions in context-aware ubiquitous learning</em> <em>(2010)</em></strong></p>



<p><a href="http://www.academia.edu/download/50315553/ijmlo.2010.03263720161114-19604-1c5exmp.pdf">http://www.academia.edu/download/50315553/ijmlo.2010.03263720161114-19604-1c5exmp.pdf</a></p>



<p>“To support the foreigners learning JPE [Japanese Politeness Expressions], a PDA-based context-aware language learning support environment was proposed. This environment supports the learners to learn JPE according to the different situations in the real world. There are two version of the prototype system for this environment. In this paper, design, implementation and evaluation of JAPELAS2 are presented. From the experiment, we found the system provides the correct polite-expression based on hyponymy, social distance and situation through the identification of the target user and the location. The experiment showed that the system was quite useful and using this system made understanding the appropriate level of politeness easy by changing roles and situations.”</p>



<figure><img loading="lazy" width="645" height="641" src="https://japanesecomplete.com/articles/wp-content/uploads/2021/01/Screen-Shot-2021-01-06-at-4.13.32-PM.png" alt=""></figure>



<hr>



<p>[5] <em><strong>COMPUTER VS. WORKBOOK INSTRUCTION IN SECOND LANGUAGE ACQUISITION (1996)</strong></em></p>



<p><a href="https://journals.equinoxpub.com/CALICO/article/viewFile/23393/19398">https://journals.equinoxpub.com/CALICO/article/viewFile/23393/19398</a></p>



<p>“The results of the study show that given the same grammar notes and exercises, <strong>ongoing intelligent computer feedback is more effective than simple workbook answer sheets for developing learners’ grammatical skill in producing Japanese particles and sentences.</strong> A significant difference between Nihongo-CALI and the workbook instruction was observed in the production tests but not in the comprehension tests. This is consistent with Flynn’s hypothesis that grammatical competence is less critical in comprehension than in production. As suggested by Pederson and Dunkel, the present study also confirms that the use of a medium (i.e., computer) alone does not bring better effects; rather the quality of the messages produced by the medium affects the result. This is based on the fact that the intelligent version of Nihongo-CALI is significantly more effective than the workbook instruction”</p>



<div><figure><img loading="lazy" width="411" height="333" src="https://japanesecomplete.com/articles/wp-content/uploads/2021/01/Screen-Shot-2021-01-06-at-4.19.44-PM.png" alt=""></figure></div>



<hr>



<p>[6] <em><strong>L2 learners’ attitudes toward, and use of, mnemonic strategies when learning Japanese Kanji (2013)</strong></em></p>



<p><a href="https://www.researchgate.net/publication/259551232_L2_learners%27_attitudes_toward_and_use_of_mnemonic_strategies_when_learning_Japanese_Kanji">https://www.researchgate.net/publication/259551232_L2_learners%27_attitudes_toward_and_use_of_mnemonic_strategies_when_learning_Japanese_Kanji</a></p>



<p>“This study investigated kanji learning (the memorization of Japanese written characters) of university students of Japanese, in order to evaluate students’ use of mnemonic strategies. The study applied …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://japanesecomplete.com/articles/?p=1282">https://japanesecomplete.com/articles/?p=1282</a></em></p>]]>
            </description>
            <link>https://japanesecomplete.com/articles/?p=1282</link>
            <guid isPermaLink="false">hacker-news-small-sites-25717393</guid>
            <pubDate>Sun, 10 Jan 2021 18:01:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Reverse Cargo Cult (2017)]]>
            </title>
            <description>
<![CDATA[
Score 132 | Comments 108 (<a href="https://news.ycombinator.com/item?id=25717075">thread link</a>) | @dredmorbius
<br/>
January 10, 2021 | https://hanshowe.org/2017/02/04/trump-and-the-reverse-cargo-cult/ | <a href="https://web.archive.org/web/*/https://hanshowe.org/2017/02/04/trump-and-the-reverse-cargo-cult/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

					<p>Trump administration lies constantly but doesn’t even attempt to make it seem like they aren’t lying.</p>
<p>After the collapse of the Soviet Union, this kind of cynicism was referred to as the “reverse cargo cult” effect.</p>
<p>In a regular cargo cult, you have people who see an airstrip, and the cargo drops, so they build one out of straw, hoping for the same outcome. They don’t know the difference between a straw airstrip and a real one, they just want the cargo.</p>
<p>In a reverse cargo cult, you have people who see an airstrip, and the cargo drops, so they build one out of straw. But there’s a twist:</p>
<p>When they build the straw airstrip, it <em>isn’t</em> because they are hoping for the same outcome. They know the difference, and know that because their airstrip is made of straw, it certainly won’t yield any cargo, but it serves another purpose. They don’t lie to the rubes and tell them that an airstrip made of straw will bring them cargo. That’s an easy lie to dismantle. Instead, what they do is make it clear that the airstrip is made of straw, and doesn’t work, but then tell you that the other guy’s airstrip doesn’t work either. They tell you that <strong>no airstrips yield cargo</strong>. The whole <em>idea of cargo</em> is a lie, and those fools, with their fancy airstrip made out of wood, concrete, and metal is just as wasteful and silly as one made of straw.</p>
<p>1980s Soviets knew that their government was lying to them about the strength and power of their society, the Communist Party couldn’t hide all of the dysfunctions people saw on a daily basis. This didn’t stop the Soviet leadership from lying. Instead, they just accused the West of being equally deceptive. <em>“Sure, things might be bad here, but they are just as bad in America, and in America people are actually foolish enough to believe in the lie! Not like you, clever people. You get it. You know it is a lie.”</em></p>
<p>Trump’s supporters don’t care about being lied to. You can point out the lies until you’re blue in the face, but it makes no difference to them. Why? Because it is just a game to them. The media lies, bloggers lie, politicians lie, it’s just all a bunch of lies. Facts don’t matter because those are lies also. Those trolls on Twitter, 4Chan, T_D, etc. are just having a good laugh. They are congratulating each other for being so smart. We are fools for still believing in anything. There is no cargo, and probably never was.</p>
<p>Source: <a href="https://np.reddit.com/r/politics/comments/5rru7g/kellyanne_conway_made_up_a_fake_terrorist_attack/dd9vxo2/">https://np.reddit.com/r/politics/comments/5rru7g/kellyanne_conway_made_up_a_fake_terrorist_attack/dd9vxo2/</a></p>
					
					<p>
						Filed under: <a href="https://hanshowe.org/category/1100733/" rel="category tag">#!!#</a> |					</p>

				</div></div>]]>
            </description>
            <link>https://hanshowe.org/2017/02/04/trump-and-the-reverse-cargo-cult/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25717075</guid>
            <pubDate>Sun, 10 Jan 2021 17:36:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Apple M1, ARM/x86 Linux Virtualization, and Boinc]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 6 (<a href="https://news.ycombinator.com/item?id=25717019">thread link</a>) | @jseliger
<br/>
January 10, 2021 | https://www.sevarg.net/2021/01/09/arm-mac-mini-and-boinc/ | <a href="https://web.archive.org/web/*/https://www.sevarg.net/2021/01/09/arm-mac-mini-and-boinc/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <p>About <a href="https://www.sevarg.net/2020/06/21/apple-and-arm-transition/">six months ago</a>, I speculated a bit on what Apple might do with their upcoming (rumored at the time) ARM transition.  Apple did it, has shipped hardware, and I’ve had a chance to play with for a while now.  I’ve also, as is usual for me, gone down some weird paths - like ARM Linux virtualization, x86 Linux emulation, and BOINC in an ARM VM!</p>

<p>The fastest Linux machine I’ve <em>ever</em> used is a hardware virtualized install on the Apple M1 - and this post covers how to do it!</p>

<picture><source srcset="https://www.sevarg.net/generated/images/2021-qemu-m1/mac_m1_virtualization-400-1c323509d.webp 400w, https://www.sevarg.net/generated/images/2021-qemu-m1/mac_m1_virtualization-800-1c323509d.webp 800w, https://www.sevarg.net/generated/images/2021-qemu-m1/mac_m1_virtualization-1600-1c323509d.webp 1600w" type="image/webp"><source srcset="https://www.sevarg.net/generated/images/2021-qemu-m1/mac_m1_virtualization-400-1c323509d.png 400w, https://www.sevarg.net/generated/images/2021-qemu-m1/mac_m1_virtualization-800-1c323509d.png 800w, https://www.sevarg.net/generated/images/2021-qemu-m1/mac_m1_virtualization-1600-1c323509d.png 1600w" type="image/png"><img src="https://www.sevarg.net/generated/images/2021-qemu-m1/mac_m1_virtualization-1600-1c323509d.png" width="4096" height="2304"></picture>



<h2 id="the-short-2020-m1-mac-mini-review">The Short 2020 M1 Mac Mini Review</h2>

<p>While I don’t generally <a href="https://www.sevarg.net/2020/02/01/finances-technology-repair-and-enough/">make a habit</a> of buying brand new, just-released hardware, I made an exception for the M1 and bought a M1 Mac Mini to replace an Intel Mac Mini (which had replaced a perfectly function 2014 iMac I’d still be running if the monitor hadn’t failed - the display assembly, used, cost $600 in not-cracked condition).  The Intel one wasn’t doing most of what I wanted (to say the GPU sucked would be an understatement), I’ve been lusting after a mid-range ARM desktop for a long while, and the fact that things would be broken on it doesn’t bother me - it’s not a production machine for me, so I’m happy to run on the bleeding, slightly broken edge.  It’s a common theme with ARM desktop use, especially 64-bit, so this is no different.</p>

<p>How is it?  It’s <em>fast.</em>  It’s <em>really, really fast.</em>  Not just for the power - that’s amazing too, but simple, flat out, using it for stuff.  It’s amazing.  I figured it would be really good, but it’s beyond good, crossing into the “Yeah, I’ll call this magical…” realm.  The fan almost never comes off idle, power consumption (I work in a solar office, remember?) is a rounding error, and it just does what I ask of it in a real hurry.</p>

<p>I mean, it’ll even play Kerbal Space Program with pretty darn good graphics!  That’s an x86 game with no native port yet, and it’s not exactly a CPU sipper!</p>

<picture><source srcset="https://www.sevarg.net/generated/images/2021-qemu-m1/ksp_m1_settings-400-b017715f9.webp 400w, https://www.sevarg.net/generated/images/2021-qemu-m1/ksp_m1_settings-800-b017715f9.webp 800w, https://www.sevarg.net/generated/images/2021-qemu-m1/ksp_m1_settings-863-b017715f9.webp 863w" type="image/webp"><source srcset="https://www.sevarg.net/generated/images/2021-qemu-m1/ksp_m1_settings-400-b017715f9.png 400w, https://www.sevarg.net/generated/images/2021-qemu-m1/ksp_m1_settings-800-b017715f9.png 800w, https://www.sevarg.net/generated/images/2021-qemu-m1/ksp_m1_settings-863-b017715f9.png 863w" type="image/png"><img src="https://www.sevarg.net/generated/images/2021-qemu-m1/ksp_m1_settings-863-b017715f9.png" width="863" height="757"></picture>



<p>This does mean that, once again, Jeb ends up stuck places he’d probably rather not be stuck.  Lock your staging, or a thumb twitch might just leave you 100m above Mun with no descent (or ascent!) stage left attached…</p>

<picture><source srcset="https://www.sevarg.net/generated/images/2021-qemu-m1/sad_jeb-400-976018ace.webp 400w, https://www.sevarg.net/generated/images/2021-qemu-m1/sad_jeb-800-976018ace.webp 800w, https://www.sevarg.net/generated/images/2021-qemu-m1/sad_jeb-1299-976018ace.webp 1299w" type="image/webp"><source srcset="https://www.sevarg.net/generated/images/2021-qemu-m1/sad_jeb-400-976018ace.png 400w, https://www.sevarg.net/generated/images/2021-qemu-m1/sad_jeb-800-976018ace.png 800w, https://www.sevarg.net/generated/images/2021-qemu-m1/sad_jeb-1299-976018ace.png 1299w" type="image/png"><img src="https://www.sevarg.net/generated/images/2021-qemu-m1/sad_jeb-1299-976018ace.png" width="1299" height="756"></picture>



<p>Should you buy one of Apple’s M1 devices?  Probably not - wait 6 months for the next round of hardware, and buy then.  Most of the software ecosystem quirks will be worked out by then, and just about everything will work.  For now, there are enough weird little broken corner cases that I’d suggest holding off unless you’re OK with that and want legitimately insane battery life and performance.</p>

<p>But an awful lot does work, and it works really well.</p>

<p>Yes, yes, I <em>know,</em> your overclocked AMD ThreadBlaster 7970XP, with enough threads, will build the kernel faster on 400W.  All this performance is on about 30W, and this is their first pass at it.  Just wait…</p>

<h2 id="rosetta-2-they-did-what">Rosetta 2: They Did WHAT?</h2>

<p>We also have an answer now to the insane x86 translation performance (around 80% of native performance, give or take - and, yes, this does mean that the M1 runs x86 binaries faster than an awful lot of x86 hardware out there).  I’ve messed around with running x86 binaries in emulation on ARM before, and got about 10% of native performance on a Rpi4.  Painful.  Apple’s Rosetta 2 gets a lot of benefit from being a pre-compiling translator (for most cases - it still has to interpret JIT type workloads), but most people figured that would get you to 50% - at best.  The ARM memory model is so radically different from x86 that you end up sprinkling memory barriers everywhere to guarantee cross threaded consistency, and that really hurts performance.</p>

<p>The issue is that on x86, if you write memory addresses in a certain sequence, <em>all other cores</em> will see the writes in the same sequence. If you write some blob of data and then write the “Ready!” flag, by the time other cores see the flag change value, you can be certain that the blob of data has been written.  ARM has no such guarantees without explicit (and slow) memory barrier instructions, and this is why the x86 emulation on ARM Windows was painful - guaranteeing correctness of a translated binary on a weaker memory model is slow.</p>

<p>As it turns out, Apple <em>isn’t</em> doing it purely in software - they have <a href="https://github.com/saagarjha/TSOEnabler">Total Store Ordering</a> support in their hardware!  When the M1 runs a translated x86 binary, the OS just tells the chip, “Hey, use x86 memory ordering for this thread.”  Things built natively for ARM can take advantage of the performance gains of the memory reordering, and things that requires strict ordering get strict ordering.  It’s a very, very clever way to totally bypass the memory ordering issues with translated binaries, and it’s not a thing in any other ARM chip on the planet (I’d say yet, but I really don’t think this will become a popular thing to implement - perks of Apple building their own silicon).</p>

<h2 id="onto-virtual-machines-lets-build-qemu">Onto Virtual Machines: Let’s Build qemu!</h2>

<p>Now, to the core of the post: Building qemu to run hardware virtualized ARM Linux!  I’m starting with <a href="https://gist.github.com/niw/e4313b9c14e968764a52375da41b4278">these excellent instructions</a> as a guide, but I’ve got some extra patches thrown in (because it doesn’t run x86 emulation my M1 with 11.1), and I’m doing a few other things in the process.</p>

<p>You’ll need XCode installed, and we’ll be using <a href="https://brew.sh/">homebrew</a> to install  some of the prerequisites for building qemu.  Plus some patches to the source, and… it’s all good fun, I promise!  What I don’t promise is that this will work perfectly for you, though I’ll try!</p>

<p>Yes, I know Parallels has a tech preview out, and you still can’t change the resolution of a Linux guest.  If you’re fine with 1024x768, it certainly works, but… we can do better with open source!</p>

<h4 id="installing-the-prerequisites-homebrew-and-xcode">Installing the Prerequisites: Homebrew and XCode</h4>

<p>You’ll need the XCode command line tools (gcc and such) to build this, so if you don’t already have those installed, go ahead and install XCode from the App Store.  It’s huge.</p>

<p>I understand you can also install them from the command line, if you don’t want the full install, by running <code>xcode-select --install</code>.  That’s the first thing I do with any Mac, so I had them laying around.  You may have to agree to some license terms as well - it’s been a while since I had a clean install.</p>

<p>If you use the normal Homebrew install path, you’ll get x86 Homebrew, running under Rosetta.  This is fine for most use cases, and it certainly <em>works</em> better than the ARM Homebrew (half the code won’t build under ARM), but it’s no good for ARM native dependencies, and we’re going to be building ARM native qemu.</p>

<p>If you rely on x86 homebrew, well… uh… fix the ARM stuff that doesn’t build?  Or install to a different directory, I suppose.  I have no great advice on parallel Homebrew installs, sorry.</p>

<div><div><pre><code>cd ~
mkdir homebrew &amp;&amp; curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip 1 -C homebrew
sudo mv homebrew /opt/homebrew
echo 'export PATH="/opt/homebrew/bin:$PATH"' &gt;&gt; ~/.zshrc
source ~/.zshrc
brew update
</code></pre></div></div>

<p>This will run some git fetches, some builds, and you should generally have a working brew install.  Like XCode, this may take a while, depending on your internet connection.  My ISPs have been sucking more than usual lately, and I don’t keep a local mirror of Homebrew, so… coffee time.</p>

<p>You should then be able to install the dependencies for qemu:</p>

<div><div><pre><code>brew install ninja pkgconfig glib pixman
</code></pre></div></div>

<h4 id="fetching-patching-and-building-qemu">Fetching, Patching, and Building qemu</h4>

<p>Next step: we’re going to download the qemu source, check out the proper version, apply a couple patches, and build it!</p>

<p>The <a href="https://patchwork.kernel.org/project/qemu-devel/list/?series=400619">first patch series</a> is the core of the updates - it adds <a href="https://developer.apple.com/documentation/hypervisor">Hypervisor.framework</a> support (Apple’s recent “So, you wanna do hardware virtualization without a kernel module…” framework), adds the ability to sign the output binary to allow it to use that, and various other things related to Apple Silicon support.</p>

<p>The <a href="https://patchwork.kernel.org/project/qemu-devel/patch/20210103145055.11074-1-r.bolshakov@yadro.com/">second patch series</a> isn’t actually required to run hardware virtualization, but if you wanted to mess around with the (somewhat awful, but still usable) performance of x86 VMs on the M1, you’ll need this.  Apple Silicon prevents memory pages from being both writable and executable at the same time, and this adds the toggles to handle things properly so the JIT engine can work.</p>

<p>If you <em>don’t</em> apply the second patches, and you try to run x86 system emulation, you’ll get the exceedingly unhelpful error “Could not allocate dynamic translator buffer” when you try to run it.</p>

<p>And, of course, if you’re not interested in x86 emulation, you can skip the <code>x86_64-softmmu</code> and <code>i386-softmmu</code> options in the target-list for configure.</p>

<div><div><pre><code>git clone https://git.qemu.org/git/qemu.git
git checkout master -b wip/hvf
curl 'https://patchwork.kernel.org/series/400619/mbox/'|git am --3way
curl 'https://patchwork.kernel.org/project/qemu-devel/patch/<a href="https://www.sevarg.net/cdn-cgi/l/email-protection" data-cfemail="1220222023222322212326272227273c23232225263f233f603c707d7e617a73797d64526b7376607d3c717d7f">[email&nbsp;protected]</a>/mbox/'|git am --3way
mkdir build
cd build
../configure --target-list=aarch64-softmmu,x86_64-softmmu,i386-softmmu --enable-cocoa
make -j 8
</code></pre></div></div>

<p>Sit back, relax, wait… actually, not very long, this system is blazing fast on all 8 cores, and you should have some qemu binaries!</p>

<h4 id="grab-an-arm-ubuntu-iso-and-an-efi-blob-create-a-disk-image">Grab an ARM Ubuntu ISO and an EFI blob, Create a Disk Image</h4>

<p>There are plenty of ways to install the ARM version of Ubuntu, but there’s a convenient desktop build now at <a href="https://cdimage.ubuntu.com/focal/daily-live/current/">https://cdimage.ubuntu.com/focal/daily-live/current/</a> - grab <code>focal-desktop-arm64.iso</code>.  You do NOT want the ‘amd64’ version - make sure you get ‘arm64’ or it won’t work!</p>

<p>You also need a EFI blob built for ARM.  The instructions I’m working from cover how to build it with your own ARM VM (and include a link to some built ones), but I’ve also uploaded one for you, if you happen to want it: <a href="https://www.sevarg.net/images/2021-qemu-m1/QEMU_EFI.fd">QEMU_EFI.fd</a></p>

<p>While you could just run the live ISO, there’s no fun in that - create a disk image for the install (you’ll be doing this from the qemu/build directory you were just in, and stick the image somewhere fast).  I’ll assume you’ve put the QEMU_EFI.fd file in the same directory.</p>

<div><div><pre><code>./qemu-img create -f qcow2 /path/to/Ubuntu.qcow2 40G
</code></pre></div></div>

<h2 id="the-fun-part-launch-the-vm">The Fun Part: Launch the VM!</h2>

<p>Now, let’s light up a VM from the installer CD!  It’s Linux, so we’re just using <em>all the virtio devices.</em>  Virtio is a way for …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.sevarg.net/2021/01/09/arm-mac-mini-and-boinc/">https://www.sevarg.net/2021/01/09/arm-mac-mini-and-boinc/</a></em></p>]]>
            </description>
            <link>https://www.sevarg.net/2021/01/09/arm-mac-mini-and-boinc/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25717019</guid>
            <pubDate>Sun, 10 Jan 2021 17:30:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[We've been running a bootstrapped startup for a year]]>
            </title>
            <description>
<![CDATA[
Score 74 | Comments 37 (<a href="https://news.ycombinator.com/item?id=25717006">thread link</a>) | @artembugara
<br/>
January 10, 2021 | https://newscatcherapi.com/blog/we-ve-been-running-a-bootstrapped-startup-for-1-year-our-top-15-takeaways | <a href="https://web.archive.org/web/*/https://newscatcherapi.com/blog/we-ve-been-running-a-bootstrapped-startup-for-1-year-our-top-15-takeaways">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content"><div><div><h3>What is the value of this article? </h3><p>We are young, inexperienced, prone to fail: <strong>we’re 2 first time founders</strong>. I always prefer to learn from people who are just a few steps ahead of me. </p><h3>Who is this article for? </h3><p>If you’re a multi-time entrepreneur then you might not find this article interesting. <strong>If you’ve been thinking about starting your own startup then I’d recommend reading as many similar articles as possible.</strong></p><h2>Our Team</h2><p>Artem (author of this article):</p><ul><li><p>First-time founder </p></li><li><p>Technical with no CS degree</p></li><li><p>24 years old</p></li><li><p>~2 years of work experience (data-oriented job)</p></li><li><p>Quit the University after Masters 1 to get a job</p></li><li><p>Full-time since April 2020</p></li><li><p>CEO</p></li></ul><p>Maksym:</p><ul><li><p>First-time founder </p></li><li><p>Technical with no CS degree</p></li><li><p>24 years old</p></li><li><p>~2 years of work experience (data-oriented job)</p></li><li><p>Full-time since December 2020</p></li><li><p>CTO</p></li></ul><h2>Our Product - News API</h2><p>We provide instant access to news article data for hedge funds, market researchers, and PR software. </p><p><strong>In simple words: </strong></p><ol><li><p><strong>we crawl news websites, </strong></p></li><li><p><strong>detect news article URLs, </strong></p></li><li><p><strong>extract all possible information (title, published date, author, content, etc.), </strong></p></li><li><p><strong>index this data</strong></p></li></ol><p>Our main product is <a href="https://newscatcherapi.com/news-api">News API</a> which allows our clients to find structured news articles by any topic, country, language, website, or keyword. </p><p><a href="https://www.notion.so/newscatcherapi/News-API-46632a5cd61548919ff0132b15b0f0fa?p=2eff4d9b6e6b4a87a8e2230424aee4be">Example of News API JSON response</a>.</p><p>We’re B2B Data-as-a-Service.</p><h3>What I and my co-founder could achieve in 12 months</h3><ul><li><p>Monthly Recurring Revenue ~$3,000</p></li><li><p>Both co-founders work full-time</p></li></ul><blockquote>All things mentioned are purely personal. </blockquote><p><strong>1. Talk to your potential clients</strong></p><p>Talk to them even if you do not have a product. </p><p>Imagine you have the very best version of what you’re building. Go ahead and see what people say. </p><p>One more thing: <strong>CLIENTS</strong>. Not your friends, or some random people. Yeah, your friends will say “Great idea”. All of them. </p><p>What you really want to hear is “I like it. What’s the price? How can I buy it?”</p><p>People only vote with their pockets.</p><p><a href="https://www.ycombinator.com/library/6g-how-to-talk-to-users">More read</a></p><p><strong></strong><a href="https://www.ycombinator.com/library/6g-how-to-talk-to-users"><strong></strong></a><strong>2. Do not think you/your product/your team/your approach are any different or unique</strong></p><p>We all want to be special. We all want to work for a company that will soon cost millions of dollars. We all want to be the first of our kind. </p><p>But, most likely, you, your team, your product is (below) average.</p><p>So, if you hear experienced people repeating the same thing to you then you should act. </p><blockquote><strong>Do not try to be “Yeah, but we’re…”</strong> </blockquote><p><strong>3. It’s almost impossible to raise money when you don’t have anything to show</strong></p><p>Do not expect investors to come and give you money. It’s a big gamble for you to start raising. Also, raising money is a full-time job!</p><p><strong>4. Start pitching investors as soon as possible</strong></p><p>Yes, do not expect them to give you money. However, there is a lot of things to do to raise money. You have to be prepared for when you’re ready. </p><blockquote>Pitch deck.  Pitching. Answering the questions. </blockquote><p>Do not spend much time on it. However, I would recommend to do it consistently. It will take months to come up with something consistent. So, you’d better start early.</p><p>Also, some investors will tell you why they will not give you money. Iterate, and work on these problems.</p><p><strong>5. Start small. Do things that do not scale. Be a consulting company</strong></p><p>Believe me or not but the only way to sell your scalable solution to millions of clients is to start by selling it one-by-one. In a non-scalable way. </p><p><a href="http://paulgraham.com/ds.html">More read</a></p><p><strong></strong><a href="http://paulgraham.com/ds.html"><strong></strong></a><strong>6. Do not be afraid to charge </strong></p><p>Charge for your service/product at a fair price. Yeah, you might lose some clients. But, how will you survive if you cannot get fair pay for your service? </p><p><strong>7. It’s a grind. Consistent one</strong></p><p>You have to repeat a lot of boring things over and over again. </p><p><strong>8. Only those who pay you money have to decide which features to add</strong></p><p>Do you think adding this feature is cool? Go ahead, and ask those who pay you. </p><p>Listen to what they say, and make a better product. </p><p><strong>9. Carefully choose co-founders</strong></p><p>We know each other for over 14 years. We knew well what to expect from each other.</p><p><strong>10. Help others &amp; Ask others for help</strong></p><p><strong>11. Things that no one actually cares about at the beginning</strong></p><ul><li>Logo</li><li>Name, website domain</li><li>Your background</li></ul><p><strong>12. Things that everyone cares about</strong></p><ul><li>Your value proposition</li></ul><p><strong>13. What is the definition of a “startup” for you? </strong></p><p>Can you give it? Maybe what you want to start is a small business. There’s nothing wrong with it. </p><p><strong>14. Do not afraid to take a step back</strong></p><p><strong>15. This list misses the other 100 points which we did not figure out yet! So, do not overlay on it</strong></p><p>All I’ve written here might be wrong/not applicable to you. But like I said, most likely you’re not different at all.</p></div></div></div></div>]]>
            </description>
            <link>https://newscatcherapi.com/blog/we-ve-been-running-a-bootstrapped-startup-for-1-year-our-top-15-takeaways</link>
            <guid isPermaLink="false">hacker-news-small-sites-25717006</guid>
            <pubDate>Sun, 10 Jan 2021 17:30:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Powerful Life Skills for the New Decade]]>
            </title>
            <description>
<![CDATA[
Score 88 | Comments 69 (<a href="https://news.ycombinator.com/item?id=25716764">thread link</a>) | @neilkakkar
<br/>
January 10, 2021 | https://neilkakkar.com/powerful-life-skills.html | <a href="https://web.archive.org/web/*/https://neilkakkar.com/powerful-life-skills.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>Over the past few years, I’ve noticed certain skills in people I admire, from Paul Graham, Vitalik Buterin, to Ender Wiggin.</p>

<p>These are rare skills, responsible for making them who they are. Most normal people, including me, don’t realise it. This makes the skills powerful - not everyone can see them, and very few people have mastered them.</p>

<p>However, I aim to change that. What follows below are 10 skills sourced from admirable people that I want to develop.</p>




<figure>
    
    <img src="https://neilkakkar.com/assets/images/divider.jpg" alt="">
    
    
    
</figure>

<h2 id="learn-to-take-compounding-seriously">Learn to take compounding seriously</h2>

<p>It’s not just your wealth that compounds, but life experience and knowledge, too.</p>

<p>So, learn the most basic, most useful skills first. The longer you wait to learn skills like these, the less time there is for compounding magic. That’s what this entire list is about: powerful skills to learn and use for the rest of your life.</p>

<p>And even though you’ve heard about compounding, this item is first on the list, because <a href="https://neilkakkar.com/taking-ideas-seriously.html">taking ideas seriously is hard</a>.</p>

<p>A good way to figure out what compounds is <a href="https://neilkakkar.com/year-in-review-2019.html#compounding-is-powerful-building-intuition-for-compounding-even-more-so">to figure out what’s a platform</a>.</p>

<h2 id="learn-to-develop-taste">Learn to develop taste</h2>

<p>Despite prevalent beliefs, taste isn’t subjective.</p>

<p>While it may seem like it on the outside, when you say “I just love this painting” or “I just love this coffee machine” - all it means is that the defining characteristics are illegible to you. And noticing this is the first step.</p>

<p>Let’s take a specific example. Say you’re designing a high quality clay pot - and you’ve never done this before.</p>

<p>What’s a good way to develop taste for quality here?</p>

<p>If you’ve heard this claypot parable, you know the answer: start by making lots of crap pots.</p>

<blockquote>
  <div><p>The ceramics teacher announced on opening day that he was dividing the class into two groups. All those on the left side of the studio, he said, would be graded solely on the quantity of work they produced, all those on the right solely on its quality. His procedure was simple: on the final day of class he would bring in his bathroom scales and weigh the work of the “quantity” group: fifty pound of pots rated an “A,” forty pounds a “B,” and so on. Those being graded on “quality,” however, needed to produce only one pot—albeit a perfect one—to get an “A.”</p><p>

Well, came grading time and a curious fact emerged: the works of highest quality were all produced by the group being graded for quantity. It seems that while the “quantity” group was busily churning out piles of work—and learning from their mistakes—the “quality” group had sat theorizing about perfection, and in the end had little more to show for their efforts than grandiose theories and a pile of dead clay. - <a href="https://amzn.to/3o9o17A" target="_blank" rel="noopener">Art and Fear</a><sup id="fnref:2"><a href="#fn:2">1</a></sup></p></div>
</blockquote>

<p>Let others tell you what you’ve made is crap. Learn why. Notice when they tell you something is great. Figure out why.</p>

<p>This transfers to writing as well: Popular advice to get better is to write a lot of junk, do it a 100 times, and pay particular attention to what is received well. Here’s <a href="http://www.paulgraham.com/taste.html" target="_blank" rel="noopener">another example - developing taste for design</a>.</p>

<p>In effect, you bootstrap good taste by first learning what others consider good. Then, <a href="#learn-to-see-systems">you see the system behind it</a>. Then you break the rules and still manage to awe.</p>

<p>Then you’ve developed taste.</p>

<h2 id="learn-to-sequence-things-well">Learn to sequence things well</h2>

<p>Waking up when others are asleep and getting lots done is a super power. It’s born out of a system of <a href="https://neilkakkar.com/sequencing-things-in-the-right-order.html">learning to sequence things well</a>.</p>

<p>It means choosing the right time for that Netflix binge.</p>

<p>It means being prepared before the meeting, not scrambling to get things done after.</p>

<p>It means reading the coursebook before the lecture, not after.</p>

<h2 id="learn-to-see-what-others-see">Learn to see what others see</h2>

<p>How well can you understand other people? Can you sense their desires, their concerns, and what events lead to those desires and concerns?</p>

<p>If you can do this, you can understand them. But not before.</p>

<blockquote>
  <p>In the moment when I truly understand my enemy, understand him well enough to defeat him, then in that very moment I also love him. I think it’s impossible to really understand somebody, what they want, what they believe, and not love them the way they love themselves. - <a href="https://amzn.to/2KKe8Px" target="_blank" rel="noopener">Ender’s Game</a><sup id="fnref:1"><a href="#fn:1">2</a></sup></p>
</blockquote>

<p>It’s worth going this far because understanding is powerful. It helps you empathise. It helps you negotiate. It helps you figure out why you don’t have product-market fit. It helps you learn quickly: you can switch through personas and see what will and won’t work.</p>

<p>How do you do learn to see? I know no better way than to practice. Try it a 100 times. <a href="https://neilkakkar.com/subscribe">Come back next year</a>, and maybe I’ll have a better way once I’ve done it a 100 times.</p>

<h2 id="learn-to-make-and-execute-decisions-quickly">Learn to make and execute decisions quickly</h2>

<p>Most people have a bias towards analysis-paralysis versus getting shit done.</p>

<p>When decisions are reversible - and they mostly are - speed is a super power. Cultivating a habit of making decisions quickly, and then executing them is better than just thinking about it.</p>

<p>Training this skill begins as easily as deciding what to eat on a huge menu. It’s a small step, but over time, <a href="#learn-to-take-compounding-seriously">even the smallest steps compound</a>.</p>

<blockquote>
  <p>“Hesitation is always easy, rarely useful” - Prof. Quirrel alterego, <a href="http://www.hpmor.com/" target="_blank" rel="noopener">HPMOR</a></p>
</blockquote>

<p>Here’s an <a href="https://firstround.com/review/speed-as-a-habit/" target="_blank" rel="noopener">example in the context of business</a>. And a <a href="https://twitter.com/sama/status/1345140364995227648" target="_blank" rel="noopener">tweet from Sam Altman</a>.</p>

<h2 id="learn-to-spot-a-convex-or-concave-world">Learn to spot a convex or concave world</h2>

<p>In the world of viral infections, a 50% lockdown is worse than a 0% and a 100% lockdown, both. The virus isn’t contained, and businesses have to shut down, too.</p>

<p>In the world of immigration policies, letting some specific people in is better than letting no one or everyone in. The middle ground is better than the extremes.</p>

<p>When the best of both worlds is great, you’re in a concave disposition.</p>

<p>When the best of both worlds is worse than either, you’re in a convex disposition.</p>

<p>The world is sometimes concave, and sometimes convex. Knowing your topology can help you make better decisions.</p>

<p>I first noted this when <a href="https://vitalik.ca/general/2020/11/08/concave.html" target="_blank" rel="noopener">Vitalik Buterin explained it</a>. Read it for more concrete examples.</p>

<!-- ## Learn to do obvious things -->

<h2 id="learn-to-tell-stories">Learn to tell stories</h2>

<p>People donate more to charity when they know a single victim’s story, versus statistics of a thousand deaths. It’s called the <a href="https://en.wikipedia.org/wiki/Identifiable_victim_effect" target="_blank" rel="noopener">Identifiable Victim Effect</a>, but it’s the power of stories over facts. The right framing gets you further than all the facts combined.</p>

<blockquote>
  <p>“A single death is a tragedy; a million deaths is a statistic.”</p>
</blockquote>

<p>Ideas &amp; facts contextualised by stories are more powerful than either alone.</p>

<p><i></i><b>The Skill of Storytelling</b><br>
There’s lots to unpack here, and this is the first skill I’ve been working on for the past few months. Watch out for a long blogpost in 2 weeks!</p>

<h2 id="learn-to-dive-into-the-source-code-when-documentation-isnt-enough">Learn to dive into the source code when documentation isn’t enough</h2>

<p>Sometimes, there’s no precedent for what you want to do. Or the people who did it before didn’t write a manual.</p>

<p>In cases like these, figuring things out for yourself is powerful. Research papers and obscure books aren’t just for scientists. They’re freely available on the internet* for all of humanity to use. Learn to use it. Learn about resources like <a href="https://sci-hub.do/" target="_blank" rel="noopener">SciHub</a>, <a href="https://libgen.xyz/" target="_blank" rel="noopener">LibGen</a>, and hiring researchers. You’re allowed to hire people (specially graduate students!) to satisfy your research concerns.</p>

<p>… and when you’re done, <a href="https://neilkakkar.com/things-I-learned-to-become-a-senior-software-engineer.html#writing-code">preserve context for future you</a>.</p>

<p>It’s a lot like trying to use an API that has no documentation. Would’ve been easy if there was documentation, but there isn’t. So you got to do it the hard way: read the source code and figure out what you need to make things work.</p>

<p>It’s also like <a href="https://neilkakkar.com/A-framework-for-First-Principles-Thinking.html">figuring out what you need to build rockets yourself</a> when existing ones are too expensive.</p>

<p><a href="https://www.lesswrong.com/posts/37sHjeisS9uJufi4u/scholarship-how-to-do-it-efficiently" target="_blank" rel="noopener">More resources here</a>.</p>

<h2 id="learn-to-be-specific">Learn to be specific</h2>

<p>Every time I gave an example above, I was training my specificity muscles.</p>

<p>Most of the time, most people don’t know what they’re talking about. Not being specific is a sign of that. The more abstract the word, the harder it is to pin down a meaning.</p>

<p>For example, “negative ramifications” doesn’t tell you what exactly happened, while “the sonic boom from the new supersonic jet destroyed windows in a 100m radius” is a lot more specific.</p>

<p>Learn to be specific, and learn to spot when others aren’t. <a href="https://www.lesswrong.com/posts/NgtYDP3ZtLJaM248W/sotw-be-specific" target="_blank" rel="noopener">Here’s how</a>.</p>

<h2 id="learn-to-see-systems">Learn to see systems</h2>

<p>There’s two kinds of people.</p>

<ul>
  <li>Bob, who will see this list, find some skills very interesting, and then go about honing those skills</li>
  <li>Alice, who will see this list, and wonder how I came up with these
<!-- - The Inspiration-Junkie, who will lurk and move on to the next inspiring post without changing anything -->
</li>
</ul>

<p>Alice would then try to understand the system that generated these ideas. Then, she’ll adopt the system, and come up with skills possibly more relevant to herself.</p>

<p>Having the option to do both is powerful. Since Bob is the default, <a href="https://neilkakkar.com/How-to-see-Systems-in-everyday-life.html">learn to be like Alice</a>. Choose <a href="https://neilkakkar.com/understanding-systems.html">systems when things are important</a> to you. Choose hacks when you need a quickfix.</p>

<figure>
    
    <img src="https://neilkakkar.com/assets/images/divider.jpg" alt="">
    
    
    
</figure>



    
  </div></div>]]>
            </description>
            <link>https://neilkakkar.com/powerful-life-skills.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25716764</guid>
            <pubDate>Sun, 10 Jan 2021 17:09:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Weirdness of Kentucky Route Zero (2016)]]>
            </title>
            <description>
<![CDATA[
Score 119 | Comments 29 (<a href="https://news.ycombinator.com/item?id=25716686">thread link</a>) | @olvy0
<br/>
January 10, 2021 | http://blog.joshhaas.com/2016/10/the-weirdness-of-kentucky-route-zero/ | <a href="https://web.archive.org/web/*/http://blog.joshhaas.com/2016/10/the-weirdness-of-kentucky-route-zero/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	<p>In <a href="http://blog.joshhaas.com/2016/10/mr-robot-is-not-weird-enough/">my last post</a>, I took issue with the TV show <i>Mr. Robot</i> for not being weird enough. Although imaginative and compelling, its universe is well-ordered: everything happens for a reason.</p>
<div>

<p>If you’re looking for media to consume that doesn’t suffer from that problem, I recommend <a href="http://kentuckyroutezero.com/"><i>Kentucky Route Zero</i>.</a></p>
</div>

<p>I <a href="http://blog.joshhaas.com/2016/10/yet-another-review-of-the-trump-clinton-debate/">previously discussed</a> <i>Kentucky Route Zero</i> (<i>KR0</i> for short) in the context of the first Trump-Clinton debate. It’s relevant to that because it’s a tour through coal-country America, and it engages with the desolation there that’s fueling Trump’s support.</p>

<p>As a portrait of the region, it’s a Picasso or maybe a Goya, not a Velázquez. It has moments where it approaches documentary realism, but it mostly traverses an imaginary landscape reflecting its creators’ perceptions, inspired by their real-life travels in Kentucky.</p>

<p><i>KR0</i>’s medium is a computer game, but “game” is misleading. There’s no element of skill. It has the interface of an adventure game, but unlike other games in that genre, your progression through the story isn’t blocked by puzzles to solve. Rather, it’s more like a work of interactive fiction. The story is mainly told through dialogue, though the soundtrack and visuals are important pieces of the experience.</p>
<div>

<p>The medium is appropriate: it makes a better game than it would a book or a tv show. The ambition seems to be to create a world, and the ability to explore it freely is important. There’s a narrative, but the world is alive beyond the narrative, and there’s a lot to discover outside the main plot.</p>

</div>
<p>The three-person studio that produced <i>KR0</i>, Cardboard Computer, has been trying to erase the lines between their fictional universe and the real one. They’ve released a number of companion pieces to the game, including <a href="http://kentuckyroutezero.com/the-entertainment/">an experience for Occulus Rift or Mac / PC</a> where you participate as a cast member of a fictional 1973 production of a one-act play. The play appears to depict events that occur in a bar a few hours before the character you play in <i>KR0</i> visits the bar in the game, but the fictional set designer for the 1973 production — ie, a real person in the production’s fictional reality — is also a character in <i>KR0</i>. In case that wasn’t confusing enough, you can buy a <a href="http://www.lulu.com/shop/http://www.lulu.com/shop/lem-doolittle/the-entertainment/paperback/product-21312732.html">print copy of the script</a>, published under the name of the fictional author.</p>

<p>It’s not a bad play, either. As the script advertises, “The one-act play “A Reckoning,” set in a tavern in central Kentucky, is Doolittle’s take on the sort of barroom tragedy made popular by O’Neill, Gorky, etc.”, and I would say it stands on its own as a piece of theater, although the ending will have more resonance if you’ve played through <i>KR0</i>.</p>

<p>This almost pedantic accumulation of fictional detail, both inside and outside the game — names, biographies, places, events — lends believability and power to <i>KR0</i>’s magical realist plot-line. Because the production team took such great pains to create verisimilitude, the more fantastic elements of the game feel justified: hauntings, strange and implausible creatures, a whiskey company whose employees are all glowing skeletons, and the titular Route 0, a hidden underground highway through non-euclidean space.</p>

<p>The game’s plot is simple and unobtrusive compared to the sprawling, strange world it is set in. Conway, a truck driver for an antiques store, is trying to make a delivery to Dogwood Drive, a street that doesn’t show up on his maps. As he looks for it, he picks up some traveling companions, and we learn more about his and their pasts. <i>KR0</i> isn’t complete yet —the game is divided into five acts, and final one hasn’t been released — so I don’t know yet if Conway ever makes it to his destination.  In fact, I still don’t know what he’s delivering, or to whom.</p>

<p>As a player, the main way you exert agency is through your choice of dialogue options. Unusually for adventure-style games, your dialogue choices don’t seem to affect the plot. Rather, they affect the past: you can give the characters different backstories, influence their temperaments, change how they see the world and treat each other. It’s a limited degree of freedom: I haven’t flexed the game aggressively to see how divergent you can make it, but my understanding is that the basic outline of who the characters are always remains the same. It’s more of a matter of altering the shadings.</p>

<p>The net effect of the simple plot, the strange, expansive world, and the freedom to emphasize and explore different aspects of the characters is that playing the game doesn’t feel like you’re being told a story, with themes and a moral. Rather, it feels more like an invitation to you, the player, to interpret what you’re confronted with. The game gives you a lot of details to work with, and powerful images and emotions, but leaves you to decide what to think about it all.</p>

<p>At its heart, I think <i>Kentucky Route Zero</i> is a meditation on entropy. Certainly, entropy is the unifying characteristic of the game world. <i>KR0</i>’s setting is a Kentucky that’s been devastated by the collapse of the coal industry and by the 2008 housing crisis. Everything you encounter is in some state of falling apart. The gas station you refuel at has overdue electric bills. The bar can’t buy alcohol any more. The coal mine is abandoned. Conway’s dog looks like she’s seen better days. All the characters have various stories of poverty, alcoholism, loss, and debt.</p>

<p>Entropy has different facets. There are many different stances that one can take towards it. <i>KR0</i> seems to explore each of them in turn, weighing them, inviting you to partake.</p>

<p>The most basic stances are the emotional ones: despair, grief, and anger. There’s certainly plenty of that throughout the game. In one particularly powerful moment, you come across a memorial for coal miners who drowned when some tunnels were flooded in an accident a decade or so ago.  The memorial is a collection of hard-hats floating in an underground lake, accompanied by an angry, hand-written sign accusing the mining company of negligence. In another moment, you meet a team of engineers who spent their lives trying to build a computer system (called Xanadu, presumably a reference to the <a href="https://www.wired.com/1995/06/xanadu/">real world could-have-been internet competitor</a>), who are now just sitting around hopelessly, having given up on ever completing their lives’ work.</p>

<p>Another stance is simple momentum: keeping going on as long as you can. One character you meet is a switchboard technician, the last one on her team after all her coworkers were laid off by the phone company automating the systems. They couldn’t quite automate her, so she keeps plugging away, alone in a tunnel, connecting call after call. There’s also a church, relocated to a warehouse by some beauracrats, where the congregation all drifted away, the preacher left, and now it’s just a janitor who puts on pre-recorded sermons every Sunday.</p>

<p>Entropy and grief can also give rise to beauty. <i>KR0</i> has plenty of beautiful moments too. The game has a gorgeous soundtrack, mixing electronic music and ambience with bluegrass classics. The bluegrass pieces, performed by a mysterious wandering trio who occasionally wander across your path, are all explorations of loss and hardship, transmuted into folk songs and hymns. The visual palette of the game is mixture of blues and oranges, mostly subdued and minimalist, but occasionally spectacular. Everything has a satisfying organic, analog feel. Radio systems crackle, televisions hum, computers react to strange magnetic fluctuations.</p>

<p>Yet another approach to entropy is to consume and exploit it. At various points in the game, you encounter modern, structured institutions, that are in the process of channeling the breakdown toward their own ends. There’s a whiskey distillery that’s steadily acquiring the balance sheets and souls of the folks you meet. You get to take a tour of its expansive, industrially-clean factory in an amazing descent-into-hell sequence that feels like Dante meets OSHA. The local power company also seems to be on the march. There are also more highbrow institutions consuming the entropy. For instance, you visit the “Bureau of Reclaimed Space”, which seems to represent government, taking in weirdness and outputting paperwork. In another interlude, you find an entire town that’s been transplanted to be inside a museum, the residents still living in their houses, enclosed in a giant glass warehouse.</p>

<p>Somewhat related to high-brow consumption, there’s intellectualization of entropy. <i>KR0</i> has a steady stream of references to academia. You meet a number of characters who have spent time in the grad student / post-doc limbo space, and there’s a lot of art and math jargon. From an academic perspective, entropy is a source of phenomena to record, analyze, and write papers about — hopefully publishable ones. This content is a reflection of the game itself, which in many ways feels like a modern art project. <i>KR0</i> is obsessed with topology and imaginary spaces. Characters muse about space aloud, and as you move around the game, you explore a number of different spaces and means of navigating them: a driving map of Kentucky as navigated by truck, the same map as navigated by a bird, a bureaucratic office building where you ride an elevator up and down, a pure mathematical abstraction that you traverse by turning around at certain symbols, an endless underground river where you are swept along on a boat, among others. This self-conscious exploration invites you to see the entropy in <i>KR0’s</i> world as something to think about and study.</p>

<p>Finally, entropy gives rise to newness: the inadvertent creativity of random processes. Two of the more memorable characters you meet are a pair of androids that, in their telling, emerged from the mines as shapeless lumps, and transformed themselves into a pair of motorcycle-riding musicians, who are now <a href="https://killscreen.com/articles/kentucky-route-zeros-junebug-set-release-tktk-album-androids/">releasing an album outside the game</a>. Abandoned things in <i>KR0</i> tend to take on a life of their own.  A hobo sets up shop as an organist in a church converted into an office …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://blog.joshhaas.com/2016/10/the-weirdness-of-kentucky-route-zero/">http://blog.joshhaas.com/2016/10/the-weirdness-of-kentucky-route-zero/</a></em></p>]]>
            </description>
            <link>http://blog.joshhaas.com/2016/10/the-weirdness-of-kentucky-route-zero/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25716686</guid>
            <pubDate>Sun, 10 Jan 2021 17:01:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Secure and blinded medical data analysis with OpenSAFELY]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25716685">thread link</a>) | @stuartbman
<br/>
January 10, 2021 | https://explainthispaper.com/identifying-covid-risk-factors-new-secure-analytics-platform/ | <a href="https://web.archive.org/web/*/https://explainthispaper.com/identifying-covid-risk-factors-new-secure-analytics-platform/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<section>
<div>
<p><span>
<div><h3>Clinical Need</h3><p>When COVID-19 first hit, governments across the globe had to make tough public health decisions with little information. The measures they put in place to reduce spread can prevent COVID-19-related deaths but can also have <i>negative</i> consequences on physical and mental health😷.</p><p>Understanding what factors determine risk of serious outcomes from COVID-19 can help guide these policies. For example, people who are high risk may be advised to shield at home.</p><p>To understand these risk factors, there was a need to analyse large volumes of medical records. Unfortunately, getting access to these records and <sample>linking them appropriately</sample> typically requires lots of regulatory approvals and takes a long time.</p></div>
</span>
</p>
<div>

<div><p>To really understand COVID, we need to link up clinical appointment notes with test results, death records, etc.</p><p>However, this information is typically kept in different locations.</p></div>
<p><img alt="chris.jpg" height="200" src="https://explainthispaper.s3.amazonaws.com/images/chris.2e16d0ba.fill-200x200.jpg" width="200">
</p>

</div>
</div>
</section>
<section>
<div>
<p><span>
<div><p>As a result, it’s hard to get really big datasets but <sample>the bigger, the better</sample>.</p><p>So what could we do? This research team came up with a great solution…</p></div>
</span>
</p>
<div>

<div><p>The larger the dataset, the greater statistical strength you have for the analysis.</p><p>In a smaller dataset, you might see trends - but then not have enough data to confidently say it is not due to random variation.</p><p>This is really important when looking at lots of different variables contributing to an outcome, as they did in this study.</p></div>
<p><img alt="chris.jpg" height="200" src="https://explainthispaper.s3.amazonaws.com/images/chris.2e16d0ba.fill-200x200.jpg" width="200">
</p>
<p><span><b>
Chris Lovejoy</b>
</span><br>
<span>Doctor, Data Scientist</span>
</p>
</div>
</div>
</section>
<section>
<div>
<p><span>
<div><h3>What did they do?</h3><p>They assembled a team of clinicians👨‍⚕️, programmers👩‍💻, data scientists👨‍🔬 and epidemiologists🧑‍💼 and came up with a new way of extracting and analysing health data. They called this platform OpenSAFELY.</p><p>The traditional approach is to: (i) clean data and <sample>pseudonymise it</sample> (ii) download it, then (iii) run an analysis.</p></div>
</span>
</p>
<div>

<p>This is a bit like anonymisation, but not as strict. Information that could enable the individual to be identified (such as date of birth or home address) is modified, but can still be re-identified by those (ie. the researchers) using a ‘de-identification key’.</p>
<p><img alt="chris.jpg" height="200" src="https://explainthispaper.s3.amazonaws.com/images/chris.2e16d0ba.fill-200x200.jpg" width="200">
</p>
<p><span><b>
Chris Lovejoy</b>
</span><br>
<span>Doctor, Data Scientist</span>
</p>
</div>
</div>
</section>
<section>
<div>
<p><span>
<div><p>However, this isn’t particularly secure (what if someone’s laptop gets stolen?), pseudonymisation isn’t fail-safe and it allows <sample>repeated analyses which risks identifying false relationships</sample>.</p><p>This team’s new approach is to upload the code for analysing the data to the electronic health record directly. The code is then run and returns the results. The data never leaves the health record. This maintains privacy and prevents repeated analyses 💯</p><p>They used this to look at the factors affecting risk of dying from COVID-19, in a GP health record dataset of 17 million individuals.</p></div>
</span>
</p>
<div>

<div><p>There’s always a possibility that patterns identified in the data are down to chance.</p><p>If a test has “95% confidence” it means there’s a 5% chance it happened by chance.</p><p>We can bear this in mind when interpreting results. However, what happens if we run multiple analyses, looking for a true result?</p><p>We give the data lots of chances to fall into that 5% that is due to chance.</p><p>With a pressure to publish interesting findings, this can incentivise researchers to run multiple tests in order to find these relationships – which may not be true. These may seem interesting, but they’re bad science.</p><p>This is known as ‘data dredging’ or ‘p-hacking’.</p></div>
<p><img alt="chris.jpg" height="200" src="https://explainthispaper.s3.amazonaws.com/images/chris.2e16d0ba.fill-200x200.jpg" width="200">
</p>
<p><span><b>
Chris Lovejoy</b>
</span><br>
<span>Doctor, Data Scientist</span>
</p>
</div>
</div>
</section>
<div><h3>How does it work?</h3><p>1️⃣ First, after the researcher decides which data they want to analyse (e.g. all patients with diabetes) — they write some code to extract that from the health records.</p><p>2️⃣ When that code is run, they receive data to download. However, the data is a placeholder. It looks like the real data - but all the values are made up! Knowing the structure of the data helps the researchers write the code.</p><p>3️⃣ The working code is sent over to the health record (packaged up in a wrapper called <a href="https://www.docker.com/">Docker</a>) where it performs the analysis. Only the results are returned to the researchers - the patient data never leaves the health records. So no-one (not even the researchers) see the raw patient data.</p></div>
<div><h3>What did they find?</h3><p>They looked at data from 17 million people and found COVID-19-related death is increased by:</p><ul><li>Being male</li><li>Being older</li><li>Higher deprivation</li><li>BAME ethnicity (part of this explained by higher prevalence of medical problems and higher levels of deprivation)</li><li>Obesity</li><li>Various other medical conditions (such as diabetes, severe asthma, cancer and dementia)</li></ul></div>
<section>
<div>
<p><span>
<div><h3>Any limitations?</h3><p>This analysis was done in the early days of the pandemic. This meant we didn’t have the testing capacity we have now🧪. To circumvent this, the researchers included ‘<sample>clinically suspected’ cases</sample> of COVID-19 – and not just ones where it had been confirmed by a COVID-19 test. Some of these ‘positive’ cases may not have actually had COVID-19.</p><p>There were other common issues seen in data analysis on this scale: Some patients had missing data, like obesity, smoking status and ethnicity. Also, health record availability varies between region. They used data from a single GP electronic record company (TPP), whereas some regions (such as Scotland and North-East England use an alternative called EMIS). This means the sample population for the study may not represent the whole population (or indeed populations outside of England).</p></div>
</span>
</p>
<div>

<p>They don't include their definition here, but from the UK Government, this includes: new continuous cough or temperature ≥37.8°C or loss of, or change in, normal sense of smell (anosmia) or taste (ageusia)</p>
<p><img alt="stu.jpg" height="200" src="https://explainthispaper.s3.amazonaws.com/images/stu.2e16d0ba.fill-200x200.jpg" width="200">
</p>
<p><span><b>
Stu Maitland</b>
</span><br>
<span>NIHR Doctoral Fellow</span>
</p>
</div>
</div>
</section>
<div><p>It was a great feat to get this platform up-and-running and publish the analysis in such a short space of time. This type of research doesn’t usually happen that fast.</p><p>The study helped public health teams and researchers make decisions. For example, in the UK this provided support for advising at-risk populations to shield🛡.</p><p>This also presents a new paradigm for data analysis of health data, that could enable faster, more secure and more reproducible research going forward. All the code is <a href="https://github.com/opensafely/risk-factors-research">open-source and freely available</a>. This means anybody can inspect the code and other researchers are free to use it.</p><p>Since this paper, the same research group have used this platform to look at the impact on COVID-19 risk of (i) living with school-age children, (ii) HIV, (iii) ethnicity, (iv) taking hydroxychloroquine and (v) steroids with asthma or COPD.</p></div>
</div></div>]]>
            </description>
            <link>https://explainthispaper.com/identifying-covid-risk-factors-new-secure-analytics-platform/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25716685</guid>
            <pubDate>Sun, 10 Jan 2021 17:01:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Applying Apple's Framework for Avoiding Mediocrity]]>
            </title>
            <description>
<![CDATA[
Score 19 | Comments 11 (<a href="https://news.ycombinator.com/item?id=25716652">thread link</a>) | @sarthakjain
<br/>
January 10, 2021 | https://www.sarthakjain.com/p/apples-framework-for-escaping-mediocrity?r=86tsj | <a href="https://web.archive.org/web/*/https://www.sarthakjain.com/p/apples-framework-for-escaping-mediocrity?r=86tsj">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><strong>Note this isn't an official framework Apple uses internally. Neither this post nor the author are in any way affiliated with Apple. It is however a framework you can use to understand how Apple prioritizes building products and the closest outside guess to Apples product prioritization framework.</strong></p><h3>Mental Framework</h3><p>Every task or product decision should be put into one of 4 buckets:</p><p><strong>1. Not important.</strong></p><p>Don't do it. </p><p><strong>2. Not important. But needs to be done.</strong></p><p>Do the bare minimum</p><p><strong>3. Important</strong></p><p>As good as the best. Copy the best.</p><p><strong>4. Most important</strong>.</p><p>Innovate to make sure it’s better than the best.</p><h3>Isnt this obvious?</h3><p>It is mostly. However there is one notable exception. Average. If it's looking like what you are going to do is average you are better off doing the bare minimum.</p><p>If a task is between important and not important go either way do as good as best or bare minimum. Don't do average.</p><h3>Apple&nbsp;as an example:</h3><p>Apple is probably the most intense when using this framework. A great example is their history with headphones. For years Apple shipped below average headphones with their iPhones and iPods. Apple's headphones never competed with the likes of Bose. There was nothing new, nothing innovative about them. They needed to be there for Apple's core products to function and they were. They fell into the bucket of ‘2. Not important, but needs to be done.’ Apple even encouraged you to buy third party headphones.</p><p>Once Apple noticed a new innovative product that was gaining traction (the Bragi Dash on kickstarter, and possibly before this) they created a best in class product the Apple Air Pods. This suddenly took their headphone offering from below average to best in class. You could say they were copies of the best or they were better than the best but they certainly weren't average. This had become a category that was now important to Apple. Once the Air Pods took off Apple pushed it&nbsp;to the next level moving it to most important creating the Air Pods Pro leaving everyone in their dust.</p><p><strong>Other examples of products from Apple in each category</strong></p><ol><li><p>Search - Don't care, won't do </p></li><li><p>iCloud/Safari- I'd argue it's the bare minimum they can get away with </p></li><li><p>Airpods Max - as good as the best </p></li><li><p>MacBook/iPad - better than the best </p></li></ol><p>Even if you don't agree with the categorisation you'll probably still agree that Apple has below average products and best in class products but they don't create average products. That is why they have the market dominance they do.</p><p><strong>Even Apple fails occasionally:</strong></p><p>One example of Apple failing to follow this philosophy is in their maps product. Apple Maps really is a very “average” product which came nowhere close to competing with Google Maps. It's very existence necessitates doing more than the bare minimum which would have been to show Google Maps pre-installed. Word is that they are taking this shortcoming seriously and looking to launch a new redesigned Maps soon.</p><h3>Using this framework daily </h3><p>This was originally a slack post shared with the team at <a href="https://nanonets.com/">Nanonets</a>. We use this on a daily basis when scoping tasks. We ask ourselves:</p><p><strong>Do we need to do the bare minimum or best in class?</strong></p><p>Depending on the the answer:</p><p>A) If bare minimum then the follow up question is do we even need to do it? </p><p>B) If best in class should we copy the best or be better than the best?</p><p>If you don't hold yourself to this standard you will mostly end up shipping average features that lead to average products.</p><p><strong>Note</strong>: everything about your product need not be best in class. Only the the features that you matter to your customers and to you as a product/company, which also depends on and will change with the stage you are at.</p><p>I'm the CEO at <a href="https://nanonets.com/">Nanonets</a>, we are building AI to help machines see the world - starting with documents. </p><p>I write about startups, products and technology.</p></div></div>]]>
            </description>
            <link>https://www.sarthakjain.com/p/apples-framework-for-escaping-mediocrity?r=86tsj</link>
            <guid isPermaLink="false">hacker-news-small-sites-25716652</guid>
            <pubDate>Sun, 10 Jan 2021 16:58:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I ignored my mental health, and got hit hard by OCD (a cautionary tale)]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25716477">thread link</a>) | @coolvision
<br/>
January 10, 2021 | https://grgv.xyz/ocd_story | <a href="https://web.archive.org/web/*/https://grgv.xyz/ocd_story">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				
<p>Have you seen “Aviator”, a movie with Leonardo DiCaprio about Howard Hughes? Then you might recall what an OCD is.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/2fXF8G50BPQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<p><strong><a href="https://en.wikipedia.org/wiki/Obsessive%E2%80%93compulsive_disorder">Obsessive–compulsive disorder</a>&nbsp;(OCD) is a&nbsp;mental disorder&nbsp;in which a person has&nbsp;certain thoughts ”repeatedly&nbsp;(called "obsessions") or feels the need to perform&nbsp;certain routines repeatedly&nbsp;(called ”compulsions") to an extent which generates distress or impairs general functioning</strong></p>
<p>I have a mild form of OCD. Had it since teenage years. But rarely bothered me, and I mainly did not care about it much. It manifested itself maybe few times per year, where I would have to re-check few times if the door is locked, for example (which does not seem like a huge burden).</p>
<p>And so, I felt great in autumn and in the first half of December. Everything was fine, I have a job that I love, great family, and hobbies that I enjoy.</p>
<p>Then, this winter came, and It got worse. I live in a northern country (Estonia), daylight time is very short, and although I don’t suffer from SAD too much usually, it might have been a factor. And I usually try to get some vacation in warmer countries, but this year did not do it because of COVID. I also started to work from home, and it probably added to the feeling of isolation and brought me down more.</p>
<p>All in all, I don’t know what were that main factors, but probably the whole combination was too much. Occasional obsessions and compulsions started to show up more, but I mainly just shrugged them off. Then, closer to the new year, it stared to get even worse.</p>
<p><strong>And now it’s at a point where I strongly need professional help, I barely can work, lost lots of weight, have horrible anxiety all the time, and all the obsessions make my behaviour erratic at times.</strong> And this is yet another problem — its actually hard to find professional help quickly! It can be weeks or months until finding a therapist.</p>
<p><em>My main mistake was not looking for warning signs of illness.</em> It was “today it’s fine, then all will be fine” attitude, which is a big mistake.</p>
<p>Yes, my case is slightly exotic, but it relates to all other mental health issues: depression, anxiety, panic attacks, etc… <a href="https://www.nimh.nih.gov/health/statistics/mental-illness.shtml">Nearly 20% of people have some form of mental illness</a>, and some of the issues might not seem like a big deal until they hit hard.</p>
<p>As a tale of caution, I have some advice, which I will follow religiously in future. This is relatively obvious and simple, but let my example be a bit of a motivation to take it more seriously.</p>
<ul>
<li>Maintain mood logging, and journal regularly. It would help to notice any anomalies and problems before they become serious. Don’t shrug off warning signs!</li>
<li>Find mental health checklists, like <a href="https://www.beyondblue.org.au/the-facts/anxiety-and-depression-checklist-k10">https://www.beyondblue.org.au/the-facts/anxiety-and-depression-checklist-k10</a>, check yourself time from time,</li>
<li>Therapy is important, especially in this times. I never did therapy, and now regret it.</li>
<li>Fitness, meditation, good diet and all kinds of self care — in this time of increased isolation it’s not just some random good habits, treating this seriously is a very important part of well-being.</li>
</ul>


			</div></div>]]>
            </description>
            <link>https://grgv.xyz/ocd_story</link>
            <guid isPermaLink="false">hacker-news-small-sites-25716477</guid>
            <pubDate>Sun, 10 Jan 2021 16:43:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Deep Learning's Most Important Ideas – A Brief Historical Review]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25715318">thread link</a>) | @coolvision
<br/>
January 10, 2021 | https://dennybritz.com/blog/deep-learning-most-important-ideas/ | <a href="https://web.archive.org/web/*/https://dennybritz.com/blog/deep-learning-most-important-ideas/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>The goal of this post is to review well-adopted ideas that have stood the test of time. I will present a small set of techniques that cover a lot of basic knowledge necessary to understand modern Deep Learning research. If you're new to the field, these are a great starting point.</p><div id="post-content"><p>Deep Learning is an extremely fast-moving field and the huge number of research papers and ideas can be overwhelming. Even seasoned researchers have a hard time telling company PR from real breakthroughs. The goal of this post is to review those ideas that have <strong>stood the test of time</strong>, which is perhaps the only significance test one should rely on. These ideas, or improvements of them, have been used over and over again. They're known to work.</p> <p>If you were to start in Deep Learning today, understanding and implementing each of these techniques would give you an excellent foundation for understanding recent research and working on your own projects. It's what I believe the best way to get started. Working through papers in historical order is also a useful exercise to understand where the current techniques come from and why they were invented in the first place. <strong><strong>Put another way, I will try to present a <em>minimal set</em> of ideas that most of the basic knowledge necessary to understand modern Deep Learning research.</strong></strong></p> <p>A rather unique thing about Deep Learning is that its application domains (Vision, Natural Language, Speech, RL, etc) share the majority of techniques. For example, someone who has worked in Deep Learning for Computer Vision his whole career could quickly be productive in NLP research. The specific network architectures may differ, but the concepts, approaches and code are mostly the same. I will try to present ideas from various fields, but there are a few caveats about this list:</p> <ul> <li>My goal is not to give in-depth explanations or code examples for these techniques. It's not easily possible to summarize long complex papers into a single paragraph. Instead, I will give a brief overview of each technique, its historical context, and links to papers and implementations. If you want to learn something, I <em>highly recommend</em> trying to re-produce some of these paper results from scratch in raw <a href="https://pytorch.org/">PyTorch</a> without using existing code bases or high-level libraries.</li> <li>The list is biased towards my own knowledge and the fields I am familiar with. There are many exciting subfields that I don't have experience with. I will stick to what most people would consider the popular mainstream domains of Vision, Natural Language, Speech, and Reinforcement Learning / Games.</li> <li>I will only discuss research that has official or semi-official open source implementations that are known to work well. Some research isn't easily reproducible because it involves huge engineering challenges, for example <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">DeepMind's AlphaGo</a> or <a href="https://openai.com/projects/five/">OpenAI's Dota 2 AI</a>, so I won't highlight it here.</li> <li>Some choices are arbitrary. Often, rather similar techniques are published at around the same time. The goal of this post is not be a comprehensive review, but to to expose someone new to the field to a cross-section of ideas that cover a lot of ground. For example, there may be hundreds of GAN variations, but to understand the general concept of GANs, it really doesn't matter which one you study.</li> </ul>  <p><strong><strong>Papers</strong></strong></p> <ul> <li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">ImageNet Classification with Deep Convolutional Neural Networks (2012)</a> <span data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (2012)</span></li> <li><a href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature detectors (2012)</a> <span data-cites="hinton_improving_2012">Hinton et al. (2012)</span></li> <li><a href="https://arxiv.org/abs/1404.5997">One weird trick for parallelizing convolutional neural networks (2014)</a> <span data-cites="krizhevsky_one_2014">Krizhevsky (2014)</span></li> </ul> <p><strong><strong>Implementations</strong></strong></p> <ul> <li><a href="https://pytorch.org/hub/pytorch_vision_alexnet">AlexNet in PyTorch</a></li> <li><a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/alexnet.py">AlexNet in TensorFlow</a></li> </ul> <figure> <img src="https://dennybritz.com/assets/deep-learning-most-important-ideas/alexnet-full.png" alt=""><figcaption>Source: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks</a></figcaption> </figure> <p>AlexNet is often considered the algorithm responsible for the recent boom in Deep Learning and Artificial Intelligence research. It is a Deep Convolutional Neural Network based on the earlier LeNet developed by Yann LeCun. AlexNet beat previous methods at classifying images from the <a href="http://image-net.org/index">ImageNet dataset</a> by a significant margin through a combination of GPU power and algorithmic advances. It demonstrated that neural networks actually work! AlexNet was also one of the first times Dropout <span data-cites="hinton_improving_2012">Hinton et al. (2012)</span> was used, which has since become a crucial component for improving the generalization ability of all kinds of Deep Learning models.</p> <p>The architecture used by AlexNet, a sequence of Convolutional layers, ReLU nonlinearity, and max-pooling, became the accepted standard that future Computer Vision architectures would extend and built upon. These days, software libraries such as PyTorch are so powerful, and compared to more recent architectures AlexNet is so simple, that it can be implemented in only a few lines of code. Note that many implementations of AlexNet, such as those linked above, use the slight variation of the network described in <a href="https://arxiv.org/abs/1404.5997">One weird trick for parallelizing convolutional neural networks</a> <span data-cites="krizhevsky_one_2014">Krizhevsky (2014)</span>.</p>  <p><strong><strong>Papers</strong></strong></p> <ul> <li><a href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning (2013)</a> <span data-cites="mnih_playing_2013">Mnih et al. (2013)</span></li> </ul> <p><strong><strong>Implementations</strong></strong></p> <ul> <li><a href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html">DQN in PyTorch</a></li> <li><a href="https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial">DQN in TensorFlow</a></li> </ul> <figure> <img src="https://dennybritz.com/assets/deep-learning-most-important-ideas/deep-q-learning-value.png" alt=""><figcaption>Source: <a href="https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning">https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning</a></figcaption> </figure> <p>Building on top of the recent breakthroughs in image recognition and GPUs, a team at DeepMind managed to train a network to <a href="https://www.youtube.com/watch?v=V1eYniJ0Rnk">play Atari Games</a> from raw pixel inputs. What's more, the <em>same</em> neural network architecture learned to play seven different games without being told any game-specific rules, demonstrating the generality of the approach.</p> <p>Reinforcement Learning differs from Supervised Learning, such as image classification, in that an agent must learn maximize to the sum of rewards over multiple time steps, such as winning a game, instead of just predicting a label. Because the agent interacts directly with the environment and each action affects the next, the training data is not independent and identically distributed (iid), which makes the training of many Machine Learning models quite unstable. This was solved by using techniques such as experience replay <span data-cites="lin_self-improving_1992">Lin (1992)</span>.</p> <p>While there was no obvious algorithmic innovation that made this work, the research cleverly combined existing techniques, convolutional neural networks trained on GPUs and experience replay, with a few data processing tricks to achieve impressive results that most people would not have expected. This gave people confidence in extending Deep Reinforcement Learning techniques to tackle even more complex tasks such as <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">Go</a>, <a href="https://openai.com/projects/five/">Dota 2</a>, <a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii">Starcraft 2</a>, and others.</p> <p>Atari Games <span data-cites="bellemare_arcade_2013">Bellemare et al. (2013)</span> have since become a standard benchmark in Reinforcement Learning research. The initial approach only solved (beat human baselines on) seven games, but over the coming years advances built on top of these ideas would start beating humans on an ever increasing number of games. One particular game, Montezuma’s Revenge, was famous for requiring long-term planning and was considered to be among the most difficult to solve. It was only recently <span data-cites="badia_agent57_2020">Badia et al. (2020)</span> <span data-cites="ecoffet_first_2020">Ecoffet et al. (2020)</span> that techniques managed to beat human baselines on all 57 games.</p>  <p><strong><strong>Papers</strong></strong></p> <ul> <li><a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a> <span data-cites="sutskever_sequence_2014">Sutskever, Vinyals, and Le (2014)</span></li> <li><a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a> <span data-cites="bahdanau_neural_2016">Bahdanau, Cho, and Bengio (2016)</span></li> </ul> <p><strong><strong>Implementations</strong></strong></p> <ul> <li><a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#">Seq2Seq with Attention in PyTorch</a></li> <li><a href="https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt">Seq2Seq with Attention in TensorFlow</a></li> </ul> <figure> <img src="https://dennybritz.com/assets/deep-learning-most-important-ideas/seq2seq-cn.gif" alt=""><figcaption>Source: <a href="https://ai.googleblog.com/2017/04/introducing-tf-seq2seq-open-source.html">https://ai.googleblog.com/2017/04/introducing-tf-seq2seq-open-source.html</a></figcaption> </figure> <p>Deep Learning's most impressive results had largely been on vision-related tasks and was driven by Convolutional Neural Networks. While the NLP community had success with <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Language Modeling</a> and Translation using LSTM networks <span data-cites="hochreiter_long_1997">Hochreiter and Schmidhuber (1997)</span> and Encoder-Decoder architectures <span data-cites="sutskever_sequence_2014">Sutskever, Vinyals, and Le (2014)</span>, it was not until the invention of the <strong>attention</strong> mechanism <span data-cites="bahdanau_neural_2016">Bahdanau, Cho, and Bengio (2016)</span> that things started to work spectacularly well.</p> <p>When processing language, each token, which could be a character, a word, or something in between, is fed into a recurrent network, such as an LSTM, which maintains a kind of memory of previously processed inputs. In other words, a sentence is very similar to a time series with each token being a time step. These recurrent models often had difficulty dealing with dependencies over long time horizons. When they process a sequence, they would easily "forget" earlier inputs because their gradients needed to propagate through many time steps. Optimizing these models with gradient descent was hard.</p> <p>The new attention mechanism helped alleviate the problem. It gave the network an option to adaptively "look back" at earlier time steps by introducing shortcut connections. These connections allowed the network to decide which inputs are important when producing a specific output. The canonical example is translation: When producing an output word, it typically maps to one or more specific input words.</p>  <p><strong><strong>Papers</strong></strong></p> <ul> <li><a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a> <span data-cites="kingma_adam_2017">Kingma and Ba (2017)</span></li> </ul> <p><strong><strong>Implementations</strong></strong></p> <ul> <li><a href="https://d2l.ai/chapter_optimization/adam.html">Implementing Adam in Python</a></li> <li><a href="https://pytorch.org/docs/master/_modules/torch/optim/adam.html">PyTorch Adam implementation</a></li> <li><a href="https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/optimizer_v2/adam.py#L32-L281">TensorFlow Adam implementation</a></li> </ul> <figure> <img src="https://dennybritz.com/assets/deep-learning-most-important-ideas/optimizer-benchmark.png" alt=""><figcaption>Source: <a href="http://arxiv.org/abs/1910.11758">http://arxiv.org/abs/1910.11758</a></figcaption> </figure> <p>Neural networks are trained by minimizing a loss function, such as the average classification error, using an optimizer. The optimizer is responsible for figuring out how to adjust the parameters of the network to make it learn the objective. Most optimizers are <a href="https://ruder.io/optimizing-gradient-descent/">based on variations of Stochastic Gradient Descent (SGD)</a>. However, many of these optimizers contain tunable parameters such as a learning rate themselves. Finding the right settings for a specific problem not only reduces training time, but can also lead to better results due to finding a better local minimum of the loss function.</p> <p>Big resarch labs often ran expensive hyperparameter searches that came up with complex learning rate schedules to get the best out of simple but hyperparameter-sensitive optimizers such as SGD. When they …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dennybritz.com/blog/deep-learning-most-important-ideas/">https://dennybritz.com/blog/deep-learning-most-important-ideas/</a></em></p>]]>
            </description>
            <link>https://dennybritz.com/blog/deep-learning-most-important-ideas/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25715318</guid>
            <pubDate>Sun, 10 Jan 2021 15:09:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The many lies about reducing complexity part 2: Cloud]]>
            </title>
            <description>
<![CDATA[
Score 215 | Comments 115 (<a href="https://news.ycombinator.com/item?id=25714822">thread link</a>) | @rapnie
<br/>
January 10, 2021 | https://ea.rna.nl/2021/01/10/the-many-lies-about-reducing-complexity-part-2-cloud/ | <a href="https://web.archive.org/web/*/https://ea.rna.nl/2021/01/10/the-many-lies-about-reducing-complexity-part-2-cloud/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>The world has been getting a lot more complex, most people will agree to that. A major element in that rising complexity has been the <a href="https://ea.rna.nl/2020/02/11/a-tipping-point-in-the-information-revolution/">insanely huge amounts of machine logic we human species have been adding to the world</a>. Both that logic itself, as what it enables — think globalisation of trade and communication — has made most of our lives more complex and complicated in one way or another. And while it has brought us much, it also has a serious <a href="https://ea.rna.nl/2020/03/04/gossip-trust-and-the-information-revolution/">number of unwanted side-effects</a>.</p>



<p>We run into the boundaries of our ability to handle that complexity on a daily basis. Be it the large IT projects that invariably run late, cost too much, maybe even straight out fail. Or how we must try to stay secure in a digital world full of brittleness of logic and the weaknesses of humans.</p>



<p>[Note 1: This article is meant to be understandable (with a bit of effort) by non-specialists. There is quite a bit of jargon in it, but I try my best to explain all of it, including a table below with extensive explanation of a number of key terms. If I don’t explain a term, it is safe to ignore it if you don’t know what it is (e.g. when I mention a ‘tomcat server’ as an example), it is helpful for those that know, but not really necessary to know what it is to follow the story]</p>



<p>So, it isn’t a surprise that in IT, a constant drive over the last decennia has been the drive to reduce complexity. <em>‘Reducing complexity’ sells.</em> Especially managers in IT are sensitive to it as complexity generally is their biggest headache. Hence, in IT, people are in a perennial fight to make the complexity bearable. One method that has been popular for decennia has been standardisation and rationalisation of the digital tools we use, a basic “let’s minimise the number of applications we use”. This was actually part 1 of this story: <a href="https://ea.rna.nl/2016/01/10/a-tale-of-application-rationalization-not/">A tale of application rationalisation (not)</a>. That story from 2015 explains how many rationalisation efforts were partly lies. (And while we’re at it: enjoy <a rel="noreferrer noopener" href="http://dilbert.com/strip/2011-01-07" target="_blank">this Dilbert cartoon</a> that is referenced therein.) Most of the time multiple <em>applications</em> were replaced by a single <em>platform</em> (in short: a platform is software that can run other software) and the <em>applications</em> had to be ‘rewritten’ to work ‘inside’ that platform. So you ended up with one <em>extra</em> platform, the <em>same</em> number of applications and generally a few new <em>extra</em> ways of ‘programming’, specific for that platform. That doesn’t mean it is <em>all</em> lies. The new platform is generally dedicated to a certain type of application, which makes programming these applications simpler. But the situation is not as simple as the platform vendors argue. As Frederick Brooks had already told us in 1986: <a href="https://en.wikipedia.org/wiki/No_Silver_Bullet">There Is No Silver Bullet</a>.</p>



<p>Another drive has been to encapsulate (hide) complexity and access it through simpler interfaces. And a third has been to automate IT itself, creating complex ‘management IT’. All three play a role when we start to outsource IT to cloud services like Microsoft Azure or AWS.</p>



<p>[Note 2: This article has gotten out of hand. Totally. Quite long while I don’t digress a lot — as I often do. But exposing hidden complexity cannot be done by not presenting it to you. And not understanding how complex the real IT world is leads to bad outcomes. When the software for supporting the Covid-19 vaccination campaign was a few weeks(!) late because testing wasn’t done yet, I read about a leading politician state something like “Come on, a bit of testing, how hard can it be?”. That is a cringeworthy display of not understanding how complex IT is. And that is partly why I write this. Because until our leaders actually start to understand this, they will create more and more disasters out of ignorance. Back to the story.]</p>



<p>Cloud services have generally been explained (sold) to us with a graphic like this:</p>



<figure><img data-attachment-id="172827" data-permalink="https://ea.rna.nl/xaas-orthodox-1/" data-orig-file="https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-1.jpeg" data-orig-size="2519,1755" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="xaas-orthodox-1" data-image-description="" data-medium-file="https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-1.jpeg?w=300" data-large-file="https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-1.jpeg?w=720" src="https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-1.jpeg?w=1024" alt="" srcset="https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-1.jpeg?w=1024 1024w, https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-1.jpeg?w=2048 2048w, https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-1.jpeg?w=150 150w, https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-1.jpeg?w=300 300w, https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-1.jpeg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Typical depiction of what is being outsourced when you move to the cloud</figcaption></figure>



<div>
<div>
<p>For non-technical people,  here is a basic explanation of terms used in the above figure and in the text.</p>



<p><span><strong>On Premises</strong></span>: Using your own hardware<br><span><strong>Application</strong></span>: Software that you use, e.g. Microsoft Word.<br><span><strong>Data</strong></span>: The data in your program, say your text in Word.<br><strong><span>Runtime</span>:</strong> Software that is required for other software to run, e.g. basic functionality for Java programs (‘Java runtime’). (Java is a programming language.)<br><span><strong>Middleware</strong></span>: More complex software that is required for other software to run but may also have its own function. E.g. a database.<br><span><strong>Operating System</strong></span>: The lowest layer of software that sits between the machine and all other software. Like Windows or macOS at home.<br><span><strong>Virtualisation</strong></span>: A way to turn one very big real ‘machine’ (computer) into many <em>virtual machines</em> by arranging multiple operating systems to share the big machine. Sharing increases efficiency because not all operating systems are busy at the same time. It also has other advantages.<br><span><strong>Server</strong></span>: The big ‘real’ machine. Like your computer at home but much bigger with multiple processors and lots of memory so it can be shared.<br><span><strong>Storage</strong></span>: Separate machine that is optimised to provide storage (disks), can be used by multiple servers. At home people sometimes have this too in the form of a NAS. These are generally ‘appliances’, that is specialised hardware (in this case with a lot of disks) with specialised software to manage them.<br><span><strong>Networking</strong></span>: Separate machine that enables data traffic between systems. Like your modem, router and Wifi Access Point at home. Again, an appliance with specialised hardware (in this case network interfaces such as wifi antennas and sockets for network cables) that has specialised software to manage these.</p>
</div>



<div>
<p>The suggestion is this: as we move away from our own IT ‘on-premises’ (which includes whatever co-hosting data center you use, in. this context it just means you own your own IT hardware) to more and more in the cloud, we are outsourcing more and more, we are responsible for less, <em>our life simplifies</em>. More cloud is cheaper, simpler and more flexible. What is there not to like?</p>



<p>What there is not to like is that this suggestion is for a large part a lie. And a nasty one.</p>



<p>Take for instance networking. According to the graphic, as soon as you move to the cloud, it’s no longer your responsibility. But that is a lie, except for the rightmost option (SAAS — more about this later). If you set up your IAAS or PAAS in the public cloud — say Microsoft Azure — you have to manage quite a bit of networking. In fact, while Microsoft runs the underlying <em>hardware</em>, much of what has to be managed, will be managed by you. You decide on segmenting, networking, VPNs (virtual private networks — a way to protect traffic between networks), routing firewalls, etc., you’re just using Azure tooling to set it up. It’s easier, but it’s far from all gone.</p>



<p>It is best explained by using an example. Suppose you open up some of your cloud-based systems to access from the public internet? You can do that. And suppose you shouldn’t have, because these systems contain sensitive data? And suppose this data is stolen in a very public breach? Who is to blame? Microsoft for providing you with enough rope to hang yourself, or you? It is clear that if this is a big news story, the heading will not be “Microsoft was lax with its security and management”, but “Company X was lax with its security and management <em>in the cloud</em>“.</p>








</div>
</div>



<p>So, the reality of the situation is therefore more like this:</p>



<figure><img data-attachment-id="172832" data-permalink="https://ea.rna.nl/xaas-orthodox-morerealistic-5/" data-orig-file="https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-morerealistic-5.jpeg" data-orig-size="2541,1766" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="xaas-orthodox-morerealistic-5" data-image-description="" data-medium-file="https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-morerealistic-5.jpeg?w=300" data-large-file="https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-morerealistic-5.jpeg?w=720" src="https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-morerealistic-5.jpeg?w=1024" alt="" srcset="https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-morerealistic-5.jpeg?w=1024 1024w, https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-morerealistic-5.jpeg?w=2048 2048w, https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-morerealistic-5.jpeg?w=150 150w, https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-morerealistic-5.jpeg?w=300 300w, https://rnaea.files.wordpress.com/2021/01/xaas-orthodox-morerealistic-5.jpeg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>A slightly more realistic depiction of self-sourcing versus cloud outsourcing</figcaption></figure>



<p>The hardware — the iron — is indeed something that is completely handled by a cloud provider. This includes things like connecting the server to networks, power, etc., and replacing disk drives, fans, etc. In a fully self-sourced setup this actually turns out to be a limited affair. Most of the work is not hardware these days, it is software. Companies that run their own on-premises data centers don’t have a lot of data center hardware operators. Take the networking engineers. They may lay a few cables to a switch, but after that it is quickly moving to the management console and manage the appliance throughputs management interface — in other words: software work. Networking engineers, storage engineers, and compute engineers alike, their main tool is not a screwdriver, their main tool is a <em>keyboard</em>. Only the basic servers for virtual machines have little in terms of configuration. Networking and storage are <em>appliances</em>, specialised hardware with specialised software. The cloud provider has an interface on top of these that gives you that ‘enough rope to hang yourself with’, i.e. much of this is actually set up and maintained by you. Microsoft doesn’t create or manage your firewall settings, it only offers you an interface to create a virtual firewall running on their appliances and manage that <em>yourself</em>.  So it is a shared responsibility, and especially in networking: you do most of the work in much of the same way you would have to do when you were running your own appliances. Using a firewall in Azure is Microsoft spinning up a virtual appliance for you. And from that moment on, the work is all yours.</p>



<p>The only form of cloud where you really get rid of a lot of responsibility is Software-as-a-Service, or SAAS. That is because SAAS actually simplifies matters… …for the vendor. As explained in the EAPJ article <a href="http://eapj.org/vertical-integration-versus-horizontal-standardisation/">Vertical Integration versus (horizontal) standardisation</a>, the big advantage of SAAS is that the vendor of an application doesn’t need to support a myriad of technical landscapes out there, no myriad of different Linux versions, Java versions, as well as their configurations (security baselines, anyone?), just a single stack they fully manage themselves. That brings a huge standardisation for the vendor, and the advantage of that can be sold (in part) to the customer. Your responsibility is generally limited to a bit of Application tinkering (maybe add plugins, do some configuration) and of course your content. (And even the almost total outsourcing of SAAS is a little lie …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ea.rna.nl/2021/01/10/the-many-lies-about-reducing-complexity-part-2-cloud/">https://ea.rna.nl/2021/01/10/the-many-lies-about-reducing-complexity-part-2-cloud/</a></em></p>]]>
            </description>
            <link>https://ea.rna.nl/2021/01/10/the-many-lies-about-reducing-complexity-part-2-cloud/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25714822</guid>
            <pubDate>Sun, 10 Jan 2021 14:20:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I ignored my slowly worsening mental health, and got hit hard by OCD (a caution)]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25714408">thread link</a>) | @coolvision
<br/>
January 10, 2021 | https://grgv.xyz/ocd/ | <a href="https://web.archive.org/web/*/https://grgv.xyz/ocd/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				
<p>Have you seen “Aviator”, a movie with Leonardo DiCaprio about Howard Hughes? Then you might recall what an OCD is.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/2fXF8G50BPQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<p><strong><a href="https://en.wikipedia.org/wiki/Obsessive%E2%80%93compulsive_disorder">Obsessive–compulsive disorder</a>&nbsp;(OCD) is a&nbsp;mental disorder&nbsp;in which a person has&nbsp;certain thoughts ”repeatedly&nbsp;(called "obsessions") or feels the need to perform&nbsp;certain routines repeatedly&nbsp;(called ”compulsions") to an extent which generates distress or impairs general functioning</strong></p>
<p>I have a mild form of OCD. Had it since teenage years. It rarely bothered me, and I mainly did not care about it much. It manifested itself maybe few times per year, where I would have to re-check if the door is locked few times, for example (which does not seem like a huge burden).</p>
<p>And so, I felt great in autumn and in the first half of December. Everything was fine, I have a job that I love, great family, and hobbies that I enjoy.</p>
<p>Then, this winter came, and It got worse. I live in a northern country (Estonia), daylight time is very short, and although I don’t suffer from SAD too much usually, it might have been a factor. And I usually try to get some vacation in warmer countries, but this year did not do it because of COVID. I also started to work from home, and it probably added to the feeling of isolation and brought me down more.</p>
<p>All in all, I don’t know what were that main factors, but probably the whole combination was just a bit  too much. Occasional obsessions and compulsions started to show up more, but I mainly just shrugged them off. Then, closer to the new year, it stared to get even worse.</p>
<p><strong>And now it’s at a point where I need professional help, It's hard to work, I lost lots of weight, have bad anxiety all the time, and all the obsessions make my behaviour erratic at times.</strong> And yet another problem — its actually hard to find professional help quickly! It can be weeks or months until finding a therapist. Thakfully, I was able to secure an appointment within a weel, but not all people are as lucky.</p>
<p><em>My main mistake was not looking for warning signs of illness.</em> It was “today it’s fine, then all will be fine” attitude, which is a big mistake.</p>
<p>Yes, my case is slightly exotic, but it relates to all other mental health issues: depression, anxiety, panic attacks, etc… <a href="https://www.nimh.nih.gov/health/statistics/mental-illness.shtml">Nearly 20% of people have some form of mental illness</a>, and some of the issues might not seem like a big deal until they hit hard.</p>
<p>As a tale of caution, I have some advice, which I will follow religiously in future. This is relatively obvious and simple, but let my example be a bit of a motivation to take it more seriously.</p>
<ul>
<li>Maintain mood logging, and journal regularly. It would help to notice any anomalies and problems before they become serious. Don’t shrug off warning signs!</li>
<li>Find mental health checklists, like <a href="https://www.beyondblue.org.au/the-facts/anxiety-and-depression-checklist-k10">https://www.beyondblue.org.au/the-facts/anxiety-and-depression-checklist-k10</a>, check yourself time from time,</li>
<li>Therapy is important, especially in this times. I never did therapy, and now regret it.</li>
<li>Fitness, meditation, good diet and all kinds of self care — in this time of increased isolation it’s not just some random good habits, treating this seriously is a very important part of well-being.</li>
</ul>


			</div></div>]]>
            </description>
            <link>https://grgv.xyz/ocd/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25714408</guid>
            <pubDate>Sun, 10 Jan 2021 13:21:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Teacher creates ingenious exam question to find cheaters and catches 14 students]]>
            </title>
            <description>
<![CDATA[
Score 83 | Comments 93 (<a href="https://news.ycombinator.com/item?id=25713861">thread link</a>) | @ColinWright
<br/>
January 10, 2021 | https://www.irishmirror.ie/news/weird-news/teacher-creates-ingenious-exam-question-23228848 | <a href="https://web.archive.org/web/*/https://www.irishmirror.ie/news/weird-news/teacher-creates-ingenious-exam-question-23228848">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody"><form data-mod="skinnySignup" data-json="{&quot;mailingListId&quot;:&quot;37995&quot;,&quot;displayName&quot;:&quot;daily&quot;,&quot;callToAction&quot;:&quot;<p>Get all the very latest news in Ireland straight to your email every single day</p>&quot;,&quot;buttonText&quot;:&quot;Sign up!&quot;,&quot;contentId&quot;:6321963,&quot;newsletterImage&quot;:&quot;https://i2-prod.irishmirror.ie/incoming/article22352084.ece/BINARY/1_Covid-19-Scenes90466558.jpg&quot;,&quot;endpointUrl&quot;:&quot;https://response.pure360.com/interface/list.php&quot;,&quot;profile&quot;:&quot;Irish_Mirror&quot;,&quot;isPure360NewsLetter&quot;:true,&quot;pure360MailingListId&quot;:&quot;Irish Mirror - Daily Newsletter&quot;,&quot;newsletterSiteName&quot;:&quot;Irish Mirror&quot;}"><div><div><div><p><span><span>When you subscribe we will use the information you provide to send you these newsletters. Sometimes theyâ€™ll include recommendations for other related newsletters or services we offer. Our</span><a href="https://www.irishmirror.ie/privacy-notice/">Privacy Notice</a><span>explains more about how we use your data, and your rights. You can unsubscribe at any time.</span></span></p></div></div><p><span>Invalid Email</span></p></div></form><!-- Article Start--><p>Students who assumed their teacher 'on the older side' wouldn't be familiar with the latest cheating methods were caught red handed when he devised a brilliant method to catch them out.</p> <p>A pupil in the engineering class explained that when they all sat down to take their final exam, about half the class left the room to use the bathroom during the test - far more than the usual.</p> <p>The student said they assumed the vast majority were looking up answers on their phone, which 'irritated me' but they stayed focused and made their way through the paper.</p> <p>After leaving the exam hall, the pupil remembered there was one particular question that wasn't related to what they had all been taught in class, which had two parts - the Mirror UK reports.</p> <p>Part A was 'fairly easy' but they had no idea how to do part B, so they simply left it blank as it only accounted for 5 marks out of 100.</p> <figure data-mod="image" itemprop="image" itemscope="" itemtype="http://schema.org/ImageObject">
<meta itemprop="url" content="https://i2-prod.mirror.co.uk/incoming/article11553649.ece/ALTERNATES/s615b/0_Tests-on-table.jpg">
<meta itemprop="width" content="615">
<meta itemprop="height" content="410">
<div>

<p><img data-src="https://i2-prod.mirror.co.uk/incoming/article11553649.ece/ALTERNATES/s615b/0_Tests-on-table.jpg" alt="" content="https://i2-prod.mirror.co.uk/incoming/article11553649.ece/ALTERNATES/s615b/0_Tests-on-table.jpg" src="https://i2-prod.mirror.co.uk/incoming/article11553649.ece/ALTERNATES/s615b/0_Tests-on-table.jpg">
</p>
</div>
<figcaption>
<span itemprop="author"> (Image: Tetra images RF)</span>
</figcaption>
</figure> <p>When all the exams had been marked, their teacher sent all the university students an email to explain his diabolical plan to catch out those who had given themselves some outside help.</p> <p>Many of the pupils used the internet to find answers to exam and homework questions.</p> <p>Their teacher decided to use it against them after becoming fed up with students using the bathroom as an excuse to look up answers on their phones.</p> <section data-embed-group="read-more" data-embed-items="2" data-ad-dockable="true">   </section> <p>The student wrote on Reddit: "He purposely made part B impossible to solve, and about a month before the final he got a teaching assistant to ask the exact question [online], which was distinctly worded to be unique.</p> <p>"He then created his own account and answered the question with a bulls*** solution that seems right at first glance but is actually fundamentally flawed and very unlikely that someone would make the same assumptions and mistakes independently."</p> <p>From the 99 exams handed in, 14 of them fell for the trick and gave the exact answer their own teacher had posted online.</p> <p>All were given an overall score of zero and reported to the university for violating the academic honor pledge they had signed.</p> <p>Their names were also circulated to all the other teachers in the department as known cheaters - and all the other students who hadn't cheated were given full marks for the bogus question.</p> <p>Others were impressed with cunning plan, with one replying: "This is Amazing! I've seen some stories like this and it always makes me glad I don't use [the internet] for tests.</p> <p>"Honestly if you're cheating on a proctored test you deserve to get caught. Study like everyone else."</p> <p>A second wrote: "Honey pot the cheating site. Genius!"</p><!-- Article End--></div></div>]]>
            </description>
            <link>https://www.irishmirror.ie/news/weird-news/teacher-creates-ingenious-exam-question-23228848</link>
            <guid isPermaLink="false">hacker-news-small-sites-25713861</guid>
            <pubDate>Sun, 10 Jan 2021 12:03:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Using flamegraphs to read big HN threads]]>
            </title>
            <description>
<![CDATA[
Score 175 | Comments 33 (<a href="https://news.ycombinator.com/item?id=25713858">thread link</a>) | @trungdq88
<br/>
January 10, 2021 | https://trungdq88.github.io/hn-big-threads/index.html | <a href="https://web.archive.org/web/*/https://trungdq88.github.io/hn-big-threads/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      
      <p>
        Loading... https://news.ycombinator.com/item?id=25706993
      </p>
    </div></div>]]>
            </description>
            <link>https://trungdq88.github.io/hn-big-threads/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25713858</guid>
            <pubDate>Sun, 10 Jan 2021 12:03:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Polar Vortex collapse sequence has begun]]>
            </title>
            <description>
<![CDATA[
Score 255 | Comments 184 (<a href="https://news.ycombinator.com/item?id=25713704">thread link</a>) | @makepanic
<br/>
January 10, 2021 | https://www.severe-weather.eu/global-weather/polar-vortex-collapse-winter-weather-europe-united-states-2021-fa/ | <a href="https://web.archive.org/web/*/https://www.severe-weather.eu/global-weather/polar-vortex-collapse-winter-weather-europe-united-states-2021-fa/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p><strong>A Polar Vortex collapse sequence has begun in late December 2020, with a major Sudden Stratospheric Warming event on January 5th, 2021. We will look at the sequence of these events, and how they can change the weather in Europe and the United States in the coming weeks.</strong></p>
<p>The main “player” in these weather events, is of course the <strong>Polar Vortex</strong>. It connects the bottom of the atmosphere (our weather) with the stratosphere above it. A strong exchange of energy between these two layers can heavily disrupt the weather development across the Northern Hemisphere.</p>
<h5><span><strong>WHAT IS THE POLAR VORTEX?</strong></span></h5>
<p>Since knowledge is the key, we will do a quick recap of what exactly is the Polar Vortex.</p>
<p>All of the clouds (and the weather that we feel) are found in the lowest part of the atmosphere called the <span><strong>troposphere</strong></span>. It reaches up to around 8 km (5 miles) altitude over the polar regions and up to around 14-16 km (9-10 miles) over the tropics.</p>
<p>Above it, there is a much deeper layer called the <span><strong>stratosphere</strong></span>. This layer is around 30 km thick and is very dry. We can see the layers of the atmosphere on the image below, with the <span>stratosphere</span> in green hues, and the <span>troposphere</span> in blue at the bottom.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-stratosphere-weather-warming-atmospheric-layers.jpg" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-stratosphere-weather-warming-atmospheric-layers.jpg" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-stratosphere-weather-warming-atmospheric-layers.jpg" data-image-id="21664" data-title="polar-vortex-stratosphere-weather-warming-atmospheric-layers" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-stratosphere-weather-warming-atmospheric-layers.jpg-nggid0521664-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.jpg" alt="polar-vortex-stratosphere-weather-warming-atmospheric-layers" title="polar-vortex-stratosphere-weather-warming-atmospheric-layers" width="475" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20475%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-stratosphere-weather-warming-atmospheric-layers.jpg-nggid0521664-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.jpg">
</a>
</p>
<p>Every year as we head into autumn, the north pole starts to cool. But the atmosphere further south is still relatively warm as it is still receiving energy from the Sun. The north pole receives very little sunlight and thermal energy, cooling at a faster rate.</p>
<p>The reduction in temperature also means a gradual pressure drop over the north pole. In the stratosphere, the process is the same. As the temperature drops over the pole and the temperature difference towards the south increases, a low-pressure area starts to develop across the stratosphere.</p>
<p>The image below shows a typical example of the Polar Vortex at around 30km altitude (10mb level) in the middle stratosphere.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-over-north-pole-winter.jpg" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-over-north-pole-winter.jpg" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-over-north-pole-winter.jpg" data-image-id="21663" data-title="polar-vortex-over-north-pole-winter" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-over-north-pole-winter.jpg-nggid0521663-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.jpg" alt="polar-vortex-over-north-pole-winter" title="polar-vortex-over-north-pole-winter" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-over-north-pole-winter.jpg-nggid0521663-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.jpg">
</a>
</p>
<p>It is almost like a very large cyclone, covering the whole north pole, down to the mid-latitudes. The polar vortex is present at all levels, almost from the ground up. The image below shows the polar vortex at different altitudes. The closer to the ground we get, the more deformed it gets, due to the complex terrain and the many weather fronts and systems.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-winter-weather-warming-north-hemisphere.png" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-winter-weather-warming-north-hemisphere.png" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-winter-weather-warming-north-hemisphere.png" data-image-id="21666" data-title="polar-vortex-winter-weather-warming-north-hemisphere" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-winter-weather-warming-north-hemisphere.png-nggid0521666-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png" alt="polar-vortex-winter-weather-warming-north-hemisphere" title="polar-vortex-winter-weather-warming-north-hemisphere" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-winter-weather-warming-north-hemisphere.png-nggid0521666-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png">
</a>
</p>
<p>We produced a high-resolution video for you below, which very nicely shows the Polar Vortex spinning over the Northern Hemisphere. It covers the period from December 2020 to January 2021, made from NASA GEOS-5 data.</p>
<p>Video shows the 10mb level (30km altitude) potential vorticity parameter, which overly simplified means, that it shows the energy of the polar vortex. Be aware of how the energy is being taken away from the polar vortex by the invisible polar Anticyclones (having a different kind of power), spinning in the opposite direction.</p>


<h5><span><strong>SUDDEN STRATOSPHERIC WARMING</strong></span></h5>
<p>&nbsp;<br>
As a general reference, we usually look at the polar vortex in the stratosphere at the 10mb level. That is around 28-32km (17-20 miles) altitude. This is considered to be around the middle stratosphere, and thus a good representation of the general dynamics of the polar vortex.</p>
<p>The strength of the polar vortex is most often measured by the power of the winds inside it. Usually, this is done is by measuring the zonal (west to east) wind speeds around the polar circle (60°N latitude).</p>
<p>Below we have an analysis from the NASA monitoring system, where we can see a very interesting progression. In early December, the polar vortex was at a quite strong level, reaching 40m/s zonal wind speeds. Problems began towards the mid-month, and especially towards late December when the stratospheric warming began. All graphics below are at the 10mb level (~30km altitude).</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-zonal-wind-forecast.jpg" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-zonal-wind-forecast.jpg" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-zonal-wind-forecast.jpg" data-image-id="21665" data-title="polar-vortex-splitting-weather-winter-united-states-europe-zonal-wind-forecast" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-zonal-wind-forecast.jpg-nggid0521665-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.jpg" alt="polar-vortex-splitting-weather-winter-united-states-europe-zonal-wind-forecast" title="polar-vortex-splitting-weather-winter-united-states-europe-zonal-wind-forecast" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-zonal-wind-forecast.jpg-nggid0521665-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.jpg">
</a>
</p>
<p>Below is an image from the video we showed you earlier in the article, and it shows a quite healthy polar vortex in early December. It has a nice shape and a healthy spinning core.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-early-december.png" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-early-december.png" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-early-december.png" data-image-id="21669" data-title="polar-vortex-splitting-weather-winter-united-states-europe-early-december" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-early-december.png-nggid0521669-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png" alt="polar-vortex-splitting-weather-winter-united-states-europe-early-december" title="polar-vortex-splitting-weather-winter-united-states-europe-early-december" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-early-december.png-nggid0521669-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png">
</a>
</p>
<p>Towards mid-December, the pressure from the North Pacific was rising, with an Anticyclonic presence there gaining strength. The anticyclonic circulation was slowly getting stronger, starting to drain some energy away from the polar vortex, and changing its shape.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-mid-december.png" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-mid-december.png" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-mid-december.png" data-image-id="21672" data-title="polar-vortex-splitting-weather-winter-united-states-europe-mid-december" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-mid-december.png-nggid0521672-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png" alt="polar-vortex-splitting-weather-winter-united-states-europe-mid-december" title="polar-vortex-splitting-weather-winter-united-states-europe-mid-december" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-mid-december.png-nggid0521672-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png">
</a>
</p>
<p>By late December, the Pacific/East Asian Anticyclone became quite a force, now draining a lot of energy from the Polar Vortex and actually becoming visible.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-late-december.png" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-late-december.png" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-late-december.png" data-image-id="21670" data-title="polar-vortex-splitting-weather-winter-united-states-europe-late-december" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-late-december.png-nggid0521670-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png" alt="polar-vortex-splitting-weather-winter-united-states-europe-late-december" title="polar-vortex-splitting-weather-winter-united-states-europe-late-december" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-late-december.png-nggid0521670-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png">
</a>
</p>
<p>At a similar time in late December, a warming sequence began, from Europe over into central Asia. It was starting to engulf the outside layers of the polar vortex. The cold-core of the polar vortex is still rather intact at this point, holding temperatures colder than -80°C in the center over Greenland.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-warming-start.png" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-warming-start.png" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-warming-start.png" data-image-id="21673" data-title="polar-vortex-splitting-weather-winter-united-states-europe-warming-start" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-warming-start.png-nggid0521673-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png" alt="polar-vortex-splitting-weather-winter-united-states-europe-warming-start" title="polar-vortex-splitting-weather-winter-united-states-europe-warming-start" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-warming-start.png-nggid0521673-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png">
</a>
</p>
<p>Just two days later, the warming wave reached a local peak over Siberia, with maximum temperatures in the wave reaching up to +5°C or higher.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-peak-stratospheric-warming.png" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-peak-stratospheric-warming.png" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-peak-stratospheric-warming.png" data-image-id="21674" data-title="polar-vortex-splitting-weather-winter-united-states-europe-peak-stratospheric-warming" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-peak-stratospheric-warming.png-nggid0521674-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png" alt="polar-vortex-splitting-weather-winter-united-states-europe-peak-stratospheric-warming" title="polar-vortex-splitting-weather-winter-united-states-europe-peak-stratospheric-warming" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-peak-stratospheric-warming.png-nggid0521674-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png">
</a>
</p>
<p>On January 5th, the preliminary date of the Sudden Stratospheric Warming event was marked, as the winds around the polar circle have reversed. We can see how massive and strong the Anticyclone has now become, pushing strongly against the polar vortex (bright white). Together with the warming wave, the strong Anticyclonic system has deformed the once circular polar vortex into a banana-shaped feature.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-january-2021.png" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-january-2021.png" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-january-2021.png" data-image-id="21668" data-title="polar-vortex-splitting-weather-winter-united-states-europe-january-2021" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-january-2021.png-nggid0521668-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png" alt="polar-vortex-splitting-weather-winter-united-states-europe-january-2021" title="polar-vortex-splitting-weather-winter-united-states-europe-january-2021" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-january-2021.png-nggid0521668-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png">
</a>
</p>
<p>The warming wave has crawled over the entire North Pole in the stratosphere, effectively splitting the cold-core of the polar vortex into two parts. One over North America and one over the European sector. At this point, this does not have much to do directly with the weather on the surface, as it is at 30km altitude, but we will get to weather effects soon.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-major-warming-event.png" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-major-warming-event.png" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-major-warming-event.png" data-image-id="21675" data-title="polar-vortex-splitting-weather-winter-united-states-europe-major-warming-event" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-major-warming-event.png-nggid0521675-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png" alt="polar-vortex-splitting-weather-winter-united-states-europe-major-warming-event" title="polar-vortex-splitting-weather-winter-united-states-europe-major-warming-event" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-major-warming-event.png-nggid0521675-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png">
</a>
</p>
<p>Looking quickly at the forecast, we can see that the stratospheric Anticyclone will hold stable and move further over the North Pole. It will continue pushing against the very broken polar vortex.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-january-2021-forecast.png" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-january-2021-forecast.png" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-january-2021-forecast.png" data-image-id="21671" data-title="polar-vortex-splitting-weather-winter-united-states-europe-january-2021-forecast" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-january-2021-forecast.png-nggid0521671-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png" alt="polar-vortex-splitting-weather-winter-united-states-europe-january-2021-forecast" title="polar-vortex-splitting-weather-winter-united-states-europe-january-2021-forecast" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-january-2021-forecast.png-nggid0521671-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png">
</a>
</p>
<p>At this point, a new warming wave is also forecasted to start, which should temporarily prevent any quick reorganization and strengthening of the stratospheric circulation.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-stratosphere-forecast.png" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-stratosphere-forecast.png" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-stratosphere-forecast.png" data-image-id="21676" data-title="polar-vortex-splitting-weather-winter-united-states-europe-stratosphere-forecast" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-stratosphere-forecast.png-nggid0521676-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png" alt="polar-vortex-splitting-weather-winter-united-states-europe-stratosphere-forecast" title="polar-vortex-splitting-weather-winter-united-states-europe-stratosphere-forecast" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-stratosphere-forecast.png-nggid0521676-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png">
</a>
</p>
<p>Taking a look at the <a href="https://ozonewatch.gsfc.nasa.gov/meteorology/NH.html">NASA</a> temperature analysis for the stratosphere, we can see the large temperature spike at the 10mb (30km) level, which is in the middle stratosphere. The second image shows the temperature analysis in the lower stratosphere at 50mb (20km) level, also having a very clear temperature spike.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-middle-stratosphere-temperature-analysis.jpg" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-middle-stratosphere-temperature-analysis.jpg" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-middle-stratosphere-temperature-analysis.jpg" data-image-id="21679" data-title="polar-vortex-splitting-weather-winter-united-states-europe-middle-stratosphere-temperature-analysis" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-middle-stratosphere-temperature-analysis.jpg-nggid0521679-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.jpg" alt="polar-vortex-splitting-weather-winter-united-states-europe-middle-stratosphere-temperature-analysis" title="polar-vortex-splitting-weather-winter-united-states-europe-middle-stratosphere-temperature-analysis" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-middle-stratosphere-temperature-analysis.jpg-nggid0521679-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.jpg">
</a>
</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-lower-stratosphere-temperature-analysis.jpg" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-lower-stratosphere-temperature-analysis.jpg" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-lower-stratosphere-temperature-analysis.jpg" data-image-id="21678" data-title="polar-vortex-splitting-weather-winter-united-states-europe-lower-stratosphere-temperature-analysis" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-lower-stratosphere-temperature-analysis.jpg-nggid0521678-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.jpg" alt="polar-vortex-splitting-weather-winter-united-states-europe-lower-stratosphere-temperature-analysis" title="polar-vortex-splitting-weather-winter-united-states-europe-lower-stratosphere-temperature-analysis" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-lower-stratosphere-temperature-analysis.jpg-nggid0521678-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.jpg">
</a>
</p>
<p>Looking even lower, we have the 150mb level, which is kinda the boundary or the “buffer zone” between the stratosphere and the troposphere. Here we can also see the temperature spike, meaning the warming event was fairly robust and fast.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-buffer-zone-temperature-analysis.jpg" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-buffer-zone-temperature-analysis.jpg" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-buffer-zone-temperature-analysis.jpg" data-image-id="21677" data-title="polar-vortex-splitting-weather-winter-united-states-europe-buffer-zone-temperature-analysis" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-buffer-zone-temperature-analysis.jpg-nggid0521677-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.jpg" alt="polar-vortex-splitting-weather-winter-united-states-europe-buffer-zone-temperature-analysis" title="polar-vortex-splitting-weather-winter-united-states-europe-buffer-zone-temperature-analysis" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-buffer-zone-temperature-analysis.jpg-nggid0521677-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.jpg">
</a>
</p>
<h5><span><strong>HISTORICAL WEATHER, FOR THE FUTURE</strong></span></h5>
<p>&nbsp;<br>
Before looking closer at the weather forecasts and their relation to the polar vortex, we need to look at some historic examples of similar events. History can sometimes be the best teacher for the future, and in science, this can be true more often than not.</p>
<p>Below we have 2 images, which both are quite simple to read and understand. They show time from left to right, and altitude from bottom to top. Colors show the temperature anomaly, with red being warmer than normal and blue was colder than normal. We can nicely track the progress of stratospheric warming events over time and altitude.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-january-2004-ssw-event.png" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-january-2004-ssw-event.png" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-january-2004-ssw-event.png" data-image-id="21683" data-title="polar-vortex-splitting-weather-winter-united-states-europe-january-2004-ssw-event" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-january-2004-ssw-event.png-nggid0521683-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png" alt="polar-vortex-splitting-weather-winter-united-states-europe-january-2004-ssw-event" title="polar-vortex-splitting-weather-winter-united-states-europe-january-2004-ssw-event" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-january-2004-ssw-event.png-nggid0521683-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png">
</a>
</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-january-2013-ssw-event.png" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-january-2013-ssw-event.png" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-january-2013-ssw-event.png" data-image-id="21681" data-title="polar-vortex-splitting-weather-winter-united-states-europe-january-2013-ssw-event" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-january-2013-ssw-event.png-nggid0521681-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png" alt="polar-vortex-splitting-weather-winter-united-states-europe-january-2013-ssw-event" title="polar-vortex-splitting-weather-winter-united-states-europe-january-2013-ssw-event" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-january-2013-ssw-event.png-nggid0521681-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png">
</a>
</p>
<p>We can see the progression of warming from the top of the stratosphere, going downwards over time. The first image shows the 2004 stratospheric warming event and the second graphic shows the 2013 event.</p>
<p>The important takeaway is that the warming progresses quite fast down into the troposphere and can start to quickly affect our weather. But it usually stops around 100mb or 150mb level (12-15km). That is normal, due to the fact that we can see a lot of strong weather systems in our troposphere, which can in some cases deflect/reverse any incoming effects from the stratosphere.</p>
<p>Below we have similar two images, but with pressure anomalies instead of temperature. But here you can actually see the connecting points between high pressure coming downwards and the surface layer in the polar region. In both cases, the final effect was seen as individual connections to the bottom levels over time, interfering with the weather development. This indicates that the influence from the Polar Vortex collapse events is periodically interfering with the weather development on a sub-seasonal scale, kinda like waving or resonating.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-2004-atmospheric-pressure-evolution.png" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-2004-atmospheric-pressure-evolution.png" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-2004-atmospheric-pressure-evolution.png" data-image-id="21680" data-title="polar-vortex-splitting-weather-winter-united-states-europe-2004-atmospheric-pressure-evolution" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-2004-atmospheric-pressure-evolution.png-nggid0521680-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png" alt="polar-vortex-splitting-weather-winter-united-states-europe-2004-atmospheric-pressure-evolution" title="polar-vortex-splitting-weather-winter-united-states-europe-2004-atmospheric-pressure-evolution" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-2004-atmospheric-pressure-evolution.png-nggid0521680-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png">
</a>
</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-2013-atmospheric-pressure-evolution.png" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-2013-atmospheric-pressure-evolution.png" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-2013-atmospheric-pressure-evolution.png" data-image-id="21682" data-title="polar-vortex-splitting-weather-winter-united-states-europe-2013-atmospheric-pressure-evolution" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-2013-atmospheric-pressure-evolution.png-nggid0521682-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png" alt="polar-vortex-splitting-weather-winter-united-states-europe-2013-atmospheric-pressure-evolution" title="polar-vortex-splitting-weather-winter-united-states-europe-2013-atmospheric-pressure-evolution" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-2013-atmospheric-pressure-evolution.png-nggid0521682-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.png">
</a>
</p>
<p>We don’t yet have the same graphic for 2021, for obvious reasons, since we still need more data to be gathered. But we can look at the forecast data in a similar image below.</p>
<p>What we are seeing is a very similar pattern as in 2004 and 2013. The negative values in this image represent higher pressure. So we can see the descending high-pressure, making individual contacts with the lower layers. This means that the influence is not a constant every time, but rather periodic.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-pressure-anomaly-over-time.PNG" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-splitting-weather-winter-united-states-europe-pressure-anomaly-over-time.PNG" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-splitting-weather-winter-united-states-europe-pressure-anomaly-over-time.PNG" data-image-id="21689" data-title="polar-vortex-splitting-weather-winter-united-states-europe-pressure-anomaly-over-time" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-pressure-anomaly-over-time.PNG-nggid0521689-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.PNG" alt="polar-vortex-splitting-weather-winter-united-states-europe-pressure-anomaly-over-time" title="polar-vortex-splitting-weather-winter-united-states-europe-pressure-anomaly-over-time" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E" data-lazy-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-splitting-weather-winter-united-states-europe-pressure-anomaly-over-time.PNG-nggid0521689-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.PNG">
</a>
</p>
<p>We also decided to look at the weather patterns prior to the 2004 and 2013 events. And we can see in the images below. We can see on the first image for 2004, that the pattern was already positioned for colder weather over Europe, thanks to the strong high-pressure system in the North Atlantic. The United States was generally milder, with southerly flow dominating much of the CONUS.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2004-ssw-event.gif" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2004-ssw-event.gif" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2004-ssw-event.gif" data-image-id="21688" data-title="polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2004-ssw-event" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2004-ssw-event.gif-nggid0521688-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.gif" alt="polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2004-ssw-event" title="polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2004-ssw-event" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E">
</a>
</p>
<p>In 2013, the picture was kinda the opposite. The pattern over the United States was generally colder than in 2004, while Europe was mild to warm even at this point.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2013-ssw-event.gif" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2013-ssw-event.gif" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2013-ssw-event.gif" data-image-id="21686" data-title="polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2013-ssw-event" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2013-ssw-event.gif-nggid0521686-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.gif" alt="polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2013-ssw-event" title="polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2013-ssw-event" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E">
</a>
</p>
<p>This winter, the pattern is kinda a combination of both. We have another cooler/colder episode over Europe like in 2004, thanks to the high-pressure systems in the North Atlantic. At the same time, we also heed a few decent cold episodes over the United States as well.</p>
<p><a href="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2021-ssw-event.gif" title="" data-src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2021-ssw-event.gif" data-thumbnail="https://www.severe-weather.eu/wp-content/gallery/andrej-news/thumbs/thumbs_polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2021-ssw-event.gif" data-image-id="21692" data-title="polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2021-ssw-event" data-description="" target="_self">
<img src="https://www.severe-weather.eu/wp-content/gallery/andrej-news/cache/polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2021-ssw-event.gif-nggid0521692-ngg0dyn-700x700x100-00f0w010c010r110f110r010t010.gif" alt="polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2021-ssw-event" title="polar-vortex-weather-winter-united-states-europe-pressure-pattern-before-2021-ssw-event" width="700" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%200'%3E%3C/svg%3E">
</a>
</p>
<p>But after the Polar Vortex breakdown and the stratospheric warming event, the pressure patterns changed quite importantly in 2004 and also in 2013. The image below is the pressure pattern one month after the Polar Vortex collapse in 2004. The strong high pressure in the North Atlantic has been replaced by a strong deep low-pressure system, and the high-pressure has moved into the Arctic …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.severe-weather.eu/global-weather/polar-vortex-collapse-winter-weather-europe-united-states-2021-fa/">https://www.severe-weather.eu/global-weather/polar-vortex-collapse-winter-weather-europe-united-states-2021-fa/</a></em></p>]]>
            </description>
            <link>https://www.severe-weather.eu/global-weather/polar-vortex-collapse-winter-weather-europe-united-states-2021-fa/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25713704</guid>
            <pubDate>Sun, 10 Jan 2021 11:40:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How Prosody developers spent 2020]]>
            </title>
            <description>
<![CDATA[
Score 69 | Comments 40 (<a href="https://news.ycombinator.com/item?id=25713679">thread link</a>) | @upofadown
<br/>
January 10, 2021 | https://blog.prosody.im/2020-retrospective/ | <a href="https://web.archive.org/web/*/https://blog.prosody.im/2020-retrospective/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          
  


  <div>

    

    
    <div>
      <p><small>
        <p><time datetime="2021-01-08T00:00:00Z">
          2021-01-08
        </time> by The Prosody Team
         
        <br>
        
        </p>
        

        
        
        <br>

      </small></p>
    </div>
    <p>Nobody here knew quite what a year 2020 was going to be! However despite
pandemics and lockdowns, we have continued to work on Prosody. This post
is a summary of how the project is doing, and what we’ve been up to in
the past year.</p>

<p>One quick note before we begin… Prosody is an independent open-source
project and exists only because the developers have been fortunate enough
to be in a position to work on it. A couple of core team members are
currently looking for freelance work. If you have projects in need of a
Prosody expert, check the bottom of this post for more details!</p>

<h2 id="more-users-than-ever-before">More users than ever before</h2>

<p>Prosody does not “phone home” in any way, which means we do not have a
lot of insight into how many people are using Prosody. But there are
some indicators that we can use to see at least the growth of the project.</p>

<p>Many years ago, circa 2014, I was filling out a form that asked how
many users the project had. I thought long and hard, but with no idea how
to measure, I wrote down an estimate of “500” based on nothing but a gut
feeling. Only a few weeks later I learned that the <a href="https://xmpp.net/">XMPP Observatory</a>
had already seen <strong>over 2200 domains</strong> submitted that were running Prosody.
As most deployments were unlikely to have been submitted to xmpp.net, my
estimate was clearly far out. These days I jump at any chance for even a
vague estimate of our userbase. It helps us to know that people are out
there!</p>

<p>One useful tool is <a href="https://shodan.io/">Shodan</a>. This project scans the
entire internet, just to see what it can find, and it records the results.
Often used by academics and security researchers, a free account can also
be used by anyone to run simple queries over the data they collect.</p>

<p>A Shodan search in 2017 turned up nearly 7000 Prosody deployments. The
same search 8 months later returned over 16000. Today we’re at <strong>over 52000
Prosody servers!</strong> And this only counts instances using port 5269 and
accessible to the internet. There are also many private/internal deployments
of Prosody that are not included in these numbers. Unfortunately we
didn’t run a report in 2019, but here’s a graph of the previous
reports we have run:</p>

<p><img src="https://blog.prosody.im/2020-retrospective/prosody-shodan-2020.svg" alt="Graph of Prosody server counts in previous paragraph"></p>

<p>Shodan reports over 85000 federating XMPP servers on port 5269 in total.
Based on this, Prosody makes up 44% of the public XMPP network. That’s
quite an achievement!</p>

<p>Another handy insight into one sector of Prosody deployments is via Debian’s
<a href="https://popcon.debian.org/">“popularity contest”</a> service. This is an
automated survey that administrators of Debian servers can choose to opt
into. It reports anonymously to Debian what software packages are installed
and in use. Although it reflects only a small slice of even Debian
installations, it is useful to see trends.</p>

<p>March 2020 marked an unprecedented spike in Prosody installations!</p>

<p><img src="https://blog.prosody.im/2020-retrospective/prosody-popcon-20201231.png" alt="Graph of Debian popularity contest data for prosody"></p>

<p>Although we don’t know for certain, we suspect this was caused by an surge
of interest in the self-hosted video conferencing software, <a href="https://jitsi.org/jitsi-meet/">Jitsi Meet</a>. Jitsi Meet integrates with Prosody, which is used to power the
authentication, signaling and chat of the video conferences. Jitsi’s <a href="https://jitsi.org/jitsi-videobridge/">Videobridge</a> component handles the media routing. Together they make
a very powerful and flexible communication system, and it is hardly surprising
that interest has spiked this year when there has been a massive shift to
remote work, and online meetings have replaced physical ones.</p>

<h2 id="the-code-counts">The code counts</h2>

<p>Now let’s look at some stats about the Prosody codebase itself.</p>

<table>
<thead>
<tr>
<th>Language</th>
<th>Files</th>
<th>Code</th>
<th>Comment</th>
<th>Comment %</th>
<th>Blank</th>
<th>Total</th>
</tr>
</thead>

<tbody>
<tr>
<td>Lua</td>
<td>295</td>
<td>48358</td>
<td>3536</td>
<td>6.8%</td>
<td>6483</td>
<td>58377</td>
</tr>

<tr>
<td>C</td>
<td>13</td>
<td>2613</td>
<td>346</td>
<td>11.7%</td>
<td>643</td>
<td>3602</td>
</tr>

<tr>
<td>Other (build tools, etc.)</td>
<td>11</td>
<td>1551</td>
<td>22</td>
<td>6.3%</td>
<td>138</td>
<td>1711</td>
</tr>

<tr>
<td>————————-</td>
<td>——</td>
<td>———-</td>
<td>———-</td>
<td>———-</td>
<td>———-</td>
<td>———</td>
</tr>

<tr>
<td>Total</td>
<td>323</td>
<td>54760</td>
<td>3909</td>
<td>6.7%</td>
<td>7265</td>
<td>65934</td>
</tr>
</tbody>
</table>

<h3 id="changes-in-2020">Changes in 2020</h3>

<p>In 2020 (looking at the development branch) we added 597 commits, changing
191 files. The changes added 9872 lines of code, and 3637 lines of code
were removed.</p>

<p>A significant portion of the new lines were in our unit and integration
tests (2200 lines, about 22%) which we have been working hard to expand
over the past couple of years.</p>



<p>If you use Prosody, you know we have an emphasis on modularity and
extensibility. We like to make <a href="https://prosody.im/doc/developers/modules">developing plugins for Prosody</a> as easy as possible, whether it’s for integration with other
systems or crazy experiments.</p>

<p>Here’s a random selection of the 363 modules currently in the repository:</p>

<dl>
<dt><a href="https://modules.prosody.im/mod_muc_eventsource">mod_muc_eventsource</a></dt>
<dd>Receive messages from MUC rooms with 4 lines of Javascript</dd>
<dt><a href="https://modules.prosody.im/mod_log_ringbuffer.html">mod_log_ringbuffer</a></dt>
<dd>Send debug logs to RAM unless they are needed.</dd>
<dt><a href="https://modules.prosody.im/mod_component_client">mod_component_client</a></dt>
<dd>Allow a Prosody server to run as a (XEP-0114) external component of
another (Prosody or not) XMPP server.</dd>
<dt><a href="https://modules.prosody.im/mod_firewall">mod_firewall</a></dt>
<dd>Powerful rules-based scripts for filtering and redirecting XMPP traffic.</dd>
<dt><a href="https://modules.prosody.im/mod_minimix">mod_minimix</a></dt>
<dd>An <strong>experiment</strong> in making MUC joins persistent (like MIX).</dd>
</dl>

<p>During 2020 we saw 37 new modules published in the <a href="https://modules.prosody.im/">community repository</a>,
and 499 commits from 19 contributors. Together they added over 10,000
lines of code (and removed 728 lines). This makes it our most active
year apart from 2018!</p>

<p>Outside of the Prosody dev team, the most active contributor was
<a href="https://wiki.xmpp.org/web/Severino_Ferrer_de_la_Pe%C3%B1ita_Application_2020">Seve</a>, who also made their first contribution this year and added a total of four new modules.
Welcome aboard!</p>

<h2 id="features">Features</h2>

<p>But the most important part… what features have we been working on? All
these things are scheduled for the 0.12 release (more on that in a bit).</p>

<h3 id="plugin-installer">Plugin installer</h3>

<p>We have been applying further polish to, and setting up the infrastructure
for the plugin installer. This was a Google Summer of Code project by
<a href="https://gsoc-prosody-2019.blogspot.com/">João Duarte</a>. It utilizes the
Lua package manager, LuaRocks, to download, install and manage community
modules.</p>

<p>Although the the installer was completed in 2019, to make it generally
usable we also had to ensure every module in the <a href="https://modules.prosody.im/">community repository</a>
could be packaged and served in an automated way by our server. We now
have this working.</p>

<h3 id="bye-bye-telnet-hello-prosodyctl-shell">Bye bye telnet, hello prosodyctl shell!</h3>

<p>The telnet console is one of the best things about Prosody, and we’ve
been working on its successor. An early version of <code>prosodyctl shell</code> is
already available to try out in trunk nightly builds.</p>

<p>Using prosodyctl allows us to more easily support advanced features such
as line editing and history (previously attainable using a third-party
utility, rlwrap). It also allows for some richer UIs and is more secure
on shared servers (it uses a unix socket instead of TCP).</p>

<h3 id="dns-improvements">DNS improvements</h3>

<p>Since Prosody needs to resolve special DNS record types (such as SRV
records) and in an asynchronous manner, the built-in operating system APIs
are generally inadequate.</p>

<p>For a long time we’ve been using an adopted library simply known as
‘dns.lua’ combined with our own asynchronous wrapper around it. Although
it hasn’t been terrible, it has a few issues, especially in some uncommon
environments. It also doesn’t support many advanced features such as DNSSEC.</p>

<p>Now we are migrating to libunbound, part of the <a href="https://github.com/coredns/unbound">unbound</a>
project. This is one of the leading DNS implementations, and will be a
big improvement over our current DNS library. To try it out, you can
simply install <a href="https://www.zash.se/luaunbound.html">luaunbound</a> (already
available in luarocks, Debian testing, AUR and others - poke your distro
maintainers if you don’t have it yet!).</p>

<h3 id="http-server-upload-performance">HTTP server upload performance</h3>

<p>We didn’t set out to write a HTTP server, but we ended up with one anyway!
Originally added so that we could natively support BOSH (XMPP over HTTP)
clients, it grew to support websockets, and various modules now provide
HTTP APIs for integration between Prosody and other systems.</p>

<p>One big problem is that the original implementation was designed for only
small amounts of data. Since the widespread of adoption of
<a href="https://xmpp.org/extensions/xep-0363.html">XEP-0363</a> people now want to
be able to upload files, pictures and videos using Prosody’s internal
HTTP server. We have limits in place to protect against denial of service
attacks, but those same limits prevent large uploads from trusted users.</p>

<p>We’ve put some work into supporting “streaming uploads”, where incoming
data can be saved directly to disk instead of RAM. This means it will be
safe to increase file upload limits without opening up your server to
increase RAM usage and denial of service attacks.</p>

<p>In general though, we do recommend using a real external HTTP server in
a production or high traffic deployment (using <a href="https://modules.prosody.im/mod_http_upload_external">mod_http_upload_external</a>).</p>

<h3 id="beyond-passwords">Beyond passwords</h3>

<p>Passwords are the fundamental means of authenticating to your server in
XMPP today. XMPP is quite good at this, adopting strong standard authentication
mechanisms such as SCRAM far earlier than the rest of the industry. But
the rest of the industry is also moving away from passwords in many places.
We’re aiming to follow this movement also. Not that we are scrapping passwords
entirely, but making it easier to offer alternatives.</p>

<p>Prosody actually has a number of non-password authentication modules already,
such as <a href="https://modules.prosody.im/mod_auth_oauthbearer">mod_auth_oauthbearer</a> (OAuth2 tokens),
 <a href="https://modules.prosody.im/mod_auth_ccert">mod_auth_ccert</a> (client certificates) and
 <a href="https://modules.prosody.im/mod_auth_token">mod_auth_token</a> (HMAC-based
 tokens).
But most of the modules have limitations and are not well integrated
(e.g. you can set up Prosody to accept passwords, or set it up to accept
tokens, but you can’t offer both methods at the same time).</p>

<p>An important related aspect is authorization. In most systems authentication
via a token also provides <em>limited access to the account</em> (e.g. if a password
is associated with an account, a session that logged in using a token
should not be allowed to reset the password).</p>

<p>We’ve been working on two things. Firstly, a built-in authorization system
(more flexible than the current <code>admins</code> configuration option) where users
and sessions can be associated with specific permissions and roles.</p>

<p>Secondly we’re using this authorization layer to add built-in support
for OAuth2-style authentication and authorization.</p>

<p>This is exciting for a number of reasons. It will allow, for example,
specialized clients to request and receive (when granted by the user)
limited …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.prosody.im/2020-retrospective/">https://blog.prosody.im/2020-retrospective/</a></em></p>]]>
            </description>
            <link>https://blog.prosody.im/2020-retrospective/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25713679</guid>
            <pubDate>Sun, 10 Jan 2021 11:37:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Campaigns: A long-running effort to enact global change safely]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25713656">thread link</a>) | @kiyanwang
<br/>
January 10, 2021 | https://kellysutton.com/2021/01/06/campaigns.html | <a href="https://web.archive.org/web/*/https://kellysutton.com/2021/01/06/campaigns.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <div>

  

  <article>
    <p>Sometimes it can take years to make a single-line code change.</p>

<p>As engineering organizations grow, the problems and their solutions become more intricate. What might have taken an afternoon now takes months of coordinated effort. The system (both the technology and the people) are larger, more complex, and more difficult to change than ever before.</p>

<p>But change is necessary.</p>

<p>We might be looking to pay off some technical debt or tee up an architectural change to unlock a better customer experience, cost savings, etc. What is a tool we can use to coordinate many groups of people, hold groups accountable, and eventually succeed?</p>

<p>Enter what I call the <strong>Campaign</strong>.</p>

<h2 id="elements-of-a-campaign">Elements of a Campaign</h2>

<p>A Campaign is a long-running effort to enact global change safely within a sociotechnical system.</p>

<p><strong>Every campaign needs the following:</strong></p>

<ul>
  <li>A Goal</li>
  <li>Metrics toward that goal</li>
  <li>Buy-in</li>
  <li>Method of Accountability</li>
  <li>A “Window”</li>
  <li>A Target Date</li>
</ul>

<p><strong>Campaigns work well to address:</strong></p>

<ul>
  <li>Technical changes with large social components.</li>
  <li>Technical changes that require everyone to do a little bit of work.</li>
  <li>High-value or inevitable future worlds</li>
</ul>

<p><strong>Campaigns don’t work with:</strong></p>

<ul>
  <li>Efforts that avoid measurement, or where measurement is likely actively harmful. Examples include “We should improve the design of our code.”</li>
  <li>Organizations that cannot buy-in to the effort for one reason or another.</li>
  <li>Low-value Campaigns. No need to coordinate many teams if efforts can be localized.</li>
</ul>

<p>Many campaigns have a trivial technical goal. My favorite example: The request timeout value is likely one line in a configuration file. Safely changing that to a lower value can be months of work.</p>

<p>Let’s step through the individal components in greater detail.</p>

<h2 id="the-goal">The Goal</h2>

<p>Every campaign needs a goal that can be expressed in a single sentence. The goal should be separate from the metric. It may or may not include implementation details. I usually recommend they include some hint of implementation so that the scope of effort can be limited a bit.</p>

<p>Here are a few examples:</p>

<ul>
  <li>“We want to improve the resiliency and throughput of our background processes by making all jobs idempotent.”</li>
  <li>“We want to improve the customer experience and resiliency of our system by limiting the amount of time a web request can take.”</li>
  <li>“We want to improve the health of our database by enforcing a timeout on all queries.”</li>
</ul>

<p>If the goal feels a bit too open, consider adding non-goals as well to keep the effort focused.</p>

<h2 id="focus-on-impact">Focus on Impact</h2>

<p>Before beginning a Campaign, you need to convice a lot of people that this is a valuable effort and that now is the right time to tackle such an effort. A crisp articulation of “Why this Campaign?” and “Why now?” is required.</p>

<p>Write these down, but expect to repeat them in other media often.</p>

<p>Focus on the benefits of achieving the goal. The more we can ground the impact in objective truths (e.g. “our system is more resilient to failure”) and less in personal preference for technology (e.g. “we were using Tech X but now we use Tech Y”), the better.</p>

<h2 id="metrics">Metrics</h2>

<p>Every Campaign needs a metric or two to enable tracking progress and accountability.</p>

<p>Each metric should be able to be assigned to a given team or individual. Once the metrics go green, we should have <em>ipso facto</em> achieved our goal.<sup id="fnref:okrs" role="doc-noteref"><a href="#fn:okrs">1</a></sup></p>

<p>Choosing a metric can be difficult. It should not be succeptible to <a href="https://brownfield.dev/post/2020-02-18-trap-rule-beating/">Rule Beating</a>, where the metric will be green but the intent of the goal will be missed.</p>

<p>Metrics need to be incremental and as fine-grained as necessary. We should see the slow march of progress, rather than a sudden “Okay, we’re done!”</p>

<p>There should be many ways to achieve a metric. In the example of global request timeouts, we can hit that timeout by reducing the total number of database queries, speeding up serialization of JSON, caching, loading less data, etc.</p>

<p>A few examples derived from the above examples goals:</p>

<ul>
  <li>Number of background jobs that are not idempotent.</li>
  <li>Endpoints that are over the threshold of request length.</li>
  <li>Database queries that are over the threshold of query length.</li>
</ul>

<p>Metrics should be easy to grok and able to be sliced a few different ways. In our request threshold example, we might want to surface both “number of endpoints remaining” and “total percentage of endpoints.”</p>

<h2 id="buy-in">Buy-in</h2>

<p>A Campaign by definition reaches across the organization and touches many teams. Every team, mission, individual, and sub-org has different priorities and different worries.</p>

<p>For a Campaign to be successful, it needs global participation of those involved. Buy-in will look different in different organizations, but will generally be a mix of <a href="https://en.wikipedia.org/wiki/Carrot_and_stick">Carrot and Stick</a> (choosing to do something vs. being coerced to do something).</p>

<p>Leading with the benefits (i.e. the Carrot) can go a long way here:</p>

<ul>
  <li>“If we are able to make jobs idempotent, the Infrastructure team can take over responsibility of background jobs and reduce costs by 90%.”</li>
  <li>“If we are able to make all requests take less than 2 seconds, the customer experience will improve and we will be able to reduce costs.”</li>
</ul>

<p>The benefits may not be convincing enough or still might not meet the priority threshold for some teams. If many teams do not find the benefits convincing enough, you might not have a Campaign worth pursuing.</p>

<h2 id="a-method-of-holding-teams-accountable">A Method of Holding Teams Accountable</h2>

<p>How much further do we have to go? Where might we need to invest more?</p>

<p>Most Campaigns will have a central place to answer questions like these. Speadsheets here work just fine. Dashboards are even better. They might be automatically populated or populated by hand, usually with the help of a script. Whatever it is, it should quickly answer a few questions:</p>

<ul>
  <li>How far along are we?</li>
  <li>Which teams are doing well? (We should follow up with these teams to see what’s working for them.)</li>
  <li>Which teams are falling behind? (We should follow up with these team to see if they need more help, resources, etc.)</li>
</ul>

<p>This should be passively available, but also circulated on some regular cadence. Weekly, monthly, or quarterly, everyone involved should get an update in their inbox.</p>

<h2 id="a-window">A “Window”</h2>

<p>The Window is a method of prioritizing work to be done while ensuring that progress is permanent. It moves. It is optional but helpful in most Campaigns.</p>

<p>A Window usually defines two threshold: Warning and Not Allowed. In a Campaign to enforce a global request timeout, here’s how the Window might look at the start and evolve:</p>

<ol>
  <li>The Goal is to make no web request take longer than 5 seconds, 99.9% of the time.</li>
  <li>The Window is set to Warn: 30 seconds, Not Allowed: 60 seconds. 60 seconds is the current limit set by our load balancer and the behavior that exists today.</li>
  <li>Teams tackle all requests that fall between Warn and Not Allowed until there are no more in the Window.</li>
  <li>We move the Window down to Warn: 15 seconds, Not Allowed: 30 seconds.</li>
  <li>GOTO Step 3.</li>
</ol>

<figure>
  <img srcset="https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=100 100w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=200 200w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=300 300w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=320 320w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=400 400w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=500 500w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=600 600w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=640 640w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=700 700w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=750 750w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=768 768w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=800 800w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=900 900w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1000 1000w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1024 1024w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1080 1080w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1100 1100w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1152 1152w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1200 1200w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1242 1242w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1300 1300w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1400 1400w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1440 1440w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1442 1442w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1500 1500w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1536 1536w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1600 1600w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1700 1700w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1800 1800w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1880 1880w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1900 1900w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=1920 1920w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2000 2000w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2048 2048w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2100 2100w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2200 2200w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2208 2208w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2280 2280w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2300 2300w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2400 2400w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2415 2415w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2500 2500w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2560 2560w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2600 2600w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2700 2700w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2732 2732w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2800 2800w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=2900 2900w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=3000 3000w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=3100 3100w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=3200 3200w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=3300 3300w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=3400 3400w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=3500 3500w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=3600 3600w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=3700 3700w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=3800 3800w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=3900 3900w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=4000 4000w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=4100 4100w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=4200 4200w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=4300 4300w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=4400 4400w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=4500 4500w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=4600 4600w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=4700 4700w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=4800 4800w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=4900 4900w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=5000 5000w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=5100 5100w, https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=5120 5120w" src="https://kellysutton-blog.imgix.net/images/campaign-windows.png?ixlib=jekyll-4.2.0&amp;w=5120" sizes="(min-width: 700px) 700px, 100vw">
</figure>

<p>Using this approach yields a few benefits:</p>

<ul>
  <li>Organizers of the campaign provide a default prioritization approach. Team do not have to develop their own.</li>
  <li>By setting and enforcing “Not Allowed,” we ensure that progress is permanent. A team cannot introduce a new request that takes longer than a given amount of time.</li>
  <li>Teams are warned in advance of a future change. They might receive a weekly email report, an exception, or a Slack message notifying them of something that is okay today but not okay in the future.</li>
  <li>Incremental value is delivered. If we stop the Campaign for whatever reason, lasting value will remain.</li>
</ul>

<h2 id="an-example-global-request-timeouts">An Example: Global Request Timeouts</h2>

<p>I’ve threaded an example throughout this post, but let’s look at a specific example compiled together.</p>

<p>One that I’ve seen in a few organizations is the need to set and/or reduce the total amount of time a web request can take. This is the type of thing that can be easy to ignore, especially in applications serving industries with high switching costs or a <a href="https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem">Principal-agent problem</a>. (High switching costs or principle-agent problem mean a slow degredation of performance will never bubble up as a priority.) Many Campaigns take the form of “If we knew to have this limit in the first place, this would be a lot easier.”</p>

<p>Let’s put it all together.</p>

<div>

<p><strong>Goal:</strong></p>

<p>We want to improve the customer experience and resiliency of our system by limiting the amount of time a request can take.</p>

<p>We want to eventually get to 2 seconds for the 99th percentile of requests, with a hard cutoff of 5 seconds.</p>

<p>We talk more about this Goal, why it was chosen, and it’s urgency in <em>this longer document</em>. (Link to something convincing.)</p>

<p><strong>Metrics:</strong></p>

<ul>
<li>Number of endpoints not adhering to the Goal.</li>
<li>Number of endpoints within the Window.</li>
</ul>

<p><strong>Window:</strong></p>

<p>At the start, we’ll set the Window to:</p>

<ul>
<li>Warn: 30 seconds.</li>
<li>Not Allowed: 60 seconds (this is the current timeout setting).</li>
</ul>

<p>From there, we’ll look to increment the Warn threshold by 15 seconds, or N/2 each time.</p>

<p><strong>Campaign Manager:</strong></p>

<p>Jane Smith will be in charge of tracking progress and reporting on this Campaign.</p>

<p>Jane and Team X will be available to help teams get their requests to the goal. Team X is also responsible for unowned code in the system.</p>

<p><strong>Target Date:</strong></p>

<p>We’d like to be complete with this effort on January 1, 2021.</p>

</div>

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><a href="https://lethain.com/migrations/">“Migrations: The sole scalable fix to tech debt,” Will Lethain.</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Project_management">Project Management</a></li>
  <li><a href="https://en.wikipedia.org/wiki/OKR">Objectives and Key Results</a></li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>This post is a generalization of an effort I find myself doing more and more at Gusto. Every campaign is a little bit different, and needs to be adapted accordingly.</p>

<p>Hopefully this framework helps you and your teams make complicated technical changes that require large-scale behavioral or process changes.</p>

<hr>

<p>Special thanks to <a href="https://www.linkedin.com/in/tonirib/">Toni Rib</a>, <a href="https://mxstbr.com/">Max Stoiber</a>, <a href="http://www.dannyzlobinsky.com/">Danny Zlobinsky</a>, <a href="https://www.estebanpastorino.com/">Kito Pastorino</a>, and <a href="https://www.shayon.dev/">Shayon Mukherjee</a> for reading early drafts of this post and providing feedback.</p>





  </article>

</div>

      </div>
    </div></div>]]>
            </description>
            <link>https://kellysutton.com/2021/01/06/campaigns.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25713656</guid>
            <pubDate>Sun, 10 Jan 2021 11:34:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Software Is Drowning the World]]>
            </title>
            <description>
<![CDATA[
Score 66 | Comments 60 (<a href="https://news.ycombinator.com/item?id=25713631">thread link</a>) | @kiyanwang
<br/>
January 10, 2021 | https://jamesabley.com/software-is-drowning-the-world/ | <a href="https://web.archive.org/web/*/https://jamesabley.com/software-is-drowning-the-world/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <div>

  

  <article>
    <p>One of the many upsides I’ve had from working at lots of organisations
is that you get to see what’s common. Are things like this everywhere?
Frequently, the answer is yes!</p>

<p>An example of this is tech debt.</p>

<p>I see organisations which are running to stand still, and I’m not
sure they realised they’re doing that.</p>

<p>What do I mean by this?</p>

<p>Every time you decide to solve a problem with code, you are committing
part of your future capacity to maintaining and operating that code.
Software is never done.</p>

<p>Here’s a few examples of demonstrating what I mean:</p>

<h2 id="security">Security</h2>

<ol>
  <li>You write a networked service to solve a business problem. Say
it has an HTML web UI</li>
  <li>It has no known security issues</li>
  <li>Time passes</li>
  <li>You now have security issues with your code, and you should assess
 whether you need to do work to address these.</li>
</ol>

<p>WAT?</p>

<p>Humans are terri-bad at writing secure code. And given enough time,
other humans will discover the security holes in your service.</p>

<p>This applies both to code your organisation writes, and the libraries
they use, or the operating systems, or web servers, or …</p>

<h3 id="security-examples">Security Examples</h3>

<p>Take your pick from browsing a CVE database, or use
<a href="https://snyk.io/">Snyk</a> or similar to look at your current codebases.</p>

<ul>
  <li>TLS / SSL issues:
    <ul>
      <li><a href="https://en.wikipedia.org/wiki/POODLE">POODLE</a></li>
      <li><a href="https://blog.zoller.lu/2011/09/beast-summary-tls-cbc-countermeasures.html">BEAST</a></li>
      <li><a href="https://en.wikipedia.org/wiki/CRIME">CRIME</a></li>
      <li><a href="https://en.wikipedia.org/wiki/BREACH">BREACH</a></li>
      <li><a href="https://en.wikipedia.org/wiki/Heartbleed">Heartbleed</a></li>
    </ul>
  </li>
  <li><a href="https://www.cvedetails.com/vulnerability-list.php?vendor_id=15183&amp;product_id=31286&amp;version_id=&amp;page=1&amp;hasexp=0&amp;opdos=0&amp;opec=0&amp;opov=0&amp;opcsrf=0&amp;opgpriv=0&amp;opsqli=0&amp;opxss=0&amp;opdirt=0&amp;opmemc=0&amp;ophttprs=0&amp;opbyp=0&amp;opfileinc=0&amp;opginf=0&amp;cvssscoremin=6&amp;cvssscoremax=0&amp;year=0&amp;month=0&amp;cweid=0&amp;order=1&amp;trc=20&amp;sha=97513f3fa07a803c5507b2cf550af9877acd90f2">Spring</a></li>
  <li><a href="https://www.cvedetails.com/vulnerability-list.php?vendor_id=45&amp;product_id=6117&amp;version_id=&amp;page=1&amp;hasexp=0&amp;opdos=0&amp;opec=0&amp;opov=0&amp;opcsrf=0&amp;opgpriv=0&amp;opsqli=0&amp;opxss=0&amp;opdirt=0&amp;opmemc=0&amp;ophttprs=0&amp;opbyp=0&amp;opfileinc=0&amp;opginf=0&amp;cvssscoremin=6&amp;cvssscoremax=0&amp;year=0&amp;month=0&amp;cweid=0&amp;order=1&amp;trc=70&amp;sha=5369e34293062ebe460c99e6878e0792ac23944c">Struts</a></li>
</ul>

<h3 id="legislation">Legislation</h3>

<ol>
  <li>You write a networked service to solve a business problem. Say
 it has an HTML web UI</li>
  <li>It has no known legal compliance issues</li>
  <li>Time passes</li>
  <li>You now have legal compliance issues with your code, and you should
 assess whether you need to do work to address these.</li>
</ol>

<p>WAT?</p>

<p>The <a href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation">General Data Protection Regulation</a>
addressed organisations not handling data very well.</p>

<p>Privacy and Electronic Communications Regulations – mostly known
for mandating cookie policy.</p>

<p>The Equality Act 2010 (UK) and the Americans with Disabilities Act
1990 (2010 update) for website accessibility. Yes, there was a time
when people didn’t consider accessibility when building web sites.</p>

<p>Brexit has meant a lot of changes for businesses in the EU and UK.
Software has been rewritten to manage the new trading relationships.
This will continue to happen for a while countries establish new
relationships.</p>

<h3 id="3rd-parties">3rd parties</h3>

<ol>
  <li>You write a service to solve a business problem</li>
  <li>You can build and release it when necessary</li>
  <li>Time passes</li>
  <li>You are now unable to build and release the service</li>
</ol>

<p>WAT?</p>

<p>3rd parties will change their APIs, or how things work. They may
do this for any number of reasons: performance, or security among
them. Older versions become deprecated, and unsupported. And these
older versions will still have new security issues reported against
them. So you need to upgrade, and adapt your code to use the new
API.</p>

<p>People building code libraries will strive to maintain backward
compatibility. But we still get <a href="https://semver.org/">semver</a> major
version changes, and breaking API changes.</p>

<h2 id="implications">Implications</h2>

<p>Most software needs constant maintenance. Building and operating
software has a cost which you should always factor in when deciding
to solve problems in that way.</p>

<p>A team working in a particular way can only be responsible for a
fixed amount of software. The amount of software should be managed,
otherwise the team will grind to a halt.</p>

<h2 id="proposition">Proposition</h2>

<blockquote>
  <p>A team working in a particular way</p>
</blockquote>

<p>What if we change how they work?</p>

<p>Well yes, there are options there.</p>

<p>I’ve got a separate post (currently brewing) about Dunbar’s numbers,
but for this post, different sized organisations might have different
options. At a certain size, it makes sense to have people dedicated
to developer productivity and creating tools which improve the
capacity of other teams.</p>

<p>You can choose higher-level languages, and use technology stacks
from SaaS vendors which need less time from your people.</p>

<p>There is one option I had planned to spend researching last year
(but I ended up getting a job instead). This feels like potentially
a big market. I’ve seen lots of organisations with decade-old
codebases which are still running unsupported versions of dependencies
or frameworks.</p>

<p>As a developer, I’m familiar with a hammer, and was curious if I
could use it.</p>

<p>Can we have tooling that automates keeping software up-to-date?</p>

<p>I see this problem in every organisation I’ve ever worked in, with
all aspects.</p>

<p>Web applications/APIs written in any language. As mentioned above,
there are many reasons that software rots if left unattended. Mobile
apps also have this. Migrating versions of Android, or iOS, or …</p>

<p>Configuration/manifests for Infrastructure as Code aslo suffer from
this. Terraform hasn’t yet released 1.x, but there have been many
changes over the years. If you’re using Cloud Foundry or Kubernetes,
you’ll <a href="https://github.com/doitintl/kube-no-trouble">have experienced
changes</a> which mean
you need to do work.</p>

<p>Automating the changes needed in YAML for upgrading from Kubernetes
<code>n</code> to <code>n+1</code> feels like a widely useful tool.</p>

<h2 id="current-state">Current State</h2>

<p>There are some commercial things which do related work.</p>

<p>Snyk, <a href="https://github.com/renovatebot/renovate">Renovate</a>,
<a href="https://dependabot.com/">Dependabot</a> and other things exist which
can make pull requests to update dependencies. Mpost languages have
a package tool and bumping numbers is pretty straightforward. These
things tend to not be able to manage breaking API changes though.
Bumping a patch or minor dependency upgrade is fine, but a major
one with breaking API changes tends to need a human to get involved.</p>

<p>Why? Could we have a tool that solves this? When a new version of
<a href="https://spring.io/">Spring</a> is released, could it include an
accompanying set of transformations which will allow the entire
ecosystem to safely and rapidly upgrade?</p>

<p>Having a minor interest in compilers (and having worked on a
commercial interpreter), I tend to think of code editing operations
as transformations, rather than characters. There’s been <a href="https://www.facebook.com/notes/kent-beck/prune-a-code-editor-that-is-not-a-text-editor/1012061842160013">some
research in transformation-based
editors</a>,
but I’ve not seen a lot else.</p>

<p>Major version upgrades could potentially be similarly expressed in
terms of transformations, which similarly might be composed. So if
a class has been removed between major versions of a dependency,
the required transformation might be composed of:</p>

<ol>
  <li>Insert new class <code>my.Y</code></li>
  <li>Implement interface <code>spring.new.Z</code></li>
  <li>Adapt method <code>A</code> from old class <code>my.Z</code> onto method <code>B</code> in new
 class <code>my.Y</code></li>
  <li>Adapt parameters from adapted method – a <code>Context</code>
 used to be obtained from <code>ApplicationSingleton</code> but is now passed
 in explicity</li>
  <li>etc</li>
</ol>

<p>And then you would need a serialisation format and publishing
mechanism for these sets of transformations.</p>

<p>The closest I’ve seen to this is where Google actually did that in
the same target langauge. They <a href="https://blog.golang.org/introducing-gofix">published a tool with for the
language Go</a>, named
<a href="https://golang.org/cmd/fix/"><code>fix</code></a>. It automated upgrades of
existing code before 1.x was released, and since then, they’ve had
<a href="https://golang.org/doc/go1compat">the Go 1 compatibility document</a>.</p>

<p>Sadly, <code>fix</code> appears to have been mostly inactive since then?</p>

<p>I’m interested (academically as well as commercially) in producing
a tool which looked something similar, but much more widely applicable.</p>

<p>So having something that can take code/configuration, generate an
Abstract Syntax Tree (AST), and then apply a set of transformations.
Transformations compose. A large one might be <strong>Upgrade Framework
<code>n</code> to <code>n+1</code></strong> involvings lots of smaller transformations. For each
transformation, you’d need to query the AST for usages of the old
API, then try to apply the transformation which maps the old API
to the new API.</p>

<p>I’ve found <a href="https://dl.acm.org/doi/10.1145/1103845.1094832">one related paper</a>.
Given that it’s not gone further, was it too hard, or not viable, or
the wrong time?</p>

<h2 id="summary">Summary</h2>

<p>So I think this would be the next evolution in automated upgrades.
It’s seems like a big market – how many companies would pay for you
to solve this problem for them and allow them to concentrate on
business logic rather than plumbing concerns?</p>

<p>But I didn’t take the time off I planned to confirm the potential
market and see how hard a problem it would be solve :)</p>

<h2 id="further-reading">Further Reading</h2>

<p><strong>Update</strong> I originally published this without a list of references.
I should have done the hard work to include them. Mostly that meant
mining my browser history and Pinboard from February and March 2019
when I spent a chunk of time first looking at this
<strong>for absolutely no reason at all, clients of the time</strong>.</p>

<ul>
  <li><a href="https://dl.acm.org/doi/10.1145/1103845.1094832">Refactoring support for class library migration</a></li>
  <li><a href="https://ercim-news.ercim.eu/en88/special/automatic-upgrade-of-java-libraries">Automatic Upgrade of Java Libraries</a>
which linked to <a href="http://web.archive.org/web/20170409031849/http://kenai.com/projects/refactoringng">a defunct Netbeans plugin</a></li>
  <li><a href="http://autorefactor.org/">Autorefactor</a></li>
  <li><a href="http://walkmod.com/">Walkmod</a></li>
  <li><a href="https://github.com/Netflix-Skunkworks/rewrite">Rewrite</a>
    <ul>
      <li>This has now become <a href="https://docs.openrewrite.org/">OpenRewrite</a>
and might be what I’m after, for Java and YAML at least. There is
an example which (when complete) is supposed to migrate from
Spring Boot 1.5.x to Spring Boot 2.x.</li>
    </ul>
  </li>
</ul>

  </article>

</div>

      </div>
    </div></div>]]>
            </description>
            <link>https://jamesabley.com/software-is-drowning-the-world/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25713631</guid>
            <pubDate>Sun, 10 Jan 2021 11:31:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What Merpeople Say About Us]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25713617">thread link</a>) | @dnetesn
<br/>
January 10, 2021 | http://oceans.nautil.us/feature/660/what-merpeople-say-about-us | <a href="https://web.archive.org/web/*/http://oceans.nautil.us/feature/660/what-merpeople-say-about-us">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
        <p><span>M</span>erpeopleâ€™s hybridity has helped them maintain a presence in both scientific and mythological camps. In many peopleâ€™s minds, mermaids and mermen remain mythical creatures more suitable for bedtime stories than scientific tracts. Yet for others merpeople symbolize the outer limits of our scientific and mythological investigations.&nbsp;</p>

<p>Just as the evolution of science has not done away with lingering notions of wonder and myth, so too has our innate need to push boundaries of knowledge led humanity into strange—often mind-blowing—frontiers of research and self-reflection. Humanityâ€™s interaction with merpeople demonstrates our ongoing need for discovery as much as our attempts at regulation and classification. Like the hybrid monstrosities with which humankind has always grappled, humanity maintains a tenuous balance between wonder and order, civilization and savagery.&nbsp;<br></p>
<p>Perhaps nowhere is this fragile equilibrium more obvious than in the early Christian Churchâ€™s myriad representations of mermaids and tritons. Murky ideologies of mermaids and mermen originated in ancient gods and goddesses of the sea; although mermaids now rule as the more popular of the two, merpeopleâ€™s predominance began with mermen. The Babylonians had their fish-god Oannes dating back to 5,000 BCE, while the Philistines, Assyrians and Israelites created the â€˜female prototypeâ€™ for the mermaid with Atargatis, a fertility goddess who was the female counterpart to Oannes. Importantly, Atargatis also symbolized the danger of love and lust, an association which Christians would later embrace wholeheartedly.</p>
<p>A spate of pagan representations of merpeople followed Oannes and Atargatis, ranging from Greek and Roman depictions of Aphrodite and Venus, respectively, to Pliny the Elderâ€™s descriptions of mysterious human-fish sea creatures in 80 CE, to the Greeksâ€™ incorporation of Triton (the origin for the merman) and his wife Amphitrite, to Odysseusâ€™ fateful encounter with the harpies (airborne daughters of sea-goddesses) on his famous voyage. Oddly, harpies and the Greek â€˜Scyllaâ€™—hybrid monstrosities with little resemblance to half-fish, half-women mermaids—would ultimately spawn modern interpretations of mermaids. Over time, artists and writers took the helm in transforming the monstrous representations of Scylla and Homerâ€™s harpies into our modern interpretations of mermaids, replete with sexual overtones, siren songs and the overtly feminine (often naked) form. Thus, while mermen found their origins in a Greek god, mermaids largely originated from hideous beasts who only intended to bring man to destruction through his own lust for sex and power. As would be demonstrated by the early Christian Church, such connotations of sex, lust and power were no coincidence.</p>
<figure><img src="https://s3.amazonaws.com/nautilus-vertical/oceans_1dc26359e27a167f1bd1825f1abacd2e.jpg" alt="Screen Shot 2021-01-07 at 2.19.08 AM"><figcaption><span>â€˜Derceto [or Atargatis] Dominating the Seaâ€™, from Athanasius Kircher, Oedipus Aegyptiacus (1652). </span></figcaption></figure>
<p>Beginning in the third to fifth centuries CE, Church leaders simultaneously adopted, transformed and harnessed ancient pagan symbols of merpeople to assert notions of piety, faith and self- control. Although mermen had long been associated with rape and violence, the early Christian Church was on a mission to dethrone femininity, and had little use for these male monstrosities. Rather, churchmen hoped to transform notions of the Homerian harpy to fit their own means, and in doing so adopted more sexual connotations and imagery in their representations of mermaids.<br></p>
<p>Physical representations of merpeople were critical to this process. Our modern conception of the mermaid stems directly from early churchmenâ€™s depictions of these mysterious creatures. Traditionally shown as human females above the waist, with long, flowing hair and bare breasts, a mirror in one hand and a comb in the other, these half-women, half-fish served as ideal symbols of wonder and danger for Church leaders. Beyond utilizing such â€˜monstersâ€™ to demonstrate Godâ€™s ability to â€˜alter his own laws of natureâ€™, churchmen especially adopted these pagan creatures in an effort to depreciate the feminine—hence the overtly sexual representation of mermaids in church carvings, bestiaries, illuminated texts and artwork. Nakedness—especially as a vehicle for sexual lust—was rare in early Christian and medieval art. Thus, as topless women (who also boasted scaly fish-tails), mermaids would have harnessed a shock factor through image alone.</p>
<blockquote>Church leaders simultaneously adopted, transformed and harnessed ancient pagan symbols of merpeople.</blockquote>

<p>Oftentimes, in fact, church sculptors portrayed mermaids â€˜spreadingâ€™ their tails apart, thereby exposing their reproductive area—or <em>vesica piscis</em> (Latin for â€˜vessel of the fishâ€™)—in graphic detail. A mermaidâ€™s accessories also revealed deeper symbolism, with her mirror and comb representing vanity (not to mention the duality of oneâ€™s soul outside the body) and her flowing hair signifying fertility. Sometimes, mermaids would hold a fish instead of a comb, which probably further symbolized her link to the fish as an early symbol of Christianity. By the medieval period (the fifth to fifteenth centuries CE), churchgoers throughout Europe worshipped in spaces decorated with overtly sexualized mermaid imagery. Church leaders, meanwhile, cultivated an intimate knowledge of these strange creatures through myriad texts, art and sculpture. Such ubiquity helped to facilitate general acceptance of, and belief in, mermaids.&nbsp;</p>
<p>In symbolism used by the early Christian Church, mermen were not as popular as mermaids. When mermen occasionally appeared in church carvings, they were almost always paired with mermaids. This representation correlated with early Christian and medieval imagery—especially cultivated in illuminated texts and bestiaries—which generally depicted mermen as partners of mermaids. Mermaids were much more likely to appear alone than were mermen. In contrast to the beautiful (and dangerous) female form of the mermaid, moreover, authors and illustrators represented mermen either as ugly creatures intended to oppose the mermaidâ€™s striking femininity and sexuality, or as symbolic of Christian piety.&nbsp;</p>

<figure><img src="https://s3.amazonaws.com/nautilus-vertical/oceans_048a295d789994cc78e70cb5e3516b0a.jpg" alt="Screen Shot 2021-01-07 at 2.21.52 AM"><figcaption><span>Mermaid in the Luttrell Psalter (1325â€“40). </span></figcaption></figure>
<p>Ultimately, mermaids—hybrid creatures of myth and lore—symbolized the early Christian Churchâ€™s willingness to hybridize itself (that is, embrace a mix of pagan and Christian belief systems) in its larger attempts to cultivate the largest following possible. Here, the Christian Church deliberately adopted and adapted pagan symbols in its holy spaces, thereby bridging the gap between the supposedly â€˜savageâ€™ and the civilized; the past and the present. And it largely worked, as Christian doctrine steadily decentered symbols of the sacred feminine by the medieval period. However, such efforts had unexpected side effects, as by utilizing these hybrid monstrosities to support religious tenets the Christian Church legitimatized such creatures, which in turn created the foundation for belief and acceptance for generations to come.&nbsp;<br></p>
<p>The â€˜Age of Discoveryâ€™ (roughly 1500 to 1700 CE) only solidified Westernersâ€™ long-standing beliefs and cultural traditions surrounding merpeople. By 1492 Westerners had long lived in a world of merpeople: especially in wealthy, sea-faring societies like Venice or Genoa, merpeople became almost ubiquitous mainstays of art, ranging from tombs to tomes, sculptures to tableware. Unsurprisingly, by the time Westerners pushed further east and west into what were, for them, uncharted territories, they fully expected to find mermaids and tritons. They had, after all, lived their lives surrounded by these mysterious creatures. Importantly, before the â€˜Age of Discoveryâ€™, Europeans placed Jerusalem in the centre of the globe in terms of religious tradition. The further one got from Jerusalem, the stranger and more dangerous the world became. If merpeople lived anywhere, many early modern Westerners believed, it must be at the ends of the Earth, where monstrosities and curiosities thrived.&nbsp;</p>
<blockquote>The&nbsp;â€˜Age of Discoveryâ€™&nbsp;only solidified Westernersâ€™&nbsp;long-standing beliefs surrounding merpeople.<br></blockquote>
<p>As Westerners heightened their interactions with the Pacific and Atlantic worlds in a search for monetary, religious and imperial power, recorded sightings of merpeople multiplied exponentially. The Atlantic Ocean and its â€˜New Worldâ€™ shores were especially rife with such interactions, as famed explorers integrated merpeople into their understandings of strange—and potentially lucrative—environments. Yet, where in the medieval period interactions with mermaids and tritons usually uncovered a deeper lesson over lust, vanity or religion, early modern explorers transformed such contact beyond manifestations of the Christian creed and instead began to reflect in their â€˜mer-sightingsâ€™ emerging notions of exploration, growth and national prowess. Each Western country had its own stories of merpeople, and in each of these interactions the nation tried to assert its understanding of the globe. Monstrosities abounded in the New World, and Europeans were intent on uncovering their secrets. This was a period of legitimization for merpeople.&nbsp;</p>
<p>As sightings proliferated and European Christians steadily colonized the Americas, Western mapmakers began in earnest to chart these strange new worlds. Of course, such cartographic creations were as much about Europeâ€™s effort to position itself as a world power as they were about accurately recreating the New World topography. These maps were also, importantly, intended to demonstrate the exotic opportunity of the world, while also shrinking its size to suit Europeansâ€™ imperialist efforts. Therefore â€˜strangeâ€™ or â€˜foreignâ€™ lands like the Americas and the â€˜Far Eastâ€™ were often depicted with merpeople in their surrounding seas.This was no mistake, nor can it be boiled down to a temporary flight of fancy. Mapmakers intentionally included merpeople …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://oceans.nautil.us/feature/660/what-merpeople-say-about-us">http://oceans.nautil.us/feature/660/what-merpeople-say-about-us</a></em></p>]]>
            </description>
            <link>http://oceans.nautil.us/feature/660/what-merpeople-say-about-us</link>
            <guid isPermaLink="false">hacker-news-small-sites-25713617</guid>
            <pubDate>Sun, 10 Jan 2021 11:29:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Acquisition of Handwriting in the UK]]>
            </title>
            <description>
<![CDATA[
Score 22 | Comments 38 (<a href="https://news.ycombinator.com/item?id=25713526">thread link</a>) | @dcminter
<br/>
January 10, 2021 | http://www.unask.com/website/handwriting/new_web_pages/acquisition.htm | <a href="https://web.archive.org/web/*/http://www.unask.com/website/handwriting/new_web_pages/acquisition.htm">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

The information in this paper is based on research done by Frances Brown
under my supervision; many of the ideas in it are hers, and many come from
collaborative discussion. The paper was originally given at the Criminalistic
Institute in Prague, and was written to be presented there. 

<p>


<strong>1. Introduction and theoretical base.</strong>This paper reports on a project that was undertaken in 1982-3,
funded by the Home Office. It came from the fact that while document examiners
in the UK know a great deal about what the handwriting of UK citizens is
like, and know in general terms how that handwriting was learned, they in
fact know very little about the way in which the learning of handwriting
actually goes on in schools. So the project began with a very general remit:
to investigate the way in which handwriting is taught and learned in the
UK. What it turned into was an investigation into the whole issue of the
relationship between the taught style of handwriting and the way in which
that handwriting is actually done.</p><p>

It was clear from the outset that there are two sources from which handwriting
is learned: from copybooks, and from teachers. The investigation thus split
into two: we used surveys and sampling investigation to find out how handwriting
is actually taught in schools, and we conducted an extensive examination
and analysis of a large sample of the copybooks used in the UK in the lifetime
of any contemporary adult. </p><p>

In general what the project showed was this. Handwriting of UK citizens
is, compared with that of those educated elsewhere in Europe or in America,
very various. No uniformity is imposed centrally, at the governmental level,
or locally, by Local Educational Authorities. There are several different
handwriting systems to be found in the available copybooks, and each copybook
may present its own version of the system it derives from. Individual schools
may favour particular systems or particular copybooks, but often it is left
to the individual teacher of handwriting to adopt whatever method they feel
comfortable with. Moreover, even at the beginning of learning to write it
is not usually expected or demanded that the child should faithfully copy
the learned system, and, later in life, it is common for teenagers who to
change their writing style to make it look beautiful. This is true for both
boys and girls, but the evidence is that girls spend a lot more time at
it. In fact handwriting that follows a copybook style closely is commonly
seen as 'immature', or 'naive', or 'lacking in personality'; and idiosyncratic
or unusual or even messy handwriting is seen as showing valuable personality
traits: individuality, creativity, and so on. As a result, the handwriting
of mature adults can vary considerably from the learned system, containing
a mixture of elements deriving from any combination of (1) the system itself,
(2) the particular copybook version of that system, (3) a school-teacher's
version of the copybook, (4) the individual's own idiosyncratic variation,
and (5) curious features that occur in no copybook but are nonetheless commonly
found, presumably having spread through the culture by some form of osmosis.
This means that when someone says: 'how would you characterize the handwriting
of the UK?' they are asking a question that is very hard to answer; in fact,
it is a question that no-one <cite>could</cite> answer before this project
was undertaken.</p><p>

So: the situation that we discovered is complex, but it is not chaotic.
General patterns were discovered, and interesting sub-patterns. Before giving
the detail of the findings, however, it is necessary to establish some terminology.</p><center><a href="http://www.unask.com/website/handwriting/new_web_pages/terms.htm"><strong>Summary of Terminology</strong></a>
</center>
<div><p>Handwriting in the UK is learned at school, usually beginning at the
age of 5 or so, by copying. What is copied is a formal <strong>system. </strong>We
discovered from a close analysis of some 50 handwriting copybooks, which
we believe represent all of the published styles available in the lifetime
of any adult, that their apparent diversity can be reduced to four basic
systems: <strong>Print Script, Round Hand, Looped Cursive, </strong>and<strong>
Italic.</strong> However the actual version of the system that the child
copies will normally come from a particular copybook, which may have its
own variation or dialect of the basic system; or what a child actually copies
may be work materials prepared by a teacher, with the teacher's own variations
added. If a child moves from one school or even one teacher to another,
he or she may be exposed to two different copybooks or even two different
systems. Nonetheless, it is usually possible to detect traces of the system,
though not the actual copy book version of it, that a given person has been
taught.</p><p>

By repeated attempts to copy this basic pattern the child learns the necessary
skills of motor-co-ordination. The child first learns Print Script, a simple
unjoined alphabet, and then learns a joined style based one of the other
three main systems. As the mature hand develops, eventually by the end of
the teenage years evolving into the stable final hand that will (usually)
serve the individual for the rest of his or her life, two kinds of characteristic
can be ascertained. We call these <strong>individual characteristics</strong>
and <strong>style characteristics</strong> . Individual characteristics
are those components of a given hand that make it unique: they are what
document examiners are mainly interested in. But in each hand there will
also be a residual set of style characteristics: those elements which are
shared with other members of a group or groups. Style characteristics come
in three kinds: <strong>system characteristics</strong> , that serve to
identify which of the four general classes or systems the learned style
derives from; <strong>copybook characteristics</strong> , that can definitely
be linked to a particular copybook version of the basic system; and, finally,
what we call <strong>underground characteristics</strong> , which are those
features which are shared with other writers, but which do not occur in
any copybook. So for example the following letter-forms, familiar to all
UK handwriting examiners, are not as far as we know part of any of the basic
systems or their copybook dialects:</p></div><center><a href="http://www.unask.com/website/handwriting/new_web_pages/underground.htm"><strong>Underground characteristics</strong></a><strong>.</strong>
</center>
<div><p>Some underground characteristics may actually be learned at the stage
when handwriting is initially acquired, deriving from the practice of teachers
who modify the copybook system that they teach, for whatever reason, thus
incorporating underground characteristics at this early stage. Normally,
however, they seem to be picked up later, presumably by imitation.</p><p>

Since in the UK (and in other countries too) the normal process of teaching
handwriting is to teach first one system, the unjoined Print Script, and
then a second, cursive or joined up system, it is the latter that normally
is the basis of the set of system characteristics in the mature hand. Rather
confusingly, however, the original Print Script is not seen as being a system
at all, but as 'basic writing', which is then 'joined up', under the influence
of some taught system, to form a cursive style. In fact what happens is
that two successive <cite>systems</cite> are learned, the latter superseding
the former: Print Script is as much a copybook system, with a history and
a designer, as any other. But because of the misconception that Print Script
is not a system, this form is not often seen in adult handwriting. It is
widely associated with semi-literacy, where the writer is assumed not to
have progressed on to the 'joined up' writing.</p><p>

The case of capitals is also curious. The child learns two forms: Print
Script is taught in a lower and upper case form. The cursive scripts that
follow it also have their own particular kind of capitals, and these are
learned; but they are only used when the writer is doing their normal writing.
When an adult is required to fill in a form and asked to write in capitals
for the purpose of clarity, they will usually produce Print Script capitals.
(The practice found elsewhere in Europe of producing a print script lower
case form when asked to write clearly--to 'print'--is rare in the UK.) These
Print Script capitals are known as <strong>block capitals</strong> , and
considered (incorrectly) to be a separate style: they are not associated
in most people's minds with the 'childish' style of Print Script. Since
for most people filling in forms is only a small part of the writing that
they do, block capitals are not much practiced and therefore retain quite
purely their Print Script style: they are less embellished than cursive
writing with individual or underground characteristics. Therefore block
capitals are commonly used in writing that wishes to remain anonymous. However,
some writers have more practice than others in writing block capitals, and
they have evolved styles of their own. All of these styles are underground:
no style of block capitals other than Print Script is taught or published.</p></div><center><a href="http://www.unask.com/website/handwriting/new_web_pages/caps.htm"><strong>Styled Block Capitals</strong> </a></center>
<div><p>As far as document examiners are concerned the relevance of this outline
of handwriting acquisition is as follows. The part of their job that concerns
handwriting examination can be said to have two aspects: <strong>identification</strong>
and <strong>categorization</strong> . Identification deals with those tasks
that involve stating whether or not a particular piece of writing was, or
was not, written by a particular individual. Categorization deals with the
problem of anonymous writing, and involves saying whatever can be said to
describe the anonymous writer. For example: was this writing done by someone
who was left-handed? At the moment the former is far more important, partly
because relatively little proper research has been done on the latter. </p><p>

Categorization deals entirely--by definition--with style characteristics:
those characteristics that are shared with other writers. Identification
deals with both: with individual characteristics--again, by definition--but
also with style characteristics, since in particular cases the appearance
of certain of these in a particular hand may serve to distinguish the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.unask.com/website/handwriting/new_web_pages/acquisition.htm">http://www.unask.com/website/handwriting/new_web_pages/acquisition.htm</a></em></p>]]>
            </description>
            <link>http://www.unask.com/website/handwriting/new_web_pages/acquisition.htm</link>
            <guid isPermaLink="false">hacker-news-small-sites-25713526</guid>
            <pubDate>Sun, 10 Jan 2021 11:18:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building a Hacker Space (2007) [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25712952">thread link</a>) | @Tomte
<br/>
January 10, 2021 | https://events.ccc.de/congress/2007/Fahrplan/attachments/1003_Building%20a%20Hacker%20Space.pdf | <a href="https://web.archive.org/web/*/https://events.ccc.de/congress/2007/Fahrplan/attachments/1003_Building%20a%20Hacker%20Space.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://events.ccc.de/congress/2007/Fahrplan/attachments/1003_Building%20a%20Hacker%20Space.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25712952</guid>
            <pubDate>Sun, 10 Jan 2021 09:52:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[CTO day 2: downsizing the team]]>
            </title>
            <description>
<![CDATA[
Score 94 | Comments 106 (<a href="https://news.ycombinator.com/item?id=25712771">thread link</a>) | @delebe
<br/>
January 10, 2021 | https://danlebrero.com/2020/12/02/cto-diary-downsizing-team-firing/ | <a href="https://web.archive.org/web/*/https://danlebrero.com/2020/12/02/cto-diary-downsizing-team-firing/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p id="entry-summary">On your first day as newly appointed CTO you are working on your hiring strategy, the second day your organization asks you to downsize the team.</p><div><p>A very important project that was a done deal, and that will secure the organization’s future for the next five years, failed to happen.</p><p>Not winning this project meant that the organization was forced to <em>“focus”</em> and reduce costs.</p><p>It never crossed my mind that one of the first things that I would be asked to do as an inexperienced CTO would be to fire part of the team.</p>
<p>when such bad news becomes public, host an open door day where the team can openly talk about the situation, share their feelings, see each other, feel connected, and be listened to. It would not stop the gossiping but it will help.</p><h2>Cuts and team principles</h2><p>The Dev team slice of the <em>“focus”</em> meant cutting cost by 10 to 15%. In human terms, firing two to four people (out of 17).</p><p>After some failed attempts to approach this task, this is what worked for me.</p><p>I started by listing some principles: </p>
<ul>
  <li>For each product, we need to cover the following disciplines: Frontend, Backend, Architecture, Operations, Mobile, UX, Design, QA, Leadership, and Product Management.</li>
  <li>There are broadly 3 experience levels:
  <ul>
    <li>Senior: creates the plan.</li>
    <li>Mid-level: follows the plan.</li>
    <li>Junior: needs to be taught to follow the plan.</li>
  </ul></li>
  <li>No number of junior people can do some work a senior person can do.</li>
  <li>Not having a senior person in a discipline will mean that the product quality will suffer:
  <ul>
    <li>If a senior creates a plan, a team of mid-level+juniors can follow the plan for some time (2-3 years?) before the product becomes a big mess.</li>
    <li>To get out of a big mess, we need senior people.
    <ul>
      <li>We are already in a big mess with Product A and Product B.</li>
      <li>With the current team, it feels Product A is getting out of the mess.</li>
    </ul></li>
  </ul></li>
  <li>Independent (cross-functional) teams are more efficient:
  <ul>
    <li>More focus.</li>
    <li>No handoffs, faster feedback.</li>
    <li>No shared “resource” contention.</li>
  </ul></li>
  <li>People working in multiple products/teams are less efficient.</li>
  <li>Multidisciplinary (generalist) people:
  <ul>
    <li>Can cover the need for several of the disciplines.</li>
    <li>They can be junior in one discipline and senior in another.</li>
    <li>Allow focusing on any priority, without creating artificially important work for single-disciplined members.</li>
  </ul></li>
  <li>Max team size: 2 pizza teams 6-8.</li>
</ul><p>The ideal team would be one that has all disciplines covered at a senior level with multidisciplinary people working in just one team, and with enough overlap to avoid a <a href="https://en.wikipedia.org/wiki/Bus_factor">bus factor</a> of one. Additional people will bring additional capacity.</p><h2>Inverse Conway’s maneuver</h2><p>We have been working on bringing three of the products together for some time. They were already under the same product manager, but they were still three teams working on their own priorities. Merging them into one team, in a classic <a href="https://danlebrero.com/2020/01/08/do-i-need-a-gateway-api-team-dynamics/#content">inverse Conway’s maneuver</a> would hopefully accelerate the integration between the products.</p><p>One of the other products was small enough that for this task I decided to temporarily ignore it.</p><p>Following the principles above and the product considerations, the plan was to move the previous team setup from:</p><p><img src="https://danlebrero.com/images/blog/cto/day2/old-team-structure.jpg" alt="old team structure"> </p><p>To something like (boxes represent skills, not people):</p><p><img src="https://danlebrero.com/images/blog/cto/day2/new-team-structure.jpg" alt="new team structure"> </p><p>So two products, two cross-functional teams. Platforms and design teams will be reshuffled inside those two teams.</p><h2>Which parent do you love most? A computer will tell</h2><p>With a clear plan for the team, the next step was to <em>“just”</em> pick up who will work on each team, and who we will need to let go. The most painful decision in my career. </p>
<blockquote><p><a href="https://danlebrero.com/2019/11/27/becoming-a-technical-leader-book-notes/#content">People with a strong technical background can convert any task into a technical task, thus avoiding work they don’t want to do.</a> <cite>Jerry Weinberg, Becoming a Technical Leader</cite></p>
</blockquote><p>And I didn’t want to do the task so, consciously ignoring Mr. Weinberg, I transformed the ordeal into an optimization problem, for which I wrote an application to help me with.</p><h3>The app</h3><p>The application took as input the amount of $$$ to cut, the list of people, their salary, and the skill level on each discipline mentioned above, and outputted the possible two teams that would be within budget and match the minimum requirements, sorted by a scoring system.</p><p>The minimum requirements and scoring system configuration looked like:</p>
<pre><code>{
"FE" [at-least-senior sum-skills]
"PM" [senior-plus-somebody-else (fix-points 5)]
"Design" [two-mid-or-senior (senior-better 6 2)]
...
}
</code></pre><p>The first function filters out invalid teams while the second scores the valid ones.</p><p>In the example we say that the team has to have:</p>
<ul>
  <li>At least one senior FE developer. The more FE developers, the better team score.</li>
  <li>At least one senior product manager and one mid or junior one. Only one senior is not enough. No matter how many PMs the team has, the score for this discipline is 5. A team with loads of FE devs will score higher than one with loads of PMs.</li>
  <li>At least a senior designer or two mid-level designers, but we prefer one senior designer (6 points) instead of two mid-level ones (2 points).</li>
</ul><h3>The result</h3><p>As heartless as this may seem:</p>
<ul>
  <li>It removed some bias. There is still bias on the skill level evaluation and in the team scoring system.</li>
  <li>I was part of the people on that list. To be honest, little consolation here. Big bias.</li>
  <li>It forced me to be very very precise on what a “functioning team” meant.</li>
  <li>It allowed me to see what different scoring systems would output.</li>
  <li>I noticed some people would never show in the output, and had to dig into why. It was enlightening.</li>
  <li>It allowed me to analyze tens of thousands of different team combinations, with different scoring systems.</li>
  <li>Programming gave me a respite from the task. This was the first time, but now I embrace more regularly “keep my sanity” programming days.</li>
</ul><p>Most important, the application gave me a few starting points. Of those, I still had to consider the team dynamics, existing teams, personalities, seniority, potential, personal situation, future needs, …</p>
<p>yes, I still have the code. No, I am not going to share it publicly. It would kill me to find there is a bug.</p><h2>Delivering the bad news</h2><p>Once the decision was made, it was time to swallow the last bitter pill.</p><p>Some tips:</p>
<ul>
  <li>Ensure that the people affected are the first ones to know.</li>
  <li>Warn beforehand:
  <ul>
    <li>Do not use your regular one-to-one meeting slot.</li>
    <li>In your message, give a strong hint: “HR person will be in the meeting”, “Really bad news”.</li>
    <li>Give them time to get ready for the meeting.</li>
  </ul></li>
  <li>You don’t need to do it alone. Our HR manager was present and was a huge support for both of us.</li>
  <li>Treat people like adults.</li>
  <li>At the meeting, follow Nadia van der Vlies’ <a href="https://danlebrero.com/2020/04/01/no-nonsense-leadership-summary/#bad-news">advice</a>:
  <ul>
    <li>Deliver the blow:
    <ul>
      <li>Go straight to the bad news.</li>
      <li>Give one or two reasons.</li>
    </ul></li>
    <li>Manage the reaction:
    <ul>
      <li>Be understanding. Do not justify yourself.</li>
      <li>Give space. Do not fill silences.</li>
    </ul></li>
    <li>Solution, explanation, follow-up appointment:
    <ul>
      <li>Wait for the employee to be ready. When she starts asking “why” or “what now”.</li>
      <li>Reiterate reasons.</li>
    </ul></li>
  </ul></li>
  <li>In a couple of days follow up with another more informal meeting. The news would have sunk, and the conversation would be more forward-thinking and productive.</li>
</ul>
<hr><p>A slap in the face to awaken me from the dream that a CTO role is mostly about technology.</p></div></div>]]>
            </description>
            <link>https://danlebrero.com/2020/12/02/cto-diary-downsizing-team-firing/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25712771</guid>
            <pubDate>Sun, 10 Jan 2021 09:27:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Noise Planets]]>
            </title>
            <description>
<![CDATA[
Score 153 | Comments 16 (<a href="https://news.ycombinator.com/item?id=25712767">thread link</a>) | @atulvi
<br/>
January 10, 2021 | https://avinayak.github.io/art/2021/01/09/noise-planets.html | <a href="https://web.archive.org/web/*/https://avinayak.github.io/art/2021/01/09/noise-planets.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p><img src="https://avinayak.github.io/uploads/erporydxmaarwcd.png" alt=""></p>

<p>I recently found this piece of art (LINES 2A (2017)) created by <a href="https://twitter.com/tylerxhobbs">Tyler Hobbs</a>. This picture kinda looked very hand drawn, but it’s completely generative. Something about this drawing and it’s texture kind of resonated with me, so I wanted to try to study and replicate (or make something inspired by this work) using p5js.</p>

<p>I started out by plotting a bunch of random points within a circle like so.</p>

<div><div><pre><code>w = 1000
function setup() {
  createCanvas(w, w);
  background('#F9F8F4');
}

function draw() {
  x = random(w)
  y = random(w)
  if (pow(w/2 - x, 2) + pow(w/2 - y, 2) &lt; 7e4) {
    point(x,y)
  }
}
</code></pre></div></div>

<p><img src="https://avinayak.github.io/uploads/download-25.png" alt=""></p>

<p>This is a painfully slow process to generate random points in a circle. I found a better way to do this later. What I wanted to do next was to generate flow fields, but restricted to the circular region.</p>

<p>It’s super easy to generate flow field patterns using perlin noise.</p>

<ol>
  <li>Choose a random point <code>&lt;x,y&gt;</code></li>
  <li>Plot <code>&lt;x,y&gt;</code></li>
  <li>Calculate <code>n = noise(x,y)</code></li>
  <li>Do <code>x+=cos(n * 2 * PI)</code> and <code>y+=sin(n * 2 * PI)</code></li>
  <li>Repeat 2.</li>
</ol>

<p>We’re going to plot flow fields inside the circle. Let’s try this.</p>

<div><div><pre><code>const is_in_circle = (x, y) =&gt; 
  (pow(w / 2 - x, 2) + pow(w / 2 - y, 2) &lt; 7e4)

function draw() {
  if (is_in_circle(x = random(w), y = random(w)))
    while (is_in_circle(x, y)) {
      n = noise(x, y)
      x += sin(n * TAU)
      y += cos(n * TAU)
      point(x, y)
    }
}
</code></pre></div></div>

<p><img src="https://avinayak.github.io/uploads/download-28.png" alt=""></p>

<p>OK, not very good. The noise at this level is pretty rough. we’re going to zoom in to the noise function (by dividing the <code>x,y</code> inputs by some constant value) and probably use <code>circle(x ,y ,0.3)</code> to plot points instead if point function, because I feel it looks way smoother. Also, I’m adding a <code>random() &gt; 0.01</code> condition in the loop so that we also get short lines that are not trimmed away by the edge of the circle.</p>

<div><div><pre><code>function draw() {
  if (is_in_circle(x = random(w), y = random(w)))
    while (is_in_circle(x, y) &amp;&amp; random() &gt; 0.01) {
      n = noise(x / 500, y / 500)
      x += sin(n * TAU)
      y += cos(n * TAU)
      circle(x, y, .3)
    }
}
</code></pre></div></div>

<p><img src="https://avinayak.github.io/uploads/download-27.png" alt=""></p>

<p>Actually.. not bad. I think we manage almost replicate the original texture. The inverted version also looks pretty good.</p>

<p><img src="https://avinayak.github.io/uploads/download-19.png" alt=""></p>

<p><img src="https://avinayak.github.io/uploads/ppanets.png" alt=""></p>

<p>I went ahead and made a つぶやきProcessing version of this.</p>

<blockquote><p lang="en" dir="ltr">function setup(){createCanvas(w=1e3,w),background("<a href="https://twitter.com/hashtag/%E3%81%A4%E3%81%B6%E3%82%84%E3%81%8DProcessing?src=hash&amp;ref_src=twsrc%5Etfw">#つぶやきProcessing</a>")}function draw(){if(g(x=random(w),y=random(w)))for(;g(x,y)&amp;&amp;random()&gt;.01;)n=noise(x/500,y/500),x+=sin(n_TAU),y+=cos(n_TAU),circle(x,y,.3)}g=((n,o)=&gt;pow(w/2-n,2)+pow(w/2-o,2)&lt;w*w/16); <a href="https://t.co/iVZTMtCn3i">pic.twitter.com/iVZTMtCn3i</a></p>— yakinavault (@yakinavault) <a href="https://twitter.com/yakinavault/status/1347903013042622467?ref_src=twsrc%5Etfw">January 9, 2021</a></blockquote>


<h2 id="going-further-animations">Going Further: Animations</h2>

<p>The code we wrote right now technically is animated. The animation however is not very smooth.</p>

<video loop="" autoplay="" muted=""> <source src="https://avinayak.github.io/uploads/simplescreenrecorder-2021-01-10_03-52-31.mp4" type="video/mp4"> </video>

<p>To make smooth animations, we need to generate new points in the circle, keep track of these points outside the <code>draw()</code> function. I found this neat <a href="https://stackoverflow.com/a/50746409">technique</a>, to find random points in a circle where a random radius <code>r</code> and angle <code>theta</code> are chosen and the <code>x,y</code> points are obtained as <code>x = centerX + r * cos(theta)</code> and <code>y = centerY + r * sin(theta)</code></p>

<p>Let’s try that first.</p>

<div><div><pre><code>function random_point() {
  r = random(w / 4)
  t = random(TAU)
  return [
    w/2 + cos(t) * r, 
    w/2 + sin(t) * r
  ]
}

function setup() {
  createCanvas((w = 1e3), w);
  background(255)
  k = w / 2
  m = (Array(w).fill(0)).map(random_point)
}

function draw() {
  for (i = k; --i;) {
    [x, y] = m[i]
    circle(x, y, .3);
  }
}
</code></pre></div></div>

<p><img src="https://avinayak.github.io/uploads/screenshot-from-2021-01-10-04-51-20.png" alt=""></p>

<p>and now we apply flow fields and try to move these points.</p>

<div><div><pre><code>function random_point() {
  r = random(w / 4)
  t = random(TAU)
  return [
    w/2 + cos(t) * r, 
    w/2 + sin(t) * r
  ]
}

const w = 1000
function setup() {
  createCanvas(w, w);
  background('#F9F8F4')
  k = w / 2
  points = (Array(k).fill(0)).map(random_point)
}

function draw() {
  for (i = k; --i;) {
    [x, y] = m[i]
    x += sin(n = noise(x / 400, y / 400) * TAU) * h
    y += cos(n) * h
    stroke(i%255)
    circle(x, y,.3)
    if (pow(k - x, 2) + pow(k - y, 2) &lt; 7e4)  // if point is in circle
      points[i] = [x, y, t]
    else points[i] = random_point() // replace with new point if not
  }
}
</code></pre></div></div>

<video loop="" autoplay="" muted=""> <source src="https://avinayak.github.io/uploads/simplescreenrecorder-2021-01-10_04-56-11.mp4" type="video/mp4"> </video>

<p>And a つぶやきProcessing version of course..</p>

<blockquote><p lang="cy" dir="ltr">t=0,p=i=&gt;\[k+(r=random(w/4))_cos(t+=.1),k+r_sin(t)\],setup=i=&gt;{createCanvas(w=1e3,w),m=Array(k=w/2).fill(0).map(p)},draw=r=&gt;{for(i=k;--i;)\[x,y\]=m\[i\],x+=sin(n=noise(x/k,y/k)_TAU),y+=cos(n),stroke(i%4_85),point(x,y),k_w+x_x+y_y-w_(x+y)&lt;7e4?m\[i\]=\[x,y\]:m\[i\]=p()};//<a href="https://twitter.com/hashtag/%E3%81%A4%E3%81%B6%E3%82%84%E3%81%8DProcessing?src=hash&amp;ref_src=twsrc%5Etfw">#つぶやきProcessing</a> <a href="https://t.co/xVhCBNUltL">pic.twitter.com/xVhCBNUltL</a></p>— yakinavault (@yakinavault) <a href="https://twitter.com/yakinavault/status/1347930637227855874?ref_src=twsrc%5Etfw">January 9, 2021</a></blockquote>


<h2 id="adding-colors">Adding Colors</h2>

<p>There are many strategies to colorizing this sketch. One is by just giving each particle a random initial color.</p>

<p><img src="https://avinayak.github.io/uploads/download-21.png" alt=""></p>

<p>However, I found that maintaining the initial x or y position in the particle array and using that to derive the hue information gives us some nice Jupiter/gaseous planet vibes.</p>

<video loop="" autoplay="" muted=""> <source src="https://avinayak.github.io/uploads/simplescreenrecorder-2021-01-10_05-18-19.mp4" type="video/mp4"> </video>

<p>The fringing at the sides can be avoided by moving 50% of the points in the reverse direction.</p>

<video loop="" autoplay="" muted=""> <source src="https://avinayak.github.io/uploads/simplescreenrecorder-2021-01-10_05-28-03.mp4" type="video/mp4"> </video>

<video loop="" autoplay="" muted=""> <source src="https://avinayak.github.io/uploads/simplescreenrecorder-2021-01-10_08-43-25.mp4" type="video/mp4"> </video>

<p>More color variations</p>

<p><img src="https://avinayak.github.io/uploads/untitled.png" alt=""></p>

<p>And that’s it. Hope this was educational!</p>

</div></div>]]>
            </description>
            <link>https://avinayak.github.io/art/2021/01/09/noise-planets.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25712767</guid>
            <pubDate>Sun, 10 Jan 2021 09:26:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Blocks Courtesy of Konrad Zuse (2014)]]>
            </title>
            <description>
<![CDATA[
Score 33 | Comments 10 (<a href="https://news.ycombinator.com/item?id=25712727">thread link</a>) | @todsacerdoti
<br/>
January 10, 2021 | https://journal.infinitenegativeutility.com/blocks-courtesy-of-konrad-zuse | <a href="https://web.archive.org/web/*/https://journal.infinitenegativeutility.com/blocks-courtesy-of-konrad-zuse">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Apparently, I've got a theme going of Weird Syntax. Let's run with it.</p>

<p>Konrad Zuse was an early pioneer in computer science, although his name is perhaps somewhat less well-known than others. Zuse holds the honor of having built the first programmable computer—-the Z3—-back in the 40's, as well as several other computing firsts<sup id="fnref:1"><a href="#fn:1" rel="nofollow">1</a></sup>. Of particular interest to this blog post is his early unimplemented programming language, Plankalkül.</p>

<p>Plankalkül was, like the Z3, in many respects ahead of its time. Zuse's explicit goal was to be able to describe programs at a high level, which meant he included control structures and datatype definitions<sup id="fnref:2"><a href="#fn:2" rel="nofollow">2</a></sup> and other high-level constructs that were often missing in languages of the early years of computing. Zuse was working on Plankalkül at a time when his machines were not useable, which meant that his language work was more theoretical than it was technical, and consequently he allowed features that he wasn't entirely sure how to program. Despite his notes on it having been written in the mid-40's, they were not published until the 70's, and it was not implemented until the year 2000.</p>

<p>One thing that struck me, as I read programs in this notation that had been set down on a typewriter<sup id="fnref:3"><a href="#fn:3" rel="nofollow">3</a></sup>, is that certain kinds of grouping were handled by explicit indication of scope: not via matched delimiters as in ALGOL-style languages, or via indentation in languages such as Python and Haskell, but by formatting the code so that a line bordered on the left of the scoped parts of the code:</p>

<p><img src="https://blog.infinitenegativeutility.com/static/zuse-01.png" alt=""></p>

<p>This is meant to capture the way grouping works in the hand-written or typeset notation, with brackets spanning multiple lines:</p>

<p><img src="https://blog.infinitenegativeutility.com/static/zuse-02.png" alt=""></p>

<p>I think this is notationally interesting: it's like Python's significant whitespace, but not, uh, whitespace. It would be incredibly tedious to type out, but still entirely compatible with current programming notation:</p>

<pre><code>class Tet
 | @staticmethod
 | def new_tet()
 |  | n = randint(0, len(Tet.Tets) - 1)
 |  | for p in Tet.Tets[n]
 |  |  | if p in Board.permanent
 |  |  |  | Game.lose()
 |  | Game.current = Tet(Tet.Tets[n], Tet.TetColors[n])
 |
 | def __init__(self, points, color)
 |  | self.points = points
 |  | self.color = color
</code></pre>

<p>and would be entirely amenable to beautifying via judicious application of Unicode:</p>

<pre><code>class Tet
 ┃ @staticmethod
 ┃ def new_tet()
 ┃  ┃ n = randint(0, len(Tet.Tets) - 1)
 ┃  ┃ for p in Tet.Tets[n]
 ┃  ┃  ┃ if p in Board.permanent
 ┃  ┃  ┗  ┗ Game.lose()
 ┃  ┗ Game.current = Tet(Tet.Tets[n], Tet.TetColors[n])
 ┃
 ┃ def __init__(self, points, color)
 ┃  ┃ self.points = points
 ┃  ┗ self.color = color
</code></pre>

<p>Looking at this notation, however, an interesting possibility struck me: a programmer could explicit annotate information about the <em>kind of scope</em> involved in a given line. In this Python-like example, I could, for example, distinguish class scope using double lines, function scope with thick lines, and control structure scope with thin lines:</p>

<pre><code>class Tet
 ║ @staticmethod
 ║ def new_tet()
 ║  ┃ n = randint(0, len(Tet.Tets) - 1)
 ║  ┃ for p in Tet.Tets[n]
 ║  ┃  │ if p in Board.permanent
 ║  ┃  └  └ Game.lose()
 ║  ┗ Game.current = Tet(Tet.Tets[n], Tet.TetColors[n])
 ║
 ║ def __init__(self, points, color)
 ║  ┃ self.points = points
 ║  ┗ self.color = color
</code></pre>

<p>One advantage of this scheme is that a handful of lines, viewed in isolation, still give you a clear view of what surrounds them. For example, I can view these two lines in isolation and still tell that they are within a control structure used within a function declared within a class:</p>

<pre><code> ║  ┃  │ if p in Board.permanent
 ║  ┃  └  └ Game.lose()
</code></pre>

<p>You could also imagine a hypothetical language in which choice of scope delimiter is important. In Python, <code>for</code> and <code>if</code> do not form a new lexical scope. What if instead we could stipulate the kind of scope they form by this notational convention?</p>

<pre><code>def okay()
 ┃ if True
 ┃  └ n = 5   # n is declared in function scope
 ┗ return n   # n leaks out of the if-scope

def not_okay()
 ┃ if True
 ┃  ┗ n = 5   # n is declared in the if's scope
 ┗ return n   # error: no n in scope here
</code></pre>

<p>That being said, there are a number of reasons that this notation is in inferior to existing notations:</p>
<ul><li>It makes refactoring code <em>much</em> more difficult.</li>
<li>It requires that the programmer <em>pay attention</em> to the sequence of enclosing scopes on a <em>line-by-line</em> basis, which is generally too pedantic and not particularly useful for a programmer.</li>
<li>The ability to select “which kind of scope” is by no means only expressible by this notation, as other syntactic features such as keywords and delimiters could express the same thing.</li>
<li>There are only so many line-like characters which can serve as a scope marker, so this scheme is not very extensible.</li>
<li>It complicates parsing (especially by introducing an entirely new class of parse errors in which adjacent lines feature incompatible sequences of delimiting lines), and so it also...</li>
<li>Complicates parse <em>error messages</em>, which are an important part of a language's UI and should be considered seriously.</li></ul>

<p>So, as in my previous post on <a href="https://journal.infinitenegativeutility.com/2014/8/noun-case" rel="nofollow">grammatical case in programming languages</a>, I urge readers <em>not</em> to use this notation as the concrete syntax for a programming language. This is merely an entertaining peek through the looking glass at a curious notational convention which was never adopted.</p>

<p>That said: this makes a very nice notation for <em>viewing</em> code, where the programmer does not have to explicitly draw ASCII art around their code; indeed, it bears more than a passing similarity to the graphical interface used in <a href="http://scratch.mit.edu/" rel="nofollow">Scratch</a>, and Sean McDirmid's <a href="http://research.microsoft.com/en-us/projects/liveprogramming/typography.aspx" rel="nofollow">Experiments in Code Typography</a> features this very convention as an interactive ornament on code in a Python-like language.</p>

</div></div>]]>
            </description>
            <link>https://journal.infinitenegativeutility.com/blocks-courtesy-of-konrad-zuse</link>
            <guid isPermaLink="false">hacker-news-small-sites-25712727</guid>
            <pubDate>Sun, 10 Jan 2021 09:21:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Gekko: OS software used by the Danish gov. for economic forecasting]]>
            </title>
            <description>
<![CDATA[
Score 21 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25712719">thread link</a>) | @stilisstuk
<br/>
January 10, 2021 | http://t-t.dk/gekko/ | <a href="https://web.archive.org/web/*/http://t-t.dk/gekko/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				
<p>Instead of Gekko, why not use for instance Python, or R, or GAMS, or MATLAB, or SAS, or Excel, or … ?</p>
<p>Software packages are typically aimed at some purpose, and this is reflected in, among other things, the underlying data structures and the syntax of the command language.</p>
<p>Since Gekko is timeseries-oriented, the syntax for handling and analyzing timeseries and timeseries-based models is concise and natural. Gekko does not try to do everything, but tries to focus on its strong points, while simultaneously providing good interfaces to other software packages.</p>

			</div></div>]]>
            </description>
            <link>http://t-t.dk/gekko/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25712719</guid>
            <pubDate>Sun, 10 Jan 2021 09:20:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Top Ranked Tweets on Hacker News 2020]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 5 (<a href="https://news.ycombinator.com/item?id=25712499">thread link</a>) | @hgarg
<br/>
January 10, 2021 | https://harishgarg.com/writing/hacker-news-front-page-tweets-2020/ | <a href="https://web.archive.org/web/*/https://harishgarg.com/writing/hacker-news-front-page-tweets-2020/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
    <ul>
<li>Data curated from <a target="_blank" rel="noopener" href="http://explorehackernews.xyz/?ref=harishgarg.com">Hacker News Front Page Explorer</a></li>
</ul>
<p>1.Every Google result now looks like an ad </p><p>2.macOS unable to open any non-Apple application </p><p>3.John Conway has died </p><p>4.iOS14 reveals that TikTok may snoop clipboard contents every few keystrokes </p><p>5.This electrical transmission tower has a problem </p><p>6.Apple does not keep the 30% commission on a refund </p><p>7.AWS forked my project and launched it as its own service </p><p>8.Google no longer providing original URL in AMP for image search results </p><p>9.Guido van Rossum joins Microsoft </p><p>10.When a customer refunds your paid app, Apple refunds its 30% cut </p>
<p>Get the Full list (162 records) <a target="_blank" rel="noopener" href="https://gum.co/hacker-news-tweets-2020">here for FREE</a></p>
<p>Want to run this and other kind of analysis on your own? Get the Full Database <a target="_blank" rel="noopener" href="http://explorehackernews.xyz/?ref=harishgarg.com">here</a></p>

  </div></div>]]>
            </description>
            <link>https://harishgarg.com/writing/hacker-news-front-page-tweets-2020/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25712499</guid>
            <pubDate>Sun, 10 Jan 2021 08:52:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Breakthrough in Measuring the Building Blocks of Nature]]>
            </title>
            <description>
<![CDATA[
Score 50 | Comments 10 (<a href="https://news.ycombinator.com/item?id=25712475">thread link</a>) | @CapitalistCartr
<br/>
January 10, 2021 | http://m.nautil.us/blog/a-breakthrough-in-measuring-the-building-blocks-of-nature | <a href="https://web.archive.org/web/*/http://m.nautil.us/blog/a-breakthrough-in-measuring-the-building-blocks-of-nature">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			

			<figure data-alt=""><img src="http://static.nautil.us/18082_ad5db5924e3e97ed8a387a499efa9fa0.jpg" width="733" alt=""><figcaption><span><i>An artistic rendering of the quarks and gluons that make up a proton.</i></span><span>Illustration by D. Dominguez / CERN</span></figcaption></figure><p><span>I</span>n a recent experiment done at the Max Planck Institute for Quantum Optics, in Germany, physicist Alexey Grinin and his colleagues came a step closer to resolving one of the more significant puzzles to have arisen in particle physics over the past decade. The puzzle is this: Ordinarily, when you set about measuring the size of something, you’d expect to get the same answer no matter what you use to measure it—a soda can has the diameter it does whether you measure it with a tape measure or callipers (provided these are properly calibrated, of course). Something must be amiss if your attempts to measure the can return different answers depending on the equipment, yet this is precisely what’s happened over multiple attempts to measure the spatial extent of a proton. What’s potentially at stake is our understanding of the building blocks of reality: the <a href="https://science.sciencemag.org/content/370/6520/1061.abstract" target="_blank">differing measurements</a> could be heralding the existence of new forces or particles.</p><p>What does it mean for a subatomic particle to have a measurable “size”? Mathematically, fundamental particles are idealized as point particles, which is to say that, as far as we can tell, they have no meaningfully discernible spatial extent, or substructure, at all. True, all fundamental particles are associated with a quantum mechanical wave packet, which does have a spatial extent that depends on the energy of the particle. Yet these basic bits of Lego are entities whose wave packets you can, in principle, pack into as small a region as you’d like before the very notion of continuum geometry starts, at the Planck scale, to lose meaning. Fundamental particles organize into something analogous to a mini periodic table—consisting of the various force carrying particles, such as photons and gluons (the carrier particles of the strong nuclear force), along with three generations of quarks and leptons and the mass-generating Higgs boson—and can stack together in different combinations to form a zoo of so-called composite particles.</p><blockquote><p>There is less than one in about a trillion chance that the discrepancy could be a statistical fluke.</p> </blockquote><p>Perhaps the most familiar and ubiquitous of these is the proton. With at least one in every kind of element, it’s made up of two up quarks and a down quark that dance around each other in a tightly bound orbit maintained by exchanging gluons. This exchange process is so energetic that most of the mass of the proton (or for that matter, most of the material that makes us up) derives from the energy contained in these gluons—a consequence, as Einstein informed us, of <i>E</i> being equal to <i>mc</i><sup>2</sup>.&nbsp;<br></p><figure data-alt=""><img src="http://static.nautil.us/18083_78b8d6620afcd434a4b7fb41b22e595b.png" width="733" alt=""><figcaption><span><i>Fundamental particles organize into something analogous to a mini periodic table (above).</i></span><span>CERN</span></figcaption></figure><p>So it’s not meaningless to ask what the “size” of the proton is. The study by Grinin’s team highlights the fact that defining this notion remains a rather tricky affair. And, as we’ll see, their results serve to sharpen the mystery as to why other measurement methods researchers have used previously disagree.<br></p><p>A physicist can reasonably infer a proton’s size from the “charge radius”—roughly the averaged spatial extent of quark orbits inside. This quantity is probed in slightly different ways by electrons and muons (another sort of fundamental particle), when you probe their orbital configurations as they form “bound states” with the proton—atomic hydrogen in the case of electrons, muonic hydrogen in the case of muons. Because muons are about 200 times heavier than electrons, their lowest energy orbital configurations are much more tightly bound around the proton than are electrons in atomic hydrogen. Consequently, the differences in the energies of various orbitals in muonic hydrogen are much more sensitive to the proton’s size as well as being more “high pitched” than that of regular atomic hydrogen.&nbsp;</p><p>In other words, similar to how plucking a guitar string at a given tension produces a much higher note were we to fret it open, or at 1/200th its open length, the typical frequencies of the radiation emitted by transitions in muonic hydrogen are about 200 times higher than that in atomic hydrogen. These frequencies relate to something called the Rydberg constant—the tension of the guitar string in the analogy—which appears to be one of the potentially more significant sources of uncertainty proton size-wise. Orbital energy levels depend on both this constant and the charge radius of the proton.</p><p>Proton-size measurements didn’t conflict for decades. Different methods—like measuring the radius by observing electrons orbit within hydrogen atoms, or by scattering energetic electrons off of unbound protons—had converged on a value of 0.875 (give or take 0.006) femtometers. That’s a little less than a trillionth of a millimeter. That convergence was disrupted in 2010, when a paper came out titled, “The size of the proton.” As the researchers reported, <a href="http://www.quantum.physik.uni-potsdam.de/teaching/ss2015/pqt/Pohl2010.pdf" target="_blank">measurements</a> involving orbital configurations in muonic hydrogen returned a value of 0.842, give or take 0.001 femtometers. This may not seem like much of a difference, but it’s the accompanying error bars that matter. The measurements are, individually, so precise that their disagreement is over seven standard deviations—there is less than one in about a trillion chance that the discrepancy could be a statistical fluke.</p><p>There are only two possibilities for the anomalous result if the equipment used in the experiments and their calibrations all check out after careful scrutiny. Either some combination of physical constants, which researchers assume in order to experimentally infer the proton charge radius, isn’t known as accurately as we thought, or there is something different about the way muons interact with protons, compared to electrons, that renders particle physics incomplete.</p><p>The latter possibility, if substantiated, would, of course, cause a flurry of excitement among theoretical physicists to say the least, as it could imply the existence of new forces and particles. Not only would it reshape our understanding of the universe, it would represent a throwback to the days when physicists discovered particles (<a href="https://timeline.web.cern.ch/anderson-and-neddermeyer-discover-muon" target="_blank">such as the muon itself</a>) using equipment that could fit on a proverbial tabletop.</p><div id="inpagesub">
	<p>Get the <span>Nautilus</span> newsletter</p>
<p>
	The newest and most popular articles delivered right to your inbox!
</p>
			<!-- Begin MailChimp Signup Form -->
			




</div><p>Over the past few years, various teams have been attempting to get to the bottom of the matter by looking at different orbital transitions in atomic hydrogen that are sensitive to different combinations of the Rydberg constant and the charge radius. A 2019 <a href="https://science.sciencemag.org/content/365/6457/1007" target="_blank">measurement</a> by a group of researchers at York University in Canada looked at a particular orbital transition that was independent of the value of this constant, finding a value of 0.833 ± 0.010 femtometers, consistent with the smaller value obtained in muonic hydrogen.&nbsp;</p><p>Grinin’s team went a step further. They used a technique known as frequency comb spectroscopy. It involves pulses of laser light that are a superposition of equally spaced frequencies—a ruler in frequency space if you will—that allowed them to look at two different orbital transitions in atomic hydrogen sensitive to two different combinations of the proton size and the Rydberg constant. This permitted them to determine both with unprecedented accuracy. The technique reduced, to only about one part in ten trillion, the observational uncertainties in the frequency of light these transitions emitted—a staggering degree of accuracy by any standard.&nbsp;</p><p>Not only did Grinin’s team find a value for the charge radius of the proton consistent with the value obtained in muonic hydrogen, they inferred a much more precise value for the Rydberg constant. This accounted for some part of the discrepancy seen in other measurements in atomic hydrogen (which presumed a less accurate value).</p><p>It thus appears that the experimental value of the proton charge radius Grinin’s team obtained in atomic hydrogen is converging on the smaller values for the proton charge radius other researchers initially obtained in muonic hydrogen. The smaller value has by now even been adopted as the <a href="https://physics.nist.gov/cgi-bin/cuu/Value?rp" target="_blank">official value</a> on the National Institute of Standards and Technology <a href="https://www.nist.gov/programs-projects/codata-values-fundamental-physical-constants" target="_blank">CODATA</a> list of recommended physical constants—the official almanac for nuclear and atomic chemists and physicists.&nbsp;</p><p>Although this convergence, based on the continued refinement of experimental techniques, did not deliver the new physics some may have been hoping for, even the most despondent theoretical physicist can acknowledge the experimental artistry that seems to be bringing the matter closer to conclusion. What remains unresolved is the reason why measurements, relying on different spectroscopic methods in atomic hydrogen, return different values for the charge radius of the proton. The mystery, and along with it, the diminishing hope of particle physicists, endures for the time being.&nbsp;</p><p>This was enough motivation for a team of theoretical physicists, led by Cliff Burgess at the Perimeter Institute, in Canada, to systematically catalogue all possible sources of theoretical uncertainty in atomic spectroscopy over a <a href="https://inspirehep.net/literature?sort=mostrecent&amp;size=25&amp;page=1&amp;q=find%20a%20burgess,%20c%20and%20a%20zalavari" target="_blank">series of papers</a>. By isolating the ways in which new forces and particles might leave a tell-tale signature, they’ve thrown the gauntlet firmly back to the experimentalists. Future experiments, as always, will be the ultimate arbiter in this matter.&nbsp;</p><p><i>Subodh Patil is an assistant professor at the Lorentz Institute for Theoretical Physics at Leiden University. He tweets on occasion at @_subodhpatil.</i></p>

		</div></div>]]>
            </description>
            <link>http://m.nautil.us/blog/a-breakthrough-in-measuring-the-building-blocks-of-nature</link>
            <guid isPermaLink="false">hacker-news-small-sites-25712475</guid>
            <pubDate>Sun, 10 Jan 2021 08:48:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Log Pattern Recognition: LogMine]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25710991">thread link</a>) | @trungdq88
<br/>
January 9, 2021 | https://sayr.us/log-pattern-recognition/logmine/ | <a href="https://web.archive.org/web/*/https://sayr.us/log-pattern-recognition/logmine/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>When retrieving logs from an application, you are often looking for outliers
(such as a unique error) or what pattern occurs the most. However, defining
patterns manually is time-consuming and requires rigour across projects. In
this series of blog posts, we are going to explore automated Log Pattern
Recognition.</p>

<h2 id="why">Why</h2>

<p>I recently began using <a href="https://docs.datadoghq.com/logs/explorer/patterns/">Datadog Log Pattern</a>
and became addicted to it. Unfortunately, I was not able to find such a feature on
Kibana or a query proxy able to do this analysis. If that does not exist, I
might as well learn more about it and make one!</p>

<p>To my surprise, I did not find as many papers as I thought I would. And the
number of papers with public implementations is even lower. Our series begins
with a paper called <a href="https://www.cs.unm.edu/~mueen/Papers/LogMine.pdf">LogMine: Fast Pattern Recognition for Log Analytics</a>
and <a href="https://github.com/trungdq88/logmine">an implementation made by Tony Dinh</a>
that helped me a lot to study LogMine’s behavior.</p>

<p>To my surprise, I searched for anything related to LogMine on Hacker News
and only found Tony Dinh’s submission.</p>

<h2 id="defining-the-terms-used-in-this-article">Defining the terms used in this article</h2>

<p>Some terms used in this article can have several meanings. In Logmine’s context,
the terms I use mean:</p>
<ul>
  <li><code>log</code>: a log line (<code>string</code>)
    <div><div><pre><code>12:00:00 - My awesome log
</code></pre></div>    </div>
  </li>
  <li><code>pattern</code>: Expression extracted by the algorithm (array of <code>field</code>)
    <div><div><pre><code>[&lt;date&gt;, "-", "My", "awesome", "log"]
</code></pre></div>    </div>
  </li>
  <li><code>field</code>: Part of a <code>pattern</code> / Word from a <code>log</code> (Either a fixed value, a wildcard or a <code>variable</code>)</li>
  <li><code>cluster</code>: represents a group of logs that were identified as close to each
others</li>
  <li><code>variable</code>: a user-provided regex used to identify a known format
    <div><div><pre><code>number = \d+
time = \d{2}:\d{2}:\d{2}
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="an-introduction-to-logmine">An introduction to LogMine</h2>

<p>The LogMine paper describes a method to parse logs, group them into clusters and
extract patterns without any prior knowledge or supervision. LogMine can be
implemented using MapReduce and works in a single pass over the dataset.</p>

<p>LogMine’s approach to grouping logs roughly works by:</p>
<ol>
  <li>Parsing logs into patterns</li>
  <li>Grouping patterns into clusters if the <a href="#clustering">distance</a> between them is small</li>
  <li>Merging clusters into a single pattern</li>
  <li>Repeat from step 2 until you are satisfied</li>
</ol>

<p>LogMine classifies fields into three categories:</p>
<ul>
  <li><code>Fixed value</code>, a field that is constant across all logs in a cluster. This is
detected at the pattern extraction step.</li>
  <li><code>Variable</code>, a field that matches a pattern provided by the user. This is
detected at the pre-processing step.</li>
  <li><code>Wildcard</code>, a field that is not constant across all logs in a cluster. This is
detected at the pattern extraction step.</li>
</ul>

<p>Let’s illustrate how it works, step by step, using three simple logs:</p>

<div><div><pre><code>10:00:00 - [DEBUG] - User 123 disconnected
10:30:00 - [DEBUG] - User 123 disconnected
11:11:11 - [ERROR] - An error occurred while disconnecting user 456
12:12:12 - [DEBUG] - User 789 disconnected
</code></pre></div></div>

<p>We will also define a variable <code>&lt;time&gt;</code> that follows the pattern
<code>\d{2}:\d{2}:\d{2}</code>.</p>

<div>
  <p><strong>Note</strong></p>

  <p>A good practice would be to define more variables to avoid forming two clusters
from logs with low similarity. For instance, we <strong>should</strong> define <code>&lt;number&gt;</code>
as <code>\d+</code>.</p>

  <p>Depending on the pattern of your logs, defining a common field like <code>&lt;log-level&gt;</code>
as <code>[(DEBUG|INFO|WARNING|ERROR)]</code> might end up hiding information. This is
because a <code>WARNING</code> log with an error message might not be that important yet
the same log message with an <code>ERROR</code> level might have the exact information
you would like to highlight.</p>
</div>

<h3 id="log-parsing-and-dense-clusters-identification">Log parsing and dense clusters identification</h3>

<p>The first step is to parse logs into patterns and identify dense clusters. Dense
clusters are clusters where <strong>raw logs are almost identical</strong>.</p>

<p>To do this, we begin by splitting logs into patterns (an array of field). The
separator used by default is any whitespace character.</p>

<table>
  <thead>
    <tr>
      <th><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>i</mi><mi>e</mi><mi>l</mi><msub><mi>d</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">Field_1</annotation></semantics></math></span></span></th>
      <th><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>i</mi><mi>e</mi><mi>l</mi><msub><mi>d</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">Field_2</annotation></semantics></math></span></span></th>
      <th><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>i</mi><mi>e</mi><mi>l</mi><msub><mi>d</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">Field_3</annotation></semantics></math></span></span></th>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
      <th><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>i</mi><mi>e</mi><mi>l</mi><msub><mi>d</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">Field_n</annotation></semantics></math></span></span></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>10:00:00</td>
      <td>-</td>
      <td>[DEBUG]</td>
      <td>-</td>
      <td>User</td>
      <td>123</td>
      <td>disconnected</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>10:30:00</td>
      <td>-</td>
      <td>[DEBUG]</td>
      <td>-</td>
      <td>User</td>
      <td>123</td>
      <td>disconnected</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>11:11:11</td>
      <td>-</td>
      <td>[ERROR]</td>
      <td>-</td>
      <td>An</td>
      <td>error</td>
      <td>occurred</td>
      <td>while</td>
      <td>…</td>
    </tr>
    <tr>
      <td>12:12:12</td>
      <td>-</td>
      <td>[DEBUG]</td>
      <td>-</td>
      <td>User</td>
      <td>789</td>
      <td>disconnected</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
  </tbody>
</table>

<p>Next, we will tokenize the logs and identify variables (regex provided by the
user).</p>

<table>
  <thead>
    <tr>
      <th><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>i</mi><mi>e</mi><mi>l</mi><msub><mi>d</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">Field_1</annotation></semantics></math></span></span></th>
      <th><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>i</mi><mi>e</mi><mi>l</mi><msub><mi>d</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">Field_2</annotation></semantics></math></span></span></th>
      <th><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>i</mi><mi>e</mi><mi>l</mi><msub><mi>d</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">Field_3</annotation></semantics></math></span></span></th>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
      <th>&nbsp;</th>
      <th><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>i</mi><mi>e</mi><mi>l</mi><msub><mi>d</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">Field_n</annotation></semantics></math></span></span></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>&lt;time&gt;</code></td>
      <td>-</td>
      <td>[DEBUG]</td>
      <td>-</td>
      <td>User</td>
      <td>123</td>
      <td>disconnected</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td><code>&lt;time&gt;</code></td>
      <td>-</td>
      <td>[DEBUG]</td>
      <td>-</td>
      <td>User</td>
      <td>123</td>
      <td>disconnected</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td><code>&lt;time&gt;</code></td>
      <td>-</td>
      <td>[ERROR]</td>
      <td>-</td>
      <td>An</td>
      <td>error</td>
      <td>occurred</td>
      <td>while</td>
      <td>…</td>
    </tr>
    <tr>
      <td><code>&lt;time&gt;</code></td>
      <td>-</td>
      <td>[DEBUG]</td>
      <td>-</td>
      <td>User</td>
      <td>789</td>
      <td>disconnected</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
  </tbody>
</table>

<p>I stopped at the word <code>while</code> for readability reasons, but there’s no limit to
the number of fields.</p>

<h4 id="identifying-dense-clusters">Identifying dense clusters</h4>

<p>Once this transformation is done, we will reduce these patterns into dense
clusters. Identifying dense clusters can be seen as a special use-case of the
clustering algorithm: we identify clusters but <strong>we skip the pattern extraction
step as these patterns are nearly identical</strong>.</p>

<p>As such, I’ll first introduce the clustering algorithm and the notion of
distance between patterns.</p>

<h3 id="grouping-patterns-into-clusters">Grouping patterns into clusters</h3>

<p>The clustering algorithm takes a list of patterns and identifies patterns that
are close to each other. Both the Tony Dinh’s implementation and the paper uses
the distance defined as:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Dist</mtext><mo stretchy="false">(</mo><mi>P</mi><mo separator="true">,</mo><mi>Q</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mtext>Min</mtext><mo stretchy="false">(</mo><mtext>len</mtext><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mtext>len</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></munderover><mfrac><mrow><mtext>Score</mtext><mo stretchy="false">(</mo><msub><mi>P</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>Q</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mtext>Max</mtext><mo stretchy="false">(</mo><mtext>len</mtext><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mtext>len</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{Dist}(P,Q) = 1 -
\sum_{i=1}^{\text{Min}(\text{len}(P),\text{len}(Q))}{\frac{\text{Score}(P_i,Q_i)}{\text{Max}(\text{len}(P),\text{len}(Q))}}</annotation></semantics></math></span></span></span></p><p>With P and Q, two patterns.</p>

<p>If the distance between the two patterns is inferior to a threshold <code>MaxDist</code>
defined internally, then the two patterns are considered a member of the same
cluster.</p>

<div>
  <p>This comparison process can be optimized by skipping unnecessary work if the
sum is already greater than our <code>MaxDist</code>.</p>

  <p>This is valid because the elements inside the sum can’t be negative.</p>
</div>

<p>The scoring function proposed in the paper allows for tweaking weights depending
on the field type.</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Score</mtext><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>k</mi><mn>1</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if&nbsp;</mtext><mi>x</mi><mo>=</mo><mi>y</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>and&nbsp;both&nbsp;are&nbsp;fixed&nbsp;values</mtext></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>k</mi><mn>2</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if&nbsp;</mtext><mi>x</mi><mo>=</mo><mi>y</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>and&nbsp;both&nbsp;are&nbsp;variable</mtext></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>otherwise</mtext></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\text{Score}(x, y) =
\begin{cases}
  k1 &amp;\text{if } x=y &amp;\text{and both are fixed values}\\
  k2 &amp;\text{if } x=y &amp;\text{and both are variable} \\
  0 &amp;\text{otherwise}
\end{cases}</annotation></semantics></math></span></span></span></p><p>In order to keep performance, the scoring function can’t return a negative
value. Therefore <code>k1</code> and <code>k2</code> can’t be negative.</p>

<div>
  <p><strong>Note</strong></p>

  <p>The definition of this scoring function seems to have left some place for interpretation:</p>
  <ul>
    <li>Tony Dinh’s implementation <a href="https://github.com/trungdq88/logmine/blob/a7595c6a0b313b43969199c18465cc8bec3b57d1/src/line_scorer.py#L29">checks for equality</a>
if both value are fixed values.</li>
    <li><a href="https://github.com/logpai/logparser/blob/master/logparser/LogMine/LogMine.py">LogPai’s LogParser</a> dropped the concept of variables and uses k2 to tune wildcards weight</li>
  </ul>

</div>

<p>Let’s define <code>k1=1</code> and <code>k2=1</code>, these are the weight used by Tony Dinh’s
implementation and recommended by the original paper.</p>

<h4 id="applying-the-clustering-algorithm">Applying the clustering algorithm</h4>

<p>For this example, I will use the original paper recommendation (<code>MaxDist=0.01</code>).
The MaxDist parameter is used to tune the algorithm sensitivity: the higher
<code>MaxDist</code>, the more patterns are detected.
If <code>MaxDist</code> is too high, patterns are grouped into very large clusters and
pattern extraction become meaningless.</p>

<table>
  <thead>
    <tr>
      <th>Fields from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>o</mi><msub><mi>g</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">Log_1</annotation></semantics></math></span></span></th>
      <th>Fields from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>o</mi><msub><mi>g</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">Log_2</annotation></semantics></math></span></span></th>
      <th>Score</th>
      <th>Sum (Total)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>&lt;time&gt;</code></td>
      <td><code>&lt;time&gt;</code></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>7</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{7}</annotation></semantics></math></span></span></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>7</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{7}</annotation></semantics></math></span></span></td>
    </tr>
    <tr>
      <td><code>-</code></td>
      <td><code>-</code></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>7</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{7}</annotation></semantics></math></span></span></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>2</mn><mn>7</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{2}{7}</annotation></semantics></math></span></span></td>
    </tr>
    <tr>
      <td><code>[DEBUG]</code></td>
      <td><code>[DEBUG]</code></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>7</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{7}</annotation></semantics></math></span></span></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>3</mn><mn>7</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{3}{7}</annotation></semantics></math></span></span></td>
    </tr>
    <tr>
      <td><code>-</code></td>
      <td><code>-</code></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>7</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{7}</annotation></semantics></math></span></span></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>4</mn><mn>7</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{4}{7}</annotation></semantics></math></span></span></td>
    </tr>
    <tr>
      <td><code>User</code></td>
      <td><code>User</code></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>7</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{7}</annotation></semantics></math></span></span></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>5</mn><mn>7</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{5}{7}</annotation></semantics></math></span></span></td>
    </tr>
    <tr>
      <td><code>123</code></td>
      <td><code>123</code></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>7</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{7}</annotation></semantics></math></span></span></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>6</mn><mn>7</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{6}{7}</annotation></semantics></math></span></span></td>
    </tr>
    <tr>
      <td><code>disconnected</code></td>
      <td><code>disconnected</code></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>7</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{7}</annotation></semantics></math></span></span></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>7</mn><mn>7</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{7}{7}</annotation></semantics></math></span></span></td>
    </tr>
  </tbody>
</table>

<p>The two logs belong to the same cluster as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mfrac><mn>7</mn><mn>7</mn></mfrac><mo>&lt;</mo><mtext>MaxDist</mtext></mrow><annotation encoding="application/x-tex">1 - \frac{7}{7} &lt; \text{MaxDist}</annotation></semantics></math></span></span>.</p>

<table>
  <thead>
    <tr>
      <th>Fields from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>o</mi><msub><mi>g</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">Log_1</annotation></semantics></math></span></span></th>
      <th>Fields from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>o</mi><msub><mi>g</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">Log_3</annotation></semantics></math></span></span></th>
      <th>Score</th>
      <th>Sum (Total)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>&lt;time&gt;</code></td>
      <td><code>&lt;time&gt;</code></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>11</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{11}</annotation></semantics></math></span></span></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>11</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{11}</annotation></semantics></math></span></span></td>
    </tr>
    <tr>
      <td><code>-</code></td>
      <td><code>-</code></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>11</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{11}</annotation></semantics></math></span></span></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>2</mn><mn>11</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{2}{11}</annotation></semantics></math></span></span></td>
    </tr>
    <tr>
      <td><code>[DEBUG]</code></td>
      <td><code>[ERROR]</code></td>
      <td>0</td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>2</mn><mn>11</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{2}{11}</annotation></semantics></math></span></span></td>
    </tr>
    <tr>
      <td><code>-</code></td>
      <td><code>-</code></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>11</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{11}</annotation></semantics></math></span></span></td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>3</mn><mn>11</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{3}{11}</annotation></semantics></math></span></span></td>
    </tr>
    <tr>
      <td><code>User</code></td>
      <td><code>An</code></td>
      <td>0</td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>3</mn><mn>11</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{3}{11}</annotation></semantics></math></span></span></td>
    </tr>
    <tr>
      <td><code>123</code></td>
      <td><code>error</code></td>
      <td>0</td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>3</mn><mn>11</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{3}{11}</annotation></semantics></math></span></span></td>
    </tr>
    <tr>
      <td><code>disconnected</code></td>
      <td><code>occurred</code></td>
      <td>0</td>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>3</mn><mn>11</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{3}{11}</annotation></semantics></math></span></span></td>
    </tr>
  </tbody>
</table>

<p>The two logs do not belong to the same cluster as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mfrac><mn>3</mn><mn>11</mn></mfrac><mo>&gt;</mo><mtext>MaxDist</mtext></mrow><annotation encoding="application/x-tex">1 - \frac{3}{11} &gt; \text{MaxDist}</annotation></semantics></math></span></span>.</p>

<div>
  <p><strong>Note</strong></p>

  <p>As we are using <code>MaxDist=0.01</code>, and since the logs in my example are very short,
we will only identify logs that are identical. For this reason, <strong>we can use the
clustering algorithm to identify dense clusters</strong> by skipping the pattern
recognition step.</p>
</div>

<h3 id="pattern-detection">Pattern detection</h3>

<p>In sequential (as opposed to MapReduce) mode, each time a pattern is inserted
in a cluster, LogMine will try to extract patterns from them. LogMine knows
only how to identify two types:</p>
<ul>
  <li>Fixed values</li>
  <li>Wildcards</li>
</ul>

<p>While it seems rather strict, remember that the early pattern detection during
tokenization already took care of identifying user-defined patterns.</p>

<div>
  <p>Generating the pattern each time a new pattern is added allows LogMine to only
keep one representative for each cluster. This is a very important factor to
reduce memory usage.</p>

  <p><strong>MapReduce behavior is described later in this article.</strong></p>
</div>

<p>As patterns in the same cluster can have a different number of fields, the paper
uses the <a href="https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm">Smith-Waterman</a>
algorithm to align them. The Smith-Waterman is mainly used in Bioinformatics to
align sequences. It is interesting to see an algorithm like this applied to
log pattern recognition.</p>

<p>Once the two patterns are aligned, we compare one by one the field:</p>
<ul>
  <li>If the value is equal, we assign a fixed value</li>
  <li>If the value is different, we assign a wildcard pattern</li>
</ul>

<p>The Smith-Waterman algorithm adds placeholders to align logs. These placeholders
are <strong>never equal</strong> to a field value. This means that aligned parts of a log are
always identified as wildcards.</p>

<p>The output of the pattern detection algorithm is a <strong>new list of patterns</strong>.</p>



<p>Let’s assume that the previous algorithm identified a cluster with two patterns:</p>
<div><div><pre><code>&lt;time&gt; - [DEBUG] - User 123 disconnected
&lt;time&gt; - [DEBUG] - User 789 disconnected
</code></pre></div></div>

<p>To stay consistent with how they were presented earlier, we will call them
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>o</mi><msub><mi>g</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">Log_1</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>o</mi><msub><mi>g</mi><mn>4</mn></msub></mrow><annotation encoding="application/x-tex">Log_4</annotation></semantics></math></span></span>.</p>

<table>
  <thead>
    <tr>
      <th><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>o</mi><msub><mi>g</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">Log_1</annotation></semantics></math></span></span></th>
      <th><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>o</mi><msub><mi>g</mi><mn>4</mn></msub></mrow><annotation encoding="application/x-tex">Log_4</annotation></semantics></math></span></span></th>
      <th>Result</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>&lt;time&gt;</code></td>
      <td><code>&lt;time&gt;</code></td>
      <td><code>&lt;time&gt;</code></td>
    </tr>
    <tr>
      <td><code>-</code></td>
      <td><code>-</code></td>
      <td><code>Fixed value</code></td>
    </tr>
    <tr>
      <td><code>[DEBUG]</code></td>
      <td><code>[DEBUG]</code></td>
      <td><code>Fixed value</code></td>
    </tr>
    <tr>
      <td><code>-</code></td>
      <td><code>-</code></td>
      <td><code>Fixed value</code></td>
    </tr>
    <tr>
      <td><code>User</code></td>
      <td><code>User</code></td>
      <td><code>Fixed value</code></td>
    </tr>
    <tr>
      <td><code>123</code></td>
      <td><code>456</code></td>
      <td><code>Wildcard</code></td>
    </tr>
    <tr>
      <td><code>disconnected</code></td>
      <td><code>disconnected</code></td>
      <td><code>Fixed value</code></td>
    </tr>
  </tbody>
</table>

<h4 id="result-of-applying-the-smith-waterman-algorithm">Result of applying the Smith-Waterman algorithm</h4>

<p>Because examples in this article would have gotten very complex, I decided to
not use non-aligned logs in my examples. However, you might be curious as to how
the Smith-Waterman works.</p>

<p>On the implementation, LogMine would have aligned the two following patterns by
adding a <code>None</code> value in a field. For instance:</p>
<div><div><pre><code>&lt;time&gt; - [DEBUG] - User 123 disconnected
&lt;time&gt; - [DEBUG] - User 123 has disconnected
</code></pre></div></div>
<p>would become:</p>
<div><div><pre><code>&lt;time&gt; - [DEBUG] - User 123 &lt;None&gt; disconnected
&lt;time&gt; - [DEBUG] - User 123 has disconnected
</code></pre></div></div>

<div>
  <p><strong>The Smith-Waterman does not understand patterns</strong>. The Smith-Waterman uses
the same scoring function as the clustering algorithm. As such, if two fields
are not equal, there is no guarantee that the placeholder will be inserted
properly.</p>

  <p>If we tweak our previous example to:</p>
  <div><div><pre><code>&lt;time&gt; - [DEBUG] - User 123 disconnected
&lt;time&gt; - [DEBUG] - User 456 has disconnected
</code></pre></div>  </div>

  <p>The Smith-Waterman algorithm would transform it to:</p>
  <div><div><pre><code>&lt;time&gt; - [DEBUG] - User &lt;None&gt; 123 disconnected
&lt;time&gt; - [DEBUG] - User 456 has disconnected
</code></pre></div>  </div>

  <p>The alignment was added <strong>before</strong> the user identifier.</p>
</div>

<h3 id="repeat-until-satisfied">Repeat until satisfied</h3>

<p>As the pattern extraction algorithm returns a list of patterns, we can relax
<code>MaxDist</code> and feed this list back into the clustering algorithm.</p>

<p>In <strong>3.4 Hierarchy of Patterns</strong>, the paper describes a process to generate a
hierarchy of possible patterns from very strict to …</p></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://sayr.us/log-pattern-recognition/logmine/">https://sayr.us/log-pattern-recognition/logmine/</a></em></p>]]>
            </description>
            <link>https://sayr.us/log-pattern-recognition/logmine/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25710991</guid>
            <pubDate>Sun, 10 Jan 2021 05:50:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A guide to SQL interview questions]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25709870">thread link</a>) | @data4lyfe
<br/>
January 9, 2021 | https://www.interviewquery.com/blog-sql-interview-questions/ | <a href="https://web.archive.org/web/*/https://www.interviewquery.com/blog-sql-interview-questions/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
                <div>
                    <p><strong>Table of Contents</strong></p><!--kg-card-begin: html-->
<!--kg-card-end: html--><h3 id="introduction">Introduction</h3><p>SQL is the good old friend that has always worked. It’s something you always come back to, even as Pandas, Julia, Spark, Hadoop, and NoSql attempt to dethrone and replace SQL as the new de-facto data tool.</p><p>Eventually though, they all fail in the face of the consistently reliable SQL. And that's why SQL continues to get asked in interviews. </p><blockquote><strong>A note before we start...</strong></blockquote><blockquote>This guide should be used for anyone who is preparing for an interview in which they know SQL will show up. This guide<strong> is not</strong> a search engine optimized listacle (top 50 sql questions for 2021...really?).</blockquote><blockquote>Rather, this is <strong>real advice and REAL interview questions and exercises </strong>gathered from hundreds of data scientists, engineers, and analysts. We sprinkle exercises throughout this post after learning concepts. Be sure to try attempting the questions first before we walk through solving them. &nbsp;</blockquote><blockquote>Lastly, if you enjoy this article, <strong>please give us a share</strong> and check out our <a href="https://www.interviewquery.com/courses/data-science-course">SQL course</a> that goes a little deeper with more exercises and problems. </blockquote><h2 id="1-why-does-sql-show-up-on-the-interview">1. Why does SQL show up on the interview?</h2><figure><img src="https://blog.interviewquery.com/content/images/2021/01/image-6.png" alt="" srcset="https://blog.interviewquery.com/content/images/2021/01/image-6.png 600w"></figure><p>SQL allows data scientists and engineers to do a couple of important things.</p><p>One is to <strong>effectively store and retrieve information at scale for analytics</strong>. Even though Google Sheets allows users to easily manipulate and visualize data, it cannot store and scale like a SQL database can. Other popular programs –namely Hadoop and Spark– can scale much further than SQL, but still don’t have a clean and easy-to-use language like SQL to retrieve data efficiently.</p><p>Another great thing about SQL is that understanding the fundamentals bridges the gap between <strong>engineering and data science</strong>. Knowing SQL well gives you a competitive edge over any other candidate, whether you're competing for a position as a product manager, software engineer, or even as a business analyst. Having the skillset to write and pull your own queries is like being a magician that can come up with analyses out of thin air. </p><p>And at the end of the day, you could just be really good at SQL if you wanted to and make tons of money creating ETL jobs or pulling dashboards with efficiency. That's how valued SQL is. </p><h3 id="how-often-does-sql-show-up-in-interviews">How often does SQL show up in interviews?</h3><p>One prevailing question is how often SQL shows up in interviews. </p><p>At Interview Query, we analyzed a dataset of Glassdoor data science interview experiences and responses submitted by our users. The analysis came back that SQL was asked:</p><ul><li><a href="https://www.interviewquery.com/interview-experiences/facebook/data-scientist" rel="nofollow">70% of the time during Facebook’s data science interviews</a></li><li><a href="https://www.interviewquery.com/interview-experiences/Amazon/business-analyst" rel="nofollow">94% of the time during Amazon’s business intelligence and analyst interviews</a></li></ul><figure><img src="https://blog.interviewquery.com/content/images/2021/01/image-7.png" alt="" srcset="https://blog.interviewquery.com/content/images/size/w600/2021/01/image-7.png 600w, https://blog.interviewquery.com/content/images/size/w1000/2021/01/image-7.png 1000w, https://blog.interviewquery.com/content/images/2021/01/image-7.png 1262w" sizes="(min-width: 720px) 720px"><figcaption><a href="https://www.interviewquery.com/interview-experiences/facebook/data-scientist">Skills tested in Facebook's Data Science Interview as of 2021</a></figcaption></figure><p>Due to its nature in being able to get and manipulate your own data, this is by far the most important skill now towards nabbing a data science position. And while pandas and other languages are useful, note that <a href="https://www.interviewquery.com/blog-why-your-data-scientist-interviewer-wont-ask-pandas-questions/" rel="nofollow">SQL will always be the one that matters</a>.</p><h2 id="2-strategies-for-the-live-sql-interview">2. Strategies for the live SQL interview</h2><p>Let's go over the common strategies when tackling SQL interview questions. </p><p><strong>1.Repeat the problem statement</strong></p><p>When presented with a SQL question, listen carefully to the problem description and repeat back what you think the crux of the problem is. The interviewer can then help verify if your understanding is correct.</p><p><strong>2. Understand the edge cases</strong></p><p>If time permits, write out a base case and an edge case to show that you understand the problem. For example: if the interviewer asks you to pull the average number of events per user per day, write out an example scenario where you're verifying this metric. </p><p>Do duplicate events matter? Are we looking at distinct users? These are questions we need to clarify. </p><p><strong>3. Try working backwards if the problem is tricky</strong></p><p>Sketching out what the output of the SQL question will look like is a great strategy towards solving the problem. Usually, if I know what the end output table is supposed to look like, I can work backwards from there on what functions need to be applied before. </p><p>For example, if the output looks like this:</p><pre><code>date        | average events per user
------------+-----------------------
2021-12-01  |  3.5
2021-12-02  |  4.0</code></pre><p>I know that the table before this aggregation would have to look something like this.</p><pre><code>date       | event | user_id
-----------+-------+--------
2021-12-01 | click | 1
2021-12-01 | view  | 1
......</code></pre><p>And then, I can figure out what functions I should use to get to my desired output!</p><p><strong>4. Pattern match to different functions</strong></p><p>As you practice more and more SQL exercises, what you'll find is that many SQL problems follow similar patterns. There are techniques we can use in SQL, like utilizing <code>HAVING</code> on aggregations, self-joins and cross-joins, and applying window functions. But, additionally, we'll see problems that run in a similar vein. </p><p>For example, writing a query to get the second highest salary or writing a query to isolate every fifth purchase by a user utilizes the same <code>RANK</code> function in SQL. </p><p>Understanding the commonalities between questions will help you understand the first step to solving SQL questions faster because you can re-use similar code and stitch together techniques on top of each other. </p><p><strong>5. Start writing SQL</strong></p><p>Finally, it's important to just start writing SQL. It's better to start writing an imperfect solution vs trying to perfectly understand the problem or trying to perfect the solution on the first try. </p><p>Verbalize your assumptions and what you're doing as you write SQL and your interviewer can then be put on the same page as you. </p><h2 id="3-the-7-different-sql-interview-questions">3. The 7 different SQL interview questions</h2><p>SQL questions asked during interviews can vary widely across companies, but even more so across positions. You won't see data scientists asked the same SQL questions as software engineers, and that's because data scientists have to write different types of queries compared to software engineers. </p><p>Generally, each SQL interview question can be bucketed into these categories:</p><ul><li>Definition based SQL questions</li><li>Basic SQL questions</li><li>Reporting and metrics SQL questions</li><li>Analytics SQL questions</li><li>ETL SQL questions</li><li>Database design questions</li><li>Logic based SQL questions</li></ul><p>In this next section, we'll go over which types of SQL questions are expected for different roles and what those different kinds of SQL questions are in detail. </p><h2 id="4-sql-questions-for-data-scientists-and-analysts">4. SQL questions for data scientists and analysts</h2><p>SQL interview questions for data scientists and data analysts will likely show up in three parts of the interview process: the technical round, the take-home challenge, and the onsite interview. </p><p>The technical round and take-home challenge will usually consist of SQL questions <strong>designed to filter out candidates</strong>. Since SQL is commonly used as a filter mechanism for data scientists, it's important to perform well on this part of the interview in order to demonstrate competence. </p><p>Depending on what type of data science role you're interviewing for, you'll find that most SQL questions will be split into these three types:</p><ul><li>Basic SQL Interview Questions</li><li>Reporting and Metrics SQL Interview Questions</li><li>Analytics SQL Interview Questions</li></ul><h3 id="basic-sql-interview-questions">Basic SQL Interview Questions</h3><p>Basic SQL questions are what they sound like. These questions will be generally easy and focus on assessing if you know the basics. </p><p><strong>Definition based SQL questions </strong>are grouped into this category because they're super easy to learn. All you have to do is study a list of definitions of SQL terms and applications. These questions will include understanding the differences between joins, what kinds of aggregations exist, and knowing the basic functions like <code>CASE WHEN</code> or <code>HAVING</code>. </p><p>Basic SQL interview questions that involve a user actually writing a query are slightly different. These will involve getting the <code>COUNT</code> of a table, knowing what the <code>HAVING</code> clause does, and figuring out how to utilize <code>LEFT JOIN</code> versus <code>INNER JOIN</code> to give you the values that you need.</p><blockquote>Read more on the the <a href="https://www.interviewquery.com/blog-three-sql-questions-you-must-know-to-pass/">basic concepts you need to know to pass your data science interview</a> here. </blockquote><figure><a href="https://www.interviewquery.com/blog-three-sql-questions-you-must-know-to-pass/"><div><p>Three SQL Concepts for your Data Scientist Interview</p><p>I’ve interviewed a lot of data scientist candidates and have found there are a a lot of SQL interview questions for data science that eventually boil down to three generalized types of conceptual understandings.</p><p><img src="https://blog.interviewquery.com/favicon.ico"><span>Interview Query Blog</span></p></div><p><img src="https://blog.interviewquery.com/content/images/2020/02/sql_join.jpeg"></p></a></figure><p><strong>Basic SQL Concepts to Review</strong></p><ul><li>What's the difference between a <code>LEFT JOIN</code> and an <code>INNER JOIN</code>?</li><li>When would you use <code>UNION</code> vs <code>UNION ALL</code>? What if there were no duplicates?</li><li>What's the difference between <code>COUNT</code>and <code>COUNT DISTINCT</code>?</li><li>When would you use a <code>HAVING</code> clause versus a <code>WHERE</code> clause?</li></ul><p><strong>Basic SQL Question Example:</strong></p><figure><img src="https://blog.interviewquery.com/content/images/2021/01/image.png" alt="" srcset="https://blog.interviewquery.com/content/images/size/w600/2021/01/image.png 600w, https://blog.interviewquery.com/content/images/size/w1000/2021/01/image.png 1000w, https://blog.interviewquery.com/content/images/2021/01/image.png 1306w" sizes="(min-width: 720px) 720px"></figure><blockquote><em>We're given two tables, a <code>users</code> table with demographic information and the neighborhood they live in and a <code>neighborhoods</code> table.</em></blockquote><blockquote><em>Write a query that returns all of the neighborhoods that have 0 users.</em></blockquote><p>Try answering this question with our <a href="https://www.interviewquery.com/questions/empty-neighborhoods">interactive SQL editor</a>.</p><p><strong>Here's a hint:</strong></p><p><em>Our predicament is to find all the neighborhoods without users that live in them. This means we have to introduce a <strong>concept of existence of a field in one table, while not existing in another.</strong></em></p><p><em>For example, let's say we generate some fake data of user's and the neighborhoods they live in. We would expect it to look something like this. </em></p><pre><code>neighborhoods.name  | users.id
____________________|__________
castro              | 1
castro              | 2
cole valley         | null
castro heights      | 3
sunset heights      | 4</code></pre><p><em>We see each user from one to four is appropriately placed in their respective neighborhood except for the neighborhood of Cole Valley. That's the neighborhood we're targeting for returning in our query. </em></p><p><em>Strategies: whenever the question asks about finding values with 0 something (users, employees, posts, etc..), immediately think of the concept of <strong><code>LEFT JOIN</code></strong>! An inner join finds any values that are in both tables, <strong>a left join keeps only the values in the left table</strong>.</em></p><p><em>Our predicament is to find all the neighborhoods without users. To do this, we must do a left join from the <code>neighborhoods</code> table to the <code>users</code> table.</em></p><p><em>If we then add in a where condition of <strong><code>WHERE users.id IS NULL</code></strong>, we will get every single neighborhood …</em></p></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.interviewquery.com/blog-sql-interview-questions/">https://www.interviewquery.com/blog-sql-interview-questions/</a></em></p>]]>
            </description>
            <link>https://www.interviewquery.com/blog-sql-interview-questions/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25709870</guid>
            <pubDate>Sun, 10 Jan 2021 04:15:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Platform Is the Enemy]]>
            </title>
            <description>
<![CDATA[
Score 198 | Comments 77 (<a href="https://news.ycombinator.com/item?id=25708099">thread link</a>) | @nomdep
<br/>
January 9, 2021 | https://danielbmarkham.com/the-platform-is-the-enemy/ | <a href="https://web.archive.org/web/*/https://danielbmarkham.com/the-platform-is-the-enemy/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div id="post-body">
        <p>The premise of the movie "Idiocracy" is simple: in the future mankind has de-evolved into morons. Technology does so much for everybody that nobody knows how it all works anymore. &nbsp;If we can't fix it, we're all going to die.</p><p>One character asks the other what he likes, The answer is money.</p><p>"I can't believe you like money too!" the first character says without irony, "We should hang out!"</p><figure><img src="https://cdn.danielbmarkham.com/2021/01/JRJcXbeoSOlkRRzbC7HJ_idiocracy.jpg" alt=""></figure><p>The gag here is that of course, most everybody likes money. If you reduce all of your life enough, it's just food, sex, money, and looking cool. But who would want to do that? Over the centuries, humans have created massively-complex societies because everybody has different things they like doing and thinking about, but all of that complexity can be reduced to, well, an idiocracy if you try hard enough.</p><p>The movie, however, is just a joke, right? We would never allow that to happen, of course, because that's not the goal of technology. Technology's goal is to make us better, not dumber.</p><p>Wait one. Is that true? What <em>is</em> the goal of technology, anyway? Has anybody ever clearly stated it?</p><p>Recently I've heard two goals:</p><ol><li>The goal of technology is to become a <strong>brain extension</strong>, <em>helping you to decide what to do</em> and then helping you get it done.</li><li>The goal of technology is to become a <strong>hand-held power tool</strong>, helping you accomplish the things you've <em>already decided to do</em></li></ol><p>That's not the same thing. It turns out the difference is critical.</p><p>The old goal was much simpler: make something people want. I like that goal! It boils down the job of creating technology to the most important parts, need and ability. But was that sustainable? At the end of the day, don't we always end up making some combination of stuff that either helps us <em>make decisions</em> or helps us <em>implement decisions</em> we've already made? And aren't the two fundamentally incompatible in a future society?</p><p>Yelp tells you which restaurant to go to. Your GPS automatically takes you there. These are not just different problems, they're different <em>kinds of problems</em>. Getting from point A to point B is a matter of math and geometry. Which restaurant is the best tonight? You could spend hours debating that with friends.</p><p>If you reduce anything down enough it becomes idiotic. Each piece of technology we deploy can have the goal of helping us do what we've already decided or helping us decide what to do. The first option leaves the thinking up to us. The second option "helps" us think.</p><figure><iframe width="267" height="200" src="https://www.youtube.com/embed/sZHCVyllnck?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><p>You like money too? Wow! I like money! We should hang out!</p><p>Human brains are not computers. Brains are designed to help us survive and pass on our genes using the minimum amount of energy available. If the GPS takes me where I'm going, I don't need to know how to use maps anymore. So I stop knowing how to use maps. Dump those neurons, they're not needed. If Yelp picks the restaurants for me enough, I stop having nuanced preferences about restaurants. That energy expenditure is no longer needed for survival and reproduction. Dump those neurons. Over time people stop caring about the tiny details of what the difference is between a good and a great restaurant. Yelp handles that.</p><p>For some folks, who cares? It's food. Go eat it. For other folks, picking the right place can be a serious undertaking, worthy of heavy thought and consideration. But if over the years apps like Yelp boil all of that down to four or five stars, then our collective brain is not going to bother with it. Human brains are not computers. If computers do the work for us, we turn off those neurons and save energy.</p><p>Meanwhile, on social media there's currently this huge discussion. One bunch of folks says that social media is being overbearing in its censorship of fringe and sometimes hateful opinions. The other bunch of folks says social media is a festering sore full of people who are ugly, hateful, and abusive to those weakest among us. The community has to set standards.</p><p>There doesn't have to be a right and wrong here. I think the crucial thing to to understand that both sides can be entirely correct. We are dealing with the same kind of question.</p><p>All three of these topics -- whether humanity is becoming idiots or not, what the ultimate goal of technology is or should be, and how social media should work -- are intricately related. They're related because of this: <em>the platform is the enemy</em>.</p><p>The minute we create a platform for something, whether it's rating movies, tracking projects, or chatting with friends about work, as that platform takes over mindshare, <em>the assumption becomes that this is a solved problem</em>.</p><p>The telephone was great. Once we had the telephone, people didn't have to worry about how to talk to people far away anymore. Just pick up the phone. Solved problem.</p><p>Facebook is great. Once we had Facebook, people didn't have to worry about how to interact with their friends in a social setting anymore. Just click on the little FB notification (Which seems to be always flashing for some reason to get my attention) Solved problem?</p><p>But these are entirely different things! With the phone, I know who I want to call and why. I push buttons and we are connected. The tech helps me do what I've already decided to do. With Facebook, on the other hand, they get paid to show me things in a certain order. The premise is that I'm waiting (or "exploring" if you prefer) until I find something to interact with. The phone is a tool for me to use. I am the tool Facebook is using. I am no longer acting. I am reacting.</p><p>And even if they weren't paid, interacting with friends socially is an extremely complex affair. What kind of mood are they in? What's their life history? What things are bad to bring up? How does their body language look? Facebook's gimmick is "Hey, we've reduced all of this to bits and bytes, and we'll even show you what bits and bytes to look at next!"</p><p>Solved problem.</p><p>Many, many people do not use the internet, the internet uses them. And this percentage is constantly growing.</p><figure><img src="https://cdn.danielbmarkham.com/2021/01/9EBNeUmSy6TMDUpKTLL6_terminator-robot.jpg" alt=""></figure><p>Just like the restaurant example, maybe that's fine. I have friends, I have opinions, who cares? It's all idle chat anyway.</p><p>That logic can be true for a bunch of things, but can't be true for <em>everything</em>. Otherwise, at some point 100 years from now, we're comparing our life values and end up saying something like "I like money too". Everything can't all be reduced down to the lowest common denominator. If it does, we all die.</p><p>Life is not a bit or byte, a number to be optimized. It's meaning we define ourselves, in ways we should not quantize.</p><p>Platforms, by their very nature, constantly send out the subtle message: <em>This is a solved problem. No further effort on your part is required here. No thinking needed.</em> Platforms resist change. They resist their own evolution by subtly poisoning the discussion before it even starts.</p><p>Are restaurant choices more or less important than which movie to watch tonight? There's no right or wrong answer to these questions. We have nice categories like restaurants and movies because currently people consider those things to be different kinds of choices. But why? If the algorithm is king, why shouldn't an algorithm determine both of those things for me? And if it does, why should I bother with worrying about which category is which?</p><p>Human brains are not computers. Let the platform decide. Energy not needed. Dump those neurons.</p><p>This is the more important point. It's not that the platforms turn what might be complex things into simple numbers, or even that they monetize attention. It's that by turning everything into numbers, over time they destroy the distinction between the categories entirely. Platforms are the enemy because they resist analysis in the areas they dominate.</p><p>Platforms turn into settled fact things that should be open for debate, like whether or not Taco Bell is a Mexican restaurant, or whether Milo is an artist with something useful to tell us. (I'm going with "no" and "no" for both of these.) More dangerously, they do the work of deciding <em>what categories various things go into</em>. This category over here is important. That category over there is not. We all make these decisions, and they're all different, and the categories each of us pays careful attention to and loves obsessing about are all different, and because we all have different viewpoints and priorities humankind advances in thousands of directions simultaneously. We survive. We evolve.</p><p>Twitter has to decide whether PERSON_X can speak or not because on the Twitter platform, that question has to have a yes or no answer based on the person. Twitter's category for deciding who can speak is "who is that?" Is that the right category for social conversations? For political conversations? For conversations about philosophy? Math? Who knows? Who cares? Twitter has decided. Solved problem.</p><p>Everybody has different things they like doing and thinking about. Different conversations and audiences have different criteria. Some problems should never be solved. Or rather more directly, some problems should never have a universal answer.</p><p>An aside: We see the same thing in programming. One bunch of folks creates various platforms in order to do the thinking for another bunch of folks. Sometimes these platforms take off and become industry standards. That's quite rare, however. Most of the time we end up training morons who can weakly code against the platform but can't reason effectively about the underlying architecture or reason for the platform to exist in the first place. In our desire to help, we harm the very people we're trying to assist -- by subtly giving them the impression that this is a solved problem. Programmers are just a decade or so ahead of the rest of us.</p><p>Popular platforms aren't just a danger economically because they control commerce. They're not just a danger politically because they selectively control and amplify political discourse. They're an extinction-level, existential danger to humans because they prevent people from seriously considering what kinds of categories are important in …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://danielbmarkham.com/the-platform-is-the-enemy/">https://danielbmarkham.com/the-platform-is-the-enemy/</a></em></p>]]>
            </description>
            <link>https://danielbmarkham.com/the-platform-is-the-enemy/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25708099</guid>
            <pubDate>Sun, 10 Jan 2021 02:23:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Podcast on innovation systems, with Ben Reinhardt]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25707442">thread link</a>) | @willbobaggins
<br/>
January 9, 2021 | https://narrativespodcast.com/2020/12/21/21-innovation-systems-with-ben-reinhardt/ | <a href="https://web.archive.org/web/*/https://narrativespodcast.com/2020/12/21/21-innovation-systems-with-ben-reinhardt/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1985">

                    
                    <div>
                        
<p>In this episode, we talk with Ben Reinhardt about different innovation systems, how to create more sci-fi technology a reality, and why our research institutions are not as effective as they used to be. You can check out Ben’s work at https://benjaminreinhardt.com/about/.</p>



<div><p>Some things mentioned in the episode: </p><p><a href="https://slatestarcodex.com/2020/05/12/studies-on-slack/">Studies on Slack</a></p></div>



<p><a href="https://en.wikipedia.org/wiki/Donald_Braben">Don Braben</a></p>



<p><a href="https://en.wikipedia.org/wiki/DARPA">DARPA</a></p>



<p>Ben’s <a href="https://twitter.com/ben_reinhardt?lang=en">Twitter</a></p>



<figure></figure>
                                            </div>

                </article></div>]]>
            </description>
            <link>https://narrativespodcast.com/2020/12/21/21-innovation-systems-with-ben-reinhardt/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25707442</guid>
            <pubDate>Sun, 10 Jan 2021 01:30:21 GMT</pubDate>
        </item>
    </channel>
</rss>
