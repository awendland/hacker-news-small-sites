<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 5]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 5. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Sun, 01 Nov 2020 12:22:53 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-5.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Sun, 01 Nov 2020 12:22:53 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[SM2 (Chinese) National Secret algorithm is accepted into Linux kernel]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24938686">thread link</a>) | @hyiltiz
<br/>
October 29, 2020 | https://www.codetd.com/en/article/12031985 | <a href="https://web.archive.org/web/*/https://www.codetd.com/en/article/12031985">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                        
                        <p><span>
                            <a href="https://www.codetd.com/en/cat/17746/1">News</a>
                        </span>
                            <span>2020-10-27 14:44:54</span>
                            <span>views: null</span>
                        </p>

                    </div><div><div> 
  
 <p><span><span>On October 25, a developer posted that the SM2 national secret algorithm was finally accepted by the Linux kernel community. </span><span>The author stated that the SM2 patch has been updated to version v7. This version of the patch was finally accepted by the community. It has been </span></span><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/log/?qt=author&amp;q=Tianjia+Zhang"><span><span>merged into the 5.10-rc1 of the Linux mainline</span></span></a><span><span> . If nothing </span><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/log/?qt=author&amp;q=Tianjia+Zhang"><span>else</span></a><span> , it will be officially released in the 5.10 kernel version.</span></span></p> 
 <p><span><span>National Secret is the abbreviation of National Commercial Encryption. The National Encryption Administration Bureau formulates algorithm standards, and it also formulates a large number of product and interface specifications and application scenarios. </span><span>Since 2012, the State Cryptography Administration has successively published SM2/SM3/SM4 and other cryptographic algorithm standards and their application specifications in the form of the "People's Republic of China Password Industry Standards". </span><span>Among them, "SM" stands for "commercial secret", which is a cryptographic technology used for commercial use that does not involve state secrets.</span></span></p> 
 <p><span><span>According to the author, the current Linux kernel has well supported the SM3 and SM4 algorithms, thanks to the widespread use of wireless LAN standards. </span><span>However, the SM2 algorithm and the national secret certificate have not been supported for a long time, and it is impossible to establish full-stack trust and integrity verification in the kernel based on the national secret. Therefore, it has become urgent to support this system in the kernel.</span></span></p> 
 <p><span><span>It took 7 rounds for the kernel community to accept SM2. </span><span>The initial consideration was to migrate from openssl, but the openssl architecture and infrastructure code needed to be ported because of the huge workload. </span><span>After several rounds of discussion and testing, I found that the existing libgcrypt already has a complete elliptic curve basic algorithm, so I tried to implement SM2 in libgcrypt first, and finally the SM2 algorithm was accepted by the community as a sub-algorithm of ECC. </span><span>After that, SM2 was gradually accepted by the kernel community.</span></span></p> 
 <p><span><span>At present, libgcrypt has fully supported the national secret algorithm SM2/3/4, and these implementations will be officially released in the next version 1.9.0. </span><span>At the same time, as a user-mode tool for IMA integrity signatures, ima-evm-utils' support for national secrets has not fallen. </span><span>Click to view </span></span><a href="https://sourceforge.net/p/linux-ima/ima-evm-utils/ci/ceecb28d3b5267c7d32c6e9401923c94f5786cfb/log/?path="><span><span>related submissions</span></span></a><span><span> .</span></span></p> 
 <p><span><span>Finally, the author also summarizes the known issues of SM2:</span></span></p> 
 <ul> 
  <li><span><span>To support national secret certificate verification, SM2 either does not compile, or it must be built-in compilation, and does not support compilation into modules. </span><span>Of course, SM2, as an asymmetric algorithm, only signs a hash or IMA verification based on national secrets, and there is no such limitation.</span></span></li> 
  <li><span><span>The IMA signature tool ima-evm-utils and the national secret algorithm used by the kernel to calculate the SM3 hash of the file do not add Za. This is a little difference from the specification.</span></span></li> 
 </ul> 
 <p><a href="https://linux.cn/article-12751-1.html"><span><span>Reference reading</span></span></a></p> 
</div></div></div>]]>
            </description>
            <link>https://www.codetd.com/en/article/12031985</link>
            <guid isPermaLink="false">hacker-news-small-sites-24938686</guid>
            <pubDate>Fri, 30 Oct 2020 03:01:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why Clojure?]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24938484">thread link</a>) | @simonpure
<br/>
October 29, 2020 | https://jeffchen.dev/posts/Why-Clojure/ | <a href="https://web.archive.org/web/*/https://jeffchen.dev/posts/Why-Clojure/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>One piece of unattributed wisdom that's stuck with me is "don't take more than one technology bet". At Ladder, our big bet is using <a href="https://clojure.org/" target="_blank" rel="nofollow noopener noreferrer">Clojure</a> for fullstack app development. Ladder's used Clojure since day 1 in 2015, and we wouldn't want it any different! In particular, Clojure's Lisp heritage, focus on pure functions and immutable data structures, unified client-server support, and superior developer experience have helped us write higher quality code faster.</p>
<!-- excerpt -->
<h2>Pure functions and immutability</h2>
<p>One of the challenges with ordinary, imperative programming languages like Javascript or Python is the increasing complexity of state management. As your application grows, it becomes harder and harder to isolate where in the codebase specific changes to your application state occur. This is because with typical application architectures in those languages, any function can perform <a href="https://en.wikipedia.org/wiki/Side_effect_%28computer_science%29" target="_blank" rel="nofollow noopener noreferrer">side effects</a> or modify incoming or global state. On the other hand, Clojure strongly emphasizes working with <a href="https://en.wikipedia.org/wiki/Pure_function" target="_blank" rel="nofollow noopener noreferrer">pure functions</a> (well, if you discount I/O...) and <a href="https://clojure.org/about/state" target="_blank" rel="nofollow noopener noreferrer">immutable data structures</a>. A Clojure programmer must be explicit when defining and modifying mutable state - this helps minimize its usage and makes it easier to reason about.</p>
<p>Immutable data structures and pure functions also lend themselves well to concurrent programming. We rarely find ourselves worrying about locks and shared data in a multi-threaded environment, because our functions are rarely modifying shared state. And when we do, Clojure provides <code>atom</code>, a thread-safe wrapper around ordinary data structures. Behind the scenes, setting an <code>atom</code>'s value calls <code>compare-and-set!</code>. That means no fussing around with locks or mutexes and no worrying about your data changing before you modify it. With this one simple construct, Clojure removes 99% of our concurrency headaches.</p>
<h2>Clojure is a Lisp</h2>
<p>There are probably enough Lisp arguments on the Internet already - I'll defer to <a href="https://clojure.org/about/rationale#_lisp_is_a_good_thing" target="_blank" rel="nofollow noopener noreferrer">Rich Hickey</a> (Clojure's creator) and <a href="http://www.paulgraham.com/avg.html" target="_blank" rel="nofollow noopener noreferrer">Paul Graham</a> instead of adding another rehash. That said, Clojure provides some advantages over other Lisps like Common Lisp and Scheme:</p>
<ul>
<li>CL only includes lists in its core language spec. Clojure introduces vectors, sets, and maps which makes reading and writing code so much less tedious. Of course Scheme has all of these except sets.</li>
<li>Clojure's core data structures are immutable which, as discussed above, makes reasoning about code, especially concurrent code, much easier.</li>
</ul>
<h2>Clojure runs everywhere</h2>
<p>Clojure provides first class support for sharing code between platforms with <a href="https://clojure.org/guides/reader_conditionals" target="_blank" rel="nofollow noopener noreferrer">reader conditionals</a>. Most of our namespaces at Ladder take advantage of this and are shared across our client (Clojurescript) and server (Clojure). In fact, all of our client React code (aside from browser-specific API calls like clipboard, input handlers, etc) supports being run on the JVM. This lets us run what we call "full-stack tests" entirely within a Java process. For example, we can run full user flows like "user can accept a life insurance policy" and assert against both client and server state <strong>in the same test</strong>. The closest analogue without this superpower would be running a Selenium test against a running webserver, which introduces all sorts of potential flakiness. For more on full-stack tests, check out <a href="https://www.youtube.com/watch?v=qijWBPYkRAQ&amp;t=346s" target="_blank" rel="nofollow noopener noreferrer">this talk</a> two of our engineers gave at Clojure West in 2017.</p>
<p>Clojure also provides easy <a href="https://clojure.org/guides/reader_conditionals#_host_interop" target="_blank" rel="nofollow noopener noreferrer">host interop</a> for each supported platform. This lets us leverage the full JVM (and Javascript) ecosystem. For example, we use popular Java libraries like <a href="https://www.eclipse.org/jetty/" target="_blank" rel="nofollow noopener noreferrer">Jetty</a>, <a href="https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients" target="_blank" rel="nofollow noopener noreferrer">kafka-clients</a>, <a href="https://github.com/google/tink" target="_blank" rel="nofollow noopener noreferrer">Tink</a>, and more. On the frontend, we use React, and can easily include other Javascript libraries for analytics, error handling, and session replays.</p>
<h2>Developer experience</h2>
<p>When I’ve worked with Typescript and Python in the past, I was constantly waiting for my development server to reload. Clojure makes updating code on your local server as simple as reloading the updated namespace in your REPL. If you want, you can even <a href="https://github.com/nrepl/nrepl" target="_blank" rel="nofollow noopener noreferrer">update remote, (hopefully) non-production webservers</a>! Being able to evaluate code in a REPL and have your running web server update in less than a second makes exploration and iteration on your actual backend so much faster. Instant feedback makes developers more playful and experimental. Ultimately, it helps them write better code faster.</p>
<p>It’s also super easy to run small chunks of code in the REPL. Ladder, like other Clojure shops, has a convention of documenting namespace usage with a <code>comment</code> block at the bottom. Developers can use the code within to learn the namespace’s API, run commonly used procedures, or test changes to the rest of the namespace - all without leaving their editor!</p>
<h2>Why not Clojure?</h2>
<p>While we're extremely satisfied with our choice of Clojure, we've had our fair share of headaches. First, Clojure processes take a long time to start up - especially as the size of the application grows. Our webserver at Ladder takes a full minute before it can accept web requests. This makes autoscaling in response to load more challenging - some of our load can spike in well under a minute, so we have to be consistently overprovisioned to handle it. Second, Clojure produces pretty big artifacts. This matters less on the backend, where our webserver JAR is over 1.5GB, but hurts us on the frontend. We still have work to do here, but our initial bundle is 7.2MB uncompressed (1.0MB gzipped)! If raw performance or bundle size is your primary concern, you might be better off choosing another language.</p>
<h2>Conclusion</h2>
<p>As a small company, we have more ideas to try than we have bandwidth to implement. Using Clojure has helped our team be more iterative and more productive, so we can ship more experiments and projects than we would otherwise be able to. I feel super lucky that Ladder introduced me to Clojure - and I'm excited to see how Clojure and our use of it continues to evolve!</p>
</div></div>]]>
            </description>
            <link>https://jeffchen.dev/posts/Why-Clojure/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24938484</guid>
            <pubDate>Fri, 30 Oct 2020 02:23:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Getting Audio Visualizations Working with Web Audio API]]>
            </title>
            <description>
<![CDATA[
Score 38 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24938292">thread link</a>) | @arcatech
<br/>
October 29, 2020 | https://dwayne.xyz/post/audio-visualizations-web-audio-api | <a href="https://web.archive.org/web/*/https://dwayne.xyz/post/audio-visualizations-web-audio-api">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <h2>Web Audio API</h2>
<p>I’ve been working on getting WebRTC video chat working here on the website for a few weeks now. I finally got to the point where both text, video chat, and screen sharing all work really well, but somewhere in the back of my mind I kept thinking about complaints about “<a href="https://www.psychologytoday.com/us/blog/brain-waves/202007/why-zoom-fatigue-is-real-and-what-you-can-do-about-it">Zoom fatigue</a>” during the pandemic:</p>
<blockquote>
<p>Zoom fatigue, Hall argues now, is real. “Zoom is exhausting and lonely because you have to be so much more attentive and so much more aware of what’s going on than you do on phone calls.” If you haven’t turned off your own camera, you are also watching yourself speak, which can be arousing and disconcerting. The blips, delays and cut off sentences also create confusion. Much more exploration needs to be done, but he says, “maybe this isn’t the solution to our problems that we thought it might have been.” Phone calls, by comparison, are less demanding. “You can be in your own space. You can take a walk, make dinner,” Hall says.</p>
</blockquote>

<p>It’s kind of an interesting thing to have on your mind while spending weeks writing/debugging/testing video chat code.</p>
<p>So I decided to add an audio-only mode. And if I was gonna do that, I had to show something cool in place of the video. So I figured I would try to add audio visualizations when one or both of the users didn’t have video on. Using the relatively recent<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API">Web Audio API</a> seemed like the right way to go.</p>
<p>Here’s what I came up with:</p>
<div><video controls="" muted="" autoplay="" playsinline="" loop=""><source src="https://media.dwayne.xyz/blog/audio-visualization.mp4" type="video/mp4"></video><p>Screen recording of the local audio visualization. I cycle through bar graph in light mode, bar graph in dark mode, sine wave in dark mode, then sine wave in light mode.</p></div>
<h2>Creating and hooking up an AnalyserNode</h2>
<p>To create audio visualizations, the first thing you’ll need is an <code>AnalyserNode</code>, which you can get from the <code>createAnalyser</code> method of a <code>BaseAudioContext</code>. You can get both of these things pretty easily<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> like this:</p>
<pre><span>1</span><span>const</span> audioContext <span>=</span> <span>new</span> <span>window</span>.AudioContext();
<span>2</span><span>const</span> analyser <span>=</span> audioContext.createAnalyser();
</pre><p>Next, create a <code>MediaStreamAudioSourceNode</code> from an existing data stream (I use either the local or remote data streams from either <code>getUserMedia</code> or from the ‘track’ event of <code>RTCPeerConnection</code> respectively) using <code>AudioContext.createMediaStreamSource</code>. Then you can connect that audio source to the analyser object like this:</p>
<pre><span>1</span><span>const</span> audioSource <span>=</span> <span>this</span>.audioContext.createMediaStreamSource(stream);
<span>2</span>audioSource.connect(analyser);
</pre>
<h2>Using requestAnimationFrame</h2>
<p><code>window.requestAnimationFrame</code> is nice. Call it, passing in your drawing function, and then inside that function call <code>requestAnimationFrame</code> again. Get yourself a nice little recursive loop going that’s automatically timed properly by the browser.</p>
<p>In my situation, there will either be 0, 1, or 2 visualizations running, since either side can choose either video chat, audio-only (…except during screen sharing), or just text chat. So I have one loop that draws both. It looks like this:</p>
<pre><span>1</span><span>const</span> drawAudioVisualizations <span>=</span> () =&gt; {
<span>2</span>    audioCancel <span>=</span> <span>window</span>.requestAnimationFrame(drawAudioVisualizations);
<span>3</span>    localAudioVisualization.draw();
<span>4</span>    remoteAudioVisualization.draw();
<span>5</span>};
</pre><p>I created the class for those visualization objects, and they handle whether or not to draw. They each contain the analyser, source, and context objects for their visualization.</p>
<p>Then when I detect that loop doesn’t have to run anymore, I can cancel it using that <code>audioCancel</code> value:</p>
<pre><span>1</span><span>window</span>.cancelAnimationFrame(audioCancel);
<span>2</span>audioCancel <span>=</span> <span>0</span>;
</pre>
<h2>Configuring the Analyser</h2>
<p>Like in the <a href="https://github.com/mdn/voice-change-o-matic/blob/gh-pages/scripts/app.js">example you’ll see a lot</a> if you look at the MDN documentation for this stuff, I provide options for two audio visualizations: frequency bars and a sine wave. Here’s how I configure the analyser for each type:</p>
<pre><span> 1</span><span>switch</span> (<span>this</span>.type) {
<span> 2</span>    <span>case</span> <span>'frequencybars'</span><span>:</span>
<span> 3</span>        <span>this</span>.analyser.minDecibels <span>=</span> <span>-</span><span>90</span>;
<span> 4</span>        <span>this</span>.analyser.maxDecibels <span>=</span> <span>-</span><span>10</span>;
<span> 5</span>        <span>this</span>.analyser.smoothingTimeConstant <span>=</span> <span>0.85</span>;
<span> 6</span>        <span>this</span>.analyser.fftSize <span>=</span> <span>256</span>;
<span> 7</span>        <span>this</span>.bufferLength <span>=</span> <span>this</span>.analyser.frequencyBinCount;
<span> 8</span>        <span>this</span>.dataArray <span>=</span> <span>new</span> Uint8Array(<span>this</span>.bufferLength);
<span> 9</span>        <span>break</span>;
<span>10</span>    <span>default</span><span>:</span>
<span>11</span>        <span>this</span>.analyser.minDecibels <span>=</span> <span>-</span><span>90</span>;
<span>12</span>        <span>this</span>.analyser.maxDecibels <span>=</span> <span>-</span><span>10</span>;
<span>13</span>        <span>this</span>.analyser.smoothingTimeConstant <span>=</span> <span>0.9</span>;
<span>14</span>        <span>this</span>.analyser.fftSize <span>=</span> <span>1024</span>;
<span>15</span>        <span>this</span>.bufferLength <span>=</span> <span>this</span>.analyser.fftSize;
<span>16</span>        <span>this</span>.dataArray <span>=</span> <span>new</span> Uint8Array(<span>this</span>.bufferLength);
<span>17</span>        <span>break</span>;
<span>18</span>}
</pre><div><p>I’ve adjusted these numbers a lot, and I’m gonna keep doing it. A note about <code>fftSize</code> and <code>frequencyBinCount</code>: <code>frequencyBinCount</code> is set right after you set <code>fftSize</code> and it’s usually just half the <code>fftSize</code> value. These values are about the amount of data you want to receive from the main analyser functions I’m about to talk about next. As you can see, they directly control the size of the data array that you’ll use to store the audio data on each draw call.
</p></div>
<h2>Using the Analyser</h2>
<p>On each draw call, depending on the type of visualization, call either <code>getByteFrequencyData</code> or <code>getByteTimeDomainData</code> with the array that was created above, and it’ll be filled with data. Then you run a simple loop over each element and start drawing. Here’s my sine wave code:</p>
<pre><span> 1</span><span>this</span>.analyser.getByteTimeDomainData(<span>this</span>.dataArray);
<span> 2</span><span>this</span>.ctx.lineWidth <span>=</span> <span>2</span>;
<span> 3</span><span>this</span>.ctx.strokeStyle <span>=</span> audioSecondaryStroke;
<span> 4</span>
<span> 5</span><span>this</span>.ctx.beginPath();
<span> 6</span>
<span> 7</span><span>let</span> v, y;
<span> 8</span><span>for</span> (<span>let</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> <span>this</span>.bufferLength; i<span>++</span>) {
<span> 9</span>    v <span>=</span> <span>this</span>.dataArray[i] <span>/</span> <span>128.0</span>;
<span>10</span>    y <span>=</span> v <span>*</span> height <span>/</span> <span>2</span>;
<span>11</span>
<span>12</span>    <span>if</span> (i <span>===</span> <span>0</span>) {
<span>13</span>        <span>this</span>.ctx.moveTo(x, y);
<span>14</span>    } <span>else</span> {
<span>15</span>        <span>this</span>.ctx.lineTo(x, y);
<span>16</span>    }
<span>17</span>
<span>18</span>    x <span>+=</span> width <span>*</span> <span>1.0</span> <span>/</span> <span>this</span>.bufferLength;
<span>19</span>}
<span>20</span>
<span>21</span><span>this</span>.ctx.lineTo(width, height <span>/</span> <span>2</span>);
<span>22</span><span>this</span>.ctx.stroke();
</pre><div><p>The fill and stroke colors are dynamic based on the website color scheme.
</p></div>
<h2>Good ol' Safari</h2>
<p>So I did all of this stuff I just talked about, but for <strong>days</strong> I could <em>not</em> get this to work in Safari. Not because of errors or anything, but because both <code>getByteFrequencyData</code> and <code>getByteTimeDomainData</code> just filled the array with 0s every time. No matter what I did. I was able to get the audio data in Firefox just fine.</p>
<p>So at first, I figured it just didn’t work at all in Safari and I would just have to wait until Apple fixed it. But then I came across <a href="https://mdn.github.io/voice-change-o-matic/">this sample audio project</a> and noticed it worked just fine in Safari.</p>
<div><p>So I studied the code for an hour trying to understand what was different about my code and theirs. I made a lot of changes to my code to make it more like what they were doing. One of the big differences is that they’re connecting the audio source to different audio distortion nodes to actually change the audio. I just want to create a visualization so I wasn’t using any of those objects.
</p></div>
<h3>Audio Distortion Effects</h3>
<p>The <code>BaseAudioContext</code> has a few methods you can use to create audio distortion objects.</p>
<ul>
<li><code>WaveShaperNode</code>: Use <code>BaseAudioContext.createWaveShaper</code> to create a non-linear distortion. You can use a custom function to change the audio data.</li>
<li><code>GainNode</code>: Use <code>BaseAudioContext.createGain</code> to control the overall gain (volume) of the audio.</li>
<li><code>BiquadFilterNode</code>: Use <code>BaseAudioContext.createBiquadFilter</code> to apply some common audio effects.</li>
<li><code>ConvolverNode</code>: Use <code>BaseAudioContext.createConvolver</code> to apply reverb effects to audio.</li>
</ul>
<p>Each one of these objects has a <code>connect</code> function where you pass another context, output, or filter. Each one has a certain number of inputs and outputs. Here’s an example from that sample project of connecting all of them:</p>
<pre><span>1</span>source <span>=</span> audioCtx.createMediaStreamSource(stream);
<span>2</span>source.connect(distortion);
<span>3</span>distortion.connect(biquadFilter);
<span>4</span>biquadFilter.connect(gainNode);
<span>5</span>convolver.connect(gainNode);
<span>6</span>gainNode.connect(analyser);
<span>7</span>analyser.connect(audioCtx.destination);
</pre><p><strong>Note</strong>: Don’t connect to your audio context <code>destination</code> if you’re just trying to create a visualization for a call. The user will hear themselves talking.</p>
<div><p>Anyway, I tried adding these things to my code to see if that would get it working in Safari, but I had no luck.
</p></div>
<h2>Figuring out the Safari issue</h2>
<p>I was starting to get <em>real</em> frustrated trying to figure this out. I was gonna let it go when I thought Safari was just broken (because it usually is), but since I knew it <em>could</em> work in Safari, I couldn’t leave it alone.</p>
<p>Eventually I downloaded the actual HTML and Javascript files from that sample and started removing shit from their code, running it locally and seeing if it worked. Which it did. So now I’m editing my own code, and <em>their code</em>, to get them to be pretty much the same. Which I did. And <strong>still</strong> theirs worked and mine didn’t.</p>
<p>Next I just started desperately logging every single object at different points in my code to figure out what the fuck was going on. Then I noticed something.</p>
<div><p><img src="https://media.dwayne.xyz/blog/audio-context-suspended.png" alt="Dev console showing the output of logging this.audioContext. The state attribute is shown as suspended"></p><p>Output of logging the audio context object.</p></div>
<p>The <code>state</code> is “suspended”? Why? I don’t know. I did the same log in the sample code (that I had downloaded and was running on my machine) and it was “running”.</p>
<p>This is the code that fixes it:</p>
<pre><span>1</span><span>this</span>.audioSource <span>=</span> <span>this</span>.audioContext.createMediaStreamSource(<span>this</span>.stream);
<span>2</span><span>this</span>.audioSource.connect(<span>this</span>.analyser);
<span>3</span><span>this</span>.audioContext.resume(); <span>// Why??????
</span></pre><div><p>Calling <code>resume</code> changes the state and then everything works. To this day I still don’t know why the sample code didn’t need that line.
</p></div>
<h2>Drawing the image and supporting light/dark modes</h2>
<p>Like everything else on my site, all of this must support different color schemes (and screen sizes, and mobile devices). That was surprisingly difficult when trying to draw an SVG on the canvas.</p>
<p>I’m using <a href="https://fontawesome.com/">FontAwesome</a> for all my icons on the site. I wanted to use one of them for these visualizations. The FontAwesome files are all SVGs (which is great), but I didn’t know how to draw the image in different colors in Javascript. The way I decided to do this was to load the SVG file into a Javascript <code>Image</code> object, then draw that onto the canvas each draw call.</p>
<p>That worked, but it only drew it black even after changing the fill and stroke colors. So after some web searching I read about someone deciding to draw out an image on an offscreen canvas, reading all the image data, and …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dwayne.xyz/post/audio-visualizations-web-audio-api">https://dwayne.xyz/post/audio-visualizations-web-audio-api</a></em></p>]]>
            </description>
            <link>https://dwayne.xyz/post/audio-visualizations-web-audio-api</link>
            <guid isPermaLink="false">hacker-news-small-sites-24938292</guid>
            <pubDate>Fri, 30 Oct 2020 01:55:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Quantbase – Deploy your own algo trader in 5 minutes with 0 code]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24938082">thread link</a>) | @tjs8rj
<br/>
October 29, 2020 | https://areyouinterested.co/site/quantbase/ | <a href="https://web.archive.org/web/*/https://areyouinterested.co/site/quantbase/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="hero-1"><p id="subheading">Deploy algorithms from top hedge funds,<br> rank and use 100s of users’ algorithms,<br> or create new algo-traders effortlessly</p>

              <div id="interested">
                <h3>Are you interested?</h3>
                <div id="buttons">
                  <p><a href="https://areyouinterested.co/site/quantbase/yes" id="yes">Yes</a>
                  <a href="https://areyouinterested.co/site/quantbase/no" id="no">No</a>
                </p></div>
              </div>

            </div></div>]]>
            </description>
            <link>https://areyouinterested.co/site/quantbase/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24938082</guid>
            <pubDate>Fri, 30 Oct 2020 01:11:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Vega-Lite: A Grammar of Interactive Graphics]]>
            </title>
            <description>
<![CDATA[
Score 152 | Comments 36 (<a href="https://news.ycombinator.com/item?id=24937954">thread link</a>) | @tosh
<br/>
October 29, 2020 | https://vega.github.io/vega-lite/ | <a href="https://web.archive.org/web/*/https://vega.github.io/vega-lite/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <section>
    <p>
  <strong>Vega-Lite</strong> is a high-level grammar of interactive graphics. It provides a concise, declarative JSON syntax to create an expressive range of visualizations for data analysis and presentation.
</p>

<p><span>
  <span>
    Vega-Lite specifications describe visualizations as encoding mappings from data to <strong>properties of graphical marks</strong> (e.g., points or bars).
    The Vega-Lite compiler <strong>automatically produces visualization components</strong> including axes, legends, and scales.
    It determines default properties of these components based on a set of <strong>carefully designed rules</strong>.
    This approach allows Vega-Lite specifications to be concise for quick visualization authoring, while giving user control to override defaults and customize various parts of a visualization.
    As we also designed Vega-Lite to support data analysis, Vega-Lite supports both <strong>data transformations</strong> (e.g., aggregation, binning, filtering, sorting) and <strong>visual transformations</strong> (e.g., stacking and faceting).
    Moreover, Vega-Lite specifications can be <strong>composed</strong> into layered and multi-view displays, and made <strong>interactive with selections</strong>.
  </span>
  <span>
  <a href="https://vega.github.io/vega-lite/tutorials/getting_started.html">Get started<br><small>Latest Version: 4.17.0</small></a>
  <a href="https://vega.github.io/editor/#/custom/vega-lite">Try online</a>
  </span>
</span></p>

<p>Compared to <a href="https://vega.github.io/vega">Vega</a>, Vega-Lite provides a more concise and convenient form to author common visualizations. As Vega-Lite can compile its specifications to Vega specifications, users may use Vega-Lite as the <em>primary</em> visualization tool and, if needed, transition to use the lower-level Vega for advanced use cases.</p>

<p>For more information, read our <a href="https://medium.com/@uwdata/de6661c12d58">introduction article to Vega-Lite v2 on Medium</a>, watch our <a href="https://www.youtube.com/watch?v=9uaHRWj04D4">OpenVis Conf talk about the new features in Vega-Lite v2</a>, see the <a href="https://vega.github.io/vega-lite/docs/">documentation</a> and take a look at our <a href="https://vega.github.io/vega-lite/examples/">example gallery</a>. Follow us on <a href="https://twitter.com/vega_vis">Twitter at @vega_vis</a> to stay informed about updates.</p>

<h2 id="example">Example</h2>



<h2 id="additional-links">Additional Links</h2>

<ul>
  <li>Award winning <a href="https://idl.cs.washington.edu/papers/vega-lite">research paper</a> and <a href="https://www.youtube.com/watch?v=9uaHRWj04D4">video of our OpenVis Conf talk</a> on the design of Vega-Lite</li>
  <li>Listen to a Data Stories episode about <a href="http://datastori.es/121-declarative-visualization-with-vega-lite-and-altair-with-dominik-moritz-jacob-vanderplas-kanit-ham-wongsuphasawat/">Declarative Visualization with Vega-Lite and Altair</a>
</li>
  <li>
<a href="http://json-schema.org/">JSON schema</a> specification for <a href="https://github.com/vega/schema">Vega-Lite</a> (<a href="https://vega.github.io/schema/vega-lite/v4.json">latest</a>)</li>
  <li>Ask questions about Vega-Lite on <a href="https://stackoverflow.com/tags/vega-lite">Stack Overflow</a> or <a href="https://bit.ly/join-vega-slack-2020">Slack</a>
</li>
  <li>Fork our <a href="https://bl.ocks.org/domoritz/455e1c7872c4b38a58b90df0c3d7b1b9">Vega-Lite Block</a>, or <a href="https://beta.observablehq.com/@domoritz/vega-lite-demo">Observable Notebook</a>.</li>
</ul>

<h2 id="users">Users</h2>

<p>Vega-Lite is used by thousands of data enthusiasts, developers, journalists, data scientists, teachers, and researchers across many organizations. Here are some of them. Learn about integrations on our <a href="https://vega.github.io/vega-lite/ecosystem.html">ecosystem page</a>.</p>



<h2 id="team">Team</h2>

<p>The development of Vega-Lite is led by the alumni and members of the <a href="https://idl.cs.washington.edu/">University of Washington Interactive Data Lab</a> (UW IDL), including <a href="https://twitter.com/kanitw">Kanit “Ham” Wongsuphasawat</a> (now at Apple), <a href="https://twitter.com/domoritz">Dominik Moritz</a> (now at CMU / Apple), <a href="https://twitter.com/arvindsatya1">Arvind Satyanarayan</a> (now at MIT), and <a href="https://twitter.com/jeffrey_heer">Jeffrey Heer</a> (UW IDL).</p>

<p>Vega-Lite gets significant contributions from its community–in particular <a href="https://willium.com/">Will Strimling</a>, <a href="https://github.com/YuhanLu">Yuhan (Zoe) Lu</a>, <a href="https://github.com/invokesus">Souvik Sen</a>, <a href="https://github.com/chanwutk">Chanwut Kittivorawong</a>, <a href="https://github.com/mattwchun">Matthew Chun</a>, <a href="https://github.com/AkshatSh">Akshat Shrivastava</a>, <a href="https://github.com/Saba9">Saba Noorassa</a>, <a href="https://github.com/sirahd">Sira Horradarn</a>, <a href="https://github.com/donghaoren">Donghao Ren</a>, and <a href="https://github.com/haldenl">Halden Lin</a>. Please see the <a href="https://github.com/vega/vega-lite/graphs/contributors">contributors page</a> for the full list of contributors.</p>

  </section>
</div></div>]]>
            </description>
            <link>https://vega.github.io/vega-lite/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24937954</guid>
            <pubDate>Fri, 30 Oct 2020 00:52:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bizarre Design Choices in Zoom’s End-to-End Encryption]]>
            </title>
            <description>
<![CDATA[
Score 63 | Comments 37 (<a href="https://news.ycombinator.com/item?id=24937298">thread link</a>) | @notRobot
<br/>
October 29, 2020 | https://soatok.blog/2020/10/28/bizarre-design-choices-in-zooms-end-to-end-encryption/?hac | <a href="https://web.archive.org/web/*/https://soatok.blog/2020/10/28/bizarre-design-choices-in-zooms-end-to-end-encryption/?hac">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<div>

			
<p>Zoom recently announced that they were going to make end-to-end encryption available to all of their users–not just customers.</p>



<figure><div>

</div></figure>



<p>This is a good move, especially for people living in countries with <a href="https://soatok.blog/2020/07/02/how-and-why-america-was-hit-so-hard-by-covid-19/">inept leadership that failed to address the COVID-19 pandemic</a> and therefore need to conduct their work and schooling remotely through software like Zoom. I enthusiastically applaud them for making this change.</p>



<div><figure><img data-attachment-id="1333" data-permalink="https://soatok.blog/soatoktelegrams2020-08/" data-orig-file="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png" data-orig-size="512,512" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SoatokTelegrams2020-08" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=512" src="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=512" alt="" srcset="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png 512w, https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=150 150w, https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=300 300w" sizes="(max-width: 512px) 100vw, 512px"><figcaption>End-to-end encryption, on by default, is a huge win for everyone who uses Zoom. (Art by <a href="https://twitter.com/lynxvsjackalope">Khia</a>.)</figcaption></figure></div>



<p>The end-to-end encryption capability arrives on the heels of their acquisition of <a href="https://keybase.io/">Keybase</a> in earlier this year. Hiring a team of security experts and cryptography engineers seems like a good move overall.</p>



<p>Upon hearing this news, I decided to be a good neighbor and take a look at their source code, with the reasoning, “If so many people’s privacy is going to be dependent on Zoom’s security, I might as well make sure they’re not doing something ridiculously bad.”</p>



<p>Except I couldn’t find their source code anywhere online. But they did publish <a href="https://github.com/zoom/zoom-e2e-whitepaper">a white paper on Github</a>…</p>







<h2>Disclaimers</h2>



<p>What follows is the opinion of some guy on the Internet with a fursona–so whether or not you choose to take it seriously should be informed by this context. It is not the opinion of anyone’s employer, nor is it endorsed by Zoom, etc. Tell your lawyers to calm their nips.</p>



<p>More importantly, I’m not here to hate on Zoom for doing a good thing, nor on the security experts that worked hard on making Zoom better for their users. The responsibility of security professionals is to the users, after all.</p>



<p>Also, these aren’t zero-days, so don’t try to lecture me about “responsible” disclosure. (That term is also <a href="https://adamcaudill.com/2015/11/19/responsible-disclosure-is-wrong/">problematic</a>, by the way.)</p>



<p>Got it? Good. Let’s move on.</p>







<h2>Bizarre Design Choices in Version 2.3 of Zoom’s E2E White Paper</h2>



<p>Note: I’ve altered the screenshots to be white text on a black background, since my blog’s color scheme is darker than a typical academic PDF. You can find the source <a href="https://github.com/zoom/zoom-e2e-whitepaper/blob/d3be2a5a3e16be04f1199b92630f180ba79cb51c/zoom_e2e.pdf">here</a>.</p>



<h3>Cryptographic Algorithms</h3>



<div><figure><img data-attachment-id="1744" data-permalink="https://soatok.blog/zoom-e2e-02/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png" data-orig-size="784,652" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-02" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png" alt=""></figure></div>



<p>It’s a little weird that they’re calculating a signature over SHA256(Context) || SHA256(M), considering Ed25519 uses SHA512 internally.</p>



<p>It would make just as much sense to sign Context || M directly–or, if pre-hashing large streams is needed, SHA512(Context || M).</p>



<div><figure><img data-attachment-id="1740" data-permalink="https://soatok.blog/zoom-e2e-01/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png" data-orig-size="1039,788" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-01" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png" alt=""></figure></div>



<p>At the top of this section, it says it uses libsodium’s <code>crypto_box</code> interface. But then they go onto… not actually use it.</p>



<p>Instead, they wrote their own protocol using HKDF, two SHA256 hashes, and XChaCha20-Poly1305.</p>



<p>While secure, this isn’t <em>really</em> using the crypto_box interface.</p>



<p>The only part of the libsodium interface that’s being used is <code><a href="https://github.com/jedisct1/libsodium/blob/927dfe8e2eaa86160d3ba12a7e3258fbc322909c/src/libsodium/crypto_box/curve25519xsalsa20poly1305/box_curve25519xsalsa20poly1305.c#L35-L46">crypto_box_beforenm()</a></code>, which could easily have been a call to <code>crypto_scalarmult()</code>instead (since they’re passing the output of the scalar multiplication to HKDF anyway).</p>







<p>Also, the SHA256(a) || SHA256(b) pattern returns. Zoom’s engineers must love SHA256 for some reason.</p>



<p>This time, it’s in the additional associated data for the XChaCha20-Poly1305. </p>



<p>Binding the ciphertext and the signature to the same context string is a sensible thing to do, it’s just the concatenation of SHA256 hashes is a bit weird when SHA512 exists.</p>



<h3>Meeting Leader Security Code</h3>



<div><figure><img data-attachment-id="1746" data-permalink="https://soatok.blog/zoom-e2e-03/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png" data-orig-size="760,733" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-03" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png" alt=""></figure></div>



<p>Here we see Zoom using the a SHA256 of a constant string (“<code>Zoombase-1-ClientOnly-MAC-SecurityCode</code>“) in a construction that tries but fails to be HMAC.</p>



<p>And then they concatenate it with the SHA256 hash of the public key (which is already a 256-bit value), and then they hash the whole thing again.</p>



<p>It’s redundant SHA256 all the way down. The redundancy of “MAC” and “SecurityCode” in their constant string is, at least, consistent with the rest of their design philosophy.</p>



<p>It would be a real shame if double-hashing carried the risk of <a href="https://eprint.iacr.org/2013/382">invalidating security proofs</a>, or if <a href="https://cseweb.ucsd.edu/~mihir/papers/kmd5.pdf">the security proof for HMAC</a> required a high Hamming distance of padding constants and this design decision also later <a href="https://eprint.iacr.org/2012/684.pdf">saved HMAC from related-key attacks</a>.</p>



<h3>Hiding Personal Details</h3>



<figure><img data-attachment-id="1750" data-permalink="https://soatok.blog/zoom-e2e-04/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png" data-orig-size="739,603" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-04" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=739" alt="" srcset="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png 739w, https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=150 150w, https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=300 300w" sizes="(max-width: 739px) 100vw, 739px"></figure>



<p>Wait, you’re telling me Zoom was aware of HMAC’s existence this whole time?</p>



<div><figure><img data-attachment-id="1202" data-permalink="https://soatok.blog/soatoktelegrams2020-02/" data-orig-file="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png" data-orig-size="512,512" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SoatokTelegrams2020-02" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=512" src="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=512" alt="" srcset="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png 512w, https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=150 150w, https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=300 300w" sizes="(max-width: 512px) 100vw, 512px"><figcaption>I give up!</figcaption></figure></div>



<h2>Enough Pointless Dunking, What’s the Takeaway?</h2>



<p>None of the design decisions Zoom made that I’ve criticized here are security vulnerabilities, but they do demonstrate an early lack of cryptography expertise in their product design.</p>



<p>After all, the weirdness is almost entirely contained in section 3 of their white paper, which describes the “Phase I” of their rollout. So what I’ve pointed out here appears to be mostly legacy cruft that wasn’t risky enough to bother changing in their final design.</p>



<p>The rest of their paper is pretty straightforward and pleasant to read. Their design makes sense in general, and each phase includes an “Areas to Improve” section.</p>



<p>All in all, if you’re worried about the security of Zoom’s E2EE feature, the only thing they can really do better is to publish the source code (and link to it from the whitepaper repository for ease-of-discovery) for this feature so independent experts can publicly review it.</p>



<p>However, they seem to be getting a lot of mileage out of the experts on their payroll, so I wouldn’t count on that happening.</p>

		</div><!-- .entry-content -->

	</div></div>]]>
            </description>
            <link>https://soatok.blog/2020/10/28/bizarre-design-choices-in-zooms-end-to-end-encryption/?hac</link>
            <guid isPermaLink="false">hacker-news-small-sites-24937298</guid>
            <pubDate>Thu, 29 Oct 2020 23:23:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Dependency inference in Pants 2.0.0: Precise caching without the boilerplate]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24937228">thread link</a>) | @stuhood
<br/>
October 29, 2020 | https://blog.pantsbuild.org/dependency-inference/ | <a href="https://web.archive.org/web/*/https://blog.pantsbuild.org/dependency-inference/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<!--kg-card-begin: markdown--><p>As discussed <a href="https://blog.pantsbuild.org/introducing-pants-v2/">in our post announcing Pants v2</a>, it's clear that Python has "grown up" by gaining facilities to help it to scale to larger projects. But as codebases grow and tool counts increase, more Python codebases need build tools. While you could write bespoke scripts to coordinate each of your tools, using Pants brings benefits like caching, concurrency, introspection, a simple and uniform user experience, and more!</p>
<p>Unfortunately, scalable build tools have historically meant a significant boilerplate burden: scattering <code>BUILD</code> files throughout your repository and then needing to edit both your code and the redundant dependency information in build definitions.</p>
<p>But it doesn’t have to be that way! Pants v2 supports the precise caching, concurrency, and introspection that you need to scale your repository, with up to <strong>90%</strong> less <code>BUILD</code> boilerplate, thanks to… Dependency inference!</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h2 id="scalingup">Scaling up</h2>
<p>Pants supports repos of all sizes with minimal boilerplate, but it is particularly helpful in repositories containing multiple deployable or publishable projects, each with potentially different requirements or interpreters: aka, monorepos.</p>
<p>Monorepos have lots of benefits (no dependency conflicts, atomic cross-project commits, easy top-to-bottom continuous integration, linear change history), but essential to making them scale are the abilities to:</p>
<ol>
<li>test, check, and deploy precisely the portion of the repository that is relevant to you</li>
<li>cache builds and tests to avoid re-building when unrelated code has changed</li>
<li>manage and configure the variety of tooling that users of the repository will want to use</li>
</ol>
<p>It's critical in a monorepo to be able to test, check, and deploy exactly the relevant portion of your code, ideally with zero impact from changes in unrelated parts of the codebase. For example: if you have three libraries <code>A</code>, <code>B</code>, and <code>C</code>, which depend on one another in a chain like <code>A -&gt; B -&gt; C</code>, a monorepo that builds from source using Pants allows you to completely ignore versioning (and <code>setup.py</code> files, per-project <code>requirements.txt</code>, etc) while you edit library <code>C</code>, even if you are running the tests for library <code>A</code>. If you have ten other projects (or one thousand!) in your repository and only a few of them depend on <code>C</code>, you'd like to avoid ever running tests or mypy for the unrelated libraries while editing <code>C</code>.</p>
<p>And in those cases when you <em>do</em> want to take advantage of a monorepo's top-to-bottom integration testing by running "all of your dependent's tests" (or maybe just typecheck them with Mypy!), you'd like to do that as quickly as possible by taking advantage of caching, concurrency, or transparently executing them on a cluster of machines using remote execution.</p>
<p>To enable this scalability, monorepo build tools like Pants v1 and Bazel required that the dependencies between libraries and files were declared in <code>BUILD</code> files. These dependencies were then used to determine which portions of the repository needed to be built, and which files needed to be included in cache keys.</p>
<p>"But wait", you say! "Doesn't that mean we've traded editing <code>setup.py</code> and <code>requirements.txt</code> files for every library for editing <code>BUILD</code> files for every library?" In most tools, that would be the case: but not in Pants v2! Pants v2 is different.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h2 id="dependencyinference">Dependency inference</h2>
<p>Pants 2.0.0 almost entirely eliminates the boilerplate of declaring dependencies between libraries thanks to "Dependency inference". Dependency inference is roughly what it sounds like: Pants supports discovering the dependencies between Python libraries by parsing <code>import</code> statements (and you can <a href="https://www.pantsbuild.org/docs/plugins-overview">use our powerful plugin API</a> to infer other dependencies, such as by parsing a Django settings file, YAML config, etc).</p>
<p>Rather than adding an <code>import</code> statement to your code resulting in your tests failing because you forgot to <em>also</em> update a <code>BUILD</code> file, Pants will use your newly added <code>import</code> statement to infer that that file now has a dependency within the repository (or outside it via your <code>requirements.txt</code>). When designing inference, we strove to remove boilerplate without introducing magic, so <code>BUILD</code> files are still used to declare any metadata your library might have (the version of the Python interpreter to use, etc), and can be used to <a href="https://www.pantsbuild.org/docs/targets#dependencies-and-dependency-inference">override or extend</a> the inferred dependencies.</p>
<p>Even better, Pants infers these dependencies <em>at the file level</em>. Rather than adding a dependency from "the library named <code>A</code>" to "the library named <code>B</code>" (or "target" in monorepo parlance), Pants tracks that "file <code>a.py</code> depends on file "<code>b.py</code>". Rather than staring at the content of your <code>BUILD</code> files to (attempt to?) understand your dependencies, you can use Pants' dependency introspection tools to easily explore them at the file level: <code>./pants dependencies $file</code>.</p>
<p>In practice, we’ve found that inferred file-level dependencies can reduce the total per-file dependency count by an average of <strong>30%</strong> (improving cache hit rates), and reduce the size of <code>BUILD</code> files by up to <strong>90%</strong> (reducing boilerplate)! See our docs for <a href="https://www.pantsbuild.org/docs/how-does-pants-work#dependency-inference">a real world example</a>.</p>
<p>And critically (as we’ll discuss in further posts!), dependency inference is both 1) very safe, and 2) very fast. Because Pants invokes processes hermetically using SHA256 fingerprinting and strong sandboxes (your test frameworks, your linters, mypy, everything), failing to infer a dependency can never cause the wrong things to be cached. And because the core of Pants is implemented in Rust — and uses a daemon, parallelism, and very-fine-grained memoization — inference won’t slow you down!</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h2 id="demonstration">Demonstration</h2>
<p>To show what it’s like to use dependency inference, we’ll quickly add a feature with assistance from some new first and third party dependencies (from sources and PyPI, respectively).</p>
<p>We’ll start with a broken test that expects TOML files to be supported by a library in a different directory:</p>
<!--kg-card-end: markdown--><figure><img src="https://blog.pantsbuild.org/content/images/2020/10/1.gif" alt=""></figure><!--kg-card-begin: markdown--><p>Because we’re curious, we’ll start by asking Pants whether the library already (directly) depends on TOML:</p>
<!--kg-card-end: markdown--><figure><img src="https://blog.pantsbuild.org/content/images/2020/10/2.gif" alt=""></figure><!--kg-card-begin: markdown--><p>Nope. Only YAML. But let’s add the <code>import</code> statement and see what happens…</p>
<!--kg-card-end: markdown--><figure><img src="https://blog.pantsbuild.org/content/images/2020/10/3.gif" alt=""></figure><!--kg-card-begin: markdown--><p>Voila! This file now declares a dependency on TOML via the <code>import</code> statement, without any modifications to <code>BUILD</code> files. But we’re not finished: neither the flake8 linter nor the customer will be satisfied with an unused <code>import</code>! Let’s finish adding the feature, and then re-run the test.</p>
<!--kg-card-end: markdown--><figure><img src="https://blog.pantsbuild.org/content/images/2020/10/4.gif" alt=""></figure><!--kg-card-begin: markdown--><p>Ok, TOML is clearly being used: but this time our test needs tweaking. To fix it we can import a <code>TomlSerializer</code> helper class from a second library (another new dependency!). We already know that we don’t need to check the <code>BUILD</code> file to see whether this test declares a dependency on the library, so we can just focus on our code!:</p>
<!--kg-card-end: markdown--><figure><img src="https://blog.pantsbuild.org/content/images/2020/10/5.gif" alt=""></figure><!--kg-card-begin: markdown--><p>Great: our test is now passing! To fix it, we edited two files and introduced two new dependencies -- but we didn’t need to edit any <code>BUILD</code> files, despite these files living in different targets!</p>
<p>Finally, let’s confirm the critical monorepo scalability property that edits to unrelated files don’t invalidate the caching of our new test. Although this test is quick, there are a few hundred files in this repository, and plenty of other tests that could take long enough to result in coffee breaks!:</p>
<!--kg-card-end: markdown--><figure><img src="https://blog.pantsbuild.org/content/images/2020/10/6.gif" alt=""></figure><!--kg-card-begin: markdown--><p>Excellent! Thanks to dependency inference’s file level precision, our test is still quickly and correctly cached, even after editing neighboring files in the target! Productivity preserved; mission accomplished.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h2 id="conclusion">Conclusion</h2>
<p>The precise, always-accurate dependencies in Pants have a lot of benefits:</p>
<ol>
<li>teams can quickly get started using Pants</li>
<li><code>BUILD</code> files (or <code>requirements.txt</code>/<code>setup.py</code> files) can't go out of sync with the code, because you don't need to repeat all of your <code>import</code> statements there</li>
<li>the cache keys for processes (and thus the number of cache hits and amount of rebuilding) are more accurate and much more fine-grained than you would ever write by hand</li>
</ol>
<p>If you're interested in speeding-up and scaling-up your builds without the boilerplate, the <a href="https://www.pantsbuild.org/docs/community">Pants community would love to help</a>!</p>
<!--kg-card-end: markdown-->
			</section></div>]]>
            </description>
            <link>https://blog.pantsbuild.org/dependency-inference/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24937228</guid>
            <pubDate>Thu, 29 Oct 2020 23:14:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: The most advanced VPN and unblocker with industry-first features]]>
            </title>
            <description>
<![CDATA[
Score 49 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24937073">thread link</a>) | @Oeck
<br/>
October 29, 2020 | http://www.oeck.com/features/ | <a href="https://web.archive.org/web/*/http://www.oeck.com/features/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	<div>
		<!--XF:EXTRA_OUTPUT-->

		

		

		
	

		
	<!--[if lt IE 9]><div class="blockMessage blockMessage&#45;&#45;important blockMessage&#45;&#45;iconic">You are using an out of date browser. It  may not display this or other websites correctly.<br />You should upgrade or use an <a href="https://www.google.com/chrome/browser/" target="_blank">alternative browser</a>.</div><![endif]-->


		
			<div>
			
				
					
				

				
					<p>The things that make us different.</p>
				
			
				
			</div>
		

		<div>
			

			<div>
				
				<div>

	



	
	
	










	<div>
	<div data-aos="fade-up">
		<div>
			<p><img src="http://www.oeck.com/assets/images/web/png/500/smartRouting.png">
			</p>
			<div>
				<div>
					<p><img src="http://www.oeck.com/assets/images/web/png/500/smartRouting.png"></p><p>
					<span>smartRouting</span>
					<br>
					Oeck takes away the need of connecting to a specific region to access streaming content. Get access to the latest shows from around the world without ever switching regions. Our revolutionary smartRouting unblocks some of the most popular services from around the globe. Enjoy fast, automated access to itv in the UK - Hulu in the US - iView in Australia and many more. Simply connect to the VPN location closest to you and we take care of the rest! 
					</p>
					
				</div>				
			</div>
		</div>
	</div>
</div>

<div>
	<div data-aos="fade-up">
		<div>
			<div>
				<p><img src="http://www.oeck.com/assets/images/web/png/500/deviceProfiles.png"></p><p>
				<span>Device Profiles</span>
				<br>
				Device Profiles allow you to take your VPN functionality to the next level. This handy feature allows you to set preferences for streaming services and traffic filtration on a per-device level.
					By doing this you can quickly set up a childs device to block adult content whilst keeping your other devices untouched. It also allows you to set up devices with different streaming regions.
					For example, you can have Netflix USA on one device, Netflix UK on another and Netflix Germany on yet another, whilst all the while being connected to the VPN region closest to you!
				</p>
				
			</div>
			<p><img src="http://www.oeck.com/assets/images/web/png/500/deviceProfiles.png">
			</p>
		</div>
	</div>
</div>

<div>
	<div data-aos="fade-up">
		<div>
			<p><img src="http://www.oeck.com/assets/images/web/png/500/adBlocker.png">
			</p>
			<div>
				<p><img src="http://www.oeck.com/assets/images/web/png/500/adBlocker.png"></p><div>
				<p><span>Cerberus</span>
				<br>
				Get powerful device-level filtering to prevent dangerous content reaching your family and devices. Our unique online guardian Cerberus is the must-have feature for families and individuals. Choose which content to block and prevent threats before they occur. Simply create a profile for your device and select the Cerberus services required.
					</p><p>
					
					You can filter Ads, Malware and Phishing, Adult and Social Networking sites individually or combined!
				</p></div>
				
			</div>
		</div>
	</div>
</div>

<div>
	<div>
		<div>
			<div>
				<p><span>Security</span>
					<br>
					<span>Built deep into our network and culture.</span>
				</p>
				<div data-aos="fade-up"><p>
					We secure your privacy using industry-leading encryption standards, on servers that we own. Our zero hard drive system won’t store any of your data, ever! Quickly block dangerous sites and services at the DNS level to prevent ads, malware, phishing sites and more.
					</p>
					
				</div>
			</div>
		</div>
	</div>	
</div>

<div>
	<div>
		<div data-aos="fade-up">
			<p><span>AES-256</span>
				<br>
				<span>Encryption</span>
			</p>
			<p><span>4096-bit</span>
				<br>
				<span>Key Exchange</span>
			</p>
			<p><span>Zero</span>
				<br>
				<span>Hard Drives</span>
			</p>
			<p><span>Zero</span>
				<br>
				<span>Logging</span>
			</p>
		</div>
	</div>
</div>

<div>
	<div data-aos="fade-up">
		<div>
			<p><img src="http://www.oeck.com/assets/images/web/png/500/portForwarding.png">
			</p>
			<div>
				<p><img src="http://www.oeck.com/assets/images/web/png/500/portForwarding.png"></p><p>
				<span>Advanced Port Forwarding</span>
				<br>
				This is a simple but handy feature with a twist. We issue you ports and you enable ports you select on your device profile(s). Then you simply tell us which port you would like to forward to. No need to configure your client or software to suit us. As an added bonus, you get your very own custom domain name per port. Regardless of which VPN region you connect to, your port-forwarding will always work!
				</p>
				
			</div>
		</div>
	</div>
</div>

<div>
	<div data-aos="fade-up">
		<div>
			<div>
				<p><img src="http://www.oeck.com/assets/images/web/png/500/customFilter.png"></p><p>
				<span>Custom DNS Filter</span>
				<br>
				Our built-in DNS filter allows you to decide what internet traffic you want hitting your device(s). Simply populate your filter list with websites that you want blocked and Oeck's VPN will follow those rules and block the traffic. Domain black lists can be set on a per-device level, which makes is perfect for parents who would like to restrict what their children can access. Best of all, it is completely unique to you.
				</p>
				
			</div>
			<p><img src="http://www.oeck.com/assets/images/web/png/500/customFilter.png">
			</p>
		</div>
	</div>
</div>

<div>
	<div data-aos="fade-up">
		<div>
			<p><img src="http://www.oeck.com/assets/images/web/png/500/customDNS.png">
			</p>
			<div>
				<p><img src="http://www.oeck.com/assets/images/web/png/500/customDNS.png"></p><p>
				<span>Secondary DNS</span>
				<br>
				A feature built for advanced users. Secondary DNS allows you to specify a DNS service to use that will bypass Oeck's DNS. To further simplify the feature, you can still allow Oeck to take control of some of the DNS queries and leave others up to your Secondary DNS.
				</p>
				
			</div>
		</div>
	</div>
</div>

<div>
	<div data-aos="fade-up">
		<div>
			<div>
				<p><img src="http://www.oeck.com/assets/images/web/png/500/serverSelection.png"></p><p>
				<span>Automatic Server Selection</span>
				<br>
				When selecting a VPN region to connect to, our network runs a check of the available servers and resources within that region. It then calculates which server will be the best server for you to connect to. It takes into account the available system resources of each server and so it will always connect you to the best available server. No more server surfing ever again!
				</p>
				
			</div>
			<p><img src="http://www.oeck.com/assets/images/web/png/500/serverSelection.png">
			</p>
		</div>
	</div>
</div>




	




</div>
				
			</div>

			
		</div>

		

		
	</div>
</div></div>]]>
            </description>
            <link>http://www.oeck.com/features/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24937073</guid>
            <pubDate>Thu, 29 Oct 2020 22:53:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Sun is more active now than over the last 8000 years (2004)]]>
            </title>
            <description>
<![CDATA[
Score 44 | Comments 18 (<a href="https://news.ycombinator.com/item?id=24937001">thread link</a>) | @firebaze
<br/>
October 29, 2020 | https://www.mpg.de/research/sun-activity-high | <a href="https://web.archive.org/web/*/https://www.mpg.de/research/sun-activity-high">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  
  
  <p>An international team of scientists has reconstructed the Sun's activity over the last 11 millennia and forecasts decreased activity within a few decades</p>
  

  

  <p>The activity of the Sun over the last 11,400 years, i.e., back to the end of the last ice age on Earth, has now for the first time been reconstructed quantitatively by an international group of researchers led by Sami K. Solanki from the Max Planck Institute for Solar System Research (Katlenburg-Lindau, Germany). The scientists have analyzed the radioactive isotopes in trees that lived thousands of years ago. As the scientists from Germany, Finland, and Switzerland report in the current issue of the science journal "Nature" from October 28, one needs to go back over 8,000 years in order to find a time when the Sun was, on average, as active as in the last 60 years. Based on a statistical study of earlier periods of increased solar activity, the researchers predict that the current level of high solar activity will probably continue only for a few more decades.</p>
  
  
<figure data-description="A large sunspot observed on the Sun in early September 2004. The field of view encompasses around 45,000 by 30,000 km of the Sun’s surface - the entire earth would fit into the area several times over. Sunspots appear dark because the strong magnetic field in the them suppresses the transport of energy through gas flow. In the central dark area of the sunspot (umbra) the magnetic field is perpendicular to the surface, whereas in the lighter coloured periphery (penumbra) the magnetic field is largely horizontal to the surface. The image was captured by Vasily Zakharov with a one-meter solar telescope on the island of La Palma. The telescope is operated by the Institute for Solar Physics of the Royal Swedish Academy of Sciences." data-picture="base64;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">
      
    

    
    <figcaption>
        <p>
          A large sunspot observed on the Sun in early September 2004. The field of view encompasses around 45,000 by 30,000 km of the Sun’s surface - the entire earth would fit into the area several times over. Sunspots appear dark because the strong magnetic field in the them suppresses the transport of energy through gas flow. In the central dark area of the sunspot (umbra) the magnetic field is perpendicular to the surface, whereas in the lighter coloured periphery (penumbra) the magnetic field is largely horizontal to the surface. The image was captured by Vasily Zakharov with a one-meter solar telescope on the island of La Palma. The telescope is operated by the Institute for Solar Physics of the Royal Swedish Academy of Sciences.
        </p>
        <p>
          © Max Planck Institute for Solar System Research
        </p>
    </figcaption>
</figure>


<p>The research team had already in 2003 found evidence that the Sun is more active now than in the previous 1000 years. A new data set has allowed them to extend the length of the studied period of time to 11,400 years, so that the whole length of time since the last ice age could be covered. This study showed that the current episode of high solar activity since about the year 1940 is unique within the last 8000 years. This means that the Sun has produced more sunspots, but also more flares and eruptions, which eject huge gas clouds into space, than in the past. The origin and energy source of all these phenomena is the Sun's magnetic field.</p>

<p>Since the invention of the telescope in the early 17th century, astronomers have observed sunspots on a regular basis. These are regions on the solar surface where the energy supply from the solar interior is reduced owing to the strong magnetic fields that they harbour. As a consequence, sunspots are cooler by about 1,500 degrees and appear dark in comparison to their non-magnetic surroundings at an average temperature of 5,800 degrees. The number of sunspots visible on the solar surface varies with the 11-year activity cycle of the Sun, which is modulated by long-term variations. For example, there were almost no sunspots seen during the second half of the 17th century.</p>

<p>For many studies concerning the origin of solar activity and its potential effect on long-term variations of Earth's climate, the interval of time since the year 1610, for which systematic records of sunspots exist, is much too short. For earlier times the level of solar activity must be derived from other data. Such information is stored on Earth in the form of "cosmogenic" isotopes. These are radioactive nuclei resulting from collisions of energetic cosmic ray particles with air molecules in the upper atmosphere. One of these isotopes is C-14, radioactive carbon with a half life of 5730 years, which is well known from the C-14 method to determine the age of wooden objects. The amount of C-14 produced depends strongly on the number of cosmic ray particles that reach the atmosphere. This number, in turn, varies with the level of solar activity: during times of high activity, the solar magnetic field provides an effective shield against these energetic particles, while the intensity of the cosmic rays increases when the activity is low. Therefore, higher solar activity leads to a lower production rate of C-14, and vice versa.</p>

<p>By mixing processes in the atmosphere, the C-14 produced by cosmic rays reaches the biosphere and part of it is incorporated in the biomass of trees. Some tree trunks can be recovered from below the ground thousands of years after their death and the content of C-14 stored in their tree rings can be measured. The year in which the C-14 had been incorporated is determined by comparing different trees with overlapping life spans. In this way, one can measure the production rate of C-14 backward in time over 11,400 years, right to the end of the last ice age. The research group have used these data to calculate the variation of the number of sunspots over these 11,400 years. The number of sunspots is a good measure also for the strength of the various other phenomena of solar activity.</p>

<p>The method of reconstructing solar activity in the past, which describes each link in the complex chain connecting the isotope abundances with the sunspot number with consistent quantitative physical models, has been tested and gauged by comparing the historical record of directly measured sunspot numbers with earlier shorter reconstructions on the basis of the cosmogenic isotope Be-10 in the polar ice shields. The models concern the production of the isotopes by cosmic rays, the modulation of the cosmic ray flux by the interplanetary magnetic field (the open solar magnetic flux), as well as the relation between the large-scale solar magnetic field and the sunspot number. In this way, for the first time a quantitatively reliable reconstruction of the sunspot number for the whole time since the end of the last ice age could be obtained.</p>
<figure data-description="Top: Reconstructed sunspot activity (10 year average) for the last 11,400 years based on C-14 data (blue curve) and the directly observed historical sunspot data since 1610 (red curve). The reliable C-14 data ends around the year 1900 so that the sharp increase in sunspot activity in the 20th century does not appear in the graph. The reconstruction shows clearly that a comparable period of high sunspot activity previously existed over 8000 years ago. Below: An enlarged section of the upper graph (hatched area) with several episodes of higher sun activity; comparable to the 20th century." data-picture="base64;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">
      
    

    
    <figcaption>
        <p>
          Top: Reconstructed sunspot activity (10 year average) for the last 11,400 years based on C-14 data (blue curve) and the directly observed historical sunspot data since 1610 (red curve). The reliable C-14 data ends around the year 1900 so that the sharp increase in sunspot activity in the 20th century does not appear in the graph. The reconstruction shows clearly that a comparable period of high sunspot activity previously existed over 8000 years ago. Below: An enlarged section of the upper graph (hatched area) with several episodes of higher sun activity; comparable to the 20th century.
        </p>
        <p>
          © Max Planck Institute for Solar System Research
        </p>
    </figcaption>
</figure>


<p>Because the brightness of the Sun varies slightly with solar activity, the new reconstruction indicates also that the Sun shines somewhat brighter today than in the 8,000 years before. Whether this effect could have provided a significant contribution to the global warming of the Earth during the last century is an open question. The researchers around Sami K. Solanki stress the fact that solar activity has remained on a roughly constant (high) level since about 1980 - apart from the variations due to the 11-year cycle - while the global temperature has experienced a strong further increase during that time. On the other hand, the rather similar trends of solar activity and terrestrial temperature during the last centuries (with the notable exception of the last 20 years) indicates that the relation between the Sun and climate remains a challenge for further research.</p>
  
</div></div>]]>
            </description>
            <link>https://www.mpg.de/research/sun-activity-high</link>
            <guid isPermaLink="false">hacker-news-small-sites-24937001</guid>
            <pubDate>Thu, 29 Oct 2020 22:44:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[For Complex Applications, Rust Is as Productive as Kotlin]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24935355">thread link</a>) | @dochtman
<br/>
October 29, 2020 | https://ferrous-systems.com/blog/rust-as-productive-as-kotlin/ | <a href="https://web.archive.org/web/*/https://ferrous-systems.com/blog/rust-as-productive-as-kotlin/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div id="preamble">
<div>
<p>In this article, we will compare one apple (IntelliJ Rust) to one orange (rust-analyzer) to reach general and sweeping conclusions.
Specifically, I want to present a case study supporting the following claim:</p>
<p>For complex applications, Rust is as productive as Kotlin.</p>
<p>For me, this is an unusual claim to argue: I always thought exactly the opposite, but I am not so sure now.
I came to Rust from C++.
I was of the opinion that this is a brilliant low-level language and always felt puzzled at people writing higher-level things in Rust.
Clearly, choosing Rust means taking a productivity hit, and using Kotlin, C# or Go just makes much more sense if you can afford GC.
My <a href="https://matklad.github.io/2020/09/20/why-not-rust.html">list of Rust criticisms</a> starts with this objection.</p>
<p>What moved my position in the other direction was my experience as the lead developer of rust-analyzer and IntelliJ Rust.
Let me introduce the two projects.</p>
<p><a href="https://github.com/intellij-rust/intellij-rust"><strong>IntelliJ Rust</strong></a> is the plugin for IntelliJ Platform, providing Rust support.
In effect, it is a Rust compiler front-end, written in Kotlin and making use of language-support features of the platform.
These features include lossless syntax trees, a parser generator, persistence and indexing infrastructure, among others.
Nonetheless, as programming languages differ lot, the bulk of logic for analyzing Rust is implemented in the plugin itself.
Presentational features like completion list come from the platform, but most of the language semantics is hand-written.
IntelliJ Rust also includes a bit of a Swing GUI.</p>
<p><a href="https://github.com/rust-analyzer/rust-analyzer"><strong>rust-analyzer</strong></a> is an implementation of
<a href="https://microsoft.github.io/language-server-protocol/">Language Server Protocol</a> for Rust.
It is a Rust compiler front-end written from scratch with an eye towards IDE support.
It makes heavy use of <a href="https://github.com/salsa-rs/salsa">salsa</a> library for incremental computations.
Beyond the compiler itself, rust-analyzer includes code for managing long-lived multithreaded process of the language server itself.</p>
<p>The projects are essentially equivalent in scope — rust compiler front-ends suitable for IDEs.
The two biggest differences are:</p>
<div>
<ul>
<li>
<p>IntelliJ Rust is a plugin, so it can re-use code and design patterns of the surrounding platform.</p>
</li>
<li>
<p>rust-analyzer is the second system, so it leverages experience of IntelliJ Rust for a from-scratch design.</p>
</li>
</ul>
</div>
<p>The internal architecture of the two projects also differs a lot.
In terms of <a href="https://rust-analyzer.github.io/blog/2020/07/20/three-architectures-for-responsive-ide.html">Three Architectures</a>, IntelliJ Rust is map-reduce, and rust-analyzer is query-based.</p>
<p>Writing an IDE-ready compiler is a high-level task.
You don’t need to talk to the operating system directly.
There are some fancy data structures and concurrency here and there, but they are also high-level.
It’s not about implementing crazy lock-free schemes, it’s about maintaining application state and sanity in the multithreaded world.
The bulk of the compiler is symbolic manipulation, arguably best suited for lisp.
Picking a VM-based language for such task (for example, OCaml), doesn’t have any intrinsic downsides.</p>
<p>At the same time, the task is pretty complex and unique.
The ratio of “your code” vs “framework code” when implementing features is much higher than in a typical CRUD backend.</p>
<p>Now that the projects are introduced, lets take two roughly equivalent slices of history.</p>

<p>Both are about 2 years old, with 1-1.5 developers working full time and vibrant and thriving community of open-source contributors.
There are 52k lines of Kotlin and 66k lines of Rust.</p>
<p>Both delivered roughly equivalent feature sets at that time.
To be honest, I still don’t really believe this :)
rust-analyzer started from zero, it didn’t have a decade worth of Java classes to bootstrap from, and the productivity drop between Kotlin and Rust is supposed to be huge.
But it’s hard to argue with reality.
Instead, let me try to reflect on my experience building both, and to try to explain Rust’s surprising productivity.</p>
</div>
</div>
<div>
<h2 id="_learning_curve">Learning Curve</h2>
<div>
<p>It’s easy to characterize Kotlin’s learning curve — it is nearly zero.
I’ve started IntelliJ Rust without Kotlin experience and never felt that I need to specifically learn Kotlin.</p>
<p>When I switched to rust-analyzer, I was pretty experienced with Rust.
I would say that one definitely needs to deliberately learn Rust, it’s hard to pick it up on the go.
Ownership and aliasing control are novel concepts (even if you come from C++), and taking holistic approach to learning them pays off.
After the initial learning step the ride is generally smooth.</p>
<p>By the way, this is the perfect place to plug our Rust courses and tailor-made <a href="https://ferrous-systems.com/training/">trainings</a> :-)
The next <a href="https://ferrous-systems.com/training/#package-intro-training">introduction to Rust</a> is happening this December!</p>
</div>
</div>
<div>
<h2 id="_modularity">Modularity</h2>
<div>
<p>This I think is the biggest factor.
Both projects are moderately large in terms of scope as well as in terms of amount of source code.
I believe that the only way to ship big things is to split them in independent-ish chunks and implement the chunks separately.</p>
<p>I also find most of the languages I am familiar with to be pretty horrible with respect to modularity.
More generally, I am amused with FP vs OO debate, as it seems that “why no one does modules right?” is a more salient issue.</p>
<p>Rust is one of the few languages which has first-class concept of libraries.
Rust code is organized on two levels:</p>
<div>
<ul>
<li>
<p>as a tree of inter-dependent modules inside a crate</p>
</li>
<li>
<p>and as a directed acyclic graph of crates</p>
</li>
</ul>
</div>
<p>Cyclic dependencies are allowed between the modules, but not between the crates.
Crates are units of reuse and privacy: only crate’s public API matters, and it is crystal clear what crate’s public API is.
Moreover, crates are anonymous, so you don’t get name conflicts and dependency hell when mixing several versions of the same crate in a single crate graph.</p>
<p>This makes it very easy to make two pieces of code <strong>not</strong> depend on each other (non-dependencies are the essence of modularity): just put them in separate crates.
During code review, only changes to Cargo.tomls need to be monitored carefully.</p>
<p>At the time of comparison, rust-analyzer is split into 23 internal crates, with a handful general-purposed ones released on crates.io.
In contrast, IntelliJ Rust is a single Kotlin module, where everything can depend on everything else.
Although internal organization of IntelliJ Rust is pretty clean, it’s not reflected in the file system layout and build system, and needs constant maintenance.</p>
</div>
</div>
<div>
<h2 id="_build_system">Build System</h2>
<div>
<p>Managing project’s build takes significant amount of times, and has multiplicative effect on everything else.</p>
<p>Rust’s build system, <a href="https://doc.rust-lang.org/cargo/index.html">Cargo</a>, is very good.
It’s not perfect, but it is a breath of fresh air after Java’s <a href="https://gradle.org/">Gradle</a>.</p>
<p>Cargo’s trick is that it doesn’t try to be a general purpose build system.
It can only build Rust projects, and it has rigid expectation about the project structure.
It’s impossible to opt out of the core assumptions.
Configuration is a static non-extensible TOML file.</p>
<p>In contrast, Gradle allows free-form project structure, and is configured via a Turing complete language.
I feel like I’ve spend more time learning Gradle than learning Rust!
Running <code>wc -w</code> gives 182_817 words for Rust book, and 280_506 for Gradle’s user guide.</p>
<p>Additionally, Cargo is just faster than Gradle in most cases.</p>
<p>Of course, the biggest downside is that custom build logic is not expressible in Cargo.
Both projects needs substantial amount of logic beyond mere compilation to deliver the final result to the user.
For rust-analyzer, this is handled by hand-written Rust script, which works perfectly at this scale.</p>
</div>
</div>
<div>
<h2 id="_ecosystem">Ecosystem</h2>
<div>
<p>Language-level support for libraries and top-notch build system/package manager allow for a thriving ecosystem.
rust-analyzer relies on third-party libraries much more than IntelliJ Rust.
Some parts of rust-analyzer are also published to crates.io for other projects to reuse.</p>
<p>Additionally, low-level nature of the Rust programming language often allows for “perfect” library interfaces.
Interfaces which exactly reflect the underlying problem, without imposing intermediate language-level abstractions.</p>
</div>
</div>
<div>
<h2 id="_basic_conveniences">Basic Conveniences</h2>
<div>
<p>I feel that Rust is significantly more productive when it comes to basic language nuts and bolts — structs, enums, functions, etc.
This is not specific to Rust — any ML-family language has them.
However, Rust is the first industrial language which wraps these features in a nice package, not constrained by backwards compatibility.
I want to list specific features which I think allow producing maintainable code faster in Rust</p>
<p><em>Emphasis on data over behavior</em>.
Aka, Rust is not an OOP language.
The core idea of OOP is that of dynamic dispatch — which code is invoked by a function call is decided at runtime (late binding).
This is a powerful pattern which allows for flexible and extensible system.
The problem is, extensibility is costly!
It’s better only to apply it in certain designated areas.
Designing for extensibility by default is not cost effective.
Rust puts static dispatch front and center: it is mostly clear whats going on by just reading the code, as it is independent of runtime types of the objects.</p>
<p>One small syntactic thing I enjoy about Rust is how it puts fields and methods into different blocks syntactically:</p>
<div>
<div>
<pre><code data-lang="rust"><span>struct</span> <span>Person</span> <span>{</span>
  <span>first_name</span><span>:</span> <span>String</span><span>,</span>
  <span>last_name</span><span>:</span> <span>String</span><span>,</span>
<span>}</span>

<span>impl</span> <span>Person</span> <span>{</span>
    <span>fn</span> <span>full_name</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> <span>-&gt;</span> <span>String</span> <span>{</span>
        <span>...</span>
    <span>}</span>
<span>}</span></code></pre>
</div>
</div>
<p>Being able to see at a glance all the fields makes understanding the code much simpler.
Fields convey much more information than methods.</p>
<p><em>Sum types</em>.
Rust’s humbly named enums are full algebraic data types.
This means that you can express the idea of disjoint union:</p>
<div>
<div>
<pre><code data-lang="rust"><span>enum</span> <span>Either</span><span>&lt;</span><span>A</span><span>,</span> <span>B</span><span>&gt;</span> <span>{</span> <span>A</span><span>(</span><span>A</span><span>),</span> <span>B</span><span>(</span><span>B</span><span>)</span> <span>}</span></code></pre>
</div>
</div>
<p>This is hugely useful in day-to-day programming in the small, and some times during programming in the large as well.
To give one example, one of the core concepts for an IDE are references and definitions.
A definition a like <code>let foo = 92;</code> assigns a name to an entity which can be used down a line.
A reference like <code>foo + 90</code> <em>refers</em> to some definition.
When you ctrl-click on reference, you go to the definition.</p>
<p>Natural way to model that in Kotlin is by adding <code>interface Definition</code> and <code>interface Reference</code>.
The problem is, some things are …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ferrous-systems.com/blog/rust-as-productive-as-kotlin/">https://ferrous-systems.com/blog/rust-as-productive-as-kotlin/</a></em></p>]]>
            </description>
            <link>https://ferrous-systems.com/blog/rust-as-productive-as-kotlin/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24935355</guid>
            <pubDate>Thu, 29 Oct 2020 20:28:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Learn Machine Learning and Deep Learning: A Guide for Software Engineers]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24934686">thread link</a>) | @renanmoura
<br/>
October 29, 2020 | https://renanmf.com/machine-learning-and-deep-learning-software-engineers/ | <a href="https://web.archive.org/web/*/https://renanmf.com/machine-learning-and-deep-learning-software-engineers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2>Introduction</h2><p>The subject of Artificial Intelligence picks my interest and I’m constantly studying and trying new things in this field.</p><p>It is notorious how the technologies related to Natural Language Processing, Computer Vision and such have emerged and evolved into solutions used by millions of users every day.</p><p>Even though people use the term "Artificial Intelligence", we are still far away from something as advanced as a Skynet from the Terminator movies.</p><p>The most common subfield of AI used today is the one called Machine Learning, which, in its turn, has Deep Learning as subfield steeply growing every day for quite some time now.</p><p>In this guide, I aim to describe a path to follow for software engineers to begin understanding how Machine Learning works and how to apply it to your projects.</p><p>Yeah, you can just go to Google API’s or Amazon and pick some magical API to do Speech Recognition for you, but the value of knowing how it works, why it works and even more, how to make your own API as a Service and tune it to your specific needs is incredible.</p><p>Remember, as a developer, every tool is a new power.</p><p>I’ve read, watched and gone through all these resources until the end, even got a paid certification for some, even though it is not necessary to learn, I find myself more engaged to finish when I have some deadline and assessment to prove I actually learned the material.</p><p>Let’s dive into the topics.</p><h2>The Basics: Math!</h2><p>Maybe you never had the chance to study some college-level math, or you did study it but you can’t remember most of the stuff because JavaScript and CSS took all the memory of those topics away.</p><p>There are 3 topics you must know beforehand, or at least have a decent grasp of to follow any good material on ML and DL: Linear Algebra, Calculus and Statistics.</p><p>If you’d like to go deep in learning the math needed to ML and DL, you can look for MIT OpenCourseWare classes like Professor Strang’s renowned <a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/">Linear Algebra</a> class.</p><p>I’ve watched it in college in parallel with my regular class and it is very good.</p><p>But, let’s face it, most people have no time for that or the patience.</p><p>So I will give you the crash course for the 3 topics mentioned above.</p><h3>Linear Algebra</h3><p>Just watch the whole series <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra</a> from the Youtube channel 3Blue1Brown.</p><p>The guy makes visual explanations of once hard concepts incredibly easy!</p><p>It is very far in terms of content compared to Professor Strang’s, but it’s enough, to begin with, and you can go after other topics as you advance in ML and DL.</p><h3>Calculus</h3><p>Guess what?</p><p>3Blue1Brown also has a whole series on Calculus on Youtube for you to watch for free: <a href="https://www.youtube.com/watch?v=WUvTyaaNkzM&amp;list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">Essence of Calculus</a>.</p><p>Again, he is very good at giving you the intuition of why and how rather than just throw some random equations on your face.</p><h3>Statistics</h3><p>This is a whole field that, in my opinion, you can learn as needed, a good reference is <a href="https://www.amazon.com/Practical-Statistics-Data-Scientists-Essential/dp/1491952962">Practical Statistics for Data Scientists: 50 Essential Concepts</a>.</p><p>An objective book with some good examples for every concept.</p><p>Fast to read too.</p><p>As the title implies, it is more suitable for Data Scientists, but understanding some basics of statistics is always good and this is what this is book is for.</p><p>You won’t become a statistician after reading it, but you will learn some good stuff.</p><h2>The Bypassed: Machine Learning</h2><p>Everybody wants to jump straight into Deep Learning and be the cool guy training a single model for a week on a 12GB GPU.</p><p>But to get Deep Learning right, you need to go through Machine Learning first!</p><h3>Start from the beginning</h3><p>The concepts, the train of thought, the "feeling" of how things work start here and there is no one else more capable of teaching those concepts than Professor Andrew Ng in his course <a href="https://www.coursera.org/learn/machine-learning">Machine Learning</a>.</p><p>You may think this course is old and outdated, well, technology-wise, maybe, but conceptually-wise, it is better than anything else out there.</p><p>Professor Ng makes it easy to understand the math applied in every technique he teaches and gives you a solid understanding of what happens underneath in a very short and concise course.</p><p>All the exercises are made in Octave, a free version of Matlab of sorts, and you finish the course implementing your own Neural Network!</p><p>The syntax in Octave is easy to grasp for any programmer, so don’t let that be a barrier for you.</p><p>Once you finish the course, you will have implemented all the major algorithms and will be able to solve several prediction problems.</p><h3>Random Forests</h3><p>I said all the major algorithms, right?</p><p>Actually, there is but one flaw in Andrew Ng’s course, he doesn’t cover Random Forests.</p><p>An awesome complement to his course is fast.ai’s <a href="http://course18.fast.ai/ml">Introduction to Machine Learning for Coders</a>.</p><p>Jeremy Howard goes super practical on the missing piece in Ng’s course covering a topic that is, for many classical problems, the best solution out there.</p><p>Fast.ai’s approach is what is called Top-Down, meaning they show you how to solve the problem and then explain why it worked, which is the total opposite of what we are used to in school.</p><p>Jeremy also uses real-world tools and libraries, so you learn by coding in industry-tested solutions.</p><h2>Deep Learning</h2><p>Finally!</p><p>The reason why we are all here, Deep Learning!</p><p>Again, the best resource for it is Professor Ng’s course, actually, a series of courses.</p><p>The <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a> is composed of 5 courses total going from the basics and evolving on specific topics such as language, images, and time-series data.</p><p>One nice thing is that he continues from the very end of his classical Machine Learning course, so it just feels like an extension of the first course.</p><p>The math, the concepts, the notion of how and why it works, he delivers it all very concisely like few I’ve seen.</p><p>The only drawback is that he uses <a href="https://www.tensorflow.org/">Tensorflow</a> 1.x (Google’s DL Framework) in this course, but that’s minimal detail in my opinion since the explanations and exercises are so well delivered.</p><p>You can pick up the most recent version of the framework relatively easy and to do so there is the final piece of this guide, a book.</p><h3>Too much stuff, give me something faster</h3><p>This book might be the only thing you need to start, it is Aurélien Géron’s <a href="https://www.amazon.com.br/Hands-Machine-Learning-Scikit-Learn-TensorFlow-ebook/dp/B07XGF2G87">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems</a>.</p><p>It covers a lot, from classical Machine Learning to the most recent Deep Learning topics. Good examples and exercises using industry-grade frameworks and libraries.</p><p>I dare say that, if you are really in a rush, you can skip everything I said before and just go for the book.</p><p>You will miss a good amount of information contained on the other resources mentioned, but the practical and actionable knowledge from Géron’s book is enough to work on many ideas for your next project.</p><p>If you feel limited after only reading the book, go back and study the rest of the material, it will fill in the gaps you might have and give you a more solid understanding.</p><h2>What about Framework X or Y?</h2><p>"Hey, I’ve heard about PyTorch and that other framework or library X everybody talks about".</p><p>As a Software Engineer, you know better than anyone how fast technology evolves.</p><p>Don’t go crazy for that, after you learn the basics in this guide, you can easily go, for instance, on <a href="https://pytorch.org/">PyTorch</a> documentation or any other library or framework of sorts and learn how to use it in a week or two.</p><p>The techniques, the concepts, are all the same, it is only a matter of syntax and application or even tastes that you might have for any given tool.</p><h2>Conclusion</h2><p>To wrap it up, I want to say that, even though it might seem a lot, I tried to remove all the noise and at the end of the process, you will feel confident that you understand what is happening behind the curtains, the jargons and even be able to read some papers published in the field to keep up with the latest advances.</p><p>TL;DR Here is the list of resources mentioned in sequence:</p><ul><li><a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra</a></li><li><a href="https://www.youtube.com/watch?v=WUvTyaaNkzM&amp;list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">Essence of Calculus</a></li><li><a href="https://www.coursera.org/learn/machine-learning">Machine Learning</a></li><li><a href="http://course18.fast.ai/ml">Introduction to Machine Learning for Coders</a></li><li><a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a></li><li><a href="https://www.amazon.com.br/Hands-Machine-Learning-Scikit-Learn-TensorFlow-ebook/dp/B07XGF2G87">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems</a></li></ul></div></div>]]>
            </description>
            <link>https://renanmf.com/machine-learning-and-deep-learning-software-engineers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24934686</guid>
            <pubDate>Thu, 29 Oct 2020 19:37:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Don't contribute anything relevant in web forums]]>
            </title>
            <description>
<![CDATA[
Score 172 | Comments 121 (<a href="https://news.ycombinator.com/item?id=24934569">thread link</a>) | @todsacerdoti
<br/>
October 29, 2020 | https://karl-voit.at/2020/10/23/avoid-web-forums/ | <a href="https://web.archive.org/web/*/https://karl-voit.at/2020/10/23/avoid-web-forums/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
<ul>
<li>Updates
<ul>
<li>2020-10-25 Comment by Erik</li>
</ul></li>
</ul>

<p>

If you're, for example, contributing to a <a href="https://en.wikipedia.org/wiki/Reddit">reddit</a> thread about something which is irrelevant or anything with only a short-term relevance, this article does not apply to you right now.

</p>

<p>

However, as soon as you're helping somebody solving an interesting issue, summarize your experiences with something or write anything that might be cool to be around in a couple of years as well, you do provide potential high-value content. My message to all those authors is: <b>don't use web-based forums</b>.

</p>

<p>

TL;DR: all of the content of closed, centralized services will be lost in the long run. Choose the platform you contribute to wisely now instead of learning through more large data loss events later-on.

</p>

<p>

The longer version is worth your time:

</p>

	  <header><h2>What Do I Mean With Web-Based Forums Here?</h2></header>

<p>

In this article, I'm using the term "web-based forums" as an umbrella term for closed, centralized services like <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a>, <a href="https://en.wikipedia.org/wiki/Hacker_News">Hacker News</a>, <a href="https://en.wikipedia.org/wiki/Slashdot">Slashdot</a>, Facebook, or any other web-based forum where you are able to add comments, articles, and so forth in most cases only after creating an account.

</p>

<p>

Typically, those services don't provide any possibility to extract or synchronize content. They don't offer open APIs that allow users to choose among different and open user interfaces. They are owned and operated by private companies.

</p>

<p>

Please note that when I'm going to mention more or less only reddit as an example in the next sections, this is because reddit is the only web-based forum <a href="https://www.reddit.com/user/publicvoit">I'm familiar with</a> to a certain level. This does not mean that reddit is worse than other closed, centralized web-based forums. Not at all.

</p>

	  <header><h2>So What's the Issue With Web-Based Forums?</h2></header>

<p>

There is not one issue. There are several things where web-based forums don't qualify for being a platform for quality content. Let's take a look at some of them.

</p>

<p>

I'm glad you're still reading this article and I hope you bear with me until the end of it. Most people will realize and learn about having contributed lots and lots of high-value information only when platforms are down for good. And this is what makes me really sad. It is just like you know that one building of the Library of Alexandria is going to burn down in a few years and people still bring many unique copies of high-quality books into its shelves, unaware of destroying knowledge this way.

</p>

	  <header><h3>Issue: No Backup, No Distribution</h3></header>

<p>

For reasons and examples stated <a href="https://karl-voit.at/cloud">in this article</a>, any centralized web-based service will go offline some day. Some sooner, some later. Popularity is not even a guarantee that a service gets continued, as you can see with <a href="https://killedbygoogle.com/">hundreds of (partly) very well known and used Google services that were shut down</a>. Nothing will be on the web forever. Most people are not aware of this fact. The books set on this machine are more likely to survive history than all of your reddit/Facebook/... contributions:

</p>

<figure>
<img src="https://karl-voit.at/2020/10/23/avoid-web-forums/2019-10-05T22.56.47%20Buchdruckmuseum%20-%20Linotype%20-%20Tastatur%20--%20cliparts%20typography%20history%20publicvoit%20-%20scaled%20width%20630.jpg" alt="" width="630">
<figcaption>A Linotype machine.</figcaption>
</figure>

<p>

So when you begin to be aware of this fact, you might want to think of things you can do to mitigate data loss when services are discontinued or "sunrized" as some marketing experts say.

</p>

<p>

You could, for example, back-up the data of this service. By providing the information on multiple servers, chances are high that not all of them are lost at the same time.

</p>

<p>

This requires certain properties. For example, you need to be able to duplicate the service on multiple servers. To be able to do so, you'll need not only the data but also the software that is providing access to the service. When different organization are running mirrored servers, it is required to openly share the data and software. This can be ensured by using Open Source software or at least open APIs and a business model that does not rely on keeping data and technical things a secret.

</p>

<p>

All major commercial services such as reddit, Facebook and so forth keep everything a secret that is not ultimately necessary to use their services. Their software is a secret, they don't offer open APIs or only very crippled ones, you don't have the possibility to get to the raw data. So no luck there. You do have <a href="https://en.wikipedia.org/wiki/Vendor_lock-in">a lock-in situation</a>.

</p>

<figure>
<img src="https://karl-voit.at/2020/10/23/avoid-web-forums/2020-10-23%20Oatmeal_-_reaching_people_on_the_internet%20--%20publicvoit%20-%20scaled%20width%20630.png" alt="" width="630">
<figcaption>https://theoatmeal.com/comics/reaching_people</figcaption>
</figure>

<p>

Even with personal blogs, "fragile" as they are, you are able to use the <a href="https://archive.org/web/">Wayback Machine of the Internet Archive</a> to back up your blog. For example, every page on my blog contains a link to its archive in the page footer. This ensures that you can not only browse the latest version of all of my blog articles in case of a server breakdown. This also enables you to browse all previous version, probably changed over time. Go ahead, try a few "Archive" links of my articles. If any of my articles start with an "Updates:" section, you know for sure that there are older versions accessible via the Internet Archive.

</p>

<p>

The Wayback Machine does not archive reddit threads. It can not properly back up Facebook pages. <a href="https://help.archive.org/hc/en-us/articles/360004651732-Using-The-Wayback-Machine">It's blinded by corporate secrecy</a> when it comes to archive content for the upcoming generations:

</p>

<blockquote>Why isn't the site I'm looking for in the archive?<br>
Some sites may not be included because the automated crawlers were
unaware of their existence at the time of the crawl. It's also
possible that some sites were not archived because they were password
protected, blocked by robots.txt, or otherwise inaccessible to our
automated systems. Site owners might have also requested that their
sites be excluded from the Wayback Machine.</blockquote>

<p>

Summarizing the things mentioned above: without very good support for data export, service duplication, open standards, any content you provide in closed web-based services will be lost just as <a href="https://www.nytimes.com/2019/03/19/business/myspace-user-data.html">MySpace already lost twelve years of content just so</a>, just to mention one big example.

</p>

	  <header><h3>Issue: User Interface Dictatorship</h3></header>

<p>

When you grew up only knowing centralized web-based forums, you can not imagine the many advantages of having the freedom to choose your preferred user interface. While some people might think this is a minor issue, let me explain a few examples where this makes a huge difference.

</p>

<p>

The first example starts with something that might only annoy people. With comments like on <a href="https://www.reddit.com/r/emacs/comments/hfamm7/those_who_have_tried_out_multiple_zettelkasten/fvx9vu5/">this thread</a>, you clutter up other people's interface for personal gain. It's selfish and distracts from the information consumption.

</p>

<p>

The reason why people are using such reminder bots is multi-fold. First, they don't use a proper todo management system that would be able to remind them to read a certain article in a few days. They externalize this inability to the web-based forum and all of its other users. <a href="https://karl-voit.at/tags/pim">I'm working on fixing these educational issues</a>. Secondly, there is no way to have features that you can use that do not affect other people's interface.

</p>

<p>

Consider people with visual impairment do have special needs. <a href="https://tinyurl.com/y6ncgvjt">The WHO reports</a> an estimate of 285 million people that do are visually impaired, ninety percent of them living in developing countries. Those are not numbers you can simply ignore. It is obvious that they do need different kind of interfaces. Either they have to use a high-contrast interface, highly unusual interface scaling factors, an interface that avoids certain color combinations, text-to-speech systems or <a href="https://en.wikipedia.org/wiki/Braille#Braille_reading">Braille readers</a> that are able to extract the content properly.

</p>

<p>

If a web-based services that - remember from before - does not offer proper open APIs and which does not implement said features, all those people simply can not participate and you can not profit from their knowledge and experience.

</p>

<p>

And even when you think that this is just a minority I can provide examples where everybody profits from choosing his or her own interface.

</p>

<p>

Some services are providing interfaces that aren't working properly on small displays or mobile devices in general. In these cases, without any ability to switch to an alternative app or web-page, you are locked out even with perfect eyesight.

</p>

<p>

When you're using an web-based forum that does not provide the feature that already read articles are marked or collapsed, you need to skim though a thread completely and re-read content to find out new postings when re-visiting the thread after a while. Our time should not spent on senseless tasks like this.

</p>

<p>

Alternative interfaces might provide advanced rating features based on your personal taste and choice so that you are able to filter out the most relevant articles easily and do not clutter your view with irrelevant articles at all. This is also called "scoring". It can be based on keywords, the amount of personal contributions to a longer thread, friendship relationships from your contact management, and so forth.

</p>

<p>

Some people prefer navigating using the keyboard. Either by personal taste or by physical restrictions. If the web-based centralized service only supports mouse-based navigation, you can not use this service.

</p>

<p>

I could continue with examples like that. The common theme is: when one particular centralized web-based forum is not implementing all of those nice features you need or like, you can not use them properly.

</p>

	  <header><h3>Issue: Rule Monopoly and Subjective Censorship</h3></header>

<p>

When you do live in a society with certain set of (legal) rules, providers of relevant web-based forums have to follow and enforce some of them. However, the issue is that this kind of censorship is and will always be related to a particular culture and society at a specific time.

</p>

<p>

For example, in Germany and Austria, being a <a href="https://en.wikipedia.org/wiki/Nazism">Nazi</a> is punishable by law. In the USA, freedom-loving people think fans of the human monsters that tortured and murdered millions of Jews in the Second World War need the possibility to express their personal "opinion". As you can see, there is a different point of view in-between the lines when I write about Nazis compared to an author from the USA who values "freedom of speech" higher than "being a die-hard fan of mass murders". It's a very difficult topic you can not enforce with a world-wide service.

</p>

<p>
</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://karl-voit.at/2020/10/23/avoid-web-forums/">https://karl-voit.at/2020/10/23/avoid-web-forums/</a></em></p>]]>
            </description>
            <link>https://karl-voit.at/2020/10/23/avoid-web-forums/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24934569</guid>
            <pubDate>Thu, 29 Oct 2020 19:27:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A12 – Advancing Network Transparency on the Desktop]]>
            </title>
            <description>
<![CDATA[
Score 30 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24934296">thread link</a>) | @todsacerdoti
<br/>
October 29, 2020 | https://arcan-fe.com/2020/10/28/a12-advancing-network-transparency-on-the-desktop/ | <a href="https://web.archive.org/web/*/https://arcan-fe.com/2020/10/28/a12-advancing-network-transparency-on-the-desktop/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
						
<p>This article is is the main course to the appetiser that was <a href="https://arcan-fe.com/2018/11/16/the-x-network-transparency-myth/">The X Network Transparency Myth</a> (2018). In it, we will go through how the pieces in the Arcan ecosystem tie together to advance the idea of network transparency for the desktop and how it sets the stage for a fully networked desktop.</p>



<p>Some of the points worth recalling from the X article are:</p>



<ol><li>‘transparency’ is evaluated from the perspective of the user; it is not even desirable for the underlying layers to be written so that they operate the same for local rendering as they would across a network. The local-optimal case is necessarily different from the remote one, the mechanisms are not the same and the differences will keep on growing organically with the advancement of hardware and display/rendering techniques.</li><li>side-band protocols splitting up the desktop into multiple IPC systems for audio, meta, fonts, … increases the difficulty to succeed with anything close to a transparent experience, as the network layer needs to take all of these into consideration as well as trying to synchronise them.</li></ol>



<p>To add a little to the first argument: it should also not be transparent to the window manager as some actions have drastically different impact on the user interface side to security and expectations. For example, Clipboard/DND locally is not (supposed to be) a complicated thing. When applied across a network, however, such things can degrade the experience for anything else. Other examples is that you want to block some sensitive inputs from being accidentally forwarded to a networked window and so on, it has happened in the past that the wrong sudo password has, indeed, been sent to the wrong ssh session.</p>



<p>This target has been worked on for a long time, as suggested by this part from the <a href="https://www.youtube.com/watch?v=3O40cPUqLb">old demo</a> from 2012/2013. Already back then the drag/slice to compose-transform-and-share case exposed out of compositor sharing and streaming; something that only now is appearing elsewhere in a comparably limited form.</p>



<p>We are on the third or fourth re-implementation of the idea, and the first one that is considered having a good enough of a design to commit to using and building upon. There are many fascinating nuances to this problem that only appear when you ‘try to go to 11’.</p>



<p>As per usual, parts of this post will be quite verbose and technical. Here are some shortcuts to jump around so that you don’t lose interest from details that seem irrelevant to you.</p>



<ul><li><a href="#primitives">Basic primitives: Arcan-net, A12 and SHMIF</a></li><li><a href="#usecases">Example Usecases</a></li><li><a href="#protocol">Protocol State and Development</a></li><li><a href="#explained">Demo Explained</a></li></ul>



<h2 id="demo">Demos</h2>



<p>Starting with some short clips of the development progress – and then work through the tools and design needed to make this happen. It might be short, but there is a whole world of nuance and detail to it.</p>



<p>(~early 2019) – forced compression, OSX viewer, (bad) audio:</p>



<figure></figure>



<p>Composited Xarcan (desktop to pinephone), compression based on window type:</p>



<figure><p><span><iframe width="640" height="360" src="https://www.youtube.com/embed/CIWZdEkgPfM?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span>
</p></figure>



<p>Here is a native arcan client with crypto, local GPU “hot-unplug” to software rendering handover and compression negotiation (h264):</p>



<figure><p><span><iframe width="640" height="360" src="https://www.youtube.com/embed/_RSvk7mmiSE?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span>
</p></figure>



<p>Here is ‘server-side’ text rendering of text-only windows, font, style and size controlled by presenting device — client migrates back when window is closed:</p>



<figure></figure>



<p><span>In the videos, you can see (if you squint) instances of </span><em>live migration</em><span> between display servers over a network, with a few twists. For example, the decorations, input mapping, font preferences and other visuals change to match the machine that the client is currently </span><em>presenting</em><span> on and that audio also comes along, because </span><a href="https://arcan-fe.com/2017/10/05/awk-for-realtime-multimedia/">Arcan does multimedia</a><span>, not only video. </span></p>



<p><span>What is less visible is that the change in border colour, a security feature in </span><a href="http://durden.arcan-fe.com/">Durden</a><span>, is used to signify that the window comes from a networked source, a property that can also be used to filter sensitive actions. The neo-vim window in the video even goes so far as to have its text surfaces rendered server side, as its </span><a href="https://github.com/letoram/nvim-arcan">UI driver</a><span> is written using our terminal-protocol liberated </span><a href="https://github.com/letoram/arcan/wiki/TUI">TUI API.</a> This is also why the font changes; it is the device you&nbsp;<em>present</em> on that defines visuals and input response, not the device you run the program on.</p>



<p>Also note how the clients “jumps” back when the window is closed on the remote side; this is one of the many payoffs from having a systemic mindset when it comes to&nbsp; ‘<a href="https://arcan-fe.com/2017/12/24/crash-resilient-wayland-compositing/">crash resilience</a>‘ – the IPC system itself is designed in such a way that <em>necessary</em> state can b<span>e reconstructed and&nbsp;</span><em>dynamic</em><span>&nbsp;state is tracked and renegotiated when needed. The effect is that a client is forcefully detached from the current display server with the instruction of switching to another.</span> The keystore (while a work in progress) allows you to define the conditions for when and how it jumps to which machines and picks keys accordingly.</p>



<p>That dynamic state is tracked and can be renegotiated as a ‘reset’ matters on the client level as well, the basic set of guaranteed features when a client opens a local connection roughly generalises between all imaginable window management styles. Those that are dynamically (re-) negotiated cannot be relied upon. So when a client is migrated to a user that has say, accessibility needs, or is in a <a href="https://arcan-fe.com/2018/03/29/safespaces-an-open-source-vr-desktop/">VR environment</a>, the appropriate extras gets added when the client connects there, and then removed when it moves somewhere else. This is an essential primitive for network transparency as a collaboration feature.</p>



<h2 id="primitives">Basic Primitives: Arcan-net, SHMIF and A12</h2>



<p>There are three building blocks in play here, a tool called <em>arcan-net&nbsp;</em>which combines the two others:&nbsp;<em>A12</em> and <a href="https://github.com/letoram/arcan/wiki/SHMIF">SHMIF</a>.</p>



<p>A12 is a <span>‘work in progress’ protocol – it’s not </span><em>the</em><span>&nbsp;</span><a href="https://www.x.org/wiki/Development/X12/">X12</a><span> that some people called for, but it’s “</span><em>a”</em><span> twelve. It strives to be remote optimal – compression tactics based on connectivity, content type and context of use, deferred (presentation side) rendering with data-native representation when possible (pixel buffers as a last resort, not the default); support caching of common states such as fonts; handle cancellation of normally ‘atomic’ operations such as clipboard cut and paste and so on.</span></p>



<p>SHMIF is the IPC system and API used to work with most other parts of Arcan. It is designed to be locally optimal: shared memory and system ABI in lock free ring-buffers preferred over socket/pipe pack/unpack transfers; minimal sustained set of system calls needed (for least-privilege sandboxing); resource allocations on a strict regimen (DoS prevention and exploit mitigation); fixed based set of necessary capabilities and user-controlled opt-in for higher level ones.</p>



<p><span>SHMIF has a large number of features that were specifically picked for correcting the wrongs done to X- like network transparency by the gradual introduction of side-bands and good old fashioned negligence. Part of this is that <em>all necessary and sufficient data exchange</em> used to compose a desktop goes over <em>the same</em> IPC system — one that is free of unnecessary Linuxisms to boot. While it would hurt a bit and take some effort, there are few stops for packing our bags and going someplace else, heck it used to run on Windows and still works on OSX. Rumour has it there are iOS and Android versions hidden away somewhere.</span></p>



<p><span>Contrast this with other setups where you need a large weave of IPC systems to get the same job done; Wayland for video and some input and some metadata; PulseAudio for audio; PipeWire for some video and some audio; D-Bus for some metadata and controls; D-Conf for some other metadata; Spice/RFB(VNC)/RDP for composited desktop sharing; Waypipe for partial Wayland sharing, X11 for partial X / XWayland sharing: SSH+VT***+Terminal emulator for CLI/TUI and less unsafe Waypipe / X11 transport; Synergy for mouse and keyboard and clipboard and so on. Each of these with their own take (or lack thereof) on authentication and synchronization, implementing many of the most difficult tasks again and again in incompatible ways yet still end up with features missing and exponentially more lines of code when compared to the solution here.</span></p>



<p>Back to Arcan-net. It exposes an a12 server and an a12 client, as well as acting as a shmif server, a shmif client and taking care of managing authentication keys. In that sense it behaves like any old network proxy. While not going too far into the practical details, showing off some of the setup might help.</p>



<p>On the active display server side:</p>



<pre>void@123.213.132.1# arcan-net -l 31337</pre>



<p>This will listen for incoming connections on the marked port, and map them to the currently active local connection point. <span>To dive further into the connection point concept, either read the comparison between </span><a href="https://arcan-fe.com/2018/10/17/arcan-versus-xorg-approaching-feature-parity/">Arcan vs Xorg</a><span> or simply think ‘Desktop UI address’; The WM exports named connection points and assigns different policies based on that.</span></p>



<p><span>On the client side we can have the complex-persistent option that forwards new clients as they come:</span></p>



<pre><em>arcan-net</em> -s <em>netdemo</em> <em>123.213.132.1 31337</em><br>ARCAN_CONNPATH=netdemo one_arcan_client &amp;<br>ARCAN_CONNPATH=netdemo another_arcan_client &amp;</pre>



<p>Or the one-time simpler version which forks/exec arcan-net and inherits the connection primitive needed to setup a SHMIF connection:</p>



<pre>ARCAN_CONNPATH=a12://keyid@host:port one_arcan_client</pre>



<p>Or, and this is important for understanding the demo, an api function through the WM:</p>



<pre>target_devicehint(client_vid,"a12://keyid@", true)</pre>



<p>This triggers the SHMIF implementation tied to the window of a client to disconnect from the current display server connection, connect to a remote one through arcan-net, then tell the application part of the client to rebuild essential state as the previous connection has, in a sense, ‘crashed’. The same mechanism is then used to define a fallback (‘should the connection be lost, go here instead’). This is the <em>self-healing</em> aspect of proper <em>resilience</em>.</p>



<p>There are WM APIs for all the possible network sharing scenarios so it can be handled as user interfaces without any command line work.</p>



<p>I mentioned ‘authentication’ before, where is that happening? So this is another part of the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://arcan-fe.com/2020/10/28/a12-advancing-network-transparency-on-the-desktop/">https://arcan-fe.com/2020/10/28/a12-advancing-network-transparency-on-the-desktop/</a></em></p>]]>
            </description>
            <link>https://arcan-fe.com/2020/10/28/a12-advancing-network-transparency-on-the-desktop/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24934296</guid>
            <pubDate>Thu, 29 Oct 2020 19:07:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building a Great Business: Advice You Won't Take (and Will Regret Not Taking)]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24934026">thread link</a>) | @rjyoungling
<br/>
October 29, 2020 | https://www.younglingfeynman.com/essays/advice | <a href="https://web.archive.org/web/*/https://www.younglingfeynman.com/essays/advice">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-ba050bc6dcf9223f8459"><div><p>A while ago, I was talking to a founder of a startup. We were talking about his experience before and after finding product/market fit.</p><p>I asked him if he could boil down his experience into 1 piece of important advice I could share with The Younglings. He did.</p><p>But it was the way he said it, rather than what he said, that stuck with me.</p><p><em>His answer, because I don’t wanna leave you hanging, was that ‘you will overbuild and it will be a mistake’.</em></p><p>The way he said it was: ‘It is impossible to follow this advice but…’</p><p>That made me think of the advice I could give that I know is impossible to take yet true. The result of that is the essay you’re reading right now.</p><p><em>The irony that this essay is a list is not lost on me. My hatred for superficial business insider listicles has become a running joke. That said, I think this will be a worthwhile exception.</em></p><p>Let’s get right into it.</p><p>If you’re extremely ambitious, the prospect of the ‘disruptive innovation’ of an industry can seem daunting. Where do you even start?</p><p>I am not a big fan of TAM at all. VC’s (and other investors) are biased. They’ll give you self-serving advice. They want you to think big.</p><p><em>More on TAM in: </em><a href="https://www.younglingfeynman.com/essays/tam" target="_blank"><em>Should You Worry About TAM And SAM?</em></a></p><p>Why? Because they don’t care about you as an individual. You’re fungible. As long as of the 100 investments they make, a few become unicorns, that’s perfectly fine!&nbsp;</p><p>While I do think there’s a good case to be made for thinking big and solving the hardest problems on the planet, it’s actually incredibly rare for a founder to start there.</p><p>Take Elon Musk, for example. Tesla, SpaceX, The Boring Company, Neuralink, etc. All incredibly ambitious.</p><p>But his first company? A videogame when he was in his early teens. His first startup? A precursor to Google maps.&nbsp;</p><p>That’s all much more doable.</p><p>He even said in an interview that he probably wouldn’t have been able to start with SpaceX and that he advises against starting with a company that is that capital intensive.&nbsp;</p><p><em>I’ve been unable to find the source. I’ll continue to search for the video on YouTube and I’ll add it to the references if I find it.</em></p><p>In fact, I don’t know of a single founder that started their company with this huge vision. What usually happens is lying. Founders (or PR) will whitewash their history ex-post facto.</p><p>Something that <a href="https://mashable.com/article/mark-zuckerberg-lying-about-facebook/?europe=true" target="_blank">Zuckerburg was recently called out</a> for.</p><p>The Collinson brothers have often <a href="https://www.youtube.com/watch?v=9DUQ7_7Pj_c" target="_blank">pointed out that had they known Stripe would’ve been this hard, they might not have started it at all</a>.</p><p>So to come back to my question in the first paragraph, where do you even start?</p><p>With 1 person. You!&nbsp;</p><p>Think about a problem you have. Or think about something that you really want to see in the world.</p><p>Then try to see if there are other people like you.&nbsp;</p><p>Forget about scale. Forget about world domination. Forget about Fortune lists.&nbsp;</p><p>Focus on your tiny audience and just build something that improves their lives. According to them, not according to you.&nbsp;</p><p>Get to a point where they love it. Get to a point where they would be deeply sad if your solution went away.</p><p><em>More on architecting user love in: </em><a href="https://www.younglingfeynman.com/essays/deeplove" target="_blank"><em>Do You Have Customers Who Deeply Love You?</em></a></p><p><em>In case you’re suffering from a restless, intellectual brain that just can’t stop asking: ‘But how do I scale?’, the answer is: ‘Keep finding more people’. How much you make people’s lives better (on the X-axis) multiplied by a lot of people (on the Y-axis), is what’ll create a large business.</em></p></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1592418092968_20301"><div><p><em>Avoid the red bar, and start with the bright blue bar. Even if your company is already creating revenue, you might not have that bright blue bar… a small set of raving fans that absolutely love your product. Then grow that bright blue bar along the Y-axis. In theory, you could also grow the red bar along the X-axis to get the same surface area. In practice that never works out.</em></p><p>There are a small number of ‘non-obvious, capital intensive startups’ (to borrow <a href="https://en.wikipedia.org/wiki/Chamath_Palihapitiya" target="_blank">Chamath</a>’s lexicon).&nbsp;</p><p>In those cases, you probably can’t be profitable from the beginning.&nbsp;</p><p><em>Unless you’re very creative and have a long time horizon. For example, one might have been able to start Tesla by starting Boosted Boards, then launching into new categories until you eventually get to cars. Dyson did a similar thing from vacuums to hand dryers to fans to blow dryers to cars. </em><a href="https://www.bbc.com/news/business-50004184" target="_blank"><em>Unfortunately, they’ve announced they’re pulling out</em></a><em>.</em></p><p>But I believe most small businesses and startups should be profitable from the beginning and scale or hover just below profitability.</p><p>It’s a mistake to solely focus on growth in hopes of one day pulling the magical profitability lever and suddenly being profitable.</p><p>Hope makes for a poor business strategy and the reason you hear about Google is that it’s so rare. More often than not it just doesn’t work out. [1]</p><p>If you do decide to put growth over profitability, you should know your numbers and be clearly able to articulate why it’s a good idea.</p><p>I’d rather see a Lambda School than a Homejoy.</p><p><em>Austen raised after being profitable (= knowing his LTV, CAC, churn etc.) in order to fund faster growth and compress the timeline. Adora raised while not knowing exactly what LTV and CAC would end up being. Although there are always many factors, the biggest one was that acquiring customers was too expensive and they had poor retention. This obviously doesn’t mean Austen is better than Adora. Adora is a legend and she added many leaves to the tree of entrepreneurial science. It just means that Adora took a more risky approach and that has a smaller chance of working out.</em></p><p>You know who obsessively focuses on the competition? People that have run out of ideas.</p><p>If you:</p><ol data-rte-list="default"><li><p>Have a clear grasp of what fucking sucks in this world.&nbsp;</p></li><li><p>A good solution for fixing it.&nbsp;</p></li><li><p>And, a group of people that love your product,</p></li></ol><p>then that’ll take up all of your time.</p><p>Obviously, the competition sucks otherwise the problem or the need wouldn't exist. They would’ve solved it. [2]</p><p>Correctly identifying a real problem, or a need that a certain audience has as well, and then building a solution that they love, is already hard enough.</p><p>Trying to simultaneously focus on what the competition is doing (or even worse, might do) is near impossible. [3]</p><p>If you're doing your job correctly, you’ll hear what the competition is up to anyway. But if that heavily affects your decision-making process, you’re doing it wrong.</p><p><em>1 important exception to this is if your company relies mainly on psychological innovation. Oatly struggled even though they had a great product. It wasn’t until </em><a href="https://thechallengerproject.com/blog/2016/oatly" target="_blank"><em>they brought in a guy with the necessary expertise in psychological innovation</em></a><em> (John Schoolcraft) that they were able to scale. He realized that they were mimicking the competition and by looking at what they were doing, he could make sure Oatly steered clear of that and develop its own voice.</em></p><p><em>More on psychological innovation in this essay series: </em><a href="https://www.younglingfeynman.com/essays/illogical" target="_blank"><em>Why Your Business Needs More Weird Ideas</em></a><em>.</em></p><p>Young companies don’t get killed by big companies. They fail to follow point 1 on this list. They make something mediocre, or they solve a problem that doesn’t exist, or they build something to address a need that no one has.</p><p>When Jack Dorsey built Twitter, he wasn’t solving a problem. He was addressing a need… his own. It’s possible that in an alternate but nearly identical universe, he is an outlier and people just aren’t that into Twitter.&nbsp;</p><p>But as it so happens, his colleagues at <a href="https://www.businessinsider.com/how-twitter-was-founded-2011-4?international=true&amp;r=US&amp;IR=T" target="_blank">Odeo</a> loved it and it started to spread.</p><p>Keep iterating your business model canvas, or pivot, until you've succeeded in making a product that makes people bang down your door to get it. [4]</p><p>To quote Andy Rachleff (created modern product/market fit theory inspired by Don Valentine):</p><blockquote><p>‘[…]if you’re really good at execution but the dogs don’t want to eat the dog food, you have no chance of winning.’</p></blockquote><p>I thought it would be nice to end this list on the advice that kicked it off.</p><p>Get your idea into the hands of users as quickly as humanly possible. Your brain is lying to you when it tells you that the first impression should be polished and amazing and that Reid Hoffman is wrong when he says:</p><blockquote><p>‘If you’re not embarrassed by the first version of your product, you’ve launched too late!’</p></blockquote><p>Remember that your brain is wrong in this case. It’s your friend, but like an overprotective mom, it doesn’t want you to get hurt. So it’ll try to trick you (successfully I might add) that you should do anything except the things that actually matter.</p><p>This is the core insight of Noah Kagan’s eminent <a href="https://www.youtube.com/watch?v=BwbtSPQ8jAY" target="_blank">Validation Theory</a>.</p><p>The reason why you shouldn’t overbuild is because you’re making a lot of assumptions, and nearly all of those assumptions are wrong.</p><p>The quicker you’re able to identify which ones are wrong the better because it’ll save resources.</p><p>Imagine spending 6 years and $500K engineering a $3500 robot that walks dogs and picks up dog poop. When you try to sell it you learn ‘ain’t nobody wanna spend $3.5K on your dog walking pooper scooper’. You could’ve avoided this by presenting a few people with your idea and ask them to prepay. When you inevitably hear: ‘yeah… uhm, that’s gonna be a hard pass for me chief!’, then you can iterate to something that would be <strong>excited</strong> to pay for.</p><p>Don’t feel bad about ignoring the advice on this list. While everything is true, your brain will find some excuse and you will buy into it. We all do, myself included.</p><p>This seems to be like parents warning you about a bad girl/boy when you’re a teenager. You just need to experience it before it sinks in.</p><p>Then why did I write this essay?</p><p>Because part of me hopes there are a few competitive people that’ll be like: ‘Don’t tell me what I can’t do!’</p><p>And for the rest of the people reading this, when you make these mistakes, I hope you’re able to recognize that you’re making them sooner and are able to course-correct faster.</p><p><em>[1] Again, be mindful of who gives you this advice. VC’s most likely. For them, it’s a win/win situation. If they give you an A round and push you to grow hard, your paper valuation will increase, they can use that to raise more capital. That means they’ll make more money because of their management fee.&nbsp;</em></p><p><em>T…</em></p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.younglingfeynman.com/essays/advice">https://www.younglingfeynman.com/essays/advice</a></em></p>]]>
            </description>
            <link>https://www.younglingfeynman.com/essays/advice</link>
            <guid isPermaLink="false">hacker-news-small-sites-24934026</guid>
            <pubDate>Thu, 29 Oct 2020 18:47:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Mall real estate company collected 5M images of shoppers]]>
            </title>
            <description>
<![CDATA[
Score 225 | Comments 148 (<a href="https://news.ycombinator.com/item?id=24933583">thread link</a>) | @voisin
<br/>
October 29, 2020 | https://www.cbc.ca/news/politics/cadillac-fairview-5-million-images-1.5781735?cmp=rss | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/politics/cadillac-fairview-5-million-images-1.5781735?cmp=rss">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>The real estate company behind some of Canada's most popular shopping centres embedded cameras inside its digital information kiosks at 12 shopping malls in major Canadian cities&nbsp;to collect millions of images — and used facial recognition technology without customers' knowledge or consent —&nbsp;according to a new investigation by the federal, Alberta and B.C. privacy commissioners.</p><div><figure><div><p><img loading="lazy" alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5499879.1584406507!/fileImage/httpImage/image.JPG_gen/derivatives/16x9_780/covid-19-pandemic-stores-closed.JPG"></p></div><figcaption>Cadillac Fairview, the real estate company behind some of Canada's most popular shopping centres, embedded cameras inside its digital information kiosks at 12 shopping malls across Canada, according to a new investigation.<!-- --> <!-- -->(Evan Mitsui/CBC)</figcaption></figure><p><span><p>The real estate company behind some of Canada's most popular shopping centres embedded cameras inside its digital information kiosks at 12 shopping malls in major Canadian cities&nbsp;to collect millions of images — and used facial recognition technology without customers' knowledge or consent —&nbsp;according to a new investigation by the federal, Alberta and B.C. privacy commissioners.</p>  <p>"Shoppers had no reason to expect their image was being collected by an inconspicuous camera, or that it would be used, with facial recognition technology, for analysis," said federal Privacy Commissioner Daniel Therrien&nbsp;in a statement.</p>  <p>"The lack of meaningful consent was particularly concerning given the sensitivity of biometric data, which is a unique and permanent characteristic of our body and a key to our identity."</p>  <p>According to the report, the technology&nbsp;Cadillac Fairview used&nbsp;— known as "anonymous video analytics" or AVA— took temporary digital images of the faces of individuals within the field of view of the camera in the directory.</p>  <p><strong><em>WATCH: Shoppers' privacy violated at major Canadian malls: Privacy commissioners:</em></strong></p>  <p><span><span><div><div role="button" tabindex="0" title="Shoppers’ privacy violated at major Canadian malls: Privacy commissioners"><div><p><img src="https://thumbnails.cbc.ca/maven_legacy/thumbnails/949/527/mall-privacy-daigle-291020.jpg" alt=""></p></div></div></div><span>Cadillac Fairview, the real estate company behind some of Canada’s biggest malls, violated the privacy of shoppers by collecting five million images without consent from cameras inside digital information kiosks, an investigation by federal, British Columbia and Alberta privacy commissioners found.<!-- --> <!-- -->2:01</span></span></span></p>  <p>It then used facial recognition software to convert those images into biometric numerical representations of&nbsp;individual faces, about five million images&nbsp;in total.</p>  <p>That sensitive personal information could be used to identify individuals based on their unique facial features, said&nbsp;the commissioners.</p>    <p>The report said the company also kept about 16 hours of video recordings, including some audio, which it had captured during a testing phase at two malls.</p>  <p>Cadillac Fairview said it&nbsp;used AVA technology&nbsp;to assess foot traffic and track shoppers' ages and genders&nbsp;— but not to identify individuals.&nbsp;</p>  <p>The company also argued shoppers were made aware of the activity through decals it placed on shopping mall entry doors that warned cameras were being used for "safety and security" and included the web address for Cadillac Fairview's&nbsp;privacy policy.</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.4762407.1533648115!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/chinook-centre-directory.jpg 300w,https://i.cbc.ca/1.4762407.1533648115!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/chinook-centre-directory.jpg 460w,https://i.cbc.ca/1.4762407.1533648115!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/chinook-centre-directory.jpg 620w,https://i.cbc.ca/1.4762407.1533648115!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/chinook-centre-directory.jpg 780w,https://i.cbc.ca/1.4762407.1533648115!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/chinook-centre-directory.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.4762407.1533648115!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/chinook-centre-directory.jpg"></p></div><figcaption>This directory in Chinook Centre mall in south Calgary uses facial recognition technology.<!-- --> <!-- -->(Sarah Rieger/CBC)</figcaption></figure></span></p>  <p>But the commissioners said that&nbsp;wasn't good enough and did not meet the standard for meaningful consent.&nbsp;</p>  <p>"An individual would not, while using a mall directory, reasonably expect their image to be captured and used to create a biometric representation of their face, which is sensitive personal information, or for that biometric information to be used to guess their approximate age and gender," they wrote.</p>  <p>The privacy watchdogs also took issue with the way the&nbsp;five&nbsp;million images were stored.</p>  <p>Cadillac Fairview&nbsp;said the&nbsp;images taken by camera were briefly analyzed then deleted&nbsp;—&nbsp;but investigators found that the sensitive biometric information generated from the images was being stored in a centralized database by&nbsp;a third-party company,</p>  <p>"Our investigation revealed that&nbsp;[Cadillac Fairview Corporation Limited's]&nbsp;AVA&nbsp;service provider had collected and stored approximately five million numerical representations of faces on&nbsp;CFCL's behalf, on a decommissioned server, for no apparent purpose and with no justification," notes the investigation.</p>  <p>"Cadillac Fairview stated that it was unaware that the database of biometric information existed, which compounded the risk of potential use by unauthorized parties or, in the case of a data breach, by malicious actors."</p>  <h2>Company&nbsp;says technology couldn't identify people</h2>  <p>The company said the technology was used&nbsp;to detect the presence of a human face and&nbsp;assign it&nbsp;"within milliseconds"&nbsp;to an approximate age and gender category and maintains it&nbsp;did not store any images during the pilot program and was not capable of recognizing anyone.&nbsp;</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5782324.1604000638!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/eaton-centre-decal.jpg 300w,https://i.cbc.ca/1.5782324.1604000638!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/eaton-centre-decal.jpg 460w,https://i.cbc.ca/1.5782324.1604000638!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/eaton-centre-decal.jpg 620w,https://i.cbc.ca/1.5782324.1604000638!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/eaton-centre-decal.jpg 780w,https://i.cbc.ca/1.5782324.1604000638!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/eaton-centre-decal.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5782324.1604000638!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/eaton-centre-decal.jpg"></p></div><figcaption>The decal found on the entrance doors of the CF Toronto Eaton Centre<!-- --> <!-- -->(Office of the Privacy Commissioner report)</figcaption></figure></span></p>  <p>"The five million representations referenced in the [Office of the Privacy Commissioner]&nbsp;report are not faces. These are sequences of numbers the software uses to anonymously categorize the age range and gender of shoppers in the camera's view," Cadillac Fairview spokesperson Jess Savage&nbsp;said in a statement to CBC News.</p>  <p>"The&nbsp;OPC report concludes there is no evidence that CF was using any technology for the purpose of identifying individuals."</p>  <p>CF&nbsp;suspended its&nbsp;use of cameras&nbsp;back in 2018&nbsp;when provincial and federal privacy commissioners launched their probe&nbsp;<a href="https://www.cbc.ca/news/canada/calgary/calgary-malls-1.4760964">following a CBC investigation</a>.</p>  <p>In a statement to CBC News on Thursday, the company said it has deleted the data.</p>  <p>"We subsequently deactivated directory cameras and the numerical representations and associated data have since been deleted," said&nbsp;Savage.</p>  <p>"We take the concerns of our visitors seriously and wanted to ensure they were acknowledged and addressed."</p>  <p>However, the three commissioners said they have concerns about the company's plans going forward.</p>    <p>"The commissioners remain concerned that Cadillac Fairview refused their request that it commit to ensuring express, meaningful consent is obtained from shoppers should it choose to redeploy the technology in the future," said&nbsp;the commissioners'&nbsp;statement.</p>  <h2>No fines under Canadian law</h2>  <p>Savage said Cadillac Fairview&nbsp;accepted and implemented all the recommendations&nbsp;"with the exception of those that speculate about hypothetical future uses of similar technology."</p>  <p>The investigation found the technology was used&nbsp;in five provinces&nbsp;at the following malls:</p>  <ul>   <li>CF Market Mall (Calgary)</li>   <li>CF Chinook Centre (Calgary)</li>   <li>CF Richmond Centre (Richmond, B.C.)</li>   <li>CF Pacific Centre (Vancouver)</li>   <li>CF Polo Park (Winnipeg)</li>   <li>CF Toronto Eaton Centre (Toronto)</li>   <li>CF Sherway Gardens (Toronto)</li>   <li>CF Fairview Mall (Toronto)</li>   <li>CF Lime Ridge (Hamilton, Ont.)</li>   <li>CF Markville Mall (Markham, Ont.)</li>   <li>CF Galeries d'Anjou&nbsp;(Montreal)</li>   <li>CF Carrefour Laval (Laval, Que.)</li>  </ul>  <p>Ann Cavoukian,&nbsp;executive director at the Global Privacy and Security by Design Centre,&nbsp;said a case like this would lead to millions of dollars in fines if it had happened&nbsp;in the United States.</p>  <p>"The commissioners are doing the best they can with the limited resources they have," she said.</p>  <p>"What we have to insist upon is that private&nbsp;sector entities like Cadillac Fairview step up and protect their customers' privacy. Otherwise, why are the customers going to continue shopping there?"</p>  <p>B.C. Information and Privacy Commissioner&nbsp;Michael McEvoy&nbsp;said&nbsp;the fact he and his counterparts can't issue a fine in a&nbsp;case like this should make the case for stronger powers at both the federal and provincial levels.</p>  <p>"Fines in a case like this would have been a consideration. It is an incredible shortcoming of Canadian law," he said.</p>  <p>"We as privacy regulators don't have any authority to levy fines on companies that violate peoples'&nbsp;personal information and that should really change."</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/politics/cadillac-fairview-5-million-images-1.5781735?cmp=rss</link>
            <guid isPermaLink="false">hacker-news-small-sites-24933583</guid>
            <pubDate>Thu, 29 Oct 2020 18:22:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Goodcover's (YC S17) Series A Funding: Accelerating Affordable Renters Insurance]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24932805">thread link</a>) | @ddispaltro
<br/>
October 29, 2020 | https://www.goodcover.com/blog/goodcovers-series-a-funding-accelerating-affordable-renters-insurance-offering-in-california-and-beyond/ | <a href="https://web.archive.org/web/*/https://www.goodcover.com/blog/goodcovers-series-a-funding-accelerating-affordable-renters-insurance-offering-in-california-and-beyond/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>At Goodcover, our mission is to elevate financial peace of mind with affordable and effective insurance for all. We <a href="https://www.goodcover.com/blog/announcing-goodcover-renters-insurance/">launched</a> earlier this year with our first product, modern renters insurance. Today we’re announcing funding that will accelerate delivering on that mission.</p><p>Our $7.5M Series A financing is led by Goodwater Capital, with participation from Fuel Capital, Broadhaven Ventures, Global Founders Capital, Liquid 2, and TransRe. We’re especially thrilled to collaborate with the team at Goodwater and their dedicated background in consumer technology who’ve helped fund companies such as Monzo, ZeroDown, and Zumper.</p><h2 id="next-step-bring-affordable-renters-insurance-to-more-states">Next Step: bring affordable renters insurance to more states</h2><p>Today, Goodcover’s renters insurance is only available in California. With this funding we will begin to roll out nationally, helping to bring affordable, effective financial peace of mind to as many people as possible. &nbsp;</p><p>Getting Goodcover launched in one state was a huge process (for gory details read <a href="https://news.ycombinator.com/item?id=22368112">this HN post</a>), but fortunately that was the heavy lifting. While we plan to eventually be available in all 50 states, launching an insurance offering in each one is a separate process - especially with a novel cooperative model like ours - so we’ll be announcing them as soon we can. &nbsp;</p><p>If you want to be notified when Goodcover starts up in your state, please <a href="http://www.goodcover.com/join">click here</a> to drop in your zip and email to let us know.</p><p>PS - I’ve focused on the fact that we’ll roll out renters in more states, but the truth is our mission demands that we offer more than just renters Insurance. We are working on home and condo as well, and evaluating opportunities in auto and life insurances, so stay tuned.</p><h2 id="member-cooperative-the-fair-model">Member Cooperative: The Fair Model</h2><p>Critical to our success has been our relationship with Goodcover Members - and this starts with our cooperative model that makes insurance fairer and more approachable. As promised when we launched, after a fixed fee we take only what’s needed to cover claims, returning the rest back to members. &nbsp;This is how insurance began - neighbors protecting neighbors - but it is a foreign concept in insurance today. The model works and we’ve proudly returned 1.89% of premium through our <a href="https://www.goodcover.com/blog/2020-member-dividend/">Member Dividend this year</a>. </p><p>Members who don’t want to keep the funds returned to them have the option of contributing them to our Goodpool fund, which is used for good – such as offering <a href="https://www.goodcover.com/blog/free-insurance-for-medical-responders-covid19/">free insurance to medical and other frontline workers fighting COVID-19</a>.</p><h2 id="introducing-the-goodprice-guarantee">Introducing the Goodprice Guarantee</h2><p>It’s also no secret that a large part of Goodcover’s popularity has been the “affordable” part of our mission. On average we are 48% less expensive than legacy providers, but we’ve clocked up to 72% less! We’ve done the work to create efficient, affordable, modern insurance -- we have no middlemen, no agents, no paperwork, no superbowl ads -- we’ve cut out waste with a streamlined digital experience and passed the savings on to members. </p><p>That said, if you have never heard of Goodcover before and are checking us out for the first time, it’s hard to know whether you are getting a good deal, or whether there’s some sort of gotcha. Well, you are, and there isn’t - but it’s our job to prove it to you. &nbsp;We get it, it’s easy to get overwhelmed browsing sales ridden comparison sites that look to complicate a pretty simple thing.</p><p>So, we’re introducing the Goodprice Guarantee. If after joining, you find a better deal somewhere else, simply send us your new policy offer - we’ll refund your Goodcover premium fully. </p><h2 id="hiring-join-us-">Hiring! Join Us.</h2><p>We’ve got a lot to do, so we’re going to need help! Join our all-remote team, we’re looking for: <a href="https://angel.co/company/goodcover-co/jobs/1038060-design-lead">Design Lead</a>, <a href="https://angel.co/company/goodcover-co/jobs/299516-senior-backend-engineer">Senior Backend Engineer</a>, <a href="https://angel.co/company/goodcover-co/jobs/1038024-senior-frontend-engineer">Senior Frontend Engineer</a>, licensed agents to support the Member Experience, Growth Marketers, and everything in between. If you don’t see a specific role open send us a note to <a href="mailto:careers@goodcover.com">careers@goodcover.com</a>.</p><h2 id="get-covered">Get Covered</h2><p>Finally, our continual PSA: if you’ve never had renters insurance there’s no better time to start than today (especially given the tragic wildfires that ravaged the west coast). It starts at $5 a month, you can cancel anytime, it takes only a few minutes (record time is 57 seconds!)... There is no reason not to have it, yet we see so many people who are not protected. Please get it, if not with us, with someone, to protect yourself and your family from unnecessary pain.</p><p>If you’re insured already, send us your policy to <a href="mailto:compare@goodcover.com">compare@goodcover.com</a> and we’ll send you back a detailed coverage comparison showing you how much you can save. If you like what you see, you can join instantly, and we’ll do the work of cancelling and refunding your old policy for you.</p><h2 id="thank-you-members-thank-you-team">Thank you members, thank you team</h2><p>This is a big day for Goodcover - I am so thankful for all the hard work and persistence of the team and everything you have done so far - and I am excited for what we’ll do next.</p><p>And to Goodcover Members - thank you for trusting us with your insurance. We know it’s an act of faith to contribute your real hard earned money in exchange for the promise that we’ll be there when you need us. We’ll make you proud!<br></p><p>Chris Lotz</p><p>CEO and co-founder</p></div></div></div>]]>
            </description>
            <link>https://www.goodcover.com/blog/goodcovers-series-a-funding-accelerating-affordable-renters-insurance-offering-in-california-and-beyond/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24932805</guid>
            <pubDate>Thu, 29 Oct 2020 17:20:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On Fexprs and Defmacro]]>
            </title>
            <description>
<![CDATA[
Score 37 | Comments 8 (<a href="https://news.ycombinator.com/item?id=24932701">thread link</a>) | @sea6ear
<br/>
October 29, 2020 | https://www.brinckerhoff.org/scraps/joe-marshall-on-FEXPRS-and-DEFMACRO.txt | <a href="https://web.archive.org/web/*/https://www.brinckerhoff.org/scraps/joe-marshall-on-FEXPRS-and-DEFMACRO.txt">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.brinckerhoff.org/scraps/joe-marshall-on-FEXPRS-and-DEFMACRO.txt</link>
            <guid isPermaLink="false">hacker-news-small-sites-24932701</guid>
            <pubDate>Thu, 29 Oct 2020 17:09:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Open-source, fully customizable voice and chat widgets for the web]]>
            </title>
            <description>
<![CDATA[
Score 68 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24932588">thread link</a>) | @JanKoenig
<br/>
October 29, 2020 | https://www.jovo.tech/news/2020-10-29-jovo-for-web-v3-2 | <a href="https://web.archive.org/web/*/https://www.jovo.tech/news/2020-10-29-jovo-for-web-v3-2">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
<p><a href="#" data-featherlight="/img/news/2020-10-29-jovo-for-web-v3-2/jovo-for-web.jpg"><img src="https://www.jovo.tech/img/news/2020-10-29-jovo-for-web-v3-2/jovo-for-web.jpg" alt="Jovo for Web Open Source Voice and Chat" title="Introducing Jovo for Web: Customizable Voice and Chat for the Browser"></a></p><p>With the release <code>v3.2</code> of the <a href="https://github.com/jovotech/jovo-framework">Jovo Framework</a>, we're excited to present a completely revamped web integration.</p><p><em>Jovo for Web</em> allows you to build fully customizable voice and chat apps that work in the browser. And it even comes with 4 open source templates (gifs below!) that help you get started.</p><ul>
<li><a href="#jovo-for-web-features">Jovo for Web Features</a></li>
<li><a href="#select-from-4-starter-templates">Select from 4 Starter Templates</a>
<ul>
<li><a href="#standalone-voice-experience">Standalone Voice Experience</a></li>
<li><a href="#voice-overlay">Voice Overlay</a></li>
<li><a href="#chat-widget">Chat Widget</a></li>
<li><a href="#embedded-chat">Embedded Chat</a></li>
</ul></li>
<li><a href="#more-new-features">More New Features</a></li>
<li><a href="#how-to-update">How to Update</a>
<ul>
<li><a href="#breaking-changes">Breaking Changes</a></li>
</ul></li>
<li><a href="#a-big-thank-you">A Big Thank You</a></li>
</ul><p><em>Like what we're doing? <a href="https://opencollective.com/jovo-framework">Support us on Open Collective!</a></em> </p><h2 id="jovo-for-web-features"><a href="#jovo-for-web-features">Jovo for Web Features</a></h2><p>Let's build voice and chat apps for the browser!</p><p>In our <a href="https://www.context-first.com/introducing-jovo-v3-the-voice-layer/">v3 announcement</a>, we already mentioned that Jovo works with web apps and websites thanks to the <a href="https://www.context-first.com/introduction-voice-multimodal-interactions/">RIDR Lifecycle</a> and <a href="https://www.jovo.tech/news/www.jovo.tech/marketplace">Jovo Marketplace</a>.</p><p><a href="#" data-featherlight="/img/news/2020-10-29-jovo-for-web-v3-2/jovo-web-ridr-lifecycle.jpg"><img src="https://www.jovo.tech/img/news/2020-10-29-jovo-for-web-v3-2/jovo-web-ridr-lifecycle.jpg" alt="Jovo for Web RIDR Lifecycle" title="Integrate ASR and NLU into Web Apps with Jovo and RIDR"></a></p><p>Today, we're thrilled to announce a completely improved verson of our <strong>Jovo for Web</strong> platform.</p><p>Features include:</p><ul>
<li>Support for speech, text, and touch input</li>
<li>Multimodal: Complex visual and audio output possible</li>
<li>Open source and fully customizable</li>
<li><a href="#select-from-4-starter-templates">4 starter templates</a> built with modern technologies like Vue.js and Tailwind CSS</li>
</ul><p>We can't wait to see and hear what you build with this!</p><h2 id="select-from-4-starter-templates"><a href="#select-from-4-starter-templates">Select from 4 Starter Templates</a></h2><p>To help you get started quickly, we built 4 templates with Vue.js and Tailwind CSS that implement use cases for both voice and chat.</p><h3 id="standalone-voice-experience"><a href="#standalone-voice-experience">Standalone Voice Experience</a></h3><blockquote>
<p>Find this starter on GitHub: <a href="https://github.com/jovotech/jovo-starter-web-standalone">github.com/jovotech/jovo-starter-web-standalone</a></p>
</blockquote><p><a href="#" data-featherlight="/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-standalone.gif"><img src="https://www.jovo.tech/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-standalone.gif" alt="Jovo Starter: Standalone Voice Experience" title="Introducing Jovo for Web: Customizable Voice and Chat in the Browser"></a></p><p>This starter brings your voice experiences into the browser as a standalone web app. This can be seen as an experience equivalent to a smart display. Many Alexa Skills and Google Actions like voice games can be brought to the web using this template.</p><p>The starter includes:</p><ul>
<li>a push-to-talk button</li>
<li>a display of the transcribed speech above the button</li>
<li>app output at the top of the screen</li>
<li>conversational logic that switches to dark/light mode using custom web actons</li>
</ul><p><a href="https://www.jovo.tech/demos/starter-web-standalone">Check out the demo here!</a> Hold the button and say "<em>switch to dark mode.</em>"</p><h3 id="voice-overlay"><a href="#voice-overlay">Voice Overlay</a></h3><blockquote>
<p>Find this starter on GitHub: <a href="https://github.com/jovotech/jovo-starter-web-overlay">github.com/jovotech/jovo-starter-web-overlay</a></p>
</blockquote><p><a href="#" data-featherlight="/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-overlay.gif"><img src="https://www.jovo.tech/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-overlay.gif" alt="Jovo Starter: Voice Overlay" title="Introducing Jovo for Web: Customizable Voice and Chat in the Browser"></a></p><p>This starter adds a speech input button as an overlay to an existing website or web app. Voice interactions like search, customizations, and deep access of features could be added using the overlay.</p><p>The starter includes:</p><ul>
<li>a push-to-talk button</li>
<li>a display of the transcribed speech left to the button</li>
<li>conversational logic that switches to dark/light mode using custom web actons</li>
</ul><p><a href="https://www.jovo.tech/demos/starter-web-overlay">Check out the demo here!</a> Hold the button and say "<em>switch to dark mode.</em>"</p><h3 id="chat-widget"><a href="#chat-widget">Chat Widget</a></h3><blockquote>
<p>Find this starter on GitHub: <a href="https://github.com/jovotech/jovo-starter-web-chatwidget">github.com/jovotech/jovo-starter-web-chatwidget</a></p>
</blockquote><p><a href="#" data-featherlight="/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-chatwidget.gif"><img src="https://www.jovo.tech/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-chatwidget.gif" alt="Jovo Starter: Open Source Chat Widget" title="Introducing Jovo for Web: Customizable Voice and Chat in the Browser"></a></p><p>This starter adds a classic chat widget to your website. Think chatbots and conversational experiences for customer support and more.</p><p>The starter includes:</p><ul>
<li>a bottom-right toggle button</li>
<li>text input and quick replies</li>
<li>conversational logic that asks the user to open the Jovo Docs (redirect not working on iOS due to platform limitations)</li>
</ul><p><a href="https://www.jovo.tech/demos/starter-web-chatwidget">Check out the demo here!</a></p><h3 id="embedded-chat"><a href="#embedded-chat">Embedded Chat</a></h3><blockquote>
<p>Find this starter on GitHub: <a href="https://github.com/jovotech/jovo-starter-web-embeddedchat">github.com/jovotech/jovo-starter-web-embbeddedchat</a></p>
</blockquote><p><a href="#" data-featherlight="/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-embeddedchat.gif"><img src="https://www.jovo.tech/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-embeddedchat.gif" alt="Jovo Starter: Open Source Embedded Chat" title="Introducing Jovo for Web: Customizable Voice and Chat in the Browser"></a></p><p>This starter adds a customizable chat interface to your website that can be used for things like conversational landing pages, FAQs, mobile chat support, and much more.</p><p>The starter includes:</p><ul>
<li>fullsize chat component that can be embedded into an existing website</li>
<li>text input and quick replies</li>
<li>conversational logic that asks the user to open the Jovo Docs (redirect not working on iOS due to platform limitations)</li>
</ul><p><a href="https://www.jovo.tech/demos/starter-web-embeddedchat">Check out the demo here!</a></p><h2 id="more-new-features"><a href="#more-new-features">More New Features</a></h2><p>Alongside the big launch of Jovo for Web, we also shipped some other improvements and bug fixes with the help of our community. <a href="https://github.com/jovotech/jovo-framework/blob/master/CHANGELOG.md">You can find the full changelog here</a>.</p><ul>
<li>We released Google Conversational Actions. <a href="https://www.jovo.tech/news/2020-10-08-google-conversational-actions-builder">Find the announcement here</a>.</li>
<li>New analytics integration: <a href="https://www.jovo.tech/marketplace/jovo-analytics-onedash">OneDash</a>. <em>Thanks to <a href="https://github.com/StepanU">StepanU</a>!</em></li>
<li><a href="https://github.com/jovotech/jovo-framework/pull/838">Dialogflow Genesys integration</a>. <em>Thanks to <a href="https://github.com/dominik-meissner">Dominik Meissner</a>!</em></li>
</ul><h2 id="how-to-update"><a href="#how-to-update">How to Update</a></h2><blockquote>
<p><a href="https://www.jovo.tech/docs/installation/upgrading">Learn more in the Jovo Upgrading Guide</a>.</p>
</blockquote><p>To update to the latest version of Jovo, use the following commands:</p><h3 id="breaking-changes"><a href="#breaking-changes">Breaking Changes</a></h3><p>The "Jovo Web Client" and "Jovo Web Platform" were completely refactored for this release.</p><h2 id="a-big-thank-you"><a href="#a-big-thank-you">A Big Thank You</a></h2><p>Thanks a lot to all the contributors of this release. Everyone of the Jovo core team worked together to make this happen! Special thanks to Max who started working on the web integration more than a year ago as part of his bachelor's thesis.</p><p>Community and core contributors:</p><ul>
<li><a href="https://github.com/StepanU">StepanU</a></li>
<li><a href="https://github.com/dominik-meissner">Dominik Meissner</a></li>
<li><a href="https://github.com/rubenaeg">Ruben Aegerter</a></li>
<li><a href="https://github.com/KaanKC">Kaan Kilic</a></li>
<li><a href="https://github.com/m-ripper">Max Ripper</a></li>
<li><a href="https://github.com/aswetlow">Alex Swetlow</a></li>
</ul><p>And to everyone else who helped with ideas and feature requests in the <a href="https://www.jovo.tech/slack">Jovo Slack</a> and <a href="https://community.jovo.tech/">Jovo Community Forum</a>!</p>
</article></div>]]>
            </description>
            <link>https://www.jovo.tech/news/2020-10-29-jovo-for-web-v3-2</link>
            <guid isPermaLink="false">hacker-news-small-sites-24932588</guid>
            <pubDate>Thu, 29 Oct 2020 16:59:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[5M Canadian shoppers' images collected at mall kiosks]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24932467">thread link</a>) | @anonymousab
<br/>
October 29, 2020 | https://www.ctvnews.ca/canada/5-million-canadian-shoppers-images-collected-at-mall-kiosks-privacy-commissioner-1.5166162 | <a href="https://web.archive.org/web/*/https://www.ctvnews.ca/canada/5-million-canadian-shoppers-images-collected-at-mall-kiosks-privacy-commissioner-1.5166162">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p>OTTAWA -- 
	Without customers’ knowledge, more than five million images of Canadian shoppers were collected through facial recognition software used by Cadillac Fairview, a parent company of malls across the country, <a href="https://www.priv.gc.ca/en/opc-actions-and-decisions/investigations/investigations-into-businesses/2020/pipeda-2020-004/" target="_blank">according to an investigation by privacy officials.</a></p>
<p>
	The federal privacy commissioner reported Thursday that Cadillac Fairview contravened federal and provincial privacy laws by embedding cameras inside digital information kiosks at 12 shopping malls across Canada, and captured users’ images without their consent.</p>
<p>
	The facial recognition software installed in Cadillac Fairview’s “wayfinding” directories was called “Anonymous Video Analytics (AVA) and through cameras installed behind protective glass, was used in Canadian malls for a brief testing period in 2017 and then was in-use between May and July of 2018.</p>
<p>
	The software took temporary digital images of the faces of any individual within the field of view of the camera inside the directory and converted the images into biometric numerical representations of each face and used that information to compile demographic information about mall visitors.</p>
<p>
	According to the report, the technology was used in directories at the following locations:</p>
<ul>
	<li>
		CF Market Mall in Alberta</li>
	<li>
		CF Chinook Centre in Alberta</li>
	<li>
		CF Richmond Centre in British Columbia</li>
	<li>
		CF Pacific Centre in British Columbia</li>
	<li>
		CF Polo Park in Manitoba</li>
	<li>
		CF Toronto Eaton Centre in Ontario</li>
	<li>
		CF Sherway Gardens in Ontario</li>
	<li>
		CF Lime Ridge in Ontario</li>
	<li>
		CF Fairview Mall in Ontario</li>
	<li>
		CF Markville Mall in Ontario</li>
	<li>
		CF Galeries d’Anjou in Quebec</li>
	<li>
		CF Carrefour Laval in Quebec</li>
</ul>
<p>
	According to a statement from Privacy Commissioner of Canada Daniel Therrien, the company said the goal of its cameras was to “analyze the age and gender of shoppers and not to identify individuals.”</p>
<p>
	The corporation said that it did not collect personal information because the images were briefly looked at and then deleted, however the information generated from the images was being stored by a third-party contractor called Mappedin, which Cadillac Fairview said it was unaware of.</p>
<p>
	“When asked the purpose for such collection, Mappedin was unable to provide a response, indicating that the person responsible for programming the code no longer worked for the company,” reads the report.</p>
<p>
	Therrien notes in his report that Cadillac Fairview not being aware of Mappedin’s storage of the information “compounded the risk of potential use by unauthorized parties or, in the case of a data breach, by malicious actors.”</p>
<p>
	In an interview on CTV’s Power Play, Deputy Commissioner Brent Homan called it a “massive invasion of privacy” and not one that shoppers would have expected while at the mall. Homan said that one of the lessons Canadians should take away from this report is that facial recognition software is available for companies to use, and while they encourage entities to ask for consent before deploying it on the public, that’s not always the case.&nbsp;</p>
<p>
	Cadillac Fairview—one of the largest owners and operators of retail and other properties in North America—“expressly disagreed” with the investigation’s findings, telling the commissioners that there were decals placed on shopping mall entry doors noting their privacy policy.</p>
<p>
	These stickers directed visitors to visit guest services to obtain a copy of the company’s privacy policy, but when the investigators asked a guest services employee at the Eaton location in Toronto, the employee was “confused by the request” and so Therrien found the stickers to be an “insufficient” measure.</p>
<p>
	“Shoppers had no reason to expect their image was being collected by an inconspicuous camera, or that it would be used, with facial recognition technology, for analysis,” said Therrien in a statement. “The lack of meaningful consent was particularly concerning given the sensitivity of biometric data, which is a unique and permanent characteristic of our body and a key to our identity.”</p>
<p>
	The investigation was launched in 2018, following several media reports about information kiosks in malls being equipped with unmarked cameras to monitor visitor demographics. Their examination in this case included visiting Cadillac Fairview’s Toronto headquarters to interview key personnel, viewing the AVA technology inside the wayfinding directories in action, and extracting records from the directories for forensic analysis.</p>
<p>
	The existence of the software came to light after a user posted an image to Reddit of a display screen at the CF Chinook Centre in Calgary showing coding language including “FaceEncoder” and “FaceAnalyzer.”</p>
<p>
	Commissioner Therrien’s office worked with Alberta Information and Privacy Commissioner Jill Clayton as well as the Information and Privacy Commissioner of British Columbia Michael McEvoy on the investigation.</p>
<p>
	“Not only must organizations be clear and up front when customers’ personal information is being collected, they must also have proper controls in place to know what their service providers are doing behind the scenes with that information,” Clayton said in a statement.</p>
<p>
	The trio of commissioners have expressed concern that the company hasn’t accepted their request to commit to ensuring meaningful and express consent is obtained from shoppers in the future should it choose to redeploy similar technology in the future.</p>
<p>
	In a statement provided to CTV News, Cadillac Fairview notes that the issue has been resolved, the data deleted, and the cameras have been deactivated. As well, the facial recognition software is no longer in use, but the company says it will not commit to its approach to “hypothetical future uses of similar technology.”</p>
<p>
	“The five million representations referenced in the OPC report are not faces. These are sequences of numbers the software uses to anonymously categorize the age range and gender of shoppers in the camera’s view,” the company said. “We thank the Privacy Commissioner for the report and recommendations on how to further strengthen our privacy practices and agree that the privacy of our visitors must always be a top priority.”&nbsp;</p>
                                              </div></div>]]>
            </description>
            <link>https://www.ctvnews.ca/canada/5-million-canadian-shoppers-images-collected-at-mall-kiosks-privacy-commissioner-1.5166162</link>
            <guid isPermaLink="false">hacker-news-small-sites-24932467</guid>
            <pubDate>Thu, 29 Oct 2020 16:48:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Large inequality in international energy footprints between income groups]]>
            </title>
            <description>
<![CDATA[
Score 33 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24932205">thread link</a>) | @tonyedgecombe
<br/>
October 29, 2020 | https://sci-hub.tf/10.1038/s41560-020-0579-8 | <a href="https://web.archive.org/web/*/https://sci-hub.tf/10.1038/s41560-020-0579-8">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://sci-hub.tf/10.1038/s41560-020-0579-8</link>
            <guid isPermaLink="false">hacker-news-small-sites-24932205</guid>
            <pubDate>Thu, 29 Oct 2020 16:25:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Introducing Semgrep and r2c]]>
            </title>
            <description>
<![CDATA[
Score 107 | Comments 20 (<a href="https://news.ycombinator.com/item?id=24931985">thread link</a>) | @pabloest
<br/>
October 29, 2020 | https://r2c.dev/blog/2020/introducing-semgrep-and-r2c/ | <a href="https://web.archive.org/web/*/https://r2c.dev/blog/2020/introducing-semgrep-and-r2c/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><div><div><div><p>Free, fast, <a href="https://github.com/returntocorp/semgrep" target="_blank" rel="noopener">open-source</a>, offline, customizable. These are not often words that describe code scanning tools, and that's a shame.</p>
<p>We founded r2c to bring world-class security tools to developers based on our conviction that software will run the most exciting parts of the future: everything from medical equipment to robots to autonomous cars. The security process should not be the foe but rather the enabler of rapid software development. If developers lack tooling that is easy to set up and understand—or if a developer has to convince their manager to spend a few million dollars on advanced security tools each time they change jobs, the future is bleak.</p>
<p>Before founding r2c, we worked on security and developer tools for large companies and governments. It was eye-opening to see that despite massive budgets, their security programs were generally a generation or more behind the tech giants. When it came to security tools for developers, most teams were jaded about scanning code for vulnerabilities; they hated the tools they had to use and usually ignored them beyond doing the minimum necessary to satisfy a compliance checkbox.</p>
<p>What about code scanning at places like Facebook, Apple, Amazon, Netflix, and Google? They don't generally use traditional commercial security tools which ask "how can we find every bug?" Instead, they focus on custom tooling that can build guardrails for developers. This doesn't require million-dollar tools, PhDs in program analysis, or days of compute time. It looks much more like unit tests for security.</p>
<p>We believe there is a gap between traditional compliance tools and simple linters that's ripe for a new approach, and we were fortunate to find partners from Redpoint Ventures and Sequoia Capital who agreed. With them, we raised a $13M Series A round of funding to build a security tool that developers might actually love. We've been working on it quietly for a while now, and we're finally ready to announce it to the world!</p>
<h2>Semgrep</h2>
<p><a href="https://semgrep.dev/" target="_blank" rel="noopener">Semgrep</a>, our open-source product, is specifically designed for eradicating bug classes.
Developers and security engineers can say "this is the safe pattern we always use for (e.g. parsing XML)", write a rule in a few minutes, and enforce that on every editor save, commit, and pull request.</p>
<p>Semgrep is ideal for building security guardrails: start by using frameworks designed with security in mind, then automatically flag code that strays from the <a href="https://semgrep.dev/explore" target="_blank" rel="noopener">secure-by-default path</a>. This is an approach used by <a href="https://landing.google.com/sre/resources/foundationsandprinciples/srs-book/" target="_blank" rel="noopener">Google</a>, <a href="https://about.fb.com/news/2019/01/designing-security-for-billions/" target="_blank" rel="noopener">Facebook</a>, <a href="https://homes.cs.washington.edu/~mernst/pubs/continuous-compliance-ase2020.pdf" target="_blank" rel="noopener">Amazon</a>, Dropbox, Stripe, <a href="https://medium.com/@NetflixTechBlog/scaling-appsec-at-netflix-6a13d7ab6043" target="_blank" rel="noopener">Netflix</a>, and others—a topic <a href="https://events.bizzabo.com/OWASPGlobalAppSec/agenda/session/315858" target="_blank" rel="noopener">Clint Gibler and I presented on at Global AppSec 2020</a>. This approach increases developer productivity, reduces attack surface, minimizes the areas for human inspection and audit, and allows the security team to scalably protect code written by thousands of developers.</p>
<p>The idea behind Semgrep is simple: it feels like a regular search (grep) but is syntax-aware. You can <a href="https://semgrep.dev/learn" target="_blank" rel="noopener">learn Semgrep</a> in a few minutes! And Semgrep can be used for <a href="https://semgrep.dev/explore" target="_blank" rel="noopener">more than just security</a> issues: performance, internationalization, or just annoyances <a href="https://r2c.dev/blog/2020/fixing-leaky-logs-how-to-find-a-bug-and-ensure-it-never-returns" target="_blank" rel="noopener">committed by accident</a>.</p>
<p><span>
      <a href="https://r2c.dev/static/730ca8541e43ed871d2edb7c02226b0c/bde6a/semgrep-foo-small.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Semgrep pattern example" title="Semgrep pattern example" src="https://r2c.dev/static/730ca8541e43ed871d2edb7c02226b0c/bde6a/semgrep-foo-small.png" srcset="https://r2c.dev/static/730ca8541e43ed871d2edb7c02226b0c/bde6a/semgrep-foo-small.png 283w" sizes="(max-width: 283px) 100vw, 283px" loading="lazy">
  </a>
    </span></p>
<p><code>$ semgrep -e foo(1)</code> matches all equivalent variations. <a href="https://semgrep.dev/s/ievans:python-exec" target="_blank" rel="noopener">See a live example of matching <em>exec</em> calls</a></p>
<h2>What's Next?</h2>
<p>Semgrep started as an open-source project at Facebook and we're lucky to have its original author, Yoann Padioleau, on our team at r2c. Since we released the first post-Facebook version (0.4) earlier this year, we've released 25 new versions, added support for 8 new languages, reworked the parsers so we could collaborate with Github on <a href="https://tree-sitter.github.io/" target="_blank" rel="noopener">tree-sitter</a>, been joined by thousands of enthusiastic GitHub followers, and seen over 100K pulls of the Semgrep Docker image.</p>
<p>Our roadmap contains more program analysis features to support the sorts of secure-by-default enforcement that large technology companies are already leveraging so heavily (constant propagation, taint tracking, and more), as well as support for many more languages.</p>
<h2>Batteries Included</h2>
<p>Along with this release of Semgrep, we're announcing the availability of <a href="https://semgrep.dev/" target="_blank" rel="noopener">Semgrep Community</a>, a free, hosted service for managing Semgrep CI as well as Semgrep Teams, a paid service which adds additional features for managing Semgrep that are useful for enterprises. Both these offerrings provide SaaS infrastructure for operating a modern AppSec program. They enable central definition of code standards for your projects and show results where you already work: GitHub, GitLab, Slack, Jira, VS Code, and more.</p>
<p>We're also excited that <a href="https://semgrep.dev/explore" target="_blank" rel="noopener">Semgrep Registry</a> already has 900+ rules written by r2c and the community—you can start running on your project right now! Or if you like to DIY, <a href="https://semgrep.dev/editor" target="_blank" rel="noopener">try writing your own</a>.</p></div></div></div></section></div>]]>
            </description>
            <link>https://r2c.dev/blog/2020/introducing-semgrep-and-r2c/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24931985</guid>
            <pubDate>Thu, 29 Oct 2020 16:07:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Optimizations as a Company of One]]>
            </title>
            <description>
<![CDATA[
Score 29 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24931633">thread link</a>) | @jnfr
<br/>
October 29, 2020 | https://lunchbag.ca/company-of-one/ | <a href="https://web.archive.org/web/*/https://lunchbag.ca/company-of-one/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
		<p><span>
				Written on <time datetime="2020-05-18 16:00:00 +0000 UTC">May 18, 2020</time>
			</span>
		</p>
		

<p>Hello! 👋 My name is Jen and I’m the founder, engineer, designer and customer support at <a href="https://lunchmoney.app/">Lunch Money</a>, a personal finance and budgeting web app.</p>

<p>In short, I am a company of one. I answer the customer support emails and I code and deploy new features. I also manage the <a href="https://developers.lunchmoney.app/" target="_blank">API docs</a> and <a href="https://support.lunchmoney.app/" target="_blank">knowledge base</a>, poke around the logs when there are issues, write the <a href="https://lunchmoney.app/sample_newsletter" target="_blank">bi-monthly newsletters</a> and I designed the logo!</p>

<p>As the company scales, so too must all aspects of my work which I break down into 4 parts: customer support, engineering, product and marketing.</p>

<p><strong>Finding opportunities for process optimization is honestly one of the more fun parts of running a business</strong>. I greatly attribute these to how I’ve been able to stay both solo &amp; sane up until now, currently with 600+ users and $45,000 ARR. In this post, I’m excited to share some of my most successful strategies along with anecdotes from my experience working on Lunch Money.</p>



<p>In the last 14 days, here’s what I got done across all departments:</p>

<ul>
<li><strong>Product:</strong> Launched an internal beta testers program and two sets of new features to test</li>
<li><strong>Engineering:</strong> Wrapped up a 2-week long refactor of some core components on the client-side</li>
<li><strong>Engineering:</strong> Launched a major feature: advanced transaction filters</li>
<li><strong>Engineering:</strong> Closed 13 tickets related to feature improvements and bug fixes</li>
<li><strong>Marketing:</strong> Sent out a newsletter outlining the latest new features</li>
<li><strong>Marketing:</strong> This blog post</li>
<li><strong>Marketing:</strong> Added new pages to the marketing site (<a href="https://lunchmoney.app/features/rules">Rules</a> and <a href="https://lunchmoney.app/features/collaboration">Collaboration</a>) and updated the icons in <a href="https://lunchmoney.app/features">Features</a></li>
<li><strong>Support:</strong> Received 239 inbound support tickets and sent out 358 emails</li>
</ul>

<p>What makes all this work bearable for me is the fact that there is so much variety (which, of course, is the spice of life!). I love being able to switch between tasks to keep the job interesting and my mind refreshed while still being productive overall.</p>

<p><em>Case in point: I just finished a major refactor and pushed out 2 major features to beta, so writing this blog post right now feels like a vacation for my brain.</em></p>

<p>To state the obvious, I enjoy being hyper-efficient (without burning myself out, of course). Even though I am a single person, I can still automate, optimize and parallelize processes.</p>



<h2 id="implement-safeguards">Implement safeguards</h2>

<p><i>Ah, the blissful beginnings of Lunch Money when I’d push code directly to production several times a day.</i></p>

<p>At 500+ users now, those days are long gone and I’ve since needed to adopt the more boring but responsible approach of implementing safeguards to prevent shipping faulty code.</p>

<p>For one, <b>I’ve been thoroughly reviewing my own code before every change</b>. Every major feature, improvement and bug fix lives in a feature branch that I still, by habit from the corporate days, prepend with <code>jen/</code>. After verifying everything works great locally, I make sure to take a break first, whether that’s working on a completely unrelated task, going for a meal or going to bed. The point of this is to ensure that I’m code reviewing with a clear head.</p>

<p>While adhering to these standards of code reviewing myself may add extra time to my overall process, it potentially saves hours of bug-fixing, answering related support tickets and self-loathing (I’m half-kidding here) down the line if I accidentally ship bad code.</p>

<center><blockquote><p lang="en" dir="ltr">New phase of Lunch Money– no more pushing major features straight to production 🤯😂 <a href="https://t.co/RYsz7BnU3L">https://t.co/RYsz7BnU3L</a></p>— Jen (@lunchbag) <a href="https://twitter.com/lunchbag/status/1256987437600927744?ref_src=twsrc%5Etfw">May 3, 2020</a></blockquote> </center>

<p><b>I also recently implemented an internal beta-testing program open to Lunch Money subscribers.</b> With more users accessing my app on a regular basis, the stakes are higher to ship a version that’s as bug-free as possible.</p>

<p>As an engineering team of one, it’s nearly impossible to always get it right the first time, despite having tests (what if I missed an edge case?) or testing locally extensively (how I think my users will use a feature is not always so).</p>

<p>The beta-testing program has provided some additional benefits. It’s nice to have a cohort of users with whom I can have a more candid conversation about Lunch Money and it’s also a great way to show users that their feedback is valued!</p>

<h2 id="automate-later-than-you-need-to">Automate later than you need to</h2>

<p>While automating tasks can save a lot of time in the long-term, it doesn’t always make sense to automate right off the bat.</p>

<p><img src="https://imgs.xkcd.com/comics/is_it_worth_the_time.png"><span>Obligatory XKCD comic (<a href="https://xkcd.com/1205/" target="_blank">link</a>)</span></p>

<p>Automating too late is not a bad thing. I’ll have done the manual work enough times to understand how to eventually automate the task and what edge cases to look out for. I equate it to doing a job yourself before hiring someone– it’s always better to grok the requirements first to some degree so you can understand how to best utilize who you’ve brought on (and appreciate them more!).</p>

<p>I manually triggered the emails for bi-monthly account summaries for months because I wasn’t confident the sending cadence was reasonable. After observing open rates and collecting user feedback, these are now automated to go out on the 5th &amp; 25th of every month.</p>

<p>Something that I’m glad I didn’t spend the time to automate at all was the referral program. If an existing user refers a new user and the new user subscribes to a plan, then both users will receive credit for 1 month.</p>

<p>In the end, the referral program only brought on 11 extra users. It was not a pain at all to manually process rewards for those who did. If I had spent time conjuring up all the edge cases and automating this process, it would not have been worth the time investment.</p>

<h2 id="keep-low-hanging-fruits-in-the-back-pocket">Keep low-hanging fruits in the back pocket</h2>

<p>In my task management system, I use a tag (🍏) to denote which tasks are low-hanging fruits. For the uninitiated, low-hanging fruits are quick tasks that are easy to knock out, such as one-liners or 5-minute fixes.</p>

<p><img src="https://media.giphy.com/media/YP1Jb0JNc7kqFDbdjm/giphy.gif"></p>

<p>I find that keeping these around and tackling them on days when you feel generally unmotivated can really help raise spirits. Still being able to get something completed and shipped is a great way to get out of a slump.</p>



<h2 id="timing-marketing-pushes">Timing marketing pushes</h2>

<p>At Lunch Money, a big part of the business is using the services of Plaid for bank syncing. Plaid charges on a monthly basis which means that if a user signs up on April 30, connects a bank account immediately and doesn’t end up subscribing at the end of their 14-day trial, they will charge me for this user in both April &amp; May’s invoices.</p>

<p>This realization coupled with my intense aversion to paying more than I need to has shaped a lot of practices at Lunch Money.</p>

<p>For one, the data retention policy used to be 30 days which means if your trial ends and you didn’t put in your billing information, your data will be deleted in 30 days. This certainly guarantees that I’ll be overpaying for churned users and is the reason why the data retention policy has since been revised to 5 days.</p>

<p>In total, a user who does not end up subscribing can spend up to 26 days in the Lunch Money system. This comprises of a 14-day trial, the potential for a 1 week trial extension and a 5 day grace period. Assuming enough users connect their bank accounts, the best way to minimize my costs for churned users is to ensure their lifetimes are within one calendar month.</p>

<p><img src="https://lunchbag.ca/uploads/calendar-1.png"></p>

<p>As a result, I always schedule marketing pushes such as blog posts and product launches in the first few days of the month. When I came to this realization, my next Plaid bill went down for the first time.</p>

<h2 id="merging-marketing-and-engineering-for-a-combo-win">Merging marketing and engineering for a combo win</h2>

<p>Taking this a step further, the perk of having a spike in new users is that their trial periods more-or-less overlap. Usually when a new user signs up, they will poke around the product and maybe send in a bug report, a feature request or another piece of feedback. If I reply and address their feedback by putting in a fix or shipping their requested feature within days, more often than not, they end up converting into a happy customer.</p>

<p>I’ve therefore identified the following cycle to maximize potential conversions from spikes in user signups:</p>

<p><img src="https://lunchbag.ca/uploads/gantt-2.png"></p>

<p>After a marketing push, let’s say a product launch, I will see an immediate increase in signups lasting about 3 days and then slowly trailing off.</p>

<p>Over the next few days, I’ll start hearing from these new users via support tickets. I prioritize responding to them and I get to work. Showing these users that I’m committed to improving Lunch Money based on their feedback is a great and honest sales tactic.</p>

<p>About 3 days before the initial wave of user trials ends, I wrap up my engineering sprint and send out a newsletter detailing the latest features and improvements. This re-enforces to new users that the product is under continuous development.</p>

<p>Finally, I have a drip campaign that automatically notifies users a few days before their trial expires and on the actual date of expiry. This period of time is when I typically see most users convert 🤞.</p>



<p>I used to think that if I were to hire someone, it would first be a customer support agent but I’ve since been moving away from that idea. Users are constantly surprised (in a good way!) when they realize the founder is responding to their bug reports or feature requests directly.</p>

<p><strong>Consistently providing great customer support is a long-term investment for Lunch Money</strong> as it is undoubtedly a great way to turn customers into champions. As I receive and respond to support tickets, I’m also able to identify and overhaul the common sources of trouble for users.</p>

<p>I’ve always believed that customer support would be the most important and the hardest part of the business to scale, especially if I have the goal of staying a company of one.</p>

<p>Corroborating data from <a href="https://emailmeter.com/">EmailMeter</a> and my database, it seems my changes are making a difference. Despite a growing user base, inbound support emails remain fairly steady!</p>

<p><img src="https://lunchbag.ca/uploads/screen-shot-2020-05-21-at-4-20-15-pm.png"><span>Handling support on my own should be sustainable at least for the foreseeable future!</span></p>

<h2 id="how-do-support-tickets-work-at-lunch-money">How do support tickets work at Lunch Money?</h2>

<p>A feedback button is located at the bottom right corner of every page. Clicking on it opens up a text area wherein users can submit feature requests, questions, bug …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://lunchbag.ca/company-of-one/">https://lunchbag.ca/company-of-one/</a></em></p>]]>
            </description>
            <link>https://lunchbag.ca/company-of-one/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24931633</guid>
            <pubDate>Thu, 29 Oct 2020 15:36:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Compiling a Lisp to x86-64: Labelled procedure calls]]>
            </title>
            <description>
<![CDATA[
Score 87 | Comments 13 (<a href="https://news.ycombinator.com/item?id=24931577">thread link</a>) | @tekknolagi
<br/>
October 29, 2020 | https://bernsteinbear.com/blog/compiling-a-lisp-11/ | <a href="https://web.archive.org/web/*/https://bernsteinbear.com/blog/compiling-a-lisp-11/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p><span data-nosnippet="">
<em><a href="https://bernsteinbear.com/blog/compiling-a-lisp-0/">first</a></em> – <em><a href="https://bernsteinbear.com/blog/compiling-a-lisp-10/">previous</a></em>
</span></p>

<p>Welcome back to the Compiling a Lisp series. Last time, we learned about Intel
instruction encoding. This time, we’re going to use that knowledge to compile
procedure calls.</p>

<p>The usual function expression in Lisp is a <code>lambda</code> — an anonymous function
that can take arguments and close over variables. Procedure calls are <em>not</em>
this. They are simpler constructs that just take arguments and return values.</p>

<p>We’re adding procedure calls first as a stepping stone to full closure support.
This will help us get some kind of internal calling convention established and
stack manipulation figured out before things get too complicated.</p>

<p>After this post, we will be able to support programs like the following:</p>

<div><div><pre><code><span>(</span><span>labels</span> <span>((</span><span>add</span> <span>(</span><span>code</span> <span>(</span><span>x</span> <span>y</span><span>)</span> <span>(</span><span>+</span> <span>x</span> <span>y</span><span>)))</span>
         <span>(</span><span>sub</span> <span>(</span><span>code</span> <span>(</span><span>x</span> <span>y</span><span>)</span> <span>(</span><span>-</span> <span>x</span> <span>y</span><span>))))</span>
    <span>(</span><span>labelcall</span> <span>sub</span> <span>4</span> <span>(</span><span>labelcall</span> <span>add</span> <span>1</span> <span>2</span><span>)))</span>
<span>; =&gt; 1</span>
</code></pre></div></div>

<p>and even this snazzy factorial function:</p>

<div><div><pre><code><span>(</span><span>labels</span> <span>((</span><span>factorial</span> <span>(</span><span>code</span> <span>(</span><span>x</span><span>)</span> 
            <span>(</span><span>if</span> <span>(</span><span>&lt;</span> <span>x</span> <span>2</span><span>)</span> <span>1</span> <span>(</span><span>*</span> <span>x</span> <span>(</span><span>labelcall</span> <span>factorial</span> <span>(</span><span>-</span> <span>x</span> <span>1</span><span>)))))))</span>
    <span>(</span><span>labelcall</span> <span>factorial</span> <span>5</span><span>))</span>
<span>; =&gt; 120</span>
</code></pre></div></div>

<p>These are fairly pedestrian snippets of code but they demonstrate some new
features we are adding, like:</p>

<ul>
  <li>A new <code>labels</code> form that all programs will now have to look like</li>
  <li>A new <code>code</code> form for describing procedures and their parameters</li>
  <li>A new <code>labelcall</code> expression for calling procedures</li>
</ul>

<p>Ghuloum does not explain why he does this, but I imagine that the <code>labels</code> form
was chosen over allowing multiple separate top-level bindings because it is
easier to parse and traverse.</p>

<h3 id="big-ideas">Big ideas</h3>

<p>In order to compile a program, we are going to traverse every binding in the
<code>labels</code>. For each binding, we will generate code for each <code>code</code> object.</p>

<p>Compiling <code>code</code> objects requires making an environment for their parameters.
We’ll establish a calling convention later so that our compiler knows where to
find the parameters.</p>

<p>Then, once we’ve emitted all the code for the bindings, we will compile the
body. The body may, but is not required to, contain a <code>labelcall</code> expression.</p>

<p>In order to compile a <code>labelcall</code> expression, we will compile all of the
arguments provided, save them in consecutive locations on the stack, and then
emit a <code>call</code> instruction.</p>

<p>When all of these pieces come together, the resulting machine code will look
something like this:</p>

<div><div><pre><code>mov rsi, rdi  # prologue
label0:
  label0_code
label1:
  label1_code
main:
  main_code
</code></pre></div></div>

<p>You can see that all of the <code>code</code> objects will be compiled in sequence,
followed by the body of the <code>labels</code> form.</p>

<s>
Because I have not yet figured out how to start executing at somewhere other
than the beginning of the generated code, and because I don't store generated
code in any intermediate buffers, and because we don't know the sizes of any
code in advance, I do this funky thing where I emit a `jmp` to the body code.

If you, dear reader, have a better solution, please let me know.
</s>

<p><strong>Edit:</strong> <em>jsmith45</em> gave me the encouragement I needed to work on this again.
It turns out that storing the code offset of the beginning of the <code>main_code</code>
(the <code>labels</code> body) adding that to the <code>buf-&gt;address</code> works just fine. I’ll
explain more below.</p>

<h3 id="a-calling-convention">A calling convention</h3>

<p>We’re not going to use the System V AMD64 ABI. That calling convention requires
that parameters are passed first in certain registers, and then on the stack.
Instead, we will pass all parameters on the stack.</p>

<p>This makes our code simpler, but it also means that at some point later on, we
will have to add a different kind of calling convention so that we can call
foreign functions (like <code>printf</code>, or <code>exit</code>, or something). Those functions
expect their parameters in registers. We’ll worry about that later.</p>

<p>If we borrow and adapt the excellent diagrams from the Ghuloum tutorial, this
means that right before we make a procedure call, our stack will look like
this:</p>

<blockquote>
  <p>Stack illustration courtesy of <a href="https://leonardschuetz.ch/">Leonard</a>.</p>
</blockquote>

<p>You can see the first return point at <code>[rsp]</code>. This is the return point placed
by the caller of the <em>current</em> function.</p>

<p>Above that are whatever local variables we have declared with <code>let</code> or perhaps
are intermediate values from some computation.</p>

<p>Above that is a blank space reserved for the second return point. This is the
return point for the <em>about-to-be-called</em> function. The <code>call</code> instruction will
fill in after evaluating all the arguments.</p>

<p>Above the return point are all the outgoing arguments. They will appear as
locals for the procedure being called.</p>

<p>Finally, above the arguments, is untouched free stack space.</p>

<p>The <code>call</code> instruction decrements <code>rsp</code> and then writes to <code>[rsp]</code>. This means
that if we just emitted a <code>call</code>, the first local would be overwritten. No
good. Worse, the way the stack would be laid out would mean that the locals
would look like arguments.</p>

<p>In order to solve this problem, we need to first adjust <code>rsp</code> to point to the
last local. That way the decrement will move it below the local and the return
address will go between the locals and the arguments.</p>

<p>After the <code>call</code> instruction, the stack will look different. Nothing will have
actually changed, except for <code>rsp</code>. This change to <code>rsp</code> means that the callee
has a different view:</p>

<blockquote>
  <p>Stack illustration courtesy of <a href="https://leonardschuetz.ch/">Leonard</a>.</p>
</blockquote>

<p>The empty colored in spaces below the return point indicate that the values on
the stack are “hidden” from view, since they are above (higher addresses than)
<code>[rsp]</code>. The called function will <em>not</em> be able to access those values.</p>

<p>If the called function wants to use one of its arguments, it can pull it off
the stack from its designated location.</p>

<blockquote>
  <p>One unfortunate consequence of this calling convention is that Valgrind does
not understand it. Valgrind cannot understand that the caller has placed data
on the stack specifically for the callee to read it, and thinks this is a
move/jump of an uninitialized value. This means that we get some errors now
on these labelcall tests.</p>
</blockquote>

<p>Eventually, when the function returns, the <code>ret</code> instruction will pop the
return point off the stack and jump to it. This will bring us back to the
previous call frame.</p>

<p>That’s that! I have yet to find a good tool that will let me visualize the
stack as a program is executing. GDB probably has a mode hidden away somewhere
undocumented that does exactly this. Cutter sort of does, but it’s finicky in
ways I don’t really understand. Maybe one day <a href="http://akkartik.name/">Kartik</a>’s
x86-64 Mu fork will be able to do this.</p>

<h3 id="building-procedure-calls-in-small-pieces">Building procedure calls in small pieces</h3>

<p>In order for this set of changes to make sense, I am going to explain all of
the pieces one at a time, top-down.</p>

<p>First, we’ll look at the new-and-improved <code>Compile_entry</code>, which has been
updated to handle the <code>labels</code> form. This will do the usual Lisp entrypoint
setup and some checks about the structure of the AST.</p>

<p>Then, we’ll actually look at compiling the <code>labels</code>. This means going through
the bindings one-by-one and compiling their <code>code</code> objects.</p>

<p>Then, we’ll look at what it means to compile a <code>code</code> object. Hint: it’s very
much like <code>let</code>.</p>

<p>Last, we’ll tie it all together when compiling the body of the <code>labels</code> form.</p>

<h3 id="compiling-the-entrypoint">Compiling the entrypoint</h3>

<p>Most of this code is checking. What used to just compile an expression now
validates that what we’ve passed in at least vaguely looks like a well-formed
<code>labels</code> form before picking it into its component parts: the <code>bindings</code> and
the <code>body</code>.</p>

<div><div><pre><code><span>int</span> <span>Compile_entry</span><span>(</span><span>Buffer</span> <span>*</span><span>buf</span><span>,</span> <span>ASTNode</span> <span>*</span><span>node</span><span>)</span> <span>{</span>
  <span>assert</span><span>(</span><span>AST_is_pair</span><span>(</span><span>node</span><span>)</span> <span>&amp;&amp;</span> <span>"program must have labels"</span><span>);</span>
  <span>// Assume it's (labels ...)</span>
  <span>ASTNode</span> <span>*</span><span>labels_sym</span> <span>=</span> <span>AST_pair_car</span><span>(</span><span>node</span><span>);</span>
  <span>assert</span><span>(</span><span>AST_is_symbol</span><span>(</span><span>labels_sym</span><span>)</span> <span>&amp;&amp;</span> <span>"program must have labels"</span><span>);</span>
  <span>assert</span><span>(</span><span>AST_symbol_matches</span><span>(</span><span>labels_sym</span><span>,</span> <span>"labels"</span><span>)</span> <span>&amp;&amp;</span>
         <span>"program must have labels"</span><span>);</span>
  <span>ASTNode</span> <span>*</span><span>args</span> <span>=</span> <span>AST_pair_cdr</span><span>(</span><span>node</span><span>);</span>
  <span>ASTNode</span> <span>*</span><span>bindings</span> <span>=</span> <span>operand1</span><span>(</span><span>args</span><span>);</span>
  <span>assert</span><span>(</span><span>AST_is_pair</span><span>(</span><span>bindings</span><span>)</span> <span>||</span> <span>AST_is_nil</span><span>(</span><span>bindings</span><span>));</span>
  <span>ASTNode</span> <span>*</span><span>body</span> <span>=</span> <span>operand2</span><span>(</span><span>args</span><span>);</span>
  <span>return</span> <span>Compile_labels</span><span>(</span><span>buf</span><span>,</span> <span>bindings</span><span>,</span> <span>body</span><span>,</span> <span>/*labels=*/</span><span>NULL</span><span>);</span>
<span>}</span>
</code></pre></div></div>
<p><code>Compile_entry</code> dispatches to <code>Compile_labels</code> for iterating over all of the
labels. <code>Compile_labels</code> is a recursive function that keeps track of all the
labels so far in its arguments, so we start it off with an empty <code>labels</code>
environment.</p>

<h3 id="compiling-labels">Compiling labels</h3>

<p>In <code>Compile_labels</code>, we have first a base case: if there are no labels we
should just emit the body.</p>

<div><div><pre><code><span>int</span> <span>Compile_labels</span><span>(</span><span>Buffer</span> <span>*</span><span>buf</span><span>,</span> <span>ASTNode</span> <span>*</span><span>bindings</span><span>,</span> <span>ASTNode</span> <span>*</span><span>body</span><span>,</span>
                   <span>Env</span> <span>*</span><span>labels</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>AST_is_nil</span><span>(</span><span>bindings</span><span>))</span> <span>{</span>
    <span>buf</span><span>-&gt;</span><span>entrypoint</span> <span>=</span> <span>Buffer_len</span><span>(</span><span>buf</span><span>);</span>
    <span>// Base case: no bindings. Compile the body</span>
    <span>Buffer_write_arr</span><span>(</span><span>buf</span><span>,</span> <span>kEntryPrologue</span><span>,</span> <span>sizeof</span> <span>kEntryPrologue</span><span>);</span>
    <span>_</span><span>(</span><span>Compile_expr</span><span>(</span><span>buf</span><span>,</span> <span>body</span><span>,</span> <span>/*stack_index=*/</span><span>-</span><span>kWordSize</span><span>,</span> <span>/*varenv=*/</span><span>NULL</span><span>,</span>
                   <span>labels</span><span>));</span>
    <span>Buffer_write_arr</span><span>(</span><span>buf</span><span>,</span> <span>kFunctionEpilogue</span><span>,</span> <span>sizeof</span> <span>kFunctionEpilogue</span><span>);</span>
    <span>return</span> <span>0</span><span>;</span>
  <span>}</span>
  <span>// ...</span>
<span>}</span>
</code></pre></div></div>

<p>We also set the buffer entrypoint location to the position where we’re going to
emit the body of the <code>labels</code>. We’ll use this later when executing, or later in
the series when we emit ELF binaries. You’ll have to add a field <code>word
entrypoint</code> to your <code>Buffer</code> struct.</p>

<p>We pass in an empty <code>varenv</code>, since we are not accumulating any locals along
the way; only labels. For the same reason, we give a <code>stack_index</code> of
<code>-kWordSize</code> — the first slot.</p>

<p>If we <em>do</em> have labels, on the other hand, we should deal with the first label.
This means:</p>

<ul>
  <li>pulling out the name and the code object</li>
  <li>binding the name to the <code>code</code> location (the current location)</li>
  <li>compiling the <code>code</code></li>
</ul>

<p>And then from there we deal with the others recursively.</p>

<div><div><pre><code><span>int</span> <span>Compile_labels</span><span>(</span><span>Buffer</span> <span>*</span><span>buf</span><span>,</span> <span>ASTNode</span> <span>*</span><span>bindings</span><span>,</span> <span>ASTNode</span> <span>*</span><span>body</span><span>,</span>
                   <span>Env</span> <span>*</span><span>labels</span><span>)</span> <span>{</span>
  <span>// ....</span>
  <span>assert</span><span>(</span><span>AST_is_pair</span><span>(</span><span>bindings</span><span>));</span>
  <span>// Get the next binding</span>
  <span>ASTNode</span> <span>*</span><span>binding</span> <span>=</span> <span>AST_pair_car</span><span>(</span><span>bindings</span><span>);</span>
  <span>ASTNode</span> <span>*</span><span>name</span> <span>=</span> <span>AST_pair_car</span><span>(</span><span>binding</span><span>);</span>
  <span>assert</span><span>(</span><span>AST_is_symbol</span><span>(</span><span>name</span><span>));</span>
  <span>ASTNode</span> <span>*</span><span>binding_code</span> <span>=</span> <span>AST_pair_car</span><span>(</span><span>AST_pair_cdr</span><span>(</span><span>binding</span><span>));</span>
  <span>word</span> <span>function_location</span> <span>=</span> <span>Buffer_len</span><span>(</span><span>buf</span><span>);</span>
  <span>// Bind the name to the location in the instruction stream</span>
  <span>Env</span> <span>entry</span> <span>=</span> <span>Env_bind</span><span>(</span><span>AST_symbol_cstr</span><span>(</span><span>name</span><span>),</span> <span>function_location</span><span>,</span> <span>labels</span><span>);</span>
  <span>// Compile the binding function</span>
  <span>_</span><span>(</span><span>Compile_code</span><span>(</span><span>buf</span><span>,</span> <span>binding_code</span><span>,</span> <span>&amp;</span><span>entry</span><span>));</span>
  <span>return</span> <span>Compile_labels</span><span>(</span><span>buf</span><span>,</span> <span>AST_pair_cdr</span><span>(</span><span>binding…</span></code></pre></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://bernsteinbear.com/blog/compiling-a-lisp-11/">https://bernsteinbear.com/blog/compiling-a-lisp-11/</a></em></p>]]>
            </description>
            <link>https://bernsteinbear.com/blog/compiling-a-lisp-11/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24931577</guid>
            <pubDate>Thu, 29 Oct 2020 15:31:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[MemgraphDB: Why and how we implemented Bolt Protocol v4]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24931364">thread link</a>) | @karimtr
<br/>
October 29, 2020 | https://memgraph.com/blog/memgraph-1-2-release-implementing-the-bolt-protocol-v4 | <a href="https://web.archive.org/web/*/https://memgraph.com/blog/memgraph-1-2-release-implementing-the-bolt-protocol-v4">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h3>Introduction</h3>
<p>Today, we’re proud to announce the release of <a href="https://memgraph.com/download">Memgraph 1.2</a>, which significantly improves Memgraph’s compatibility with the broader graph ecosystem. This makes it easier for developers and data scientists to work with Memgraph using their favourite tools.</p>
<p>One of the biggest changes in this release, is the addition of Bolt v4 and v4.1 support.</p>
<p>In this post, we will explore what exactly is the Bolt protocol, what it brings to the table, and how we implemented it into Memgraph.</p>
<h2>The Bolt Protocol</h2>
<p>If you’re thinking about using Memgraph in your application, one of the
requirements is the possibility of querying Memgraph directly from your
application with as little effort as possible. You can achieve that by writing
drivers for the Memgraph server in the language you want to support. Drivers are
special libraries that follow predefined rules, aka a protocol, to communicate between
your application and a server.</p>
<p>Instead of defining its own rules, Memgraph decided to use Neo4j’s protocol
called <a href="https://7687.org/">Bolt</a>. There are 3 important reasons for this
decision:</p>
<ol>
<li>Defining a protocol is not easy</li>
<li>Neo4j also uses <a href="https://www.opencypher.org/">Cypher</a> (that doesn’t mean that the Bolt protocol can’t be used for other query languages!)</li>
<li>By supporting Neo4j’s protocol we automatically become compatible with their drivers</li>
</ol>
<p>The drivers that Neo4j currently maintains are:</p>
<ul>
<li><a href="https://github.com/neo4j/neo4j-java-driver">Java Driver</a></li>
<li><a href="https://github.com/neo4j/neo4j-javascript-driver">JavaScript Driver</a></li>
<li><a href="https://github.com/neo4j/neo4j-dotnet-driver">.NET Driver</a></li>
<li><a href="https://github.com/neo4j/neo4j-python-driver">Python Driver</a></li>
<li><a href="https://github.com/neo4j/neo4j-go-driver">Go Driver</a></li>
</ul>
<p>In other words, by making our server compatible with the Bolt protocol, you can
use Memgraph in any of the languages and frameworks listed above just by using
Neo4j’s libraries.</p>
<h3>Bolt Protocol Rule Examples</h3>
<p>First, we need to know how to exchange messages. Bolt exchanges its messages using a request-response pattern between the client and the server. Each request message can be followed by zero or record messages  which are then followed by one summary message. The different possibilities for record messages depends on the type of the request message.</p>
<hr>
<p><strong>NOTE</strong></p>
<p>A record message is a type of message which contains records, aka result rows.</p>
<hr>
<p>Also, we need to know how to serialize our data. Bolt uses its own
<a href="https://7687.org/packstream/packstream-specification-1.html#version1">PackStream</a> which provides specification for serializing a bunch of different types of data. It is fully compatible with the types
supported by Cypher. We won’t go into details but every type is defined with its marker, its size and its data.</p>
<p><img src="https://i.imgur.com/2GkXojm.png" alt=""></p>
<p><em>Source:</em> <em><a href="https://7687.org/packstream/packstream-specification-1.html">https://7687.org/packstream/packstream-specification-1.html</a></em></p>
<p>One of those types is a structure. The size of the structure defines how many fields
it contains and the fields can be of any other type. But we’re missing an
important information. How do we know what the structure represents? Structures
carry additional data, its tag byte. This tag tells us what does the structure
represent. We’re now half way through to understanding how to define request and
response messages.</p>
<p>We can’t expect that our data will always be small enough to send it all at
once. To solve this problem, Bolt defines how the message is chunked. Each chunk
is starts with two-byte header that tells us the size of chunk data in bytes
followed by the chunk data itself. Now, we have another problem. How do we know
if we received the last chunk of message? We just add a marker! In our case we
append to the end of the last chunk <code>00 00</code>.</p>
<p>Now, we have everything we need to define our messages. We can define each type
of request/response message as a unique structure, having a unique set of fields.
We can send the defined structure using the chunking method defined before.
We’re set! We can serialize and deserialize messages now!</p>
<h4>Bolt Protocol Specifications</h4>
<p>A good protocol specification should contain as much information as possible.
Without enough information, we can only guess how our server or client should
behave in some situations, causing a lot of headache for every developer that
tries to implement that protocol.</p>
<p>So, as a good protocol, Bolt defines how to parse different types of request
message and send the correct response message. It defines how each request
message looks and what to send as a response message in each possible
situation. Also, it defines the state of server after each request message and
its outcome.</p>
<p>If want to delve deeper into the Bolt Protocol specifications, you can find everything <a href="https://7687.org/">here</a>.</p>
<hr>
<p><strong>NOTE</strong></p>
<p>Implementing rules is not hard, but to do it efficiently requires a lot of
careful planning and having a good understanding of how the protocol works.</p>
<hr>
<h2>Evolution of the Bolt Protocol</h2>
<p>As with any software, protocols are susceptible to change. Bolt defines it’s
version using major and minor versions. At the start of each connection, the
client needs to do a handshake with the server.</p>
<p>The handshake is really simple and consists of only two steps:</p>
<ul>
<li>client sends at most 4 versions it supports, sorted by priority</li>
<li>server responds with the first version in the list it supports</li>
</ul>
<h3>But doesn’t Memgraph support Bolt protocol?</h3>
<p>Yes, Memgraph does support the Bolt protocol. But up until now, it only supported Bolt v1, while the current version is 4.1. By looking at the handshake process we can conclude that the client can support <strong>at most</strong> four versions. The logical thinking is that the client will always support the latest 4 versions. At the time of writing this, the latest version is v4.1, which pushed v1 out of the support list, making us, and everyone else that wanted to try Memgraph using Neo4j’s drivers, very sad.</p>
<p><img src="https://i.imgur.com/OC6Tzd2.png" alt=""></p>
<hr>
<p><strong>NOTE</strong></p>
<p>It’s important to emphasize that after version 1.0, newer versions weren’t documented,
which made keeping up with the newer versions really hard. But, after v4.1, Neo4j
decided to document every version nicely making our lives much easier. Thanks
Neo4j!</p>
<hr>
<h2>The Road to Bolt v4.(1)</h2>
<p>Since Memgraph was only compatible with the first version of the Bolt protocol, we had three major and one minor version change to catch up with.Most of it was just some basic additions to the already existing messages, but there were also some bigger changes. For example, we made the decision to preserve support for Bolt v1. This has been very challenging as one of the hardest things in programming is making bigger changes to an existing code while not breaking the old behaviour.</p>
<h3>Supporting multiple versions</h3>
<p>Handling a code that behaves differently for each version can be hard. After we
decide on a version for a specific connection, we need to be careful which messages
are allowed for that version, which response should each message produce, what
parameters are allowed, and many more things. And to do that while reusing as much
of code as possible, with the addition of keeping the readability can be a challenge.
The only real advice I can give you here is write as many tests that will cover as
much as possible because a smallest detail can make your server misbehave while
implementing a support for a protocol.</p>
<h3>Making transaction handling easier and more powerful</h3>
<p>In Bolt v3, new request messages for handling transactions were added. Those messages
are for starting an explicit transaction and ending the transaction by
committing or rollbacking the changes.
Because we already had support for transactions and you could already do
the same thing by running queries consisting of <code>BEGIN</code>, <code>COMMIT</code> and <code>ROLLBACK</code>
commands, the only thing we had to do was add functions that directly run those
queries when the corresponding request was received.</p>
<h3>Getting some results from here and some from there</h3>
<p>The biggest change to the Bolt protocol was the change to the <code>PULL</code> and
<code>DISCARD</code> message.
Before we delve deeper, let’s explain those messages.
When you want to run a query on a server using Bolt messages, first you need to
send a <code>RUN</code> message that contains the query we want to execute. To get the results
of the query we send a <code>PULL</code> message, and if we want to discard the results,
we simply send the <code>DISCARD</code> message. The natural way of handling this is preparing
the query when we receive the <code>RUN</code> message and executing it when we receive the
<code>PULL</code> message. Additionally, to avoid wasting memory, we don’t keep the result, we
just forward it to the encoder and send it directly to the client.</p>
<p>In Bolt v1, there were <code>PULL_ALL</code> and <code>DISCARD_ALL</code> messages. As their name suggests,
the only options you had was all or nothing. Taking this into account, we developed
a solution that would simply stream all the results to the client after it receives
<code>PULL_ALL</code> message. But, since v4.0, things got a little more complicated.
The <code>PULL_ALL</code> message was renamed to <code>PULL</code>. Additionally, the <code>PULL</code> message can come with some extra parameters.</p>
<p><img src="https://i.imgur.com/4ibg9Db.png" alt=""></p>
<h4><code>n</code> parameter</h4>
<p>You can now pull an arbitrary number of results. This small change implies a lot of
changes to the existing code. The easiest solution would be to execute the query on the
first pull and save all of the results in memory. After that, for each pull, we just
send next <code>n</code> results. Even though it’s the easiest solution to implement, it’s too
inefficient memory-wise. Taking this into account, we have a hard requirement of
keeping the old, lazy behaviour while not keeping any of the results in memory.</p>
<p>There are different types of queries and each query demands a different approach to
achieve this behaviour. Queries with a constant size of the result, like profiling and
explain queries, can have a simple vector of results from which the results are
lazily pulled. For most of the queries that have variable size of the result, we
prepare all the necessary resources for the execution and ask for the next result only
when it’s needed after which the results are streamed instantly to the client.
The resources are cleaned after the <code>PULL</code> request that returned the
last result. This is possible because of Memgraph’s lazy way of handling the execution.</p>
<p>The query that was surprisingly the hardest to implement lazily was the <code>DUMP</code> query.
By itself, it’s really simple to implement this query. You analyse different parts of
your database and, as a result, send a query that defines that part. For example, we
iterate each vertex in our database, and we send back …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://memgraph.com/blog/memgraph-1-2-release-implementing-the-bolt-protocol-v4">https://memgraph.com/blog/memgraph-1-2-release-implementing-the-bolt-protocol-v4</a></em></p>]]>
            </description>
            <link>https://memgraph.com/blog/memgraph-1-2-release-implementing-the-bolt-protocol-v4</link>
            <guid isPermaLink="false">hacker-news-small-sites-24931364</guid>
            <pubDate>Thu, 29 Oct 2020 15:13:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How I built a dashboard template and sold it for $90.000]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24930252">thread link</a>) | @pixelcave
<br/>
October 29, 2020 | https://pixelcave.com/blog/how-i-built-a-dashboard-template-and-sold-it-for-90000-usd | <a href="https://web.archive.org/web/*/https://pixelcave.com/blog/how-i-built-a-dashboard-template-and-sold-it-for-90000-usd">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-content">
                        <p>I bought my first PC back in 1998 and started experimenting with web design a few years later. It was amazing how easily you could create web pages with rich content and navigate between them just like in interactive desktop apps. Let’s not talk about all those texture backgrounds along with animated gifs that no one could resist using. It was a playground and I loved it.</p>

<p>Microsoft Frontpage and Macromedia Dreamweaver (which was later acquired by Adobe) were the first tools I used to design and code websites just for the fun of it. At first, it was a more ‘what you see is what you get’ (WYSIWYG) approach but soon, I started playing more directly with HTML and CSS through Dreamweaver’s code editor.</p>

<p>A passion was born with various fun web projects being built on the side. I loved that aspect and while I was studying for my bachelor’s degree in Informatics, I got involved with all the web projects I could find. Those included web development and helped me improve my skills in those areas as well.</p>

<p>The idea that I might be able to work online from my home quickly became an obsession back then. In 2009, demand for web applications was rising, so I started experimenting with dashboard templates and eventually, 3 projects were created and released in the next 3 years.</p>

<p>I built them in my free time while I was working with a tech company on a few web apps and studying for my master’s degree at the same time. They did just ok but a whole new world was opened in front of me.</p>

<p>Unfortunately, due to the various responsibilities I had at the time, I kept postponing any further action but I eventually did a dynamic comeback in 2013 when I was ready to put all my energy into designing and coding dashboard templates full-time.</p>

<h3>Passionate with UI design</h3>

<p>You’ve heard before that you need to love your work if you would like to be successful but that’s only half the story. Loving what you do can motivate you, inspire you and make things a bit easier but that’s not necessarily enough in the long run.</p>

<p>I was and I am extremely passionate with UI design and coding. Back then, I used Photoshop to design each project and afterwards, I did my best to code it into HTML/CSS and make sure it works great in each major desktop and mobile browser. I used to spend so many hours testing and making sure that the result would be as perfect as possible based on my skills.</p>

<p>In 2013, I was able to build and release 3 more projects which were built with love and care. That’s also when the mottos “Crafted with love” and “Happy coding” came to life and follow pixelcave since then. The templates did good but unfortunately it was nothing sustainable.</p>

<p>I loved what I was doing and felt really good designing and coding but that on its own wasn’t enough. I needed to start thinking differently because if the whole plan didn’t work out, pixelcave most probably wouldn’t exist today.</p>

<h3>What people need</h3>

<p>I knew that I needed to deliver value to the people using those projects and save them time, so I had to do a far better job researching their needs before even start creating the next project, <a href="https://pixelcave.com/products/proui">ProUI</a>. It might seem obvious now but wasn’t the case back then.</p>

<p>I went through all the feedback (emails and comments) I had received from the first 3 projects and kept putting together a list of all the things people liked, struggled with, or wish they had. A few things on the list kept coming back, so I knew that those were crucial, and I had to prioritize them.</p>

<p>Next up, I researched public feedback regarding similar projects and got a feeling of what didn’t work and the problems most of the people were having when using such products. There were many issues that also kept coming back, so I’ve already had a good list of features and solutions that I had to work on ProUI.</p>

<p>What kept me researching for a while was the feeling that creating another project in the same way, would give me the same mediocre results. You can’t expect to have a different outcome when doing the same things repeatedly. I’m glad I’ve followed that path before getting my hands dirty and start coding the new template.</p>

<h3>Deliver under pressure</h3>

<p>This project was everything back then because its success or failure would completely change my life. If it didn’t work, it was the end of my working from home career and I had to completely change my approach and start looking for alternatives. There was no money or time to waste and for my mind, it was a matter of survival, I wanted it to succeed so much.</p>

<p>That was the perfect timing and the pressure helped me take it very seriously. I tried to be positive and passionate about the result and kept working day and night towards making a great product. It’s funny how pressure can help or harm your work. I have experienced both outcomes in the past but thankfully it was one of the things that pushed me forward, helped me overcome my fears and boosted my creativity. ProUI was live and the pressure had delivered. Sales started coming in, a new world appeared in front of my eyes and I knew that nothing was going to be the same again.</p>

<h3>Be original</h3>

<p>ProUI was designed and coded from scratch by hand. In contrary to the way other products were created by only using readymade layouts, navigation elements and other major building blocks, ProUI foundation was based on a solid structure built exclusively for it. I think that’s what gave it an identity in the first place and helped it in the long run.</p>

<p>I did my best to implement many popular features, provide solutions to issues people were experiencing, make a template that is straightforward, easy to use and most importantly that works as advertised. When inspiration hit, personal design touches were applied to make the design original and give it the feeling of a fresh experience.</p>

<h3>Test everything like crazy</h3>

<p>Testing was one of the main features of ProUI and continue being for all current projects. It might be simpler now with most popular browsers being chromium-based but that wasn’t the case back in the days. Internet Explorer 8 was the baseline and the newly introduced popularity of mobile browsers with their own set of issues wasn’t making things any better.</p>

<p>Responsive design was in its glory days and started becoming mainstream back in 2013 but testing tools weren’t on par yet. I still remembered resizing the browser like 1000 times each day to ensure that all content would appear as supposed to from mobile screens up to desktop monitors.</p>

<p>Testing on devices was also a big issue because I only had access to 2 older smartphones. New devices kept releasing at the time with various browsers popping up, so my solution was to visit stores for testing on their promo devices! I uploaded versions of the work-in-progress template to the demo server and tested it against various devices, from Macbooks to iPhones, iPads, and latest Android devices.</p>

<p>One of the main issues people were experiencing at the time was the poor mobile performance, so I was determined to make ProUI as fast and responsive as possible. During my visits, I kept notes and tried to fix the bugs when I got back home on the fly hoping that they will work. Of course, that wasn’t always the case, so visiting the stores became part of my weekly work schedule! I switched stores occasionally, so it doesn’t get too awkward...</p>

<p>Thankfully, the effort fulfilled its purpose and ProUI was released in a good stable state. I kept visiting the stores for a few months though (before I was able to get my hands on my own testing devices) to handle any reported bugs. My goal was to always reply in less than 24hrs during business days to support requests (something that I still do), so there were days of me rushing through stores to handle the situation. I tried to keep it cool with my responses despite the store situation (how professional is to test against demo devices in a store?) but in the end, I want to believe that the effort and care I gave, really paid off and helped ProUI be as bug free as possible.</p>

<h3>Balance time and delivery</h3>

<p>I learned that you must keep a balance between the features you want to implement in your project and the time it takes you to do it. It’s far better to integrate less and put your project out there sooner than trying to make it as complete as possible.</p>

<p>While building ProUI, bills kept coming in, which in the end I think helped me because I had to put it out there as quickly as possible. That might not made it the perfect release I might had in my mind but in the end, ProUI provided value and helped people in their projects.</p>

<h3>Not knowing stuff</h3>

<p>This is important and seems to apply in everything I do. As you learn more about your product’s market or about the tech you are using, it gets harder and harder to put something out there. You analyze everything way too much and easily spot the things that might go, or you do wrong.</p>

<p>There was this guy, who started selling WordPress plugins without knowing much about the market but focused on creating great products. After managing to reach 1 million in sales, he stated that if he knew the things he discovered afterwards, he would probably have never started selling WordPress plugins in the first place because it would be too difficult for him.</p>

<p>It was early days, and this is how I felt when I was building ProUI. It was liberating not analyzing what works and what doesn’t and focus on the product itself. Since then, I try to keep a balance between the things I discover and the things I want to experiment with when working on something. It’s exactly like the designer who redesigns his website and before he even finishes, he already finds the new design awful. Don’t be like that, try to fight back!</p>

<h3>Being perfect is subjective</h3>

<p>I would describe myself as perfectionist, but I try not to. Thankfully, the characterization does not apply, most of the time. When you are aiming for perfection, the only one you satisfy is yourself. You set the bar of perfection based on the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pixelcave.com/blog/how-i-built-a-dashboard-template-and-sold-it-for-90000-usd">https://pixelcave.com/blog/how-i-built-a-dashboard-template-and-sold-it-for-90000-usd</a></em></p>]]>
            </description>
            <link>https://pixelcave.com/blog/how-i-built-a-dashboard-template-and-sold-it-for-90000-usd</link>
            <guid isPermaLink="false">hacker-news-small-sites-24930252</guid>
            <pubDate>Thu, 29 Oct 2020 13:31:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Secrets of the best product teams]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24929484">thread link</a>) | @scotthtaylor
<br/>
October 29, 2020 | https://st.im/secrets-of-the-best-product-teams/ | <a href="https://web.archive.org/web/*/https://st.im/secrets-of-the-best-product-teams/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <p>Travelling back in time to 2004, the “World Wide Web” was dominated by Microsoft, AOL, and Jeeves. (the)Facebook had just launched, and it would be three more years until the first iPhone. I was just fifteen, and didn’t have many friends. I did, however, have a keen interest in computers, and was blown away by the connectivity of the Internet. It wasn’t long before I started immersing myself in the coding languages that powered it -- building websites that I wanted to use, but didn’t yet exist.</p><p>I didn’t know it back then, but I was dipping my toes in the waters of <strong>product management</strong>, or <strong>product</strong> as it’s better known now.</p><p>Product essentially being the ability to work with a multidisciplinary team of people and build a simple solution to a real customer problem in a way that meets the needs of the business. All with an understanding of how design, business, technology and users intersect and overlap.</p><p>Back then product management was more associated with enterprise software, with Product Managers (PMs) being concentrated in the likes of Cisco and Oracle.</p><p>As Internet adoption exploded over the next decade and startups became part of everyday life, PMs were no longer building highly specialised or corporate software exclusively. They were the founders building stuff that helped with everything from dating through to grabbing a taxi. They were also building products in larger corporates; for example Gmail within Google.</p><p>The PM concept had hit the mainstream. </p><p>Amazon, Google, Facebook, Netflix, Tesla -- the list could go on -- have all successfully scoped out, built, and launched products that are used by billions of people. All because they have a solid product culture, that connects all the dots. It’s hard to imagine a modern-day tech company not having ‘product’ at their core.</p><p>With this context, I wanted to talk about the secrets that I’ve noticed in high performing product teams over the years. So whether you’re an early stage startup working on achieving product/market fit, or a growth stage company working on scale, or even a large corporate trying to regain your ability to consistently deliver new value to your customers, I’m sure that there will be something to takeaway.</p><h2 id="making-sure-the-basics-are-covered">Making sure the basics are covered </h2><p>Before jumping into the secrets, I want to make sure we’re on the same page with regard to the basics. Without these being covered no product team will be able to have consistent break-out success.</p><!--kg-card-begin: markdown--><pre><code>def sum_product(n): 
    s = 0 
    while n: 
        vision +
        functionality +
        technology +
        user experience design +
        monetisation +
        acquisition +
        offline experience
    return s
</code></pre>
<!--kg-card-end: markdown--><p>Distilling it down to first principles, I think ‘product’ is really about evaluating opportunities and determining what gets built and delivered to customers. Everything stems from this. And to do it successfully, you must:</p><ol><li>Understand your customer,</li><li>Understand the data, and</li><li>Understand your business and the industry it operates in</li></ol><p>Understanding the customer means you’re an expert on their issues, pains, desires, and how they think. Without this, you’re just guessing. And it has to be a mix of both quantitative and qualitative learning. </p><p>Understanding the data covers a wide gamut, not only the ability to understand your customer but to know what they’re doing with your product. Successful products need to be loved by your customers but also need to work for your business. This means knowing who your stakeholders are and the constraints they operate under.</p><p>Finally understanding your market. You need to know who your competitors are – in addition to key trends, customer behaviours and expectations.</p><p>Each of these principles deserves its own in-depth post – but, for now, I wanted to provide some brief context before touching on the lesser known secrets. </p><h2 id="first-you-need-a-big-mission">First, you need a big mission</h2><p>It’s critical for your team to be organised around something that’s motivating. </p><p>It has to be a mission that’s worthwhile. Something that gets them out of bed. But it also has to be somewhat ambiguous, or unattainable, as well. Think of Elon Musk’s mission for SpaceX – to colonise Mars. </p><p>This helps people become missionaries. It also helps align the skills of the people that are potentially going to be working on the product. Finally, it serves as a filter. A filter for people who are able to connect the dots, from the potentially mundane tasks of today to the exciting vision of tomorrow. These are the people that you want around you. Those who can’t connect the dots, will naturally fall by the wayside.</p><p>Next, even though the <em>mission</em> may sound crazy, this doesn’t mean the <em>vision</em> is.</p><p>The product vision has to be consumer centric yet aligned with the mission. The product team are usually the ones translating that mission into reality. And that’s what the vision helps you do. </p><p>Product are usually the ones initiating the conversation around what the product vision should be, because we’re the ones talking to the customer.</p><p>So an idea might be seeded by the founders or execs, hypothesising about an area of opportunity. But it’s only once you start interacting with the customer, that you can say, “Oh yeah, that’s a good idea” or, “No that’s an awful idea, let’s not do that.”</p><p>The product vision gives the organisation its purpose, and we only want people in our organisation that are excited about, and dedicated to this vision —missionaries. PM thought leader Marty Cagan describes this well:</p><blockquote>There are many benefits of product teams, but a big goal is captured best by a quote from John Doerr, the famous Silicon Valley venture capitalist: “we need a team of missionaries, not teams of mercenaries.” Mercenaries build whatever they’re told to build. Missionaries are true believers in the vision and are committed to solving problems for their customers. <br></blockquote><h2 id="next-a-finger-on-the-pulse-of-innovation">Next, a finger on the pulse of innovation</h2><p>High performing product teams are always seeking improvement. They're open to adopting new methodologies (e.g. think Agile a few years ago) and implementing emerging technologies. They have a high risk tolerance, and love placing educated bets.</p><p>As an example, pretty much all the awesome product teams I know are currently, as I type this, immersing themselves in artificial intelligence and machine learning. They understand that AI is the next general purpose technology – something that will affect the world, and an opportunity that usually only comes along once in a generation. </p><p>These teams understand that scale economies accrue to first movers in AI, and second movers will find it difficult to catch up. By adopting early they reap the rewards of a positive feedback loop. They will capture early customers who, in turn, will create more data for the product. Resulting in a virtuous cycle that gives them a real, tangible and defensible advantage.</p><p>AI is transcending every industry. PMs used to be confined to just the tech sector, but the PMs of tomorrow will be needed in industries as diverse as farming and transportation; &nbsp;construction and medicine. Knowing how the AI product life cycle differs from that which has gone before will be a core competitive advantage.</p><p>An ingrained culture of continuous improvement and evolution is at the heart of every high performing team. The key is to be continuously scanning the market for trends and evolutions, not just in the markets for which you are building products, but in how and why.</p><h2 id="it-s-not-a-finite-game">It’s not a finite game</h2><p>Strong product teams don’t think in terms of a first or second half of the game – trying to get points on the board. But rather, they think of it as an infinite game. Always trying to get better. Understanding that there’s mastery beyond where they are, at every step.</p><p>This means they’re always going to beat their competition, because the competition is always going to be seeking this quarterly result, or that end event. Maybe ’the event’ is an IPO or an acquisition. Whereas the high performing team isn’t focussed on any particular business event, they’re seeking, “What’s the best possible outcome for this brand?”; “What’s the best possible outcome for this customer?”</p><p>This fundamentally changes the mentality of the team and the nature of how they show up every day. </p><p>It also relates to open mindedness – is your team open to new learning? Or do they assume they know everything?</p><p>The open mindset reminds your team of the idea that we’re not perfect, that we could fail, and that we may not have the answer. You need to have a culture where it’s okay to say “I don’t know” even when someone might expect you to.</p><h2 id="safe-psychological-spaces">Safe psychological spaces</h2><p>Do your team members feel like they can express an opinion or share what they think without feeling like they’re going to be repressed by your opinion, or that of the loudest person in the room? &nbsp;</p><p>Strong product teams don't look for consensus. They understand that innovation and collaboration are not correlated to consensus. </p><p>People should be encouraged to share their opinions. They can have disagreements about approaches but there needs to be an open forum where people have an opportunity to share. </p><p>A common problem is that people go to work and they don’t feel like they can express their opinions because they've got a structure, or a manager, or a situation that doesn’t allow for that to happen. </p><p>Safe psychological space is something that managers and leaders of high performing teams work a lot on. And it’s never over. You're always changing it because you're always bringing new people in. And every time you bring a new person in, you’ve got a change in dynamic. How do you make sure those people feel safe and that the people that who were previously interacting in a safe way don’t feel like they’re being adjusted in some way that’s negative.</p><h2 id="many-other-contributing-factors">Many other contributing factors</h2><p>The selection of secrets I’ve offered here are just a few of my favourites. </p><p>Others on the shortlist include:</p><ol><li>Empowerment and accountability -- ensuring that your team members are assigned problems to solve, rather than just given lists of …</li></ol></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://st.im/secrets-of-the-best-product-teams/">https://st.im/secrets-of-the-best-product-teams/</a></em></p>]]>
            </description>
            <link>https://st.im/secrets-of-the-best-product-teams/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24929484</guid>
            <pubDate>Thu, 29 Oct 2020 11:48:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Page Load Time Comparison of Raspberry Pi 3 and 4 Web Servers]]>
            </title>
            <description>
<![CDATA[
Score 45 | Comments 20 (<a href="https://news.ycombinator.com/item?id=24929444">thread link</a>) | @sT370ma2
<br/>
October 29, 2020 | https://cheapskatesguide.org/articles/raspberry-pi-3-4-web-server.html | <a href="https://web.archive.org/web/*/https://cheapskatesguide.org/articles/raspberry-pi-3-4-web-server.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://cheapskatesguide.org/articles/raspberry-pi-3-4-web-server.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24929444</guid>
            <pubDate>Thu, 29 Oct 2020 11:43:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[For Complex Applications, Rust Is as Productive as Kotlin]]>
            </title>
            <description>
<![CDATA[
Score 18 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24929335">thread link</a>) | @todsacerdoti
<br/>
October 29, 2020 | https://ferrous-systems.com/blog/rust-as-productive-as-kotlin/ | <a href="https://web.archive.org/web/*/https://ferrous-systems.com/blog/rust-as-productive-as-kotlin/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div id="preamble">
<div>
<p>In this article, we will compare one apple (IntelliJ Rust) to one orange (rust-analyzer) to reach general and sweeping conclusions.
Specifically, I want to present a case study supporting the following claim:</p>
<p>For complex applications, Rust is as productive as Kotlin.</p>
<p>For me, this is an unusual claim to argue: I always thought exactly the opposite, but I am not so sure now.
I came to Rust from C++.
I was of the opinion that this is a brilliant low-level language and always felt puzzled at people writing higher-level things in Rust.
Clearly, choosing Rust means taking a productivity hit, and using Kotlin, C# or Go just makes much more sense if you can afford GC.
My <a href="https://matklad.github.io/2020/09/20/why-not-rust.html">list of Rust criticisms</a> starts with this objection.</p>
<p>What moved my position in the other direction was my experience as the lead developer of rust-analyzer and IntelliJ Rust.
Let me introduce the two projects.</p>
<p><a href="https://github.com/intellij-rust/intellij-rust"><strong>IntelliJ Rust</strong></a> is the plugin for IntelliJ Platform, providing Rust support.
In effect, it is a Rust compiler front-end, written in Kotlin and making use of language-support features of the platform.
These features include lossless syntax trees, a parser generator, persistence and indexing infrastructure, among others.
Nonetheless, as programming languages differ lot, the bulk of logic for analyzing Rust is implemented in the plugin itself.
Presentational features like completion list come from the platform, but most of the language semantics is hand-written.
IntelliJ Rust also includes a bit of a Swing GUI.</p>
<p><a href="https://github.com/rust-analyzer/rust-analyzer"><strong>rust-analyzer</strong></a> is an implementation of
<a href="https://microsoft.github.io/language-server-protocol/">Language Server Protocol</a> for Rust.
It is a Rust compiler front-end written from scratch with an eye towards IDE support.
It makes heavy use of <a href="https://github.com/salsa-rs/salsa">salsa</a> library for incremental computations.
Beyond the compiler itself, rust-analyzer includes code for managing long-lived multithreaded process of the language server itself.</p>
<p>The projects are essentially equivalent in scope — rust compiler front-ends suitable for IDEs.
The two biggest differences are:</p>
<div>
<ul>
<li>
<p>IntelliJ Rust is a plugin, so it can re-use code and design patterns of the surrounding platform.</p>
</li>
<li>
<p>rust-analyzer is the second system, so it leverages experience of IntelliJ Rust for a from-scratch design.</p>
</li>
</ul>
</div>
<p>The internal architecture of the two projects also differs a lot.
In terms of <a href="https://rust-analyzer.github.io/blog/2020/07/20/three-architectures-for-responsive-ide.html">Three Architectures</a>, IntelliJ Rust is map-reduce, and rust-analyzer is query-based.</p>
<p>Writing an IDE-ready compiler is a high-level task.
You don’t need to talk to the operating system directly.
There are some fancy data structures and concurrency here and there, but they are also high-level.
It’s not about implementing crazy lock-free schemes, it’s about maintaining application state and sanity in the multithreaded world.
The bulk of the compiler is symbolic manipulation, arguably best suited for lisp.
Picking a VM-based language for such task (for example, OCaml), doesn’t have any intrinsic downsides.</p>
<p>At the same time, the task is pretty complex and unique.
The ratio of “your code” vs “framework code” when implementing features is much higher than in a typical CRUD backend.</p>
<p>Now that the projects are introduced, lets take two roughly equivalent slices of history.</p>

<p>Both are about 2 years old, with 1-1.5 developers working full time and vibrant and thriving community of open-source contributors.
There are 52k lines of Kotlin and 66k lines of Rust.</p>
<p>Both delivered roughly equivalent feature sets at that time.
To be honest, I still don’t really believe this :)
rust-analyzer started from zero, it didn’t have a decade worth of Java classes to bootstrap from, and the productivity drop between Kotlin and Rust is supposed to be huge.
But it’s hard to argue with reality.
Instead, let me try to reflect on my experience building both, and to try to explain Rust’s surprising productivity.</p>
</div>
</div>
<div>
<h2 id="_learning_curve">Learning Curve</h2>
<div>
<p>It’s easy to characterize Kotlin’s learning curve — it is nearly zero.
I’ve started IntelliJ Rust without Kotlin experience and never felt that I need to specifically learn Kotlin.</p>
<p>When I switched to rust-analyzer, I was pretty experienced with Rust.
I would say that one definitely needs to deliberately learn Rust, it’s hard to pick it up on the go.
Ownership and aliasing control are novel concepts (even if you come from C++), and taking holistic approach to learning them pays off.
After the initial learning step the ride is generally smooth.</p>
<p>By the way, this is the perfect place to plug our Rust courses and tailor-made <a href="https://ferrous-systems.com/training/">trainings</a> :-)
The next <a href="https://ferrous-systems.com/training/#package-intro-training">introduction to Rust</a> is happening this December!</p>
</div>
</div>
<div>
<h2 id="_modularity">Modularity</h2>
<div>
<p>This I think is the biggest factor.
Both projects are moderately large in terms of scope as well as in terms of amount of source code.
I believe that the only way to ship big things is to split them in independent-ish chunks and implement the chunks separately.</p>
<p>I also find most of the languages I am familiar with to be pretty horrible with respect to modularity.
More generally, I am amused with FP vs OO debate, as it seems that “why no one does modules right?” is a more salient issue.</p>
<p>Rust is one of the few languages which has first-class concept of libraries.
Rust code is organized on two levels:</p>
<div>
<ul>
<li>
<p>as a tree of inter-dependent modules inside a crate</p>
</li>
<li>
<p>and as a directed acyclic graph of crates</p>
</li>
</ul>
</div>
<p>Cyclic dependencies are allowed between the modules, but not between the crates.
Crates are units of reuse and privacy: only crate’s public API matters, and it is crystal clear what crate’s public API is.
Moreover, crates are anonymous, so you don’t get name conflicts and dependency hell when mixing several versions of the same crate in a single crate graph.</p>
<p>This makes it very easy to make two pieces of code <strong>not</strong> depend on each other (non-dependencies are the essence of modularity): just put them in separate crates.
During code review, only changes to Cargo.tomls need to be monitored carefully.</p>
<p>At the time of comparison, rust-analyzer is split into 23 internal crates, with a handful general-purposed ones released on crates.io.
In contrast, IntelliJ Rust is a single Kotlin module, where everything can depend on everything else.
Although internal organization of IntelliJ Rust is pretty clean, it’s not reflected in the file system layout and build system, and needs constant maintenance.</p>
</div>
</div>
<div>
<h2 id="_build_system">Build System</h2>
<div>
<p>Managing project’s build takes significant amount of times, and has multiplicative effect on everything else.</p>
<p>Rust’s build system, <a href="https://doc.rust-lang.org/cargo/index.html">Cargo</a>, is very good.
It’s not perfect, but it is a breath of fresh air after Java’s <a href="https://gradle.org/">Gradle</a>.</p>
<p>Cargo’s trick is that it doesn’t try to be a general purpose build system.
It can only build Rust projects, and it has rigid expectation about the project structure.
It’s impossible to opt out of the core assumptions.
Configuration is a static non-extensible TOML file.</p>
<p>In contrast, Gradle allows free-form project structure, and is configured via a Turing complete language.
I feel like I’ve spend more time learning Gradle than learning Rust!
Running <code>wc -w</code> gives 182_817 words for Rust book, and 280_506 for Gradle’s user guide.</p>
<p>Additionally, Cargo is just faster than Gradle in most cases.</p>
<p>Of course, the biggest downside is that custom build logic is not expressible in Cargo.
Both projects needs substantial amount of logic beyond mere compilation to deliver the final result to the user.
For rust-analyzer, this is handled by hand-written Rust script, which works perfectly at this scale.</p>
</div>
</div>
<div>
<h2 id="_ecosystem">Ecosystem</h2>
<div>
<p>Language-level support for libraries and top-notch build system/package manager allow for a thriving ecosystem.
rust-analyzer relies on third-party libraries much more than IntelliJ Rust.
Some parts of rust-analyzer are also published to crates.io for other projects to reuse.</p>
<p>Additionally, low-level nature of the Rust programming language often allows for “perfect” library interfaces.
Interfaces which exactly reflect the underlying problem, without imposing intermediate language-level abstractions.</p>
</div>
</div>
<div>
<h2 id="_basic_conveniences">Basic Conveniences</h2>
<div>
<p>I feel that Rust is significantly more productive when it comes to basic language nuts and bolts — structs, enums, functions, etc.
This is not specific to Rust — any ML-family language has them.
However, Rust is the first industrial language which wraps these features in a nice package, not constrained by backwards compatibility.
I want to list specific features which I think allow producing maintainable code faster in Rust</p>
<p><em>Emphasis on data over behavior</em>.
Aka, Rust is not an OOP language.
The core idea of OOP is that of dynamic dispatch — which code is invoked by a function call is decided at runtime (late binding).
This is a powerful pattern which allows for flexible and extensible system.
The problem is, extensibility is costly!
It’s better only to apply it in certain designated areas.
Designing for extensibility by default is not cost effective.
Rust puts static dispatch front and center: it is mostly clear whats going on by just reading the code, as it is independent of runtime types of the objects.</p>
<p>One small syntactic thing I enjoy about Rust is how it puts fields and methods into different blocks syntactically:</p>
<div>
<div>
<pre><code data-lang="rust"><span>struct</span> <span>Person</span> <span>{</span>
  <span>first_name</span><span>:</span> <span>String</span><span>,</span>
  <span>last_name</span><span>:</span> <span>String</span><span>,</span>
<span>}</span>

<span>impl</span> <span>Person</span> <span>{</span>
    <span>fn</span> <span>full_name</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> <span>-&gt;</span> <span>String</span> <span>{</span>
        <span>...</span>
    <span>}</span>
<span>}</span></code></pre>
</div>
</div>
<p>Being able to see at a glance all the fields makes understanding the code much simpler.
Fields convey much more information than methods.</p>
<p><em>Sum types</em>.
Rust’s humbly named enums are full algebraic data types.
This means that you can express the idea of disjoint union:</p>
<div>
<div>
<pre><code data-lang="rust"><span>enum</span> <span>Either</span><span>&lt;</span><span>A</span><span>,</span> <span>B</span><span>&gt;</span> <span>{</span> <span>A</span><span>(</span><span>A</span><span>),</span> <span>B</span><span>(</span><span>B</span><span>)</span> <span>}</span></code></pre>
</div>
</div>
<p>This is hugely useful in day-to-day programming in the small, and some times during programming in the large as well.
To give one example, one of the core concepts for an IDE are references and definitions.
A definition a like <code>let foo = 92;</code> assigns a name to an entity which can be used down a line.
A reference like <code>foo + 90</code> <em>refers</em> to some definition.
When you ctrl-click on reference, you go to the definition.</p>
<p>Natural way to model that in Kotlin is by adding <code>interface Definition</code> and <code>interface Reference</code>.
The problem is, some things are …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ferrous-systems.com/blog/rust-as-productive-as-kotlin/">https://ferrous-systems.com/blog/rust-as-productive-as-kotlin/</a></em></p>]]>
            </description>
            <link>https://ferrous-systems.com/blog/rust-as-productive-as-kotlin/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24929335</guid>
            <pubDate>Thu, 29 Oct 2020 11:27:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[  You will need a subscription license to access Qt 6 (non-LGPL)]]>
            </title>
            <description>
<![CDATA[
Score 32 | Comments 12 (<a href="https://news.ycombinator.com/item?id=24928720">thread link</a>) | @deng
<br/>
October 29, 2020 | https://www.qt.io/faq/what-happens-to-my-perpetual-licenses-once-i-convert-to-subscription | <a href="https://web.archive.org/web/*/https://www.qt.io/faq/what-happens-to-my-perpetual-licenses-once-i-convert-to-subscription">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p><span><span>Yes. If your </span><span>Qt licenses are perpetual, you may continue to use the product in perpetuity after your maintenance expires.&nbsp; Access to product and technical support will only be available via the purchase of an Extended Maintenance Contract for software releases that are end-of-life. If you opt not to renew, please note that Qt will not guarantee to support software versions acquired with a perpetual license.</span></span></p>
<p><span><span>Please note, you will need a subscription license to access Qt 6.</span></span></p>
<!--more-->
<p><span><span>Qt versions can be viewed <a href="https://wiki.qt.io/QtReleasing" rel="noopener">here</a>.</span></span></p>
</span></p></div>]]>
            </description>
            <link>https://www.qt.io/faq/what-happens-to-my-perpetual-licenses-once-i-convert-to-subscription</link>
            <guid isPermaLink="false">hacker-news-small-sites-24928720</guid>
            <pubDate>Thu, 29 Oct 2020 09:44:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The remote work tools we'd love to see next year]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24927955">thread link</a>) | @dmonn
<br/>
October 28, 2020 | https://nohq.co/blog/the-remote-work-tools-wed-love-to-see-in-2021/ | <a href="https://web.archive.org/web/*/https://nohq.co/blog/the-remote-work-tools-wed-love-to-see-in-2021/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                    <p>In 2020, we were happy to have talked to <strong>94</strong> (!) remote tool companies about what they are up to. With more than a dozen we talked a little closer and many of them we've shown you. However, amongst the sea of amazing new innovations in the remote work space, we still found gaps.</p>
<p>Struggles we've heard from remote teams that don't have a solution yet. Solutions that are missing that little something to make them amazing. These are our wishes for the next year.</p>

<h2>Operations</h2>
<p>Running an internationally distributed team has a lot of challenges to overcome, but the most rigid of them all are international regulations and specializations when it comes to running a multi-national organization.</p>
<p>The operations side of remote work is usually deemed pretty unsexy. It's all about working with regulators, shaking hands and most likely an expensive journey to expand in a lot of countries. Hard work, but one that wouldn't go unappreciated.</p>

<h3>1. Better payment gateways for remote teams</h3>
<p>For how far we've come with remote work, we really haven't made a ton of strides with cross-continental payments. In my first-ever remote job, I used to get paid from a regular US corporate bank account directly to my swiss bank account. Payments got routed wrongly here and there, the fees were sky-high and the payments often took a week to get to me.</p>
<p>Since then, we've made a stride or two. With something like <a href="https://bit.ly/3kzvDyo">TransferWise,</a> you can make the whole process a little bit speedier and cheaper. Transfers now only take 1-2 working days and are fairly transparent. Even better: Fees and availability aren't confined to one nation, TransferWise is almost available anywhere.</p>
<p>Then, there are online wallets like Venmo and PayPal. They are really handy to guarantee instant transfers at a cheap price and are very handy for folks that are unbanked. Skipping the dusty banking infrastructure? All for it!</p>
<p>And then, of course, there's cryptocurrency. Now, Bitcoin &amp; co. were never meant to become investment instruments. In the past year, I asked to get paid for something through crypto twice. The payments were instant, extremely cheap (<em>I paid around $0.001 in fees</em>) and my bank didn't ask me about that weird big-money-transfer from the US. Now, if it only was easier to access.</p>

<p><img src="https://nohq.co/media/undraw_online_payments_luau.png" alt="" width="600" height="370"></p>

<p>So, how does the payment gateway of the future look like? Well, first of all, it should be accessible. Too many financial services are confined to the US, Canada and the UK. It should also be internationally receivable. While a bank transfer almost goes anywhere, I've heard multiple times now that people would prefer not getting foreign direct-to-bank payments to accounts in India, for example. The remote payment stack accounts for that.</p>
<p>Finally, waiting for your pay stub and then the money a couple of days later really doesn't fit in the internet age anymore. <strong>Maybe you can leverage cryptocurrency and existing infrastructure to create something instant, simple and internationally accessible?</strong></p>

<h3>2. Making 401k international</h3>
<p>When it comes to retirement benefits, many remote companies don't take it 100% seriously just yet and according to the latest surveys, employees don't mind either. With work-life balance as a commonly cited benefit, being remote in itself is an amazing perk. People are happy to take a pay cut or even let go of their retirement benefits.</p>
<p>Even if a company is <a href="https://about.gitlab.com/handbook/total-rewards/benefits/general-and-entity-benefits/#general-and-entity-benefits">open to having a 401k</a>, doing it the "right" away is almost impossible with every country having its own system. From the <a href="https://de.wikipedia.org/wiki/Pensionskasse">Pensionskasse</a> in Germany to the <a href="https://en.wikipedia.org/wiki/Pensions_in_Japan">EPS</a> in Japan, countries run their own systems that are usually not accessible to foreign entities.</p>
<p>This might not seem important at first, but according to <a href="http://www.mit.edu/~vchern/papers/ch_401k.pdf">researchers at MIT</a> and multiple financial advisories, taking part in and maxing out 401k accounts (and subsequent alternatives in other countries) is a cornerstone to building life-changing wealth. For all personal finance lovers out there, not having that option may be a dealbreaker. Offering 401k and other retirement benefits is about to get a lot more important.</p>
<p>Our idea: <strong>What can a private company build within the regulations that allows for true, employer-matched and internationally available retirement accounts?</strong> Piggy-back off existing infrastructure and regulations? Sell gold bars and store them in a few storage units? Some sort of e-insurance? Again, cryptocurrency? We'd love to hear ideas.</p>

<h3>3. Benefits that are not only show</h3>
<p>For most remote companies, "perks" and "benefits" under the hood only mean a bit more money. In all remote jobs I've had so far, I've claimed my perk payouts as part of my regular salary payout. That means additional taxable income, not what perks are about.</p>
<p>Let me explain. A few years ago, my friend got hired at a very generous company local to us. He got a generous salary but was additionally able to lease a car through the company, set up a subscription for lunch delivery and for a while was even able to rent a room out of the company's real estate arm. The costs of that were deducted directly from his salary. As a result of that, he paid a few $1,000 less in taxes than in the previous years.</p>
<p>Many of those perks – from a company car to new Macbook – are much easier to reimburse in cash for remote companies, so no pre-tax perks at all. <strong>As a potential new service, what can you do to make this happen?</strong></p>
<p>One option is to hire through one of the many <a href="https://nohq.co/hire/">EOR services</a> that specialize in providing a full-service. For many companies that already have their payroll in order, that's not interesting though. You could save people multiple $1,000s in taxes here, so it's a service you don't have to sell for cheap.</p>

<h2>Communication</h2>
<p>Communication is still the king of all remote tools. Out of the many tools we've seen this year, probably 2/3 were in the communication and collaboration space of some sort. So, what gaps are supposed to still be there? Let's walk through it.</p>

<h3>4. All-hands meetings that work</h3>
<p>I believe meetings are largely figured out. This year, we've seen some iterations on the traditional meeting experience, but most of them are a minor improvement for teams that have an amazing product-fit. One thing that isn't solved yet is large, moderated all-hands meetings.</p>
<p>All-hands meetings are meetings that include all, or at least most of the company in the same meeting. The meeting experience is fundamentally different from your standard small group meeting: The speaker is in focus and other participants rarely speak.</p>
<p>The relationship between speaker and listener is quite clear – one speaker, many participants (often over 100) that should be able to speak when called to do so but not otherwise. The experience of such meetings has improved with new meeting software, compared to teams that had to make-do with Skype &amp; co., but it's still not seamless.</p>

<p><em>"At some point, to have everyone on a Skype call for an all-hands meeting, we would have two laptops set up next to each other and have one laptop call half the company and the other laptop call the other half of the company. That was nonsense ðŸ˜„"</em></p>
<p><strong><a href="https://nohq.co/blog/michael-fey-of-1password/">Michael Frey, VP Engineering at 1Password</a></strong></p>

<p>The last time I had an all-hands meeting on Zoom, things didn't go great. Some people called in and were unable to mute themselves. People tried to ask questions but got interrupted by either the speaker or another person in the audience.</p>
<p>With over 100 participants, latency was all over the place and the worst part – the meeting host had a disconnect at some point in the meeting and while the room was able to continue, the recording stopped there. A third of the team was not able to re-play the meeting.</p>
<p>I believe all-hands meetings warrant a new piece of tech that is rarely used – probably only for larger talks and announcements once a month – but works for that type of meeting every single time, including server-side recording, low latency, speaking requests and a larger range of moderation features.</p>

<h3>5. Communication &amp; Writing Training</h3>
<p>Communication and writing skills have been essential requirements for modern remote teams that largely rely on sparse and long-form communication. Many talented people struggle with that, are either not used or accommodated to typing a lot of simply prefer the face-to-face way of doing things.</p>
<p>Just like we build our knowledge worker's skills using education stipends, conferences, courses and books, we should also start looking into active communication training.</p>
<p>In the past, I have benefitted from coaching a lot in my career. Coaches have helped me push through some tough professional issues and have helped me build a small successful business. In the future, coaching will not only be interesting for hard skills but possibly even more soft skills. Getting continuous training in communication and writing will be popular with goal-getters and teams alike.</p>
<p>Now, I'm not necessarily advocating for plain and simple writing training. <a href="https://www.grammarly.com/">Grammarly</a> has put an automated writing coach in many author's pockets, but what if you could push that further? Remind you that you didn't give a status update in a while? Notifying you if your text lacks some context or reads passive-aggressive. Grammarly works great, even as I am writing this post, but it's not an end-to-end communication coach, just an integral part of it. <strong>We'd love to see something new in that space.</strong></p>

<h3>6. Asynchronous Brainstorm</h3>
<p>A consistent piece of feedback we've gotten this year is that teams struggle to find a good way to brainstorm new innovations. Remote work is amazing to get into a deep state, collect some thoughts and find new ideas, but when it comes to collaboration and getting those thoughts out, it gets more difficult.</p>
<p>The way we've learned to brainstorm in the professional world is using visualizations. We like to draw mind maps or sketch graphs on a whiteboard. Filling out the blank space can indeed yield new ideas and has helped really innovative teams come up with amazing ideas.</p>

<p><img src="https://nohq.co/media/undraw_miro_qvwm.png" alt="" width="600" height="407"></p>

<p>The options we have to do so across the globe are not always fitting to …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://nohq.co/blog/the-remote-work-tools-wed-love-to-see-in-2021/">https://nohq.co/blog/the-remote-work-tools-wed-love-to-see-in-2021/</a></em></p>]]>
            </description>
            <link>https://nohq.co/blog/the-remote-work-tools-wed-love-to-see-in-2021/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24927955</guid>
            <pubDate>Thu, 29 Oct 2020 06:48:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Image Scaling Attacks]]>
            </title>
            <description>
<![CDATA[
Score 416 | Comments 67 (<a href="https://news.ycombinator.com/item?id=24927655">thread link</a>) | @wendythehacker
<br/>
October 28, 2020 | https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/ | <a href="https://web.archive.org/web/*/https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
    <p>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag “huskyai” to see related posts.</p>
<ul>
<li><a href="https://embracethered.com/blog/posts/2020/husky-ai-walkthrough/">Overview</a>: How Husky AI was built, threat modeled and operationalized</li>
<li><a href="https://embracethered.com/blog/posts/2020/husky-ai-threat-modeling-machine-learning/">Attacks</a>: Some of the attacks I want to investigate, learn about, and try out</li>
</ul>
<p>A few weeks ago while preparing demos for my GrayHat 2020 - Red Team Village presentation I ran across “Image Scaling Attacks” in <a href="https://www.usenix.org/system/files/sec20-quiring.pdf">Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning</a> by Erwin Quiring, et al.</p>
<p>I thought that was so cool!</p>
<h2 id="what-is-an-image-scaling-attack">What is an image scaling attack?</h2>
<p>The basic idea is to hide a smaller image inside a larger image (it should be about 5-10x the size). The attack is easy to explain actually:</p>
<ol>
<li>Attacker crafts a malicious input image by hiding the desired target image inside a benign image</li>
<li>The image is loaded by the server</li>
<li>Pre-processing resizes the image</li>
<li>The server acts and makes decision based on a different image then intended</li>
</ol>
<p>My goal was to hide a husky image inside another image:</p>
<p><a href="https://embracethered.com/blog/images/2020/image-rescale-attack.gif"><img src="https://embracethered.com/blog/images/2020/image-rescale-attack.gif" alt="Image Rescaling Attack"></a></p>
<p>Here are the two images I used - before and after the modification:
<a href="https://embracethered.com/blog/images/2020/image-rescaling-attack-schoenbrunn.png"><img src="https://embracethered.com/blog/images/2020/image-rescaling-attack-schoenbrunn.png" alt="Image Rescaling Attack"></a></p>
<p>If you look closely, you can see that the second image does have some strange dots all around. But this is not noticable when viewed in smaller version.</p>
<p>You can find the code on <a href="https://github.com/EQuiw/2019-scalingattack">Github</a>. I used Google Colab to run it, and there were some errors initialy but it worked - let me know if interested and I can clean up and share the Notebook also.</p>
<h2 id="rescaling-and-magic-happens">Rescaling and magic happens!</h2>
<p>Now, look what happens when the image is loaded and resized with <code>OpenCV</code> using default settings:</p>
<p><a href="https://embracethered.com/blog/images/2020/image-rescaling-attack.png"><img src="https://embracethered.com/blog/images/2020/image-rescaling-attack.png" alt="Image Rescaling Attack"></a></p>
<p>On the left you can see the original sized image, and on the left the same image downsized to 128x128 pixels.</p>
<p><strong>That’s amazing!</strong></p>
<p>The downsized image is an entirely different picture now! Of course I picked a husky, since I wanted to attack “Husky AI” and find another bypass.</p>
<h2 id="implications">Implications</h2>
<p>This can have a set of implications:</p>
<ol>
<li><strong>Training process:</strong> Images that poisen the training data (as pre-processing rescales images)</li>
<li><strong>Model queries:</strong> The model might predict on a different image than the one the user uploaded</li>
<li><strong>Non ML related attacks:</strong> This can also be an issue in other, non machine learning areas.</li>
</ol>
<p>I guess security never gets boring, there is always something new to learn.</p>
<h2 id="mitigations">Mitigations</h2>
<p>Turns out that Husky AI uses PIL and that was not vulnerable to this attack by default.</p>
<p>I got lucky, because initially Husky AI did use <code>OpenCV</code> and it’s default settings to resize images. But for some reason I changed that early on (not knowing it would also mitigate this attack).</p>
<p>If you use <code>OpenCV</code> the issue can be fixed by using the <code>interpolation</code> argument when calling the <code>resize</code> API to not have it use the default.</p>
<p>Hope that was useful and interesting.</p>
<p>Cheers,
Johann.</p>
<p><a href="https://twitter.com/wunderwuzzi23">@wunderwuzzi23</a></p>
<h2 id="references">References</h2>
<ul>
<li>Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning (<a href="https://www.usenix.org/system/files/sec20-quiring.pdf">https://www.usenix.org/system/files/sec20-quiring.pdf</a>) (Erwin Quiring, TU Braunschweig)</li>
<li><a href="https://github.com/EQuiw/2019-scalingattack">https://github.com/EQuiw/2019-scalingattack</a></li>
</ul>

  </section></div>]]>
            </description>
            <link>https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24927655</guid>
            <pubDate>Thu, 29 Oct 2020 05:48:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Sale of Amateur Radio AMPRnet TCP/IP Addresses Raised $108M]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24927037">thread link</a>) | @todsacerdoti
<br/>
October 28, 2020 | https://www.icqpodcast.com/news/2020/10/25/sale-of-amateur-radio-amprnet-tcpip-addresses-raised-108m | <a href="https://web.archive.org/web/*/https://www.icqpodcast.com/news/2020/10/25/sale-of-amateur-radio-amprnet-tcpip-addresses-raised-108m">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-7b3dd6c3950208794ea7"><div><p>President of Amateur Radio Digital Communications (ARDC) has confirmed they received $108 million from Amazon for 4 million amateur radio TCP/IP addresses </p><p>Since its allocation to Amateur Radio in the mid-1980s, Internet network 44 (44.0.0.0/8), known as the AMPRNet™, has been used by amateur radio operators to conduct scientific research and to experiment with digital communications over the radio with a goal of advancing the state of the art of Amateur Radio networking, and to educate amateur radio operators in these techniques.</p><p>Amateur Radio Digital Communications (ARDC) is a non-profit California corporation formed to further these goals.</p><p>In mid-2019 a block (44.192.0.0/10) of approximately four million AMPRNet™ IP addresses, out of the 16 million available, was sold to Amazon by ARDC but it is only now that the sale price has been released. Amazon paid $27 for each IPv4 address.</p></div></div></div>]]>
            </description>
            <link>https://www.icqpodcast.com/news/2020/10/25/sale-of-amateur-radio-amprnet-tcpip-addresses-raised-108m</link>
            <guid isPermaLink="false">hacker-news-small-sites-24927037</guid>
            <pubDate>Thu, 29 Oct 2020 03:57:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to walk upright and stop living in a cave]]>
            </title>
            <description>
<![CDATA[
Score 17 | Comments 13 (<a href="https://news.ycombinator.com/item?id=24927008">thread link</a>) | @taylorlunt
<br/>
October 28, 2020 | https://taylor.gl/blog/9/ | <a href="https://web.archive.org/web/*/https://taylor.gl/blog/9/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
    
    <div>
      
<p>
<a href="https://taylor.gl/">Home</a>

  
  
  <a href="https://taylor.gl/"> </a>

  
  
  »
  
  <a href="https://taylor.gl/blog/"> Blog</a>

</p>

<p><span>
  Reading time: 9 minutes.

  Written in 2020.
</span></p>

<p>Call me arrogant, but I’d rather optimize my indoor environment than try to spend more time in the capricious outdoors. I think it’s defeatism to give up on improving our indoor spaces and resign ourselves to the fickle weather and seasons. </p>
<p>If I was going to create an ideal environment for a human, I think there are several things I would include that we routinely fail to include in our homes and offices.</p>
<h3 id="lighting">Lighting</h3>
<p>Our indoor lighting situation usually sucks. The fact that “natural lighting” is a selling point in real estate shows how terrible a job we are doing in this department. We rely on the sun naturally providing us with sufficient light, and if it’s an overcast day or the days have grown shorter in the winter, then I guess we’re shit out of luck. </p>
<p>Usually, indoor areas are around 50-500 lux. This is hundreds of times dimmer than the sunlight. Clearly, we weren’t designed to thrive in such dim environments, and science does verify a connection between brighter light and alertness. If we don’t want to be sleepy like it’s nighttime, we shouldn’t light our rooms like it’s nighttime. For some, the effects of dim lighting go beyond simple lethargy and, especially in the winter, cause serious mood problems like seasonal affective disorder or the winter blues. This is common, but it’s not necessary. Bright light, particularly blue light, can also generally boost mood and may be a comparable stimulant to caffeine. (Those who are prone to mania should be careful, as intense light can trigger mania or hypomania in those predisposed.) Brighter lighting can also help circadian rhythm issues (which I, for example, have struggled with for years), both by entraining your circadian rhythm so your body better knows when it’s day, and by shortening it if it’s too long. </p>
<p>Lighting isn’t as expensive as it used to be, so we can do better than we have in the past. The cost of electricity for LED lighting is now negligible, and the only real factor is the cost of the bulbs themselves. Reaching for the full 100,000 lux of sunlight would still be prohibitively expensive, but going for at least 10,000 lux is doable with only a few hundred dollars. I won’t go into specifics here, but you can get more information on specific lighting setups <a href="https://www.lesswrong.com/posts/hC2NFsuf5anuGadFm/how-to-build-a-lumenator">here</a> or <a href="https://meaningness.com/metablog/sad-light-lumens">here</a>. In particular, get bulbs with a color temperature close to sunlight (5600k), but make sure the bulbs have a <span>good<span>good means 90+</span></span> Color Rendering Index (CRI), otherwise the light will feel harsh.</p>
<p>I recommend putting any bright lighting you buy for your home on electrical timers so you don’t accidentally leave them on during the evening and screw up your sleep. You may also want to set your phone/computer brightness on a timer, if you can. The goal is to mimic the natural day/night cycle of our evolutionary environment, but without all the pesky volatility of nature. You can get programs like f.lux too, which reduce the amount of blue light emitted by your device in the evening, but in my experience this isn’t good enough and reducing the actual brightness of the device at night is also important.</p>
<p>“But what about vitamin D? Just go outside!” This is terrible advice, and I hear it too often. Sunlight is a powerful carcinogen, and vitamin D supplements are not, and they’re cheap. </p>
<h3 id="carbon-dioxide">Carbon dioxide</h3>
<p>Carbon MON-oxide is the deadly one you probably already have a monitor for in your house. Carbon DI-oxide is the feeble cousin of carbon monoxide, but it still has a negative effect on human health: <span>high (but common)<span>1,000 ppm or higher</span></span> levels impairs our ability to think. Just what you don’t want in an office. High levels may also have a negative long-term impact in other areas of our health. </p>
<p>Hold your breath. When it sucks and you decide to start breathing again, it’s carbon dioxide buildup, not lack of oxygen, causing you to feel panic and the need to breath. Carbon dioxide is a toxin. And we breath it out into poorly ventilated rooms, where the levels can rise to double or triple what they are <span>outdoors<span>around 400 ppm</span></span>.</p>
<p>Several studies have shown significant (temporary) cognitive impairments due to carbon dioxide levels over 1,000 ppm, but such levels are <span>common<span>I recently bought a carbon dioxide meter and found such levels in my home.</span></span> in poorly ventilated shared spaces. Fortunately, the solution is simple: open a window. Unfortunately, this doesn’t work when it’s raining, or when it’s too hot outside, or when it’s too cold outside… In particular, I have to contend with Canadian winters, which means opening the window is a valid strategy for a minority of the year unless I buy an expensive heat recovery ventilator. I don’t have a good solution for mitigating carbon dioxide buildup in the winter. Let me know if you do.</p>
<p>And, by the way, plants won’t work. They won’t suck up nearly enough carbon dioxide. You would need hundreds of plants per person, or roughly a dozen full-size trees per person, to offset the carbon dioxide exhaled by humans in a room.</p>
<p>A fun fact: if we don’t stop pumping carbon dioxide into the atmosphere, then in about a century, carbon dioxide <em>outdoors</em> may reach cognitively impairing levels. Then what do we do? </p>
<h3 id="temperature-and-humidity">Temperature and Humidity</h3>
<p>High/low humidity and high/low temperature both lead to discomfort and lower scores on concentration measures. People generally have temperature under control, or at least it’s something they’re aware of. Humidity is less common to measure, but a $10 hygrometer should help you get your indoor space to the ideal 30-50% humidity range if it isn’t already. Air conditioners also tend to reduce humidity as well as temperature, so air-condition in the summer and use a humidifier in the winter.</p>
<p>At night, drop the temperature a few degrees if you can; It’s easier to sleep in a cool room. I wonder how many hours of sleep have been reclaimed already due to the advent of smart thermostats.</p>
<h3 id="background-noise">Background Noise</h3>
<p>I imagine this factor is more subjective than the others, but too loud is distracting, even aggrivating; too quiet makes your sniffles and sighs painfully audible to others, and so is distracting. Uneven background noise like traffic is worse than the uniform background noise of white noise or trickling water. Bad background noise leads to poorer cognition and focus.</p>
<p>It’s easy to be bothered by noise and not realize it until the noise stops and a wave of relief finally makes you aware of how annoyed you were by the sound. Noise issues are happily easy to control: earplugs or noise-cancelling headphones will generally do the trick. It would be utopic to eliminate bothersome noise from the environment altogether, but it’s not necessary. </p>
<h3 id="segregation-of-activities">Segregation of Activities</h3>
<p>A heroin addict who normally takes their dose in their car decides one day to inject in their bathroom. They die of an overdose, even though they took the same amount they normally do. Why? Our brains maintain associations with different environments. If you normally inject heroin when you get in your car, then your body starts to prepare you for the drug as soon as you get in the car. Drug tolerance, then, is partly environmental. (This <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1196296/">actually happened</a> and happens regularly.) Your mind and body are affected by your environment due to Pavlovian conditioning. When the bell rings, the dog salivates. When the lunch bell rings, so do you. </p>
<p>One common piece of advice given by doctors to insomniacs is to only use your bed for sleeping and for sex, and it’s good advice. If you use your bed for reading, studying, and watching TV, then your mind will not form a strong association between the bed and sleep, and you will have a harder time falling asleep. </p>
<p>Likewise, if you do all your slacking off at the same desk you do your work at, you will probably have a harder time focusing. Even having your smartphone within your field of view while you work has been shown to reduce focus. So it wouldn’t hurt to have different areas for work and play, and to not eat at your desk. (And even different user accounts on your computer for work and non-work, if you don’t find that idea to be a pain in the ass like I do.)</p>
<p>We also form associations not just with space, but with time. Hence another piece of common sleep hygeine advice: go to sleep at the same time every night. Your body will learn to expect sleep at that time. Likewise, people who eat at the same time every day eat with their bodies prepared to receive food, and so are less likely to become obese. Studies have shown this. Unfortunately, setting every aspect of your life to a clock can make you feel like a robot, so I usually don’t tolerate such rigidity in my life. But it’s worth thinking about.</p>
<h3 id="conclusion">Conclusion</h3>
<ul>
<li>Brighter lights for your poor eyes</li>
<li>Better ventilation for your poor lungs</li>
<li>Optimal temperature and humidity for your poor skin</li>
<li>Less distracting background noise for your poor ears</li>
<li>Activity-specific areas for your poor brain</li>
</ul>
<p>I also think the aesthetics of most of our indoor environments could use an upgrade, but I don’t have much to say on the subject besides simply saying so. (Though I would bet: green lush &gt; grey drab.)</p>
<p>We sometimes act like we are just <span>machines<span>caffeine in ⟶ code out</span></span>, but we are not. We’re mushy creatures with delicate bodies and delicate minds, too. And we evolved for one specific environment. There is no guarantee that the indoor environment which is cheapest to produce is going to be just as good for us as a bespoke imitation of our evolutionary environment, and in fact it is not. I think life would be more pleasant if people took these factors more serously when designing indoor environments, and our work would be more efficient and less prone to mistakes.</p>


    </div>
  </section></div>]]>
            </description>
            <link>https://taylor.gl/blog/9/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24927008</guid>
            <pubDate>Thu, 29 Oct 2020 03:52:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA['Digital natives' first generation with a lower IQ than their parents]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24926594">thread link</a>) | @respinal
<br/>
October 28, 2020 | https://moneytrainingclub.com/2020/10/28/alarming-trend-between-digital-natives-and-their-iq-digital-trends-spanish/ | <a href="https://web.archive.org/web/*/https://moneytrainingclub.com/2020/10/28/alarming-trend-between-digital-natives-and-their-iq-digital-trends-spanish/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://moneytrainingclub.com/2020/10/28/alarming-trend-between-digital-natives-and-their-iq-digital-trends-spanish/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24926594</guid>
            <pubDate>Thu, 29 Oct 2020 02:51:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Falsehoods programmers believe about addresses (2013)]]>
            </title>
            <description>
<![CDATA[
Score 57 | Comments 62 (<a href="https://news.ycombinator.com/item?id=24926417">thread link</a>) | @gk1
<br/>
October 28, 2020 | https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/ | <a href="https://web.archive.org/web/*/https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>Perhaps you've read posts like <a href="http://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/">Falsehoods Programmers Believe About Names</a>
and <a href="http://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time">Falsehoods programmers believe about time</a>.
Maybe you've also read <a href="http://wiesmann.codiferes.net/wordpress/?p=15187&amp;lang=en">Falsehoods programmers believe about geography</a>.</p>

<p>Addressing is a fertile ground for incorrect assumptions, because everyone's used to dealing with addresses and 99% of the time they seem so simple.
Below are some incorrect assumptions I've seen made, or made myself, or had reported to me.
(If you want to look up an address for a UK postcode or vice-versa to confirm what I'm telling you, try the <a href="http://www.royalmail.com/postcode-finder/">Royal Mail Postcode Finder</a>)</p>

<!-- Composition of building numbers -->

<ul>
<li><p><strong>An address will start with, or at least include, a building number.</strong></p>

<p>Counterexample: Royal Opera House, Covent Garden, London, WC2E 9DD, United Kingdom.</p></li>
<li><p><strong>When there is a building number, it will be all-numeric.</strong></p>

<p>Counterexample: 1A Egmont Road, Middlesbrough, TS4 2HT</p>

<p>4-5 Bonhill Street, London, EC2A 4BX</p></li>
<li><p><strong>No buildings are numbered zero</strong></p>

<p>Counterexample: 0 Egmont Road, Middlesbrough, TS4 2HT</p></li>
<li><p><strong>Well, at the very least no buildings have negative numbers</strong></p>

<p>Guy Chisholm provided this counterexample: Minusone Priory Road, Newbury, RG14 7QS</p>

<p>(none of the databases I've checked render this as -1)</p></li>
<li><p><strong>We can put those funny numbers into the building name field, as no buildings have both a name and a funny number</strong></p>

<p>Counterexample: Idas Court, 4-6 Princes Road, Hull, HU5 2RD</p></li>
<li><p><strong>When there's a building name, there won't be a building number (or vice-versa)</strong></p>

<p>Counterexample: Flat 1.4, Ziggurat Building, 60-66 Saffron Hill, London, EC1N 8QX, United Kingdom</p></li>
<li><p><strong>A building number will only be used once per street</strong></p>

<p>The difference between 50 Ammanford Road, Tycroes, Ammanford, SA18 3QJ and 50 Ammanford Road, Llandybie, Ammanford, SA18 3YF is about 4 miles (<a href="https://maps.google.co.uk/maps?q=SA18+3QJ+to+SA18+3YF">Google Maps</a>).</p></li>
<li><p><strong>When there's line with a number in an address, it's the building number.</strong></p>

<p>Counterexample: Flat 18, Da Vinci House, 44 Saffron Hill, London, EC1N 8FH, United Kingdom</p>

<p>You also get suite numbers, floor numbers, unit numbers, and organisations with numbers in their names.</p>

<p>Adrien Piérard contributes an address from Japan with fifteen digits in six separate numbers (five if you count the zip code as a single number). The format is: 980-0804 (zip code), Miyagi-ken (prefecture) Sendai-shi (city) Aoba-ku (ward) Kokubuncho (district) 4-10-20 (sub-district-number block-number lot-number) Sendai (building name) 401 (flat number).</p></li>
<li><p><strong>OK, the first line starting with a number then</strong></p>

<p>Counterexample: 3 Store, 311-318 High Holborn, London, WC1V 7BN</p></li>
<li><p><strong>A building will only have one number</strong></p>

<p>Benton Lam offers this address from the Hong Kong Special Administrative Region - it has both a number on its road (14) and in its group of buildings (3): 15/F, Cityplaza 3, 14 TaiKoo Wan Road, Island East, HKSAR</p></li>
<li><p><strong>The number of buildings is the difference between the highest and lowest building numbers</strong></p>

<p>Tibor Schütz points out building numbers may be skipped - for example, on a street where even-numbered buildings are on one side, odd numbers on the other; multiple buildings sharing the same number (such as where a new house has been built) and buildings with more than one number.</p>

<p>Cyrille Chépélov and Sami Lehtinen tell me in Antibes, France and rural Finland some buildings are numbered based on the distance from the start of the road - such as Longroad 65 for the building 750m from the start of longroad.</p></li>
<li><p><strong>If the addresses on the left of the road are even, the addresses on the right must be odd</strong></p>

<p>Cyrille Chépélov points out that in places, <a href="https://maps.google.fr/maps?q=48.857415,2.467167">Boulevard Théophile Sueur, Montreuil, Seine-Saint-Denis, France</a> has evens-only on both sides. The two sides are also in different cities and Départements.</p></li>
<li><p><strong>A building name won't also be a number</strong></p>

<p>Ben Tilly reports on Ten Post Office Sq, Boston MA 02109 USA - which is not, reportedly, the same as 10 Post Office Sq, Boston MA 02109 USA.</p></li>
<li><p><strong>Well, at least you can omit leading zeros</strong></p>

<p>Shaun Crampton reports living at 101 Alma St, Apartment 001, Palo Alto - where apartments 1 and 001 were on different floors.</p></li>
<li><p><strong>A street with a building A will not also have a building Alpha</strong></p>

<p>Douglas Perreault reports he lived in a block within a condo association; it was a large association, with blocks A through Z then Alpha, Beta, Gamma, Delta, and Theta. Mail and deliveries were often misrouted from block Alpha to block A and vice-versa. His address at the time was: 14100 N 46th St., Alpha 39, Tampa, FL 33613</p></li>
</ul>

<!-- Composition of street names -->

<ul>
<li><p><strong>A street name won't include a number</strong></p>

<p>8 Seven Gardens Burgh, WOODBRIDGE, IP13 6SU (pointed out by Raphael Mankin)</p></li>
<li><p><strong>OK, but numbers in street names are expressed as words, not digits</strong></p>

<p>Jan Jongboom reports streets can be numbered in the Netherlands - for example, Plein 1944 in Nijmegen.</p></li>
<li><p><strong>When there's a numbered street and a house number, there will be a separator between them</strong></p>

<p>Another from Jan Jongboom: Gondel 2695, Lelystad, means area Gondel, street 26, number 95</p></li>
<li><p><strong>Street names always end in descriptors like 'street', 'avenue', 'drive', 'square', 'hill' or 'view'</strong></p>

<p>They don't always - for example: Piccadilly, London, W1J 9PN</p></li>
<li><p><strong>OK, but when they do have a descriptor there will only be one</strong></p>

<p>A street name can be entirely descriptors: 17 Hill Street, London, W1J 5LJ or <a href="https://en.wikipedia.org/wiki/Avenue_Road">Avenue Road, Toronto, Ontario</a>.</p></li>
<li><p><strong>OK, but when they do have a descriptor it will be at the end</strong></p>

<p>French addresses use prefix descriptors like 'rue', 'avenue', 'place' and 'allee'.</p></li>
<li><p><strong>OK, but if there's a descriptor it'll be at the start or end of the street name.</strong></p>

<p>Or the middle, like 3 Bishops Square Business Park, Hatfield, AL10 9NA</p></li>
<li><p><strong>OK, but at the very least you wouldn't name a town Street</strong></p>

<p><a href="https://maps.google.co.uk/maps?q=Street,+Somerset">Actually there's a town called Street in Somerset, UK</a>.</p></li>
<li><p><strong>Street numbers (and building numbers) don't contain fractions</strong></p>

<p>Dan, Fred Kroon, David Underwood and Daniel Dickison submitted examples of fractional street numbers like <a href="https://maps.google.com/maps?q=43rd%20%C2%BD%20st,%20Pittsburgh,%20PA">43rd ½ St, Pittsburgh, PA</a>, and of fractional building numbers. These can be written in unicode (43rd ½ St), as a fraction with a slash (43 1/2) or as a decimal (43.5)</p>

<p>Gene Wirchenko reports a fractional building number: 1313 1/2 Railroad Ave Bellingham WA 98225-4729</p></li>
<li><p><strong>Street names don't recurr in the same city</strong></p>

<p><a href="https://maps.google.co.uk/maps?q=from:W3+6LJ+to:W5+5DB+to:N8+7PB+to:SE25+6EP+to:E13+0AJ+to:E17+7LD+to:NW10+4LX+to:N1+9TR+to:E1+6PG+to:NW1+0JH+to:W14+8NL+to:SE13+6AD+to:SW19+5DX+to:E11+2AJ+to:SW19+2AE+to:E6+2HJ+&amp;saddr=W3+6LJ&amp;daddr=W5+5DB+to:N8+7PB+to:SE25+6EP+to:E13+0AJ+to:E17+7LD+to:NW10+4LX+to:N1+9TR+to:E1+6PG+to:NW1+0JH+to:W14+8NL+to:SE13+6AD+to:SW19+5DX+to:E11+2AJ+to:SW19+2AE+to:E6+2HJ">Here's a map of the following addresses:</a></p>

<ul>
<li>High Street, London, W3 6LJ</li>
<li>High Street, London, W5 5DB</li>
<li>High Street, London, N8 7PB</li>
<li>High Street, London, SE25 6EP</li>
<li>High Street, London, E13 0AJ</li>
<li>High Street, London, E17 7LD</li>
<li>High Street, London, NW10 4LX</li>
<li>Islington High Street, London, N1 9TR</li>
<li>Shoreditch High Street, London, E1 6PG</li>
<li>Camden High Street, London, NW1 0JH</li>
<li>Kensington High Street, London, W14 8NL</li>
<li>Lewisham High Street, London, SE13 6AD</li>
<li>High Street Wimbledon, London, SW19 5DX</li>
<li>High Street Wanstead, London, E11 2AJ</li>
<li>High Street Colliers Wood, London, SW19 2AE</li>
<li>High Street North, London, E6 2HJ </li>
</ul></li>
<li><p><strong>But street names don't recurr in close proximity</strong></p>

<p>Julian Fleischer provides an example from Bocholt in Germany showing several roads in close proximity all called <a href="https://maps.google.com/maps?q=51.853945,6.615334">Up de Welle</a>.</p></li>
<li><p><strong>An address will be comprised of road names</strong></p>

<p>Kirk Kerekes spent several years using an address of the form "2 mi N then 3 mi W of Jennings, OK 74038" which regularly got successful deliveries. Mike Riley used to mail the Very Large Array radio telescope at "50 miles (80 km) West of Socorro, New Mexico, USA"</p>

<p>Sam pointed me to <a href="http://www.menomoneefallsnow.com/news/99857214.html">Menomonee Falls</a> where houses are addressed using Milwaukee County's grid system instead of house numbers - giving addresses like N88 W16541 Foobar St.</p>

<p>Andy Monat sent the following address example, from a <a href="http://ciapa.tulane.edu/uploads/1_EE_2012_Acceptance_Packet_INFORMATION-1340749206.pdf">semester abroad program at Tulane University </a>: CIAPA, 50 meters north of the Hypermas/Walmart of Curridabat, San Jose, Costa Rica. Adrien Piérard and Luke Allardyce point out street names are seldom used in Japan - instead, districts and blocks and lot numbers are used (more info on the <a href="https://en.wikipedia.org/wiki/Japanese_addressing_system">Wikipedia entry for the Japanese addressing system</a>).  A <a href="http://www.worldpress.org/Americas/592.cfm">2002 World Press Review report</a> gave this sample address: From where the Chinese restaurant used to be, two blocks down, half a block toward the lake, next door to the house where the yellow car is parked, Managua, Nicaragua. Shaun Crampton sent <a href="https://vianica.com/nicaragua/practical-info/14-addresses.html">an article with more details and examples of the Nicaraguan system</a>. Stig Brautaset pointed out <a href="http://www.bbc.co.uk/news/magazine-14806350">a BBC article about post in Kabul</a> gives this example: "Hamid Jaan, behind Darul-Aman palace". Nathan Fellman reports similar addressing is used in Nicaragua and Costa Rica.</p>

<p>Paul Puschmann and Tibor Schütz pointed out the city of <a href="http://de.wikipedia.org/wiki/Quadratestadt">Mannheim in Germany is sometimes called Quadratestadt (City of Squares)</a> as the city centre is arranged in a grid, with blocks assigned a letter (along the north-south axis) and a number (along the east-west axis) then buildings numbered by block number. So an example address at numbers 6 to 13 on block R 5 would be: Institut für Deutsche Sprache, R 5, 6-13, D-68161 Mannheim </p>

<p>Leoni Lubbinge gives an example of a South African address: Part 84, Strydfontein 306 JR, Pretoria which means the 84th plot of the farm Strydfontein 306 JR.</p></li>
</ul>

<!-- Elements being present or absent -->

<ul>
<li><p><strong>A road will have a name</strong></p>

<p>Plenty of roads like driveways, onramps and the aisles of carparks don't have names. Some roads in Japan also don't have names, as <a href="https://en.wikipedia.org/wiki/Japanese_addressing_system">the prevalent addressing system works on districts, subdistricts, blocks, lots and lot numbers</a>.</p>

<p>Peter Kenway points out in America some homes are addressed as Rural Routes, where numbers are allocated to boxes on a route covering multiple roads. For example: Box 1234, R.R. 1, Winthrop, ME 04364.</p></li>
<li><p><strong>A road will only have one name</strong></p>

<p>Many different roads, from Goswell Road in London to Regent Road in Edinburgh, make up the 410 mile <a href="https://en.wikipedia.org/wiki/A1_road_%28Great_Britain%29">A1</a>. And while there may only be one "1 Goswell Road" and only one "1 Regent Road" there are multiple buildings numbered 1 on the road designated A1.</p>

<p>Roads may also be named in multiple languages. For example, in Ireland roads may be named in both English and Irish</p></li>
<li><p><strong>Addresses will only have one street</strong></p>

<p>The Royal Mail have what they call a 'dependent street' - for example: 6 Elm Avenue, Runcorn Road, Birmingham, B12 8QX, United Kingdom (Runcorn Road is the street, Elm Avenue is the stubby 'dependent street' and isn't unique within the city. <a href="http://maps.google.co.uk/maps?q=B12+8QX">Google Maps</a> )</p>

<p>Another counterexample: Rogue Hair, 1 Hopton Parade, Streatham High Road, London, SW16 …</p></li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/">https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/</a></em></p>]]>
            </description>
            <link>https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24926417</guid>
            <pubDate>Thu, 29 Oct 2020 02:25:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I Violated a Code of Conduct]]>
            </title>
            <description>
<![CDATA[
Score 1157 | Comments 879 (<a href="https://news.ycombinator.com/item?id=24926214">thread link</a>) | @tosh
<br/>
October 28, 2020 | https://www.fast.ai/2020/10/28/code-of-conduct/ | <a href="https://web.archive.org/web/*/https://www.fast.ai/2020/10/28/code-of-conduct/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>

<p><span>Written: 28 Oct 2020 by <i>Jeremy Howard</i></span></p><blockquote>
<p><em>Update Oct 20, 2020</em>: NumFOCUS <a href="https://numfocus.org/blog/jeremy-howard-apology">has apologized</a> to me. I accept their apology. I do not accept their assertion that “At the time of the interview, the committee had not determined that there was a violation of the code of conduct, only that there were two complaints filed and being examined.” The email to set up the call said “We would like to schedule a meeting so that we can discuss the results of our investigation with you” - nothing further. During the call, the committee stated the list of violations, and said “that is what the reporters stated, and what we found”. I asked why they didn’t take a statement from me before that finding, and they said “we all watched the video, so we could see for ourselves the violation”. The committee offered in their apology email to me to have a follow-up discussion, and I declined the offer.</p>
</blockquote>
<blockquote>
<p>Summary: NumFOCUS found I violated their Code of Conduct (CoC) at JupyterCon because my talk was not “kind”, because I said Joel Grus was “wrong” regarding his opinion that Jupyter Notebook is not a good software development environment. Joel (who I greatly respect, and consider an asset to the data science community) was not involved in NumFOCUS’s action, was not told about it, and did not support it. NumFOCUS did not follow their own enforcement procedure and violated their own CoC, left me hanging for over a week not even knowing what I was accused of, and did not give me an opportunity to provide input before concluding their investigation. I repeatedly told their committee that my emotional resilience was low at the moment due to medical issues, which they laughed about and ignored, as I tried (unsuccessfully) to hold back tears. The process has left me shattered, and I won’t be able to accept any speaking requests for the foreseeable future. I support the thoughtful enforcement of Code of Conducts to address sexist, racist, and harassing behavior, but that is not what happened in this case.</p>
</blockquote>
<h2 id="overview">Overview</h2>
<p>In my recent JupyterCon keynote, “I Like Jupyter Notebooks” (re-recording provided at the bottom of this post, if you’re interested in seeing it for yourself), I sought to offer a rebuttal to Joel Grus’ highly influential JupyterCon presentation “<a href="https://www.youtube.com/watch?v=7jiPeIFXb6U">I Don’t Like Notebooks</a>”. Joel claimed in his talk that Jupyter is a poor choice for software development and teaching, and I claimed in my talk that it is a good choice. The NumFOCUS committee found me guilty of violating their code of conduct for having not been “kind” in my disagreement with Joel, and for “insulting” him. The specific reasons given were that:</p>
<ul>
<li>I said that Joel Grus was “wrong”</li>
<li>I used some of his slides (properly attributed) and a brief clip from one of his videos to explain why I thought he was wrong</li>
<li>That I made “a negative reference” to his prior talk</li>
<li>I was also told that “as a keynote speaker” I would “be held to a higher standard than others” (although this was not communicated to me prior to my talk, nor what that higher standard is)</li>
</ul>
<p>Code of Conducts can be a useful tool, when thoughtfully created and thoughtfully enforced, to address sexism, racism, and harassment, all of which have been problems at tech conferences. Given the <a href="https://medium.com/tech-diversity-files/if-you-think-women-in-tech-is-just-a-pipeline-problem-you-haven-t-been-paying-attention-cb7a2073b996">diversity issues in the tech industry</a>, it is important that we continue the work of making conferences more inclusive, particularly to those from marginalized backgrounds. Having a code of conduct with explicit rules against violent threats, unwelcome sexual attention, repeated harassment, sexually explicit pictures, and other harmful behavior is the first step towards addressing and stopping those behaviors. The JupyterCon code provides the following examples of unacceptable behavior, none of which are at all similar to what I did (i.e. saying that someone was wrong on a technical topic, and explaining how and why):</p>
<ul>
<li>Violent threats or violent language directed against another person</li>
<li>Discriminatory jokes and language</li>
<li>Posting sexually explicit or violent material</li>
<li>Posting (or threatening to post) other people’s personally identifying information (“doxing”)</li>
<li>Personal insults, especially those using racist or sexist terms</li>
<li>Unwelcome sexual attention</li>
<li>Advocating for, or encouraging, any of the above behavior</li>
<li>Repeated harassment of others. In general, if someone asks you to stop, then stop</li>
</ul>
<p>My experience with the NumFOCUS code of conduct raises a few key issues:</p>
<ul>
<li>The CoC enforcement process involved conflicting &amp; changing information, no opportunity for me to give input, the stress of a long wait of unknown duration with no information about what I was accused of or what would happen next, and the committee members violated their own CoC during the process</li>
<li>There were two totally different Codes of Conduct with different requirements linked in different places</li>
<li>I was held to a different, undocumented and uncommunicated standard</li>
<li>The existence of, or details about, the CoC were not communicated prior to confirmation of the engagement</li>
<li>CoC experts recommend avoiding requirements of politeness or other forms of “proper” behavior, but should focus on a specific list of unacceptable behaviors. The JupyterCon CoC, however, is nearly entirely a list of “proper” behaviors (such as “Be welcoming”, “Be considerate”, and “Be friendly”) that are vaguely defined</li>
<li>CoC experts recommend using a CoC that focuses on a list of unacceptable behaviors. Both the codes linked to JupyterCon have such a link, and none of the unacceptable behavior examples are in any way related or close to what happened in this case. But NumFOCUS nonetheless found me in violation.</li>
</ul>
<p>I would rather not have to write this post at all. However I know that people will ask about why my talk isn’t available on the JupyterCon site, so I felt that I should explain exactly what happened. In particular, I was concerned that if only partial information became available, the anti-CoC crowd might jump on this as an example of problems with codes of conduct more generally, or might point at this as part of “cancel culture” (a concept I vehemently disagree with, since what is referred to as “cancellation” is often just “facing consequences”). Finally, I found that being on the “other side” of a code of conduct issue gave me additional insights into the process, and that it’s important that I should share those insights to help the community in the future.</p>
<h2 id="details">Details</h2>
<p>The rest of this post is a fairly detailed account of what happened, for those that are interested.</p>
<h3 id="my-talk-at-jupytercon">My talk at JupyterCon</h3>
<p>I recently gave a talk at <a href="https://jupytercon.com/">JupyterCon</a>. My partner Rachel gave a <a href="https://www.youtube.com/watch?v=frc7FgheUj4">talk at JupyterCon</a> a couple of years ago, and had a wonderful experience, and I’m a huge fan of Jupyter, so I wanted to support the project. The conference used to be organized by O’Reilly, who have always done a wonderful job of conferences I’ve attended, but this year the conference was instead handled by <a href="https://numfocus.org/">NumFOCUS</a>.</p>
<p>For my talk, I decided to focus on Jupyter as a literate and <a href="https://www.fast.ai/2019/12/02/nbdev/">exploratory programming environment</a>, using <a href="https://nbdev.fast.ai/">nbdev</a>. One challenge, however, is that two years earlier Joel Grus had given a brilliant presentation called <a href="https://www.youtube.com/watch?v=7jiPeIFXb6U">I Don’t Like Notebooks</a> which had been so compelling that I have found it nearly impossible to talk about programming in Jupyter without being told “you should watch this talk which explains why programming in Jupyter is a terrible idea”.</p>
<p>Joel opened and closed his presentation with some light-hearted digs at me, since I’d asked him ahead of time <em>not</em> to do such a presentation. So I thought I’d kill two birds with one stone, and take the opportunity to respond directly to him. Not only was his presentation brilliant, but his slides were hilarious, so I decided to directly parody his talk by using (with full credit of course) some of his slides directly. That way people that hadn’t seen his talk could both get to enjoy the fantastic content, and also understand just what I was responding to. For instance, here’s how Joel illustrated the challenge of running cells in the right order:</p>
<figure>
<img srcset="https://www.fast.ai/images/numfocus/joel-order.png 2w" sizes="1px" src="https://www.fast.ai/images/numfocus/joel-order.png">
</figure>
<p>I showed that slide, explaining that it’s Joel’s take on the issue, and then followed up with a slide showing how easy it actually is to run all cells in order:</p>
<figure>
<img srcset="https://www.fast.ai/images/numfocus/jeremy-order.png 2w" sizes="1px" src="https://www.fast.ai/images/numfocus/jeremy-order.png">
</figure>
<p>Every slide included a snippet from Joel’s title slide, which, I explained, showed which slides were directly taken from his presentation. I was careful to ensure I did not modify any of his slides in any way. When first introducing his presentation, I described Joel as “a brilliant communicator, really funny, and wrong”. I didn’t make any other comments about Joel (although, for the record, I think he’s awesome, and highly recommend <a href="https://www.amazon.com/Data-Science-Scratch-Principles-Python/dp/1492041130">his book</a>.</p>
<h3 id="the-code-of-conduct-violation-notice">The Code of Conduct violation notice</h3>
<p>A week later, I received an email telling me that two CoC reports were filed regarding my JupyterCon keynote presentation. I was told that “The Code of Conduct Enforcement Team is meeting tomorrow to review the incident and will be contacting you to inform you of the nature of the report and to understand your perspective”.</p>
<p>The CoC wasn’t mentioned at all until after I’d been invited to speak, had accepted, and had completed the online registration. I had reviewed it at that time, and had been a bit confused. The email I received linked to a <a href="https://jupytercon.com/codeofconduct/">JupyterCon Code of Conduct</a>, but that in turn didn’t provide much detail about what is and isn’t OK, and that in turn linked to a different <a href="https://numfocus.org/code-of-conduct">NumFOCUS Code of Conduct</a>. A link was also provided to <a href="https://numfocus.typeform.com/to/ynjGdT">report violations</a>, which also linked to and named the NumFOCUS CoC.</p>
<p>I was concerned that I had done something which might be viewed as a violation, and looked forward to hearing about the nature of the report and having a chance to share my perspective. I was heartened that JupyterCon documented that they follow the <a href="https://numfocus.org/code-of-conduct/response-and-enforcement-events-meetups">NumFOCUS Enforcement Manual</a>. I was also heartened that the manual has a section “Communicate with the Reported Person about the Incident” which says they will “Let the reported person tell someone on the CoC response team their side of the story; the person who receives their side of the story should be prepared to …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.fast.ai/2020/10/28/code-of-conduct/">https://www.fast.ai/2020/10/28/code-of-conduct/</a></em></p>]]>
            </description>
            <link>https://www.fast.ai/2020/10/28/code-of-conduct/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24926214</guid>
            <pubDate>Thu, 29 Oct 2020 01:47:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Generating Realistic Test Traffic Using Markov Chains]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24925655">thread link</a>) | @tinrab
<br/>
October 28, 2020 | https://outcrawl.com/markov-chains-test-traffic/ | <a href="https://web.archive.org/web/*/https://outcrawl.com/markov-chains-test-traffic/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>This article shows how you can generate realistic user traffic for testing purposes by sampling requests in production and generating fake ones using <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov chains</a>.</p>
<p>Example code is available on <a href="https://github.com/tinrab/rusty-markov-traffic">GitHub</a>.</p>
<h2 id="Markov-chains">Markov chains<a href="#Markov-chains" aria-label="Markov chains permalink"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 4 24 16"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2>
<p>A Markov chain is a stochastic model telling us probability of an event based on previously observed events.
Probability of next event <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">X_{n+1}</annotation></semantics></math></span></span> is determined by last known event <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>X</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(X_{n+1}|X_{n})</annotation></semantics></math></span></span> or previous <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span></span> events, which gives us a Markov chain of order <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span></span>:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>n</mi></msub><mo>=</mo><msub><mi>x</mi><mi>n</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>X</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>X</mi><mrow><mi>n</mi><mo>−</mo><mi>m</mi></mrow></msub><mo>=</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>−</mo><mi>m</mi></mrow></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>n</mi><mo>&gt;</mo><mi>m</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(X_{n}=x_{n}|X_{n-1}=x_{n-1},...,X_{n-m}=x_{n-m}), n\gt m.</annotation></semantics></math></span></span></span></p><p>We can track actual user behaviour and build a model by counting events and calculating weighted probabilities for each event based on events preceding them.</p>
<p>For example, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn></mrow><annotation encoding="application/x-tex">10</annotation></semantics></math></span></span> users finished writing a blog post.
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn></mrow><annotation encoding="application/x-tex">9</annotation></semantics></math></span></span> users later published it and one of them trashed it.
This gives us probabilities <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>P</mi><mi>o</mi><mi>s</mi><mi>t</mi><mi>P</mi><mi>u</mi><mi>b</mi><mi>l</mi><mi>i</mi><mi>s</mi><mi>h</mi><mi>e</mi><mi>d</mi><mi mathvariant="normal">∣</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>t</mi><mi>F</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>s</mi><mi>h</mi><mi>e</mi><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">P(PostPublished|PostFinished)=0.9</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>P</mi><mi>o</mi><mi>s</mi><mi>t</mi><mi>T</mi><mi>r</mi><mi>a</mi><mi>s</mi><mi>h</mi><mi>e</mi><mi>d</mi><mi mathvariant="normal">∣</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>t</mi><mi>F</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>s</mi><mi>h</mi><mi>e</mi><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">P(PostTrashed|PostFinished)=0.1</annotation></semantics></math></span></span>.
We can then selected a random event with <a href="https://en.wikipedia.org/wiki/Fitness_proportionate_selection">roulette wheel selection</a>.</p>
<p>This is useful for stress or smoke testing when you need some fake traffic that resembles real users.</p>
<h2 id="Implementation">Implementation<a href="#Implementation" aria-label="Implementation permalink"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 4 24 16"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2>
<p>Let's declare a struct for a Markov chain.
Map <code>occurrences</code> will hold event counts <code>BTreeMap&lt;T, usize&gt;</code> for any previous series of events <code>Vec&lt;T&gt;</code>.</p>
<div data-language="rust"><pre><code><span>#[derive(Clone)]</span>
<span>pub</span> <span>struct</span> MarkovChain<span>&lt;</span>T<span>&gt;</span>
<span>where</span>
    T<span>:</span> Clone <span>+</span> Ord<span>,</span>
<span>{</span>
    order<span>:</span> <span>usize</span><span>,</span>
    occurrences<span>:</span> BTreeMap<span>&lt;</span>Vec<span>&lt;</span>T<span>&gt;</span><span>,</span> BTreeMap<span>&lt;</span>T<span>,</span> <span>usize</span><span>&gt;&gt;</span><span>,</span>
    memory<span>:</span> Vec<span>&lt;</span>T<span>&gt;</span><span>,</span>
    rng<span>:</span> ThreadRng<span>,</span>
<span>}</span></code></pre></div>
<p>To build a model we have to go through all observed events and count the number of times <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span>-th event occured after all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>&gt;</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">n&gt;m</annotation></semantics></math></span></span> events.
We also track last known <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span></span> events inside the <code>memory</code> vector, which will be used to generate the next event.</p>
<div data-language="rust"><pre><code><span>impl</span><span>&lt;</span>T<span>&gt;</span> MarkovChain<span>&lt;</span>T<span>&gt;</span>
<span>where</span>
    T<span>:</span> Clone <span>+</span> Ord<span>,</span>
<span>{</span>
    <span>pub</span> <span>fn</span> <span>update</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> events<span>:</span> <span>&amp;</span><span>[</span>T<span>]</span><span>)</span> <span>{</span>
        <span>let</span> events<span>:</span> Vec<span>&lt;</span>_<span>&gt;</span> <span>=</span> events<span>.</span><span>to_vec</span><span>(</span><span>)</span><span>;</span>
        <span>for</span> history <span>in</span> events<span>.</span><span>windows</span><span>(</span><span>self</span><span>.</span>order <span>+</span> <span>1</span><span>)</span> <span>{</span>
            
            <span>let</span> previous <span>=</span> history<span>[</span><span>0</span><span>..</span><span>self</span><span>.</span>order<span>]</span><span>.</span><span>to_vec</span><span>(</span><span>)</span><span>;</span>
            <span>let</span> current <span>=</span> history<span>.</span><span>last</span><span>(</span><span>)</span><span>.</span><span>cloned</span><span>(</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
            
            <span>self</span><span>.</span>occurrences
                <span>.</span><span>entry</span><span>(</span>previous<span>)</span>
                <span>.</span><span>or_default</span><span>(</span><span>)</span>
                <span>.</span><span>entry</span><span>(</span>current<span>)</span>
                <span>.</span><span>and_modify</span><span>(</span><span><span>|</span>count<span>|</span></span> <span>*</span>count <span>+=</span> <span>1</span><span>)</span>
                <span>.</span><span>or_insert</span><span>(</span><span>1</span><span>)</span><span>;</span>
        <span>}</span>
        
        <span>self</span><span>.</span>memory<span>.</span><span>reserve</span><span>(</span><span>self</span><span>.</span>order<span>)</span><span>;</span>
        <span>for</span> event <span>in</span> events<span>.</span><span>into_iter</span><span>(</span><span>)</span><span>.</span><span>rev</span><span>(</span><span>)</span><span>.</span><span>take</span><span>(</span><span>self</span><span>.</span>order<span>)</span> <span>{</span>
            <span>self</span><span>.</span>memory<span>.</span><span>insert</span><span>(</span><span>0</span><span>,</span> event<span>)</span><span>;</span>
        <span>}</span>
        <span>self</span><span>.</span>memory<span>.</span><span>truncate</span><span>(</span><span>self</span><span>.</span>order<span>)</span><span>;</span>
    <span>}</span>
    </code></pre></div>
<p>The <code>generate_from</code> function takes in the memory, finds occurrences and chooses an event from those.
We use <a href="https://docs.rs/rand/0.7.3/rand/seq/trait.SliceRandom.html#tymethod.choose_weighted">choose_weighted</a> function provided by the <a href="https://crates.io/crates/rand">rand</a> crate.</p>
<div data-language="rust"><pre><code><span>pub</span> <span>fn</span> <span>generate_from</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> memory<span>:</span> <span>&amp;</span><span>[</span>T<span>]</span><span>)</span> <span>-&gt;</span> Option<span>&lt;</span>T<span>&gt;</span> <span>{</span>
    <span>assert_eq!</span><span>(</span>memory<span>.</span><span>len</span><span>(</span><span>)</span><span>,</span> <span>self</span><span>.</span>order<span>,</span> <span>"invalid memory size"</span><span>)</span><span>;</span>
    <span>if</span> <span>let</span> Some<span>(</span>occurrences<span>)</span> <span>=</span> <span>self</span><span>.</span>occurrences<span>.</span><span>get</span><span>(</span>memory<span>)</span> <span>{</span>
        
        
        <span>let</span> occurrence_counts<span>:</span> Vec<span>&lt;</span>_<span>&gt;</span> <span>=</span> occurrences
            <span>.</span><span>iter</span><span>(</span><span>)</span>
            <span>.</span><span>map</span><span>(</span><span><span>|</span>(event, count)<span>|</span></span> <span>(</span>event<span>.</span><span>clone</span><span>(</span><span>)</span><span>,</span> <span>*</span>count<span>)</span><span>)</span>
            <span>.</span><span>collect</span><span>(</span><span>)</span><span>;</span>
        
        occurrence_counts
            <span>.</span><span>choose_weighted</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>.</span>rng<span>,</span> <span><span>|</span>(_, count)<span>|</span></span> <span>*</span>count<span>)</span>
            <span>.</span><span>map</span><span>(</span><span><span>|</span>(event, _)<span>|</span></span> event<span>)</span>
            <span>.</span><span>ok</span><span>(</span><span>)</span>
            <span>.</span><span>cloned</span><span>(</span><span>)</span>
    <span>}</span> <span>else</span> <span>{</span>
        
        None
    <span>}</span>
<span>}</span></code></pre></div>
<p>After generating a new event, we update the internal memory.
Next events will then be calculated from the most recent memory.</p>
<div data-language="rust"><pre><code><span>pub</span> <span>fn</span> <span>generate</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> update_memory<span>:</span> <span>bool</span><span>)</span> <span>-&gt;</span> Option<span>&lt;</span>T<span>&gt;</span> <span>{</span>
    <span>let</span> last_memory <span>=</span> <span>self</span><span>.</span>memory<span>.</span><span>clone</span><span>(</span><span>)</span><span>;</span>
    <span>if</span> <span>let</span> Some<span>(</span>next<span>)</span> <span>=</span> <span>self</span><span>.</span><span>generate_from</span><span>(</span><span>&amp;</span>last_memory<span>)</span> <span>{</span>
        <span>if</span> update_memory <span>{</span>
            
            <span>self</span><span>.</span>memory<span>.</span><span>insert</span><span>(</span><span>0</span><span>,</span> next<span>.</span><span>clone</span><span>(</span><span>)</span><span>)</span><span>;</span>
            <span>self</span><span>.</span>memory<span>.</span><span>truncate</span><span>(</span><span>self</span><span>.</span>order<span>)</span><span>;</span>
        <span>}</span>
        Some<span>(</span>next<span>)</span>
    <span>}</span> <span>else</span> <span>{</span>
        None
    <span>}</span>
<span>}</span></code></pre></div>
<p>We can also write an iterator that returns generated events.</p>
<div data-language="rust"><pre><code><span>pub</span> <span>struct</span> MarkovChainIter<span>&lt;</span><span>'a</span><span>,</span> T<span>&gt;</span>
<span>where</span>
    T<span>:</span> Clone <span>+</span> Ord<span>,</span>
<span>{</span>
    chain<span>:</span> <span>&amp;</span><span>'a</span> <span>mut</span> MarkovChain<span>&lt;</span>T<span>&gt;</span><span>,</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>'a</span><span>,</span> T<span>&gt;</span> Iterator <span>for</span> MarkovChainIter<span>&lt;</span><span>'a</span><span>,</span> T<span>&gt;</span>
<span>where</span>
    T<span>:</span> Clone <span>+</span> Ord<span>,</span>
<span>{</span>
    <span>type</span> Item <span>=</span> T<span>;</span>

    <span>fn</span> <span>next</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>)</span> <span>-&gt;</span> Option<span>&lt;</span><span>Self</span><span>::</span>Item<span>&gt;</span> <span>{</span>
        <span>self</span><span>.</span>chain<span>.</span><span>generate</span><span>(</span><span>true</span><span>)</span>
    <span>}</span>
<span>}</span>

<span>impl</span><span>&lt;</span>T<span>&gt;</span> MarkovChain<span>&lt;</span>T<span>&gt;</span>
<span>where</span>
    T<span>:</span> Clone <span>+</span> Ord<span>,</span>
<span>{</span>
    <span>pub</span> <span>fn</span> <span>iter</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>)</span> <span>-&gt;</span> MarkovChainIter<span>&lt;</span>T<span>&gt;</span> <span>{</span>
        MarkovChainIter <span>{</span> chain<span>:</span> <span>self</span> <span>}</span>
    <span>}</span>
    
<span>}</span></code></pre></div>
<h2 id="Example">Example<a href="#Example" aria-label="Example permalink"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 4 24 16"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2>
<p>To see it in action, we declare an enum of all possible actions.
Of course, these can be way more complicated in the real world use-case.</p>
<div data-language="rust"><pre><code><span>#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]</span>
<span>enum</span> UserAction <span>{</span>
    SignIn<span>,</span>
    SignOut<span>,</span>
    CreateTodo<span>,</span>
    DeleteTodo<span>,</span>
    ListTodos<span>,</span>
<span>}</span></code></pre></div>
<p>We build a chain from a sample of actions.</p>
<div data-language="rust"><pre><code><span>let</span> <span>mut</span> chain <span>=</span> MarkovChain<span>::</span><span>new</span><span>(</span><span>1</span><span>)</span><span>;</span>
<span>let</span> actions <span>=</span> <span>vec!</span><span>[</span>
    UserAction<span>::</span>SignIn<span>,</span>
    UserAction<span>::</span>ListTodos<span>,</span>
    UserAction<span>::</span>CreateTodo<span>,</span>
    UserAction<span>::</span>CreateTodo<span>,</span>
    UserAction<span>::</span>SignOut<span>,</span>
    UserAction<span>::</span>SignIn<span>,</span>
    UserAction<span>::</span>ListTodos<span>,</span>
    UserAction<span>::</span>DeleteTodo<span>,</span>
    UserAction<span>::</span>DeleteTodo<span>,</span>
    UserAction<span>::</span>CreateTodo<span>,</span>
    UserAction<span>::</span>CreateTodo<span>,</span>
    UserAction<span>::</span>SignOut<span>,</span>
    UserAction<span>::</span>SignIn<span>,</span>
    UserAction<span>::</span>ListTodos<span>,</span>
    UserAction<span>::</span>CreateTodo<span>,</span>
    UserAction<span>::</span>DeleteTodo<span>,</span>
    UserAction<span>::</span>CreateTodo<span>,</span>
    UserAction<span>::</span>DeleteTodo<span>,</span>
    UserAction<span>::</span>ListTodos<span>,</span>
    UserAction<span>::</span>DeleteTodo<span>,</span>
    UserAction<span>::</span>SignOut<span>,</span>
<span>]</span><span>;</span>
chain<span>.</span><span>update</span><span>(</span><span>&amp;</span>actions<span>)</span><span>;</span></code></pre></div>
<p>Then generate a few actions.</p>
<div data-language="rust"><pre><code><span>for</span> action <span>in</span> chain<span>.</span><span>iter</span><span>(</span><span>)</span><span>.</span><span>take</span><span>(</span><span>16</span><span>)</span> <span>{</span>
    <span>if</span> action <span>==</span> UserAction<span>::</span>SignIn <span>{</span>
        <span>println!</span><span>(</span><span>"## New session ##"</span><span>)</span><span>;</span>
    <span>}</span>
    <span>println!</span><span>(</span><span>"{:?}"</span><span>,</span> action<span>)</span><span>;</span>
<span>}</span></code></pre></div>
<p>Which gives us the following output.</p>
<div data-language="bash"><pre><code><span><span data-user="root" data-host="localhost"></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span>cargo run --example traffic

SignIn
ListTodos
DeleteTodo
SignOut

SignIn
ListTodos
DeleteTodo
DeleteTodo
CreateTodo
SignOut

SignIn
ListTodos
CreateTodo
CreateTodo
CreateTodo
DeleteTodo</code></pre></div>
<p>Notice how after each <code>SignIn</code> there's a <code>ListTodos</code>, which means <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>L</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>T</mi><mi>o</mi><mi>d</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">∣</mi><mi>S</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>I</mi><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">P(ListTodos|SignIn)=1</annotation></semantics></math></span></span>.
Built model can represent inherent rules of our application.
Some series of actions will not be generated, those having probability <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span></span>, which is a lot better than uniformly generated random data.</p>
<p>Some combination of actions might not be possible, because they'd break business rules.
Those can be filtered out, or left in to test invalid requests.</p>
<h2 id="Conclusion">Conclusion<a href="#Conclusion" aria-label="Conclusion permalink"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 4 24 16"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2>
<p>We can do much more with this.
This was only a brief introduction to a handy tool that can be used to generate some fake requests.</p>
<p>Sample code is available on <a href="https://github.com/tinrab/rusty-markov-traffic">GitHub</a>.</p></div></div>]]>
            </description>
            <link>https://outcrawl.com/markov-chains-test-traffic/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24925655</guid>
            <pubDate>Thu, 29 Oct 2020 00:10:08 GMT</pubDate>
        </item>
    </channel>
</rss>
