<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 5]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 5. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Mon, 05 Oct 2020 01:07:05 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-5.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Mon, 05 Oct 2020 01:07:05 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Reverse Engineering a North Korean Sim City Game]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24670827">thread link</a>) | @pcr910303
<br/>
October 3, 2020 | https://digitalnk.com/blog/2019/04/21/reverse-engineering-a-north-korean-sim-city-game/ | <a href="https://web.archive.org/web/*/https://digitalnk.com/blog/2019/04/21/reverse-engineering-a-north-korean-sim-city-game/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-371">
	<!-- .entry-header -->

	<div>
		<p><em>Reverse engineering the North Korean version of a popular Sim City-like game using Ghidra and ndSpy to understand video game monetization strategies in the DPRK and the marketization of the country’s economy. </em></p><p><em>Key takeaways:</em></p><p><em>
<li>Android devices and applications are increasingly common in North Korea. Physical “app stores” can be found on every street corner in Pyongyang.</li>
<li>The game considered in this post is based on a Chinese version of a popular Android game developed in the Netherlands</li>
<li>The game’s monetization strategy was adapted to the country’s infrastructure (low internet/intranet availability, physical app stores)</li>
<li>The North Korean version eschews the original freemium + online microtransaction model for a one-time licence purchase + offline microtransaction model</li>
<li>File integrity checks added by North Korean developers shows that piracy is a concern and suggests the existence a warez/cracking scene in the DPRK</li>
<li>The cryptographic algorithms used for the licence are MD5, SHA1, RSA and AES. The library used by the game included the domestically developed private key algorithms Pilsung and Jipsam, but they were not used as part of the licencing system</li></em></p><p><a href="#intro">0. Introduction</a><br>
<a href="#licence">1. Licensing system</a><br>
<a href="#check">2. File integrity checks</a><br>
<a href="#money">3. In-game monetization strategy and key generation</a><br>
<a href="#end">4. Conclusion</a></p>
<h4 id="intro">0. Introduction </h4><p>During a recent trip to North Korea, I noticed the recent and ubiquitous presence of <em>Information Technology Exchange Rooms</em> (정보기술교류실), physical stores where one can purchase a variety of electronic devices – from laptops and tablets to USB sticks and chargers – as well as software and video games for PC, mobile and tablets (for an in-depth look at what goes on inside those stores as well as what the app selection looks like, <a href="https://www.nknews.org/2019/02/what-to-buy-inside-a-north-korean-app-store/">this article</a> by Alek Sigley provides a an excellent description. There are also a few <a href="https://www.youtube.com/watch?v=1ujblnigJmM">videos</a> on YouTube). After looking through the catalogue of available games at different stores, I eventually decided to try and buy a Sim City-like game called <em>City Management</em> (도시경경).</p>
<figure id="attachment_426"><a href="http://digitalnk.com/blog/wp-content/uploads/2019/04/chongbo.png"><img src="http://digitalnk.com/blog/wp-content/uploads/2019/04/chongbo.png" alt="" width="845" height="760"></a><figcaption>Billboards for various app stores in Pyongyang</figcaption></figure><p>The game only cost 5000 wons (less than 1 USD) which I paid to have the app installed on the phone I had, a Samsung Galaxy A5 running Android 8. The vendor connected the phone to his PC, transferred the APK and tried to install it, but to no avail. After multiple attempts, he eventually informed me that North Korean apps most likely could not run on phones from other countries. </p>
<figure id="attachment_428"><a href="http://digitalnk.com/blog/wp-content/uploads/2019/04/marrichakyongchu.png"><img src="http://digitalnk.com/blog/wp-content/uploads/2019/04/marrichakyongchu.png" alt="" width="551" height="827"></a><figcaption>Advertisement for a car racing game inside a North Korean app store</figcaption></figure><p>Fortunately, I was later able to purchase one of the different tablets sold in North Korea. I got the <em>Morning</em> (아침) brand, which is geared towards students and quite affordable. The tablet ran Android 4 (Kit Kat) on an ARM cpu and came loaded with a few educational apps: language learning courses, dictionaries and several e-book libraries containing the complete works of Kim Il Sung, school textbooks and a collection of literary works. No games, but that could now be fixed quite easily.</p>
<figure id="attachment_431"><a href="http://digitalnk.com/blog/wp-content/uploads/2019/04/achimtablet.png"><img src="http://digitalnk.com/blog/wp-content/uploads/2019/04/achimtablet.png" alt="" width="800" height="490"></a><figcaption>A North Korean Ach’im (Morning) tablet</figcaption></figure><p>I retrieved the <em>City Management</em> APK from my phone and installed it on the tablet, where it ran perfectly. Unfortunately, after the game’s initial splash screen, I landed on this:</p>
<figure id="attachment_434"><a href="http://digitalnk.com/blog/wp-content/uploads/2019/04/buyserial.png"><img src="http://digitalnk.com/blog/wp-content/uploads/2019/04/buyserial.png" alt="" width="1024" height="600"></a><figcaption>Licence key needed</figcaption></figure><p>The screen tells us that there is no “key file” (열쇠화일) and that we should purchase one at a store. There is a “request number” (요청번호) likely used to generate the licence key and make sure it can’t be shared with other devices. Unfortunately, since the APK never installed, the vendor did not put a licence file on my phone when I bought the app. My stay in North Korea was coming to an end too and I did not have time to go back to an app store to buy a new key. So I figured I would take a look inside the app and see if I could get it running nonetheless. </p>
<hr id="licence">
<h4>1. Licensing system </h4><p>To start looking into the APK’s code, I’ll use the standard suite of tools to decompress, decompile and rebuild android apps: <a href="https://sourceforge.net/projects/dex2jar/">dex2jar</a>, <a href="http://java-decompiler.github.io/">jd-gui</a>, <a href="https://ibotpeaches.github.io/Apktool/install/">apktool</a> and <a href="https://github.com/appium/sign">apksign</a>. I’ll also use <a href="https://developer.android.com/studio">Android Studio</a> to run and debug the app. The fact that I couldn’t run the app on my phone may have just come from an Android version compatibility issue: I had no problem running it on an emulated Android 4.4 device with Android Studio. The decompilation of the <code>classes.dex</code> file gives us some interesting information right away:</p>
<figure id="attachment_438"><a href="http://digitalnk.com/blog/wp-content/uploads/2019/04/dex2jar1.png"><img src="http://digitalnk.com/blog/wp-content/uploads/2019/04/dex2jar1.png" alt="" width="211" height="326"></a><figcaption>Packages and classes from the decompiled <code>classes.dex</code> file</figcaption></figure><p>The name of the <code>com.bz.cityisland2</code> package actually refers to the original game that <em>City Management</em> is based on: <a href="https://www.sparklingsociety.net/sparkling-games/city-building-games/city-island-2/">City Island 2</a> by the Dutch game studio Sparkling Society. The name of the package <code>com.smartions.appprotected</code> refers to <a href="https://www.crunchbase.com/organization/smartions-ag#section-overview">Smartions</a>, a company that offers solutions to “monetize your mobile game or app in China” and are apparently also City Island’s <a href="https://en.wikipedia.org/wiki/Sparkling_Society">distributor in China</a>. There are no mentions of those companies in the game itself however. The game’s loading splash screen only tells us that the game was made by the Ryusong (meteor) Technology Exchange Center (류성기술교류소) and that it is protected by the law for the protection of software (<a href="https://www.kisdi.re.kr/kisdi/common/premium?file=1%7C10360">콤퓨터쏘프트웨어보호법</a>). The law has been in place since 2003 to regulate the sales and distribution of software in the country and guarantees software developers the private ownership of their creation.</p>
<figure id="attachment_502"><a href="http://digitalnk.com/blog/wp-content/uploads/2019/04/kyongyong-1.png"><img src="http://digitalnk.com/blog/wp-content/uploads/2019/04/kyongyong-1.png" alt="" width="1000" height="640"></a><figcaption>Loading screen for the game</figcaption></figure><p>It’s hard to tell whether the North Korean version is based on the source code of the original game or if it’s entirely reverse engineered. In any case, the North Korean version does not use Smartions’s monetization system nor Sparkling Society’s but relies on a different system, which is the main difference from the original game. Save for the translation and some minor renames, the game is otherwise similar to the original (from a cursory examination) in its design, gameplay, features… to the original. </p>
<figure id="attachment_442"><a href="http://digitalnk.com/blog/wp-content/uploads/2019/04/UnityStructure.png"><img src="http://digitalnk.com/blog/wp-content/uploads/2019/04/UnityStructure.png" alt="" width="339" height="221"></a><figcaption>Structure a Unity APK. From Shim et al., <a href="https://www.hindawi.com/journals/scn/2018/6280768/"><em>Static and Dynamic Analysis of Android Malware and Goodware Written with Unity Framework</em></a> (2018).<br></figcaption></figure><p>There’s not much more we can glean from the Java code for now since, as the classes in <code>unity3dplayer</code> and <code>AndroidManifest.xml</code> file make clear, it is used to run code that was written with <a href="https://en.wikipedia.org/wiki/Unity_(game_engine)">Unity</a>, a popular cross-plaform video game framework which uses C# as its main programming language. The Unity code is stored in various library with the developer’s C# code being compiled to <code>Assembly-CSharp.dll</code>. C# compiled code is easily decompilable using tools such as <a href="https://github.com/0xd4d/dnSpy">dnSpy</a>. Once the dll is decompiled, we can look for the message we got earlier “열쇠화일이 존재하지 않습니다” (“The key file does not exist”) to find the bits of code we are interested in. The string search takes us to the <code>CIGLoadingScreen</code> class where we find the string among other variables:</p>
<pre title="">
	// Token: 0x0400056A RID: 1386
	private string userKey;

	// Token: 0x0400056B RID: 1387
	private string tapjoyCurrencyIdentifier;

	// Token: 0x0400056C RID: 1388
	private bool bannerVisible;

	// Token: 0x0400056D RID: 1389
	private int _loadingScreenShownCount;

	// Token: 0x0400056E RID: 1390
	private Dictionary&lt;int, bool&gt; m_gameObjectStatus = new Dictionary&lt;int, bool&gt;();

	// Token: 0x0400056F RID: 1391
	private bool m_isVerify;

	// Token: 0x04000570 RID: 1392
	private Font kfont;

	// Token: 0x04000571 RID: 1393
	private string reqMsg = "열쇠화일이 존재하지 않습니다.\r\n열쇠화일을 판매소에서 구입하십시오.";

	// Token: 0x04000572 RID: 1394
	private string reqNumLabel = "요청번호 : ";

	// Token: 0x04000573 RID: 1395
	private string reqNum;

	// Token: 0x04000574 RID: 1396
	private string finishLabel = "끝내기";
</pre><p>Looking for the name of the string variable <code>reqMsg</code> takes us here:</p>
<pre title="">	// Token: 0x06000928 RID: 2344 RVA: 0x00026E60 File Offset: 0x00025060
	private void OnGUI()
	{
		if (!this.m_isVerify &amp;&amp; this.loadingDone)
		{
			GUI.skin.font = this.kfont;
			GUI.DrawTexture(new Rect(0f, 0f, (float)Screen.width, (float)Screen.height), this.blackBg, ScaleMode.StretchToFill);
			GUI.Label(this.GetTextLabelRect(this.reqMsg, 0.5f, 0.3f), this.reqMsg);
			GUI.Label(this.GetTextLabelRect(this.reqNumLabel, 0.3f, 0.5f), this.reqNumLabel);
			GUI.Label(this.GetTextLabelRect(this.reqNum, 0.6f, 0.5f), this.reqNum);
			RectOffset padding = GUI.skin.button.padding;
			GUI.skin.button.padding = new RectOffset(20, 20, 10, 10);
			if (GUI.Button(this.GetButtonRect(this.finishLabel, 0.5f, 0.8f), this.finishLabel))
			{
				Application.Quit();
			}
		}
	}
</pre><p>This is the code used to display the splashscreen we encountered earlier. If the boolean property <code>this.m_isVerify</code>, presumably the result of a call to a function checking the existence and validity of a licence key, is <code>False</code> then, the screen is displayed with the message we saw earlier and the “request number”. The verification function and the generation of the request number are handled in another class <code>GameCus</code>:</p>
<pre title="">using System;
using System.IO;
using System.Runtime.InteropServices;

// Token: 0x02000147 RID: 327
public class GameCus
{
	// Token: 0x06000AA7 RID: 2727
	[DllImport("Game")]
	private static extern int vProcess(byte[] key, int keyLen, byte[] certData, int certDataLen);

	// Token: 0x06000AA9 RID: 2729 RVA: 0x0002E910 File Offset: 0x0002CB10
	public string GetReqNumber()
	{
		string deviceIdString = this.GetDeviceIdString();
		return string.Format("{0:d4} {1:d4} {2:d4} {3:d4}", new object[]
		{
			deviceIdString.Substring(0, 4),
			deviceIdString.Substring(4, 4),
			deviceIdString.Substring(8, 4),
			deviceIdString.Substring(12, 4)
		});
	}

	// Token: 0x06000AAA RID: 2730 RVA: 0x0002E968 File Offset: 0x0002CB68
	public string GetDeviceIdString()
	{
		string text = Utils.GetDeviceModel();
		text = string.Format("{0:d10}", (uint)text.GetHashCode());
		string str = text.Substring(2, 8);
		string text2 = Utils.GetDeviceUid();
		text2 = string.Format("{0:d10}", (uint)text2.GetHashCode());
		string str2 = text2.Substring(2, 8);
		return str + str2;
	}

	// Token: 0x06000AAB RID: 2731 RVA: 0x0002E9CC File Offset: 0x0002CBCC
	public bool checkCertData(byte[] certData)
	{
		if (certData == null || certData.Length == 0)
		{
			return false;
		}
		string text = this.GetDeviceIdString() + …</pre></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://digitalnk.com/blog/2019/04/21/reverse-engineering-a-north-korean-sim-city-game/">https://digitalnk.com/blog/2019/04/21/reverse-engineering-a-north-korean-sim-city-game/</a></em></p>]]>
            </description>
            <link>https://digitalnk.com/blog/2019/04/21/reverse-engineering-a-north-korean-sim-city-game/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24670827</guid>
            <pubDate>Sat, 03 Oct 2020 09:39:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An Honest Review of Gatsby]]>
            </title>
            <description>
<![CDATA[
Score 74 | Comments 46 (<a href="https://news.ycombinator.com/item?id=24670252">thread link</a>) | @ehfeng
<br/>
October 3, 2020 | https://cra.mr/an-honest-review-of-gatsby/ | <a href="https://web.archive.org/web/*/https://cra.mr/an-honest-review-of-gatsby/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p>We decided to adopt Gatsby for <a href="https://docs.sentry.io/">Sentry’s customer-facing documentation</a> - well, I should say that <em>I</em> decided. We were already using it successfully for a variety of static marketing content, and I knew it had a lot of hype, so after a brief proof-of-concept it seemed like a safe choice.</p>
<p>To help contextualize everything I’m about to say, it’s important to understand the scope of our usage. Sentry’s documentation is not as straightforward as you might think - in fact, there are over 3,000 pages as of writing. We have a large amount of templated content designed to render language-specific examples, as well as a variety of different types of documentation (user guides, help desk-y articles, code-rich technical docs). Originally we had extended Jekyll to support a lot of this, but Ruby isn’t widely used at Sentry (approximately 0% of the engineering team knows Ruby), and it had become a big mess of spaghetti code with slow build times.</p>
<p>I also want to note that while this blog post is primarily focusing on the flaws of Gatsby as a framework, I’m not here to tell you that it’s not good for your use case. That said, I was not able to discover many of these short comings easily when evaluating Gatsby, and many things you read on the internet don’t stem out of real-world usage. My hope here is that Gatsby continues to improve over time, and that, as a user, you can be more informed about if it’s the right choice for you.</p>
<h2>Adopting Gatsby</h2>
<p>So, enter Gatsby. It seemed fast, was built on React (we’re experts on that here, with our gigabyte-sized Sentry frontend app), and had a huge adoption (assumed future existence and stability). While we didn’t have the desire to use MDX, it also seemed like a positive outcome given we could more easily deal with some of the rich aspects of our docs site, without having to resort to 2010-era JavaScript. We assumed a bunch of the other features of Gatsby had value-add, but we didn’t have an immediate need. These were things like dynamic source data - thus the need for a GraphQL engine at all - as well as the large plug-in ecosystem.</p>
<p>We started by iteratively converting sections of the Jekyll site into Gatsby - running them side by side for a time. At one point we eventually bulk converted pages, and ripped off the band aid. At this point though it was becoming clear build times were a problem. You’d spend at least 5 minutes on image optimization alone, with no way to even disable that. Slowly but surely we were depleting the ozone later on re-optimizing images which had already been pre-optimized. Oh yeah, and we were crippling our iteration speed as well, since the build cache would invalidate under a variety of situations in early development.</p>
<p>Making this worse was how we deployed Gatsby. We started off leveraging what we had already done: deploying Jekyll with Docker onto our own infrastructure - effectively just proxied via a CDN. We continued that for a period of time, but deploy times were far too long - upwards of 30-40 minutes for everything to build. Eventually we moved over to <a href="https://vercel.com/">Vercel</a> which dropped it down closer to 10 minutes, but ultimately it can’t fix what it doesn’t control.</p>
<p>The build and deploy times were the first of many woes, and they represent what would become a continued frustration: a problem without a clear solution.</p>
<h2>Enter MDX</h2>
<p>Rewind time a little bit - this actually wasn’t our first project converting documentation to Gatsby. The proof-of-concept I mentioned earlier was actually our <a href="https://develop.sentry.dev/">developer documentation</a>, which I had migrated out of Notion to make public. While doing that we had gotten our hands dirty with some initial MDX usability and extensions - like our code samples which support toggling between different languages. This was one of the many things we needed to solve for, but MDX made it look like it’d be seemingly easy. No more jQuery DOM manipulation, just clean, encapsulated React components. Or so we thought.</p>
<p>Almost immediately we hit rough spots with MDX. We were coming from Jekyll - which was Liquid-rendered (a template engine) markdown - to MDX - a strange offspring of Markdown and JSX, attempting all of the benefits of both, but missing by a fairly large margin. Let’s illustrate the crux of the issue with what has got to be one of the most common needs in a documentation system: an alert (or callout) component:</p>
<div data-language="markdown"><pre><code><span><span><span>&lt;</span>Alert</span> <span>level</span><span><span>=</span><span>"</span>info<span>"</span></span><span>&gt;</span></span>You should know something important about this!<span><span><span>&lt;/</span>Alert</span><span>&gt;</span></span></code></pre></div>
<p>At face value this looks great. <code>Alert</code> is just a React component, and JSX is close enough to HTML that non-technical folks are able to pick it up fairly easily. Now the problem comes into play when you actually want to do something in the real world. Here’s an example from our API docs:</p>
<div data-language="markdown"><pre><code><span><span><span>&lt;</span>Alert</span> <span>level</span><span><span>=</span><span>"</span>warning<span>"</span></span> <span>title</span><span><span>=</span><span>"</span>Note<span>"</span></span><span>&gt;</span></span>
    <span><span>**</span><span>PUT/DELETE</span><span>**</span></span> methods only apply to updating/deleting issues.
Events in sentry are immutable and can only be deleted by deleting the whole issue.
<span><span><span>&lt;/</span>Alert</span><span>&gt;</span></span></code></pre></div>
<p>How would you expect this to render? Both as an engineer and a non-engineer, I would expect - given this is markdown - that the “PUT/DELETE” text would be bold. It’s not. Because the MDX interpreter decides that once you enter a component block, it’s no longer markdown. So instead, we’re forced with this monstrosity <em>everywhere</em> in our documentation:</p>
<div data-language="markdown"><pre><code><span><span><span>&lt;</span>Alert</span> <span>level</span><span><span>=</span><span>"</span>warning<span>"</span></span> <span>title</span><span><span>=</span><span>"</span>Note<span>"</span></span><span>&gt;</span></span><span><span><span>&lt;</span>markdown</span><span>&gt;</span></span>

<span><span>**</span><span>PUT/DELETE</span><span>**</span></span> methods only apply to updating/deleting issues.
Events in sentry are immutable and can only be deleted by deleting the whole issue.

<span><span><span>&lt;/</span>markdown</span><span>&gt;</span></span><span><span><span>&lt;/</span>Alert</span><span>&gt;</span></span></code></pre></div>
<p>There’s two things you should note here: 1) we have to use this <code>&lt;markdown&gt;</code> tag, 2) we have to put empty new-lines to ensure paragraph tags render.</p>
<p>“But David”, you might say, “why don’t you just tell the <code>Alert</code> component to render the text as markdown?“. If only you could, or at least, if only I could have possibly found a way to achieve that as a user with minimal Gatsby or MDX internals knowledge.</p>
<p>To Gatsby, or at least to the MDX team’s credit, they recognize some of these problems and <a href="https://github.com/mdx-js/mdx/issues/1041">there is work underway</a> on a 2.0 of the MDX dialect. While I’m confident they will improve things, I’m not confident MDX can ultimately succeed. It’s likely going to tradeoff one problem for another due to what it’s trying to achieve in the first place. It may get to a good place, but frankly, we need to step back and look at what we’re trying to solve, instead of creating a solution to a problem we don’t have. I don’t need JSX syntax in my markdown, I need a way to include JSX components. That might sound similar, but its quite a different thing.</p>
<p>As an example, there’s no reason I couldn’t simply use markdown syntax, and provide a way to achieve something akin to:</p>
<div data-language="markdown"><pre><code><span><span><span>&lt;</span>a-valid-html-tag-because-markdown-allows-that</span> <span>a-valid-property</span><span><span>=</span><span>"</span>a-value<span>"</span></span><span>&gt;</span></span></code></pre></div>
<p>This wouldn’t force us to work around quirks in a new language (or interpreter even), and could be solved in a much more sustainable way. There are other alternatives as well. A generic way to render extensions in markdown could simply call into a React component, and avoid even trying to hijack HTML in the first place. While I don’t know what this might look like in Markdown, in <a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/index.html">Sphinx’s use of reStructuredText</a> this was solved early on with Directives:</p>
<div data-language="text"><pre><code>.. my-directive:: some data
   :property-name: property-value</code></pre></div>
<p>I will hold out for MDX 2.0 and hope that finds a nice minimal-compromise place, but if not, we’ll be looking for a way to extend native markdown.</p>
<h2>A Broken DOM</h2>
<p>While we were able to work around the kinks of MDX, there’s been some things not yet solved. One of those is the layer which Gatsby uses to apply diffs to the DOM. I’m going to caveat this section with <em>I don’t know what the technical implementation is</em>, but I can make some assumptions given what I know of the domain. The system itself is intended to apply deltas to the DOM. This is naively also how React works, and I imagine under the hood it’s relying on React at least for part of it. We’ve had issues with this identified in two places already:</p>
<ul>
<li>progressive image loading</li>
<li>dynamic JSX components</li>
</ul>
<p>While they might not be linked to the same issue, they smell like they are, so we’re going to roll with it. The problem exhibits itself when you have a bunch of DOM that to a naive robot might look the same:</p>
<div data-language="text"><pre><code>&lt;div&gt;foo&lt;/div&gt;
&lt;div&gt;bar&lt;/div&gt;
&lt;div&gt;baz&lt;/div&gt;
&lt;div&gt;foobizbar&lt;/div&gt;</code></pre></div>
<p>In React it uses the graph to identify which node is which - effectively creating a unique entity ID based on its location. In cases where that’s difficult, React will warn you to explicitly bind a <code>key</code> attribute on each element to ensure it can more accurately deal with updates. While I would assume Gatsby is at least partially using React’s DOM engine, what we see in production effectively takes the above example, and replaces some of the content with other subsets of content - meaning it’s unable to accurately identify which nodes need updated.</p>
<p>We’ve seen this where a progressive image is replaced with an entirely different image that’s present near it on the page. We’ve also seen this happen for a dynamically loaded section of content (our language-selector include tags). While we’ve yet to identify a fix for the image tags, our other issue was resolved by literally changing a <code>div</code> tag to a different tag, one which is less commonly used (in our case, <code>section</code>).</p>
<p>All of the cases happen after Gatsby’s initial static render and exist only when applying some form of delta.</p>
<h2>Let’s Talk GraphQL</h2>
<p>It’s a static website generator. It literally does not need GraphQL all over the place. While there are few instances in the real world where that is valuable, it shouldn’t require a GraphQL API to read objects that are already in memory.</p>
<p>I don’t want to spend the energy to hammer this in, but take a look at Jared Palmer’s <a href="https://jaredpalmer.com/gatsby-vs-nextjs">Gatsby vs. Next.js</a> as it echoes my thoughts.</p>
<p>So, let’s actually not talk about GraphQL, but all its done is create complexity for us.</p>
<h2>Minor Gripes</h2>
<p>There’s a number of other things we’ve found fairly frustrating at this point, but this post is already getting long, so I’m choosing to summarize them.</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://cra.mr/an-honest-review-of-gatsby/">https://cra.mr/an-honest-review-of-gatsby/</a></em></p>]]>
            </description>
            <link>https://cra.mr/an-honest-review-of-gatsby/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24670252</guid>
            <pubDate>Sat, 03 Oct 2020 07:18:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Iron, How Did They Make It, Part III: Hammer-Time]]>
            </title>
            <description>
<![CDATA[
Score 125 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24668125">thread link</a>) | @dddddaviddddd
<br/>
October 2, 2020 | https://acoup.blog/2020/10/02/collections-iron-how-did-they-make-it-part-iii-hammer-time/ | <a href="https://web.archive.org/web/*/https://acoup.blog/2020/10/02/collections-iron-how-did-they-make-it-part-iii-hammer-time/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>This week, we continue our four-part (<a href="https://acoup.blog/2020/09/18/collections-iron-how-did-they-make-it-part-i-mining/">I</a>, <a href="https://acoup.blog/2020/09/25/collections-iron-how-did-they-make-it-part-ii-trees-for-blooms/">II</a>, III, IV) look at pre-modern iron and steel production.  Last week we took our ore and smelted it into a rough, spongy mass of iron called a bloom; this week we’re going to go through the processes to reshape that bloom first into a consolidated billet, then into a bar that is useful for forging and finally into some useful final object.</p>



<p>I want to stress at the outset that we are not going to cover anything close to the whole of blacksmithing practice in this post.  Blacksmithing is fairly complex and any given object, shape or tool is going to have its own set of processes and techniques to produce the required shape at the required hardness and malleability characteristics.  If you <em>are</em> interested in that sort of information, I recommend A.W. Bealer’s <em>The Art of Blacksmithing</em> (1969) as a fairly good starting point, though there is no substitute to speaking with a practicing blacksmith.</p>



<p>As always, if you like what you are reading here, please share it; if you really like it, you can support me on <a href="https://www.patreon.com/user?u=20122096">Patreon</a>. And if you want updates whenever a new post appears, you can click below for email updates or follow me on twitter (@BretDevereaux) for updates as to new posts as well as my occasional ancient history, foreign policy or military history musings.</p>





<h2>Heat, Hammers and Hardness</h2>



<p>There are a few basic behaviors of iron that fundamentally control what blacksmiths are going to do with it in this stage.  To begin with, we need to introduce some terminology to avoid this coming confusing: a given piece of metal can be <em>hard</em> (resistant to deformation) or <em>soft</em>; they can also be <em>ductile </em>(able to deform significantly before breaking) or <em>brittle</em> (likely to break without deformation).  This is easiest to understand at the extremes: a soft, brittle material (like a thin wooden dowel) takes very little energy and breaks immediately without behind, while a hard, ductile material (the same dowel, made of spring-steel) bends more easily under stress but resists breaking.  But it is also possible to hard hard brittle materials (pottery being a classic example) which fiercely resist deforming but break catastrophically the moment they exceed their tolerances or a soft, ductile material (think wet-noodle) which bends very easily.</p>



<p>(I should note that all of these factors are, in fact, very complex – far more complex than we are going to discuss.  In particular, as I understand it, some of what I am using ‘hardness’ to describe also falls under the related category of yield strength.  Hopefully you will all pardon the necessary simplification; if it makes you feel any better, ancient blacksmiths didn’t understand how any of this worked either, only that it worked.)</p>



<p>Of course these treats are not binaries but a spectrum.  Materials have a degree of hardness or ductility; as we’ll see, these are not quite <em>opposed</em>, but changing one does change the other – increasing hardness often reduces ductility.</p>



<p>The sort of things that pre-modern people are going to want to be made in iron are going to have fairly tight tolerances for these sorts of things.  Objects that had wide tolerances (that is, things which could be weak or a little bendy or didn’t have to take much force) got made out of other cheaper, easier materials like ceramics, stone or wood; metals were really only used for things that had to be both strong and relatively light for precisely the reasons we’ve seen: they were too expensive for anything else.  <strong>That means that a blacksmith doesn’t merely need to bring the metal to the right shape but also to the right <em>characteristics</em></strong><em><strong>.</strong>  </em>Some tools would need to finish up being quite hard (like the tip of a pick, or the edge of a blade), while others needed to be able to bend to absorb strain (like the core of a blade or the back of a saw).</p>



<p>Keep all of that in mind as we discuss:</p>



<h2>Forge Techniques</h2>



<p>I realize this is a long aside to leave our bloom waiting, but as we’ll see, the remaining steps share a basic set of techniques, making it easier to discuss those techniques together.</p>



<p>Fundamentally, each stage of forging iron revolves around a basic cycle: <strong>by heating the metal, the smith makes it soft enough to <em>work</em> </strong>(that is, hammer into shape).  Technically, it is possible to shape relatively thin masses of iron by hammering when cold (this is called cold-working) but in contrast to other metals (tin, copper and bronze all come to mind) nearly all serious iron-working was done ‘hot.’  In smithing terminology, each of these cycles is referred to as a ‘heat’ – the more heats a given project requires, the more fuel it is going to consume, the longer and more expensive it is going to be (but a skilled smith can often finish the work in fewer heats than an unskilled smith).</p>



<figure><img data-attachment-id="4712" data-permalink="https://acoup.blog/202833001/" data-orig-file="https://acoupdotblog.files.wordpress.com/2020/10/202833001.jpg" data-orig-size="2500,2148" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="202833001" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2020/10/202833001.jpg?w=300" data-large-file="https://acoupdotblog.files.wordpress.com/2020/10/202833001.jpg?w=1024" src="https://acoupdotblog.files.wordpress.com/2020/10/202833001.jpg?w=1024" alt="" srcset="https://acoupdotblog.files.wordpress.com/2020/10/202833001.jpg?w=1024 1024w, https://acoupdotblog.files.wordpress.com/2020/10/202833001.jpg?w=2048 2048w, https://acoupdotblog.files.wordpress.com/2020/10/202833001.jpg?w=150 150w, https://acoupdotblog.files.wordpress.com/2020/10/202833001.jpg?w=300 300w, https://acoupdotblog.files.wordpress.com/2020/10/202833001.jpg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><a href="https://www.britishmuseum.org/collection/object/P_1836-0811-78">Via the British Museum</a>, A Dutch drawing (c. 1624-1640) of blacksmiths at work, with the blacksmith himself in the center and a striker (with the two-handed hammer) to the left.  An assistant mans the bellows on the forge to maintain the temperature.</figcaption></figure>



<p>A modern blacksmith can gauge the temperature of a metal using sophisticated modern thermometers, but pre-modern smiths had no recourse to such things (and most traditional smiths I’ve met don’t use them anyway).  Instead, the temperature of the metal is gauged by looking at its <em>color</em>: as things get hotter, they glow from brown to dark red through to a light red into yellow and then finally white.  For iron heated in a forge, a blacksmith can control the temperature of the forge’s fire by controlling the air-input through the bellows (pushing in more air means more combustion, which means more heat, but also more fuel consumed).  As we’ve seen, charcoal (and we will need to use charcoal, not wood, to hit the necessary heat required), while not cripplingly expensive, was not trivial to produce either.  A skilled smith is thus going to try to do the work in as few heats as possible and not excessively hot either (there are, in fact, other reasons to avoid excessive heats, this is just one).</p>



<p>One hot the metal can be shaped by hammering.  The thickness of a bar of metal could be thickened by <em>upsetting</em> (heating the center of the bar and them hammering down on it like a nail to compress the center, causing it to thicken) or thinned by <em>drawing</em> (hammering out the metal to create a longer, thinner shape).  If the required shape needed the metal to be bent it could be heated and bent either over the side of the anvil or against a tool; many anvils had (and still have) a notch in the back where such a tool could be fitted.  A good example of this kind of thing would be hammering out a sheet of iron over a dome-shape to create the bowl of a helmet (a task known as ‘raising’ or ‘sinking’ depending on precisely how it is done).  A mass of iron can also be divided by heating it at the intended cutting point and then using a hammer and chisel to cut through the hot, soft metal.</p>



<p>But for understanding the entire process, the most important of these operations is the<strong> <em>fire weld</em></strong>.  Much like bloomery furnaces, the forges available to pre-modern blacksmiths could not reach the temperatures necessary to melt or cast iron, but it was necessary to be able to join smaller bits of iron into larger ones which was done through a fire weld (sometimes called a forge weld).  In this process, the iron is heated very hot, typically to a ‘yellow’ or ‘white’ heat (around 1100 °C).  The temperature range for the operation is quite precise: too cold and the iron will not weld, too hot and it will ‘burn’ making the weld brittle.  Once at the right temperature, the two pieces of iron are put next to each other and hammered into each other with heavy blows.  If done properly, the two pieces of metal join completely, leaving a weld that is as strong as every other part of the bar.</p>



<figure><img data-attachment-id="4703" data-permalink="https://acoup.blog/fire-welds/" data-orig-file="https://acoupdotblog.files.wordpress.com/2020/10/fire-welds.png" data-orig-size="1057,282" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fire-welds" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2020/10/fire-welds.png?w=300" data-large-file="https://acoupdotblog.files.wordpress.com/2020/10/fire-welds.png?w=1024" src="https://acoupdotblog.files.wordpress.com/2020/10/fire-welds.png?w=1024" alt="" srcset="https://acoupdotblog.files.wordpress.com/2020/10/fire-welds.png?w=1024 1024w, https://acoupdotblog.files.wordpress.com/2020/10/fire-welds.png?w=150 150w, https://acoupdotblog.files.wordpress.com/2020/10/fire-welds.png?w=300 300w, https://acoupdotblog.files.wordpress.com/2020/10/fire-welds.png?w=768 768w, https://acoupdotblog.files.wordpress.com/2020/10/fire-welds.png 1057w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><a href="https://youtu.be/cc_n8H-2-I0">From this demonstration</a>, a series of hammer blows in the process of making a fire or forge weld.  The sparks you see are flux and hammer scale (and possibly some amount of slag and excess iron material) being ejected out of the weld.</figcaption></figure>



<p>That’s not all there is to say about these processes (we’ll come back to them in a moment) but we now have enough of the basics to begin processing our bloom.</p>



<h2>From Bloom to Billet to Bar</h2>



<p>As you may recall, when we finished our process last time, we ended with a ‘bloom’ of iron: a spongy mass of pure, metallic iron interspersed with inclusions of waste materials called slag:</p>



<figure><img data-attachment-id="4630" data-permalink="https://acoup.blog/1024px-iron_bloom/" data-orig-file="https://acoupdotblog.files.wordpress.com/2020/09/1024px-iron_bloom.jpg" data-orig-size="1024,853" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="1024px-iron_bloom" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2020/09/1024px-iron_bloom.jpg?w=300" data-large-file="https://acoupdotblog.files.wordpress.com/2020/09/1024px-iron_bloom.jpg?w=1024" src="https://acoupdotblog.files.wordpress.com/2020/09/1024px-iron_bloom.jpg?w=1024" alt="" srcset="https://acoupdotblog.files.wordpress.com/2020/09/1024px-iron_bloom.jpg 1024w, https://acoupdotblog.files.wordpress.com/2020/09/1024px-iron_bloom.jpg?w=150 150w, https://acoupdotblog.files.wordpress.com/2020/09/1024px-iron_bloom.jpg?w=300 300w, https://acoupdotblog.files.wordpress.com/2020/09/1024px-iron_bloom.jpg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The iron of the bloom itself is also likely to be quite brittle because of these slag inclusions.  This isn’t a product that can be sent directly to a blacksmith.  It needs to be consolidated first into a billet and most of that slag needs to be forced out, both of which can be achieved via liberal application of <strong>fire welding</strong>.</p>



<p>This step is sometimes called <strong>bloomsmithing</strong>.  The bloom is heated to roughly 1100 °C (gauged, as above, by the color of the iron) – it seems plausible that it may have been broken up into smaller chunks to make this more useful – and then hammered into a single mass through a series of fire welds.  We’re not very well informed how this was done in the ancient world (save ‘with hammers’) because bloomsmithing doesn’t tend to leave a lot of evidence for us to observe.  The end shape of the process was generally a very thick rectangular bar called a <strong>billet</strong>, ready for relatively easy transport.</p>



<p>This process has some advantages and disadvantages, beyond merely shaping the metal into a more usable and transportable form.  Remember that our bloom contains a lot of material which isn’t iron (the slag); fire welding, especially when repeated, tends to expel this slag – as the iron is compressed in the weld, the slag is forced out.  There is some debate (note Sim &amp; Kaminski, <em>op. cit.</em>) if this process is sufficient to explain the <em>very</em> low slag counts seen in high quality weapons and armor, but it is certainly true that fire welding reduces the overall slag count.  That …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://acoup.blog/2020/10/02/collections-iron-how-did-they-make-it-part-iii-hammer-time/">https://acoup.blog/2020/10/02/collections-iron-how-did-they-make-it-part-iii-hammer-time/</a></em></p>]]>
            </description>
            <link>https://acoup.blog/2020/10/02/collections-iron-how-did-they-make-it-part-iii-hammer-time/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24668125</guid>
            <pubDate>Fri, 02 Oct 2020 23:35:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Archillect – an AI created to discover and share stimulating visual content]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24667732">thread link</a>) | @dewey
<br/>
October 2, 2020 | https://archillect.com/about | <a href="https://web.archive.org/web/*/https://archillect.com/about">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://archillect.com/about</link>
            <guid isPermaLink="false">hacker-news-small-sites-24667732</guid>
            <pubDate>Fri, 02 Oct 2020 22:35:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why most Hacktoberfest PRs are from India]]>
            </title>
            <description>
<![CDATA[
Score 224 | Comments 116 (<a href="https://news.ycombinator.com/item?id=24667488">thread link</a>) | @pulkitsh1234
<br/>
October 2, 2020 | https://pulkitsharma07.github.io/2020/10/02/hacktoberfest-india/ | <a href="https://web.archive.org/web/*/https://pulkitsharma07.github.io/2020/10/02/hacktoberfest-india/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><em>Before reading the post</em></p>

<p>Now that I have mentioned the name of a country in the title, please read the following points:</p>
<ul>
  <li>I am an Indian and have been living in the country since my birth.</li>
  <li>I do not claim to be right on all of this and there are many anecdotal generalisations here. Somethings are more like human problems instead of Indian problems <strong>but</strong> just the sheer number of people here puts India on everyone’s radar.</li>
  <li>This post is NOT to denigrate India. It is definitely critical of some of its social and psychological aspects. As a proof for that I want to mention that I am writing a series on the wisdom of ancient Indian philosophical ideas here: <a href="https://pulkitsharma07.github.io/2020/06/25/source-0/">Source [0]</a>, just so you know that I am not writing all my posts with the sole intention to show what is “wrong” with India. We should be brave enough to face the good and bad of it.</li>
  <li>It is a shame that I need to write this header in the first place. But the reality is with or without this, people will still get offended.</li>
</ul>

<hr>

<h2 id="index">Index</h2>

<ul>
<li><a href="#what">What</a></li>

<li><a href="#how">How</a></li>

<li><a href="#why">Why</a>
<ul>
  <li><a href="#the-signalling-problem">The Signalling Problem</a></li>
  <li><a href="#the-jugaad-mentality">The Jugaad Mentality</a></li>
  <li><a href="#computer-science-education-in-india">Computer Science Education in India</a></li>
  <li><a href="#but-why-for-hacktoberfest">But why for Hacktoberfest?</a></li>
  <li><a href="#the-problem-with-high-population-density">The problem with high population density</a></li>
  <li><a href="#hierarchy-of-needs">Hierarchy of Needs</a></li>
</ul></li>
<li><a href="#closing-thoughts">Closing Thoughts</a>
<ul>
  <li><a href="#probable-future">Probable Future</a></li>
  <li><a href="#improbable-future">Improbable Future</a></li>
</ul>
</li>
</ul>
<hr>

<h2 id="what">What</h2>

<p>As most of you may know, <a href="https://hacktoberfest.digitalocean.com/">Hacktoberfest</a> started on 1st October 2020. And as it with most things in human life, people came with <a href="https://mobile.twitter.com/shitoberfest">different</a> ways to hack hacktoberfest to get the free t-shirt.</p>

<p>Now apart from the sheer number of spam PRs which were opened, copious amount of time is being spent by the comparatively small number of open source maintainers who have comment, close and label each individual PR.</p>

<p>I became aware of this happening from this <a href="https://news.ycombinator.com/item?id=24643894">post</a>, and in a days time several more <a href="https://joel.net/how-one-guy-ruined-hacktoberfest2020-drama">posts</a> popped up. 
I got motivated (triggered?) to write this post when I saw the following on HN:</p>

<p><img src="https://pulkitsharma07.github.io/assets/hacktober/hn_comment.png" alt="hn-comment"></p>

<p>After some investigation, I found the original claim to be true, <code>more than 60% of the spam PRs are coming from one country: India</code> and that is just 1 day after hacktoberfest started, I am sure the percentage will increase as days go by (unless DigitalOcean does something).</p>
<hr>

<h2 id="how">How</h2>

<p><a href="https://joel.net/how-one-guy-ruined-hacktoberfest2020-drama">This</a> post talks about one Indian YouTube channel which apparently told people to open spammy PRs. After I looked around some people were claiming he was falsely accused, anyways the video is deleted now, so I can’t comment on that.</p>

<p>Anyways, it didn’t take long and I was able to find few more Indian YouTubers who were pretty directly promoting ways to open PRs to get those sweet T-Shirts. One created his own <a href="https://github.com/seeditsolution/cprogram/pulls">repo</a> and encouraged people to google different programs and copy-paste them into a new PR against the empty repo.</p>

<p>I do not want to focus on the <em>How</em>, you will find find plenty videos/blogs on it, I am sure some Indian YouTuber has started working on creating a video on it.</p>
<hr>

<h2 id="why">Why</h2>
<p>One of the simplest explanation is that, DigitalOcean should have been very well anticipated that this is going to happen, after all they must have all the stats from the previous HacktoberFests.</p>

<p>We need to understand that Hacktoberfest is an marketing event. And all these spam and blogs posts around it is definitely serving their main purpose, there is no such thing as bad publicity indeed. Of course I could be totally wrong, all of this could be completely unintentional.</p>

<hr>

<p>Now, coming to the Indian side of the ongoing issue. Out of all the countries why does India have the most spammy PRs ? As a seasoned armchair philosopher, the following are my thoughts on the “Why” of all this.</p>
<hr>

<h3 id="the-signalling-problem">The Signalling Problem</h3>
<p>Whenever an online article/documentary/report claim to give a view into the “real” India, most of us (Indians) get offended.</p>

<p>I <em>was</em> in the bubble which was in the “offended” bucket for long time. The reason was simple: I, the people around me and the societal structures around me were all part of the same bubble. We do not have to worry about paying school fees or rent, we do not have worry about sending part of salary back home just so that our parents can buy groceries to survive..</p>

<p>I can go on and on, the overall gist is that I got offended in seeing what other people claim as “real” India, because that India was not part of my reality. Sure, I saw that when travelling from one place to another, saw that on news, read about that in the newspaper; but I was at a very safe distance from all of that.</p>

<p>These days, I try to not get offended when people say India has problem X or problem Y, because I realise I am living inside a cocoon where all of my needs are met, and one of the most serious issues my country is facing right now is <strike>witch-hunting </strike> nabbing the “drug mafia” amongst the Bollywood celebrities who allegedly murdered an actor by giving CBD oil (The absurdity of all this is unfathomable)</p>

<p>I mention all of this, because whenever I read posts which show India in some <a href="https://news.ycombinator.com/item?id=24552047">bad light</a>, there is always someone somewhere who gives anecdotes of how that is not true.</p>

<p>Here are some facts: we have a culture of <strong>extreme signalling</strong>. Signalling is the core building block of our society, most people don’t even realise that how big we are into signalling until they study about “signalling” as a phenomena and start becoming conscious of it. You notice that in the way your parents view you, how you make choices, how people around you make choices.</p>

<p>Of course, people will say “that is not an Indian problem, signalling is just a social construct and literally everyone does that on some level”. I am not denying not that, the very fact that I am writing this post is a kind of signalling I am doing.</p>

<p>It is well known fact that India is one of the densest countries on our <a href="https://ourworldindata.org/grapher/population-density?time=2017">planet</a>, extreme siginalling is just one of the consequences of the myriad social problems created by high population density. Signalling in an high density environment is now ever more important as the people you interact or the people who notice your activities/accomplishments are multiples higher than in any other place on the planet.</p>

<p>We people like the wear the “tightly knit society” as badge of honour, let me tell you this “tightly knit society” has done more harm than good. We Indian people have no India of the amount of mental harassment we all go through, because that is just normalized as being just phases of “life”.</p>

<p>The prevalence of extreme signalling brews the classic and infamous <em>herd mentality</em> in our minds. In middle school children have dreams to become a Pilot in the airforce, or maybe a Police officer, or maybe a Opera Singer ! But by the time of high school, everyone is just either on road to become an Engineer, a Doctor, a Lawyer, a Chartered Accountant …. or a <strong>failure</strong>. This may seem harsh but that’s how most of the society operates here in India.</p>

<p>Now once your track is chosen for you (or fortunately, if you get to choose the track), you have some set goals you “must” achieve because they guarantee monetary success with an extremely high probability. If you are Engineer, you need to get into IIT. If you are a Doctor, you need to get into AIIMS. If you are doing management, you need to get into IIM.</p>

<p>Now, if you are an Engineer then your success criteria is not just any IIT, it should be one of the top ones (Bombay, Delhi, Kharagpur, Roorkee) and not just any branch in IIT, it should be <em>Computer Science</em>. Because that’s how you get <a href="https://www.newindianexpress.com/nation/2019/dec/04/five-iitians-bag-pay-packages-of-over-rs-15-crore-2071120.html">Rs 1.5 crore “packages” (equivalent to 200K USD)</a>.</p>

<p>You can give me examples of how not all IIT toppers choose “Computer Science”, but that’s not the point, for the vast majority of people (more than a million), the success criteria is: getting Computer Science at IIT Bombay.</p>

<p>As might know (or have guessed) getting a seat in IIT Bombay is next to impossible for more than 99% people giving IIT, so they “lower” their goal by aiming for other IITs, but “Computer Science” remains the top priority.</p>

<h3 id="the-jugaad-mentality">The Jugaad mentality</h3>
<p>Hacking systems is so ingrained in our society that we have a word for it: <a href="https://en.wikipedia.org/wiki/Jugaad">Jugaad</a>. The whole scene with Hacktoberfest is just a demo of our Jugaad skills. Wait till you find out that <a href="https://indianexpress.com/article/india/inside-indias-fake-research-paper-shops-pay-publish-profit-5265402">most of the research papers</a> published in India are <a href="https://scroll.in/article/908230/indian-academics-lead-the-world-in-publishing-in-fake-journals-tarring-the-whole-education-sector">fake</a>. Even the orthodontist I was visiting for my checkup turned out to be fake and had forged her certificates (as told by by one other dentist).</p>

<p>The coaching industry actively promotes “cracking” these exams, and I am so tired writing on that topic that I do not want to write more about it here. Refer to my post on this <a href="https://www.linkedin.com/pulse/coding-interviewing-coaching-industry-prologue-pulkit-sharma?articleId=6662006663559684097">here</a>. Some children start joining these coaching classes from as low as 5th grade! (apart from doing regular school), just to be able to “crack” the IITs after 7-8 years !</p>

<p>Now how is coaching industry a Jugaad ? Simply because from the point of view of IITs, attending regular school should be enough to prep you for the exams (I think). These coaching industries have made the process significantly harder as you can’t even hope of getting a low tier IIT without attending the coaching classes.</p>

<p>The IIT coaching industry is minting fat cheques out of this entire situation, look at these ads on the front page of the news paper. These ads create a vicious cycle by reinforcing the core “signalling” construct of our society.</p>

<p><img src="https://pulkitsharma07.github.io/assets/hacktober/paper_1.png" alt="hn-comment"></p>

<p><img src="https://pulkitsharma07.github.io/assets/hacktober/paper_2.png" alt="hn-comment"></p>

<p><img src="https://pulkitsharma07.github.io/assets/hacktober/paper_3.png" alt="hn-comment"></p>

<p>Do not quote me on this, but I think the IIT coaching industries make more <a href="https://www.businessinsider.in/education/news/iits-iisc-say-that-patchy-funding-is-delaying-institute-of-eminence-plans/articleshow/72103538.cms">money than the IITs</a> themselves. I understand the purpose of IITs is not to directly generate money, but if it would have more resources they can probably improve the infrastructure, employ better professors and improve the overall level of education.</p>

<h3 id="computer-science-education-in-india">Computer Science Education in India</h3>
<p>For vast majority of engineers in India, Computer Science is  one of the subjects you study in order to succeed in life. Just like you study Chemistry, you “study” Computer Science and once you learn all the “concepts”, you get a good job. (Apologies for so many quoted words, I can’t help putting them in quotes because they carry so much weight for me).</p>

<p>When people are in an average Indian college and if they are are lucky, information about some permutation/combination of the following is spread amongst the …</p></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pulkitsharma07.github.io/2020/10/02/hacktoberfest-india/">https://pulkitsharma07.github.io/2020/10/02/hacktoberfest-india/</a></em></p>]]>
            </description>
            <link>https://pulkitsharma07.github.io/2020/10/02/hacktoberfest-india/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24667488</guid>
            <pubDate>Fri, 02 Oct 2020 22:04:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What is the best dumb TV?]]>
            </title>
            <description>
<![CDATA[
Score 413 | Comments 412 (<a href="https://news.ycombinator.com/item?id=24666968">thread link</a>) | @evo_9
<br/>
October 2, 2020 | https://pointerclicker.com/best-dumb-tv/ | <a href="https://web.archive.org/web/*/https://pointerclicker.com/best-dumb-tv/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text"><p>There are a few things you need to consider when you’re looking to buy a good dumb TV. Let’s take a closer look at each of them.</p><h3>Screen size</h3><p>The biggest challenge you will face when you’re looking for a good dumb TV is getting a good screen size. You will soon realize that there are not a lot of dumb TVs that come in the latest size standards for home entertainment.</p><p>A lot of dumb TVs only have 30-40-inch screens. If this size works for you, then you are in luck. However, in today’s standards, 30-40 inches is not enough screen real estate.</p><p>You may want to find something with at least a 50-inch screen. The good news is that they do exist and, according to our research, you can even go up to 65 inches.</p><p><strong>If you’re in hurry, here’s our recommendations</strong></p><table><thead><tr><th>Image</th><th>Product</th><th>Features</th><th>Price</th></tr></thead><tbody><tr id="product-723"><td><a href="https://www.amazon.com/dp/B0198XNF6U?tag=pointerclicke-20&amp;linkCode=ogi&amp;th=1&amp;psc=1" target="_blank" rel="nofollow noopener noreferrer"><img src="https://m.media-amazon.com/images/I/419R0211LBL.jpg" data-src="https://m.media-amazon.com/images/I/419R0211LBL.jpg" alt="Sceptre 65 Inches 4K UHD LED TV"></a></td><td><a href="https://www.amazon.com/dp/B0198XNF6U?tag=pointerclicke-20&amp;linkCode=ogi&amp;th=1&amp;psc=1" target="_blank" rel="nofollow noopener noreferrer">Sceptre 65 Inches 4K UHD LED TV</a></td><td><div><ul><li>Bright LED display and sharp contrast</li><li>65-inch 4K UHD, HDR and MEMC120</li><li>HDMI ports, component ports, optical and line audio outputs</li></ul></div></td><td><a href="https://www.amazon.com/dp/B0198XNF6U?tag=pointerclicke-20&amp;linkCode=ogi&amp;th=1&amp;psc=1" target="_blank" rel="nofollow noopener noreferrer" data-style="">Check Price On Amazon</a></td></tr><tr id="product-724"><td><a href="https://www.amazon.com/dp/B07PW4M13Y?tag=pointerclicke-20&amp;linkCode=ogi&amp;th=1&amp;psc=1" target="_blank" rel="nofollow noopener noreferrer"><img src="https://m.media-amazon.com/images/I/51S-2IYjHFL.jpg" data-src="https://m.media-amazon.com/images/I/51S-2IYjHFL.jpg" alt="Sceptre 50"></a></td><td><a href="https://www.amazon.com/dp/B07PW4M13Y?tag=pointerclicke-20&amp;linkCode=ogi&amp;th=1&amp;psc=1" target="_blank" rel="nofollow noopener noreferrer">Sceptre 50″ 4K UHD Ultra Slim LED TV</a></td><td><div><ul><li>50-inch screen that supports 4K UHD</li><li>Mobile High-Definition Link (MHL) to stream videos from a smartphone</li><li>Stunning colors and image contrast</li></ul></div></td><td><a href="https://www.amazon.com/dp/B07PW4M13Y?tag=pointerclicke-20&amp;linkCode=ogi&amp;th=1&amp;psc=1" target="_blank" rel="nofollow noopener noreferrer" data-style="">Check Price On Amazon</a></td></tr></tbody></table><h3>Input ports</h3><p>Another thing that turns a regular dumb TV into a good dumb TV is the number of input ports it has. Since your dumb TV relies on external devices for display content, you will want several ports to ensure that you can plug in all your devices.</p><p>Having several input ports will allow you to switch between devices without having to physically unplug and plug them. Reaching behind your TV to do this can be very inconvenient.</p><p>HDMI ports are what most external devices today use. You will want at least 3 HDMI ports on your dumb TV. Having more HDMI ports will be better, especially if you’re thinking about getting a few more external devices.</p><h3>Monitors</h3><p>When looking for a good dumb TV, people often make the mistake of looking in the monitor category. A monitor may work for you, but there are a couple of reasons why they may not work as well as a TV.</p><p><a href="https://pointerclicker.com/how-to-clean-a-matte-monitor/" target="_blank" rel="noopener noreferrer">Monitor displays</a> are relatively darker than TV displays. This is because monitors are designed for people who sit up close.</p><p>It may not be a problem when you’re watching a movie in a dark room. If you watch with the windows open during the day, however, your monitor won’t be able to produce enough brightness to give you pleasant viewing experience.</p><p>Monitors also do not come with a TV tuner. This will be a problem if you’re thinking about watching local channels.</p><h2><span id="Does_a_great_dumb_TV_exist_in_2020">Does a great dumb TV exist in 2020?</span></h2><p>The short answer is yes. Although smart TVs are more popular, there are still a few great dumb TVs being manufactured.</p><p>You can visit your local electronic shop or search for one online. Below are some of our dumb TV recommendations.</p><h2><span id="Editors_recommendations">Editor’s recommendations</span></h2><p>We’ve put together a shortlist of our top dumb TV picks. Take a moment to review each one so you can make a better decision when you plan to make a purchase.</p><h3>1. Sceptre 50” 4K UHD (U518CV-UM)</h3> <p>Sceptre sells quite a number of dumb TVs as well as smart TVs. This particular one has a large 50-inch screen that supports 4K UHD.</p><p>It also comes with Mobile High-Definition Link (MHL) that allows you to stream videos from your smartphone. Sceptre also boasts about its stunning colors and image contrast.</p><h3 id="title"><span id="productTitle">2. Sceptre 65 inches 4K LED TV (U658CV-UMC)</span></h3><div><div data-aawp-product-id="B0198XNF6U" data-aawp-product-title="Sceptre 65 Inche 4K UHD LED TV 3840x2160 MEMC 120 Ultra Thin HDMI 2.0 Upscaling U658CV-UMC 2018" data-aawp-click-tracking="true"> <p><span>Sale</span></p><p><a href="https://www.amazon.com/dp/B0198XNF6U?tag=pointerclicke-20&amp;linkCode=ogi&amp;th=1&amp;psc=1" title="Sceptre 65 Inche 4K UHD LED TV 3840x2160 MEMC 120 Ultra Thin HDMI 2.0 Upscaling U658CV-UMC, 2018" rel="nofollow noopener" target="_blank"> <img src="https://m.media-amazon.com/images/I/41Q8eA+4zwL.jpg" data-src="https://m.media-amazon.com/images/I/41Q8eA+4zwL.jpg" alt="Sceptre 65 Inche 4K UHD LED TV 3840x2160 MEMC 120 Ultra Thin HDMI 2.0 Upscaling U658CV-UMC, 2018"> </a></p></div></div><p>If you want to step it up, you can opt for the 65-inch 4K UHD TV from Sceptre. It is going to cost you more, but it also comes with additional features.</p><p>It has a bright <a href="https://pointerclicker.com/can-you-use-a-laser-pointer-on-a-tv-screen/" target="_blank" rel="noopener noreferrer">LED display</a> and a sharp contrast. Its UHD upscaling will enhance your SD or HD videos so they display excellently in 4K. It also comes with HDR and MEMC120 to give you the best viewing experience.</p><p>This particular TV also has great connectivity options. HDMI ports, component ports, optical and line audio outputs. You won’t be needing additional ports with this TV.</p><h2><span id="What_is_the_dumb_TV">What is the dumb TV?</span></h2><p>When you walk through the video section in an electronic shop, you’re going to find an array of different TVs. Most of the newer ones you will have large high definition screens. And almost all of them are going to be smart TVs.</p><p>Smart TVs have taken over the home entertainment industry by storm. To the point where it has become more difficult to find one that does not have smart features. TVs lacking smart features are also called dumb TVs.</p><p>Dumb TVs are display screens with a built-in TV tuner. They also come with different input ports where you can input video information from an HDTV antenna, Blu-ray player, etc. The most common input ports are HDMI and AV.</p><p>What makes a dumb TV “dumb”? The difference between a dumb TV and a smart TV is that the former does not come with an operating system. It relies on external devices to convert data into video information that it can display.</p><p>All televisions that were manufactured before the invention of smart TVs are dumb TVs. Dumb TVs are still being manufactured today for various reasons, although they are less popular.</p><p>Smart TVs, on the other hand, are more like smartphones or computers. They come with an operating system and a handful of pre-installed apps. You can connect them directly to the internet and stream videos on YouTube, Netflix, and other popular platforms.</p><h2><span id="Why_do_people_need_a_dumb_TV_without_smart_features">Why do people need a dumb TV without smart features?</span></h2><p>As you learn more about what smart TVs offer, you may start to wonder why anyone would want to settle for a dumb TV. Smart TVs are more convenient, and they offer so many useful features.</p><p>While the popularity of smart TV increases, there are quite a few people who still prefer dumb TVs. There are a few reasons why you might opt to get a dumb TV. Let’s take a closer look at each of those reasons in more detail.</p><h3>Security and privacy</h3><p>When it comes to the internet, security and privacy are huge topics. There have been countless horror stories that resulted from having personal devices connected to the internet. If you’re connected, there is always going to be some sort of risk.</p><p>Smart TVs are said to be one of the most vulnerable to hacking and data theft. The FBI has even issued a warning of the risks.</p><p>One of the reasons for this is that smart TVs use automatic content recognition or ACR. ACR gathers information about what you watch and sends it back to the manufacturer. With this information, more relative ads can be shown when you’re browsing for something to watch.</p><p>Unfortunately, there are times when a third party receives the information captured by ACR. These third parties can do whatever they want with that information.</p><p>Many newer smart TVs also come with webcams and microphones. These can be used by hackers to spy on you while you’re watching your favorite show.</p><p>Many people are aware of the dangers of this. This is one of the main reasons there are still quite a few people who opt to get dumb TVs instead of TVs with smart features.</p><h3>Better set-top and stick options</h3><p>Another reason why people purchase dumb TVs is that they prefer to use other TV stick and set-top devices. These devices don’t cost a lot of money and they often work better than smart TVs.</p><p>A few of the more popular ones are the <span><a href="https://www.amazon.com/dp/B075XLWML4?tag=pointerclicke-20&amp;linkCode=ogi&amp;th=1&amp;psc=1" title="Roku Streaming Stick+" target="_blank" rel="nofollow noopener" data-aawp-product-id="B075XLWML4" data-aawp-product-title="Roku Streaming Stick+ | HD/4K/HDR Streaming Device with Long-range Wireless and Voice Remote with TV Controls  updated for 2019" data-aawp-click-tracking="true">Roku Streaming Stick+</a>&nbsp;<a href="https://www.amazon.com/dp/B075XLWML4?tag=pointerclicke-20&amp;linkCode=ogi&amp;th=1&amp;psc=1" title="Roku Streaming Stick+" target="_blank" rel="nofollow noopener" data-aawp-product-id="B075XLWML4" data-aawp-product-title="Roku Streaming Stick+ | HD/4K/HDR Streaming Device with Long-range Wireless and Voice Remote with TV Controls  updated for 2019" data-aawp-click-tracking="true"><span></span></a></span>, <span><span>No products found.</span></span>, <span><a href="https://www.amazon.com/dp/B0791TX5P5?tag=pointerclicke-20&amp;linkCode=ogi&amp;th=1&amp;psc=1" title="Amazon Fire TV Stick" target="_blank" rel="nofollow noopener" data-aawp-product-id="B0791TX5P5" data-aawp-product-title="Fire TV Stick streaming media player with Alexa built in includes Alexa Voice Remote HD easy set-up released 2019" data-aawp-click-tracking="true">Amazon Fire TV Stick</a>&nbsp;<a href="https://www.amazon.com/dp/B0791TX5P5?tag=pointerclicke-20&amp;linkCode=ogi&amp;th=1&amp;psc=1" title="Amazon Fire TV Stick" target="_blank" rel="nofollow noopener" data-aawp-product-id="B0791TX5P5" data-aawp-product-title="Fire TV Stick streaming media player with Alexa built in includes Alexa Voice Remote HD easy set-up released 2019" data-aawp-click-tracking="true"><span></span></a></span>, and <span><span>No products found.</span></span>. There are many more options in the market to choose from.</p><p>These devices connect to your WI-FI and allow you to stream content on popular platforms such as Netflix and YouTube.</p><p>All these devices have seamless UI’s and tons of different apps and features. They also come with <a href="https://pointerclicker.com/presentation-pointers-lcd-tv-screens/" target="_blank" rel="noopener noreferrer">remote controls</a> that support voice commands.</p><p>One feature many people find very useful in these devices is the casting feature. It allows you to use your TV as a larger display for your smartphone. You can play videos, view images, and browse the web on your smartphone and have it cast onto your TV.</p><p>You should also remember that technology is advancing a lot quicker than ever before. Your brand-new smart TV will be outdated in just a couple of years. The only way you’re going to be able to keep up with new features and security fixes is to buy a new TV.</p><p>You won’t need to replace your dumb TV to stay updated. All you need to do is purchase the latest stick or set-top device. You can get brand-new ones that already come with voice control for an affordable price.</p><p>Check out this video to learn more about stick and set-top devices.</p><div title="4K Streaming Device Round Up 2019: Apple TV vs Chromecast vs Roku vs Fire TV, Which is best for you?"><div id="WYL_H2Bq9X3a41A"><div id="lyte_H2Bq9X3a41A" data-src="https://pointerclicker.com/wp-content/plugins/wp-youtube-lyte/lyteCache.php?origThumbUrl=https%3A%2F%2Fi.ytimg.com%2Fvi%2FH2Bq9X3a41A%2Fhqdefault.jpg"><div><p>4K Streaming Device Round Up 2019: Apple TV vs Chromecast vs Roku vs Fire TV, Which is best for you?</p></div></div></div></div><h3>Interface issues on smart TVs</h3><p>When you use your smartphone or computer, you have two main input sources: pointing and typing. You don’t have either of those in a smart TV interface.</p><p>For the most part, you will use your TV’s remote to click through menus and an onscreen keyboard for typing. It will take you quite a number of button presses to type something into your TV’s search service.</p><p>Some smart TVs also come with poorly designed interfaces. It will take a lot of time trying to navigate around the interface.</p><p>On the other hand, the interfaces that come with newer sticks and set-box devices are more seamless. They also include voice commands and many support keyboard input from your smartphone.</p><h3>Unreliable apps on smart TVs</h3><p>App developers today need to work harder when it comes to compatibility. They need to make sure apps work well with smartphones, browsers, stick and set-top devices, smart TVs and more. Unfortunately, smart TVs are often the last priority.</p><p>This leaves smart TVs with unreliable apps that may crash or freeze. Older smart TVs may also not be compatible with app updates.</p><h2><span id="Conclusion">Conclusion</span></h2><p>Although smart TVs are the most popular choice, there are still a few reasons why you might opt to get a dumb TV. The good news is that there are still quite a few of them being manufactured.</p><p>It may be more challenging to find a good dumb TV, but there’s a good chance a few of them are being sold at your nearest electronics shop. If you want more options, finding them online will be your best bet.</p><p>You can check out online shops such as Amazon, BestBuy, Walmart, and Costco. You may also visit manufacturer websites and look through their …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pointerclicker.com/best-dumb-tv/">https://pointerclicker.com/best-dumb-tv/</a></em></p>]]>
            </description>
            <link>https://pointerclicker.com/best-dumb-tv/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24666968</guid>
            <pubDate>Fri, 02 Oct 2020 20:59:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Startup Hiring 101: A Founder's Guide]]>
            </title>
            <description>
<![CDATA[
Score 105 | Comments 13 (<a href="https://news.ycombinator.com/item?id=24666580">thread link</a>) | @ivankirigin
<br/>
October 2, 2020 | https://www.notion.so/Startup-Hiring-101-A-Founder-s-Guide-946dad6dd9fd433abdd12338a83e931f | <a href="https://web.archive.org/web/*/https://www.notion.so/Startup-Hiring-101-A-Founder-s-Guide-946dad6dd9fd433abdd12338a83e931f">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.notion.so/Startup-Hiring-101-A-Founder-s-Guide-946dad6dd9fd433abdd12338a83e931f</link>
            <guid isPermaLink="false">hacker-news-small-sites-24666580</guid>
            <pubDate>Fri, 02 Oct 2020 20:12:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[“Fungi Can Teach Us a New Way of Looking at the World”]]>
            </title>
            <description>
<![CDATA[
Score 144 | Comments 56 (<a href="https://news.ycombinator.com/item?id=24666521">thread link</a>) | @jseliger
<br/>
October 2, 2020 | https://www.spiegel.de/international/zeitgeist/mushroom-expert-merlin-sheldrake-fungi-can-teach-us-a-new-way-of-looking-at-the-world-a-a3dd9530-dc15-4aa9-bb03-b23e1adc7e2f | <a href="https://web.archive.org/web/*/https://www.spiegel.de/international/zeitgeist/mushroom-expert-merlin-sheldrake-fungi-can-teach-us-a-new-way-of-looking-at-the-world-a-a3dd9530-dc15-4aa9-bb03-b23e1adc7e2f">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-article-el="body">
<section data-app-hidden="true">


</section>
<section>
<div>
<figure data-component="Image" data-settings="{&quot;id&quot;:&quot;14a48b6b-06ab-4edb-83c8-8a827e78971b&quot;, &quot;zoomable&quot;:true,&quot;zoomId&quot;:&quot;cb1fb386-110c-428a-8794-8acaa2a62941&quot;}">
<p><span>
<span data-image-el="aspect">
<span>
<img data-image-el="img" src="https://cdn.prod.www.spiegel.de/images/14a48b6b-06ab-4edb-83c8-8a827e78971b_w948_r1.77_fpx39.41_fpy49.97.jpg" srcset="https://cdn.prod.www.spiegel.de/images/14a48b6b-06ab-4edb-83c8-8a827e78971b_w520_r1.77_fpx39.41_fpy49.97.jpg 520w, https://cdn.prod.www.spiegel.de/images/14a48b6b-06ab-4edb-83c8-8a827e78971b_w948_r1.77_fpx39.41_fpy49.97.jpg 948w" width="948" height="536" sizes="948px" title="Merlin Sheldrake: &quot;When food becomes scarce, they some fungi can switch to a hunting mode.&quot;" alt="Merlin Sheldrake: &quot;When food becomes scarce, they some fungi can switch to a hunting mode.&quot;">
</span>
</span>
</span>

</p>
<figcaption>
<p><strong>Merlin Sheldrake:</strong> "When food becomes scarce, they some fungi can switch to a hunting mode."</p>
<span>
Foto: Andrea Artz / DER SPIEGEL
</span>
</figcaption>
</figure>
</div><div>
<p><em>Merlin Sheldrake, 32, earned his Ph.D. in tropical ecology at the University of Cambridge for his research into underground fungal networks in the tropical forests of Panama. Since then, he has not lost his fascination for them. He is the author of "Entangled Life: How Fungi Make Our Worlds, Change Our Minds and Shape Our Futures," which was published in early September.</em></p>


<div>
<p><strong>DER SPIEGEL:</strong> Dr. Sheldrake, we are here in London's Hampstead Heath. This place, you write in your book, means more to you than any other. Why is that?</p><p><strong>Sheldrake:</strong>&nbsp;I grew up here. This is where I learned to walk. Later, I climbed trees here, and still later, I had parties with friends. And my interest in nature has been incubated by this place.</p><p><strong>DER SPIEGEL:&nbsp;</strong>Your interest in nature in general, or fungi in particular?</p><p><strong>Sheldrake:&nbsp;</strong>Both. I've always been particularly interested in how things transform, how they grow and decompose. I was amazed how piles of leaves disappear over time. How did this transformation come about without me being able to see anything? Composting, I understood, is largely the work of fungi.</p>
</div>

<div>
<p><strong>DER SPIEGEL:&nbsp;</strong>For most people, nature is primarily made up of plants and animals. What role do fungi play in between those two realms?</p><p><strong>Sheldrake:&nbsp;</strong>Fungi are of enormous importance. The basic principle of ecology is the relationships between organisms - and fungi form connections between organisms and so embody this idea.</p><p><strong>DER SPIEGEL:&nbsp;</strong>If fungi are so important, why don’t we see them all over the place?</p><p><strong>Sheldrake:</strong>&nbsp;Oh, fungi are everywhere. Just take this leaf: Between tens and hundreds of species of fungi live on and in it. No plant has ever been found in nature which does not have fungi in its leaves and in its shoots. Or take the roots of the grass we are walking on, the rotting twigs, the soil under our feet: There are fungi everywhere. You have yeast all over your body, in the lining of your ears, in your nostrils. Even in the air: At this moment, you are breathing fungi. Fifty million tons of fungal spores are floating in the atmosphere, the largest source of living particles in the air. And they change the weather by causing water droplets to form.</p>
</div>


<div>
<p><strong>DER SPIEGEL:&nbsp;</strong>If fungi are so ubiquitous, why we know so little about them?</p><p><strong>Sheldrake:</strong>&nbsp;There are many reasons. The most obvious one is access. The fungus we see is nothing more than the fruit of the organism itself. The mycelium network that belongs to it is buried in the ground. It is as if we only saw acorns for one moment every year, but we couldn't see the magnificent oak trees.</p><p><strong>DER SPIEGEL:</strong>&nbsp;Do even scientists underestimate the importance of fungi?</p><p><strong>Sheldrake:</strong>&nbsp;They did so for a long time, at least. Until the 1960s, fungi were thought to be plants. Only then did they gain taxonomical independence. The new sequencing techniques have changed that. Today, we can read the DNA in every teaspoon of soil and find out who is there.</p>
</div>


<section data-area="contentbox">
<div>
<p><span>DER SPIEGEL 39/2020</span></p><figure data-component="Image" data-settings="{&quot;id&quot;:&quot;996d8f55-bef6-471b-ad10-5c788bf27a9e&quot;, &quot;zoomable&quot;:false,&quot;zoomId&quot;:&quot;6898059e-48ed-4593-8f39-f3151744b819&quot;}">
<span>
<span data-image-el="aspect">
<span>

<img data-image-el="img" src="https://cdn.prod.www.spiegel.de/images/996d8f55-bef6-471b-ad10-5c788bf27a9e_w568_r0.7571817357121258_fpx50.98_fpy47.69.jpg" srcset="https://cdn.prod.www.spiegel.de/images/996d8f55-bef6-471b-ad10-5c788bf27a9e_w284_r0.7571817357121258_fpx50.98_fpy47.69.jpg 284w, https://cdn.prod.www.spiegel.de/images/996d8f55-bef6-471b-ad10-5c788bf27a9e_w335_r0.7571817357121258_fpx50.98_fpy47.69.jpg 335w, https://cdn.prod.www.spiegel.de/images/996d8f55-bef6-471b-ad10-5c788bf27a9e_w568_r0.7571817357121258_fpx50.98_fpy47.69.jpg 568w" width="568" height="750" sizes="568px" loading="lazy" data-srcset="https://cdn.prod.www.spiegel.de/images/996d8f55-bef6-471b-ad10-5c788bf27a9e_w284_r0.7571817357121258_fpx50.98_fpy47.69.jpg 284w, https://cdn.prod.www.spiegel.de/images/996d8f55-bef6-471b-ad10-5c788bf27a9e_w335_r0.7571817357121258_fpx50.98_fpy47.69.jpg 335w, https://cdn.prod.www.spiegel.de/images/996d8f55-bef6-471b-ad10-5c788bf27a9e_w568_r0.7571817357121258_fpx50.98_fpy47.69.jpg 568w">
</span>
</span>
</span>
</figure>

</div>
</section>
<div>
<p><strong>DER SPIEGEL:&nbsp;</strong>And? What does one find?</p><p><strong>Sheldrake:</strong>&nbsp;The kingdom of fungi is vast. There are six times more species of fungi than of plants, and only 6 to 8 percent of them have even been described. We still know so little! ! Just one thing is clear: There are many ways to be a fungus.</p><p><strong>DER SPIEGEL:</strong>&nbsp;Is perhaps the lack of appreciation for fungi because of the fact that they are not very nutritious and often even poisonous?</p><p><strong>Sheldrake:</strong>&nbsp;Many people think like that. But in fact, many mushrooms contain important minerals and they have a high content of antioxidants. They produce an amazing variety of substances that affect cancer, viruses or our immune system. And mushrooms are high in protein. Truffles are a good example of an edible fungus. After all, they want to be eaten. Truffles sit deep in the ground where no wind can spread their spores. They attract animals with a very subtle mixture of odors, so that these animals then eat them and spread their spores.</p><p><strong>DER SPIEGEL:</strong>&nbsp;Some mushrooms lure us with substances that have a direct effect on our consciousness ...</p><p><strong>Sheldrake:</strong>&nbsp;Yes, about 200 fungal species contain psilocybin, a substance that people have been interested in because of its strong psychedelic effects.</p>
</div>

<div>
<p><strong>DER SPIEGEL:</strong>&nbsp;Such mushrooms cause hallucination and change the way we think. How do mushrooms benefit from making psychedelic drugs for humans?</p><p><strong>Sheldrake:</strong>&nbsp;We don’t know. The first mushrooms to make psilocybin lived 75 million years ago, long before humans arose. But the receptors that this substance binds to can also be found in many animals. Does psilocybin change the behavior of certain insects in a way that induces them to spread fungal spores? Or do they change the behavior of insects in a way that deters them from eating the mushrooms?</p><p><strong>DER SPIEGEL:</strong>&nbsp;Have you personally tried the effects of psychedelic mushrooms?</p><p><strong>Sheldrake:</strong>&nbsp;Yes, under their influence I realized that most of my consciousness was unknown to me. It was as if I had spent my life in a garden until then, and now I suddenly discovered that this garden has a gate through which I can enter a strange and wonderful forest, that was largely unknown to me.</p><p><strong>DER SPIEGEL:</strong>&nbsp;Does the gate disappear once the effects of psilocybin fade away?</p><p><strong>Sheldrake:</strong>&nbsp;Not necessarily. Once you know that this forest exists, it is much easier to find your way into it.</p><p><strong>DER SPIEGEL:</strong>&nbsp;You even took part in a scientific study.</p><p><strong>Sheldrake:</strong>&nbsp;Yes, though it was LSD tested in that study. But both substances have similar effects. Among other things, it was to be examined whether LSD promotes creativity. Each participant had to name a problem they were currently working on and, under the influence of LSD, &nbsp;we were to try to solve that problem.</p><p><strong>DER SPIEGEL:</strong>&nbsp;And?</p>
</div>

<div>
<p><strong>Sheldrake:</strong>&nbsp; I found the effects of LSD very helpful in allowing me to approach questions from new angles and imagine the relationships between plant and fungus from different points of view.</p><p><strong>DER SPIEGEL:</strong>&nbsp;You attribute cognitive abilities to fungi. What makes you think so?</p><p><strong>Sheldrake:</strong>&nbsp;I've been thinking about this for a while. I'm interested in the way fungi perceive their environment and how they react to it. Information is continuously flowing through their decentralized bodies.</p><p><strong>DER SPIEGEL:</strong>&nbsp;What do fungi perceive?</p><p><strong>Sheldrake:</strong>&nbsp;Most importantly, they have extremely diverse chemical sensors. A fungus can be seen as a large, chemically sensitive membrane, so to speak, as one big olfactory epithelium. But many mushrooms can also perceive light and they are sensitive to gravity, to changes in temperature and to changes in pressure.</p><p><strong>DER SPIEGEL:</strong>&nbsp;So the fungi under our feet can sense that we are here?</p><p><strong>Sheldrake:</strong> Some fungi would detect the pressure of our steps, yes. And now the question is, how do they process all this information without a brain and how do they translate it into behavior, into action?</p><p><strong>DER SPIEGEL:</strong>&nbsp;Action? Behavior? What do fungi do?</p>
</div>

<div>
<p><strong>Sheldrake:</strong>&nbsp;Fungi are quite active. Take hunting, for example.</p><p><strong>DER SPIEGEL:</strong>&nbsp;Excuse me? Mushrooms can hunt?</p><p><strong>Sheldrake:</strong>&nbsp;Yes. When food becomes scarce, some fungi can switch to a hunting mode. They build traps consisting of sticky loops or poisonous droplets. And with special substances, they lure nematodes into these traps.</p><p><strong>DER SPIEGEL:</strong>&nbsp;Is this really "behavior" of the kind we see in animals?</p><p><strong>Sheldrake:</strong>&nbsp;Well, we can run away from danger, fungi have to face it. Therefore, they defend themselves with the help of chemicals, or they regenerate. But that doesn't change the fact that fungi do make decisions, just as we do.</p><p><strong>DER SPIEGEL:</strong>&nbsp;What kind of decisions?</p><p><strong>Sheldrake:</strong>&nbsp;Fungi have many options: where to grow, what to eat, what nutrients to transport, whether to withdraw and when to hunt nematodes. Each fungus forms thousands of so-called hyphae - tiny tubes that can either grow, divide or fuse.</p><p><strong>DER SPIEGEL:</strong>&nbsp;If fungi make decisions, are they also capable of solving problems?</p>
</div>

<div>
<p><strong>Sheldrake:</strong>&nbsp;Absolutely. For example, their growth follows very efficient navigation algorithms. There are various experiments in which fungi very rapidly found the shortest route through a maze.</p><p><strong>DER SPIEGEL:</strong>&nbsp;If fungi are, as you claim, complex information processing networks, are they essentially a kind of brain?</p><p><strong>Sheldrake:</strong>&nbsp;No, I wouldn’t say that. But you are right: Neurons are tip-growing, electrically excitable, network-forming cells. And so are fungal cells.</p><p><strong>DER SPIEGEL:</strong>&nbsp;So mushrooms have a form of intelligence?</p><p><strong>Sheldrake:</strong>&nbsp;It depends on your definition of "intelligence." In a broad sense, all organisms show intelligence, albeit to different degrees. The study of cognition and intelligence arose from the study of the human mind. This resulted in a very human- and brain-centered view. I find it refreshing to extend these considerations to organisms that do not have brains. We shouldn't use ourselves as the yardstick to judge everything else in this world.</p><p><strong>DER SPIEGEL:</strong>&nbsp;Is there still a lot to discover in the field of fungi cognition?</p><p><strong>Sheldrake:&nbsp;</strong>Absolutely. Little is known about how fungi coordinate their behavior. We don't know the mechanisms by which they pass signals around. We've not fully understood the basic biology of mycelial growth.</p>
</div>

<div>
<p><strong>DER SPIEGEL:&nbsp;</strong>But we do know a lot about the symbiotic relationship between mushrooms and plants …</p><p><strong>Sheldrake:&nbsp;</strong>… exactly, via the mycorrhiza, through which the fungus supplies the plant with minerals such as nitrogen and phosphorus, and the plant in turn provides the fungus with energy-rich sugars.</p><p><strong>DER SPIEGEL:</strong>&nbsp;How important is this symbiosis? If all fungi were wiped out in this forest floor, could the trees survive?</p><p><strong>Sheldrake:</strong>&nbsp;No. They would be prone to disease, just as we would be if it weren't for the bacteria in our intestines. This microbiome keeps us healthy. In this sense, soil is sort of the gut of our planet.</p><p><strong>DER SPIEGEL:</strong>&nbsp;Many ecologists are enthusiastic about the "Wood Wide Web," by which trees are mysteriously connected via the fungi in the soil and allegedly even communicate via this …</p></div></div></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.spiegel.de/international/zeitgeist/mushroom-expert-merlin-sheldrake-fungi-can-teach-us-a-new-way-of-looking-at-the-world-a-a3dd9530-dc15-4aa9-bb03-b23e1adc7e2f">https://www.spiegel.de/international/zeitgeist/mushroom-expert-merlin-sheldrake-fungi-can-teach-us-a-new-way-of-looking-at-the-world-a-a3dd9530-dc15-4aa9-bb03-b23e1adc7e2f</a></em></p>]]>
            </description>
            <link>https://www.spiegel.de/international/zeitgeist/mushroom-expert-merlin-sheldrake-fungi-can-teach-us-a-new-way-of-looking-at-the-world-a-a3dd9530-dc15-4aa9-bb03-b23e1adc7e2f</link>
            <guid isPermaLink="false">hacker-news-small-sites-24666521</guid>
            <pubDate>Fri, 02 Oct 2020 20:04:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Data from 70 Offer Negotiations Using a Career Agent]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24665710">thread link</a>) | @brianliou91
<br/>
October 2, 2020 | https://www.withralph.com/blog/salary-negotiation-report-how-to-negotiate-salary | <a href="https://web.archive.org/web/*/https://www.withralph.com/blog/salary-negotiation-report-how-to-negotiate-salary">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content"><div><div><p>Consider two graduating PhDs who are transitioning into industry. Both are incredibly busy finishing their research, writing their dissertation, and teaching. Both spend the same amount of time recruiting for industry roles - and both join the same company in a similar role. PhD #1, let’s call her Jane,  soon discovers she is being paid less than her peers for the same work and has a boss that won’t promote her. She’s forced to work doubly hard just to achieve parity with her co-workers, or else leave and start over somewhere new, wasting a year. PhD #2, let’s call her Joanne, in contrast, is happy with her job. She is paid similarly to her peers and has a clear path for growth at the company. What did Joanne do better?</p><p>We have now advised 71 PhDs to follow this second, better track and we understand what defines these trajectories. We hope this salary negotiation report educates similar individuals how to negotiate salary. The difference between these two individuals is not merit, nor is it about who interviewed better or does better work. The difference is in how they think and negotiate. Joanne understands that she is joining a business. Compensation and promotions are respectfully taken by her self-advocacy, not given by the company. Jane assumes that the company will take care of her once she proves herself and so does not ask for much at the beginning. Joanne sets expectations and gives feedback to her manager for a productive working relationship. This is a <em>virtuous</em> cycle: you ask and your manager sets the higher expectations that enable you to work upwards. Jane is reticent and does not want to damage relationships. This is a <em>vicious</em> cycle: you don’t ask and your manager doesn’t expect more, so you stagnate and it becomes even harder to advance. </p><p>This power of setting expectations is evident at the very beginning of your relationship with your employer, in the offer negotiation. You haven’t started work yet, but simply setting higher expectations from the outset enables you to get paid more and begins the positive, virtuous cycle. Here we quantify the impact of negotiating using Ralph by looking at the data from our first 71 PhD clients. Most of them came from computation-heavy PhDs and were transitioning into their first industry role in engineering or data science, or a research role in technology or quantitative finance. Our results reveal just how beneficial it can be to advocate for yourself, and how beneficial a Career Agent can be to advise you through this process of multiple offers and changes. Because while most people know they <em>should</em> negotiate, they don’t know how <em>far</em> to negotiate. Our data gives you insight into just how much you’re worth and how much room there is for offer changes. </p><p><img src="https://landen.imgix.net/blog_VkwwMuKsXVDkwaYZ/assets/KnTGkpeVnsGRSckK.jpg" alt="Tables.jpg"></p><p><img src="https://landen.imgix.net/blog_VkwwMuKsXVDkwaYZ/assets/uOGQqqzabfskEZzm.jpg" alt="Visualizations.jpg"></p><p><strong>High Level Results</strong></p><p>On average, our clients have increased their initial offer by 30% through negotiation using our insight and advising. This increase is calculated from changes in base salary, equity, and any annual, signing, or relocation bonus changes. This increase represents an average $75K more in the offer. As expected, having offers from multiple companies resulted in a larger increase from the baseline offer; however, even if clients only had an offer from a single company, they were still able to secure an average 20% increase from their initial offer. Having four offers resulted in a dramatic increase (56%) in the negotiated accepted offer.&nbsp;</p><p><strong>Negotiating You Are Likely Not Doing</strong></p><p>The majority of our clients (52%) changed their offer twice. This means they negotiated an increase once and then negotiated <strong>another</strong> increase. Usually this second change would occur after the company said no to any further changes. We were able to advise our clients to keep advocating for themselves and set initial expectations high, resulting in an average total increase of 39%.  When the offer changed once, it increased on average 18%. While the majority of this data comes from before COVID-19, we have advised 9 clients during COVID-19 with similar results. Our largest negotiated offer ever was achieved in March 2020 (<a href="https://www.withralph.com/blog/negotiated-a-143-offer-increase">the story here</a>). Our data from working with 70+ clients is clear: you can negotiate significant (and often multiple) increases in your offer if you know your true, competitive worth.&nbsp;</p><p>These results we hope debunk three common misconceptions that hinder a candidates' ability to negotiate a strong starting offer and set the stage for your upward growth.&nbsp;</p><p><strong>Misconception #1: Companies pay equally for the same role, level, and location in a new grad offer.</strong></p><p>We have seen two candidates with the same role, level, company, and location have a $35,000 base salary difference. We have seen equity range by more than $900,000 in a 4-year package in public company offers. We have data for new grad Google offers that start as low as ~$180K/year and end as high as ~$550K/year.</p><p>Even independent of a company's intent, if you don't negotiate you will be paid unequally because the highest paid individuals are always negotiating. The squeaky wheel gets the oil. </p><p><strong>Misconception #2: Interviewing with one company at a time is ok.</strong></p><p>The factor that affects your compensation the most is having multiple offers at the same time. Companies are a business. They will pay what they have to, not what they can. Our data shows having two offers increases your compensation by 5% and 4 offers increases your compensation by 56%.&nbsp;Companies interview candidates they have no intention of hiring just to see the market. So should you.</p><p><strong>Misconception #3: The company increased my offer so I’ve successfully negotiated.</strong></p><p>The majority of companies start with a low offer that leaves room for the candidate to negotiate. You should define successfully negotiating as getting a change <strong>after</strong> the company has said no. This is when the negotiation has actually begun. Otherwise, you have just asked and they conceded. There has been no negotiation. Our data shows that it is rare for an offer <strong>not</strong> to change after it is initially given: 90% of offers change from the initial offer after negotiating.</p><p>Almost all of the negotiation challenges PhDs face come from a lack of information and experience in industry. They don’t know what to expect, don’t have time to research, and assume whatever the company says is correct. As our data shows, there is room for negotiating multiple times to achieve a 30% or greater increase in your initial offer. We hope that by sharing our findings, we can help you educate and advocate for yourself and feel more confident to set high expectations even before you begin working at your company. Ask yourself the question: <strong>how do I know I am getting the best offer possible?</strong> You might be surprised to know that you are likely worth a lot more than the initial offer you receive. </p><p>--</p><p>To read more content: <a href="https://www.withralph.com/blog/where-to-start">start here</a></p><p><a href="https://www.withralph.com/blog/where-to-start"></a>To get updated with insights and learnings: <a href="https://www.linkedin.com/company/ralph-inc">Follow us on LinkedIn</a></p><p>Questions? Email hi@withralph.com</p></div></div></div></div>]]>
            </description>
            <link>https://www.withralph.com/blog/salary-negotiation-report-how-to-negotiate-salary</link>
            <guid isPermaLink="false">hacker-news-small-sites-24665710</guid>
            <pubDate>Fri, 02 Oct 2020 18:45:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Scientists develop 'super enzyme' that breaks down plastic faster than ever]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24664510">thread link</a>) | @soperj
<br/>
October 2, 2020 | https://www.cbc.ca/radio/asithappens/as-it-happens-thursday-edition-1.5746442/scientists-develop-super-enzyme-that-breaks-down-plastic-faster-than-ever-1.5746444 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/radio/asithappens/as-it-happens-thursday-edition-1.5746442/scientists-develop-super-enzyme-that-breaks-down-plastic-faster-than-ever-1.5746444">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>A team of international scientists have developed what they call a "super enzyme" that can break down plastic into is original building blocks so it can be recycled infinitely.&nbsp;</p><div><figure><div><p><img alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5746639.1601578207!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/1190912666.jpg"></p></div><figcaption>Plastic waste litters the shoreline in Koattey wetlands on Dec. 14, 2019, in Hithadhoo, Maldives.<!-- --> <!-- -->(Carl Court/Getty Images)</figcaption></figure><p><span><div><div role="button" tabindex="0" title="Scientists develop 'super enzyme' that breaks down plastic faster than ever"><div><p><img src="https://thumbnails.cbc.ca/maven_legacy/thumbnails/91/606/AsItHappens-podcast-640x360.jpg" alt=""></p><p><span>As It Happens</span><span>6:33</span><span>Scientists develop 'super enzyme' that breaks down plastic faster than ever</span></p></div></div></div></span></p><p><span><p><a href="https://www.cbc.ca/radio/asithappens/as-it-happens-thursday-edition-1.5746442/october-1-2020-episode-transcript-1.5748666">Read Story Transcript</a></p>  <p>A team of international scientists have developed what they call a "super enzyme" that can break down plastic into its original building blocks so it can be recycled infinitely.&nbsp;</p>  <p>The team, <a href="https://www.cbc.ca/news/technology/plastic-eating-enzyme-pollution-1.4622923">which made waves in 2018 for engineering&nbsp;a plastic-eating enzyme</a>,&nbsp;has&nbsp;now combined it with a second enzyme to create&nbsp;a "<a href="https://www.eurekalert.org/pub_releases/2020-09/uop-pe092520.php">cocktail</a>" that can break down plastic six times faster.</p>  <p>"The enzymes are really specific to certain types of bonds in the molecular structure of the plastic. This means that it breaks it down into the same starting materials that were used to make the product to begin with," Erika Erickson, a bioengineering researcher at the U.S. Department of Energy's&nbsp;National Renewable Energy Laboratory (NERL), told <em>As It Happens </em>host Carol Off.&nbsp;</p>  <p>"So instead of having an inferior product in the end, you could start with the same starting materials and come back to an equal value plastic water bottle or food package, etc., on&nbsp;the other side, without needing to use petroleum products to get there."</p>  <p>The findings were <a href="https://www.eurekalert.org/pub_releases/2020-09/uop-pe092520.php">published this week in the journal Proceedings of the National Academy of Sciences.</a></p>  <h2>Nature finds a way — and scientists speed it up&nbsp;</h2>  <p>The whole thing began when scientists at NERL and Britain's University of Portsmouth discovered a naturally occurring enzyme in a waste recycling centre in Japan that was helping bacteria break down&nbsp;polyethylene terephthalate&nbsp;(PET), a common plastic developed in the '40s that's used to make&nbsp;water bottles, food packaging, film and more.&nbsp;</p>  <p>"There are natural enzymes that have been evolved to break down plastic," Erickson said. "And if you think about that, it's quite extraordinary that an organism has been able to do this in such a short amount of time."</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5746626.1601577848!/fileImage/httpImage/image.jpeg_gen/derivatives/original_300/plastic-eating-enzymes.jpeg 300w,https://i.cbc.ca/1.5746626.1601577848!/fileImage/httpImage/image.jpeg_gen/derivatives/original_460/plastic-eating-enzymes.jpeg 460w,https://i.cbc.ca/1.5746626.1601577848!/fileImage/httpImage/image.jpeg_gen/derivatives/original_620/plastic-eating-enzymes.jpeg 620w,https://i.cbc.ca/1.5746626.1601577848!/fileImage/httpImage/image.jpeg_gen/derivatives/original_780/plastic-eating-enzymes.jpeg 780w,https://i.cbc.ca/1.5746626.1601577848!/fileImage/httpImage/image.jpeg_gen/derivatives/original_1180/plastic-eating-enzymes.jpeg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5746626.1601577848!/fileImage/httpImage/image.jpeg_gen/derivatives/original_780/plastic-eating-enzymes.jpeg"></p></div><figcaption>Researchers from Britain's University of Portsmouth and the U.S. Department of Energy's National Renewable Energy Laboratory have combined two plastic-eating enzymes to create what they call a 'super enzyme.'<!-- --> <!-- -->(University of Portsmouth)</figcaption></figure></span></p>  <p>However, the natural process is a slow one. So the scientists tweaked the enzyme by adding amino acids to speed things up.</p>  <p>The resulting&nbsp;engineered enzyme, called&nbsp;PETase, could break down&nbsp;one water bottle in a couple months, Erickson estimated — a big step up from the hundreds of years it takes to break down in nature.</p>    <p>Now the team has combined&nbsp;PETase&nbsp;with a second enzyme from the same garbage eating bacteria, called MHETase, making the process even faster. The new super enzyme, Erickson&nbsp;said, could potentially break down one bottle in as little as six weeks.&nbsp;</p>  <p>She admits that's still "a little bit too slow for a real recycling process," but says it's a major step forward to creating a commercially viable system.&nbsp;</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5746648.1601578311!/fileImage/httpImage/image.JPG_gen/derivatives/original_300/erika-erickson.JPG 300w,https://i.cbc.ca/1.5746648.1601578311!/fileImage/httpImage/image.JPG_gen/derivatives/original_460/erika-erickson.JPG 460w,https://i.cbc.ca/1.5746648.1601578311!/fileImage/httpImage/image.JPG_gen/derivatives/original_620/erika-erickson.JPG 620w,https://i.cbc.ca/1.5746648.1601578311!/fileImage/httpImage/image.JPG_gen/derivatives/original_780/erika-erickson.JPG 780w,https://i.cbc.ca/1.5746648.1601578311!/fileImage/httpImage/image.JPG_gen/derivatives/original_1180/erika-erickson.JPG 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5746648.1601578311!/fileImage/httpImage/image.JPG_gen/derivatives/original_780/erika-erickson.JPG"></p></div><figcaption>Erika Erickson is postdoctoral researcher in bioengineering a the U.S. Department of Energy's National Renewable Energy Laboratory.<!-- --> <!-- -->(Werner Slocum/NREL)</figcaption></figure></span></p>  <p>The way plastic is recycled now is not very efficient or cost-effective, says Erickson.</p>  <p>"In&nbsp;mechanical recycling, the plastic gets ground down into small pieces and then melted and then reformed into a new product," she said.</p>  <p>"But in the process of doing that, all of the contaminated dirt or food products or other types of plastic get mixed into that. So the quality of the recycled good is usually quite low compared to the original."</p>  <p>With an enzymatic approach, however, the plastic is recycled in its entirety&nbsp;— turning a bottle, for example, back into the same material used to make the bottle, and potentially creating an infinite loop of recycling.</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5489233.1601578943!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/996523044.jpg 300w,https://i.cbc.ca/1.5489233.1601578943!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/996523044.jpg 460w,https://i.cbc.ca/1.5489233.1601578943!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/996523044.jpg 620w,https://i.cbc.ca/1.5489233.1601578943!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/996523044.jpg 780w,https://i.cbc.ca/1.5489233.1601578943!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/996523044.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5489233.1601578943!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/996523044.jpg"></p></div><figcaption>Workers sort recycling material at the Waste Management Material Recovery Facility in Elkridge, Md.<!-- --> <!-- -->(Saul Loeb/AFP/Getty Images)</figcaption></figure></span></p>  <p>Another big problem with modern recycling is the amount of energy used to collect materials and deliver them to a central location for sorting.&nbsp;</p>  <p>"That won't necessarily be a problem that disappears with a new strategy for recycling," Erickson said.</p>  <p>"The difference, however, would be that the embedded use of fossil fuels for the extraction of petroleum from the Earth, you would lose a lot of that, which is also quite costly.... If we could separate some of the products that we use from that cycle, then the greenhouse gas emissions and fossil fuel utilization would be lower."</p>  <h2>Technology helps — but people have to step up&nbsp;</h2>  <p>The team has touted the potential of this method to one day revolutionize recycling, should it be developed on a commercial scale.&nbsp;</p>  <p>But Erickson notes that technology alone won't fix the problem of plastic pollution.</p>    <p>"It's difficult to convey the ability to sort of shirk off responsibility for our daily choices toward this kind of technology in general&nbsp;…&nbsp;each of us&nbsp;can make a difference in our daily choices," she said.</p>  <p>"And so I hope that people both understand that [with this]&nbsp;technology, we're hoping we can we can make some big impacts and in good directions, but it still comes down to individual choices."</p>  <hr>  <p><em>Written by Sheena Goodyear. Interview produced by Menaka Raman-Wilms and Kate Cornick.&nbsp;</em></p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/radio/asithappens/as-it-happens-thursday-edition-1.5746442/scientists-develop-super-enzyme-that-breaks-down-plastic-faster-than-ever-1.5746444</link>
            <guid isPermaLink="false">hacker-news-small-sites-24664510</guid>
            <pubDate>Fri, 02 Oct 2020 16:52:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Linux Security Hardening and Other Tweaks]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24664507">thread link</a>) | @aytekat
<br/>
October 2, 2020 | https://vez.mrsk.me/linux-hardening.html#hn | <a href="https://web.archive.org/web/*/https://vez.mrsk.me/linux-hardening.html#hn">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

<strong>Linux Security Hardening and Other Tweaks</strong>
<hr>

<p>
Last updated: 08/18/2020,
by <a href="https://twitter.com/blakkheim">@blakkheim</a>

</p><p>
This page lists the changes I make to a vanilla install of Arch Linux
for security hardening, as well as some other changes I find useful.
While Arch is my target platform, most of the changes will work on
any Linux system that's reasonably up to date.

</p><p>
I typically favor security over performance. You may also see suggestions
merely to make something more useful or shave precious seconds off
a wait time. It's not a one-size-fits-all setup, but hopefully certain
pieces of it will be useful.

</p><p>
Arch is worth considering for a few reasons:

</p><ul>
<li><strong>The install size:</strong> The base install is relatively minimal
    compared to a "prebuilt" distro like Fedora or Mint. This lets me focus
    on adding just what I want, rather than constantly trying to strip out
    things I don't need.
</li><li><strong>The kernel:</strong> A common misconception about the Linux
    kernel is that it's secure, or that one can go a long time without
    worrying about kernel security updates. Neither of these are even
    remotely true. New versions of Linux are released almost every week,
    often containing security fixes buried among the many other changes.
    These releases typically
    <a href="https://youtu.be/5PmHRSeA2c8?t=4075">don't</a> make explicit
    mention of the changes having security implications. As a result, many
    "stable" or "LTS" distributions don't know
    <a href="https://web.archive.org/web/20200623161340/https://www.openwall.com/lists/oss-security/2020/06/23/2">which commits</a> should be
    backported to their old kernels, or even that something needs backporting
    at all. If the problem has a public CVE assigned to it, maybe your distro
    will pick it up. Maybe not. Even if a CVE exists, at least in the case
    of Ubuntu and Debian especially, users are often left with kernels full
    of <a href="https://security-tracker.debian.org/tracker/source-package/linux">known holes</a>
    for months at a time. Arch doesn't play the backporting game, instead
    opting to provide the newest stable releases shortly after they come out.
</li><li><strong>The <a href="https://wiki.archlinux.org/index.php/Arch_Build_System">Arch Build System</a></strong>:
    Having enjoyed the
    <a href="https://en.wikipedia.org/wiki/Ports_collection">ports</a>
    system of <a href="https://vez.mrsk.me/freebsd-defaults.html">FreeBSD</a>
    and <a href="https://www.openbsd.org/">OpenBSD</a> for a long time, the ABS
    has been a pleasure to use. It makes building/rebuilding packages easy.
    It makes updating packages easy. It shows how things are actually built
    and with what options. This BSD-borrowed concept makes interacting with
    the package system simple and intuitive.
</li></ul>

Now on to how I set things up.

<hr>
<p>

<strong>Security Hardening</strong>
</p><ul>
  <li><a href="#disks">Disk Layout</a>
  </li><li><a href="#pacman">Pacman</a>
  </li><li><a href="#kern">Kernel Options</a>
  </li><li><a href="#fw">Firewall</a>
  </li><li><a href="#sudo">Sudo</a>
  </li><li><a href="#firejail">Application Sandboxing</a>
  </li><li><a href="#rfk">RFKill (Disable WiFi / Bluetooth)</a>
</li></ul>

<strong>Other Tweaks</strong>
<ul>
  <li><a href="#ntp">NTP (Network Time Protocol)</a>
  </li><li><a href="#mkinit">mkinitcpio</a>
  <!--
  <li><a href="#login"    >Automatic Login</a>
  -->
  </li><li><a href="#pulse">PulseAudio</a>
  </li><li><a href="#misc">Miscellaneous</a>
  </li><li><a href="#closing">Closing</a>
</li></ul>

<hr>

<h2 id="disks">Disk Layout</h2>

This section contains a few tips to consider during your initial disk layout
creation. The concepts should apply to any distrbution.

<p>
To start, consider using
<a href="https://wiki.archlinux.org/index.php/Dm-crypt/Encrypting_an_entire_system">full disk encryption</a>
along with a
<a href="https://wiki.archlinux.org/index.php/LVM">Logical Volume Manager</a>
setup. Disk encryption protects data at rest, while LVM allows for some
flexibility that can be quite useful. A simple disk layout might look like
this:

</p><ul>
<li><code>/dev/sda1</code> (a small, unencrypted
<a href="https://wiki.archlinux.org/index.php/EFI_system_partition">EFI System Partition</a>,
FAT32)
mounted at <code>/efi</code> (assuming this is
a PC with UEFI, otherwise not needed)
</li><li><code>/dev/sda2</code> (a small, unencrypted ext4 partition)
mounted at <code>/boot</code>.
</li><li><code>/dev/sda3</code> (using the rest of the drive space)
as the encrypted
<a href="https://wiki.archlinux.org/index.php/Dm-crypt">LUKS</a> container
for LVM
</li></ul>

Splitting up the logical volumes for different mount points provides some
benefits, including the ability to set mount flags on specific directories.
Consider creating separate logical volumes for <code>/</code>,
<code>/var</code>, and <code>/home</code> in the install. For a typical
desktop, you probably want to give <code>/home</code> most of the disk space.
The other two don't need much unless there's a specific use case in mind.
25GB and 8GB are used in this example. If you need to have a huge database
in <code>/var</code> or something, make adjustments accordingly.

<p>
There are a lot of user-writable directories in Linux, each one providing
an opportunity for attackers to execute their own binaries.
Once the <a href="https://wiki.archlinux.org/index.php/Fstab">fstab</a>
file is created, add the <code>noexec</code> and <code>nodev</code> flags
to <code>/var</code> and <code>/home</code>. Doing so will disallow execution
of binaries on these mount points, as well as prevent interpreting character
or block special devices on them. Two temporary filesystems (<code>/tmp</code>
and <code>/dev/shm</code>) can also be locked down with the same flags by
adding the following:

</p><pre># /etc/fstab
[...]
tmpfs /tmp     tmpfs rw,noexec,nodev,size=1G,mode=1777 0 0
tmpfs /dev/shm tmpfs rw,noexec,nodev,size=1G 0 0
</pre>

Adjust the <code>1G</code> size limit value as desired.

<p>
Once booted into the finished installation, it should look something like this:

</p><pre># <strong>lvs</strong>
  LV   VG   Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  home lvm  -wi-ao---- 189.12g                                                    
  root lvm  -wi-ao----  25.00g                                                    
  var  lvm  -wi-ao----   8.00g                                                    
# <strong>mount | egrep '(lvm|/tmp|shm)' | sort</strong>
/dev/mapper/lvm-home on /home type ext4 (rw,nodev,noexec,relatime)
/dev/mapper/lvm-root on / type ext4 (rw,relatime)
/dev/mapper/lvm-var on /var type ext4 (rw,nodev,noexec,relatime)
tmpfs on /dev/shm type tmpfs (rw,nodev,noexec,size=1048576k)
tmpfs on /tmp type tmpfs (rw,nodev,noexec,relatime,size=1048576k)
</pre>

Another user-writable directory to consider is <code>/run</code>, specifically
the <code>/run/user/$UID</code> directories that systemd spawns when someone
logs in, but their transience is
<a href="https://lists.freedesktop.org/archives/systemd-devel/2015-February/028429.html">annoying</a>
and <a href="https://lwn.net/Articles/436012/">complicated</a>.
I have yet to find the perfect solution there that won't break other things.
<a href="https://wiki.archlinux.org/index.php/FUSE">FUSE</a> is another way
for non-root users to create new mount points and execute binaries. If FUSE
functionality isn't needed, the kernel module can be
<a href="https://wiki.archlinux.org/index.php/Kernel_module#Blacklisting">blacklisted</a>.

<hr>

<h2 id="pacman">Pacman</h2>

Package managers usually don't need much additional configuration.
<a href="https://wiki.archlinux.org/index.php/Pacman">Pacman</a>, the one
Arch uses, is no different. My recommendation for any package manager is
simply to make sure that
<a href="https://web.archive.org/web/20200528161634/https://blog.packagecloud.io/eng/2018/02/21/attacks-against-secure-apt-repositories/">only HTTPS mirrors</a>
are used.

<pre># /etc/pacman.d/mirrorlist

Server = <strong>https</strong>://example.com/[...]/$repo/os/$arch
</pre>

Check the
<a href="https://www.archlinux.org/mirrorlist/all/https/">mirrorlist
generator</a> to see a list of TLS-capable servers near you.

<p>
Using an HTTPS mirror with Pacman is especially important because it
<a href="https://security.archlinux.org/package/pacman">doesn't validate</a>
the package database files and it
<a href="https://en.wikipedia.org/wiki/Privilege_separation">runs everything as root</a>.
HTTPS doesn't mitigate either of these problems, but it is one line of defense
against a MITM attack. I hope the developers will make fixing these two
security issues a priority for the project soon. Other package managers
have been doing it the right way for a long time.

</p><hr>

<h2 id="kern">Kernel Options</h2>

The
<a href="https://www.archlinux.org/packages/extra/x86_64/linux-hardened/">linux-hardened</a>
kernel package in Arch includes some compile-time security improvements
that can't be set at runtime.
If your distribution doesn't have a package for it, applying the
<a href="https://github.com/anthraxx/linux-hardened/releases">patchset</a>
to upstream sources and building your own kernel is pretty easy. If you go
that route, have a look at the
<a href="https://github.com/a13xp0p0v/kconfig-hardened-check">kconfig-hardened-check</a>
script for more compile-time settings to consider.

<!--
<p>
The main issue I've found with linux-hardened is that it's often outdated.
Upstream Linux development moves quickly, so out-of-tree patches will always
require extra work to maintain. Why the (relatively small) patches aren't
upstreamed is unknown to me. There are times when linux-hardened is lagging
multiple versions behind the latest kernel, thus missing out on many
important security fixes. In such a situation, the user must choose between
a more secure kernel with known vulnerabilities and a less secure kernel
with fewer known vulnerabilities. Not a great situation.
-->

<p>
Runtime configuration of the kernel can be done in a number of ways.
Desired flags may be passed on startup in the form of
<a href="https://wiki.archlinux.org/index.php/Kernel_parameters">kernel parameters</a>,
of which there is an
<a href="https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt">extensive list</a>.
Parameters are usually passed by the
<a href="https://wiki.archlinux.org/index.php/Bootloader#Boot_loader">bootloader</a>, so
<a href="https://wiki.archlinux.org/index.php/Kernel_parameters#Configuration">configuration details</a>
vary depending whether the system uses
<a href="https://wiki.archlinux.org/index.php/GRUB">GRUB</a>,
<a href="https://wiki.archlinux.org/index.php/Systemd-boot">systemd-boot</a>,
or something else.

</p><p>
The following options, split up into categories, are worth considering for
security improvements:

</p><pre>l1tf=full,force
spec_store_bypass_disable=on
spectre_v2=on
</pre>

These are addtional mitigations for certain CPU security flaws.
While the <code>mitigations=auto</code> option is used by default in upstream
Linux, some of the mitigations it enables have been "toned down" for
performance reasons.
Examples of this include the
<a href="https://en.wikipedia.org/wiki/Foreshadow_(security_vulnerability)">L1TF</a>
and
<a href="https://en.wikipedia.org/wiki/Microarchitectural_Data_Sampling">Microarchitectural Data Sampling</a>
vulnerabilities, which can't be fully mitigated unless HyperThreading
is disabled.
The
<a href="https://en.wikipedia.org/wiki/Speculative_Store_Bypass">Speculative Store Bypass</a>
vulnerability is only partially mitigated by default, with applications being
allowed to opt-in for protections via prctl or seccomp.
Finally, we enable all mitigations (including those against userspace) for
<a href="https://en.wikipedia.org/wiki/Spectre_(security_vulnerability)">Spectre V2</a>.

<pre>apparmor=1
lsm=lockdown,yama,apparmor
lockdown=<span color="#ff0000"><strong>XXX</strong></span>
</pre>

These enable the
<a href="https://wiki.archlinux.org/index.php/AppArmor">AppArmor</a>, 
<a href="https://www.kernel.org/doc/Documentation/security/Yama.txt">Yama</a>,
and
<a href="https://web.archive.org/web/20200525014035/https://mjg59.dreamwidth.org/55105.html">Lockdown</a>
features, with the lockdown mode left for the reader to choose.
Valid options are <code>integrity</code> and <code>confidentiality</code>,
both described briefly
<a href="https://wiki.archlinux.org/index.php/Security#Kernel_lockdown_mode">here</a>.
Replace <code><strong><span color="#ff0000">XXX</span></strong></code> with
whichever you see fit, or omit this option entirely if the feature isn't
wanted.

<p>
For what it's worth, running in <code>confidentiality</code> mode on my
desktop hasn't caused any problems. Your mileage and use case may vary.
Lockdown will break suspend-to-disk and any out-of-tree kernel modules
like ZFS, as well as
<a href="https://wiki.archlinux.org/index.php/Dynamic_Kernel_Module_Support">DKMS</a> modules.

</p><pre>init_on_alloc=1
init_on_free=1
page_alloc.shuffle=1
slab_nomerge
vsyscall=none
</pre>

This group will instruct the kernel to fill newly allocated pages and heap
objects with zeroes, fill freed pages and heap objects with zeroes, tell the
page allocator to randomize its free lists, disable merging of
<a href="https://en.wikipedia.org/wiki/Slab_allocation">slabs</a>
with similar size, and disable
<a href="https://web.archive.org/web/20200526182112/https://lwn.net/Articles/446528/">vsyscalls</a>
due to their history of making exploits easier.
All five options are all set by default when using the linux-hardened kernel.

<pre>slub_debug=F
</pre>

This enables sanity checks in the
<a href="https://www.kernel.org/doc/Documentation/vm/slub.txt">SLUB allocator</a>.
Two other flags to consider for non-hardened kernels are <code>Z</code>
(redzoning, to detect when a slab is overwritten past its real size) and
<code>P</code> (to enable poisoning on slab cache allocations).

<p>
The full list of kernel parameters to be used must be specified on a single
line, separated by spaces, in the bootloader's config file. An example for
GRUB might look like this:

</p><pre># /etc/default/grub

[...]
GRUB_CMDLINE_LINUX_DEFAULT="apparmor=1 init_on_alloc=1 init_on_free=1 l1tf=full,force lockdown=confidentiality lsm=lockdown,yama,apparmor page_alloc.shuffle=1 slab_nomerge slub_debug=F spec_store_bypass_disable=on spectre_v2=on vsyscall=none"
[...]
</pre>

Depending on the bootloader in use, the file may need to be regenerated after
any edits are made.

<p>
Changes to the kernel parameters won't take effect until after a reboot.
To verify they were applied, run:

</p><pre>$ <strong>cat /proc/cmdline</strong>
</pre>

<p>
Yet more runtime options of the kernel can be configured through the
<a href="https://wiki.archlinux.org/index.php/Sysctl">sysctl</a> utility.
The values specified by …</p></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://vez.mrsk.me/linux-hardening.html#hn">https://vez.mrsk.me/linux-hardening.html#hn</a></em></p>]]>
            </description>
            <link>https://vez.mrsk.me/linux-hardening.html#hn</link>
            <guid isPermaLink="false">hacker-news-small-sites-24664507</guid>
            <pubDate>Fri, 02 Oct 2020 16:51:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Brief History of Neural Nets and Deep Learning]]>
            </title>
            <description>
<![CDATA[
Score 35 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24664223">thread link</a>) | @andreyk
<br/>
October 2, 2020 | https://www.skynettoday.com/overviews/neural-net-history | <a href="https://web.archive.org/web/*/https://www.skynettoday.com/overviews/neural-net-history">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <h2 id="prologue-the-deep-learning-tsunami">Prologue: The Deep Learning Tsunami</h2>

<blockquote>
  <p>“Deep Learning waves have lapped at the shores of computational linguistics for several years now, but 2015 seems like the year when the full force of the tsunami hit the major Natural Language Processing (NLP) conferences.” -<a href="http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00239">Dr. Christopher D. Manning, Dec 2015</a> <sup id="fnref:part1_1" role="doc-noteref"><a href="#fn:part1_1">1</a></sup></p>
</blockquote>

<p>This may sound hyperbolic - to say the established methods of an entire field of research are quickly being superseded by a new discovery, as if hit by a research ‘tsunami’. But, this catastrophic language is appropriate for describing the meteoric rise of Deep Learning over the last several years - a rise characterized by drastic improvements over reigning approaches towards the hardest problems in AI, massive investments from industry giants such as Google, and exponential growth in research publications (and Machine Learning graduate students). Having taken several classes on Machine Learning, and even used it in undergraduate research, I could not help but wonder if this new ‘Deep Learning’ was anything fancy or just a scaled up version of the ‘artificial neural nets’ that were already developed by the late 80s. And let me tell you, the answer is quite a story - the story of not just neural nets, not just of a sequence of research breakthroughs that make Deep Learning somewhat more interesting than ‘big neural nets’  (that I will attempt to explain in a way that just about anyone can understand), but most of all of how several unyielding researchers made it through dark decades of banishment to finally redeem neural nets and achieve the dream of Deep Learning.</p>


<blockquote><p id="sources">
I am certainly not a foremost expert on this topic. In depth technical overviews with long lists of references written by those who actually made the field what it is include Yoshua Bengio's <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf">"Learning Deep Architectures for AI"</a>, Jürgen Schmidhuber's <a href="http://arxiv.org/pdf/1404.7828v4.pdf">"Deep Learning in Neural Networks: An Overview"</a> and LeCun et al.s' <a href="http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf">"Deep learning"</a>. In particular, this is mostly a history of research in the US/Canada AI community, and even there will not mention many researchers; a particularly in depth history of the field that covers these omissions is Jürgen Schmidhuber's <a href="http://people.idsia.ch/~juergen/deep-learning-overview.html">"Deep Learning in Neural Networks: An Overview"</a>. I am also most certainly not a professional writer, and will cop to there being shorter and much less technical overviews written by professional writers such as Paul Voosen's <a href="http://chronicle.com/article/The-Believers/190147">"The Believers"</a>, John Markoff's <a href="http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-intelligence.html">"Scientists See Promise in Deep-Learning Programs"</a> and Gary Marcus's <a href="http://www.newyorker.com/news/news-desk/is-deep-learning-a-revolution-in-artificial-intelligence">"Is “Deep Learning” a Revolution in Artificial Intelligence?"</a>. I also will stay away from getting too technical here, but there is a plethora of tutorials on the internet on all the major topics covered in brief by me.
<br>
Any corrections would be appreciated, though I will note some ommisions are intentional since I want to try and keep this 'brief' and a good mix of simple technical explanations and storytelling. 
<br>
This piece is an updated and expanded version of blog posts originally released in 2015 on www.andreykurenkov.com. 
</p></blockquote>

<ul id="markdown-toc">
  <li><a href="#prologue-the-deep-learning-tsunami" id="markdown-toc-prologue-the-deep-learning-tsunami">Prologue: The Deep Learning Tsunami</a></li>
  <li><a href="#part-1-the-beginnings-1950s-1980s" id="markdown-toc-part-1-the-beginnings-1950s-1980s">Part 1: The Beginnings (1950s-1980s)</a>    <ul>
      <li><a href="#the-centuries-old-machine-learning-algorithm" id="markdown-toc-the-centuries-old-machine-learning-algorithm">The Centuries Old Machine Learning Algorithm</a></li>
      <li><a href="#the-folly-of-false-promises" id="markdown-toc-the-folly-of-false-promises">The Folly of False Promises</a></li>
      <li><a href="#the-thaw-of-the-ai-winter" id="markdown-toc-the-thaw-of-the-ai-winter">The Thaw of the AI Winter</a></li>
    </ul>
  </li>
  <li><a href="#part-2-neural-nets-blossom-1980s-2000s" id="markdown-toc-part-2-neural-nets-blossom-1980s-2000s">Part 2: Neural Nets Blossom (1980s-2000s)</a>    <ul>
      <li><a href="#neural-nets-gain-vision" id="markdown-toc-neural-nets-gain-vision">Neural Nets Gain Vision</a></li>
      <li><a href="#neural-nets-go-unsupervised" id="markdown-toc-neural-nets-go-unsupervised">Neural Nets Go Unsupervised</a></li>
      <li><a href="#neural-nets-gain-beliefs" id="markdown-toc-neural-nets-gain-beliefs">Neural Nets Gain Beliefs</a></li>
      <li><a href="#neural-nets-make-decisions" id="markdown-toc-neural-nets-make-decisions">Neural Nets Make Decisions</a></li>
      <li><a href="#neural-nets-get-loopy" id="markdown-toc-neural-nets-get-loopy">Neural Nets Get Loopy</a></li>
      <li><a href="#neural-nets-start-to-speak" id="markdown-toc-neural-nets-start-to-speak">Neural Nets Start to Speak</a></li>
      <li><a href="#a-new-winter-dawns" id="markdown-toc-a-new-winter-dawns">A New Winter Dawns</a></li>
    </ul>
  </li>
  <li><a href="#part-3-deep-learning-2000s-2010s" id="markdown-toc-part-3-deep-learning-2000s-2010s">Part 3: Deep Learning (2000s-2010s)</a>    <ul>
      <li><a href="#the-funding-of-more-layers" id="markdown-toc-the-funding-of-more-layers">The Funding of More Layers</a></li>
      <li><a href="#the-development-of-big-data" id="markdown-toc-the-development-of-big-data">The Development of Big Data</a></li>
      <li><a href="#the-importance-of-brute-force" id="markdown-toc-the-importance-of-brute-force">The Importance of Brute Force</a></li>
      <li><a href="#the-deep-learning-equation" id="markdown-toc-the-deep-learning-equation">The Deep Learning Equation</a></li>
    </ul>
  </li>
  <li><a href="#epilogue-the-decade-of-deep-learning" id="markdown-toc-epilogue-the-decade-of-deep-learning">Epilogue: The Decade of Deep Learning</a></li>
</ul>


<p>The beginning of a story spanning half a century, about how we learned to make computers learn. In this part, we shall cover the birth of neural nets with the Perceptron in 1958, the AI Winter of the 70s, and neural nets’ return to popularity with backpropagation in 1986.</p>

<h2 id="the-centuries-old-machine-learning-algorithm">The Centuries Old Machine Learning Algorithm</h2>
<figure>
    <img src="https://www.skynettoday.com/assets/img/overviews/2020-09-27-a-brief-history-of-neural-nets-and-deep-learning/Linear_regression.svg" alt="Linear Regression">     
    <figcaption>Linear regression <a href="https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg">(Source)</a></figcaption>
</figure>

<p>Let’s start with a brief primer on what Machine Learning is. Take some points on a 2D graph, and draw a line that fits them as well as possible. What you have just done is generalized from a few example of pairs of input values (x) and output values (y) to a general function that can map any input value to an output value. This is known as linear regression, and it is a wonderful little <a href="https://en.wikipedia.org/wiki/Linear_regression#cite_note-4">200 year old</a> technique for extrapolating a general function from some set of input-output pairs. And here’s why having such a technique is wonderful: there is an incalculable number of functions that are hard to develop equations for directly, but are easy to collect examples of input and output pairs for in the real world - for instance, the function mapping an input of recorded audio of a spoken word to an output of what that spoken word is.</p>

<p>Linear regression is a bit too wimpy a technique to solve the problem of speech recognition, but what it does is essentially what <strong>supervised Machine Learning</strong> is all about: ‘learning’ a function given a <strong>training set</strong> of <strong>examples</strong>, where each example is a pair of an input and output from the function (we shall touch on the unsupervised flavor in a little while). In particular, machine learning methods should derive a function that can generalize well to inputs not in the training set, since then we can actually apply it to inputs for which we do not have an output. For instance, Google’s current speech recognition technology is powered by Machine Learning with a massive training set, but not nearly as big a training set as all the possible speech inputs you might task your phone with understanding.</p>

<p>This generalization principle is so important that there is almost always a <strong>test set</strong> of data (more examples of inputs and outputs) that is not part of the training set. The separate set can be used to evaluate the effectiveness of the machine learning technique by seeing how many of the examples the method correctly computes outputs for given the inputs. The nemesis of generalization is <strong>overfitting</strong> - learning a function that works really well for the training set but badly on the test set. Since machine learning researchers needed means to compare the effectiveness of their methods, over time there appeared standard <strong>datasets</strong> of training and testing sets that could be used to evaluate machine learning algorithms.</p>

<p>Okay okay, enough definitions. Point is - our line drawing exercise is a very simple example of supervised machine learning: the points are the training set (X is input and Y is output), the line is the approximated function, and we can use the line to find Y values for X values that don’t match any of the points we started with. Don’t worry, the rest of this history will not be nearly so dry as all this. Here we go.</p>

<h2 id="the-folly-of-false-promises">The Folly of False Promises</h2>

<p>Why have all this prologue with linear regression, since the topic here is ostensibly neural nets? Well, in fact linear regression bears some resemblance to the first idea conceived specifically as a method to make machines learn: <a href="http://psycnet.apa.org/index.cfm?fa=buy.optionToBuy&amp;id=1959-09865-001">Frank Rosenblatt’s <strong>Perceptron</strong></a><sup id="fnref:part1_2" role="doc-noteref"><a href="#fn:part1_2">2</a></sup>.</p>
<figure>
    <img src="https://www.skynettoday.com/assets/img/overviews/2020-09-27-a-brief-history-of-neural-nets-and-deep-learning/34998.jpg" alt="Perceptron">
    <figcaption>A diagram showing how the Perceptron works. <a href="http://cse-wiki.unl.edu/wiki/images/0/0f/Perceptron.jpg">(Source)</a></figcaption>    
</figure>

<p>A psychologist, Rosenblatt conceived of the Percetron as a simplified mathematical model of how the neurons in our brains operate: it takes a set of binary inputs (nearby neurons), multiplies each input by a continuous valued weight (the synapse strength to each nearby neuron), and thresholds the sum of these weighted inputs to output a 1 if the sum is big enough and otherwise a 0 (in the same way neurons either fire or do not). Most of the inputs to a Perceptron are either some data or the output of another Perceptron, but an extra detail is that Perceptrons also have one special ‘bias’ input, which just has a value of 1 and basically ensures that more functions are computable with the same input by being able to offset the summed value. This model of the neuron built on the work of Warren McCulloch and Walter Pitts <a href="http://www.minicomplexity.org/pubs/1943-mcculloch-pitts-bmb.pdf">Mcculoch-Pitts</a><sup id="fnref:part1_3" role="doc-noteref"><a href="#fn:part1_3">3</a></sup>, who showed that a neuron model that sums binary inputs and outputs a 1 if the sum exceeds a certain threshold value, and otherwise outputs a 0, can model the basic OR/AND/NOT functions. This, in the early days of AI, was a big deal - the predominant thought at the time was that making computers able to perform formal logical reasoning would essentially solve AI.</p>

<figure>
    <img src="https://www.skynettoday.com/assets/img/overviews/2020-09-27-a-brief-history-of-neural-nets-and-deep-learning/34832.jpg" alt="Perceptron 2"> 
    <figcaption>Another diagram, showing the biological inspiration. The <b>activation function</b> is what people now call the non-linear function applied to the weighted input sum to produce the output of the artificial neuron - in the case of Rosenblatt's Perceptron, the function just a thresholding operation.  <a href="http://cs231n.github.io/neural-networks-1/">(Source)</a> </figcaption>    
</figure>

<p>However, the Mcculoch-Pitts model lacked a mechanism for learning, which was crucial for it to be usable for AI. This is where the Perceptron excelled - Rosenblatt came up with a way to make such artificial neurons learn, inspired by the <a href="http://onlinelibrary.wiley.com/doi/10.1002/cne.900930310/abstract">foundational work</a><sup id="fnref:part1_4" role="doc-noteref"><a href="#fn:part1_4">4</a></sup> of Donald Hebb. Hebb put forth the unexpected and hugely influential idea that knowledge and learning occurs in the brain primarily through the formation and change of synapses between neurons - concisely stated as Hebb’s Rule:</p>

<blockquote>
  <p>“When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.”</p>
</blockquote>

<p>The Perceptron did not follow this idea exactly, but having weights on the inputs allowed for a very simple and intuitive learning scheme: given a <strong>training set</strong> of input-output examples the Perceptron should ‘learn’ a function from, for each example increase the weights if the Perceptron output for that example’s input is too low compared to the example, and otherwise decrease the weights if the output is too high. Stated ever so slightly more formally, the algorithm is as follows:</p>

<ol>
  <li>Start off with a Perceptron having random weights and a training set</li>
  <li>For the inputs of an example in the …</li></ol></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.skynettoday.com/overviews/neural-net-history">https://www.skynettoday.com/overviews/neural-net-history</a></em></p>]]>
            </description>
            <link>https://www.skynettoday.com/overviews/neural-net-history</link>
            <guid isPermaLink="false">hacker-news-small-sites-24664223</guid>
            <pubDate>Fri, 02 Oct 2020 16:25:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pressing YubiKeys]]>
            </title>
            <description>
<![CDATA[
Score 390 | Comments 188 (<a href="https://news.ycombinator.com/item?id=24663989">thread link</a>) | @bertrandom
<br/>
October 2, 2020 | https://bert.org/2020/10/01/pressing-yubikeys/ | <a href="https://web.archive.org/web/*/https://bert.org/2020/10/01/pressing-yubikeys/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>If you work in tech, you probably have a YubiKey. I have this one, the <a href="https://www.yubico.com/product/yubikey-5c-nano/">YubiKey 5C Nano</a>:</p>

<p><img src="https://bert.org/assets/posts/yubikey/nano.jpg" alt="YubiKey 5C Nano"></p>

<p>If you don’t work in tech but primarily work on your laptop, you probably <em>should</em> have a YubiKey. And if you work on a political campaign or as a journalist, you should definitely have one (or something similar). Talk to your IT Security department about that. This post will mostly be about something your IT Security department doesn’t want to hear about, though, so maybe don’t mention it to them.</p>

<p>YubiKeys act as two-factor authentication. This means that after you log-in to a system with your username and password, the system requires you to authorize in a second way as well. This way if your login credentials are compromised, the attacker would also have to compromise the second form of authentication, which is harder.</p>

<p>There are different forms of two-factor authentication - a common one is that a website will ask you to scan a QR code with the Google Authenticator app (or similar) on your phone which will generate 6 digit codes. The way this works is that the server and the app both have a shared secret. The phone generates codes based on that secret and the current timestamp and the server generates the same codes and sees if they match.</p>

<p><img src="https://bert.org/assets/posts/yubikey/qr-code.png" alt="QR Code"></p>

<p><img src="https://bert.org/assets/posts/yubikey/authenticator.png" alt="Google Authenticator"></p>

<p>Another one is SMS-based 2FA, which is pretty widely regarded as insecure. In this case, the server generates a code and sends it to your phone via SMS. The reason it’s considered insecure is that an attack exists called <a href="https://en.wikipedia.org/wiki/SIM_swap_scam">SIM-jacking</a> where someone convinces a cell phone carrier to port a number to a new SIM card, effectively directing all SMS traffic to their phone instead of yours.</p>

<p><img src="https://bert.org/assets/posts/yubikey/wells.jpg" alt="Wells Fargo"></p>

<p>YubiKeys are small devices that plug in to the USB port of your computer and emulate a keyboard. When tapped, they emit a one-time password (OTP) which can be then verified by a validation server. A private key exists on the device which is used to sign information, but it can never leave the device because it is stored in a tamper-resistant environment.</p>

<p>The YubiKey that I use is designed to always sit in a USB port of my laptop, so whenever I would take my laptop from my desk to a conference room or to another office, it was always available. But like many new remote workers, my laptop never leaves my desk anymore. I have it hooked up to an external monitor and to save some desk space, I have it in clamshell mode sitting vertically on a stand.</p>

<p>This makes tapping the YubiKey difficult, especially when I store my laptop far away from my keyboard and mouse. I solved this by buying a <a href="https://smile.amazon.com/gp/product/B071DMMW4J/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1">USB-C extension cable</a>, which brought the YubiKey closer to my keyboard.</p>

<p>One thing I haven’t mentioned about the YubiKey 5C Nano is that it’s kind of difficult to tap, even without the distance issues. The target area that you need to touch is extremely small:</p>

<p><img src="https://bert.org/assets/posts/yubikey/nano-big.png" alt="YubiKey 5C Nano"></p>

<p>One of the features of the YubiKey is that the little metal strip determines that it is being tapped by a human - this prevents it from being accidentally triggered by bumping your laptop into something, but if you’ve ever seen a one-time password in a Slack channel or Google Doc like <code>tlerefhcvijlngibueiiuhkeibbcbecehvjiklltnbbl</code>, you know it isn’t a perfect system. I would estimate that 1 in 5 times that I attempt to trigger it, it doesn’t register.</p>

<p>A lot of thought has gone into ensuring that the YubiKey can’t be triggered from software on the computer itself.</p>

<p>Before we go any further, I’d like to acknowledge the reasons for this. If a remote attacker were to compromise your laptop, being able to trigger the YubiKey from software on the computer defeats the whole point of using the YubiKey. But I think we always make tradeoffs between security and convenience - for example, you often don’t have to enter your YubiKey every time you access a system, some systems will only ask you once and not ask you again on subsequent logins for a certain amount of time. When you use a 2FA system and it gives you “backup codes”, do you always print those out and store them in a safe location? Everyone should figure out what level of security and convenience they are okay with.</p>

<p>With that being said, let’s talk about how you could trigger a YubiKey with software.</p>



<p>I’ve been calling this mechanism <strong>The Finger</strong>.</p>

<h2 id="hardware">Hardware</h2>

<p>First, we need some way for the computer to talk to <strong>The Finger</strong>. I had a bunch of these <a href="https://smile.amazon.com/gp/product/B076F53B6S/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1">IZOKEE D1 Mini</a> development boards lying around, they are smaller versions of boards that use the infamous <a href="https://en.wikipedia.org/wiki/ESP8266">ESP8266</a> chip found in a lot of IoT devices.</p>

<p><img src="https://bert.org/assets/posts/yubikey/d1-mini.jpg" alt="IZOKEE D1 Mini"></p>

<p>We can connect this to the laptop and talk to it over USB serial, but since it has WiFi, we can also just run a webserver on it and send it HTTP requests.</p>

<p>Next, we need some way to push <strong>The Finger</strong> towards the Yubikey. After a little googling, I found that the <a href="https://smile.amazon.com/gp/product/B01CP18J4A/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1">28BYJ-48 stepper motor</a> interfaces well with the D1 Mini board.</p>

<p><img src="https://bert.org/assets/posts/yubikey/stepper-motor.jpg" alt="stepper motor"></p>

<p>Stepper motors convert electrical pulses into mechanical rotation and the D1 Mini has pins for sending electrical pulses.</p>

<p><img src="https://bert.org/assets/posts/yubikey/stepper-motor.gif" alt="stepper motor"></p>

<p>But stepper motors rotate and we mostly just need to poke in a straight direction. So I searched on Thingiverse for “28BYJ-48” and found this: <a href="https://www.thingiverse.com/thing:3593641">28BYJ-48 Motor Halter</a>.</p>

<p><img src="https://bert.org/assets/posts/yubikey/stepper-motor-case.jpg" alt="28BYJ-48 Motor Halter"></p>

<p>This attaches a gear to the motor which can guide a long rack forward and backward. But if we’re going to push a long plastic thing toward the YubiKey, it might as well look like a finger. Back to Thingiverse, this time searching for “finger” and I found this model someone made for Halloween:</p>

<p><img src="https://bert.org/assets/posts/yubikey/finger_model.jpg" alt="finger"></p>

<p>I opened up these two models in Fusion 360 and used an advanced CAD technique called “smooshing”, resulting in this:</p>

<p><img src="https://bert.org/assets/posts/yubikey/finger_smoosh.png" alt="finger"></p>

<p>Next, I exported the smooshed STL and 3D printed it in <a href="https://shop.prusa3d.com/en/prusament/715-prusament-pla-lipstick-red-1kg.html">Prusament PLA Lipstick Red</a> because that’s what I happened to have in my printer at the time. Then I took the plastic finger and touched the YubiKey which.. didn’t do anything. I picked up a metal screw on my desk and touched the YubiKey, which immediately spit out a OTP. So then I took the finger and secured it to my desk with a vise and drilled a small hole in it, then screwed the metal screw into it and touched it to the YubiKey, which again did nothing.</p>

<p><img src="https://bert.org/assets/posts/yubikey/vise.jpg" alt="vise"></p>

<p>That’s when I realized that I’m an idiot and that when I had touched the metal screw to the Yubikey, it was just transmitting the electrical charge from my body to the metal screw, which then transmitted it to the capacitive touch sensor on the YubiKey. So how could I trick the capacitive touch sensor into thinking it was a real finger?</p>

<p>I guessed that the way that capacitive touch sensors work is that they’re measuring your body’s capacitance to ground, so if we just hook up the sensor directly towards ground, it’ll think that its really conductive or at least conductive enough for a human finger to be between the two. So I took an insulated wire, unscrewed the metal screw slightly, wrapped it around the screw and tightened it again. Then I took the other end and connected it the GND port on the D1 Mini board, touched it to the YubiKey, and it worked!</p>

<p>Now the driver board for the stepper motor already connects to the 5V and GND on the D1 Mini, so I thought I might have to strip the GND wire and run it to both the driver board and the screw, but on a whim I decided to just wedge the end of the wire from the metal screw between the stepper motor metal body (figuring the metal body case was grounded) and the plastic housing. This also worked!</p>

<p><img src="https://bert.org/assets/posts/yubikey/ground.jpg" alt="grounding"></p>

<p>Once I confirmed that the finger would trigger the YubiKey, I needed a way to mount the YubiKey close to the finger, so I used my digital calipers to measure the size of the USB-C extension cable and designed a holder in Fusion 360.</p>

<p><img src="https://bert.org/assets/posts/yubikey/holder.png" alt="holder"></p>

<p>The USB-C extension cable would go into the hole on the left and the motor would mount on the right.</p>

<p>At this point, we have to wire the stepper motor driver board to the D1 Mini. This can be done by soldering some headers onto the D1 Mini and then connecting some Dupont jumper wires between them.</p>

<p><img src="https://bert.org/assets/posts/yubikey/pins.jpg" alt="pins"></p>

<table>
  <thead>
    <tr>
      <th>D1 Mini</th>
      <th>28BYJ-48 Driver Board</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>5V</td>
      <td>5V</td>
    </tr>
    <tr>
      <td>GND</td>
      <td>GND</td>
    </tr>
    <tr>
      <td>D1</td>
      <td>IN1</td>
    </tr>
    <tr>
      <td>D2</td>
      <td>IN2</td>
    </tr>
    <tr>
      <td>D3</td>
      <td>IN3</td>
    </tr>
    <tr>
      <td>D4</td>
      <td>IN4</td>
    </tr>
  </tbody>
</table>

<p>Once we put the stepper motor into the housing and screw everything together, it should look like this:</p>

<p><img src="https://bert.org/assets/posts/yubikey/setup.jpg" alt="setup"></p>

<h2 id="software">Software</h2>

<p>The software is much more straightforward. The D1 Mini can be programmed using the Arduino IDE. First, we go into Preferences and add <code>https://arduino.esp8266.com/stable/package_esp8266com_index.json</code> under <em>Additional Board Manager URLs</em>. Then when you go into the <em>Boards Managers</em>, you can install the <code>esp8266</code> package which includes the board <strong>LOLIN(WEMOS) D1 R2 &amp; mini</strong>, which should be selected under <em>Tools</em>.</p>

<p>At this point I’ll run a sketch for blinking the LED just to verify that it’s working:</p>

<div><div><pre><code>#define LED 2 //Define blinking LED pin

void setup() {
  pinMode(LED, OUTPUT); // Initialize the LED pin as an output
}
// the loop function runs over and over again forever
void loop() {
  digitalWrite(LED, LOW); // Turn the LED on (Note that LOW is the voltage level)
  delay(1000); // Wait for a second
  digitalWrite(LED, HIGH); // Turn the LED off by making the voltage HIGH
  delay(1000); // Wait for two seconds
}
</code></pre></div></div>

<p>I found this <a href="https://robojax.com/learn/arduino/?vid=robojax_ESP8266_28BYJ-48_Stepper_ESP8STP-1">sketch</a> that shows how to control the 28BYJ-48 Stepper Motor using WiFi.</p>

<p>Here are the parts that have to do with the motor:</p>

<div><div><pre><code>int Pin1 = D1; //IN1 is connected 
int Pin2 = D2; //IN2 is connected   
int Pin3 = D3; //IN3 is connected 
int Pin4 = D4; //IN4 is connected 
 
int pole1[] ={0,0,0,0, 0,1,1,1, 0}; //pole1, 8 step values
int pole2[] ={0,0,0,1, 1,1,0,0, 0}; //pole2, 8 step values
int pole3[] ={0,1,1,1, 0,0,0,0, 0}; //pole3, 8 step values
int pole4[] ={1,1,0,0, 0,0,0,1, 0}; //pole4, 8 step values

int poleStep = 0; 
int dirStatus = 3; // stores direction status 3= stop (do not change)
String argId[] ={"ccw", "cw"};

...

void loop(void) {
    server.handleClient();
    MDNS.update();

    if (dirStatus == 1) {
        poleStep++;
        driveStepper(poleStep);
    } else if (dirStatus == 2) {
        poleStep--;
        driveStepper(poleStep);
    } else {
        driveStepper(8);
    }
    
    if (poleStep&gt;7) { 
        poleStep=0; 
    }

    if (poleStep&lt;0) {
        …</code></pre></div></div></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://bert.org/2020/10/01/pressing-yubikeys/">https://bert.org/2020/10/01/pressing-yubikeys/</a></em></p>]]>
            </description>
            <link>https://bert.org/2020/10/01/pressing-yubikeys/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24663989</guid>
            <pubDate>Fri, 02 Oct 2020 16:03:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What We Learned by Closing a $4M Investment from Accel]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24663314">thread link</a>) | @hodgesrm
<br/>
October 2, 2020 | https://altinity.com/blog/what-we-learned-by-closing-a-4m-investment-from-accel | <a href="https://web.archive.org/web/*/https://altinity.com/blog/what-we-learned-by-closing-a-4m-investment-from-accel">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	

	<p>I’m pleased to share the news that Altinity has raised a $4M seed investment from Accel. Dan Levine led the round. We are honored by Accel’s trust and delighted to work with Dan. We plan to use the investment to roll out our new <a href="https://altinity.com/cloud/">Altinity.Cloud platform</a> and to strengthen <a href="https://clickhouse.tech/">ClickHouse </a>into the best analytic database on the planet.&nbsp;</p><p>Dan was the first venture capitalist to contact us, as he tells in his <a href="http://www.accel.com/noteworthy/our-seed-in-altinity" target="_blank" rel="noreferrer noopener">blog article about the seed investment</a>. He was enthusiastic but also very patient.That was fortunate, because we then talked to 43 other VCs at greater or lesser length over the next year and a half. The count omits those who greeted our overtures with stony silence. In the end we were absolutely confident Dan and Accel were the right choice. At the same time, we learned from many others.</p><p>Looking back, it is apparent we did more than just collect a check from a great investment team. We also learned a number of valuable lessons about early stage venture investment.&nbsp; Many of these were not obvious, at least to me. In this article I will share what we learned, along with a spreadsheet we developed to help with investment math. I hope our account will be useful — or at least entertaining!</p><p><h2 id="h-what-do-vcs-really-want">What do VCs really want?</h2>
</p><p>VC websites often sport brave slogans like “we are looking for bold entrepreneurs who will change the world.” What they are actually looking for, of course, is far more concrete: a big return on a speculative bet about a new business. The first thing we learned was how venture capital actually works and how we fit in.&nbsp;</p><p>Let’s start with where the money comes from and how it is managed. Venture capital firms operate one or more funds, which they use to make investments. Each venture capital firm has general partners who work for the company, decide where to invest, and take care of serving on boards and other duties required to supervise each investment.&nbsp; There is also another type of partner, known as a limited partner or LP. LPs can be wealthy individuals, pension funds, sovereign investment funds, you name it. They supply cash but have no role in making investment decisions. Here is a picture.&nbsp;</p><div><figure><img loading="lazy" width="954" height="368" src="https://altinity.com/wp-content/uploads/2020/09/What-we-learned-from-accel-blog-investment-fund-picture.png" alt="" srcset="https://altinity.com/wp-content/uploads/2020/09/What-we-learned-from-accel-blog-investment-fund-picture.png 954w, https://altinity.com/wp-content/uploads/2020/09/What-we-learned-from-accel-blog-investment-fund-picture-300x116.png 300w, https://altinity.com/wp-content/uploads/2020/09/What-we-learned-from-accel-blog-investment-fund-picture-768x296.png 768w, https://altinity.com/wp-content/uploads/2020/09/What-we-learned-from-accel-blog-investment-fund-picture-600x231.png 600w" sizes="(max-width: 954px) 100vw, 954px" data-lazy-srcset="https://altinity.com/wp-content/uploads/2020/09/What-we-learned-from-accel-blog-investment-fund-picture.png 954w, https://altinity.com/wp-content/uploads/2020/09/What-we-learned-from-accel-blog-investment-fund-picture-300x116.png 300w, https://altinity.com/wp-content/uploads/2020/09/What-we-learned-from-accel-blog-investment-fund-picture-768x296.png 768w, https://altinity.com/wp-content/uploads/2020/09/What-we-learned-from-accel-blog-investment-fund-picture-600x231.png 600w" data-lazy-src="https://altinity.com/wp-content/uploads/2020/09/What-we-learned-from-accel-blog-investment-fund-picture.png?is-pending-load=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
</div><p>In the first meeting with a new VC you typically hear about the current funds, how big they are, and other details like reserves for follow-on investments in promising companies. When you ask what payback they are seeking the most frequent answer is “return the fund.” This is so common that I stopped writing it down unless the answer was something different. It means the investment in your company needs to pay out at the value of the entire fund, not just what the VC put into your company. The reason has to do with the mechanics of the funds.</p><p>Most startups fail or return money that does not come close to covering the investment. To provide a decent return to limited partners and VC general partners plus pay overhead, at least a couple of investments need to hit home runs and to pay off at full value of the fund. It’s basic math, but the numbers are big. Say a $400M fund invests $40M total for a 25% share of your startup, a typical target percentage. To return the fund means that the startup will have to exit for $1.6B ($400M = $1.6B * 25%). Grand slams like <a href="https://news.crunchbase.com/news/these-were-the-biggest-winners-in-snowflakes-record-busting-ipo" target="_blank" rel="noreferrer noopener">Snowflake</a> return far more and make the fund very successful.&nbsp;</p><p>At first it was disconcerting when A-list VC prospects baldly asked how we would get them an exit in the $1B+ range. Over time I developed empathy for this attitude. VC general partners have to make the math work or find another job. Meanwhile entrepreneurs need to have a problem that fits the pattern of investment or venture funding does not make a lot of sense. That’s the economic reality and has little to do with how individual VCs feel about you personally, or even about your business.&nbsp;</p><p>We needed to articulate to ourselves how we would win in a large market–SQL data warehouses–filled with a lot of savvy competitors like Amazon and Snowflake. The classic strategy is to create a <em><span>new</span> </em>market that did not previously exist, then become the leader. Anurag Gupta and his colleagues at Amazon put it brilliantly <a href="https://dl.acm.org/doi/10.1145/2723372.2742795" target="_blank" rel="noreferrer noopener">in a paper on Amazon Redshift</a>:&nbsp;</p><p><em>Our goal with Amazon Redshift was not to compete with other data warehousing engines, but to compete with non-consumption.&nbsp;</em></p><p>The unique differentiation of ClickHouse is that it is open source and runs anywhere: from public clouds down to Android phones. Any developer on the planet can download it and add high performance analytics to any application without sacrificing portability or scaling. That’s an enormous expansion of the market that will fuel innovation not just at the database level but will extend to new applications of big data as well as the tools and platforms to run them.&nbsp;</p><p>Here’s another key insight: it took months to be able to state that value proposition in three sentences. It’s like learning a new language — anyone can learn to say hello but achieving fluency requires real work.&nbsp;</p><p>We did a lot of modeling to understand the growth trajectory needed to achieve the kind of revenue our predecessors are making. One of the conclusions was that we needed to build a great cloud platform for ClickHouse. It’s a tried-and-true way to build a successful business, especially for companies that manage data, and one that our customers have confirmed as a fruitful path to growth. We believe in the plan and it matches venture capital economics.&nbsp;</p><p><h2 id="h-vcs-work-off-a-thesis">VCs Work Off a Thesis</h2>
</p><p>I didn’t know a lot of early stage VCs when we began fund-raising, though like everyone I heard they were fine human beings worthy of acquaintance. The initial conversations were illuminating in one particular respect. Venture capitalists don’t necessarily know that much about specific technology or markets.&nbsp;</p><p>Here’s an example. My favorite demo for ClickHouse is the <a href="https://youtu.be/zDIK3Ej86GU" target="_blank" rel="noreferrer noopener">ClickHouse-fast demo</a> where I first run a query on data generated purely in memory followed by a similar query that accesses 1.3 billion rows of taxi data in slowish network-attached storage. I usually pause dramatically after the in-memory query to ask which query is going to be faster. Everybody knows access to memory is faster than storage, right?</p><p>Actually, in this demo it’s not. You have to be very careful to make an apples-to-apples comparison when comparing memory and storage access speeds. ClickHouse compresses stored data and parallelizes I/O extremely well. Reading from storage is therefore very fast. With ClickHouse it is not hard to choose in-memory queries that look similar but run far slower because they have a different execution path with less parallelization or other inefficiencies. It’s a subtle point that experienced database people understand, whereas VCs I talked to often got it wrong. (And then argued about it, too.)&nbsp;</p><p>This experience illustrates that deep dives on technology are not always the best way to evaluate early stage businesses. Good VCs tend to look for proxies that indicate signs of traction. In our case Dan Levine knew about ClickHouse because his other start-up investments used it. Dan pays really close attention to things they like. Dan picked up on ClickHouse earlier and more clearly than anyone we spoke to. The fact that we were an experienced team already selling services profitably was perhaps another useful signal. But Dan was also looking for more than just specific signals–he was looking for a pattern related to data, backed by a solid team.&nbsp;</p><p>Over time, we found that the VCs who really picked up on our story had a thesis about the value of combining two things:&nbsp;</p><div><ul>
<li>Data – Faster and more cost effective ways of analyzing large datasets are inherently valuable to enterprises.&nbsp;&nbsp;</li>
<li>Open source – There are standard models for marketing and monetizing open source projects to build very large businesses</li>
</ul>
</div><p>VCs with these convictions tended to like what we were doing overall, though they often found specific things they didn’t like: open source community too small, too much competition, not the right team, already made a competing investment, etc. That said, we didn’t argue about the size of the market or whether open source was the right overall strategy to reach it.&nbsp;It helped that the original developers of <a href="https://en.wikipedia.org/wiki/ClickHouse">ClickHouse at Yandex</a> did an amazing job of open sourcing the code and starting a great community around it. </p><p>Not surprisingly, we learned that those same investors were precisely the people we wanted backing the company. Not only did we share key assumptions about the business, but they had funded such businesses before with successful outcomes. Because of that they could offer useful advice on big topics like strategy to build open source communities or workable business models.&nbsp; They could also connect us with outstanding people like Mike Olson of Cloudera (and many others) who had worked through similar problems and could help us see around corners.&nbsp;</p><p>Here’s a final insight that relates back to the technology point I made above. VCs can identify promising companies, but they can’t tell you how to run yours. As an entrepreneur you understand the technology, your customers, and what is feasible to achieve. We had a number of debates with potential investors about details of the business plan.</p><p>For example, many VCs favor pure cloud services, because the best ones experience explosive growth and high margins. However, a push-button service is not a complete solution, especially for complex enterprise products like databases. Altinity has been in business since 2017 and we have articulate users who say they want us to take care of running ClickHouse in the cloud. They also want application tools, new server features, training, support, and implementation help. Their problem is not just to deploy a database but to create applications that add value to their own business. If you help them do that you have a much more competitive business.&nbsp;</p><p>Our mission is to help any enterprise that uses ClickHouse. We provide everything customers need to be successful with ClickHouse, <em>including</em> a great cloud service. We also support the ClickHouse …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://altinity.com/blog/what-we-learned-by-closing-a-4m-investment-from-accel">https://altinity.com/blog/what-we-learned-by-closing-a-4m-investment-from-accel</a></em></p>]]>
            </description>
            <link>https://altinity.com/blog/what-we-learned-by-closing-a-4m-investment-from-accel</link>
            <guid isPermaLink="false">hacker-news-small-sites-24663314</guid>
            <pubDate>Fri, 02 Oct 2020 14:59:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[BMW fined $18M for providing inaccurate retail sales information to investors]]>
            </title>
            <description>
<![CDATA[
Score 91 | Comments 32 (<a href="https://news.ycombinator.com/item?id=24663027">thread link</a>) | @zachshefska
<br/>
October 2, 2020 | https://yourautoadvocate.com/guides/bmw-fraud/ | <a href="https://web.archive.org/web/*/https://yourautoadvocate.com/guides/bmw-fraud/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p><iframe src="https://www.youtube.com/embed/MBe7PNfmvAw" frameborder="0" allowfullscreen=""></iframe></p><p>The Securities and Exchange Commission recently announced $18M in fines that BMW and two of their subsidiaries must pay for having provided misleading and inaccurate retail sales information to their investors.</p><p>The SEC report reads:</p><blockquote><p>According to the SEC’s order, from 2015 to 2019, BMW inflated its reported retail sales in the U.S., which helped BMW close the gap between its actual retail sales volume and internal targets and publicly maintain a leading retail sales position relative to other premium automotive companies. The order finds that BMW of North America LLC (BMW NA) maintained a reserve of unreported retail vehicle sales — referred to internally as the “bank” — that it used to meet internal monthly sales targets without regard to when the underlying sales occurred. The order also finds that BMW NA paid dealers to inaccurately designate vehicles as demonstrators or loaners so that BMW would count them as having been sold to customers when they had not been. Additionally, the order finds that BMW NA improperly adjusted its retail sales reporting calendar in 2015 and 2017 to meet internal sales targets or bank excess retail sales for future use. As a result, according to the order, the information that BMW provided to investors in the bond offerings by BMW’s U.S. financing subsidiary, BMW US Capital LLC, and to credit rating agencies contained material misstatements and omissions regarding BMW’s U.S. retail vehicle sales.</p><cite><a href="https://www.sec.gov/news/press-release/2020-223" target="_blank" rel="noreferrer noopener">https://www.sec.gov/news/press-release/2020-223</a></cite></blockquote><p>After having spent 43 years in the car business (many of which with BMW North America), I can unequivocally say these practices are routine and commonplace within car dealerships. Fraudulent behavior like this is not limited to BMW. Every manufacturer I have ever worked for encourages this.</p><p>When I worked for Penske Automotive Group we were explicitly instructed not to fudge any numbers. If our BMW rep asked us to “pad the numbers” one month, we didn’t. Penske didn’t want to participate in that type of activity. They were the exception to the rule.</p><p>As a dealer you have very little choice but to “play the game.” As I’ve talked about in other videos and guides here on the blog, car dealers don’t make much of anything when they sell vehicles. Instead, <a href="https://yourautoadvocate.com/guides/how-do-car-dealerships-make-money/" target="_blank" rel="noreferrer noopener">they make their money from factory incentives and from selling finance and insurance products</a>.</p><p><iframe src="https://www.youtube.com/embed/RTYnhidJMe8" frameborder="0" allowfullscreen=""></iframe></p><p>With that in mind, it’s clear why dealers “play the game.” If you have a $250,000 incentive that is based on the number of cars you sell in any given month, and the person writing you that check (BMW) is encouraging you to “fake” sales so that you can actually attain the bonus, what would you do? The answer is simple.</p><p>Car manufacturers are happy to pay out giant monthly bonuses to subsidize their dealers, but only if they hit certain sales volume thresholds. This is because manufacturers are then able to report better than expected sales volumes to their investors.</p><p>How many fraudulently reported vehicles are “sold” in any given month? In any given month we would designate 15 Mini Coopers as “sold,” even though they hadn’t been. In that same month we may have actually sold 35 or 40 vehicles. Each month, upwards of 20% of our “sales” were fake.</p><p>It’s surprising to think that BMW was only fined $18M. Considering a nontrivial amount of their sold inventory is not actually sold, you would think the fine should be $180M instead of $18M.</p><p>Fiat Chrysler paid $40M in fines a few years ago for similar practices. Regardless of who it is, it’s clear that the fines aren’t enough to stop the fraudulent behavior.</p>
</div></div></div>]]>
            </description>
            <link>https://yourautoadvocate.com/guides/bmw-fraud/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24663027</guid>
            <pubDate>Fri, 02 Oct 2020 14:35:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Apache OpenWhisk is a truly portable Serverless Platform]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24662975">thread link</a>) | @kiyanwang
<br/>
October 2, 2020 | https://openwhisk.blog/post/advocate/openwhisk-portable-serverless/ | <a href="https://web.archive.org/web/*/https://openwhisk.blog/post/advocate/openwhisk-portable-serverless/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Apache OpenWhisk is a truly portable and multiplatform Serverless engine and it is available now on all the major clouds from multiple commercial vendors. Here is a Chess Engine running on:</p><ul><li><a href="https://whisk-chess.adobeioruntime.net/api/v1/web/default/chess">Adobe I/O</a></li><li><a href="https://eu-de.functions.appdomain.cloud/api/v1/web/a1d40f6b-e5e3-4f07-8f92-77b525392253/default/chess">IBM Cloud</a></li><li><a href="https://wka9bi13u3.apigw.ntruss.com/chess/chess/ZC2o7bFh0x/http">Naver</a></li><li><a href="https://apigcp.nimbella.io/api/v1/web/msciabar-zc3thebgxgh/default/chess">Nimbella</a></li></ul><p>And see below for instructions how to run it also locally and in any Kubernetes cluster, for example AWS EKS…</p><p><iframe src="https://www.youtube.com/embed/02Xezhf_j4U" allowfullscreen="" title="YouTube Video"></iframe></p><p>Apache OpenWhisk is a Serverless Cloud Platform, developed as an open source project at the Apache Software Foundations. It is similar to Amazon Lambda, Google Functions or Azure Functions. The main difference is that it is an Open Source project, it is offered by multiple commercial vendors, and it has a rich serverless programing model for composing functions into workflows.</p><p>Many vendors today offer cloud functions based on OpenWhisk, and it runs on all the major public clouds. However not all the vendors disclose where they run their services, so I will refer to the vendor and not to the cloud that runs it. It can also be installed on any Kubernetes cluster, so you can install in any cloud, either your private cloud or the public one you prefer.</p><p>In this article I am going to show that OpenWhisk is a truly portable serverless solution, and that you can write a single serverless application and then run it on multiple vendors.</p><p>To prove my point, I wrote an open source serverless application and ran it on all the OpenWhisk vendors I got access to. I also created a custom Kubernetes cluster and installed OpenWhisk on it to run my application.</p><p>The application is a chess engine, written in the Go programming language, and that includes backend and frontend. You can use it to play chess using a web interface, while the opponent is an AI algorithm running as a serverless function in OpenWhisk.</p><p>For testing and development you can use the Standalone OpenWhisk. It is a single node installation that can run in your machine and only requires <a href="https://docker.com/"><code>Docker</code></a> to run. You also need to download the <a href="https://github.com/apache/openwhisk-cli/releases/tag/1.0.0">OpenWhisk CLI tool <code>wsk</code></a> for your operating system in order to interact with OpenWhisk.</p><p>Once prerequisites are satisfied, you can start a local OpenWhisk with the following command:</p><div><pre><code data-lang="fallback">bash &lt;(curl -sL https://s.apache.org/openwhisk.sh)
</code></pre></div><p>The command will download a Docker image for standalone OpenWhisk and it will start it. It will also open the playground, that you can use to create and run a function on the fly from your browser.</p><p><img src="https://openwhisk.blog/post/advocate/openwhisk-portable-serverless/playground-ui.png" alt="Playground"></p><p>Once you have OpenWhisk up and running you can configure the <code>wsk</code> tool to access it. OpenWhisk access is protected by a key that you have to retrieve and use to configure <code>wsk</code>, as follows:</p><div><pre><code data-lang="fallback">AUTH=$(docker exec openwhisk wsk property get --auth | awk '{ print $3}')
wsk property set --auth $AUTH --apihost http://localhost:3233
</code></pre></div><p>Now let’s build our chess engine and use the local OpenWhisk to test it locally. The source code of the chess engine <a href="https://github.com/openwhisk-blog/whisk-chess">is available on GitHub</a>.</p><p>The code is based on a freely available chess engine called <a href="https://github.com/ChizhovVadim/CounterGo/pulls">CounterGo</a>. It is written in Go. I adapted it to run as a stateless serverless action, and I added a frontend in JavaScript, using the libraries <a href="https://chessboardjs.com/">Chessboardjs</a> and <a href="https://github.com/jhlywa/chess.js">chess.js</a>.</p><p>In order to build the action, you need common tools like <code>git</code>, <code>make</code> and <code>docker</code>. Once you have them you can download and build the sources with the commands:</p><div><pre><code data-lang="fallback">git clone https://github.com/openwhisk-blog/whisk-chess
cd whisk-chess
make
</code></pre></div><p>Note that you do not need a Go compiler to build the action, just Docker, as you can compile the action using the OpenWhisk Go runtime itself. The result is the file <code>chess.zip</code> containing a pre-compiled Go action ready to be deployed.</p><p>Once you have the action, you use the following command to deploy it in OpenWhisk:</p><div><pre><code data-lang="fallback">wsk action update chess chess.zip --kind go:1.11 --web true
</code></pre></div><p>Finally you can retrieve the URL of the action with the command:</p><div><pre><code data-lang="fallback">wsk action get chess --url
</code></pre></div><p>If you now type the URL in a browser you will see the user interface of our chess engine, a chessboard, and you can play chess against the computer.</p><p><img src="https://openwhisk.blog/post/advocate/openwhisk-portable-serverless/chess.png" alt="Chess"></p><p>Now let’s start deploying our chess in the services of the various vendors that offer OpenWhisk.</p><p><a href="https://nimbella.com/">Nimbella</a> offers a serverless solution based on OpenWhisk and focused on providing an “awesome developer experience”.</p><p>I think it is appropriate to say that I work for Nimbella, but I am trying to be neutral in this article and offer a fair comparison of all the OpenWhisk vendors I am aware of.</p><p><img src="https://openwhisk.blog/post/advocate/openwhisk-portable-serverless/020.png" alt="Nimbella"></p><p>Nimbella uses its own CLI called <code>nim</code> for deployment. The Nimbella CLI was recently <a href="https://github.com/nimbella/nimbella-cli/">open sourced</a>. You need to sign-up and login to use their service. Once you are logged in, you can deploy our chess action and get an URL for it. The <code>nim login</code> command conveniently permits sign-up.
The CLI is available <a href="https://nimbella.io/downloads/nim/nim.html#install-the-nimbella-command-line-tool-nim">for download</a> for Mac OS, Windows and Linux.</p><div><pre><code data-lang="fallback">nim login
nim action update chess chess.zip --kind go:1.12 --web true
nim action get chess --url
</code></pre></div><p>It is possible to use the <code>wsk</code> CLI with Nimbella if one prefers it. You’ll notice the command is identical here to the one shown earlier but replaced <code>wsk</code> with <code>nim</code>.</p><p><a href="https://apigcp.nimbella.io/api/v1/web/msciabar-zc3thebgxgh/default/chess">Follow this link to play chess on Nimbella</a>.</p><p>The IBM cloud was the original cloud offering OpenWhisk as a service.</p><p><img src="https://openwhisk.blog/post/advocate/openwhisk-portable-serverless/030.png" alt="IBM"></p><p>You need to download and install the <code>ibmcloud</code> CLI in order to deploy actions to IBM. There are also some requirements like downloading a plugin and to target a space; all the steps are explained on their website.</p><p>They offer a generous free tier for running functions. You need to register on their website to use a very large number of function invocations for free.</p><p>Once you downloaded the tool, the commands to deploy the chess engine and get an URL to run the action are:</p><div><pre><code data-lang="fallback">ibmcloud login -u "$IBMUSER" -p "$IBMPASS"
ibmcloud fn action update chess chess.zip --kind go:1.11
ibmcloud fn action get chess --url
</code></pre></div><p><a href="https://eu-de.functions.appdomain.cloud/api/v1/web/a1d40f6b-e5e3-4f07-8f92-77b525392253/default/chess">Follow this link to play Chess on IBM Cloud.</a></p><p>Naver is a Korean company, owner of the main search engine in the Korean language, but also offering cloud services. The Naver Cloud Platform uses OpenWhisk to implement cloud functions.</p><p><img src="https://openwhisk.blog/post/advocate/openwhisk-portable-serverless/040.png" alt="Naver"></p><p>Currently Naver does not offer a CLI to deploy actions, however I was told a CLI is actually under development. For now I deployed the chess action using their web interface.</p><p><img src="https://openwhisk.blog/post/advocate/openwhisk-portable-serverless/041.png" alt="Naver Deploy"></p><p><a href="https://wka9bi13u3.apigw.ntruss.com/chess/chess/ZC2o7bFh0x/http">Follow this link to play Chess on Naver.</a></p><p>Adobe has a serverless offering based on OpenWhisk too. It is called the <a href="https://www.adobe.io/apis/experienceplatform/runtime.html">Adobe I/O Runtime</a>.</p><p><img src="https://openwhisk.blog/post/advocate/openwhisk-portable-serverless/045.png" alt="Adobe I/O"></p><p>Adobe I/O Runtime currently supports only Node.js based runtimes, so if you pick them as your serverless function providers you have to write your serverless functions in JavaScript. However being based on OpenWhisk, it is possible to use other runtimes by request, and so we can also run our chess engine. I thank the team at Adobe for their kind support and help in deploying my action for demonstration purposes.</p><p><a href="https://whisk-chess.adobeioruntime.net/api/v1/web/default/chess">Follow this link to play Chess on Adobe I/O.</a></p><p>Finally, you can run OpenWhisk in any cluster supporting Kubernetes. For this purpose, I created an EKS cluster on AWS and installed OpenWhisk on it, then I deployed my chess application. I will show here how to do that quickly and easily.</p><p><img src="https://openwhisk.blog/post/advocate/openwhisk-portable-serverless/053.png" alt="AWS"></p><p>You will need to create and configure an AWS account. I refer you to AWS documentation for information how to do this.</p><p>Once I created an account, I installed the <a href="https://eksctl.io/"><code>eksctl</code></a> tool that makes easy to create a Kubernetes cluster on AWS.</p><p>Also you need to install the <a href="https://helm.sh/"><code>helm</code></a> deployment tool and use it to actually install OpenWhisk. You can download the helm chart from GitHub and install OpenWhisk as follows:</p><div><pre><code data-lang="fallback">git clone https://github.com/apache/openwhisk-deploy-kube
cd openwhisk-deploy-kube/helm
</code></pre></div><p>Once everything is ready your can create a Kubernetes cluster and install OpenWhisk with just 3 commands:</p><div><pre><code data-lang="fallback">eksctl create cluster --name openwhisk
eksctl create nodegroup --cluster openwhisk --node-labels openwhisk-role=invoker
helm install --set whisk.ingress.type=LoadBalancer openwhisk ./openwhisk
</code></pre></div><p>The cluster creation will take a while. Once it is completed you will get your private OpenWhisk service running in AWS, and you can deploy your chess application to it.</p><p>You can use the <code>wsk</code> or <code>nim</code> CLIs to deploy to OpenWhisk. You have to retrieve the location of the Apache OpenWhisk entry point, and the authorization key and pass them to the CLI tool. The required commands using <code>nim</code> are:</p><div><pre><code data-lang="fallback">cd whisk-chess
APIHOST=$(kubectl  get svc | awk '/openwhisk-nginx/ { print $4}')
AUTH=$(cat openwhisk/values.yaml |  awk '/guest/ { print $2}' | tr -d '"')
nim auth login --apihost http://$APIHOST --auth $AUTH
</code></pre></div><p>It is important to note that we configured an insecure setup because we are accessing OpenWhisk over the unencrypted HTTP protocol.</p><p>In a real world setup you will need additional steps to setup an HTTPS endpoint with a certificate. You will find relevant details in the <a href="https://github.com/apache/openwhisk-deploy-kube">helm chart GitHub repository</a>.</p><p>Once you retrieved the API host and authentication key, you can deploy your chess app, and get the URL.</p><div><pre><code data-lang="fallback">nim action create chess chess.zip --web true --kind go:1.11
nim action get chess --url
</code></pre></div><p>I cannot provide a URL in this case as I a destroyed the cluster after testing, however, you can see the result in the image at the beginning of the paragraph.</p></div></div>]]>
            </description>
            <link>https://openwhisk.blog/post/advocate/openwhisk-portable-serverless/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24662975</guid>
            <pubDate>Fri, 02 Oct 2020 14:30:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Boa: an experimental Javascript lexer, parser and compiler written in Rust]]>
            </title>
            <description>
<![CDATA[
Score 75 | Comments 27 (<a href="https://news.ycombinator.com/item?id=24662756">thread link</a>) | @jayflux
<br/>
October 2, 2020 | https://boa-dev.github.io/2020/10/02/boa-release-10.html | <a href="https://web.archive.org/web/*/https://boa-dev.github.io/2020/10/02/boa-release-10.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <div>

  

  <article>
    <p>Boa is an experimental Javascript lexer, parser and compiler written in Rust. It has support for some of the language, can be embedded in Rust projects fairly easily and also used from the command line.
Boa also exists to serve as a Rust implementation of the EcmaScript specification, there will be areas where we can utilise Rust and its fantastic ecosystem to make a fast, concurrent and safe engine.</p>

<p>We have a long way to go, however v0.10 has been the biggest release to date, with 138 issues closed!</p>

<p>We have some highlights, but if you prefer to read the full changelog, you can do that <a href="https://github.com/boa-dev/boa/blob/master/CHANGELOG.md">here</a></p>

<h2 id="test262">Test262</h2>

<p>One question we’ve been asked for a long time is “how conformant are you to the spec?”. It’s been tough to answer as we’ve been unable to run against the official test suite.</p>

<p>Test262 is the official ECMAScript Test Suite and exists to provide conformance tests for the latest drafts of the Ecma specification. It is used for all engines, you can even run it in your <a href="https://bakkot.github.io/test262-web-runner/">browser</a>.<br>
Thanks to @Razican in v0.10 we now have a test harness that allows us to run it against Boa at any time.</p>

<p>This is a new crate inside the Boa repository that can parse through all of the tests (roughly 40,000 of them) in under 10 minutes and tell us how conformant we are.</p>

<p><img src="https://boa-dev.github.io/images/2020-10-02/test262-screenshot.png" alt="image"></p>

<p>Today Boa has <span>18</span>% conformity to the specification. We’ll be keeping an eye on this number over the releases. We expect to achieve around 30% by 0.11 due to some of the fixes we’re adding which should pass a few thousand tests.</p>

<p>These are run via Github Actions against PRs and for our master branch so that we can keep track of where we are and if there are regressions.</p>

<h2 id="built-ins">Built-ins</h2>

<p>We’ve added support for <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date"><code>Date</code></a>, <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map"><code>Map</code></a> and <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Symbol">well-known symbols</a>. Supporting Well-known symbols unblocks a lot of work around adding <code>@@iterators</code> to some of our global objects which is coming up in the next release.<br>
Both <code>Math</code> and <code>Number</code> have had their remaining methods implemented.</p>

<h2 id="lexer">Lexer</h2>

<p>The lexer has been rebuilt from scratch. Just like the old parser it was a single file before looping through and becoming unmaintainable. Today we’ve reorganised it into separate modules which know how to lex certain areas. The new lexer <a href="https://github.com/boa-dev/boa/issues/294">now supports goal symbols</a> and can now tokenize with the correct context at any time.</p>

<h3 id="goal-symbols">Goal Symbols</h3>

<p>Our issue with goal symbols is explained by the V8 team:
<a href="https://v8.dev/blog/understanding-ecmascript-part-3#lexical-grammar">https://v8.dev/blog/understanding-ecmascript-part-3#lexical-grammar</a></p>

<p>Previously we weren’t distinguishing between the contexts where some input elements are permitted and some are not, so lexing <code>/</code> would yeild a <code>division</code> symbols when it should be a <code>RegularExpressionLiteral</code> for example. This change unblocked us being able to run Test262.</p>

<p>Performance wise it is much faster for larger files. The lexer is far more efficient at streaming tokens to the parser than previously so in some scenarios we have big gains.</p>

<p><em>You can see all the benchmarks <a href="https://boa-dev.github.io/boa/dev/bench/">here</a></em></p>

<h2 id="repl-syntax-highlighting">Repl syntax highlighting</h2>

<p>Syntax highlighting was added to the repl this release thanks to @HalidOdat<br>
Our repl is made possible due to the great work of <a href="https://github.com/kkawakam/rustyline">RustyLine</a></p>

<p><img src="https://boa-dev.github.io/images/2020-10-02/syntaxHighlighting.gif" alt="image"></p>

<h2 id="looking-forward">Looking forward</h2>

<p>There are plenty of fixes and performance changes still needed, we also hope to experiment with producing Bytecode from our AST in future. Test262 coverage will almost certainly increase, and we are polishing the public API for easier use when embedding into other Rust projects.</p>

<p>Thanks to all those who contributed to 0.10, you can see the names in the full changelog linked above.</p>

<p>You can checkout Boa via <a href="https://github.com/boa-dev/boa">Github</a> or on <a href="https://crates.io/crates/Boa">crates.io</a></p>

  </article>

</div>

      </div>
    </div></div>]]>
            </description>
            <link>https://boa-dev.github.io/2020/10/02/boa-release-10.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24662756</guid>
            <pubDate>Fri, 02 Oct 2020 14:09:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Effective Ways to Market Yourself as a Developer]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24662671">thread link</a>) | @codersrank
<br/>
October 2, 2020 | https://blog.codersrank.io/the-5-most-effective-ways-to-market-yourself-as-a-developer/ | <a href="https://web.archive.org/web/*/https://blog.codersrank.io/the-5-most-effective-ways-to-market-yourself-as-a-developer/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>


			
<p>Knowing Javascript inside and out or all the programming languages in the world won’t be enough to help you land a great job that pays incredibly well or secure those amazing opportunities that are hard to come by.</p>



<p>If you want to truly advance your career and succeed as a developer, you need to market yourself. Sure, coding is your passion and you’d rather bury yourself in it, but how would anyone know that you’re good at what you do or even discover you unless you put yourself out there?</p>



<p>When screening for developer positions, many companies pay attention to candidates who have active <a href="https://blog.codersrank.io/profile-2-0/" target="_blank" rel="noreferrer noopener">programmer profiles</a>—a blog, podcast, open-source contributions, YouTube channel, or a history of speaking at tech events—that speaks to their abilities. They view developers with these achievements and experiences as more likely to be talented because their reputation suggests it.</p>



<p><a href="https://giphy.com/gifs/the-office-michael-scott-5B6PQ4lDOlbB6">via GIPHY</a></p>



<p>A little marketing can help you shine brightly in the eyes of potential employers. The last thing you want is to be just another candidate or resume when applying for a job. You need to find a way to stand out from the other stack of papers so that companies are pushed to invite you for an interview or make you an offer.</p>



<p>With effective marketing, you may not even have to go job hunting, opportunities will come knocking at your door. When you share your goals, skills, experiences, and knowledge about your field publicly, it helps to establish you as an expert. As such, companies will be happy to pay you a premium salary rather than hiring one of your seemingly less qualified counterparts.</p>



<p>Now that you understand how important marketing is and how it can enhance your reputation and turn you into a job magnet, we’re going to walk you through actionable steps you can take to successfully market yourself as a developer, stand out from the competition, get on recruiters’ radars, and bag the job offer of your dreams.</p>



<p>We have ranked these steps according to how useful and important they can be in helping you market yourself and your skills effectively.</p>







<ol><li><a href="#portfolio">Build your portfolio</a></li><li><a href="#brand">Build a personal brand</a></li><li><a href="#codersrank">Register a profile on CodersRank</a></li><li><a href="#network">Network with fellow tech professionals</a></li><li><a href="#linkedin">Tidy up your LinkedIn profile</a></li></ol>



<p>Following these steps will help you enlarge your horizons and place your best foot forward so life-changing opportunities can find you. While other developers are scrambling to submit resumes and nail their technical interviews, you’ll already be far ahead.</p>



<p>Let’s take an in-depth look into each of these steps and how you can use them to enhance your career as a software developer and go from chasing after the prize to becoming the prize.</p>



<h2 id="portfolio" data-amp-original-style="color:#50b0ba"><strong>1. Build your portfolio</strong></h2>



<p>As a developer, you know that you need to keep practicing and refining your skills. What you might not know is that you can use the assignments and projects you do to create a portfolio that showcases your expertise.</p>



<p>If you don’t already have a GitHub profile, start by creating one and start pushing code to it regularly, and make your experiments public. This is non-negotiable. GitHub is your <a href="https://blog.codersrank.io/the-evolution-of-the-most-popular-repositories-since-2012/" target="_blank" rel="noreferrer noopener">code repository</a> and it should be used to display all the code you’ve written, projects you’ve worked on, and other interesting code-related activities you’ve been involved in.</p>



<p>Your GitHub account is basically your developer resume because it serves as proof of how well you can code. It says more about your skills than any CV or interview can. Contribute to as many open source libraries as you can. The more open source contributions you have the greater the value prospective employers will see in hiring you.</p>



<p>If you’d like to make valuable contributions to open source libraries, but you’re not sure how to go about submitting one, finding projects to contribute to, or even what kind of contributions you can make, check out these resources:</p>



<ul><li><a href="https://opensource.guide/how-to-contribute/" target="_blank" rel="noreferrer noopener">How to contribute to open source</a></li><li><a href="https://auth0.com/blog/a-first-timers-guide-to-an-open-source-project/" target="_blank" rel="noreferrer noopener">A first timer’s guide to an open source project</a></li><li><a href="https://rubygarage.org/blog/how-contribute-to-open-source-projects" target="_blank" rel="noreferrer noopener">How to contribute to open source projects</a></li></ul>



<p>It’s also important to have a portfolio website where potential employers can go to learn more about you, the work you’ve done, and how your skills and experience can benefit their organization.</p>



<p>When building your portfolio, here are the things you’ll want to pay attention to:</p>



<h3>Set up a professional site</h3>



<p>It makes sense for your domain to be in your name since this is a personal portfolio and people should be able to find it by simply entering your name into a search engine. Make sure you purchase the domain so that you can have full control over it and be able to migrate to a different web platform.</p>



<p>You cannot expect employers to take you seriously if you proclaim yourself to be a talented developer, but your website looks shabby and amateurish. You want anyone who stumbles on your site to be immediately impressed by the layout and design even before they go through any of your pages.</p>



<figure><amp-img src="https://file.mockplus.com/image/2019/07/75f1f76e-eeb4-4166-8cf1-3500d6256538.png" alt="" object-fit="contain" width="1170" height="400" layout="intrinsic" i-amphtml-layout="intrinsic"><img src="https://file.mockplus.com/image/2019/07/75f1f76e-eeb4-4166-8cf1-3500d6256538.png" alt="" width="1170" height="400" data-old-src="data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9JzQwMCcgd2lkdGg9JzExNzAnIHhtbG5zPSdodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZycgdmVyc2lvbj0nMS4xJy8+"></amp-img><figcaption><a href="https://www.mockplus.com/blog/post/web-developer-portfolio" target="_blank" rel="noreferrer noopener">Source</a></figcaption></figure>



<p>Your site should be easy to navigate and visually pleasing. You can decide to code your website from scratch and display it as one of the projects in your portfolio or create one using your preferred web platform. Whatever you choose, remember to keep it simple.</p>



<p>Include a well-designed logo that communicates your values and serves as an accurate representation of who you are and what you do.</p>



<p>Keep in mind that a <strong>personal website is going to mean different things to a back end developer and a front end developer</strong> since they’re different fields. Whichever faction you belong to, you just need to find an approach to your website design and presentation that best represents who you are and what you do.</p>



<p>If you’re a newbie developer, this <a href="https://mikkegoes.com/portfolio-site-on-wordpress/" target="_blank" rel="noreferrer noopener">detailed step-by-step guide</a> will help you build a great-looking portfolio website from scratch to showcase your skills and value to potential employers and help you get hired faster. It covers everything from registering a domain name to choosing a reliable website, creating eye-catching home and about me pages, building contact forms, and more.</p>



<p>Check out these <a href="https://www.springboard.com/blog/programmer-portfolio/" target="_blank" rel="noreferrer noopener">7 best practices for creating a programming portfolio website</a> that stands out. The article also contains tips on mistakes to avoid when building your website and what recruiters look for in a developer portfolio, as well as stunning website examples that are sure to get your creativity flowing.</p>



<h3>Showcase your work</h3>



<p><a href="https://giphy.com/gifs/kodewithklossy-coding-karlie-kloss-kwk-ZG719ozZxGuThHBckn">via GIPHY</a></p>



<p>The point of having an online portfolio is to highlight the work you’ve done in the past and the accomplishments you’re proud of. If you don’t have any concrete work experience yet, you can start by creating a single web page and adding links to other online profiles you have like your social media and GitHub account.</p>



<p>When you write articles, host webinars, give talks, contribute to open source libraries, create tutorial videos, or work on anything interesting, update your site accordingly. Explain what each project is about, why it’s important, and when it was done.</p>



<p>Don’t just go on and on about the agile methodologies, frameworks, and programming languages that you know. Display that project you built using Javascript, PHP, CSS, or whatever tech stacks you say you’re familiar with. </p>



<p>Let visitors see the skills you possess in action and how you can use these skills to grow their business. This will establish your expertise, credibility, and trustworthiness.</p>



<h3>Share your story</h3>



<p>Don’t be shy about being yourself. Let your personality shine through. Describe yourself as honestly as you can. What is it that makes you special? What struggles, failures, or challenges have you encountered over the course of your career? Employers don’t want to hire mindless code monkeys, but people they can relate to.</p>



<h3>Include your contact details</h3>



<p>Give visitors and potential employers a way to reach you. Add your email address and phone number or create a simple contact form they can fill out. If they have to jump through hoops to find your contact information, you might miss out on many good opportunities.</p>



<h2 id="brand" data-amp-original-style="color:#50b0ba"><strong>2. Build a personal brand</strong></h2>



<p>Personal branding is simply a way of making yourself known for something. As a programmer, you not only want to be competent in your field, you also want people to see you that way. Thanks to the internet, it’s easier than ever to create a brand around yourself.</p>



<p>Ask yourself what you want to be known for? Who are you and what do you want to represent? What is your core message? What do you want people to think of when they see or hear your name? Once you have this figured out, start putting this message out there and making sure it’s reflected in everything you do.</p>



<p>Here are some of the ways you can create a strong personal brand and actively promote yourself:</p>



<h3>Clean up assets related to communication</h3>



<p>Come up with a logo for your brand if you don’t already have one. It should be something simple, eye-catching, and an accurate representation of who you are and what you’re about. Don’t go changing your logo every week or so. Find one that works for you and use it everywhere.</p>



<p>Get professional headshots taken to use as cover images for your online profiles. Go through your social media accounts and public forums and delete any inappropriate messages or comments that don’t align with the image you want to project or reflect badly on you as a person.</p>



<h3>Start a blog</h3>



<p>A blog can be a wonderful way to showcase your skills as a developer, become well-known in your industry, and attract potential clients or employers.</p>



<p>Your blog doesn’t have to have tens of thousands of readers, but you do need to build a decent audience. You can do this by sharing useful information that adds some kind of value to your readers’ lives. Talk about your professional journey, the challenges you’ve faced along the way, and how you overcame them.</p>



<p>Teach people how to solve problems. Show examples of work you’ve done in the past to help junior developers find their way and build themselves up. Think of something you’ve struggled with that you found a solution …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.codersrank.io/the-5-most-effective-ways-to-market-yourself-as-a-developer/">https://blog.codersrank.io/the-5-most-effective-ways-to-market-yourself-as-a-developer/</a></em></p>]]>
            </description>
            <link>https://blog.codersrank.io/the-5-most-effective-ways-to-market-yourself-as-a-developer/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24662671</guid>
            <pubDate>Fri, 02 Oct 2020 14:00:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Rust lets us monitor 30k API calls/min]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24662530">thread link</a>) | @jerodsanto
<br/>
October 2, 2020 | https://blog.bearer.sh/how-rust-lets-us-monitor-30k-api-calls-min/ | <a href="https://web.archive.org/web/*/https://blog.bearer.sh/how-rust-lets-us-monitor-30k-api-calls-min/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
		<p>At Bearer, we are a polyglot engineering team. Both in spoken languages and programming languages. Our stack is made up of services written in Node.js, Ruby, Elixir, and a handful of others in addition to all the languages our agent library supports. Like most teams, we balance using the right tool for the job with using the right tool for the time. Recently, we reached a limitation in one of our services that led us to transition that service from Node.js to Rust. This post goes into some of the details that caused the need to change languages, as well as some of the decisions we made along the way.</p><h2 id="a-bit-of-context"><strong>A bit of context</strong></h2><p>We are building a solution to help developers monitor their APIs. Every time a customer’s application calls an API, a log gets sent to us where we monitor and analyze it.</p><p>At the time of the issue, we were processing an average of 30k API calls per minute. That's a lot of API calls made across all our customers. We split the process into two key parts: Log ingestion and log processing.</p><figure><img src="https://blog.bearer.sh/content/images/2020/06/Log-ingestion-service---node.jpg" alt="Original architecture with Node.js" srcset="https://blog.bearer.sh/content/images/size/w600/2020/06/Log-ingestion-service---node.jpg 600w, https://blog.bearer.sh/content/images/size/w1000/2020/06/Log-ingestion-service---node.jpg 1000w, https://blog.bearer.sh/content/images/size/w1600/2020/06/Log-ingestion-service---node.jpg 1600w, https://blog.bearer.sh/content/images/size/w2400/2020/06/Log-ingestion-service---node.jpg 2400w"></figure><p>We originally built the ingestion service in Node.js. It would receive the logs, communicate with an elixir service to check customer access rights, check rate limits using Redis, and then send the log to CloudWatch. There, it would trigger an event to tell our processing worker to take over.</p><p>We capture information about the API call, including the payloads (both the request and response) of every call sent from a user's application. These are currently limited to 1MB, but that is still a large amount of data to process. We send and process everything asynchronously and the goal is to make the information available to the end-user as fast as possible.</p><p>We hosted everything on AWS Fargate, a serverless management solution for Elastic Container Service (ECS), and set it to autoscale after 4000 req/min. Everything was great! Then, the invoice came 😱.</p><p>AWS invoices based on CloudWatch storage. The more you store, the more you pay.</p><p>Fortunately, we had a backup plan.</p><h2 id="kinesis-to-the-rescue"><strong>Kinesis to the rescue?</strong></h2><p>Instead of sending the logs to CloudWatch, we would use<a href="https://aws.amazon.com/kinesis/data-firehose/"> Kinesis Firehose</a>. Kinesis Firehose is basically a Kafka equivalent provided by AWS. It allows us to deliver a data stream in a reliable way to several destinations. With very few updates to our log processing worker, we were able to ingest logs from both CloudWatch and Kinesis Firehose. With this change, daily costs would drop to about 0.6% of what they were before.</p><figure><img src="https://blog.bearer.sh/content/images/2020/06/Log-ingestion---node_kinesis.jpg" alt="Architecture after adding Kenesis" srcset="https://blog.bearer.sh/content/images/size/w600/2020/06/Log-ingestion---node_kinesis.jpg 600w, https://blog.bearer.sh/content/images/size/w1000/2020/06/Log-ingestion---node_kinesis.jpg 1000w, https://blog.bearer.sh/content/images/size/w1600/2020/06/Log-ingestion---node_kinesis.jpg 1600w, https://blog.bearer.sh/content/images/size/w2400/2020/06/Log-ingestion---node_kinesis.jpg 2400w"></figure><p>The updated service now passed the log data through Kinesis and into s3 which triggers the worker to take over with the processing task. We rolled the change out and everything was back to normal... or we thought. Soon after, we started to notice some anomalies on our monitoring dashboard.</p><p><strong>We were Garbage Collecting</strong>, a lot. Garbage collection (GC) is a way for some languages to automatically free up memory that is no longer in use. When that happens, the program pauses. This is known as a <em>GC pause</em>. The more writes you make to memory, the more garbage collection needs to happen and as a result, the pause time increases. For our service, these pauses were growing high enough that they caused the servers to restart and put stress on the CPU. When this happens, it can look like the server is down—because it temporarily is—and our customers started to see 5xx errors for roughly 6% of the logs our agent was trying to ingest.</p><p>Below we can see the pause time and pause frequency of the garbage collection:</p><figure><img src="https://blog.bearer.sh/content/images/2020/06/gc-pause.jpg" alt="GC pause and frequency charts" srcset="https://blog.bearer.sh/content/images/size/w600/2020/06/gc-pause.jpg 600w, https://blog.bearer.sh/content/images/size/w1000/2020/06/gc-pause.jpg 1000w, https://blog.bearer.sh/content/images/size/w1600/2020/06/gc-pause.jpg 1600w, https://blog.bearer.sh/content/images/size/w2400/2020/06/gc-pause.jpg 2400w"></figure><p>In some instances, the pause time breached <strong>4 seconds</strong> (as shown on the left), with up to <strong>400 pauses per minute</strong> (as shown on the right) across our instances.</p><p>After some more research, we appeared to be another victim of a<a href="https://github.com/aws/aws-sdk-js/issues/329"> memory leak in the AWS Javascript SDK</a>. We tried increasing the resource allocations to extreme amounts, like autoscaling after 1000 req/min, but nothing worked.</p><h2 id="possible-solutions"><strong>Possible solutions</strong></h2><p>With our backup plan no longer an option, we moved on to new solutions. First, we looked at those with the easiest transition path.</p><h3 id="elixir"><strong>Elixir</strong></h3><p>As mentioned earlier, we are checking the customer access rights using an Elixir service. This service is private and only accessible from within our Virtual Private Cloud (VPC). We have never experienced any scalability issues with this service and most of the logic was already there. We could simply send the logs to Kinesis from within this service and skip over the Node.js service layer. We decided it was worth a try.</p><p>We developed the missing parts and tested it. It was better, but still not great. Our benchmarks showed that there were still high levels of Garbage Collecting, and we were still returning 5xx to our users when consuming the logs. At this point, the heavy load triggered a <a href="https://github.com/benoitc/hackney/issues/594">(now resolved) issue</a> with one of our elixir dependencies.</p><h3 id="go"><strong>Go</strong></h3><p>We considered Golang as well. It would have been a good candidate, but in the end, it is another Garbage Collected Language. While likely more efficient than our previous implementation, as we scale there is a high chance we'd run into similar problems. With these limitations in mind, we needed a better option.</p><h2 id="re-architecting-with-rust-at-the-core"><strong>Re-architecting with Rust at the core</strong></h2><p>In both our original implementation and our backup, the core issue remained the same: garbage collection. The solution was to move to a language with better memory management and no garbage collection. Enter Rust.</p><p>Rust isn't a garbage-collected language. Instead, it relies on a concept called <em>ownership</em>.</p><blockquote>Ownership is Rust’s most unique feature, and it enables Rust to make memory safety guarantees without needing a garbage collector. <br>— <a href="https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html">The Rust Book</a></blockquote><p>Ownership is the concept that often makes Rust difficult to learn and write, but also what makes it so well suited for situations like ours. Each value in Rust has a single owner variable and as a result a single point of allocation in memory. Once that variable goes out of scope the memory is immediately returned.</p><p>Since the code required to ingest the logs is quite small, we decided to give it a try. To test this we addressed the very thing that we had issues with—sending large amounts of data to Kinesis.</p><p>Our first benchmarks proved to be very successful.</p><p>From that point, we were pretty confident that Rust could be the answer and we decided to flesh out the prototype into a production-ready application.</p><p>Over the course of these experiments, rather than directly replacing the original Node.js service with Rust, we restructured much of the architecture surrounding log ingestion. The core of the new service is an <a href="https://www.envoyproxy.io/">Envoy</a> proxy with the Rust application as a sidecar.</p><p>Now, when the Bearer Agent in a user's application sends log data to Bearer, it goes into the Envoy proxy. Envoy looks at the request and communicates with Redis to check things like rate limits, authorization details, and usage quotas. Next, the Rust application running alongside Envoy prepares the log data and passes it through Kinesis into an s3 bucket for storage. S3 then triggers our worker to fetch and process the data so Elastic Search can index it. At this point, our users can access the data in our dashboard.</p><figure><img src="https://blog.bearer.sh/content/images/2020/06/Log-ingestion---rust.jpg" alt="Diagram of new rust service" srcset="https://blog.bearer.sh/content/images/size/w600/2020/06/Log-ingestion---rust.jpg 600w, https://blog.bearer.sh/content/images/size/w1000/2020/06/Log-ingestion---rust.jpg 1000w, https://blog.bearer.sh/content/images/size/w1600/2020/06/Log-ingestion---rust.jpg 1600w, https://blog.bearer.sh/content/images/size/w2400/2020/06/Log-ingestion---rust.jpg 2400w"></figure><p>What we found was that with fewer—and smaller—servers, we are able to process even more data without any of the earlier issues.</p><p>If we look at the latency numbers for the Node.js service, we can see peaks with an average response time nearing 1700ms.</p><figure><img src="https://blog.bearer.sh/content/images/2020/06/before-latency.png" alt="Latency with original Node.js service" srcset="https://blog.bearer.sh/content/images/size/w600/2020/06/before-latency.png 600w, https://blog.bearer.sh/content/images/size/w1000/2020/06/before-latency.png 1000w, https://blog.bearer.sh/content/images/size/w1600/2020/06/before-latency.png 1600w, https://blog.bearer.sh/content/images/size/w2400/2020/06/before-latency.png 2400w"></figure><p>With the Rust service implementation, the latency dropped to below 90ms, even at its highest peak, keeping the average response time below 40ms.</p><figure><img src="https://blog.bearer.sh/content/images/2020/06/after-latency.png" alt="Latency after re-architecture" srcset="https://blog.bearer.sh/content/images/size/w600/2020/06/after-latency.png 600w, https://blog.bearer.sh/content/images/size/w1000/2020/06/after-latency.png 1000w, https://blog.bearer.sh/content/images/size/w1600/2020/06/after-latency.png 1600w, https://blog.bearer.sh/content/images/size/w2400/2020/06/after-latency.png 2400w"></figure><p>The original Node.js application used about 1.5GB of memory at any given time, while the CPUs ran at around 150% load. The new Rust service used about 100MB of memory and only 2.5% of CPU load.</p><h2 id="conclusion"><strong>Conclusion</strong></h2><p>As with most startups, we move fast. Sometimes the best solution at the time isn't the best solution forever. This was the case with Node.js. It allowed us to move forward, but as we grew we also outgrew it. As we started to handle more and more requests, we needed to make our infrastructure evolve to address the new requirements. While this process started with a fix that merely replaced Node.js with Rust, it led to a rethinking of our log ingestion service as a whole.</p><p>We still use a variety of languages throughout our stack, including Node.js, but will now consider Rust for new services where it makes sense.<br></p>
	</section></div>]]>
            </description>
            <link>https://blog.bearer.sh/how-rust-lets-us-monitor-30k-api-calls-min/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24662530</guid>
            <pubDate>Fri, 02 Oct 2020 13:47:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[One function is all you need for ML Experiments]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24662085">thread link</a>) | @LexSiga
<br/>
October 2, 2020 | https://www.logicalclocks.com/blog/hopsworks-ml-experiments | <a href="https://web.archive.org/web/*/https://www.logicalclocks.com/blog/hopsworks-ml-experiments">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><div><div><p>TLDR; Hopsworks provides support for machine learning (ML) experiments. That is, it can automatically track the artifacts, graphs, performance, logs, metadata, and dependencies of your ML programs.Many of you already know about platforms like <a href="https://mlflow.org/">MLflow</a>, so why should you read about Hopsworks Experiments?&nbsp; Because you do not have to rewrite your TensorFlow/PyTorch/Scikit-learn programs to get <strong>tracking and distributed ML for free</strong>, and TensorBoard comes built-in. We discuss how Hopsworks uniquely supports implicit provenance to transparently create metadata and how it is combined with the oblivious training function to make your training distribution transparent.&nbsp;</p><h2>Hopsworks Introduction</h2><p>Hopsworks is a single platform for both data science and data engineering that is available as both an <a href="http://github.com/logicalclocks/hopsworks">open-source platform</a> and a <a href="http://www.hopsworks.ai/">SaaS platform</a>, including a built-in <a href="https://www.logicalclocks.com/hopsworks-featurestore">feature store</a>. You can train models on GPUs at scale, easily install any Python libraries you want using pip/conda, run Jupyter notebooks as jobs, put those jobs in Airflow pipelines, and even write (Py)Spark or Flink applications that run at scale.&nbsp;</p><p>As a development environment, Hopsworks provides a central, collaborative development environment that enables machine learning teams to easily share results and experiments with teammates or generate reports for project stakeholders. All resources have strong security, data governance, backup and high availability support in Hopsworks, while assets are stored in a single distributed file system (with data stored on S3 in the cloud).<br></p><figure id="w-node-a0d33d55738e-b88dcb71"><p><img src="https://uploads-ssl.webflow.com/5e6f7cd3ee7f51ec3ea4da0f/5f74366329e691163a9537f8_mT5Xl3PGQNailTwXRoPhMMlEZePoa3PjlnagKaWj7mJxckcqP1SfcSbkOS3P-adIEnIq7kURxZ-TJ4ypWTt7yw94d_vqkB9o2FMTUrosMB8Pnxz0pPYkehYlOoJySGBdjPuDNQ7I.gif" alt=""></p><figcaption>A Hopsworks ML experiment stores information about your ML training run: logs, images, metrics of interest (accuracy, loss), the program used to train the model, its input training data, and the conda dependencies used. Optional outputs are hyperparameters, a TensorBoard, and a Spark history server.</figcaption></figure><figure id="w-node-06188dbd9c79-b88dcb71"><p><img src="https://uploads-ssl.webflow.com/5e6f7cd3ee7f51ec3ea4da0f/5f743664aeafc406b94b027c_hraL3X_VAEzOtdesnelgqqb4FQcVGC8Q6J0-KQM0UPGGQxgU_TlMb_-LIZuMOszzdZIhxEZogwxlSSfOMdZvcAIgRlLZzoNg2dLmoPUSrNWyK0CABpAglOV9q9SqfogrRxoO6k29.gif" alt=""></p><figcaption>The logs of each hyperparameter trial are retrieved by clicking on its log, and TensorBoard visualizes the different trials results. The TensorBoard HParams plugin is also available to drill down further on the trials.</figcaption></figure><p>When you run a Python or PySpark application on the Hopsworks platform, it can create an<strong> experiment</strong> that includes both the traditional information a program generates (results, logs, errors) as well as ML-specific information to help track, debug, and reproduce your program and its inputs and outputs:</p><ul role="list"><li><strong>hyperparameters</strong>: parameters for training runs that are not updated by the ML programs themselves;&nbsp;</li><li><strong>metrics</strong>: the loss or accuracy of the model(s) trained in this experiment;</li><li><strong>program artifacts</strong>: <em>python/pyspark/airflow</em> <em>programs, </em>and their <em>conda environments</em>;</li><li><strong>model artifacts</strong>: serialized <em>model objects,</em> <em>model schemas</em>, and <em>model checkpoints</em>;</li><li><strong>executions</strong>: information to be able to re-execute the experiment, including parameters, versioned features for input, output files,&nbsp; etc;&nbsp;</li><li><strong>versioned features</strong>: to be able to reproduce an experiment, we need the exact training/test data from the run and how it was created from the feature store;</li><li><strong>visualizations</strong>: images generated during training and score. Also use TensorBoard to visualize training runs - Hopsworks aggregates results from all workers transparently;</li><li><strong>logs (for debugging)</strong>: model weights, gradients, losses, optimizer state;</li><li><strong>custom metadata</strong>: tag experiments and free-text search for them, govern experiments (label as ‘PII’, ‘data-retention-period’, etc), and reproduce training runs.</li></ul><figure id="w-node-55f328a117c2-b88dcb71"><p><img src="https://uploads-ssl.webflow.com/5e6f7cd3ee7f51ec3ea4da0f/5f748ce6a667fe73eb582948_Screenshot%202020-09-30%20at%2015.48.58.png" loading="lazy" alt=""></p></figure><h2>Experiment Tracking and Distributed ML in One Library</h2><figure id="w-node-be7859ba57d5-b88dcb71"><p><img src="https://uploads-ssl.webflow.com/5e6f7cd3ee7f51ec3ea4da0f/5ef49c1e473b2c283eb616aa_nXpYXacWzP67N5MmHnhldoZJw6qVoDwpBnTd3JQzx9nFuX9_FVm-fmWztjYeLdun5BI83RGOdD1ibvFFWBHUCvQGtbenUY1f6haaE58VP5aAHHJOSWpf0P8FJkfPuE5JMAfMlcOk.png" alt=""></p></figure><p>
    -- CODE language-bash --
def train(data_path, max_depth, min_child_weight, estimators): 
    X_train, X_test, y_train, y_test = build_data(..)
    ...
    print("hello world") # monkeypatched - prints in notebook
    ...
    model.fit(X_train, y_train) # auto-logging
    ...
    hops.export_model(model, "tensorflow",..,model_name)
    ...
    # create local files ‘logile.txt’, ‘diagram.png’ 
    return {'accuracy': accuracy, 'loss': loss, 'logfile':
       'logfile.txt', 'diagram': 'diagram.png'} # track dict

from maggy import experiment
experiment.lagom(train, name="My Experiment", ...) 

# To launch as a distributed ML HParam Tuning job:
# sp=Searchspace(max_depth=('INTEGER',[2,8]),min_child_weight
# =('INTEGER', [2, 8]), )
# experiment.lagom(train, name=“HP, optimizer='randomsearch',                          
# direction='max', num_trials=15,)
</p><p>Platforms that support experiment tracking require the user to refactor their training code in a function or some explicit scope (such as “with … as xx:” in MLFlow, see Appendix A) to identify when an experiment begins and when an experiment ends. In Hopsworks, we require the developer to write their training code inside a function.&nbsp;</p><p>We call this Python function an <em>oblivious training function</em> because the function is oblivious of whether it is being run on a Python kernel in a Jupyter notebook or on many workers in a cluster, see our <a href="https://www.logicalclocks.com/blog/unifying-single-host-and-distributed-machine-learning-with-maggy">blog </a>and <a href="https://www.logicalclocks.com/blog/unifying-single-host-and-distributed-machine-learning-with-maggy">Spark/AI summit talk</a> for details. That is, you write your training code once and reuse the same function when training a small model on your laptop or when performing hyperparameter tuning or distributed training on a large cluster of GPUs or CPUs.</p><p>We double down on this “wrapper” Python function by also using it to start/stop experiment tracking. Experiment tracking and distribution transparency in a single function, nice!&nbsp;</p><p>In Hopsworks, the <a href="https://github.com/logicalclocks/maggy">Maggy</a> library runs experiments, see code snippet above. As you can see, the only code changes a user needed compared to a best-practice TensorFlow program are:&nbsp;<br></p><ol role="list"><li>factor the training code in a user-defined function (<strong>def train(..):</strong>);</li><li>return a Python dict containing the results, images, and files that the user wants to be tracked for the experiment and accessible later in the Experiments UI; and</li><li>invoke the training function using the <em>experiment.lagom</em> function.<br></li></ol><p>The hyperparameters can be fixed for a single execution run, or as shown in the last 4 lines of the code snippet, you can execute the <em>train function </em>as a distributed hyperparameter tuning job across many workers in parallel (with GPUs, if needed).&nbsp;</p><p>Hopsworks will automatically:</p><ul role="list"><li>track all parameters of the train function as hyperparameters for this experiment,&nbsp;</li><li>auto-log using Keras callbacks in model.fit;</li><li>create a versioned directory in HopsFS, where a copy of the program, its conda environment, and all logs from all workers are aggregated;</li><li>track all provenance information for this application - input data from HopsFS used in this experiment (train/test datasets from the Feature Store), and all output artifacts (models, model checkpoints, application logs);</li><li>redirect all print statements executed in workers to the Jupyter notebook cell for easier debugging (see GIF below - each print statement is prefixed by the worker ID).<br></li></ul><figure id="w-node-756fb7ca0b23-b88dcb71"><p><img src="https://uploads-ssl.webflow.com/5e6f7cd3ee7f51ec3ea4da0f/5f74366291399a79d18af461_4BATblXBajOTlBXREg6utXoqcRgymvySZx03Hh_JeYvjJ7YLGzxYpnUgMwCvHaNxMFgj_XepZ23xJh3QFnfhHsAetRlt24IkRw4BI8R4Wo5gaQLkVuuIRRbE8CK7swOEJaa-Lm-k.gif" alt=""></p><figcaption>In Hopsworks, logs from workers can be printed in your Jupyter notebook during training. Take that Databricks!</figcaption></figure><h2>TensorBoard support</h2><p>
    -- CODE language-bash --
    
    def train():
    from maggy import tensorboard
    ...
    model.fit(.., callbacks=[TensorBoard(log_dir=tensorboard.logdir(),..)], ...)
</p><p>TensorBoard is arguably the most common and powerful tool used to visualize, profile and debug machine learning experiments. Hopsworks Experiments integrates seamlessly with TensorBoard. Inside the training function, the data scientist can simply import the <em>tensorboard</em> python<em> </em>module and get the folder location to write all the TensorBoard files. The content of the folder is then collected from each Executor and placed in the experiment directory in HopsFS. As TensorBoard supports showing multiple experiment runs in the same graph, visualizing and comparing multiple hyperparameter combinations becomes as simple as starting the TensorBoard integrated in the Experiments service. By default, Tensorboard is configured with useful plugins such as HParam, Profiler, and Debugging.&nbsp;</p><h3>Profiling and debugging</h3><p>Hopsworks 1.4.0 comes with TensorFlow 2.3, which includes the TensorFlow profiler. A new long-awaited feature that finally allows users to profile model training to identify bottlenecks in the training process such as slow data loading or poor operation placement in CPU + GPU configurations.&nbsp;</p><p>TensorFlow 2.3 also includes Debugger V2, making it easy to find model issues such as NaN which are non-trivial to find the root cause of in complex models.<br></p><figure id="w-node-821b5c19e74e-b88dcb71"><p><img src="https://uploads-ssl.webflow.com/5e6f7cd3ee7f51ec3ea4da0f/5f743661e73aacd6dc310040_Rywm-fmUfHouqW1zVdYsZ88LvtAYDvCZPpze3hHJeENCBjPVPkkpy_J-2bescj5Z-Xlb7A7DNpmNws1H4lsmUsuOpLROLO_S16jFM_CI-6JdACYY5Rp3Q3yYVMfkecV7aK7ECsf_.png" alt=""></p></figure><figure id="w-node-fc6081bc518f-b88dcb71"><p><img src="https://uploads-ssl.webflow.com/5e6f7cd3ee7f51ec3ea4da0f/5f743662849afea39f14cb69_S0v-cuc6N5MT4gsC3KUBFa3dQi7ZZBEaF9w684FzTrmXH4FPHkDEFCaMy2ThIpmSHDHSY-vmXCvXyDVrMVS_FYy3vnODkL8uXcHrm4uIlNjhNHHhsxoMghDrFfX_Yn_eVe1eYBbE.png" alt=""></p></figure><h2>Model Registry</h2><figure id="w-node-6cad6e721c68-b88dcb71"><p><img src="https://uploads-ssl.webflow.com/5e6f7cd3ee7f51ec3ea4da0f/5f743662ec097905b0778b6b_KcTuR12rSnRVQSSDh-eLxqM0ad3eAXSGCkehOJ8ik2BPfnohgCfudLLx7HkUIk3DKfTTxz-DzRwlJ7OAYU0eafq0bwSN2tYy7dt_rnOeth550yYPqa-esKRO6uGREvB1C4iNjk3l.png" alt=""></p></figure><p>In the training code models may be exported and saved to HopsFS. Using the <em>model </em>python module in the <a href="https://hops-py.logicalclocks.com/">hops library</a>, it is easy to version and attach meaningful metadata to models to reflect the performance of a given model version.&nbsp;</p><p>The Hopsworks Model Registry, is a service where all models are listed in addition to useful information such as which user created the model, different versions, time of creation and evaluation metrics such as accuracy.&nbsp;</p><p>The Model Registry provides functionality to filter based on the model name, version number and the user that exported the model. Furthermore the evaluation metrics of model versions can be sorted in the UI to find the best version for a given model.&nbsp;</p><p>In the Model Registry UI, you can also navigate to the experiment used to train the model, and from there to the train/test data used to train the model, and from there to the features in the feature store used to create the train/test data. Thanks, provenance!<br></p><h3>Exporting a model</h3><p>A model can be exported programmatically by using the <em>export</em> function in the <em>model</em> module. Prior to exporting the model, the experiment needs to have written a model to a folder or to a path on HopsFS. Then that path is supplied to the function along with the name of the model and the evaluation metrics that should be attached. The <em>export</em> call will upload the contents of the folder to your Models dataset and it will also appear in the Model Registry with an incrementing version number for each export.</p><p>
    -- CODE language-bash --
    from hops import model

# local path to directory containing model (e.g. .pb or .pk) 
path = os.getcwd() + “/model_dir”

# uploads path to the model repository, metadata is a dict of metrics</p></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.logicalclocks.com/blog/hopsworks-ml-experiments">https://www.logicalclocks.com/blog/hopsworks-ml-experiments</a></em></p>]]>
            </description>
            <link>https://www.logicalclocks.com/blog/hopsworks-ml-experiments</link>
            <guid isPermaLink="false">hacker-news-small-sites-24662085</guid>
            <pubDate>Fri, 02 Oct 2020 12:55:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Fistful of States: More State Machine Patterns in Rust]]>
            </title>
            <description>
<![CDATA[
Score 95 | Comments 22 (<a href="https://news.ycombinator.com/item?id=24661395">thread link</a>) | @lukastyrychtr
<br/>
October 2, 2020 | https://deislabs.io/posts/a-fistful-of-states/ | <a href="https://web.archive.org/web/*/https://deislabs.io/posts/a-fistful-of-states/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      

<p>Earlier this year, DeisLabs released Krustlet, a project to implement Kubelet
in Rust. <sup id="fnref:Fisher"><a href="#fn:Fisher">1</a></sup> <sup id="fnref:Squillace"><a href="#fn:Squillace">2</a></sup> Kubelet is the component of Kubernetes that
runs on each node which is assigned Pods by the control plane and runs them on
its node. Krustlet defines a flexible API in the <code>kubelet</code> crate, which allows
developers to build Kubelets to run new types of workloads. The project
includes two such examples, which can run Web Assembly workloads on WASI or
waSCC runtimes. <sup id="fnref:wasmtime"><a href="#fn:wasmtime">3</a></sup> <sup id="fnref:waSCC"><a href="#fn:waSCC">4</a></sup> Beyond this, I have been working to
develop a Rust Kubelet for traditional Linux containers using the Container
Runtime Interface (CRI). <sup id="fnref:krustlet-cri"><a href="#fn:krustlet-cri">5</a></sup> <sup id="fnref:CRI"><a href="#fn:CRI">6</a></sup></p>

<p>Over the last few releases, Krustlet has focused on expanding the functionality
of these Web Assembly Kubelets, such as adding support for init containers,
fixing small bugs, and log streaming. This, in turn, has built quite a bit of
interest in alternative workloads and node architectures on Kubernetes, as well
as demonstrated the many strengths of Rust for development of these types of
applications.</p>

<p>For the <code>v0.5.0</code> release, we turned our attention to the internal architecture
of Krustlet, in particular how Pods move through their lifecycle, how
developers write this logic, and how updates to the Kubernetes control plane
are handled. <sup id="fnref:Pod-Lifecycle"><a href="#fn:Pod-Lifecycle">7</a></sup> We settled upon a state machine implementation
which should result in fewer bugs and greater fault tolerance. This refactoring
resulted in substantial changes for consumers of our API; however we believe
this will result in code that is much easier to reason about and maintain. For
an excellent summary of these changes, and description of how you can migrate
code that depends on the <code>kubelet</code> crate, please see Taylor Thomas’ excellent
<a href="https://github.com/deislabs/krustlet/releases/tag/v0.5.0">Release Notes</a>.
In this post I will share a deep dive into our new architecture and the
development journey which led to it.</p>

<h2 id="the-trailhead">The Trailhead</h2>

<p>Before <code>v0.5.0</code>, developers wishing to implement a Kubelet using Krustlet
primarily needed to implement the <code>Provider</code> trait, which allowed them to write
methods for handling events like <code>add</code>, <code>delete</code>, and <code>logs</code> for Pods scheduled
to that node. This offered a lot of flexibility, but was a very low-level API.
We identified a number of issues with this architecture:</p>

<ul>
<li>The entire lifecycle of a Pod was defined in 1 or 2 monolithic methods of the
<code>Provider</code> trait. This resulted in messy code and a very poor understanding
of error handling in the many phases of a Pod’s lifecycle.</li>
<li>Pod and Container status patches to the Kubernetes control plane were
scattered throughout the codebase, both in the <code>kubelet</code> crate and the
<code>Provider</code> implementations. This made it very difficult to reason about what
was actually reported back to the user and when, and involved lot of repeated
code.</li>
<li>Unlike the Go Kubelet, if Krustlet encountered an error it would report the
error back to Kubernetes and then (most of the time) end execution of the
Pod. There was no built-in notion of the reconciliation loop that one expects
from Kubernetes.</li>
<li>We recognized that a lot of these issues were left to each developer to
solve, but were things that any Kubelet would need to handle. We wanted to
move this kind of logic into the <code>kubelet</code> crate, so that each provider did
not have to reinvent things.</li>
</ul>

<h2 id="our-mission">Our Mission</h2>

<p>At its core, Kubernetes relies on declarative (mostly immutable) manifests, and
controllers which run reconciliation loops to drive cluster state to match this
configuration. Kubelet is no exception to this, with its focus being
indivisible units of work, or Pods. Kubelet simply monitors for changes to Pods
that have been assigned to it by <code>kube-scheduler</code>, and runs a loop to attempt
to run this work on its node. In fact, I would describe Kubelet as no different
from any other Kubernetes controller, except that it has the additional
first-class capability for streaming logs and exec sessions. However these
capabilities, as they are implemented in Krustlet, are orthogonal to this
discussion.</p>

<p>Our goal with this rewrite was to ensure that Krustlet would mirror the official
Kubelet’s behavior as closely as possible. We found that many details about
this behavior are undocumented, and spent considerable time running the
application to infer its behavior and inspecting the Go source code. Our
understanding is as follows:</p>

<ul>
<li>The Kubelet watches for Events on Pods that have been scheduled to it by
<code>kube-scheduler</code>.</li>
<li>When a Pod is added, the Kubelet enters a control loop to attempt to run the
Pod which only exits when the Pod is <code>Completed</code> (all containers
exit successfully) or <code>Terminated</code> (Pod is marked for deletion via the
control plane and execution is interrupted).</li>
<li>Within the control loop, there are various steps such as <code>Image Pull</code>,
<code>Starting</code>, etc., as well as back-off steps which wait some time before
retrying the Pod. At each of these steps, the Kubelet updates the control
plane.</li>
</ul>

<p>We recognized this pretty quickly as a finite-state machine design pattern,
which consists of infallible state handlers and valid state transitions. This
allows us to address the issues mentioned above:</p>

<ul>
<li>Break up the <code>Provider</code> trait methods for running the Pod into short,
single-focus state handler methods.</li>
<li>Consolidate status patch code to where a Pod enters a given state.</li>
<li>Include error and back-off states in the state graph, and only stop
attempting to execute a Pod on <code>Terminated</code> or <code>Complete</code>.</li>
<li>Move as much of this logic into <code>kubelet</code> as possible so that providers need
only focus on implementing the state handlers.
<br></li>
</ul>

<p>With this architecture it becomes very easy to understand the behavior of the
application, and strengthens our confidence that the application will not enter
undefined behavior. In addition, we felt that Rust would allow us to achieve
our goals while presenting an elegant API to developers, and with full
compile-time enforcement of our state machine rules.</p>

<h2 id="our-animal-guide">Our Animal Guide</h2>

<p>When first discussing the requirements of our state machine, and the daunting
task of integrating it with the existing Krustlet codebase, we recalled an
excellent blog post,
<a href="https://hoverbear.org/blog/rust-state-machine-pattern/">Pretty State Machine Patterns in Rust</a>,
by Ana Hobden (hoverbear), which I think has inspired a lot of Rust developers.
The post explores patterns in Rust for implementing state machines which
satisfy a number of constraints and leverage Rust’s type system. I encourage
you to read the original post, but for the sake of this discussion I will
paraphrase the final design pattern here:</p>

<pre><code>struct StateMachine&lt;S&gt; {
    state: S,
}

struct StateA;

impl StateMachine&lt;StateA&gt; {
    fn new() -&gt; Self {
        StateMachine {
            state: StateA
        }
    }
}

struct StateB;

impl From&lt;StateMachine&lt;StateA&gt;&gt; for StateMachine&lt;StateB&gt; {
    fn from(val: StateMachine&lt;StateA&gt;) -&gt; StateMachine&lt;StateB&gt; {
        StateMachine {
            state: StateB 
        }
    }
}

struct StateC;

impl From&lt;StateMachine&lt;StateB&gt;&gt; for StateMachine&lt;StateC&gt; {
    fn from(val: StateMachine&lt;StateB&gt;) -&gt; StateMachine&lt;StateC&gt; {
        StateMachine {
            state: StateC 
        }
    }
}

fn main() {
    let in_state_a = StateMachine::new();

    // Does not compile because `StateC` is not `From&lt;StateMachine&lt;StateB&gt;&gt;`.
    // let in_state_c = StateMachine::&lt;StateC&gt;::from(in_state_a);

    let in_state_b = StateMachine::&lt;StateB&gt;::from(in_state_a);

    // Does not compile because `in_state_a` was moved in the line above.
    // let in_state_b_again = StateMachine::&lt;StateB&gt;::from(in_state_a);

    let in_state_c = StateMachine::&lt;StateC&gt;::from(in_state_b);
}
</code></pre>

<p>Ana introduces a number of requirements for a good state machine
implementation, and achieves them with concise and easily interpretable code.
In particular, these requirements (some based on the definition of a state
machine, and some on ergonomics) were a high priority for us:</p>

<ul>
<li>One state at a time.</li>
<li>Capability for shared state.</li>
<li>Only explicitly defined transitions should be permitted.</li>
<li>Any error messages should be easy to understand.</li>
<li>As many errors as possible should be identified at <strong>compile-time</strong>.</li>
</ul>

<p>In the next section I will discuss some additional requirements that we
introduced and how these impacted the solution. In particular, we relaxed some
of Ana’s goals in exchange for greater flexibility, while satisfying those
listed above.</p>

<h2 id="tribulation">Tribulation</h2>

<p>We were off to a great start, but it was time to consider how we want
downstream developers to interact with our new state machine API. In
particular, while the Kubelets we are familiar with all follow roughly the same
Pod lifecycle, we wanted developers to be able to implement arbitrary state
machines for their Kubelet. For example, some workloads or architectures may
need to have additional provisioning states for infrastructure or data, or
to introduce post-run states for proper garbage collection of resources.
Additionally, it felt like an anti-pattern to have a parent method (<code>main</code> in
the example above) which defines the logic for progressing through the states,
as this felt like having two sources of truth and was not something we could
implement on behalf of our downstream developers for arbitrary state machines.
Ana had discussed how to hold the state machine in a parent structure using an
<code>enum</code>, but it felt clunky to introduce large match statements which could
introduce runtime errors.</p>

<p>We knew that to allow arbitrary state machines we would need a <code>State</code> trait to
mark types as valid states. We felt that it would be possible for this trait to
have a <code>next()</code> method which runs the state and then returns the next <code>State</code>
to transition to, and we wanted our code to be able to simply call <code>next()</code>
repeatedly to drive the machine to completion. This pattern, we soon found,
introduced a number of challenges.</p>

<pre><code>/// Rough pseudocode of our plan.

trait State {
    /// Do work for this state and return next state.
    async fn next(self) -&gt; impl State;
}

fn drive_state_machine(mut state: impl State) {
    loop {
        state = state.next().await;
    }
}
</code></pre>

<h3 id="what-does-next-return">What does <code>next()</code> return?</h3>

<p>Within our loop, we are repeatedly overwriting a local variable with
<em>different</em> types that all implement <code>State</code>. …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://deislabs.io/posts/a-fistful-of-states/">https://deislabs.io/posts/a-fistful-of-states/</a></em></p>]]>
            </description>
            <link>https://deislabs.io/posts/a-fistful-of-states/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24661395</guid>
            <pubDate>Fri, 02 Oct 2020 11:16:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Privacy is the most important concept of our time]]>
            </title>
            <description>
<![CDATA[
Score 416 | Comments 198 (<a href="https://news.ycombinator.com/item?id=24661271">thread link</a>) | @umilegenio
<br/>
October 2, 2020 | https://inre.me/why-privacy-is-the-most-important-concept-of-our-time/ | <a href="https://web.archive.org/web/*/https://inre.me/why-privacy-is-the-most-important-concept-of-our-time/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-25770" itemtype="https://schema.org/CreativeWork" itemscope="itemscope">

	
	
<div>

	
	<!-- .entry-header -->

	
	<div itemprop="text">

		
		
<p><em>In case you are coming from Hacker News and are confused about some comments, be aware that I updated the essay to deal with some criticism</em>.</p>



<p>The title is not hyperbole. I do think that privacy is the most important concept of our time. Let me tell you why:</p>



<ul><li><strong>internet is not a virtual world anymore</strong>, it is a dimension that permeates our lives; we work, socialize and get informed through the internet</li><li><strong>our society is more diverse</strong>; we have some things in common with our neighbors and some with separate communities</li><li><strong>privacy is integral to separate the</strong> <strong>different parts of our lives</strong>; once the separation could be just physical and accidental (i.e., you live here and work there), now it must be built intentionally because there are no natural barriers in information spreading</li></ul>



<p>In short, since our lives are more complex, both socially and technologically, we need to evolve our understanding of rules and norms, too. </p>



<p>I have always believed in the importance of privacy, but I felt that common definitions (e.g., <em>the right to be left alone</em>) were lacking. In fact, I think that the whole conceptualization of privacy as simply a right of an individual and regarding private information as partial and limiting.</p>



<p>Think about this: the government can send policemen to surveil you and everybody they deem interesting. However, it can do this only for few people. <strong>This limitation is due to physical constraints, not legal ones</strong>. There is a limited number of policemen and you would notice if there was a police car in front of each house of the neighborhood. <strong>This is not true for internet communications: the government can spy everyone at once and you would never notice</strong>. As many whistleblowers have revealed, this is what the NSA has actually done.</p>



<p>So, the changes in reality affect privacy directly but may also affect every single all our rights indirectly. Privacy is the fundamental principle that must respond to these changes.</p>



<p>You might say that then, maybe, I am not really thinking about privacy, but rather something else. That might be true, so let’s not talk about privacy, instead let’s talk about <a href="https://en.wikipedia.org/wiki/Definitions_of_fascism#Umberto_Eco">Ur-Privacy</a>, the principles of any possible concept of privacy. Take this essay as the opinion of a random guy that cares about the issue.</p>



<div><h2>What is Ur-Privacy</h2><p><em>A few principles for privacy</em></p></div>







<p><strong>Privacy is not just something we need to separate our private live from our public live. It is necessary to separate our private live, the communities we belong to and the public sphere from each other.</strong></p>



<p><strong>Privacy is about boundaries.</strong> It is not about hiding something but allowing to create a space with rules decided by its members. I like to compare it to borders. Some people say that borders are a restriction, something that limit freedom of movement and we do not need in the contemporary world. As if they were arbitrary obstacles put there by petty people. It almost makes sense if you do not think about them, after all you are actually stopped at a border.</p>



<p>However, that is not true, that is not why they exist. Borders delimit the area that a certain state control, an area where a specific set of rules and laws applies. There was a time before borders, in fact most of human history did not have clear borders. It was not a time of freedom, but anarchy, where bands of barbarians could roam into your home and pillage everything.</p>



<p>In this context is also important to remember that before the <a href="https://en.wikipedia.org/wiki/Peace_of_Westphalia">Peace of Westphalia</a> modern European states were plagued by continual wars. The short version is that this was due to the combination of two facts:</p>



<ul><li>modernity begets differences, different kings choose different religions<sup><a id="link_1" href="#note_1" data-type="internal" data-id="#note_1">1</a></sup> and separated societies</li><li>however, the legitimacy of kings was still based on shared medieval ideals, like the concept of divine rule</li></ul>



<p>In short, the issue was not that <strong>leaders wanted to make war all the time, they needed to do so</strong> because the legitimacy of their power depended, at least on some level, to what the rest of the European world was doing. If you claim to be a divine king there better be agreement on what the divine is, otherwise a guy that picks a different religion can also pick a different king. And, according to some, he could be a legitimate king. To change the situation this peace treaty established the principle that the internal affairs of a state are the exclusive interest of said state.</p>



<p>The connection with privacy is this: without clear rules on what is private and what is public, nobody knows which stuff belongs to whom. This means chaos and often that all belong to the strongest. Somebody might say that what you do in private, it is not private at all but political. It concerns the society at large. Therefore, it must be regulated according to their rules.</p>



<p><strong>Privacy does not imply hiding the truth.</strong> <strong>Meaning depends on context, therefore everything should be considered within its context.</strong></p>



<p><strong>Privacy is about control</strong>. Without privacy we cannot decide for ourselves how to live our lives. If there is no privacy, all become public. Whoever has more power and an interest can affect your life according to their own rules. Then, I have to care about what other people think, otherwise they will control how I can behave. As before the peace of Westphalia, the issue is not that other people are bad, <em>they have to do it</em>. When everything is subject to public scrutiny, you either control the rules and judge others or you are judged and controlled by others.</p>



<p>Think about this way: we say a lot of things in our private lives that are not meant to be taken literally. In private we say something and then we add: <em>you know what I mean</em>. And that is actually true. We can do that because the people we talk to in private know us; they understand the context in which our words must be understood. And even more importantly: they care about us; they do not want to intentionally misunderstand us.</p>



<p>When I was a child I would sometimes say and think that I wanted to kill my brother. I did not mean it literally and everybody knew it. If I said the same thing now, in public, to somebody that does not know me, the phrase would be different. It would be a threat.</p>



<p>Why is that? They are the exact same words. You know why, of course. I am different and the context is different. The real meaning of something, whether an action or a word, is not absolute, in most cases it is relative. When we speak in public, we share a different context, therefore our words have a different meaning. </p>



<p>So even if I say something as a hyperbole, or something that can be construed as an implicit threat (e.g., <em>they must be stopped at all costs</em>!), they might protest. You might say that they are overreacting, that it was just a joke, but how can they be sure of it? <strong>They do not know me.</strong> It is true that acts of violence are prepared by violent words. Even if you are unsure if something is really violent, you have to take a stand. You have to make clear that any attack against you is not permissible. Otherwise, <a href="https://en.wikipedia.org/wiki/Christchurch_mosque_shootings" data-type="URL" data-id="https://en.wikipedia.org/wiki/Christchurch_mosque_shootings">somebody, maybe a crazy guy, might think that it is permissible and the right course of action</a>. Somebody might feel legitimated to take your land and kingdom.</p>



<p><strong>Privacy is not just needed to protect us from the government or exceptional situations. It is about understanding the rules that applies to every aspect of our life so that they can be fair for everybody.</strong></p>



<p><strong>Privacy is about everyday life</strong>. The issue is not simply that something we say can be considered a threat. When you are communicating with someone you need to be able to understand them. Communication requires a shared understanding at some level.</p>



<p>The easiest example to understand this are work discussions. When we talk with people that work in our field, we can communicate more easily the impact of a choice. This goes beyond the ability to use technical terminology: we know which are the main things to care about. The same discussion with our bosses would be different. Even to make them understand the basic strength and weaknesses would be more challenging.</p>



<p>Now imagine being forced to communicate everything you do in the most general terms to people that do not care about you, because <strong>everybody can see you</strong>. So, they can use any piece of information for their own needs. This could mean a policeman investigating you. It could also mean a company making you pay more for a pair sneakers, because they know you have much disposable income you have and that you really love sneakers.</p>



<p>We need privacy to be aware of what is happening to us. It is too much to demand we know how other people interpret what we say. However, it is not excessive to ask that we can control what is shared about us.</p>



<div><h2>Privacy Affects Everything</h2><p><em>Defending privacy would require all-around changes</em></p></div>







<p><strong>Privacy is the most important concept of our time, because it influences everything else. Without privacy we do not know what rules applies. Our lives will be judged according to the rules of somebody else</strong> <strong>in ways we cannot even imagine.</strong></p>



<p>We cannot discuss all of the possible implications of privacy on other rights, so let’s see the example of <em>freedom of speech</em>. Of course, sometimes you can also be judged for who you are: your religion or lack thereof, political opinion or sexual orientation.</p>



<blockquote><p>Give me six lines written by the most honest man, and there I will find something to hang him.</p><cite>Cardinal Richelieu</cite></blockquote>



<p>People lost jobs and had their lives ruined, because the mob judged something they said in private in a different way from what they expected. And they paid a price. You might say: that was fair. We might judge ourselves by our intentions, but others by their actions, which are real and objective.</p>







<p>I, for once, disagree with XKCD and this view. There are a couple of different issues here:</p>



<ul><li>how we should react to speech we disagree with</li><li>what was meant to be shared among friends was taken out of context and made public</li></ul>



<p>This complicates the whole matter. At a first glance the first …</p></div></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://inre.me/why-privacy-is-the-most-important-concept-of-our-time/">https://inre.me/why-privacy-is-the-most-important-concept-of-our-time/</a></em></p>]]>
            </description>
            <link>https://inre.me/why-privacy-is-the-most-important-concept-of-our-time/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24661271</guid>
            <pubDate>Fri, 02 Oct 2020 10:55:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Flatpak: A security nightmare – two years later]]>
            </title>
            <description>
<![CDATA[
Score 365 | Comments 320 (<a href="https://news.ycombinator.com/item?id=24661126">thread link</a>) | @krimeo
<br/>
October 2, 2020 | https://www.flatkill.org/2020/ | <a href="https://web.archive.org/web/*/https://www.flatkill.org/2020/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">


<p>Two years ago I <a href="https://flatkill.org/">wrote</a> about then heavily-pushed Flatpak, self-proclaimed "Future of Apps on Linux". The article criticized the following three major flows in Flatpak:</p><ul>
<li>Most of the apps have full access to the host system but users are misled to believe the apps are sandboxed
</li><li>The flatpak runtimes and apps do not get security updates
</li><li>Flatpak breaks many aspects of desktop integration
</li></ul>

<!-- <p>A lot has changed in the 2 years (the company behind Flatpak is now just another IBM's brand for one thing) so let's see how Flatpak developers addressed these fundamental issues.</p> -->
<p>So let's see how Flatpak developers addressed these fundamental issues.</p>

<h2>The sandbox is STILL a lie</h2>

<p>Almost all popular apps on Flathub still come with <span>filesystem=host</span> or <span>filesystem=home</span> permissions, in other words, <b>write access to the user home directory</b> (and more) so all it takes to escape the sandbox is trivial <span>echo download_and_execute_evil &gt;&gt; ~/.bashrc</span>. That's it.</p>


<p>The most popular applications on Flathub still suffer from this - Gimp, VSCodium, PyCharm, Octave, Inkscape, Audacity, VLC are still not sandboxed.</p>

<p>And, indeed, users are still mislead by the reassuring blue "sandboxed" icon. Two years is not enough to add a warning that an application is <b>not</b> sandboxed if it comes with dangerous permissions (like full access to your home directory)? Seriously?</p>

<img src="https://www.flatkill.org/2020/sandboxlie.png" alt="sandboxlie">

<h2>Flatpak apps and runtimes STILL contain long known security holes</h2>
<p>It took me about 20 minutes to find the first vulnerability in a Flathub application with full host access and I didn't even bother to use a vulnerability scanner.</p>

A perfect example is <a href="https://www.cvedetails.com/cve/CVE-2019-17498">CVE-2019-17498</a> with public exploit <a href="https://github.com/github/securitylab/tree/main/SecurityExploits/libssh2/out_of_bounds_read_disconnect_CVE-2019-17498">available</a> for some 8 months. The first app on Flathub I find to use libssh2 library is Gitg and, indeed, it does ship with unpatched libssh2.

<p>But is it just this one application? Let's look at the <b>official runtimes</b> at the heart of Flatpak (org.freedesktop.Platform and org.gnome.Platform <b>3.36</b> - as of time of writing used by most of the applications on Flathub). The first unpatched vulnerable dependency I found in the offical runtime is ffmpeg in version 4.2.1 with no security patches backported, <a href="https://www.cvedetails.com/cve/CVE-2020-12284">CVE-2020-12284</a>.</p><p>Recently I stumbled upon an article from 2011 which started what is today known as flatpak, <a href="https://people.gnome.org/~alexl/glick2">in the words of the project founder:</a></p>

<p><a href="https://people.gnome.org/~alexl/glick2"><b><i>"Another problem is with security (or bugfix) updates in bundled libraries. With bundled libraries its much harder to upgrade a single library, as you need to find and upgrade each app that uses it. Better tooling and upgrader support can lessen the impact of this, but not completely eliminate it."</i></b></a></p>

<p>After reading that it comes as no surprise flatpak still suffers from the same security issues as 2 years ago because flatpak developers knew about these problems from the beginning.</p>

<h2>Local root exploits are NOT considered a minor issue anymore!</h2>
<p>Great! Two years ago I wrote about a trivial local root exploit using flatpak to install suid binaries (<a href="https://www.cvedetails.com/cve/CVE-2017-9780/">CVE-2017-9780</a>) and how it was downplayed by Flatpak developers as a minor security issue <a href="https://github.com/flatpak/flatpak/releases/tag/0.8.7">here</a>. I am happy to see at least the attitude to local root exploits has changed and today <a href="https://github.com/containers/bubblewrap/security/advisories/GHSA-j2qp-rvxj-43vj">local root exploits</a> are considered high severity.

</p><h2>Desktop integration</h2>
<p>System and user fonts are now available to flatpak applications and basic font rendering settings are respected as well, however do not expect your changes in /etc/fonts, typically setting a proper fallback font for CJK characters, to work with flatpak.  KDE applications in flatpak are still ignoring themes, fonts and icon settings (tested with Qt5ct). Applications installed from the distribution sources do not have these problems, of course. <a href="https://www.flatkill.org/2020/desktopbrokenation.mp4">A quick screen capture to demonstrate</a>.</p>

<p>More importantly, fcitx, <i>the</i> IME for Chinese is still broken - it has been 2 years. Here is the <a href="https://github.com/flatpak/flatpak/issues/2031">issue</a> I linked 2 years ago - especially of interest is <a href="https://github.com/flatpak/flatpak/issues/2031#issuecomment-655134889">the following comment</a> directly from fcitx developer:

</p><p><i>"Because fcitx im module in flatpak is from 4.2.97 and using a different dbus object path. <b>It need to be the same version of fcitx on your host</b>."</i></p>

So I need to run multiple fcitx daemons on my desktop and switch between them as I switch flatpak apps depending on which fcitx libraries are bundled with that app or maybe in the future of linux apps it's not possible to type chinese anymore and it's fine?

<p>While the "bundle everything" approach has proven very useful on servers it clearly does not work for desktop applications, let's keep linking system libraries in desktop applications (and use the bundled libraries as a fallback only) to avoid introducing all these problems to Linux desktop.</p>



</div>]]>
            </description>
            <link>https://www.flatkill.org/2020/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24661126</guid>
            <pubDate>Fri, 02 Oct 2020 10:35:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Node.js malware caught posting IPs, username, and device info on GitHub]]>
            </title>
            <description>
<![CDATA[
Score 31 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24660810">thread link</a>) | @axsharma
<br/>
October 2, 2020 | https://securityreport.com/nodejs-malware-caught-exfiltrating-ips-username-and-device-information-on-github/ | <a href="https://web.archive.org/web/*/https://securityreport.com/nodejs-malware-caught-exfiltrating-ips-username-and-device-information-on-github/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                                    <!-- .entry-header -->

                
                        <div>
                                    <p><img width="1024" height="683" src="https://securityreport.com/wp-content/uploads/2020/10/sig5rzqmv3o-1024x683.jpg" alt="red and blue hearts illustration" loading="lazy" srcset="https://securityreport.com/wp-content/uploads/2020/10/sig5rzqmv3o-1024x683.jpg 1024w, https://securityreport.com/wp-content/uploads/2020/10/sig5rzqmv3o-300x200.jpg 300w, https://securityreport.com/wp-content/uploads/2020/10/sig5rzqmv3o-768x512.jpg 768w, https://securityreport.com/wp-content/uploads/2020/10/sig5rzqmv3o-1536x1024.jpg 1536w, https://securityreport.com/wp-content/uploads/2020/10/sig5rzqmv3o-2048x1365.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px" data-srcset="https://securityreport.com/wp-content/uploads/2020/10/sig5rzqmv3o-1024x683.jpg 1024w, https://securityreport.com/wp-content/uploads/2020/10/sig5rzqmv3o-300x200.jpg 300w, https://securityreport.com/wp-content/uploads/2020/10/sig5rzqmv3o-768x512.jpg 768w, https://securityreport.com/wp-content/uploads/2020/10/sig5rzqmv3o-1536x1024.jpg 1536w, https://securityreport.com/wp-content/uploads/2020/10/sig5rzqmv3o-2048x1365.jpg 2048w" data-src="https://securityreport.com/wp-content/uploads/2020/10/sig5rzqmv3o-1024x683.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">                </p>
            
                                            </div>

            

        <!-- end slider-section -->
                                    

    <div>
        <div>
            




<p>Multiple NodeJS packages laden with malicious code have been spotted on npm registry.</p>



<p>These “typosquatting” packages served no purpose other than collecting data from the user’s device and broadcasting it on public GitHub pages.</p>



<p>The findings were spotted by Sonatype’s <a href="https://blog.sonatype.com/sonatype-spots-malicious-npm-packages" target="_blank" rel="noreferrer noopener">automated malware detection systems</a> and further investigated by the company’s Security Research team which includes me. </p>



<p>The packages previously present on the open source npm registry included:</p>



<ol><li><a rel="noreferrer noopener" href="https://www.npmjs.com/package/electorn" target="_blank">electorn</a> (intentional misspelling of a legitimate package “electron”)</li><li><a rel="noreferrer noopener" href="https://www.npmjs.com/package/loadyaml" target="_blank">loadyaml</a> </li><li>loadyml</li><li>lodashs (intentional misspelling of a legitimate package “lodash”)</li></ol>



<p>All four packages were published by the same user “simplelive12” and have now been removed, with the first two having been taken down by npm as of October 1, 2020. The previous two packages were unpublished by the author themselves.</p>



<p>Once installed, <code>electorn</code> ran a script in the background <strong>every</strong> <strong>hour</strong> which collected the logged-in user’s IP, geolocation data, username, path to home directory, and CPU model information.</p>



<figure><img loading="lazy" width="1024" height="356" src="https://securityreport.com/wp-content/uploads/2020/10/image-1-1024x356.png" alt="" srcset="https://securityreport.com/wp-content/uploads/2020/10/image-1-1024x356.png 1024w, https://securityreport.com/wp-content/uploads/2020/10/image-1-300x104.png 300w, https://securityreport.com/wp-content/uploads/2020/10/image-1-768x267.png 768w, https://securityreport.com/wp-content/uploads/2020/10/image-1-1536x534.png 1536w, https://securityreport.com/wp-content/uploads/2020/10/image-1-2048x712.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px" data-srcset="https://securityreport.com/wp-content/uploads/2020/10/image-1-1024x356.png 1024w, https://securityreport.com/wp-content/uploads/2020/10/image-1-300x104.png 300w, https://securityreport.com/wp-content/uploads/2020/10/image-1-768x267.png 768w, https://securityreport.com/wp-content/uploads/2020/10/image-1-1536x534.png 1536w, https://securityreport.com/wp-content/uploads/2020/10/image-1-2048x712.png 2048w" data-src="https://securityreport.com/wp-content/uploads/2020/10/image-1-1024x356.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="><figcaption>The malicious code within <code>electorn</code> and 3 other identical packages which exfiltrated user information</figcaption></figure>



<p>This information, part of which constitutes the device “fingerprint” was uploaded and published on <a rel="noreferrer noopener" href="http://web.archive.org/web/20201001065601/https://github.com/h4ppyl1ve/collect/issues/4" target="_blank">GitHub</a> in real-time.</p>



<p>Some of the information being published is base64-encoded but this can be trivially decoded by anyone who has access to it:</p>



<figure><img loading="lazy" width="1024" height="488" src="https://securityreport.com/wp-content/uploads/2020/10/image-1024x488.png" alt="" srcset="https://securityreport.com/wp-content/uploads/2020/10/image-1024x488.png 1024w, https://securityreport.com/wp-content/uploads/2020/10/image-300x143.png 300w, https://securityreport.com/wp-content/uploads/2020/10/image-768x366.png 768w, https://securityreport.com/wp-content/uploads/2020/10/image-1536x732.png 1536w, https://securityreport.com/wp-content/uploads/2020/10/image.png 1632w" sizes="(max-width: 1024px) 100vw, 1024px" data-srcset="https://securityreport.com/wp-content/uploads/2020/10/image-1024x488.png 1024w, https://securityreport.com/wp-content/uploads/2020/10/image-300x143.png 300w, https://securityreport.com/wp-content/uploads/2020/10/image-768x366.png 768w, https://securityreport.com/wp-content/uploads/2020/10/image-1536x732.png 1536w, https://securityreport.com/wp-content/uploads/2020/10/image.png 1632w" data-src="https://securityreport.com/wp-content/uploads/2020/10/image-1024x488.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></figure>



<p>Sonatype’s Security Research team has accounted for these malicious packages into their products, and had notified both npm and GitHub teams of the malicious activity stemming from the components. This led to the takedown of these malicious packages. </p>



<p>To this date, all 4 packages have scored a little over <strong>400</strong> total downloads.</p>



<p>It is not exactly clear what was the purpose of collecting this data and why was it being published on the web for the world to see, however, incidents like these highlight the potential of typosquatting attacks on the open-source ecosystem.</p>



<p>We can only imagine what the next possible version of these packages could have been capable of – possibly carrying out even more sinister activities. </p>



<p>By tricking an unsuspecting developer into mistakenly installing a misspelled package, attackers can push their malicious code “downstream” into any other open-source projects that use the misspelled malicious component as a transitive dependency.</p>



<p>Adopting DevSecOps best practices and building security early on into your software development lifecycle can prevent “counterfeit components” such as electorn and loadyaml from entering, and thriving in your software supply chains.</p>



<p>The complete research findings are available on the <a href="https://blog.sonatype.com/sonatype-spots-malicious-npm-packages" target="_blank" rel="noreferrer noopener">Sonatype blog</a>.</p>
                <div>
                    <h3>About the author</h3>
                                        
        <div>

                <p><a href="https://securityreport.com/author/ax-sharma/"><img alt="" src="https://secure.gravatar.com/avatar/d0d95768ca47b1764f5fb964cf860afa?s=150&amp;d=retro&amp;r=pg" srcset="https://secure.gravatar.com/avatar/d0d95768ca47b1764f5fb964cf860afa?s=300&amp;d=retro&amp;r=pg 2x" height="150" width="150" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></a>
                </p>
                <div>
                    <h4>
                        <a href="https://securityreport.com/author/ax-sharma/">Ax Sharma</a>
                    </h4>

                    
                    
                                            <p>
                        <a href="https://axsharma.com/" target="_blank">https://axsharma.com/</a>
                        </p>
                                        <div>
                        <p>Ax Sharma is a Security Researcher, Engineer, and Tech Columnist. His works and expert analyses have frequently been featured by leading media outlets like Fortune, The Register, TechRepublic, CIO, etc.</p>
<p>Ax’s expertise lies in vulnerability research, reverse engineering, software development, and web app security. He’s an active community member of the OWASP Foundation and the British Association of Journalists (BAJ).</p>
<p>Send any tips via email or Twitter DM.</p>
                    </div>
                    
                                    </div>
        </div>

                                            </div>
                                            
                        
	<nav role="navigation" aria-label="Continue Reading">
		<h2>Continue Reading</h2>
		
	</nav>                    </div><!-- .entry-content -->
    </div>
                        </div></div>]]>
            </description>
            <link>https://securityreport.com/nodejs-malware-caught-exfiltrating-ips-username-and-device-information-on-github/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24660810</guid>
            <pubDate>Fri, 02 Oct 2020 09:47:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Looking at the experience of black Britons through an American lens]]>
            </title>
            <description>
<![CDATA[
Score 395 | Comments 354 (<a href="https://news.ycombinator.com/item?id=24660682">thread link</a>) | @dgellow
<br/>
October 2, 2020 | https://www.persuasion.community/p/please-stop-imposing-american-views | <a href="https://web.archive.org/web/*/https://www.persuasion.community/p/please-stop-imposing-american-views">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3a4a8fb-963e-4611-8bc2-448a875dfb14_5184x3456.jpeg"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3a4a8fb-963e-4611-8bc2-448a875dfb14_5184x3456.jpeg" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/e3a4a8fb-963e-4611-8bc2-448a875dfb14_5184x3456.jpeg&quot;,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:18107111,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null}" alt=""></a></p><p><em>Construction workers reopen Winston Churchill statue in Parliament Square after it was covered with metal panels to protect it against defacement by protestors.</em></p><p>Over the past couple of months, many Britons have imported American discourse on race wholesale. When asked to analyze the experiences of black people in the United Kingdom, we now talk with an American accent.</p><p>Take a look, for instance, at a meme that has been circulating among some of my white friends on Facebook and Instagram:</p><blockquote><p>I have privilege as a White person because I can do all of these things without thinking twice about it … I can go jogging (#AmaudArbery). I can relax in the comfort of my own home (#BothemSean and #AtatianaJefferson). I can ask for help after being in a car crash (#Jonathan Ferrell and #RenishaMcBride). I can have a cellphone (#StephonClark). I can leave a party to get to safety (#JordanEdwards). I can play loud music (#JordanDavis). I can sell CDs (#AltonSterling). I can sleep (#AiyanaJones). I can walk from the corner store (#MikeBrown).</p></blockquote><p>The post goes on and on, like an interminable spoken-word poem. All the individuals listed are American, but most of the people who have shared this on my timeline are British. In trying to express their solidarity with black Britons, they are affirming a supposedly transcendental truth: to be black is to live in perpetual terror of being murdered by the state. </p><p>But Britain is not America. And importing American race discourse into the United Kingdom not only prevents us from recognizing the specific ways in which racial injustice manifests in this country—it cloaks the reality of black British lives behind an abstraction that flattens our humanity. </p><p>Britain has a long and painful history of anti-black racism. In the twentieth century alone, the growing black presence led to a long catalogue of abuses: the 1919 race riots in Liverpool, Cardiff and London; the 1958 race riots in Nottingham and Notting Hill; the 1969 police murder of David Oluwale, a Nigerian immigrant who was tortured, pissed on and finally drowned in a river in Leeds. I could list other examples. It is not hard to see why the horrific killing of George Floyd has evoked such strong feelings in this country as well.</p><p>But for all of the country’s flaws, Britain is not America. Trying to understand its racial dynamics through the lens of another country’s does more to obscure than to illuminate the situation that black Britons like myself actually face.</p><p>The average black American in the United States can trace his ancestry further back than the average white American. Most black Americans are descended from enslaved Africans. Their forebears suffered through the segregation and racial terror of the Jim Crow era. The majority of black people in the United Kingdom, by contrast, are immigrants or the children of immigrants. Though many of them have certainly had harrowing experiences with injustice or discrimination, they do not have the same history of racist disadvantage.</p><p>To understand the experience of black Britons, it is not only necessary to grasp how different their history is from that of black Americans: we need to understand the diversity captured by the label “black British.” For example, around two out of every three students with Congolese or Somali origins get free school meals, a standard indicator that their parents are poor. Among students with Nigerian or Ghanaian origins, only one in five do. It is also noteworthy that black Caribbean students are twice as likely to be excluded from school as black African students. </p><p>The discrepancy in educational attainment is just as stark. On average, 58% of black African students graduate from middle school at grade level (defined as achieving A* to C grades at GCSE)—about the same number as white students. But black Caribbean students are significantly less likely to do so—while those whose parents hail from Nigeria actually outperform their white peers by a considerable margin. </p><p>None of this is to disavow the label “black British.” But we need to invest it with the nuance consonant with its reality—and to cast doubt on the idea that every discrepancy in representation must be explained by structural injustice or white supremacy.</p><p>There has, for example, been a lot of concern about the underrepresentation of black Britons in professions like the arts and publishing. But why would you choose to go into theater or journalism—rather than law, medicine or finance—if you are a talented child of ambitious but not well off immigrants? </p><p>This is not a flippant question. While representation can be important, anybody who actually wants to improve the condition of black Britons should at least be a little curious about why they are overrepresented in some prestigious professions and underrepresented in others. In a country in which black people make up only three percent of the population, for example, six percent of junior doctors are black. Would the country—or the black community—really benefit if more black Britons chose to ditch medicine for the theater? The debate is worth having. But in the place of that debate, there have only been pious paeans to diversity.</p><p>The stereotype of the West African parent who wants their child to study law or medicine bears some relation to reality; but the widespread view of black people as perennial victims devoid of agency is a defamatory abstraction. The black person in Britain, like Ralph Ellison’s iconic protagonist, is “invisible because no one wants to see him.”</p><p>So much of the British reaction to the death of George Floyd has constituted a failure of nerve. Desperately seeking to assuage their feelings of guilt, to do <em>something</em>, many Britons have sacrificed their critical faculties to a narrative that does not actually help black people—a narrative that, by reducing us to passive abstractions, only makes us more invisible.</p><p>Racists assume that black people are all the same. Ironically, anti-racists sometimes do so too. But anybody who is truly committed to racial equality needs to recognize that this kind of simplification neither serves justice nor reflects the truth.</p><p><strong>Tomiwa Owolade is a writer who lives in London.</strong></p></div></div>]]>
            </description>
            <link>https://www.persuasion.community/p/please-stop-imposing-american-views</link>
            <guid isPermaLink="false">hacker-news-small-sites-24660682</guid>
            <pubDate>Fri, 02 Oct 2020 09:20:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bead Sort]]>
            </title>
            <description>
<![CDATA[
Score 88 | Comments 34 (<a href="https://news.ycombinator.com/item?id=24659668">thread link</a>) | @kkaranth
<br/>
October 1, 2020 | https://karthikkaranth.me/blog/bead-sort/ | <a href="https://web.archive.org/web/*/https://karthikkaranth.me/blog/bead-sort/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<header id="top">
    <section>
        <a href="https://karthikkaranth.me/">Karthik Karanth</a>
    </section>

    <div>
        <section>
            
                
                

                <a href="https://karthikkaranth.me/blog/">Blog</a>
            
                
                

                <a href="https://karthikkaranth.me/art/">Art</a>
            
                
                

                <a href="https://karthikkaranth.me/projects/">Projects</a>
            
        
        </section>
    </div>
</header>


<header>
  
</header>
<section id="category-pane">
  
  <p>
    <h6>
        PUBLISHED ON OCT 1, 2020 
      
    </h6>
  </p>
  
</section>
<section id="content-pane">
  <div>
    <div>
    <canvas id="bead-sort-canvas">
    </canvas>

    
</div>

<p>Bead sort<sup id="fnref:wiki"><a href="#fn:wiki">1</a></sup> is a sorting algorithm powered by gravity!</p>

<ul>
<li>For each number <code>x</code> in the array we want to sort, we arrange <code>x</code> beads in a row.</li>
<li>Let them all drop.</li>
<li>Count the number of beads in each row from top to bottom, and we have our sorted array!</li>
</ul>



<div>

<hr>

<ol>
<li id="fn:wiki"><a href="https://en.wikipedia.org/wiki/Bead_sort">Bead sort - Wikipedia</a>
 <a href="#fnref:wiki"><sup>[return]</sup></a></li>
</ol>
</div>

  </div>
</section>
<section id="tag-pane">
  
  
  
</section>








<section id="menu-pane">
  
  
  

  
  
  
  
  
  
  
  
  

  
  
  <div><p><span><a href="https://karthikkaranth.me/blog/starting-with-order/">&lt; PREV</a></span><span><a href="https://karthikkaranth.me/blog">BLOG</a></span><span></span></p></div>
  
  <div><p><span><a href="https://karthikkaranth.me/">HOME</a></span></p></div>
</section>





</div></div>]]>
            </description>
            <link>https://karthikkaranth.me/blog/bead-sort/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24659668</guid>
            <pubDate>Fri, 02 Oct 2020 06:44:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Falsehoods Programmers Believe About Map Coordinates]]>
            </title>
            <description>
<![CDATA[
Score 64 | Comments 36 (<a href="https://news.ycombinator.com/item?id=24659039">thread link</a>) | @boyter
<br/>
October 1, 2020 | https://engineering.kablamo.com.au/posts/2020/falsehoods-about-map-coordinates | <a href="https://web.archive.org/web/*/https://engineering.kablamo.com.au/posts/2020/falsehoods-about-map-coordinates">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><span>
      <span></span>
  <img alt="Mercator projection SW" title="Mercator projection SW" src="https://engineering.kablamo.com.au/static/e780902de4e676c5b62d7e1a4a139f42/72e01/Mercator_projection_SW.jpg" srcset="https://engineering.kablamo.com.au/static/e780902de4e676c5b62d7e1a4a139f42/e4a55/Mercator_projection_SW.jpg 256w,https://engineering.kablamo.com.au/static/e780902de4e676c5b62d7e1a4a139f42/36dd4/Mercator_projection_SW.jpg 512w,https://engineering.kablamo.com.au/static/e780902de4e676c5b62d7e1a4a139f42/72e01/Mercator_projection_SW.jpg 1024w,https://engineering.kablamo.com.au/static/e780902de4e676c5b62d7e1a4a139f42/ac99c/Mercator_projection_SW.jpg 1536w,https://engineering.kablamo.com.au/static/e780902de4e676c5b62d7e1a4a139f42/e1596/Mercator_projection_SW.jpg 2048w,https://engineering.kablamo.com.au/static/e780902de4e676c5b62d7e1a4a139f42/1cd85/Mercator_projection_SW.jpg 2058w" sizes="(max-width: 1024px) 100vw, 1024px" loading="lazy">
    </span>
(Map image by Daniel R. Strebe, licensed under CC BY-SA 3.0)</p><h2>1. The only projection that is important is Web Mercator</h2><p>While <a href="https://en.wikipedia.org/wiki/Web_Mercator_projection">Web Mercator</a> is
probably the most popular projection that most people will run into, the
<a href="https://en.wikipedia.org/wiki/Albers_projection">Albers</a> and
<a href="https://en.wikipedia.org/wiki/Lambert_cylindrical_equal-area_projection">Lambert</a>
equal-area projections are fairly common for when the projection needs to maintain
the area rather than the navigational direction (which is one of the main features
of the Mercator projection).</p><h2>2. All coordinates are latitude/longitude pairs</h2><p>In addition to latitude/longitude coordinates, <a href="https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system">Universal Transverse Mercator (UTM)
coordinates</a>
are also fairly common. UTM splits the Earth into 60 zones, and then
further specifies northings and eastings in metres (as opposed to degrees, minutes and seconds).</p><p>The UTM notably omits the polar areas - which are covered by the <a href="https://en.wikipedia.org/wiki/Universal_polar_stereographic_coordinate_system">Universal Polar Stereographic (UPS)
coordinate system</a>
instead.</p><h2>3. Latitude always comes before longitude in a coordinate pair</h2><p>While it is common to see items in (latitude,longitude) order, some formats
(e.g. <a href="https://en.wikipedia.org/wiki/GeoJSON">GeoJSON</a>) dictate that coordinates
follow (longitude,latitude) order instead. This matches the typical way coordinates
are specified in a Cartesian coordinate system: (x,y).</p><h2>4. A degree of latitude or longitude always represents the same distance</h2><p>In the Mercator projection, the Earth - which, in reality, is an
<a href="https://en.wikipedia.org/wiki/Spheroid#Oblate_spheroids">oblate spheroid</a> -
is projected as a simple cylinder. This means that "parallel" longitude lines
meet at the poles, so the distance between degrees of longitude are much shorter
as they get closer to the poles than they are at the equator (~111 km).</p><p>The variance in latitude is not as large - but it still varies by about 1km going
from the equator to the poles.</p><h2>5. The shortest path between two points is a straight line</h2><p>The Earth isn't flat - as such, although your map may be projected to be flat,
the distance between two points needs to follow the curvature of
the Earth and can usually be approximated by the
<a href="https://en.wikipedia.org/wiki/Haversine_formula">Haversine formula</a>.</p><h2>6. Coordinates for a given landmark are always fixed</h2><p><a href="https://en.wikipedia.org/wiki/Continental_drift">Movements of the Earth's tectonic plates</a>
mean that the land masses are moving slowly with the passage of time.
For example, Australia has shifted about 1.8 metres from where it
was in 1994 (about 7 centimetres per year). This also means that <a href="http://www.ga.gov.au/scientific-topics/positioning-navigation/geodesy/datums-projections/gda2020">geocentric
datums</a>
have to be updated to account for these changes every once in a while.</p><h2>7. Given a pair of coordinates, you can plot it on a map</h2><p>In addition to coordinates, we also need to know the datum, which is
the coordinate system and its specific set of reference points on the Earth.
While most coordinates often follow the
<a href="https://en.wikipedia.org/wiki/World_Geodetic_System">WGS84 datum</a>,
care should be taken to ensure that the map and the coordinates plotted
are using the same datum.</p><h2>8. There is one global ellipsoid to base coordinates on</h2><p>Most modern datums are based on the WGS84
<a href="https://en.wikipedia.org/wiki/Ellipsoid">ellipsoid</a>
as the surveys are often completed using GPS as a reference, but notably
Russia and China still base their local datums on different reference ellipsoids.</p><p>As a result, conversions to and from datums based on different
ellipsoids may result in inaccuracies and deviations and may be of concern
if you have to deal with GPS, GLONASS, and BeiDou data at the same time.</p></div></div>]]>
            </description>
            <link>https://engineering.kablamo.com.au/posts/2020/falsehoods-about-map-coordinates</link>
            <guid isPermaLink="false">hacker-news-small-sites-24659039</guid>
            <pubDate>Fri, 02 Oct 2020 04:59:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Transport Tycoon a.k.a. the great optimiser, Chris Sawyer]]>
            </title>
            <description>
<![CDATA[
Score 101 | Comments 36 (<a href="https://news.ycombinator.com/item?id=24658958">thread link</a>) | @wizardfeet
<br/>
October 1, 2020 | https://lifeandtimes.games/episodes/files/28 | <a href="https://web.archive.org/web/*/https://lifeandtimes.games/episodes/files/28">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a href="https://pdcn.co/e/traffic.megaphone.fm/ADL8847793182.mp3" target="_blank">Click/tap here to download this episode.</a></p><p><a href="https://ratethispodcast.com/ltvg" target="_blank"><img src="https://storage.googleapis.com/rtp-assets/buttons/lifetimevideogames.png" width="198" height="56" alt="Rate This Podcast"></a></p>
<p><img alt="A screenshot of the main menu from Transport Tycoon for DOS" src="https://lifeandtimes.games/episodes/files/transport-tycoon-dos-screenshot-main-menu.png" width="276" height="212"></p><p>On the rise and, um...<em>fade out(?)</em> of Chris Sawyer, the genius creator of bestselling, critically-acclaimed simulation games Transport Tycoon and RollerCoaster Tycoon — who made a career out of working at the cutting-edge, in bare metal assembly code that he wrote and optimised (and optimised again) on his own.</p><p>Until the cutting-edge left him behind.</p><p><strong>Hello Hacker News readers! As of this update, the post there erroneously labels this as an interview. It's not. Chris doesn't do interviews, except via an intermediary (which doesn't really work in an audio format), so the story is based entirely on my in-depth research and analysis. I hope you still enjoy it and learn something interesting. (I have stories on many other things that do involve interviews, though, like </strong><strong><a href="https://lifeandtimes.games/episodes/files/27" title="Episodes:27 - Links">1990 golf game Links</a></strong><strong>, Bungie's </strong><strong><a href="https://lifeandtimes.games/episodes/files/25" title="Episodes:25 - Pimps at Sea">fake game Pimps at Sea</a></strong><strong>, and </strong><strong><a href="https://lifeandtimes.games/episodes/files/22" title="Episodes:22 - Wololo">the "Wololo" sound effect</a></strong><strong> from Age of Empires.)</strong></p><p>Chris was only a design consultant on 2004 game RollerCoaster Tycoon 3, but its remastered "Complete" edition has just come out on Nintendo Switch and the PC version is free on the Epic Games Store right now (until October 2). The original two games are also still sold via the likes of Steam and GOG.</p><p>Transport Tycoon, meanwhile, lives on in open-source project <a href="https://www.openttd.org/" target="_blank">OpenTTD</a> and in a mobile port (<a href="https://play.google.com/store/apps/details?id=com.thirtyonex.TransportTycoon&amp;hl=en_US" target="_blank">Android</a>, <a href="https://apps.apple.com/us/app/transport-tycoon/id634013256" target="_blank">iOS</a>) of the original game by Chris's company 31X. You can see a snippet of his source code in the image below:</p><div><p><img alt="cstg_code1" src="https://lifeandtimes.games/episodes/files/cstg_code1.jpg" width="1262" height="1775"></p></div><div><p>Thanks as always to my supporters on Patreon — especially my $10+ backers Carey Clanton, Rob Eberhardt, Simon Moss, Vivek Mohan, Wade Tregaskis, and Seth Robinson. If you'd like to become a supporter, for as little as $1 a month, head to <a href="https://www.patreon.com/lifeandtimesofvideogames">my Patreon page</a> and sign up. Or for one-off donations you can use <a href="https://paypal.me/mossrc">paypal.me/mossrc</a>.</p><p>Please remember to tell other people about the show, and to leave a review by following the links at <a href="https://ratethispodcast.com/ltvg">ratethispodcast.com/ltvg</a>.</p><p>I'm currently writing a new book called Shareware Heroes: Independent Games at the Dawn of the Internet. You can learn more and/or pre-order your copy <a href="https://unbound.com/books/shareware-heroes/" target="_blank">from Unbound</a>.</p></div><hr>
<h3>(Partial) Transcript</h3>
<p><strong><em>[Most episode transcripts/scripts are reserved for my Patreon supporters (at least for the time being), but I like to give you at least a taster here — or in this case, the first half of the episode.]</em></strong></p><p><em>Welcome to the Life and Times of Video Games, an audio series about video games and the video game industry, as they were in the past and how they’ve come to be the way they are today. I'm Richard Moss, and this is episode 28, Transport Tycoon, or the tale of the great optimiser and his two greatest works.</em></p><p>We’ll get going in just a moment. </p><p>*pause for pre-roll ad/cross-promo slot*</p><p>***</p><p>You may have heard the expression that every overnight sensation is a decade in the making — a decade of hard work, toiling in obscurity…or <em>relative</em> obscurity, honing a talent, perfecting a craft, <em>optimising</em> a skill set and envisioning whatever it is that breaks through.</p><p>In reality the actual duration is rarely a decade — it’s five years or eight years or eighteen years, or however long it takes for the pieces to all fall into place: the talent, timing, and product. But the idea bears repeating: the greatest accolades, the greatest achievements, the greatest games are the product of hard work built atop years of invisible labour.</p><p>And such it was that Chris Sawyer, like John Romero, Carol Shaw, Gunpei Yokoi, and many others before and since — such it was that in 1994 Chris Sawyer suddenly shifted from a little-known (though well-respected) figure in the games industry, a programmer who converted Amiga games to the PC, to become an industry icon.</p><p>Nineteen-ninety-four was the year when his first original game was published, the year when big-name PC game publisher Microprose put his transportation-focused business simulation game Transport Tycoon, an incredible solo development effort, in a box and sold it in stores to widespread acclaim. </p><p><img alt="SCR1" src="https://lifeandtimes.games/episodes/files/scr1.jpg" width="866" height="362"><br><em>Transport Tycoon, the game that made Chris Sawyer into a games industry icon (</em><em><a href="https://www.tt-forums.net/viewtopic.php?f=47&amp;t=29058" target="_blank">image source</a></em><em>)</em><br></p><div><p>The game itself had taken Chris just a year to develop, but the journey to making it had begun much earlier.</p><p>Chris had started programming as a teenager in 1981, largely out of curiosity, through trying to make things appear on the screen on a range of different computers he’d encountered. There was the Commodore PET at his high school, the Sinclair ZX81 demonstration unit in a W H Smiths store, and the Texas Instruments TI99/4A one of his neighbours owned, as well as the Commodore VIC-20 a different neighbour had. And eventually, after diligently saving up his pocket money, he’d become engrossed in a machine of his own, a Camputers Lynx, a now-forgotten, obscure-even-then 8-bit computer with fancier graphics and more horsepower than the leading systems of its day (the leading systems at the time being the Apple II, ZX Spectrum, and Commodore 64).</p><p>Here, in 1983, is where the journey really starts — where Chris set off towards the lands where he’d make his name. And I find it fascinating how serendipitous this was — for, you see, Chris’s two great successes, Transport Tycoon and RollerCoaster Tycoon, were both made possible by his phenomenal systems knowledge; by his immense capacity to hand-code complex interactions of data at low levels of abstraction.</p><p>And here is where he began to learn those skills, to internalise them to the point of becoming natural talents. He later told Arcade Attack in an interview that he’d not had access to an assembler for that Lynx computer, so when he’d wanted to move beyond coding in BASIC he’d needed to write his programs byte-by-byte in machine code — the lowest-level programming language, the numerical instructions that computers themselves use. And with scant resources available to teach him these skills, he mostly figured it out on his own, just trying different things until he got his ideas to work. Always chasing the next exhilarating breakthrough.</p><p>Chris continued to dabble in machine code, though somewhat less than before, when he upgraded to a similarly-obscure machine called the Memotech MTX500, which actually did come with a built-in assembler, which enabled him to write programs in the abbreviation-heavy Z80 assembly language. Programs that, beginning in 1984, he very often had published commercially.</p></div><p><img alt="Memotech_MTX500-wide" src="https://lifeandtimes.games/episodes/files/memotech_mtx500-wide.jpg" width="1020" height="584"><br><em>The Memotech MTX500 (</em><em><a href="https://en.wikipedia.org/wiki/File:Memotech_MTX500.jpg" target="_blank">Image source</a></em><em>)</em><br></p><div><p>Chris had sent Memotech cassette tapes of some games he’d made through copying the designs of popular titles, like Missile Kommand, which was the 1980 Atari arcade game converted to the capabilities of the MTX500, using a mix of BASIC and machine code, with the name intentionally misspelled (a ‘k’ rather than a ‘c’) as though that somehow made his unapologetic, blatant clone of another’s work okay. </p><p>But this was the wild west of the computer games business, and Memotech weren’t much concerned. Or at least their games guy Jim Wills wasn’t much concerned, neither at this point nor a few months later when he left to start a company called Megastar Games. Jim liked Chris’s work enough to publish it, for meagre royalties but invaluable experience. And so Chris was commercially published with his unlicensed MTX500 versions of Missile Command, Q*bert, Manic Miner, and a few others.</p><p>After high school he enrolled in a computer science and microprocessor systems degree, where he studied the fundamentals of both software and hardware design in computers — an experience he found invaluable, as it taught him how to push computers further by learning how their hardware worked. And it taught him the theories behind the sorts of nitty-gritty software-systems things he’d already been practising at home: optimisation, sorting, algorithms, and even more varieties of machine code.</p></div><p><img alt="escape-from-zarcos-memotech-mtx500" src="https://lifeandtimes.games/episodes/files/escape-from-zarcos-memotech-mtx500.png" width="1044" height="788"><em><br>Escape from Zarcos, Chris Sawyer clone of Manic Miner for the Memotech MTX500</em><br></p><div><p>At home, meanwhile, he’d shifted over to the Amstrad CPC, which technologically-speaking wasn’t hugely different to the Memotech system he’d been on before — but it was a modest upgrade, and unlike his previous computers it was actually a popular system. And for Chris it was a gateway to the PC, because in the course of studying at university and making computer games on the side he wound up getting an Amstrad-made IBM-PC clone.</p><p>Chris had during this period been getting his games published through Ariolasoft, a German company with a UK subsidiary that promised him a job programming games for them once he graduated. Except some promises can’t be kept, especially in an industry that moves as fast as computer games publishing.</p><p>The home computer business was by that point deep into its transition from 8-bit to 16-bit hardware, and that transition came with adjustments to the standard of game graphics and design required, and to the way marketing and sales worked, and the cost of publishing, and so on, and Ariolasoft wasn’t doing too well at managing the transition. </p><p>So Chris didn’t have a job waiting for him after all, and he’d missed out on all the great electronics engineering jobs his classmates applied for. (Oops!) But not to worry — he’d made enough connections and enough headway as a programmer that he could get himself a business agent, and that agent in turn connected him to the booming Amiga-to-PC games porting industry.</p><p>He later said he’d thought it a “stop-gap” measure, just “a bit of fun” while he looked for more permanent employment in the electronics industry. But Chris took to his new conversions work like a duck to water. The kid who’d had to get creative and remain patient to make anything work on his Camputers Lynx machine now excelled in an environment where he had to contend with the vast gap in multimedia capabilities between the Amiga and the PC.</p><p>PCs of the day were pathetically inept as games machines, compared to a system like the Amiga. Whereas the PC had just a CPU, and maybe, in a minority of machines, a dedicated sound card like the SoundBlaster 16, every Amiga came with a custom chipset that contained audio and video co-processors that could take some strain off the …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://lifeandtimes.games/episodes/files/28">https://lifeandtimes.games/episodes/files/28</a></em></p>]]>
            </description>
            <link>https://lifeandtimes.games/episodes/files/28</link>
            <guid isPermaLink="false">hacker-news-small-sites-24658958</guid>
            <pubDate>Fri, 02 Oct 2020 04:45:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Jargon File]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24658797">thread link</a>) | @purec
<br/>
October 1, 2020 | http://jargon-file.org/archive/jargon-4.4.7.dos.txt | <a href="https://web.archive.org/web/*/http://jargon-file.org/archive/jargon-4.4.7.dos.txt">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://jargon-file.org/archive/jargon-4.4.7.dos.txt</link>
            <guid isPermaLink="false">hacker-news-small-sites-24658797</guid>
            <pubDate>Fri, 02 Oct 2020 04:12:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Big O, Little N]]>
            </title>
            <description>
<![CDATA[
Score 76 | Comments 128 (<a href="https://news.ycombinator.com/item?id=24657747">thread link</a>) | @adamzerner
<br/>
October 1, 2020 | https://adamzerner.bearblog.dev/big-o-little-n/ | <a href="https://web.archive.org/web/*/https://adamzerner.bearblog.dev/big-o-little-n/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div><p>I remember when I was first learning about hash tables. I thought I stumbled across an ingenious optimization for dealing with hash collisions. Before explaining my optimization, let me first briefly explain how hash tables work and what hash collisions are. I also made a video if you're interested.</p>
<p><a href="http://www.youtube.com/watch?v=pM9zhZB2KkM" title="Hash Tables"><img alt="" src="http://img.youtube.com/vi/pM9zhZB2KkM/0.jpg"></a></p>
<p>Say that we have a hash that maps someone's name to their age. <code>"adam"</code> is <code>27</code>, so we want to enter that into our hash.</p>
<p><img alt="" src="https://i.ibb.co/qWdjbhM/Slice.png"></p>
<p>Here's what happens:</p>
<ul>
<li><code>"adam"</code> gets put through a hash function.</li>
<li>The hash function spits out <code>7</code>.</li>
<li><code>7</code> is the index of the array where we are going to store <code>"adam"</code>'s value.</li>
<li>So we store <code>27</code> into <code>array[7]</code>.</li>
</ul>
<p>If we run <code>hash.get("adam")</code> in the future and need to look up the value for <code>"adam"</code>, we:</p>
<ul>
<li>Run <code>"adam"</code> through the hash function.</li>
<li>Get <code>7</code> as the output.</li>
<li>This means we have to look at <code>array[7]</code> to get the value.</li>
<li>So we look at <code>array[7]</code> to get our value.</li>
</ul>
<p>Hopefully if we have a key of <code>"alice"</code> or <code>"bob"</code> it'll hash to a different index. But... what happens if it hashes to the same index? That's called a hash collision.</p>
<p><img alt="" src="https://i.ibb.co/sJbJKnD/Slice.png"></p>
<p>A common approach is that if there's a collision, you store the values in a linked list.</p>
<p><img alt="" src="https://i.ibb.co/VJsSW3T/Slice.png"></p>
<p>Fair enough. That makes sense.</p>
<p>But wait!!! It takes <code>O(n)</code> time to look up a value in a linked list. Can't we use a binary search tree to speed that up to <code>O(log n)</code>???</p>
<p><img alt="" src="https://i.ibb.co/R3Vrc10/Slice.png"></p>
<p>Or... can't we take it a step further and use a <em>hash inside of a hash</em> to get the lookup time all the way down to <code>O(1)</code>?!</p>
<p><img alt="" src="https://i.ibb.co/7CyYNWY/Slice.png"></p>
<p>I hate to say it, but however many years ago when I had these thoughts, there were a few moments where I thought I was a genius. Now when I look back at that former self, I facepalm.</p>
<p>It is true that going from a linked list to a BST to a hash takes you from <code>O(n)</code> to <code>O(logn)</code> to <code>O(1)</code> lookup time. However, since collisions are rare, <code>n</code> is going to be small. And when <code>n</code> is small, <code>O(n)</code> might be faster than <code>O(1)</code>.</p>
<p>To understand why that is, think back to what Big-O really means. I think it helps to think about it as a "math thing" instead of a "programming thing". Consider two functions:</p>
<pre><code>f(n) = 4n + 1000
g(n) = 2n^2 + 5
</code></pre>
<p>Big-O of <code>f</code> is <code>O(n)</code> and Big-O of <code>g</code> is <code>O(n^2)</code>. We just focus on the part that "really matters". When <code>n</code> gets really big, the fact that it's <code>4n</code> doesn't really matter. Nor does the <code>+ 1000</code>.</p>
<p>But what about when <code>n</code> is small? Well, let's look at happens when <code>n</code> is <code>5</code>:</p>
<pre><code>f(5) = 4(5) + 1000 = 20 + 1000 = 1020
g(5) = 2(5^2) + 5 = 2(25) + 5 = 50 + 5 = 55
</code></pre>
<p>Look at that! The <code>O(n^2)</code> function is almost 20 times faster than the <code>O(n)</code> function!</p>
<p>And <em>that</em> is basically why they use a linked list instead of a BST or hash to handle collisions. Since <code>n</code> is small, Big-O isn't the right question to ask.</p>
<hr>
<p>Here's another example. I just wrote the following code while prepping for an interview:</p>
<pre><code>const isVowel = (char) =&gt; ["a", "e", "i", "o", "u"].includes(char);
</code></pre>
<p>Normally I wouldn't think twice about it, but since interviewers care so much about time complexity, I stopped to think about whether it could be improved.</p>
<p>And it hit me that it's actually <code>O(n)</code>(<a href="https://news.ycombinator.com/item?id=24660824">caveat</a>), because we've got an array and have to iterate over every element to see if the element matches <code>char</code>. It's easy to overlook this because we're using <code>includes</code> and not writing the code ourselves.</p>
<p>So then I thought that maybe we could use a hash instead to get <code>O(1)</code> lookup. Something like this:</p>
<pre><code>const isVowel = (char) =&gt; {
  const vowels = {
    a: true,
    e: true,
    i: true,
    o: true,
    u: true,
  };

  return !!vowels[char];
};
</code></pre>
<p>But then I realized that this is the same mistake I made with the hash collision stuff however many years ago. <em><code>n</code> is small, so Big-O isn't the question we should be asking</em>. Here <code>n</code> is <code>5</code>.</p>
<p>I think that the array approach would actually be faster than the hash approach, even though it's <code>O(n)</code> instead of <code>O(1)</code>. The reason for this is called locality.</p>
<p>If you dive deep under the hood, when you look up an element in an array, the CPU actually grabs a bunch of adjacent elements as well as the one you wanted, and it stores the adjacent elements in a cache. So here when we look up <code>"a"</code> in the array, it'll probably grab <code>"a"</code>, <code>"e"</code>, <code>"i"</code>, <code>"o"</code>, and <code>"u"</code>. And so next time when we want to grab <code>"e"</code>, it can take it from the cache, which is a lot faster. This works because the elements in the array are close to each other in the physical memory. But with a hash, my understanding is that they wouldn't be so close, and thus we wouldn't benefit from this spatial locality effect.</p>
<p>I'm no low-level programming wiz so I'm not sure about any of this. That's ok, I think it's beside the point of this post. The real point of this post is that when you have a little <code>n</code>, Big-O doesn't matter.</p>
</div>
</div></div>]]>
            </description>
            <link>https://adamzerner.bearblog.dev/big-o-little-n/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24657747</guid>
            <pubDate>Fri, 02 Oct 2020 00:59:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Asteroids: By the Numbers]]>
            </title>
            <description>
<![CDATA[
Score 44 | Comments 16 (<a href="https://news.ycombinator.com/item?id=24655752">thread link</a>) | @rbanffy
<br/>
October 1, 2020 | http://www.retrogamedeconstructionzone.com/2019/10/asteroids-by-numbers.html?m=1 | <a href="https://web.archive.org/web/*/http://www.retrogamedeconstructionzone.com/2019/10/asteroids-by-numbers.html?m=1">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div id="post-body-3855344372715435262"><p>
By 1979, arcade games were rapidly becoming more complex and colorful, and a game like <i>Asteroids </i>might have seemed quaint by comparison to the likes of <i><a href="http://www.retrogamedeconstructionzone.com/2019/09/galaxian-aesthetics-of-simple-patterns.html">Galaxian</a>, <a href="http://www.retrogamedeconstructionzone.com/2019/09/star-fire-weirdness-of-pseudo-3d.html">Star Fire</a></i>, or <i>Radar Scope.&nbsp; </i>But beneath its simple exterior lies a challenging shooter with surprisingly complex physics.&nbsp; The image below shows a sample still from it, including the player's ship (the triangle on the left), three sizes of asteroids, and an alien ship.</p><p><a href="https://1.bp.blogspot.com/-XOyg91qwUV4/XZwhbHR4tnI/AAAAAAAAOXs/DJ89MzjAODU3mAoraZKUaq6vl9nKvdsSACLcBGAsYHQ/s1600/Asteroids_sample%2B%25282%2529.png"><img data-original-height="461" data-original-width="598" height="492" src="https://1.bp.blogspot.com/-XOyg91qwUV4/XZwhbHR4tnI/AAAAAAAAOXs/DJ89MzjAODU3mAoraZKUaq6vl9nKvdsSACLcBGAsYHQ/s640/Asteroids_sample%2B%25282%2529.png" width="640"></a></p>

<p>
The goal of the game is to maximize your score, destroying as many asteroids and aliens as possible before you run out of ships.&nbsp; When an asteroid is hit by something (usually the player's bullets), large asteroids turn into two medium ones and medium asteroids turn into two small ones, while small asteroids and aliens are destroyed when hit.</p><p><a href="https://1.bp.blogspot.com/-cEzHmkcz3bA/XaU1zGl7IBI/AAAAAAAAObY/2GDlGaxtEc0ytzWpyQRinUxnbioeVsTbQCLcBGAsYHQ/s1600/Asteroids%2BPlay.gif"><img data-original-height="357" data-original-width="600" height="237" src="https://1.bp.blogspot.com/-cEzHmkcz3bA/XaU1zGl7IBI/AAAAAAAAObY/2GDlGaxtEc0ytzWpyQRinUxnbioeVsTbQCLcBGAsYHQ/s400/Asteroids%2BPlay.gif" width="400"></a></p>
<p>
For the designers of the game, balancing the relative sizes of the objects would have been important in such a dense field, because it determines how often things run into one another other.&nbsp; The table below outlines the sizes of the objects in <i>Asteroids</i>, expressed relative to the length of the player's ship.</p><table>
<caption>The approximate horizontal lengths of objects in <i>Asteroids</i></caption>
    <tbody>
<tr>
    <th>Object</th>
    <th>Length (in player ship lengths)</th>
    </tr>
<tr>
    <td>Screen</td>
    <td>25 x 36</td>
    </tr>
<tr>
    <td>Large Asteroids</td>
    <td>2.4</td>
    </tr>
<tr>
    <td>Medium Asteroid</td>
    <td>1.2</td>
    </tr>
<tr>
    <td>Small Asteroid</td>
    <td>0.6</td>
    </tr>
<tr>
    <td>Alien Ship (large)</td>
    <td>1.5</td>
    </tr>
<tr>
    <td>Alien Ship (small)</td>
    <td>0.75</td>
    </tr>
</tbody>
    </table>
<p>
Notice how the medium asteroid is twice the size of the small one, and the large is twice the size of the medium.&nbsp; If you think about it, this doesn't actually make much physical sense -- you can break a two-dimensional object into four pieces half its size, not two.&nbsp; But the size ratios do make game sense, because what really matters to the player is not how much area an asteroid takes up, but how likely it is to hit them.</p><p><a href="https://1.bp.blogspot.com/-5wVPSIS6W_M/XZwmjsjcYII/AAAAAAAAOX4/a7MThKiGRH8vhydf83eCiOmmJuKUULHIwCLcBGAsYHQ/s1600/Asteroids_cross_section%2B%25282%2529.png"><img data-original-height="281" data-original-width="170" src="https://1.bp.blogspot.com/-5wVPSIS6W_M/XZwmjsjcYII/AAAAAAAAOX4/a7MThKiGRH8vhydf83eCiOmmJuKUULHIwCLcBGAsYHQ/s1600/Asteroids_cross_section%2B%25282%2529.png"></a></p>

<p>
In the picture above, you can see that while the four small asteroids take up less area on the screen than the big one, they present the same cross section to your ship.&nbsp; This means that breaking up the asteroids doesn't greatly increase the probability that the player will be struck by one.&nbsp; If the asteroids had been broken up into four half-sized pieces at each blow, each large asteroid would result in sixteen small ones (that's four times the cross section!) and the player would be quickly overwhelmed.&nbsp; Note that the small asteroids do move faster than the large ones, and speed increases the chance of getting hit, but I'll return to that in a moment.</p>

<p>
Another big reason that size matters in <i>Asteroids</i>&nbsp;is that it determines how close something has to be before you're likely to be able to hit it.&nbsp; In 1979, vector arcade cabinet screens had an effective resolution of 1024 x 768, and the player's ship was only connected graphically by about 20 points.&nbsp; This meant, in turn, that there were only a limited number of orientations that they could render for the ship, in this case intervals of 5 degrees.&nbsp; To see how this might affect gameplay, imagine your ship is located at the fixed position shown in the picture below.&nbsp; Anything located entirely between the two solid lines will not be accessible by your bullets because your ship can't turn to the appropriate angle to hit it.&nbsp; This restriction isn't a big deal for large asteroids, which are still accessible over most of the screen, but small asteroids can be out of reach of your guns at relatively close range, sometimes as close as 7 ship lengths away!&nbsp; So even if you aim as accurately as possible, chances are you won't be able to hit a small asteroid on the other side of the screen, at least not on your first shot.&nbsp; This is probably one of the reasons that <i>Asteroids</i>&nbsp;allows you to fire up to four bullets in succession: even if you can't hit an object right away, it will eventually move into your line of fire.</p>

<p><a href="https://1.bp.blogspot.com/-EX2V1tAwdnM/XZvKFmnQZoI/AAAAAAAAOXI/cd411AFReLcKkCQGooANzsmxVD7QV3ikgCLcBGAsYHQ/s1600/Asteroids_hittable%2B%25285%2529.png"><img data-original-height="397" data-original-width="1024" height="248" src="https://1.bp.blogspot.com/-EX2V1tAwdnM/XZvKFmnQZoI/AAAAAAAAOXI/cd411AFReLcKkCQGooANzsmxVD7QV3ikgCLcBGAsYHQ/s640/Asteroids_hittable%2B%25285%2529.png" width="640"></a></p>
<p>
And what about speed?&nbsp; There is some variability in the speeds of individual asteroids, but small asteroids can move as much as 63% faster than large ones (see the table below), meaning that even if four small asteroids present the same cross section for hitting your ship as a large one does, their higher speed means they're more likely to hit you.&nbsp; The reasoning for this is simple: the faster something moves, the longer the path it can traverse in a given amount of time, and the larger the likelihood that you'll be in that path.&nbsp; Fortunately, even the fastest asteroids are much slower than your ship, which can traverse the screen in about two seconds; so if you're skilled enough, you should be able to maneuver around the asteroid field without a problem.</p><table>
<caption>The approximate speeds of objects in <i>Asteroids</i></caption>
    <tbody>
<tr>
    <th>Object</th>
    <th>Speed (in ship lengths per second)</th>
    </tr>
<tr>
    <td>Your ship</td>
    <td>0 - 17</td>
    </tr>
<tr>
    <td>Asteroids</td>
    <td>4 - 6.5</td>
    </tr>
<tr>
    <td>Alien ships (both sizes)</td>
    <td>4 - 6.5 (depending on your score)</td>
    </tr>
<tr>
    <td>Bullets</td>
    <td>17 (ship at rest)&nbsp;</td></tr>
</tbody></table>
<p>
One other interesting thing about speed: notice in the table above that the speed of a bullet, when fired at rest, is the same as your ship's maximum speed.&nbsp; This means that when you're moving rapidly and fire a bullet in the opposite direction of your motion, the two speeds should cancel for a stationary observer and the bullet should appear roughly stationary.</p>

<p><a href="https://1.bp.blogspot.com/-pRIxsTPNfSY/XaD7O0foijI/AAAAAAAAOZQ/JWZGFR26JEY3UM43RiruPhhGfHOCW21DgCLcBGAsYHQ/s1600/Asteroids_bullet_stationary.gif"><img data-original-height="175" data-original-width="468" height="239" src="https://1.bp.blogspot.com/-pRIxsTPNfSY/XaD7O0foijI/AAAAAAAAOZQ/JWZGFR26JEY3UM43RiruPhhGfHOCW21DgCLcBGAsYHQ/s640/Asteroids_bullet_stationary.gif" width="640"></a></p>


<div><p>
Sure enough, when I fire a bullet while moving quickly in the opposite direction, it just floats near where I fired it, allowing me to place bullets like mines for the asteroids to run into.</p>
<p>
The developers of <i>Asteroids </i>wanted the game to simulate the real laws of physics (<a href="http://www.technologyuk.net/computing/computer-gaming/gaming-landmarks-1960-1985/asteroids.shtml">see here</a>), and I think in many respects they succeeded, at least compared to most other arcade games of the time.&nbsp; It's not entirely realistic, however.&nbsp; In my article on <a href="http://www.retrogamedeconstructionzone.com/2019/10/impossible-motion-in-video-games.html">motion in early arcade games</a>, I discussed how the acceleration of the ship in <i>Asteroids</i> is too rapid for a human-occupied vehicle.&nbsp; We might suppose that the vehicle is automated, but even then, engineering spacecraft to withstand 30+ g's of acceleration is a non-trivial challenge.</p><p><a href="https://1.bp.blogspot.com/-eBpcOCL_1a0/XaU-bXjLJjI/AAAAAAAAObk/N_FxduJSebIMaqzrq7BhDmNFufzmyBRfgCLcBGAsYHQ/s1600/Acceleration%2BAsteroids.gif"><img data-original-height="175" data-original-width="634" height="176" src="https://1.bp.blogspot.com/-eBpcOCL_1a0/XaU-bXjLJjI/AAAAAAAAObk/N_FxduJSebIMaqzrq7BhDmNFufzmyBRfgCLcBGAsYHQ/s640/Acceleration%2BAsteroids.gif" width="640"></a></p>
<p>
Another issue is the way the asteroids break up.&nbsp; In physics, there's a principle called the conservation of momentum that says that when objects interact with one another or break apart, the total mass times the velocity must remain the same after the interaction as it was before.&nbsp; In layman's terms, this means that things can't suddenly go from moving one direction to moving in another unless there is something else carry its previous momentum.&nbsp; In the example shown below, an asteroid does just that, and the only thing that could have absorbed its previous momentum is the bullet.&nbsp; But the bullet is so tiny that it's difficult to imagine how that would be possible.</p><p><a href="https://1.bp.blogspot.com/-vsDHHIPNXrQ/XaEAAKYXRII/AAAAAAAAOZc/-lhbXy0TSV4nEiLvOf-pe7zyt9sPGLcEACLcBGAsYHQ/s1600/Asteroids_momentum.gif"><img data-original-height="248" data-original-width="533" height="185" src="https://1.bp.blogspot.com/-vsDHHIPNXrQ/XaEAAKYXRII/AAAAAAAAOZc/-lhbXy0TSV4nEiLvOf-pe7zyt9sPGLcEACLcBGAsYHQ/s400/Asteroids_momentum.gif" width="400"></a></p>

<p>
These are mere quibbles, however, and shouldn't dissuade you from giving the game a try.&nbsp; <i>Asteroids </i>ended up being one of Atari's biggest arcade hits, selling over 70,000 cabinets, and remains one of their most recognizable games to this day.&nbsp; Many gamers who grew up in the '80s, myself included, are more familiar with the Atari 2600 version of the game.&nbsp; Unfortunately, the designers had to make a lot of sacrifices in game physics in order to make it work on a home console, so I think the arcade version is really the way to go.&nbsp; Outside of visiting a vintage arcade, your best bet is probably the <a href="https://www.mamedev.org/">Multiple Arcade Machine Emulator</a> (MAME).&nbsp; Happy hunting!</p></div><div><p><span>NOTE: &nbsp;A previous version of this article stated that the game has limited pixel resolution, but <i>Asteroids </i>uses a vector display that is not composed of pixels. &nbsp;The resolution of the <i>Asteroids </i>vector display is 1024 x 768.</span></p><p><a href="https://1.bp.blogspot.com/-7gLKzZ6gULM/XaDx0uT70-I/AAAAAAAAOYo/aLUcLQJkl9AY1k2OYsewuUHydaxbkT0gwCLcBGAsYHQ/s1600/Asteroids_loop_transparent.gif"><img data-original-height="151" data-original-width="600" height="160" src="https://1.bp.blogspot.com/-7gLKzZ6gULM/XaDx0uT70-I/AAAAAAAAOYo/aLUcLQJkl9AY1k2OYsewuUHydaxbkT0gwCLcBGAsYHQ/s640/Asteroids_loop_transparent.gif" width="640"></a></p>
<br></div>
</div>
</div></div>]]>
            </description>
            <link>http://www.retrogamedeconstructionzone.com/2019/10/asteroids-by-numbers.html?m=1</link>
            <guid isPermaLink="false">hacker-news-small-sites-24655752</guid>
            <pubDate>Thu, 01 Oct 2020 20:19:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Use the internet, not just companies (2018)]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24655745">thread link</a>) | @downshun
<br/>
October 1, 2020 | https://sive.rs/netskill | <a href="https://web.archive.org/web/*/https://sive.rs/netskill">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

<article>
<header>


<small>2018-02-12</small>
</header>

<p>
	I’ve been online since 1994, and seen so many companies come and go.
</p><p>
	In the year 2000, the place to be was mp3.com.
	Every musician would keep all of their music and fans there.
	A few years later, it was gone — shut down — all music and fan lists deleted.
</p><p>
	In 2005, it was MySpace.
	Again, musicians kept all of their music, photos, and fans there.
	A few years later, it was gone.
	Not shut down, but basically moot.
	There was no way to communicate with all of those people, because you didn’t have their direct contact info — you only had their MySpace inbox, which nobody checked anymore.
</p><p>
	As I’m writing this now in 2018, it’s Facebook, YouTube, and Spotify.
	Just like with mp3.com and MySpace, people act like these websites are everything, and keep all of their music, photos, and fans there.
	By the time you read this, they might be gone.
</p><p>
<strong>
	Don’t depend on a company.
	They come and go.
</strong>
	Think long-term.
	You’re going to be creating stuff, making fans, and building relationships for the rest of your life — much longer than these companies will last.
</p><p>
<strong>
	So have your own website.
</strong>
	Instead of sending your fans to some company’s site, send them to yours.
	Get everyone’s direct contact information so you don’t have to go through a company to reach them.
</p><p>
<strong>
	Your website should be the definitive place to get everything you create.
</strong>
	If you put your stuff on some company’s site, have it be secondary — a copy of the stuff that’s already on your site.
	That way you can use the popular networks without depending on them.
</p><p>
	Only rely on open standards that aren’t owned by any company — like email and the web.
</p>
<h3>
	Email skills:
</h3>
<p>
	Go into your email settings, and make sure you <strong>have a signature</strong>.
	You need this because you’re going to be emailing people who have no idea who or where you are!
	Give them some context.
	Your signature should say who, what, and where, with a URL or two.
	For example:
</p>
<pre>--
Maya Danubé, fragrant jazz bass clarinet, New York City
http://mayadanube.com  <a href="https://sive.rs/cdn-cgi/l/email-protection" data-cfemail="3a575f7a575b435b5e5b544f585f14595557">[email&nbsp;protected]</a>  (917)611-5310
Watch &amp; listen: https://www.youtube.com/user/mayadanube
Friend me, baby: https://www.facebook.com/mayadanube
</pre>
<p>
	When you email people, write a <strong>descriptive subject</strong>.
	Never “hey” or “booking”.
	Try “Available June 6 for showcase?” or “introduction to photographer”.
	This is considerate.
	Now when your email is one of hundreds in an inbox, it will say exactly what is contained inside.
</p><p>
	Make it <strong>as short as possible</strong>.
	The shorter your email, the more likely it will get a response.
	Be direct.
	Five sentences is ideal.
	If your email is too long, they are likely to procrastinate, and never get back to it.
</p><p>
	Use short paragraphs.
	Leave plenty of space.
	Reading a screen is different from reading a book.
</p>
<h3>
	Web skills:
</h3>
<p>
<strong>
	Know how to update your website.
</strong>
	Don’t depend on someone else to do this for you.
	Know how to add new songs or videos, and how to make any changes.
</p><p>
<strong>
	Know your URLs.
</strong>
	Telling someone to go search for you is like telling them to look up your phone number.
	Instead, know your exact URLs (yoursite.com, twitter.com/something, facebook.com/whatever) so you can give it to people directly.
	If you don’t, they’ll probably never bother to go search for you.
</p><p>
<strong>
	Know how to make an MP3.
</strong>
	Give it a good filename like YOUR_NAME-Song_Title.mp3 (not mix7.mp3)
	Don’t use spaces in the filename.
	Edit the ID3 tags to put your full name and URL in the info, so whoever has this MP3 knows who it is and how to find you.
</p><p>
	Sorry if these sound too basic to you.
	But you’d be surprised by how many people don’t know these skills, and so are silently handicapped when interacting with the world.
</p>
<img alt="" src="https://sive.rs/images/internet-skills.gif">


</article>



</div></div>]]>
            </description>
            <link>https://sive.rs/netskill</link>
            <guid isPermaLink="false">hacker-news-small-sites-24655745</guid>
            <pubDate>Thu, 01 Oct 2020 20:18:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to build an open source business]]>
            </title>
            <description>
<![CDATA[
Score 48 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24654427">thread link</a>) | @mattgreg
<br/>
October 1, 2020 | https://www.ockam.io/learn/blog/zero_ipo/ | <a href="https://web.archive.org/web/*/https://www.ockam.io/learn/blog/zero_ipo/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p font-family="body" font-weight="body" font-size="body" color="text">Ockam’s Zero-to-IPO map is a key strategy input to our tactical short, medium and long-term business planning. It focuses on the one-thing that <em>really</em> matters, at specific points in time. We live our values at Ockam, and as an open source company, we want to share our roadmap.</p><p font-family="body" font-weight="body" font-size="body" color="text">As outlined in the progression below, we’ve plotted a course from stoking awareness to operating an enterprise sales machine.</p><p font-family="body" font-weight="body" font-size="body" color="text"><span>
      <a href="https://www.ockam.io/static/6a42376b8825dbf0d0fb048487413853/dbe83/map.png">
    <span></span>
  </a><a href="https://www.ockam.io/static/6a42376b8825dbf0d0fb048487413853/e3189/map.png" rel="noopener noreferrer" target="_blank"><img src="https://www.ockam.io/static/6a42376b8825dbf0d0fb048487413853/e3189/map.png" alt="Zero to IPO map" title="Zero to IPO map" srcset="https://www.ockam.io/static/6a42376b8825dbf0d0fb048487413853/a2ead/map.png 259w,https://www.ockam.io/static/6a42376b8825dbf0d0fb048487413853/6b9fd/map.png 518w,https://www.ockam.io/static/6a42376b8825dbf0d0fb048487413853/e3189/map.png 1035w,https://www.ockam.io/static/6a42376b8825dbf0d0fb048487413853/44d59/map.png 1553w,https://www.ockam.io/static/6a42376b8825dbf0d0fb048487413853/a6d66/map.png 2070w,https://www.ockam.io/static/6a42376b8825dbf0d0fb048487413853/dbe83/map.png 3652w" sizes="(max-width: 1035px) 100vw, 1035px" loading="lazy"></a>
  
    </span></p><p font-family="body" font-weight="body" font-size="body" color="text">The time scale for our route to IPO is, as you’d expect, years long. Given that startups plan around funding cycles, let’s plot funding cycles as waypoints on our course. It can generally be assumed that there is 18-24 months between these waypoints.</p><p font-family="body" font-weight="body" font-size="body" color="text"><span>
      <a href="https://www.ockam.io/static/f9b1d634d67c6a0cf8c599df614453fa/4eef4/funding.png">
    <span></span>
  </a><a href="https://www.ockam.io/static/f9b1d634d67c6a0cf8c599df614453fa/e3189/funding.png" rel="noopener noreferrer" target="_blank"><img src="https://www.ockam.io/static/f9b1d634d67c6a0cf8c599df614453fa/e3189/funding.png" alt="Funding time scale" title="Funding time scale" srcset="https://www.ockam.io/static/f9b1d634d67c6a0cf8c599df614453fa/a2ead/funding.png 259w,https://www.ockam.io/static/f9b1d634d67c6a0cf8c599df614453fa/6b9fd/funding.png 518w,https://www.ockam.io/static/f9b1d634d67c6a0cf8c599df614453fa/e3189/funding.png 1035w,https://www.ockam.io/static/f9b1d634d67c6a0cf8c599df614453fa/44d59/funding.png 1553w,https://www.ockam.io/static/f9b1d634d67c6a0cf8c599df614453fa/a6d66/funding.png 2070w,https://www.ockam.io/static/f9b1d634d67c6a0cf8c599df614453fa/4eef4/funding.png 3660w" sizes="(max-width: 1035px) 100vw, 1035px" loading="lazy"></a>
  
    </span></p><p font-family="body" font-weight="body" font-size="body" color="text">The cloud, edge, and open source landscape continues to evolve - which means that we need to chart our own course into the future. However, Ockam’s route to IPO also considers the various ways that other companies have run the gauntlet from Zero-to-IPO. I’ve been fortunate to have been ‘in the rooms where it happened’. Over the past 10 years I’ve directly worked with well over 100 companies that were underpinned by open source software projects. I’ve seen spectacular successes, breathtaking failures, modest acquisitions, and some companies that simply fade into the darkness. I'll save those stories for another time, maybe over a beer.</p><p font-family="body" font-weight="body" font-size="body" color="text">In the image below are experiences that I’ve drawn from the previous decade in the open source, cloud, and developer tool space.</p><p font-family="body" font-weight="body" font-size="body" color="text"><span>
      <a href="https://www.ockam.io/static/61433a9b6094d6dc270ffe138701bb4c/8ddda/rooms.png">
    <span></span>
  </a><a href="https://www.ockam.io/static/61433a9b6094d6dc270ffe138701bb4c/e3189/rooms.png" rel="noopener noreferrer" target="_blank"><img src="https://www.ockam.io/static/61433a9b6094d6dc270ffe138701bb4c/e3189/rooms.png" alt="Rooms where it happened" title="Rooms where it happened" srcset="https://www.ockam.io/static/61433a9b6094d6dc270ffe138701bb4c/a2ead/rooms.png 259w,https://www.ockam.io/static/61433a9b6094d6dc270ffe138701bb4c/6b9fd/rooms.png 518w,https://www.ockam.io/static/61433a9b6094d6dc270ffe138701bb4c/e3189/rooms.png 1035w,https://www.ockam.io/static/61433a9b6094d6dc270ffe138701bb4c/44d59/rooms.png 1553w,https://www.ockam.io/static/61433a9b6094d6dc270ffe138701bb4c/a6d66/rooms.png 2070w,https://www.ockam.io/static/61433a9b6094d6dc270ffe138701bb4c/8ddda/rooms.png 3724w" sizes="(max-width: 1035px) 100vw, 1035px" loading="lazy"></a>
  
    </span></p><p font-family="body" font-weight="body" font-size="body" color="text">Let’s dive into each stage, in turn, to unpack what we are doing, when we are doing it, and how we are going to measure it.</p><h2 id="motion" color="heading" font-family="heading" font-weight="heading">Motion<a href="#motion"></a></h2><p font-family="body" font-weight="body" font-size="body" color="text">In order to recruit our team, or for a developer to consider using Ockam, first they have to know we exist. We create and distribute a tremendous amount of content at Ockam with one goal - driving developer awareness.</p><p font-family="body" font-weight="body" font-size="body" color="text">For example, The first product Ockam shipped was <a href="https://www.ockam.io/learn/guides/team/values_and_virtues_on_the_Ockam_Team/">a blog on our Values</a>. The second was a white paper that shared our vision. Even this post is an example!  We have a learning library that outlines our thesis on the open source ecosystem, teaches computer science fundamentals, gives insights into our team culture, and demonstrates our technology. We’ve sat down for dozens of podcasts and interviews over the past two years. Ockam’s content is based around teaching. Being an effective listener and a great teacher are core underpinnings when building an open source community.</p><h2 id="metrics" color="heading" font-family="heading" font-weight="heading">Metrics<a href="#metrics"></a></h2><p font-family="body" font-weight="body" font-size="body" color="text">To gauge awareness we track activity including page views on ockam.io, 'contact us' webform inquiries, GitHub stars, social media mentions, followers and, most importantly, applications to join our team.</p><h2 id="motion-1" color="heading" font-family="heading" font-weight="heading">Motion<a href="#motion-1"></a></h2><p font-family="body" font-weight="body" font-size="body" color="text">This is a critically important step in our progression to IPO. Building Ockam's community is a never-ending endeavor. It takes years of focus and unrelenting attention to get this step right. For example, Kafka spent it's first 5 years in this phase as an Apache project before Confluent was started.</p><p font-family="body" font-weight="body" font-size="body" color="text">We have three code interfaces to Ockam, which means that there are three different personas in our community:</p><p font-family="body" font-weight="body" font-size="body" color="text"><strong>Application layer developers</strong></p><p font-family="body" font-weight="body" font-size="body" color="text">Ockam’s users build systems and applications with our simple APIs, OckamD binary downloads, and hosted cloud services.</p><p font-family="body" font-weight="body" font-size="body" color="text">To simplify what’s going on at this stage, we create packages that any developer can grab in the middle of the night, on the other side of the world, and get a quick win for their demo day at work. You’ve got a job to be done, and we’ve got a simple solution for you. You can get it right now and we will measure your time to a technical-win in the scale of minutes.</p><p font-family="body" font-weight="body" font-size="body" color="text"><strong>Partners</strong></p><p font-family="body" font-weight="body" font-size="body" color="text">Community partners build add-ons, connectors, and plug-ins to connect Ockam to other codebases, cloud services and hardware components. Examples include InfluxData, Confluent - Kafka, Microchip, NXP, MacOS, and Microsoft Azure.</p><p font-family="body" font-weight="body" font-size="body" color="text"><strong>Open source developers</strong></p><p font-family="body" font-weight="body" font-size="body" color="text">Ockam’s open source builders are engaged in development of Ockam's core codebase. They attend our monthly community meetings, and are hands-on with our OSS codebase on GitHub. Their participation ranges from updating a typo in documentation, to building complex features.</p><h2 id="metrics-1" color="heading" font-family="heading" font-weight="heading">Metrics<a href="#metrics-1"></a></h2><p font-family="body" font-weight="body" font-size="body" color="text">We track Monthly Active Users across all three personas in our community:</p><p font-family="body" font-weight="body" font-size="body" color="text">Binary downloads, account signups, or SaaS service IOPS are all indicators of usage. As hinted above, time to ‘technical win’, for an individual developer is also paramount. We’ve defined time to ‘technical win’ as the time it takes to go from an individual developer’s initial discovery to a working prototype that includes Ockam features.</p><p font-family="body" font-weight="body" font-size="body" color="text">The easiest user growth to track is the number of partner integrations. Since partners engage with us 1:1 on an integration, we are highly selective and deliberate about the partnerships that we support. Eventually the development of our technical partnerships will become programmatic. Programmatic examples from my past include the partner program for Heroku Add-ons and the Azure Marketplace partner portal.</p><p font-family="body" font-weight="body" font-size="body" color="text">We also track the intersection of partnerships and usage. For example, the number of Ockam Daemons that run alongside Influx Telegraf, or the number of IOPS in Ockam Routers that securely move packets to a Kafka Connector.</p><p font-family="body" font-weight="body" font-size="body" color="text">Finally open source activity and engagement is transparent through the tools in GitHub. Check out <a href="https://github.com/ockam-network">how we are doing</a> with stars, forks and commits.</p><h2 id="motion-2" color="heading" font-family="heading" font-weight="heading">Motion<a href="#motion-2"></a></h2><p font-family="body" font-weight="body" font-size="body" color="text">This is a really fun stage for a product-and-pricing-nut like me. By this point in our journey we have users, but not customers. To satisfy investor expectations and to further fund product development, we start to feed our product development machine with revenue.</p><p font-family="body" font-weight="body" font-size="body" color="text">This stage is far simpler than it’s often made out to be. Here’s my basic formula;</p><ul><li>If you are an individual developer, then Ockam is free.</li><li>If you are a commercial enterprise, but have not yet had a ‘technical win’ with Ockam, then Ockam is free.</li><li>If you are a commercial enterprise, and have had a ‘technical win’ with Ockam, then you pay.</li></ul><p font-family="body" font-weight="body" font-size="body" color="text">From here things get a bit more complicated. Services need to be packaged and priced. This, in my opinion, is the most challenging, but also the most fun part of product development. The classic product marketing mix (aka the 4P’s) framework is durable and applies for Ockam’s planned product offerings. In this phase we are packaging <strong>P</strong>roducts (say S, M, L sizes), establishing a <strong>P</strong>rice for each product, <strong>P</strong>romoting the product through rigorous segmentation and targeting, and <strong>P</strong>lacing it into various channels and partner marketplaces for distribution.</p><p font-family="body" font-weight="body" font-size="body" color="text">Ockam’s SaaS products will have a freemium pricing and packaging structure. It’s worth calling out that freemium is not a pricing strategy. It’s a customer acquisition tactic that aligns with the formula above.</p><h2 id="metrics-2" color="heading" font-family="heading" font-weight="heading">Metrics<a href="#metrics-2"></a></h2><p font-family="body" font-weight="body" font-size="body" color="text">Monthly Recurring Revenue (MRR) is the top line / key metric during this phase.</p><p font-family="body" font-weight="body" font-size="body" color="text">The free-to-paid funnel is another key metric since it is a leading indicator and helps to forecast MRR. We will track both conversion and velocity of our freemium SaaS users.</p><p font-family="body" font-weight="body" font-size="body" color="text">The metrics we track in the Self-Serve SaaS phase allow us to A/B test in our demand generation funnel. A/B testing allows us to optimize month-over-month revenue growth.  The target is 10-15% MoM growth.</p><h2 id="motion-3" color="heading" font-family="heading" font-weight="heading">Motion<a href="#motion-3"></a></h2><p font-family="body" font-weight="body" font-size="body" color="text">Inside Sales is a channel strategy for specific types of large customers we already have. This is mainly a cultivate-and-grow tactic. Our bottoms-up, Self Serve SaaS product model feeds leads to our Inside Sales Team. This team is technical, includes sales engineers and provides world-class support.</p><p font-family="body" font-weight="body" font-size="body" color="text">There are two separate objectives during this phase.</p><ul><li>Increase MRR through an increase in our customer base, and in the average ticket size.</li><li>Learn about Customer Acquisition Costs (CAC) for specific segments, prior to launching the Enterprise Sales phase.</li></ul><p font-family="body" font-weight="body" font-size="body" color="text">Monthly recurring revenue is still our top priority during the Inside Sales phase.</p><p font-family="body" font-weight="body" font-size="body" color="text">What’s less obvious is the second objective. Understanding CAC prepares us for an all-in Enterprise Sales motion. Moving from here onto the Enterprise Sales waypoint is probably the most challenging. It’s fraught with peril. Many, many smart companies, with great products, and ‘developer love’ die right here.</p><p font-family="body" font-weight="body" font-size="body" color="text">How can that be? It’s because Inside Sales is bottoms-up and Enterprise Sales is tops-down. This means entirely new buyers, new product-marketing mix, and new internal talent. We must hold onto our developer roots, while we also learn to sell to the suits. While we are executing on Inside Sales we are doing the primary research that will help spawn a new company from our company.</p><p font-family="body" font-weight="body" font-size="body" color="text">This is fantastically difficult - mostly from a cultural standpoint. Fortunately there are a lot of people with a lot of scar tissue from the past 10 years - including myself - and we will push through. The key is patience. We need to use our inside sales motion to find specific beachheads to land our Enterprise Sales motion.</p><h2 id="metrics-3" color="heading" font-family="heading" font-weight="heading">Metrics<a href="#metrics-3"></a></h2><p font-family="body" font-weight="body" font-size="body" color="text">MRR carries over as our key metric from the Self Serve SaaS phase.</p><p font-family="body" font-weight="body" font-size="body" color="text">CAC analysis for multiple customer segments.</p><h2 id="anti-metrics" color="heading" font-family="heading" font-weight="heading">Anti-Metrics<a href="#anti-metrics"></a></h2><p font-family="body" font-weight="body" font-size="body" color="text">There will be noise in our Inside Sales data!</p><p font-family="body" font-weight="body" font-size="body" color="text">The noise is any sale that looks like it could be Enterprise Sales. Up to this point, non-recurring engineering (NRE) and enterprise-like sales don’t count as Enterprise Sales, as we define the term in the next section. Typically they are one-off deals because the motion to win these deals isn’t scalable. We will do large custom deals to gain access to smart teams that deploy interesting technology. I prefer to categorize this class of revenue as ‘business development’ or even R&amp;D.</p><p font-family="body" font-weight="body" font-size="body" color="text">Why is this an anti-metric? Because other Open Source startups typically stand up a couple one-off enterprises like sales as a way to puff themselves up and to convince themselves that they are ready to move to the next phase. I strongly caution my future self to parse the noise from the signal prior to launching Enterprise Sales.</p><p font-family="body" font-weight="body" font-size="body" color="text">Furthermore, there are other Open Source companies that entirely bypass the Self Serve SaaS phase in favor of the chunky revenue that comes with Enterprise Sales. Those companies tend not to be product companies. They become …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.ockam.io/learn/blog/zero_ipo/">https://www.ockam.io/learn/blog/zero_ipo/</a></em></p>]]>
            </description>
            <link>https://www.ockam.io/learn/blog/zero_ipo/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24654427</guid>
            <pubDate>Thu, 01 Oct 2020 18:26:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Startup Lessons I Needed to Learn First Hand (But Maybe You Don’t)]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24654192">thread link</a>) | @jlrubin
<br/>
October 1, 2020 | http://www.nancyhua.com/2020/10/01/startup-lessons-i-needed-to-learn-first-hand-but-maybe-you-dont/ | <a href="https://web.archive.org/web/*/http://www.nancyhua.com/2020/10/01/startup-lessons-i-needed-to-learn-first-hand-but-maybe-you-dont/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<div id="content">

<div>
	<div id="primary">
		<main id="main" role="main">

			
<article id="post-1586">
		<!-- .entry-header -->

	
	<div>
		
<p>When I stepped down as CEO in 2018, I wrote a post mortem and shared it privately with founder friends who directly asked <a rel="noreferrer noopener" href="http://www.nancyhua.com/2020/01/06/2019-2020-post-pre-mortems/" target="_blank">when I blogged here</a>. The company got acquired in 2019 and now I’m sharing the post mortem publicly because readers told me they saw novel concepts in my document they hadn’t heard elsewhere (as I write this I wonder if it’s because I was wrong. LMK!). </p>



<p>I used to abhor failure, but publicly releasing this post mortem no longer holds charge for me. Through Apptimize, I’ve learned and changed such that my subsequent companies will be very different. One of my biggest learnings is that I’d played a finite game and missed <a rel="noreferrer noopener" href="https://youtu.be/3QyurhNwk14?t=56" target="_blank">the infinite game</a>. I didn’t know those concepts at the time and saw “product innovation” as a separate category of work. After shifting my reference frame, I now know innovation as a sign of infinite game behavior. Anyway, I hope the below is useful to founders whose sales motions aren’t getting easier years into their venture backed company and want to consider frameworks for evaluating their position.&nbsp;</p>



<p>=================</p>



<p><strong>Startup Post Mortem, </strong>written Q3 2018</p>



<p>At times, the company we founded in 2013 seemed to be doing well by various objective metrics— we had a prestigious customers list ranging from CNN to Comcast, we raised 3 funding rounds summing to over $20MM in venture capital investment, and our revenue grew exponentially for the first few years (obviously easier to 3x when x is small). As the cofounder and CEO, I always bet on our ability to figure it out and be a financial success. I put in the first $50K and bought our domain for an additional $10K, which isn’t much money in the scheme of startup funding, but this was before we had users, before we’d gotten into Y Combinator, before it was anyone other than me and my cofounder. I used to be a trader, so I wasn’t goofing around— I fully expected to eventually make tons of money off our startup. I wrote a draft S-1 for how we would IPO, I didn’t pay myself for the first year, I was the lowest paid person in the company for years, and I guarded our equity like it was the blood of my children. I always wanted more equity because I valued it so highly. When founder friends told me to pay myself more, I asked for more equity instead. When we raised an oversubscribed Series B, founder friends told me to ask if I could sell some of my shares or take money off the table, but again I asked for more equity instead. Suggestions to get cash seemed ridiculous to me because I didn’t think I deserved cash yet; we weren’t a success and I, more than anyone, knew all our warts. When we were getting acquired, founder friends suggested I block the acquisition unless I made money off it, but that also sounded ridiculous to me because I felt I deserved money least of all. I’m sure my VC’s would’ve agreed.</p>



<p>Our company didn’t exit at anywhere near as well as I’d pitched, and I felt sad to fail after so many years of everyone working so hard. For years, we worked weekends and holidays, regularly in the office till 10pm. My VP of marketing was back at work weeks after birthing each of her babies, working through her pregnancies, and we forced anyone who entered my house or office to do user tests. Had it all been a waste? Should we have spent that time partying instead? Being successful is important to me and I felt ashamed my company wasn’t a financial success despite how hard everyone worked on it and how much money we raised. Sure, I could twist the story to make it sound like a success in terms of learning and building, and we made a product people used, and we got acquired, but the fact is that the company didn’t make money the way I’d imagined and pitched. I felt scared my investors would view me as a failure and dislike me or view me as incompetent. We should’ve done better— we had some of the smartest people you’d ever meet working on this problem that I convinced them was important enough to warrant their time and resources. How had I been so wrong about the financial outcome?&nbsp;</p>



<p><strong>Two Key Qualifying Questions:</strong></p>



<p>One of my investors put it well: everyone in a company is either a) making the product or b) selling the product. I learned there were 2 key questions that separated successful vs unsuccessful hires in our company:</p>



<ol><li>How hard is it to make this product?</li><li>How hard is it to sell this product?</li></ol>



<p>Our product was both hard to make and hard to sell. What do I mean by this and how does this impact the hiring profile?</p>



<p><strong>Product vs Sales Driven Company:</strong></p>



<p>On the spectrum of how hard it is to build a product, web forms are on the easier side. Easy products are anything that a person could do with a series of google docs and sheets, anything that you’re 100% sure is possible to make. On the harder side, there are products like a rocket or a flying car, where it’s &lt;100% guaranteed the engineering will get there in the time required. If the product is easy to build, then engineering is easier and it’s more on the sales and marketing teams to drive the company forward and show why your company is better even though others can make this commoditizable product (through network effect/ better land grab execution, brand/ trust, integrations/ partnerships, “thought leadership,” customer service/ support, etc).&nbsp;</p>



<p>In contrast, the harder a product is to build, the better engineers you need and the more everything depends on the product team shipping something 10x better.&nbsp;</p>



<p>Our product was nowhere as hard to build as a rocket, but it was harder than a webapp, and we made design choices that increased the difficulty of building and maintaining our product in exchange for gaining competitive advantage, which was high at one point but eroded. This means our company had to be product driven. But after the first few years, we failed to be product driven because 1) I struggled to hire product leadership that was technical enough and 2) I was short term focused on revenue goals. Single-threaded on sales, I didn’t focus on the product roadmap because all I cared about were short term goals to lead us to the next funding round because I was mainly driven by my fear of the startup failing versus any love for shipping a better product.&nbsp;</p>



<p><strong>Transactional vs. Consultative Sales:</strong></p>



<p>Everyone in B2B SaaS knows from SaaStr etc you’re supposed to distinguish between sales people who sold to technical vs non-technical teams, and differentiate sales candidates based on the price point they were comfortable selling at, but I learned an additional point of differentiation: how consultative must the sale be? On the spectrum of how hard it is to sell a product, widgets like video conferencing software are on the easier end, easy to explain and demo. On the harder end, there’s consulting services to suggest TBD process improvements. Even harder is stuff that’s a new category where you have to educate the buyer on the need. The hardest sales require founders to drive sales; the salesperson needs to be at least as smart as the buyer so they can credibly educate the buyer on how the product will urgently impact their revenue. Buyers of video conferencing software don’t expect to get promoted because they chose Zoom over Webex or talk about the impact of their choice at a conference, but buyers of analytics software do want to hear how they’re going to become Chief Product Officer vs VP, that they’re going to show their CEO a powerpoint with graphs clearly illustrating the revenue their analytics choices have created for their team, and how they’re going to speak at the conference on their data driven decision making processes.</p>



<p>If the product fulfills a clear, established need, you can hire a wider variety of salespeople. But when the product’s harder to sell, you need a “consultative” salesperson, a specific profile correlated but distinct from price point. When the product differences/ usage/ impact are hard to explain, or there’s no category yet, or it’s not a drastic, budgeted need, you need sales people who are like consultants, subject matter experts who are smarter than the buyers.&nbsp;</p>



<p>If the sales person is interested in presenting a custom, strategic overview of how our product impacts the buyer’s product strategy, they eventually want to become customer success managers. Other than our first business hire, who was more like a cofounder to me and eventually founded his own company, I couldn’t get anyone to do both sales and customer success at the same time. I think our deals weren’t big enough and our customer success process was in the awkward gap between easy and hard— not hard enough to warrant consulting services, but not easy enough to remain a yearly check-in to upgrade the account.&nbsp;</p>



<figure><img loading="lazy" width="1024" height="856" src="http://www.nancyhua.com/wp-content/uploads/2020/10/Pros-and-cons-of-product-vs.-sales-driven-startups--1024x856.png" alt="" srcset="http://www.nancyhua.com/wp-content/uploads/2020/10/Pros-and-cons-of-product-vs.-sales-driven-startups--1024x856.png 1024w, http://www.nancyhua.com/wp-content/uploads/2020/10/Pros-and-cons-of-product-vs.-sales-driven-startups--400x334.png 400w, http://www.nancyhua.com/wp-content/uploads/2020/10/Pros-and-cons-of-product-vs.-sales-driven-startups--768x642.png 768w, http://www.nancyhua.com/wp-content/uploads/2020/10/Pros-and-cons-of-product-vs.-sales-driven-startups-.png 1404w" sizes="(max-width: 767px) 89vw, (max-width: 1000px) 54vw, (max-width: 1071px) 543px, 580px"><figcaption><em>Pros and cons of product vs. sales driven startups</em></figcaption></figure>



<p><strong>Fear/ Ego:</strong></p>



<p>Shifting gears from the tactical company building stuff to the touchy feely, the following section is philosophical.</p>



<p>It had started out really fun. In the 2nd year of the company, one executive told me that she got all her social fulfillment from our work. We were always together, working from my house on weekends, engineers sleeping over when they got tired, cooking together, talking about each other’s love languages, a group of friends going on an adventure together.&nbsp;</p>



<p>But now I see that I didn’t start the company with a pure heart. I started the company because I thought it’d be successful and I wanted to prove I could contribute something to the world, not because I specifically cared about our product or market, which I learned matters for me as time passes.&nbsp;</p>



<p>I thought I could get passionate about anything, and that was true at first, but it drained me to force myself to be an expert on our product for years because it wasn’t something I would’ve done if it weren’t for the company, the team, and my ego. For years, I always knew the most about our market and would send links to the rest of the team for news that had come out, anything they …</p></div></article></main></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.nancyhua.com/2020/10/01/startup-lessons-i-needed-to-learn-first-hand-but-maybe-you-dont/">http://www.nancyhua.com/2020/10/01/startup-lessons-i-needed-to-learn-first-hand-but-maybe-you-dont/</a></em></p>]]>
            </description>
            <link>http://www.nancyhua.com/2020/10/01/startup-lessons-i-needed-to-learn-first-hand-but-maybe-you-dont/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24654192</guid>
            <pubDate>Thu, 01 Oct 2020 18:06:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Programmers need to think like hackers]]>
            </title>
            <description>
<![CDATA[
Score 29 | Comments 17 (<a href="https://news.ycombinator.com/item?id=24654136">thread link</a>) | @gexos
<br/>
October 1, 2020 | https://gexos.org/2020/10/01/Programmers-need-to-think-like-hackers!/ | <a href="https://web.archive.org/web/*/https://gexos.org/2020/10/01/Programmers-need-to-think-like-hackers!/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://gexos.org/2020/10/01/Programmers-need-to-think-like-hackers!/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24654136</guid>
            <pubDate>Thu, 01 Oct 2020 18:01:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AWS Is Not Your Ideal]]>
            </title>
            <description>
<![CDATA[
Score 32 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24653013">thread link</a>) | @amortize
<br/>
October 1, 2020 | https://sujithjay.com/not-aws | <a href="https://web.archive.org/web/*/https://sujithjay.com/not-aws">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div>
  
  <p><span> 01 Oct 2020  • <span>
  
    
    
    <a href="https://sujithjay.com/tag/product"><code><nobr>PRODUCT</nobr></code>&nbsp;</a>
  
    
    
    <a href="https://sujithjay.com/tag/management"><code><nobr>MANAGEMENT</nobr></code>&nbsp;</a>
  
</span></span></p><p>Let me start with an assertion. Every platform engineering team <sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup> in every organisation aspires to be like AWS <sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup>.</p>

<p>Every platform team wants to be like AWS, because like AWS, they provide infrastructure abstractions to users. AWS provides infrastructure via the abstractions of VMs and disks and write-capacity-units, while platform teams provide infrastructure using higher abstractions which solve service definitions, database or message queue provisioning, and service right-sizing <sup id="fnref:3" role="doc-noteref"><a href="#fn:3">3</a></sup>.</p>

<p>This similarity prompts leaders of platform engineering teams to model their teams as agnostic providers of universal, non-leaky (within SLO bounds), self-served abstractions for their engineering organisation. Platform teams structured as such detached units struggle to define cohesive roadmaps which provide increasing value to business. But how does your platform differ from AWS?</p>

<h2 id="your-platform-vs-the-platform">Your Platform vs. The Platform</h2>

<h3 id="1-the-middle-ground">1. The Middle Ground</h3>
<p>As an agnostic service provider, AWS can afford to cater to median use-cases. The reason platform engineering teams exist is to bridge the gap between PaaS abstractions which work for the median use-case to your business’ specific use-cases. AWS can afford to target the median (economy of scale etc.), but you cannot.</p>

<p><img src="https://sujithjay.com/public/notaws/Median.jpeg" alt="AWS can afford to stay within a single σ. You cannot."></p>
<p><span> AWS can afford to stay within a single σ. You cannot.</span>
</p>

<p>Agnostic platform engineering teams which emulate AWS try to get away from this responsibility by proposing abstractions which target the median use-case. A tell-tale sign of this is when the lack in wide usability of internal abstractions is compensated for by extensive onboarding &amp; repeated training. This is also a side-effect of the relative valuation of engineering time vs. the time of another function <sup id="fnref:4" role="doc-noteref"><a href="#fn:4">4</a></sup>.</p>

<h3 id="2-follow-the-money">2. Follow the Money</h3>
<p>The dictum ‘follow the money’ works beautifully for customer-front products. When faced with a choice between two competing features to prioritise, a common tactical play is to make something which leads to more (immediate &amp; long-term) revenue. The proxy for increased revenue could be increased acquisition conversion, better retention or improved user experience – metrics which ensure increased revenue for the company over time. In short, revenue growth is the north star <sup id="fnref:5" role="doc-noteref"><a href="#fn:5">5</a></sup>.</p>

<p>Not so much in platform engineering. There is no revenue since your customers are internal, captive ones. Captive audiences are forced to use a solution by the force of dictum and lack of choice. The metrics used in platform products are proxies for usability and user satisfaction – but there are no foolproof ways to measure it for captive audiences. For captive audiences, solutions can not compete and better solutions cannot win. Like a command economy, platform products are designed rather than evolved. Design takes priority over market economy. So why is design bad?</p>

<h2 id="bad-design">Bad Design</h2>
<p>For design to work, there has to be an objective function against which we can design. A specification is an objective function against which engineering teams design a solution. Since we do not have reliable metrics <sup id="fnref:6" role="doc-noteref"><a href="#fn:6">6</a></sup> to rely on for platform engineering, how do we come up with specifications? And without rigorous specifications, new features created by the platform run a high risk of not solving worthwhile problems for the users.The current accepted methodology among platform engineering leaders to solve this paucity of specifications is to rely on user-interviews. This is, as mentioned before, an unreliable source since captive users do not have the best view of the ideal state of tooling and abstractions that could be available to them.</p>

<p>The only way to flip this situation is to let go of command-economy-style designed abstractions, and to let your platform self-organise along the principle of markets. How does that look in practice?</p>

<h3 id="1-market-ftw">1. Market, FTW</h3>
<p>Camille Fournier mentions in <a href="https://medium.com/@skamille/product-for-internal-platforms-9205c3a08142">Product for Internal Platforms</a> how her team partners with customer teams to develop prototypes for specific problems. These specific solutions are later honed and iterated on to become general solutions provided by your team. I would go a step further on this route, where possible. Partner to prototype with multiple teams facing related problems to develop multiple specific solutions. These specific solutions can be seen as competing candidates to solve a general problem. Bring in user-interviews at this point to gauge pain-points, and iterate individually on these specific solutions. This switches the economy of your team to a self-organised market. Once considerable thought and iteration has gone into each solution, it is time to assimilate. Assimilate the best solution(s) while migrating the rest to the chosen solution. As emphasised in <a href="https://medium.com/@skamille/product-for-internal-platforms-9205c3a08142">Product for Internal Platforms</a>, an early investment of time into migration strategies is essential for such a scheme to sustain.</p>

<p>In platforms designed with experimentation, you will find that innovation continues to thrive at the edges of the platform’s domain while the stable core of the platform is subject to periodic rework or maintenance. The use-cases a platform supports grows in a controlled manner to address an ever-growing percent of the consumers, and does not stagnate after addressing just the median users.</p>

<h3 id="2-overloaded-use-cases">2. Overloaded Use-cases</h3>
<p>Although agnostic platform engineering teams might only be catering to very specific median use-cases, the customer teams with specific needs cannot afford to be blocked and they cannot stop delivering their deliverables. These teams sometimes create their own solutions, and in such cases the above strategy of assimilation works wonders. You get a prototype for free on which the team can iterate on. However, this scenario is rarer in cases where it requires specific skills to build such solutions, such as in data platforms. One common pattern in such knowledge-constricted situations is that users find ways to overload the existing solutions with minor tweaks to fit their use-case. Look out for such overloaded use-cases within your platform, for they are excellent guides to unmet needs of the users. You can leverage them to advocate for newer features to explicitly support those use-cases.</p>

<h3 id="3-listen-to-them-only-at-the-start">3. Listen To Them (Only At The Start!)</h3>
<p>As a parting note, I will take a jab at user-interviews again. The above tactics work when you are trying to scale your platform from 1 to N. When taking a platform from 0 to 1, the only solution to creating specifications is to listen to the users. Give them exactly what they want. Listen to their exact demands. A propensity of platform product managers is to rely on this excessively at a much later stage in the product’s lifecycle. User-interviews have their place in evolving products, but the over-reliance on the methodology is a bane to platform product management.</p>

<p><strong>P.S.</strong> As I read back the above essay, the heavy influence of <a href="https://medium.com/@skamille/product-for-internal-platforms-9205c3a08142">Product for Internal Platforms</a> is clear. I would like to say that was the intention: to reassert the ideas in it which resounded with me, while stating a few of my own.</p>

<h3 id="footnotes">Footnotes</h3>


  </div>









      </div></div>]]>
            </description>
            <link>https://sujithjay.com/not-aws</link>
            <guid isPermaLink="false">hacker-news-small-sites-24653013</guid>
            <pubDate>Thu, 01 Oct 2020 16:37:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Compiling a Lisp to x86-64: Let expressions]]>
            </title>
            <description>
<![CDATA[
Score 129 | Comments 36 (<a href="https://news.ycombinator.com/item?id=24652842">thread link</a>) | @todsacerdoti
<br/>
October 1, 2020 | https://bernsteinbear.com/blog/compiling-a-lisp-7/ | <a href="https://web.archive.org/web/*/https://bernsteinbear.com/blog/compiling-a-lisp-7/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p><em><a href="https://bernsteinbear.com/blog/compiling-a-lisp-0/">first</a></em> – <em><a href="https://bernsteinbear.com/blog/compiling-a-lisp-6/">previous</a></em></p>

<p>Welcome back to the “Compiling a Lisp” series. Last time we added a reader
(also known as a parser) to our compiler. This time we’re going to compile a
new form: <em>let</em> expressions.</p>

<p>Let expressions are a way to bind variables to values in a particular scope.
For example:</p>

<div><div><pre><code><span>(</span><span>let</span> <span>((</span><span>a</span> <span>1</span><span>)</span> <span>(</span><span>b</span> <span>2</span><span>))</span>
  <span>(</span><span>+</span> <span>a</span> <span>b</span><span>))</span>
</code></pre></div></div>

<p>Binds <code>a</code> to <code>1</code> and <code>b</code> to <code>2</code>, but only for the body of the <code>let</code> — the
rest of the S-expression — and then executes the body.</p>

<p>This is similar in C to opening a new block:</p>

<div><div><pre><code><span>int</span> <span>result</span><span>;</span>
<span>{</span>
  <span>int</span> <span>a</span> <span>=</span> <span>1</span><span>;</span>
  <span>int</span> <span>b</span> <span>=</span> <span>2</span><span>;</span>
  <span>result</span> <span>=</span> <span>a</span> <span>+</span> <span>b</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>but it’s a little different because C has a divide between <em>statements</em> and
<em>expressions</em>, whereas Lisp does not.</p>

<p>It’s <em>also</em> different because let-expressions do not make previous binding
names available to expressions being bound. For example, the following program
should fail because it cannot find the name <code>a</code>:</p>



<p>There is a form that makes bindings available serially, but that is called
<code>let*</code> and we are not implementing that today.</p>

<p>For completeness’ sake, there is also <code>let rec</code>, which makes names available
serially and also within the same binding. This is useful for binding recursive
or mutually recursive functions. Again, we are not implementing that today.</p>

<h3 id="name-binding-implementation-strategy">Name binding implementation strategy</h3>

<p>You’ll notice two new things about let expressions:</p>

<ol>
  <li>They introduce ways to bind names to values, something we have to figure out
how to keep track of</li>
  <li>In order to use those names we have to figure out how to look up what the
name means</li>
</ol>

<p>In more technical terms, we have to add <em>environments</em> to our compiler. We can
then use those environments to map <em>names</em> to <em>stack locations</em>.</p>

<p>“Environment” is just a fancy word for “look-up table”. In order to implement
this table, we’re going to make an <em>association list</em>.</p>

<p>An <em>association list</em> is a list of <code>(key value)</code> pairs. Adding a pair means
tacking it on at the end (or beginning) of the list. Searching through the
table involves a linear scan, checking if keys match.</p>

<blockquote>
  <p>You may be wondering why we’re using this data structure to implement
environments. Didn’t I even take a data structures course in college?
Shouldn’t I know that <em>linear</em> equals <em>slow</em> and that I should <em>obviously</em>
use a hash table?</p>

  <p>Well, hash tables have costs too. They are hard to implement right; they have
high overhead despite being technically constant time; they incur higher
space cost per entry.</p>

  <p>For a compiler as small as this, a tuned hash table could easily be as long
as the rest of the compiler. Since we’re also compiling small <em>programs</em>,
we’ll worry about time complexity later. It is only an implementation detail.</p>
</blockquote>

<p>In order to do this, we’ll first draw up an association list. We’ll use a
linked list, just like cons cells:</p>

<div><div><pre><code><span>// Env</span>

<span>typedef</span> <span>struct</span> <span>Env</span> <span>{</span>
  <span>const</span> <span>char</span> <span>*</span><span>name</span><span>;</span>
  <span>word</span> <span>value</span><span>;</span>
  <span>struct</span> <span>Env</span> <span>*</span><span>prev</span><span>;</span>
<span>}</span> <span>Env</span><span>;</span>
</code></pre></div></div>

<p>I’ve done the usual thing and overloaded <code>Env</code> to mean both “a node in the
environment” and “a whole environment”. While one little <code>Env</code> struct only
holds a one name and one value, it also points to the rest of them, eventually
ending with <code>NULL</code>.</p>

<p>This <code>Env</code> will map names (symbols) to <em>stack offsets</em>. This is because we’re
going to continue our strategy of <em>not doing register allocation</em>.</p>

<p>To manipulate this data structure, we will also have two functions<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup>:</p>

<div><div><pre><code><span>Env</span> <span>Env_bind</span><span>(</span><span>const</span> <span>char</span> <span>*</span><span>name</span><span>,</span> <span>word</span> <span>value</span><span>,</span> <span>Env</span> <span>*</span><span>prev</span><span>);</span>
<span>bool</span> <span>Env_find</span><span>(</span><span>Env</span> <span>*</span><span>env</span><span>,</span> <span>const</span> <span>char</span> <span>*</span><span>key</span><span>,</span> <span>word</span> <span>*</span><span>result</span><span>);</span>
</code></pre></div></div>

<p><code>Env_bind</code> creates a new node from the given name and value, borrowing a
reference to the name, and prepends it to <code>prev</code>. Instead of returning an
<code>Env*</code>, it returns a whole struct. We’ll learn more about why later, but the
“TL;DR” is that I think it requires less manual cleanup.</p>

<p><code>Env_find</code> takes an <code>Env*</code> and searches through the linked list for a <code>name</code>
matching the given <code>key</code>. If it finds a match, it returns <code>true</code> and stores the
<code>value</code> in <code>*result</code>. Otherwise, it returns <code>false</code>.</p>

<p>We can stop at the first match because Lisp allows name <em>shadowing</em>. Shadowing
occurs when a binding at a inner scope has the same name as a binding at an
outer scope. The inner binding takes precedence:</p>

<div><div><pre><code><span>(</span><span>let</span> <span>((</span><span>a</span> <span>1</span><span>))</span>
  <span>(</span><span>let</span> <span>((</span><span>a</span> <span>2</span><span>))</span>
    <span>a</span><span>))</span>
<span>; =&gt; 2</span>
</code></pre></div></div>

<p>Let’s learn about how these functions are implemented.</p>

<h3 id="name-binding-implementation">Name binding implementation</h3>

<p><code>Env_bind</code> is a little silly looking, but it’s equivalent to prepending a
node onto a chain of linked-list nodes. It returns a struct <code>Env</code> containing
the parameters passed to the function. I opted <em>not</em> to return a heap pointer
(allocated with <code>malloc</code>, etc) so that this can be easily stored in a
stack-allocated variable.</p>

<div><div><pre><code><span>Env</span> <span>Env_bind</span><span>(</span><span>const</span> <span>char</span> <span>*</span><span>name</span><span>,</span> <span>word</span> <span>value</span><span>,</span> <span>Env</span> <span>*</span><span>prev</span><span>)</span> <span>{</span>
  <span>return</span> <span>(</span><span>Env</span><span>){.</span><span>name</span> <span>=</span> <span>name</span><span>,</span> <span>.</span><span>value</span> <span>=</span> <span>value</span><span>,</span> <span>.</span><span>prev</span> <span>=</span> <span>prev</span><span>};</span>
<span>}</span>
</code></pre></div></div>

<p><em>Note</em> that we’re <strong>pre</strong>pending, not <strong>ap</strong>pending, so that names we add deeper
in a let chain shadow names from outside.</p>

<p><code>Env_find</code> does a recursive linear search through the linked list nodes. It may
look familiar to you if you’ve already written such a function in your life.</p>

<div><div><pre><code><span>bool</span> <span>Env_find</span><span>(</span><span>Env</span> <span>*</span><span>env</span><span>,</span> <span>const</span> <span>char</span> <span>*</span><span>key</span><span>,</span> <span>word</span> <span>*</span><span>result</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>env</span> <span>==</span> <span>NULL</span><span>)</span>
    <span>return</span> <span>false</span><span>;</span>
  <span>if</span> <span>(</span><span>strcmp</span><span>(</span><span>env</span><span>-&gt;</span><span>name</span><span>,</span> <span>key</span><span>)</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
    <span>*</span><span>result</span> <span>=</span> <span>env</span><span>-&gt;</span><span>value</span><span>;</span>
    <span>return</span> <span>true</span><span>;</span>
  <span>}</span>
  <span>return</span> <span>Env_find</span><span>(</span><span>env</span><span>-&gt;</span><span>prev</span><span>,</span> <span>key</span><span>,</span> <span>result</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>We search for the node with the string <code>key</code> and return the stack offset associated
with it.</p>

<p>Alright, now we’ve got names and data structures. Let’s implement some name
resolution and name binding.</p>

<h3 id="compiling-name-resolution">Compiling name resolution</h3>

<p>Up until now, <code>Compile_expr</code> could only compile integers, characters, booleans,
<code>nil</code>, and some primitive call expressions (via <code>Compile_call</code>). Now we’re
going to add a new case: symbols.</p>

<p>When a symbol is compiled, the compiler will look up its stack offset in the
current environment and emit a load. This opcode, <code>Emit_load_reg_indirect</code>, is
very similar to <code>Emit_add_reg_indirect</code> that we implemented for primitive
binary functions.</p>

<div><div><pre><code><span>int</span> <span>Compile_expr</span><span>(</span><span>Buffer</span> <span>*</span><span>buf</span><span>,</span> <span>ASTNode</span> <span>*</span><span>node</span><span>,</span> <span>word</span> <span>stack_index</span><span>,</span>
                 <span>Env</span> <span>*</span><span>varenv</span><span>)</span> <span>{</span>
  <span>// ...</span>
  <span>if</span> <span>(</span><span>AST_is_symbol</span><span>(</span><span>node</span><span>))</span> <span>{</span>
    <span>const</span> <span>char</span> <span>*</span><span>symbol</span> <span>=</span> <span>AST_symbol_cstr</span><span>(</span><span>node</span><span>);</span>
    <span>word</span> <span>value</span><span>;</span>
    <span>if</span> <span>(</span><span>Env_find</span><span>(</span><span>varenv</span><span>,</span> <span>symbol</span><span>,</span> <span>&amp;</span><span>value</span><span>))</span> <span>{</span>
      <span>Emit_load_reg_indirect</span><span>(</span><span>buf</span><span>,</span> <span>/*dst=*/</span><span>kRax</span><span>,</span> <span>/*src=*/</span><span>Ind</span><span>(</span><span>kRbp</span><span>,</span> <span>value</span><span>));</span>
      <span>return</span> <span>0</span><span>;</span>
    <span>}</span>
    <span>return</span> <span>-</span><span>1</span><span>;</span>
  <span>}</span>
  <span>assert</span><span>(</span><span>0</span> <span>&amp;&amp;</span> <span>"unexpected node type"</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>If the variable is not in the environment, this is a compiler error and we
return <code>-1</code> to signal that. This is not a tremendously helpful signal. Maybe
soon we will add more helpful error messages.</p>

<p>Ah, yes, <code>varenv</code>. You will, like I had to, go and add an <code>Env*</code> parameter to
all relevant <code>Compile_XYZ</code> functions and then plumb it through the recursive
calls. Have fun!</p>

<h3 id="compiling-let-finally">Compiling let, finally</h3>

<p>Now that we can resolve the names, let’s go ahead and compile the expressions
that bind them.</p>

<p>We’ll have to add a case in <code>Compile_expr</code>. We could add it in the body of
<code>Compile_expr</code> itself, but there is some helpful setup in <code>Compile_call</code>
already. It’s a bit of a misnomer, since it’s not a call, but oh well.</p>

<div><div><pre><code><span>int</span> <span>Compile_call</span><span>(</span><span>Buffer</span> <span>*</span><span>buf</span><span>,</span> <span>ASTNode</span> <span>*</span><span>callable</span><span>,</span> <span>ASTNode</span> <span>*</span><span>args</span><span>,</span>
                 <span>word</span> <span>stack_index</span><span>,</span> <span>Env</span> <span>*</span><span>varenv</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>AST_is_symbol</span><span>(</span><span>callable</span><span>))</span> <span>{</span>
    <span>// ...</span>
    <span>if</span> <span>(</span><span>AST_symbol_matches</span><span>(</span><span>callable</span><span>,</span> <span>"let"</span><span>))</span> <span>{</span>
      <span>return</span> <span>Compile_let</span><span>(</span><span>buf</span><span>,</span> <span>/*bindings=*/</span><span>operand1</span><span>(</span><span>args</span><span>),</span>
                         <span>/*body=*/</span><span>operand2</span><span>(</span><span>args</span><span>),</span> <span>stack_index</span><span>,</span>
                         <span>/*binding_env=*/</span><span>varenv</span><span>,</span>
                         <span>/*body_env=*/</span><span>varenv</span><span>);</span>
    <span>}</span>
  <span>}</span>
  <span>assert</span><span>(</span><span>0</span> <span>&amp;&amp;</span> <span>"unexpected call type"</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>We have two cases to handle: no bindings and some bindings. We’ll tackle these
recursively, with no bindings being the base case. For that reason, I added a
helper function <code>Compile_let</code>.</p>

<p>As with all of the other compiler functions, we pass it an machine code buffer,
a stack index, and an environment. Unlike other functions, we passed it two
expressions and two environments.</p>

<p>I split up the bindings and the body so we can more easily recurse on the
bindings as we go through them. When we get to the end (the base case), the
bindings will be <code>nil</code> and we can just compile the <code>body</code>.</p>

<p>We have two environments for the reason I mentioned above: when we’re
evaluating the expressions that we’re binding the names to, we can’t add
bindings iteratively. We have to evaluate them in the parent environment. It’ll
be come clearer in a moment how that works.</p>

<p>We’ll tackle the simple case first — no bindings:</p>

<div><div><pre><code><span>int</span> <span>Compile_let</span><span>(</span><span>Buffer</span> <span>*</span><span>buf</span><span>,</span> <span>ASTNode</span> <span>*</span><span>bindings</span><span>,</span> <span>ASTNode</span> <span>*</span><span>body</span><span>,</span>
                <span>word</span> <span>stack_index</span><span>,</span> <span>Env</span> <span>*</span><span>binding_env</span><span>,</span> <span>Env</span> <span>*</span><span>body_env</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>AST_is_nil</span><span>(</span><span>bindings</span><span>))</span> <span>{</span>
    <span>// Base case: no bindings. Compile the body</span>
    <span>_</span><span>(</span><span>Compile_expr</span><span>(</span><span>buf</span><span>,</span> <span>body</span><span>,</span> <span>stack_index</span><span>,</span> <span>body_env</span><span>));</span>
    <span>return</span> <span>0</span><span>;</span>
  <span>}</span>
  <span>// ...</span>
<span>}</span>
</code></pre></div></div>

<p>In that case, we compile the body using the <code>body_env</code> as the environment. This
is the environment that we will have added all of the bindings to.</p>

<p>In the case where we <em>do</em> have bindings, we can take the first one off and pull
it apart:</p>

<div><div><pre><code>  <span>// ...</span>
  <span>assert</span><span>(</span><span>AST_is_pair</span><span>(</span><span>bindings</span><span>));</span>
  <span>// Get the next binding</span>
  <span>ASTNode</span> <span>*</span><span>binding</span> <span>=</span> <span>AST_pair_car</span><span>(</span><span>bindings</span><span>);</span>
  <span>ASTNode</span> <span>*</span><span>name</span> <span>=</span> <span>AST_pair_car</span><span>(</span><span>binding</span><span>);</span>
  <span>assert</span><span>(</span><span>AST_is_symbol</span><span>(</span><span>name</span><span>));</span>
  <span>ASTNode</span> <span>*</span><span>binding_expr</span> <span>=</span> <span>AST_pair_car</span><span>(</span><span>AST_pair_cdr</span><span>(</span><span>binding</span><span>));</span>
  <span>// ...</span>
</code></pre></div></div>

<p>Once we have the <code>binding_expr</code>, we should compile it. The result will end up
in <code>rax</code>, per our internal compiler convention. We’ll then store it in the next
available stack location:</p>

<div><div><pre><code>  <span>// ...</span>
  <span>// Compile the binding expression</span>
  <span>_</span><span>(</span><span>Compile_expr</span><span>(</span><span>buf</span><span>,</span> <span>binding_expr</span><span>,</span> <span>stack_index</span><span>,</span> <span>binding_env</span><span>));</span>
  <span>Emit_store_reg_indirect</span><span>(</span><span>buf</span><span>,</span> <span>/*dst=*/</span><span>Ind</span><span>(</span><span>kRbp</span><span>,</span> <span>stack_index</span><span>),</span>
                          <span>/*src=*/</span><span>kRax</span><span>);</span>
  <span>// ...</span>
</code></pre></div></div>

<p>We’re compiling this binding expression in <code>binding_env</code>, the parent
environment, because we don’t want the previous bindings to be visible.</p>

<p>Once we’ve generated code to store it on the stack, we should register that
stack location with the binding name in the environment:</p>

<div><div><pre><code>  <span>// ...</span>
  <span>// Bind the name</span>
  <span>Env</span> <span>entry</span> <span>=</span> <span>Env_bind</span><span>(</span><span>AST_symbol_cstr</span><span>(</span><span>name</span><span>),</span> <span>stack_index</span><span>,</span> <span>body_env</span><span>);</span>
  <span>// ...</span>
</code></pre></div></div>

<p>Note that we’re binding it in the <code>body_env</code> because we want this to be
available to the body, but not the other bindings.</p>

<p>At this point we’ve done all the work required for …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://bernsteinbear.com/blog/compiling-a-lisp-7/">https://bernsteinbear.com/blog/compiling-a-lisp-7/</a></em></p>]]>
            </description>
            <link>https://bernsteinbear.com/blog/compiling-a-lisp-7/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24652842</guid>
            <pubDate>Thu, 01 Oct 2020 16:26:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Data Science Pull Requests– Review and merge code, data and experiments]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24652832">thread link</a>) | @Dean-DAGsHub
<br/>
October 1, 2020 | https://dagshub.com/blog/data-science-pull-requests/ | <a href="https://web.archive.org/web/*/https://dagshub.com/blog/data-science-pull-requests/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
              
              <h3 id="a-step-forward-for-mlops-and-unlocking-open-source-data-science">A step forward for MLOps and unlocking Open Source Data Science</h3><p>Today, we're releasing Data Science Pull Requests (DS PRs), which are Pull Requests (PRs), re-imagined for the data science (DS) workflow. This new capability unlocks a standard review process for data science teams, enabling them to merge data across different branches and accept data contributions across forks. This provides a better collaborative experience for teams in data science organizations and enables truly Open Source Data Science (OSDS) projects. </p><p>For more details, read on...</p><h2 id="introduction">Introduction</h2><p>When we started DAGsHub, we were focused on making data science collaboration possible. Specifically, we deeply <em>care</em> and <em>rely on</em> Open Source Software (OSS), and we set out on a mission to make OSDS as accessible and prevalent as OSS is today.</p><p>This meant that we were concerned about <strong><em>discoverability </em></strong>of data science projects and experiments to work on, <strong><em>understandability</em></strong> of the context of an experiment, <strong><em>reproducibility</em> of </strong>its results, and finally, <strong><em>contributability</em></strong> of code-, data- and models- changed back to the original project.</p><p>When reviewing these processes and the existing solutions some things become clear:</p><ul><li><em><strong>Discoverability </strong></em>means being able to answer the question "<em>What should I do next?</em>" – finding a project to work on, and within that project finding what experiments might be interesting or important. <br>It is solved mainly by <strong>experiment tracking</strong> systems, many of them using proprietary or black box formats that are hard to understand and migrate to/from.<p>DAGsHub goes beyond this by creating an experiment tracking system that relies on simple open formats (<code>YAML</code> and <code>CSV</code>). This means you don't need to add obscure lines of code – everything works by automatically scanning and analyzing the git commits pushed into the platform.</p></li><li><em><strong>Understandability</strong></em> means being able to answer the question "<em>How should I do what I want to do?</em>" – this usually consists of reviewing why, how, and what was already done in a project or experiment. The solution for this step is mostly manual and relies on self-documenting one's work and discussions with collaborators.<p>DAGsHub improves on this by providing a convenient interface into projects' code, data, models, and pipelines which give users a window into their projects' components, and how they interact with each other.</p></li><li><em><strong>Reproducibility</strong></em> means setting up an exact copy of the experiment you want to work on. Many times this process is reduced to a Git commit and the experiment parameters (logged in the experiment tracking system). However, the true standard for reproducibility involves <em><strong>easily </strong></em>retrieving the same version of data, models, and other artifacts. It is best solved by using Git with some dedicated data versioning solution.<p>DAGsHub solves this by relying on open source tools such as Git and DVC to provide the standard discussed above – a complete copy of your project (code, data, models, parameters, and other artifacts) with one (or two) commands.</p></li><li><em><strong>Contributability</strong></em> means that you can take a new experiment or result, and incorporate them back into the project you started from so that you don't need to maintain your result separately. Today, this is entirely manual, full of friction, and fundamentally <strong>non-existent</strong>.</li></ul><p>We have many more things to build, but it was clear that one aspect needed to be covered first – a <strong><em>CONTRIBUTION </em></strong>mechanism.</p><h2 id="contributing-data-science-pull-requests">Contributing – Data Science Pull Requests</h2><p>The final step of the collaborative process is arguably the most important one. Without it, the workflow is one-sided, a monologue, which means collaboration isn't happening. Practically, <strong><em>Contributing</em></strong> can be broken down into two tasks - <strong>reviewing</strong> and <strong>merging </strong>contributions.</p><p>In software, both reviewing and merging are a part of the <strong>pull request</strong> process, but their focus was solely on code. </p><blockquote>Data Science Pull Requests let you <strong>review experiments<u>,</u></strong> <strong>code, data, models, </strong>and your<strong> pipelines</strong>, and <strong><u>merge changes to all of them automatically.</u></strong> </blockquote><h3 id="data-science-review">Data Science Review</h3><p>If you've ever worked on a data science project with other people or tried reviewing someone else's data science work, you know how hard it is to get the information you need to understand someone else's work, or explain your own, so that the review process is meaningful. The process is slow and manual because systems are not built for review.</p><p>An automatic review process means changes and updates can be discussed and integrated faster into your project.<strong> You need to quickly see what has changed, discuss it, in context, and decide how to move forward.</strong></p><p>What this means in practice:</p><ul><li>Commenting on experiments, in context – you can look at the new experiments that are being contributed as part of the DS PR, and compare them to the base experiment in the original project. See all the visualization and information, and add comments on these within the PR discussion with links to the relevant comparison/visualization.</li></ul><figure><img src="https://dagshub.com/blog/content/images/2020/09/exp-comment-long-hq.gif" alt="Commenting on experiments"><figcaption>Commenting on experiments in DS PRs</figcaption></figure><ul><li>See what data and models have changed (not just code) – view what data, model, and artifact files were added, removed, or modified. This means you can easily pinpoint changes and focus the discussion on what's important.</li></ul><figure><img src="https://dagshub.com/blog/content/images/2020/09/Screen-Shot-2020-09-25-at-18.51.25.png" alt="Viewing data changes in a DAGsHub project" srcset="https://dagshub.com/blog/content/images/size/w600/2020/09/Screen-Shot-2020-09-25-at-18.51.25.png 600w, https://dagshub.com/blog/content/images/size/w1000/2020/09/Screen-Shot-2020-09-25-at-18.51.25.png 1000w, https://dagshub.com/blog/content/images/2020/09/Screen-Shot-2020-09-25-at-18.51.25.png 1189w" sizes="(min-width: 720px) 720px"><figcaption>Data Comparison Example</figcaption></figure><ul><li>Compare and diff notebooks side-by-side – notebooks are an important part of many data science projects. However, for a very long time, they haven't received adequate treatment in the review process, relying on diffs to the raw <code>JSON</code> file, which were mostly unreadable. You can now review the changes in an intuitive UI as part of the DS PR. Another benefit of this is that if you require a special visualization, you can commit a notebook with that visualization, and view the changes conveniently.</li></ul><figure><img src="https://dagshub.com/blog/content/images/2020/09/Screen-Shot-2020-09-25-at-19.04.33.png" alt="Notebook comparison" srcset="https://dagshub.com/blog/content/images/size/w600/2020/09/Screen-Shot-2020-09-25-at-19.04.33.png 600w, https://dagshub.com/blog/content/images/size/w1000/2020/09/Screen-Shot-2020-09-25-at-19.04.33.png 1000w, https://dagshub.com/blog/content/images/size/w1600/2020/09/Screen-Shot-2020-09-25-at-19.04.33.png 1600w, https://dagshub.com/blog/content/images/2020/09/Screen-Shot-2020-09-25-at-19.04.33.png 2298w" sizes="(min-width: 720px) 720px"><figcaption>Notebook Diffing Example</figcaption></figure><p>After reviewing a collaborator's work, we need a way to incorporate those changes, automatically. That's why we built data science merging.</p><h3 id="data-science-merging">Data Science Merging</h3><p>Merging code is possible with Git, but as we already discussed, that is not the full picture for data science projects. With DS PRs, you can merge your data and other artifacts as well. </p><h4 id="data-merging">Data Merging</h4><p>Everyone knows about bugs in code, but you might also have data bugs that you're not aware of. Examples include data that is not up to date, biased, or mislabeled. Assuming you found out about such a bug and you wanted to fix it – that would usually mean you need to agree on and perform some manual operation to update or add new data. With data merging, once you accept a DS PR, the new data would automatically be copied into your project in an entirely automatic process.</p><h4 id="artifact-merging">Artifact Merging</h4><p>This doesn't end with just the <em>raw data – </em>data merging lets you merge models and any other artifact of your data pipeline (e.g. preprocessed data or 3d models). Take a case where one of the steps in a pipeline takes 2 weeks to run and results in some trained model or a processed dataset. If only raw data was merged, you'd have to run that excruciating 2-week process again. Artifact merging means that after a DS PR is merged, the resulting project is as reproducible as the original contribution.</p><p>After accepting a DS PR you are in the same state of your DS project, as you would after accepting a PR in a software project.</p><figure><img src="https://dagshub.com/blog/content/images/2020/09/Screen-Shot-2020-09-25-at-19.23.30.png" alt="Data Merging on DAGsHub" srcset="https://dagshub.com/blog/content/images/size/w600/2020/09/Screen-Shot-2020-09-25-at-19.23.30.png 600w, https://dagshub.com/blog/content/images/size/w1000/2020/09/Screen-Shot-2020-09-25-at-19.23.30.png 1000w, https://dagshub.com/blog/content/images/size/w1600/2020/09/Screen-Shot-2020-09-25-at-19.23.30.png 1600w, https://dagshub.com/blog/content/images/2020/09/Screen-Shot-2020-09-25-at-19.23.30.png 1846w" sizes="(min-width: 720px) 720px"><figcaption>Data Science Merging – Note that 171 MB of data will be copied on accepting this DS PR</figcaption></figure><p>Data merging means you can accept data and models from contributors with ease, without giving each one full access to your data storage. This can reduce friction and speed up team efforts.</p><p>This last capability is especially useful for OSDS.</p><h2 id="what-does-this-mean-for-osds">What does this mean for OSDS?</h2><p><a href="https://dagshub.com/blog/a-case-for-open-source-data-science/">Open Source Data Science (OSDS) has the potential to have a similar effect on the world</a>, as Open Source Software (OSS) had. It is DAGsHub's stated goal to promote OSDS and build the technology to make it as easy as possible. OSDS must come first, and industry workflows will mirror those in OSDS projects, as they have for OSS. </p><p>But let's face it – OSDS doesn't <em>really</em> exist yet. If you maintain some OSDS project and you want to accept contributions from people (like you would for OSS) – you have to do it entirely manually or <strong>resort to accepting only code changes</strong> (no way to accept data bug fixes – and we all know there are plenty).</p><p>From the individual contributor side, if you want to improve your ML portfolio by contributing to some OSDS project, you're also stuck. You have to either fork the project and not contribute your changes (which means their quality is never reviewed – you don't learn as much) or go through a painstaking manual effort<sup>[1]</sup>.</p><blockquote>DS PRs make OSDS possible by providing a standard interface and workflow to review and accept contributions from anyone, anywhere, and for any type of data science component.</blockquote><p><strong>We'd love to support open source data science projects that want to accept data science contributions from the community. Please reach out to us at <a href="mailto:osds@dagshub.com">osds@dagshub.com</a> if this is relevant for you.</strong></p><h2 id="thank-you-">Thank You!</h2><p>Thank you to all the people that gave us feedback before and while we were building DS PRs. We'd love to get your feedback as well on how DS PRs could be improved for the community – the best way to do this is to join our <a href="https://discord.com/invite/9gU36Y6">Discord channel</a>. Looking forward to hearing your thoughts and seeing what people build with open source data science.</p><hr><!--kg-card-begin: markdown--><p>
[1] Kaggle is worth a mention here – It is a common way to show some of your DS chops. However, it's competitive (as opposed to collaborative). Furthermore, data science projects in the wild rarely have one all-encompassing metric to optimize at the expense of everything else - 80% of the work is just gathering data and deciding what is even worth optimizing! 
Our goal with DAGsHub is to enable a collaborative way to showcase your capabilities while encouraging interoperability – i.e. working together rather than everyone doing their own thing and ending up with a ton of fragmentation.
</p><!--kg-card-end: markdown-->
                <section>
                  
                  <ul>
                      <li>
                        <a href="https://dagshub.com/blog/tag/data-science/" title="Data Science">Data Science</a>
                      </li>
                      <li>
                        <a href="https://dagshub.com/blog/tag/data-science-workflow/" title="Data Science Workflow">Data Science Workflow</a>
                      </li>
                      <li>
               …</li></ul></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dagshub.com/blog/data-science-pull-requests/">https://dagshub.com/blog/data-science-pull-requests/</a></em></p>]]>
            </description>
            <link>https://dagshub.com/blog/data-science-pull-requests/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24652832</guid>
            <pubDate>Thu, 01 Oct 2020 16:26:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on QA]]>
            </title>
            <description>
<![CDATA[
Score 21 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24651571">thread link</a>) | @tigranhakobian
<br/>
October 1, 2020 | https://blog.superannotate.com/how-to-detect-mislabeled-annotations | <a href="https://web.archive.org/web/*/https://blog.superannotate.com/how-to-detect-mislabeled-annotations">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p><em>Manual QA is a significant part of the annotation pipeline. Annotation companies report that 40 percent of the annotation time can be spent on manual QA. As a result, finding ways to reduce QA testing time can have a significant impact on annotation costs.&nbsp;</em></p>
<!--more--><p><em>At SuperAnnotate, we’ve developed a tool to accelerate the QA process. This article discusses SuperAnnotate’s features that speed up the quality assurance process substantially. It presents several automation tools within the platform listing specific use cases in which a major acceleration of the QA process can be obtained. We also explored various ML algorithms that can detect over 90 percent of mislabeled instances in data while accelerating the QA process up to 4 times.&nbsp;</em></p>
<h3><strong><span>Outline</span></strong></h3>
<ul>
<li><em>Problem with noisy annotation in data&nbsp;</em></li>
<li><em>Manual QA acceleration</em></li>
<li><em>QA automation</em></li>
<li><em>Conclusion</em></li>
</ul>
<h2><span>Problem with annotation noise in data</span></h2>
<p><strong><span>1.1. The importance of model accuracy and the impact of annotation noise&nbsp;&nbsp;</span></strong></p>
<p>In real-world applications, the performance of machine learning (ML) systems is of crucial importance. ML models heavily rely on the quality of annotated data, but obtaining high-quality annotations is costly and requires extensive manual labor.&nbsp;</p>
<p><span>In any annotation pipeline, regardless of the data collection method, i.e., human or machine, several factors inject annotation noise in data. As a result, even the most celebrated datasets contain mislabeled annotations.</span></p>

<p><img src="https://lh3.googleusercontent.com/mL1r0JU9VM5-WflEsLU04zQ7vhYhl8cKRux8LvOnUTYEKZvnIUD9Gv8HmkVTqPfgoDtEkb_5ApX3SjgJjdTAHNfG6I-5Q7_fag_cra8IhXTWp-uVdrvQCf7kzoqt03BwtgKlTb4i" width="720" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"><em>Figure: Ambiguous or Mislabeled annotations from ImageNet. Source (</em><a href="https://arxiv.org/abs/2001.10528"><em><span>Pleiss et al</span></em></a><em>.)</em></p>

<p><span>Recent studies show that both natural and malicious corruptions of annotations tend to radically degrade the performance of machine learning systems. Deep Neural Networks (DNNs) tend to memorize noisy labels in data, resulting in less generalizable features and poor model performance (<a href="https://arxiv.org/abs/1611.03530">Zhang et al.</a>)</span></p>
<p><span>Therefore, extensive quality control of annotated data is required to clean annotation noise and improve model performance.</span><em><br></em></p>

<p><img src="https://lh4.googleusercontent.com/7BKHH4Am8Z_PZebzMe7mkbGCA6J_UyNRZE-ClAMwP5qVo52ZEuybUx81EOWYSdKrz2Mz7P4zf2cHuoz9nlyl4Alg-D16cD58mSCLf1CAOUwwDEp2MUkIsaTi37D6y9aliNShqdUz" width="720" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p><em>Figure: Accuracy drop in multiple datasets when injecting label noise. Source (</em><a href="https://arxiv.org/abs/1611.03530"><em><span>Rolnick et al.</span></em></a><em>)</em></p>

<h2>Manual QA Acceleration</h2>
<p><strong><span>2.1 SuperAnnotate’s QA pipeline&nbsp;</span></strong></p>
<p><span>Quality Assurance of annotated data is time-consuming and requires particular attention. Annotation tools need to provide reliable and scalable QA pipelines to accelerate the QA process. <a href="https://annotate.online/login">SuperAnnotate</a> provides interlinked annotation and QA processes within the same platform. As a result, QA systems do not require additional management.</span></p>
<p><span>The design of SuperAnnotate’s QA system guarantees an efficient process and ensures a minimal probability of error.&nbsp;</span></p>
<p><strong><span>2.2 Pinning images to reduce common errors&nbsp;</span></strong></p>
<p><span>Sharing repetitive labeling mistakes across the annotation team is essential to reduce systematic errors throughout single or multiple annotation projects. </span>SuperAnnotate’s pin functionality is designed specifically for this cause.&nbsp;</p>
<p>Once the reviewer notices a recurring error, they can share this information through pinned annotations instead of extensive project instructions. Pinned annotations will appear first in the annotation editor to immediately grab the annotation team’s attention.&nbsp;</p>
<p>This functionality is highly efficient since it allows the project coordinator to instantly share common instructions, eliminating the spread of systematic errors.&nbsp;</p>

<p><img src="https://lh6.googleusercontent.com/g1sh6D7Xy1hLYBMczMWfFGdO1_1gktJn7dNF1Spx3lwUxeZd8isGaMHgYLB1GoT0lY8jJKTemdqvtExgPJIocgjNEFuzYGjbPQYeRo31873mdN3U4U10DMjW7DZOnr1eAh5_o-bc" width="720" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p><em>Figure: Pin functionality</em></p>

<p><strong><span>2.3 Approve/Disapprove functionality</span></strong></p>
<p>Apart from various image-level QA tools, SuperAnnotate also provides instance-level QA functionalities. The latter is designed to help the QA focus on a specific instance area. As a result, no error is overlooked, at the time a meticulous QA is time-efficient.</p>
<p>Additionally, the Approve/Disapprove functionality works on the level of individual instances<span>.</span> If a QA specialist disapproves of an annotation, they can send it back to the Annotator for correction. The QA specialist can send the annotation back to the Annotator as many times as needed until the annotation is corrected. Once approved, the annotations can be exported from the platform.</p>

<p><img src="https://lh4.googleusercontent.com/BpQPXRUuTMRYUP-YGJTSRJwFUtdLr6wsB5LVwn66wK6TqFJCTRW5LLwhTp4sRttqgzQaCngKzm7Z6ONZBpPhwzxifj1KlBRs5_FOl_Nk0cFpaC_jn7-l9CMn1WJX2FENx9f9NI24" width="720" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p>Figure: Approve/Disapprove functionality</p>

<p><span>The QA mode is another useful tool for manual instance inspection. When enabled, the annotations are visually isolated from the background. This allows the user to distinguish between instances and the underlying objects, making the instance inspection easier.</span></p>
<p><span>All listed features extensively accelerate the manual QA process. However, machine learning techniques that automatically detect annotation noise can provide an additional level of automation.</span></p>

<h2><span>QA Automation</span></h2>
<p><strong><span>3.1 ML for QA Automation</span></strong></p>
<p><span>QA of annotated data takes around 40 percent of the annotation time.</span></p>
<p><span>On average, only a small fraction of annotated data contains noise. Still, QA is applied to the entire dataset, and monitoring clean data costs the annotators extra time and resources. An automation method could substantially cut down the QA process of clean data by isolating a set of risky annotations. So, our goal is to determine ML techniques that identify noisy annotations in data with high precision and recall.</span></p>
<p><strong><span>3.2 Current research</span></strong></p>
<p><span>Learning on datasets that contain annotation noise has been an active research area in ML. Several methods use predictions from DNNs to detect label noise and modify training loss (Reed et al., 2015; Tanaka et al., 2018). These methods do not perform well under high noise ratio as the domination of noisy annotations in data causes overfitting of DNNs. Another approach is to treat small loss samples as clean annotations and allow only clean samples to contribute to the training loss (Jiang et al., 2017). Ultimately, this research area’s core challenge is to design a reliable criterion capable of identifying the annotation noise.</span></p>
<p><span>A significant amount of research in this area is focused on classification with noisy labels. The proposed methods range from detecting and correcting noisy samples to using noise-robust loss functions. Unsupervised and semi-supervised learning techniques are also relevant to this task since those require few or no labels. Mislabeled samples that are detected without label correction can be used as unlabeled data in a semi-supervised setting (Li et al. 2020).&nbsp;</span></p>
<p><span>Going beyond classification makes things far more challenging. In classification, the existence of an object per image is guaranteed. So, the noisiness criterion can be defined between predicted and annotated image labels. However, in more complex tasks such as object detection, the correspondence between predicted and annotated instances is less trivial. Even though research in this area is in its initial state, several methods suggest valid measures to indicate both localization and label noise in object detection (Pleiss et al 2020, Chadwick et al. 2019).</span></p>
<p><strong><span>3.3&nbsp; Proposed method</span></strong></p>
<p>Consider the task of object detection on a dataset that contains mislabeled annotations. Several techniques use DNN predictions to identify label noise in data. Based on this concept, we propose the following algorithm.</p>
<ul>
<li>For each bounding box annotation, we obtain the matching prediction that has the maximum IOU.&nbsp;</li>
<li><span>Compute <strong>L2 distance</strong> between one hot vector of an annotated class and Fast RCNN softmax logits of the matched prediction. </span>This distance serves as a mislabel metric for annotations. We treat this number as the probability of annotation being mislabeled. As we aim to achieve maximal recall and precision in mislabel detection, we select an optimal threshold to attain the desired objective. This defines the split of data between clean and mislabeled annotations by the given criterion.</li>
</ul>

<p><img src="https://lh6.googleusercontent.com/JJtyM51584r8uJCIbHuW0UKhdgxQIIMm4G23vuD8lxnn5yrYHONPRytIxl-9QjFodM51zy7d8pvswhiMYokXY8XOz-DxqTd4ua-_DHGemiuXULNKqCbq_ePQfzulkbsrNu_jyX0D" width="720" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p><em>Figure: L2 distance on gt one hot and prediction logits</em></p>

<p>Along with mislabeled detections, we also suggest considering the most confident predictions as missing annotations.&nbsp;</p>
<p><strong><span>3.4 Experiments and results</span></strong></p>
<p>To evaluate the performance of our method, we used PASCAL VOC as a toy dataset. When we manually injected asymmetric label noise in 20 percent of bbox annotations, the described mislabel criterion resulted in the precision-recall curve shown below.</p>
<p><img src="https://lh5.googleusercontent.com/OhgoPNrwmPWqR21Y1jnMY-1rbgKV8cxdFd8F0lZsVCZTcrN9C77EBX-0yqvZfVbYOVAFH_pxHNh88T7r26Gbp6hxWWdF5XqyLkN8SS5G46q512ZOHILlP1wmeR20aCo2QDWsOMms" width="400" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p><em>Figure: PR curve with optimal mismatch threshold selected</em></p>

<p>Here, recall determines the portion of annotation noise captured, and precision identifies the fraction of data validated as clean.</p>
<p><span>Based on the PR curve above, using optimal mislabel threshold results in over 93 percent recall. This proves the reliability of our method, as we capture the dominant fraction of annotation noise. Along with high recall, we obtained over 75 percent precision, which attributes to validating ¾ of annotations as clean. This cuts manual inspection time over the whole dataset by a margin of 4, resulting in extensive automation for the manual QA.&nbsp;</span></p>
<p>Note that detected risky annotations contain clean samples apart from correctly detected mislabeled instances. Those are the images that were hard to capture by the detection model and thus are misclassified as risky. Distinguishing between these two categories is a challenging problem for further research.&nbsp;</p>
<p>You can find the source code for the discussed experiments at our <a href="https://github.com/superannotateai/qa-automation"><span>GitHub repository</span></a>.</p>
<p>Also, consider our <a href="https://colab.research.google.com/drive/1Xbt3dxkmX4ozQhdY_vnHXAH67OUeg0Nj#scrollTo=7unkuuiqLdqd&amp;uniqifier=2"><span>Colab tutorial</span></a> as a step-by-step guide to reproduce the given results.&nbsp;</p>
<p><strong><span>3.5 Automate Approve/Disapprove functionality&nbsp;</span></strong></p>
<p>Once mislabeled annotations are captured by ML techniques, SuperAnnotate allows users to import the detected information to the platform through the <strong>error </strong>key of SA formatted annotations. Just set the <strong>error</strong> key in mislabeled annotations and import SA formatted JSONs to SuperAnnotate via Python SDK.&nbsp;</p>

<p><img src="https://lh3.googleusercontent.com/to5CwRUFKERkSeyPgA1Nzl84I0ijhR8bQCpsUHhIxi_zhsm7FnNGOxSJ-i0V8HXnJSzN4VOxDnVgj_JWo2QCwQF6ECSjO4CrLShBB9V3YPHp8SWxO9D2CEwC6UbfqClElWgxIOf3" width="654" height="183" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p><em>Figure: JSON Code Block</em></p>

<p><img src="https://lh3.googleusercontent.com/0SJ8E5OnOSxB50Fh_4d4_qjJNT0lygY1yHzFGuFlZ9fMDjIGyUGQDqxGsDDG9j2dMcfmf6PLOHvj3JRJ4F3UtaQup-muynR2TFd18W6vbjI1ANF8Ll99dK_b8v1GIzkBUbpwVaPn" width="654" height="121" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p><em>Figure: SDK Code Block</em></p>

<p><span>Discussed pipeline provides complete automation of Approve/Disapprove functional.</span></p>

<p><img src="https://lh5.googleusercontent.com/w9VXvs6INaP4WaEADW2pW6fEVj6s8sj5FZd44gN-10LcKplspRX3N7tt-aRuV1BUmNFzANwQTjKgq9rt-EbND0_SpY2z5Ikcpc3xo6wZHQMm2-TB3iY6ao5LboT86FK8qaw8ZwPu" width="720" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p><em>Figure: Before v.s After autoqa in SA platform&nbsp;</em></p>

<h3><span>Conclusion</span></h3>
<p><span>QA automation is of crucial importance as it constitutes a significant portion of annotation time. This article shows that using proper algorithms and associated tools can help us detect mislabeled annotations with high precision while spending 4x less time on QA.</span></p></span>
</p>


</div></div>]]>
            </description>
            <link>https://blog.superannotate.com/how-to-detect-mislabeled-annotations</link>
            <guid isPermaLink="false">hacker-news-small-sites-24651571</guid>
            <pubDate>Thu, 01 Oct 2020 14:57:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Memgraph DB 1.1]]>
            </title>
            <description>
<![CDATA[
Score 33 | Comments 10 (<a href="https://news.ycombinator.com/item?id=24651091">thread link</a>) | @karimtr
<br/>
October 1, 2020 | https://memgraph.com/blog/memgraph-1-1-benchmarks | <a href="https://web.archive.org/web/*/https://memgraph.com/blog/memgraph-1-1-benchmarks">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2>Introduction</h2>
<p>At Memgraph, we put great effort into delivering a high-performance in-memory graph storage and analytics engine. We do this by investing a lot of time optimizing and continuously improving various aspects of the Memgraph core engine. In this blog post, we will explore some of the improvements we have made on the storage layer and their impact on performance and memory usage.</p>
<p>The two most significant improvements we introduced in recent years are a new storage engine and a new way of storing properties on both nodes and edges.  Prior to version v0.50.0 Memgraph had an <a href="https://en.wikipedia.org/wiki/Multiversion_concurrency_control">MVCC</a> storage where copies of nodes and edges were used to version the data. <strong>Memgraph v0.50.0</strong> introduced a new way of managing graph data where each node or edge consists of the latest version of data, and associated changes of data required to reconstruct previous versions.</p>
<p><img src="https://i.imgur.com/bQbvvp6.png" alt=""></p>
<p><strong>Memgraph v1.1.0</strong> introduced a new way of storing properties. Each node or edge has a property store that takes at least 16B of memory. Memgraph tries to hold properties data in 16B on the stack, if possible. If the properties data exceeds 16B, the first 8B of the stack buffer indicates the total number of bytes required for the storage of properties, and the second 8B a pointer to the array on the heap that stores properties. This technique is called small buffer optimization.</p>
<p><img src="https://i.imgur.com/LWP2te8.png" alt=""></p>
<p>Additionally, there were various other smaller improvements of existing internal data structures, most notably the <a href="https://en.wikipedia.org/wiki/Skip_list">skip list</a> which is used as an indexing data structure. The combined result was a massive improvement in memory usage with a substantial reduction in memory fragmentation while ensuring equivalent or better performance. Before jumping into the analysis and explanations, let’s have a look at the benchmark setup.</p>
<h2>Setup</h2>
<h3>Target Systems</h3>
<p>The benchmark tests different Memgraph versions. The release dates and details of each version are the following:</p>
<ul>
<li><strong>v0.15.2</strong>, October 23, 2019, the last version of Memgraph that used an in-memory storage engine based on copies of nodes/edges.</li>
<li><strong>v0.50.0</strong>, December 11, 2019, introduced the new in-memory storage engine based on data changes and a C-based storage API.</li>
<li><strong>v1.0.0</strong>, April 6, 2020, introduced a Python-based storage API.</li>
<li><strong>v1.1.0</strong>, July 1, 2020, added encoding and compression to node and edge properties.</li>
</ul>
<p>The <a href="https://docs.memgraph.com/memgraph/changelog">Memgraph Changelog Docs</a> page contains more details about changes in each version.</p>
<h3>Hardware</h3>
<ul>
<li>Server: HP DL360 G6</li>
<li>CPU: 2x Intel Xeon X5650 6C12T @ 2.67GHz</li>
<li>RAM: 144GB</li>
<li>Disk: 120GB SSD</li>
<li>OS: Debian 9 Stretch</li>
</ul>
<h3>Workload</h3>
<p>The benchmark consists of several different queries that test the performance of the graph database. Typical graph database workloads consist of READ, CREATE, and ANALYZE queries.</p>
<p>Memgraph is particularly well suited for hybrid transactional-analytical workloads where it’s crucial to ingest data as fast as possible and simultaneously deliver analytics as quickly as possible. For this reason, we are mostly interested in traversal queries (1-Hop, 2-Hop, etc.) which represent the majority of analytical queries.</p>
<p>In the following table, you can find the exact queries we have used for benchmarking.</p>
<pre><code>|            Query Name             |                                                Query                                                        |  Query Type |
| --------------------------------- | ----------------------------------------------------------------------------------------------------------- | ----------- |
| Aggregation                       | `MATCH (n:User) RETURN n.age, COUNT(*);`                                                                    | ANALYZE     |
| 1-Hop Expand                      | `MATCH (s:User {id: $id})--&gt;(n:User) RETURN n.id;`                                                          | ANALYZE     |
| 2-Hop Expand                      | `MATCH (s:User {id: $id})--&gt;()--&gt;(n:User) RETURN DISTINCT n.id;`                                            | ANALYZE     |
| 3-Hop Expand                      | `MATCH (s:User {id: $id})--&gt;()--&gt;()--&gt;(n:User) RETURN DISTINCT n.id;`                                       | ANALYZE     |
| 4-Hop Expand                      | `MATCH (s:User {id: $id})--&gt;()--&gt;()--&gt;()--&gt;(n:User) RETURN DISTINCT n.id;`                                  | ANALYZE     |
| 2-Hop Variable Expand             | `MATCH (s:User {id: $id})-[*1..2]-&gt;(n:User) RETURN DISTINCT n.id;`                                          | ANALYZE     |
| 2-Hop Variable Expand with Result | `MATCH (s:User {id: $id})-[*1..2]-&gt;(n:User) RETURN DISTINCT n.id, n;`                                       | ANALYZE     |
| Shortest Path                     | `MATCH p=(n:User {id: $from})-[*bfs..15]-&gt;(m:User {id: $to}) RETURN extract(n in nodes(p) | n.id) AS path;` | ANALYZE     |
| Insert New Relationship           | `MATCH (n:User {id: $from}), (m:User {id: $to}) WITH n, m CREATE (n)-[e:Temp]-&gt;(m) RETURN e;`               | CREATE      |
| Find Node                         | `MATCH (n:User {id : $id}) RETURN n;`                                                                       | READ        |
| Insert a New Node                 | `CREATE (n:UserTemp {id : $id}) RETURN n;`                                                                  | CREATE      |
</code></pre>
<p>The benchmarking harness executed all queries against the <a href="https://snap.stanford.edu/data/soc-Pokec.html">Pokec dataset</a> which contains 1.6M nodes and 30.6M edges. Given that the goal of each benchmark is to saturate the target system to extract its peak characteristics, we took great care in carefully designing and polishing our setup. Some of the essential elements of the harness are:</p>
<ul>
<li>optimized C/C++ client to minimize client overhead.</li>
<li>fresh dataset load before each execution.</li>
<li>concurrent execution (up to 12 cores) to push Memgraph to its limits.</li>
</ul>
<h2>Data Import Analysis</h2>
<p>Before delving deep into the workload queries execution analysis, a couple of notes about the data import.</p>
<p>The harness imported data in a real-time manner, equivalent to normal query execution by running queries. The data was imported using 8 concurrent clients.  Throughput and peak memory usage were measured during the data import. A couple of interesting insights emerged. On the memory usage side, there is already a massive difference between Memgraph versions. <strong>Before v0.50.0</strong>, Memgraph stored all data modifications as whole copies of the modified objects. By removing the need to make whole copies of database objects, versions <strong>after v0.50.0</strong> provide a significantly less memory usage. The same benefits apply to the runtime environment since the import uses regular queries.</p>
<p><img src="https://i.imgur.com/VfDzuCw.png" alt=""></p>
<p>At this point, you might be wondering about the import speed. As the chart below shows, throughput on basic CREATE queries also improved. Since data copying is generally a fast operation, throughput improvement is not huge but is still significant. One important thing to notice is the difference between v1.0.0 and v1.1.0. <strong>v1.1.0</strong> has almost the same throughput as versions before even though more work is involved in property compression.</p>
<p><img src="https://i.imgur.com/hsh6uDj.png" alt=""></p>
<h2>Query Execution Analysis</h2>
<p>Let’s start analyzing the <strong>workload queries</strong>. The following radar chart shows peak memory usage during query execution across different Memgraph versions. Keep in mind that less is better.</p>
<p>As you can see, <strong>v1.1.0 uses ~50%</strong> less memory compared to v0.15.2. v0.50.0 and v1.0.0 fall in-between with almost no difference because not much from the storage perspective changed between these two versions. The most significant difference is between v0.15.2 and v0.50.0 (introduction of the new storage engine), and v1.0.0 and v1.1.0 (introduction of encoded and compressed properties).</p>
<p><img src="https://i.imgur.com/4ETpDXT.png" alt=""></p>
<p>On the other hand, while looking at memory, it’s also critical to observe what is happening with the throughput. Generally, there is a well-known trade-off between space and time. But, as you can see in the following chart, v1.1.0 has the best performance. Please note that in this case, more is better.</p>
<p><img src="https://i.imgur.com/c0nBqJS.png" alt=""></p>
<p>Of course, not all queries yield significant throughput improvements. E.g., the <code>Insert New Node</code> query performance stayed similar across different Memgraph versions. Nonetheless, the following chart illustrates well how Memgraph scales with the number of concurrent requests.</p>
<p><img src="https://i.imgur.com/vIy7rTM.png" alt=""></p>
<p>The new storage engine also introduced a feature where it is possible to disable storing properties on edges. Sometimes graph datasets don’t have any data attached to edges. Memgraph offers a configuration option to remove all associated data structures required to store data on edges. As you can see in the following chart, by not having properties on edges, memory usage goes down by almost 50% (in addition to the reductions mentioned above). v0.15.2 can’t disable the property storage on edges, so the chart shows nothing there. Using all the improvements in new versions of Memgraph you can store the same dataset in 3.36x less memory (comparing v0.15.2 with properties enabled and v1.1.0 with properties disabled).</p>
<p><img src="https://i.imgur.com/PtQviBf.png" alt=""></p>
<p>The rest of the charts show performance for each query with regards to linear scalability. Linear scalability is the ability of a system to handle more work by adding more resources linearly. E.g., by doubling the number of cores, the system is capable of executing twice as many queries. In practice, it’s impossible to reach perfect linear scalability due to various overheads.  The real question is how far Memgraph is from linear scalability? As you can see below, not by a lot.</p>
<p>The following chart shows throughput data for the simple read query (Find Node). The node lookup inside Memgraph has an O(logN) complexity.</p>
<p><img src="https://i.imgur.com/0z46Rg5.png" alt=""></p>
<p>In the next case, instead of reading, Memgraph creates an edge. The operation contains two node lookups and one edge write. Which means it’s very similar to the Find Node query. Instead of one lookup, the action has two lookups and one write. The chart looks very similar to the Find Node chart.</p>
<p><img src="https://i.imgur.com/ZWzeFc6.png" alt=""></p>
<p>By moving towards more complex queries, there is a more significant scalability difference between previous versions of Memgraph and the latest ones. v0.15.2 is far behind the latest versions.</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://memgraph.com/blog/memgraph-1-1-benchmarks">https://memgraph.com/blog/memgraph-1-1-benchmarks</a></em></p>]]>
            </description>
            <link>https://memgraph.com/blog/memgraph-1-1-benchmarks</link>
            <guid isPermaLink="false">hacker-news-small-sites-24651091</guid>
            <pubDate>Thu, 01 Oct 2020 14:19:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ontario police used Covid-19 database illegally, civil rights groups find]]>
            </title>
            <description>
<![CDATA[
Score 252 | Comments 73 (<a href="https://news.ycombinator.com/item?id=24650515">thread link</a>) | @seigando
<br/>
October 1, 2020 | https://www.cbc.ca/news/canada/toronto/covid-police-database-1.5745481 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/canada/toronto/covid-police-database-1.5745481">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Police forces across&nbsp;Ontario&nbsp;engaged in broad, illegal searches&nbsp;of a now-defunct COVID-19 database, two civil rights groups alleged Wednesday, claiming the use of the portal violated individual privacy rights for months.</p><div><figure><div><p><img alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5704129.1598642644!/cumulusImage/httpImage/image.jpg_gen/derivatives/16x9_780/shutterstock-medium-file-typing-on-laptop.jpg"></p></div><figcaption>The Canadian Civil Liberties Association and the Canadian&nbsp;Constitution Foundation say in separate reports that many services used the database to look at COVID-19 test results for wide geographic areas and sometimes pulled up personal information unrelated to active calls.<!-- --> <!-- -->(maradon 333 / Shutterstock)</figcaption></figure><p><span><p>Police forces across&nbsp;Ontario&nbsp;engaged in broad, illegal searches&nbsp;of a now-defunct COVID-19 database, two civil rights groups alleged Wednesday, claiming the use of the portal violated individual privacy rights for months.</p>  <p>The Canadian Civil Liberties Association (CCLA) and the Canadian&nbsp;Constitution Foundation (CCF) said in separate reports that many services used the database to look at COVID-19 test results for wide geographic areas and sometimes pulled up personal information unrelated to active calls.</p>  <p>"People weren't told that when they went for COVID tests that&nbsp;this information was being shared with police and they certainly weren't asked for their consent," said Abby Deshman, the criminal&nbsp;justice program director for the CCLA.&nbsp;</p>  <p>"That should be a decision every person makes about what they&nbsp;want to do with their own personal medical information."</p>  <p>In early April, the&nbsp;Ontario&nbsp;government passed an emergency order&nbsp;that allowed police to obtain the names, addresses and dates of birth of Ontarians who had tested positive for COVID-19. The portal was aimed at helping to protect first responders.</p>  <p>Police access to that database ended on Aug. 17, after a legal&nbsp;challenge was filed by a group of human rights&nbsp; organizations.</p>  <p>The group, which included the CCLA, argued that allowing police&nbsp;to access personal health records violated individuals'<br> constitutional rights to privacy and equality.</p>  <h2>Police conducted 95,000 searches of database</h2>  <p>Data released in the context of the legal action showed that&nbsp; Ontario&nbsp;police services conducted over 95,000 searches of the&nbsp;database while it was active.</p>  <p>The CCF filed a freedom of information act request to the&nbsp;province related to police use of the database.&nbsp;</p>    <blockquote><span><span><svg version="1.1" focusable="false" x="0px" y="0px" width="30px" height="25px" viewBox="0 0 52.157 39.117" enable-background="new 0 0 52.157 39.117" space="preserve"><g><g><path fill="000000" d="M22.692,10.113c-5.199,1.4-8.398,4.4-8.398,8.801c0,2.4,2,3,3.6,4.199c2.2,1.602,3.4,3.4,3.4,6.602   c0,3.799-3.4,6.799-7.4,6.799c-4.6,0-8.8-3.199-8.8-10.6c0-13.6,7-20,17.599-21.799V10.113z M45.692,10.113   c-5.199,1.4-8.399,4.4-8.399,8.801c0,2.4,2,3,3.601,4.199c2.2,1.602,3.399,3.4,3.399,6.602c0,3.799-3.399,6.799-7.399,6.799   c-4.601,0-8.8-3.199-8.8-10.6c0-13.6,7-20,17.599-21.799V10.113z"></path></g></g><g display="none"><g display="inline"> <path fill="000000" d="M6.648,29.759c5.199-1.4,8.398-4.4,8.398-8.801c0-2.398-2-3-3.599-4.199c-2.2-1.6-3.399-3.4-3.399-6.6   c0-3.801,3.399-6.801,7.399-6.801c4.599,0,8.8,3.201,8.8,10.6c0,13.6-7,20-17.6,21.801V29.759z M29.648,29.759   c5.199-1.4,8.398-4.4,8.398-8.801c0-2.398-2-3-3.599-4.199c-2.2-1.6-3.401-3.4-3.401-6.6c0-3.801,3.401-6.801,7.401-6.801   c4.599,0,8.8,3.201,8.8,10.6c0,13.6-7,20-17.6,21.801V29.759z"></path></g></g></svg>Police were caught using the COVID-19 database to look up names&nbsp;unrelated to active calls, to do wholesale postal&nbsp; code searches for COVID-19 cases, and to even do broad based searches outside officers' own cities.<svg focusable="false" x="0px" y="0px" width="23px" height="22px" viewBox="0 0 52.157 39.117" enable-background="new 0 0 52.157 39.117" space="preserve"><g display="none"><g display="inline"><path fill="000000" d="M22.692,10.113c-5.199,1.4-8.398,4.4-8.398,8.801c0,2.4,2,3,3.6,4.199c2.2,1.602,3.4,3.4,3.4,6.602   c0,3.799-3.4,6.799-7.4,6.799c-4.6,0-8.8-3.199-8.8-10.6c0-13.6,7-20,17.599-21.799V10.113z M45.692,10.113   c-5.199,1.4-8.399,4.4-8.399,8.801c0,2.4,2,3,3.601,4.199c2.2,1.602,3.399,3.4,3.399,6.602c0,3.799-3.399,6.799-7.399,6.799   c-4.601,0-8.8-3.199-8.8-10.6c0-13.6,7-20,17.599-21.799V10.113z"></path></g></g><g><g><path fill="000000" d="M6.648,29.759c5.199-1.4,8.398-4.4,8.398-8.801c0-2.398-2-3-3.599-4.199c-2.2-1.6-3.399-3.4-3.399-6.6   c0-3.801,3.399-6.801,7.399-6.801c4.599,0,8.8,3.201,8.8,10.6c0,13.6-7,20-17.6,21.801V29.759z M29.648,29.759   c5.199-1.4,8.398-4.4,8.398-8.801c0-2.398-2-3-3.599-4.199c-2.2-1.6-3.401-3.4-3.401-6.6c0-3.801,3.401-6.801,7.401-6.801   c4.599,0,8.8,3.201,8.8,10.6c0,13.6-7,20-17.6,21.801V29.759z"></path></g></g></svg></span><cite>- Christine Van Geyn, Canadian Constitution Foundation</cite></span></blockquote>    <p>On Wednesday, the CCF made public a June memo from the Solicitor&nbsp;General's office to chiefs of police that warned against using the&nbsp;database beyond the "express purpose" of the emergency order.</p>  <p>The CCF said the memo revealed a "shocking misuse" of personal&nbsp;health information by police.</p>  <p>"Police were caught using the COVID-19 database to look up names&nbsp;unrelated to active calls, to do wholesale postal&nbsp; code searches for COVID-19 cases, and to even do broad based searches outside officers' own cities," said CCF litigation director, Christine Van&nbsp;Geyn.&nbsp;<span><ul><li><a href="https://www.cbc.ca/news/canada/toronto/covid-ont-police-database-1.5690220" data-contentid="" flag="" text="Ontario ends police access to COVID-19 database after legal challenge"><span>Ontario ends police access to COVID-19 database after legal challenge</span></a></li></ul></span></p>  <p>The CCF said it has filed a complaint with&nbsp;Ontario's privacy&nbsp;commissioner over violations of the Personal Health Information Protection Act, and with the&nbsp;Ontario&nbsp;Independent Police Review Director for officer misconduct.</p>  <p>Meanwhile, the CCLA sent letters to 37 police forces, asking them&nbsp;for details of how the database was used and if any information was retained from it.</p>  <p>Twenty-three responded and Deshman said she expects more to do&nbsp;so.</p>  <h2>Thunder Bay, Durham police conducted more than 40% of searches</h2>  <p>Many forces found the database difficult to use and resorted to&nbsp;problematic broad searches in an attempt to find workarounds, the CCLA said.</p>  <p>The association notes that more than 40 per cent of the 95,000&nbsp;searches of the database were conducted by either the Thunder Bay police or Durham Region police.&nbsp;</p>    <p>In Durham Region, police continued to run unauthorized searches&nbsp;even after provincial audits called attention to the inappropriate&nbsp;searches taking place, the CCLA said. The force's access to the portal was cut off by the province as a result, the CCLA said.</p>  <p>"Durham is a particularly concerning example," said Deshman.&nbsp;"In those cases there needs to be disclosure (to citizens whose information was accessed) and accountability by following up with the individuals in the police service that looked up information inappropriately."</p>  <h2>Anyone concerned can contact police, privacy commissioner&nbsp;</h2>  <p>Holly Walbourne, the legal counsel for Thunder Bay police, said&nbsp;in a letter sent to the groups that filed the legal challenge that&nbsp;the force understood their concerns but that police had "lawful authority" to use the database to protect first responders.</p>    <p>Durham police Supt. Peter Cousins wrote a report to the force's&nbsp;police services board on the issue on Sept. 1 saying&nbsp; access to theportal and its information was treated "seriously and with due care."&nbsp;</p>  <p>Toronto police never used the database because of "issues with&nbsp;the accuracy and reliability of the information," the CCLA reported. York Region police said they asked the province to revoke access to the database after an internal review found the risks associated with accessing personal health information outweighed any benefits.</p>  <p>Deshman said anyone concerned about police access to the province's COVID-19 database should contact their local police force&nbsp;and the&nbsp;Ontario&nbsp;Privacy Commissioner.</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/canada/toronto/covid-police-database-1.5745481</link>
            <guid isPermaLink="false">hacker-news-small-sites-24650515</guid>
            <pubDate>Thu, 01 Oct 2020 13:23:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Segment Tree]]>
            </title>
            <description>
<![CDATA[
Score 116 | Comments 13 (<a href="https://news.ycombinator.com/item?id=24650084">thread link</a>) | @pmoriarty
<br/>
October 1, 2020 | https://cp-algorithms.com/data_structures/segment_tree.html | <a href="https://web.archive.org/web/*/https://cp-algorithms.com/data_structures/segment_tree.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="container">
    


<p>A Segment Tree is a data structure that allows answering range queries over an array effectively, while still being flexible enough to allow modifying the array. 
This includes finding the sum of consecutive array elements $a[l \dots r]$, or finding the minimum element in a such a range in $O(\log n)$ time. 
Between answering such queries the Segment Tree allows modifying the array by replacing one element, or even change the elements of a whole subsegment (e.g. assigning all elements $a[l \dots r]$ to any value, or adding a value to all element in the subsegment).</p>

<p>In general a Segment Tree is a very flexible data structure, and a huge number of problems can be solved with it. 
Additionally it is also possible to apply more complex operations and answer more complex queries (see <a href="https://cp-algorithms.com/data_structures/segment_tree.html#advanced-versions-of-segment-trees">Advanced versions of Segment Trees</a>).
In particular the Segment Tree can be easily generalized to larger dimensions. 
For instance with a two-dimensional Segment Tree you can answer sum or minimum queries over some subrectangle of a given matrix.
However only in $O(\log^2 n)$ time.</p>

<p>One important property of Segment Trees is, that they require only a linear amount of memory.
The standard Segment Tree requires $4n$ vertices for working on an array of size $n$.</p>

<h2>Simplest form of a Segment Tree</h2>

<p>To start easy, we consider the simplest form of a Segment Tree. 
We want to answer sum queries efficiently. 
The formal definition of our task is:
We have an array $a[0 \dots n-1]$, and the Segment Tree must be able to find the sum of elements between the indices $l$ and $r$ (i.e. computing the sum $\sum_{i=l}^r a[i]$), and also handle changing values of the elements in the array (i.e. perform assignments of the form $a[i] = x$). 
The Segment Tree should be able to process both queries in $O(\log n)$ time.</p>

<h3>Structure of the Segment Tree</h3>

<p>So, what is a Segment Tree?</p>

<p>We compute and store the sum of the elements of the whole array, i.e. the sum of the segment $a[0 \dots n-1]$. 
We then split the array into two halves $a[0 \dots n/2]$ and $a[n/2+1 \dots n-1]$ and compute the sum of each halve and store them. 
Each of these two halves in turn also split in half, their sums are computed and stored. 
And this process repeats until all segments reach size $1$. 
In other words we start with the segment $a[0 \dots n-1]$, split the current segment in half (if it has not yet become a segment containing a single element), and then calling the same procedure for both halves. 
For each such segment we store the sum of the numbers on it.</p>

<p>We can say, that these segments form a binary tree: 
the root of this tree is the segment $a[0 \dots n-1]$, and each vertex (except leaf vertices) has exactly two child vertices. 
This is why the data structure is called "Segment Tree", even though in most implementations the tree is not constructed explicitly (see <a href="https://cp-algorithms.com/data_structures/segment_tree.html#implementation">Implementation</a>).</p>

<p>Here is a visual representation of such a Segment Tree over the array $a = [1, 3, -2, 8, -7]$:</p>

<p><img src="https://raw.githubusercontent.com/e-maxx-eng/e-maxx-eng/master/img/sum-segment-tree.png" alt="&quot;Sum Segment Tree&quot;"></p>

<p>From this short description of the data structure, we can already conclude that a Segment Tree only requires a linear number of vertices. 
The first level of the tree contains a single node (the root), the second level will contain two vertices, in the third it will contain four vertices, until the number of vertices reaches $n$. 
Thus the number of vertices in the worst case can be estimated by the sum $1 + 2 + 4 + \dots + 2^{\lceil\log_2 n\rceil} = 2^{\lceil\log_2 n\rceil + 1} \lt 4n$.</p>

<p>It is worth noting that whenever $n$ is not a power of two, not all levels of the Segment Tree will be completely filled. 
We can see that behavior in the image.
For now we can forget about this fact, but it will become important later during the implementation.</p>

<p>The height of the Segment Tree is $O(\log n)$, because when going down from the root to the leaves the size of the segments decreases approximately by half.</p>

<h3>Construction</h3>

<p>Before constructing the segment tree, we need to decide:</p>

<ol>
<li>the <em>value</em> that gets stored at each node of the segment tree.
For example, in a sum segment tree, a node would store the sum of the elements in its range $[l, r]$.</li>
<li>the <em>merge</em> operation that merges two siblings in a segment tree.
For example, in a sum segment tree, the two nodes corresponding to the ranges $a[l_1 \dots r_1]$ and $a[l_2 \dots r_2]$ would be merged into a node corresponding to the range $a[l_1 \dots r_2]$ by adding the values of the two nodes.</li>
</ol>

<p>Note that a vertex is a "leaf vertex", if its corresponding segment covers only one value in the original array. It is present at the lowermost level of a segment tree. Its value would be equal to the (corresponding) element $a[i]$.</p>

<p>Now, for construction of the segment tree, we start at the bottom level (the leaf vertices) and assign them their respective values. On the basis of these values, we can compute the values of the previous level, using the <code>merge</code> function.
And on the basis of those, we can compute the values of the previous, and repeat the procedure until we reach the root vertex.</p>

<p>It is convenient to describe this operation recursively in the other direction, i.e., from the root vertex to the leaf vertices. The construction procedure, if called on a non-leaf vertex, does the following:</p>

<ol>
<li>recursively construct the values of the two child vertices</li>
<li>merge the computed values of these children.</li>
</ol>

<p>We start the construction at the root vertex, and hence, we are able to compute the entire segment tree.</p>

<p>The time complexity of this construction is $O(n)$, assuming that the merge operation is constant time (the merge operation gets called $n$ times, which is equal to the number of internal nodes in the segment tree).</p>

<h3>Sum queries</h3>

<p>For now we are going to answer sum queries. As an input we receive two integers $l$ and $r$, and we have to compute the sum of the segment $a[l \dots r]$ in $O(\log n)$ time.</p>

<p>To do this, we will traverse the Segment Tree and use the precomputed sums of the segments.
Let's assume that we are currently at the vertex that covers the segment $a[tl \dots tr]$.
There are three possible cases.</p>

<p>The easiest case is when the segment $a[l \dots r]$ is equal to the corresponding segment of the current vertex (i.e. $a[l \dots r] = a[tl \dots tr]$), then we are finished and can return the precomputed sum that is stored in the vertex.</p>

<p>Alternatively the segment of the query can fall completely into the domain of either the left or the right child.
Recall that the left child covers the segment $a[tl \dots tm]$ and the right vertex covers the segment $a[tm + 1 \dots tr]$ with $tm = (tl + tr) / 2$. 
In this case we can simply go to the child vertex, which corresponding segment covers the query segment, and execute the algorithm described here with that vertex.</p>

<p>And then there is the last case, the query segment intersects with both children. 
In this case we have no other option as to make two recursive calls, one for each child.
First we go to the left child, compute a partial answer for this vertex (i.e. the sum of values of the intersection between the segment of the query and the segment of the left child), then go to the right child, compute the partial answer using that vertex, and then combine the answers by adding them. 
In other words, since the left child represents the segment $a[tl \dots tm]$ and the right child the segment $a[tm+1 \dots tr]$, we compute the sum query $a[l \dots tm]$ using the left child, and the sum query $a[tm+1 \dots r]$ using the right child.</p>

<p>So processing a sum query is a function that recursively calls itself once with either the left or the right child (without changing the query boundaries), or twice, once for the left and once for the right child (by splitting the query into two subqueries). 
And the recursion ends, whenever the boundaries of the current query segment coincides with the boundaries of the segment of the current vertex. 
In that case the answer will be the precomputed value of the sum of this segment, which is stored in the tree.</p>

<p>In other words, the calculation of the query is a traversal of the tree, which spreads through all necessary branches of the tree, and uses the precomputed sum values of the segments in the tree.</p>

<p>Obviously we will start the traversal from the root vertex of the Segment Tree.</p>

<p>The procedure is illustrated in the following image.
Again the array $a = [1, 3, -2, 8, -7]$ is used, and here we want to compute the sum $\sum_{i=2}^4 a[i]$.
The colored vertices will be visited, and we will use the precomputed values of the green vertices.
This gives us the result $-2 + 1 = -1$.</p>

<p><img src="https://raw.githubusercontent.com/e-maxx-eng/e-maxx-eng/master/img/sum-segment-tree-query.png" alt="&quot;Sum Segment Tree Query&quot;"></p>

<p>Why is the complexity of this algorithm $O(\log n)$?
To show this complexity we look at each level of the tree. 
It turns out, that for each level we only visit not more than four vertices. 
And since the height of the tree is $O(\log n)$, we receive the desired running time.</p>

<p>We can show that this proposition (at most four vertices each level) is true by induction.
At the first level, we only visit one vertex, the root vertex, so here we visit less than four vertices. 
Now let's look at an arbitrary level.
By induction hypothesis, we visit at most four vertices. 
If we only visit at most two vertices, the next level has at most four vertices. That trivial, because each vertex can only cause at most two recursive calls. 
So let's assume that we visit three or four vertices in the current level. 
From those vertices, we will analyze the vertices in the middle more carefully. 
Since the sum query asks for the sum of a continuous subarray, we know that segments corresponding to the visited vertices in the middle will be completely covered by the segment of the sum query. 
Therefore these vertices will not make any recursive calls. 
So only the most left, and the most right vertex will have the potential to make recursive calls. 
And those will only create at most four recursive calls, so also the next level will satisfy the assertion.
We can say that one branch approaches the left boundary of the query, and the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://cp-algorithms.com/data_structures/segment_tree.html">https://cp-algorithms.com/data_structures/segment_tree.html</a></em></p>]]>
            </description>
            <link>https://cp-algorithms.com/data_structures/segment_tree.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24650084</guid>
            <pubDate>Thu, 01 Oct 2020 12:25:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Web Neural Network API]]>
            </title>
            <description>
<![CDATA[
Score 89 | Comments 30 (<a href="https://news.ycombinator.com/item?id=24649616">thread link</a>) | @rajveermalviya
<br/>
October 1, 2020 | https://webmachinelearning.github.io/webnn/ | <a href="https://web.archive.org/web/*/https://webmachinelearning.github.io/webnn/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
   <h2 data-level="1" id="intro"><span>1. </span><span>Introduction</span><a href="#intro"></a></h2>
   <p>We’re working on this section. Meanwhile, please take a look at the <a href="https://github.com/webmachinelearning/webnn/blob/master/explainer.md">explainer</a>.</p>
   <h2 data-level="2" id="usecases"><span>2. </span><span>Use cases</span><a href="#usecases"></a></h2>
   <h3 data-level="2.1" id="usecases-application"><span>2.1. </span><span>Application Use Cases</span><a href="#usecases-application"></a></h3>
   <p>This section illustrates application-level use cases for neural network
inference hardware acceleration. All applications in those use cases can be
built on top of pre-trained deep neural network (DNN) models.</p>
   <h4 data-level="2.1.1" id="usecase-person-detection"><span>2.1.1. </span><span>Person Detection</span><a href="#usecase-person-detection"></a></h4>
   <p>A user opens a web-based video conferencing application, but she temporarily
leaves from her room. The application is watching whether she is in front of her
PC by using object detection (for example, using object detection approaches
such as <a data-link-type="biblio" href="#biblio-ssd">[SSD]</a> or <a data-link-type="biblio" href="#biblio-yolo">[YOLO]</a> that use a single DNN) to detect regions in a camera
input frame that include persons.</p>
   <p>When she comes back, the application automatically detects her and notifies
other online users that she is active now.</p>
   <h4 data-level="2.1.2" id="usecase-segmentation"><span>2.1.2. </span><span>Semantic Segmentation</span><a href="#usecase-segmentation"></a></h4>
   <p>A user joins a teleconference via a web-based video conferencing application at
her desk since no meeting room in her office is available. During the
teleconference, she does not wish that her room and people in the background are
visible. To protect the privacy of the other people and the surroundings, the
application runs a machine learning model such as <a data-link-type="biblio" href="#biblio-deeplabv3">[DeepLabv3+]</a> or <a data-link-type="biblio" href="#biblio-maskr-cnn">[MaskR-CNN]</a> to semantically split an image into segments and replaces
segments that represent other people and background with another picture.</p>
   <h4 data-level="2.1.3" id="usecase-skeleton-detection"><span>2.1.3. </span><span>Skeleton Detection</span><a href="#usecase-skeleton-detection"></a></h4>
   <p>A web-based video conferencing application tracks a pose of user’s skeleton by
running a machine learning model, which allows for real-time human pose
estimation, such as <a data-link-type="biblio" href="#biblio-posenet">[PoseNet]</a> to recognize her gesture and body language. When
she raises her hand, her microphone is automatically unmuted and she can start
speaking on the teleconference.</p>
   <h4 data-level="2.1.4" id="usecase-face-recognition"><span>2.1.4. </span><span>Face Recognition</span><a href="#usecase-face-recognition"></a></h4>
   <p>There are multiple people in the conference room and they join an online meeting
using a web-based video conferencing application. The application detects faces
of participants by using object detection (for example, using object detection
approaches such as <a data-link-type="biblio" href="#biblio-ssd">[SSD]</a>) and checks whether each face was present at the
previous meeting or not by running a machine learning model such as <a data-link-type="biblio" href="#biblio-facenet">[FaceNet]</a>,
which verifies whether two faces would be identical or not.</p>
   <h4 data-level="2.1.5" id="usecase-facial-landmarks"><span>2.1.5. </span><span>Facial Landmark Detection</span><a href="#usecase-facial-landmarks"></a></h4>
   <p>A user wants to find new glasses that beautifully fits her on an online glasses
store. The online store offers web-based try-on simulator that runs a machine
learning model such as Face Alignment Network <a data-link-type="biblio" href="#biblio-fan">[FAN]</a> to detect facial landmarks
like eyes, nose, mouth, etc. When she chooses a pair of glasses, the simulator
properly render the selected glasses on the detected position of eyes on her
facial image.</p>
   <h4 data-level="2.1.6" id="usecase-style-transfer"><span>2.1.6. </span><span>Style Transfer</span><a href="#usecase-style-transfer"></a></h4>
   <p>A user is looking for cosmetics on an online store and wondering which color may
fit her face. The online store shows sample facial makeup images of cosmetics,
and offers makeup simulator that runs a machine learning model like <a data-link-type="biblio" href="#biblio-contextualloss">[ContextualLoss]</a> or <a data-link-type="biblio" href="#biblio-pairedcyclegan">[PairedCycleGAN]</a> to transfer the makeup style of the
sample makeup image to her facial image. She can check how the selected makeup
looks like on her face by the simulator.</p>
   <h4 data-level="2.1.7" id="usecase-super-resolution"><span>2.1.7. </span><span>Super Resolution</span><a href="#usecase-super-resolution"></a></h4>
   <p>A web-based video conferencing is receiving a video stream from its peer, but
the resolution of the video becomes lower due to network congestion. To prevent
degradation of the perceived video quality, the application runs a machine
learning model for super-resolution such as <a data-link-type="biblio" href="#biblio-srgan">[SRGAN]</a> to generate
higher-resolution video frames.</p>
   <h4 data-level="2.1.8" id="usecase-image-captioning"><span>2.1.8. </span><span>Image Captioning</span><a href="#usecase-image-captioning"></a></h4>
   <p>For better accessibility, a web-based presentation application provides
automatic image captioning by running a machine learning model such as <a data-link-type="biblio" href="#biblio-im2txt">[im2txt]</a> which predicts explanatory words of the presentation slides.</p>
   <h4 data-level="2.1.9" id="usecase-translation"><span>2.1.9. </span><span>Machine Translation</span><a href="#usecase-translation"></a></h4>
   <p>Multiple people from various countries are talking via a web-based real-time
text chat application. The application translates their conversation by using a
machine learning model such as <a data-link-type="biblio" href="#biblio-gnmt">[GNMT]</a> or <a data-link-type="biblio" href="#biblio-opennmt">[OpenNMT]</a>, which translates every
text into different language.</p>
   <h4 data-level="2.1.10" id="usecase-emotion-analysis"><span>2.1.10. </span><span>Emotion Analysis</span><a href="#usecase-emotion-analysis"></a></h4>
   <p>A user is talking to her friend via a web-based real-time text chat application,
and she is wondering how the friend feels because she cannot see the friend’s
face. The application analyses the friend’s emotion by using a machine learning
model such as <a data-link-type="biblio" href="#biblio-deepmoji">[DeepMoji]</a>, which infers emotion from input texts, and displays
an emoji that represents the estimated emotion.</p>
   <h4 data-level="2.1.11" id="usecase-video-summalization"><span>2.1.11. </span><span>Video Summarization</span><a href="#usecase-video-summalization"></a></h4>
   <p>A web-based video conferencing application records received video streams, and
it needs to reduce recorded video data to be stored. The application generates
the short version of the recorded video by using a machine learning model for
video summarization such as <a data-link-type="biblio" href="#biblio-video-summarization-with-lstm">[Video-Summarization-with-LSTM]</a>.</p>
   <h4 data-level="2.1.12" id="usecase-noise-suppression"><span>2.1.12. </span><span>Noise Suppression</span><a href="#usecase-noise-suppression"></a></h4>
   <p>A web-based video conferencing application records received audio streams, but
usually the background noise is everywhere. The application leverages real-time 
noise suppression using Recurrent Neural Network such as <a data-link-type="biblio" href="#biblio-rnnoise">[RNNoise]</a> for 
suppressing background dynamic noise like baby cry or dog barking to improve 
audio experiences in video conferences.</p>
   <h3 data-level="2.2" id="usecases-framework"><span>2.2. </span><span>Framework Use Cases</span><a href="#usecases-framework"></a></h3>
   <p>This section collects framework-level use cases for a dedicated low-level API
for neural network inference hardware acceleration. It is expected that Machine
Learning frameworks will be key consumers of the Web Neural Network API (WebNN
API) and the low-level details exposed through the WebNN API are abstracted out
from typical web developers. However, it is also expected that web developers
with specific interest and competence in Machine Learning will want to interface
with the WebNN API directly instead of a higher-level ML framework.</p>
   <h4 data-level="2.2.1" id="usecase-custom-layer"><span>2.2.1. </span><span>Custom Layer</span><a href="#usecase-custom-layer"></a></h4>
   <p>A web application developer wants to run a DNN model on the WebNN API. However,
she has found that some of activation functions like <a data-link-type="biblio" href="#biblio-leakyrelu">[LeakyReLU]</a>, <a data-link-type="biblio" href="#biblio-elu">[ELU]</a>,
etc. are not included in the WebNN API. To address this issue, she constructs
custom layers of the additional activation functions on top of the WebNN API.
Note that the scope of custom layers may include convolution, normalization,
etc. as well as activation.</p>
   <h4 data-level="2.2.2" id="usecase-network-concat"><span>2.2.2. </span><span>Network Concatenation</span><a href="#usecase-network-concat"></a></h4>
   <p>A web application uses a DNN model, and its model data of upper convolutional
layers and lower fully-connected layers are stored in separate files, since
model data of the fully-connected layers are periodically updated due to fine
tuning at the server side.</p>
   <p>Therefore, the application downloads both partial model files at first and
concatenates them into a single model. When the model is updated, the
application downloads fine-tuned part of the model and replace only the
fully-connected layers with it.</p>
   <h4 data-level="2.2.3" id="usecase-perf-adapt"><span>2.2.3. </span><span>Performance Adaptation</span><a href="#usecase-perf-adapt"></a></h4>
   <p>A web application developer has a concern about performance of her DNN model on
mobile devices. She has confirmed that it may run too slow on mobile devices
which do not have GPU acceleration. To address this issue, her web application
refers to the WebNN API to confirm whether acceleration is available or not, so
that the application can display the warning for devices without acceleration.</p>
   <p>After several weeks, she has developed a tiny DNN model that can even run on
CPU. In order to accommodate CPU execution, she modifies the application
so that the application loads the tiny model in the case of CPU-only devices.</p>
   <h2 data-level="3" id="api"><span>3. </span><span>API</span><a href="#api"></a></h2>
   <h3 data-level="3.1" id="api-navigator"><span>3.1. </span><span>Navigator</span><a href="#api-navigator"></a></h3>
<pre><c- b="">partial</c-> <c- b="">interface</c-> <a data-link-type="interface" href="https://html.spec.whatwg.org/multipage/system-state.html#navigator" id="ref-for-navigator"><c- g="">Navigator</c-></a> {
  <c- b="">readonly</c-> <c- b="">attribute</c-> <a data-link-type="idl-name" href="#ml" id="ref-for-ml"><c- n="">ML</c-></a> <dfn data-dfn-for="Navigator" data-dfn-type="attribute" data-export="" data-readonly="" data-type="ML" id="dom-navigator-ml"><code><c- g="">ml</c-></code><a href="#dom-navigator-ml"></a></dfn>;
};
</pre>
   <h3 data-level="3.2" id="api-ml"><span>3.2. </span><span>ML</span><a href="#api-ml"></a></h3>
<pre><c- b="">interface</c-> <dfn data-dfn-type="interface" data-export="" id="ml"><code><c- g="">ML</c-></code></dfn> {
  <a data-link-type="idl-name" href="#neuralnetworkcontext" id="ref-for-neuralnetworkcontext"><c- n="">NeuralNetworkContext</c-></a> <dfn data-dfn-for="ML" data-dfn-type="method" data-export="" data-lt="getNeuralNetworkContext()" id="dom-ml-getneuralnetworkcontext"><code><c- g="">getNeuralNetworkContext</c-></code><a href="#dom-ml-getneuralnetworkcontext"></a></dfn>();
};
</pre>
   <h3 data-level="3.3" id="api-operanddescriptor"><span>3.3. </span><span>OperandDescriptor</span><a href="#api-operanddescriptor"></a></h3>
<pre><c- b="">enum</c-> <dfn data-dfn-type="enum" data-export="" id="enumdef-operandlayout"><code><c- g="">OperandLayout</c-></code></dfn> {
  <dfn data-dfn-for="OperandLayout" data-dfn-type="enum-value" data-export="" id="dom-operandlayout-nchw"><code><c- s="">"nchw"</c-></code><a href="#dom-operandlayout-nchw"></a></dfn>,
  <dfn data-dfn-for="OperandLayout" data-dfn-type="enum-value" data-export="" id="dom-operandlayout-nhwc"><code><c- s="">"nhwc"</c-></code><a href="#dom-operandlayout-nhwc"></a></dfn>
};

<c- b="">enum</c-> <dfn data-dfn-type="enum" data-export="" id="enumdef-operandtype"><code><c- g="">OperandType</c-></code></dfn> {
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-float32"><code><c- s="">"float32"</c-></code><a href="#dom-operandtype-float32"></a></dfn>,
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-float16"><code><c- s="">"float16"</c-></code><a href="#dom-operandtype-float16"></a></dfn>,
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-int32"><code><c- s="">"int32"</c-></code><a href="#dom-operandtype-int32"></a></dfn>,
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-uint32"><code><c- s="">"uint32"</c-></code><a href="#dom-operandtype-uint32"></a></dfn>,
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-tensor-float32"><code><c- s="">"tensor-float32"</c-></code><a href="#dom-operandtype-tensor-float32"></a></dfn>,
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-tensor-float16"><code><c- s="">"tensor-float16"</c-></code><a href="#dom-operandtype-tensor-float16"></a></dfn>,
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-tensor-int32"><code><c- s="">"tensor-int32"</c-></code><a href="#dom-operandtype-tensor-int32"></a></dfn>,
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-tensor-quant8-asymm"><code><c- s="">"tensor-quant8-asymm"</c-></code><a href="#dom-operandtype-tensor-quant8-asymm"></a></dfn>
};

<c- b="">dictionary</c-> <dfn data-dfn-type="dictionary" data-export="" id="dictdef-operanddescriptor"><code><c- g="">OperandDescriptor</c-></code></dfn> {
  // The operand type.
  <c- b="">required</c-> <a data-link-type="idl-name" href="#enumdef-operandtype" id="ref-for-enumdef-operandtype"><c- n="">OperandType</c-></a> <dfn data-dfn-for="OperandDescriptor" data-dfn-type="dict-member" data-export="" data-type="OperandType " id="dom-operanddescriptor-type"><code><c- g="">type</c-></code><a href="#dom-operanddescriptor-type"></a></dfn>;

  // The dimensions field is only required for tensor operands.
  // The negative value means an unknown dimension.
  <c- b="">sequence</c->&lt;<a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-long" id="ref-for-idl-long"><c- b="">long</c-></a>&gt; <dfn data-dfn-for="OperandDescriptor" data-dfn-type="dict-member" data-export="" data-type="sequence<long> " id="dom-operanddescriptor-dimensions"><code><c- g="">dimensions</c-></code><a href="#dom-operanddescriptor-dimensions"></a></dfn>;

  // The following two fields are only required for quantized operand.
  // scale: an non-negative floating point value
  // zeroPoint: an integer, in range [0, 255]
  // The real value is (value - zeroPoint) * scale
  <a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-float" id="ref-for-idl-float"><c- b="">float</c-></a> <dfn data-dfn-for="OperandDescriptor" data-dfn-type="dict-member" data-export="" data-type="float " id="dom-operanddescriptor-scale"><code><c- g="">scale</c-></code><a href="#dom-operanddescriptor-scale"></a></dfn>;
  <a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-long" id="ref-for-idl-long①"><c- b="">long</c-></a> <dfn data-dfn-for="OperandDescriptor" data-dfn-type="dict-member" data-export="" data-type="long " id="dom-operanddescriptor-zeropoint"><code><c- g="">zeroPoint</c-></code><a href="#dom-operanddescriptor-zeropoint"></a></dfn>;
};
</pre>
   <h3 data-level="3.4" id="api-operand"><span>3.4. </span><span>Operand</span><a href="#api-operand"></a></h3>
<pre><c- b="">interface</c-> <dfn data-dfn-type="interface" data-export="" id="operand"><code><c- g="">Operand</c-></code></dfn> {};
</pre>
   <h3 data-level="3.5" id="api-neuralnetworkcontext"><span>3.5. </span><span>NeuralNetworkContext</span><a href="#api-neuralnetworkcontext"></a></h3>
   <p>The <code><a data-link-type="idl" href="#neuralnetworkcontext" id="ref-for-neuralnetworkcontext①">NeuralNetworkContext</a></code> defines a set of operations derived from the first-wave models <a data-link-type="biblio" href="#biblio-models">[Models]</a> that address identified <a href="#usecases">§ 2 Use cases</a>.</p>
<pre><c- b="">typedef</c-> <a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-double" id="ref-for-idl-double"><c- b="">double</c-></a> <dfn data-dfn-type="typedef" data-export="" id="typedefdef-number"><code><c- g="">number</c-></code></dfn>;

<c- b="">dictionary</c-> <dfn data-dfn-type="dictionary" data-export="" id="dictdef-namedoperand"><code><c- g="">NamedOperand</c-></code></dfn> {
  <c- b="">required</c-> <a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-DOMString" id="ref-for-idl-DOMString"><c- b="">DOMString</c-></a> <dfn data-dfn-for="NamedOperand" data-dfn-type="dict-member" data-export="" data-type="DOMString " id="dom-namedoperand-name"><code><c- g="">name</c-></code><a href="#dom-namedoperand-name"></a></dfn>;
  <c- b="">required</c-> <a data-link-type="idl-name" href="#operand" id="ref-for-operand"><c- n="">Operand</c-></a> <dfn data-dfn-for="NamedOperand" data-dfn-type="dict-member" data-export="" data-type="Operand " id="dom-namedoperand-operand"><code><c- g="">operand</c-></code><a href="#dom-namedoperand-operand"></a></dfn>;
};

<c- b="">interface</c-> <dfn data-dfn-type="interface" data-export="" id="neuralnetworkcontext"><code><c- g="">NeuralNetworkContext</c-></code></dfn> {
  // Create an Operand object that represents a model input.
  <a data-link-type="idl-name" href="#operand" id="ref-for-operand①"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext" data-dfn-type="method" data-export="" data-lt="input(name, desc)" id="dom-neuralnetworkcontext-input"><code><c- g="">input</c-></code><a href="#dom-neuralnetworkcontext-input"></a></dfn>(<a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-DOMString" id="ref-for-idl-DOMString①"><c- b="">DOMString</c-></a> <dfn data-dfn-for="NeuralNetworkContext/input(name, desc)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-input-name-desc-name"><code><c- g="">name</c-></code><a href="#dom-neuralnetworkcontext-input-name-desc-name"></a></dfn>, <a data-link-type="idl-name" href="#dictdef-operanddescriptor" id="ref-for-dictdef-operanddescriptor"><c- n="">OperandDescriptor</c-></a> <dfn data-dfn-for="NeuralNetworkContext/input(name, desc)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-input-name-desc-desc"><code><c- g="">desc</c-></code><a href="#dom-neuralnetworkcontext-input-name-desc-desc"></a></dfn>);

  // Create an Operand object that represents a model constant.
  <a data-link-type="idl-name" href="#operand" id="ref-for-operand②"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext" data-dfn-type="method" data-export="" data-lt="constant(desc, value)" id="dom-neuralnetworkcontext-constant"><code><c- g="">constant</c-></code><a href="#dom-neuralnetworkcontext-constant"></a></dfn>(<a data-link-type="idl-name" href="#dictdef-operanddescriptor" id="ref-for-dictdef-operanddescriptor①"><c- n="">OperandDescriptor</c-></a> <dfn data-dfn-for="NeuralNetworkContext/constant(desc, value)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-constant-desc-value-desc"><code><c- g="">desc</c-></code><a href="#dom-neuralnetworkcontext-constant-desc-value-desc"></a></dfn>, <a data-link-type="idl-name" href="https://heycam.github.io/webidl/#ArrayBufferView" id="ref-for-ArrayBufferView"><c- n="">ArrayBufferView</c-></a> <dfn data-dfn-for="NeuralNetworkContext/constant(desc, value)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-constant-desc-value-value"><code><c- g="">value</c-></code><a href="#dom-neuralnetworkcontext-constant-desc-value-value"></a></dfn>);

  // Create a single-value tensor from the specified number of the specified type.
  <a data-link-type="idl-name" href="#operand" id="ref-for-operand③"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext" data-dfn-type="method" data-export="" data-lt="constant(value, type)|constant(value)" id="dom-neuralnetworkcontext-constant-value-type"><code><c- g="">constant</c-></code><a href="#dom-neuralnetworkcontext-constant-value-type"></a></dfn>(<a data-link-type="idl-name" href="#typedefdef-number" id="ref-for-typedefdef-number"><c- n="">number</c-></a> <dfn data-dfn-for="NeuralNetworkContext/constant(value, type), NeuralNetworkContext/constant(value)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-constant-value-type-value"><code><c- g="">value</c-></code><a href="#dom-neuralnetworkcontext-constant-value-type-value"></a></dfn>, <c- b="">optional</c-> <a data-link-type="idl-name" href="#enumdef-operandtype" id="ref-for-enumdef-operandtype①"><c- n="">OperandType</c-></a> <dfn data-dfn-for="NeuralNetworkContext/constant(value, type), NeuralNetworkContext/constant(value)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-constant-value-type-type"><code><c- g="">type</c-></code><a href="#dom-neuralnetworkcontext-constant-value-type-type"></a></dfn> = "float32");

  // Create a Model object by identifying output operands.
  <c- b="">Promise</c->&lt;<a data-link-type="idl-name" href="#model" id="ref-for-model"><c- n="">Model</c-></a>&gt; <dfn data-dfn-for="NeuralNetworkContext" data-dfn-type="method" data-export="" data-lt="createModel(outputs)" id="dom-neuralnetworkcontext-createmodel"><code><c- g="">createModel</c-></code><a href="#dom-neuralnetworkcontext-createmodel"></a></dfn>(<c- b="">sequence</c->&lt;<a data-link-type="idl-name" href="#dictdef-namedoperand" id="ref-for-dictdef-namedoperand"><c- n="">NamedOperand</c-></a>&gt; <dfn data-dfn-for="NeuralNetworkContext/createModel(outputs)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-createmodel-outputs-outputs"><code><c- g="">outputs</c-></code><a href="#dom-neuralnetworkcontext-createmodel-outputs-outputs"></a></dfn>);
};
</pre>
   <h4 data-level="3.5.1" id="api-neuralnetworkcontext-batchnorm"><span>3.5.1. </span><span>batchNormalization</span><a href="#api-neuralnetworkcontext-batchnorm"></a></h4>
    Normalize the tensor values across dimensions in a batch through a specialized <a data-link-type="biblio" href="#biblio-batchnorm">[BatchNorm]</a> <a href="https://en.wikipedia.org/wiki/Batch_normalization#Batch_Normalizing_Transform">transform</a>. The <em>mean</em> and <em>variance</em> tensors are previously calculated during model training pass. 
<pre><c- b="">partial</c-> <c- b="">interface</c-> <a data-link-type="interface" href="#neuralnetworkcontext" id="ref-for-neuralnetworkcontext②"><c- g="">NeuralNetworkContext</c-></a> {
  <a data-link-type="idl-name" href="#operand" id="ref-for-operand④"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext" data-dfn-type="method" data-export="" data-lt="batchNormalization(input, mean, variance, scale, bias, axis, epsilon)|batchNormalization(input, mean, variance, scale, bias, axis)|batchNormalization(input, mean, variance, scale, bias)|batchNormalization(input, mean, variance, scale)|batchNormalization(input, mean, variance)" id="dom-neuralnetworkcontext-batchnormalization"><code><c- g="">batchNormalization</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization"></a></dfn>(<a data-link-type="idl-name" href="#operand" id="ref-for-operand⑤"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis, epsilon), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias), NeuralNetworkContext/batchNormalization(input, mean, variance, scale), NeuralNetworkContext/batchNormalization(input, mean, variance)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-input"><code><c- g="">input</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-input"></a></dfn>, <a data-link-type="idl-name" href="#operand" id="ref-for-operand⑥"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis, epsilon), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias), NeuralNetworkContext/batchNormalization(input, mean, variance, scale), NeuralNetworkContext/batchNormalization(input, mean, variance)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-mean"><code><c- g="">mean</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-mean"></a></dfn>, <a data-link-type="idl-name" href="#operand" id="ref-for-operand⑦"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis, epsilon), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias), NeuralNetworkContext/batchNormalization(input, mean, variance, scale), NeuralNetworkContext/batchNormalization(input, mean, variance)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-variance"><code><c- g="">variance</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-variance"></a></dfn>, 
                             <c- b="">optional</c-> <a data-link-type="idl-name" href="#operand" id="ref-for-operand⑧"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis, epsilon), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias), NeuralNetworkContext/batchNormalization(input, mean, variance, scale), NeuralNetworkContext/batchNormalization(input, mean, variance)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-scale"><code><c- g="">scale</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-scale"></a></dfn>, <c- b="">optional</c-> <a data-link-type="idl-name" href="#operand" id="ref-for-operand⑨"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis, epsilon), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias), NeuralNetworkContext/batchNormalization(input, mean, variance, scale), NeuralNetworkContext/batchNormalization(input, mean, variance)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-bias"><code><c- g="">bias</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-bias"></a></dfn>, 
                             <c- b="">optional</c-> <a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-long" id="ref-for-idl-long②"><c- b="">long</c-></a> <dfn data-dfn-for="NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis, epsilon), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias), NeuralNetworkContext/batchNormalization(input, mean, variance, scale), NeuralNetworkContext/batchNormalization(input, mean, variance)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-axis"><code><c- g="">axis</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-axis"></a></dfn> = 1, <c- b="">optional</c-> <a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-float" id="ref-for-idl-float①"><c- b="">float</c-></a> <dfn data-dfn-for="NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis, epsilon), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias), NeuralNetworkContext/batchNormalization(input, mean, variance, scale), NeuralNetworkContext/batchNormalization(input, mean, variance)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-epsilon"><code><c- g="">epsilon</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-epsilon"></a></dfn> = 1e-5);
};
</pre>
   <div data-algorithm="batchnorm">
     <p><strong>Arguments:</strong></p><ul>
     <li data-md="">
      <p><em>input</em>: an <code><a data-link-type="idl" href="#operand" id="ref-for-operand①⓪">Operand</a></code>. The input N-D tensor.</p>
     </li><li data-md="">
      <p><em>mean</em>: an <code><a data-link-type="idl" href="#operand" id="ref-for-operand①①">Operand</a></code>. The 1-D tensor of the batch mean values
whose length is equal to the size of the input dimension denoted by <em>axis</em>.</p>
     </li><li data-md="">
      <p><em>variance</em>: an <code><a data-link-type="idl" href="#operand" id="ref-for-operand①②">Operand</a></code>. The 1-D tensor of the batch variance values
whose length is equal to the size of the input …</p></li></ul></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://webmachinelearning.github.io/webnn/">https://webmachinelearning.github.io/webnn/</a></em></p>]]>
            </description>
            <link>https://webmachinelearning.github.io/webnn/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24649616</guid>
            <pubDate>Thu, 01 Oct 2020 11:18:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why are secrets in Git such a threat?]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24649034">thread link</a>) | @oodelally
<br/>
October 1, 2020 | https://www.gitguardian.com/secrets-detection/secret-sprawl | <a href="https://web.archive.org/web/*/https://www.gitguardian.com/secrets-detection/secret-sprawl">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>What are secrets in the software development world?</p><div><p>In the common language, a secret can be any sensitive data that we want to keep private. When discussing secrets in the context of software development, secrets generally refer to digital authentication credentials that grant access to systems or data. These are most commonly API keys, usernames and passwords, or security certificates.</p><p>Secrets exist in the context of applications that are no longer standalone monoliths. Applications nowadays rely on thousands of independent building blocks: cloud infrastructure, databases, SaaS components such as Stripe, Slack, HubSpot…&nbsp;</p><p>Secrets are what tie together these different building blocks of a single application by creating a secure connection between each component.</p></div><p>What is secret sprawl?</p><div><p>Secret sprawl is the unwanted distribution of secrets like API keys and credentials through multiple systems.&nbsp;</p><p>In modern software development, secrets are regularly used by developers and applications. As a result they often get shared through different services like Slack or email and can be stored in multiple locations including different machines, git repositories or inside company wikis. This is a phenomenon we call secret sprawl.</p></div><p>What are some of the best practices to securely manage secrets like API keys?</p><div><p>Storing and managing secrets like API keys and other credentials can be challenging. Even the most careful policies can sometimes be circumvented in exchange for convenience. We have put together a helpful <a href="https://blog.gitguardian.com/secrets-api-management/" target="_blank">cheat sheet</a> and article explaining the <a href="https://blog.gitguardian.com/secrets-api-management/" target="_blank">best practices for managing secrets</a>.</p><p>As a minimum here are some points you should consider:</p></div><div><p>Never store unencrypted secrets in git repositories</p></div><div><p>Avoid git add * commands on git</p></div><div><p>Add sensitive files in .gitignore</p></div><div><p>Don’t rely on code reviews to discover secrets</p></div><div><p>Use automated secrets scanning on repositories</p></div><div><p>Don’t share your secrets unencrypted in messaging systems like Slack</p></div><div><p>Store secrets safely (method to be used depends on every use case)</p></div><div><p>Default to minimal permission scope for APIs</p></div><div><p>Whitelist IP addresses where appropriate</p></div><p>What are the threats associated with secret sprawl?</p><div><p>When secrets are sprawled through multiple systems it increases what is referred to as the ‘attack surface’. This is the amount of points where an unauthorized user could gain access to your systems or data. In the case of secret sprawl, each time a secret enters another system it is another point where an attacker could gain access to your secrets.</p><p>Most internal systems are not an appropriate place to store sensitive information, even if those systems are private. No company wants credit card numbers in plaintext in databases, PII in application logs, bank account credentials in a Google Doc. Secrets benefit from the same kind of protective measures.</p><p>As a general security principle, where feasible, data should remain safe even if it leaves the devices, systems, infrastructure or networks that are under organizations’ control, or if they are compromised. This helps prevent credential stealing, which is a well-known adversary technique described in the MITRE ATT&amp;CK framework:</p></div><p>“ Adversaries may search local file systems and remote file shares for files containing passwords. These can be files created by users to store their own credentials, shared credential stores for a group of individuals, configuration files containing passwords for a system or service, or source code/ binary files containing embedded passwords. ”<br></p><p>Secrets accessed by malicious threat actors can lead to information leakage and allow lateral movement or privilege escalation, as secrets very often lead to other secrets. Furthermore, once an attacker has the credentials to operate like a valid user, it is extremely difficult to detect the abuse and the threat can become persistent.<br></p><p>What are some of the biggest challenges associated with secret sprawl?</p><div><p>Because secrets tie together each component of an application, developers and ops need access to these secrets to build, connect, test and deploy applications. The result is that secrets need to be both tightly wrapped and also widely distributed. </p><p>Enforcing good security practices at the organization level is hard. Developers today can have large turnovers inside companies, be spread through many different teams and geographies. They have a growing number of technologies they must master and are under hard pressure due to shortened release cycles. This makes secret management very complicated and an ever changing challenge. </p><p>This is further complicated when VCS like git are introduced because secrets can be buried deep inside the history. The actual version of source code might look clean, while the history might present credentials that were added than later removed. Any secret reaching the Version Control System must be considered compromised and the presence of valid secrets in the git history presents a threat!</p></div></div></div>]]>
            </description>
            <link>https://www.gitguardian.com/secrets-detection/secret-sprawl</link>
            <guid isPermaLink="false">hacker-news-small-sites-24649034</guid>
            <pubDate>Thu, 01 Oct 2020 09:27:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why is it hard to detect API keys in source code?]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24649026">thread link</a>) | @oodelally
<br/>
October 1, 2020 | https://www.gitguardian.com/secrets-detection/how-to-detect-secrets-in-source-code | <a href="https://web.archive.org/web/*/https://www.gitguardian.com/secrets-detection/how-to-detect-secrets-in-source-code">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Why is it hard to detect secrets like API keys and other credentials?</p><div><p>Secrets detection is probabilistic—that is to say that it is not always possible to determine what is a true secret (or true positive). Because secrets share very few common, distinctive factors, decisions must be taken by aggregating weak signals to make strong predictions. </p><p>One of the most common factors is that almost all secrets are strings that look random. We call these strings high entropy strings. The issue is that this common factor is not very distinctive: 99% of strings that look random in source code aren’t secrets. They are for example database IDs or other types of false positives. </p><p>Of course, some secrets have fixed patterns: AWS keys often start with AKIA for example. But most secrets do not. The portions of code surrounding them are also very different, depending on what the secrets are used for and how they are used by the developers in the context of specific applications. Usernames and passwords can be used to authenticate in many different ways, and it is really hard to distinguish between real and fake credentials used as placeholders for example. </p><p>All this makes it extremely challenging to accurately capture all true secrets without also capturing false positives. At some point, a line in the sand needs to be drawn that considers the cost of a secret going undetected (a false negative) and compares it to the outcome that too many false positives would create. Different organizations with different people, cultures and workflows will draw different lines!</p></div><p>Why do code reviews fail at finding secrets in source code?</p><p>Code reviews are great overall for detecting logic flaws or maintaining certain good coding practices. But they are not adequate protection for detecting secrets, mostly for two reasons:<br></p><div><p>Reviews generally only consider the net difference between the current and proposed states. Not the entire history of changes. If a commit adds a secret and another one later deletes it, this has a zero net effect that is not of any interest to reviewers. But the vulnerability is there.</p></div><div><p>Reviewers prefer to focus on errors that cannot be automatically detected, like design flaws. As a general principle, security automation should be implemented wherever it can be, so that humans focus on where they bring the most value.</p></div><p>What is a "good" secrets detection algorithm?</p><div><p>Detecting secrets in source code is like finding needles in a haystack: there are a lot more sticks than there are needles, and you don’t know how many needles might be in the haystack. In the case of secrets detection, you don’t even know what all the needles look like!</p><p>Ideally, you want your detection system to achieve at the same time:</p></div><div><p>A low number of false alerts raised. We call this high precision. Precision answers the question: «What is the percentage of the secrets that you detect that are actual secrets?». This question is perfectly legitimate, especially in the context of security teams being overwhelmed with too many alerts.</p></div><div><p>A low number of secrets missed. This is what we call high recall. Considering that a single undetected credential can have a big impact for an organization, some organizations prefer to triage more false alerts but make sure they don’t miss a secret.</p></div><p>Balancing the equation to ensure that the algorithm captures as many secrets as possible without flagging too many false results is an intricate and extremely difficult challenge. Read more on evaluating <a href="https://blog.gitguardian.com/secrets-detection-accuracy-precision-recall-explained/" target="_blank">secrets detection algorithms</a>.<br></p><p>What is a false positive in secrets detection?</p><div><p>A false positive in secrets detection refers to when a secret candidate is wrongly marked as a true secret when it is in fact a non-sensitive string. </p><p>Typical examples of strings that can be mistaken for true secrets are:</p></div><div><p>UUIDs (Universally Unique Identifiers) that are used for example by databases as unique keys.</p></div><div><p>High entropy URLs or file paths. They often contain random strings that look like keys.</p></div><p>Examples of server-side hooks:<br></p><div><p>Test keys. Service Providers sometimes provide developers with a set of test credentials with a very restricted scope so developers can exercise parts of the API without charging their account</p></div><div><p>Public keys. As surprising as it is, a very small number of keys are really meant to be public (like Firebase keys, see this Stack Overflow <a href="https://stackoverflow.com/questions/37482366/is-it-safe-to-expose-firebase-apikey-to-the-public" target="_blank">conversation</a>).</p></div><p>Are secrets detection algorithms language-dependent?</p><p>Secrets detection is, for the most part, not language specific. Of course, there are some subtleties to take into account, like the way variables are assigned in any programming language. But there is no need to support all the different syntaxes in their greatest details. This means that the same algorithms can be applied to any project, in any programming language, without using things like Abstract Syntax Trees.<br></p></div></div>]]>
            </description>
            <link>https://www.gitguardian.com/secrets-detection/how-to-detect-secrets-in-source-code</link>
            <guid isPermaLink="false">hacker-news-small-sites-24649026</guid>
            <pubDate>Thu, 01 Oct 2020 09:26:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why automate secrets scanning throughout your SDLC?]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24649022">thread link</a>) | @oodelally
<br/>
October 1, 2020 | https://www.gitguardian.com/secrets-detection/secrets-detection-application-security | <a href="https://web.archive.org/web/*/https://www.gitguardian.com/secrets-detection/secrets-detection-application-security">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>My source code is private, so why is hardcoding credentials in git considered a bad practice?</p><div><p>While private repositories offer some level of protection for your code they still do not have adequate protection to store information as sensitive as secrets. </p><p>Imagine if there was a plain text file with all your credit card numbers within it, you hopefully wouldn’t put this into the companies git repository. Secrets are just as sensitive.</p><p>A few things to consider when storing secrets in private repositories:</p></div><div><p>Source code is made to be duplicated and distributed, therefore lives in multiple places. Source code is a leaky asset and you never know where it is going to end up. It can be cloned to a compromised workstation or server, intentionally or accidentally published in whole or in part, uploaded to your website, released to a customer, pasted in Slack, or end up in your package manager or mobile application...</p></div><div><p>It would just take one compromised developer account to compromise all the secrets they have access to.</p></div><div><p>Hardcoded credentials make it impossible to know what secrets a developer accessed, and very difficult to rotate keys.</p></div><p>Why automate secrets scanning throughout the Software Development Life Cycle (SLDC)?</p><p>While DevOps, Continuous Integration and Continuous Delivery speed up software development, they can also significantly increase security risks.<br></p><p>“DevOps is rapid and requires lots of small, iterative changes. But this increases complexity and opens up a new set of security problems. With DevOps, existing security vulnerabilities can be magnified and manifest themselves in new ways. The speed of software creation can mean new vulnerabilities are created unseen by developers. The solution is to build security monitoring into the DevOps process from the start.”<br></p><p>Greg Day, RSA Conference Organizer (<a href="https://www.securityroundtable.org/the-biggest-cybersecurity-risks-in-2020/" target="_blank">source</a>)<br></p><p>Security now needs to be part of the SDLC from the start, and part of every incremental change. This concept is called Shifting Left, a development principle which states that security should move from the right (or end) of the SDLC to the left (the beginning). A great deal of automation is needed in order to be able to cope with running security checks at each incremental change. &nbsp;Automation helps streamline the detection, alerting and remediation processes and workflows.<br></p><p>How does secrets detection compare with Static Application Security Testing (SAST)?</p><div><p>Secrets detection is often confused with SAST because both scan through static source code. </p><p>SAST is mostly testing control structure, input validation, error handling, etc. Such vulnerabilities like SQL injection vulnerabilities only express themselves the moment the code is deployed. Exposed secrets are unlike these vulnerabilities, because any secret reaching version control system must be considered compromised and requires immediate attention. This is true even if the code is never deployed. </p><p>Implementing secrets detection is not only about scanning the most actual version of your master branch before deployment. It is also about scanning through every single commit of your git history, covering every branch, even development or test ones.</p><p>To conclude, SAST is concerned only with the current version of a project, the version that is going to be deployed, whereas secrets detection is concerned with the entire history of the project.</p></div><p>What are git hooks?</p><div><p>Git hooks are scripts that are triggered by certain actions in the software development process, like committing or pushing. By automatically pointing out issues in code, they allow reviewers not to waste time on mistakes that can be easily diagnosed by a machine. </p><p>There are client-side hooks, that execute locally on the developers’ workstation, and server-side hooks, that execute on the centralized version control system.</p><p>Examples of client-side hooks:</p></div><p>Examples of server-side hooks:<br></p><p>What is a pre-commit hook?</p><div><p>"The pre-commit hook is run first when committing, before you even type in a commit message. It’s used to inspect the snapshot that’s about to be committed, to see if you’ve forgotten something, to make sure tests run, or to examine whatever you need to inspect in the code. </p><p>Exiting non-zero from this hook aborts the commit, although you can bypass it with <span>git commit --no-verify</span>. You can do things like check for code style (run lint or something equivalent), check for trailing whitespace, or check for appropriate documentation on new methods."</p></div><p>Source: <a href="https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks" target="_blank">git-scm</a><br></p><p>What is a pre-receive hook?</p><div><p>A pre-receive hook is a script that runs on the server. It performs checks on the content of the push; if it exits non-zero, the push is rejected. You can use this hook to do things like prevent a PR author from merging their own changes, or require commit messages to follow some specific guidelines. </p><p>Be careful when using pre-receive hooks as they are blocking: if the checks don’t pass, the server is not updated.</p></div><p>What is a post-receive hook?</p><p>"The post-receive hook runs after the entire process of pushing code to the server is completed and can be used to update other services or notify users. Examples include emailing a list, notifying a continuous integration server, or updating a ticket-tracking system – you can even parse the commit messages to see if any tickets need to be opened, modified, or closed. This script can’t stop the push process, but the client doesn’t disconnect until it has completed, so be careful if you try to do anything that may take a long time."<br></p><p>Source: <a href="https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks" target="_blank">git-scm</a><br></p><p>Unlike the pre-receive hook, post-receive is non-blocking.<br></p><p>Where in the DevOps pipeline to implement automated secrets scanning? Client-side or server-side?</p><div><p>The earlier a security vulnerability is uncovered, the less costly it is to correct. Hardcoded secrets are no exceptions. If the secret is uncovered after the secret reaches centralized version control server-side, it must be considered compromised, which requires rotating (revoking and redistributing) the exposed credential. This operation can be complex and typically involves multiple stakeholders.</p><p>Client-side secrets detection early in the software development process is a nice-to-have as it will prevent secrets entering the VCS earlier. </p><p>Server-side secrets detection is a must have:</p></div><div><p>&nbsp;Server-side is where the ultimate threat lies. From the server, the code can uncontrollably spread in a lot of different places, with the hardcoded secrets in it.</p></div><div><p>Implementing client-side hooks on an organization level is hard. This is something we have heard many application security professionals claim they are not confident to do, due to the difficulty to deploy and update this on every developer’s workstation.</p></div><div><p>Client-side hooks can and must be easy to bypass (remember secret detection is probabilistic). Usage of client side hooks largely comes down to the individual developers’ responsibility. Hence the need to have visibility over the later stages.</p></div><p>Should secrets detection be blocking or non-blocking in the SDLC?</p><div><p>From our experience, when trying to impose rules that are too constraining, people will bend them, often in an effort to collaborate better and do their job. Security must not be a blocker. It should allow flexibility and enable information to flow, yet enable visibility and control. </p><p>On one hand, security measures will be bypassed, sometimes for the worst. But on the other hand, it is also good sometimes that the developer can take the responsibility to bypass them. </p><p>Because even the best algorithms can fail and need human judgement. Secrets detection is probabilistic: algorithms achieve a tradeoff between not raising false alerts and not missing keys.</p></div></div></div>]]>
            </description>
            <link>https://www.gitguardian.com/secrets-detection/secrets-detection-application-security</link>
            <guid isPermaLink="false">hacker-news-small-sites-24649022</guid>
            <pubDate>Thu, 01 Oct 2020 09:25:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Paralyzed Dutch man can temporarily move and talk again thanks to sleeping pill]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24648831">thread link</a>) | @jacquesm
<br/>
October 1, 2020 | https://www.world-today-news.com/paralyzed-dutch-man-can-temporarily-move-and-talk-again-thanks-to-sleeping-pill-now/ | <a href="https://web.archive.org/web/*/https://www.world-today-news.com/paralyzed-dutch-man-can-temporarily-move-and-talk-again-thanks-to-sleeping-pill-now/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>A Dutch man who has been unable to move and speak for eight years due to brain damage, can temporarily do so after taking the sleeping drug Zolpidem.  Brain scans show that the sleeping pill removes an obstacle to these actions, scientists from Radboudumc and Amsterdam UMC report Thursday.</p>
<p>In men, the brain activity for moving and talking is drowned out by the brain activity for the transfer of information.</p>
<p>“You could compare the brain, as it were, to a large string orchestra”, explains fellow researcher Hisse Arnts.  “With Richard (the man, ed.) The first violins play so loud that they drown out the other members of the string orchestra and people can no longer hear each other. Zolpidem ensures that these first violins play more ‘pianissimo’, so that everyone back within time. “</p>
<p>The fact that the man does not go to sleep because of Zolpidem is probably due to the way his brain has become confused, Arnts thinks.  The problem, however, is that the man gets used to the sleeping aid, so that the effect becomes shorter and shorter.  Yet he still receives it regularly and the discovery is a promising starting point for further research.</p>
<p>The man is not the first to return to a normal situation briefly with such a means;  there are similar reports from Italy and South Africa.  But how that is possible has now been determined for the first time on the basis of scans, according to the medical centers.</p>

<p>The man, in his late twenties at the time, suffered a severe lack of oxygen eight years ago due to choking.  He ended up in a nursing home with the very rare diagnosis of akinetic mutism.  Because Richard’s situation seemed hopeless, it was decided to give him the remedy.</p>

</div><p>    .</p></div>]]>
            </description>
            <link>https://www.world-today-news.com/paralyzed-dutch-man-can-temporarily-move-and-talk-again-thanks-to-sleeping-pill-now/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24648831</guid>
            <pubDate>Thu, 01 Oct 2020 08:48:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Asteroids: By the Numbers (2019)]]>
            </title>
            <description>
<![CDATA[
Score 40 | Comments 10 (<a href="https://news.ycombinator.com/item?id=24648639">thread link</a>) | @rbanffy
<br/>
October 1, 2020 | http://www.retrogamedeconstructionzone.com/2019/10/asteroids-by-numbers.html?m=1 | <a href="https://web.archive.org/web/*/http://www.retrogamedeconstructionzone.com/2019/10/asteroids-by-numbers.html?m=1">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div id="post-body-3855344372715435262"><p>
By 1979, arcade games were rapidly becoming more complex and colorful, and a game like <i>Asteroids </i>might have seemed quaint by comparison to the likes of <i><a href="http://www.retrogamedeconstructionzone.com/2019/09/galaxian-aesthetics-of-simple-patterns.html">Galaxian</a>, <a href="http://www.retrogamedeconstructionzone.com/2019/09/star-fire-weirdness-of-pseudo-3d.html">Star Fire</a></i>, or <i>Radar Scope.&nbsp; </i>But beneath its simple exterior lies a challenging shooter with surprisingly complex physics.&nbsp; The image below shows a sample still from it, including the player's ship (the triangle on the left), three sizes of asteroids, and an alien ship.</p><p><a href="https://1.bp.blogspot.com/-XOyg91qwUV4/XZwhbHR4tnI/AAAAAAAAOXs/DJ89MzjAODU3mAoraZKUaq6vl9nKvdsSACLcBGAsYHQ/s1600/Asteroids_sample%2B%25282%2529.png"><img data-original-height="461" data-original-width="598" height="492" src="https://1.bp.blogspot.com/-XOyg91qwUV4/XZwhbHR4tnI/AAAAAAAAOXs/DJ89MzjAODU3mAoraZKUaq6vl9nKvdsSACLcBGAsYHQ/s640/Asteroids_sample%2B%25282%2529.png" width="640"></a></p>

<p>
The goal of the game is to maximize your score, destroying as many asteroids and aliens as possible before you run out of ships.&nbsp; When an asteroid is hit by something (usually the player's bullets), large asteroids turn into two medium ones and medium asteroids turn into two small ones, while small asteroids and aliens are destroyed when hit.</p><p><a href="https://1.bp.blogspot.com/-cEzHmkcz3bA/XaU1zGl7IBI/AAAAAAAAObY/2GDlGaxtEc0ytzWpyQRinUxnbioeVsTbQCLcBGAsYHQ/s1600/Asteroids%2BPlay.gif"><img data-original-height="357" data-original-width="600" height="237" src="https://1.bp.blogspot.com/-cEzHmkcz3bA/XaU1zGl7IBI/AAAAAAAAObY/2GDlGaxtEc0ytzWpyQRinUxnbioeVsTbQCLcBGAsYHQ/s400/Asteroids%2BPlay.gif" width="400"></a></p>
<p>
For the designers of the game, balancing the relative sizes of the objects would have been important in such a dense field, because it determines how often things run into one another other.&nbsp; The table below outlines the sizes of the objects in <i>Asteroids</i>, expressed relative to the length of the player's ship.</p><table>
<caption>The approximate horizontal lengths of objects in <i>Asteroids</i></caption>
    <tbody>
<tr>
    <th>Object</th>
    <th>Length (in player ship lengths)</th>
    </tr>
<tr>
    <td>Screen</td>
    <td>25 x 36</td>
    </tr>
<tr>
    <td>Large Asteroids</td>
    <td>2.4</td>
    </tr>
<tr>
    <td>Medium Asteroid</td>
    <td>1.2</td>
    </tr>
<tr>
    <td>Small Asteroid</td>
    <td>0.6</td>
    </tr>
<tr>
    <td>Alien Ship (large)</td>
    <td>1.5</td>
    </tr>
<tr>
    <td>Alien Ship (small)</td>
    <td>0.75</td>
    </tr>
</tbody>
    </table>
<p>
Notice how the medium asteroid is twice the size of the small one, and the large is twice the size of the medium.&nbsp; If you think about it, this doesn't actually make much physical sense -- you can break a two-dimensional object into four pieces half its size, not two.&nbsp; But the size ratios do make game sense, because what really matters to the player is not how much area an asteroid takes up, but how likely it is to hit them.</p><p><a href="https://1.bp.blogspot.com/-5wVPSIS6W_M/XZwmjsjcYII/AAAAAAAAOX4/a7MThKiGRH8vhydf83eCiOmmJuKUULHIwCLcBGAsYHQ/s1600/Asteroids_cross_section%2B%25282%2529.png"><img data-original-height="281" data-original-width="170" src="https://1.bp.blogspot.com/-5wVPSIS6W_M/XZwmjsjcYII/AAAAAAAAOX4/a7MThKiGRH8vhydf83eCiOmmJuKUULHIwCLcBGAsYHQ/s1600/Asteroids_cross_section%2B%25282%2529.png"></a></p>

<p>
In the picture above, you can see that while the four small asteroids take up less area on the screen than the big one, they present the same cross section to your ship.&nbsp; This means that breaking up the asteroids doesn't greatly increase the probability that the player will be struck by one.&nbsp; If the asteroids had been broken up into four half-sized pieces at each blow, each large asteroid would result in sixteen small ones (that's four times the cross section!) and the player would be quickly overwhelmed.&nbsp; Note that the small asteroids do move faster than the large ones, and speed increases the chance of getting hit, but I'll return to that in a moment.</p>

<p>
Another big reason that size matters in <i>Asteroids</i>&nbsp;is that it determines how close something has to be before you're likely to be able to hit it.&nbsp; In 1979, vector arcade cabinet screens had an effective resolution of 1024 x 768, and the player's ship was only connected graphically by about 20 points.&nbsp; This meant, in turn, that there were only a limited number of orientations that they could render for the ship, in this case intervals of 5 degrees.&nbsp; To see how this might affect gameplay, imagine your ship is located at the fixed position shown in the picture below.&nbsp; Anything located entirely between the two solid lines will not be accessible by your bullets because your ship can't turn to the appropriate angle to hit it.&nbsp; This restriction isn't a big deal for large asteroids, which are still accessible over most of the screen, but small asteroids can be out of reach of your guns at relatively close range, sometimes as close as 7 ship lengths away!&nbsp; So even if you aim as accurately as possible, chances are you won't be able to hit a small asteroid on the other side of the screen, at least not on your first shot.&nbsp; This is probably one of the reasons that <i>Asteroids</i>&nbsp;allows you to fire up to four bullets in succession: even if you can't hit an object right away, it will eventually move into your line of fire.</p>

<p><a href="https://1.bp.blogspot.com/-EX2V1tAwdnM/XZvKFmnQZoI/AAAAAAAAOXI/cd411AFReLcKkCQGooANzsmxVD7QV3ikgCLcBGAsYHQ/s1600/Asteroids_hittable%2B%25285%2529.png"><img data-original-height="397" data-original-width="1024" height="248" src="https://1.bp.blogspot.com/-EX2V1tAwdnM/XZvKFmnQZoI/AAAAAAAAOXI/cd411AFReLcKkCQGooANzsmxVD7QV3ikgCLcBGAsYHQ/s640/Asteroids_hittable%2B%25285%2529.png" width="640"></a></p>
<p>
And what about speed?&nbsp; There is some variability in the speeds of individual asteroids, but small asteroids can move as much as 63% faster than large ones (see the table below), meaning that even if four small asteroids present the same cross section for hitting your ship as a large one does, their higher speed means they're more likely to hit you.&nbsp; The reasoning for this is simple: the faster something moves, the longer the path it can traverse in a given amount of time, and the larger the likelihood that you'll be in that path.&nbsp; Fortunately, even the fastest asteroids are much slower than your ship, which can traverse the screen in about two seconds; so if you're skilled enough, you should be able to maneuver around the asteroid field without a problem.</p><table>
<caption>The approximate speeds of objects in <i>Asteroids</i></caption>
    <tbody>
<tr>
    <th>Object</th>
    <th>Speed (in ship lengths per second)</th>
    </tr>
<tr>
    <td>Your ship</td>
    <td>0 - 17</td>
    </tr>
<tr>
    <td>Asteroids</td>
    <td>4 - 6.5</td>
    </tr>
<tr>
    <td>Alien ships (both sizes)</td>
    <td>4 - 6.5 (depending on your score)</td>
    </tr>
<tr>
    <td>Bullets</td>
    <td>17 (ship at rest)&nbsp;</td></tr>
</tbody></table>
<p>
One other interesting thing about speed: notice in the table above that the speed of a bullet, when fired at rest, is the same as your ship's maximum speed.&nbsp; This means that when you're moving rapidly and fire a bullet in the opposite direction of your motion, the two speeds should cancel for a stationary observer and the bullet should appear roughly stationary.</p>

<p><a href="https://1.bp.blogspot.com/-pRIxsTPNfSY/XaD7O0foijI/AAAAAAAAOZQ/JWZGFR26JEY3UM43RiruPhhGfHOCW21DgCLcBGAsYHQ/s1600/Asteroids_bullet_stationary.gif"><img data-original-height="175" data-original-width="468" height="239" src="https://1.bp.blogspot.com/-pRIxsTPNfSY/XaD7O0foijI/AAAAAAAAOZQ/JWZGFR26JEY3UM43RiruPhhGfHOCW21DgCLcBGAsYHQ/s640/Asteroids_bullet_stationary.gif" width="640"></a></p>


<div><p>
Sure enough, when I fire a bullet while moving quickly in the opposite direction, it just floats near where I fired it, allowing me to place bullets like mines for the asteroids to run into.</p>
<p>
The developers of <i>Asteroids </i>wanted the game to simulate the real laws of physics (<a href="http://www.technologyuk.net/computing/computer-gaming/gaming-landmarks-1960-1985/asteroids.shtml">see here</a>), and I think in many respects they succeeded, at least compared to most other arcade games of the time.&nbsp; It's not entirely realistic, however.&nbsp; In my article on <a href="http://www.retrogamedeconstructionzone.com/2019/10/impossible-motion-in-video-games.html">motion in early arcade games</a>, I discussed how the acceleration of the ship in <i>Asteroids</i> is too rapid for a human-occupied vehicle.&nbsp; We might suppose that the vehicle is automated, but even then, engineering spacecraft to withstand 30+ g's of acceleration is a non-trivial challenge.</p><p><a href="https://1.bp.blogspot.com/-eBpcOCL_1a0/XaU-bXjLJjI/AAAAAAAAObk/N_FxduJSebIMaqzrq7BhDmNFufzmyBRfgCLcBGAsYHQ/s1600/Acceleration%2BAsteroids.gif"><img data-original-height="175" data-original-width="634" height="176" src="https://1.bp.blogspot.com/-eBpcOCL_1a0/XaU-bXjLJjI/AAAAAAAAObk/N_FxduJSebIMaqzrq7BhDmNFufzmyBRfgCLcBGAsYHQ/s640/Acceleration%2BAsteroids.gif" width="640"></a></p>
<p>
Another issue is the way the asteroids break up.&nbsp; In physics, there's a principle called the conservation of momentum that says that when objects interact with one another or break apart, the total mass times the velocity must remain the same after the interaction as it was before.&nbsp; In layman's terms, this means that things can't suddenly go from moving one direction to moving in another unless there is something else carry its previous momentum.&nbsp; In the example shown below, an asteroid does just that, and the only thing that could have absorbed its previous momentum is the bullet.&nbsp; But the bullet is so tiny that it's difficult to imagine how that would be possible.</p><p><a href="https://1.bp.blogspot.com/-vsDHHIPNXrQ/XaEAAKYXRII/AAAAAAAAOZc/-lhbXy0TSV4nEiLvOf-pe7zyt9sPGLcEACLcBGAsYHQ/s1600/Asteroids_momentum.gif"><img data-original-height="248" data-original-width="533" height="185" src="https://1.bp.blogspot.com/-vsDHHIPNXrQ/XaEAAKYXRII/AAAAAAAAOZc/-lhbXy0TSV4nEiLvOf-pe7zyt9sPGLcEACLcBGAsYHQ/s400/Asteroids_momentum.gif" width="400"></a></p>

<p>
These are mere quibbles, however, and shouldn't dissuade you from giving the game a try.&nbsp; <i>Asteroids </i>ended up being one of Atari's biggest arcade hits, selling over 70,000 cabinets, and remains one of their most recognizable games to this day.&nbsp; Many gamers who grew up in the '80s, myself included, are more familiar with the Atari 2600 version of the game.&nbsp; Unfortunately, the designers had to make a lot of sacrifices in game physics in order to make it work on a home console, so I think the arcade version is really the way to go.&nbsp; Outside of visiting a vintage arcade, your best bet is probably the <a href="https://www.mamedev.org/">Multiple Arcade Machine Emulator</a> (MAME).&nbsp; Happy hunting!</p></div><div><p><span>NOTE: &nbsp;A previous version of this article stated that the game has limited pixel resolution, but <i>Asteroids </i>uses a vector display that is not composed of pixels. &nbsp;The resolution of the <i>Asteroids </i>vector display is 1024 x 768.</span></p><p><a href="https://1.bp.blogspot.com/-7gLKzZ6gULM/XaDx0uT70-I/AAAAAAAAOYo/aLUcLQJkl9AY1k2OYsewuUHydaxbkT0gwCLcBGAsYHQ/s1600/Asteroids_loop_transparent.gif"><img data-original-height="151" data-original-width="600" height="160" src="https://1.bp.blogspot.com/-7gLKzZ6gULM/XaDx0uT70-I/AAAAAAAAOYo/aLUcLQJkl9AY1k2OYsewuUHydaxbkT0gwCLcBGAsYHQ/s640/Asteroids_loop_transparent.gif" width="640"></a></p>
<br></div>
</div>
</div></div>]]>
            </description>
            <link>http://www.retrogamedeconstructionzone.com/2019/10/asteroids-by-numbers.html?m=1</link>
            <guid isPermaLink="false">hacker-news-small-sites-24648639</guid>
            <pubDate>Thu, 01 Oct 2020 08:15:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Russian opposition leader Alexei Navalny describes his poisoning with Novichok]]>
            </title>
            <description>
<![CDATA[
Score 18 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24648502">thread link</a>) | @FrojoS
<br/>
October 1, 2020 | https://www.spiegel.de/international/world/alexei-navalny-thanks-germany-and-describes-his-poisoning-with-novichok-a-f1249540-76f7-43ce-aa14-358d71256684 | <a href="https://web.archive.org/web/*/https://www.spiegel.de/international/world/alexei-navalny-thanks-germany-and-describes-his-poisoning-with-novichok-a-f1249540-76f7-43ce-aa14-358d71256684">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-article-el="body">
<section data-app-hidden="true">


</section>
<section>
<div>
<figure data-component="Image" data-settings="{&quot;id&quot;:&quot;83eb2320-0da2-4c9c-971a-560530cea088&quot;, &quot;zoomable&quot;:true,&quot;zoomId&quot;:&quot;90b9f427-fb52-48a9-b441-009431a3d1a6&quot;}">
<p><span>
<span data-image-el="aspect">
<span>
<img data-image-el="img" src="https://cdn.prod.www.spiegel.de/images/83eb2320-0da2-4c9c-971a-560530cea088_w948_r1.77_fpx28.11_fpy49.98.jpg" srcset="https://cdn.prod.www.spiegel.de/images/83eb2320-0da2-4c9c-971a-560530cea088_w520_r1.77_fpx28.11_fpy49.98.jpg 520w, https://cdn.prod.www.spiegel.de/images/83eb2320-0da2-4c9c-971a-560530cea088_w948_r1.77_fpx28.11_fpy49.98.jpg 948w" width="948" height="536" sizes="948px" title="Alexei Nawalny" alt="Alexei Nawalny">
</span>
</span>
</span>

</p>
<figcaption>
<p>Alexei Nawalny</p>
<span>
Foto: Peter Rigaud&nbsp;/ DER SPIEGEL
</span>
</figcaption>
</figure>
</div><div>
<p>In his first media interview following his poisoning, Russian opposition leader Alexei Navalny expresses his "tremendous gratitude to all Germans" in an interview with the newsmagazine DER SPIEGEL. In the interview, he says that although he never had close ties to Germany, it was German politicians and Chancellor Angela Merkel who saved his life. He says the doctors at Berlin's Charité University Hospital saved his life for a second time and nursed him back to health. "I know it sounds a bit over the top, but Germany has become a special country for me," Navalny says. Describing the personal visit paid to him by Merkel last week, he says: "I was impressed by the detail she knows about <a href="https://www.spiegel.de/thema/russia_en/" data-link-flag="english">Russia</a> and my case."</p>



<section data-area="contentbox">

</section>
<p>Navalny says he is doing "much better than three weeks ago, and things are getting better each day." The doctors say he could get back to 90 percent of his former self, perhaps even 100 percent, although no one knows for sure. "Basically, I’m a bit of a guinea pig," he says. "There aren’t many people you can observe who are still alive after being poisoned with a nerve agent." Describing the moment in the airplane when the poison started to take effect, Navalny says: "You feel no pain, but you know you’re dying. And I mean, right now. Even though nothing hurts you." You just think: This is the end. "Organophosphorus compounds attack your nervous system like a DDos attack attacks the computer - it's an overload that breaks you," he told DER SPIEGEL.</p>

<div>
<p>"I assert that <a href="https://www.spiegel.de/thema/vladimir_putin_en/" data-link-flag="english">Putin</a> was behind the crime, and I have no other explanation for what happened." Despite the attempt on his life, he says he wants to return to Russia. "And my job now is to remain the guy who isn’t afraid. And I’m not afraid! When my hands shake, it’s not from fear – it’s from this stuff. I would not give Putin the gift of not returning to Russia."</p><p>When asked whether he thinks Germany should stop the completion of the Nord Stream 2 Russian gas pipeline project, Navalny didn’t want to answer. "That’s Germany’s business. Decide for yourself!" Any strategy toward Russia must "take into account the level of insanity that Putin has reached."</p>
</div>

<div>
<p><em>The full interview with Navalny will be posted in English later today.</em></p>
<p><span><svg aria-labelledby="title-f2f6c24e-2448-4a58-81af-d99a30576d47" width="10" height="20" viewBox="0 0 10 20" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><title id="title-f2f6c24e-2448-4a58-81af-d99a30576d47">Icon: Der Spiegel</title><g id="l-s-flag-f2f6c24e-2448-4a58-81af-d99a30576d47"><path id="vector-f2f6c24e-2448-4a58-81af-d99a30576d47" d="M9.85 16.293v-8H3.212V4.667h3.533v2.24h3.212v-3.2C9.85 2.747 8.993 2 8.03 2H1.713C.749 2 0 2.747 0 3.707v7.253h6.638v4.373H3.105v-2.986H0v3.84c0 .96.75 1.706 1.713 1.706H8.03c.963.107 1.82-.64 1.82-1.6z" fill="#000"></path></g></svg>
</span>
</p></div>

</div>
</section>

</div></div>]]>
            </description>
            <link>https://www.spiegel.de/international/world/alexei-navalny-thanks-germany-and-describes-his-poisoning-with-novichok-a-f1249540-76f7-43ce-aa14-358d71256684</link>
            <guid isPermaLink="false">hacker-news-small-sites-24648502</guid>
            <pubDate>Thu, 01 Oct 2020 07:54:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Baremetal programming on the tinyAVR 0 micro-controllers]]>
            </title>
            <description>
<![CDATA[
Score 105 | Comments 95 (<a href="https://news.ycombinator.com/item?id=24648397">thread link</a>) | @unwind
<br/>
October 1, 2020 | https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers | <a href="https://web.archive.org/web/*/https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><section><div><h4>All you need is a text editor, a makefile and a USB-serial cable.</h4></div></section><section><div><p><img src="https://www.omzlo.com/uploads/std_attiny-406-macro.jpg" alt=""></p>

<h2 id="introduction">Introduction</h2>

<p>Are you an 8-bit or a 32-bit programmer? </p>

<p>At OMZLO, we have been mainly focussing our development efforts on newer 32-bit Arm-Cortex chips (STM32 and SAMD), which typically offer more RAM, more speed, more peripherals, at a similar or lower price-point than older 8-bit MCUs. But 8-bit MCUs are far from dead. Microchip has notably released a new series of chips, collectively branded as the "tinyAVR 0-series", which offer more modern peripherals than the older AVR chips at a very competitive price point. They seem like the perfect candidate for simple products that don't need all the features and fine-tuning capabilities of the newer 32-bit MCUs. 8-bit MCUs are also substantially simpler to program, which translates into faster development time. </p>

<p>Thanks to the success of the Arduino UNO, there are tons of tutorials online that explain how to program 8-bit Atmega328 microcontrollers and their cousins like the Attiny85 using direct register access, without the Arduino language or any vendor IDE such as Atmel Studio. Just google "atmega328 blinky". All you need is an AVR C-compiler, a text editor, <a href="https://www.nongnu.org/avrdude/">avrdude</a>, and an AVR programmer. Some <a href="https://www.instructables.com/id/Getting-Started-With-the-ATMega328P/">resources</a> even show how to also build the electronics needed to get a basic atmega328 running on a breadboard. However, it's hard to find the same information for these newer "tinyAVR 0" chips.</p>

<p>Of course, Microchip offers all the tools necessary to program these newer "TinyAVR" MCUs with their windows-only IDE. There are also "Arduino cores" for some of these newer "TinyAVR" MCUs that let you program them with the Arduino IDE. But again, if you like to write code for MCUs in "baremetal" style, with your favorite text editor, a makefile, and a c-compiler, there are few resources available online. </p>

<p>In this blog post, we will describe how to program a <a href="https://www.ladyada.net/learn/proj1/blinky.html">blinky</a> firmware on an <strong>Attiny406</strong>, from the ground up, using the simplest tools. Most of the things described here can be easily transposed to other TinyAVR MCUs. Our approach is generally guided toward macOS or Linux users, but should also be applicable in an MS-Windows environment with a few minor changes.</p>

<h2 id="hardware">Hardware</h2>

<p>We decided to play with the <a href="https://www.microchip.com/wwwproducts/en/ATTINY406">Attiny406</a>, with a view of using it in the future to replace the Attiny45 we currently use on the <a href="https://www.omzlo.com/articles/the-piwatcher">PiWatcher</a>, our Raspberry-Pi watchdog. The Attiny406 has 4K of flash space, 256 bytes of RAM, and can run at 20Mhz without an external clock source.</p>

<p>One of the most important differences between the new TinyAVR MCUs and the older classic AVR MCU like the Attiny85 is that the newer chips use a different programming protocol called UPDI, which requires only 3 pins, as opposed to the 6-pin ISP on the classic AVRs.</p>

<p>A little research shows that programming TinyAVRs with UPDI can be achieved with a simple USB-to-serial cable and a resistor, thanks to a python tool called <a href="https://github.com/mraardvark/pyupdi">pyupdi</a>, which suggests the following connection diagram for firmware upload:</p>
<pre>                        Vcc                     Vcc
                        +-+                     +-+
                         |                       |
 +---------------------+ |                       | +--------------------+
 | Serial port         +-+                       +-+  AVR device        |
 |                     |      +----------+         |                    |
 |                  TX +------+   4k7    +---------+ UPDI               |
 |                     |      +----------+    |    |                    |
 |                     |                      |    |                    |
 |                  RX +----------------------+    |                    |
 |                     |                           |                    |
 |                     +--+                     +--+                    |
 +---------------------+  |                     |  +--------------------+
                         +-+                   +-+
                         GND                   GND
</pre>
<h3 id="shematic">shematic</h3>

<p>We created a minimalistic breakout board for the Attiny406. The board can be powered by 5V through USB or a lower 3.3V through dedicated VCC/GND pins. An LED and a button were also fitted on the board. For testing purposes, we decided to embed the 4.7K resistor needed for the UPDI programming directly in the hardware (i.e. resistor <em>R2</em>). 
This gives us the following schematic:</p>

<p><img src="https://www.omzlo.com/uploads/attiny406-schematic.png" alt="schematic"></p>

<h3 id="board">Board</h3>

<p>The resulting breakout board is tiny and fits conveniently on a small breadboard. The design files are <a href="https://aisler.net/p/SIEGFIYL">shared on aisler.net</a>.</p>

<p><img src="https://www.omzlo.com/uploads/std_attiny-406-breadboard.jpg" alt=""></p>

<p>Programming the Attiny406 on the board with a USB-serial cable is done by connecting the headers on the board edge:</p>

<p><img src="https://www.omzlo.com/uploads/attiny406-prog.png" alt=""></p>

<h2 id="software">Software</h2>

<h3 id="pyudpi">pyudpi</h3>

<p>We installed <strong>pyupdi</strong> following the instructions provided on <a href="https://github.com/mraardvark/pyupdi">their webpage</a>. </p>

<p>We connected our USB-Serial cable to the board with the 4 dedicated UPDI pins available on the board. Our USB-Serial converter shows up as the file <code>/dev/tty.usbserial-FTF5HUAV</code> on a MacOS system.</p>

<p>To test that the programmer recognizes the Attiny406, you can issue a command similar to the following, adapting the path for the USB-serial converter to your setup:</p>
<pre>pyupdi -d tiny406 -c /dev/tty.usbserial-FTF5HUAV -i
</pre>
<p>This should result in the following output if all goes well:</p>
<pre>Device info: {'family': 'tinyAVR', 'nvm': 'P:0', 'ocd': 'D:0', 'osc': '3', 'device_id': '1E9225', 'device_rev': '0.1'}
</pre>
<h3 id="the-c-compiler">The C compiler</h3>

<p>The typical <strong>avr-gcc</strong> available on macOS with <a href="https://brew.sh/">homebrew</a> did not seem to recognize the Attiny406 as a compiler target, so we went off to install the avr-gcc compiler provided by Microchip, which is available <a href="https://www.microchip.com/mplab/avr-support/avr-and-arm-toolchains-c-compilers">here</a>. Downloading the compiler requires you to create an account on the Microchip website, which is a bit annoying. </p>

<p><img src="https://www.omzlo.com/uploads/avr-toolchain.png" alt="AVR toolchain link"></p>

<p>Once downloaded, we extracted the provided archive in a dedicated directory. The <code>bin</code> directory in the archive should be added to the <code>PATH</code> variable to make your life easier. Assuming the downloaded compiler is stored in the directory <code>$HOME/Src/avr8-gnu-toolchain-darwin_x86_64</code>, the PATH can be altered by adding the following line to your <code>.bash_profile</code> file:</p>
<pre>export PATH=$PATH:$HOME/Src/avr8-gnu-toolchain-darwin_x86_64/bin/
</pre>
<p>Newer Attiny MCUs are not supported out of the box by the Microchip <strong>avc-gcc</strong> compiler. You need to download a dedicated <em>Attiny Device Pack</em> from <a href="http://packs.download.atmel.com/">their website</a>, as shown below:</p>

<p><img src="https://www.omzlo.com/uploads/microchip-pack-repository.png" alt="AVR toolchain link"></p>

<p>The resulting downloaded <em>Device Pack</em> is named <code>Atmel.ATtiny_DFP.1.6.326.atpack</code> (or similar depending on versioning). Though the extension is <code>.atpack</code>, the file is actually a zip archive. We changed the extension to <code>.zip</code> and extracted the package in the directory <code>$HOME/Src/Atmel.ATtiny_DFP.1.6.326</code> next to the compiler files. </p>

<h3 id="c-program">C program</h3>

<p>We created the following program that blinks the LED on pin PB5 of our Attiny board at a frequency of 1Hz.</p>
<pre><span>#include &lt;avr/io.h&gt;
#include &lt;util/delay.h&gt;
</span>
<span>int</span> <span>main</span><span>()</span> <span>{</span>
    <span>_PROTECTED_WRITE</span><span>(</span><span>CLKCTRL</span><span>.</span><span>MCLKCTRLB</span><span>,</span> <span>0</span><span>);</span> <span>// set to 20Mhz (assuming fuse 0x02 is set to 2)</span>

    <span>PORTB</span><span>.</span><span>DIRSET</span> <span>=</span> <span>(</span><span>1</span><span>&lt;&lt;</span><span>5</span><span>);</span>
    <span>for</span> <span>(;;)</span> <span>{</span>
        <span>PORTB</span><span>.</span><span>OUTSET</span> <span>=</span> <span>(</span><span>1</span><span>&lt;&lt;</span><span>5</span><span>);</span>
        <span>_delay_ms</span><span>(</span><span>500</span><span>);</span>
        <span>PORTB</span><span>.</span><span>OUTCLR</span> <span>=</span> <span>(</span><span>1</span><span>&lt;&lt;</span><span>5</span><span>);</span>
        <span>_delay_ms</span><span>(</span><span>500</span><span>);</span>
    <span>}</span>
<span>}</span>
</pre>
<p>The code looks very similar to what you would see on a classic AVR "blinky" program. One visible change is the use of structures to access various registers of the MCU: e.g instead of setting bits in <code>PORTB</code>, you access <code>PORTB.DIRSET</code>.</p>

<p>The other visible change is the clock setup code <code>_PROTECTED_WRITE(CLKCTRL.MCLKCTRLB, 0)</code>. Out of the box, at reset, the Attiny406 runs at 3.33Mhz, which corresponds to a base frequency of 20Mhz with a 6x clock divider applied. To enable the full 20Mhz speed, the register <code>CLKCTRL.MCLKCTRLB</code> is cleared. Because this register needs to be protected against accidental changes, the Attiny406 requires a specific programming sequence to modify it. Fortunately, this is natively offered by the macro <code>_PROTECTED_WRITE</code>. More details are available in the <a href="http://ww1.microchip.com/downloads/en/DeviceDoc/ATtiny406-DataSheet-DS40001976B.pdf">Attiny406 datasheet</a>.</p>

<p>In comparison with an STM32 or a SAMD21, the code is blissfully simple. </p>

<h3 id="makefile">Makefile</h3>

<p>We assume the following directory structure where:</p>

<ul>
<li><code>Src/Atmel.ATtiny_DFP.1.6.326/</code> is the location of the Microchip <em>Device Pack</em></li>
<li><code>Src/attiny406-test/</code> is the directory where the code above is stored in a file called <code>main.c</code></li>
</ul>

<p>Compiling the code can be done by issuing the following command within <code>attiny406-test/</code> directory,:</p>
<pre>avr-gcc -mmcu=attiny406 -B ../Atmel.ATtiny_DFP.1.6.326/gcc/dev/attiny406/ -O3 -I ../Atmel.ATtiny_DFP.1.6.326/include/ -DF_CPU=20000000L -o attiny406-test.elf main.c
</pre>
<p>An <code>-O</code> optimization flag is required to make the <code>_delay_ms()</code> function calls work successfully, as well as defining the variable F_CPU to reflect the expected chip clock speed. The rest of the parameters provide the location of the Attiny406 device-specific files we previously extracted from the <em>Device Pack</em>.</p>

<p>Uploading the firmware to the MCU requires a conversion to the intel HEX format and a call to the <strong>pyupdi</strong> tool. To address all these steps, we created a simple Makefile.</p>
<pre><span>OBJS</span><span>=</span>main.o
<span>ELF</span><span>=</span><span>$(</span>notdir <span>$(</span>CURDIR<span>))</span>.elf  
<span>HEX</span><span>=</span><span>$(</span>notdir <span>$(</span>CURDIR<span>))</span>.hex
<span>F_CPU</span><span>=</span>20000000L


<span>CFLAGS</span><span>=</span><span>-mmcu</span><span>=</span>attiny406 <span>-B</span> ../Atmel.ATtiny_DFP.1.6.326/gcc/dev/attiny406/ <span>-O3</span>
CFLAGS+<span>=</span><span>-I</span> ../Atmel.ATtiny_DFP.1.6.326/include/ <span>-DF_CPU</span><span>=</span><span>$(</span>F_CPU<span>)</span>
<span>LDFLAGS</span><span>=</span><span>-mmcu</span><span>=</span>attiny406 <span>-B</span> ../Atmel.ATtiny_DFP.1.6.326/gcc/dev/attiny406/
<span>CC</span><span>=</span>avr-gcc
<span>LD</span><span>=</span>avr-gcc

all:    <span>$(</span>HEX<span>)</span>  

<span>$(</span>ELF<span>)</span>: <span>$(</span>OBJS<span>)</span>
                <span>$(</span>LD<span>)</span> <span>$(</span>LDFLAGS<span>)</span> <span>-o</span> <span>$@</span> <span>$(</span>OBJS<span>)</span> <span>$(</span>LDLIBS<span>)</span>

<span>$(</span>HEX<span>)</span>: <span>$(</span>ELF<span>)</span>
                avr-objcopy <span>-O</span> ihex <span>-R</span> .eeprom <span>$&lt;</span> <span>$@</span>

flash:  <span>$(</span>HEX<span>)</span>
                pyupdi <span>-d</span> tiny406 <span>-c</span> /dev/tty.usbserial-FTF5HUAV <span>-f</span> attiny406-test.hex

read-fuses:
                pyupdi <span>-d</span> tiny406 <span>-c</span> /dev/tty.usbserial-FTF5HUAV <span>-fr</span>

clean:
                <span>rm</span> <span>-rf</span> <span>$(</span>OBJS<span>)</span> <span>$(</span>ELF<span>)</span> <span>$(</span>HEX<span>)</span>
</pre>
<p>To compile the code, we simply type <code>make</code>. Uploading is done with <code>make flash</code>. This Makefile can be further enhanced as needed.</p>

<h2 id="conclusion">Conclusion</h2>

<p>With the right tools, baremetal programming on the new TinyAVR MCUs is as simple as on its older AVR cousins. </p>

<p>If you have programming tips for the AVRTiny, please share them with us on <a href="https://twitter.com/OmzloElec">on Twitter</a> or in the comments below.</p>
</div></section><section><div><h4>Comments</h4><div><div><p>Hi, </p>

<p>It's great that …</p></div></div></div></section></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers">https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers</a></em></p>]]>
            </description>
            <link>https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers</link>
            <guid isPermaLink="false">hacker-news-small-sites-24648397</guid>
            <pubDate>Thu, 01 Oct 2020 07:37:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[MobX 6]]>
            </title>
            <description>
<![CDATA[
Score 159 | Comments 77 (<a href="https://news.ycombinator.com/item?id=24648363">thread link</a>) | @nikivi
<br/>
October 1, 2020 | https://michel.codes/blogs/mobx6 | <a href="https://web.archive.org/web/*/https://michel.codes/blogs/mobx6">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="___gatsby"><div tabindex="-1" role="group"><div><section><center><br><small>September 24, 2020</small></center><div><h2>Five years of MobX</h2>
<p>Time flies, and it has been 5.5 years since the first commit to MobX was made to build <a href="https://www.mendix.com/">Mendix Studio</a>.
In those years MobX has been adopted by well-known Software companies like Microsoft (Outlook), Netflix, Amazon and, my personal favorite, it runs in the Battlefield games by EA.
<a href="https://www.amazon.co.uk/MobX-Quick-Start-Guide-Supercharge/dp/1789344832/ref=sr_1_1?crid=BRUIHPUQL64D&amp;dchild=1&amp;keywords=mobx&amp;qid=1600809874&amp;s=books&amp;sprefix=mobx%2Caps%2C138&amp;sr=1-1">Books</a> and video courses have been written, and so have implementations in other languages.</p>
<p><span>
    <span></span>
    <img alt="Battlefield &amp; MobX" title="" src="https://michel.codes/static/5e0cc6842fdf39e4a7bf69a61fa15151/128ae/dice.jpg" srcset="https://michel.codes/static/5e0cc6842fdf39e4a7bf69a61fa15151/08cbc/dice.jpg 160w,
https://michel.codes/static/5e0cc6842fdf39e4a7bf69a61fa15151/37ac5/dice.jpg 320w,
https://michel.codes/static/5e0cc6842fdf39e4a7bf69a61fa15151/128ae/dice.jpg 640w,
https://michel.codes/static/5e0cc6842fdf39e4a7bf69a61fa15151/52793/dice.jpg 800w" sizes="(max-width: 640px) 100vw, 640px">
  </span></p>
<p>Yet, since that first commit the philosophy of the MobX hasn't changed: <em>Anything that can be derived from the application state, should be derived. Automatically.</em>.
The API hasn't changed too much since those days either,
and you will find the original <a href="https://www.mendix.com/blog/making-react-reactive-pursuit-high-performing-easily-maintainable-react-apps/">introduction of MobX</a> still pretty recognizable.
If <code>React.createClass</code> still rings a bell that is.</p>
<p>In contrast, the JavaScript eco-system has changed significantly over the years.
TypeScript and Babel have become the de-facto standards.
React went from <code>createClass</code> to classes to hook-based function components.
Yet, relevant JavaScript proposals for observables, Object.observe and decorators have never materialized.</p>
<p>MobX 6 is a new major version that doesn't bring many new features, but is rather a consolidation of MobX on the current state of affairs in JavaScript.
That doesn't come without a few plot twists, so if you are an existing MobX user, please read till the end!</p>
<h2>Bye bye decorators</h2>
<p>Let's start with the bad news: Using decorators is no longer the norm in MobX.
This is good news to some of you, but others will hate it.
Rightfully so, because I concur that the declarative syntax of decorators is still the best that can be offered.
When MobX started, it was a TypeScript only project, so decorators were available.
Still experimental, but obviously they were going to be standardized soon.
That was my expectation at least (I did mostly Java and C# before).
However, that moment still hasn't come yet, and two decorators proposals have been cancelled in the mean time.
Although they still can be transpiled.</p>
<p>So why did we stop using decorators by default?</p>
<p>First of all, the current experimental decorator implementations are incompatible with the soon-to-be-standardized class-fields proposal.
The legacy (Babel) and experimental (TypeScript) decorator implementations will no longer be able to <a href="https://github.com/tc39/proposal-class-fields/issues/151">trap class fields initializations</a>.</p>
<p>Secondly, using decorators has always been a serious hurdle in adopting and advocating MobX.
In Babel, it is quite fragile to set up.
<code>create-react-app</code> doesn't support it out of the box, and many developers rightfully don't like to use non-standard features.
Even though decorators have always been optional in MobX, the fact that they were prominent in the docs left many confused.
Or as one MobX fan <a href="https://github.com/mobxjs/mobx/issues/2325#issuecomment-693130586">puts it</a>:</p>
<p><em>I am guessing this choice [to drop decorators] was probably a good call. Maybe now without decorators I am hopeful I will be able to convey to people how amazing Mobx is and at least I won't hear the decorators excuse anymore. I have never seen an "@" sign scare so many people.</em></p>
<p>So, what does MobX after decorators look like?
Simply put, instead of decorating class members during the class definition, instance members need to be annotated in the constructor instead, using the new <code>makeObservable</code> utility:</p>
<div data-language="typescript"><pre><code><span>import</span> <span>{</span>observable<span>,</span> computed<span>,</span> action<span>,</span> makeObservable<span>}</span> <span>from</span> <span>"mobx"</span>


<span>class</span> <span>TodoStore</span> <span>{</span>
    @observable
    todos<span>:</span> Todo<span>[</span><span>]</span> <span>=</span> <span>[</span><span>]</span>

    @computed
    <span>get</span> <span>unfinishedTodoCount</span><span>(</span><span>)</span> <span>{</span>
        <span>return</span> <span>this</span><span>.</span>todos<span>.</span><span>filter</span><span>(</span>todo <span>=&gt;</span> <span>!</span>todo<span>.</span>done<span>)</span><span>.</span>length
    <span>}</span>

    @action
    <span>addTodo</span><span>(</span>todo<span>:</span> Todo<span>)</span> <span>{</span>
        <span>this</span><span>.</span>todos<span>.</span><span>push</span><span>(</span>todo<span>)</span>
    <span>}</span>
<span>}</span>


<span>class</span> <span>TodoStore</span> <span>{</span>
    todos<span>:</span> Todo<span>[</span><span>]</span> <span>=</span> <span>[</span><span>]</span>

    <span>constructor</span><span>(</span><span>)</span> <span>{</span>
        <span>makeObservable</span><span>(</span><span>this</span><span>,</span> <span>{</span>
            todos<span>:</span> observable<span>,</span>
            unfinishedTodoCount<span>:</span> computed<span>,</span>
            addTodo<span>:</span> action
        <span>}</span><span>)</span>
    <span>}</span>

    <span>get</span> <span>unfinishedTodoCount</span><span>(</span><span>)</span> <span>{</span>
        <span>return</span> <span>this</span><span>.</span>todos<span>.</span><span>filter</span><span>(</span>todo <span>=&gt;</span> <span>!</span>todo<span>.</span>done<span>)</span><span>.</span>length
    <span>}</span>

    <span>addTodo</span><span>(</span>todo<span>:</span> Todo<span>)</span> <span>{</span>
        <span>this</span><span>.</span>todos<span>.</span><span>push</span><span>(</span>todo<span>)</span>
    <span>}</span>
<span>}</span></code></pre></div>
<p>Admittedly, this is a slightly worse DX than before, since member and annotation are no longer co-located.
But the good news is that using <code>makeObservable</code> doesn't require any fancy build setup.
It should work everywhere out of the box.</p>
<p>Migrating an entire code-base from decorators to <code>makeObservable</code> might be challenging, so that is why we released a <a href="https://www.npmjs.com/package/mobx-undecorate">code-mod</a> together with MobX 6 to do that automatically!
Just run the command <code>npx mobx-undecorate</code> inside the folder where your source files live, and after that all decorators should have been magically rewritten!
After that, make sure to <a href="https://mobx.js.org/migrating-from-4-or-5.html#getting-started">update your TypeScript / babel config</a>, and you should be good to go!</p>
<h2>Introducing <code>makeAutoObservable</code></h2>
<p>We realize it is easier to make mistakes now that the annotations are no longer adjacent to the fields they are decorating.
Hence a convenience utility has been introduced that automates the annotation process by picking sane defaults: <code>makeAutoObservable</code>.
It will automatically pick the best annotation for every member of a class, thereby simplifying the above listing to:</p>
<div data-language="typescript"><pre><code><span>class</span> <span>TodoStore</span> <span>{</span>
    todos<span>:</span> Todo<span>[</span><span>]</span> <span>=</span> <span>[</span><span>]</span>

    <span>constructor</span><span>(</span><span>)</span> <span>{</span>
        <span>makeAutoObservable</span><span>(</span><span>this</span><span>)</span>
    <span>}</span>

    <span>get</span> <span>unfinishedTodoCount</span><span>(</span><span>)</span> <span>{</span>
        <span>return</span> <span>this</span><span>.</span>todos<span>.</span><span>filter</span><span>(</span>todo <span>=&gt;</span> <span>!</span>todo<span>.</span>done<span>)</span><span>.</span>length
    <span>}</span>

    <span>addTodo</span><span>(</span>todo<span>:</span> Todo<span>)</span> <span>{</span>
        <span>this</span><span>.</span>todos<span>.</span><span>push</span><span>(</span>todo<span>)</span>
    <span>}</span>
<span>}</span></code></pre></div>
<p>Note that it is possible to pass a map of <em>overrides</em> as second argument, in case you want to use a modifier.
For example: <code>makeAutoObservable(this, { todos: observable.shallow })</code>.
Pass <code>member: false</code> to have MobX ignore that member entirely.
(Technical fineprint: class methods will not be decorated with <code>action</code>, but with the new <code>autoAction</code>, this annotation will make methods suitable to be used both as a state updating action, or as function that derives information from state).</p>
<p>What I personally like about <code>makeAutoObservable</code> is that it plays really nice with factory functions.
Which is great if you prefer to not use classes (factory functions make it is easy to hide members and prevent issues with <code>this</code> and <code>new</code>. And they compose more easily).
The same store expressed as factory function will look as follows. Pick the style that suits you:</p>
<div data-language="typescript"><pre><code><span>function</span> <span>createTodoStore</span><span>(</span><span>)</span> <span>{</span>
    <span>const</span> store <span>=</span> <span>makeAutoObservable</span><span>(</span><span>{</span>
        todos<span>:</span> <span>[</span><span>]</span> <span>as</span> Todo<span>[</span><span>]</span><span>,</span>
        <span>get</span> <span>unfinishedTodoCount</span><span>(</span><span>)</span> <span>{</span>
            <span>return</span> store<span>.</span>todos<span>.</span><span>filter</span><span>(</span>todo <span>=&gt;</span> <span>!</span>todo<span>.</span>done<span>)</span><span>.</span>length
        <span>}</span><span>,</span>
        <span>addTodo</span><span>(</span>todo<span>:</span> Todo<span>)</span> <span>{</span>
            store<span>.</span>todos<span>.</span><span>push</span><span>(</span>todo<span>)</span>
        <span>}</span>
    <span>}</span><span>)</span>
    <span>return</span> store
<span>}</span></code></pre></div>
<h2>Fresh docs!</h2>
<p>Since decorators are no longer the norm, <a href="https://twitter.com/faassen">Martijn Faassen</a>, <a href="https://twitter.com/zangornjak">Žan Gornjak</a> and yours truly went over all the documentation.
We updated all examples and significantly restructured the docs that have grown quite organically over the years.
We feel the current docs are shorter, have less repetition, and better discuss common scenarios.
Also, we marked all non-essential knowledge in the docs with a {🚀} rocket emoji, to make it clear which knowledge is optional.
Hopefully it is much quicker now to find your way around in MobX!</p>
<p>On a similar note, we've updated all <a href="https://mobx.js.org/react-integration.html">React related documentation</a> to use function components instead of class components. And added documentation on how to use MobX with hooks, context and effects.
This knowledge existed before (little has changed technically), but was scattered all over the place.
As a result, we now recommend <code>mobx-react-lite</code> over <code>mobx-react</code> for (greenfield) projects that don't use class components.
As a result the separate mobx-react.js.org/ website has been deprecated.
All credits go to <a href="https://twitter.com/danielk_cz">Daniel K</a> for maintaining those two projects!</p>
<p>And finally, there is now a <a href="https://gum.co/fSocU">one pager MobX 6 cheat sheet 👨‍🎓</a> covering all the import mobx / mobx-react(-lite) API's.
(It is a great way to one-time sponser the project in an invoicable way).</p>
<h2>Improved browser support</h2>
<p>A probably little surprising improvement in MobX 6 is that it supports <em>more</em> JavaScript engines than MobX 5.
MobX 5 required proxy support, making MobX unsuitable for Internet Explorer or React Native (depending on the engine).
For this reason MobX 4 was still actively maintained.
However, MobX 6 replaces both at once.</p>
<p>By default MobX 6 will still require Proxies, but it is possible to opt-out from Proxy usage in case you need to support older engines.
And, as a result, it is now possible for MobX 6 to warn in development mode when features that would require proxies are used.
See the documentation for more <a href="https://mobx.js.org/configuration.html#proxy-support">details</a>.</p>
<div data-language="typescript"><pre><code><span>import</span> <span>{</span> configure <span>}</span> <span>from</span> <span>"mobx"</span>

<span>configure</span><span>(</span><span>{</span>
    
    
    
    useProxies<span>:</span> <span>"never"</span>
<span>}</span><span>)</span></code></pre></div>
<h2>Decorators are back!</h2>
<p>Ok, time for the plot twist. MobX 6 stills supports decorators!
The decorator implementation in MobX 6 is entirely different from the one in earlier versions, but does work with the current implementations in TypeScript and Babel.
It basically provides an alternative way to construct the annotations map for <code>makeObservable</code>, and allows us to rewrite the first example as:</p>
<div data-language="typescript"><pre><code><span>class</span> <span>TodoStore</span> <span>{</span>
    @observable
    todos<span>:</span> Todo<span>[</span><span>]</span> <span>=</span> <span>[</span><span>]</span>

    <span>constructor</span><span>(</span><span>)</span> <span>{</span>
        <span>makeObservable</span><span>(</span><span>this</span><span>)</span>
    <span>}</span>

    @computed
    <span>get</span> <span>unfinishedTodoCount</span><span>(</span><span>)</span> <span>{</span>
        <span>return</span> <span>this</span><span>.</span>todos<span>.</span><span>filter</span><span>(</span>todo <span>=&gt;</span> <span>!</span>todo<span>.</span>done<span>)</span><span>.</span>length
    <span>}</span>

    @action
    <span>addTodo</span><span>(</span>todo<span>:</span> Todo<span>)</span> <span>{</span>
        <span>this</span><span>.</span>todos<span>.</span><span>push</span><span>(</span>todo<span>)</span>
    <span>}</span>
<span>}</span></code></pre></div>
<p>Note that we still need to add a constructor to the class, but this time we omit the second argument to <code>makeObservable</code>, so that it will rely on the decorators instead.
We don't recommend this set up for greenfield projects, after all decorators are still experimental, but this is a great compromise for
existing code bases.
Generating constructors without removing decorators is supported by <code>mobx-undecorate</code> as well, and can be achieved by running <code>npx mobx-undecorate --keepDecorators</code>.</p>
<p>And here is even more good news: There is a fresh <a href="https://github.com/tc39/proposal-decorators">decorators proposal</a> being championed by the tireless hero <a href="https://twitter.com/littledan">Daniel Ehrenberg</a>.
I've been a bit involved in it, and the MobX use case has inspired the proposal.
So the benefits of the fresh decorator implementation are that it a) solves the compatibility issue with the class fields spec discussed above, and
b) it also paves the way …</p></div></section></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://michel.codes/blogs/mobx6">https://michel.codes/blogs/mobx6</a></em></p>]]>
            </description>
            <link>https://michel.codes/blogs/mobx6</link>
            <guid isPermaLink="false">hacker-news-small-sites-24648363</guid>
            <pubDate>Thu, 01 Oct 2020 07:33:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Worst Decision of My Career]]>
            </title>
            <description>
<![CDATA[
Score 19 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24648256">thread link</a>) | @fribmendes
<br/>
October 1, 2020 | https://subvisual.com/blog/posts/the-worst-decision-of-my-career/ | <a href="https://web.archive.org/web/*/https://subvisual.com/blog/posts/the-worst-decision-of-my-career/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><article><section><div><div><div><div><div><div>
<p>This is a reflection on software development and complexity. Let's start with
some quotes to make me look smart:</p>
<blockquote>
<p>A complex system that works is invariably found to have evolved from a simple
system that worked. A complex system designed from scratch never works and
cannot be patched up to make it work. You have to start over with a working
simple system -- <em>Gall's law</em></p>
</blockquote>
<p>I only came across this quote recently, but I think that it summarizes what
I've learned these past seven years. Another idea that I've found in a few books
is that programming in isolation is problem-solving, but software engineering
is all about managing complexity. This distinction between programming and
software engineering makes sense to me, probably because of my mental models.
I'm sure we can all disagree on this, but that's not the topic of today.
Complexity is.</p>
<h2>Complexity</h2>
<p>Almost everyone I know in the software industry wants to build products that
solve real and challenging problems. Change the world! The last thing you want
is to build another CRUD application. They are fine, but it gets boring after
a while.</p>
<p>Most of us will get bored without novelty. That's why the software industry has
a recycling mechanism to keep things fresh and interesting for us: every once
in a while a new, or different, language/framework will rise and light the path
for a brighter future, overthrowing the existing standard, demanding that we
learn it to be on top of the game.</p>
<p>We change our tools, but we keep building the same things over and over. I've
been doing this long enough to see that we are just going in circles. Let's be
honest, most of us aren't building life-changing products that wouldn't have
been possible ten years ago, so let's not jump straight into another shiny
technology that just came out and is going to solve all of our problems.</p>
<h2>Solutions, solutions, solutions</h2>
<blockquote>
<p>For a while, the solution to every problem I encountered was a Rails app. --
<em>this one is mine</em></p>
</blockquote>
<p>Over the years, I've seen companies rewrite their products to change languages,
frameworks, or architectures because they were told that the change would make
their product better, run faster, and scale. By the way, you're only a true
Ruby developer after you've heard the question "but does it scale?" at least
one hundred times (kidding, but it'll happen).</p>
<p>Think about it, how many times were you sold a new programming language,
technology, or a concept like microservices, serverless, event sourcing, clean
architecture, micro frontends. There are many preachers in the software world.</p>
<p>At Subvisual, we started using Elixir a lot these past years, so I've learned
a bit about the history of Erlang. If you don't know, Erlang uses the Actor
Model, and the interesting part is that the designers of Erlang only learned
about the Actor Model after having designed Erlang. This is important because
the designers of Erlang didn't start with the intent of applying the Actor
Model; it came as a solution to fault-tolerant distributed programming.</p>
<blockquote>
<p>If all you have is a hammer, everything looks like a nail <em>Abraham Maslow</em></p>
</blockquote>
<p>The designers of Erlang picked the right tool for the job and most of us want
to do the same. Unfortunately, there's usually more than one "tool" that would
be "right," but to know which ones, you need to know what "job" you're solving.
All of us <strong>should</strong> know this, but I keep learning about teams that fail to do
it. There are so many companies, with a handful of experienced developers, that
don't know what they are building, don't have a clear business yet, but their
product is already built on complex architectures and technologies such as
microservices, Kubernetes or event-sourcing.</p>
<h2>Improving</h2>
<p>Every project we start from scratch is an opportunity to do it right. We'll
think to ourselves: <em>This time I won't fall into the same traps! I have learned
my lessons, I've studied the books, and I even met some of my gurus that wrote
them! This time I'll follow "industry standards"!</em></p>
<p><em>I am the "architect"; I will design the perfect system! Without me, none of
this will be possible. Those mindless programmers have no idea what they are
doing. I, and only I, am the true heir of Martin Fowler!</em></p>
<p>This was a bit dramatic, but I needed a break from all of that whining. I know
it's easy to fall into these traps. It's even easier when your company raised
money, and everyone is expecting you to deliver the absolute best product ever
because they are paying you for it! This is why your starting team is so
important; they have to withstand the pressure. They must know that to build
the grand vision, they have to go one step at a time.</p>
<h2>The decision</h2>
<p>I've made many bad decisions, and I've been fortunate enough to suffer the
consequences of those decisions. A lot of developers don't get to experience
consequences, so they never learn.</p>
<p>So what was the worst decision of my career? I don't know. But the title of
this blog post is inspired by something an old colleague said. At the time, we
were working for a product company that reached for event-driven architecture
and event-sourcing too soon. They knew little about their market, and the
choices the software team made were crippling their ability to change. Business
rules were almost set in stone. Migrating data was a pain and the source of
many bugs. And unfortunately, because the team didn't have experience with
event-sourcing, the event store, which kept the state of all services, was also
being used as an event-bus to communicate between services. Because of that, it
was possible to couple one service to the internal state of another, which
happened a lot. This almost invisible coupling made everything worse.</p>
<p>What did we do against such an unpredictable system? We took it apart: merging
services that were too coupled; defining clear boundaries between services;
moving some services away from the event-store into a traditional database;
making some communication channels synchronous; writing integration and
end-to-end tests.</p>
<p>When we were finished it was still an unnecessarily complex system, but it was
one that we could change with some confidence. After that, we defined
a long-term plan for the product's architecture and technology, but we didn't
implement it. We waited for the right moment when something was starting to
slow us down to make a small step in that direction. When the business goals
changed, we changed our long-term plan, and once again made small steps in that
direction when we felt the need for it.</p>
<p>Eventually, complexity found its way again into the codebase, but it was fine
because the codebase was evolving slowing, adding and removing complexity when
necessary.</p>
<h2>Making the right call</h2>
<p>Was that the worst decision of his career? I don't think so. The issue with him
going for event-sourcing and event-driven architecture was that it wasn't the
right moment, but my colleague didn't know that. He thought the goals for the
next years were well defined, but unfortunately, they never are.</p>
<p>Should you use microservices or event-sourcing? Maybe, it depends on the
context and the client. The decisions I make are the best that I can with the
information that I have. For instance, when I'm part of the team that's
starting a product, the technology we pick will depend on how we'll hire: if
you want to build an office in Portugal, we have to make sure we have
developers for that technology available. We have to think things through, and
some things you only learn from experience. This applies to the systems we
design as well. I've seen enough people design around what they believe the
product will become in two years to know that those designs always fail to
accommodate the changes that will come. So we design systems for small,
incremental changes. Do the smallest thing that will get us started and
doesn't compromise our ability to change once we know what the business needs.</p></div></div></div></div></div></div></section></article></div></div></div>]]>
            </description>
            <link>https://subvisual.com/blog/posts/the-worst-decision-of-my-career/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24648256</guid>
            <pubDate>Thu, 01 Oct 2020 07:18:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[You're Allowed to Write Slow Rust Code]]>
            </title>
            <description>
<![CDATA[
Score 32 | Comments 54 (<a href="https://news.ycombinator.com/item?id=24647910">thread link</a>) | @ingve
<br/>
September 30, 2020 | https://blog.jonstodle.com/youre-allowed-to-write-slow-rust-code/ | <a href="https://web.archive.org/web/*/https://blog.jonstodle.com/youre-allowed-to-write-slow-rust-code/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="blog-post">
    
    
    <article>
        <p>There's a thing that comes up from time to time in the Rust community: people wanting to optimize their code. Make the code run faster. Make it allocate less memory. These are worthy goals, but maybe not necessary for all projects. Just because your program is written in Rust, it doesn't have to be optimized to the moon and back to do it's job. In a lot of cases, you'll be fine using <code>.clone()</code>.</p>
<p>There's a fantastic quote from a discussion on the <a href="https://users.rust-lang.org/">Rust user forums</a> which I always think of when these discussions emerge:</p>
<blockquote>
<p>Just because Rust allows you to write super cool non-allocating zero-copy algorithms safely, doesn't mean every algorithm you write should be super cool, zero-copy and non-allocating.<br>
-- <a href="https://users.rust-lang.org/t/feeling-rust-is-so-difficult/29962/15">trentj on The Rust Programming Language Forum</a></p>
</blockquote>
<p>Of course there's a time and place for <em>super cool no-allocating zero-copy algorithms</em>, but very often it's not the time, nor the place. You can write fantastically effective code with Rust, but <strong>you don't have to</strong>! A lot of the time it's entirely fine to just write code that works with minimal effort. You don't have to spend 4 days trying to figure out how lifetimes work just to avoid calling <code>.clone()</code> a few times.</p>
<p>Don't spend time trying to make micro optimizations now. Take the easy route. You'll probably come back to that piece of code in a few months time, smile, and change that <code>.clone()</code> to something more efficient.</p>
<p>Happy coding!</p>
<p><small>NB: Watch this <a href="https://youtu.be/rAl-9HwD858">video by Jon Gjengset</a> if you want a good, pragmatic walk through of lifetimes in Rust.</small></p>

    </article>
</div></div>]]>
            </description>
            <link>https://blog.jonstodle.com/youre-allowed-to-write-slow-rust-code/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24647910</guid>
            <pubDate>Thu, 01 Oct 2020 06:19:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Running GitHub Actions for Certain Commit Messages]]>
            </title>
            <description>
<![CDATA[
Score 77 | Comments 29 (<a href="https://news.ycombinator.com/item?id=24647722">thread link</a>) | @kiyanwang
<br/>
September 30, 2020 | https://ryangjchandler.co.uk/articles/running-github-actions-for-certain-commit-messages | <a href="https://web.archive.org/web/*/https://ryangjchandler.co.uk/articles/running-github-actions-for-certain-commit-messages">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div>
            
                <section>
        
        <p>A quick look at how you can configure your GitHub Actions workflows to only run when a certain phrase is present in the commit message.</p>
                    <small>Published 5 days ago</small>
                            <span>|</span>
                <small>Updated 5 days ago</small>
                                                <p><span>
    Tooling
</span>
 
                                     <span>
    GitHub Actions
</span>
 
                            </p>
                        <article>
            <p>I'm going to be honest with you all for a second. I write a lot of <code>wip</code> commits. These commits are normally small changes that I want to push up to GitHub so that:</p>
<ol>
<li>I don't lose things if anything goes wrong and my backup hasn't picked it up.</li>
<li>If I can't describe the change I have just made.</li>
<li>If I'm demonstrating something to somebody on a pull-request.</li>
</ol>
<p>The problem is, my actions are setup to run on <code>push</code>, so every single <code>wip</code> commit gets run through the CI process, whether it be running tests, linting or formatting.</p>
<p>After doing some research, I found a way of preventing these from running on every single commit.</p>
<pre><code data-lang="yml"><span>jobs:</span>
  <span>format:</span>
    <span>runs-on:</span> <span>ubuntu-latest</span>
    <span>if:</span> <span>"! contains(github.event.head_commit.message, 'wip')"</span>
</code></pre>
<p>Now, whenever I push a <code>wip</code> commit or any commit that contains the word <code>wip</code>, it will be marked as skipped inside of GitHub actions.</p>
<p>You could also flip the logic and perhaps do something like:</p>
<pre><code data-lang="yml"><span>jobs:</span>
  <span>format:</span>
    <span>runs-on:</span> <span>ubuntu-latest</span>
    <span>if:</span> <span>"contains(github.event.head_commit.message, '[build]')"</span>
</code></pre>
<p>Any commit that contains <code>[build]</code> will now trigger these jobs, everything else will be skipped.</p>
<p>You can thank me later! 😉</p>

        </article>
    </section>
        </div>
    </div></div>]]>
            </description>
            <link>https://ryangjchandler.co.uk/articles/running-github-actions-for-certain-commit-messages</link>
            <guid isPermaLink="false">hacker-news-small-sites-24647722</guid>
            <pubDate>Thu, 01 Oct 2020 05:41:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Grand Design (2010)]]>
            </title>
            <description>
<![CDATA[
Score 18 | Comments 51 (<a href="https://news.ycombinator.com/item?id=24646120">thread link</a>) | @got-any-grapes
<br/>
September 30, 2020 | https://blas.com/the-grand-design/ | <a href="https://web.archive.org/web/*/https://blas.com/the-grand-design/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="page">
	<!-- #masthead -->

	<div id="main">

	<div id="primary">
		<div id="content" role="main">

			
				
	<article id="post-5144">
				<!-- .entry-header -->

				<div>
			
<p>Summary</p>



<ol><li>Traditionally these are questions for philosophy, but philosophy is dead. Philosophy has not kept up with modern developments in science, particularly physics. Scientists have become the bearers of the torch of discovery in our quest for knowledge. The purpose of this book is to give the answers that are suggested by recent discoveries and theoretical advances. They lead us to a new picture of the universe and our place in it that is very different from the traditional one, and different even from the picture we might have painted just a decade or two ago.</li></ol>



<p>Key Takeaways</p>



<ol><li>According to the traditional conception of the universe, objects move on well-defined paths and have definite histories. We can specify their precise position at each moment in time. Although that account is successful enough for everyday purposes, it was found in the 1920s that this “classical” picture could not account for the seemingly bizarre behavior observed on the atomic and subatomic scales of existence. Instead it was necessary to adopt a different framework, called quantum physics. Quantum theories have turned out to be remarkably accurate at predicting events on those scales, while also reproducing the predictions of the old classical theories when applied to the macroscopic world of daily life. But quantum and classical physics are based on very different conceptions of physical reality.</li><li><strong>We will explain Feynman’s approach in detail, and employ it to explore the idea that the universe itself has no single history, nor even an independent existence. That seems like a radical idea, even to many physicists. Indeed, like many notions in today’s science, it appears to violate common sense. But common sense is based upon everyday experience, not upon the universe as it is revealed through the marvels of technologies such as those that allow us to gaze deep into the atom or back to the early universe.</strong></li><li><strong>To deal with such paradoxes we shall adopt an approach that we call model-dependent realism. It is based on the idea that our brains interpret the input from our sensory organs by making a model of the world. When such a model is successful at explaining events, we tend to attribute to it, and to the elements and concepts that constitute it, the quality of reality or absolute truth. But there may be different ways in which one could model the same physical situation, with each employing different fundamental elements and concepts. If two such physical theories or models accurately predict the same events, one cannot be said to be more real than the other; rather, we are free to use whichever model is most convenient.</strong><ol><li><em>Model-dependent realism, mental models</em></li></ol></li><li><strong>M-theory is not a theory in the usual sense. It is a whole family of different theories, each of which is a good description of observations only in some range of physical situations. It is a bit like a map. As is well known, one cannot show the whole of the earth’s surface on a single map. The usual Mercator projection used for maps of the world makes areas appear larger and larger in the far north and south and doesn’t cover the North and South Poles. To faithfully map the entire earth, one has to use a collection of maps, each of which covers a limited region. The maps overlap each other, and where they do, they show the same landscape. M-theory is similar. The different theories in the M-theory family may look very different, but they can all be regarded as aspects of the same underlying theory. They are versions of the theory that are applicable only in limited ranges—for example, when certain quantities such as energy are small. Like the overlapping maps in a Mercator projection, where the ranges of different versions overlap, they predict the same phenomena. But just as there is no flat map that is a good representation of the earth’s entire surface, there is no single theory that is a good representation of observations in all situations.</strong><ol><li><em>The Mp is Not the Terrain</em></li></ol></li><li>Today most scientists would say a law of nature is a rule that is based upon an observed regularity and provides predictions that go beyond the immediate situations upon which it is based.<ol><li><em>General and broadly applicable</em></li></ol></li><li>Because it is so impractical to use the underlying physical laws to predict human behavior, we adopt what is called an effective theory. In physics, an effective theory is a framework created to model certain observed phenomena without describing in detail all of the underlying processes.</li><li>There is no picture- or theory-independent concept of reality. Instead we will adopt a view that we will call model-dependent realism: the idea that a physical theory or world picture is a model (generally of a mathematical nature) and a set of rules that connect the elements of the model to observations. This provides a framework with which to interpret modern science.</li><li><strong>We make models in science, but we also make them in everyday life. Model-dependent realism applies not only to scientific models but also to the conscious and subconscious mental models we all create in order to interpret and understand the everyday world. There is no way to remove the observer—us—from our perception of the world, which is created through our sensory processing and through the way we think and reason. Our perception—and hence the observations upon which our theories are based—is not direct, but rather is shaped by a kind of lens, the interpretive structure of our human brains.</strong><ol><li><em>Mental Models, Galilean Relativity</em></li></ol></li><li><strong>A model is a good model if it: Is elegant, Contains few arbitrary or adjustable elements, Agrees with and explains all existing observations, Makes detailed predictions about future observations that can disprove or falsify the model if they are not borne out.</strong></li><li>Another of the main tenets of quantum physics is the uncertainty principle, formulated by Werner Heisenberg in 1926. The uncertainty principle tells us that there are limits to our ability to simultaneously measure certain data, such as the position and velocity of a particle. According to the uncertainty principle, for example, if you multiply the uncertainty in the position of a particle by the uncertainty in its momentum (its mass times its velocity) the result can never be smaller than a certain fixed quantity, called Planck’s constant. That’s a tongue-twister, but its gist can be stated simply: The more precisely you measure speed, the less precisely you can measure position, and vice versa.</li><li><strong>In other words, nature does not dictate the outcome of any process or experiment, even in the simplest of situations. Rather, it allows a number of different eventualities, each with a certain likelihood of being realized.</strong></li><li>Given the state of a system at some time, the laws of nature determine the probabilities of various futures and pasts rather than determining the future and past with certainty.</li><li><strong>Quantum physics tells us that no matter how thorough our observation of the present, the (unobserved) past, like the future, is indefinite and exists only as a spectrum of possibilities. The universe, according to quantum physics, has no single past, or history. The fact that the past takes no definite form means that observations you make on a system in the present affect its past.</strong></li><li>Electric and magnetic forces are far stronger than gravity, but we don’t usually notice them in everyday life because a macroscopic body contains almost equal numbers of positive and negative electrical charges. This means that the electric and magnetic forces between two macroscopic bodies nearly cancel each other out, unlike the gravitational forces, which all add up.</li><li>Maxwell’s equations dictate that electromagnetic waves travel at a speed of about 300,000 kilometers a second, or about 670 million miles per hour. But to quote a speed means nothing unless you specify a frame of reference relative to which the speed is measured. That’s not something you usually need to think about in everyday life. When a speed limit sign reads 60 miles per hour, it is understood that your speed is measured relative to the road and not the black hole at the center of the Milky Way. But even in everyday life there are occasions in which you have to take into account reference frames. For example, if you carry a cup of tea up the aisle of a jet plane in flight, you might say your speed is 2 miles per hour. Someone on the ground, however, might say you were moving at 572 miles per hour. Lest you think that one or the other of those observers has a better claim to the truth, keep in mind that because the earth orbits the sun, someone watching you from the surface of that heavenly body would disagree with both and say you are moving at about 18 miles per second, not to mention envying your air-conditioning.<ol><li><em>Galilean Relativity</em></li></ol></li><li>Electromagnetic forces are responsible for all of chemistry and biology.</li><li>The histories that contribute to the Feynman sum don’t have an independent existence, but depend on what is being measured. We create history by our observation, rather than history creating us. The idea that the universe does not have a unique observer-independent history might seem to conflict with certain facts we know. There might be one history in which the moon is made of Roquefort cheese. But we have observed that the moon is not made of cheese, which is bad news for mice. Hence histories in which the moon is made of cheese do not contribute to the present state of our universe, though they might contribute to others. That might sound like science fiction, but it isn’t.</li><li>What makes this universe interesting is that although the fundamental “physics” of this universe is simple, the “chemistry” can be complicated. That is, composite objects exist on different scales. At the smallest scale, the fundamental physics tells us that there are just live and dead squares. On a larger scale, there are gliders, blinkers, and still-life blocks. At a still larger scale there are even more complex objects, such as …</li></ol></div></article></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blas.com/the-grand-design/">https://blas.com/the-grand-design/</a></em></p>]]>
            </description>
            <link>https://blas.com/the-grand-design/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24646120</guid>
            <pubDate>Thu, 01 Oct 2020 01:08:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Computer Dungeon Slash Postmortem]]>
            </title>
            <description>
<![CDATA[
Score 31 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24646063">thread link</a>) | @panic
<br/>
September 30, 2020 | https://museumofzzt.com/article/496/a-computer-dungeon-slash-postmortem | <a href="https://web.archive.org/web/*/https://museumofzzt.com/article/496/a-computer-dungeon-slash-postmortem">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article data-article_id="496">


    

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image2.png"></p>


        

    


<p>I consider <a href="https://museumofzzt.com/file/c/CDZ20MOZ.ZIP" target="_blank"><i>Computer Dungeon Slash: ZZT 2.0</i></a> my personal best for ZZT games. I've now released two versions, one in September of 2019 and then another with some updates and optimizations in May of 2020. Aside from a couple of smaller errors and special-thanks additions, there's not much more to fix, so this will probably be the final version with any major changes.</p>

<p>Like some of my previous ZZT games, it relies heavily on procedural generation, with apparently such complexity that Dr. Dos called it "basically sorcery" during their <a href="https://museumofzzt.com/article/479/livestream-computer-dungeon-slash-zzt-20">playthrough on Twitch</a>, which I took as a great compliment. He also mentioned that this game, its generators in particular, could use a write-up. So I wrote one!</p>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image24.png"></p>

<p>In case you're reading this and unfamiliar with this game, it concerns a player finding themselves in the town of Habeas Corpus, where almost everyone and everything has been swallowed by the dungeon.</p>

<p>The player's task is to brave the randomly-generated dungeon, rescuing the town and townspeople from its clutches.</p>

<p>There's not much more plot or lore than the above, since I focused mainly on characterization in my worldbuilding and most of the actual plot isn't revealed until the end.</p>

<p>So what about the workings of this game?</p>

<p>I'll start with "sorcery"--there's something to that, for sure. When I work with procedural generation it feels a bit like alchemy. Scientific, but also a little bit mysterious and magical. I hope this article helps you better understand the scientific side of that coin.</p>

<p>For readers not intimately familiar with ZZT, one reason a veteran ZZTer would call my level generation "sorcery"  is its limitations. ZZT is a 1991 DOS game-creation-system, after all, simple enough for 8-year-old me to use. So if you don't know anything about ZZT coming in, I hope this write-up helps you better understand the absurdity and appeal of working in it and pushing its limits.</p>

<p>Further, if you came into this wanting to know how this stuff works so that you can experiment with it or even improve on it, I want you to be empowered to do so.</p>

<h2>Goals</h2>

<p>With <i>Computer Dungeon Slash: ZZT</i> (called <i>CDZ</i> here for short) I had three goals for generation. I wanted levels to be interesting to look at, fun to play, and fair. What does fair mean?</p>

<p>Most previous ZZT releases with procedural generation would probably not soft-lock players (block them off from finishing the game). In the early 2000s when this fad hit, most ZZTers involved were in high school and "probably not" was enough. But that small chance of soft-locking players with my generators annoyed me even then.</p>

<p>With <i>CDZ</i>, I wanted some certainty that my algorithms absolutely would not soft-lock players.</p>

<p>With these goals in mind, let's look at the tools I had to accomplish them.</p>

<h2>Building Blocks for Procedural Level Generation in ZZT</h2>

<p>ZZT is tile-based, so making it create its own levels is kind of like generating dungeons for classic ASCII roguelikes, except with much more restrictive tools and limits than, for example, C++ or Python. There's no way to easily store level layout data outside of "in the level", and ZZT boasts precious few variables and only ten true-or-false flags. In addition, the entire game is made up of "boards" of only 60x25 tiles! Despite being such a limited system, ZZT lets us do a fair amount.</p>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image4.png">
<img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image7.gif"></p>

<p>Almost any element (terrain or other object) that takes up a space on a board is helpful, but a few basic ones are laid out in a reference image on the left. Fake walls prove pretty invaluable because they don't block objects but help the engine easily keep track of where another terrain type <i>will</i> later appear.</p>

<p>While <i>CDZ</i> uses sliders and boulders to some extent, the classic examples of ZZT level generators using sliders and boulders are probably WiL's <i>Run-On</i> and Benco's <i>Lost</i>.</p>

<p>And of course, one of the elements of play is actually <i>called</i> an object, and can run little scripts in ZZT-OOP, the engine's default "language." ZZT-OOP has a number of helpful commands for procedural generation.</p>

<p>The <code>#change</code> command can change almost any ZZT gameplay element into almost any other, which is invaluable both for structuring and theming levels. <code>#put THING DIRECTION</code> and <code>#become THING</code> allow for level builder objects to modify environments directly, and they have a more complex use we'll go into later.</p>

<p>Procedural level generation in <i>CDZ</i> is random. Appropriately, ZZT-OOP's random directions proved immensely useful in making it happen. A number of situations call for a "four-sided" dice-roll, and ZZT does have an <i>rnd</i> direction (randomly north, south, east, or west), but as the reader may know, <i>rnd</i> is twice as likely to give an east-west result as a north-south. So what to do?</p>

<p>Well, there are also <i>rndne</i> (north or east) and <i>rndp DIRECTION</i> (one of two directions perpendicular to <i>DIRECTION</i>). Because <i>rndne</i> is a valid direction in itself, you can use <i>rndp rndne</i> to get a random cardinal direction with equal probability of each. (I'm ashamed that I didn't realize that one until reading Anna Anthropy's <i>ZZT</i> a few years back.)</p>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image14.gif"></p>

<p>In ZZT, <code>#if blocked DIRECTION</code> tells an object whether it is blocked in a given direction. And <code>#send OBJECTNAME:LABEL</code> tells another object, <i>OBJECTNAME</i>, to immediately begin executing code starting with <i>:LABEL</i>.</p>

<p>Slime enemies move erratically and leave breakable walls behind, so you can change their color frequently to fill empty spaces with different colors of breakable walls.</p>

<p>One last limitation and one last "coding" trick deserves mention. ZZT has a limit of about 20 kilobytes per board (including code) before the board misbehaves or gets corrupted at runtime. Objects can use the command <code>#bind OBJECTNAME</code> to work from the same exact instance of the same code as the other object <i>OBJECTNAME</i>, saving valuable code space.</p>

<p>Nowadays, one can "pre-bind" objects with external editors, causing them to share identical labels and behaviors but eliminating the need for the 5+ bytes of space that a <code>#bind</code> command needs.</p>

<h2>Two Big Insights</h2>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image18.png">
<img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image20.png"></p>

<p>In addition to the more concrete "tools" above, two big conceptual insights greatly impacted this project. The first is about level connectedness, and it hit me after reading Rabbitboots's musings on house generation in <a href="https://museumofzzt.com/file/d/dood2018.zip" target="_blank"><i>Doodads 2018</i></a>.</p>

<p>Say I have a house with four rooms, and I block off only one doorway in the house, as on the left. Any room in the house can still visit any other.</p>

<p>Rabbitboots goes on to note that if we make a larger house out of smaller ones, the large house remains fully connected.</p>

<p>If House A connects to House B and B connects to House C, then A connects to C.</p>

<p>This was huge for me. It gave me hope that I could avoid soft-locks while also making more interesting, varied levels.</p>

<p>Shortly after the 2019 release, while looking at TriphEd's maze generation in <a href="https://museumofzzt.com/file/b/BACKTRAX.zip" target="_blank"><i>Backtrax</i></a>, I had another "Aha!" moment.</p>


<p>The algorithm is best explained step-by-step, and you can follow along with the following roughly equivalent method if you have a pencil and paper.</p>

<p>(1) Start with a grid of dots. Any size 3 x 3 or up will do.</p>

<p>(2) Connect all the outer dots along the edge so that they make a rectangle.</p>

<p>(3) For each "middle dot" within the square, draw <i>exactly one line</i> from that dot to any dot directly above, below, left, or right of it. Edge dots can receive lines.</p>

<p>If you used a 3 x 3 grid of dots, the end result will resemble the 2 x 2 house above.</p>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/backtrax-levels.gif"></p>

<p>Any edge tile can access any other edge tile in this algorithm. It can leave some squares inaccessible from the edges, but this is acceptable if nothing essential to player progress is placed in those tiles.</p>

<p>In ZZT, you can accomplish this by having objects use <code>#put rndp rndne ELEMENT</code> on a grid, and that's exactly what <i>Backtrax</i> is doing.</p>

<p>A <i>very</i> small set of soft-lock possibilities does exist in <i>Backtrax</i>. Because there's a blocking linewall diagonally southeast of the lower-right-hand wall generator object, the wrong combinations of walls <i>could</i> block the player, but it is <i>incredibly</i> unlikely and easily remedied.</p>

<p>This is the most elegant level generator in ZZT to date--and also, in case it wasn't already elegant enough, it weighs in at only 10.9 kilobytes and re-uses its "grid" after you reach the exit, using a separate trick (that I won't discuss in detail here) to teleport the player back to the start for a new level.</p>

<p>Recognize TriphEd's genius.</p>

<p>After the initial <i>CDZ</i> release, I experimented a lot with this algorithm. Several generators in version 2.0 benefited from its use. We can now move slowly but surely into the actual inner workings of this game.</p>

<h2>Primary Generators</h2>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image6.png"></p>

<p><i>Computer Dungeon Slash</i> level generators have a "primary generator" object that handles a lot of needed dice-rolls and uses <code>#send</code> to communicate its randomization to different groups of objects. It's usually set up as on the left, or similar.</p>

<p>This particular setup involves fake walls placed north, south, east, and west of the primary generator and water (a blocking terrain) in the four corners. To make a four-direction roll, it uses <code>#put rndp rndne gem</code> and then iterates over some variation of:</p>

<code>#if blocked n #send object:option1
#if blocked e #send object:option2
#if blocked s #send object:option3
#if blocked w #send object:option4</code>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image16.png"></p>

<p>The objects referred to in the <code>#send object:option</code> commands  are pre-bound objects with exactly the same code, and positioned so that only one of them will place a wall, generate an important key, etc. at the time they're called. In this example, the pre-bound group of objects has this code:</p><p>

<code>@object
#cycle 1
#end
:option1
#if blocked n become solid
#die
:option2
#if blocked e become solid
#die
:option3
#if blocked s become solid
#die
:option4
#if blocked w become solid
#die</code></p><p>There is a fake wall in the middle of the house, so that only one object becomes a wall, and the rest die. Then the primary generator turns the fake into a solid wall.</p>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image23.gif"></p>

<p>This method also extends to larger houses similar to those in the <i>Doodads 2018</i> example, and can …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://museumofzzt.com/article/496/a-computer-dungeon-slash-postmortem">https://museumofzzt.com/article/496/a-computer-dungeon-slash-postmortem</a></em></p>]]>
            </description>
            <link>https://museumofzzt.com/article/496/a-computer-dungeon-slash-postmortem</link>
            <guid isPermaLink="false">hacker-news-small-sites-24646063</guid>
            <pubDate>Thu, 01 Oct 2020 00:59:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ontario doctors sign letter to Premier advising against sweeping lockdowns]]>
            </title>
            <description>
<![CDATA[
Score 108 | Comments 175 (<a href="https://news.ycombinator.com/item?id=24645821">thread link</a>) | @mrfusion
<br/>
September 30, 2020 | https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html | <a href="https://web.archive.org/web/*/https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div>



<div>
    
        <p>Published Sept. 30, 2020 9:40 a.m. ET</p>
        <p>Updated  Sept. 30, 2020 7:26 p.m. ET</p>
    
</div>



  


    
        
        
    
    
    <amp-iframe resizable="" width="16" height="17" layout="responsive" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation" allowfullscreen="" frameborder="0" src="https://ampvideo.ctvnews.ca/content/ctvnews/en/local/ottawa/2020/9/30/1_5126193.vidi.root-responsivegrid-vidicomponent.html?preventDefault=true">
        
        <p>Click to Expand</p>
    </amp-iframe>





    <p>OTTAWA -- 	Twenty-one Ontario doctors have signed a joint letter to Premier Doug Ford, urging him not to issue a new lockdown this fall because of rising COVID-19 case numbers.</p><p>	Daily numbers of new cases have risen dramatically in recent days, with Ontario recording <a href="https://toronto.ctvnews.ca/new-covid-19-cases-in-ontario-reach-highest-mark-ever-1.5122863" target="_blank">700 new cases of COVID-19 on Monday</a> – the highest number of new cases recorded in a single day.</p><p>	However, the 21 doctors who signed the letter to the premier say another provincewide lockdown—similar to what was in place in the spring—would not be helpful.</p><ul>	<li>		<a href="#Full letter"><i>Read the full letter below</i></a></li></ul><p>	"We are writing this letter in support of the governments’ plan to use a tactical localized approach, rather than sweeping new lockdown measures, to deal with the increasing COVID case numbers in Ontario," the letter says.&nbsp;</p><p>	"Lockdowns have been shown not to eliminate the virus. While they slow the spread of the virus, this only lasts as long as the lockdown lasts. This creates a situation where there is no way to end the lockdown, and society cannot move forward in vitally important ways including in the health sector, the economy and other critically important instrumental goods including education, recreation, and healthy human social interactions."</p><p>	Speaking on Newstalk 580 CFRA's "The Morning Rush with Bill Carroll" in Ottawa, CTV's infectious disease specialist Dr. Neil Rau—a signatory of the letter—pointed to two data points to consider when looking at the spread of COVID-19.</p><p>	"We're reacting to increased aggregate case numbers, but the percentage positive is not really as bad as it used to be," he said. "We're testing more people, so we're finding more cases […] We're driving our numbers up, it's worse than it was in the summer, but it's not what it was last winter and life has to go on."</p><ul>	<li>		<a href="https://www.iheartradio.ca/580-cfra/podcasts/the-morning-rush-dr-neil-rau-interview-why-we-don-t-want-another-lockdown-1.13612912?mode=Article" target="_blank"><strong>LISTEN NOW: "The Morning Rush": Dr. Neil Rau - Why we don't want another lockdown</strong></a></li></ul><p>	Monday's 700 cases in Ontario came from 41,111 total tests, for a positive percentage rate of 1.7 per cent. On April 24, <a href="https://toronto.ctvnews.ca/highest-number-of-new-covid-19-cases-in-a-single-day-reported-by-ontario-health-officials-1.4910216" target="_blank">the previous watermark of 640 new cases in a single day</a>, the result came from 12,295 tests, for a positive percentage rate of 5.2 per cent.</p><p>	Testing capacity was lower in the spring than it is now, and testing criteria has changed over time; however, Dr. Rau and the other signatories of the letter said the increasing case numbers are not leading to unmanageable levels of hospitalizations.</p><p>	"In Ontario and other parts of the world, such as the European Union, increasing case loads are not necessarily translating into unmanageable levels of hospitalizations and ICU admissions," the letter says.</p><p>	The letter also points to other health impacts linked to the lockdown.</p><p>	"Hard data now exist showing the significant negative health effects shutting down society has caused. Overdoses have risen 40% in some jurisdictions. Extensive morbidity has been experienced by those whose surgery has been cancelled, and the ramifications for cancer patients whose diagnostic testing was delayed has yet to be determined," the letter states.</p><p>	"Economic harms are health harms," Dr. Rau told CFRA. "It sounds horrible to say, but it's true. Health is wealth. We all know this."</p><h2>	<strong>LETTER POINTS TO DISAGREEMENT AMONG HEALTH PROFESSIONALS</strong></h2><p>	Other Ontario health professionals have been arguing for increased restrictions as cases rise.</p><p>	Last week, the Ontario Hospital Association released a letter signed by 38 health professionals which <a href="https://ottawa.ctvnews.ca/ontario-hospital-association-calls-for-return-to-restrictions-on-non-essential-businesses-1.5119832" target="_blank">called for immediate restrictions to be re-imposed on non-essential businesses</a>, such as gyms, dine-in restaurants and bars, nightclubs, and theatres. It also calls on restrictions on other places where people can gather, such as places of worship.</p><p>	The letter from the OHA said regions where the speed of transmission was underestimated are “now facing the consequence of increased hospitalization rates, including a rise in intensive care unit (ICU) admissions and more deaths.”</p><p>	Hospitalizations in Ontario have been increasing, but have not yet reached the same level that was seen in the spring.</p><p>	According to <a href="https://covid-19.ontario.ca/data" target="_blank">data from the Ontario government</a>, there were 128 people in hospital in Ontario with COVID-19 complications on Monday—the day 700 new cases were recorded—up from 65 a week before; however, on April 24—when 640 new cases were recorded—government data shows that there were 910 people in hospital. The peak for hospitalizations in Ontario came in May, when there were days when more than 1,000 people were hospitalized. That number steadily decreased from May through the summer before it began going up again in September.</p><p>	Speaking on CTV Morning Live Ottawa, infectious disease specialist Dr. Abdu Sharkawy suggested t<a href="https://ottawa.ctvnews.ca/video?clipId=2045684" target="_blank">emporary restrictions on gathering would help curb the spread of COVID-19</a>.&nbsp;</p><p>	"We don't want to see our hospitals overwhelmed," he said. "We're still waiting for flu season and a whole bunch of other respiratory viruses to hit us and our capacity to be challenged. We don't want that to happen. We need everybody to try and simplify their lives and minimize anything that's non-essential."</p><p>	Dr. Sharkawy said regions where cases are rapidly rising may need to impose new restrictions to get the spread of the virus under control.</p><p>	"I think it's abundantly clear that, particularly in hot spots like Ottawa, Toronto and Peel region, the situation is not well controlled," he said. "Sometimes you need blunt instruments, even if they're temporary in nature, to make sure that you curtail the spread of this virus because it gets away from us a lot more quickly than many of us can anticipate sometimes."</p><p>	Dr. Sharkawy suggested hot spots follow the lead of Quebec, which imposed <a href="https://montreal.ctvnews.ca/life-in-the-red-zone-here-s-what-you-can-and-can-t-do-1.5125093" target="_blank">harsh restrictions on three areas in the province</a>, including Montreal and Quebec City, banning private gatherings and closing bars and restaurant dining rooms.</p><p>	"I think we should do it now," Dr. Sharkawy said of Ontario. "I think we all need to adopt an attitude and an approach that recognizes that we have to do what's absolutely necessary to keep everybody safe."</p><h2>	<strong>FULL LETTER FROM ONTARIO DOCTORS TO THE PREMIER</strong></h2><p>	Dear Premier Ford,</p><p>	We are writing this letter in support of the governments’ plan to use a tactical localized approach, rather than sweeping new lockdown measures, to deal with the increasing COVID case numbers in Ontario. Lockdowns have been shown not to eliminate the virus. While they slow the spread of the virus, this only lasts as long as the lockdown lasts. This creates a situation where there is no way to end the lockdown, and society cannot move forward in vitally important ways including in the health sector, the economy and other critically important instrumental goods including education, recreation, and healthy human social interactions.</p><p>	In Ontario the increase in cases at this time are in people under 60 years of age who are unlikely to become very ill. At the peak of the pandemic in Ontario in mid-April, 56% of cases were in ≥60 year olds, now in Sept only 14% of cases are in ≥60 year olds. In Ontario and other parts of the world, such as the European Union, increasing case loads are not necessarily translating into unmanageable levels of hospitalizations and ICU admissions. This is not a result of a lag in reporting of severe and fatal cases. While we understand the concerns that these cases could spill into vulnerable communities, we also need to balance the actual risk. As the virus circulates at manageable levels within the community, we need to continue the gains we have made in the protection of the vulnerable in long-term care and retirement institutions, and continue to educate other people about their individual risk, so that they can observe appropriate protective measures.</p><p>	Lockdowns have costs that have, to this point, not been included in the consideration of further measures. A full accounting of the implications on health and well-being must be included in the models, and be brought forward for public debate. Hard data now exist showing the significant negative health effects shutting down society has caused. Overdoses have risen 40% in some jurisdictions. Extensive morbidity has been experienced by those whose surgery has been cancelled, and the ramifications for cancer patients whose diagnostic testing was delayed has yet to be determined. A huge concern is the implication of closure of schools, and the ongoing reluctance we have seen in the large urban centers of sending children back to the classroom due to safety concerns. Global data clearly now show that children have an extremely low risk of serious illness, but they are disproportionately harmed by precautions. Children’s rights to societal care, mental health support and education must be protected. This cannot be achieved with ongoing or rotating lockdown.</p><p>	The invitation and involvement of other health experts to advise the government’s response beside individuals in Public Health and Infectious Diseases in addition to leaders in the business, securities and arts communities is essential. We also call for increased open debate, in the public forum, that hears voices from outside the medical and public health communities, in order to consider all points of view from society. This is a fundamental principle upon which democratic societies are built. All stakeholders should have an equal right to participation in public discourse when it comes to setting such fundamental and sweeping societal interventions.</p><p>	All have the right to feel their voices have been heard, and moreover to ensure factual credible data is openly debated, in contrast to the personal and political slants that have had apparent significant impacts on the management of the virus to date. Our society has borne enormous pain over the past 6 months. It’s time to do something different.</p><p>	Sincerely,</p><p>	Jane Batt MD, PhD, FRCPC. Respirologist, Associate Professor, Department of …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html">https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html</a></em></p>]]>
            </description>
            <link>https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24645821</guid>
            <pubDate>Thu, 01 Oct 2020 00:23:56 GMT</pubDate>
        </item>
    </channel>
</rss>
