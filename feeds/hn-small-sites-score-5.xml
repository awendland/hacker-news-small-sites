<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 5]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 5. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Sat, 12 Dec 2020 04:34:53 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-5.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Sat, 12 Dec 2020 04:34:53 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Digital Twins, a Requirement for Industrial AI]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25371343">thread link</a>) | @MorganeR
<br/>
December 10, 2020 | https://blog.senx.io/digital-twins-requirement-for-industrial-ai/ | <a href="https://web.archive.org/web/*/https://blog.senx.io/digital-twins-requirement-for-industrial-ai/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Using AI to make industrial assets more efficient and reduce their downtime is on many agendas. Learn how digital twins and time series data play a major role in this plan.</p><article>
      
<p><strong>When interviewed, CEOs across industries all state that AI is part of their top priorities.</strong> But when it comes to actual implementation AI projects are not very glamorous. Past simple proofs of concept and the hiring of a team of data scientists, there is usually no sign of the highly anticipated digital transformation wished by the CEOs.</p>



<p>There are multiple reasons for this disenchantment, far too many to list here. But among those, some are directly related to what we focus on at <a href="https://senx.io/" target="_blank" rel="noreferrer noopener">SenX</a>, data, and the way industries introduce them in their environment.</p>



<h2>No digital transformation without data</h2>



<p>The willingness to transform is genuine in many organizations, driven by ambitious visions or just the consciousness that the competitive landscape is evolving.</p>



<p>The next step is usually for those businesses to pick some quick wins to prove that the transformation can be initiated and comfort everyone that it does not mean changing teams or radically modifying their way of working.</p>



<p>Those short projects aim at demonstrating the methodology for transforming limited operational perimeters. They often involve solving a problem with approaches to leveraging new technologies. Those technologies, 100% digital, need fuel to work, and that fuel is data. <strong>So the first step is to ensure data are available</strong>.</p>



<p>The firms hired to help in building those quick wins will then wander among departments. They will harvest datasets here and there until they have sufficient matter for implementing their solutions.</p>



<p>This step can sometimes take time if the data is not well identified and distributed across the organization. But it is a mandatory path to follow as without data no digital transformation will happen.</p>



<figure></figure>



<h2>No AI without big data</h2>



<p>Past the simple quick wins done to bootstrap the transformation comes a time when more ambitious projects are brought on the table, and that is when AI (Artificial Intelligence) comes into the conversation. The hype around AI is so strong that projects around AI and ML (Machine Learning) cannot be neglected.</p>



<p>The problem with the current hype is that very few people really understand what AI actually implies. <strong>For many</strong>,<strong> you buy an AI like you buy a Microsoft Office 365 subscription</strong>, this is just not true. The promise of AI is to bring new, automatic, ways to use data to help in or even completely assume the decision process. This promise can only be fulfilled if the actual AI put to work, otherwise called the model, is actually trained on the data in your very own organization, and this requires once again the same digital transformation fuel, data. The difference is that this time you need more of it. You are no longer trying to light a fondue burner but a rocket engine!</p>



<p>Training a model does indeed require a lot of data covering the various aspects of your business operations you want the model to focus on, also covering a long period of time so trend and seasonality can be modeled. </p>



<h4>This has several impacts</h4>



<ul><li>The first is that you cannot expect to train a model and efficiently introduce AI in your operations until you actually have collected enough meaningful data. And if your organization has not done so so far you need to start as soon as possible. </li><li>The second impact is that this data collection process is not a one time job. It does not stop once you have enough data for training a model. It needs to go on and on so you keep on accumulating signals on how your business operates to retrain your models in the future if their performance starts to degrade. This means that prior to your journey into the core of AI you need to plan for big data to be collected, stored and made available to teams across your organization so they can start looking at the data and imagine possible uses and models.</li></ul>



<h2>No industrial AI without Digital Twins</h2>



<p>Among verticals, industrial organizations face the hardest problems of data collection. Industries whose data mainly relates to users using their services are lucky. In the end, their data are not that massive. Sure we have all heard stories of banks or retailers hoarding piles of data. But we are talking about a few thousand interactions per year per user. So even with a billion users, which not that many banks or retailers have, we are talking a few trillion events per year.</p>



<p>In the industrial world, things are different, the assets producing data do not eat or sleep. They work day and night and sometimes produce thousands of measurements per second.</p>



<h3>For example...</h3>



<p>Take for example the CERN experiments at the LHC. They produced 600 million events per second during the campaigns for the quest of the Higgs boson. That is 51 trillion events per day. Luckily for the CERN, not all events needed to be retained. With highly efficient AI-based detectors, which needed to be trained with massive data themselves, they were able to limit the production to 100 000 events per second sent for digital reconstruction and ultimately 200 events persisted per second. </p>



<p>But other sectors need to retain more data. Synchro phasors (or PMUs, phase monitoring units) monitoring electrical grids, for example. They each produce several 1000s measures per second, and there are thousands of those at the scale of a country like France. This means millions of C37.118.2 messages sent every second, not to mention the IEC61850 messages sent to supervise the substations. </p>



<p>Same thing in aeronautics where aircraft typically produce 5 000 to 15 000 data points per second they are operating, or industrial assets whose PLC (Programmable Logic Controllers) track the state of many sensors and actuators.</p>



<p>The use of AI in those verticals requires that those truly massive data be collected and organized. Since they are data related to physical assets, it is wise to use an approach which mimics these assets in a digital form, this approach is called Digital Twins. </p>



<figure></figure>



<h3>What are Digital Twins?</h3>



<p>The Digital Twin of an asset is the set of measures coming from its sensors and actuators. Those measures need to be tracked in time to catch the dynamics of the assets' operations. And the technology of choice to do so is a <a href="https://blog.senx.io/which-time-series-database-suited-to-your-needs/" target="_blank" rel="noreferrer noopener">Time Series Database</a>. Indeed Digital Twins are nothing else than time series, some for the sensors, some for the actuators with their states. And if you want more advanced digital twins, some with the control commands sent to the assets to modify how it behaves.</p>



<p>Once you start collecting the data from your assets in a Time Series Database, you can easily access the state of those assets at any point in time. More importantly, you can start extracting features to train models to detect anomalies and perform predictive maintenance.</p>



<h2>Takeaways</h2>



<p>AI is on every business' agenda, but the importance of data is too often overlooked. <strong>When it comes to industrial AI, the first step towards a successful implementation is the collection of all sensor data to build Digital Twins of the physical assets involved.</strong> This approach needs to leverage a Time Series Database, the kind of database SenX offers with the <a href="https://warp10.io/" target="_blank" rel="noreferrer noopener">Warp 10 Time Series Platform</a>.</p>



<p><a href="mailto:contact@senx.io" target="_blank" rel="noreferrer noopener">Contact us</a> to learn how SenX and its technologies can help you master your industrial AI adventure.</p>








<!-- relpost-thumb-wrapper --><!-- close relpost-thumb-wrapper -->      
           
    </article></div>]]>
            </description>
            <link>https://blog.senx.io/digital-twins-requirement-for-industrial-ai/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25371343</guid>
            <pubDate>Thu, 10 Dec 2020 08:45:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Filmbox – Physically accurate motion picture film emulation]]>
            </title>
            <description>
<![CDATA[
Score 74 | Comments 43 (<a href="https://news.ycombinator.com/item?id=25367371">thread link</a>) | @wilg
<br/>
December 9, 2020 | https://videovillage.co/filmbox/ | <a href="https://web.archive.org/web/*/https://videovillage.co/filmbox/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><img src="https://videovillage.co/images/filmbox/hero-ee5486c3.jpg" alt="Hero"></p><p><h2>A complete reproduction of photochemical motion picture imaging.</h2><h2>Driven by empirical data and tailored to specific digital sensors.</h2><h2>Built for high-end production. Right in DaVinci Resolve.</h2></p><div><div><p><h2>The look and feel of motion picture film defines a century of art.</h2><h2>As we move beyond the photochemical process, we need not leave behind its aesthetic quality.</h2></p><div><div><div><p><h2>Film negative has a unique response to light intensity and wavelength. Filmbox reproduces this behavior using rich empirical datasets to transform digital sensor values to exhibit the same non-linearities.</h2></p></div></div></div><div><div><div><p><h2>As light strikes color film negative, different wavelengths scatter to different degrees within the layers of emulsion and affect neighboring image regions. Filmbox convolves the digital image data to recreate the soft yet detailed quality of the negative.</h2></p></div></div></div><div><div><div><p><h2>The perceptual intensity of film grain varies with the density of the developed negative. Filmbox reproduces the quality and tonal distribution of grain as well as other subtle effects of the development process.</h2></p></div></div></div><div><div><div><p><h2>Film camera transport mechanisms are not perfectly stable, and labs are not perfectly clean. If desired, Filmbox can model the subtle instability of real 35mm and 16mm cameras, and procedurally place samples of real dust.</h2></p></div></div></div><div><div><div><p><h2>The look of projected film is the combination of the characteristics of the negative and the print.</h2><h2>Filmbox maps the emulated negative to the light output of the digital display using a characterization of the combined photometric response of an actual contact printed negative.</h2></p></div></div></div></div></div><div><div><video autoplay="" data-sources="W3sic3JjIjoiL2ltYWdlcy9maWxtYm94L3ZpZGVvL2NvbXBhcmlzb24vdmlk
ZW8ubXA0IiwidHlwZSI6InZpZGVvL21wNDsgY29kZWNzPVwiYXZjMVwiIn1d
" loop="" muted="" playsinline="" poster="https://videovillage.co/images/filmbox/video/comparison/poster-c124bf44.jpg"><img alt="Comparison of Filmbox and actual film" src="https://videovillage.co/images/filmbox/video/comparison/poster-c124bf44.jpg"></video></div></div><div><div><p><h2>Built for simplicity, no tweaking necessary.</h2><h2>Consistent, predictable, understandable by the whole creative team.</h2><h2>Plenty of knobs under the hood if you want to tinker.</h2></p><div><p><img src="https://videovillage.co/images/filmbox/features/default-17385d13.jpeg" alt="Default"></p></div></div></div><div><div><p><img src="https://videovillage.co/images/filmbox/workflow_fb-f68b50df.jpg" alt="Workflow fb"></p><p>Pro Workflows</p><div><p>Profiled for Alexa, Venice, RED, Varicam, Blackmagic URSA, C300II</p><p>Work in the camera's native space, Resolve Intermediate, or ACES</p><p>Set looks between the Negative and Print to simulate DI and printer lights</p><p>Output as negative, or as print to standard display spaces or ACES</p></div></div><div><p><img src="https://videovillage.co/images/scatter/gpu-52db23e1.jpg" alt="Gpu"></p><p>Fast</p><div><p>Built for DaVinci Resolve using GPU acceleration</p><p>Realtime performance at DCI 4K</p></div></div></div><section><div><p><img width="128" height="128" src="https://videovillage.co/images/filmbox-6cdfa86d.png" alt="Filmbox"></p><p>Filmbox</p><p>Really good film emulation</p></div><div><div><p>Filmbox is still in early access so we can make sure it works great.</p>

<p>Plugin for DaVinci Resolve. Requires macOS 10.15 or later and DaVinci Resolve 16 or later.</p>
</div></div></section><section id="footer"><p>© &amp; ™ 2014-2020 Video Village, LLC</p><p>Made in California by <a href="http://gregcotten.com/">Greg Cotten</a> &amp; <a href="http://wilgieseler.com/">Wil Gieseler</a> &amp; <a href="http://afinch.com/">Andrew Finch</a>.</p></section></div></div>]]>
            </description>
            <link>https://videovillage.co/filmbox/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25367371</guid>
            <pubDate>Wed, 09 Dec 2020 23:57:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Common Problems in Financial Services Reconciliation]]>
            </title>
            <description>
<![CDATA[
Score 87 | Comments 14 (<a href="https://news.ycombinator.com/item?id=25363650">thread link</a>) | @kunle
<br/>
December 9, 2020 | https://kunle.app/dec-2020-financial-reconciliation.html | <a href="https://web.archive.org/web/*/https://kunle.app/dec-2020-financial-reconciliation.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  				
  				 <p>December 2020</p>

<p>If your product moves money on behalf of customers, and you manage the ledger, you need reconciliation. You can think of recon as the process of making sure every transaction in your system matches one in the external world. For every dollar you’ve moved, an external entity agrees with the amount and direction, and has provided “documentation” to that effect.</p>
<p>I’ve seen a few performant recon systems that came under stress as they scaled, and I’ve worked with startups at early enough stages that getting recon right was not existential. I’ve also never built a recon system from scratch. Everything I'll say here is from the perspective of a user of recon systems rather than a maker of them. I’ve been a customer of a couple, and their performance impacted my output so I have strong opinions about what I wish would exist. Since I'm not sure what a perfect recon system would look like, I'm writing this to flesh out the thought, and to smoke out anyone who already knows.</p>
<p>I’ve interacted with reconciliation systems aimed at two types of problems:</p>
<h2>Reconciliation of transactions</h2>
<p>Transaction oriented recon makes sure that some external party agrees with every money movement in your ledger. I saw this philosophy in the recon systems I interacted with at Cash App &amp; Square in general. In this model, the objective is to make sure that every money movement action matches your intention. This means the state of the transaction, the direction, and the amount are what you expect. A secondary objective is ensuring the timing matches your intention. This is secondary because in a lot of cases, the actual precise timing doesn't matter as long as it happens&nbsp; "soon", and as long as the underlying accounts aren’t run at a $0 balance. In this case reconciliation solves an accounting problem, ensuring money movements are correct. It also helps ensure that the company's receivables and payables are complete, is useful for regulatory &amp; financial audits, and empowers your treasury team to make good cash management decisions. I suspect most acquirers (Stripe, Square, Adyen, etc.) at least start by pursuing transaction oriented recon.</p>
<p>Typically in an acquiring world, you construct the “internal” ledger from the settlement/capture messages generated by the card networks. This is what product teams look at to inform customer facing features. You construct the “external” ledger from settlement files generated by the acquiring bank. Accounting teams look at the external ledger (technically accounting teams look at both ledgers, but product teams rarely look at the external ledger on an ongoing basis).</p>
<p>A recon system often includes an engineering team paired with an operations team, working together. In cases where the internal and external ledgers disagree, a human (on the ops team) reviews the data. They determine what’s causing the exception, whether it's systematic, how frequently it occurs, and what to do to fix it. The eng team continually optimizes the process to reduce the exception rate over time. Transaction oriented recon primarily solves accounting problems. You’re typically working towards SLAs designed for monthly/quarterly earnings close, and your outputs feed into income/cash flow statements.</p>
<h2>Reconciliation of balances</h2>
<p>Balance oriented recon ensures precise amounts in bank or customer accounts on a periodic basis. You use the same internal and external ledgers as in transaction oriented recon. However, you're comparing not only the amounts, state and direction of a transaction, but also its timing. This type of recon system can be useful for accounting, but is ideal for building systems that report a balance at a point in time. One example is a banking system of record. In the case of a system of record, a balance oriented recon system informs customer-facing balances and FDIC insurance.</p>
<p>Balance oriented recon systems are required for organizations that issue instruments and are the final source of truth for their own ledger. Most financial technology companies today rely on the ledgers managed by their infrastructure providers. For instance, if you issue cards, the banking as a service platform typically connects to the bank’s core, and most traditional bank cores have a balance oriented recon framework built in.</p>
<p>For context - in order to provide FDIC insurance to customers, banks are required to provide an auditable record of customer balances at any point in time. This is usually solved by being able to provide a daily snapshot of customer balances. This function is one of several provided by core processors, and as a result most core processors have a balance oriented recon process built in by default.</p>
<p>However if you’re the rare card issuer managing your own ledger (or really building any kind of financial product where you’re responsible for your own ledger, such as a digital wallet where you own the money transmission licenses), you’ll need to build a balance oriented recon system eventually. It's the way you’ll be sure you have the money that you’re telling customers you have.</p>
<h2>Common Problems in Recon</h2>
<h3>Adding a new money movement type to a single balance</h3>
<p>One overarching problem that affects all recon systems is what happens when new types of money movement impact a balance. For instance, imagine you run a digital wallet where your primary funding and cash-out transaction types are ACH debits and credits. Also, imagine you’ve built a perfect reconciliation system, with the combination of technology and human process that allows you to tie out balances and payments with zero failures (this is super unlikely). The moment you add payment cards as funding/cash out instruments, you now have a different external ledger to integrate with. It will have different edges than you're used to. You'll deal with potentially different organizations, who have different processes for resolving exceptions. No matter what you do, this will take time to get right, and long after your new feature is launched, you’ll probably discover new, undocumented quirks. Some of these quirks will only be clear when you’re processing money movements at scale. I’ve seen cases where the incorrect MID set with a card network resulted in hundreds of millions of dollars routed to the wrong (internal) account. Survivable error as the transactions were reconciled in aggregate, but bad for accounting and distraction caused to cross functional team members pulled in to swarm the problem.</p>
<h3>Timing differences between authorization and settlement</h3>
<p>For balance oriented reconciliation systems in particular, solving timing problems is critical. Timing problems typically occur when a) the payment authorization time and the settlement time are different, and your system’s not necessarily aware, b) you’re dealing with payment types where the settlement amount can be adjusted multiple times c) your ledger updates customers balances when a new payment authorization comes in, rather than a settlement message. In all these cases you’re grappling with a few questions (I don’t actually know the right answers to these):</p>
<ul>
<li>When should you update a customer’s balance? When you know there is a transaction (when the auth comes in) or when amount is finalized (when the settlement comes in)</li>
<li>When the authorization and settlement amounts are different, do you retroactively adjust the balance for the day the authorization came in? Or do you fix that in place and only adjust the balance with the delta on the day the settlement arrives?</li>
<li>Is your snapshot on a particular day immutable (i.e. it can never be adjusted) and if so, how do you handle changes in amounts between the authorization and the settlement?</li>
<li>Traditional core systems will have an available balance (which is how much you can access, with pending transaction amounts removed), and an account balance (which includes pending transactions, and is typically higher than the available balance).</li>
</ul>
<h3>Relying on aged systems for exception handling</h3>
<p>Very often you work with a wholesale bank whose systems are seasoned and handle the majority of exceptions using manual workflows. This can be frustrating; you’re faced with either adopting their manual processes, which bind your cost structure to theirs, or accepting a higher exception rate temporarily while you build technical systems around their process. There’s no easy trade-off here.</p>
<h3>Early prototyping and float problems</h3>
<p>In the course of product development you’ll often prototype by adding new money movement types to your ledger. A lot of these prototypes (as should happen) will be discarded. Despite this, they will have moved real money and affected your real ledger, and (at least for your accounting team's sanity) you’ll need a stateful way to reconcile the money that moved to your ledger. While at Cash I once spent a year integrating into 6 card issuer processor systems while prototyping the Cash Card. With each integration we needed a float (depositing funds with the card issuer so we could test transactions in the real world) which meant our accounting team now had 6 new banking relationships to monitor, 4 of which lasted less than 6 months, but all of which required material floats amounts. In a few cases, the issuer processor didn’t actually enable us to manage our own ledger, so we’d have a parallel ledger (one on our databases and a mirror on theirs) that we’d have to keep in sync. There was at least one integration that we ultimately discarded, which took us several months to reconcile, long after we’d walked away from the partnership. How you handle these cases will depend on what’s financially “material” for your organization. In our cases, prototype floats were all sub $100k, so survivable at our scale. But tracking these down repeatedly was an insane level of tedium.</p>
<h3>Managing ledgers across many internal bank accounts</h3>
<p>Sometimes you’ll contract with multiple banks for different financial services. For instance one bank for merchant acquiring and another for …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://kunle.app/dec-2020-financial-reconciliation.html">https://kunle.app/dec-2020-financial-reconciliation.html</a></em></p>]]>
            </description>
            <link>https://kunle.app/dec-2020-financial-reconciliation.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25363650</guid>
            <pubDate>Wed, 09 Dec 2020 19:50:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Open source Internet-less IRC using LoRa, for disaster resilience]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25363427">thread link</a>) | @spiritplumber
<br/>
December 9, 2020 | http://f3.to/cellsol/ | <a href="https://web.archive.org/web/*/http://f3.to/cellsol/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                
    


    








    <article>
        
        



    



<p>Welcome to CellSol!</p>
<p>This website and its associated project are now in beta! Come take a look at our <a href="https://github.com/RbtsEvrwhr-Riley/CellSol/">GitHub</a></p>
<p><img src="http://f3.to/cellsol/media/cellsol_large_236.png" alt="CellSol Logo - Large"></p>

<div><hr>
<h2 id="lastmod-2020-11-24">publishdate: 2019-11-17
lastmod: 2020-11-24</h2>

<p>What if the internet and cell phone towers went down for more than a few hours - or during an emergency situation? No communication could lead to lost lives.</p>

<p>The end goal of the project is to have a widespread network able to handle low-bandwidth traffic (text, compressed images) for a large number of users, to fill gaps when the larger Internet is unavailable.</p>
<p>In the event of an emergency, the CellSol network, much like the Internet, can be used as a knowledge base, as well as a rally point, giving people a tool to use to coordinate and organize even if 
other communication systems go down. We intend to scale the design, with long-haul routing capabilities, so that regional networks can intercommunicate and interoperate, allowing for a wider breadth of use cases.</p>

<p>The overall design is a <a href="https://en.wikipedia.org/wiki/Mesh_networking">mesh network</a> of <a href="https://www.semtech.com/lora/what-is-lora">LoRa</a> devices, called “Pylons”
that act as repeaters (extending the range of the network). Terminals (devices that users access the network with) also repeat packets, so that a network
made up entirely of end users is possible.</p>
<p>The two basic types of pylons are the ESP32 WiFi Pylon (a terminal device) and the Ardunio Repeater Pylon (a pure repeater, but can have bluetooth to use as a terminal).</p>
<p>Pylons with more specialized uses, such as data repositories for local emergency resources (phone numbers, shelter locations, etc.) and knowledge bases (such as a Wikipedia mirror) are also intended to be
developed in the future, to add to the overall usefulness of the network.</p>
</div>



    </article>

                








    
    

    







            </div></div>]]>
            </description>
            <link>http://f3.to/cellsol/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25363427</guid>
            <pubDate>Wed, 09 Dec 2020 19:39:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Native JavaScript Document API for Cassandra]]>
            </title>
            <description>
<![CDATA[
Score 23 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25362944">thread link</a>) | @Gulthor
<br/>
December 9, 2020 | https://stargate.io/2020/12/09/announcing-stargate-10-ga-rest-graphql-schemaless-json-for-your-cassandra-development.html | <a href="https://web.archive.org/web/*/https://stargate.io/2020/12/09/announcing-stargate-10-ga-rest-graphql-schemaless-json-for-your-cassandra-development.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <p>
        
        <h4>Level-up your app dev with fast and easy data APIs for the world’s most battle tested database.</h4>
      </p>
    </div><div>
      <div>
        <p><img data-src="/assets/images/stargate-profile.png" alt="Denise Gosnell" width="32" height="32" src="https://stargate.io/assets/images/stargate-profile.png">
          <span>By <span>Denise Gosnell</span></span>
          •
          <span>Dec 9, 2020</span>
        </p>
      </div>
    </div><div>
      <div>
        <div>
          <p>It is a really great time to be a developer.</p>

<p>We have tons of APIs integrated within great tools for building dynamic, full stack apps. If you are a developer, you probably are using technologies like schemaless data stores, serverless architectures, JSON APIs, and/or the GraphQL language.</p>

<p>Further, there are a bunch of cool frameworks like the <strong>Jam</strong>stack (<strong>J</strong>avaScript, <strong>A</strong>PIs, and <strong>M</strong>arkup) and services like Netlify to make it fast to deploy a serverless app.</p>

<p>And now, for the first time ever, Apache Cassandra is a part of this stack because <a href="https://astra.datastax.com/">Stargate is now live on Astra</a> as the official data API.</p>

<p>The modern apps we build need data APIs which integrate into our toolset and work with native data shapes (JSON, REST, GraphQL, etc). These data APIs need to support schemaless JSON, while simultaneously providing speed and scalability.</p>

<p>Most importantly, it better only take a few minutes for us to use them within our project.</p>

<p>DataStax built <a href="https://stargate.io/">Stargate</a> into Astra to give us, app developers, a natural data API stack which meshes with the Jamstack (or serverless stack of your choice). Stargate in Astra is built on the rock solid NoSQL data engine (Apache Cassandra) which powers Netflix, Instagram, Yelp, iCloud and other apps we all use everyday.</p>

<h2 id="what-exactly-is-stargate">What exactly is Stargate?</h2>
<p>Stargate is an <a href="https://stargate.io/2020/09/14/init-stargate.html">open source data gateway</a> that sits between your app server and your databases. Stargate brings together an API platform and data request coordination code into one OSS project.</p>

<p>Multiple successful app companies - like Netflix and Yelp - built their own data gateways to help internal app developers create features using simple APIs, without needing to learn the underlying database or mess with schema.</p>

<p>DataStax integrated Stargate into Astra to give you the same power and ease of access to your data.</p>

<p>What does this mean for you?</p>

<ul>
  <li>No upfront data modelling needed for Documents.</li>
  <li>Less custom code to maintain.</li>
  <li>More time to build what you care about.</li>
</ul>

<p><img alt="" data-src="/assets/images/stargate-astra/stargate-astra.png" src="https://stargate.io/assets/images/stargate-astra/stargate-astra.png"></p>

<p>You can work with your data the way you want – JSON via schemaless document APIs or database schema aware GraphQL and RESTful APIs – while Stargate serves as the proxy that coordinates these requests to different flavors of Cassandra.</p>

<p>To see it in action, let’s see how this works by using JSON with Stargate’s schemaless Document API in a TikTok clone. Because, if Instagram and Snapchat have a TikTok clone, we should have one, too. Right?</p>

<h2 id="real-quick-note-first">Real Quick Note First</h2>

<p>Slinging JSON to and from Apache Cassandra without data modeling is just too much fun. You gotta <a href="http://astra.datastax.com/">try this out in Astra for yourself</a>. You can get <a href="https://www.datastax.com/dev/documents-api">hands on with it right away</a> or check out our <a href="https://astra.datastax.com/sample-app-gallery">sample app gallery</a> to see schemaless Cassandra in action.</p>

<p>We are stoked to have engineers from Netflix, Burberry, Macquarie Bank, USAA, and Yelp creating Stargate with us. They are already hard at work battletesting the APIs and collaborating on new features.</p>

<p>Ok, onto the code!</p>

<h2 id="posts-in-tiktok">Posts in TikTok</h2>

<p>We are going to walk through using Stargate’s APIs in Astra for creating and updating posts within a TikTok clone. We’re walking through examples that are ready to be pasted into your latest Jamstack app.</p>

<p>To use Stargate in Astra in your app, first install and set up our <a href="https://www.npmjs.com/package/@astrajs/collections">JavaScript SDK</a>. You can learn about storing environment <a href="https://www.youtube.com/watch?v=vSmzEGZQI5A">variables in your .env file here</a>.</p>

<p>Let’s start with a basic TikTok post: a video with a caption, like:</p>

<figure><pre><code data-lang="javascript"><span>const</span> <span>postData</span> <span>=</span> <span>{</span>
  <span>"</span><span>postId</span><span>"</span><span>:</span> <span>0</span><span>,</span>
  <span>"</span><span>video</span><span>"</span><span>:</span> <span>"</span><span>https://i.imgur.com/FTBP02Y.mp4</span><span>"</span><span>,</span>
  <span>"</span><span>caption</span><span>"</span><span>:</span> <span>"</span><span>These ducks are cute</span><span>"</span><span>,</span>
  <span>"</span><span>timestamp</span><span>"</span><span>:</span> <span>"</span><span>2020-12-09T09:08:31.020Z</span><span>"</span><span>,</span>
  <span>"</span><span>likes</span><span>"</span><span>:</span> <span>0</span><span>,</span>
<span>}</span></code></pre></figure>

<p>After connecting to Stargate in Astra with a nodejs client, let’s create a new collection in our app and add the post with:</p>

<figure><pre><code data-lang="javascript"><span>const</span> <span>postsCollection</span> <span>=</span> <span>astraClient</span><span>.</span><span>namespace</span><span>(</span><span>"</span><span>tikTokClone</span><span>"</span><span>).</span>
  <span>collection</span><span>(</span><span>"</span><span>posts</span><span>"</span><span>);</span>

<span>const</span> <span>post</span> <span>=</span> <span>await</span> <span>postsCollection</span><span>.</span><span>create</span><span>(</span><span>postData</span><span>);</span></code></pre></figure>

<p>If you’ve ever used Cassandra before, you know this is amazing. Look at what we didn’t do: no data modeling, no table creation, no configuration code, no partition keys, no clustering columns. I think you get my drift.</p>

<p>Stargate in Astra allows you to add data to Apache Cassandra in one line of code.</p>

<p>This level of ease of use hasn’t previously been possible with Cassandra. Insert JSON and move on.</p>

<p>Next up, let’s say you want to find all posts about ducks. You can do that via:</p>

<figure><pre><code data-lang="javascript"><span>// find all posts about ducks</span>
<span>const</span> <span>posts</span> <span>=</span> <span>await</span> <span>postsCollection</span><span>.</span><span>find</span><span>({</span> <span>caption</span><span>:</span> 
  <span>{</span> <span>$in</span><span>:</span>  <span>[</span><span>"</span><span>ducks</span><span>"</span><span>]</span> <span>}</span> <span>});</span></code></pre></figure>

<p>And boom. Now you have your ducks channel all set up for your users. Because who doesn’t want a stream fully dedicated to ducks?</p>

<p>Now, your app isn’t going to <a href="https://www.newsweek.com/twitter-fleets-reactions-memes-edit-button-1548037">be like Twitter</a>. We can edit stuff here. Let’s show how to edit your post’s caption. Stories tho? That’s on you</p>

<figure><pre><code data-lang="javascript"><span>// update the post’s caption</span>
<span>const</span> <span>post</span> <span>=</span> <span>await</span> <span>postsCollection</span><span>.</span><span>update</span><span>(</span><span>post</span><span>.</span><span>documentId</span><span>,</span> <span>{</span>
  <span>caption</span><span>:</span> <span>"</span><span>These ducks are MEGA cute</span><span>"</span><span>,</span>
<span>});</span></code></pre></figure>

<p>The above was just a quick tour on how to do a few data API calls for a basic TikTok clone. Want to see the full thing? Check out <a href="https://www.youtube.com/watch?v=IATOicvih5A">Ania Kubow</a>’s tutorial to see how to wire this up into a full React app with Netlify.</p>

<h2 id="whats-next">What’s next?</h2>

<p>For more examples, we have hands-on tutorials for using <a href="https://www.datastax.com/dev/rest">Stargate’s REST</a>, <a href="https://www.datastax.com/dev/documents-api">Document</a> and <a href="https://www.datastax.com/dev/graphql">GraphQL APIs</a>. Check ‘em out and let us know what you think.</p>

<p>Have an app idea or want to join the fun? <a href="https://discord.gg/2Xt8QNyFZA">You can join the Stargate community, too</a>.</p>

<p>We would love to see how you customize your TikTok clone to show off more ways to feature data in your app. Or, you can create your own non-TikTok example. We would love to showcase your example in our <a href="https://astra.datastax.com/sample-app-gallery">sample app gallery</a>, so tell us about it in <a href="https://discord.gg/33mKDHHFUE">our contribute channel</a>.</p>

<h2 id="so-you-are-down-here-looking-for-a-few-more-details">So, you are down here looking for a few more details</h2>
<p>If you came down here, maybe you are looking for a few more lines of code.</p>

<p>No problem.</p>

<p>Let’s show how to set up the node JS client and a few more data API calls. For starters, let’s take a look at how to set up your client to connect to Stargate in Astra.</p>

<figure><pre><code data-lang="javascript"><span>// npm install @astrajs/collections</span>
<span>const</span> <span>{</span> <span>createClient</span> <span>}</span> <span>=</span> <span>require</span><span>(</span><span>"</span><span>@astrajs/collections</span><span>"</span><span>);</span>

<span>// create an Astra client</span>
<span>const</span> <span>astraClient</span> <span>=</span> <span>await</span> <span>createClient</span><span>(</span>
<span>{</span>   <span>astraDatabaseId</span><span>:</span> <span>process</span><span>.</span><span>env</span><span>.</span><span>ASTRA_DB_ID</span><span>,</span>
    <span>astraDatabaseRegion</span><span>:</span> <span>process</span><span>.</span><span>env</span><span>.</span><span>ASTRA_DB_REGION</span><span>,</span>
    <span>username</span><span>:</span> <span>process</span><span>.</span><span>env</span><span>.</span><span>ASTRA_DB_USERNAME</span><span>,</span>
    <span>password</span><span>:</span> <span>process</span><span>.</span><span>env</span><span>.</span><span>ASTRA_DB_PASSWORD</span><span>,</span>
<span>});</span></code></pre></figure>

<p>Easy enough.</p>

<p>Then, let’s create a users collection in our database to store documents about our TikTok users:</p>

<figure><pre><code data-lang="javascript"><span>// create the users collection in the app</span>
<span>const</span> <span>usersCollection</span> <span>=</span> <span>astraClient</span><span>.</span><span>namespace</span><span>(</span><span>"</span><span>tikTokClone</span><span>"</span><span>).</span><span>collection</span><span>(</span><span>"</span><span>users</span><span>"</span><span>);</span></code></pre></figure>

<p>A TikTok user in our app will have the basics: a unique id, a name, username, etc.</p>

<figure><pre><code data-lang="javascript"><span>const</span> <span>userData</span> <span>=</span> <span>{</span>
  <span>"</span><span>id_3</span><span>"</span><span>:</span> <span>"</span><span>0</span><span>"</span><span>,</span>
  <span>"</span><span>name</span><span>"</span><span>:</span> <span>"</span><span>Mo Farooq</span><span>"</span><span>,</span>
  <span>"</span><span>username</span><span>"</span><span>:</span> <span>"</span><span>mofarooq32</span><span>"</span><span>,</span>
  <span>"</span><span>avatar</span><span>"</span><span>:</span> <span>"</span><span>https://i.imgur.com/9KYq7VG.png</span><span>"</span>
<span>};</span></code></pre></figure>

<p>So, let’s add our user into our collection:</p>

<figure><pre><code data-lang="javascript"><span>// create a new user</span>
<span>const</span> <span>user</span> <span>=</span> <span>await</span> <span>usersCollection</span><span>.</span><span>create</span><span>(</span><span>userData</span><span>);</span></code></pre></figure>

<p>You can check to make sure your user was stored in the database by reading the user back by any of their properties, like their username.</p>

<figure><pre><code data-lang="javascript"><span>// find our user by username</span>
<span>const</span> <span>users</span> <span>=</span> <span>await</span> <span>usersCollection</span><span>.</span><span>find</span><span>({</span> <span>username</span><span>:</span> <span>{</span> <span>$eq</span><span>:</span> 
  <span>"</span><span>mofarooq32</span><span>"</span> <span>}</span> <span>});</span></code></pre></figure>

<p>Or, you can lookup a user by their <strong>documentId</strong>:</p>

<figure><pre><code data-lang="javascript"><span>// get the user by document id</span>
<span>const</span> <span>user</span> <span>=</span> <span>await</span> <span>usersCollection</span><span>.</span><span>get</span><span>(</span><span>user</span><span>.</span><span>documentId</span><span>);</span></code></pre></figure>

<p>And, lastly, if you need to delete that user:</p>

<figure><pre><code data-lang="javascript"><span>// delete the post</span>
<span>const</span> <span>user</span> <span>=</span> <span>await</span> <span>usersCollection</span><span>.</span><span>delete</span><span>(</span><span>user</span><span>.</span><span>documentId</span><span>);</span></code></pre></figure>

<p>Want to see the full code? Check out <a href="https://github.com/kubowania/stargate-tik-tok">Ania Kubow’s app</a> to get all the goodness and start customizing it on your own. Let me know when you have stories up and I can subscribe to your ducks channel.</p>

<p>Thank you for following along all the way down here.</p>

<p>Happy building!</p>


        </div>
      </div>
    </div></div>]]>
            </description>
            <link>https://stargate.io/2020/12/09/announcing-stargate-10-ga-rest-graphql-schemaless-json-for-your-cassandra-development.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25362944</guid>
            <pubDate>Wed, 09 Dec 2020 19:11:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Code Review Best Practices – Lessons from the Trenches]]>
            </title>
            <description>
<![CDATA[
Score 57 | Comments 4 (<a href="https://news.ycombinator.com/item?id=25362375">thread link</a>) | @eric_cartman
<br/>
December 9, 2020 | https://blogboard.io/blog/code-review-best-practices | <a href="https://web.archive.org/web/*/https://blogboard.io/blog/code-review-best-practices">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://images.unsplash.com/photo-1516259762381-22954d7d3ad2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDd8fGNvZGV8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000 300w,
                            https://images.unsplash.com/photo-1516259762381-22954d7d3ad2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDd8fGNvZGV8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000 600w,
                            https://images.unsplash.com/photo-1516259762381-22954d7d3ad2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDd8fGNvZGV8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000 1000w,
                            https://images.unsplash.com/photo-1516259762381-22954d7d3ad2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDd8fGNvZGV8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://images.unsplash.com/photo-1516259762381-22954d7d3ad2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MXwxMTc3M3wwfDF8c2VhcmNofDd8fGNvZGV8ZW58MHx8fA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Code Review Best Practices - Lessons from the Trenches">
            </figure>

            <section>
                <div>
                    <p>There's a ton of resources scattered around the web dealing with code review fundamentals, best practices, tools, etc. In this article we'll summarize the lessons from a dozen of official company engineering blogs. You can find links to the original articles in <a href="http://localhost:8080/blog/code-reviews"></a><a href="https://blogboard.io/search?searchQuery=code%20review">this blogboard search</a>.</p><h2 id="what-s-in-this-article">What's in this article?</h2><p>We'll cover several topics:</p><ol><li>Why do code reviews?<br>Besides the obvious, quality assurance, there are other benefits to code reviews</li><li>Code reviews as quality assurance<br>We'll cover the general recommendations on what to look for in a code review, why having a review checklist is beneficial, and you'll get a fairly long checklist that you can use as a base for your own list</li><li>Code reviews as a team improvement tool<br>If you've done more than a few code reviews, you know they're useful for more than just preventing bugs. We'll summarize common views on how reviews are beneficial as a learning and team bonding tool</li><li>Preparing a pull request for review<br>Lessons for pull request authors. There are rules of thumb consistently pointed out that help to prepare a PR for a smooth review</li><li>Reviewing code - Be human!<br>Lessons for reviewers on how wording and tone of your comments can make a huge difference in effectiveness of the whole review effort.</li></ol><p>The topics are covered fairly independently, so if you're curious about a particular topic feel free to skip ahead.</p><h2 id="why-do-code-reviews">Why do code reviews?</h2><p>It should be obvious that the primary purpose of code review is to assess quality of the changes being introduced. I mean, the dictionary definition of <em>review </em>says precisely that</p><blockquote><strong>review</strong> <em>(noun) - </em>a formal assessment of something with the intention of instituting change if necessary.</blockquote><p>Of course, code being code, there's a lot of things that can be checked and tested automatically, so there's nuance to what actually needs to be checked in an actual code review. We cover that in the next section.</p><p>On the other hand, code review is a form of communication between the <em><strong>author</strong> </em>of the change (these days usually <em>a pull request</em>) and one or several <em><strong>reviewers</strong>. </em>So it has side effects that go beyond preventing bugs from slipping in or keeping the codebase consistent in terms of style and architecture. </p><p>When done well, code reviews help accelerate learning across the team, create psychological safety for all team members, help establish and communicate best practices, teach proper communication and improve team dynamics. When done poorly, they can help deteriorate all of the above.</p><h2 id="code-reviews-as-quality-assurance">Code reviews as quality assurance </h2><p>There are a bunch of ways in which code reviews help maintain the quality bar for the codebase and the product. In the end it comes down to catching mistakes at the level which can hardly be automatically tested, such as architectural inconsistencies. Also, the code for automated tests should be reviewed, so there's a meta level at which reviews help with QA. </p><p>In <a href="https://engineering.gusto.com/high-leverage-code-reviews/">Giving High Leverage Code Reviews</a>, Casey Rollins advocates for having a checklist with all the usual things that need attention. </p><blockquote>When I’m reviewing a pull request, I often do multiple “passes” where I focus on one attribute at a time. I start at the beginning and review the pull request with a single attribute in mind before moving on to the next. When I’ve worked through the checklist, I submit the review.<p>This checklist moves from general to specific checks because it’s important to focus on the high-level attributes first. It doesn’t make sense to offer a variable name suggestion if you’re also suggesting that an entire class or function be refactored.</p></blockquote><p>You can have your own checklist or make it a shared list for the team or a project. There's a ton of material written on the usefulness of checklists. In <em><a href="https://en.wikipedia.org/wiki/Getting_Things_Done">Getting Things Done</a>, </em>David Allen puts forward a simple idea -<em> </em>our minds are great at processing information, but terrible at storing and recalling it. That's why checklists are a great way of externally storing and breaking down a planned or repetitive task.</p><p>Compiled from several articles (<a href="https://medium.com/paypal-engineering/effective-code-reviews-53d62a203b2f">1</a>, <a href="https://engineering.gusto.com/high-leverage-code-reviews/">2</a>, <a href="https://medium.com/palantir/code-review-best-practices-19e02780015f">3</a>) here's a high-level list of things to be concerned about when reviewing a code change:</p><ul><li>Story alignment - does the change meet the requirements of the task at all; ie. does the code implement any and all of the specified functionalities?</li><li>Consistency across the codebase</li><li>Architectural considerations - how does the new piece of code fit the existing architecture. Can the new feature architecture be improved, is it too generic or not extensible enough?</li><li>Simplicity/over-engineering</li><li>Performance concerns - are there specific cases (eg. peak load times) when the code will break? Do the queries pull more data than necessary? Could new queries benefit from adding new indexes to the database?</li><li>Accidental errors such as typos or errors in math formulas - these can be either obvious or really tricky to notice, especially with math heavy code</li><li>Compliance with laws and regulations - depending on the business this might be the most important thing</li><li>Security concerns - are there any exploitable pieces of code being introduced? Are any secrets being shared or stored unsafely?</li><li>Readability and style - a seemingly perfect piece of code might not be immediately understandable and readable to a different pair of eyes. Is it possible to understand the changes without the author explaining them?</li><li>Best practices - programming languages usually have their best practices - are they met in the pull request? Also, with time any project, team and company will evolve their own set of best practices - code reviews are a way to enforce and spread knowledge about them</li><li>Localization - are all language dependent resources localized properly?</li><li>Dependencies - are there external libraries or APIs being introduced? Are there other simpler/faster/better ways to do this with different dependencies or without any?</li><li>Interactions and side effects - how does the new piece of code interact with the rest of the codebase; does the new function implementation break any existing functionality; are all relevant unit tests updated/added</li><li>Logging - it's practically impossible to debug server code properly without good logging. Is everything logged/traced correctly</li><li>Error handling - how are the errors handled on the backend; how are they communicated to the user; are fallbacks activated where possible?</li><li>Testability/Test coverage - is the new piece of code covered with automated tests? Have all the suspicious test cases been checked either automatically or manually? Is the code written in a way that's suitable for unit testing?</li><li>External documentation - in case it's necessary is the external documentation updated to reflect the change?</li></ul><p>It's a pretty long list. In addition to it, a recurring piece of advice is not to use code reviews in place of static code analysis tools. If your review is mostly about code formatting, variable naming and alphabetical ordering, it might be a good time to include an automated code analysis tool into your development workflow.</p><p>In <em><a href="https://medium.com/paypal-engineering/effective-code-reviews-53d62a203b2f">Effective Code Reviews: Bettering Products, Teams, and Engineers</a> </em>from PayPal engineering<em>, </em>Gabriel McAdams points out several important benefits of code reviews related to team dynamics:</p><ul><li>Team cohesion - by making everyone's code subject to peer review, code review process promotes <em>individual accountability, healthy conflict</em> and the idea that everyone's<em> working together</em> to make the product better. As said in <a href="https://medium.com/palantir/code-review-best-practices-19e02780015f">Code Review Best Practices</a>: <em>Code reviews are classless: being the most senior person on the team does not imply that your code does not need review.</em><br>In summary, McAdams puts it nicely: <em>Trust + healthy conflict + individual accountability + working together to better the team = team cohesion.</em></li><li>Free career improvement training - simply by virtue of reviewing other people's code you become more skilled at reading and understanding new code. I've heard it said that one of the foremost traits of great engineers is the ability to dive into and dissect a completely unfamiliar piece of code. Over time you learn how to spot common practices, little tricks, pieces of syntactic sugar, architectural abstractions and how to appreciate different mental models used to solve the same problem.</li></ul><p>In <a href="https://medium.com/palantir/code-review-best-practices-19e02780015f">Code Review Best Practices</a> from the Palantir Blog, Robert Fink lists several ways in which knowledge sharing and social side-effects happen via code reviews:</p><ul><li>Authors are motivated by the peer review process to do all the necessary pre-checks, tighten the loose ends and generally tidy up the code before sending to review</li><li>A code review explicitly communicates changes made to product functionality to team members</li><li>The author maybe used a technique, abstraction or an algorithm that reviewers are unfamiliar with. The opposite can also be the case - reviewers might be aware of a more appropriate way to solve a given problem</li><li>Positive communication strengthens social bonds within the team (might especially be true for remote teams)</li></ul><h2 id="preparing-a-pull-request-for-review-help-the-reviewer">Preparing a pull request for review - help the reviewer</h2><p>Code reviews should be seen as a team effort. Once you view them that way it becomes clear that both sides - the author and the reviewers - have their distinct sets of responsibilities.</p><p>In <a href="https://medium.engineering/the-code-review-mindset-3280a4af0a89">this short post</a> on Medium Engineering blog, Xiao Ma describes how a different perspective changes the way code reviews are done, how feedback is taken and how people on each side benefit by adopting a <em>positive mindset</em> about code reviews.</p><p>When we talk about the responsibilities of the pull request author, there are several key things recurring in all code review guides.</p><ol><li><strong>Make pull requests as atomic as possible</strong><br><a href="https://shopify.engineering/great-code-reviews">At Shopify</a> they advise to keep <em>your pull requests small </em>- it helps the reviewer dive into it and finish it as an atomic piece of work in their workday. In practice this can mean keeping your pull requests limited to <em>a single concern. </em>A single concern here means a single bug fix, a feature, an API change etc. Don't mix refactoring that doesn't alter behavior with bug fixes or new features. This is …</li></ol></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blogboard.io/blog/code-review-best-practices">https://blogboard.io/blog/code-review-best-practices</a></em></p>]]>
            </description>
            <link>https://blogboard.io/blog/code-review-best-practices</link>
            <guid isPermaLink="false">hacker-news-small-sites-25362375</guid>
            <pubDate>Wed, 09 Dec 2020 18:45:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[New York Times Best Seller Business Book]]>
            </title>
            <description>
<![CDATA[
Score 36 | Comments 18 (<a href="https://news.ycombinator.com/item?id=25361439">thread link</a>) | @dubeyaayush07
<br/>
December 9, 2020 | https://dubeyaayush07.github.io/deliberate-mistakes/new-york-times-best-seller-business-book/ | <a href="https://web.archive.org/web/*/https://dubeyaayush07.github.io/deliberate-mistakes/new-york-times-best-seller-business-book/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/Article"><header><p>December 08, 2020</p></header><section itemprop="articleBody"><h3>Oversimplified, Overgeneralized Rules to Make <del>Me</del> You Rich</h3>
<ul>
<li>It is good to be a contrarian but do you know what is even better? To be a contrarian contrarian and since according to an unrelated mathematical rule inverse of an inverse is equal to the same thing you should do what everyone is doing, that is buying my book.</li>
<li>Steve Jobs, Elon musk.</li>
<li><strong>PASSION</strong>, Do what you want, Quit your Job.</li>
<li>Case studies of why I am right based on misinterpretations of what really happened.</li>
<li>Correlation == Causation.</li>
<li>Cliched  general advice: Exercise, Brush Your teeth before bed, work hard to succeed, clean your room, etc.</li>
<li>Filler                 </li>
<li>Confirmation Bias, Anecdotal evidence.</li>
<li>Graphs, Plots(Who needs axes and scales), Math is on my side.</li>
<li>Random Philosophy detour which has nothing to do with my thesis</li>
<li>Arguments that look logical on paper but are just superficial overgeneralizations  targeted towards inexperienced individuals.</li>
<li>My life and how I applied these principles to get rich. People disagreed with me. I succeeded and now I am rich.</li>
</ul>
<hr>
<p>I wrote this post as a result of my frustration with popular business books and with non-fiction books in general. I am not saying every book is like this, some books are insightful and bring new ideas and perspectives to the table. But I have noticed that some authors have an idea and without fleshing it out and properly researching it they decide to write a book about it. Throw in cognitive dissonance, confirmation bias and an incompetent publishers and you have got yourself a popular non-fiction book. There is no peer review and if you sensationalize things you can earn a lot of money. </p>
<p>You cannot even trust books by experts in the field. I bought into the hype of the book Why We Sleep  by Matthew Walker the author seemed legit and well respected within his field. He even appeared on the Joe Rogan Experience. After reading the book I  began espousing the benefits of 8 hour sleep until I read <a href="https://guzey.com/books/why-we-sleep/">this</a> wonderful article by Alexey Guzey. So who should you trust? Should you stop reading non-fiction altogether? I suggest doing the opposite, read as much as you can and be wary of any broad sweeping statements. Even though some books might get something wrong, the good ideas are still valuable. And as with business you can only get better at it (i.e. filtering signal) by exposing yourself to more of it.</p></section><hr></article></div>]]>
            </description>
            <link>https://dubeyaayush07.github.io/deliberate-mistakes/new-york-times-best-seller-business-book/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25361439</guid>
            <pubDate>Wed, 09 Dec 2020 17:57:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Brief Goodbye to CentOS]]>
            </title>
            <description>
<![CDATA[
Score 176 | Comments 151 (<a href="https://news.ycombinator.com/item?id=25359951">thread link</a>) | @notadeveloper
<br/>
December 9, 2020 | https://www.clementchiew.me/blog/blog-013 | <a href="https://web.archive.org/web/*/https://www.clementchiew.me/blog/blog-013">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
<hr>
<header>
		
</header>

<p>The traditional CentOS Linux distribution as we know it is dead. Here is another drop in the ocean of opinion pieces that follow the news of its death. After cooling down from the initial rush of blood to my head, here is my take on this event.</p>

<h2>Why Did This Probably Happen</h2>
<p>With the advent of DevOps and SRE, businesses and startups are moving away from the old-school concept of traditional server clusters to running their applications on disposable containers. The trend is clear and true. Developers are increasingly less reliant on a tried-and-true Linux distribution that lasts for a decade. With containers, developers can develop, test, deploy, and rollback with blazing fast velocity.

</p><h2>How It Will Affect All of Us</h2>
<p>Without a doubt one of the most popular Linux distributions to ever exist, CentOS was prevalent among all kinds of computing systems ranging from simple database servers to billion-dollar computer clusters. There are countless organizations have made the business decision to keep using the traditional model, or organizations that do not require microservices at all. With CentOS drawn from below their feet, a lot of organizations will be forced to migrate to another option, or fork out a pretty penny for RHEL. Besides, on-prem deployment of any container orchestration tool still requires a stable Linux distribution.</p>

<p>The second ripple effect it will have is towards the skilled professionals who have spend decades on CentOS. Not every company is willing to pay up for RHEL or risk using CentOS Stream. For those who migrate to Debian or OpenSUSE, they will have to retrain and adapt with different tools.</p>

<h2>Questioning IBM/Red Hat Decisions</h2>
<p>The most obvious of them all was, was it necessary for CentOS to die? With CentOS Stream to track ahead of RHEL, it is still possible for CentOS to remain functional and serve its purpose. This is clearly a business decision to increase profits. It used to be that developers wanted to write for RHEL but did not want pay for it; CentOS filled that need. What also happened was that some companies decided that they wanted the free experience all the way. Red Hat now provides free use of the Red Hat Universal Base Image for developers. With this, companies no longer have an excuse.</p>

<p>Secondly, why the PR disaster? In hindsight, there is no way to deliver this news gently to the public. However, I felt that Red Hat gave the bird to the open source community, especially those who contributed to CentOS, by pulling the plug on Centos 8 towards the end of 2021. There wasn't even a courtesy to end it later then CentOS 7's EOL date, June 30th 2024. A raw-dogged "Pay up, now" to everyone. </p>

<p>Last of all, what is the next move from Red Hat/IBM? With CentOS gone, there is a huge vacuum for another to take its place. RHEL sources are still available and can still be repackaged. While Red Hat currently has massive influence over Linux in general, is this a arrogant statement proclaiming "Hey, you can't live without me"? Another ominuous take with conspiratorial undertones would be that Red Hat plans to eventually scrap the FOSS model, but I would have to wear my tin hat for this one.

</p><h2>So, What Happens Now?</h2>
<p>Almost immediately after the release, all the attention is now directed to towards filling the space that CentOS will leave behind. Undoubtedly, Ubuntu and SUSE would try to assert their presence with their open source alternatives. Debian, the largest behemoth of them all, hopefully will receive funding and participation like never before. A silver lining of this event would perhaps be the buzzing excitement of what will be and can be. It is time to be excited about Linux again. I, for one, have to begin migrating my CentOS containers and virtual machines to Debian.</p>

<p>CentOS's founder, Gregory Kurtzer, is working with the community to establish Rocky Linux. Join them at https://webchat.freenode.net/#rockylinux .</p>
<hr>
<blockquote>
I doubt that the imagination can be suppressed. If you truly eradicated it in a child, he would grow up to be an eggplant.
<br>
- Ursula K. Le Guin
</blockquote>
</div>]]>
            </description>
            <link>https://www.clementchiew.me/blog/blog-013</link>
            <guid isPermaLink="false">hacker-news-small-sites-25359951</guid>
            <pubDate>Wed, 09 Dec 2020 15:58:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[DAML: A Haskell-Based Language for Blockchain]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25359249">thread link</a>) | @NaeosPsy
<br/>
December 9, 2020 | https://serokell.io/blog/daml-interview | <a href="https://web.archive.org/web/*/https://serokell.io/blog/daml-interview">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Digital Asset is a fintech company that helps companies design and run the next generation of business applications. One of their products is <a href="https://daml.com/">DAML</a>, a functional smart contract language.</p><p>While we have already covered Digital Asset in our <a href="https://serokell.io/blog/functional-programming-in-fintech">functional programming in fintech overview</a>, I recently got a chance to talk with Anthony Lusardi, a developer advocate at DAML, and delve deeper into the product. In the interview, we talk about DAML, the benefits and downsides of functional programming languages, and their practical experience while building DAML.</p><p><strong>Hi! Could you shortly explain what DAML is?</strong></p><p>Sure! DAML is an open-source smart contract language with roots firmly planted in Haskell. It’s designed so multiple parties/business entities can perform workflows with high assurances and consistency between parties. So you have this transactional language that is atomic with every update, high level, portable across data persistence backends, and with strongly enforced permissions over who can update data when.</p><p>In more practical terms, imagine you’re managing an operationally complex workflow where multiple different stakeholders (ie. parties) need to see and interact with different parts of the workflow, definitely should not see other parts, and have a complex tree of dependencies. Today implementing such a thing is truly hard to manage with complex access control schemes implemented at a level outside of your program, and substantial difficulties maintaining data privacy. DAML treats these concerns as first-class elements of every class (what DAML calls templates) and thus makes it much easier to implement and manage these types of workflows.</p><p><strong>Why should customers choose DAML over building their project on Ethereum, Tezos, or other public blockchains?</strong></p><p>This really boils down to their needs. If you’re building something that truly needs permissionless, censorship-resistant, and entirely public transactions, then a public blockchain might be a good fit despite what would be substantial tradeoffs for most real-world applications in terms of low data storage, poor privacy, and high cost. DAML, on the other hand, won’t give you a permissionless architecture but will allow you to have high data storage, strong privacy, and significantly lower costs.</p><p>In practice, very few use cases actually need the properties of a permissionless public blockchain. The one that comes to mind for me where a public blockchain is a better fit would be Bitcoin for permissionless value storage/transfer.</p><p>While I think there are some interesting projects on Ethereum, I’m not personally convinced most use cases need Ethereum’s architecture as, with a few exceptions, most have components that in practice are replaceable and centrally administered by their development team. In such cases, these teams are trading off ease of operation for decentralized architectures and really getting neither. That is, of course, open for extensive theoretical debate that is well beyond the scope of this interview :)</p><p><strong>What’s the main thing that separates DAML from other enterprise blockchain platforms like Corda and Hyperledger Fabric?</strong></p><p>DAML applications can be written once and deployed on any supported platform without changing a single line of code. In this way, your code written in DAML is decoupled from the underlying backend allowing for much greater architectural flexibility.</p><p>In fact, DAML actually runs on these platforms (and many others) with a runtime that runs alongside them and uses them for data persistence and consensus. It’ll even run on PostgreSQL. It’s truly platform-agnostic.</p><p><strong>What’s the benefit of basing DAML on functional programming languages like Haskell and Scala?</strong></p><p>Simple. Functional programmers are the best programmers.</p><p>More seriously, though, the language is based on Haskell but has conventions and differences that make it uniquely its own language. The general benefits are a high degree of composability, as anyone who writes in functional languages knows, when your types match your functions and components can easily work together and extend each other. Functional languages are also beneficial for the more distributed applications that DAML is designed for because they reduce bugs and allow for better ensuring that operations will or won’t complete; both of which are big concerns whenever you’re writing a distributed application.</p><p><strong>Are there any features of the language that really help smart contract language development?</strong></p><p>Most definitely. DAML has two features that really help. The first is that all data concerns are laid out in templates and strongly typed. In a lot of ways, these templates are much like classes in imperative languages <strong><em>but</em></strong> they will always do what you expect them to.</p><p>The other feature (and this is really smart-contract specific) is that DAML treats permissions as first-class citizens, so we have observers, signatories, and choices which you can consider respectively akin to UNIX’s <code>rwx </code>permissions. Every template specifies ahead of time who has the authority to read, write, and execute functions on a given instance of that template (what we call a contract in DAML).</p><p><strong>Have you seen any non-technical benefits? ( e.g. is it easier to hire good developers, etc.)</strong></p><p>Reductions in codebase size and operational complexity are definitely benefits. It’s really designed from the ground up to allow developers to focus solely on business logic without having to worry about the backend. These factors, in turn, make DAML applications cheaper to maintain and easier to extend. Basically, all the benefits functional programmers have been touting for a long time.</p><p>One other great benefit is readability. While it takes a programmer to write DAML, many non-programmers can comprehend much of a DAML contract with just a little bit of familiarization. This really comes as a direct consequence of how explicit DAML is about data concerns and permissions. You can check out an example of this readability at <a href="https://beer.woah.xyz/">https://beer.woah.xyz</a>.</p><p><strong>What about downsides? Are there any downsides to choosing a programming language for your project that is not that popular?</strong></p><p>There certainly are, but if you’re reading this blog post, then you probably already use non-mainstream programming languages that are still wonderful and let you get your work done in effective ways that mainstream languages don’t support. Innovation happens at the edges so I think DAML’s benefits in the smart contract space and its enthusiastic and supportive engineers on our forum more than outweigh the tradeoffs.</p><p>Really the biggest concern people have is essentially “if I choose to invest time in learning this language, will it still be there next year?” and for that, the answer is yes. DAML is an open-source language maintained by Digital Asset, which is a company of 140+ people currently and growing. DAML will be here for years to come.</p><p><strong>Do you feel happy about your choice to create an FP language for your project?</strong></p><p>I don’t think you could build a non-functional language that accomplished what DAML does. It’s really a prerequisite for the rest of the stack. So in that sense, yes, and also the decision was made well before I joined Digital Asset.</p><p><strong>If you had to give one tip to customers looking to come into the blockchain/DLT space, what would it be?</strong></p><p>If the app needs you to first buy a token to use it, then run away.</p><hr><p>I would like to thank Anthony for the interview and wish DAML the best of luck in conquering the private blockchain market!</p><p>If you wish to get more details on DAML, go straight to their <a href="https://daml.com/">homepage</a>. You can also follow DAML on <a href="https://twitter.com/damldriven">Twitter</a> for updates and cool blog articles.</p><p>For more interviews with interesting projects in the functional programming space, be sure to check out the <a href="https://serokell.io/blog/interviews">interview</a> section of our blog.</p></div></div>]]>
            </description>
            <link>https://serokell.io/blog/daml-interview</link>
            <guid isPermaLink="false">hacker-news-small-sites-25359249</guid>
            <pubDate>Wed, 09 Dec 2020 15:10:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[YouTube to remove content that alleges widespread election fraud]]>
            </title>
            <description>
<![CDATA[
Score 972 | Comments 2562 (<a href="https://news.ycombinator.com/item?id=25359003">thread link</a>) | @1cvmask
<br/>
December 9, 2020 | https://blog.youtube/news-and-events/supporting-the-2020-us-election | <a href="https://web.archive.org/web/*/https://blog.youtube/news-and-events/supporting-the-2020-us-election">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div tabindex="-1">
            
  <article>
    
    


<div>

<section>
    
    <div>
      
      
      <p>
        <article>
          Updates to our work supporting the integrity of the 2020 U.S. election.
        </article>
      </p>
    </div>
  </section>
</div>


    
      
        



<section data-component="yt-paragraph-media" data-media-type="paragraph">
  <div>
    
    <div>
      <div>
        <div><p>Over the past weeks and months, we’ve seen people coming to YouTube to learn more about where and how to vote or learning more about a candidate or an issue. We’ve seen news organizations grow their audience. And we’ve seen people turn to YouTube for the latest election results or simply to follow an historic event with the highest voting turnout in over a century in the U.S.&nbsp;&nbsp;</p><p>Our main goal going into the election season was to make sure we’re connecting people with authoritative information, while also limiting the reach of misinformation and removing harmful content. The work here is ongoing and we wanted to provide an update.&nbsp;&nbsp;</p></div>
      </div>
      
    </div>
  </div>
</section>

      
    
      
        



<section data-component="yt-paragraph-media" data-media-type="paragraph">
  <div>
    
    <div>
      <div>
        <div><h2>Removing content that violates our policies</h2><p>Our <a href="https://www.youtube.com/howyoutubeworks/policies/community-guidelines/">Community Guidelines</a> prohibit spam, scams, or other manipulated media, coordinated influence operations, and any content that seeks to incite violence. Since September, we've terminated over 8000 channels and thousands of harmful and misleading elections-related videos for violating our existing policies. Over 77% of those removed videos were taken down before they had 100 views.&nbsp;</p><p>We also work to make sure that the line between what is removed and what is allowed is drawn in the right place. Our policies prohibit misleading viewers about where and how to vote. We also disallow content alleging widespread fraud or errors changed the outcome of a historical U.S. Presidential election. However in some cases, that has meant allowing controversial views on the outcome or process of counting votes of a current election as election officials have worked to finalize counts.&nbsp;</p><p>Yesterday was the safe harbor deadline for the U.S. Presidential election and enough states have certified their election results to determine a President-elect. Given that, we will start removing any piece of content uploaded today (or anytime after) that misleads people by alleging that widespread fraud or errors changed the outcome of the 2020 U.S. Presidential election, in line with our approach towards historical U.S. Presidential elections. For example, we will remove videos claiming that a Presidential candidate won the election due to widespread software glitches or counting errors. We will begin enforcing this policy today, and will ramp up in the weeks to come. As always, news coverage and commentary on these issues can remain on our site if there’s sufficient <a href="https://blog.youtube/inside-youtube/look-how-we-treat-educational-documentary-scientific-and-artistic-content-youtube/">education, documentary, scientific or artistic</a> context.</p></div>
      </div>
      
    </div>
  </div>
</section>

      
    
      
        



<section data-component="yt-paragraph-media" data-media-type="image_paragraph">
  <div>
    <div>
      
        <div>
          
              <p><img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/MostViewed-Elections-Blog_Lists_AddedBorder.png" alt="most viewed u.s. election-related content">
                
              
              </p>
          
          

          
        </div>
        
      
    </div>
    <div>
      <div>
        <div><h2>Connecting people to authoritative information</h2><p>While only a small portion of watch time is election-related content, YouTube continues to be an important source of election news. On average 88% of the videos in top 10 search results related to elections came from authoritative news sources (amongst the rest are things like newsy late-night shows, creator videos and commentary). And the most viewed channels and videos are from news channels like NBC and CBS.</p></div>
      </div>
      
    </div>
  </div>
</section>

      
    
      
        



<section data-component="yt-paragraph-media" data-media-type="image_paragraph">
  <div>
    <div>
      
        <div>
          
              <p><img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/Election_Results_Blog_437_x_879_1.gif" alt="election results gif">
                
              
              </p>
          
          

          
        </div>
        
      
    </div>
    <div>
      <div>
        <div><p>We also showed information panels linking both to Google’s election results feature, which sources election results from The Associated Press, and to the Cybersecurity &amp; Infrastructure Security Agency’s (CISA) “Rumor Control” page for debunking election integrity misinformation, alongside these and over 200,000 other election-related videos. Collectively, these information panels have been shown over 4.5 billion times. Starting today, we will update this information panel, linking to the “2020 Electoral College Results” page from the Office of the Federal Register, noting that as of December 8, states have certified Presidential election results, with Joe Biden as the President-elect. It will also continue to include a link to CISA, explaining that states certify results after ensuring ballots are properly counted and correcting irregularities and errors.</p></div>
      </div>
      
    </div>
  </div>
</section>

      
    
      
        



<section data-component="yt-paragraph-media" data-media-type="image_paragraph">
  <div>
    <div>
      
        <div>
          
              <p><img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/Fact_Check_Dominion_voting__Michigan_recount.png" alt="fact check">
                
              
              </p>
          
          

          
        </div>
        
      
    </div>
    <div>
      <div>
        <div><p>Additionally, since Election Day, relevant <a href="https://blog.youtube/news-and-events/expanding-fact-checks-on-youtube-to-united-states">fact check information panels</a>, from third party fact checkers, were triggered over 200,000 times above relevant election-related search results, including for voter fraud narratives such as “Dominion voting machines” and “Michigan recount.”</p></div>
      </div>
      
    </div>
  </div>
</section>

      
    
      
        



<section data-component="yt-paragraph-media" data-media-type="image_paragraph">
  <div>
    <div>
      
        <div>
          
              <p><img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/Recommended-Elections-Blog_Lists_AddedBorder.png" alt="most recommended u.s. election-related content">
                
              
              </p>
          
          

          
        </div>
        
      
    </div>
    <div>
      <div>
        <div><p>Now let’s look at recommendations, one of the main ways our viewers find content. Limiting the reach of borderline content and prominently surfacing authoritative information are important ways we protect people from problematic content that doesn’t violate our Community Guidelines. Over 70% of recommendations on election-related topics came from authoritative news sources and the top recommended videos and channels for election-related content were primarily authoritative news. In fact, the top 10 authoritative news channels were recommended over 14X more than the top 10 non-authoritative channels on election-related content.&nbsp;</p><p>Despite these encouraging results, we recognize there's always more to do. For example, while problematic misinformation represents a fraction of 1% of what's watched on YouTube in the U.S., we know we can bring that number down even more. And some videos, while not recommended prominently on YouTube, continue to get high views, sometimes coming from other sites. We're continuing to consider this and other new challenges as we make ongoing improvements.&nbsp;</p><p>We understand the need for intense scrutiny on our elections-related work. Our teams work hard to ensure we are striking a balance between allowing for a broad range of political speech and making sure our platform isn't abused to incite real-world harm or broadly spread harmful misinformation. We welcome ongoing debate and discussion and will keep engaging with experts, researchers and organizations to ensure that our policies and products are meeting that goal. And as always, we'll apply learnings from this election to our ongoing efforts to protect the integrity of elections around the world.<br></p></div>
      </div>
      
    </div>
  </div>
</section>

      
    

    


<section>
  <article>
    
  </article>
</section>


    
    
  


  </article>


        </div></div>]]>
            </description>
            <link>https://blog.youtube/news-and-events/supporting-the-2020-us-election</link>
            <guid isPermaLink="false">hacker-news-small-sites-25359003</guid>
            <pubDate>Wed, 09 Dec 2020 14:50:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Prisma Migrate is now ready]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25357937">thread link</a>) | @sorenbs
<br/>
December 9, 2020 | https://www.prisma.io/blog/prisma-migrate-preview-b5eno5g08d0b?a | <a href="https://web.archive.org/web/*/https://www.prisma.io/blog/prisma-migrate-preview-b5eno5g08d0b?a">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><div><div><h2 id="contents"><a href="#contents" aria-label="contents permalink"></a>Contents</h2><ul><li><a href="#schema-migrations-with-prisma-migrate">Schema migrations with Prisma Migrate</a></li><li><a href="#how-does-prisma-migrate-work">How does Prisma Migrate work?</a></li><li><a href="#what-has-changed-since-the-experimental-version">What has changed since the Experimental version?</a></li><li><a href="#whats-next">What's next</a></li><li><a href="#try-prisma-migrate-and-share-your-feedback">Try Prisma Migrate and share your feedback</a></li></ul><h2 id="schema-migrations-with-prisma-migrate"><a href="#schema-migrations-with-prisma-migrate" aria-label="schema migrations with prisma migrate permalink"></a>Schema migrations with Prisma Migrate</h2><p>Today we're excited to share the new version of Prisma Migrate! 🎊</p><p>Prisma Migrate is a data modeling and migrations tool that simplifies evolving the database schema with the application in-tandem. Migrate is based on the <a href="https://www.prisma.io/docs/concepts/components/prisma-schema#example">Prisma schema</a> – a declarative data model definition that codifies your database schema.</p><p>This Preview release is the evolution of the Experimental version of Migrate that we released last year. Since then, we've been gathering feedback from the community and incorporating it into Prisma Migrate.</p><h3 id="making-schema-migrations-predictable"><a href="#making-schema-migrations-predictable" aria-label="making schema migrations predictable permalink"></a>Making schema migrations predictable</h3><p>Database schema migrations play a crucial role in software development workflows and affect the most critical component in your application – the database. We've built Migrate to be predictable while allowing you to control how database schema changes are carried out.</p><p>Prisma Migrate generates migrations as plain SQL files based on changes in your Prisma schema. These SQL files are fully customizable and allow you to use any feature of the underlying database, such as manipulating data supporting a migration, setting up triggers, stored procedures, and views.</p><p>Prisma Migrate treads the balance between productivity and control by automating the repetitive and error-prone aspects of writing database migrations while giving you the final say over how they are executed.</p><h3 id="integration-with-prisma-client"><a href="#integration-with-prisma-client" aria-label="integration with prisma client permalink"></a>Integration with Prisma Client</h3><p>Prisma Migrate integrates with Prisma Client using the Prisma schema as their shared source of truth. In other words, both Prisma Client and migrations are generated based on the Prisma schema. This makes synchronizing and verifying database schema changes in your application code easier by leveraging Prisma Client's type safety.</p><h3 id="prisma-migrate-is-ready-for-broader-testing"><a href="#prisma-migrate-is-ready-for-broader-testing" aria-label="prisma migrate is ready for broader testing permalink"></a>Prisma Migrate is ready for broader testing</h3><p>Prisma Migrate has passed rigorous testing internally and is now ready for broader testing by the community. You can use it with PostgreSQL, MySQL, SQLite, and SQL Server. <strong>However, as a Preview feature, it is not fully production-ready yet.</strong> To read more about what Preview means, check out the <a href="https://www.prisma.io/docs/more/releases#preview">maturity levels</a> in the Prisma docs.</p><p>Thus, we're inviting you to try it out and <a target="_blank" rel="noopener noreferrer" href="https://github.com/prisma/prisma/issues/4531">give us feedback</a> so we can bring Prisma Migrate to General Availability. 🚢</p><p>Your feedback and suggestions will help us shape the future of Prisma Migrate. 🙌</p><hr><h2 id="how-does-prisma-migrate-work"><a href="#how-does-prisma-migrate-work" aria-label="how does prisma migrate work permalink"></a>How does Prisma Migrate work?</h2><p>Prisma Migrate is based on the <a href="https://www.prisma.io/docs/reference/tools-and-interfaces/prisma-schema">Prisma schema</a> and works by generating <code>.sql</code> migration files that are executed against your database.</p><p>The Prisma schema is the starting point for schema migrations and provides an overview of your desired end-state of the database. Prisma Migrate inspects changes in the Prisma schema and generates the necessary <code>.sql</code> migration files to apply.</p><p>Applying migrations looks very different depending on the stage of development. For example, during development, there are scenarios where resetting the database can be tolerated for quicker prototyping, while in production, great care must be taken to avoid data loss and breaking changes.</p><p>Prisma Migrate accommodates for this with workflows for local development and applying migrations in production.</p><h3 id="evolving-the-schema-in-development"><a href="#evolving-the-schema-in-development" aria-label="evolving the schema in development permalink"></a>Evolving the schema in development</h3><p>To use the new version of Prisma Migrate, you should have at least version <code>2.13.0</code> of the <a href="https://www.prisma.io/docs/concepts/components/prisma-cli/installation"><code>@prisma/cli</code></a> package installed.</p><p>During development, you first define the Prisma schema and then run the <code>prisma migrate dev --preview-feature</code> command, which generates the migration, applies it, and generates Prisma Client:</p><p><span><img src="https://d33wubrfki0l68.cloudfront.net/9dee8cc50b930a017447904d95e15e0e82f9a3bf/426d4/blog/posts/2020-12-migrate-development-workflow.png" alt="Development workflow"><span>Development workflow</span></span></p><p>Here is an example showing it in action:</p><p><strong>1. Define your desired database schema using the Prisma schema:</strong></p><pre><code><span>datasource</span> <span>db</span> <span>{</span>
  provider <span>=</span> <span>"postgresql"</span>
  url      <span>=</span> <span>env</span><span>(</span><span>"DATABASE_URL"</span><span>)</span>
<span>}</span>

<span>model</span> <span>User</span> <span>{</span>
  id    <span>Int</span>      <span>@id</span> <span>@default</span><span>(</span><span>autoincrement</span><span>(</span><span>)</span><span>)</span>
  name  <span>String</span>
  posts <span>Post</span><span>[</span><span>]</span>
<span>}</span>

<span>model</span> <span>Post</span> <span>{</span>
  id        <span>Int</span>     <span>@id</span> <span>@default</span><span>(</span><span>autoincrement</span><span>(</span><span>)</span><span>)</span>
  title     <span>String</span>
  published <span>Boolean</span> <span>@default</span><span>(</span><span>true</span><span>)</span>
  authorId  <span>Int</span>
  author    <span>User</span>    <span>@relation</span><span>(</span><span>fields:</span> <span>[</span>authorId<span>]</span><span>,</span> <span>references:</span> <span>[</span>id<span>]</span><span>)</span>
<span>}</span>
</code></pre><p><strong>2. Run <code>prisma migrate dev --preview-feature</code> to create and execute the migration.</strong></p><div><div><svg width="6" height="9" viewBox="0 0 6 9" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M7.3273 0C7.88605 0 8.20036 0.653318 7.85732 1.1017L4.53001 5.45076C4.26119 5.80213 3.73881 5.80213 3.46999 5.45076L0.142684 1.1017C-0.200356 0.653318 0.113948 0 0.672698 0H7.3273Z" transform="rotate(-90 4.5 4.357)" fill="#8FA6B2"></path></svg><p><label for="tab-1">Expand to view the SQL contents of the generated migration</label></p><div><pre><code>
<span>CREATE</span> <span>TABLE</span> <span>"User"</span> <span>(</span>
  <span>"id"</span> <span>SERIAL</span><span>,</span>
  <span>"name"</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span><span>,</span>
  <span>PRIMARY</span> <span>KEY</span> <span>(</span><span>"id"</span><span>)</span>
<span>)</span><span>;</span>

<span>CREATE</span> <span>TABLE</span> <span>"Post"</span> <span>(</span>
  <span>"id"</span> <span>SERIAL</span><span>,</span>
  <span>"title"</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span><span>,</span>
  <span>"published"</span> <span>BOOLEAN</span> <span>NOT</span> <span>NULL</span> <span>DEFAULT</span> <span>true</span><span>,</span>
  <span>"authorId"</span> <span>INTEGER</span> <span>NOT</span> <span>NULL</span><span>,</span>
  <span>PRIMARY</span> <span>KEY</span> <span>(</span><span>"id"</span><span>)</span>
<span>)</span><span>;</span>

<span>ALTER</span> <span>TABLE</span> <span>"Post"</span> <span>ADD</span> <span>FOREIGN</span> <span>KEY</span><span>(</span><span>"authorId"</span><span>)</span><span>REFERENCES</span> <span>"User"</span><span>(</span><span>"id"</span><span>)</span> <span>ON</span> <span>DELETE</span> <span>CASCADE</span> <span>ON</span> <span>UPDATE</span> <span>CASCADE</span><span>;</span>
</code></pre></div></div></div><p>After the migration has been executed, the migration files are typically committed to the repository so that the migration can be applied in other environments.</p><p>Further changes to the database schema follow the same workflow and begin with updating the Prisma schema.</p><h3 id="customizing-sql-migrations"><a href="#customizing-sql-migrations" aria-label="customizing sql migrations permalink"></a>Customizing SQL migrations</h3><p>You can customize the migration SQL with the following workflow:</p><ol><li>Run <strong><code>prisma migrate dev --create-only --preview-feature</code></strong> to create the SQL migration without applying it.</li><li>Edit the migration SQL.</li><li>Run <strong><code>prisma migrate dev --preview-feature</code></strong> to apply it.</li></ol><h3 id="applying-migrations-in-production-and-other-environments"><a href="#applying-migrations-in-production-and-other-environments" aria-label="applying migrations in production and other environments permalink"></a>Applying migrations in production and other environments</h3><p>To apply migrations to other environments such as production, you pull changes to the repository containing the migrations and run the <code>prisma migrate deploy</code> command:</p><p><span><img src="https://d33wubrfki0l68.cloudfront.net/5d9831941c87b7e24646bca3d96f91d4b799af6a/b7004/blog/posts/2020-12-migrate-production-workflow.png" alt="Production workflow"><span>Production workflow</span></span></p><hr><h2 id="what-has-changed-since-the-experimental-version"><a href="#what-has-changed-since-the-experimental-version" aria-label="what has changed since the experimental version permalink"></a>What has changed since the Experimental version?</h2><p>The most significant change since the Experimental version is the use of SQL as the format for migrations, making migrations <strong>deterministic</strong>. In other words, the exact steps of the migration are determined when the migration is created, allowing you to inspect the SQL (and make changes if necessary) before running.</p><p>This approach has the following benefits:</p><ul><li>The generated SQL is editable, thereby allowing you to control the exact schema changes.</li><li>The migration is predictable with the exact SQL that will be applied.</li><li>You don't need to write SQL unless you want to change a migration.</li><li>You can perform data migrations using SQL as part of a migration.</li></ul><p>Editable SQL for migrations is useful in scenarios where there are multiple ways to map changes in the Prisma schema to the database, and the desired path cannot be automatically determined.</p><p>For example, when you rename a field in the Prisma schema, that can be interpreted as either deleting the column and adding an unrelated new one or as you renaming the column. By allowing you to inspect and edit the migration SQL, you can decide whether to rename the column (and retain the data in the column) or drop it and add a new one.</p><p>If you're upgrading Prisma Migrate from the Experimental version, check out the <a href="https://www.prisma.io/docs/guides/prisma-guides/prisma-migrate-guides/add-prisma-migrate-to-a-project">upgrade guide</a>.</p><hr><h2 id="whats-next"><a href="#whats-next" aria-label="whats next permalink"></a>What's next</h2><p>This Preview version of Prisma Migrate lays the foundations for the upcoming General Availability release. Some of the improvements we are considering are improved support for native database types, seeding functionality, and finding a way to make database resets in development less disruptive.</p><h3 id="native-database-types"><a href="#native-database-types" aria-label="native database types permalink"></a>Native database types</h3><p>One of the most requested features in Prisma is support for the database's native types. This release is a step closer to that – however, there's still more work to be done for native types to be fully supported.</p><p>Currently, the Prisma schema can only represent a limited set of types: <code>String</code>, <code>Int</code>, <code>Float</code>, <code>Boolean</code>, <code>DateTime</code>, and <code>Json</code>. Each of these types has a default mapping to an underlying database type that's specified for each database connector (see the mappings for <a href="https://www.prisma.io/docs/concepts/database-connectors/postgresql#prisma-migrate">PostgreSQL</a> and <a href="https://www.prisma.io/docs/concepts/database-connectors/mysql#prisma-migrate">MySQL</a>).</p><p>In version <a target="_blank" rel="noopener noreferrer" href="https://github.com/prisma/prisma/releases/tag/2.11.0">2.11.0</a>, we released the <code>nativeTypes</code> Preview feature – the ability to annotate fields in the Prisma schema with the specific native database type that it should be mapped to. <strong>However, the native types preview feature doesn't work with Prisma Migrate yet</strong>.</p><p>Even so, you can still change the types of columns in the generated SQL as long as they are supported, as documented in the <a href="https://www.prisma.io/docs/concepts/database-connectors/postgresql#prisma-migrate">PostgreSQL</a> and <a href="https://www.prisma.io/docs/concepts/database-connectors/mysql#prisma-migrate">MySQL</a> connector docs.</p><hr><p>We built Prisma Migrate for you and are keen to hear your feedback.</p><p>We want to understand how Prisma Migrate fits into your development workflow and how we can help you stay productive and confident while building and evolving data-centric applications.</p><p>🐛 Tried it out and found that it's missing something or stumbled upon a bug? Please <a target="_blank" rel="noopener noreferrer" href="https://github.com/prisma/prisma/issues/new/choose">file an issue</a> so we can look into it.</p><p>🏗 Share your feedback about how the new Prisma Migrate is working out for you on <a target="_blank" rel="noopener noreferrer" href="https://github.com/prisma/prisma/issues/4531">GitHub</a>.</p><p>🌍 Join us on our <a target="_blank" rel="noopener noreferrer" href="https://slack.prisma.io/">Slack</a> in the <a target="_blank" rel="noopener noreferrer" href="https://app.slack.com/client/T0MQBS8JG/C01ACF1DJ1M"><code>#prisma-migrate</code></a> channel for help.</p><p>👷‍♀️ We are thrilled to finally share the Preview version of Prisma Migrate and can't wait to see what you all build with it.</p></div></div></article></div>]]>
            </description>
            <link>https://www.prisma.io/blog/prisma-migrate-preview-b5eno5g08d0b?a</link>
            <guid isPermaLink="false">hacker-news-small-sites-25357937</guid>
            <pubDate>Wed, 09 Dec 2020 12:47:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Games people play with cash flow]]>
            </title>
            <description>
<![CDATA[
Score 329 | Comments 126 (<a href="https://news.ycombinator.com/item?id=25357669">thread link</a>) | @kalonis
<br/>
December 9, 2020 | https://commoncog.com/blog/cash-flow-games/ | <a href="https://web.archive.org/web/*/https://commoncog.com/blog/cash-flow-games/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      
      <section>
        <p>In my <a href="https://commoncog.com/blog/how-first-principles-thinking-fails/">last post</a> I examined how first principles thinking fails. This post is going to be about a single, concrete example — about an argument that started me down this path in the first place.</p><p>A couple of months ago, a friend sent me a blog post titled <em><a href="https://ensorial.com/2020/dont-raise-money/">Startups Shouldn’t Raise Money</a></em>, over at a website called ensorial.com. I thought that the post was tightly argued and reasonably put together, with each proposition leading logically and coherently to the next. I also noticed that the author had taken the time to construct their argument from first principles … which meant it was difficult to refute any individual clause in their chain of reasoning.</p><p>But I also thought it was wrong. I told my friend as much.</p><p>“How is it wrong?” he immediately challenged.</p><p>“Well …” I began. And then I stopped. I realised I didn’t have a good argument for <em>why</em> it was wrong. Every axiom and intermediate proposition were ideas that I agreed with. And it wasn’t so simple as the conclusion being flat out mistaken — you <em>could</em> probably run a small, successful internet business using the ideas laid out in the posts’s argument (internet-based businesses tend to be simpler to manage, and there are many niches you can occupy).</p><p>But I felt uneasy because I thought the framing wasn’t as <em>useful</em>. This was a more complex thing to debunk.</p><h2 id="the-setup">The Setup</h2><p>It’s easy to think that arguments have just three terminal truth values: right, maybe, and wrong. In practice, arguments (and in particular, the sort of argument that we use to justify actions) have many possible truth values. These include things like ‘got the details wrong, but is by-and-large correct’, or ‘is correct but for a <a href="https://commoncog.com/blog/the-right-level-of-abstraction/">different level of abstraction</a>; doesn’t apply here’, or ‘is partially correct, but isn’t as useful compared to a different framing of things.’ The ensorial.com piece is interesting because I think it is an instance of that last one. It was what pushed me to start thinking about all the various ways first principles thinking could go wrong.</p><p>The author’s argument unfolds as follows:</p><ol><li>Startups are risky.</li><li>Raising capital to do a startup reduces skin in the game (you’re spending other people’s money, after all).</li><li>Once you have less skin in the game, it is easier to make bad decisions. The author argues this is due to a) having a capital buffer to cushion you, and b) having more time to waste.</li><li>The alternative is to forego raising venture capital and to create a sustainable business from the beginning, ‘growing linearly with the number of people that give you money for your product.’</li><li>This aligns incentives: you grow only by solving customer problems that they would pay you for. And you’ll pick the shortest path, because you don’t have the luxury of time given to you by an infusion of other people’s money.</li><li>Therefore: startups shouldn’t raise money.</li></ol><p>At first glance, there doesn’t seem to be anything that’s explicitly <em>wrong</em> with this argument. I agree with all the base ideas, and I found myself nodding to the intermediate propositions. The logical correctness of the argument wasn’t a problem. No, my unease stemmed from experience: I <em>knew</em> this wasn’t the right way to think about raising capital. But I couldn’t begin to construct an argument that went against it.</p><figure><img src="https://commoncog.com/blog/content/images/2020/12/argument_chain.f369ba07141442ea959636c21f56e207.png" alt=""></figure><p>My friend and I spent no more than 10 minutes discussing this piece. But in the months after our conversation, I continued to return to the author’s argument. I thought it was interesting because it represented a type of thinking error that you and I are likely to encounter in our lives. The form of the error is subtle, and therefore more difficult to detect; the best description I have for it is: ‘perfectly rational, logically constructed, and not really <em>wrong</em> — but not as useful or as powerful as some other framing.’</p><p>Of course, my obsession was for instrumental reasons: how might you recognise a better framing when you found one? I’ll admit that I was a little naive here: I thought that if I could generalise the structure of this argument, I would be better able to recognise similar errors in the future. Alas, I have not been able to do this to my satisfaction.</p><p>(In practice, most of the older entrepreneurs I know seem to understand the problems with such sensemaking. Plausible arguments are dealt with in a simple manner: you try the recommendations that unfold from the analysis, but you remain alert to see if they give you exactly the results you want. If they don’t, you keep the frame for the time being, but you continue to look out for a better explanation. And how would you know if you have found a better way of thinking about your situation? Simple: you listen carefully. In the words of Malaysian magnate Robert Kuok, “you learn to distill wisdom from the air.”)</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>The most I’ve been able to do is to articulate <em>how</em> the author messed up — and therefore how first principles thinking may fail — something that I <a href="https://commoncog.com/blog/how-first-principles-thinking-fails/">explored in my previous post</a>. The core idea is simple: I believe the author started from a limited set of axioms. If you start from a wrong set of axioms, you would eventually end up with a flawed conclusion. In this case, I think the ensorial.com author started from a deficient understanding of business.</p><p>To generalise a little, people with limited understanding of business think that business is all about making profits. But those who actually run businesses know that running a business is all about managing cash flows.</p><p>And the ensorial.com author’s argument fails because he doesn’t appear to understand this.</p><h2 id="john-malone-and-the-invention-of-ebitda">John Malone and the Invention of EBITDA</h2><p>In 1972, a 32 year old man named John Malone was offered the top job at Tele-Communications Inc (TCI), a cable company. He took charge on April Fool’s Day, 1973.</p><p>At the time of his hiring, Malone was president of Jerrold Electronics, a division of General Instrument that supplied cable boxes and credit to the cable systems companies. He had been offered the Jerrold Electronics job when he was 29 years old, just two years earlier. Before JE, he was at McKinsey Consulting. And before McKinsey, he had a job at AT&amp;T’s famed Bell Labs, where he applied operations research to find optimal company strategies in monopoly markets. Malone concluded that AT&amp;T should increase its debt load and aggressively reduce its equity base through share repurchases — a highly unorthodox recommendation at the time. His advice was delivered to AT&amp;T’s board and then promptly ignored.</p><p>Malone had been thinking about the interplay between debt, profit, cash flow, and corporate taxes for some time. In 1972, when he was first offered the TCI job, he had already noticed a number of structural properties in the cable industry that piqued his interest:</p><ol><li>The cable industry had highly predictable subscription revenues. Cable television customers in the 60s — especially those in rural communities — were eager to upgrade to cable for better TV reception. These subscribers paid monthly fees and rarely cancelled.</li><li>Cable franchises were essentially a legal right to a local monopoly, which meant that cable system operators had limited competition once it established itself in a given locale.</li><li>The industry itself had very favourable tax characteristics — smart cable operators could shelter their cash flow from taxes by using debt to build new systems, and by aggressively depreciating the costs of construction. Once the depreciation ran out on particular systems, they could then sell them to another operator, where the depreciation clock would start anew.</li><li>Most importantly, the entire market was growing like a weed: over the course of the 60s and into the start of the 70s, subscriber counts had grown over twentyfold.</li></ol><p>Of course, Malone didn’t have much time to reflect on these observations. He landed at TCI and found the company at the brink of bankruptcy.</p><p>Bob Magness, the founder of TCI, had grown the company over the course of two decades using a ridiculous pile of debt — about 17 times revenues, at the time of Malone’s hiring. Malone spent his first couple of years at TCI fighting to keep the company alive. He flew into New York every couple of weeks, hat in hand, renegotiating <a href="https://www.investopedia.com/terms/c/covenant.asp">covenants</a> and asking for extensions on debt repayments. At one point during a meeting with TCI’s bankers, Malone threw his keys on the table and threatened to walk, leaving the company to the banks. The bankers capitulated, granting TCI a much needed extension.</p><p>Malone and Magness also had to worry about hostile takeovers, given TCI’s low stock price in the early 70s. They executed a series of complicated financial manoeuvres a year or so after Malone took over, placing a large chunk of stock in a holding company to grant them majority control. Later, they created a separate class of voting stock. These moves gave them hard control of the company, allowing Malone the freedom to focus on righting its finances.</p><p>After three years of hell, TCI was finally pulled back from the brink of financial disaster. And then Malone got to work.</p><p>Malone understood a few things about the cable industry that many outsiders didn’t. First, he understood that cable was like real estate: incredibly high fixed costs up front as you built or bought the systems, and then highly predictable, monopoly cash flows for a long time afterwards. He understood that if he used debt to finance acquisitions, he could keep growing the company, and use the depreciation on acquired systems (plus the write-offs from the loans itself) to delay paying taxes on that cash flow. Third, Malone understood that untaxed cash flows from all of those cable subscribers could be used to a) service the debt, b) pay down some of those loans — only when necessary; Malone wanted to keep the debt-to-earnings ratio at a five-to-one level — but more importantly c) demonstrate to creditors that TCI was a worthy debtor. And finally, Malone understood the benefits of size: the larger TCI got, the lower the cost of acquiring programming (i.e. shows and programs), because it could amortise those costs across its entire subscriber …</p></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://commoncog.com/blog/cash-flow-games/">https://commoncog.com/blog/cash-flow-games/</a></em></p>]]>
            </description>
            <link>https://commoncog.com/blog/cash-flow-games/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25357669</guid>
            <pubDate>Wed, 09 Dec 2020 11:53:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why is the Google Cloud UI so slow?]]>
            </title>
            <description>
<![CDATA[
Score 468 | Comments 370 (<a href="https://news.ycombinator.com/item?id=25357409">thread link</a>) | @mostlystatic
<br/>
December 9, 2020 | https://www.debugbear.com/blog/slow-google-cloud-ui | <a href="https://web.archive.org/web/*/https://www.debugbear.com/blog/slow-google-cloud-ui">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  <div>
    <div>
    
      
      

      

      <div>
        
        

        <p>Opening a page in the Google Cloud Console always takes a long time.</p>
<p>Here are some metrics I collected on a high-end 2018 MacBook Pro on a UK-based Gigabit internet connection.</p>

<div id="slow-gcp-table">
<table>
<thead>
<tr>
<th>Page</th>
<th>Download</th>
<th>JavaScript</th>
<th>CPU Time</th>
<th>Main Content</th>
<th>Fully Loaded</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cloud Functions</td>
<td>4.2 MB</td>
<td>15.7 MB</td>
<td>5.3s</td>
<td>6.7s</td>
<td>8.1s</td>
</tr>
<tr>
<td>Compute Engine</td>
<td>4.5 MB</td>
<td>15.1 MB</td>
<td>6.5s</td>
<td>6.7s</td>
<td>8.1s</td>
</tr>
<tr>
<td>Cloud Storage</td>
<td>4.3 MB</td>
<td>16.2 MB</td>
<td>6.2s</td>
<td>6.5s</td>
<td>8.2s</td>
</tr>
</tbody>
</table>
</div>
<p>Download size is the compressed size, JavaScript size is uncompressed. Main Content is the time when e.g. the Cloud Functions become visible, Fully Loaded is when no more changes are made to the UI.</p>

<p>We can see that each page loads over 15 MB of JavaScript code. A look at the performance timeline in Chrome DevTools confirms that running this code is the primary cause of the poor page performance.</p>
<p><img src="https://www.debugbear.com/public/blog/slow-google-cloud-ui/timeline.png" alt="DevTools CPU timeline showing a large amount of JavaScript work"></p>
<p>This article will take a closer look at the page load process of the Google Cloud Functions page, and examine how it could be sped up.</p>
<p>You can use these strategies to investigate and improve the performance of the apps you're working on.</p>
<h2 id="loading-the-html-document">Loading the HTML document</h2>
<p>The initial HTML request is very fast and only takes about 150ms. It contains an embedded SVG spinner that shows while the first chunk of JavaScript code is loading.</p>
<video autoplay="" muted="" loop="" playsinline="">
    <source src="https://www.debugbear.com/public/blog/slow-google-cloud-ui/gcp-spinner.mp4" type="video/mp4">
</video>
<h2 id="loading-the-initial-java-script-bundles">Loading the initial JavaScript bundles</h2>
<p>These are the first two JavaScript bundles the page starts loading.</p>
<ul>
<li><strong>routemap</strong> 21 KB (103 KB uncompressed)</li>
<li><strong>core,pm_ng1_bootstrap</strong> 1.3 MB (4.8 MB uncompressed)</li>
</ul>
<p>These files don't take too long to download, but running the code freezes the UI for a while. The spinner SVG becomes stuck at this point, until it's replaced by a skeleton UI for Google Cloud Console page.</p>
<p><img src="https://www.debugbear.com/public/blog/slow-google-cloud-ui/gcp-initial-load.png" alt="Filmstrip showing initial rendering of the GCP page"></p>
<p>Here's what happens when the browser wants to run some JavaScript code.</p>
<ol>
<li><strong>Parsing</strong> (done lazily at first, and then as needed later on)</li>
<li><strong>Compilation</strong> (also happens lazily)</li>
<li><strong>Initialization</strong> –&nbsp;the browser runs module initialization code, i.e. code that runs when loading a module rather than when calling one of its functions</li>
<li><strong>Running core app code</strong> – renders the application using the initialized modules</li>
</ol>
<p>For the whole Google Cloud page, just parsing the source code takes 250ms, and compilation takes another 750ms (not including the 113 ms spent on "Compile Script").</p>
<p><img src="https://www.debugbear.com/public/blog/slow-google-cloud-ui/devtools-profile.png" alt="DevTools profile showing a breakdown of CPU activity"></p>
<p>The initial render of the Angular app takes about 1s.</p>
<p><img src="https://www.debugbear.com/public/blog/slow-google-cloud-ui/initial-bundle.png" alt="JavaScript execution flamechart"></p>
<p>Eventually we start to see a new spinner.</p>
<p><img src="https://www.debugbear.com/public/blog/slow-google-cloud-ui/second-spinner.png" alt="Page frame and new spinner"></p>
<h2 id="loading-page-bundles">Loading page bundles</h2>
<p>Once the generic Google Cloud UI is rendered the page starts loading 18 additional JavaScript files with an overall size of 1.5 MB.</p>
<p>Making a lot of separate requests isn't actually a problem though – it can improve performance by increasing the likelinhood of cache hits, and splitting up bundles makes it easy to load only necessary code.</p>
<p>After loading the first set up bundles the app starts making fetch requests and loads 3 more bundles at a total size of 6 MB.</p>
<p>When loading the page on my normal network the requests all kind of blurred together and it was hard to see which requests were sequential. So this screenshot shows the request chart on a throttled connection.</p>
<p><img src="https://www.debugbear.com/public/blog/slow-google-cloud-ui/more-page-bundles.png" alt="Request waterfall showing three sets of JavaScript being loaded sequentially"></p>
<h2 id="loading-the-list-of-cloud-functions">Loading the list of Cloud Functions</h2>
<p>The request loading the list of Cloud Functions takes about 700ms. But it doesn't start as soon as the bundles are loaded, in part because there's a <code>testIamPermissions</code> request that needs to finish first.</p>
<p>As a result the CPU ends up being idle for half a second –&nbsp;this time could be used better if the request started sooner.</p>
<p><img src="https://www.debugbear.com/public/blog/slow-google-cloud-ui/loading-functions.png" alt="Waterfall showing requests made to load the list of cloud functions"></p>
<p>Finally the app re-renders and we get the list of Cloud Functions we wanted to see.</p>
<p><img src="https://www.debugbear.com/public/blog/slow-google-cloud-ui/cloud-functions.png" alt="Page showing GCP Cloud Functions"></p>

<p>Chrome DevTools has a code coverage tool tracks which parts of the code actually run on the current page. This can help identify code that doesn't have to be loaded.</p>
<p>The Cloud Functions page runs 53% of the JavaScript code it downloads. This is actually a bit disappointing, as it means that even if only necessary code is loaded it would still only cut the total JavaScript size of the page in half.</p>
<p><img src="https://www.debugbear.com/public/blog/slow-google-cloud-ui/code-coverage.png" alt="Chrome DevTools Code Coverage tool"></p>
<h2 id="moving-configuration-into-json">Moving configuration into JSON</h2>
<p>A good amount of the code actually consists of configuration objects. For example, this 200 KB object with 4997 keys.</p>
<p><img src="https://www.debugbear.com/public/blog/slow-google-cloud-ui/configuration.png" alt="Configuration object in a JavaScript bundle"></p>
<p>Loading this as a JSON string with <code>JSON.parse</code> could be faster, as JSON is simpler to parse than a JavaScript object. This would be easy to do, but might not result in a huge performance improvement.</p>
<p>Ideally the app wouldn't need to load the full list on the client, but this would be harder to implement.</p>
<h2 id="reduce-code-duplication">Reduce code duplication</h2>
<p>The 200KB JSON object above is actually included in two of the JavaScript bundles. Breaking it out and reusing it would save download and processing time.</p>
<p>The same seems to apply to a bunch of UI components, like this one.</p>
<p><img src="https://www.debugbear.com/public/blog/slow-google-cloud-ui/code-duplication.png" alt="Duplicate code in DevTools code search"></p>
<h2 id="prioritize-primary-content">Prioritize primary content</h2>
<p>The Google Cloud page loads a large initial JavaScript bundle. The longer it takes to load and initialize this code, the longer it takes to load page-specific code and to render the list of Cloud Functions the user wants to see.</p>
<p>But the initial bundle also contains secondary content, like the complex navigation sidebar. This menu becomes functional before the main page content is loaded, but it should only be loaded after the primary content.</p>
<p><img src="https://www.debugbear.com/public/blog/slow-google-cloud-ui/prioritize-primary-content.png" alt="Sidebar menu is open while main content is still loading"></p>
<p>Google Cloud already does this in some cases. For example, the page initially renders a simpler version of the header and then loads more complex features later on.</p>
<p><img src="https://www.debugbear.com/public/blog/slow-google-cloud-ui/header.png" alt="Header doesn't show project dropdown at first and then shows it later"></p>
<h2 id="conclusion">Conclusion</h2>
<p>While the performance of static pages tends to be dominated by render-blocking network requests, single-page apps are often blocked by JavaScript execution or loading account data.</p>
<p>Downloading large amounts of code can hurt performance on slow connections, but due to compression and caching CPU processing often has a greater impact.</p>
<p>If you want to track the performance of your website, including logged-in pages, <a href="https://www.debugbear.com/">give DebugBear a try</a>.</p>
<blockquote><p lang="en" dir="ltr">Why is the Google Cloud UI so slow? This article looks at the performance of a large JavaScript application and explores how it could be made faster.<a href="https://t.co/HSHhCXYQCi">https://t.co/HSHhCXYQCi</a></p>— DebugBear (@DebugBear) <a href="https://twitter.com/DebugBear/status/1336621651669213186?ref_src=twsrc%5Etfw">December 9, 2020</a></blockquote> 

        

        
        <div>
            <div>
                <p>
                    DebugBear is a website monitoring tool built for front-end teams.
                    Track performance metrics and Lighthouse scores in CI and production.
                    <a href="https://www.debugbear.com/?noredirect&amp;from_blog">Learn more</a>.
                </p>
                
            </div>
        </div>
        <div>
        
                    <!-- Begin Mailchimp Signup Form -->
            
            
            <div>
                <div>
                    
                    <div>
                        
                        <h2>Get new articles on web performance <!-- and debugging --> by email.</h2>
                    </div>
                </div>
                
            </div>
        
            
        
        </div>      </div>
    </div>
  </div>
</div></div>]]>
            </description>
            <link>https://www.debugbear.com/blog/slow-google-cloud-ui</link>
            <guid isPermaLink="false">hacker-news-small-sites-25357409</guid>
            <pubDate>Wed, 09 Dec 2020 11:08:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AsyncAPI partners with Postman to boost development of Asynchronous APIs]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25356677">thread link</a>) | @derberg
<br/>
December 9, 2020 | https://www.asyncapi.com/blog/asyncapi-partners-with-postman | <a href="https://web.archive.org/web/*/https://www.asyncapi.com/blog/asyncapi-partners-with-postman">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><img src="https://www.asyncapi.com/img/posts/asyncapi-partners-with-postman.png" alt="Post cover image"><p>I'm proud and honored to let you know that we're partnering with <a href="https://www.postman.com/">Postman</a><undefined> to boost the development of Asynchronous APIs to a new level <span role="img" aria-label="rocket">🚀</span></undefined></p><p>Since the very beginning, I knew the duty we had at hand was challenging. And still is! The specification was just the trigger of a snowball effect. What's the spec for if you can't do anything with it? Tooling is as important as the specification. However, tooling is a number of times more complex than the specification. We engineers don't want to abandon our favorite programming language and framework, therefore, it's AsyncAPI's responsibility to integrate with the existing tools in the market. <strong>The specification (and tools) should work for the user, not the other way around.</strong> Partnering with Postman allows us to boost the development of more and better tools to help engineers create and maintain Asynchronous APIs while using their favorite programming languages and frameworks.</p><p><strong>Our goal is to make Asynchronous APIs as successful and mature as REST APIs.</strong> We are aware this is a long journey but, with Postman's help, we'll be able to grow the team and continue working on the AsyncAPI specification and all the necessary tools to create a delightful developer experience. The AsyncAPI Initiative team is fully committed to open source software (OSS), and the partnership with Postman will help us keep doing our job with freedom and independence.</p><h2 id="next-steps">Next steps</h2><p>We want to make the AsyncAPI Initiative a neutral and independent place for collaborating on defining the future of Asynchronous APIs. Next step for us is to host the project in a neutral foundation to guarantee the long-term success of the initiative. We're currently in conversations with different actors of the OSS world to make sure the initiative remains independent.</p><p>Also, we want you to work with us. <a href="https://www.asyncapi.com/jobs">We are hiring</a> at Postman to work full-time on AsyncAPI. In the first half of 2021, we'll open a bunch of positions, including Software Engineers, Graphic Designers, Technical Writers, and more. Make sure you don't miss them!</p><div><h3>Receive an email when we publish a new job offer:</h3><p>We respect your inbox. No spam, promise ✌️</p></div><p>Before I finish, I would love to thank <a href="https://twitter.com/kinlane/">Kin Lane</a> and <a href="https://twitter.com/a85">Abhinav Asthana</a> for being so supportive. And of course, a huge shout out to <a href="https://twitter.com/derberq">Łukasz Gornicki</a> and <a href="https://twitter.com/e_morcillo">Eva Morcillo</a> for their tireless support. None of these would be possible without their help.</p><p>There's a bright future ahead for Asynchronous APIs. 2021 will be the year of AsyncAPI, the year of you, our beloved open-source community.</p><p><undefined>Cheers! <span role="img" aria-label="clinking beer mugs">🍻</span></undefined></p></article></div>]]>
            </description>
            <link>https://www.asyncapi.com/blog/asyncapi-partners-with-postman</link>
            <guid isPermaLink="false">hacker-news-small-sites-25356677</guid>
            <pubDate>Wed, 09 Dec 2020 08:57:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Progress Over Perfection]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25356266">thread link</a>) | @phughes1980
<br/>
December 8, 2020 | https://www.thedailymba.com/2020/12/08/my-new-side-hustle-mantra-progress-over-perfection/ | <a href="https://web.archive.org/web/*/https://www.thedailymba.com/2020/12/08/my-new-side-hustle-mantra-progress-over-perfection/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><main><article aria-label="My New Side Hustle Mantra: Progress Over Perfection"><div><figure><img loading="lazy" width="1880" height="1249" src="https://i1.wp.com/www.thedailymba.com/wp-content/uploads/2020/12/pexels-photo-117602.jpeg?resize=1880%2C1249&amp;ssl=1" alt="traffic red blue sign" srcset="https://i1.wp.com/www.thedailymba.com/wp-content/uploads/2020/12/pexels-photo-117602.jpeg?w=1880&amp;ssl=1 1880w, https://i1.wp.com/www.thedailymba.com/wp-content/uploads/2020/12/pexels-photo-117602.jpeg?resize=300%2C199&amp;ssl=1 300w, https://i1.wp.com/www.thedailymba.com/wp-content/uploads/2020/12/pexels-photo-117602.jpeg?resize=1024%2C680&amp;ssl=1 1024w, https://i1.wp.com/www.thedailymba.com/wp-content/uploads/2020/12/pexels-photo-117602.jpeg?resize=768%2C510&amp;ssl=1 768w, https://i1.wp.com/www.thedailymba.com/wp-content/uploads/2020/12/pexels-photo-117602.jpeg?resize=1536%2C1020&amp;ssl=1 1536w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"><figcaption>Photo by Mabel Amber on <a href="https://www.pexels.com/photo/traffic-red-blue-sign-117602/" rel="nofollow">Pexels.com</a></figcaption></figure><p>A Guest Post by Phil Hughes</p><p>“Progress over perfection”. I’ve written this on the whiteboard next to my standing desk in the home office/guitar studio/mancave/junk room.&nbsp;</p><p>I heard this statement earlier on in the year. Must have been around May or June, it instantly struck a chord with me.</p><p>Working full time and trying to get no less than 3 side hustles and a podcast off the ground is a big ask.&nbsp;</p><p>I have used the progress over perfection mantra before. It was worded slightly differently; I can’t remember the exact statement.</p><p>However, I hadn’t fully embraced the concept.&nbsp;</p><p>When it came to my core skill of software development, I could easily apply this principle.&nbsp;</p><p>Every other part of the side hustle lifestyle was still “I got to make sure this is the best it can be”.</p><h2 id="h-already-integrated-into-my-9-5"><span id="Already_Integrated_into_My_95"></span>Already Integrated into My 9-5<span></span></h2><p>Like I’ve just touched on. I’ve used progress over perfection, day to day with my software development ‘hat’ on.</p><p>I’m going to sound arrogant now. Having spent 14+ years developing software, I’m confident in my skills and the fact I can get stuff done and out the door, quicker than a lot of my “development” peers.</p><p>By using this principle, I can get the feature ‘X’ out to the world. Start feature ‘Y’ and deliver that in rapid time. Same too with Feature ‘Z’</p><p>Then go back to feature ‘X’ and make it better. Progress over perfection in action.</p><h3 id="h-progress-over-perfection-a-real-world-app-example"><span id="Progress_Over_Perfection_%E2%80%93_A_Real_World_App_Example"></span>Progress Over Perfection – A Real World App Example<span></span></h3><p>I’m writing this post towards the end of 2020, having gone through the Coronavirus pandemic.</p><p>At the start of lockdown, I had more spare time and I came up with a <a href="https://baitcamp.net/">mobile app idea for a hobby of mine, fishing</a>.&nbsp;</p><p>After doing a bit of product validation and finding out what other fishermen would like to see in the app. I sketched out the features I was going to include in version 1 of the app.</p><p>One of the features was to allow the user to upload photos to the app, linking them to a fishing trip.</p><p>Having never done anything like this before, it was a learning curve, to say the least. I decided to use the “progress over perfection” mantra by making sure the uploading, editing, removing, and retrieving of the photos, from a functionality point of view, was “perfect”</p><p>However, the way the user uploaded the photos and then viewing them was shocking, “perfect” was just a dot on the horizon.&nbsp;</p><p>That said, I went with it as I wanted to get the app published on both the <a href="https://apps.apple.com/us/app/id1519992229">Apple App Store</a> and <a href="https://play.google.com/store/apps/details?id=uk.co.phhdigital.baitcamp">Google Play</a> as quickly as possible.&nbsp;</p><p>5 months after the initial idea came to me, I had published the app on both platforms. WAHOO!</p><p>After feedback from people testing version 1. And the fact I wasn’t happy with the photo upload/viewing features. I have been able to start work on version 1.1 to sort this problem out.</p><p>Another day or two and I will have a slick way of managing the photos. Be able to release an update very quickly. As well as ironing out a few bugs that have come to light.</p><p>I genuinely believe if I had kept on working on the app until I was entirely happy with the photo functionality, I wouldn’t have it published or have people downloading the app and using it.</p><p>The key here was “progress over perfection” to get something live that was usable, then go from there.</p><h2 id="h-side-hustling-and-what-you-need-to-learn"><span id="Side_Hustling_and_What_You_Need_To_Learn"></span>Side Hustling and What You Need To Learn<span></span></h2><p>When it comes to other aspects of side hustling, I’m still “newborn” in a lot of the skills you need.&nbsp;</p><p>Writing blog posts, writing social media posts, copywriting, creating nice-looking webpages, building sales funnels, creating ads, shooting walkthroughs for YouTube, recording video sales letters, editing videos, coming up with offers, researching, scheming and plotting, creating eBooks, creating eBook covers, creating eBook mock-ups, recording podcasts, editing podcasts.</p><p>How did I come up with that list of things to do? I looked at my planner at what tasks I wanted to achieve over the last 2 weeks.&nbsp;</p><p>That’s on top of coding 3 software products by myself. Working 8-4 Monday through Thursday. And, having a life, like spending time with my wife, playing guitar, and going fishing.</p><p>It can be very overwhelming.&nbsp;</p><p>You see how great other people are at all these other things and want your stuff to be as “perfect” as theirs</p><p>That’s why this new mantra has been a bit of a breakthrough for me.</p><h2 id="h-progress-over-perfection-and-switching-your-mindset"><span id="Progress_Over_Perfection_and_Switching_Your_Mindset"></span>Progress Over Perfection and Switching Your Mindset<span></span></h2><p>As I’ve touched upon. For certain aspects of my day to day I would apply “progress over perfection”. Not all aspects though.</p><p>Hearing this really got me thinking about what I wanted to achieve.&nbsp;</p><p>One thing it also did was remove some fear I had around things.&nbsp;</p><p>For example, I had started digging deep into the concept of creating online sales funnels to promote my products and reach my “golden goose” of 350 paying customers.</p><p>My dream is to work for myself running a software product. After a bit of number crunching, I worked out I would need 350 monthly subscribers for me to realize my dream.&nbsp;</p><p>Sales funnels could help me achieve this.</p><h3 id="h-sales-funnel-progress-over-perfection"><span id="Sales_Funnel_Progress_Over_Perfection"></span>Sales Funnel Progress Over Perfection<span></span></h3><p>This is where the fear kicked in. I was worried that I would create a sales funnel, it would completely bomb, and I would feel like a failure.&nbsp;</p><p>I wanted my sales funnels to be successful as soon as I start driving traffic to it. If this were a software product, I wouldn’t think like that. I know they will be some bugs and users would probably want something different than what I had built.</p><p>After consuming a lot of content around the sales funnel process and how even the most experienced “funnel hackers” can’t get their funnel to work out of the gate.&nbsp;</p><p>I decided that I just need to get it out there and see what “feedback” I get.</p><p>So, I continued to put it together as best I could, giving myself a time constraint that it must be done by the end of that week. Then the week after I could drive some paid traffic to it.</p><p>I got the opt-in page up and running, where someone could submit their details and in return get a free eBook user guide.</p><p>Next, I spent most of my time working on the sales page and putting together an offer and pricing.&nbsp;</p><p>Thirdly, I added a “One Time Offer” that the person would be shown if they subscribed to the product I was promoting on the sales page.</p><p>Finally, I put together an email sequence that would be sent to anyone who opted in for the free eBook, as a follow up to get them to revisit my funnel.</p><p>The next week I created a Facebook ad campaign and got loads of people into the funnel. Guess how many sales I made?</p><p>ZERO.</p><h3 id="h-analyzing-the-results"><span id="Analyzing_the_Results"></span>Analyzing the Results<span></span></h3><p>Sounds terrible right? It was, but I wasn’t downhearted.&nbsp;</p><p>Looking into the stats, the opt-in page was working. I was getting almost 45% of the visitors to put in their email address and request a copy of the eBook.</p><p>Yes, no one bought from me. I knew that part of my sales funnel did work. However, I was building an email list of people that I could keep in contact with, which I hadn’t be able to do before.</p><p>“Progress over perfection”.</p><p>I ended up scrapping this funnel as I still didn’t ‘convert’ after a few attempts at rewriting parts of the sales page.</p><p>Taking the learnings from the opt-in success, I created a brand-new funnel. This targeted a different group of people. Again, this funnel wasn’t the success I wanted. My opt-in rate was still high, 35% and I got my first ever paying customer.</p><p>Someone signed up and subscribed to <a href="https://outflash.xyz/">my software product called Outflash</a>. They even took my up one-time offer.</p><p>I was blown away. Yes, it wasn’t the riches you pray for, but I had made progress on this skill.</p><p>Progress! Progress towards what I deem successful.</p><h2 id="h-podcasting-and-getting-yourself-out-there"><span id="Podcasting_and_Getting_Yourself_Out_There"></span>Podcasting and Getting Yourself Out There<span></span></h2><p>A problem with being a developer, is you think it will be like the movie Field of Dreams. “Build it and they will come”. With a software product, this NEVER happens.</p><p>Someone said if you want to get yourself out there to promote your products and services. Use a platform or media that you enjoy yourself. For me this was podcasting.</p><p>I’m an avid podcast listener. Whether it be working out, going for a run, listening while coding. I even put a podcast on while cooking a Sunday Dinner.</p><p>Starting a podcast can be scary though. What do I talk about? Do I have enough content to get past the first 10 episodes? How do I even publish a podcast? Which platforms do I publish to?</p><h3 id="h-excuses-excuses-just-do-it"><span id="Excuses,_Excuses,_Just_Do_It"></span>Excuses, Excuses, Just Do It<span></span></h3><p>Having mild success with the sales funnels gave me a lot of belief in the saying “Just Do It”.</p><p>One of the podcasts I listen to is by two guys that have launched their own podcasting hosting and publishing platform. I know, a bit Inception right!</p><p>So, I decided to use their platform to publish and host <a href="https://www.philliphughes.co.uk/podcast/">my podcast</a>. Which platforms to publish to? Their service had guides on how to publish to <a href="https://podcasts.apple.com/gb/podcast/find-your-side-hustle/id1523991465">Apple</a> and <a href="https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy50cmFuc2lzdG9yLmZtL2ZpbmQteW91ci1zaWRlLWh1c3RsZQ?sa=X&amp;ved=0CBUQ27cFahcKEwiIrMe2-eLqAhUAAAAAHQAAAAAQAQ">Google podcasts</a>, <a href="https://open.spotify.com/show/19pwsqTl75RSGrhhrsFIWW">Spotify</a>, and <a href="https://www.stitcher.com/podcast/find-your-side-hustle">Stitcher</a>. I didn’t overthink it, I registered with those 4 providers first and thought, “progress over perfection”, I can publish to more platforms later.</p><p>People also think they need a mass of equipment to record and edit a podcast. I was on a roll; nothing was stopping me from getting my first episode out to the world.&nbsp;</p><p>Quickly opening Amazon and searching for “cheap podcast mic”. I bought the 3<sup>rd</sup> one I saw for £20, with Amazon Prime it was delivered the next day.</p><p>In another quick search for “free podcast editing software,” I found a piece of software that I could do basic audio editing with, like snipping or increasing the volume.&nbsp;</p><h3 id="h-other-people-skills-to-progress-over-perfection"><span id="Other_People_Skills_To_Progress_Over_Perfection"></span>Other People Skills To Progress Over Perfection<span></span></h3><p>The last two pieces of the podcasting puzzle were that I needed a logo/header for the podcast that will be unique to me and my chosen topic.</p><p>I have used Fiverr in the past and thought it would be a good place to find someone to do it for me, quickly.&nbsp;</p><p>If I didn’t like it, I could always change it a few months down the line. I found someone who had a decent portfolio of similar work, that wasn’t too pricy. I put the order in, with a description of what I was looking for, and forgot about it for the rest of the evening.</p><p>While the logo was being designed, I needed to get together a list of episode ideas so I could record a least a handful to get me started.</p><p>This time I turned to Evernote, created …</p></div></article></main></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.thedailymba.com/2020/12/08/my-new-side-hustle-mantra-progress-over-perfection/">https://www.thedailymba.com/2020/12/08/my-new-side-hustle-mantra-progress-over-perfection/</a></em></p>]]>
            </description>
            <link>https://www.thedailymba.com/2020/12/08/my-new-side-hustle-mantra-progress-over-perfection/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25356266</guid>
            <pubDate>Wed, 09 Dec 2020 07:38:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Emacs Survey 2020 Results]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 6 (<a href="https://news.ycombinator.com/item?id=25354551">thread link</a>) | @pama
<br/>
December 8, 2020 | https://emacssurvey.org/2020/ | <a href="https://web.archive.org/web/*/https://emacssurvey.org/2020/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <h2>Questions</h2>
      <p>For reference, this was <a href="https://emacssurvey.org/2020/emacs-user-survey-2020.org">the survey questions</a> in org-mode format.</p>
      <h2>Data</h2>
      <ul>
        <li>
          <a href="https://emacssurvey.org/2020/Emacs-User-Survey-2020-raw.csv">Raw data</a>
          <ul>
            <li>the reconciled data from both webform and email submissions</li>
            <li>absolutely no change made aside from a few instances where PII and email addresses were redacted</li>
          </ul>
        </li>
        <li>
          <a href="https://emacssurvey.org/2020/Emacs-User-Survey-2020-clean.csv">Cleaned up data</a><br>
          It might get updated in the future, but right now it was derived from the raw data in a best-effort attempt:
          <ul>
            <li>removed negative years in "For how many years have you been using Emacs?"</li>
            <li>unified responses for "How did you hear about this survey?" as Hacker News, Emacs China and Emacs News weren't part of the options</li>
            <li>unified responses for "Which theme do you use?", especially around spelling</li>
            <li>general cleanup and unified of responses which only differed by punctuation and casing</li>
          </ul>
        </li>
      </ul>
      <h2>Statistics about the survey</h2>
      
      <h2>Analysis</h2>
      <p>There is a lot of data to look at in many different ways. For now, I performed a simple question-by-question analysis using a <a href="https://github.com/abrochard/emacs-survey/blob/main/2020/Emacs%20User%20Survey%202020.ipynb">Jupyter Notebook</a>.</p>
      <p>Also, since free text was available for most questions, it can be hard to categorize some of the results. For multiple choice questions, I did a best effort attempt to bundle responses with low cardinality into an "other" section, which can get quite big in some cases! I also did not attempt to graph anything for pure free text questions. I encourage anyone who is curious to inspect the full responses, either in the notebook or looking at the data directly. The omitted free text questions are:
        </p><ul>
          <li>If you use org-mode, for what purpose?</li>
          <li>Do you use a language server with lsp-mode or eglot? With what languages?</li>
          <li>Do you use an Emacs debugger interface? What do you use? (Gdb, dap-mode etc)</li>
          <li>What are some of the Emacs improvements you are the most interested in?</li>
          <li>What do you think are Emacs' greatest strengths?</li>
          <li>Can you recall any difficulties you faced initially learning Emacs?</li>
          <li>What is the one thing you would like Emacs to do differently?</li>
          <li>If there is another survey in 2021, would you be opposed to it containing optional &amp; general demographics questions?</li>
          <li>Do you have a preferred platform for filling out the survey in the future?</li>
          <li>Do you have general feedback about the survey process?</li>
        </ul>
      

      <p>Also if you have some cool analysis and want to share it, please <a href="mailto:contact@emacssurvey.org">let us know</a> and we can link to you.</p>
      <p><img src="https://emacssurvey.org/2020/how-would-you-characterize-your-use-of-emacs.png">
      <img src="https://emacssurvey.org/2020/what-do-you-use-emacs-for.png">
      <img src="https://emacssurvey.org/2020/for-how-many-years-have-you-been-using-emacs.png">
      <img src="https://emacssurvey.org/2020/which-version-of-emacs-do-you-primarily-use.png">
      <img src="https://emacssurvey.org/2020/which-os-do-you-primarily-use-emacs-on.png">
      <img src="https://emacssurvey.org/2020/how-do-you-run-emacs.png">
      <img src="https://emacssurvey.org/2020/how-do-you-use-emacs.png">
      <img src="https://emacssurvey.org/2020/if-you-use-emacs-gui-do-you-disable-any-of-the-graphical-elements.png">
      <img src="https://emacssurvey.org/2020/is-your-configuration-based-on-any-starter-kit.png">
      <img src="https://emacssurvey.org/2020/what-keybindings-do-you-use-now.png">
      <img src="https://emacssurvey.org/2020/when-you-started-using-emacs-what-keybindings-did-you-use-then.png">
      <img src="https://emacssurvey.org/2020/prior-to-using-emacs-what-was-your-primary-editor.png">
      <img src="https://emacssurvey.org/2020/describe-your-org-mode-usage.png"></p><!-- <p>If you use org-mode, for what purpose?</p> -->
      <p><img src="https://emacssurvey.org/2020/which-completionselection-framework-do-you-use.png">
      <img src="https://emacssurvey.org/2020/how-do-you-manage-third-party-elisp.png">
      <img src="https://emacssurvey.org/2020/how-do-you-get-emacs-packagesif-applicable.png">
      <img src="https://emacssurvey.org/2020/can-you-list-some-of-your-favorite-packages.png">
      <img src="https://emacssurvey.org/2020/which-theme-do-you-use.png">
      <img src="https://emacssurvey.org/2020/what-package-do-you-use-for-error-checking.png">
      <img src="https://emacssurvey.org/2020/do-you-use-tramp.png">
      <img src="https://emacssurvey.org/2020/do-you-use-magit.png">
      <img src="https://emacssurvey.org/2020/what-package-do-you-use-for-project-management.png">
      <img src="https://emacssurvey.org/2020/do-you-use-a-shellterminal-emulator-in-emacs.png">
      <img src="https://emacssurvey.org/2020/do-you-use-an-email-client-in-emacs.png">
      <img src="https://emacssurvey.org/2020/what-is-your-elisp-proficiency.png">
      <img src="https://emacssurvey.org/2020/if-you-use-emacs-for-programming-which-languages-do-you-program-in.png"></p><!-- <p>Do you use a language server with lsp-mode or eglot? With what languages?</p>
           <p>Do you use an Emacs debugger interface? What do you use? (Gdb, dap-mode etc)</p> -->
      <p><img src="https://emacssurvey.org/2020/have-you-ever-contributed-to-gnu-emacs-coreelpa.png">
      <img src="https://emacssurvey.org/2020/have-you-ever-contributed-to-melpa-package.png">
      <img src="https://emacssurvey.org/2020/have-you-ever-contributed-financially-to-emacs-development-either-via-fsf-or-directly.png">
      <img src="https://emacssurvey.org/2020/what-emacs-community-forums-have-you-visited-in-the-past-year.png"></p><!-- <p>What are some of the Emacs improvements you are the most interested in?</p>
           <p>What do you think are Emacs' greatest strengths?</p>
           <p>Can you recall any difficulties you faced initially learning Emacs?</p>
           <p>What is the one thing you would like Emacs to do differently?</p> -->
      <p><img src="https://emacssurvey.org/2020/how-did-you-hear-about-this-survey.png"></p><!-- <p>If there is another survey in 2021, would you be opposed to it containing optional & general demographics questions?</p>
           <p>Do you have a preferred platform for filling out the survey in the future?</p>
           <p>Do you have general feedback about the survey process?</p> -->
    </div></div>]]>
            </description>
            <link>https://emacssurvey.org/2020/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25354551</guid>
            <pubDate>Wed, 09 Dec 2020 02:17:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Kite Power for Mauritius]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 6 (<a href="https://news.ycombinator.com/item?id=25353559">thread link</a>) | @usrusr
<br/>
December 8, 2020 | https://skysails-power.com/index.html?artikel=Kite-Power-For-Mauritius | <a href="https://web.archive.org/web/*/https://skysails-power.com/index.html?artikel=Kite-Power-For-Mauritius">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://skysails-power.com/index.html?artikel=Kite-Power-For-Mauritius</link>
            <guid isPermaLink="false">hacker-news-small-sites-25353559</guid>
            <pubDate>Wed, 09 Dec 2020 00:16:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building a Better Monolith (With WebAssembly)]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25353335">thread link</a>) | @cohix
<br/>
December 8, 2020 | https://blog.suborbital.dev/building-a-better-monolith | <a href="https://web.archive.org/web/*/https://blog.suborbital.dev/building-a-better-monolith">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text"><h2 id="the-monolith-gets-a-bad-rap">The monolith gets a bad rap</h2>
<p>When you hear that a company runs a monolith, you may think they're old-fashioned and they must have trouble scaling it, right? I'm here to tell you that <a target="_blank" href="https://m.signalvnoise.com/the-majestic-monolith/">some people</a> (myself included) think monoliths are awesome for a whole lot of teams. That said, technology has advanced, and I truly think it's time to revisit the monolith with a new approach.</p>
<p>If you're new to back-end development, a monolith is a server-side system that runs as one.... thing. It's a single program that starts up, serves some network requests, and then terminates. Alternatives to 'the monolith' include service-oriented architectures (SOA), microservices, serverless functions, and probably a few that I haven't heard of. Each of these approaches has their time and place, and I salute anyone who has made an educated decision to build a system with these patterns. I'm of the opinion that a large number of web applications and services would be well served by a monolith.... but with some upgrades.</p>
<h2 id="you-need-some-services-to-lighten-the-load">You need some services to lighten the load</h2>
<p>The reasoning behind the alternative design patterns is very sound. By distributing the work among many different “things”, you make the system as a whole able to handle more. It is well known that by doing so, you introduce more complexity, which requires more effort, and therefore often more people-power and more money. The one possible exception I can see is serverless functions, which do indeed simplify many things, but whose downsides comes in the form of more difficult testing, vendor dependence, or the need for non-commodity tooling. When done right, the extra effort can lead to a very capable system whose benefits are truly remarkable. The classic example is Netflix, and they’ve done very well for themselves.</p>
<h2 id="finding-a-middle-ground">Finding a middle ground</h2>
<p>If monoliths are hard to scale and microservices are too complex, then how do you design a system that can scale with your traffic and your development team without becoming a pain to operate, maintain and expand its functionality? Over the past few years, it has become clear that a middle-ground is needed. I don’t expect this solution to work for everyone, but most products aren’t serving the kind of traffic that <strong>really</strong> makes the microservice effort worth it.</p>
<h2 id="enter-the-sufa-design-pattern">Enter the SUFA design pattern</h2>
<p>Before getting into the core of what SUFA is, I want to mention that this is not an entirely new way of thinking. Things like the actor pattern, the <a target="_blank" href="https://inconshreveable.com/10-07-2015/the-neomonolith/">Neomonolith</a>, and others have stipulated some similar ideas over the years, and SUFA is just one way of combining several concepts into one straightforward design pattern. So, what is it?</p>
<p><strong>Simple, Unified, Function-based Applications.</strong></p>
<p>Let’s break that down:</p>
<h3 id="simple">Simple</h3>
<p>A system designed with SUFA can be run in the simplest of deployment scenarios. Auto-scaling groups have existed for a long time, and they’re made even easier by container orchestration systems. A SUFA system can be run on one ASG, or can be expanded with a service mesh to allow for capability groups (which we’ll talk about in a future post).</p>
<h3 id="unified">Unified</h3>
<p>Rather than multiple services who each exist as something to be deployed, SUFA systems exist as one single deployable. This can be a Docker image, an AMI, or some other artifact, but there is only one <strong>thing</strong> that needs to be built. It should be built by CI/CD on a continuous or tagged release cadence, and it should be made available in an artifact registry such as a Docker registry or S3 bucket. </p>
<h3 id="function-based">Function-based</h3>
<p>A standard monolith probably includes a handler layer which is responsible for taking API requests and making calls to a business logic or data storage layer to handle those requests. SUFA systems instead handle requests by chaining together a series of functions, each completely independent and unaware of one another. Functions should expect a particular input, perform some operations, and produce an output to be passed into functions further down the chain. Functions should be easily testable and reusable across different scenarios (such as for different API requests). SUFA systems should also be designed to consume and produce event-based traffic as a primary method of communication.</p>
<h3 id="applications">Applications</h3>
<p>Well this seems like it should be straightforward, but in SUFA design, “Application” has a very particular meaning. A SUFA system should serve one single application, meaning that it should encompass all of the capabilities needed for a fully formed product. This can be up for some interpretation (such as whether a company should have one SUFA for their whole business, even if they have distinct product areas), but the point is to avoid having multiple “things” serving one application. If functionality needs to be shared across multiple applications, the functions comprising the SUFA system should be easily reusable and composed for other purposes. </p>
<p>You'll notice that this is all very technology-agnostic and vendor-agnostic. SUFA is meant to span across languages, cloud vendors, and deployment environments. SUFA is a way of designing your server-side system such that it is testable, scalable, and secure. You'll notice I haven't touched on scalability yet, so let's discuss that</p>
<h2 id="sufa-at-scale">SUFA at scale</h2>
<p>The critical factor that allows a SUFA system to scale is that it is composed of independent functions. SUFA systems should rely on an underlying framework to orchestrate the execution of these functions such that it can scale effectively. By using a function runner or job scheduler to run the required functions, a SUFA framework abstracts away <em>how</em> the functions are executed, and the programmer writing the code only needs to indicate which functions to run, and in what order.</p>
<p>Additional scalability is provided by <strong>capability groups</strong> and <strong>meshing</strong>, which I plan on writing follow-up posts for, as they deserve to be explored at length.</p>
<h2 id="suborbital-atmo">Suborbital Atmo</h2>
<p>The SUFA pattern was designed in concert with <a target="_blank" href="https://github.com/suborbital/atmo">Atmo</a>, which is an all-in-one framework upon which SUFA systems can be built. Atmo uses a file known as a 'Directive' to describe all aspects of your application, including how to chain functions to handle requests. You can write your functions using several languages to be run atop Atmo, as it is built to use WebAssembly modules as the unit of compute. Atmo will automatically scale out to handle your application load, and includes all sorts of tooling and built-in best practices to ensure you're getting the best performance and security without needing to write a single line of boilerplate ever again.</p>
<p>The awesome capabilities of WebAssembly and the design thinking behind SUFA are being harnessed by the open source <a target="_blank" href="https://suborbital.dev/">Suborbital Development Platform</a> to introduce a new way to build your web applications. I've been working for over a year to realize this goal, and I'm extremely happy with the results thus far. Riding the wave of new technologies and practices such as JAMStack and edge computing means that we have a opportunity to bring the best of the old and the new to the next generation of software makers to do incredible things. I hope you'll come and join me!</p>
<p>Please reach out on Twitter (<a target="_blank" href="https://twitter.com/cohix">@cohix</a> or <a target="_blank" href="https://twitter.com/suborbitaldev">@SuborbitalDev</a> if you'd like to talk about SUFA design or the Suborbital project!</p>
</div></div>]]>
            </description>
            <link>https://blog.suborbital.dev/building-a-better-monolith</link>
            <guid isPermaLink="false">hacker-news-small-sites-25353335</guid>
            <pubDate>Tue, 08 Dec 2020 23:52:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building a scalable e-commerce data model]]>
            </title>
            <description>
<![CDATA[
Score 127 | Comments 25 (<a href="https://news.ycombinator.com/item?id=25353148">thread link</a>) | @robric
<br/>
December 8, 2020 | https://resources.fabric.inc/blog/ecommerce-data-model | <a href="https://web.archive.org/web/*/https://resources.fabric.inc/blog/ecommerce-data-model">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
<div data-widget-type="cell" data-x="0" data-w="12">

<div>
<div>
<div data-widget-type="custom_widget" data-x="0" data-w="12">
<div id="hs_cos_wrapper_module_151388194052436" data-hs-cos-general-type="widget" data-hs-cos-type="module">
    
<div>
<div>
<div>

 
<div>
<p><img width="100" height="100" alt="James Hickey" src="https://resources.fabric.inc/hs-fs/hubfs/james-hickey.jpeg?width=100&amp;height=100&amp;name=james-hickey.jpeg" srcset="https://resources.fabric.inc/hs-fs/hubfs/james-hickey.jpeg?width=50&amp;height=50&amp;name=james-hickey.jpeg 50w, https://resources.fabric.inc/hs-fs/hubfs/james-hickey.jpeg?width=100&amp;height=100&amp;name=james-hickey.jpeg 100w, https://resources.fabric.inc/hs-fs/hubfs/james-hickey.jpeg?width=150&amp;height=150&amp;name=james-hickey.jpeg 150w, https://resources.fabric.inc/hs-fs/hubfs/james-hickey.jpeg?width=200&amp;height=200&amp;name=james-hickey.jpeg 200w, https://resources.fabric.inc/hs-fs/hubfs/james-hickey.jpeg?width=250&amp;height=250&amp;name=james-hickey.jpeg 250w, https://resources.fabric.inc/hs-fs/hubfs/james-hickey.jpeg?width=300&amp;height=300&amp;name=james-hickey.jpeg 300w" sizes="(max-width: 100px) 100vw, 100px"> 
</p>
<div>
<p id="hubspot-author_data" data-hubspot-form-id="author_data" data-hubspot-name="Blog Author">
James Hickey
</p>
<p> December 08</p><p> &nbsp; • &nbsp;</p>
<p>
7 minute read
</p>
</div>
</div>
</div>
<p><img src="https://resources.fabric.inc/hubfs/ecommerce-data-model.png" alt="ecommerce data model">
</p>
<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p>If selling products online is a core part of your business, then you need to build an e-commerce data model that’s scalable, flexible, and fast. Most off-the-shelf providers like Shopify and BigCommerce are built for small stores selling a few million dollars in orders per month, so many e-commerce retailers working at scale start to investigate creating a bespoke solution.</p>
<!--more-->
<p>This article will look at what it takes to start building this infrastructure on your own. What are some of the areas to consider? What might the data model look like? How much work is involved?</p>
<p>Along the way, we’ll explore an alternative: API-based commerce platforms that manage data for you across product catalogs, pricing, and orders—without locking you into a monolith, and without requiring you to replatform.</p>
<p><em><strong>Note:</strong> A full summary diagram of the e-commerce data model is at the end of the article.</em></p>

<h2 id="who-are-your-customers-">Who Are Your Customers?</h2>
<p>First, you need to consider <em>who</em> will be purchasing items from your e-commerce application. How might you model customer information in a database as a result? You’ll probably want to have basic information like your customer's name, email address, etc. Do you want your customers to be able to create a profile in your system? Or just fill out a form each time they want to purchase something?</p>
<p>Just starting out, a basic model might look like this:</p>
<p><img src="https://resources.fabric.inc/hs-fs/hubfs/Basic%20e-commerce%20customer%20data%20model.png?width=308&amp;name=Basic%20e-commerce%20customer%20data%20model.png" alt="Basic e-commerce customer data model" width="308" srcset="https://resources.fabric.inc/hs-fs/hubfs/Basic%20e-commerce%20customer%20data%20model.png?width=154&amp;name=Basic%20e-commerce%20customer%20data%20model.png 154w, https://resources.fabric.inc/hs-fs/hubfs/Basic%20e-commerce%20customer%20data%20model.png?width=308&amp;name=Basic%20e-commerce%20customer%20data%20model.png 308w, https://resources.fabric.inc/hs-fs/hubfs/Basic%20e-commerce%20customer%20data%20model.png?width=462&amp;name=Basic%20e-commerce%20customer%20data%20model.png 462w, https://resources.fabric.inc/hs-fs/hubfs/Basic%20e-commerce%20customer%20data%20model.png?width=616&amp;name=Basic%20e-commerce%20customer%20data%20model.png 616w, https://resources.fabric.inc/hs-fs/hubfs/Basic%20e-commerce%20customer%20data%20model.png?width=770&amp;name=Basic%20e-commerce%20customer%20data%20model.png 770w, https://resources.fabric.inc/hs-fs/hubfs/Basic%20e-commerce%20customer%20data%20model.png?width=924&amp;name=Basic%20e-commerce%20customer%20data%20model.png 924w" sizes="(max-width: 308px) 100vw, 308px"></p>
<p>If you want your customers to have a persistent profile, then you need to build some way for them to log in to your application. Moving forward with more real-world requirements, you might also want to keep track of their login attempt history and password history.</p>
<p><img src="https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20e-commerce%20customer%20data%20model.png?width=891&amp;name=More%20complex%20e-commerce%20customer%20data%20model.png" alt="More complex e-commerce customer data model" width="891" srcset="https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20e-commerce%20customer%20data%20model.png?width=446&amp;name=More%20complex%20e-commerce%20customer%20data%20model.png 446w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20e-commerce%20customer%20data%20model.png?width=891&amp;name=More%20complex%20e-commerce%20customer%20data%20model.png 891w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20e-commerce%20customer%20data%20model.png?width=1337&amp;name=More%20complex%20e-commerce%20customer%20data%20model.png 1337w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20e-commerce%20customer%20data%20model.png?width=1782&amp;name=More%20complex%20e-commerce%20customer%20data%20model.png 1782w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20e-commerce%20customer%20data%20model.png?width=2228&amp;name=More%20complex%20e-commerce%20customer%20data%20model.png 2228w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20e-commerce%20customer%20data%20model.png?width=2673&amp;name=More%20complex%20e-commerce%20customer%20data%20model.png 2673w" sizes="(max-width: 891px) 100vw, 891px"></p>
<p>You might also want to consider whether your customers are part of a large organization; and, if so, how would they like to handle password resets? Do they need single sign-on or OAuth support?</p>
<h4 id="deep-dive-addresses">Deep Dive: Addresses</h4>
<p>Did you notice there’s no address tied to a customer in any of the data models shown so far? It might be your first inclination to include a customer’s address as part of those models. However, most customers will have multiple addresses and multiple <em>kinds</em> of addresses, like billing and shipping. B2B retailers might also have to consider multiple delivery locations based on the number of warehouses and offices they support.</p>
<p>What happens if the billing and shipping address are different? Well, you’ll need to do more than just add extra columns to the <code>Customer</code> table! It’s not that simple.</p>
<p>So how <em>does</em> storing a billing address affect the scalability of your application?</p>
<p>If you were to split the payment and shipping areas into separate (micro)services each having their own database, then putting billing and payment addresses into the <code>Customer</code> area would lead to having “chatty” services. This is a <a href="https://docs.aws.amazon.com/whitepapers/latest/microservices-on-aws/chattiness.html" rel="noopener" target="_blank">well-known <em>design smell</em> when building microservices</a>.</p>
<p>To avoid this issue, you’re better off putting the addresses within the appropriate area/service that requires them, but with that, your data model becomes more complex.</p>
<p>One way to avoid much of this complexity is to consider an <a href="https://resources.fabric.inc/glossary/oms-software" rel="noopener">order management system (OMS)</a> by an API-first software provider. With this software, you can integrate the OMS into your data model without spending months of engineering time.</p>

<h2 id="how-do-you-organize-products-and-catalog-">How Do You Organize Products And Catalog?</h2>
<p>The first thing you see when you enter a store (either in-person or digitally) are products ready for you to purchase, and usually displayed with some thought for how you might be likely to shop.</p>
<p>For an e-commerce web application, you will probably want to highlight things like:</p>
<ul>
<li>Best selling products</li>
<li>Trending products</li>
<li>New products</li>
<li>The ability to browse products by search criteria</li>
</ul>
<p>Providing customers with that information means you first need to keep track of a lot of data about your products: their prices, historical purchase data, and so on.</p>
<p>Let’s see what a “first shot” at creating a data model for a product catalog might look like:</p>
<p><img src="https://resources.fabric.inc/hs-fs/hubfs/Simple%20e-commerce%20product%20data%20model.png?width=562&amp;name=Simple%20e-commerce%20product%20data%20model.png" alt="Simple e-commerce product data model" width="562" srcset="https://resources.fabric.inc/hs-fs/hubfs/Simple%20e-commerce%20product%20data%20model.png?width=281&amp;name=Simple%20e-commerce%20product%20data%20model.png 281w, https://resources.fabric.inc/hs-fs/hubfs/Simple%20e-commerce%20product%20data%20model.png?width=562&amp;name=Simple%20e-commerce%20product%20data%20model.png 562w, https://resources.fabric.inc/hs-fs/hubfs/Simple%20e-commerce%20product%20data%20model.png?width=843&amp;name=Simple%20e-commerce%20product%20data%20model.png 843w, https://resources.fabric.inc/hs-fs/hubfs/Simple%20e-commerce%20product%20data%20model.png?width=1124&amp;name=Simple%20e-commerce%20product%20data%20model.png 1124w, https://resources.fabric.inc/hs-fs/hubfs/Simple%20e-commerce%20product%20data%20model.png?width=1405&amp;name=Simple%20e-commerce%20product%20data%20model.png 1405w, https://resources.fabric.inc/hs-fs/hubfs/Simple%20e-commerce%20product%20data%20model.png?width=1686&amp;name=Simple%20e-commerce%20product%20data%20model.png 1686w" sizes="(max-width: 562px) 100vw, 562px"></p>
<p>Here’s a <code>Product</code> table with some basic information, like a product’s name, SKU, and price. The product is also linked to another table representing various categories that product is associated with. You might also strategically add indexes and full-text search to the <code>Product</code> table to enable site visitors to efficiently search for various products.</p>
<p>This is a decent first attempt. However, to get an even more realistic and useful e-commerce product catalog, you’ll need to support more requirements such as:</p>
<ul>
<li>Tracking pricing history so site administrators can analyze trends in product pricing</li>
<li>Supporting related products to display on a product’s page</li>
<li>Incorporating product vendors so customers can view all products sold by an individual vendor/company</li>
</ul>
<p>To address those extra requirements, you might end up with the following data model:</p>
<p><img src="https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20e-commerce%20product%20data%20model.png?width=816&amp;name=More%20complex%20e-commerce%20product%20data%20model.png" alt="More complex e-commerce product data model" width="816" srcset="https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20e-commerce%20product%20data%20model.png?width=408&amp;name=More%20complex%20e-commerce%20product%20data%20model.png 408w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20e-commerce%20product%20data%20model.png?width=816&amp;name=More%20complex%20e-commerce%20product%20data%20model.png 816w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20e-commerce%20product%20data%20model.png?width=1224&amp;name=More%20complex%20e-commerce%20product%20data%20model.png 1224w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20e-commerce%20product%20data%20model.png?width=1632&amp;name=More%20complex%20e-commerce%20product%20data%20model.png 1632w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20e-commerce%20product%20data%20model.png?width=2040&amp;name=More%20complex%20e-commerce%20product%20data%20model.png 2040w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20e-commerce%20product%20data%20model.png?width=2448&amp;name=More%20complex%20e-commerce%20product%20data%20model.png 2448w" sizes="(max-width: 816px) 100vw, 816px"></p>
<p>This model still isn’t perfect as it embeds your prices into the product itself, but at least it lets you maintain a previous pricing history.</p>
<p>Another option is to integrate your e-commerce store with a <a href="https://resources.fabric.inc/glossary/promotions-engine" rel="noopener">pricing and promotions engine</a> from an API-first software provider that handles pricing for you. This will let you roll out different prices to different users based on their intent, location, cart, or order history.</p>
<h4 id="deep-dive-pricing">Deep Dive: Pricing</h4>
<p>While the more complex product data model still has a product’s price in the same table, this may not be the best thing to do in a real large-scale application.</p>
<p>Consider that your organization has various departments, such as inventory/warehousing, sales, marketing, customer support, etc. You might have dedicated systems that allow merchandisers to change the price of an item since they are the experts in determining how much a product should sell for. Similar to the considerations with a customer’s billing and shipping addresses, this would lead to cross-boundary/service communication if we left the price in the core <code>Product</code> table.</p>
<p>Therefore, you might want to store product prices under the data stores that the sales department owns. But don’t forget, there are many different kinds of “prices” that haven’t been taken into consideration yet, including:</p>
<ul>
<li>Price (cost) when purchasing stock from vendors</li>
<li>Customer sale price</li>
<li>Discounted sale prices</li>
<li>Manufacturer’s suggested retail price</li>
</ul>
<p>Handling all these in context of your organizational structure would require even more exploration and complexity in your data model. While your engineering team could likely accomplish this task, it’s going to take time. Using ready-made solutions can shave weeks or months off your e-commerce data modeling timeline.</p>

<h2 id="how-do-you-streamline-orders-">How Do You Streamline Orders?</h2>
<p>Now that you have customers in your database and products available to purchase, you’ll need to think about how to design the order-taking process and data model.</p>
<p>The process of placing an order might look something like this:</p>
<ol>
<li>A customer places products into their cart while browsing.</li>
<li>The customer decides they want to purchase the products that are in their cart.</li>
<li>They proceed to purchase the order.</li>
<li>The customer gets an emailed receipt or confirmation number.</li>
</ol>
<p>However, it’s rarely so simple. Placing orders can be deceptively tricky as there are many moving parts:</p>
<ul>
<li>Products</li>
<li>An active cart</li>
<li>Cart converted into an order</li>
<li>A finalized order with confirmation</li>
</ul>
<p>If you were to look at a simple data model for an order placement, it might look something like this:</p>
<p><img src="https://resources.fabric.inc/hs-fs/hubfs/Orders%20data%20model.png?width=837&amp;name=Orders%20data%20model.png" alt="Orders data model" width="837" srcset="https://resources.fabric.inc/hs-fs/hubfs/Orders%20data%20model.png?width=419&amp;name=Orders%20data%20model.png 419w, https://resources.fabric.inc/hs-fs/hubfs/Orders%20data%20model.png?width=837&amp;name=Orders%20data%20model.png 837w, https://resources.fabric.inc/hs-fs/hubfs/Orders%20data%20model.png?width=1256&amp;name=Orders%20data%20model.png 1256w, https://resources.fabric.inc/hs-fs/hubfs/Orders%20data%20model.png?width=1674&amp;name=Orders%20data%20model.png 1674w, https://resources.fabric.inc/hs-fs/hubfs/Orders%20data%20model.png?width=2093&amp;name=Orders%20data%20model.png 2093w, https://resources.fabric.inc/hs-fs/hubfs/Orders%20data%20model.png?width=2511&amp;name=Orders%20data%20model.png 2511w" sizes="(max-width: 837px) 100vw, 837px"></p>
<p>Notice that each row in the <code>ShoppingCartItem</code> table contains the “captured” price of the product. When the customer puts an item into their shopping cart should the price at that moment be “locked-in”? If so, for how long?</p>
<p><em><span>Note:</span> How the price functions is a business requirement that would need to be discussed with your product owners, and so on, as mentioned in the "Deep Dive: Pricing" section earlier.</em></p>
<p>The same question applies to an unpaid order. If a customer has ordered a discounted item, should they be able to keep the promise of that discounted price forever until they pay? Or does it expire?</p>
<p>Other questions to consider for an orders data model might include:</p>
<ul>
<li>Are you tracking analytics on orders?</li>
<li>What happens if a customer returns a defective item?</li>
<li>Should you handle shipping within the same data model or have a dedicated shipping context/schema?</li>
</ul>
<p>With some of these concerns in mind, you might end up with a data model that looks more like this:</p>
<p><img src="https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20orders%20data%20model.png?width=1112&amp;name=More%20complex%20orders%20data%20model.png" alt="More complex orders data model" width="1112" srcset="https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20orders%20data%20model.png?width=556&amp;name=More%20complex%20orders%20data%20model.png 556w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20orders%20data%20model.png?width=1112&amp;name=More%20complex%20orders%20data%20model.png 1112w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20orders%20data%20model.png?width=1668&amp;name=More%20complex%20orders%20data%20model.png 1668w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20orders%20data%20model.png?width=2224&amp;name=More%20complex%20orders%20data%20model.png 2224w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20orders%20data%20model.png?width=2780&amp;name=More%20complex%20orders%20data%20model.png 2780w, https://resources.fabric.inc/hs-fs/hubfs/More%20complex%20orders%20data%20model.png?width=3336&amp;name=More%20complex%20orders%20data%20model.png 3336w" sizes="(max-width: 1112px) 100vw, 1112px"></p>
<p>Some things to take note of in this more complex orders model:</p>
<ul>
<li><code>ShoppingCartItem</code> now supports an expiration date for a locked-in price.</li>
<li><code>ShoppingCartHistory</code> tracks when items are added, removed, etc.</li>
<li>An order item may be returned (this still does not handle cases where 1 out of X items of the same product are returned).</li>
<li>An order may have multiple shipments (eg, how Amazon will sometimes split an order up into multiple packages/shipments).</li>
</ul>
<p>This article also hasn’t even touched the surface of using alternative data storage methods like JSON documents or event sourcing!</p>

<h2 id="conclusion">Conclusion</h2>
<p>To help you see how all the pieces fit together, here are all the diagrams shown together. I’ve removed a number of links/lines to the <code>Customer</code> table to increase readability:</p>
<p><img src="https://resources.fabric.inc/hs-fs/hubfs/Summary%20e-commerce%20data%20model.png?width=1396&amp;name=Summary%20e-commerce%20data%20model.png" alt="Summary e-commerce data model" width="1396" srcset="https://resources.fabric.inc/hs-fs/hubfs/Summary%20e-commerce%20data%20model.png?width=698&amp;name=Summary%20e-commerce%20data%20model.png 698w, https://resources.fabric.inc/hs-fs/hubfs/Summary%20e-commerce%20data%20model.png?width=1396&amp;name=Summary%20e-commerce%20data%20model.png 1396w, https://resources.fabric.inc/hs-fs/hubfs/Summary%20e-commerce%20data%20model.png?width=2094&amp;name=Summary%20e-commerce%20data%20model.png 2094w, https://resources.fabric.inc/hs-fs/hubfs/Summary%20e-commerce%20data%20model.png?width=2792&amp;name=Summary%20e-commerce%20data%20model.png 2792w, https://resources.fabric.inc/hs-fs/hubfs/Summary%20e-commerce%20data%20model.png?width=3490&amp;name=Summary%20e-commerce%20data%20model.png 3490w, https://resources.fabric.inc/hs-fs/hubfs/Summary%20e-commerce%20data%20model.png?width=4188&amp;name=Summary%20e-commerce%20data%20model.png 4188w" sizes="(max-width: 1396px) 100vw, 1396px"></p>
<p>As I mentioned above, this article still doesn’t even cover many of the basics like payment processing and invoicing. Beyond the features covered here, you might eventually require more advanced features like:</p>
<ul>
<li>Coupon codes</li>
<li>Taxes</li>
<li>Third-party integrations with OAuth providers, other retailers, or partners</li>
<li>Shipment tracking notifications</li>
</ul>
<p>Building a data model for an e-commerce application, as you can see, is not so simple. What looks up front to be a straightforward set of database tables is not so simple once you dig into real-world requirements.</p>
<h4 id="there-s-another-way">There’s Another Way</h4>
<p>What if you could have more of these abilities out-of-the-box?</p>
<p><a href="https://fabric.inc/solutions">Fabric</a> is an all-in-one commerce platform that helps you do everything this article talked about, like manage customers, orders, and shipments. Most importantly, it is a microservices-based and API-first platform. This means you can choose the services you need and integrate them …</p></span></p></div></div></div></div></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://resources.fabric.inc/blog/ecommerce-data-model">https://resources.fabric.inc/blog/ecommerce-data-model</a></em></p>]]>
            </description>
            <link>https://resources.fabric.inc/blog/ecommerce-data-model</link>
            <guid isPermaLink="false">hacker-news-small-sites-25353148</guid>
            <pubDate>Tue, 08 Dec 2020 23:38:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Working with Unikernel Volumes in Nanos]]>
            </title>
            <description>
<![CDATA[
Score 42 | Comments 10 (<a href="https://news.ycombinator.com/item?id=25352718">thread link</a>) | @eyberg
<br/>
December 8, 2020 | https://nanovms.com/dev/tutorials/working-with-unikernel-volumes-in-nanos | <a href="https://web.archive.org/web/*/https://nanovms.com/dev/tutorials/working-with-unikernel-volumes-in-nanos">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

    <p>Today's article is about working with external volumes for your
unikernel instance. Many applications won't use this feature but there
are also situations where this makes sense.</p>

<p>The most obvious time when you would want to attach a volume to your
unikernel instance is when you are working with a database. Your
database image probably doesn't change that much but many databases can
grow very large and it makes no sense to keep the same data volume in
your base image.</p>

<p>The second use-case is when you have a base image, say a webserver,
and you want to package additional files or configuation as a build
step. For instance some companies will rotate certificates every few
hours in the day to protect access to various services and this rotation
is usually done out-of-band of deploys. Now unikernel deploys for small
webservers are typically fairly fast but the ability to put your
configuration on a separate partition and re-mount the volume on the fly
every few hours is definitely enticing.</p>

<p>Ok, let's start with the code. For this example we have a simple
little go webserver that implements a root filesystem filewalker. We've
declared that there is a separate partition called 'mnt' in the code but
it is non-existent right now.</p>

<pre><code>import (
  "fmt"
  "io/ioutil"
  "net/http"
  "os"
  "path/filepath"
)

func printDir() {
  err := filepath.Walk("/",
    func(path string, info os.FileInfo, err error) error {
      if err != nil {
        return err
      }
      fmt.Println(path, info.Size())
      return nil
    })
  if err != nil {
    fmt.Println(err)
  }
}

func main() {
  printDir()

  b, err := ioutil.ReadFile("/mnt/bob.txt")
  if err != nil {
    fmt.Println(err)
  }

  fmt.Println(string(b))

  http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {
    printDir()

    b, err := ioutil.ReadFile("/mnt/bob.txt")
    if err != nil {
      fmt.Println(err)
    }

    fmt.Println(string(b))
    fmt.Fprintf(w, "Welcome to my website!")
  })

  fs := http.FileServer(http.Dir("static/"))
  http.Handle("/static/", http.StripPrefix("/static/", fs))

  go func() {
    err = http.ListenAndServe(":80", nil)
    if err != nil {
      fmt.Println(err)
    }
  }()

  http.ListenAndServe(":8080", nil)
}
</code></pre><p>

If you run it locally you should see something like this:
</p><pre><code>➜  g2 ops run -p 8080 g2
booting /Users/eyberg/.ops/images/g2.img ...
assigned: 10.0.2.15
/ 0
/dev 0
/dev/null 0
/dev/urandom 0
/etc 0
/etc/passwd 33
/etc/resolv.conf 18
/etc/ssl 0
/etc/ssl/certs 0
/etc/ssl/certs/ca-certificates.crt 207436
/g2 7533614
/lib 0
/lib/x86_64-linux-gnu 0
/lib/x86_64-linux-gnu/libnss_dns.so.2 26936
/proc 0
/proc/self 0
/proc/self/exe 0
/proc/self/maps 0
/proc/sys 0
/proc/sys/kernel 0
/proc/sys/kernel/hostname 7
/sys 0
/sys/devices 0
/sys/devices/system 0
/sys/devices/system/cpu 0
/sys/devices/system/cpu/cpu0 0
/sys/devices/system/cpu/online 0
open /mnt/bob.txt: no such file or directory
</code></pre>

<h3>Creating a Volume</h3>

<p>Let's create a simple volume with one file in it - bob.txt.</p>

<pre><code>mkdir mnt
echo "Hi - I'm a text file" &gt; mnt/bob.txt
➜  g2 ops volume create mnt -d mnt
2020/12/08 11:12:35 volume: mnt created with UUID 04c56e4a-5b8b-512c-eaa3-b82b4cd46d9e and label mnt
</code></pre><p>

You'll see that we can now see it in our local volume store:

</p><pre><code>➜  g2 ops volume list
+--------------------------------------+------+--------+-----------+-------------------------------------------------------------------------+---------+----------+
|                 UUID                 | NAME | STATUS | SIZE (GB) | LOCATION                                 | CREATED | ATTACHED |
+--------------------------------------+------+--------+-----------+-------------------------------------------------------------------------+---------+----------+
| 04c56e4a-5b8b-512c-eaa3-b82b4cd46d9e | mnt  |        | 1.6 MB    | /Users/eyberg/.ops/volumes/mnt:04c56e4a-5b8b-512c-eaa3-b82b4cd46d9e.raw |         |          |
+--------------------------------------+------+--------+-----------+-------------------------------------------------------------------------+---------+----------+
</code></pre>

<h3>Attaching a Volume</h3>

<p>You can attach a volume to an instance that is expecting one. So that
means when we create the image we'll want to pass any mount points with
the volume label and mount path - this is loosely similar to how
something in /etc/fstab would work. Ran locally you can test with 'ops
run' but you can pass the same '--mounts' flag when issuing 'ops image
create' for images ran on Google or AWS. Let's try it out locally:
</p>

<pre><code>➜  g2 ops run -p 8080 g2 --mounts mnt:/mnt
booting /Users/eyberg/.ops/images/g2.img ...
assigned: 10.0.2.15
/ 0
/dev 0
/dev/null 0
/dev/urandom 0
/etc 0
/etc/passwd 33
/etc/resolv.conf 18
/etc/ssl 0
/etc/ssl/certs 0
/etc/ssl/certs/ca-certificates.crt 207436
/g2 7533614
/lib 0
/lib/x86_64-linux-gnu 0
/lib/x86_64-linux-gnu/libnss_dns.so.2 26936
/mnt 0
/mnt/bob.txt 21
/proc 0
/proc/self 0
/proc/self/exe 0
/proc/self/maps 0
/proc/sys 0
/proc/sys/kernel 0
/proc/sys/kernel/hostname 7
/sys 0
/sys/devices 0
/sys/devices/system 0
/sys/devices/system/cpu 0
/sys/devices/system/cpu/cpu0 0
/sys/devices/system/cpu/online 0
Hi - I'm a text file
</code></pre>

<p>Cool! It works! Now let's edit the file.</p>

<pre><code>echo "New text has come to light." &gt; mnt/bob.txt
➜  g2 ops volume create mnt2 -d mnt
2020/12/08 11:19:41 volume: mnt2 created with UUID f82da0e3-3980-ddd8-5720-e1b320e21371 and label mnt2
</code></pre>

<p>Keep in mind we are creating a *new* volume with new contents and then re-attaching the volume to the instance.</p>

<pre><code>➜  g2 ops run -p 8080 g2 --mounts mnt2:/mnt
booting /Users/eyberg/.ops/images/g2.img ...
assigned: 10.0.2.15
/ 0
/dev 0
/dev/null 0
/dev/urandom 0
/etc 0
/etc/passwd 33
/etc/resolv.conf 18
/etc/ssl 0
/etc/ssl/certs 0
/etc/ssl/certs/ca-certificates.crt 207436
/g2 7533614
/lib 0
/lib/x86_64-linux-gnu 0
/lib/x86_64-linux-gnu/libnss_dns.so.2 26936
/mnt 0
/mnt/bob.txt 28
/proc 0
/proc/self 0
/proc/self/exe 0
/proc/self/maps 0
/proc/sys 0
/proc/sys/kernel 0
/proc/sys/kernel/hostname 7
/sys 0
/sys/devices 0
/sys/devices/system 0
/sys/devices/system/cpu 0
/sys/devices/system/cpu/cpu0 0
/sys/devices/system/cpu/online 0
New text has come to light.
</code></pre>

<p>If you are attaching the volume to an instance on Google or AWS you'd
use the attach command:</p>

<pre><code>ops volume attach g2 mnt mnt2 -t gcp -c config.json</code></pre>

<p>Similarly, you can detach as well:</p>

<pre><code>ops volume detach g2 mnt -t gcp -c config.json</code></pre>

<p>What's really great about unikernel volumes when working on AWS or
Google is that this is all managed for you by the cloud provider of choice. There is no duplicate storage layer
you have to manage like you do in container land. Now you know the basics of mounting external volumes into your unikernel images.</p>
                    </div></div>]]>
            </description>
            <link>https://nanovms.com/dev/tutorials/working-with-unikernel-volumes-in-nanos</link>
            <guid isPermaLink="false">hacker-news-small-sites-25352718</guid>
            <pubDate>Tue, 08 Dec 2020 22:59:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Measuring "efficiency" in document prepration: Microsoft Word vs. LaTex]]>
            </title>
            <description>
<![CDATA[
Score 87 | Comments 155 (<a href="https://news.ycombinator.com/item?id=25350851">thread link</a>) | @1vuio0pswjnm7
<br/>
December 8, 2020 | https://blog.cr.yp.to/20201206-msword.html | <a href="https://web.archive.org/web/*/https://blog.cr.yp.to/20201206-msword.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

<hr>
<hr>

<p>
The boss needed item 3 inserted into a numbered list of hundreds of items.
The intern used a mouse to select the original 3 on the screen,
then typed 4,
then selected the original 4,
then typed 5,
then scrolled down,
then selected the original 5,
then typed 6,
and so on.
Another intern sat watching the screen to make sure there were no mistakes.
</p>
<p>
I happened to be in the room for other reasons.
I remember the horror of watching the beginning of this barbaric editing process.
Those poor interns!
</p>
<p>
When I enter a list of items into the computer,
what I'm typing doesn't look like
</p>
<pre>     1. ...
     2. ...
     3. ...
</pre>
<p>
but more like
</p>
<pre>     * ...
     * ...
     * ...
</pre>
<p>
Each asterisk is a special command to the computer,
telling the computer to automatically display the next number for the reader.
The reader eventually sees
</p>
<pre>     1. ...
     2. ...
     3. ...
</pre>
<p>
but that isn't what I typed.
This small difference produces a tremendous savings of time
whenever I insert an item, or delete an item, or move an item.
</p>
<p>
If I decide later
to skip the numbers
and use bullets instead,
I tell the computer
to introduce each list item with a bullet.
This is one command covering the whole list.
There's also a command that does the same thing
for the whole document.
There isn't separate work for each item.
It's no problem if a coauthor
later wants to change bullets back to numbers.
</p>
<p>
The interns, I suppose,
would be manually changing "1." and "2." and "3."
and so on to "•" and "•" and "•" and so on.
Or maybe they would be trying to figure out
how some search-and-replace feature could do the same thing;
let's hope the document doesn't have a sentence somewhere
that talks about something that happened in the year 2001.
Or maybe the interns would be quitting and finding a better job.
</p>
<p>
[Note added 2020.12.07:
I was expecting that many of my readers
would already be accustomed to relying on the computer
for automatic numbering.
I was surprised, however,
to see some comments along the lines of "Inconceivable!"
from readers unable to imagine
how the interns could have been in a different situation,
going through such a shockingly inefficient revision process.
Here's a hint:
Each item in the list looked like a flush-left paragraph,
like the paragraphs in this blog post,
adjacent to the left margin.
The text being selected by the mouse,
for example to change "3" to "4",
was to the right of the margin,
like the rest of the text in each item.]
</p>
<p>
<b>Abstraction as a time-saver for authors.</b>
This use of asterisks
is just one example of how I'm often typing something more abstract
than what's seen by the ultimate reader.
I don't type "Figure 12" or "see [41]", for example;
I type things like "Figure \ref{network-measurements}" and "see \cite{multiplication-survey}",
and I let the computer automatically convert
"\ref{network-measurements}" and "\cite{multiplication-survey}"
into numbers to display for the reader.
</p>
<p>
With one extra command,
covering the entire document,
I can tell the computer
to include section numbers
as part of all figure numbers in the document,
so that the figures are easier for the reader to find:
e.g., Figures 3.1 and 3.2 and 3.3 are in Section 3.
With another command,
again covering the entire document,
I can tell the computer
to cite all authors by name rather than by number.
</p>
<p>
As another example,
I was recently editing a mathematical paper,
and I decided that a particular concept
would be easier for the reader to remember
if I changed the notation that I was using for the concept.
The notation was all over the paper,
but this change took just a few seconds of editing.
I had given a name to the concept,
had told the computer <i>once</i> to display this name as a particular notation,
and had then typed this name throughout the paper,
so there was only one place where I had to change the notation.
</p>
<p>
Of course one can't,
and shouldn't try to,
prepare in advance for <i>every</i> possible change to a document.
But it's not hard to prepare for the most likely changes.
This small initial effort
saves a tremendous amount of time later.
When I say "small",
I'm including the effort to select a document-creation system
that's designed to
<a href="https://en.wikipedia.org/wiki/LaTeX">make this sort of thing easy</a>.
</p>
<p>
(As a side note,
programmers will recognize this strategy
as an example of the
<a href="https://en.wikipedia.org/wiki/Information%5Fhiding">information hiding</a>
strategy introduced by Parnas,
and will recognize that modern program-creation systems
are designed to make this easy.)
</p>
<p>
Microsoft Word isn't completely missing abstractions,
but these abstractions
are competing for user-interface resources
against features encouraging the user
to work at lower abstraction layers.
The extra effort to use the abstractions
ends up pushing users into doing something simpler,
something that just works now,
and paying heavily for this choice
later when the document is being revised.
</p>
<p>
Have I done a scientific study
<i>proving</i> that Microsoft Word
is less efficient than LaTeX?
No.
I'd love to see a careful study of this topic.
Short-term,
this would help guide new authors to make sensible choices.
Longer-term,
insights from this sort of study could be the basis for further improving
our document-creation systems.
I certainly don't think that the existing systems are perfect.
(<a href="https://cr.yp.to/writing/visual.html">Example.</a>)
</p>
<p>
Imagine, however, that a study looks only at
<i>the time for someone looking at a printout to create a document matching this printout</i>.
This would be blind to the time for subsequent edits.
This would be blind to the suffering of those interns.
This would incorrectly conclude that typing
"1. ... 2. ... 3. ..." and "see [41]"
is more efficient than typing
"* ... * ... * ..." and "see \cite{multiplication-survey}".
It <i>is</i> slightly more efficient in this limited metric,
but it is much less efficient in the metric that matters,
namely the total time spent by the user.
</p>
<p>
<b>An example of a "scientific" study.</b>
At this point you're probably thinking that
nobody could possibly miss such an obvious issue.
This brings me to the main topic of this blog post,
a 2014 peer-reviewed study
<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0115069">"An Efficiency Comparison of Document Preparation Systems Used in Academic Research and Development"</a>
by two psychologists,
Knauff and Nejasmic ("KN").
</p>
<p>
Participants in the study were given a page of text
and were given a limited time to type the page into the computer.
There were three different types of text:
</p>
<ul>
<li>simple prose with a few footnotes;
</li>
<li>a page with a complicated table of data;
and
</li>
<li>a page with many mathematical formulas.
</li>
</ul>
<p>
Participants were scored
on the basis of how much text they typed and how accurately they typed it.
The time was so rushed that a significant fraction of participants
didn't finish typing the whole page,
even for the case of simple prose.
</p>
<p>
The study considered two document-creation systems:
LaTeX and Microsoft Word,
in each case with "all tools, editors, plug-ins, and add-ons"
that participants were "accustomed to using".
Of course different "add-ons" could have different efficiency,
and of course there are other document-creation systems,
but these are topics for another blog post.
</p>
<p>
The study produced many pages of results,
which I'll summarize by saying that
Word did slightly better on the prose
and much better on the table,
while LaTeX did better on the formulas.
The study authors made no effort to measure any subsequent document-editing step.
</p>
<p>
<b>Slithering from one metric to another.</b>
The fundamental mistake in the KN paper
is the change of cost metric.
</p>
<p>
The original question was how efficiently authors are creating documents:
in particular, how efficiently authors are creating academic research papers.
KN claimed in their title to be comparing "efficiency"
of "document preparation systems used in academic research".
But they then quietly changed this metric in three ways:
</p>
<ul>
<li>
They considered only the efficiency of an initial fragment of the document-creation process,
ignoring the time spent revising documents.
They provided no reason to believe that the efficiency of this fragment
was well correlated with what they had previously claimed to be measuring.
Nothing in their paper acknowledges
the most obvious reason for a negative correlation,
namely that slightly more work at the outset
makes revisions much easier later.
In my experience,
document-creation systems vary in how well they support this work.
</li>
<li>
KN didn't even measure the time taken for this initial fragment of the process.
Instead they imposed a rushed time limit,
and measured how incomplete and inaccurate the resulting document was.
Again they provided no reason to believe that what they measured
was well correlated with what they had previously claimed to be measuring.
Perhaps they were assuming that more mistakes will take more time to fix,
but my experience is that some types of mistakes are much easier to fix than others,
and that document-creation systems vary in the types of mistakes they encourage.
</li>
<li>
KN didn't even measure creating a <i>new</i> document,
which is what academics are actually spending their time doing.
People who were writing papers in the age of typewriters
will remember writing and editing papers <i>by hand</i>
before tediously typing the final pages,
but that was because editing a typed page
ranged from annoying
(<a href="https://en.wikipedia.org/wiki/Correction%5Ffluid">white-out</a>,
or sometimes scissors and tape)
to super-annoying
(retyping the whole page).
Today the initial writing on paper is skipped,
and typing is interleaved in small chunks
with parts of the author's thought process,
making the typing process much less boring.
I'm continually re-reading
and thinking about what I just typed.
Is the error rate of the academic's modern typing process
well correlated with the error rate of the
archaic retyping process that KN measured?
Again KN provide no reason to believe this.
</li>
</ul>
<p>
Did KN use the honest title
"A comparison of the unreliability of
rushed retyping of a page
using document preparation systems
that are also used for academic research and development"?
No.
Would you expect a journal to accept a paper
with such a title?
</p>
<p>
Instead they used a title …</p></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.cr.yp.to/20201206-msword.html">https://blog.cr.yp.to/20201206-msword.html</a></em></p>]]>
            </description>
            <link>https://blog.cr.yp.to/20201206-msword.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25350851</guid>
            <pubDate>Tue, 08 Dec 2020 20:25:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Meso-computing and meso-data: the forgotten middle]]>
            </title>
            <description>
<![CDATA[
Score 26 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25350717">thread link</a>) | @milliams
<br/>
December 8, 2020 | https://milliams.com/posts/2020/mesocomputing/ | <a href="https://web.archive.org/web/*/https://milliams.com/posts/2020/mesocomputing/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>I've been working as a <a href="https://www.software.ac.uk/blog/2016-08-17-not-so-brief-history-research-software-engineers-0">Research Software Engineer</a> (RSE) at universities in the UK for many years now, across a variety of fields of research (particle physics, synthetic biology, cardiology, etc.) as both a user of computing services and as a provider of them.
One thing that I've seen cropping up repeatedly is a tendency to want to chase the latest fad.
Now, in academia the latest fad is often up to ten years behind the state-of-the-art so, for example, it's only in the last few years that we've seen "deep learning" cropping up in every grant application.
One of the things I do in my job is try to <a href="https://milliams.com/courses">educate researchers</a> about these technologies so that they know what they're actually asking for and to ensure that if researchers are talking about doing "big data" since they have "megabytes of data" they won't be embarased by a grant reviewer who knows their stuff.</p>
<p>The push in this direction is completely understandable. The grant application business is extremely competitive and anything you can put in your proposal to catch the eye of a reviewer is going to count in your favour.</p>
<p>This is all particularly evident when talking about data.
Research money goes to the problems which are most worthy and there's an assumption that the harder the problem, the more worthy it is.
Furthermore, there's an assumption that the more data one has, the bigger and more difficult the research problem.
This naturally pulls together into an idea that if your problem is a "big data" problem then it's more likely to get funded.
You therefore see grant applications trying to convince reviewers that they're doing "big data" when really they're dealing with linear or spatial datasets of maybe only gigabytes to a terabyte.
That's certainly <em>a lot of</em> data but it often lacks the complexity that requires what one would really consider <em>big</em> data solutions.</p>
<p>Now, I'm not trying to downplay these areas of research.
Instead I'm trying to argue that the problems being solved here are just as worthy of investigation, even without couching it in terms of "big data".
<em>Most</em> research problems I see discussed at Universites are not big data and they are all interesting problems.
I think it would be healthy to reduce the allure of big data and let people know that it's ok to not fall into that category.
Part of the problem is that there's no good name for this scale of data problem: that which is a little too big to realistically do on a single laptop or desktop but well below the size or complexity to require a big data machine or a Hadoop cluster.</p>
<p>I've toyed with many names when trying to teach how to tackle problems of this size: <em>Large Data</em>, <em>Biggish Data</em>, <em>Medium Data</em>.
Until now, none have ever grabbed me so I've decided to coin a new term, <strong>meso-data</strong>.
<em>Meso</em> here means "middle" or "intermediate" cf. <a href="https://en.wikipedia.org/wiki/Mesopotamia#Etymology">Mesopotamia</a> ("between rivers").</p>
<div id="meso-computing">
<h2>Meso-computing</h2>
<p>Along with the problem in the data domain (mostly coming from buzzword-chasing), there's a similar issue in the computing-power domain.
Most research follows a common path of starting with a small investigation on a researcher's laptop until they have too many simulations to run or they are taking longer than the working day and so can't be finished in time.
At this point most research institutes will encourage the use of whatever central computing resources they have, usually a single large HPC cluster.</p>
<p>Research institutes such as Universities face a pressure of trying to justify their expenditure on computing resources by extolling all the big problems they're solving: how many nano-seconds of molecular dynamics they're pushing thorugh or how fine-grained the meteorological grid they can simulate.
This encourages the creation of systems which cater towards those few groups in the university who are able to make really good use of a supercomputer — those who can run large multi-node MPI jobs with optimised code for the specific hardware and who have experts in their team who understand high-performance computing.</p>
<p>The problem with this is that it further increases the divide in power and complexity between running on one's laptop and using a central facility.
Similar to with meso-data, there's a large number of researchers — I would argue <em>most</em> researchers — whose needs sit right in the middle.
They're not doing <em>super</em>computing, they're doing <em>meso</em>-computing.</p>
<p>These researchers are best supported with small domain-specific batch clusters, with cloud computing (perhaps using <a href="https://cluster-in-the-cloud.readthedocs.io/">Cluster in the Cloud</a>), with software-as-a-service or with just some hands-on help from an RSE to get their code running more efficiently on their laptop.</p>
<p>Maybe Pandas is sufficient or perhaps they need to use Dask.
Maybe <a href="https://milliams.com/courses/parallel_python">a course</a> on <tt>concurrent.futures</tt> to magically make their code finish in a quarter of the time is the right solution.
Regardless, the solution is probably not to rewrite it in Fortran, using MPI to scale across 64 nodes or to rent a Hadoop cluster.</p>
</div>
<div id="the-middle">
<h2>The middle</h2>
<p>There will be those reading this who think I'm stating the obvious, who think "I've been working in this area for years, what's new?".
That's kind of the point, a lot of researchers sit here but the fact is that they are under-served.
Most aren't computer experts and will use whichever tools are available, advertised to them and are easy to use.
This inevitably means that they email to one another Excel spreadsheets with maybe some R or Python scripts with hard-coded paths.
These researchers are getting stuck as "<a href="https://daedtech.com/how-developers-stop-learning-rise-of-the-expert-beginner/">Expert Beginners</a>".
People who know their entry-level tools so well the short-term barrier to learning how to use them properly or using a better tool for the job is higher than seems worthwhile.</p>
<p>They want to scale their research but when they look around themselves to see what the University can provide, they're told about getting access to the supercomputer or how if they put their data into an Elasticsearch database things would be better.
That jump is far too big and we need to solve the social problem of allowing them to take only as many steps across the meso-data divide as they need to.
We need well-explained and easy-to-use tools at every stage of the scaling process, not just at the top end.</p>
<p>These terms, <em>meso-computing</em> and <em>meso-data</em>, are deliberately humble.
They are explicitly not about trying to be the biggest but rather about thoughtfully considering the problem at hand and choosing the right hammer.
Unlike with big data, people shouldn't have to ask the question "is this a meso-data problem" because if they're asking the question, the answer is "yes".
I want people to be comfortable saying in grant applications "since this is a meso-data challenge, we request funding for the skills and resources needed to tackle it" and ask for a full-time RSE without having to pretend that they're doing big data or need a dedicated supercomputer.
Labels are helpful and I think that these labels apply well to a good chunk of the research community.</p>
<p>Meso-data comes with its own set of tools and solutions which are partially distinct to big data's.
I haven't invented a whole new area of endeavour, many people have been working on solutions here for decades, but it's certainly not an area that attracts excitement or research money as it should.</p>
<p>These are still hard problems, and in my experience they are solving real-world challenges or furthering our understanding of the universe.
Meso-computing and meso-data projects still need expertise from RSEs or data scientists to make sure that the research is still reliable, reproducible, tested and understandable.</p>
</div>

  </div></div>]]>
            </description>
            <link>https://milliams.com/posts/2020/mesocomputing/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25350717</guid>
            <pubDate>Tue, 08 Dec 2020 20:13:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Serverless Express project graduates from AWS to Vendia]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25349602">thread link</a>) | @timawagner
<br/>
December 8, 2020 | https://vendia.net/blog/serverless-express-finds-home-at-vendia | <a href="https://web.archive.org/web/*/https://vendia.net/blog/serverless-express-finds-home-at-vendia">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><p><span><blockquote><p><em>Tl;dr</em>: The <a href="https://github.com/vendia/serverless-express">aws-serverless-express</a> open source project has graduated from <a href="http://github.com/awslabs/">AWS Labs</a> and is now being actively supported and sponsored by Vendia!</p></blockquote><p><img width="400" src="https://user-images.githubusercontent.com/532272/101377847-35c7b700-3867-11eb-9198-4fcaa66ddfdd.jpg"></p><h2 id="about-serverless-express"><span data-href="#about-serverless-express">
      <svg>
        <use href="#link-out"></use>
      </svg>
    </span><a href="#about-serverless-express">About Serverless Express</a></h2><p>Back in 2016 I was General Manager of two up-and-coming AWS Services: <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> and <a href="https://aws.amazon.com/api-gateway/">Amazon API Gateway</a>.</p><p>APIs were already well understood by our customer base, but selling Lambda was initially a little more difficult - not only was the idea of a fully managed, pay-by-the-call function hosting service a novelty among public cloud providers, it was also very different from existing developer practices. Case in point: <em>Express-based web services</em>.</p><p>Lambda launched with support for <a href="https://nodejs.org/en/">Node.js</a>, which helped a lot of customers start using it easily, given their existing familiarity with <a href="https://www.javascript.com/">JavaScript</a>. But for many customers, just having a familiar language wasn’t enough — they were also looking for familiar frameworks, such as <a href="https://expressjs.com/">Express</a>, on which to build their Lambda-based web applications.</p><p>After hearing this feedback from many customers, we realized we needed to “meet customers where they were” by bringing Serverless solutions to existing frameworks….and thus the <a href="https://github.com/vendia/serverless-express">Serverless Express GitHub project</a> was born!</p><p>Over the course of the last four years many developers have adopted Serverless solutions, including Serverless Express, to help accelerate development and gain Serverless benefits like lower operational overhead, automatic per-request scaling, built-in fault tolerance, and pay-per-call pricing. Today, the <a href="https://www.npmjs.com/package/aws-serverless-express">aws-serverless-express</a> NPM package gets over 1.3M downloads per month, and its GitHub repository has over 3.7K stars.</p><p>The original author of the Serverless Express project, <a href="https://twitter.com/AWSbrett">Brett Andrews</a>, joined <a href="http://vendia.net/">Vendia</a> recently, and we realized we can do even more in our mission to help customers share code and data effectively by giving back to the open source community by helping the Serverless Express project “graduate” from its initial location in AWS Labs to a permanent place in Vendia’s repository. As the original project sponsor, and with the guidance of the project’s original author, AWS trusted us to being thoughtful and proactive stewards of this project. We will continue meeting with AWS monthly to gather customer feedback, and be briefed on upcoming AWS Serverless releases so that we may be a launch partner and provide day 1 support in Serverless Express.</p><h2 id="migration-instructions"><span data-href="#migration-instructions">
      <svg>
        <use href="#link-out"></use>
      </svg>
    </span><a href="#migration-instructions">Migration Instructions</a></h2><p>If you’re an existing Serverless Express user, you don’t need to take any immediate action. For convenience we’ve published <a href="https://www.npmjs.com/package/aws-serverless-express">aws-serverless-express v3.4.0</a> that takes a direct dependency on the new official package <a href="https://www.npmjs.com/package/@vendia/serverless-express">@vendia/serverless-express@^3.4.0</a>.</p><p>This means that by simply upgrading to <a href="mailto:aws-serverless-express@3.4.0">aws-serverless-express@3.4.0</a> you’ll get all downstream patches and features without needing to update your code. Alternatively, you can run the following command  to take a direct dependency on the new official package</p><pre><code><span>npm</span> uninstall aws-serverless-express <span>&amp;&amp;</span> <span>npm</span> <span>install</span> @vendia/serverless-express
</code></pre><p>After updating the deps, update your code accordingly - e.g., change your imports.</p><pre><code><span>- require('aws-serverless-express')
</span><span>+ require('@vendia/serverless-express')
</span></code></pre><p>If you're new to Serverless Express and wondering how to get started, you can deploy a Serverless REST API to AWS in under 5 minutes by following the <a href="https://github.com/vendia/serverless-express/tree/master/examples/basic-starter">Serverless Express Basic Starter on GitHub</a>.</p><h2 id="whats-next"><span data-href="#whats-next">
      <svg>
        <use href="#link-out"></use>
      </svg>
    </span><a href="#whats-next">Whats Next</a></h2><p>In January 2021 we'll be releasing Serverless Express v4 that:</p><ol><li>Supports event sources other than API Gateway such as ALB, Lambda Edge, HTTP API, and makes it easy to provide your own custom event source mapping for other services that are integrated with Lambda</li><li>Has improved logging and debugging support</li><li>Is more extensible and simpler to use with an improved developer experience</li><li>Uses Promise resolution by default</li><li>Is upgraded to Node.js 10 (compatible with Node.js 12+)</li><li>Makes it easier to work with Custom Domain Names</li><li>Has improved documentation and examples</li><li>Supports Multiple header values</li></ol><p><em>What’s in it for Vendia?</em> First of all, Brett and I feel a strong connection to helping the Serverless community succeed. Technologies like Lambda, managed API and database services, and other Serverless offerings power Vendia’s implementations and help us deliver scalable, low-cost code and data sharing solutions for our customers. We also continue to believe strongly in the mission of Serverless Express: To give developers a simple, scalable platform that meets them where they are, and then helps them gain the best of what the cloud has to offer...without unnecessary retraining, rewriting, or porting exercises. We're also excited to find ways to bring Vendia Share - the next generation Serverless platform - and Serverless Express users closer together with easy ways to use both platforms in combination.</p><p>To all the existing (and future) Serverless Express users, we look forward to making your experience even better. To all the project contributors, we look forward to being great partners, building on your existing contributions and successes in this project. And to AWS, a huge “Thank You” for believing in this idea back when it felt risky and uncertain,and for allowing it to leave the nest now that it’s grown. Stay tuned for more technical and project roadmap details coming soon...and <em>Go Serverless!</em></p><p>Tim Wagner, Vendia CEO (and former AWS Lambda General Manager) &amp;
Brett Andrews, Vendia Senior Developer (and former Amazon API Gateway Developer)</p></span></p></div></div></div></div>]]>
            </description>
            <link>https://vendia.net/blog/serverless-express-finds-home-at-vendia</link>
            <guid isPermaLink="false">hacker-news-small-sites-25349602</guid>
            <pubDate>Tue, 08 Dec 2020 18:43:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Notion API]]>
            </title>
            <description>
<![CDATA[
Score 62 | Comments 15 (<a href="https://news.ycombinator.com/item?id=25349335">thread link</a>) | @theBashShell
<br/>
December 8, 2020 | https://www.notion.so/api-beta | <a href="https://web.archive.org/web/*/https://www.notion.so/api-beta">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.notion.so/api-beta</link>
            <guid isPermaLink="false">hacker-news-small-sites-25349335</guid>
            <pubDate>Tue, 08 Dec 2020 18:24:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Feature flags and staging environments can be best friends]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25349269">thread link</a>) | @tommy_mcclung
<br/>
December 8, 2020 | https://releaseapp.io/blog/feature-flags-and-ephemeral-environments | <a href="https://web.archive.org/web/*/https://releaseapp.io/blog/feature-flags-and-ephemeral-environments">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><div><h2 id="feature-flags-and-ephemeral-environments">Feature Flags and Ephemeral Environments</h2><p>Feature Flags are a necessary and ubiquitous part of modern software development.  As your company and the complexity of your application grows it becomes imperative to be able to control what features are available to your internal development teams, stakeholders and customers.  In the long-ago, before-times, we would just have a variable that you would toggle between true and false to control behavior of your application.  However, as application development transitioned to the Web we needed the same kind of control, except that hard-coded feature flags just weren’t going to cut it. Enter Dynamic Feature Flags!</p><p>Dynamic feature flags were a big improvement over static feature flags, but also added complexity and presented challenges different from static feature flags.  Gone were hard-coded flags, but they were replaced with if statements and more importantly, retrieving the appropriate flags for your application.  Most people started by rolling their own, but as developing with feature flags gained popularity many different companies popped into existence looking to solve the problems of:</p><ul><li>One interface to manage your flags</li><li>Easy maintenance of your flags</li><li>Very fast and reliable retrieval of your flags</li><li>Splitting traffic to one feature or another </li></ul><p>While companies like LaunchDarkly, Optimizely, Rollout, Split.io and others made it fairly easy to create and manage these flags this doesn’t solve all of your issues.  Many software orgs, especially as they grow, need lots of environments for testing. This poses a challenge to your Feature Flag setup specifically if your environments are ephemeral.</p><p>Ephemeral environments are like any environment except they will be removed in a relatively short amount of time unlike your staging or production environments.  Good examples are:</p><ul><li>Feature branches</li><li>Sales Demos</li><li>Load Testing</li><li>Refactors</li></ul><p>These environments may not last a long time, but they are exceedingly important and can be just as complex as production.  While a sales demo environment may be able to function with seed data, a load testing environment will need production or production-like data and many replicas of each service to give a valid result.  These can be super complex to create and manage and their ephemeral nature can play havoc with your feature flag setup.</p><h2 id="feature-flag-environments-to-the-rescuesort-of">Feature Flag Environments to the Rescue…Sort of</h2><p>LaunchDarkly (and others) recognized this issue and created the concept of environments in their own applications.  You can read about their implementation here.  They have apis that allow you to create and manipulate these sets of feature flags on an environment by environment basis. This works great if you have a finite set of environments and the set of them doesn’t change often, but with ephemeral environments the ability to spin them up and down is a feature not a bug.</p><p>In order to simplify this issue most people create two kinds of environments in their favorite Feature Flag provider: one for development (or test) and one for production.  In larger organizations development teams may have a few, such as development, test, uat, staging, and production.  This works fine as long as you don’t want to add another one or you never take the plunge toward truly ephemeral application environments.  </p><p>Once you move to ephemeral environments most people take the shortcut of assigning every ephemeral environment to a single Feature Flag environment, which is simple enough, but creates a large problem with people stepping on each other’s toes.  </p><p>Imagine you have 10 environments all pointing to a single database with writes happening from all those environments: it’s the same issue here.  The great thing about feature flags is the ability to toggle them and see different behavior, but if every environment is pointing to the same one you now have another resource contention problem.  If you toggle Feature A ‘on’ what’s to stop your co-worker from toggling it ‘off’?  Any issues you have with permanent staging environments are magnified with ephemeral environments.</p><p>The best solution would be upon the creation of an ephemeral environment you would create an environment in LaunchDarkly based on something unique about your ephemeral environment and when it comes up, you would make sure it was using the unique SDK api for that particular Feature Flag Environment.  Let’s implement the workflow and see how that would work with Release!</p><p><img src="https://images.ctfassets.net/qf96nnjfyr2y/6iJFF3Zu70PwOxkxXCKtp3/24f7412df0886df79f4ff1239d1eeffc/image9.png" alt="FF workflow"></p><h2 id="working-with-ephemeral-environments">Working with Ephemeral Environments</h2><p>In order to try this out with Release we need a repository with a Docker File that has implemented Feature Flags with LaunchDarkly.  I’m going to use <a href="https://github.com/elanderholm/rails_postgres_redis" target="_blank" rel="noreferrer">this</a> repository on Github and you can do the same by first forking the repository so you can use it to create an application with Release.</p><p>Once you have forked the repository you can navigate to <a href="https://releaseapp.io/" target="_blank" rel="noreferrer">releaseapp.io </a>and sign-in using github in order to follow along with this example.</p><p>The steps to get our ephemeral environments created in Release with support for environments in LaunchDarkly are:</p><ol><li>Create our application in Release</li><li>Create a job with Release to create the environment in LaunchDarkly</li><li>Add some environment variables so the application can contact LaunchDarkly and pull in the SDK Api key from our newly created LaunchDarkly Environment</li><li>Deploy our Ephemeral Environment</li></ol><blockquote><p>If you don’t have a launch darkly account, you can create one for free for 30 days to use for this example.  You will also need to create at least one feature flag.  If you already have a launch darkly account with a lot of feature flags you can just skip this step.</p></blockquote><p><img src="https://images.ctfassets.net/qf96nnjfyr2y/4O6rr31peejXL43aLTPrAi/dc7022076bd7b28e2e189431a5262c7b/image3.png" alt="LaunchDarkly Test Flag"></p><h2 id="create-the-application-in-release">Create the Application In Release</h2><p>Once we are logged into Release we want to click <strong>Create New Application</strong> in the left-hand sidebar.  After doing that we will be presented with Create New Application Workflow.</p><p><img src="https://images.ctfassets.net/qf96nnjfyr2y/5LbYmJLNvAIZwuUqcjMtsv/5aeb59c6fef3b73532e04125a676dd0e/image12_png.png" alt="Create Application With Refresh Button"></p><p>First, we will click the “refresh” button to find our newly forked repository.  Then, we will select that repository and “Docker” for the “api” service.  Finally,  name your application.  Once you have finished click the purple “Generate App Template” button.</p><p><img src="https://images.ctfassets.net/qf96nnjfyr2y/7HFUIDbur1n6r5aELXgb01/f8e12dfed6ca2cb6130fbdb68c98342a/image2.png" alt="Pick your Repository"></p><p>Lastly name your application and generate the template for your configuration.</p><p><img src="https://images.ctfassets.net/qf96nnjfyr2y/WKSmTIAeyTjNKCHLTVbEe/a17a56b380c61066b4fa89fc85690475/image4.png" alt="Name your Application"></p><h2 id="modify-the-application-template">Modify the Application Template</h2><p>Before we can deploy our environment(s) we need to make a modification to our application template and add a few environment variables.  We also need to create a job that will create our LaunchDarkly environment upon initial environment deployment.  Jobs in Release are described in detail <a href="https://docs.releaseapp.io/reference-guide/application-settings/application-template#jobs" target="_blank" rel="noreferrer">here</a>.  The TL;DR is that with a small amount of configuration you can run any arbitrary script or task in a container.  For example, these jobs are very useful for running migrations before a deployment of your backend service.  In this case we will run a rake task to set up our LaunchDarkly Environment.</p><div><pre><p><span>1</span><span>jobs:</span></p><p><span>2</span><span>- name: create-launch-darkly-env</span></p><p><span>3</span><span>  from_services: api</span></p><p><span>4</span><span>  args:</span></p><p><span>5</span><span>  - bundle</span></p><p><span>6</span><span>  - exec</span></p><p><span>7</span><span>  - rake</span></p><p><span>8</span><span>  - launch_darkly:create_environment</span></p></pre></div><blockquote><p>The above yaml represents a job in Release</p></blockquote><p>We will place the above lines right before the “services” stanza in our application template.</p><div><pre><p><span>1</span><span>memory:</span></p><p><span>2</span><span>   limits: 1Gi</span></p><p><span>3</span><span>   requests: 100Mi</span></p><p><span>4</span><span> replicas: 1</span></p><p><span>5</span><span>jobs:</span></p><p><span>6</span><span>  - name: create-launch-darkly-env</span></p><p><span>7</span><span>    from_services: api</span></p><p><span>8</span><span>    args:</span></p><p><span>9</span><span>    - bundle</span></p><p><span>10</span><span>    - exec</span></p><p><span>11</span><span>    - rake</span></p><p><span>12</span><span>    - launch_darkly:create_environment</span></p><p><span>13</span><span>services:</span></p><p><span>14</span><span>  - name: api</span></p><p><span>15</span><span>    image: erik-opsnuts-test-001/rails_postgres_redis/api</span></p><p><span>16</span><span>    has_repo: true</span></p><p><span>17</span><span>    static: false</span></p></pre></div><blockquote><p>Place the jobs snippet into the Application Template</p></blockquote><p>In order for Release to utilize this job as part of the workflow to deploy an environment we will need to add one line near the bottom of the file in the ‘workflows` section.  Under ‘setup’:’order<!-- -->_<!-- -->from’ we will add <strong>jobs.create-launch-darkly-env</strong>. Then, click “Save and Continue.”</p><div><pre><p><span>1</span><span>workflows:</span></p><p><span>2</span><span>- name: setup</span></p><p><span>3</span><span>  order_from:</span></p><p><span>4</span><span>  - jobs.create-launch-darkly-env</span></p><p><span>5</span><span>  - services.all</span></p><p><span>6</span><span>- name: patch</span></p><p><span>7</span><span>  order_from:</span></p><p><span>8</span><span>  - services.api</span></p><p><span>9</span><span>  - services.sidekiq</span></p><p><span>10</span><span>  - services.db</span></p><p><span>11</span><span>  - services.redis</span></p></pre></div><blockquote><p>Place jobs.create-launch-darkly-env before services.all under the workflows stanza</p></blockquote><p><strong>That’s all the configuration needed, now we just need to add two environment variables before we deploy!</strong></p><h2 id="adding-environment-variables">Adding Environment Variables</h2><p><img src="https://images.ctfassets.net/qf96nnjfyr2y/2afDqMwgfbBgsxc5WHAlCL/f43f8e929dcd4e73d0efdb46cab6dc3b/image11.png" alt="Adding Env Variables"></p><p>Click ‘EDIT’ for ‘Default Environment Variables’ to bring up the editor.  We will add two environment variables that contain information about LaunchDarkly.  They are:</p><p><strong>LAUNCH<!-- -->_<!-- -->DARKLY<!-- -->_<!-- -->API<!-- -->_<!-- -->KEY</strong>: Your LaunchDarkly Api Key which is found here.  If you don’t have an api token create the “+ TOKEN” button to make one.  You will want to give it admin privileges.  If you can’t do that contact your administrator.  <em><strong>Once you create it, make sure you copy it and paste it somewhere you can retrieve it.</strong></em>  LaunchDarkly will obfuscate your token and if you don’t save it somewhere you will need to generate a new one.</p><p><img src="https://images.ctfassets.net/qf96nnjfyr2y/6jdsjV6ETNJUOnmCNWP9tZ/75653f2eeb2217930777e900b4ff6746/image7.png" alt="Create LD token"></p><p><strong>LAUNCH<!-- -->_<!-- -->DARKLY<!-- -->_<!-- -->PROJECT<!-- -->_<!-- -->NAME</strong>:  We will just use ‘default’ for this example, but if there is another project you would like to test with feel free.</p><div><pre><p><span>1</span><span>defaults:</span></p><p><span>2</span><span>- key: POSTGRES_USER</span></p><p><span>3</span><span>  value: postgres</span></p><p><span>4</span><span>- key: POSTGRES_PASSWORD</span></p><p><span>5</span><span>  value: postgres</span></p><p><span>6</span><span>- key: LAUNCH_DARKLY_PROJECT_NAME</span></p><p><span>7</span><span>  value: default</span></p><p><span>8</span><span>- key: LAUNCH_DARKLY_API_KEY</span></p><p><span>9</span><span>  value: your-api-key</span></p><p><span>10</span><span>  secret: true</span></p></pre></div><p>Click ‘Save’ to save your environment variables as part of your application configuration.  Then, click ‘Build and Deploy’. You will be redirected to the activity dashboard for that application and a Docker build was kicked off in the background. This will be followed by the deployment of the environment for your application.  You can view the build and the deployment under the ‘builds’ and ‘deploys’ sections respectively.</p><p><img src="https://images.ctfassets.net/qf96nnjfyr2y/0RxHLU0FDRkPRdE8NRzDf/b086b5e3ff3fb0cec0b744ec4e43f8d9/image10.png" alt="left-hand-nav"></p><h2 id="your-environment">Your Environment</h2><p>This process of doing the docker build will take a few minutes the first time.  Once the build and deployment have finished you can find the url for your new environment by clicking ‘Environments’ on the left and then by clicking into your new environment.</p><p><img src="https://images.ctfassets.net/qf96nnjfyr2y/3JfGh3cQnbXMea0fkcCZd1/061efc9db04bfcb0c34e66826a0b06cf/image6.png" alt="Enviro"></p><p>Once you click on the url for your newly created ephemeral environment another window in your browser will open to the example rails site with postgres and redis.  It should look something like this:</p><p><img src="https://images.ctfassets.net/qf96nnjfyr2y/7uUfgJGrmodl3QwVsAI8wk/07de4d7da560256378414160f1cca5c3/image8.png" alt="test-a"></p><p>If you have a flag named ‘test-flag’ in your launch darkly account you can go …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://releaseapp.io/blog/feature-flags-and-ephemeral-environments">https://releaseapp.io/blog/feature-flags-and-ephemeral-environments</a></em></p>]]>
            </description>
            <link>https://releaseapp.io/blog/feature-flags-and-ephemeral-environments</link>
            <guid isPermaLink="false">hacker-news-small-sites-25349269</guid>
            <pubDate>Tue, 08 Dec 2020 18:19:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Prisma Migrate – Database Migrations Simplified]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25347519">thread link</a>) | @sologuardsman2
<br/>
December 8, 2020 | https://www.prisma.io/blog/prisma-migrate-preview-b5eno5g08d0b | <a href="https://web.archive.org/web/*/https://www.prisma.io/blog/prisma-migrate-preview-b5eno5g08d0b">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><div><div><h2 id="contents"><a href="#contents" aria-label="contents permalink"></a>Contents</h2><ul><li><a href="#schema-migrations-with-prisma-migrate">Schema migrations with Prisma Migrate</a></li><li><a href="#how-does-prisma-migrate-work">How does Prisma Migrate work?</a></li><li><a href="#what-has-changed-since-the-experimental-version">What has changed since the Experimental version?</a></li><li><a href="#whats-next">What's next</a></li><li><a href="#try-prisma-migrate-and-share-your-feedback">Try Prisma Migrate and share your feedback</a></li></ul><h2 id="schema-migrations-with-prisma-migrate"><a href="#schema-migrations-with-prisma-migrate" aria-label="schema migrations with prisma migrate permalink"></a>Schema migrations with Prisma Migrate</h2><p>Today we're excited to share the new version of Prisma Migrate! 🎊</p><p>Prisma Migrate is a data modeling and migrations tool that simplifies evolving the database schema with the application in-tandem. Migrate is based on the <a href="https://www.prisma.io/docs/concepts/components/prisma-schema#example">Prisma schema</a> – a declarative data model definition that codifies your database schema.</p><p>This Preview release is the evolution of the Experimental version of Migrate that we released last year. Since then, we've been gathering feedback from the community and incorporating it into Prisma Migrate.</p><h3 id="making-schema-migrations-predictable"><a href="#making-schema-migrations-predictable" aria-label="making schema migrations predictable permalink"></a>Making schema migrations predictable</h3><p>Database schema migrations play a crucial role in software development workflows and affect the most critical component in your application – the database. We've built Migrate to be predictable while allowing you to control how database schema changes are carried out.</p><p>Prisma Migrate generates migrations as plain SQL files based on changes in your Prisma schema. These SQL files are fully customizable and allow you to use any feature of the underlying database, such as manipulating data supporting a migration, setting up triggers, stored procedures, and views.</p><p>Prisma Migrate treads the balance between productivity and control by automating the repetitive and error-prone aspects of writing database migrations while giving you the final say over how they are executed.</p><h3 id="integration-with-prisma-client"><a href="#integration-with-prisma-client" aria-label="integration with prisma client permalink"></a>Integration with Prisma Client</h3><p>Prisma Migrate integrates with Prisma Client using the Prisma schema as their shared source of truth. In other words, both Prisma Client and migrations are generated based on the Prisma schema. This makes synchronizing and verifying database schema changes in your application code easier by leveraging Prisma Client's type safety.</p><h3 id="prisma-migrate-is-ready-for-broader-testing"><a href="#prisma-migrate-is-ready-for-broader-testing" aria-label="prisma migrate is ready for broader testing permalink"></a>Prisma Migrate is ready for broader testing</h3><p>Prisma Migrate has passed rigorous testing internally and is now ready for broader testing by the community. You can use it with PostgreSQL, MySQL, SQLite, and SQL Server. <strong>However, as a Preview feature, it is not fully production-ready yet.</strong> To read more about what Preview means, check out the <a href="https://www.prisma.io/docs/more/releases#preview">maturity levels</a> in the Prisma docs.</p><p>Thus, we're inviting you to try it out and <a target="_blank" rel="noopener noreferrer" href="https://github.com/prisma/prisma/issues/4531">give us feedback</a> so we can bring Prisma Migrate to General Availability. 🚢</p><p>Your feedback and suggestions will help us shape the future of Prisma Migrate. 🙌</p><hr><h2 id="how-does-prisma-migrate-work"><a href="#how-does-prisma-migrate-work" aria-label="how does prisma migrate work permalink"></a>How does Prisma Migrate work?</h2><p>Prisma Migrate is based on the <a href="https://www.prisma.io/docs/reference/tools-and-interfaces/prisma-schema">Prisma schema</a> and works by generating <code>.sql</code> migration files that are executed against your database.</p><p>The Prisma schema is the starting point for schema migrations and provides an overview of your desired end-state of the database. Prisma Migrate inspects changes in the Prisma schema and generates the necessary <code>.sql</code> migration files to apply.</p><p>Applying migrations looks very different depending on the stage of development. For example, during development, there are scenarios where resetting the database can be tolerated for quicker prototyping, while in production, great care must be taken to avoid data loss and breaking changes.</p><p>Prisma Migrate accommodates for this with workflows for local development and applying migrations in production.</p><h3 id="evolving-the-schema-in-development"><a href="#evolving-the-schema-in-development" aria-label="evolving the schema in development permalink"></a>Evolving the schema in development</h3><p>To use the new version of Prisma Migrate, you should have at least version <code>2.13.0</code> of the <a href="https://www.prisma.io/docs/concepts/components/prisma-cli/installation"><code>@prisma/cli</code></a> package installed.</p><p>During development, you first define the Prisma schema and then run the <code>prisma migrate dev --preview-feature</code> command, which generates the migration, applies it, and generates Prisma Client:</p><p><span><img src="https://d33wubrfki0l68.cloudfront.net/9dee8cc50b930a017447904d95e15e0e82f9a3bf/426d4/blog/posts/2020-12-migrate-development-workflow.png" alt="Development workflow"><span>Development workflow</span></span></p><p>Here is an example showing it in action:</p><p><strong>1. Define your desired database schema using the Prisma schema:</strong></p><pre><code><span>datasource</span> <span>db</span> <span>{</span>
  provider <span>=</span> <span>"postgresql"</span>
  url      <span>=</span> <span>env</span><span>(</span><span>"DATABASE_URL"</span><span>)</span>
<span>}</span>

<span>model</span> <span>User</span> <span>{</span>
  id    <span>Int</span>      <span>@id</span> <span>@default</span><span>(</span><span>autoincrement</span><span>(</span><span>)</span><span>)</span>
  name  <span>String</span>
  posts <span>Post</span><span>[</span><span>]</span>
<span>}</span>

<span>model</span> <span>Post</span> <span>{</span>
  id        <span>Int</span>     <span>@id</span> <span>@default</span><span>(</span><span>autoincrement</span><span>(</span><span>)</span><span>)</span>
  title     <span>String</span>
  published <span>Boolean</span> <span>@default</span><span>(</span><span>true</span><span>)</span>
  authorId  <span>Int</span>
  author    <span>User</span>    <span>@relation</span><span>(</span><span>fields:</span> <span>[</span>authorId<span>]</span><span>,</span> <span>references:</span> <span>[</span>id<span>]</span><span>)</span>
<span>}</span>
</code></pre><p><strong>2. Run <code>prisma migrate dev --preview-feature</code> to create and execute the migration.</strong></p><div><div><svg width="6" height="9" viewBox="0 0 6 9" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M7.3273 0C7.88605 0 8.20036 0.653318 7.85732 1.1017L4.53001 5.45076C4.26119 5.80213 3.73881 5.80213 3.46999 5.45076L0.142684 1.1017C-0.200356 0.653318 0.113948 0 0.672698 0H7.3273Z" transform="rotate(-90 4.5 4.357)" fill="#8FA6B2"></path></svg><p><label for="tab-1">Expand to view the SQL contents of the generated migration</label></p><div><pre><code>
<span>CREATE</span> <span>TABLE</span> <span>"User"</span> <span>(</span>
  <span>"id"</span> <span>SERIAL</span><span>,</span>
  <span>"name"</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span><span>,</span>
  <span>PRIMARY</span> <span>KEY</span> <span>(</span><span>"id"</span><span>)</span>
<span>)</span><span>;</span>

<span>CREATE</span> <span>TABLE</span> <span>"Post"</span> <span>(</span>
  <span>"id"</span> <span>SERIAL</span><span>,</span>
  <span>"title"</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span><span>,</span>
  <span>"published"</span> <span>BOOLEAN</span> <span>NOT</span> <span>NULL</span> <span>DEFAULT</span> <span>true</span><span>,</span>
  <span>"authorId"</span> <span>INTEGER</span> <span>NOT</span> <span>NULL</span><span>,</span>
  <span>PRIMARY</span> <span>KEY</span> <span>(</span><span>"id"</span><span>)</span>
<span>)</span><span>;</span>

<span>ALTER</span> <span>TABLE</span> <span>"Post"</span> <span>ADD</span> <span>FOREIGN</span> <span>KEY</span><span>(</span><span>"authorId"</span><span>)</span><span>REFERENCES</span> <span>"User"</span><span>(</span><span>"id"</span><span>)</span> <span>ON</span> <span>DELETE</span> <span>CASCADE</span> <span>ON</span> <span>UPDATE</span> <span>CASCADE</span><span>;</span>
</code></pre></div></div></div><p>After the migration has been executed, the migration files are typically committed to the repository so that the migration can be applied in other environments.</p><p>Further changes to the database schema follow the same workflow and begin with updating the Prisma schema.</p><h3 id="customizing-sql-migrations"><a href="#customizing-sql-migrations" aria-label="customizing sql migrations permalink"></a>Customizing SQL migrations</h3><p>You can customize the migration SQL with the following workflow:</p><ol><li>Run <strong><code>prisma migrate dev --create-only --preview-feature</code></strong> to create the SQL migration without applying it.</li><li>Edit the migration SQL.</li><li>Run <strong><code>prisma migrate dev --preview-feature</code></strong> to apply it.</li></ol><h3 id="applying-migrations-in-production-and-other-environments"><a href="#applying-migrations-in-production-and-other-environments" aria-label="applying migrations in production and other environments permalink"></a>Applying migrations in production and other environments</h3><p>To apply migrations to other environments such as production, you pull changes to the repository containing the migrations and run the <code>prisma migrate deploy</code> command:</p><p><span><img src="https://d33wubrfki0l68.cloudfront.net/5d9831941c87b7e24646bca3d96f91d4b799af6a/b7004/blog/posts/2020-12-migrate-production-workflow.png" alt="Production workflow"><span>Production workflow</span></span></p><hr><h2 id="what-has-changed-since-the-experimental-version"><a href="#what-has-changed-since-the-experimental-version" aria-label="what has changed since the experimental version permalink"></a>What has changed since the Experimental version?</h2><p>The most significant change since the Experimental version is the use of SQL as the format for migrations, making migrations <strong>deterministic</strong>. In other words, the exact steps of the migration are determined when the migration is created, allowing you to inspect the SQL (and make changes if necessary) before running.</p><p>This approach has the following benefits:</p><ul><li>The generated SQL is editable, thereby allowing you to control the exact schema changes.</li><li>The migration is predictable with the exact SQL that will be applied.</li><li>You don't need to write SQL unless you want to change a migration.</li><li>You can perform data migrations using SQL as part of a migration.</li></ul><p>Editable SQL for migrations is useful in scenarios where there are multiple ways to map changes in the Prisma schema to the database, and the desired path cannot be automatically determined.</p><p>For example, when you rename a field in the Prisma schema, that can be interpreted as either deleting the column and adding an unrelated new one or as you renaming the column. By allowing you to inspect and edit the migration SQL, you can decide whether to rename the column (and retain the data in the column) or drop it and add a new one.</p><p>If you're upgrading Prisma Migrate from the Experimental version, check out the <a href="https://www.prisma.io/docs/guides/prisma-guides/prisma-migrate-guides/add-prisma-migrate-to-a-project">upgrade guide</a>.</p><hr><h2 id="whats-next"><a href="#whats-next" aria-label="whats next permalink"></a>What's next</h2><p>This Preview version of Prisma Migrate lays the foundations for the upcoming General Availability release. Some of the improvements we are considering are improved support for native database types, seeding functionality, and finding a way to make database resets in development less disruptive.</p><h3 id="native-database-types"><a href="#native-database-types" aria-label="native database types permalink"></a>Native database types</h3><p>One of the most requested features in Prisma is support for the database's native types. This release is a step closer to that – however, there's still more work to be done for native types to be fully supported.</p><p>Currently, the Prisma schema can only represent a limited set of types: <code>String</code>, <code>Int</code>, <code>Float</code>, <code>Boolean</code>, <code>DateTime</code>, and <code>Json</code>. Each of these types has a default mapping to an underlying database type that's specified for each database connector (see the mappings for <a href="https://www.prisma.io/docs/concepts/database-connectors/postgresql#prisma-migrate">PostgreSQL</a> and <a href="https://www.prisma.io/docs/concepts/database-connectors/mysql#prisma-migrate">MySQL</a>).</p><p>In version <a target="_blank" rel="noopener noreferrer" href="https://github.com/prisma/prisma/releases/tag/2.11.0">2.11.0</a>, we released the <code>nativeTypes</code> Preview feature – the ability to annotate fields in the Prisma schema with the specific native database type that it should be mapped to. <strong>However, the native types preview feature doesn't work with Prisma Migrate yet</strong>.</p><p>Even so, you can still change the types of columns in the generated SQL as long as they are supported, as documented in the <a href="https://www.prisma.io/docs/concepts/database-connectors/postgresql#prisma-migrate">PostgreSQL</a> and <a href="https://www.prisma.io/docs/concepts/database-connectors/mysql#prisma-migrate">MySQL</a> connector docs.</p><hr><p>We built Prisma Migrate for you and are keen to hear your feedback.</p><p>We want to understand how Prisma Migrate fits into your development workflow and how we can help you stay productive and confident while building and evolving data-centric applications.</p><p>🐛 Tried it out and found that it's missing something or stumbled upon a bug? Please <a target="_blank" rel="noopener noreferrer" href="https://github.com/prisma/prisma/issues/new/choose">file an issue</a> so we can look into it.</p><p>🏗 Share your feedback about how the new Prisma Migrate is working out for you on <a target="_blank" rel="noopener noreferrer" href="https://github.com/prisma/prisma/issues/4531">GitHub</a>.</p><p>🌍 Join us on our <a target="_blank" rel="noopener noreferrer" href="https://slack.prisma.io/">Slack</a> in the <a target="_blank" rel="noopener noreferrer" href="https://app.slack.com/client/T0MQBS8JG/C01ACF1DJ1M"><code>#prisma-migrate</code></a> channel for help.</p><p>👷‍♀️ We are thrilled to finally share the Preview version of Prisma Migrate and can't wait to see what you all build with it.</p></div></div></article></div>]]>
            </description>
            <link>https://www.prisma.io/blog/prisma-migrate-preview-b5eno5g08d0b</link>
            <guid isPermaLink="false">hacker-news-small-sites-25347519</guid>
            <pubDate>Tue, 08 Dec 2020 16:34:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Towards a Strong Mental Model of Docker]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25346980">thread link</a>) | @throughnothing
<br/>
December 8, 2020 | http://blog.andrewray.me/towards-a-strong-mental-model-of-docker/ | <a href="https://web.archive.org/web/*/http://blog.andrewray.me/towards-a-strong-mental-model-of-docker/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
                <p><strong>TL;DR</strong> I'm a full stack engineer who works almost exclusively on Macs. I don't have a computer science degree nor do I have strong experience with Linux architecture. This post documents my journey of learning Docker.</p>



<p><strong>If you stick with this blog post, you'll have decent answers for the following questions.</strong> If you've already used Docker, these questions will strengthen your understanding.</p>

<ol>
<li>What is "Docker"?  </li>
<li>What is a "container"? Did Docker invent containers?  </li>
<li>What does it mean to "run a container"?  </li>
<li>Are containers stateful? What does container state even mean?  </li>
<li>Do containers run only one process? Or can they run multiple processes?  </li>
<li>What exactly is "Docker for Mac"? I thought Docker was cross platform?  </li>
<li>Does Docker work for Windows development?  </li>
<li>What are "docker-compose" and "BuildKit"? Why are there so many different applications?  </li>
<li>Most Docker images start with something like <code>FROM ubuntu</code>. Is my container running a full Ubuntu operating system? If not, why is it <em>from</em> Ubuntu? In fact, what does it even mean to "run an operating system?"</li>
</ol>

<p>The summarized answers for all of these questions are at the end of this post.</p>



<p>How might you <a href="https://blog.andrewray.me/reactjs-for-stupid-people/">explain React.js</a> to a newcomer? You <em>could</em> say:</p>

<blockquote>
  <p>React is a JavaScript library for building user interfaces</p>
</blockquote>

<p>I don't think this would help a newcomer form a mental model of React, as it's <strong>too high level</strong>. You could also say:</p>

<blockquote>
  <p>React is a declarative DSL that outputs a tree-like "virtual DOM," mutating the DOM (or another view target) with O(n) reconciliation performance, as a pure function of component state and props</p>
</blockquote>

<p>And I think, justifiably, the newcomer would slap you in the face, because you just told them to go to hell.</p>

<p>Something more approachable is:</p>

<blockquote>
  <p>React is mainly a Javascript library that takes in data, like a username, and spits out HTML based on that data. It gives you a performant way to update the view in real time based on changes to that data. React can do a lot more than this, but the most common use case is taking in data and spitting out HTML.</p>
</blockquote>

<p>Does this accurately describe all of React? No, and once someone dives in, they'll have to learn the intricacies of the API. But it's more <strong>approachable.</strong></p>

<p>I find the Docker documentation is written by, and targeted to, people who already have a deeper understanding of Linux fundamentals, virtualization, and kernels versus operating systems.</p>

<p>If someone tells you that Docker is "cgroups and namespaces" with no other information, it's technically accurate, but not approachable.</p>



<p>If you just want to build web servers, why care about Docker at all? Well, I've personally encountered many of the issues containers claim to solve:</p>

<ul>
<li>Requiring multiple versions of <strong>userland libraries</strong> (like Node, and also npm packages) in different repositories, causing conflicts for local development because they aren't isolated</li>
<li>Requiring <strong>multiple service or database versions</strong> causing conflict in local development (have you ever tried to run two versions of MySQL on a Mac using Homebrew?)</li>
<li>Running into <strong>dependency installation issues</strong> when checking out both old and new projects (Ruby / Rails are particularly bad at this, damn you <a href="https://stackoverflow.com/q/33996523/743464">Nokogiri</a>). Who hasn't run an old project only to learn everything is broken?</li>
<li>Needing to <strong>easily run multiple services</strong> for local development, some of which need multiple commands (such as both a server process and a static file builder process like Webpack)</li>
<li>Losing <strong>parity between environments</strong> due to version or process execution discrepancies, making it impossible to reproduce situations that cause some production bugs</li>
</ul>



<p>This post <strong>isn't</strong> a deep dive on Docker's API. We won't explain the syntax of <code>Dockerfile</code>s, <code>docker-compose.yml</code> files, nor the CLI. We also won't talk much about "orchestration," the verb used to describe running multiple containers and their dependencies and dealing with things like networking and automatically restarting containers.</p>



<h4 id="whatisdocker">What is Docker?</h4>

<p>Believe it or not, let's start with the Docker logo. It's great for our mental model!</p>

<p><img src="http://blog.andrewray.me/content/images/2020/05/Moby-logo.png" alt="The Docker Logo, which is a whale with a stack of shipping containers on its back"></p>

<p>We have a whale carrying some shipping containers. (His name is "Moby Dock," <strong>Moby</strong> for short). He's a whale because, a long time ago, he beat out other animals in <a href="https://99designs.com/logo-design/contests/create-cool-open-source-project-logo-219415/entries">a community logo design contest</a>.</p>

<p>In our mental model, the whale is Docker. <strong>Docker is a <em>platform</em>.</strong> It's not one thing, it's a collection of different technologies and standards that provide a platform to <strong>run containers!</strong> The whale is supporting, or "running" containers. We'll learn more about "containers" soon.</p>

<p><img src="http://blog.andrewray.me/content/images/2014/Oct/key.png#inline-block" alt="Key icon" title=""> <strong>Key concept:</strong> Docker is a platform, including online hosting, and a suite of local tools, like <code>docker-compose</code> and the Docker engine.</p>

<p>When learning Docker, you might wonder what the "right" or "Docker" way is. By viewing Docker as a platform, we realize there are multiple ways to do things. <strong>It's like Git:</strong> Git is a platform and suite of command line tools. In Git there are different ways to use the suite of tools. Some people use forks, some use feature branches, some use rebasing, some use merging. Docker provides you tools to build "images" and "run" "containers," with multiple ways to achieve the same thing.</p>

<p>"Docker" also means the <strong>company.</strong> I think this overloading makes learning Docker confusing. The Docker <em>platform</em> is <a href="https://github.com/docker">entirely open source</a>, so what does the company do? It makes money by <a href="https://hub.docker.com/pricing">charging for private hosting and management tools</a>, and offering <a href="https://hub.docker.com/pricing">enterprise services</a> for complex image/container management.</p>

<p>An aside: In 2019 Docker was <a href="https://techcrunch.com/2019/11/13/mirantis-acquires-docker-enterprise/">acquired</a> by a company named <a href="https://www.mirantis.com/software/mcp/">Mirantis</a> which makes money by supporting cloud application development around Kubernetes.</p>



<h4 id="whatisacontainerdiddockerinventcontainers">What is a "container"? Did Docker invent containers?</h4>

<p>Containers were the most difficult part of Docker for me to understand. I had some idea they were running a process in isolation, but I didn't really know what that meant. To answer this, let's make sure we understand the Docker platform by looking at a different question.</p>

<h4 id="whatexactlyisdockerformacithoughtdockerworkedonallplatforms">What exactly is "Docker for Mac?" I thought Docker worked on all platforms?</h4>

<p>I initially thought Docker was "cross platform." This is both true and a complete, filthy lie.</p>

<p>Take Docker for Mac out of the equation and pretend like you're developing directly on a Linux operating system, like Ubuntu. All operating systems have a "<a href="https://en.wikipedia.org/wiki/Kernel_(operating_system)"><strong>kernel</strong></a>", the core computer program of the operating system that controls everything and facilitates access to hardware.</p>

<p>The Linux kernel has features that allow you to run processes (computer programs) in "isolation":</p>

<ul>
<li>"<a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/ch01">Control groups</a>", usually called "cgroups," let you run a Linux process with a specifically allocated set of resources, such as how much memory it has.</li>
<li>"<a href="https://en.wikipedia.org/wiki/Linux_namespaces">Namespaces</a>", which add additional isolation, like making your process look like it's the only one on the system.</li>
</ul>

<p><img src="http://blog.andrewray.me/content/images/2014/Oct/key.png#inline-block" alt="Key icon" title=""> <strong>Key concept:</strong>
Docker uses existing Linux features to run containers. These features <strong>only</strong> work on Linux. MacOS and Windows don't have cgroups nor namespaces.</p>

<p>So what is Docker for Mac, if these technologies only exist on Linux? Docker for Mac runs a full <strong>Linux "virtual machine."</strong> Virtual machines are a broader topic, but they contain full operating systems, including kernels, so we have access to these Linux features.</p>

<p>My mental image is the whale icon on my Mac's status bar holds a little Linux computer.</p>

<p>This also has the implication that the most common type of application developed with Docker is an application <em>for</em> Linux. For example, you <a href="https://serverfault.com/questions/607443/can-mac-os-x-be-run-inside-docker">can't run a Mac native application</a> like Photoshop in a Docker container. On Windows there's Docker container support for both Windows (.exe) applications as well as Linux applications, but they use different underlying technologies. And you can't run a Windows container on a Mac. See what I mean about Docker not really being "cross platform"?</p>

<p>On Mac and Windows, the Linux virtual machine is hidden from you as an implementation detail. You can typically SSH into computers and run commands on them. Docker for Mac <strong>doesn't let you directly SSH into this virtual machine.</strong> Instead, the Docker commands you run locally are what coordinate sending files and commands into this Linux virtual machine.</p>

<p>Finally, did Docker invent containers? No, containers are a <a href="https://en.wikipedia.org/wiki/List_of_Linux_containers">general concept</a>. <strong>Docker didn't invent containers.</strong> You can create and run containers <a href="https://jvns.ca/blog/2016/10/26/running-container-without-docker/">without Docker</a>. Cgroups and namespaces also aren't the only way to run processes in isolation, they're just the tools Docker uses.</p>

<h4 id="sodoesdockerworkonwindows">So does Docker work on Windows?</h4>

<p>The <a href="" windows="" container"="" https:="" www.youtube.com="" watch?v="8gG6_Xr68D8" are="" the"="">official Docker Enterprise video, at 0:45,</a> mentions "Windows containers." So you <em>can</em> use Docker on Windows?</p>

<p>It depends on what you mean by "use on Windows." If you're developing Linux applications, then <strong>Docker for Windows does the same thing as Docker for Mac,</strong> it runs a Linux virtual machine, and you communicate with that.</p>

<p>If however you want to develop an application that only runs on Windows, like a file ending in ".exe," you have to run your containers on a Windows host somehow. This is supported by Docker Enterprise, but much less common.</p>

<h4 id="whatdoesitmeantorunacontainer">What does it mean to "run a container"?</h4>

<p>"Container" is a noun. What we've talked about so far is the verb of "running" a container.</p>

<p><strong>Recap:</strong> When Docker runs your container, it starts your process (your computer program, developed for Linux architecture) wrapped in suite of Linux kernel tools to make it look like it's the only process on the system. Docker for Mac (and Windows) provide a Linux virtual machine so you can use these technologies. Running the container is basically the same as what you'd do on your host machine (like <code>npm start</code> or <code>rails s</code>), it's just in "isolation" inside the Linux VM.</p>

<p>Let's poke at containers a little more to strengthen our understanding.</p>

<h4 id="docontainershave_only_oneprocessorcantheyhavemultipleprocesses">Do containers have <em>only</em> one process? Or can they have multiple processes?</h4>

<p>Let's check! First let's install <code>htop</code>, the awesome process viewer on your Mac (not in a container): <code>brew install htop</code>.</p>

<p>Now run <code>htop</code> on your Mac. If you haven't used <code>htop</code>, don't worry too much about the interface, just note you see all sorts of processes running:</p>

<p><img src="http://blog.andrewray.me/content/images/2020/05/Screen-Shot-2020-05-07-at-8.42.50-PM.png" alt="A screenshot of htop running on Mac, showing multiple processes"></p>

<p>Press <code>ctrl-c</code> to exit <code>htop</code>. …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://blog.andrewray.me/towards-a-strong-mental-model-of-docker/">http://blog.andrewray.me/towards-a-strong-mental-model-of-docker/</a></em></p>]]>
            </description>
            <link>http://blog.andrewray.me/towards-a-strong-mental-model-of-docker/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25346980</guid>
            <pubDate>Tue, 08 Dec 2020 16:01:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[We Love GPLv3, but Are Switching License to Apache 2.0]]>
            </title>
            <description>
<![CDATA[
Score 159 | Comments 243 (<a href="https://news.ycombinator.com/item?id=25346965">thread link</a>) | @vivek9209
<br/>
December 8, 2020 | https://terminusdb.com/blog/2020/12/08/we-love-gplv3-but-are-switching-license-to-apache-2-0-terminusdb/ | <a href="https://web.archive.org/web/*/https://terminusdb.com/blog/2020/12/08/we-love-gplv3-but-are-switching-license-to-apache-2-0-terminusdb/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <div id="markdown" itemprop="articleBody">
          <h2 id="changing-license-from-gplv3-to-apache-20-terminusdb">Changing License from GPLv3 to Apache 2.0: TerminusDB</h2>

<p>We have decided to re-license TerminusDB from GPLv3 to Apache 2.0. We want independent software developers (ISVs) to embed TerminusDB in their applications and those developers feel that Apache is a lower risk option. The substantive points of practical difference are far less important – sufficient people believe it to be true and sufficient lawyers have advised teams to be wary of GPL.</p>

<p>In our experience, ISVs and devs in large companies/institutions size up their options at project conception and there remains a niggling doubt that ‘GPL might limit commercial prospects and cause me headaches’. The world has changed – and code freedom is being overtaken by developer freedom.</p>

<p>Open-source software is everywhere. It’s eating the world. In the top 10 databases on <a href="https://db-engines.com/en/ranking">DB-Engines</a>, the remaining proprietary databases were released in 1980 (Oracle), 1983 (IBM Db2) and 1989 (Microsoft SQL Server). It is hard to imagine another non-OSS database ever entering the top 10.</p>

<p>We had hoped that our association with the principals of the free software movement would result in community adoption and contribution, but that hasn’t really been the case. We see limited community input that relates to our choice of GPL. That might not be too surprising as when you investigate which license to choose on Stackoverflow, you get popular but wrong-headed comments like:</p>

<p><code>the GNU/GPL bunch are generally extremists when you encounter them in the wild.</code></p>

<p><code>don’t use GPL if you want your project to be commercial</code></p>

<p>With the shift to Apache, TerminusDB is, in a sense, becoming more open source as we are removing restrictions on how you can use the software.</p>

<h3 id="debate">Debate</h3>

<p>The core TerminusDB team had a long debate about licenses before the release of 1.0 last year. The main topics of discussion were:</p>

<ol>
<li>The risks of a cloud provider forking the code then hosting the database</li>
<li>What open source means to TerminusDB as a group</li>
<li>What we, as a community of devs and users, are most comfortable with</li>
</ol>

<h4 id="1-the-big-bad-cloud-providers">1. The big bad cloud providers</h4>

<p>In the past there was an unwritten rule, that big platforms wouldn’t come along and fork open source code and deliver the same product as a service. Unfortunately, those days are gone. AWS in particular has actively sought to offer very similar services to open source products. This led to <a href="https://www.mongodb.com/licensing/server-side-public-license/faq">MongoDB</a>, <a href="https://www.cockroachlabs.com/blog/oss-relicensing-cockroachdb/">CockroachDB</a> and <a href="https://www.confluent.io/confluent-community-license-faq/">Confluent</a> (among others) changing their licences to variations of ‘server side’ and moving away from the open-source tradition. They try to say ‘we are still open source, we are just forbidding a specific type of action’, but it can feel like window dressing for ‘we want to sweat our assets’.</p>

<p>Mongo, for example, is hardly suffering – it’s valued at over $15 billion. With such vast resources, they should be (and are) able to compete in the provision of their own database. MongoDB’s technology is more than competitive with AWS’ DocumentDB, and Mongo’s Atlas DBaaS - which runs on AWS infra - has been a huge success.</p>

<p>I’m sure our perspective will shift over time, but from where we’re standing, having a cloud provider launch a competing service would be a sign of enormous success. (And this is not to say that the cloud providers’ parasitic approach to OSS projects is not a genuine problem, it simply acknowledges that you have to be a widely used OSS project before it *becomes* a problem).</p>

<h3 id="2-free-software">2. Free software</h3>

<p>We don’t think it should be our job to provide corporations with free labor.</p>

<p>We do think that the software community should be able to access and use TerminusDB.</p>

<p>In 1974 software became copyrightable in the USA. It subsequently became obvious that researchers were giving out software for free, but businesses were not giving back. GNU/GPL came along to provide a new framework for that interface – the software would be free as in freedom (libre). Everybody would be free to modify and distribute, but proprietary additions would not be allowed.</p>

<p>The problem of businesses not giving back remains today.</p>

<p>GPL and copyleft provisions work well when they dominate open-source, but their waning popularity increases the ability for ISVs and corporates to go for more open licensing and for legal teams to write anti-GPL provisions into internal rules. Only significant developer push back can change that reality (and why push back when there are so many OSS options with permissive licenses).</p>

<p>Maybe if <a href="https://db-engines.com/en/system/MySQL">MySQL</a>, its offshoot <a href="https://db-engines.com/en/system/MariaDB">MariaDB</a>, and our graph brothers <a href="https://db-engines.com/en/system/Neo4j">Neo4j</a> weren’t the only GPL flag flyers in the top 20 databases, it might be easier to gain adoption with GPL; however, the other big OSS players: Postgres, Cassandra, Elastic &amp; Redis all go for less restrictive licenses.</p>

<p>The Affero GPL (AGPL) is treated as an even greater pariah than the GPLv3. The Google internal policy] bans all use of AGPL:</p>

<p><img src="https://terminusdb.com/blog/assets/uploads/agpl-at-google.jpg" alt=""></p>

<p><img src="https://terminusdb.com/blog/assets/uploads/agpl-at-google-2.jpg" alt=""></p>

<p>It reminds of <a href="https://news.ycombinator.com/item?id=13421608">this HN comment</a> on the <a href="http://www.defmacro.org/2017/01/18/why-rethinkdb-failed.html">excellent postmortem</a> of the demise of RethinkDB (recommended reading for all OSS folk):</p>

<p><img src="https://terminusdb.com/blog/assets/uploads/agpl-hn.jpg" alt=""></p>

<p>Our graph database brothers and sisters in <a href="https://neo4j.com/open-core-and-neo4j/">Neo4j moved</a> from the AGPL to ‘AGPL with a commons clause’ to GPLv3 to making some of the code proprietary. Getting the right license for an OSS project that also allows for some commercialization is far from straightforward. (As Neo certainly know – check out this <a href="https://public.igovsol.com/neo4j-court-records/graphfoundation/80-main.pdf">current court case about the formerly GPL enterprise code</a>).</p>

<p>It seems that <a href="https://github.com/graknlabs/grakn">Grakn</a> and <a href="https://github.com/fluree/db">Fluree</a> are seeing some success with the AGPL. We genuinely wish them well as we know it is a hard path to walk.</p>



<p>We did get some negative feedback about the GPL prior to launch. Some people working in corporates weren’t comfortable and thought that it would prevent them from using Terminus. Our response was – <strong>we didn’t develop for them</strong>. And that remains the case; however, the GPL skeptical environment is pervasive. There is a person who tweets TerminusDB after every release asking when the Apache version will land – I always ask him why he thinks he needs an Apache version and he doesn’t really know… though he has a vague feeling that the application he builds on top will be less valuable.</p>

<p>We thought we would focus on the cloud offering – <a href="https://terminusdb.com/hub/">TerminusHub</a>, which, as SaaS built on the database, has the benefit of being an in-house ‘product’ that doesn’t worry about the license of the underlying software. However, the community wants to build applications on the software and we want to offer an easy way to build a version control and collaboration layer into ISV applications. We think TerminusDB is the perfect infrastructure to build the next <a href="https://www.notion.so/">Notion</a> or <a href="https://roamresearch.com/">Roam Research</a>. We worry about the syncing, versioning and data collaboration – you worry about your users.</p>

<h3 id="why-shift-licenses">Why shift Licenses</h3>

<p>We do not think that our job is to provide corporations with free labor.</p>

<p>We still believe that <a href="https://www.gnu.org/gnu/manifesto.en.html">“the fundamental act of friendship among programmers is the sharing of programs.”</a></p>

<p>We would welcome a GPL fork of the TerminusDB code – we are happy to work with any such project should it emerge.</p>

<p>But we wish to build a community first and foremost. In order to facilitate that community, we will be moving to Apache 2.0 immediately.</p>

<p>We are going to continue to focus on creating a great open-source database and allow everybody to use that software in their projects.</p>

        </div>

        



      </div>
    </div></div>]]>
            </description>
            <link>https://terminusdb.com/blog/2020/12/08/we-love-gplv3-but-are-switching-license-to-apache-2-0-terminusdb/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25346965</guid>
            <pubDate>Tue, 08 Dec 2020 16:00:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Kafka Is Not a Database]]>
            </title>
            <description>
<![CDATA[
Score 309 | Comments 161 (<a href="https://news.ycombinator.com/item?id=25346851">thread link</a>) | @andrioni
<br/>
December 8, 2020 | https://materialize.com/kafka-is-not-a-database/ | <a href="https://web.archive.org/web/*/https://materialize.com/kafka-is-not-a-database/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><em>This post is co-authored by George Fraser, the CEO of <a href="http://fivetran.com/">Fivetran</a>, and Arjun Narayan, the CEO of Materialize. This blog post is cross-posted <a href="https://fivetran.com/blog/kafka-is-not-a-database">on the Fivetran blog</a>.</em></p>
<p>It’s important to understand the uses and abuses of streaming infrastructure.</p>
<p>Apache Kafka is a message broker that has rapidly grown in popularity in the last few years. Message brokers have been around for a long time; they’re a type of datastore specialized for “buffering” messages between producer and consumer systems. Kafka has become popular because it’s open-source and capable of scaling to very large numbers of messages.</p>
<p>Message brokers are classically used to decouple producers and consumers of data. For example, at Fivetran, we use a message broker similar to Kafka to buffer customer-generated webhooks before loading them in batches into your data warehouse:</p>
<p><img src="https://materialize.com/wp-content/uploads/2020/12/kafka_overview.png" data-src="https://materialize.com/wp-content/uploads/2020/12/kafka_overview.png" alt="A Message Broker Used as a buffer before loading into a data warehouse" width="960" height="124" data-srcset="https://materialize.com/wp-content/uploads/2020/12/kafka_overview.png 960w, https://materialize.com/wp-content/uploads/2020/12/kafka_overview-300x39.png 300w, https://materialize.com/wp-content/uploads/2020/12/kafka_overview-768x99.png 768w" data-sizes="(max-width: 960px) 100vw, 960px" srcset="https://materialize.com/wp-content/uploads/2020/12/kafka_overview.png 960w, https://materialize.com/wp-content/uploads/2020/12/kafka_overview-300x39.png 300w, https://materialize.com/wp-content/uploads/2020/12/kafka_overview-768x99.png 768w"></p>
<p>In this scenario, the message broker is providing durable storage of events between when a customer sends them, and when Fivetran loads them into the data warehouse.</p>
<p>However, Kafka has occasionally been described as something much more than just a better message broker. Proponents of this viewpoint position Kafka as a fundamentally new way of managing data, where <a href="https://www.confluent.io/blog/okay-store-data-apache-kafka/" target="_blank" rel="noopener noreferrer">Kafka replaces the relational database as the definitive record of what has happened</a>. Instead of reading and writing a traditional database, you append events to Kafka, and read from downstream views that represent the present state. This architecture has been described as “<a href="https://www.confluent.io/blog/turning-the-database-inside-out-with-apache-samza/" target="_blank" rel="noopener noreferrer">turning the database inside out</a>“.</p>
<p>In principle, it is possible to implement this architecture in a way that supports both reads and writes. However, during that process you will eventually confront every hard problem that database management systems have faced for decades. You will more or less have to write a full-fledged DBMS in your application code. And you will probably not do a great job, because databases take years to get right. You will have to deal with dirty reads, phantom reads, write skew, and all the other symptoms of a hastily implemented database.</p>

<p>The fundamental problem with using Kafka as your primary data store is it provides no isolation. Isolation means that, globally, all transactions (reads and writes) occur along some consistent history. Jepsen provides a <a href="https://jepsen.io/consistency">guide</a> of isolation levels (inhabiting an isolation level means that the system will never encounter certain anomalies).</p>
<p>Let’s consider a simple example of why isolation is important: suppose we’re running an online store. When a user checks out, we want to make sure all their items are actually in stock. The way to do this is to</p>
<ol>
<li>Check the inventory level for each item in the user’s cart.</li>
<li>If an item is no longer available, abort the checkout.</li>
<li>If all items are available, subtract them from the inventory and confirm the checkout.</li>
</ol>
<p>Suppose we are using Kafka to manage this process. Our architecture might look something like this:</p>
<p><img src="https://materialize.com/wp-content/uploads/2020/12/kafka_checkout.png" data-src="https://materialize.com/wp-content/uploads/2020/12/kafka_checkout.png" alt="A microservice workflow for processing checkouts" width="960" height="753" data-srcset="https://materialize.com/wp-content/uploads/2020/12/kafka_checkout.png 960w, https://materialize.com/wp-content/uploads/2020/12/kafka_checkout-300x235.png 300w, https://materialize.com/wp-content/uploads/2020/12/kafka_checkout-768x602.png 768w" data-sizes="(max-width: 960px) 100vw, 960px" srcset="https://materialize.com/wp-content/uploads/2020/12/kafka_checkout.png 960w, https://materialize.com/wp-content/uploads/2020/12/kafka_checkout-300x235.png 300w, https://materialize.com/wp-content/uploads/2020/12/kafka_checkout-768x602.png 768w"></p>
<p>The web server reads the inventory level from a view downstream from Kafka, but it can only commit the transaction <em>upstream</em> in the checkouts topic. The problem is one of <strong>concurrency control</strong>: if there are two users racing to buy the last item, <em>only one must succeed</em>. We need to read the inventory view and confirm the checkout at <em>a single point in time</em>. However, there is no way to do this in this architecture.</p>
<p>The problem we now have is called <a href="http://justinjaffray.com/what-does-write-skew-look-like/">write skew</a>. Our reads from the inventory view can be out of date by the time the checkout event is processed. If two users try to buy the same item at nearly the same time, they will both succeed, and we won’t have enough inventory for them both.</p>
<p>Event-sourced architectures like these suffer many such isolation anomalies, which constantly gaslight users with “time travel” behavior that <a href="https://www.google.com/search?q=facebook+unread+notification+glitch">we’re all familiar with</a>. Even worse, research shows that anomaly-permitting architectures create outright security holes that allow hackers to steal data, as covered in <a href="https://www.cockroachlabs.com/blog/acid-rain/">this excellent blog post</a> on <a href="http://www.bailis.org/papers/acidrain-sigmod2017.pdf">this research paper.</a></p>

<p>These problems can be avoided if you use Kafka as a <em>complement</em> to a traditional database:</p>
<p><img src="https://materialize.com/wp-content/uploads/2020/12/kafka_oltp_flow.png" data-src="https://materialize.com/wp-content/uploads/2020/12/kafka_oltp_flow.png" alt="kafka used for Change Data Capture from an OLTP database" width="960" height="658" data-srcset="https://materialize.com/wp-content/uploads/2020/12/kafka_oltp_flow.png 960w, https://materialize.com/wp-content/uploads/2020/12/kafka_oltp_flow-300x206.png 300w, https://materialize.com/wp-content/uploads/2020/12/kafka_oltp_flow-768x526.png 768w" data-sizes="(max-width: 960px) 100vw, 960px" srcset="https://materialize.com/wp-content/uploads/2020/12/kafka_oltp_flow.png 960w, https://materialize.com/wp-content/uploads/2020/12/kafka_oltp_flow-300x206.png 300w, https://materialize.com/wp-content/uploads/2020/12/kafka_oltp_flow-768x526.png 768w"></p>
<p>OLTP databases perform a crucial task that message brokers are not well suited to provide: admission control of events. Rather than using a message broker as a receptacle for “fire and forget” events, forcing your event schema into an “intent pattern”, an OLTP database can <em>deny</em> events that conflict, ensuring that only a single consistent stream of events are ever emitted. OLTP databases are really good at this core <strong>concurrency control</strong> task – scaling to many millions of transactions per second.</p>
<p>Using a database as the point-of-entry for writes, the best way to extract events from a database is via streaming <strong>change-data-capture</strong>. There are several great open CDC frameworks like <a href="http://debezium.io/">Debezium</a> and <a href="http://maxwells-daemon.io/">Maxwell</a>, as well as native CDC from <a href="https://www.cockroachlabs.com/docs/stable/change-data-capture.html">modern</a> <a href="https://docs.oracle.com/cd/B28359_01/server.111/b28313/cdc.htm">SQL</a> <a href="https://docs.yugabyte.com/latest/architecture/docdb-replication/change-data-capture/">databases</a>. Change-data-capture also sets up an elegant operational story. In recovery scenarios, everything can be purged downstream and rebuilt from the (very durable) OLTP database.</p>

<p>The database community has learned (and re-learned) several important lessons over decades. Each one of these lessons was obtained at the high prices of data corruption, data loss, and numerous user-facing anomalies. The last thing you want to do is to find yourself relearning these lessons because you <a href="https://www.oreilly.com/library/view/strata-hadoop/9781491944608/video244677.html">accidentally misbuilt a database</a>.</p>
<p>Real-time streaming message brokers are a great tool for managing high-velocity data. But you will still need a traditional DBMS for isolating transactions. The best reference architecture for “turning your database inside out” is to use OLTP databases for admission control, use CDC for event generation, and model downstream copies of the data as materialized views.</p>

<p>If you’re interested in getting fully consistent views of your Kafka data updated in realtime, <a href="https://materialize.com/quickstart/">try Materialize out</a>&nbsp;to see if it’s the right solution for you!</p>
<div><h3>Subscribe to our Newsletter</h3>
        
        

        </div></div></div>]]>
            </description>
            <link>https://materialize.com/kafka-is-not-a-database/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25346851</guid>
            <pubDate>Tue, 08 Dec 2020 15:52:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Little Tasks, Little Trust]]>
            </title>
            <description>
<![CDATA[
Score 230 | Comments 81 (<a href="https://news.ycombinator.com/item?id=25346460">thread link</a>) | @aard
<br/>
December 8, 2020 | http://adamard.com/little_tasks.html | <a href="https://web.archive.org/web/*/http://adamard.com/little_tasks.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <h2>{<br>&nbsp;&nbsp;&nbsp;&nbsp;Adam Ard<br>}</h2><br>
  <h2>Little Tasks, Little Trust</h2>
  <h3>How Delegating Responsibilities is Better Than Managing By Tasks</h3>
  <blockquote>
    Trust is the highest form of motivation. It brings out the very
    best in people.
  </blockquote>
  <blockquote>
    --Stephen R. Covey, The 7 Habits of Highly Effective People
  </blockquote>
  <p>
It’s basically a sacred project management mantra by
now: <strong>divide your work into tasks that are as small as
possible</strong>. Estimate them with your team. Then drop them into
the all-knowing product backlog. But no one seems to be looking very
critically at how this practice has affected the software engineering
profession. Back in the 90’s, when I started programming, this was not
how it worked. It was, dare I say, a little more
professional.</p><p>Back then, your boss had a dozen or more things
they were responsible for and when you got hired they breathed a deep
sigh of relief.</p><p>“Finally, now that Vincent is here, I can make
him do A and B; and with Ted doing C, D, and E and Jan doing F, G, and
H, I can finally get to I, J, K, L, and M.”</p><p>Most importantly, A
and B were big things, like whole products or large system
libraries. Building and maintaining them consumed all of your
time. They were your delegated responsibilities, not mere tasks. It
wasn’t that hard to manage people this way either. If you weren’t
doing a good job, the boss would let you know.</p><p>“Hey, Vincent, A
isn’t turning out quite like I was imagining. Can you do a little more
of this — and definitely more cowbell.”</p><p>Then you went back to
your private office (I sure miss private offices, but that is a topic
for another day) and fixed a few things. Later, in a weekly status
meeting you would tell people how it went — yep, that’s right, no
daily stand-ups where you look mournfully at your shoes and admit that
you didn’t make any notable progress yet.</p><p>No backlog grooming
meetings or burn-down charts either. Your manager simply looked at
how <em>your </em>products were coming along. A little trust, some
accountability, and a healthy portion of “give me some space to do my
work.”</p><p>The way we work now is different. Sadly, it’s less
motivating, less efficient, and profoundly less respectful of
individual abilities.</p><h3>whose vision is it again?</h3><p>As I
see it, little tasks are born of a fundamentally different management
attitude. Small tasks are a not-so-subtle way of saying that all the
product vision lies with management. Keep away. Don’t
touch.</p><p>While larger tasks send a completely different
message. Here, management is giving you bigger pieces to chew on,
inviting you to mix some of your own creativity with the end
product. You get to do some design work, think about what the customer
needs, and take pride in what comes out the other end. Sure, the
organization has an overarching strategy, but they still want people
to take on responsibilities, not just errands. They trust you to align
with the overall vision and because you feel like you are part of the
“club”, you actually want to.</p><h3>too much love for the
metrics</h3><p>Small tasks have also taken hold because they fit
nicely with the age-old assembly-line mentality. Sadly, there are
armies of managers that still live by that dogma. For them, it is all
about picking <em>metrics and optimizing them </em>— management by
chart and graph.</p><p>Unfortunately, little tasks in Jira (or any of
the dozens of other issue tracking systems out there) bring the
promise of a whole host of tasty new charts and graphs: burn-down,
burn-up, velocity, lead-time, cycle time, task age, throughput, failed
deployment, flow and control. It’s as irresistible as candy to a
baby.</p><p>But, assigning responsibilities instead of tasks takes
away an assembly-line manager’s favorite tools. Because they are
larger, responsibilities can’t be so easily measured and tracked. So
metrics-managers will fight both tooth and nail to keep your work divided
and cataloged in tiny, traceable instructions.</p><h3>when will I get some design
experience?</h3><p>Sadly, as developers, we do it to ourselves as
well. Once someone gives us a better title, we are right on board with
the program. When a regular developer might have had a chance to do
some research or design, we immediately snatch it away for
ourselves.</p><p>As technical management, we standardize our
frameworks, languages, deployment operating systems, and cloud service
providers. We write wrappers around networking, logging, and
monitoring libraries and demand they always be used. Then, after we
have taken the task of designing and researching the CI/CD tools and
pipeline, we write coding standards for our coding
standards.</p><p>Worse yet, we design every product’s architecture,
and expect any deviation to be approved by us first. All that is left
are tiny morsels. Grunt work for the foot soldiers once the fun has
been stripped away.</p><p>Poor front-line programmers are left
wide-eyed with empty bowls, asking “please sir, can I have some more?
I just wanted to design one little service. I know I am not worthy,
but can I please just write my own sql queries without using that
awful ORM?  PLEASE?”</p><p>Sadly, when those poor programmers finally
seek promotion, hoping for their first real shot at higher level
engagement, they are rebuffed: “You don’t really have any design
experience I am afraid. We are looking for someone who has designed
large systems.”</p><p>To their managers they could rightfully reply,
“that was your doing, not mine!”</p><h3>estimation is never
free</h3><p>There is a grave misconception circulating that if you
just sit down, in a comfy conference room chair, and split a project
into tiny tasks, small enough to be individually estimated,
then when you add them up, Viola! You’ll have a accurate
estimate for the whole project! Easy peasy.</p><p>There are two
problems with this delusion. First, no task, even a small one,
is easy to estimate. I have seen many “tiny”, one-day tasks blow up
into week long campaigns. All because of hidden complexity that comes
popping out like a Pandora’s box once you start coding on
it.</p><p>Second, when you divide work into little tasks, before
actually working on any of them, you are making untested
assumptions. Many of them. The more tasks you define, the more facets
of a hypothetical design you must assume (implicitly of course, since
no one ever writes design assumptions in task descriptions). Soon,
you’ve created a long chain of design choices, all depending on
previous ones, sitting on sticky notes on the wall.</p><p>The problem
is, as soon as you start working on one of them, you will
realize that your implicit design decisions are wrong. Now, you will spend MUCH more time than was
previously estimated for this task and all other tasks that depend on
its faulty design are invalid. The whole house of cards comes
tumbling down. Time for another all-day backlog grooming session? What
a waste!</p><h3>conclusion</h3><p>Back in the day, before everyone
realized that software companies were positioned to make lots of
money, we had some elbow room. We had a lot of responsibility and the
ability to make a lot of decisions. But now, a lot more people have
piled onto the island. Unfortunately, some of them have slowly chipped
away at the domain of the software engineer. One by one, descending
from their vessels they planted their flags:</p><p>“I am Amerigo, the
product guy. Heretofore, no developer will make product decisions, for
they are mine.”</p><p>“And I am Ferdinand, process guy. Heretofore,
no developer will make process decisions, for they are
mine.”</p><p>“I Bartolomeu will enforce compliance.”</p><p>“I Vasco
used to be pretty good at Microsoft Access, I guess I’ll be the
database guy.”</p><p>One by one, until every responsibility that
wasn’t actual open-up-emacs-and-start-typing-stuff programming was
taken away, forbidden even. And then, the remaining, purely technical
tasks were carved up by architecture/standard hoarding engineers,
hungry for something of substance. Only dry, broken carcasses were
left scattered on the ground.</p><p>Of course, there is a solution to
this predicament — delegate <em>responsibilities</em> to everyone,
all the way down to the bottom of the hierarchy. Or better yet,
flatten or abolish the hierarchy all together. But until that
happens, you’ll just have to content yourself with measly for-loops
and if-statements — following the coding standard of
course.</p></div></div>]]>
            </description>
            <link>http://adamard.com/little_tasks.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25346460</guid>
            <pubDate>Tue, 08 Dec 2020 15:29:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[When Humor Becomes Horror]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25346392">thread link</a>) | @leoschwartz
<br/>
December 8, 2020 | https://restofworld.org/2020/when-humor-becomes-horror/ | <a href="https://web.archive.org/web/*/https://restofworld.org/2020/when-humor-becomes-horror/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
<p><span>F</span>eel-good moments are difficult to come by on Twitter in Pakistan. In between the political infighting and trolls, though, there was a brief moment where the ugliness the platform often brings out in people gave way to the app’s potential for humanity.&nbsp;</p>



<p>In October, a Karachi school teacher, Aimun Faisal, tweeted questions from her students about space and space travel. “Do you get scared that your space shuttle might get lost?” one 10-year-old asked. Ms. Faisal tagged NASA and other space agencies in the post, encouraging readers to retweet it and bring it to their attention. <a href="https://gulfnews.com/world/asia/pakistan/nasa-astronauts-scientists-answer-pakistani-fourth-graders-science-questions-on-twitter-1.1602755591289">It worked</a>.</p>



<p>A Canadian astronaut who has flown in a space shuttle twice replied that he wasn’t scared because Earth was nearby, and he could use the stars to find his way. “I felt especially comforted when I flew over home,” <a href="https://twitter.com/Cmdr_Hadfield/status/1316390465277767682?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1316390465277767682%7Ctwgr%5E%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fgulfnews.com%2Fworld%2Fasia%2Fpakistan%2Fnasa-astronauts-scientists-answer-pakistani-fourth-graders-science-questions-on-twitter-1.1602755591289">he tweeted</a>. “Here’s a photo I took of Karachi — can you find your school?”</p>



<p>It’s the kind of interaction that leads one to believe that perhaps all you need in life is a Twitter account and a heart that still beats. The media ate it up too: Ms. Faisal and her tweet were in all the major newspapers and even on TV. We were gushy over Ms. Faisal; we wished there were more teachers like her.&nbsp;</p>



<p>Just a few weeks later, people were suggesting that Ms. Faisal should go jump into a river or move to another country.&nbsp;</p>



<p>It all turned after — what else? — a series of tweets.&nbsp;</p>



<p>One evening this past September, a woman set out in her car on a motorway outside Lahore, her two children in tow. Motorways in Pakistan are safer than most roads there; they have their own police, who have a reputation, unlike other police units, for being well-dressed and resistant to corruption.&nbsp;</p>



<p>That evening, the woman’s car ran out of fuel. She locked her car doors and made some phone calls, but the rescue service didn’t answer, and she couldn’t get through to her family. By now, it was past midnight. She made more calls and waited for a friend to come pick her up. While she waited, two men emerged from the darkness, broke her car window, and when she fought back, took her children into some nearby bushes. She scrambled to save them, and while she fought, she was gang-raped.&nbsp;</p>



<p>In the pre–social media age, if a rape story ever got more than a couple of column inches on the inside pages of a newspaper, the victim not only had asked for it but, by reporting it and making it public, was asking for something more.&nbsp;</p>



<p>In an interview with the <em>Washington Post</em>, Pakistan’s former president and army chief Pervez Musharraf (now absconded from the country) once said <a href="https://www.washingtonpost.com/archive/politics/2005/09/19/musharraf-denies-rape-comments/5f2e0d4c-5ff2-4273-81e5-878cd59a1744/">Pakistani women get themselves raped so they can get a Canadian visa</a>. He probably thought he was being funny or insightful. When his office denied his having said it, the <em>Post</em> made the audio public. You can hear one of his aides laughing in the background.&nbsp;</p>



<p>Attitudes toward rape haven’t changed much since the Musharraf days, but there was something about the awfulness of a woman trapped with her children on a motorway at night that struck a chord with the Pakistani public. As the news rippled through Twitter, #LahoreMotorwayIncident became a trending hashtag. The details of the crime were so grotesque that, for a few days, it <a href="https://www.bbc.com/news/world-asia-54186609">seemed the nation actually believed that rape was a crime</a> and that the woman hadn’t asked for it.&nbsp;</p>



<p>Pakistan’s traditional and social media go into this kind of outrage cycle when children get raped, or get raped and then killed, which happens more often than one can stomach. There was genuine fury across the country when a 7-year-old girl was raped, strangled, and left in a rubbish heap two years ago. <a href="https://www.dawn.com/news/1439587">Her name was Zainab</a>. On Twitter and Facebook, users argued over whether her rapist and killer should be hanged in public, quartered, doused in acid, or just locked up for life.</p>



<p>After the motorway incident, the usual bloodlust was on display. There were calls for public hangings and for chopping up the culprits. Prime Minister Imran Khan even appeared on a TV interview saying <a href="https://www.cnn.com/2020/09/16/asia/imran-khan-chemical-castration-rapists-intl/index.html">he has been contemplating chemical castration as a punishment for such crimes</a>.</p>


		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/12/GettyImages-1228489392-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/12/GettyImages-1228489392-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/12/GettyImages-1228489392-400x267.jpg 400w, https://restofworld.org/wp-content/uploads/2020/12/GettyImages-1228489392-600x400.jpg 600w, https://restofworld.org/wp-content/uploads/2020/12/GettyImages-1228489392-1000x667.jpg 1000w, https://restofworld.org/wp-content/uploads/2020/12/GettyImages-1228489392-1600x1067.jpg 1600w, https://restofworld.org/wp-content/uploads/2020/12/GettyImages-1228489392-2800x1867.jpg 2800w, " sizes="(max-width: 640px) 100vw, 600px(max-width: 992px) calc(100vw - 40px), (max-width: 1140px) calc(100vw - 290px), calc(100vw - ((100vw - 640px)/2))" alt="People protesting in Karachi after the gang rape of a woman on a motorway near Lahore.">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				<span itemprop="copyrightHolder">Rizwan Tabassum/AFP via Getty Images</span>
			</figcaption>
		</figure>


<p>And while the Lahore rapists were still on the loose, the newly appointed Lahore police chief appeared on one of the nightly news shows during the wall-to-wall media coverage. In effect, he said he was surprised that a mother of two had decided to take the motorway when another road would have sufficed for her journey. And if she had to take the motorway, he went on to ask, couldn’t she at least have checked her gas tank?&nbsp;</p>



<p>Ms. Faisal, the teacher we had fawned over just weeks prior for her tweets about space travel, was outraged. On Twitter, she was one of many to demand the police chief’s resignation, tweeting in disgust at the craven responses to the news she and others were seeing, rhetorical questions as to why the woman had failed to prevent her own assault.&nbsp;</p>



<p>In return, our beloved Ms. Faisal was met with a flood of insults; trolls suggested she leave Pakistan if she had a problem with it. Being stuck on the motorway without gas became an internet meme that small-time politicians and social media trolls lobbed at one another. Somewhere along the way, “Why didn’t she check her petrol tank before leaving her home?” replaced the original question: “What kind of beasts would rape a woman in front of her children?”&nbsp;</p>



<p>How did this horror morph into a meme? Does every one of us carry inside us a bit of General Musharraf, smirking while his aides giggle?&nbsp;</p>



<p>Long before we had social media, Pakistan swam through its sea of miseries on the backs of jokes. The mausoleum of Pakistan’s longest-serving dictator, General Zia’ul Haq, is jokingly referred to as Jaws Square. Guess why? Because, after ruling over us for 11 years, he died in a plane crash, and only his teeth survived.&nbsp;</p>



<p>Jokes back then went viral before we ever began to call them “viral.” You would hear one about the dictator in Urdu in Karachi one day, and the next day in Peshawar, someone would tell you it in Pashto. Jokes eased us out of our terror.&nbsp;</p>



<p>But now the jokes themselves have become the terror. You can make a joke about a dead dictator, or Osama bin Laden becoming fish food, but how do you laugh at a joke about a woman raped in front of her children on the side of the motorway in the dead of night? Jokes were meant to subvert the powerful, not kick the poor to the ground.&nbsp;</p>



<p>For a brief moment, social media in Pakistan made it easy to laugh at despots and oppressors. They answered with troll farms, indoctrinating the youth to be subservient to power. Even in the midst of incidents so nauseating that they capture the attention of our fractured social media, the horrors of which seem undeniable, the trolls find ways to mock and belittle.&nbsp;</p>



<p>In November, another shocking piece of news roiled Pakistan’s social media: A 4-year-old rape survivor told her gut-wrenching story in a video that went viral. It was so harrowing, I couldn’t bring myself to watch it, even for reporting purposes. In another video, also shared widely online, the doctor who treated her after the assault broke down crying.&nbsp;</p>



<p>On Twitter, it’s easy to find redemption in the narrative too. In the middle of the outrage cycle, social media made a hero of the policeman <a href="https://www.gulftoday.ae/news/2020/11/14/pakistani-policeman-uses-own-daughter-as-bait-to-nab-rapists">who had found the girl’s rapist by enlisting his own daughter to lure him</a>. The rapist was killed in a police encounter.&nbsp;</p>



<p>The policeman and his underage daughter were forced to take this extreme step because there were no women officers in his district. The prime minister called to congratulate the man, and his daughter was awarded a million rupees as a prize along with a guaranteed college scholarship.&nbsp;</p>



<p>Ms. Faisal, who by now has endured a litany of abuses online, questioned the collective satisfaction on social media. As proud as we are of the police officer, she tweeted, the story said more about the country’s broken system than it did about heroism. “No police officer should be forced to risk his family to perform his duty,”<a href="https://twitter.com/bluemagicboxes/status/1327209635594588165"> she wrote</a>.</p>



<p>“Tum kabhi khush na hona,” someone tweeted back. “You’ll never be happy.”</p>
		</div></div>]]>
            </description>
            <link>https://restofworld.org/2020/when-humor-becomes-horror/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25346392</guid>
            <pubDate>Tue, 08 Dec 2020 15:25:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[FedEx and UPS hit companies with unexpected holiday shipping limits]]>
            </title>
            <description>
<![CDATA[
Score 209 | Comments 198 (<a href="https://news.ycombinator.com/item?id=25346386">thread link</a>) | @mooreds
<br/>
December 8, 2020 | https://www.modernretail.co/retailers/this-is-going-to-ruin-us-fedex-and-ups-hit-companies-with-unexpected-holiday-shipping-limits/ | <a href="https://web.archive.org/web/*/https://www.modernretail.co/retailers/this-is-going-to-ruin-us-fedex-and-ups-hit-companies-with-unexpected-holiday-shipping-limits/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Johnny Galbraith, co-founder of Salt Lake City-based e-commerce company Letterfolk, knew that it was going to be difficult to get orders to customers in a timely manner this holiday season, with more people ordering online than ever before.</p><p>He thought he had come up with a solution: to hold his Black Friday sale a week earlier this year. That way, he would be shipping out his products before carriers like FedEx — which he estimates ships 75% of Letterfolk’s packages — would be under the most strain.</p><div id="piano-cta">
<p>But on the Saturday before Thanksgiving, Galbraith got a call from his FedEx representative: through the holidays, FedEx would only pick up 110 Letterfolk packages per day — something that has never happened to Galbraith in the five years he’s been in business. It was a number that was calculated based on the average number of packages FedEx was picking up from Letterfolk per day in September, plus 10%. But thanks to his early Black Friday sale, Galbraith had a queue of 3,000 orders waiting to go out.</p>

<p>“It was like, ‘this is going to ruin us as a business,'” said Galbraith, whose company sells letterboards and other home decor. “We are being told that just to ship out our existing queue — without any other orders coming it — it would take us up until the holidays to get them out to customers.”</p>
<p>Carriers like FedEx and UPS have been warning for months to <a href="https://www.wsj.com/articles/this-holiday-crunch-starts-early-with-more-packages-than-means-to-deliver-them-11603013401">expect holiday shipping delays</a>. But some retail executives, like Galbraith, say they were not given advanced notice by carriers that they could face package pickup limits this holiday. It’s an issue that’s affecting companies both big and small — the Wall Street Journal <a href="https://www.wsj.com/articles/ups-slaps-shipping-limits-on-gap-nike-to-manage-e-commerce-surge-11606926669">reported</a> last week that UPS had temporarily stopped accepting shipments from major retailers like Nike and Macy’s.</p>
<p>Some executives who spoke with Modern Retail said that they feel like their carriers did not give them enough advance notice before applying a limit, and are frustrated by the fact that limits seem to be arbitrarily applied. In the meantime, they’re scrambling to find often expensive alternatives, including renting their own trucks or trying to find space with other carriers.</p>
<p>Representatives from FedEx and UPS would not provide details to Modern Retail about how they calculate package pickup limits.</p>
<p>FedEx has been “proactively working with our customers to understand their expected volume,” a spokeswoman told Modern Retail, and said the company has been preparing for the holidays by hiring more than 70,000 seasonal workers this year, and for the first time, picking up packages seven days a week.</p>

<p>“<span>UPS</span>&nbsp;continues to work closely with our largest customers to steer volume to capacity and ensure the&nbsp;<span>UPS</span> network is reliable for all customers,” a UPS spokesman said. “Agreed upon strategies for our largest customers include shifting package volume away from the heaviest demand shipping days, fully utilizing weekend capacity, and aligning promotional strategies with capacity.”</p>
<p><strong>Making their own deliveries<br>
</strong>For the most part, UPS and FedEx first started imposing shipping limits after Black Friday. Helena Price Hambrecht, founder of aperitif brand Haus, <a href="https://www.nytimes.com/2020/12/05/business/ecommerce-shipping-holiday-season.html?referringSource=articleShare">told the New York Times</a> that FedEx informed her that starting on Wednesday, its drivers would only pick up 500 packages from Haus per week day, through the holidays. FedEx however, did give Haus a larger cap on the weekends. In a follow-up email to Modern Retail, Price Hambrecht did not say exactly when FedEx informed her of the limit, only that “FedEx has been great about communicating changes to us as soon as possible.”</p>
<p>David Malka, chief sales officer of returns processing company goTRG — which works with major retailers like Target, Walmart and Lowe’s to process some of their returns — told Modern Retail that shipping limits of roughly 200 packages per day had been imposed at four of the company’s warehouses. Though he added that the company is getting “conflicting reports” from FedEx about what exactly the limit is at some facilities. Malka, like Gilbraith said that goTRG received no advanced notice from FedEx that package pickup limits would be applied.</p>
<p>The most immediate way that retailers are trying to deal with pickup limits imposed by UPS or FedEx is to redirect some shipments to another carrier. Shortly after Galbraith’s FedEx representative informed him of Letterfolk’s new limit, he immediately set up deliveries through UPS — something he said he hadn’t done previously because he had a good relationship with FedEx.</p>
<p>That’s helped Letterfolk get through some of the 3,000 order backlog, though its <a href="https://www.letterfolk.com/pages/holiday-shipping-info">website is still advising customer</a>s that most orders placed during its Black Friday sale will ship within one to three business weeks. However, he said that it’s more expensive right now for Letterfolk to ship through UPS, because his company didn’t have time to negotiate discounted rates.</p>
<p>Some companies are setting up other contingency plans to ensure their packages get into customers hands more quickly. Price Hambrecht told Modern Retail that Haus will be relying on Ohi, a last-mile delivery provider, to deliver more of its packages in the Bay Area and Southern California.</p>
<p>Jay Sauceda, founder of Austin-based 3PL Sauceda Industries, said that his company rented its own truck for the first time for the holidays. Sauceda — which works with companies including apparel brand Rowing Blazers and bra startup Pepper — has yet to be hit with a package pickup per day limit. But if it does, the plan then is to load packages onto the truck and deliver them directly to a FedEx or UPS sorting facility.</p>
<p>Sauceda is also advising its customers to try and encourage customers to get orders in two to three days ahead of shipping cutoffs given by FedEx, UPS and other carriers to ensure packages arrive in time for Christmas and other key dates.</p>
<p>“[The carriers] are in a very unenviable position,” Sauceda said. “So the biggest thing that we are trying to do this year to try to take the tension off.”</p>
<p><em>This story has been updated to clarify that FedEx has capped the number of Haus packages it will pick up at 500 per week day, not per day as previously stated.&nbsp;</em></p>
</div></div>]]>
            </description>
            <link>https://www.modernretail.co/retailers/this-is-going-to-ruin-us-fedex-and-ups-hit-companies-with-unexpected-holiday-shipping-limits/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25346386</guid>
            <pubDate>Tue, 08 Dec 2020 15:25:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Adaptive Request Concurrency: Resilient Observability at Scale]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25346094">thread link</a>) | @zhs
<br/>
December 8, 2020 | https://vector.dev/blog/adaptive-request-concurrency/ | <a href="https://web.archive.org/web/*/https://vector.dev/blog/adaptive-request-concurrency/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p>Observability pipelines have become critical infrastructure in the current technological landscape, which is why we've built <a href="https://vector.dev/">Vector</a> to provide extremely high throughput with the tiniest resource footprint we can manage (<a href="https://rust-lang.org/">Rust</a> is a huge help here). But this is not enough in the real world: your observability pipeline needs to provide optimal performance and efficiency while <em>also</em> being a good infrastructure citizen and playing nicely with services like <a href="https://vector.dev/docs/reference/sinks/elasticsearch">Elasticsearch</a> and <a href="https://vector.dev/docs/reference/sinks/clickhouse">Clickhouse</a>.</p><p>And so we're excited to announce that Vector version 0.11 includes support for <strong>Adaptive Request Concurrency</strong> (ARC) in all of its HTTP-based <a href="https://vector.dev/docs/reference/sinks">sinks</a>. This feature does away with static rate limits and automatically optimizes HTTP concurrency limits based on downstream service responses. The underlying <a href="#how-it-works">mechanism</a> is a feedback loop inspired by TCP congestion control algorithms.</p><h2>The lead-up</h2><p>One of the most common support questions we get about Vector involves logs like this:</p><div><div><pre><div><p><span>TRACE tower_limit::rate::service: rate limit exceeded, disabling service</span></p></div></pre></div></div><p>Users typically have two questions about this:</p><ol><li>What does it mean?</li><li>How can I fix it?</li></ol><p>The answer to the first question is simple: Vector has <em>internally</em> rate-limited processing to respect user-configured limits—<a href="https://vector.dev/docs/reference/sinks/http/#rate_limit_duration_secs"><code>request.rate_limit_duration_secs</code></a>, <a href="https://vector.dev/docs/reference/sinks/http/#rate_limit_num"><code>request.rate_limit_num</code></a>, and <a href="https://vector.dev/docs/reference/sinks/http/#in_flight_limit"><code>request.in_flight_limit</code></a>—for that particular <a href="https://vector.dev/docs/reference/sinks">sink</a>. In other words, Vector has intentionally reduced performance to stay within static limits.</p><p>The answer to the second question—how to fix it—is more complex because it depends on a variety of factors that change over time (covered in more detail <a href="#the-second-problem-rate-limiting-is-not-a-panacea">below</a>). Telling the user to raise their limits would be irresponsible since we'd then risk overwhelming the downstream service and causing an outage; but not changing them could mean limiting performance in a dramatic way.</p><blockquote><p>"In one case, we found that rate limits were limiting performance by over 80%."</p></blockquote><p>The crux of the matter is that Vector's high throughput presents a major challenge for HTTP-based sinks like <a href="https://vector.dev/docs/reference/sinks/elasticsearch">Elasticsearch</a> because those services can't always handle event payloads as quickly as Vector can send them. And when data services are heavily interdependent—which is almost always!—letting Vector overhwelm one of them can lead to system-wide performance degradation or even cascading failures.</p><p>In versions of Vector prior to 0.11, you could address this problem by setting <a href="https://vector.dev/docs/reference/sinks/http/#rate-limits"><strong>rate limits</strong></a> on outbound HTTP traffic to downstream services. Rate limiting certainly <em>does</em> help prevent certain worst-case scenarios but customer feedback and our own internal QA has revealed that this approach also has deep limitations.</p><h2>The problem: rate limiting is not a panacea</h2><p>Rate limiting is nice to have as a fallback but it's a blunt instrument, a static half-solution to a dynamic problem. The core problem is that configuring your own rate limits locks you into a perpetual loop:</p><p><img src="https://vector.dev/img/rate-limiting-loop.png" alt="The rate limiting decision loop"></p><p>Within this vicious loop, you need to constantly avoid two outcomes:</p><ul><li>You set the limit too high and thus <strong>compromise system reliability</strong> by overwhelming your services. Time to lower the limit and re-assess.</li><li>You set the limit too low and <strong>waste resources</strong>. Your Elasticsearch cluster may be capable of handling more concurrency than you're providing—at least for now. Time to raise the limit and re-assess.</li></ul><p>Not only do you need to perform this balancing act on a per-sink basis and on each Vector instance—that may be a <em>lot</em> of application points in your system—but the optimal rate is an elusive target that shifts along with changes in a number of factors:</p><ul><li>The number of Vector instances currently sending traffic</li><li>The current capacity of downstream services</li><li>The volume of data you're currently sending</li></ul><p>These changes are especially pronounced in highly elastic environments, like <a href="https://kubernetes.io/">Kubernetes</a>, that are essentially <em>designed</em> to let you tweak cluster topologies, configuration, and much more with very little friction, which compounds the problem.</p><p>And don't forget, of course, that this chasing-the-dragon decision loop has its own cognitive and operational costs.</p><h2>The solution: Adaptive Request Concurrency</h2><p>We feel strongly that Vector's <strong>Adaptive Request Concurrency</strong> (ARC) feature provides a qualitatively better path than rate limiting. With ARC <a href="#the-role-of-configuration">enabled</a> on any given sink, Vector determines the optimal network concurrency based on current environment conditions and continuously re-adjusts in light of new information.</p><p>Here's how that plays out in some example scenarios:</p><table><thead><tr><th>Change</th><th></th><th>Response</th></tr></thead><tbody><tr><td><strong><em>You deploy more Vector instances</em></strong></td><td>➔</td><td>Vector automatically redistributes HTTP throughput across both current and new instances</td></tr><tr><td><strong><em>You scale up your Elasticsearch cluster</em></strong></td><td>➔</td><td>Vector automatically increases concurrency to take full advantage of the new capacity</td></tr><tr><td><strong><em>You scale your Elasticsearch cluster back down</em></strong></td><td>➔</td><td>Vector lowers concurrency to avoid any risk of destabilizing the cluster (while still taking full of advantage of the now-decreased bandwidth)</td></tr><tr><td><strong><em>Your Elasticsearch cluster experiences a temporary outage</em></strong></td><td>➔</td><td>Vector lowers concurrency dramatically and provides backpressure by <a href="https://vector.dev/docs/reference/sinks/http/#buffer">buffering</a> events</td></tr></tbody></table><p>With ARC, these scenarios require no human intervention. Vector quietly hums along making these decisions for you with a speed and granularity that rate limits simply cannot provide.</p><h2>How it works</h2><p>ARC in Vector is based on a decision-making process that’s fairly simple at a high level. When Vector POSTs data to downstream services via HTTP, it continuously keeps track of downstream service performance and uses that information to make precise concurrency decisions.</p><p>The diagram below shows Vector's decision chart:</p><p><img src="https://vector.dev/img/adaptive-concurrency.png" alt="The Adaptive Request Concurrency decision chart"></p><p>With ARC enabled, Vector watches for significant movements in two things: the round-trip time (RTT) of requests and HTTP response codes (failure vs. success).</p><ul><li>If the RTT is declining/constant and/or response codes are consistently successful (200-299), Vector sees 🟢 and increases the throughput linearly. This is the "additive increase" in AIMD.</li><li>If the RTT is increasing and/or response codes consistently indicate failure—codes like <code>429 Too Many Requests</code> and <code>503 Service Unavailable</code>—Vector sees 🟡 and exponentially decreases concurrency. This is the "multiplicative decrease" in AIMD.</li></ul><p>This decision tree is always active and Vector always "knows" what to do, even in extreme cases like total service failure.</p><h3>The role of configuration</h3><p>Vector never stops quietly making the linear up vs. exponential down decision in the background, and it works out of the box with zero configuration beyond enabling the feature, which is currently on an opt-in basis in version 0.11. You can enable ARC in an HTTP sink by setting the <a href="https://vector.dev/blog/adaptive-request-concurrency/request_concurrency"><code>request.concurrency</code></a> parameter to <code>adaptive</code>. Here's an example for a Clickhouse sink:</p><div><div><pre><div><p><span>[</span><span>sinks.clickhouse_internal</span><span>]</span><span></span></p><p><span></span><span>type</span><span> </span><span>=</span><span> </span><span>"clickhouse"</span><span></span></p><p><span></span><span>inputs</span><span> </span><span>=</span><span> </span><span>[</span><span>"log_stream_1"</span><span>,</span><span> </span><span>"log_stream_2"</span><span>]</span><span></span></p><p><span></span><span>host</span><span> </span><span>=</span><span> </span><span>"http://clickhouse-prod:8123"</span><span></span></p><p><span></span><span>table</span><span> </span><span>=</span><span> </span><span>"prod-log-data"</span><span></span></p><p><span></span><span>request.concurrency</span><span> </span><span>=</span><span> </span><span>"adaptive"</span></p></div></pre></div></div><p>There's also room for fine-tuning if you find yourself needing additional knobs:</p><ul><li><strong>Buffering</strong>. What happens when Vector needs to lower concurrency and thus throttle the output? What happens to data that needs to be sent later? Vector lets you choose between an on-disk and an in-memory <a href="https://vector.dev/docs/reference/sinks/http/#buffer">buffer</a> and to set a <a href="https://vector.dev/docs/reference/sinks/http/#max_size">max_size</a> for that buffer. The <code>memory</code> buffer is the default, which maximizes performance, but you can always choose <code>disk</code> if your use case requires stronger durability guarantees. As always, this can be configured on a per-sink basis.</li><li><strong>The adaptive concurrency algorithm itself</strong>. In general, you shouldn't need to adjust the algorithm, but in case you need to resort to that, there are three parameters available:<ul><li><code>decrease_ratio</code> — This determines how rapidly Vector lowers the limit in response to failures or higher latency.</li><li><code>ewma_alpha</code> — Vector uses an exponentially weighted moving average (EWMA) of past RTT measurements as a reference to compare with the current RTT. The <code>ewma_alpha</code> parameter determines how heavily new measurements are weighted compared to older ones.</li><li><code>rtt_threshold_ratio</code> — The minimal change in RTT necessary for the algorithm to respond and adjust concurrency; changes below that threshold are ignored.</li></ul></li></ul><p>The defaults should work just fine for these parameters in most cases, but we know that some scenarios may call for a highly targeted approach.</p><h2>How we built it</h2><p>The development process behind ARC was highly methodical and data-driven. To summarize:</p><ul><li>Customer feedback has pinpointed concurrency management as a pain point since very early in the life of Vector.</li><li>The initial foray in addressing the problem came in <a href="https://github.com/timberio/vector/blob/master/rfcs/2020-04-06-1858-automatically-adjust-request-limits.md">RFC 1858</a>, which called for a qualitatively better option for users and <a href="https://github.com/timberio/vector/blob/master/rfcs/2020-04-06-1858-automatically-adjust-request-limits.md#prior-art">gestured toward</a> some possible inspirations.</li><li>Our engineers ultimately opted for a solution deeply inspired by analogous work on the Netflix engineering team, which is beautifully summarized in the <a href="https://medium.com/@NetflixTechBlog/performance-under-load-3e6fa9a60581">Performance Under Load</a> piece on their blog. Our respective approaches utilize an <a href="https://en.wikipedia.org/wiki/Additive_increase/multiplicative_decrease">additive-increase/multiplicative-decrease</a> (AIMD) algorithm inspired by TCP <a href="https://en.wikipedia.org/wiki/TCP_congestion_control">congestion control</a> algorithms. We'll have a lot more to say about this here on the blog next week. If you want to see the in-depth discussion that drove this process, see GitHub <a href="https://github.com/timberio/vector/issues/3255">issue #3255</a> on the Vector repo. There you'll see a pretty epic back and forth within our engineering team along with a slew of visualizations. It's quite the read.</li><li>As Netflix is largely a Java shop and there was nothing immediately usable "off the shelf" in the Rust ecosystem, we needed to create our own Rust implementation, which you can see in the <a href="https://github.com/timberio/vector/tree/master/src/sinks/util/adaptive_concurrency">adaptive_concurrency</a> module. Of special importance is the concurrency <a href="https://github.com/timberio/vector/blob/master/src/sinks/util/adaptive_concurrency/controller.rs#L23-L31">Controller</a>, which is responsible for the linear up/exponential down decision that I alluded to above.</li><li>For testing, the team mostly relied on our in-house <a href="https://github.com/timberio/http_test_server">http_test_server</a>, a pretty straightforward but highly customizable HTTP server written in Go.</li></ul><p>It took several months, some hefty PRs, and even a handful of <a href="https://github.com/timberio/vector/pull/3671">dead ends</a>, but we think that both the process and the end result are wholly consistent with the fastidious approach we strive for in building Vector.</p><h2>More to come</h2><p>Next week, we'll follow up on this announcement with a post from Timber's <a href="https://github.com/bruceg">Bruce Guenter</a>…</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://vector.dev/blog/adaptive-request-concurrency/">https://vector.dev/blog/adaptive-request-concurrency/</a></em></p>]]>
            </description>
            <link>https://vector.dev/blog/adaptive-request-concurrency/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25346094</guid>
            <pubDate>Tue, 08 Dec 2020 15:06:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[StackHawk Launches Free Developer Plan for Application Security Testing]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25345114">thread link</a>) | @sevs
<br/>
December 8, 2020 | https://www.stackhawk.com/blog/free-plan-announcement/ | <a href="https://web.archive.org/web/*/https://www.stackhawk.com/blog/free-plan-announcement/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-id="795a82f" data-element_type="widget" data-widget_type="theme-post-content.default">
				<div>
			
<p>Today we are excited to announce the launch of StackHawk’s free Developer Plan. With this launch, engineering teams or individual developers can take ownership of security in the development and delivery of their applications. Read on to learn a bit more about why we are so excited to launch this, or jump right in to <a href="https://auth.stackhawk.com/signup">sign up for a free account</a>.</p>



<h3 id="h-application-security-s-new-paradigm">Application Security’s New Paradigm</h3>



<p>It is no secret that the speed of application delivery has rapidly accelerated over the past decade. Most companies are pushing to production several times per week, or even multiple times per day. Originally, security struggled to keep up, with the dated models of quarterly penetration tests or manual scans by security teams clearly <a href="https://www.stackhawk.com/blog/application-security-is-broken/">no longer cutting it</a>.</p>



<p>These legacy models were predicated on finding vulnerabilities in production, often awhile after code had been deployed. This is at best, inefficient, and at worst a security issue. Once a vulnerability was found, it then had to navigate internal silos to actually get a fix released.</p>



<p>Modern engineering teams, however, have not only accelerated delivery speed, but have also improved security in the process. Here is how.</p>



<h4 id="h-find-vulnerabilities-with-ci-cd-automation">Find Vulnerabilities with CI/CD Automation</h4>



<p>Companies using modern approaches check for security vulnerabilities on every pull request (or even every commit) in the same way that they run unit tests and integration tests. With automated testing in CI/CD, developers can catch vulnerabilities early in the software development lifecycle, and fix issues while they are still in the context of the code.</p>



<p>After an initial triage of any existing security bugs, application security tooling should at least be configured to break the build if any new high criticality vulnerabilities are identified. This doesn’t mean that every vulnerability should prevent a deploy to production, but that deploy should be done eyes wide open to the risk it presents.</p>



<h4 id="h-triage-and-initial-risk-decisions-live-with-developers">Triage and Initial Risk Decisions Live with Developers</h4>



<p>When a vulnerability is found, the developer(s) who were recently working on the application are best equipped to review the finding and make risk-based decisions. Should this block the deploy to production? Can this be added to a backlog and addressed later? Is this low enough risk that it isn’t worth fixing?</p>



<p>The individuals who are intimately involved in creating the application are best equipped as the first-line of defense for these decisions. Internal security teams are undoubtedly called upon for clarification and support (…and are often reviewing historical decisions as well), but modern teams are empowering developers to make these triage decisions themselves.</p>



<h4 id="h-fast-developer-fixes">Fast Developer Fixes</h4>



<p>When a security bug requires a fix, the responsibility for the fix squarely lives with the developer who introduced the vulnerability in modern teams. This individual is in the codebase, familiar with the context of the recent vulnerability addition. Not only are they best equipped to implement the fix, but by democratizing this responsibility across the team, no person or team bears the burden of interrupt driven work.</p>



<h3 id="h-taking-ownership-of-application-security">Taking Ownership of Application Security</h3>



<p>Ever since we embarked upon building StackHawk, we have been laser focused on building a tool for developers. The market is rife with security tools built for the CISO that no developer wants to use, and these tools typically start at six-figure contracts. StackHawk is different, and we are excited to equip engineers to own the security of their application. This shows up in our product, and with our new free Developer Plan, it also shows up in our pricing.</p>



<p>Now individuals or teams can start running application security tests against their first application for free. There is no longer an excuse not to be looking at the security of your application.&nbsp;</p>



<p>Getting started with StackHawk is easy, with most developers completing their first security test in under 20 minutes. You can get the full details in <a href="https://docs.stackhawk.com/">our docs</a>, but in short it is as simple as building a yaml config, pointing the scanner at your application, and taking a look at the results.&nbsp;<br><a href="https://auth.stackhawk.com/signup">Sign up</a> for a free account and give it a try today!</p>
		</div>
				</div></div>]]>
            </description>
            <link>https://www.stackhawk.com/blog/free-plan-announcement/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25345114</guid>
            <pubDate>Tue, 08 Dec 2020 13:59:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Qt 6.0]]>
            </title>
            <description>
<![CDATA[
Score 444 | Comments 355 (<a href="https://news.ycombinator.com/item?id=25344826">thread link</a>) | @milliams
<br/>
December 8, 2020 | https://www.qt.io/blog/qt-6.0-released | <a href="https://web.archive.org/web/*/https://www.qt.io/blog/qt-6.0-released">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                            

                            <p>
                                Tuesday December 08, 2020 by <a href="https://www.qt.io/blog/author/lars-knoll">Lars Knoll</a> | <a href="#commento">Comments</a>
                            </p>
                            
                            <p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p><span>I am really excited to announce today’s release of Qt 6.0. It is the first release of a new major version, and marks a major milestone for Qt. </span><strong><em>We started working on the initial ideas a few years ago, and since then, have put a massive effort into creating the next generation of Qt. </em></strong></p>
<!--more-->
<p><a href="https://www.qt.io/product/qt6/" rel="noopener" target="_blank"><span><img src="https://www.qt.io/hubfs/Qt%206%20email%20launch%20hero%201-png-1.png" alt="Qt 6"></span></a></p>

<p><span>Qt 5 has been a fantastic success over the years, and we have seen an enormous growth of our user base and Qt usage over eight years since we released Qt 5.0. But the world&nbsp; has undergone significant changes since 2012. Qt usage in embedded systems has skyrocketed, C++ has evolved, and new 3D graphics APIs have emerged. These are examples of factors that have directly affected Qt.</span></p>
<p><span>As a cross-platform framework, Qt needs to adjust to those changing requirements. We have managed to adapt very well to many of those requirements during the lifetime of Qt 5. However, maintaining full source and binary compatibility within the Qt 5 series made certain things impossible to fix within its lifetime. With Qt 6, we now have the opportunity to make changes and build Qt to be better suited for the years to come.</span><span></span></p>
<div>
<p>Thus, the mission of Qt 6 is to enable Qt to be the productivity platform for the future. Qt 6.0, as a major release of Qt, gave us a higher degree of freedom to implement new features, functionality, and better support today and tomorrow's requirements. Qt 6.0 is a continuation of the Qt 5 series, and we have focused on making migration non-disruptive for users. I published a <a href="https://www.qt.io/blog/2019/08/07/technical-vision-qt-6" rel="noopener">Qt 6 vision blog post</a> capturing those ideas around 18 months ago.</p>
<p>When creating Qt 6, we ensured that Qt’s core values have been adhered to and upheld, including:</p>
<ul>
<li>Its cross-platform nature, allowing users to deploy their applications to all desktop, mobile, and embedded platforms using one technology and from a single codebase</li>
<li>Its scalability from low-end, single-purpose devices to high-end complex desktop applications or connected system</li>
<li>Its world-class APIs and tools and <a href="https://doc.qt.io/" rel="noopener">documentation</a>, simplifying the creation of applications and devices</li>
<li>Its maintainability, stability, and compatibility, allowing users to maintain large codebases with minimal effort</li>
<li>Its large developer ecosystem with more than 1.5 million users</li>
</ul>
<p>Qt 6.0 is the first release of the Qt 6 series addressing new market demands while keeping the core values at the heart of what we do.</p>
<p>When developing Qt 6, we had an in-depth look at some of Qt's most central parts to identify how we could improve them. We discovered a couple of core focus areas that we invested considerable time in improving. Those areas include:</p>
<div>
<ul>
<li>Leveraging C++17</li>
<li>Next generation QML</li>
<li>New graphics architecture</li>
<li>Unified 2D and 3D for Qt Quick</li>
<li>CMake build system (with qmake still supported for applications)</li>
</ul>
</div>
<p>We have of course also spent time doing numerous improvements in other areas, too many to mention them all here, and I suggest you look at the more detailed <span><span><a href="https://wiki.qt.io/New_Features_in_Qt_6.0" rel="noopener">wiki page</a>. We are also hosting Meet Qt 6.0 webinar sessions covering the <a href="https://www.qt.io/events/meet-qt-6-1607340244" rel="noopener">Americas/EMEIA</a> and <a href="https://www.qt.io/events/meet-qt-6-1607339968" rel="noopener">EMEIA/APAC</a> time zones. But let's take a look at some of the highlights.</span></span></p>
</div>
<h3><span>C++17</span></h3>
<p><span><img src="https://www.qt.io/hs-fs/hubfs/cpp_logo-png.png?width=168&amp;name=cpp_logo-png.png" width="168" alt="C++ 17 in Qt 6" srcset="https://www.qt.io/hs-fs/hubfs/cpp_logo-png.png?width=84&amp;name=cpp_logo-png.png 84w, https://www.qt.io/hs-fs/hubfs/cpp_logo-png.png?width=168&amp;name=cpp_logo-png.png 168w, https://www.qt.io/hs-fs/hubfs/cpp_logo-png.png?width=252&amp;name=cpp_logo-png.png 252w, https://www.qt.io/hs-fs/hubfs/cpp_logo-png.png?width=336&amp;name=cpp_logo-png.png 336w, https://www.qt.io/hs-fs/hubfs/cpp_logo-png.png?width=420&amp;name=cpp_logo-png.png 420w, https://www.qt.io/hs-fs/hubfs/cpp_logo-png.png?width=504&amp;name=cpp_logo-png.png 504w" sizes="(max-width: 168px) 100vw, 168px"></span></p>
<p><span>With Qt 6 we now require a C++17 compatible compiler enabling the use more modern C++ language constructs when developing Qt and also allows for integration points on the API side.</span></p>
<h3><span>Core libraries and APIs</span></h3>
<p><span>Much work has gone into Qt Core, as it is the module that implements the most central parts of Qt. We've gone through many areas there and made improvements. To name some of the most central ones:</span></p>
<ul>
<li><span>The new <a href="https://www.qt.io/blog/property-bindings-in-qt-6" rel="noopener">property and binding system</a>: This system now brings the concept of bindings that made QML such a huge success in Qt 5 available from C++. </span></li>
<li><span>Strings and Unicode: With Qt 5, we started aligning Qt fully with Unicode, where we completed a lot of the work, but a few items remained that we now cleaned up for Qt 6. More details will come in a separate blog post later on.</span></li>
<li><span>QList has been a class that was often criticized in Qt 5, as it was heap allocating objects stored in there that were larger than a pointer, leading to pressure on heap allocation methods. In Qt 6, we changed this and unified QList and QVector into one class. See our <a href="https://www.qt.io/blog/qlist-changes-in-qt-6" rel="noopener">blog post about QList</a> in Qt 6 for details.</span></li>
<li><span>QMetaType and QVariant are fundamental to how our Qt’s meta-object system works. Signals and slots would not be possible without QMetaType and QVariant is required for dynamic invocations. Those two classes got an almost complete rewrite with Qt 6, and you can read about the details <a href="https://www.qt.io/blog/whats-new-in-qmetatype-qvariant" rel="noopener">here</a>.</span></li>
</ul>
<p><span>Other parts of Qt that are not related to graphics have also seen large changes. For example, Qt Concurrent has undergone an almost complete rewrite and now makes development of multi-threaded applications more effortless than ever. Qt Network has seen lots of clean-up and improvements. See this <a href="https://www.qt.io/blog/qt-network-in-qt-6" rel="noopener">blog post</a> for details.</span></p>
<h3><span>New graphics architecture</span><span>&nbsp;</span><span></span></h3>
<p><span>The graphics architecture of Qt 5 was very much dependent on OpenGL as the underlying 3D graphics API. While this was the right approach in 2012 when we created Qt 5, the market around us has changed significantly over the last couple of years with the introduction of Metal and Vulkan. We now have a large set of different graphics APIs that are commonly being used on different platforms. For Qt as a cross-platform framework, this, of course, meant that we had to adjust to this and ensure our users can run Qt on all of them with </span>maximum performance.</p>
<p><img src="https://www.qt.io/hs-fs/hubfs/image-png-Nov-03-2020-07-47-23-41-AM-png-2.png?width=367&amp;name=image-png-Nov-03-2020-07-47-23-41-AM-png-2.png" width="367" alt="New graphics architecture in Qt 6" srcset="https://www.qt.io/hs-fs/hubfs/image-png-Nov-03-2020-07-47-23-41-AM-png-2.png?width=184&amp;name=image-png-Nov-03-2020-07-47-23-41-AM-png-2.png 184w, https://www.qt.io/hs-fs/hubfs/image-png-Nov-03-2020-07-47-23-41-AM-png-2.png?width=367&amp;name=image-png-Nov-03-2020-07-47-23-41-AM-png-2.png 367w, https://www.qt.io/hs-fs/hubfs/image-png-Nov-03-2020-07-47-23-41-AM-png-2.png?width=551&amp;name=image-png-Nov-03-2020-07-47-23-41-AM-png-2.png 551w, https://www.qt.io/hs-fs/hubfs/image-png-Nov-03-2020-07-47-23-41-AM-png-2.png?width=734&amp;name=image-png-Nov-03-2020-07-47-23-41-AM-png-2.png 734w, https://www.qt.io/hs-fs/hubfs/image-png-Nov-03-2020-07-47-23-41-AM-png-2.png?width=918&amp;name=image-png-Nov-03-2020-07-47-23-41-AM-png-2.png 918w, https://www.qt.io/hs-fs/hubfs/image-png-Nov-03-2020-07-47-23-41-AM-png-2.png?width=1101&amp;name=image-png-Nov-03-2020-07-47-23-41-AM-png-2.png 1101w" sizes="(max-width: 367px) 100vw, 367px"></p>
<p><span>So while Qt 5 relied on OpenGL for hardware-accelerated graphics, the picture completely changes with Qt 6. All of our 3D graphics in Qt Quick is now built on top of a new abstraction layer for 3D graphics called RHI (Rendering Hardware Interface). RHI makes it possible for Qt to use the native 3D graphics API of the underlying OS/platform. So Qt Quick will now use Direct3D on Windows and Metal on macOS by default. For details, have a look at the <a href="https://www.qt.io/blog/graphics-in-qt-6.0-qrhi-qt-quick-qt-quick-3d" rel="noopener">blog post series about the RHI</a>.</span><span>&nbsp;</span></p>
<p><span>The OpenGL specific classes in Qt still exist, but are now moved out of QtGui in the <a href="https://doc.qt.io/qt-6/qtopengl-index.html" rel="noopener">QtOpenGL</a> module. We also added a new module called <a href="https://doc.qt.io/qt-6/qtshadertools-index.html" rel="noopener">QtShaderTools</a> to deal with the different shading languages of those APIs in a cross-platform way.</span></p>
<h3><span>Qt Quick 3D and Qt 3D</span></h3>
<p><span>Qt Quick 3D is a relatively new module. It seamlessly extends Qt Quick with 3D capabilities. With Qt Quick 3D, our focus was to create an API that is as easy to use as the existing parts of Qt Quick (for 2D user interfaces) while providing full support for creating complex 3D scenes. The main goal behind this effort has been to enable seamless integration between 2D and 3D content.</span><span></span></p>
<p><span>This module has seen significant improvements with Qt 6 that we wouldn’t have been able to do in the Qt 5 series. Most importantly it is now always using the RHI abstraction layer to make optimal use of the underlying graphics API and Hardware. Additionally, it now features a much deeper and more performant integration between 2D and 3D content, allowing you to place 2D items into a 3D scene. It also has vastly improved support for glTF2 and physics-based rendering, making it trivial to import assets created in other design tools. There are many other major improvements in the module, a more in-depth description can be found in a <a href="https://www.qt.io/blog/what-is-new-in-qt-quick-3d-6.0" rel="noopener">separate blog post</a>.</span></p>
<p><span>Qt 3D is now also based on top of the RHI abstraction layer, has seen some performance improvements and cleanups. You can find more details in two blog posts by our partner KDAB (<a href="https://www.kdab.com/qt-3d-changes-in-qt-6/" rel="noopener">here</a> and <a href="https://www.kdab.com/qt3d-renderer-qt6/" rel="noopener">here</a>).</span></p>
<h3><span>Desktop styling for Qt Quick</span></h3>
<p><span><img src="https://www.qt.io/hubfs/Qt%206%20Desktop%20Styling-png.png" alt="Desktop Styling in Qt 6"></span></p>
<p><span>When we created the set of controls for Qt Quick, our focus was to make them lightweight and performant. For that reason, they did not support desktop styling in Qt 5. However, in Qt 6, we found a way to make them look &amp; feel native on desktop operating systems. With 6.0, Qt Quick now supports native styling on both macOS and Windows. See this <a href="https://www.qt.io/blog/desktop-styling-with-qt-quick-controls" rel="noopener">blog post for details</a>.</span><span> Native look &amp; feel for Android and Linux already existed with the Material and Fusion styles in Qt 5. We are improving those for future Qt releases and are also planning to implement a native style for iOS.</span></p>
<h3><span>Interfacing with platform specific functionality</span></h3>
<p><span>Even with Qt offering most functionality required to develop your application platform-independently, there is sometimes a need to interface with platform-specific functionality. In Qt 5, we provided a set of add-on modules (QtX11Extras, QtWinExtras, QtMacExtras) to help with this purpose. But this full separation from the rest of Qt has led to a couple of architectural issues, inconsistencies and code duplication within Qt. In Qt 6, we made an effort to clean this up and fold the functionality offered by those add-on modules into platform specific APIs offered directly in Qt. This will make interfacing with OS/platform-specific APIs much easier in Qt 6. Have a look <a href="https://www.qt.io/blog/platform-apis-in-qt-6" rel="noopener">here</a>&nbsp;for more details.</span></p>
<h3><span>Build system and&nbsp;</span><span>Packaging</span></h3>
<p><span><img src="https://www.qt.io/hs-fs/hubfs/600px-Cmake-svg-png.png?width=189&amp;name=600px-Cmake-svg-png.png" width="189" srcset="https://www.qt.io/hs-fs/hubfs/600px-Cmake-svg-png.png?width=95&amp;name=600px-Cmake-svg-png.png 95w, https://www.qt.io/hs-fs/hubfs/600px-Cmake-svg-png.png?width=189&amp;name=600px-Cmake-svg-png.png 189w, https://www.qt.io/hs-fs/hubfs/600px-Cmake-svg-png.png?width=284&amp;name=600px-Cmake-svg-png.png 284w, https://www.qt.io/hs-fs/hubfs/600px-Cmake-svg-png.png?width=378&amp;name=600px-Cmake-svg-png.png 378w, https://www.qt.io/hs-fs/hubfs/600px-Cmake-svg-png.png?width=473&amp;name=600px-Cmake-svg-png.png 473w, https://www.qt.io/hs-fs/hubfs/600px-Cmake-svg-png.png?width=567&amp;name=600px-Cmake-svg-png.png 567w" sizes="(max-width: 189px) 100vw, 189px"></span></p>
<p><span>We also made some considerable changes in how we build and distribute Qt.&nbsp; Worth mentioning is that Qt 6 itself is now <a href="https://www.qt.io/blog/qt-6-build-system" rel="noopener">built using CMake</a>. This has also led to significant improvements for all our users that use CMake to build their projects. We will continue to support qmake for the lifetime of Qt 6, so there is no need to make any changes to your build system if you're using it, but we recommend to use CMake for all new projects.</span></p>
<p><span>Qt 6 also comes with a much smaller default package, and many of the add-ons are now distributed as separate packages through a package manager. This gives us more flexibility in adapting release schedules of add-ons to market requirements, allowing, for example, for more frequent feature releases as the core Qt packages or making them available for multiple Qt versions at the same time. In addition, we can use the package manager as a delivery channel for 3rd party content. And finally, it gives our users more flexibility as they can choose to download only what they really need. </span></p>
<p><span>Currently, we are using the existing Qt …</span></p></span></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.qt.io/blog/qt-6.0-released">https://www.qt.io/blog/qt-6.0-released</a></em></p>]]>
            </description>
            <link>https://www.qt.io/blog/qt-6.0-released</link>
            <guid isPermaLink="false">hacker-news-small-sites-25344826</guid>
            <pubDate>Tue, 08 Dec 2020 13:39:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Dataform (YC W18) is joining Google Cloud]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25344703">thread link</a>) | @oliver101
<br/>
December 8, 2020 | https://dataform.co/blog/dataform-is-joining-google-cloud | <a href="https://web.archive.org/web/*/https://dataform.co/blog/dataform-is-joining-google-cloud">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Today we're excited to announce that we've joined Google Cloud. Over the course of the past several months, our partnership with Google Cloud has deepened and we believe that our new combined efforts can make our customers and partners even more successful.</p><div><div><p>To our customers, users, supporters and contributors - today, we are extremely excited to share that Dataform is joining Google Cloud!</p>
<p>Before we jump into the story, let us quickly answer what we think will be the most important questions for existing and new users:</p>
<ul>
<li>Dataform Web will continue to be run and maintained by us, as part of Google Cloud.</li>
<li>Dataform Web will be free for all new users as of today, December 8th 2020.</li>
<li>Existing customers will be transitioned onto the free plan immediately.</li>
<li>Dataform Web will focus on support for BigQuery going forward.</li>
<li>Weâ€™ll be working closely with the rest of the Google Cloud Data Analytics team over the next year to deliver our vision for Dataform within Google Cloud.</li>
</ul>
<p>Three years ago we started Dataform with a mission to <strong>empower analysts to manage the entire flow of data in the warehouse, using a single, unified workflow</strong>. Throughout our journey weâ€™ve been focused on giving data analysts and data engineers the tools they need to take raw, messy data, transform it, and put it into the hands of their organization. Weâ€™ve only just scratched the surface of this problem and solving it will continue to be our core mission and focus going forward.</p>
<p>Weâ€™ve followed Googleâ€™s products and successes within analytics closely over the past few years, and always felt that they were a product leader when it came to cloud data warehousing, SQL, and now business intelligence; with products such as Looker that inspired us significantly in our own journey. After several conversations with the Google Cloud team it became clear that we are deeply aligned on the importance of serving analysts with the right tools and technology in order to fill what we all perceive as a missed opportunity in existing solutions.</p>
<p>At the same time, as a team of just 7, in a complex, competitive and rapidly changing market, we had more ideas than we had people or resources to accomplish. There has always been so much more we wanted to do each quarter than we could achieve. With the support of the BigQuery and Cloud Analytics teams and our combined thought leadership and efforts, we felt that together we could achieve something bigger than we could separately.</p>
<p>There will inevitably be some changes to the product and how we operate over the coming years, but one thing that will not change is our commitment to the principles that we feel have gotten us this far: working closely with our customers, making sure we continue to collect and act upon your feedback, and a laser focus on doing one thing and doing it well.</p>
<p>At Dataform we always felt we were delivering a unique and innovative product to our customers and the data community. With Googleâ€™s support, we have even more confidence in our ability to build and deliver a product that can truly change the way that all companies, large and small, work with data. </p>
<p>In the coming months weâ€™ll be able to share more details of our product roadmap. In the meantime please reach out to us if you have any questions. Thank you again for joining us on this journey. Weâ€™re uncomfortably excited about what we can accomplish together over the coming years!</p>
<p>Guillaume-Henri Huon &amp; Lewis Hemens, co-founders of Dataform</p></div></div></div>]]>
            </description>
            <link>https://dataform.co/blog/dataform-is-joining-google-cloud</link>
            <guid isPermaLink="false">hacker-news-small-sites-25344703</guid>
            <pubDate>Tue, 08 Dec 2020 13:24:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Are people with dark personality traits more likely to succeed?]]>
            </title>
            <description>
<![CDATA[
Score 213 | Comments 206 (<a href="https://news.ycombinator.com/item?id=25344640">thread link</a>) | @known
<br/>
December 8, 2020 | https://psyche.co/ideas/are-people-with-dark-personality-traits-more-likely-to-succeed | <a href="https://web.archive.org/web/*/https://psyche.co/ideas/are-people-with-dark-personality-traits-more-likely-to-succeed">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p><strong>â€˜Darkâ€™ personalities come in</strong> various shades, but at the core of all of them is a tendency to callously use others for personal gain. What is it that these types of people are really gaining, though? Might a benevolent approach to life and others be even more advantageous?</p>
<p>For <span>15 years,</span> research into dark personality traits (including narcissism, psychopathy and Machiavellianism) has been rapidly <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/spc3.12018" rel="nofollow noreferrer noopener">expanding</a>. We now know that these traits are far more <a href="https://europepmc.org/article/med/32484407" rel="nofollow noreferrer noopener">evident</a>, on average, in men than women. We know that <a href="https://pubmed.ncbi.nlm.nih.gov/19243821/" rel="nofollow noreferrer noopener">approximately</a> <a href="https://psycnet.apa.org/record/2008-13625-018" rel="nofollow noreferrer noopener"><span>1-2 per cent</span></a> of individuals in the general population display extremely dark personality features â€“ enough to meet the clinical threshold for a personality disorder â€“ and about <span>10-20 per</span> cent of individuals <a href="https://pubmed.ncbi.nlm.nih.gov/22996170/" rel="nofollow noreferrer noopener">have</a> moderately elevated levels. We know that even people with moderate levels of dark traits can wreak havoc: they are more likely to <a href="https://journals.sagepub.com/doi/10.1177/1745691616666070" rel="nofollow noreferrer noopener">lie and cheat</a>, show <a href="https://www.sciencedirect.com/science/article/pii/S0191886920305468" rel="nofollow noreferrer noopener">racist</a> attitudes, and be <a href="https://psycnet.apa.org/record/2014-04417-005" rel="nofollow noreferrer noopener">violent</a> towards others.</p>
<p>As researchers, we have studied these traits ourselves. But in a bid to balance out the extensive literature on dark traits, we have recently started to <a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00467/full" rel="nofollow noreferrer noopener">focus</a> on the light side of human personality instead â€“ the â€˜everyday saintsâ€™ among us. These people are genuinely interested in others and treat them well without question, not as a means to an end. They applaud the success of others, believe in the fundamental goodness of humans, and respect the dignity of everyone. Our recent <a href="https://www.sciencedirect.com/science/article/pii/S019188692030310X" rel="nofollow noreferrer noopener">study</a> of more than 36,000 adults suggests that these traits are common: around <span>30-50 per cent</span> of people show prominent light personality trait profiles, depending on world region, and these traits are particularly common in women.</p>
<p>We wanted to understand which personality profile â€“ dark or light â€“ leads to more success and happiness in the long run. There is an oft-touted saying that â€˜Nice guys finish lastâ€™ and, on the face of it, this might seem correct. If youâ€™re always expending your energy caring about others, perhaps youâ€™re bound to get left behind. If youâ€™re willing to deceive and exploit others without worrying about their feelings, you can look after â€˜number oneâ€™ and rise faster to the top. But does the research back this up?</p>
<p>Experimental studies support the idea of a â€˜successfulâ€™ dark personality, but only up to a point. One <a href="https://psycnet.apa.org/record/2015-11168-026" rel="nofollow noreferrer noopener">study</a> found that people with psychopathic personality traits win more points on a negotiation task where they are required to compete with a partner, but fewer points on a task that involves cooperation. Those with dark traits are <a href="https://link.springer.com/article/10.1007%2Fs12144-018-9823-9" rel="nofollow noreferrer noopener">more</a> likely to â€˜defectâ€™ in the classic Prisonerâ€™s Dilemma task â€“ an approach that means maximising your own outcome while duping the other participant.</p>
<p>But their success in the real world is questionable. In corporate settings, those with dark personality traits are slightly more likely to <a href="https://psycnet.apa.org/record/2018-51219-001" rel="nofollow noreferrer noopener">emerge</a> as leaders and are seen as <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/bsl.925" rel="nofollow noreferrer noopener">charismatic</a> but, when it comes to getting the job done, they tend to achieve less and are considered poor team players. Our recent study also found that political figures with dark personality traits are more likely to get elected and hold their positions, but other studies <a href="https://journals.sagepub.com/doi/abs/10.1177/0956797615611922" rel="nofollow noreferrer noopener">show</a> that they are much poorer at getting legislation passed. Hedge fund managers with these traits generally <a href="https://journals.sagepub.com/doi/abs/10.1177/0146167217733080" rel="nofollow noreferrer noopener">obtain</a> significantly lower financial returns on the investment funds they manage. Overall, individuals with dark traits <a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2Fa0025679" rel="nofollow noreferrer noopener">engage</a> in more counterproductive work behaviour, such as theft and abusive supervision. Perhaps unsurprisingly, they donâ€™t end up with higher average incomes than their peers with light personalities.</p>
<p>Through deceitful manipulation and callous use of others, these individuals are cutting off the very (social) branch theyâ€™re sitting on</p>
<p>On top of this, those with dark personality traits donâ€™t have much luck outside of work. Even if they manage to avoid prison (imprisonment being a <a href="https://pubmed.ncbi.nlm.nih.gov/18837606/" rel="nofollow noreferrer noopener">high</a> possibility for those with extreme traits), they are at increased risk of <a href="https://pubmed.ncbi.nlm.nih.gov/19243821/" rel="nofollow noreferrer noopener">suicide</a> and <a href="https://pubmed.ncbi.nlm.nih.gov/28556964/" rel="nofollow noreferrer noopener">violent death</a>. They are also not particularly happy: people with dark traits tend to report poor self-image, an inability to intimately connect with others, and little life satisfaction. In contrast, we found that those with light personality trait profiles have fulfilling, intrinsically rewarding lives: they generally have a more positive view of themselves, more positive connections with others and find life more satisfying.</p>
<p>The key factor here seems to be empathy: the capacity to resonate with â€“ and understand the perspective of â€“ the emotional experiences of others. Individuals with light personality traits show a great deal of empathy for others, while those with dark personality traits tend to show very little. In our new research, we found that this seems to be what leads to a more satisfying life. Similarly, being prosocial â€“ acting kindly, cooperatively and with compassion toward others â€“ is also significantly <a href="https://psycnet.apa.org/record/2020-65092-001" rel="nofollow noreferrer noopener">linked</a> with higher wellbeing.</p>
<p>As a species, weâ€™re fundamentally built for social connectedness, and we depend on cooperation and <a href="https://www.aeaweb.org/articles?id=10.1257/jep.14.3.137" rel="nofollow noreferrer noopener">trust</a>. When those with dark personalities try to take advantage of this for their own personal gain, they do so at their own peril. In essence, through deceitful manipulation and callous use of others, these individuals are cutting off the very (social) branch theyâ€™re sitting on. While those with dark traits might initially capture the attention of others, their social behaviour ultimately leads to limited success in work or politics, and little satisfaction with their lives.</p>
<p><strong>So far, we have</strong> made it seem as though people fall into one of two binary groups: dark or light. But in reality, thereâ€™s a third group: we found that about <span>40 per cent</span> of individuals show a balance of dark and light traits. People in this mixed group are similar to the light group when it comes to critical variables involving empathy and social connectedness, but they still show some dark tendencies â€“ hampering their relationships to some degree with deceitful, self-absorbed or hurtful behaviour toward others.</p>
<p>Some might think that the mixed group is the optimal place to be: youâ€™re able to have some connection with others, but wonâ€™t be taken as a pushover. But compared with those with a light personality, mixed individuals have lower levels of life satisfaction and a less positive self-image. It seems that the mixed group are on the way to the light personality profile, so to speak, but fall short of the full expression â€“ and the added dark traits are whatâ€™s holding them back.</p>
<p>As people age â€“ particularly from 30 to 40 â€“ they become more likely to display light personality trait profiles</p>
<p>Regardless of where you fall on these dimensions of personality, we <a href="https://scottbarrykaufman.com/books/transcend/" rel="nofollow noreferrer noopener">believe</a> in the fundamental ability to grow and change. Large-scale studies have <a href="https://doi.apa.org/doiLanding?doi=10.1037%2Fa0024950" rel="nofollow noreferrer noopener">documented</a> that your general personality (eg, neuroticism, extraversion, conscientiousness) continues to change throughout your lifetime, and weâ€™ve found the same to be true when it comes to light and dark traits.</p>
<p>Specifically, we found that the extent to which you exhibit light or dark personality traits tends to shift as you get older. As people age â€“ particularly as they progress from 30 to 40 â€“ they become more likely to display light personality trait profiles. Other <a href="https://content.apa.org/record/2014-34008-001" rel="nofollow noreferrer noopener">research</a> has shown that moral character traits, such as conscientiousness and self-control, are generally more common in older people. Age doesnâ€™t completely account for the results â€“ younger people can display light personality traits â€“ but the research suggests that what can fundamentally differentiate light and dark profiles is a process of psychological maturation.</p>
<p>We are <a href="https://journals.sagepub.com/doi/10.1177/0963721417734875" rel="nofollow noreferrer noopener">born</a> with an innate sense of fairness, though this is a limited ability. Thus, like language, it is a skill that requires further development. Our research, and studies of our closest relatives, nonhuman primates, both show that moral behaviour can emerge and change across development â€“ in large part through cooperative social interactions. Thus, by embracing and trusting social connections, we can progress toward a light personality trait profile â€“ a pathway that appears to lead to healthy self-actualisation and even transcendence.</p>
<p>If youâ€™re curious about where you fall on the light vs dark personality spectrum, you can answer the <a href="https://scottbarrykaufman.com/lighttriadscale/" rel="nofollow noreferrer noopener">questions</a> we used in our study. We have now tested more than 250,000 individuals from across the globe, and you can see how your responses compare with the average. And remember, there is always scope for change: we believe in the fundamental ability to grow and transcend self-centredness through deliberate and sustained changes in our patterns of behaviour and thinking.</p>
<p>The most important thing is to <em>want</em> to change. Unfortunately, most people with extreme levels of dark personality traits donâ€™t want to change who they are. Despite not being particularly happy with their lives, they remain fixated on what they think they need: more and more power, wealth and domination over others.</p>
<p>Of course, some days weâ€™re all motivated to shut out other people, and simply look out for ourselves. This can be especially true when it seems that those who cheat, deceive and take advantage of others are somehow getting ahead. But rest assured that this doesnâ€™t seem to be the case. We have found that being empathic and connected to others â€“ capitalising on our fundamentally social nature â€“ is ultimately the pathway to a more rewarding life.</p></div></div></div>]]>
            </description>
            <link>https://psyche.co/ideas/are-people-with-dark-personality-traits-more-likely-to-succeed</link>
            <guid isPermaLink="false">hacker-news-small-sites-25344640</guid>
            <pubDate>Tue, 08 Dec 2020 13:17:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[API for Editing Videos]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25344099">thread link</a>) | @sabbakeynejad
<br/>
December 8, 2020 | https://www.veed.io/api#hn | <a href="https://web.archive.org/web/*/https://www.veed.io/api#hn">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div id="w-node-1e5e5906b9c0-6422a266"><div><div id="w-node-1e5e5906b9c2-6422a266"><h3>Build fast &amp; reliable video experiences with well-documented APIs</h3><p>Having processed <strong>over a million</strong> videos to date, we are making it possible for you to do the same. Through our well documented APIs, you can build a wide range of video experiences on the web quickly and reliably.</p><ul role="list"><li><p>Video editing</p></li><li><p>Transcoding and compression</p></li><li><p>Auto-subtitling</p></li></ul><a href="https://veed.readme.io/docs/making-first-render"><p>VEED API&nbsp;Quickstart</p><img src="https://global-uploads.webflow.com/5fb54d297dbd1e8a953dcea9/5fb54d297dbd1efe633dcf9d_arrow.svg" loading="lazy" alt=""></a></div><div id="w-node-1e5e5906b9d8-6422a266"><p><img src="https://global-uploads.webflow.com/5fb54d297dbd1e8a953dcea9/5fb54d297dbd1e55733dcfc0_Subtitle%20Image.png" loading="lazy" alt=""></p></div></div></div><div id="w-node-f964c72a8ee6-6422a266"><h3>Build awesome products</h3><p>What can you build with Veed? Here’s a few creative things for you!</p><div data-duration-in="300" data-duration-out="100"><div><div data-w-tab="Tab 1"><div><div id="w-node-4a212a4e2e46-6422a266"><p><img src="https://global-uploads.webflow.com/5ea7ec2f0fe2ee14fc247bf0/5fb7b990f7f122c956e9a330_BrandKit.svg" loading="lazy" alt=""></p><h4>Asset management</h4><p>You can store and process your videos, images and audio files as assets, using our API.</p><p><a href="https://docs.veed.io/use-cases/creating-an-asset">Learn more</a></p></div><div id="w-node-cae6855cebfe-6422a266"><p><img src="https://global-uploads.webflow.com/5ea7ec2f0fe2ee14fc247bf0/5fb7b9907adc7962695f158a_Scissors2.svg" loading="lazy" alt=""></p><h4>Video trimming</h4><p>If you want to trim the beginning or end from a video, the API Video Trimming functionality is what you're looking for.</p><p><a href="https://docs.veed.io/use-cases/trimming-a-video">Learn more</a></p></div><div id="w-node-1e68831bb48c-6422a266"><p><img src="https://global-uploads.webflow.com/5ea7ec2f0fe2ee14fc247bf0/5fb7b9d1b459b8fe5504d403_Subtitle2.svg" loading="lazy" width="48" alt=""></p><h4>Transcribing and subtitling</h4><p>Following modern trends, subtitles have never been more important. Our API can transcribe and subtitle your videos automatically.</p><p><a href="https://docs.veed.io/use-cases/transcribing-and-subtitling-a-video">Learn more</a></p></div></div></div><div data-w-tab="Tab 2"><div><div id="w-node-c692ca4ce845-6422a266"><h4>Video cropping</h4><p>Make the video fit your format</p></div><div id="w-node-c692ca4ce84d-6422a266"><h4>Watermark Video</h4><p>For example TikTok watermark</p></div><div id="w-node-c692ca4ce855-6422a266"><h4>Transcoding</h4><p>Make any file play and make it the right size</p></div></div></div><div data-w-tab="Tab 3"><div><div id="w-node-5590a8ff75b1-6422a266"><h4>Transcribing and subtitling</h4><p>Following modern trends, subtitles have never been more important. Our API can transcribe and subtitle your videos automatically.</p><p><a href="https://docs.veed.io/use-cases/transcribing-and-subtitling-a-video">Learn more</a></p></div></div></div><div data-w-tab="Tab 4"><div><div id="w-node-c1244a8e9ab5-6422a266"><h4>SDK</h4><p>Editor light, Editor Pro, Video Recorder</p></div></div></div></div></div><p><a href="https://veed.readme.io/" target="_blank">Browse entire API documentation</a></p></div><div id="w-node-a474301ee60e-6422a266"><h3>A seamless integration <span>guaranteed</span></h3><p>Enjoy a stress-free and easy integration on your next project.</p></div><div id="w-node-e23a9130e867-6422a266"><div><div data-duration-in="300" data-duration-out="100"><div><div data-w-tab="Tab 1"><div><div><p>curl \<br> &nbsp;-X POST \<br> &nbsp;--header "Authorization: <br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;veed_test_oEnZhnd3vg0LEoTs6Z9Ij" \<br> &nbsp;--data @render.json \<br> &nbsp;https://api.veed.dev/render</p></div></div></div></div></div></div></div><div id="w-node-75332b20435c-6422a266"><div><h3>Ready to integrate with VEED?</h3><p>Our engineers are available to pair-program with you for FREE.</p></div></div></div></div></div>]]>
            </description>
            <link>https://www.veed.io/api#hn</link>
            <guid isPermaLink="false">hacker-news-small-sites-25344099</guid>
            <pubDate>Tue, 08 Dec 2020 11:48:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A package for presenting person names]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 16 (<a href="https://news.ycombinator.com/item?id=25343112">thread link</a>) | @robinvdvleuten
<br/>
December 8, 2020 | https://robinvdvleuten.nl/blog/package-for-presenting-person-names/ | <a href="https://web.archive.org/web/*/https://robinvdvleuten.nl/blog/package-for-presenting-person-names/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div itemscope="" itemtype="https://schema.org/Blog"><article itemscope="" itemtype="https://schema.org/BlogPosting"><div itemprop="articleBody"><p>When building applications, you’ll probably have to show an user’s name in one or more formats (first, last, initials, etc.). I’ve created a package called <a href="https://github.com/webstronauts/php-person-name">php-person-name</a>, that will make this a lot easier in Laravel (or any PHP framework).</p><p>It is based on the <a href="https://github.com/basecamp/name_of_person">name_of_person</a> gem from the awesome people at <a href="https://basecamp.com/">Basecamp</a>. They had a similar issue in their application; how do we present a person’s name in different formats if we only have a first and a last name.</p><p>It’s public API is quite simple and can be summarized in the following snippet.</p><div><pre><code data-lang="php"><span>&lt;?php</span>

<span>$name</span> <span>=</span> <span>new</span> <span>PersonName</span><span>::</span><span>make</span><span>(</span><span>'David Heinemeier Hansson'</span><span>)</span>

<span>echo</span> <span>$name</span><span>-&gt;</span><span>full</span>        <span>// "David Heinemeier Hansson"
</span><span></span><span>echo</span> <span>$name</span><span>-&gt;</span><span>first</span>       <span>// "David"
</span><span></span><span>echo</span> <span>$name</span><span>-&gt;</span><span>last</span>        <span>// "Heinemeier Hansson"
</span><span></span><span>echo</span> <span>$name</span><span>-&gt;</span><span>initials</span>    <span>// "DHH"
</span><span></span><span>echo</span> <span>$name</span><span>-&gt;</span><span>familiar</span>    <span>// "David H."
</span><span></span><span>echo</span> <span>$name</span><span>-&gt;</span><span>abbreviated</span> <span>// "D. Heinemeier Hansson"
</span><span></span><span>echo</span> <span>$name</span><span>-&gt;</span><span>sorted</span>      <span>// "Heinemeier Hansson, David"
</span><span></span><span>echo</span> <span>$name</span><span>-&gt;</span><span>mentionable</span> <span>// "davidh"
</span><span></span><span>echo</span> <span>$name</span><span>-&gt;</span><span>possessive</span>  <span>// "David Heinemeier Hansson's"
</span></code></pre></div><p>Need integration with Laravel? Just use the following to add a <code>name</code> attribute to your models.</p><div><pre><code data-lang="php"><span>&lt;?php</span>

<span>use</span> <span>Webstronauts\PersonName\PersonName</span><span>;</span>

<span>class</span> <span>User</span> <span>extends</span> <span>Model</span>
<span>{</span>
    <span>/**
</span><span>     * The attributes that are mass assignable.
</span><span>     *
</span><span>     * @var array
</span><span>     */</span>
    <span>protected</span> <span>$fillable</span> <span>=</span> <span>[</span>
        <span>'name'</span><span>,</span> <span>'first_name'</span><span>,</span> <span>'last_name'</span><span>,</span>
    <span>];</span>

    <span>/**
</span><span>     * Return a PersonName instance composed from the `first_name` and `last_name` attributes.
</span><span>     *
</span><span>     * @return PersonName
</span><span>     */</span>
    <span>public</span> <span>function</span> <span>getNameAttribute</span><span>()</span>
    <span>{</span>
        <span>return</span> <span>new</span> <span>PersonName</span><span>(</span><span>$this</span><span>-&gt;</span><span>first_name</span><span>,</span> <span>$this</span><span>-&gt;</span><span>last_name</span><span>);</span>
    <span>}</span>

    <span>/**
</span><span>     * Sets the `first_name` and `last_name` attributes from a full name.
</span><span>     *
</span><span>     * @param  string $name
</span><span>     * @return void
</span><span>     */</span>
    <span>public</span> <span>function</span> <span>setNameAttribute</span><span>(</span><span>$name</span><span>)</span>
    <span>{</span>
        <span>$fullName</span> <span>=</span> <span>PersonName</span><span>::</span><span>make</span><span>(</span><span>$name</span><span>);</span>
        <span>[</span><span>$this</span><span>-&gt;</span><span>first_name</span><span>,</span> <span>$this</span><span>-&gt;</span><span>last_name</span><span>]</span> <span>=</span> <span>$fullName</span> <span>?</span> <span>[</span><span>$fullName</span><span>-&gt;</span><span>first</span><span>,</span> <span>$fullName</span><span>-&gt;</span><span>last</span><span>]</span> <span>:</span> <span>[</span><span>null</span><span>,</span> <span>null</span><span>];</span>
    <span>}</span>
<span>}</span>
</code></pre></div><p>As you can see, you’ll just have to store the <code>first_name</code> and <code>last_name</code> attributes and you’ll can generate all those variants through <code>name</code>.</p><div><pre><code data-lang="php"><span>&lt;?php</span>

<span>$user</span> <span>=</span> <span>new</span> <span>User</span><span>([</span><span>'first_name'</span> <span>=&gt;</span> <span>'Robin'</span><span>,</span> <span>'last_name'</span> <span>=&gt;</span> <span>'van der Vleuten'</span><span>])</span>

<span>echo</span> <span>$user</span><span>-&gt;</span><span>name</span><span>-&gt;</span><span>full</span> <span>// Robin van der Vleuten
</span></code></pre></div><p>A small but nifty helper that could be useful in almost any application. Go check it out on <a href="https://github.com/webstronauts/php-person-name">Github</a> and don’t forget to star it.</p></div><section><hr><h2>Webmentions</h2><ol><li></li><li><p>A package for presenting person names robinvdvleuten.nl/blog/package-f…</p></li><li><p>A package for presenting person names robinvdvleuten.nl/blog/package-f…</p></li><li><p>A package for presenting person names
L: robinvdvleuten.nl/blog/package-f…
C: news.ycombinator.com/item?id=253431…</p></li><li><p>A package for presenting person names: robinvdvleuten.nl/blog/package-f… Comments: news.ycombinator.com/item?id=253431…</p></li></ol></section></article></div></div></div>]]>
            </description>
            <link>https://robinvdvleuten.nl/blog/package-for-presenting-person-names/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25343112</guid>
            <pubDate>Tue, 08 Dec 2020 08:23:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Road to Rome: Fundraising and Project Goals]]>
            </title>
            <description>
<![CDATA[
Score 25 | Comments 4 (<a href="https://news.ycombinator.com/item?id=25342500">thread link</a>) | @terabytest
<br/>
December 7, 2020 | https://rome.tools/funding/ | <a href="https://web.archive.org/web/*/https://rome.tools/funding/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main-content"> <section>  <section> <h2 id="introduction">Introduction <a href="#introduction" aria-label="introduction"></a></h2>  <p>I’m Sebastian McKenzie, the creator of <a href="https://babeljs.io/">Babel</a> and <a href="https://yarnpkg.com/">Yarn</a>. These tools have both inspired me to create Rome, a new project that aims to simplify and improve JavaScript and web development.</p> <p>Rome consolidates dozens of separate tools into one. Rome can install your dependencies, check your code for errors, run your tests, bundle your code, and more, all via a single CLI. Rome will be able to replace Babel, ESLint, Prettier, Yarn, and webpack. <a href="https://rome.tools/">Learn more</a>.</p> <p>It’s been three months since we announced our initial <a href="https://rome.tools/blog/2020/08/08/introducing-rome.html">beta release</a>. Since then, we’ve received a tremendous amount of enthusiasm from the community. As that enthusiasm has grown, it’s become clear that Rome will require a full-time developer to be successful and deliver on our ambitious goals and release a stable v1.0.</p> <p><strong>I need your help to make it a reality.</strong></p> </section> <section> <h2 id="funding">Funding <a href="#funding" aria-label="funding"></a></h2> <p>I have left my job so I can work independently and focus on what the community needs. This includes a <a href="#allow-users-to-extend-functionality-with-plugins">plugin system</a>, <a href="#add-more-configuration-and-have-less-opinions">more configuration</a>, and <a href="#integrate-with-existing-tools">dedicated integrations for existing tools</a>.</p> <p>We have an initial goal of <strong>$100,000</strong>. This will allow myself to work independently on our first stable release. Additional funding would allow us to expand upon our release goals, fund future maintenance, and compensate other contributors.</p> <p>If you’re passionate about what we’re building, or have otherwise benefited from my work, I would appreciate your financial support.</p>    <section> <h3 id="recent-contributions">Recent Contributions <a href="#recent-contributions" aria-label="recent-contributions"></a></h3> <ul> <li>Loading...</li> </ul> </section> <section> <h3 id="contribute">Contribute <a href="#contribute" aria-label="contribute"></a></h3> <p>Prices are in USD. Includes sales tax and international shipping. Refer to <a href="#questions-and-answers">Questions and Answers</a> for more information.</p> <p> Loading... </p> <div> <h4>Custom</h4> <p>Want to donate under $10? Something else? Select your own amount!</p>  </div> <section> <h4 id="business">Business <a href="#business" aria-label="business"></a></h4> <p>These tiers include dedicated support, migration assistance, and website advertisement. I’ll make sure Rome works well for you and your organization.</p> <p>Migration support is where I personally help your organization adopt and use Rome. This could include porting configuration, integrating with CI, or even adding new features and configuration to Rome.</p> <p>Interested in something else or have questions? Get in touch at <a href="https://rome.tools/cdn-cgi/l/email-protection#463523242735322f27280634292b23683229292a35"><span data-cfemail="760513141705021f17183604191b13580219191a05">[email&nbsp;protected]</span></a>!</p> <p> Loading... </p> </section> </section> </section> <section> <h2 id="goals">Goals <a href="#goals" aria-label="goals"></a></h2> <p>Funding will allow us to focus on usage and labor-intensive goals. We can make Rome easier to use and work for more people.</p> <section> <h3 id="add-more-configuration-and-have-less-opinions">Add more configuration and have less opinions <a href="#add-more-configuration-and-have-less-opinions" aria-label="add-more-configuration-and-have-less-opinions"></a></h3> <p>We have deliberately tried to keep configuration to a minimum. While this does produce a minimal API surface, it makes it almost impossible to easily migrate without losing functionality or changing conventions.</p> <p>We should aim to reduce the functional differences between Rome and other tools by introducing additional configuration and supported languages. This could include:</p> <ul> <li>Code formatting options</li> <li>Ability to customize expected filenames and directories</li> <li>Support for other configuration languages such as YAML and TOML</li> <li>More CLI flags</li> <li>Public JavaScript API</li> <li>Dynamic configuration (as opposed to static JSON-only configuration files)</li> <li><a href="#allow-users-to-extend-functionality-with-plugins">Allow extending functionality with plugins</a></li> </ul> <p>We have so far kept configuration light, as by reducing the amount of configuration options supported, we reduce maintenance cost and the potential for internal bugs.</p> <p>While this makes it easier for us as maintainers, it makes it drastically more difficult for users. No matter how persuasive our arguments may be for why you should use hard tabs instead of spaces, they seem like artificial and arbitrary constraints and introduces excessive prerequisites for adoption.</p> <p>Strong defaults and guided documentation for new users can provide the experience we ultimately want to offer, while removing our existing adoption restrictions.</p> </section> <section> <h3 id="integrate-with-existing-tools">Integrate with existing tools <a href="#integrate-with-existing-tools" aria-label="integrate-with-existing-tools"></a></h3> <p>Rome attempts to replace many tools. However we should still strive to support scenarios where another tool is better situated or preferred. This can also help during a migration where Rome is used in conjunction with another tool. We can do this in a couple of ways:</p> <p><strong>Integrating Rome as a first-class plugin in tools such as Babel, eslint, and webpack</strong></p> <p>Rome could be exposed as a plugin for those tools to allow you to adopt the Rome compiler without having to adopt the bundler first. This would reduce adoption prerequisites and allow easier experimentation inside of existing setups.</p> <p><strong>Seamlessly integrate other tools into Rome</strong></p> <p>We can introduce compatibility layers to have ESLint, Babel, and other tools run inside of Rome itself. ESLint errors could be displayed alongside Rome linter errors with the same UI and output format Instantly you could benefit from Rome’s file caching and parallelisation without needing a major migration.</p> </section> <section> <h3 id="assist-in-migrating-from-existing-tools">Assist in migrating from existing tools <a href="#assist-in-migrating-from-existing-tools" aria-label="assist-in-migrating-from-existing-tools"></a></h3> <p>It should be easy to migrate from other tools to Rome. First we need to ensure popular configuration options from other tools are supported. Then, offer automated tools to migrate basic setups without users needing to it manually.</p> <p>This needs to be accompanied with dedicated documentation and guides that can explain the differences between the tools, why you might want to use one over the other, similar concepts, new terminology, and equivalent config options.</p> </section> <section> <h3 id="allow-users-to-extend-functionality-with-plugins">Allow users to extend functionality with plugins <a href="#allow-users-to-extend-functionality-with-plugins" aria-label="allow-users-to-extend-functionality-with-plugins"></a></h3> <p>One of the fears with Rome is creating a monoculture where it’s impossible to innovate and experiment with new ideas. While it’s extremely optimistic to think we’ll ever get into any sort of monopolistic position, not allowing extensions does stiffle innovation regardless of our market position by restricting the viability and adoption of new ideas.</p> <p>Plugins allow us to avoid supporting functionality that we might not want, while still giving users a choice. It reduces our role as an arbiter and allows new languages, non-standard JavaScript features, code conventions, and ideas that interact with Rome to be viable, receive support, and proliferate.</p> <p>We need to be extremely careful not to get into the position where Babel and Webpack are today, where they’re heavily restricted by the usage of internal APIs. We need to be able to maintain our autonomy when it comes to making architectural changes. Balancing this with a powerful plugin API will be a challenge and will likely require several iterations.</p> </section> <section> <h3 id="release-undocumented-features">Release undocumented features <a href="#release-undocumented-features" aria-label="release-undocumented-features"></a></h3> <p>Rome currently does a lot more than linting. It’s a major challenge today to market and explain Rome when so much of the project isn’t officially supported. While we strive to make each individual component of Rome competitive on it’s own, to some the biggest advantage and compelling reason for using Rome might be the reduction in dependencies.</p> <p>We should focus on releasing and maturing basic versions of all core functionality. This would increase user confidence in our architecture and show that Rome is viable as the comprehensive replacement that we want to be.</p> </section> <section> <h3 id="provide-accessible-and-comprehensive-documentation">Provide accessible and comprehensive documentation <a href="#provide-accessible-and-comprehensive-documentation" aria-label="provide-accessible-and-comprehensive-documentation"></a></h3> <p>Documentation for developer tools is generally quite obtuse and relies a lot on prerequisite knowledge. This can make it intimidating and inaccessible for developers new to the ecosystem. Further complicating that is the broad scope of what Rome is trying to do.</p> <p>We have tried to address some of this by making our documentation a single page. This makes it easy to search, and it can be read from top to bottom without needing to jump around to learn about different concepts. However as our supported features grow, it will be more difficult to use this structure without oversimplifcation and doesn’t allow different paths for different demographics.</p> <p>We need to invest in a more scalable approach for our documentation. We can offer dedicated sections that explain features like linting end-to-end without needing to introduce other components like the compiler and bundler that contain significantly more concepts and overwhelm the reader. Separate guides can be offered for new users and those already experienced with other tools to properly cater for multiple audiences.</p> </section> <section> <h3 id="regularly-release-new-versions">Regularly release new versions <a href="#regularly-release-new-versions" aria-label="regularly-release-new-versions"></a></h3> <p>One of the reasons Babel was successful is how quickly I was able to quickly fix bugs and release new versions. I would regularly have releases out within minutes of a bug report. This was critical during the early days when adoption was low. Being able to unblock users quickly would often make them more excited to use Babel even though they ran into a bug.</p> <p>Similarly, we should try and replicate this by building out our release infrastructure to allow the rapid testing and release of versions. We need to maintain momentum as the scope of supported features grow.</p> <p>We can achieve this with automated releases that can be manually triggered or deployed on a schedule. Automatic changelog generation would also take a lot of the manual work out of producing releases. Nightly releases would allow users to test experimental features and provide early feedback.</p> </section> </section> <section> <h2 id="questions-and-answers">Questions and Answers <a href="#questions-and-answers" aria-label="questions-and-answers"></a></h2> <section> <h3 id="when-will-physical-rewards-be-shipped">When will physical rewards be shipped? <a href="#when-will-physical-rewards-be-shipped" aria-label="when-will-physical-rewards-be-shipped"></a></h3> <p>We are tentatively aiming for the end of April 2021, however due to COVID delays or order volume this could be extended. We’ll make sure to keep you updated via email.</p> </section> <section> <h3 id="what-is-my-email-used-for">What is my email used for? <a href="#what-is-my-email-used-for" aria-label="what-is-my-email-used-for"></a></h3> <p>We use your email address to send information about your order such as order questions, shipping status and delays. We may also send a survey to decide on customization options for rewards.</p> <p>Your email address will not be used for any other purpose or be displayed publicly.</p> </section> <section> <h3 id="how-is-payment-information-stored">How is payment information stored? <a href="#how-is-payment-information-stored" aria-label="how-is-payment-information-stored"></a></h3> <p>Payment information is entered and stored via Stripe. We do not have access to full payment details. Your billing address is used if we need to calculate and pay sales tax in your jurisdiction.</p> </section> <section> <h3 id="what-do-tier-prices-include">What do tier prices include? <a href="#what-do-tier-prices-include" aria-label="what-do-tier-prices-include"></a></h3> <p>Prices include processing fees, international shipping, and sales tax. This does mean the effective donation is reduced if you live in a country with import duty or high shipping cost.</p> <p>You have the option to add an additional donation in the order review screen if you would like to cover those costs.</p> </section> <section> <h3 id="why-do-you-need-my-usernames">Why do you need my usernames? <a href="#why-do-you-need-my-usernames" aria-label="why-do-you-need-my-usernames"></a></h3> <p>Usernames are used to allocate tier rewards. They are not required and you can …</p></section></section></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://rome.tools/funding/">https://rome.tools/funding/</a></em></p>]]>
            </description>
            <link>https://rome.tools/funding/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25342500</guid>
            <pubDate>Tue, 08 Dec 2020 06:28:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Alzheimer’s: New protein found in spinal fluid indicates stage of the disease]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25342434">thread link</a>) | @finphil
<br/>
December 7, 2020 | https://nuadox.com/post/636913398485073920/alzheimers-new-protein-in-spinal-fluid | <a href="https://web.archive.org/web/*/https://nuadox.com/post/636913398485073920/alzheimers-new-protein-in-spinal-fluid">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> 
                 
                    
                    <article id="636913398485073920">
                        <div>
                            <div>
                                <a href="https://nuadox.com/post/636913398485073920/alzheimers-new-protein-in-spinal-fluid"><h2>Alzheimer’s: New protein found in spinal fluid indicates stage of the disease</h2></a>
                                <figure data-orig-width="1920" data-orig-height="1322"><img src="https://64.media.tumblr.com/a98b1861b351c0e344a0b741a8dbb2e4/a732b01a66a4a7cb-bd/s1280x1920/7ada61fe5c6d735e94585f0e12fc1d58f842e15d.jpg" alt="image" data-orig-width="1920" data-orig-height="1322" width="1280" height="881"></figure><p><b>- By <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fmedicine.wustl.edu%2Fnews%2Fauthor%2Ftbhandari%2F&amp;t=NDFiM2RkODY3YWVmNzM4ZTk2YmY5YTY1Njk0OTdhMzE3YjNhYmE1YyxES3V5WjNqTQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F636913398485073920%2Falzheimers-new-protein-in-spinal-fluid&amp;m=0&amp;ts=1607747757">Tamara Bhandari</a> ,&nbsp;<a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fmedicine.wustl.edu%2F&amp;t=Y2ViMTM5YjM5NTYyYzg1YmU1NjhkMzNmMDVhZGUwZGI0M2MyYmE3NyxES3V5WjNqTQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F636913398485073920%2Falzheimers-new-protein-in-spinal-fluid&amp;m=0&amp;ts=1607747757">Washington University School of Medicine</a> -</b></p><p>A novel form of an Alzheimer’s protein found in the fluid that surrounds the brain and spinal cord indicates what stage of the disease a person is in, and tracks with tangles of tau protein in the brain, according to a study from researchers at Washington University School of Medicine in St. Louis.&nbsp;</p><p>Tau tangles are thought to be toxic to neurons, and their spread through the brain foretells the death of brain tissue and cognitive decline. Tangles appear as the early, asymptomatic stage of Alzheimer’s develops into the symptomatic stage.</p><p>The discovery of so-called microtubule binding region tau (MTBR tau) in the cerebrospinal fluid could lead to a way to diagnose people in the earliest stages of Alzheimer’s disease, before they have symptoms or when their symptoms are still mild and easily misdiagnosed. It also could accelerate efforts to find treatments for the devastating disease, by providing a relatively simple way to gauge whether an experimental treatment slows or stops the spread of toxic tangles.</p><p>The study is published Dec. 7 in the journal <i><a href="https://t.umblr.com/redirect?z=https%3A%2F%2Facademic.oup.com%2Fbrain%2Fadvance-article%2Fdoi%2F10.1093%2Fbrain%2Fawaa373%2F6024973&amp;t=M2VjOGFjMzYzYTJmYmRhNDFhMzQ2ZTg2ZDk3NjNkODE4NjY0NWRiNSxES3V5WjNqTQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F636913398485073920%2Falzheimers-new-protein-in-spinal-fluid&amp;m=0&amp;ts=1607747757">Brain</a></i>.</p><p>“This MTBR tau fluid biomarker measures tau that makes up tangles and can confirm the stage of Alzheimer’s disease by indicating how much tau pathology is in the brains of Alzheimer’s disease patients,” said senior author <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fwuphysicians.wustl.edu%2Ffor-patients%2Ffind-a-physician%2Frandall-j-bateman&amp;t=NTYzMjE5MjExOGM5MzI3NzZkNDU1YjA1MTA1ODUyMDcyM2JjN2Q2MixES3V5WjNqTQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F636913398485073920%2Falzheimers-new-protein-in-spinal-fluid&amp;m=0&amp;ts=1607747757">Randall J. Bateman, MD</a>, the Charles F. and Joanne Knight Distinguished Professor of Neurology. Bateman treats patients with Alzheimer’s disease on the Washington University Medical Campus. “If we can translate this into the clinic, we’d have a way of knowing whether a person’s symptoms are due to tau pathology in Alzheimer’s disease and where they are in the disease course, without needing to do a brain scan. As a physician, this information is invaluable in informing patient care, and in the future, to guide treatment decisions.”</p><figure data-orig-height="467" data-orig-width="700"><img src="https://64.media.tumblr.com/fc417759120d05f3ed2b3f3c88456b16/a732b01a66a4a7cb-5b/s1280x1920/833d10cd4ac25b6179bb912ffe11c36eb2e47a6d.jpg" data-orig-height="467" data-orig-width="700" width="700" height="467" alt="image"></figure><p><i>Image:&nbsp;A “heat map” of the brain of a person with mild Alzheimer’s dementia shows where tau protein has accumulated, with areas of higher density in red and orange, and lower density in green and blue. Researchers at Washington University School of Medicine in St. Louis have found a form of tau in spinal fluid that tracks with tau tangles in the brain and indicates what stage of the disease a person is in. Credit: <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fmedicine.wustl.edu%2Fnews%2Fnovel-form-of-alzheimers-protein-found-in-spinal-fluid-indicates-stage-of-the-disease%2F&amp;t=YTU2Nzc3NjgwM2U5OWFmNTMzOGQ5NWIxNjU1ZTJjM2ZkZDM2M2Q5ZixES3V5WjNqTQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F636913398485073920%2Falzheimers-new-protein-in-spinal-fluid&amp;m=0&amp;ts=1607747757">Tammie Benzinger/Knight ADRC</a>.</i></p><p>Alzheimer’s begins when a brain protein called amyloid starts forming plaques in the brain. During this amyloid stage, which can last two decades or more, people show no signs of cognitive decline. However, soon after tangles of tau begin to spread in the neurons, people start exhibiting confusion and memory loss, and brain scans show increasing atrophy of brain tissue.</p><p>Tau tangles can be detected by positron emission tomography (PET) brain scans, but brain scans are time-consuming, expensive and not available everywhere. Bateman and colleagues are developing diagnostic blood tests for Alzheimer’s disease based on <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fmedicine.wustl.edu%2Fnews%2Fblood-test-is-94-accurate-at-identifying-early-alzheimers-disease%2F&amp;t=NzE5MzJmZDA4NTdjMDUwNTk1ZWZhNjgzYmFkZTIxOTQ1NzdhMDA5MCxES3V5WjNqTQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F636913398485073920%2Falzheimers-new-protein-in-spinal-fluid&amp;m=0&amp;ts=1607747757">amyloid</a> or a different <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fmedicine.wustl.edu%2Fnews%2Falzheimers-protein-in-blood-indicates-early-brain-changes%2F&amp;t=MTZiNWM3ZjhiMGE3OWU5MTZiNDgwYTY5MTMxNzI0YjJlMDE4ZjAyYSxES3V5WjNqTQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F636913398485073920%2Falzheimers-new-protein-in-spinal-fluid&amp;m=0&amp;ts=1607747757">form of tau</a>, but neither test can pin down the amount of tau tangles across the stages of disease.</p><p>MTBR tau is an insoluble piece of the tau protein, and the primary component of tau tangles. Bateman and first author Kanta Horie, PhD, a visiting scientist in Bateman’s lab, realized that specific MTBR tau species were enriched in the brains of people with Alzheimer’s disease, and that measuring levels of the species in the cerebrospinal fluid that bathes the brain might be a way to gauge how broadly the toxic tangles have spread through the brain. Previous researchers using antibodies against tau had failed to detect MTBR tau in the cerebrospinal fluid. But Horie and colleagues developed a new method based on using chemicals to purify tau out of a solution, followed by mass spectrometry.</p><p>Using this technique, Horie, Bateman and colleagues analyzed cerebrospinal fluid from 100 people in their 70s. Thirty had no cognitive impairment and no signs of Alzheimer’s; 58 had amyloid plaques with no cognitive symptoms, or with mild or moderate Alzheimer’s dementia; and 12 had cognitive impairment caused by other conditions. The researchers found that levels of a specific form — MTBR tau 243 — in the cerebrospinal fluid were elevated in the people with Alzheimer’s and that it increased the more advanced a person’s cognitive impairment and dementia were.</p><p>The researchers verified their results by following 28 members of the original group over two to nine years. Half of the participants had some degree of Alzheimer’s at the start of the study. Over time, levels of MTBR tau 243 significantly increased in the Alzheimer’s disease group, in step with a worsening of scores on tests of cognitive function.</p><figure data-orig-height="467" data-orig-width="700"><img src="https://64.media.tumblr.com/b80f5e6d9222f0044e0c36a76f464ec8/a732b01a66a4a7cb-da/s1280x1920/353cb72a4ec9acf360e43a1eb0a68bb6998b1887.jpg" data-orig-height="467" data-orig-width="700" width="700" height="467" alt="image"></figure><p><i>Image:&nbsp;Researchers at Washington University School of Medicine in St. Louis have found a novel form of the Alzheimer’s protein tau in the fluid surrounding the brain and spinal cord. This form of tau — known as MTBR tau — indicates what stage of Alzheimer’s a person is in and tracks with tangles of tau protein in the brain. Credit: <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fmedicine.wustl.edu%2Fnews%2Fnovel-form-of-alzheimers-protein-found-in-spinal-fluid-indicates-stage-of-the-disease%2F&amp;t=YTU2Nzc3NjgwM2U5OWFmNTMzOGQ5NWIxNjU1ZTJjM2ZkZDM2M2Q5ZixES3V5WjNqTQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F636913398485073920%2Falzheimers-new-protein-in-spinal-fluid&amp;m=0&amp;ts=1607747757">Sara Moser</a>.</i></p><p>The gold standard for measuring tau in the living brain is a tau-PET brain scan. The amount of tau visible in a brain scan correlates with cognitive impairment. To see how their technique matched up to the gold standard, the researchers compared the amount of tau visible in brain scans of 35 people — 20 with Alzheimer’s and 15 without — with levels of MTBR tau 243 in the cerebrospinal fluid. MTBR tau 243 levels were highly correlated with the amount of tau identified in the brain scan, suggesting that their technique accurately measured how much tau — and therefore damage — had accumulated in the brain.</p><p>“Right now there is no biomarker that directly reflects brain tau pathology in cerebrospinal fluid or the blood,” Horie said. “What we’ve found here is that a novel form of tau, MTBR tau 243, increases continuously as tau pathology progresses. This could be a way for us to not only diagnose Alzheimer’s disease but tell where people are in the disease. We also found some specific MTBR tau species in the space between neurons in the brain, which suggests that they may be involved in spreading tau tangles from one neuron to another. That finding opens up new windows for novel therapeutics for Alzheimer’s disease based on targeting MTBR tau to stop the spread of tangles.”</p><p>–</p><p><b>Source:&nbsp;<a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fmedicine.wustl.edu%2Fnews%2Fnovel-form-of-alzheimers-protein-found-in-spinal-fluid-indicates-stage-of-the-disease%2F&amp;t=YTU2Nzc3NjgwM2U5OWFmNTMzOGQ5NWIxNjU1ZTJjM2ZkZDM2M2Q5ZixES3V5WjNqTQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F636913398485073920%2Falzheimers-new-protein-in-spinal-fluid&amp;m=0&amp;ts=1607747757">Washington University School of Medicine</a></b></p><p><b>Full study:</b>&nbsp;“CSF tau microtubule binding region identifies tau tangle and clinical stages of Alzheimer’s disease”, <i>Brain</i>.</p><p><a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fdoi.org%2F10.1093%2Fbrain%2Fawaa373&amp;t=NDcyZTU1NWE0NTg0YmViMDBhOGE0NGU0NzRlOGUxZGNhNjg2MzUzNSxES3V5WjNqTQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F636913398485073920%2Falzheimers-new-protein-in-spinal-fluid&amp;m=0&amp;ts=1607747757">https://doi.org/10.1093/brain/awaa373</a><br></p><h2><b>Read Also</b></h2><p><a href="https://nuadox.com/post/625031181195509760/new-blood-test-alzheimers">Novel blood test could greatly improve diagnosis of Alzheimer’s</a></p>
                    
                      
                    
                    
                    
                    
                    
                    
                    
                    
                                             
                                <p><span>
                                    <p>
                                    
                                        <a href="https://nuadox.com/tagged/alzheimers">alzheimers</a>
                                    
                                        <a href="https://nuadox.com/tagged/neuroscience">neuroscience</a>
                                    
                                        <a href="https://nuadox.com/tagged/brain">brain</a>
                                    
                                        <a href="https://nuadox.com/tagged/medicine">medicine</a>
                                    
                                        <a href="https://nuadox.com/tagged/health">health</a>
                                    
                                        <a href="https://nuadox.com/tagged/biology">biology</a>
                                    
                                        <a href="https://nuadox.com/tagged/aging">aging</a>
                                    
                                    </p>
                                </span></p>
                                
                            </div>
                        </div>
                    </article>
                 
                </div></div>]]>
            </description>
            <link>https://nuadox.com/post/636913398485073920/alzheimers-new-protein-in-spinal-fluid</link>
            <guid isPermaLink="false">hacker-news-small-sites-25342434</guid>
            <pubDate>Tue, 08 Dec 2020 06:14:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Evolution of my role as a founder CTO]]>
            </title>
            <description>
<![CDATA[
Score 98 | Comments 15 (<a href="https://news.ycombinator.com/item?id=25342265">thread link</a>) | @kwindla
<br/>
December 7, 2020 | https://miguelcarranza.es/cto | <a href="https://web.archive.org/web/*/https://miguelcarranza.es/cto">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  
  <p>There is a lot written about the importance of scaling as a founder in a fast-growing startup. Most of it focused on the CEO role. The generic advice on leadership also applies to other non-CEO roles, but I could not find a lot of content targeted to technical founders. In fact, after reading a <a href="https://twitter.com/elwatto/status/1230977002192031744?s=20" target="_blank">bunch of S-1 forms</a>, <strong>it was hard to find first-time CTOs going all the way from MVP to IPO</strong> (as opposed to founding CEOs). I found this fact really intriguing, and I wanted to dig deeper to try to understand the reasons. It was also stressing me out to some degree: <em>what if I don’t manage to scale fast enough? What does scaling even mean? I’d rather prepare before it becomes a real problem! I want to be the CTO who makes <a href="https://revenuecat.com/" target="_blank">RevenueCat</a> a public company!</em><sup>1</sup></p>

<p>Is it really much harder for non-CEO founders to scale quickly? Or perhaps it is that CEOs have a stronger support network. These could be the reasons. Or maybe I was asking the wrong question. After talking to a lot of founder CTOs, there was something clear: <strong>there is no standard definition for the CTO role</strong>, responsibilities will totally change <strong>depending on the company and the stage</strong>. At inception, the CTO is probably a glorified individual contributor, but this can escalate very quickly. Everyone’s experiences are different. Unfortunately, I don’t have the answers for other non-CEO founder roles, and maybe I don’t even have it for first-time CTOs either. However, I thought it would be a good self-awareness exercise to reflect on the things I’ve learned and how my responsibilities have changed during the first 3 years at <a href="https://revenuecat.com/" target="_blank">RevenueCat</a>.</p>

<h3 id="the-cto-vs-vp-of-engineering-dilemma">The CTO vs VP of Engineering dilemma</h3>
<p>Simplifying, we could say that the CTO role is closer to architecture and code; whereas the VPE would be in charge of processes and management. A simple analogy could be the Senior/Staff Engineer path versus the Engineering Management career path.</p>

<p>During the early days, you need a CTO to architect and coordinate a small group of ICs hacking the minimum viable product. At this stage, being a founder is extremely helpful to set the vision of the product and the engineering culture. Ideally, the CTO is a domain expert in the problem that the startup is trying to solve.</p>

<p>But what if the product becomes successful? What if you reach product-market fit? You will need to hire a lot more engineers to satisfy customer demands. That’s a great problem to have. You might get lucky and get funding, or you might even be profitable already. But a higher headcount means more polished processes. At some point, somebody will inevitably need to start wearing the VP of Engineering hat. It will happen, and it will become quite obvious as the previous (or lack of) processes start to break. You have a couple of options at this stage. The CTO can start acting as VPE, or you can hire externally. It is probably a matter of personal preference or previous management experience. But <strong>it is a totally different set of skills</strong>, which might be difficult to acquire in a hyper-growth environment. Therefore, most commonly, the VP of Engineering is <a href="https://www.saastr.com/makes-bad-cto/" target="_blank">hired externally</a>.</p>

<p>If you are a founder you have some flexibility. You might have control to decide which way you want to go. Perhaps you hate people management and want to keep using your technical skills and problem knowledge to influence the technology directly. Or maybe you want to improve your management skills. Early employees tend to be more forgiving about founder managerial flaws. They joined because they trusted the founders, believed in the vision, and know they have good intentions. In the beginning, they might even prefer you as a direct manager than an external person. But eventually, there will be several layers of management, so you better learn fast if you want to follow this path and don’t have the experience.</p>

<p>Bryan Helmig, Zapier’s co-founder and CTO, says you need to <a href="https://zapier.com/engineering/startup-cto/" target="_blank"><em>figure out where you get your dopamine hits</em></a>. Personally, I have always been more of a <em>computer person</em> than a <em>people person</em>. 
I was not sure which path I should take, but I would have bet on the one involving computers. I’ve always loved them, and I would say I am a better, more experienced engineer than I am a manager. I feel  more energized after shipping a new feature than during a one-on-one meeting.</p>

<p>However, as a founder, <strong>I get the dopamine hits when the company is doing well</strong>. When a customer recommends our product. When we hit the revenue goal. When we hire engineers that are better than me. When these engineers are happy and successfully shipping ambitious features. When code review is exhaustive but collaborative, not adversarial.</p>

<p>So, ultimately, I’ve taken the approach to not simply follow my personal preference, but to <strong>do whatever is more impactful for the company at each stage</strong>. I am a problem solver after all. That is how I have been thinking about my role. My <strong>founder identity should be more important than my CTO title</strong>. In the long run, as a major shareholder, I need to do whatever is best for the company.</p>

<p>Typically breaking points start happening <strong>somewhere between 8 and 12 engineers</strong>. But it can be different depending on the product and the environment. In our case, as a fully distributed company, we encountered several breaking points as we were adding new time zones. During this <em>no man’s land stage</em> most technical founders are temporarily forced to wear both CTO and VPE hats simultaneously. Being flexible turned out to be very advantageous. I tried to step up and (poorly and temporarily) do work that nobody had the bandwidth to do. This helped me to identify the pain points and tweak the process or hire somebody to take over these tasks.</p>

<figure>
  <img src="https://miguelcarranza.es/assets/posts/notebooks.jpeg" alt="RevenueCat journals">
</figure>

<p>One thing that helped me a lot to navigate my role was to learn how other founders were spending their time at each stage. I have been journaling since we started the company. Based on these notes, I will briefly describe the tasks I’ve been focused on and how my role has evolved.</p>

<table>
  <thead>
    <tr>
      <th>Metrics (EOY)</th>
      <th>2018</th>
      <th>2019</th>
      <th>2020</th>
      <th>2021</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Product Engineers</td>
      <td>2</td>
      <td>4</td>
      <td>8</td>
      <td>~18</td>
    </tr>
    <tr>
      <td>Total team size</td>
      <td>5</td>
      <td>9</td>
      <td>19</td>
      <td>~45</td>
    </tr>
    <tr>
      <td>Number of cities</td>
      <td>1</td>
      <td>5</td>
      <td>14</td>
      <td>??</td>
    </tr>
    <tr>
      <td>Number of time zones</td>
      <td>1</td>
      <td>3</td>
      <td>6</td>
      <td>??</td>
    </tr>
  </tbody>
</table>

<h2 id="2018-yc-mvp-and-first-hires">2018: YC, MVP, and first hires</h2>
<p>When we did Y-Combinator it was only <a href="https://twitter.com/jeiting" target="_blank">Jacob</a> and me. Jacob would write SDK and frontend code and I would focus on the backend and infrastructure. After YC we hired our first two engineers to take over Jacob’s coding responsibilities full time. We were all based in San Francisco and these hires were people we already knew. Easy (almost inexistent) management. <strong>There was no process overhead</strong>, we had one-week sprints and <strong>we were moving really fast</strong>.</p>

<p>These days were stressful but fun. We were setting the foundations of the engineering culture and seeing customers signing up one by one. Most of my time was spent architecting the initial version of features, listening to customers, and building their requests as fast as we humanly could. <strong>We would ship most requests on the same day</strong>.</p>

<p>My biggest concern here was making sure we were building something people wanted and the ability to keep growing to justify our <a href="https://techcrunch.com/2018/10/24/revenuecat-seed-funding/" target="_blank">$1.5M seed round</a>.</p>

<p><strong>Main learnings</strong>: Too many, impossible to summarize. We learned a lot while doing Y-Combinator. The main thing would be that talking to customers, building what they want, and making them happy was paramount. We even made this and shipping fast two of our <a href="https://www.revenuecat.com/blog/values" target="_blank">core values</a>.</p>

<h2 id="2019-keeping-up-with-customers-and-scaling-the-tech">2019: Keeping up with customers and scaling the tech</h2>
<p>It looks like we had achieved some kind of product-market fit. <strong>Customers were coming to us, support tickets started piling up and our API throughput would increase every day</strong>. We added our first remote engineer, located in Taiwan. The time zone difference was hard initially and we needed to adapt processes. But it all worked out fine. We got better coverage for customer requests and monitoring.</p>

<p><strong>Onboarding was a complete, one-off manual process</strong>. I started doing one-on-ones (not very regularly, maybe once a month), but <strong>management was still pretty light</strong>. Most conversations were still very technical. I was still an individual contributor. I was also on booth duty at a couple of conferences.</p>

<figure>
  <img src="https://miguelcarranza.es/assets/posts/boothduty.JPG" alt="Booth Duty, AltConf 2019">
</figure>

<p>My main concern at this stage was purely technical: <strong>the scalability of our systems</strong>. All of our engineers had a more product-oriented background, and we had some clear single points of failure. Not going down was constantly in my mind. The scale was outgrowing my comfort zone every single day. We did a decent job optimizing the most common scenarios, <a href="https://www.revenuecat.com/blog/aurora-migration-zero-downtime" target="_blank">migrating infrastructure before reaching breaking points</a>, and <strong>paying down the technical debt we took the year before</strong>.</p>

<p>Up until Q4, I was the de-facto on-call engineer. I did not want to disturb other team members outside working hours, and I felt reliability was my ultimate responsibility as a technical founder. I would carry my laptop literally everywhere. Eventually, <strong>we implemented an on-call rotation, and in retrospect, we should have done it earlier</strong>.</p>

<p><strong>Main learnings</strong>: Do not stress over scalability, but monitor as much as possible and always look out for the next bottleneck and single point of failure. Again, these are stressful but great problems to have. Establish an on-call policy as soon as possible, train engineers, and document resolution for known incidents. Rely on your team. <a href="https://miguelcarranza.es/technical-debt" target="_blank">Technical debt is actually good</a> if taken responsibly while trying to find product-market fit.</p>

<h2 id="2020-delegation-and-planning-the-future-organization">2020: Delegation and planning the future organization</h2>
<p>In 2020 we doubled the team again. We added engineers in Europe and Latin America. By the end of the year, we had multiple members in every team (SDK, frontend, backend…). We were able to work on several projects at the same time, and finally tackle more ambitious features. But we needed more coordination. <strong>A little bit of management structure was unavoidable at this stage.</strong></p>

<p>During Q1 and Q2 I spent most of my time reviewing code, providing architectural guidance, and coding a little bit on the side. I was still the person who had more context about our systems. By mid-year, it became very obvious that I was the bottleneck for …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://miguelcarranza.es/cto">https://miguelcarranza.es/cto</a></em></p>]]>
            </description>
            <link>https://miguelcarranza.es/cto</link>
            <guid isPermaLink="false">hacker-news-small-sites-25342265</guid>
            <pubDate>Tue, 08 Dec 2020 05:35:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[DIY Covid-19 Vaccine]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25341719">thread link</a>) | @alfongj
<br/>
December 7, 2020 | https://radvac.org/vaccine/ | <a href="https://web.archive.org/web/*/https://radvac.org/vaccine/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<div id="content">

<div>
	<div id="primary">
		<main id="main" role="main">

			
<article id="post-9" class="page">
	<!-- .entry-header -->
	<div>
		




<ul><li><strong>Intranasally delivered.</strong> Very simple to self administer. No injection, no needles. </li><li><strong>Synthetic peptide epitopes / antigens</strong>. These peptides are small synthetically produced portions of viral sequences. A peptide-based vaccine is not infectious. We have selected these peptides as the basis for our vaccine against SARS-CoV-2. The antigen portion of the vaccine can be substituted by other antigens; for example, recombinant SARS-CoV-2 Spike RBD. By appropriate selection of antigens, the intranasal vaccine design described here can also be adapted for use against other viruses, especially respiratory viruses, such as influenza or non-SARS coronaviruses.</li><li><strong>Chitosan nanoparticle delivery</strong>. Chitosan is a form of chitin, which is found in mushrooms and the shells of crustaceans such as shrimp and crabs (seafood allergies are not allergies to chitin). Chitosan acts as both delivery vehicle and immunostimulatory adjuvant.</li><li><strong>Extremely simple and inexpensive preparation</strong> with easily obtained materials.</li><li><strong>Short-term safety. </strong>This type of vaccine has shown excellent safety in animal studies and human clinical trials. The RaDVaC vaccine has been used repeatedly over several months,  by over 100 self-experimenters, with the most extreme complication in some recipients of stuffy noses. </li></ul>



<p>Protocols for the simple and robust production of chitosan nanoparticle vaccines for intranasal delivery have been published for over two decades. Intranasal delivery of chitosan-based vaccines have shown mild side effects and high levels of efficacy of both mucosal and systemic immunity, when delivered in a prime-boost regimen (in both animal models and human trials). </p>



<p>Click <a href="https://radvac.org/materials-and-equipment/">here to see a list of essential <strong>Materials and equipment</strong></a></p>



<p>Click <a href="https://radvac.org/protocols-for-making-and-taking-the-vaccine">here to access <strong>Protocols for making and taking the vaccine</strong></a></p>



<p>Click <a href="https://radvac.org/white-paper/">here for access to our <strong>White Paper</strong></a>, which contains all relevant scientific details, methods and protocols</p>
	</div><!-- .entry-content -->
</article><!-- #post-9 -->

		</main><!-- #main -->
	</div><!-- #primary -->
</div><!-- .wrap -->


		</div><!-- #content -->

		<!-- #colophon -->
	</div></div>]]>
            </description>
            <link>https://radvac.org/vaccine/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25341719</guid>
            <pubDate>Tue, 08 Dec 2020 04:18:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Game of Life – Hexagonal Version]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25340971">thread link</a>) | @pjama
<br/>
December 7, 2020 | http://pjama.github.io/projects/life/ | <a href="https://web.archive.org/web/*/http://pjama.github.io/projects/life/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Although similar to the original Game of Life, the rules of these <a href="https://en.wikipedia.org/wiki/Cellular_automaton">cellular automata</a> allow cell state to take a continuous value between 0 and 1. Furthermore, the cell state depends on the state of its 6 neighbours, using averages and thresholds. Time remains discrete generations, and exhibits looping (cyclicity) after some initial sequence (depending on the initial conditions and the threshold values).</p><hr><p><a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Wikipedia</a>:</p><quote>The <em>Game of Life</em>, also known simply as <em>Life</em>, is a cellular automaton devised by the British mathematician John Horton Conway in 1970.</quote><quote>The game is a zero-player game, meaning that its evolution is determined by its initial state, requiring no further input. One interacts with the Game of Life by creating an initial configuration and observing how it evolves, or, for advanced players, by creating patterns with particular properties.</quote>
</div></div>]]>
            </description>
            <link>http://pjama.github.io/projects/life/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25340971</guid>
            <pubDate>Tue, 08 Dec 2020 02:33:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Drive My Robot]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 16 (<a href="https://news.ycombinator.com/item?id=25340599">thread link</a>) | @tomjacobs
<br/>
December 7, 2020 | http://teleportconnect.com/teleport.html | <a href="https://web.archive.org/web/*/http://teleportconnect.com/teleport.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

  <div>

    <center>
    <a href="http://teleportconnect.com/"><img src="http://teleportconnect.com/img/teleport_logo.png"></a>
    &nbsp;<a href="http://teleportconnect.com/">About</a> | 
    &nbsp;<a href="https://www.tindie.com/products/teleport/teleport">Buy a Teleport</a>
    </center>

    <!--
    <script>
    	var player = new WSAudioAPI.Player({
        server: {
            host: window.location.hostname,
            port: 5000
	}});
    </script>

    <script>
    	var streamer = new WSAudioAPI.Streamer({
        server: {
            host: window.location.hostname, 
            port: 5000 
	}});
    </script>
    <button onclick="player.start()">Play stream</button>
    <button onclick="player.stop()">Stop playing</button>
    <button onclick="streamer.start()">Start stream</button>
    <button onclick="streamer.stop()">Stop stream</button>
    -->

    <div>

      <!-- Asleep? -->
      

      <div>

        <!-- Video canvas -->
        

      </div>

      <div>
       

    <!-- List of tracks -
    <div id="tracksTitle" style="background:#AAFDCC;">Tracks</div>
    <div id="tracks" style="background:#DDFFDD;"></div><br>-->

    <!-- List of devices online -->
    
    

    </div>

    </div>

    

</div></div>]]>
            </description>
            <link>http://teleportconnect.com/teleport.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25340599</guid>
            <pubDate>Tue, 08 Dec 2020 01:47:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Cemetery of Soviet Computers]]>
            </title>
            <description>
<![CDATA[
Score 136 | Comments 15 (<a href="https://news.ycombinator.com/item?id=25340452">thread link</a>) | @detaro
<br/>
December 7, 2020 | https://rusue.com/cemetery-of-soviet-computers/ | <a href="https://web.archive.org/web/*/https://rusue.com/cemetery-of-soviet-computers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody"><p>The building did not stand out. Unremarkable industrial building, which was built in hundreds of Soviet cities.<span id="more-10215"></span></p>
<p>Non-broken glass, burning lights, live plants inside, modern plastic entrance doors. Except for one floor.</p>
<!-- WP QUADS Content Ad Plugin v. 2.0.17.1 -->


<p>Despite the twilight, the floor remained lifelessly dark. Somewhere in the depths, there was a dim glow of electric light, hardly penetrating through old glass blocks.</p>
<p>Inside the floor was empty and black, but not completely.&nbsp;Inside burned several fluorescent lamps, spotlighting dozens of silhouettes of tall cabinets.</p>
<p>Some of them were covered with darkened translucent film.</p>
<p>The surface of the floor, tables and enclosures covered with black spots of soot, sometimes diluted with white stains of dried extinguishing mixture.</p>
<p>The air felt a persistent, but not strong smell of burning.&nbsp;The fire walked here a few years ago but did not touch the equipment.</p>
<p>Part of the cabinets were antique electronic computers. Others served to measure signals, and computers controlled this process. Dozens of terminals froze on the tables with extinct screens.</p>

<p><a href="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/2.jpg?ssl=1"><img loading="lazy" src="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/2.jpg?resize=1050%2C788&amp;ssl=1" alt="" width="1050" height="788" srcset="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/2.jpg?w=1050&amp;ssl=1 1050w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/2.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/2.jpg?resize=768%2C576&amp;ssl=1 768w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/2.jpg?resize=1024%2C768&amp;ssl=1 1024w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/2.jpg?resize=200%2C150&amp;ssl=1 200w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/2.jpg?resize=360%2C270&amp;ssl=1 360w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/2.jpg?resize=545%2C409&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p><a href="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/3.jpg?ssl=1"><img loading="lazy" src="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/3.jpg?resize=1050%2C1438&amp;ssl=1" alt="" width="1050" height="1438" srcset="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/3.jpg?w=1050&amp;ssl=1 1050w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/3.jpg?resize=219%2C300&amp;ssl=1 219w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/3.jpg?resize=768%2C1052&amp;ssl=1 768w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/3.jpg?resize=748%2C1024&amp;ssl=1 748w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/3.jpg?resize=360%2C493&amp;ssl=1 360w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/3.jpg?resize=545%2C746&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p><a href="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121.jpg?ssl=1"><img loading="lazy" src="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121.jpg?resize=1050%2C788&amp;ssl=1" alt="" width="1050" height="788" srcset="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121.jpg?w=1050&amp;ssl=1 1050w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121.jpg?resize=768%2C576&amp;ssl=1 768w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121.jpg?resize=1024%2C768&amp;ssl=1 1024w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121.jpg?resize=200%2C150&amp;ssl=1 200w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121.jpg?resize=360%2C270&amp;ssl=1 360w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121.jpg?resize=545%2C409&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p>Suddenly it became clear that before them the legendary machine “Saratov-2”.&nbsp;The machine, which was massively placed on many enterprises of the Soviet Union in the 70s, but at the same time, not a single high-quality (I’m not talking about color) photos remained. Not on the Internet, not even in the museum of the enterprise developer.</p>
<p><a href="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/4-1.jpg?ssl=1"><img loading="lazy" src="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/4-1.jpg?resize=1050%2C1400&amp;ssl=1" alt="" width="1050" height="1400" srcset="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/4-1.jpg?w=1050&amp;ssl=1 1050w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/4-1.jpg?resize=225%2C300&amp;ssl=1 225w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/4-1.jpg?resize=768%2C1024&amp;ssl=1 768w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/4-1.jpg?resize=360%2C480&amp;ssl=1 360w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/4-1.jpg?resize=545%2C727&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p><a href="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/5.jpg?ssl=1"><img loading="lazy" src="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/5.jpg?resize=1050%2C1400&amp;ssl=1" alt="" width="1050" height="1400" srcset="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/5.jpg?w=1050&amp;ssl=1 1050w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/5.jpg?resize=225%2C300&amp;ssl=1 225w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/5.jpg?resize=768%2C1024&amp;ssl=1 768w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/5.jpg?resize=360%2C480&amp;ssl=1 360w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/5.jpg?resize=545%2C727&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p>This computer did not even have a traditional microprocessor. Domestic clone of the popular American PDP-8, Saratov-2 was produced in two versions.</p>
<p>Steel frame, like a chest of drawers, filled with drawers.&nbsp;Different boxes responded to various computer nodes — a twelve-bit computing unit, input-output device interfaces, and RAM.</p>
<p>The memory was ferromagnetic – two boxes of four cubes in each. Reading or writing programs occurred through punched tapes, and to display the results of calculations used electric typewriter CONSUL-260.</p>
<p>The monitor and keyboard in that era were not yet a much-needed</p>
<p>part of the computer. The necessary input of programs into the operative memory was carried out in binary codes, manually using a group of switches on the front panel. Bulbs controlled the correctness of the input.</p>
<p><a href="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/6.jpg?ssl=1"><img loading="lazy" src="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/6.jpg?resize=1050%2C788&amp;ssl=1" alt="" width="1050" height="788" srcset="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/6.jpg?w=1050&amp;ssl=1 1050w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/6.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/6.jpg?resize=768%2C576&amp;ssl=1 768w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/6.jpg?resize=1024%2C768&amp;ssl=1 1024w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/6.jpg?resize=200%2C150&amp;ssl=1 200w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/6.jpg?resize=360%2C270&amp;ssl=1 360w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/6.jpg?resize=545%2C409&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p><a href="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/7.jpg?ssl=1"><img loading="lazy" src="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/7.jpg?resize=1050%2C773&amp;ssl=1" alt="" width="1050" height="773" srcset="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/7.jpg?w=1050&amp;ssl=1 1050w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/7.jpg?resize=300%2C221&amp;ssl=1 300w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/7.jpg?resize=768%2C565&amp;ssl=1 768w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/7.jpg?resize=1024%2C754&amp;ssl=1 1024w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/7.jpg?resize=360%2C265&amp;ssl=1 360w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/7.jpg?resize=545%2C401&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p><a href="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/8.jpg?ssl=1"><img loading="lazy" src="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/8.jpg?resize=1050%2C774&amp;ssl=1" alt="" width="1050" height="774" srcset="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/8.jpg?w=1050&amp;ssl=1 1050w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/8.jpg?resize=300%2C221&amp;ssl=1 300w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/8.jpg?resize=768%2C566&amp;ssl=1 768w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/8.jpg?resize=1024%2C755&amp;ssl=1 1024w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/8.jpg?resize=360%2C265&amp;ssl=1 360w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/8.jpg?resize=545%2C402&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p><a href="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/9.jpg?ssl=1"><img loading="lazy" src="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/9.jpg?resize=1050%2C1088&amp;ssl=1" alt="" width="1050" height="1088" srcset="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/9.jpg?w=1050&amp;ssl=1 1050w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/9.jpg?resize=290%2C300&amp;ssl=1 290w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/9.jpg?resize=768%2C796&amp;ssl=1 768w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/9.jpg?resize=988%2C1024&amp;ssl=1 988w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/9.jpg?resize=360%2C373&amp;ssl=1 360w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/9.jpg?resize=545%2C565&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p><a href="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/10.jpg?ssl=1"><img loading="lazy" src="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/10.jpg?resize=1050%2C828&amp;ssl=1" alt="" width="1050" height="828" srcset="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/10.jpg?w=1050&amp;ssl=1 1050w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/10.jpg?resize=300%2C237&amp;ssl=1 300w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/10.jpg?resize=768%2C606&amp;ssl=1 768w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/10.jpg?resize=1024%2C807&amp;ssl=1 1024w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/10.jpg?resize=360%2C284&amp;ssl=1 360w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/10.jpg?resize=545%2C430&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p><a href="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121-1.jpg?ssl=1"><img loading="lazy" src="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121-1.jpg?resize=1050%2C788&amp;ssl=1" alt="" width="1050" height="788" srcset="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121-1.jpg?w=1050&amp;ssl=1 1050w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121-1.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121-1.jpg?resize=768%2C576&amp;ssl=1 768w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121-1.jpg?resize=1024%2C768&amp;ssl=1 1024w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121-1.jpg?resize=200%2C150&amp;ssl=1 200w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121-1.jpg?resize=360%2C270&amp;ssl=1 360w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/121-1.jpg?resize=545%2C409&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p>The next generation of computers was Electronics 100/25. These machines were clones of American PDP-11.</p>
<p>They thought faster, had more memory, allowed them to work with magnetic tape drives and punched tapes, but the general principle remained the same.</p>
<p>It was possible to connect a monitor and a keyboard to this computer, while still having the ability to enter programs through the front switches.</p>
<p><a href="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/11.jpg?ssl=1"><img loading="lazy" src="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/11.jpg?resize=1050%2C778&amp;ssl=1" alt="" width="1050" height="778" srcset="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/11.jpg?w=1050&amp;ssl=1 1050w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/11.jpg?resize=300%2C222&amp;ssl=1 300w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/11.jpg?resize=768%2C569&amp;ssl=1 768w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/11.jpg?resize=1024%2C759&amp;ssl=1 1024w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/11.jpg?resize=360%2C267&amp;ssl=1 360w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/11.jpg?resize=545%2C404&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p><a href="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/12.jpg?ssl=1"><img loading="lazy" src="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/12.jpg?resize=1050%2C788&amp;ssl=1" alt="" width="1050" height="788" srcset="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/12.jpg?w=1050&amp;ssl=1 1050w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/12.jpg?resize=300%2C225&amp;ssl=1 300w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/12.jpg?resize=768%2C576&amp;ssl=1 768w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/12.jpg?resize=1024%2C768&amp;ssl=1 1024w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/12.jpg?resize=200%2C150&amp;ssl=1 200w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/12.jpg?resize=360%2C270&amp;ssl=1 360w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/12.jpg?resize=545%2C409&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<!-- WP QUADS Content Ad Plugin v. 2.0.17.1 -->


<p><a href="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/13.jpg?ssl=1"><img loading="lazy" src="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/13.jpg?resize=1050%2C801&amp;ssl=1" alt="" width="1050" height="801" srcset="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/13.jpg?w=1050&amp;ssl=1 1050w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/13.jpg?resize=300%2C229&amp;ssl=1 300w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/13.jpg?resize=768%2C586&amp;ssl=1 768w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/13.jpg?resize=1024%2C781&amp;ssl=1 1024w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/13.jpg?resize=360%2C275&amp;ssl=1 360w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/13.jpg?resize=545%2C416&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p>Electronics-60 – the further development of Electronics 100/25. The same architecture, but bulky tape drives are a thing of the past.</p>
<p>They were replaced by flexible eight-inch floppy disks. The new chipset, allowed to fit the processor module, power supply and control devices, in a very compact size.</p>
<p>I note that all these computers were managers, that is, they worked with a bunch of external equipment. It could be machine tools, laboratory complexes, measuring devices.</p>
<p><a href="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/16.jpg?ssl=1"><img loading="lazy" src="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/16.jpg?resize=1050%2C779&amp;ssl=1" alt="" width="1050" height="779" srcset="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/16.jpg?w=1050&amp;ssl=1 1050w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/16.jpg?resize=300%2C223&amp;ssl=1 300w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/16.jpg?resize=768%2C570&amp;ssl=1 768w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/16.jpg?resize=1024%2C760&amp;ssl=1 1024w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/16.jpg?resize=360%2C267&amp;ssl=1 360w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/16.jpg?resize=545%2C404&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a> <a href="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/17.jpg?ssl=1"><img loading="lazy" src="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/17.jpg?resize=1050%2C1385&amp;ssl=1" alt="" width="1050" height="1385" srcset="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/17.jpg?w=1050&amp;ssl=1 1050w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/17.jpg?resize=227%2C300&amp;ssl=1 227w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/17.jpg?resize=768%2C1013&amp;ssl=1 768w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/17.jpg?resize=776%2C1024&amp;ssl=1 776w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/17.jpg?resize=360%2C475&amp;ssl=1 360w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/17.jpg?resize=545%2C719&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p><a href="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/18.jpg?ssl=1"><img loading="lazy" src="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/18.jpg?resize=1050%2C1418&amp;ssl=1" alt="" width="1050" height="1418" srcset="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/18.jpg?w=1050&amp;ssl=1 1050w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/18.jpg?resize=222%2C300&amp;ssl=1 222w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/18.jpg?resize=768%2C1037&amp;ssl=1 768w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/18.jpg?resize=758%2C1024&amp;ssl=1 758w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/18.jpg?resize=360%2C486&amp;ssl=1 360w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/18.jpg?resize=545%2C736&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p>15VM16-1 some early version of Electronics-60, having a control panel of light bulbs and switches. Assembled on the element base of the previous Electronics 100/25. I occupied a small nightstand built into the table on which the controlled equipment was placed.</p>
<p><a href="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/19.jpg?ssl=1"><img loading="lazy" src="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/19.jpg?resize=1050%2C858&amp;ssl=1" alt="" width="1050" height="858" srcset="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/19.jpg?w=1050&amp;ssl=1 1050w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/19.jpg?resize=300%2C245&amp;ssl=1 300w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/19.jpg?resize=768%2C628&amp;ssl=1 768w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/19.jpg?resize=1024%2C837&amp;ssl=1 1024w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/19.jpg?resize=360%2C294&amp;ssl=1 360w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/19.jpg?resize=545%2C445&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a> <a href="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/20.jpg?ssl=1"><img loading="lazy" src="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/20.jpg?resize=1050%2C814&amp;ssl=1" alt="" width="1050" height="814" srcset="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/20.jpg?w=1050&amp;ssl=1 1050w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/20.jpg?resize=300%2C233&amp;ssl=1 300w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/20.jpg?resize=768%2C595&amp;ssl=1 768w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/20.jpg?resize=1024%2C794&amp;ssl=1 1024w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/20.jpg?resize=360%2C279&amp;ssl=1 360w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/20.jpg?resize=545%2C423&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p>DVK-2M or Interactive Computing Complex. Massive, stylish in appearance, the computer of the 80s, which could be considered a personal computer.</p>
<p>It consisted of two desktop blocks – processor and pairing. A set of interchangeable interface cards allowed connecting drives of various types, a monochrome monitor on an openwork leg, and a keyboard.</p>
<p>Back in 1993, when we were studying, one teaching DVK could distribute programs for a couple of dozen Spectrums through the network.</p>
<p><a href="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/21.jpg?ssl=1"><img loading="lazy" src="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/21.jpg?resize=1050%2C788&amp;ssl=1" alt="" width="1050" height="788" srcset="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/21.jpg?w=1050&amp;ssl=1 1050w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/21.jpg?resize=300%2C225&amp;ssl=1 300w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/21.jpg?resize=768%2C576&amp;ssl=1 768w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/21.jpg?resize=1024%2C768&amp;ssl=1 1024w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/21.jpg?resize=200%2C150&amp;ssl=1 200w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/21.jpg?resize=360%2C270&amp;ssl=1 360w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/21.jpg?resize=545%2C409&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p><a href="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/22.jpg?ssl=1"><img loading="lazy" src="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/22.jpg?resize=1050%2C788&amp;ssl=1" alt="" width="1050" height="788" srcset="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/22.jpg?w=1050&amp;ssl=1 1050w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/22.jpg?resize=300%2C225&amp;ssl=1 300w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/22.jpg?resize=768%2C576&amp;ssl=1 768w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/22.jpg?resize=1024%2C768&amp;ssl=1 1024w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/22.jpg?resize=200%2C150&amp;ssl=1 200w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/22.jpg?resize=360%2C270&amp;ssl=1 360w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/22.jpg?resize=545%2C409&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p><a href="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/23.jpg?ssl=1"><img loading="lazy" src="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/23.jpg?resize=1050%2C788&amp;ssl=1" alt="" width="1050" height="788" srcset="https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/23.jpg?w=1050&amp;ssl=1 1050w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/23.jpg?resize=300%2C225&amp;ssl=1 300w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/23.jpg?resize=768%2C576&amp;ssl=1 768w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/23.jpg?resize=1024%2C768&amp;ssl=1 1024w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/23.jpg?resize=200%2C150&amp;ssl=1 200w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/23.jpg?resize=360%2C270&amp;ssl=1 360w, https://i2.wp.com/rusue.com/wp-content/uploads/2019/01/23.jpg?resize=545%2C409&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p>The DVK-3 in the monoblock plastic case is the next stage of the DVK-2M.</p>
<p><a href="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/24.jpg?ssl=1"><img loading="lazy" src="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/24.jpg?resize=1050%2C790&amp;ssl=1" alt="" width="1050" height="790" srcset="https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/24.jpg?w=1050&amp;ssl=1 1050w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/24.jpg?resize=300%2C226&amp;ssl=1 300w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/24.jpg?resize=768%2C578&amp;ssl=1 768w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/24.jpg?resize=1024%2C770&amp;ssl=1 1024w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/24.jpg?resize=200%2C150&amp;ssl=1 200w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/24.jpg?resize=360%2C270&amp;ssl=1 360w, https://i0.wp.com/rusue.com/wp-content/uploads/2019/01/24.jpg?resize=545%2C410&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p><a href="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/25.jpg?ssl=1"><img loading="lazy" src="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/25.jpg?resize=1050%2C811&amp;ssl=1" alt="" width="1050" height="811" srcset="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/25.jpg?w=1050&amp;ssl=1 1050w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/25.jpg?resize=300%2C232&amp;ssl=1 300w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/25.jpg?resize=768%2C593&amp;ssl=1 768w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/25.jpg?resize=1024%2C791&amp;ssl=1 1024w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/25.jpg?resize=360%2C278&amp;ssl=1 360w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/25.jpg?resize=545%2C421&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a> <a href="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/26.jpg?ssl=1"><img loading="lazy" src="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/26.jpg?resize=1050%2C740&amp;ssl=1" alt="" width="1050" height="740" srcset="https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/26.jpg?w=1050&amp;ssl=1 1050w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/26.jpg?resize=300%2C211&amp;ssl=1 300w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/26.jpg?resize=768%2C541&amp;ssl=1 768w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/26.jpg?resize=1024%2C722&amp;ssl=1 1024w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/26.jpg?resize=360%2C254&amp;ssl=1 360w, https://i1.wp.com/rusue.com/wp-content/uploads/2019/01/26.jpg?resize=545%2C384&amp;ssl=1 545w" sizes="(max-width: 1050px) 100vw, 1050px" data-recalc-dims="1"></a></p>
<p><a href="https://ralphmirebs.livejournal.com/226286.html" target="_blank" rel="noopener noreferrer">Source</a></p>
<div><!--Yasr Visitor Votes Shortcode--><div id="yasr_visitor_votes_10215"><p>Our Reader Score</p><p><span data-postid="10215" id="yasr-total-average-dashicon-10215"></span><span id="yasr-total-average-text-4fe4b3f1b8d50">Total: <span id="yasr-vv-votes-number-container-4fe4b3f1b8d50">83</span>  Average: <span id="yasr-vv-average-container-4fe4b3f1b8d50">4.8</span></span></p></div><!--End Yasr Visitor Votes Shortcode--></div>
<!-- WP QUADS Content Ad Plugin v. 2.0.17.1 -->


</div></div>]]>
            </description>
            <link>https://rusue.com/cemetery-of-soviet-computers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25340452</guid>
            <pubDate>Tue, 08 Dec 2020 01:29:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Spend your money where you spend your time]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25339736">thread link</a>) | @mcrittenden
<br/>
December 7, 2020 | https://critter.blog/2020/12/07/spend-your-money-where-you-spend-your-time/ | <a href="https://web.archive.org/web/*/https://critter.blog/2020/12/07/spend-your-money-where-you-spend-your-time/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-content" role="main">

	
<article id="post-4015">

	
<!-- .entry-header -->

	<div>

		<div>

			
<p>5 years ago, I bought some new glasses. I found a cheap $60 frame and I was proud. I’ve worn those glasses every waking hour for the last 5 years, which adds up to about 30,000 hours. They’re slightly uncomfortable and I have to push them back up on my nose a few times an hour, but I make do because you can’t argue with that price!</p>



<p>Meanwhile, I recently bought a new minivan to lug the kids around. I could have made do with one for $10,000-ish less, but I wanted a nice shiny one with a DVD player and all that. I use it maybe 30 minutes a day. </p>



<p>I’m realizing lately how backwards that logic is. I saved a couple hundred bucks on glasses and <a href="https://critter.blog/2020/11/06/all-self-help-boils-down-to-choose-long-term-over-short-term/">cursed myself with 30,000 hours</a> of discomfort and annoyance. And I spent an extra $10,000 on something that I barely even use compared to those dang glasses.</p>



<p>I think it makes sense to spend our money where we spend our time. What are the things we spend the most time with? Glasses, shoes, a computer and desk and chair, a bed, a belt, whatever it is. Even if one of those things is <em>slightly</em> worse than it could be, when you multiply that by <a href="https://critter.blog/2020/11/04/what-we-have-left/">a bajillion hours of usage</a>, it explodes.</p>



<p>Say that I had spent $500 on a pair of glasses that felt great. That’s an increase of $440. Divide $440 by 30,000 hours, and it turns out that I’d be paying about 1 penny per hour for that added comfort. No brainer, right? </p>



<p>Now let’s look at the car. If I use that car for 7 years that’s about 1300 hours of driving time. That means that the extra $10,000 comes out to about $7.70 per hour. I’m paying minimum wage to no one in exchange for access to a DVD player we barely use and a van that looks a little nicer.</p>



<p>Never again. Never again will I skimp on glasses and splurge on a car. I don’t even have a commute!</p>





		</div><!-- .entry-content -->

	</div><!-- .post-inner -->

	<!-- .section-inner -->

	
	<!-- .pagination-single -->

	
</article><!-- .post -->

</div></div>]]>
            </description>
            <link>https://critter.blog/2020/12/07/spend-your-money-where-you-spend-your-time/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25339736</guid>
            <pubDate>Tue, 08 Dec 2020 00:23:08 GMT</pubDate>
        </item>
    </channel>
</rss>
