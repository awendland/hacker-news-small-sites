<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 10]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 10. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Fri, 09 Oct 2020 08:29:16 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-10.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Fri, 09 Oct 2020 08:29:16 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[VSCode on Google Colab]]>
            </title>
            <description>
<![CDATA[
Score 91 | Comments 10 (<a href="https://news.ycombinator.com/item?id=24705599">thread link</a>) | @amitness
<br/>
October 6, 2020 | https://amitness.com/vscode-on-colab/ | <a href="https://web.archive.org/web/*/https://amitness.com/vscode-on-colab/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<header>

<p>
<span>

2 minute read
</span>
</p>
</header>
<section itemprop="text">
<p>I recently discovered a way to set up VSCode on Google Colab and use it as an editor to write code and run experiments on the Colab VM.</p>
<p>With this setup, you can still prototype in the Colab Notebook while also using VSCode for all the advantages of a full-fledged code editor. Here is how you can replicate my setup.</p>
<h2 id="approach-1-python-package">Approach 1: Python Package</h2>
<p>In this setup, we use the <a href="https://github.com/abhishekkrthakur/colabcode">colab-code</a> package that automates all the manual setup steps previously described in the <strong>Approach 2</strong> section of this blog post. You can make a copy of this <a href="https://colab.research.google.com/github/abhishekkrthakur/colabcode/blob/master/colab_starter.ipynb">notebook</a> directly to get started.</p>
<ol>
<li>
<p>First, install the <code>colab-code</code> package using the following command:</p>

</li>
<li>
<p>Now, import <code>ColabCode</code> class from the package and specify the port and password.</p>
<div><div><pre><code> <span>from</span> <span>colabcode</span> <span>import</span> <span>ColabCode</span>
 <span>ColabCode</span><span>(</span><span>port</span><span>=</span><span>10000</span><span>,</span> <span>password</span><span>=</span><span>"password123"</span><span>)</span>
</code></pre></div> </div>
<p>You can also use it directly with the default port and without any password as shown below.</p>
<div><div><pre><code> <span>from</span> <span>colabcode</span> <span>import</span> <span>ColabCode</span>
 <span>ColabCode</span><span>()</span>
</code></pre></div> </div>
</li>
<li>
<p>You will get the ngrok URL in the output. Click the link and a login page will open in a new tab.</p>
<p><img src="https://amitness.com/images/colab-code-step-1.png" alt="Generated NGROK URL"></p>
</li>
<li>
<p>Type the password you had set in step 2 and click submit. If the page gets stuck for more than 4-5 seconds, refresh the page and you should be redirected to the editor.</p>
<p><img src="https://amitness.com/images/colab-code-step-2.png" alt="Authenticating with password in VSCode"></p>
</li>
<li>
<p>Now you will get access to the editor interface and can use it to work on python files.</p>
<p><img src="https://amitness.com/images/colab-code-step-3.png" alt="VSCode Interface"></p>
</li>
</ol>
<h2 id="approach-2-manual-setup">Approach 2: Manual Setup</h2>
<p>I have described the setup steps in detail below. After going through all the steps, please use this <a href="https://colab.research.google.com/drive/1yvUy5Gn9lPjmCQH6RjD_LvUO2NE0Z7RM?usp=sharing">colab notebook</a> to try it out directly.</p>
<ol>
<li>
<p>First, we will install the <a href="https://github.com/cdr/code-server">code-server</a> package to run VSCode editor as a web app. Copy and run the following command on colab to install <code>code-server</code>.</p>
<div><div><pre><code> !curl -fsSL https://code-server.dev/install.sh | sh
</code></pre></div> </div>
</li>
<li>
<p>After the installation is complete, we will expose a random port <code>9000</code> to an external URL we can access using the <code>pyngrok</code> package. To install <code>pyngrok</code>, run</p>
<div><div><pre><code> <span>!</span>pip <span>install</span> <span>-qqq</span> pyngrok
</code></pre></div> </div>
</li>
<li>
<p>Then, run the following command to get a public ngrok URL. This will be the URL we will use to access VSCode.</p>
<div><div><pre><code> <span>from</span> <span>pyngrok</span> <span>import</span> <span>ngrok</span>
 <span>url</span> <span>=</span> <span>ngrok</span><span>.</span><span>connect</span><span>(</span><span>port</span><span>=</span><span>9000</span><span>)</span>
 <span>print</span><span>(</span><span>url</span><span>)</span>
</code></pre></div> </div>
</li>
<li>
<p>Now, we will start the VSCode server in the background at port 9000 without any authentication using the following command.</p>
<div><div><pre><code> !nohup code-server --port 9000 --auth none &amp;
</code></pre></div> </div>
</li>
<li>
<p>Now, you can access the VSCode interface at the URL you got from step 3. The interface and functionality are the same as the desktop version of VSCode.</p>
</li>
</ol>
<p><img src="https://amitness.com/images/colab-vscode.png" alt="Example of a running instance of VSCode server"></p>
<h2 id="usage-tips">Usage Tips</h2>
<ol>
<li>
<p>You can switch to the dark theme by going to the bottom-left corner of the editor, clicking the <strong>settings icon</strong>, and then clicking ‚Äò<strong>Color Theme</strong>‚Äô.</p>
<p><img src="https://amitness.com/images/colab-dark-theme-step-1.png" alt="Switching to dark theme on VSCode"></p>
<p>A popup will open. Select <strong>Dark (Visual Studio)</strong> in the options and the editor will switch to a dark theme.
<img src="https://amitness.com/images/colab-dark-theme-step-2.png" alt="Theme selection interface on VSCode"></p>
</li>
<li>
<p>All the keyword shortcuts of regular VSCode works with this. For example, you can use <code>Ctrl + Shift + P</code> to open a popup for various actions.</p>
<p><img src="https://amitness.com/images/vscode-ctrl-shift-p.png" alt="Action popup in VSCode"></p>
</li>
<li>
<p>To open a terminal, you can use the shortcut <code>Ctrl + Shift + `</code>.</p>
<p><img src="https://amitness.com/images/vscode-terminal.png" alt="Opening integrated terminal in VSCode"></p>
</li>
<li>
<p>To get python code completions, you can install the Python(<code>ms-python</code>) extension from the extensions page on the left sidebar.</p>
<p><img src="https://amitness.com/images/vscode-code-completions.png" alt="Installing extensions in VSCode"></p>
</li>
<li>
<p>The Colab interface is still usable as a notebook and regular functions to upload and download files and mount with Google Drive. Thus, you get the benefits of both a notebook and a code editor.</p>
</li>
</ol>
<h2 id="references">References</h2>
<ul>
<li><a href="https://github.com/cdr/code-server/blob/v3.5.0/doc/FAQ.md">Code-Server FAQs</a></li>
<li><a href="https://pyngrok.readthedocs.io/en/latest/">pyngrok - a Python wrapper for ngrok</a></li>
</ul>
</section>



</div></div>]]>
            </description>
            <link>https://amitness.com/vscode-on-colab/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24705599</guid>
            <pubDate>Wed, 07 Oct 2020 05:38:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Dissertation: Marine Insurance in the Netherlands 1600-1870 (2009) [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24705036">thread link</a>) | @johntfella
<br/>
October 6, 2020 | https://research.vu.nl/ws/portalfiles/portal/42182698/complete+dissertation.pdf | <a href="https://web.archive.org/web/*/https://research.vu.nl/ws/portalfiles/portal/42182698/complete+dissertation.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://research.vu.nl/ws/portalfiles/portal/42182698/complete+dissertation.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24705036</guid>
            <pubDate>Wed, 07 Oct 2020 03:30:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[USB3: Why it's a bit harder than USB2]]>
            </title>
            <description>
<![CDATA[
Score 117 | Comments 55 (<a href="https://news.ycombinator.com/item?id=24704298">thread link</a>) | @panic
<br/>
October 6, 2020 | https://lab.ktemkin.com/post/why-is-usb3-harder/ | <a href="https://web.archive.org/web/*/https://lab.ktemkin.com/post/why-is-usb3-harder/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    

    <p>A few people on twitter have asked me to explain why the USB3 winds up being much harder to implement than USB2.
The answer is more than will fit in a single tweet, so I thought I'd put a quick-but-rough answer, here. This is
by no means comprehensive; consider it <del>a longer tweet</del> what a tweet would be given I had more than 240 characters and a proclivity to babble. (I do.)</p>
<p>A lot of the challenges come from the way we work around <em>physical-layer</em> limitations. Put poetically, physics gives
us lots of little obstacles we have to work around in order to talk at 5 billion transfers per second (5GT/s).</p>
<h5 id="its-hard-to-establish-common-dc-operating-conditions-on-both-sides-of-a-link">It's hard to establish common ‚ÄúDC operating conditions‚Äù on both sides of a link.</h5>
<p>It's not trivial to get the same bias voltages ‚Äì and common grounds ‚Äì across a long motherboard or down a cable ‚Äì and when you're operating at really high frequencies, you're a lot more sensitive to changes in your operating environment. In USB3, we work around this by <em>capacitively isolating</em> both sides of the link from each other ‚Äì in short, we use capacitors to ensure only signal <em>changes</em> are carried across the link, which means that both sides can establish their own local operating conditions.</p>
<figure>
    <img src="https://lab.ktemkin.com/post-media/why-is-usb3-harder/circuit.png" alt="diagram showing the transmitter is connected to the receiver through a pair of AC coupling capacitors"> <figcaption>
            <h4>From the USB3.2 specification: diagram showing how signals are isolated</h4>
        </figcaption>
</figure>

<p>This puts some requirements on the digital protocols used to exchange data. Because data currents are exchanged as the relevant capacitors charge and discharge, <em>capacitive coupling</em> only works when those capacitors have room to charge and discharge. <strong>This means our data must be DC-balanced; we have to spend as much time charging those capacitors as we do discharging them</strong>. In digital terms, this means we have to encode the data in a way that sends the same amount of <code>1</code>s and <code>0</code>s.</p>
<h5 id="its-hard-to-establish-a-common-clock-across-both-sides-of-a-link">It's hard to establish a ‚Äúcommon clock‚Äù across both sides of a link.</h5>
<p>When sending serial data, you typically have two challenges: you need to make sure both sides are sampling the data <em>at the same rate</em>, and that both sample clocks are <em>synchronized enough</em> that you're sampling at the right point. Many high-speed protocols deal with this using a technique called <em>clock recovery</em>, which essentially means that each receiver looks at the data it receives and tries to figure out what the clock that produced it looks like.</p>
<p>If both sides have agreed on a clock rate, this can be simple, in theory: if the receiver sees a change in its
received data, it can infer that that changed happened <em>on an active edge of the transmitter's clock</em>, and so it can start to figure out how to align its internal clock with the transmitter's.</p>
<p>This introduces another protocol requirement: <strong>for <em>clock recovery</em> to work, the data has to change frequently enough that the two sides can keep synchronized</strong>. At 5GT/s and high data throughputs, there's not much time for clocks to become synchronized when a packet is received; accordingly, it's important that data is encoded with lots of transitions, even when the line is idle.</p>
<p><strong>To ensure both <em>DC-Balance</em> and <em>sufficient transition density</em>, USB3 uses a method of encoding called 8b10b encoding.</strong>
In this encoding scheme, every single byte of data is transmitted as ten bits, with encodings chosen so that:</p>
<ul>
<li>A typical data byte can be transmitted <em>either</em> as a code with <em>one more one than zero</em>, or <em>one more zero than one</em>.
This allows the transmitter to choose between the two encodings, in order to keep the data stream at 50% ones.</li>
<li>Every valid encoding has sufficient <em>transition density</em> to ensure that it's useful for clock recovery.</li>
</ul>
<p>I won't go into more 8b10b background here, but you can read about the typical IBM implementation <a href="https://en.wikipedia.org/wiki/8b/10b_encoding">on wikipedia</a>.</p>
<h5 id="its-hard-to-run-both-sides-of-the-link-at-the-same--clock-rate-">It's hard to run both sides of the link at the same <em>clock rate</em>.</h5>
<p>Even with successful <em>clock recovery</em>, it's difficult to have both sides of the link produce and consume data at
the same rate. Each side's internal logic is running off of its own <em>clock source</em>; and every clock has a bit of deviation from its nominal frequency. For the protocol to function despite these differences, the USB3 specification allows each clock to deviate from its nominal value by up to a certain <em>tolerance</em>; and specifies a method for compensating for this tolerance. This technique is appropriately named <em>clock tolerance compensation</em>, or CTC.</p>
<p><strong>To compensate for mismatches in sender/receiver clock rates, USB3 requires senders to periodically insert filler data into their transmitted data-stream</strong>. Receivers can then discard this data; allowing a brief pause in which the slower
side of the link can ‚Äúcatch up‚Äù. For this to be useful, the filler data (called ‚Äòskip sets‚Äô) must be sent regularly;
which means additional logic on the transmitter side for insertion, and additional logic on the receiver side for
removal.</p>
<h5 id="its-hard-to-deal-with-varying-electrical-properties-of-different-transmitters-receivers-and-cables">It's hard to deal with varying electrical properties of different transmitters, receivers, and cables.</h5>
<p>When operating at very high frequencies, all of the little non-idealities along your transmission path really add up. At slower data rates, there's plenty of time for digital signals to ‚Äúsettle‚Äù after a change; making the non-ideal properties of your transmission lines less important. The faster your data gets, the more important it is for your data
to reach a ‚Äúreadable‚Äù value quickly.</p>
<p>To help with this, most high-speed receivers employ a technique called <em>receiver equalization</em>, which uses analog hardware
to help reshape signal transitions, so they can be more reliably sampled. Equalization helps to ‚Äúcancel out‚Äù some of the ways the non-ideal transmission path adversely affects the signal.</p>
<figure>
    <img src="https://lab.ktemkin.com/post-media/why-is-usb3-harder/eye.png" alt="diagram showing a variety of slow rises and falls; illustrating that the physical link slows transitions"> <figcaption>
            <h4>From the USB3.2 specification: an 'eye diagram', which shows an overlay of many rising and falling transitions, illustrating how non-ideal properties affect the link.</h4>
        </figcaption>
</figure>

<p>Since every transmission path is different ‚Äì due to different transmitter, receiver, and cable properties ‚Äì it's impossible to create a single ‚Äúone size fits all‚Äù equalizer. Instead, each USB3 equalizer needs to be tuned to its transmission path via a process called <em>link training</em>.</p>
<p><strong>At the start of each USB3 communication, link partners repeatedly exchange collections of known data called <em>training sets</em>, which give the opportunity for each side to tune their equalizer.</strong> Training sets include both sets of data chosen to have high transition density and sets designed to include a wide range of ‚Äúnormally-distributed‚Äù data.</p>
<p>During a few milliseconds of data exchange ‚Äì an eternity in fast-protocol terms ‚Äì both sides of the link gradually
tweak their equalizer settings until they're clearly seeing the expected values from the other side.</p>
<h5 id="its-hard-not-to-generate-harmful-interference">It's hard not to generate harmful interference.</h5>
<p>USB3 has a very high transition rate ‚Äì it easily qualifies as high radio-frequency signaling ‚Äì and its link
often tends to exchange repeating data. This has a nasty side effect: even a well-functioning link can act as an
antenna; unintentionally emitting RF that can interfere with nearby systems. The more repeating elements this signaling
has, the more troublesome the interference tends to be.</p>
<p><strong>To reduce the amount of harmful interference generated, USB3 links use a technique called <em>scrambling</em>, in which data is XOR'd with a fixed pattern before transmission.</strong> The receiver is then capable of applying the same transform to <em>descramble</em> the data stream, recovering the relevant data.</p>
<p>You can think of scrambling as being very similar to encryption ‚Äì except everyone knows the key. Once data is scrambled, it looks a lot more like ‚Äúrandom numbers‚Äù than the pre-scrambling data ‚Äì and accordingly, it's a lot less likely to
generate troublesome interference. Once the scrambled data travels the link, it can be <em>descrambled</em> by the receiving end ‚Äì a process similar to decryption ‚Äì restoring the original data stream.</p>
<h5 id="in-summary">In summary‚Ä¶</h5>
<p>In summary, before you can even exchange meaningful data, the digital side of your device needs:</p>
<ul>
<li><strong>8b10b encoding and decoding hardware</strong>, so the data exchanged is <em>DC-balanced</em> and contains sufficient transitions as to allow <em>clock recovery</em>;</li>
<li><strong>Clock Tolerance Compensation hardware</strong>, which allows the two sides to communicate even with slightly-varying clock frequencies;</li>
<li>Hardware to orchestrate <strong>link training</strong> and <strong>receiver equalization</strong>, which helps to deal with non-ideal transmission properties;</li>
<li><strong>Scrambling</strong> and <strong>descrambling</strong> hardware, which help to reduce harmful interference.</li>
</ul>
<p>This omits a few minor things, such as USB3's <em>Low Frequency Periodic Signaling</em>; but these are the major components.</p>
<h5 id="oh-and-one-more-thing-its-hard-to-get-good-resources">Oh, and one more thing: it's hard to get good resources.</h5>
<p>Finally, ignoring all the physical layer challenges associated with bringing a link up, there's one more major obstacle: it's hard to get good resources for working with USB3:</p>
<ul>
<li>Most hardware enabling custom USB designs is expensive; and <a href="https://lab.ktemkin.com/post/ab07-usb3fmc-wtf/">still rife with issues</a>.</li>
<li>Most USB3 tooling is <a href="https://www.totalphase.com/products/beagle-usb5000-v2-ultimate/">very expensive</a>, and still rife with issues.</li>
<li>There's very little documentation in support of the specification; and what documentation exists still hasn't been
used enough to <a href="https://lab.ktemkin.com/post/mindshare-usb3/">identify all of its errors</a>.</li>
</ul>
<p>Hopefully, at some point, I'll have built enough tooling to change this.</p>


  </article></div>]]>
            </description>
            <link>https://lab.ktemkin.com/post/why-is-usb3-harder/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24704298</guid>
            <pubDate>Wed, 07 Oct 2020 01:11:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Thoughts on Meaning and Writing]]>
            </title>
            <description>
<![CDATA[
Score 21 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24703646">thread link</a>) | @exolymph
<br/>
October 6, 2020 | https://dormin.org/2020/10/06/thoughts-on-meaning-and-writing/ | <a href="https://web.archive.org/web/*/https://dormin.org/2020/10/06/thoughts-on-meaning-and-writing/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-870">

	
	<!-- .entry-header -->


			<div>

			
<p>I was once at a dinner party and someone was telling me about her recent trip to Arizona. One of the highlights was visiting a biodome project where scientists had attempted to achieve a totally self-sustained structure in preparation for the colonization of other planets. I told her that Steve Bannon used to run that place. She didn‚Äôt believe me. I obnoxiously pulled out my phone and showed her Bannon‚Äôs Wikipedia page.</p>



<p>Steve Bannon has lived a fascinating life. That‚Äôs not an evaluation of his politics or morality, it‚Äôs a statement of fact. Witness Bannon‚Äôs life in bullet points:</p>



<ul><li>Born in 1953 in Norfolk, Virginia to a telephone lineman and a housewife</li><li>Attended military prep school and then Virginia Tech for a degree in Urban Planning, was elected president of the student body, worked in a junk yard</li><li>Served in the navy for seven years in the Pacific fleet</li><li>While in the navy, earned a Masters in National Security Studies from Georgetown</li><li>After leaving the navy, earned an MBA from Harvard</li><li>Got a job at Goldman Sachs as an investment banker in the mergers &amp; acquisitions division, worked way up to a Vice President position</li><li>Left Goldman Sachs with some colleagues to launch Bannon &amp; Co., a boutique investment bank specializing in media, nabbed a small slice of syndication rights to mega-hit tv show <strong>Seinfeld</strong>, still receives residual payments from the show to this day</li></ul>



<ul><li>While running Bannon &amp; Co., became the Acting Director of Biosphere 2 in Arizona, a project to design a self-sustaining habitat in preparation for the colonization of other planets</li><li>Dove into film production, executive produced 18 Hollywood films</li><li>Wrote and directed 12 documentaries</li><li>With an investment from Goldman Sachs, founded Internet Gaming Media, a World of Warcraft gold mining operation</li><li>Married and divorced three times, has three daughters</li><li>Charged with misdemeanor domestic violence, battery &amp; assault, and dissuading a witness based on accusations from his wife</li><li>Co-founded Government Accountability Institute, a conservative think tank</li><li>Was senior vice president of Cambridge Analytica, the company known for its influence in the 2016 presidential election and Brexit referendum</li><li>On the founding board of Breitbart News, served as its Editor-in-Chief and eventual Executive Chair, hosted its radio show</li><li>Became Chief Executive of Trump‚Äôs presidential campaign 88 days before Election Day, was considered the only person on Trump‚Äôs staff (including Trump himself) who thought he could win</li><li>Is generally considered the ideological mastermind behind the ‚ÄúTrumpism‚Äù ideology ‚Äì populist, anti-globalist, culturally conservative, economically liberal</li><li>Served as Chief Strategist in Trump‚Äôs White House</li><li>After leaving the White House, built an international ‚Äúinfrastructure‚Äù for Trumpism by supporting the formation of a dozen significant political parties across Europe</li><li>Formed a partnership with outlaw Chinese billionaire Guo Wengui, pronounced the formation of an independent Chinese state to overthrow the current communist regime</li><li>Recently arrested for alleged fraud and money laundering connected to the We Build the Wall campaign, currently awaiting trial</li></ul>



<p>Again, I‚Äôm not making any moral judgement on Bannon or any of particular actions. I‚Äôm just amazed that he has done <em>so much</em>. Almost every one of these bullet points would be one of the most significant events in a normal person‚Äôs life.</p>



<p>Arnold Schwarzenegger‚Äôs life in bullet points is also incredible:</p>



<ul><li>Born in 1947 in rural Austrian poverty</li><li>Father was a physically abusive (willing) ex-Nazi wounded at Stalingrad who supposedly resented his son for suspected illegitimacy</li><li>Brother died in car crash while drunk driving, left behind a three-year-old son whom Schwarzenegger would later support</li><li>Started lifting weights at age 14 or 15</li><li>Served in the Austrian army for mandatory one year of service</li><li>While in the army, won Junior Mr. Europe weightlifting contest, was arrested and imprisoned for a week for going AWOL to attend the competition</li><li>Moved to London in his late teens to live and train with a weightlifting judge</li><li>At age 20, became the youngest Mr. Universe winner ever, first won the amateur contest, then won the professional contest three years in a row</li><li>Moved to Los Angeles, most likely lived as an illegal immigrant for years</li><li>First won Mr. Olympia at age 23, youngest ever; won contest six more times; became known as one of the greatest bodybuilders of all time</li><li>Attended classes at three different colleges to attain a degree in Business Administration and Marketing</li><li>Appeared in a handful of action films, was told by agents that his body was too ‚Äúweird,‚Äù accent too thick, and name too long</li><li>Started a surprisingly successful bricklaying business and a mail-order business with another bodybuilder, also invested in numerous real estate companies</li><li>Became a millionaire by age 30</li><li>Starred in <strong>Pumping Iron</strong> and then <strong>Conan the Barbarian</strong>, breakthrough roles</li><li>Starred in <strong>Terminator, Predator, Commando, Running Man, Total Recall, Kindergarten Cop, Terminator 2, True Lies, </strong>and became known as one of the great action stars ever</li><li>Invested in Planet Hollywood, a shopping mall in Ohio, a restaurant, Dimensional Fund Advisors, a movie production company, numerous fitness magazines, and a fitness competition</li><li>Appointed Chairman of President Clinton‚Äôs Council of Physical Fitness and Sports</li><li>Ran for Governor of California (the most populous and wealthiest state in the wealthiest country in the world) and won despite having no political experience</li><li>Won re-election as a Republican during the height of anti-Bush sentiment</li><li>Went back to acting and continues to star in movies to this day</li><li>Married and divorced a Kennedy, had four children with her, plus another child with his housekeeper</li><li>Current net worth estimated at $100-200 million, may have peaked at $800 million before the divorce</li></ul>



<p>I‚Äôve read a lot of Wikipedia pages and I find they all end up as bullet points in my head. That‚Äôs kind of how I think about everything‚Ä¶ as lists of the most important things. Everything else fades away over time.</p>



<p>As you can tell from this blog, I‚Äôve spent a lot of time thinking about <em>great</em> people. Napoleon, Cortes, Genghis Khan, Mark Schilling, Hideo Kojima, and Tommy Wiseau are not all good people, but they are <em>great</em>. Each one could easily earn a bullet point list like Bannon‚Äôs and Schwarzenegger‚Äôs. They‚Äôve all built lives full of notable activities and accomplishments which will live on in history in one way or another.</p>



<p>I don‚Äôt think I‚Äôll live on in history for very long after I die except for boring stuff like my tax records. And I‚Äôm fine with that. It‚Äôs not clear that living a historically noteworthy life is desirable in and of itself.</p>



<p>But I do want a noteworthy life on my own terms. I want my life to resemble one of these bullet point lists within the reasonable bounds of what I can experience and achieve given my abilities, resources, and will. Especially when I‚Äôm older, I want to be able to sit at a computer and type out the actions, events, and people that I remember and be proud of the list before me.</p>



<p>This is my personal heuristic for <em>meaning</em> in life. Both in the short and long term I try to do things which one day could be put on a bullet point list about me. Or maybe I just try to do things I‚Äôll <em>remember</em>.</p>



<p>Pick a random year from your life and try to write a bullet point list of things that happened that year. If I do that for any year in the past decade, I usually get‚Ä¶</p>



<ul><li>The girl I was dating at the time (if any)</li><li>Places I traveled</li><li>The job I had (though I rarely remember any particular thing I did in that year on the job)</li><li>Significant money I made or lost</li><li>Significant events experienced with friends</li><li>It‚Äôs a bit harder to pinpoint specific years, but I can always remember the general era when I experienced particular passion projects/activities, like movies/tv shows/books/video games/athletics, etc. For instance, I distinctly remember playing <em>Skyrim</em> in college and <em>Faster than Light</em> while at the office in 2015.</li></ul>



<p>So what I tend to remember are relationships, work, money, and passion projects. I don‚Äôt think that‚Äôs uncommon, though the weight I give each one might be. Regardless, these are the things that matter to me. They should be my major goals in life. They are what I‚Äôll look back on and remember when I‚Äôm 30, 40, 50, and close to death. Hopefully.</p>



<p>There‚Äôs this Anthony Bourdain quote I like:</p>



<blockquote><p>‚ÄúI understand there‚Äôs a guy inside me who wants to lay in bed, smoke weed all day, and watch cartoons and watch old movies. My whole life is a series of stratagems to avoid and outwit that guy.‚Äù</p></blockquote>



<p>I very much sympathize. Though my version of that is sitting on the couch, watching old episodes of <strong>Archer</strong>, and playing my hundredth <strong>Crusader Kings 2</strong> campaign.</p>



<p>I think what Bourdain‚Äôs struggle comes down to is finding meaning every day. It‚Äôs the easiest thing in the world to let each day go by filled with nothing but obligations and short term stimulation. Nothing long-term is gained, nothing is remembered, and when you wake up the next morning you‚Äôre a day older with nothing important added to your existence. This will be 99.9% of the days in your life.</p>



<p>Resisting this reality is literally a constant challenge. One way I try to resist is to consider which specific actions which will result in discrete memorable experiences. That is, I try to do things I‚Äôll remember even if I don‚Äôt remember the day itself. For instance, what am I more likely to remember doing tonight: playing another three out of literally thousands of hours of <strong>Crusader Kings 2</strong>, or watching a new movie?</p>



<p>The movie, of course. Even if the movie is bad, or boring, or forgettable, I will still vaguely recall having seen it years from now, but there is almost no chance I‚Äôll remember those random three hours of <strong>Crusader Kings 2</strong> regardless of how much I enjoyed them. </p>



<p>Alternatively, you can deal with Bourdain‚Äôs problem by trying to make small contributions to greater acts of meaning each day. I‚Äôve kept an IMDB account since I was ‚Ä¶</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dormin.org/2020/10/06/thoughts-on-meaning-and-writing/">https://dormin.org/2020/10/06/thoughts-on-meaning-and-writing/</a></em></p>]]>
            </description>
            <link>https://dormin.org/2020/10/06/thoughts-on-meaning-and-writing/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24703646</guid>
            <pubDate>Tue, 06 Oct 2020 23:25:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I will never buy another CyberPower UPS]]>
            </title>
            <description>
<![CDATA[
Score 18 | Comments 10 (<a href="https://news.ycombinator.com/item?id=24703593">thread link</a>) | @monstermunch
<br/>
October 6, 2020 | https://blog.networkprofile.org/cyberpower-ups-avoid/ | <a href="https://web.archive.org/web/*/https://blog.networkprofile.org/cyberpower-ups-avoid/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://blog.networkprofile.org/content/images/size/w300/2020/10/2020-10-06-16.50.41-1.JPG 300w,
                            https://blog.networkprofile.org/content/images/size/w600/2020/10/2020-10-06-16.50.41-1.JPG 600w,
                            https://blog.networkprofile.org/content/images/size/w1000/2020/10/2020-10-06-16.50.41-1.JPG 1000w,
                            https://blog.networkprofile.org/content/images/size/w2000/2020/10/2020-10-06-16.50.41-1.JPG 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://blog.networkprofile.org/content/images/size/w2000/2020/10/2020-10-06-16.50.41-1.JPG" alt="Cyberpower UPS's - Try something else...">
            </figure>

            <section>
                <div>
                    <p>This is now the fourth time I have come across these issues with a Cyberpower UPS. It means your UPS really isn't a UPS, so you may want to think twice about picking one up</p><p>First thing to keep in mind is that these are line interactive UPS's, that means the input power isn't going through the batteries at all like with a double conversion UPS, but its just passing right through to your devices while maybe doing some minor filtering and surge protection. These devices simply do not need batteries in them to provide output power</p><h2 id="issue-1">Issue 1</h2><p>What happens is that the UPS will work fine, until it can no longer charge the batteries. When that happens, the UPS will both shut down output power (Even though the power from the wall is fine) and continue to try and charge the batteries while giving you an error code E24 that says "Internal Fault - CONTACT SUPPORT" in the manual </p><p>If you do contact support, they tell you that the UPS needs to be replaced entirely. What it really means is that you have bad batteries, and for around $40 you can be back in business.</p><p>They also claim that the reason it shutdown output power is for safety reasons, but then why does it try and charge the batteries still? Your guess is as good as mine. Even worse, when you turn the UPS off and take the batteries out, they are burning hot, because the UPS is trying its best to charge bad batteries.</p><h2 id="issue-2">Issue 2</h2><p>The second issue with them is that when the batteries are clearly bad and cannot supply a load, but still give a nominal 12v charge, the UPS thinks its fine. Even if it fails the self test, it reports that is has succeeded.</p><p>I have a small USB LED Light that probably doesn't even draw 1w. If I initiate a self test, the UPS just shuts off completely, and when powered back on, reports everything is great, and the software says the test passed. Awesome!</p><figure><img src="https://blog.networkprofile.org/content/images/2020/10/2020-10-06-16.34.55.JPG" alt="" srcset="https://blog.networkprofile.org/content/images/size/w600/2020/10/2020-10-06-16.34.55.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2020/10/2020-10-06-16.34.55.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2020/10/2020-10-06-16.34.55.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2020/10/2020-10-06-16.34.55.JPG 2400w" sizes="(min-width: 720px) 720px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2020/10/2020-10-06-16_46_30-PowerPanel--Personal-_-CyberPower-Systems.png" alt="" srcset="https://blog.networkprofile.org/content/images/size/w600/2020/10/2020-10-06-16_46_30-PowerPanel--Personal-_-CyberPower-Systems.png 600w, https://blog.networkprofile.org/content/images/2020/10/2020-10-06-16_46_30-PowerPanel--Personal-_-CyberPower-Systems.png 861w" sizes="(min-width: 720px) 720px"></figure><p>Every other UPS I come across would fail the self test while NOT shutting down output power, and it would alert you that you need new batteries.</p><p>Watch this video: </p><figure></figure><p>In the end of the video you can see the UPS lost connection, but that doesn't even matter as the Cyberpower software marks the test complete before that even happens, as you can see in the screenshot I posted above the video.</p><p>This issue is compounded by the fact that the UPS completely yet again turns itself off. That means if you had a PC plugged into the UPS, and another plugged into the wall and had a very minor power dip, the one directly in the wall would probably remain on, and the one plugged into the UPS would be turned off.</p><p>Doesn't that defeat the entire point of a UPS? If you have one of these units, you need to be prepared that you will lose power unexpectedly at some point. The fact you can't test the batteries without losing power means that you will either need to shutdown your computer/servers/etc every 2 weeks and test, or just replace the batteries WELL ahead of schedule, say every 2 years just to avoid losing power.</p><p>A better solution would be to toss the UPS, and get a better one </p><p>I have found this issue in every single Cyberpower UPS I have ever used which is 4 units in the PFCLCD series and AVRLCD series</p><p>Just avoid them entirely, and go with a different brand.</p>
                </div>
            </section>



        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://blog.networkprofile.org/cyberpower-ups-avoid/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24703593</guid>
            <pubDate>Tue, 06 Oct 2020 23:19:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[DOMPurify bypass: XSS via HTML namespace confusion]]>
            </title>
            <description>
<![CDATA[
Score 138 | Comments 59 (<a href="https://news.ycombinator.com/item?id=24703230">thread link</a>) | @fanf2
<br/>
October 6, 2020 | https://research.securitum.com/mutation-xss-via-mathml-mutation-dompurify-2-0-17-bypass/ | <a href="https://web.archive.org/web/*/https://research.securitum.com/mutation-xss-via-mathml-mutation-dompurify-2-0-17-bypass/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1639">
	<!-- .entry-header -->

	
	<div>
		
<p>In this blogpost I‚Äôll explain my recent bypass in <a href="https://github.com/cure53/DOMPurify/">DOMPurify</a> ‚Äì the popular HTML sanitizer library. In a nutshell, DOMPurify‚Äôs job is to take an untrusted HTML snippet, supposedly coming from an end-user, and remove all elements and attributes that can lead to Cross-Site Scripting (XSS).</p>



<p>This is the bypass:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f801f5815cb7983640176" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>form</span><span>&gt;</span></p><p><span>&lt;</span><span>math</span><span>&gt;</span><span>&lt;</span><span>mtext</span><span>&gt;</span></p><p><span>&lt;</span><span>/</span><span>form</span><span>&gt;</span><span>&lt;</span><span>form</span><span>&gt;</span></p><p><span>&lt;</span><span>mglyph</span><span>&gt;</span></p><p><span>&lt;</span><span>style</span><span>&gt;</span><span>&lt;</span><span>/</span><span>math</span><span>&gt;</span><span>&lt;</span><span>img </span><span>src </span><span>onerror</span><span>=</span><span>alert</span><span>(</span><span>1</span><span>)</span><span>&gt;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0003 seconds] -->




<p>Believe me that there‚Äôs not a single element in this snippet that is superfluous üôÇ </p>



<p>To understand why this particular code worked, I need to give you a ride through some interesting features of HTML specification that I used to make the bypass work.</p>



<h2>Usage of DOMPurify</h2>



<p>Let‚Äôs begin with the basics, and explain how DOMPurify is usually used. Assuming that we have an untrusted HTML in <code>htmlMarkup</code> and we want to assign it to a certain <code>div</code>, we use the following code to sanitize it using DOMPurify and assign to the <code>div</code>:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f801f5815cbe576224927" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>div</span><span>.</span><span>innerHTML</span><span> </span><span>=</span><span> </span><span>DOMPurify</span><span>.</span><span>sanitize</span><span>(</span><span>htmlMarkup</span><span>)</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0001 seconds] -->




<p>In terms of parsing and serializing HTML as well as operations on the DOM tree, the following operations happen in the short snippet above:</p>



<ol><li><code>htmlMarkup</code> is parsed into the DOM Tree.</li><li>DOMPurify sanitizes the DOM Tree (in a nutshell, the process is about walking through all elements and attributes in the DOM tree, and deleting all nodes that are not in the allow-list).</li><li>The DOM tree is serialized back into the HTML markup.</li><li>After assignment to <code>innerHTML</code>, the browser parses the HTML markup again.</li><li>The parsed DOM tree is appended into the DOM tree of the document.</li></ol>



<p>Let‚Äôs see that on a simple example. Assume that our initial markup is <code>A&lt;img src=1 onerror=alert(1)&gt;B</code>. In the first step it is parsed into the following tree:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-1-1024x104.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-1-1024x104.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-1-300x30.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-1-768x78.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-1-1536x155.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-1-1320x134.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-1.png 1956w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Then, DOMPurify sanitizes it, leaving the following DOM tree:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-2-1024x107.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-2-1024x107.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-2-300x31.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-2-768x80.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-2-1536x161.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-2-1320x138.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-2.png 1952w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Then it is serialized to:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		
<!-- [Format Time: 0.0001 seconds] -->




<p>And this is what <code>DOMPurify.sanitize</code> returns. Then the markup is parsed again by the browser on assignment to innerHTML:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-3-1024x107.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-3-1024x107.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-3-300x31.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-3-768x80.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-3-1536x161.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-3-1320x138.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-3.png 1952w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The DOM tree is identical to the one that DOMPurify worked on, and it is then appended to the document.</p>



<p>So to put it shortly, we have the following order of operations: <strong>parsing ‚û°Ô∏è serialization ‚û°Ô∏è parsing</strong>. The intuition may be that serializing a DOM tree and parsing it again should always return the initial DOM tree. But this is not true at all. There‚Äôs even <a href="https://html.spec.whatwg.org/multipage/parsing.html#serialising-html-fragments:escapingString-3:~:text=It%20is%20possible%20that%20the%20output,not%20return%20the%20original%20tree%20structure">a warning in the HTML spec</a> in a section about serializing HTML fragments:</p>



<blockquote><p>It is possible that the output of this algorithm [serializing HTML], if parsed with an HTML parser, will not return the original tree structure. <strong>Tree structures that do not roundtrip a serialize and reparse step can also be produced by the HTML parser itself</strong>, although such cases are typically non-conforming.</p></blockquote>



<p>The important take-away is that serialize-parse roundtrip is not guaranteed to return the original DOM tree (this is also a root cause of a type of XSS known as <strong>mutation XSS</strong>). While usually these situations are a result of some kind of parser/serializer error, there are at least two cases of spec-compliant mutations.</p>



<h2>Nesting FORM element</h2>



<p>One of these cases is related to the FORM element. It is quite special element in the HTML because it cannot be nested in itself. The specification is explicit that<a href="https://html.spec.whatwg.org/#the-form-element"> it cannot have any descendant that is also a FORM</a>:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-4-1024x279.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-4-1024x279.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-4-300x82.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-4-768x209.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-4-1536x419.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-4-1320x360.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-4.png 1936w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>This can be confirmed in any browser, with the following markup:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f801f5815cc6154054943" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>form1</span><span>&gt;</span></p><p><span>INSIDE_FORM1</span></p><p><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>form2</span><span>&gt;</span></p><p><span>INSIDE_FORM2</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0001 seconds] -->




<p>Which would yield the following DOM tree:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-5-1024x80.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-5-1024x80.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-5-300x24.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-5-768x60.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-5-1536x121.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-5-1320x104.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-5.png 1960w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The second <code>form</code> is completely omitted in the DOM tree just as it wasn‚Äôt ever there.</p>



<p>Now comes the interesting part. If we keep reading the HTML specification, it actually gives <a href="https://html.spec.whatwg.org/multipage/parsing.html#serialising-html-fragments:the-script-element-4:~:text=DOM.-,For%20example%2C%20consider%20the%20following%20markup%3A,%3Cform">an example</a> that with a slightly broken markup with mis-nested tags, it is possible to create nested forms. Here it comes (taken directly from the spec):</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f801f5815ccd748015350" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>"outer"</span><span>&gt;</span><span>&lt;</span><span>div</span><span>&gt;</span><span>&lt;</span><span>/</span><span>form</span><span>&gt;</span><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>"inner"</span><span>&gt;</span><span>&lt;</span><span>input</span><span>&gt;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0001 seconds] -->




<p>It yields the following DOM tree, which contains a nested form element:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-6-1024x141.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-6-1024x141.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-6-300x41.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-6-768x106.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-6-1536x211.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-6-1320x182.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-6.png 1948w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>This is not a bug in any particular browser; it results directly from the HTML spec, and is described in the algorithm of parsing HTML. Here‚Äôs the general idea:</p>



<ul><li>When you open a <code>&lt;form&gt;</code> tag, the parser needs to keep record of the fact that it was opened with a <strong>form element pointer</strong> (that‚Äôs how it‚Äôs called in the spec). If the pointer is not <code>null</code>, then <code>form</code> element cannot be created.</li><li>When you end a <code>&lt;form&gt;</code> tag, the form element pointer is always set to <code>null</code>. </li></ul>



<p>Thus, going back to the snippet:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f801f5815ccf064463256" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>"outer"</span><span>&gt;</span><span>&lt;</span><span>div</span><span>&gt;</span><span>&lt;</span><span>/</span><span>form</span><span>&gt;</span><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>"inner"</span><span>&gt;</span><span>&lt;</span><span>input</span><span>&gt;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0001 seconds] -->




<p>In the beginning, the form element pointer is set to the one with <code>id="outer"</code>. Then, a <code>div</code> is being started, and the <code>&lt;/form&gt;</code> end tag set the form element pointer to <code>null</code>. Because it‚Äôs <code>null</code>, the next form with <code>id="inner"</code> can be created; and because we‚Äôre currently within <code>div</code>, we effectively have a <code>form</code> nested in <code>form</code>.</p>



<p>Now, if we try to serialize the resulting DOM tree, we‚Äôll get the following markup:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f801f5815cd4655660939" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>"outer"</span><span>&gt;</span><span>&lt;</span><span>div</span><span>&gt;</span><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>"inner"</span><span>&gt;</span><span>&lt;</span><span>input</span><span>&gt;</span><span>&lt;</span><span>/</span><span>form</span><span>&gt;</span><span>&lt;</span><span>/</span><span>div</span><span>&gt;</span><span>&lt;</span><span>/</span><span>form</span><span>&gt;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0001 seconds] -->




<p>Note that this markup no longer has any mis-nested tags. And when the markup is parsed again, the following DOM tree is created:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-7-1024x101.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-7-1024x101.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-7-300x30.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-7-768x76.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-7-1536x151.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-7-2048x202.png 2048w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-7-1320x130.png 1320w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>So this is a proof that serialize-reparse roundtrip is not guaranteed to return the original DOM tree. And even more interestingly, this is basically <strong>a spec-compliant mutation</strong>.</p>



<p>Since the very moment I was made aware of this quirk, I‚Äôve been pretty sure that it must be possible to somehow abuse it to bypass HTML sanitizers. And after a long time of not getting any ideas of how to make use of it, I finally stumbled upon another quirk in HTML specification. But before going into the specific quirk itself, let‚Äôs talk about my favorite Pandora‚Äôs box of the HTML specification: foreign content.</p>



<h2>Foreign content</h2>



<p>Foreign content is a like a Swiss Army knife for breaking parsers and sanitizers. I used it in my <a href="https://research.securitum.com/dompurify-bypass-using-mxss/">previous DOMPurify bypass</a> as well as in <a href="https://research.securitum.com/html-sanitization-bypass-in-ruby-sanitize-5-2-1/">bypass of Ruby sanitize library</a>.</p>



<p>The HTML parser can create a DOM tree with elements of three namespaces:</p>



<ul><li>HTML namespace (<code>http://www.w3.org/1999/xhtml</code>)</li><li>SVG namespace (<code>http://www.w3.org/2000/svg</code>)</li><li>MathML namespace (<code>http://www.w3.org/1998/Math/MathML</code>)</li></ul>



<p>By default, all elements are in HTML namespace; however if the parser encounters <code>&lt;svg&gt;</code> or <code>&lt;math&gt;</code> element, then it ‚Äúswitches‚Äù to SVG and MathML namespace respectively. And both these namespaces make foreign content.</p>



<p>In foreign content markup is parsed differently than in ordinary HTML. This can be most clearly shown on parsing of <code>&lt;style&gt;</code> element. In HTML namespace, <code>&lt;style&gt;</code> can only contain text; no descendants, and HTML entities are not decoded. The same is not true in foreign content: foreign content‚Äôs <code>&lt;style&gt;</code> can have child elements, and entities are decoded.</p>



<p>Consider the following markup:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f801f5815cd6348574795" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;style&gt;</span><span>&lt;a&gt;</span><span>ABC&lt;/style&gt;</span><span>&lt;</span><span>svg</span><span>&gt;</span><span>&lt;</span><span>style</span><span>&gt;</span><span>&lt;</span><span>a</span><span>&gt;</span><span>ABC</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0004 seconds] -->




<p>It is parsed into the following DOM tree</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-8-1024x206.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-8-1024x206.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-8-300x60.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-8-768x154.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-8-1536x308.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-8-1320x265.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-8.png 1962w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><strong>Note:</strong> from now on, all elements in the DOM tree in this blogpost will contain a namespace. So <code>html style</code> means that it is a <code>&lt;style&gt;</code> element in HTML namespace, while <code>svg style</code> means that it is a <code>&lt;style&gt;</code> element in SVG namespace.</p>



<p>The resulting DOM tree proves my point: <code>html style</code> has only text content, while <code>svg style</code> is parsed just like an ordinary element.</p>



<p>Moving on, it may be tempting to make a certain observation. That is: if we are inside <code>&lt;svg&gt;</code> or <code>&lt;math&gt;</code> then all elements are also in non-HTML namespace. But this is not true. There are certain elements in HTML specification called <strong>MathML text integration points</strong> and <strong>HTML integration point</strong>. And the children of these elements have HTML namespace (with certain exceptions I‚Äôm listing below).</p>



<p>Consider the following example:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f801f5815cdb908326048" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>math</span><span>&gt;</span></p><p><span>&lt;style&gt;</span><span>&lt;/style&gt;</span></p><p><span>&lt;</span><span>mtext</span><span>&gt;</span><span>&lt;style&gt;</span><span>&lt;/style&gt;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0001 seconds] -->




<p>It is parsed into the following DOM tree:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-9-1024x138.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-9-1024x138.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-9-300x40.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-9-768x104.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-9-1536x207.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-9-1320x178.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-9.png 1956w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Note how the <code>style</code> element that is a direct child of <code>math</code> is in MathML namespace, while the <code>style</code> element in <code>mtext</code> is in HTML namespace. And this is because <code>mtext</code> is <strong>MathML text integration points</strong> and makes the parser switch namespaces. </p>



<p>MathML text integration points are:</p>



<ul><li><code>math mi</code></li><li><code>math mo</code></li><li><code>math mn</code></li><li><code>math ms</code></li></ul>



<p>HTML integration points are:</p>



<ul><li><code>math annotation-xml</code> if it has an attribute called <code>encoding</code> whose value is equal to either <code>text/html</code> or <code>application/xhtml+xml</code></li><li><code>svg foreignObject</code></li><li><code>svg desc</code></li><li><code>svg title</code></li></ul>



<p>I always assumed that all children of MathML text integration points or HTML integration points have HTML namespace by default. How wrong was I! The HTML specification says that children of MathML text integration points are by default in HTML namespace with two exceptions: <code>mglyph</code> and <code>malignmark</code>. And this only happens if they are a direct child of MathML text integration points.</p>



<p>Let‚Äôs check that with the following markup:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f801f5815cdc148662785" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>math</span><span>&gt;</span></p><p><span>&lt;</span><span>mtext</span><span>&gt;</span></p><p><span>&lt;</span><span>mglyph</span><span>&gt;</span><span>&lt;</span><span>/</span><span>mglyph</span><span>&gt;</span></p><p><span>&lt;</span><span>a</span><span>&gt;</span><span>&lt;</span><span>mglyph</span><span>&gt;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0001 seconds] -->




<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-10-1024x168.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-10-1024x168.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-10-300x49.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-10-768x126.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-10-1536x252.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-10-1320x217.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-10.png 1974w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Notice that <code>mglyph</code> that is a direct child of <code>mtext</code> is in MathML namespace, while the one that is a child of <code>html a</code> element is in HTML namespace.</p>



<p>Assume that we have a ‚Äúcurrent element‚Äù, and we‚Äôd like determine its namespace. I‚Äôve compiled some rules of thumb:</p>



<ul><li>Current element is in the namespace of its parent unless conditions from the points below are met.</li><li>If current element is <code>&lt;svg&gt;</code> or <code>&lt;math&gt;</code> and parent is in HTML namespace, then current element is in SVG or MathML namespace respectively.</li><li>If parent of current element is an HTML integration point, then current element is in HTML namespace unless it‚Äôs <code>&lt;svg&gt;</code> or <code>&lt;math&gt;</code>.</li><li>If parent of current element is an MathML integration point, then current element is in HTML namespace unless it‚Äôs <code>&lt;svg&gt;</code>, <code>&lt;math&gt;</code>, <code>&lt;mglyph&gt;</code> or <code>&lt;malignmark&gt;</code>.</li><li>If current element is one of <code>&lt;b&gt;, &lt;big&gt;, &lt;blockquote&gt;, &lt;body&gt;, &lt;br&gt;, &lt;center&gt;, &lt;code‚Ä¶</code></li></ul></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://research.securitum.com/mutation-xss-via-mathml-mutation-dompurify-2-0-17-bypass/">https://research.securitum.com/mutation-xss-via-mathml-mutation-dompurify-2-0-17-bypass/</a></em></p>]]>
            </description>
            <link>https://research.securitum.com/mutation-xss-via-mathml-mutation-dompurify-2-0-17-bypass/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24703230</guid>
            <pubDate>Tue, 06 Oct 2020 22:43:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[‚ÄúA server hall draws as much electricity as half a nuclear power plant creates‚Äù]]>
            </title>
            <description>
<![CDATA[
Score 45 | Comments 68 (<a href="https://news.ycombinator.com/item?id=24702206">thread link</a>) | @draugadrotten
<br/>
October 6, 2020 | https://tekdeeps.com/a-server-hall-draws-as-much-electricity-as-half-a-nuclear-power-plant-creates/ | <a href="https://web.archive.org/web/*/https://tekdeeps.com/a-server-hall-draws-as-much-electricity-as-half-a-nuclear-power-plant-creates/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://tekdeeps.com/a-server-hall-draws-as-much-electricity-as-half-a-nuclear-power-plant-creates/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24702206</guid>
            <pubDate>Tue, 06 Oct 2020 20:51:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hootsuite employee fired after speaking out about company's ICE deal]]>
            </title>
            <description>
<![CDATA[
Score 62 | Comments 20 (<a href="https://news.ycombinator.com/item?id=24702114">thread link</a>) | @foofoo55
<br/>
October 6, 2020 | https://bc.ctvnews.ca/hootsuite-employee-fired-after-speaking-out-about-company-s-ice-deal-1.5135073 | <a href="https://web.archive.org/web/*/https://bc.ctvnews.ca/hootsuite-employee-fired-after-speaking-out-about-company-s-ice-deal-1.5135073">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://bc.ctvnews.ca/hootsuite-employee-fired-after-speaking-out-about-company-s-ice-deal-1.5135073</link>
            <guid isPermaLink="false">hacker-news-small-sites-24702114</guid>
            <pubDate>Tue, 06 Oct 2020 20:40:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Clarifying exceptions and visualizing tensor operations in deep learning code]]>
            </title>
            <description>
<![CDATA[
Score 25 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24701739">thread link</a>) | @parrt
<br/>
October 6, 2020 | https://explained.ai/tensor-sensor/index.html | <a href="https://web.archive.org/web/*/https://explained.ai/tensor-sensor/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">






<p><a href="http://parrt.cs.usfca.edu/">Terence Parr</a></p>

<p>(Terence teaches in <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">University of San Francisco's MS in Data Science program</a>. You might know Terence as the creator of the ANTLR parser generator.)</p>






<p>	Most people solve deep learning problems using high-level libraries such as <a href="https://keras.io/">Keras</a> or <a href="https://www.fast.ai/">fastai</a>,  which makes sense. These libraries hide a lot of implementation details that we either don't care about or can learn later.  To truly understand deep learning, however, I think it's important at some point to implement your own network layers and training loops. For example, see my recent article called <a href="https://explained.ai/rnn/index.html">Explaining RNNs without neural networks</a>. If you're comfortable building deep learning models while leaving some of the details a bit fuzzy, then this article is not for you.  In my quirky case, I care more about learning something deeply than actually applying it to something useful, so I go straight for the details. (I guess that's why I work at a university, not in industry √∞≈∏Àú‚Ç¨.)  This article is in response to a pain point I experienced during an obsessive coding and learning burn through the fundamentals of deep learning in the isolation of Covid summer 2020.</p>

<center>
<a href="https://explained.ai/tensor-sensor/images/teaser.png">
<img src="https://explained.ai/tensor-sensor/images/teaser.png" width="40%" url="images/teaser.png">
</a>
</center>
One of the biggest challenges when writing code to implement deep learning networks, particularly for us newbies, is getting all of the tensor (matrix and vector) dimensions to line up properly. It's really easy to lose track of tensor dimensionality in complicated expressions involving multiple tensors and tensor operations.  Even when just feeding data into predefined <a href="https://www.tensorflow.org/">Tensorflow</a> network layers, we still need to get the dimensions right. When you ask for improper computations, you're going to run into some less than helpful exception messages.  To help myself and other programmers debug tensor code, I built a new library called <a href="https://github.com/parrt/tensor-sensor">TensorSensor</a> (<span>pip install tensor-sensor</span>).  TensorSensor clarifies exceptions by augmenting messages and visualizing Python code to indicate the shape of tensor variables (see figure to the right for a teaser). It works with <a href="https://www.tensorflow.org/">Tensorflow</a>, <a href="https://pytorch.org/">PyTorch</a>, and <a href="https://numpy.org/">Numpy</a>, as well as higher-level libraries like <a href="https://keras.io/">Keras</a> and <a href="https://www.fast.ai/">fastai</a>.

<p><i>TensorSensor is currently at 0.1b1 so I'm happy to receive issues created at the</i> <a href="https://github.com/parrt/tensor-sensor">repo</a> <i>or direct email</i>.</p>



<h2 id="sec:1.1">Isolating issues in tensor code is maddening!</h2>


<p>Even for experts, it can be hard to quickly identify the cause of an exception in a line of Python code performing tensor operations.  The debugging process usually involves adding a print statement in front of the offending line to emit the shape of each tensor operand.  That requires editing the code to create the debugging statement and rerunning the training process. Or, we can manually click or type commands to request all operand shapes using an interactive debugger. (This can be less practical in an IDE like PyCharm where executing code in debug mode seems to be much slower.)  The following subsections illustrate the anemic default exception messages and my proposed TensorSensor approach, rather than a debugger or print statements.</p>



<h3 id="sec:1.1.1">Debugging a simple linear layer</h3>


<p>Let's look at a simple tensor computation to illustrate the less-than-optimal information provided by the default exception message. Consider the following simple NumPy implementation for a hardcoded single (linear) network layer that contains a tensor dimension error.</p>


<p>import numpy as np

n = 200                          # number of instances
d = 764                          # number of instance features
n_neurons = 100                  # how many neurons in this layer?

W = np.random.rand(d,n_neurons)  # Ooops! Should be (n_neurons,d) &lt;=======
b = np.random.rand(n_neurons,1)
X = np.random.rand(n,d)          # fake input matrix with n rows of d-dimensions

Y = W @ X.T + b                  # pass all X instances through layer</p>


<p>Executing that code triggers an exception whose important elements are:</p>

<p>...
---&gt; 10 Y = W @ X.T + b
	
ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 764 is different from 100)</p>


<p>The exception identifies the offending line and which operation (<span>matmul</span>: matrix multiply) but would be more useful if it gave the complete tensor dimensions. Also, the exception would be unable to distinguish between multiple matrix multiplications occurring in one line of Python.</p>

<p>Next, let's see how TensorSensor makes debugging that statement much easier. If we wrap the statement using a Python <span>with</span> statement and <span>tsensor</span>'s <span>clarify()</span>, we get a visualization and an augmented error message. </p>


<p>import tsensor
with tsensor.clarify():
    Y = W @ X.T + b</p>


<p>
<a href="https://explained.ai/tensor-sensor/images/numpy-mm-py.svg">
<img nocenter="true" src="https://explained.ai/tensor-sensor/images/numpy-mm-py.svg" url="images/numpy-mm-py.svg">
</a>
</p>

<p>...
ValueError: matmul: Input operand ...
Cause: @ on tensor operand W w/shape (764, 100) and operand X.T w/shape (764, 200)</p>


<p>It's clear from the visualization that <span>W</span>'s dimensions should be flipped to be <span>n_neurons x d</span>; the columns of <span>W</span> must match the rows of <span>X.T</span>. You can also checkout a <a href="https://explained.ai/tensor-sensor/images/numpy-mm.png">complete side-by-side image</a> with and without <span>clarify()</span> to see what it looks like in a notebook.</p>

<p>The <span>clarify()</span> functionality incurs no overhead on the executing program until an exception occurs. Upon exception, <span>clarify()</span>:</p>
<ol>
<li> Augments the exception object's message created by the underlying tensor library.</li>
<li> Gives a visual representation of the tensor sizes involved in the offending operation; only the operands and operator involved in the exception are highlighted, while the other Python elements are de-highlighted.</li>
</ol>
<p>TensorSensor also clarifies tensor-related exceptions raised by PyTorch and TensorFlow. Here are the equivalent code snippets and resulting augmented exception error messages (<span>Cause: @ on tensor ...</span>) and visualization from TensorSensor:</p>
<center>
<table>
<thead>
<tr>
<th>PyTorch</th><th>TensorFlow</th>
</tr>
</thead>
<tbody>
<tr>
<td>

<p>import torch
W = torch.rand(d,n_neurons)
b = torch.rand(n_neurons,1)
X = torch.rand(n,d)
with tsensor.clarify():
    Y = W @ X.T + b</p>

</td><td>

<p>import tensorflow as tf
W = tf.random.uniform((d,n_neurons))
b = tf.random.uniform((n_neurons,1))
X = tf.random.uniform((n,d))
with tsensor.clarify():
    Y = W @ tf.transpose(X) + b</p>

</td>
</tr>
<tr>
<td>

<a href="https://explained.ai/tensor-sensor/images/mm.svg">
<img nocenter="true" src="https://explained.ai/tensor-sensor/images/mm.svg" url="images/mm.svg">
</a>



<p>RuntimeError: size mismatch, m1: [764 x 100], m2: [764 x 200] at /tmp/pip-req-build-as628lz5/aten/src/TH/generic/THTensorMath.cpp:41
Cause: @ on tensor operand W w/shape [764, 100] and operand X.T w/shape [764, 200]</p>

</td><td>
<img src="https://explained.ai/tensor-sensor/images/tf-mm.svg" nocenter="true">

<p>InvalidArgumentError: Matrix size-incompatible: In[0]: [764,100], In[1]: [764,200] [Op:MatMul]
Cause: @ on tensor operand W w/shape (764, 100) and operand tf.transpose(X) w/shape (764, 200)</p>
</td>
</tr>
</tbody>
</table>
</center>
<p>The PyTorch message does not identify which operation triggered the exception, but TensorFlow's message does indicate matrix multiplication. Both show the operand dimensions. These default exception messages are probably good enough for this simple tensor expression for a linear layer. Still, it's easier to see the problem with the TensorSensor visualization.</p>

<p>You might be wondering, though, why tensor libraries don't generate a more helpful exception message that identified the names of the Python variables involved in the offending subexpression.  It's not that the library authors couldn't be bothered. The fundamental issue is that Python tensor libraries are wrappers around extremely efficient cores written in C or C++. Python passes, say, the data for two tensors to a C++ function, but not the associated tensor variable names in Python space. An exception caught deep in C++ has no access to the local and global variable spaces in Python, so it just throws a generic exception back over the fence.  Because Python traps exceptions at the statement level, it also cannot isolate the subexpression within the statement.  (To learn how TensorSensor manages to generate such specific messages, check out <b>Section</b> <i>Key TensorSensor implementation Kung Fu</i> below.)</p>



<h3 id="sec:1.1.2">Debugging a complex tensor expression</h3>


<p>That lack of specificity in default messages makes it hard to identify bad subexpressions within more complicated statements that contain lots of operators. For example, here's a statement pulled from the guts of a Gated Recurrent Unit (GRU) implementation:</p>


<p>h_ = torch.tanh(Whh_ @ (r*h) + Uxh_ @ X.T + bh_)</p>


<p>It doesn't matter what it's computing or what the variables represent, just that they are tensor variables. There are two matrix multiplications, two vector additions, and even a vector element-wise modification (<span>r*h</span>).  Without augmented error messages or visualizations we wouldn't know which operator and operands caused an exception. To demonstrate how TensorSensor clarifies exceptions in this case, we need to give some fake definitions for the variables used in the statement (the assignment to <span>h_</span>) to get executable code:</p>


<p>nhidden = 256
Whh_ = torch.eye(nhidden, nhidden)  # Identity matrix
Uxh_ = torch.randn(d, nhidden)
bh_  = torch.zeros(nhidden, 1)
h = torch.randn(nhidden, 1)         # fake previous hidden state h
r = torch.randn(nhidden, 1)         # fake this computation
X = torch.rand(n,d)                 # fake input

with tsensor.clarify():
    h_ = torch.tanh(Whh_ @ (r*h) + Uxh_ @ X.T + bh_)</p>


<p>Again, you can ignore the actual computation performed by the code to focus on the shape of the tensor variables.  </p>

<p>For most of us, it's impossible to identify the problem just by looking at the tensor dimensions and the tensor code.  The default exception message is helpful of course, but most of us will still struggle to identify the problem.  Here are the key bits of the default exception message (note the less-than-helpful reference to the C++ code):</p>

<p>---&gt; 10     h_ = torch.tanh(Whh_ @ (r*h) + Uxh_ @ X.T + bh_)
RuntimeError: size mismatch, m1: [764 x 256], m2: [764 x 200] at /tmp/pip-req-build-as628lz5/aten/src/TH/generic/THTensorMath.cpp:41
</p>


<p>What we need to know is which operator and operands failed, then we can look at the dimensions to identify the problem.  Here is TensorSensor's visualization and augmented exception message:</p>

<p>
<a href="https://explained.ai/tensor-sensor/images/torch-gru.svg">
<img nocenter="true" src="https://explained.ai/tensor-sensor/images/torch-gru.svg" url="images/torch-gru.svg">
</a>

</p><p>---&gt; 10 h_ = torch.tanh(Whh_ @ (r*h) + Uxh_ @ X.T + bh_)
RuntimeError: size mismatch, m1: [764 x 256], m2: [764 x 200] at ‚Ä¶</p></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://explained.ai/tensor-sensor/index.html">https://explained.ai/tensor-sensor/index.html</a></em></p>]]>
            </description>
            <link>https://explained.ai/tensor-sensor/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24701739</guid>
            <pubDate>Tue, 06 Oct 2020 20:01:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Simulating Machines in Clojure]]>
            </title>
            <description>
<![CDATA[
Score 145 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24701737">thread link</a>) | @stopachka
<br/>
October 6, 2020 | https://stopa.io/post/255 | <a href="https://web.archive.org/web/*/https://stopa.io/post/255">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><span><p><a href="https://twitter.com/stopachka/status/1295411936625074178" target="_blank">My cofounder Joe and I recently finished SICP.</a> It was a mind-bending experience: you start from just 3 concepts, and you recursively build up algebraic equation solvers, circuit simulators, 4 interpreters, and a compiler. </p><p>At some point you experience a visceral feeling: If you were dropped in a forest‚Ä¶you could create your own computer. The project that contributed most significantly to this feeling was creating a machine simulator.</p><p>We diverged from the book by writing the simulator in Clojure rather than Scheme. Immutable data structures and higher-level concepts available to us in Clojure compressed the solution, to the point where I think you can build your own in a few days worth of hacking.</p><p>This essay will guide you through doing just that: let‚Äôs build a machine simulator, over a good few days worth of hacking! I hope this inspires you to play with Clojure and to take a deeper look at SICP. </p><p>Before we simulate general machines, let‚Äôs think about a concrete machine. <strong>How could we create a machine that could figure out factorials?</strong> </p><p>If we were writing code, factorial could look something like this:</p><pre><code><span>(</span><span>defn</span><span> factorial [n]</span>
<span>  (</span><span>loop</span><span> [res </span><span>1</span>
<span>         counter </span><span>1</span><span>]</span>
<span>    (</span><span>if</span><span> (</span><span>&gt;</span><span> counter n)</span>
<span>      res</span>
<span>      (</span><span>recur</span>
<span>        (</span><span>*</span><span> counter res)</span>
<span>        (</span><span>inc</span><span> counter)))))</span></code></pre><p>Let‚Äôs see if we could build factorials using <em>physical</em> devices.</p><p>Well, we need a way to keep track of <code value="counter">counter</code>, <code value="res">res</code>, and <code value="n">n</code>. To do that, we‚Äôll need a device that stores information. </p><h2>Bulbs</h2><p>Imagine a device that has some light bulbs inside of it. </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwNDUyLTFlNzkzODgwLTA3ZTgtMTFlYi04MjI4LWQwYzE4ZTcwNTZhYS5wbmc" alt="image"></span></p><p>We can say that if a light bulb is <em>on,</em> that represents the number 1, and if a light bulb is <em>off</em> that represents the number 0. </p><p>If we had a bunch of light bulbs in the device, we could interpret the state of these bulbs as larger and larger binary numbers. The light bulbs in the device I just showed you for example, would represent ‚Äú10101‚Äù, which is binary for ‚Äú21‚Äù.</p><h2>Incoming Current</h2><p>Now, imagine that at all times there are a bunch of other wires connected to this device. These wires carry ‚Äúnew‚Äù charges for the light bulbs, but with a twist: the incoming charges <em>do not</em> affect the light bulbs inside just yet.</p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwNjIxLTVhMTQwMjgwLTA3ZTgtMTFlYi05ZjM2LTU2MDFkZTIyOWUwOS5wbmc" alt="image"></span></p><p>Notice how the <em>incoming charge</em> for the ‚Äúa‚Äù light bulb is ‚Äúoff‚Äù, but the bulb inside is still on. Conversely, the incoming charge "b" is "on", but the bulb is off. If our device can do this, it means that whatever the charges for the light bulbs are inside is a <em>stored value</em>. Very cool! </p><h2>Save</h2><p>Now, we need these incoming wires to do something at some point. What if this device had a ‚Äúsave‚Äù button. </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwNjQ1LTYzMDRkNDAwLTA3ZTgtMTFlYi04NjVjLWE3YTg3OTE5Njg3ZC5wbmc" alt="image"></span></p><p>Once we pressed ‚Äúsave‚Äù, the incoming current would transfer inside the box:  </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwNjU0LTY3Yzk4ODAwLTA3ZTgtMTFlYi04MjU1LTExNmEzMGUxNTcwMy5wbmc" alt="image"></span></p><p>Here, light bulb ‚Äúa‚Äù changes from ‚Äúon‚Äù to ‚Äúoff‚Äù, and the light bulb "b" changed from "on" to "off".</p><p>Great, now we have a way to ‚Äúsave‚Äù new numbers inside! </p><h2>Outgoing current</h2><p>We also need a way to share the state of what‚Äôs inside to other devices.  All we‚Äôd need to do to make that work, is to have a bunch of wires that leave our device, which carry the sames charges as the light bulbs: </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUxNzQ2LWY5ODVjNTAwLTA3ZTktMTFlYi05YzczLTY4ZWYwNzc3OGJlMi5wbmc" alt="image"></span></p><p>Now, if we hooked those outgoing charges to some other device, that device would receive the ‚Äúnumber‚Äù that was stored in this one. </p><h2>Registers</h2><p>What we just described is analogous to a computer‚Äôs <em>register</em> (1). Registers let us store and share information. </p><p>Now, we could use three of registers to store the value of <code value="res">res</code> <code value="counter">counter</code> and <code value="n">n</code>.</p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwODUxLWIxYjI2ZTAwLTA3ZTgtMTFlYi04NzRjLTQxODgyODAxNTQ2OC5wbmc" alt="image"></span></p><p>Next up, we‚Äôll need a device that that can ‚Äúadd‚Äù two registers. Imagine a device that had two register‚Äôs worth of incoming wires, and one register‚Äôs worth of outgoing wires: </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwODY5LWI4ZDk3YzAwLTA3ZTgtMTFlYi05ZDM0LTQwMDE0YmQ5NDAyOC5wbmc" alt="image"></span></p><h2>Adder</h2><p>If the device could connect those incoming wires in such a way, that the outgoing wires represented the ‚Äúaddition‚Äù of those registers, we‚Äôd have an ‚Äúadder‚Äù device! </p><p>In the example above, the left register represents ‚Äú10101‚Äù (21), and the right represents ‚Äú00001‚Äù (1). The output wires are charged as ‚Äú10110‚Äù‚Ä¶which represent 22!</p><p>Similarly, we could have a device that has two register‚Äôs worth input wires, and one register‚Äôs worth of output wires: </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwODkyLWMxMzFiNzAwLTA3ZTgtMTFlYi05OTQ1LTI4ZDljNmU4N2IzNy5wbmc" alt="image"></span></p><h2>Multiplier</h2><p>If we could connect the incoming wires in such a way, that the outgoing wires represent the result of a multiplication, boom we would have a multiplying device! </p><p>The left register above represents ‚Äú00101‚Äù (5), the right register represents ‚Äú00010‚Äù (2), and the charge of the outgoing wire is ‚Äú01010‚Äù (10). Nice! That gives us a multiplier machine. </p><h2>Comparator</h2><p>We need one more device. Imagine a device that takes two register‚Äôs worth of input wires, and only has <em>one</em> output wire: </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwOTE1LWNhMjI4ODgwLTA3ZTgtMTFlYi05Y2RiLWM2ZDEwZTYxNzc1Yi5wbmc" alt="image"></span></p><p>If we could combine the input wires in such a way, that the output wire was ‚Äúon‚Äù when the left register was bigger, and off otherwise, we could use this as a comparator machine! </p><p>If we had all these machines, we can wire them in such a way, that lets us compute factorials: </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwOTg1LWRlZmYxYzAwLTA3ZTgtMTFlYi05MDRjLTE2NmUzNTIyMjJkOS5wbmc" alt="image"></span></p><p>Here, we wired the output wires of <code value="res">res</code> and <code value="counter">counter</code> to the <code value="*">*</code> machine. We wired the output wires of the <code value="*">*</code> machine, to <em>be</em> the input wires of <code value="res">res</code>. </p><p>This way, if we press ‚ÄúA‚Äù, we would ‚Äústore‚Äù the result of multiplying <code value="counter">counter</code> with <code value="res">res</code>! </p><p>Similarly, we wired up the output wires of <code value="counter">counter</code> and a register that keeps the value <code value="1">1</code>, to the <code value="+">+</code> machine. We wired the output wires of the <code value="+">+</code> machine, to <em>be</em> the input wires of <code value="counter">counter</code>. </p><p>Now, If we pressed ‚ÄúB‚Äù, <code value="counter">counter</code> would be stored with the result of adding <code value="1">1</code>! </p><p>Next up, we also wired up <code value="counter">counter</code> and <code value="n">n</code> with the <code value=">">&gt;</code> machine. If we hooked up a light bulb to the output wire of the <code value=">">&gt;</code> machine for example, then whenever it was on, we would know that <code value="counter">counter</code> was larger than <code value="n">n</code>. </p><p>We‚Äôve just drawn out the ‚Äúdata path‚Äù of our machine. </p><h2>Manual Recipe</h2><p>Let‚Äôs remember our code for factorial: </p><pre><code><span>  (</span><span>loop</span><span> [res </span><span>1</span>
<span>         counter </span><span>1</span><span>]</span>
<span>    (</span><span>if</span><span> (</span><span>&gt;</span><span> counter n)</span>
<span>      res</span>
<span>      (</span><span>recur</span>
<span>        (</span><span>*</span><span> counter res)</span>
<span>        (</span><span>inc</span><span> counter))))</span></code></pre><p>imagine if we had our ‚Äúdata path‚Äù machine. What would happen if we followed the following recipe:</p><ol><li>Take a look at the output of the <code value=">">&gt;</code> machine. </li><li>If the light bulb connected to the <code value=">">&gt;</code>  machine is on, <strong>stop</strong></li></ol><p><em>Otherwise‚Ä¶</em></p><ol><li>"Press A". This will update <code value="res">res</code>  with the result of the <code value="*">*</code>  machine on <code value="res">res</code> and <code value="counter">counter</code> </li><li>‚ÄúPress B‚Äú. This will update <code value="counter">counter</code> with the result of the <code value="+">+</code>  machine on <code value="1">1</code> and <code value="counter">counter</code></li><li>Go back up to the start of the recipe</li></ol><p><strong>If we did this over and over again, once the light bulb connected to the output of the</strong>  <strong><code value=">">&gt;</code></strong> <strong>machine turns on,</strong> <strong><code value="res">res</code></strong> <strong>would contain the result of factorial!</strong> </p><h2>Automation</h2><p>Pretty cool, but this kind of manual work would be annoying. If you look at these instructions though, there‚Äôs a pretty significant insight: <em>all of those instructions are simple: "look at charge of light bulb", "press button..."</em></p><p>In fact, they‚Äôre so simple that we could wire up a machine that goes through that recipe! Imagine if we created a machine that could ‚Äúpress‚Äù buttons for us, depending on whether the output wire of the <code value=">">&gt;</code> machine is on:</p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUxMDIwLWU5MjExYTgwLTA3ZTgtMTFlYi04MDJlLTI4YzIyNjZiNzQwOC5wbmc" alt="image"></span></p><p>We would be able to automate computing factorials üôÇ</p><h2>Balls and Hills</h2><p>Now, at this stage, you may be wondering: exactly <em>how</em> would <code value="*">*</code> produce output wires that represent the multiplication? How would <code value="+">+</code> work, and how would the <code value="controller">controller</code> move along? </p><p>If you think about it, these can all be reduced to very simple machines. They don‚Äôt even necessarily have to be electronic: </p><p>Imagine you had a ball rolling down some hill. You could construct something like the <code value=">">&gt;</code> machine, by putting <code value="res">res</code> and <code value="counter">counter</code> on a scale: based on what‚Äôs bigger, the ball would take a different path</p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUxMDQ4LWY4MDdjZDAwLTA3ZTgtMTFlYi04NTg3LWRiYjE2ZDU3NGZkMy5wbmc" alt="image"></span></p><p>With sufficient energy, space, time, and ingenuity you really could build all of this with a ball on a hill. Now, you wouldn‚Äôt necessarily do that (2), but you can imagine how the electronic parts that make up our machines are similarly simple, logical machines: <em>turn on if off, turn off if on, etc</em>. These logical machines are called ‚Äúlogic gates‚Äù. You can look them up, but hopefully I‚Äôll have an essay for you about these machines soon üôÇ. </p><p>Now, we drew out our machine and saw how we could build them with simple devices. How could we simulate these machines? </p><p>To simulate these machines, we need to transform our <em>picture descriptions</em> into something that computers can manipulate. Computers can manipulate text much better: let‚Äôs create a <em>language</em> for describing these machines. </p><p>If we remember the pictures again:</p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUxMTg3LTJmNzY3OTgwLTA3ZTktMTFlYi05ZTNlLTg3Zjg0NDkxZjQxYS5wbmc" alt="image"></span></p><p>we could transform them into a language that looks like this:</p><pre><code><span>(</span><span>def</span><span> factorial-instructions</span>
<span>  '(</span>
<span>     start</span>
<!-- -->
<span>     (</span><span>test</span><span> (</span><span>op</span><span> &gt;) (</span><span>reg</span><span> counter) (</span><span>reg</span><span> n))</span>
<span>     (</span><span>branch</span><span> (</span><span>label</span><span> done))</span>
<!-- -->
<span>     (</span><span>assign</span><span> res (</span><span>op</span><span> *) (</span><span>reg</span><span> counter) (</span><span>reg</span><span> res))</span>
<span>     (</span><span>assign</span><span> counter (</span><span>op</span><span> +) (</span><span>reg</span><span> counter) (</span><span>const</span><span> </span><span>1</span><span>))</span>
<span>     (</span><span>goto</span><span> (</span><span>label</span><span> start))</span>
<!-- -->
<span>     done))</span></code></pre><p>When the <code value="test">test</code> instruction runs, we run the <code value=">">&gt;</code> machine with <code value="counter">counter</code> and <code value="n">n</code>.</p><p>Our <code value="branch">branch</code> instruction checks if the <code value="test">test</code> instruction said <code value="yes">yes</code>. If it did, it moves to <code value="done">done</code>. Otherwise it no-ops and the machine moves forward by one.</p><p>After that, our <code value="(assign res">(assign res</code> expression is analogous to ‚Äúpress A‚Äù. <code value="(assign counter">(assign counter</code> is analogous to ‚Äúpress B‚Äù, and <code value="(goto (label start)">(goto (label start)</code> is analogous to the arrow bringing us back to the start.</p><p>With this textual representation, we can build an interpreter and simulate our machine. Let‚Äôs do this! </p><p>What does the state of our machine look like in Clojure? Well, how do we represent most things in Clojure? With maps!  Let‚Äôs represent the state of our machine as a map:</p><pre><code><span>(</span><span>def</span><span> ex-machine-state-v0</span>
<span>  {</span><span>:registry-map</span><span> {'n </span><span>10</span><span> 'res </span><span>1</span><span> 'counter </span><span>1</span><span>}</span>
<span>   </span><span>:label-&gt;idx</span><span> {'start </span><span>0</span><span> 'done </span><span>5</span><span>}})</span></code></pre><p><code value="registry-map">registry-map</code> could keep a mapping of registers to values. 
<code value="label‚Üíidx">label‚Üíidx</code> could keep a mapping of labels to their <code value="idx">idx</code> in the instruction list</p><p>With this, we can get the most foundational part of our language to work: We use <code value="(const‚Ä¶">(const‚Ä¶</code>  <code value="(reg...">(reg...</code> and <code value="(label‚Ä¶">(label‚Ä¶</code> all over the place.</p><ol><li>If our machine sees <code value="(const 1)">(const 1)</code>, it should return the actual value <code value="1">1</code></li><li>If our machine sees <code value="(reg foo)">(reg foo)</code>, it should look up whatever is in the <code value="foo">foo</code> register, and return that </li><li>If our machine sees <code value="(label foo)">(label foo)</code>, it should return the correct index in our instruction list.</li></ol><p>Let‚Äôs write this out in Clojure:</p><pre><code><span>(</span><span>def</span><span> tag first) </span><span>; (tag '(const 1)) =&gt; const</span>
<span>(</span><span>defn</span><span> tag-of? [sym s] (</span><span>=</span><span> sym (</span><span>tag</span><span> s))) </span><span>; (tag-of? 'const '(const 1)) =&gt; true</span>
<!-- -->
<span>(</span><span>defn</span><span> parse-primitive [{</span><span>:keys</span><span> [registry-map label-&gt;idx] </span><span>:as</span><span> machine-state}</span>
<span>                       prim-exp]</span>
<span>  (</span><span>condp</span><span> tag-of? prim-exp</span>
<span>    'const</span>
<span>    (</span><span>second</span><span> prim-exp)</span>
<span>    'reg</span>
<span>    (</span><span>g‚Ä¶</span></code></pre></span></p></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://stopa.io/post/255">https://stopa.io/post/255</a></em></p>]]>
            </description>
            <link>https://stopa.io/post/255</link>
            <guid isPermaLink="false">hacker-news-small-sites-24701737</guid>
            <pubDate>Tue, 06 Oct 2020 20:01:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Celebrities Explain DevOps]]>
            </title>
            <description>
<![CDATA[
Score 28 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24700712">thread link</a>) | @jacksonpollock
<br/>
October 6, 2020 | https://cto.ai/blog/celebrities-explain-devops/ | <a href="https://web.archive.org/web/*/https://cto.ai/blog/celebrities-explain-devops/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<h3 id="david-hasselhoff-flavor-flav-and-carole-baskin-help-simplify-aws-kubernetes-and-docker"><em>David Hasselhoff, Flavor Flav, and Carole Baskin help simplify AWS, Kubernetes, and Docker</em></h3><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><p>üëã &nbsp;Hey all you cool cats and kittens! üò∫ </p><p>You probably thought this day would never come.</p><p>You might be thinking, have DevOps tools reached this level of adoption? That even the some of the biggest personalities on TV are excited by them.</p><p>Well, the day has come. It's today. And the proof is in the pudding (and the videos below).</p><p>In an effort to bring some lighthearted fun to the complex and serious nature of DevOps, we asked three of our favorite celebrities to explain some of the most popular technologies used in DevOps.</p><p>The Guinness World Record holder as The Most Watched Man on TV, <strong>David 'The Hoff' Hasselhoff</strong>,<strong> </strong>shares with us what Docker means to him.</p><p><strong>Flavor Flav</strong> (yeahhh boy!) of Public Enemy and Flavor of Love teaches us a thing or two about Kubernetes.</p><p>And<strong> Carole Baskin</strong> of Big Cat Rescue and Tiger King shares her excitement of AWS.</p><p>All are in support of our mission to simplify the snowballing complexity of the DevOps universe.</p><p>Please enjoy thoroughly:</p><!--kg-card-begin: embed--><figure><iframe width="480" height="270" src="https://www.youtube.com/embed/QxvmO-QlxJQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><!--kg-card-end: embed--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><p><em>Love this video? Love DevOps? Come join <a href="https://w.cto.ai/community">The Ops Community on Slack</a> and trade tips and tricks on workflow automation with us!</em></p>
			</div></div>]]>
            </description>
            <link>https://cto.ai/blog/celebrities-explain-devops/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24700712</guid>
            <pubDate>Tue, 06 Oct 2020 18:15:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How I remember what I learn]]>
            </title>
            <description>
<![CDATA[
Score 436 | Comments 107 (<a href="https://news.ycombinator.com/item?id=24700647">thread link</a>) | @flreln
<br/>
October 6, 2020 | https://vasilishynkarenka.com/learning/ | <a href="https://web.archive.org/web/*/https://vasilishynkarenka.com/learning/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://vasilishynkarenka.com/content/images/size/w300/2020/09/IMG_1517.jpg 300w,
                            https://vasilishynkarenka.com/content/images/size/w600/2020/09/IMG_1517.jpg 600w,
                            https://vasilishynkarenka.com/content/images/size/w1000/2020/09/IMG_1517.jpg 1000w,
                            https://vasilishynkarenka.com/content/images/size/w2000/2020/09/IMG_1517.jpg 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://vasilishynkarenka.com/content/images/size/w2000/2020/09/IMG_1517.jpg" alt="How to remember what you learn">
            </figure>

            <section>
                <div>
                    <p><em>‚ÄúI don‚Äôt remember a damn thing.‚Äù</em></p><p>The book I held my hands was full of highlights. It seemed like I‚Äôve got all colors of the rainbow on a page. Apparently, this didn‚Äôt help. When I tried recalling ideas from the book, I didn‚Äôt hear a thing. Just. Silence.</p><p>Terrified, I started questioning how much I <em>really</em> know. If I forget everything I read, I can‚Äôt apply my knowledge to the problem at hand. I can‚Äôt transfer it. And without transfer, knowledge is very much like music for deaf ears.</p><p>I quickly did the math. I was planning to invest in learning a few hours a day for the next ~75 years of my life. Staring at the number of potentially wasted hours, I knew exactly what I had to do.</p><hr><p>In the past six months, I‚Äôve devoured dozens of books, research papers, and studies on how people learn. As a result, I‚Äôve designed a learning process that works for me. It‚Äôs not perfect, but an order of magnitude better than what I had before. </p><p>In this work, I outline my workflow so that you can try it out. It applies to any subject or discipline, from programming to economics. If you stumble upon something where it doesn‚Äôt work, let me know.</p><blockquote><em>Make it time-based, take regular breaks, and learn what you‚Äôre curious about.</em></blockquote><p>The most important thing is that my learning is time-based, not goal-based. Setting learning goals such as ‚Äúread X pages today‚Äù is a way to fail because you set up the wrong incentives. When you plan to read X pages by lunch, you can‚Äôt help but begin optimizing for the goal, which leads to focusing on speed instead of understanding. And when you don‚Äôt have those ‚Äúaha‚Äù moments, it is hard to remember what you learn.</p><p>It‚Äôs also important to not overload yourself and take breaks. I do 3h learning sessions every day split into 30 min intervals with 5 min breaks. Breaks help to fall back into the diffuse mode of thinking and get access to a broader set of neural networks in my head. They also warm up my body, and I feel better after moving around for a few minutes.</p><p>As for material, I learn what I‚Äôm interested in. First, because life is <a href="http://www.paulgraham.com/vb.html">too short</a> to do things that you don‚Äôt love. Second, I‚Äôve found that studying stuff I genuinely like awakens my curiosity. And curiosity is essential to develop mastery because mastery is about depth and breadth of knowledge. </p><p>For example, if you‚Äôre learning JavaScript and you‚Äôre curious about it, you‚Äôll go and figure out how JS runtime environment works in Chrome even though the tutorial doesn‚Äôt cover it. Just because you‚Äôre interested. But if you‚Äôre not curious, then you‚Äôll just memorize the tutorial, and your knowledge will be shallow.</p><h2 id="how-my-learning-session-works">How my learning session works</h2><blockquote>Clean up working memory, apply metacognition, and "siege" the thing with questions to improve understanding.</blockquote><p>When I learn, I always have two devices on my desk. I have my laptop with the study material (ie, a book, a video, an article) on the right, and I have my iPad with a text editor open on the left.</p><p>This is how my current setup looks like:</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/09/IMG_2658.jpg" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/09/IMG_2658.jpg 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/09/IMG_2658.jpg 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/09/IMG_2658.jpg 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/09/IMG_2658.jpg 2400w" sizes="(min-width: 720px) 720px"><figcaption>Morning learning session.</figcaption></figure><p>When I begin learning, I set a timer for 30 minutes and create three files in Drafts:</p><ol><li>A file with a timestamp where my random thoughts go.</li><li>A file with a timestamp where I think about the subject.</li><li>A file with questions.</li></ol><p><strong>The first file is a mind dump.</strong> When I start learning, I immediately begin thinking about things. It‚Äôs almost as if my brain wakes up and starts throwing ideas, tasks, and memories at me. I suspect this comes from the associative memory because I present myself with many triggers when I‚Äôm learning; words and sentences that bear special meaning to me and invoke these ideas.</p><p>But here‚Äôs the problem. If I don‚Äôt write thoughts down, I can‚Äôt focus. My working memory is overloaded with todos, ideas, and emotions. You‚Äôve probably experienced this for yourself ‚Äì your mind is running too fast, and you can‚Äôt really concentrate on what you‚Äôre learning. Having this ‚Äúdump‚Äù file is immensely useful to a) free up my working memory to focus on my learning instead of thinking about these things, and b) store these thoughts somewhere safe to go back to them later and take action.</p><p><strong>The second file is where I write about what I‚Äôm learning.</strong> Folks in the kitchen call it metacognition, which means thinking about thinking. Metacognition is the single best trick I‚Äôve found to improve understanding, and I will write more about it in the future. Whenever I don‚Äôt understand something or see that my understanding is shallow, I begin writing in the first person. It looks like this: ‚ÄúSo Peter explains that there are four characteristics of a monopoly, but I don‚Äôt really understand why branding is one of them; why so?‚Äù</p><p>It‚Äôs also important to note that I don‚Äôt write in a usual sentence-paragraph manner. Instead, I write every thought on a new line. I don‚Äôt even put dots at the end of the sentences. This helps me to focus on understanding instead of nitty-gritty styling and typos. The ‚Äúenter‚Äù key on a keyboard serves as the ‚Äúend of thought‚Äù symbol and helps formulate ideas more clearly.</p><p>Another important idea is that my editor is plain text. I‚Äôve found it incredibly liberating to operate in a plain text environment where you don‚Äôt have incentives to color, underline, bold, italicize, or do some other weird things with the text you‚Äôre writing. Instead of choosing the right font for my heading, I can focus on meaning instead. Also, my plain text app is way faster than all feature-rich text editors, and I‚Äôve found it essential for a thought input environment to be fast. Otherwise, I can't think.</p><p>Here‚Äôs a fragment from my learning of React yesterday:</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/09/IMG_D7F6BD12F133-1.jpeg" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/09/IMG_D7F6BD12F133-1.jpeg 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/09/IMG_D7F6BD12F133-1.jpeg 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/09/IMG_D7F6BD12F133-1.jpeg 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/09/IMG_D7F6BD12F133-1.jpeg 2400w" sizes="(min-width: 720px) 720px"><figcaption>Metacognition notes from studying React.js.</figcaption></figure><p>You‚Äôre probably thinking that it‚Äôs quite a bit of writing. It is. For an hour-long learning session, I usually do about 500-1000 words in this file. But it‚Äôs worth every character, and here‚Äôs why:</p><ul><li>When I apply metacognition, I understand things <em>way</em> better than when I don‚Äôt. I‚Äôve tested many different learning modes and found metacognition to perform at least 2x better based on my later ability to recall and transfer knowledge. Also, there‚Äôs <a href="https://academicpublishingplatforms.com/downloads/pdfs/ati/volume18/201607070302_09_ATI_Vol11_Issue2_2015_Todorova_and_Karamanska_Study_motivation_satisfaction_students_e-learning_pp.82-89.pdf">some</a> <a href="http://www.csun.edu/science/ref/reasoning/how-students-learn/1.html">research</a> on metacognition as well.</li><li>Having a file with my thinking about the subject keeps my working memory clean. I don‚Äôt feel overloaded as I usually feel after reading many articles at one go. You‚Äôve probably experienced this yourself; your brain is almost melting after an hour of scrolling through the web. That‚Äôs because you present yourself with too much information without really making sense of it. After a few months of applying metacognitive practices, I realized that I can‚Äôt go back. It just feels so strange to experience that cognitive load again.</li><li>Metacognition improves remembering through elaboration and interleaving. When I‚Äôm writing my thoughts in the file, I can‚Äôt help but begin connecting them with other ideas on that topic because of associative memory. And interleaving leads to mastery.</li><li>(Speculation) Training metacognition improves my ability to transform vague notions and thoughts that I have during the day into specific words that I can write down for later analysis. This one is particularly interesting to me, but there‚Äôs no evidence besides my own experiments. And I might be biased because I‚Äôve come up with this method.</li></ul><p>Moreover, I type 2-3x faster than most people because I use <a href="https://barehands.substack.com/p/how-to-type-3x-faster">shortcuts</a>. So it‚Äôs not that bad.</p><p><strong>The third file is questions. </strong>Whenever I stumble upon something that I don‚Äôt understand, I try to break it down into a set of simple questions. Each question in the group takes on a small part of the problem. If the concept is particularly challenging, I try to ‚Äúsiege‚Äù it with questions from many many different angles and break it down even further.</p><p>When I‚Äôm beginning a new session, I always start from the previous one‚Äôs questions file. I only look at questions and answer them before I‚Äôm beginning new learning. This doesn‚Äôt sound like very much fun, but it‚Äôs actually pretty interesting to explain stuff to yourself if you do it out loud. Answering questions improves my understanding and helps to connect ideas together. And yes, answering questions counts as learning ‚Äì probably the most efficient learning you could be doing.</p><p><em>I'm not going into much detail on questions because Michael Nielsen has done a phenomenal job describing it <a href="http://augmentingcognition.com/ltm.html">here</a>.</em></p><h2 id="what-happens-after-the-session">What happens after the session</h2><blockquote>Write a dense summary, provoke elaboration, interleaving, and transfer, and choose what to never forget.</blockquote><p>After the session is done, and my three files are full of information, I begin the recap process.</p><p><strong>First, I write a three to five sentence-long summary of what I‚Äôve just studied.</strong> Here I try to distill the material‚Äôs core idea and compress the whole thing into a maximally dense chunk. When I‚Äôm summarizing, my laptop is closed. Not looking at the text helps to ‚Äúcompress‚Äù the idea to its core and make a small ‚Äúhook‚Äù to my memory to later see what the whole book was about.</p><p>Here‚Äôs how my summary note looks like: </p><figure><img src="https://vasilishynkarenka.com/content/images/2020/09/IMG_9771D059CFD1-1.jpeg" alt="Recap of studying React.js." srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/09/IMG_9771D059CFD1-1.jpeg 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/09/IMG_9771D059CFD1-1.jpeg 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/09/IMG_9771D059CFD1-1.jpeg 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/09/IMG_9771D059CFD1-1.jpeg 2400w" sizes="(min-width: 720px) 720px"><figcaption>Recap of studying React.js.</figcaption></figure><p>Very often, what I‚Äôm writing in the summary section is not what the text was about, but what it means to me. In other words, if both of us read this text and wrote a summary of it, mine would be very different than yours.</p><p>After I‚Äôm done with the summary, I write down the answers to three questions:</p><ol><li>What are the key ideas? </li><li>How can I apply this knowledge that I learned? </li><li>How do these ideas relate to what I already know?</li></ol><p><strong>The first question speaks for itself.</strong> I try to remember what I just read and write down as many ideas as I can bring back. When I began applying the metacognition trick that I mentioned earlier, I noticed a 3x increase in the number of concepts I could recall. And as I speculate that long-term memory recall is influenced by initial interleaving and recall, this might actually help to improve your long-term memory.</p><p><strong>The second question is about transfer.</strong> The sole purpose of learning is to apply the knowledge that we learn. Without application, ‚Ä¶</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://vasilishynkarenka.com/learning/">https://vasilishynkarenka.com/learning/</a></em></p>]]>
            </description>
            <link>https://vasilishynkarenka.com/learning/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24700647</guid>
            <pubDate>Tue, 06 Oct 2020 18:06:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Best of Both Worlds: A New Take on Metal‚ÄìPlastic Hybrid 3D Printing]]>
            </title>
            <description>
<![CDATA[
Score 21 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24700553">thread link</a>) | @rustoo
<br/>
October 6, 2020 | https://www.waseda.jp/top/en/news/73810 | <a href="https://web.archive.org/web/*/https://www.waseda.jp/top/en/news/73810">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-type="narrow">
              <div>
                
<h5>Scientists develop a novel and surprisingly simple method to print 3D structures made of metal and plastic, paving the way for 3D electronics</h5>
<p><strong>Current 3D printers employ either plastic or metal only, and the conventional method to coat 3D plastic structures with metal is not environment-friendly and yields poor results. Now, scientists from Waseda University, Japan, have developed a metal‚Äìplastic hybrid 3D printing technique that produces plastic structures with a highly adhesive metal coating on desired areas. This approach extends the use of 3D printers to 3D electronics for future robotics and Internet-of-Things applications.</strong></p>
<p><a href="https://www.waseda.jp/top/en/assets/uploads/2020/10/Infographic_Oct_01_2020.jpg"><img src="https://www.waseda.jp/top/en/assets/uploads/2020/10/Infographic_Oct_01_2020-2000x1125.jpg" alt="" width="2000" height="1125" srcset="https://www.waseda.jp/top/en/assets/uploads/2020/10/Infographic_Oct_01_2020-2000x1125.jpg 2000w, https://www.waseda.jp/top/en/assets/uploads/2020/10/Infographic_Oct_01_2020-610x343.jpg 610w, https://www.waseda.jp/top/en/assets/uploads/2020/10/Infographic_Oct_01_2020-768x432.jpg 768w" sizes="(max-width: 2000px) 100vw, 2000px"></a></p>
<p>Three-dimensional (3D) printing technology has evolved tremendously over the last decade to the point where it is now viable for mass production in industrial settings. Also known as ‚Äúadditive manufacturing,‚Äù 3D printing allows one to create arbitrarily complex 3D objects directly from their raw materials. In fused filament fabrication, the most popular 3D printing process, a plastic or metal is melted and extruded through a small nozzle by a printer head and then immediately solidifies and fuses with the rest of the piece. However, because the melting points of plastics and metals are very different, this technology has been limited to creating objects of either metal or plastic only‚Äîuntil now.</p>
<p>In a recent study published in <a href="https://www.sciencedirect.com/science/article/pii/S2214860420309283?via%3Dihub">Additive Manufacturing</a>, scientists from Waseda University, Japan, developed a new hybrid technique that can produce 3D objects made of both metal and plastic. <a href="http://www.umeshin.mmech.waseda.ac.jp/en/">Professor Shinjiro Umezu</a>, who led the study, explains their motivation: ‚ÄúEven though 3D printers let us create 3D structures from metal and plastic, most of the objects we see around us are a combination of both, including electronic devices. Thus, we thought we‚Äôd be able to expand the applications of conventional 3D printers if we managed to use them to create 3D objects made of both metal and plastic.‚Äù</p>
<p>Their method is actually a major improvement over the conventional metallization process used to coat 3D plastic structures with metal. In the conventional approach, the plastic object is 3D-printed and then submerged in a solution containing palladium (Pd), which adheres to the object‚Äôs surface. Afterwards, the piece is submerged in an electroless plating bath that, using the deposited Pd as a catalyst, causes dissolved metal ions to stick to the object. While technically sound, the conventional approach produces a metallic coating that is non-uniform and adheres poorly to the plastic structure.</p>
<p>In contrast, in the new hybrid method, a printer with a dual nozzle is used; one nozzle extrudes standard melted plastic (acrylonitrile butadiene styrene, or ABS) whereas the other extrudes ABS loaded with PdCl2. By selectively printing layers using one nozzle or the other, specific areas of the 3D object are loaded with Pd. Then, through electroless plating, one finally obtains a plastic structure with a metallic coating over selected areas only.</p>
<p>The scientists found the adhesion of the metal coating to be much higher when using their approach. What‚Äôs more, because Pd is loaded in the raw material, their technique does not require any type of roughening or etching of the ABS structure to promote the deposition of the catalyst, unlike the conventional method. This is especially important when considering that these extra steps cause damage not only to the 3D object itself, but to the environment as well, owing to the use of toxic chemicals like chromic acid. Lastly, their approach is entirely compatible with existing fused filament fabrication 3D printers.</p>
<p>Umezu believes that metal‚Äìplastic hybrid 3D printing could become very relevant in the near future considering its potential use in 3D electronics, which is the focus of upcoming Internet-of-Things and artificial intelligence applications. In this regard, he adds: ‚ÄúOur hybrid 3D printing method has opened up the possibility of fabricating 3D electronics so that devices and robots used in healthcare and nursing care could become significantly better than what we have today.‚Äù</p>
<p>This study hopefully paves the way for hybrid 3D printing technology that will enable us to get the best of both worlds‚Äîmetal and plastic combined.</p>
<h3>Reference</h3>
<h4>Authors</h4>
<p>Jing Zhan<sup>(a)</sup>, Takayuki Tamura<sup>(b)</sup>, Gyotong Ri<sup>(b)</sup>, Zhenghao Ma<sup>(b)</sup>, Michinari Sone<sup>(c)</sup>, Masahiro Yoshino<sup>(c)</sup>, Shinjiro Umezu<sup>(b)</sup>, and Hirotaka Sato<sup>(a)</sup></p>
<h4>Title of original paper</h4>
<p>Metal-Plastic Hybrid 3D Printing Using Catalyst-Loaded Filament and Electroless Plating</p>
<h4>Journal</h4>
<p>Additive Manufacturing</p>
<h4>DOI</h4>
<p>10.1016/j.addma.2020.101556</p>
<h4>Affiliations</h4>
<ul>
<li>a: School of Mechanical and Aerospace Engineering, Nanyang Technological University</li>
<li>b: Department of Modern Mechanical Engineering, Waseda University</li>
<li>c: Research and Development div., Yoshino Denka Kogyo, Inc.</li>
</ul>
<p><img src="https://www.waseda.jp/top/en/assets/uploads/2020/10/umezu.jpg" alt="" width="1144" height="817" srcset="https://www.waseda.jp/top/en/assets/uploads/2020/10/umezu.jpg 1144w, https://www.waseda.jp/top/en/assets/uploads/2020/10/umezu-610x436.jpg 610w, https://www.waseda.jp/top/en/assets/uploads/2020/10/umezu-768x548.jpg 768w" sizes="(max-width: 1144px) 100vw, 1144px"></p>
              </div>
                          </div></div>]]>
            </description>
            <link>https://www.waseda.jp/top/en/news/73810</link>
            <guid isPermaLink="false">hacker-news-small-sites-24700553</guid>
            <pubDate>Tue, 06 Oct 2020 17:56:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Q3 Linux touchpad update: Multitouch gesture test packages now ready]]>
            </title>
            <description>
<![CDATA[
Score 379 | Comments 101 (<a href="https://news.ycombinator.com/item?id=24700537">thread link</a>) | @wbharding
<br/>
October 6, 2020 | https://bill.harding.blog/2020/10/06/q3-linux-touchpad-like-macbook-update-multitouch-gesture-test-packages-are-ready/ | <a href="https://web.archive.org/web/*/https://bill.harding.blog/2020/10/06/q3-linux-touchpad-like-macbook-update-multitouch-gesture-test-packages-are-ready/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="comments">

		<div id="respond">
		<h3 id="reply-title">Leave a Reply <small></small></h3><form action="https://bill.harding.blog/wp-comments-post.php" method="post" id="commentform" novalidate=""><p><span id="email-notes">Your email address will not be published.</span> Required fields are marked <span>*</span></p><p><label for="comment">Comment</label> </p><p><label for="author">Name <span>*</span></label> </p>
<p><label for="email">Email <span>*</span></label> </p>
<p><label for="url">Website</label> </p>
<p> <label for="wp-comment-cookies-consent">Save my name, email, and website in this browser for the next time I comment.</label></p>
<!-- Anti-spam plugin wordpress.org/plugins/anti-spam/ --><div><p><label>Current ye@r <span>*</span></label>
					
					
				  </p>

</div><!--\End Anti-spam plugin --></form>	</div><!-- #respond -->
	
</div></div>]]>
            </description>
            <link>https://bill.harding.blog/2020/10/06/q3-linux-touchpad-like-macbook-update-multitouch-gesture-test-packages-are-ready/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24700537</guid>
            <pubDate>Tue, 06 Oct 2020 17:54:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A quick introduction to data parallelism in Julia]]>
            </title>
            <description>
<![CDATA[
Score 152 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24700436">thread link</a>) | @amkkma
<br/>
October 6, 2020 | https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/ | <a href="https://web.archive.org/web/*/https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> <p>If you have a large collection of data and have to do similar computations on each element, <a href="https://en.wikipedia.org/wiki/Data_parallelism">data parallelism</a> is an easy way to speedup computation using multiple CPUs and machines as well as GPU(s). While this is not the only kind of parallelism, it covers a vast class of compute-intensive programs. A major hurdle for using data parallelism is that you need to unlearn some habits useful in sequential computation (i.e., patterns result in mutations of data structure). In particular, it is important to use libraries that help you describe <em>what</em> to compute rather than <em>how</em> to compute. Practically, it means to use generalized form of map and reduce operations and learn how to express your computation in terms of them. Luckily, if you already know how to write <a href="https://docs.julialang.org/en/v1/manual/arrays/#Generator-Expressions">iterator comprehensions</a>, there is not much more to learn for accessing a large class of data parallel computations.</p>  <p>This introduction primary focuses on the Julia packages that I (Takafumi Arakaki <strong><code>@tkf</code></strong>) have developed. As a result, it currently focuses on thread-based parallelism. There is simple distributed computing support. GPU support is a frequently requested feature but <a href="https://github.com/JuliaFolds/Transducers.jl/issues/236">it hasn't been implemented yet</a>. See also <a href="https://juliafolds.github.io/data-parallelism/explanation/libraries/">other parallel-computation libraries in Julia</a>.</p> <p>Also note that this introduction does not discuss how to use threading primitives such as <a href="https://docs.julialang.org/en/v1/base/multi-threading/"><code>Threads.@spawn</code></a> since it is too low-level and error-prone. For data parallelism, a higher-level description is much more appropriate. It also helps you write more reusable code; e.g., using the same code for single-threaded, multi-threaded, and distributed computing.</p>   <h2 id="getting_julia_and_libraries"><a href="#getting_julia_and_libraries">Getting <code>julia</code> and libraries</a></h2> <p>Most of the examples here may work in all Julia 1.x releases. However, for the best result, it is highly recommended to get the latest released version (1.5.2 as of writing). You can download it at <a href="https://julialang.org/">https://julialang.org/</a>.</p> <p>Once you get <code>julia</code>, you can get the dependencies required for this tutorial by running <code>using Pkg; Pkg.add(["Transducers", "ThreadsX", "OnlineStats", "FLoops", "MicroCollections", "BangBang", "Plots", "BenchmarkTools"])</code> in Julia REPL.</p> <p>If you prefer using exactly the same environment used for testing this tutorial, run the following commands</p> <pre><code>git <span>clone</span> https://github.com/JuliaFolds/data-parallelism
<span>cd</span> data-parallelism
julia --project</code></pre> <p>and then in the Julia REPL:</p> <pre><code><span>julia&gt;</span><span> <span>using</span> Pkg
</span>
<span>julia&gt;</span><span> Pkg.instantiate()</span></code></pre> <h2 id="starting_julia"><a href="#starting_julia">Starting <code>julia</code></a></h2> <p>To use multi-threading in Julia, you need to start it with multiple execution threads. If you have Julia 1.5 or higher, you can start it with the <code>-t auto</code> (or, equivalently, <code>--threads auto</code>) option:</p> <pre><code>$ julia -t auto
               _
   _       _ _(_)_     |  Documentation: https://docs.julialang.org
  (_)     | (_) (_)    |
   _ _   _| |_  __ _   |  Type "?" for help, "]?" for Pkg help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 1.5.2 (2020-09-23)
 _/ |\__'_|_|_|\__'_|  |  Official https://julialang.org/ release
|__/                   |

julia&gt; Threads.nthreads()  # number of core you have
8</code></pre> <p>The command line option <code>-t</code>/<code>--threads</code> can also take the number of threads to be used. In older Julia releases, use the <code>JULIA_NUM_THREADS</code> environment variable. For example, on Linux and macOS, <code>JULIA_NUM_THREADS=4 julia</code> starts <code>juila</code> with 4 execution threads.</p> <p>For more information, see <a href="https://docs.julialang.org/en/v1/manual/multi-threading/#Starting-Julia-with-multiple-threads">Starting Julia with multiple threads</a> in the Julia manual.</p> <h3 id="starting_julia_with_multiple_worker_processes"><a href="#starting_julia_with_multiple_worker_processes">Starting <code>julia</code> with multiple worker processes</a></h3> <p>A few examples below mention <a href="https://docs.julialang.org/en/v1/stdlib/Distributed/">Distributed.jl</a>-based parallelism. Like how multi-threading is setup, you need to setup multiple worker processes to get speedup. You can start <code>julia</code> with <code>-p auto</code> (or, equivalently, <code>--procs auto</code>). Distributed.jl also lets you add worker processes after starting Julia with <a href="https://docs.julialang.org/en/v1/stdlib/Distributed/#Distributed.addprocs"><code>addprocs</code></a>:</p> <pre><code><span>using</span> Distributed
addprocs(<span>8</span>)</code></pre> <p>For more information, see <a href="https://docs.julialang.org/en/v1/manual/distributed-computing/#Starting-and-managing-worker-processes">Starting and managing worker processes</a> section in the Julia manual.</p> <h2 id="mapping"><a href="#mapping">Mapping</a></h2> <p>Mapping is probably the most frequently used function in data parallelism. Recall how Julia's sequential <code>map</code> works:</p> <pre><code>a1 = map(string, <span>1</span>:<span>9</span>, <span>'a'</span>:<span>'i'</span>)</code></pre>
<pre><code>9-element Array{String,1}:
 "1a"
 "2b"
 "3c"
 "4d"
 "5e"
 "6f"
 "7g"
 "8h"
 "9i"</code></pre>
<p>We can simply replace it with <a href="https://github.com/tkf/ThreadsX.jl"><code>ThreadsX.map</code></a> for thread-based parallelism (see also <a href="https://juliafolds.github.io/data-parallelism/explanation/libraries/">other libraries</a>):</p>
<pre><code><span>using</span> ThreadsX
a2 = ThreadsX.map(string, <span>1</span>:<span>9</span>, <span>'a'</span>:<span>'i'</span>)
<span>@assert</span> a1 == a2</code></pre>

<p>Julia's standard library Distributed.jl contains <a href="https://docs.julialang.org/en/v1/stdlib/Distributed/#Distributed.pmap"><code>pmap</code></a> as a distributed version of <code>map</code>:</p>
<pre><code><span>using</span> Distributed
a3 = pmap(string, <span>1</span>:<span>9</span>, <span>'a'</span>:<span>'i'</span>)
<span>@assert</span> a1 == a3</code></pre>

<div><div><p>üî¨ Test Code</p>
<pre><code><span>using</span> Test
    <span>@testset</span> <span>begin</span>
        <span>@test</span> a1 == a2
        <span>@test</span> a1 == a3
    <span>end</span></code></pre></div> <div><p>‚òë Pass</p>
<pre><code>Test Summary: | Pass  Total
test set      |    2      2
</code></pre></div></div>
<h3 id="practical_example_stopping_time_of_collatz_function"><a href="#practical_example_stopping_time_of_collatz_function">Practical example: Stopping time of Collatz function</a></h3>
<p>As a slightly more "practical" example, let's play with the <a href="https://en.wikipedia.org/wiki/Collatz_conjecture">Collatz conjecture</a> which states that recursive application the <em>Collatz function</em> defined as</p>
<pre><code>collatz(x) =
    <span>if</span> iseven(x)
        x √∑ <span>2</span>
    <span>else</span>
        <span>3</span>x + <span>1</span>
    <span>end</span></code></pre>

<p>reaches the number 1 for all positive integers.</p>
<p>I'll skip the mathematical background of it (as I don't know much about it) but let me mention that there are plenty of fun-to-watch explanations in YouTube :)</p>
<p>If the conjecture is correct, the number of iteration required for the initial value is finite.  In Julia, we can calculate it with</p>
<pre><code><span>function</span> collatz_stopping_time(x)
    n = <span>0</span>
    <span>while</span> <span>true</span>
        x == <span>1</span> &amp;&amp; <span>return</span> n
        n += <span>1</span>
        x = collatz(x)
    <span>end</span>
<span>end</span></code></pre>

<p>Just for fun, let's plot the stopping time of the initial values from 1 to 10,000:</p>
<pre><code><span>using</span> Plots
plt = scatter(
    map(collatz_stopping_time, <span>1</span>:<span>10_000</span>),
    xlabel = <span>"Initial value"</span>,
    ylabel = <span>"Stopping time"</span>,
    label = <span>""</span>,
    markercolor = <span>1</span>,
    markerstrokecolor = <span>1</span>,
    markersize = <span>3</span>,
    size = (<span>450</span>, <span>300</span>),
)</code></pre>
<p><img src="https://juliafolds.github.io/data-parallelism/assets/tutorials/quick-introduction/code/output/collatz_stopping_time_scatter.png" alt=""></p><p>We can easily parallelize <code>map(collatz_stopping_time, 1:10_000)</code> and get a good speedup:</p>
<pre><code><span>julia&gt;</span><span> Threads.nthreads()  
</span>4

<span>julia&gt;</span><span> <span>using</span> BenchmarkTools
</span>
<span>julia&gt;</span><span> <span>@btime</span> map(collatz_stopping_time, <span>1</span>:<span>100_000</span>);
</span>  18.116 ms (2 allocations: 781.33 KiB)

<span>julia&gt;</span><span> <span>@btime</span> ThreadsX.map(collatz_stopping_time, <span>1</span>:<span>100_000</span>);
</span>  5.391 ms (1665 allocations: 7.09 MiB)</code></pre>
<h2 id="iterator_comprehensions"><a href="#iterator_comprehensions">Iterator comprehensions</a></h2>
<p>Julia's <a href="https://docs.julialang.org/en/v1/manual/arrays/#Generator-Expressions">iterator comprehension syntax</a> is a powerful tool for composing mapping, filtering, and flattening. Recall that mapping can be written as an array or iterator comprehension:</p>
<pre><code>b1 = map(x -&gt; x + <span>1</span>, <span>1</span>:<span>3</span>)
b2 = [x + <span>1</span> <span>for</span> x <span>in</span> <span>1</span>:<span>3</span>]         
b3 = collect(x + <span>1</span> <span>for</span> x <span>in</span> <span>1</span>:<span>3</span>)  
<span>@assert</span> b1 == b2 == b3
b1</code></pre>
<pre><code>3-element Array{Int64,1}:
 2
 3
 4</code></pre>
<p>The iterator comprehension can be executed with threads by using <a href="https://github.com/tkf/ThreadsX.jl"><code>ThreadsX.collect</code></a>:</p>
<pre><code>b4 = ThreadsX.collect(x + <span>1</span> <span>for</span> x <span>in</span> <span>1</span>:<span>3</span>)
<span>@assert</span> b1 == b4</code></pre>

<div><div><p>üî¨ Test Code</p>
<pre><code><span>using</span> Test
    <span>@testset</span> <span>begin</span>
        <span>@test</span> b1 == b2 == b3
    <span>end</span></code></pre></div> <div><p>‚òë Pass</p>
<pre><code>Test Summary: | Pass  Total
test set      |    1      1
</code></pre></div></div>
<p>Note that more complex composition of mapping, filtering, and flattening can also be executed in parallel:</p>
<pre><code>c1 = ThreadsX.collect(y <span>for</span> x <span>in</span> <span>1</span>:<span>3</span> <span>if</span> isodd(x) <span>for</span> y <span>in</span> <span>1</span>:x)</code></pre>
<pre><code>4-element Array{Int64,1}:
 1
 1
 2
 3</code></pre>
<p><a href="https://juliafolds.github.io/Transducers.jl/dev/reference/manual/#Transducers.dcollect"><code>Transducers.dcollect</code></a> is for using iterator comprehensions with a distributed backend:</p>
<pre><code><span>using</span> Transducers
c2 = dcollect(y <span>for</span> x <span>in</span> <span>1</span>:<span>3</span> <span>if</span> isodd(x) <span>for</span> y <span>in</span> <span>1</span>:x)
<span>@assert</span> c1 == c2</code></pre>

<div><div><p>üî¨ Test Code</p>
<pre><code><span>@test</span> c1 == c2 == [<span>1</span>, <span>1</span>, <span>2</span>, <span>3</span>]</code></pre></div> </div>
<h2 id="pre-defined_reductions"><a href="#pre-defined_reductions">Pre-defined reductions</a></h2>
<p>Functions such as <code>sum</code>, <code>prod</code>, <code>maximum</code>, and <code>all</code> are the examples of <em>reduction</em> (aka <a href="https://en.wikipedia.org/wiki/Fold_(higher-order_function)"><em>fold</em></a>) that can be parallelized.  They are very powerful tools when combined with iterator comprehensions.  Using ThreadsX.jl, a sum of an iterator created by the comprehension syntax</p>
<pre><code>d1 = sum(x + <span>1</span> <span>for</span> x <span>in</span> <span>1</span>:<span>3</span>)</code></pre>
<pre><code>9</code></pre>
<p>can easily be parallelized by</p>
<pre><code>d2 = ThreadsX.sum(x + <span>1</span> <span>for</span> x <span>in</span> <span>1</span>:<span>3</span>)</code></pre>
<pre><code>9</code></pre>
<div><div><p>üî¨ Test Code</p>
<pre><code><span>@test</span> d1 == d2</code></pre></div> </div>
<p>For the full list of pre-defined reductions and other parallelized functions, type <code>ThreadsX.</code> and press <kbd>TAB</kbd> in the REPL.</p>
<h3 id="practical_example_maximum_stopping_time_of_collatz_function"><a href="#practical_example_maximum_stopping_time_of_collatz_function">Practical example: Maximum stopping time of Collatz function</a></h3>
<p>We can use <code>maximum</code> to compute the maximum stopping time of Collatz function on a given the range of initial values</p>
<pre><code>max_time = ThreadsX.maximum(collatz_stopping_time, <span>1</span>:<span>100_000</span>)</code></pre>
<pre><code>350</code></pre>
<div><div><p>üî¨ Test Code</p>
<pre><code><span>@test</span> max_time == <span>350</span></code></pre></div> </div>
<p>We get a speedup similar to the <code>map</code> example above:</p>
<pre><code><span>julia&gt;</span><span> <span>@btime</span> maximum(collatz_stopping_time, <span>1</span>:<span>100_000</span>)
</span>  17.625 ms (0 allocations: 0 bytes)
350

<span>julia&gt;</span><span> <span>@btime</span> ThreadsX.maximum(collatz_stopping_time, <span>1</span>:<span>100_000</span>)
</span>  5.024 ms (1214 allocations: 69.17 KiB)
350</code></pre>
<h3 id="onlinestatsjl"><a href="#onlinestatsjl">OnlineStats.jl</a></h3>
<p><a href="https://github.com/joshday/OnlineStats.jl">OnlineStats.jl</a> provides a <a href="https://joshday.github.io/OnlineStats.jl/latest/stats_and_models/">very rich</a> and <a href="https://joshday.github.io/OnlineStats.jl/latest/collections/">composable</a> set of reductions.  You can pass it as the first argument to <a href="https://github.com/tkf/ThreadsX.jl#onlinestatsjl"><code>ThreadsX.reduce</code></a>:</p>
<pre><code><span>using</span> OnlineStats: Mean
e1 = ThreadsX.reduce(Mean(), <span>1</span>:<span>10</span>)</code></pre>
<pre><code>Mean: n=10 | value=5.5</code></pre>
<div><div><p>üî¨ Test Code</p>
<pre><code><span>using</span> OnlineStats; <span>@test</span> e1 == fit!(Mean(), <span>1</span>:<span>10</span>)</code></pre></div> </div>
<div><p>üí° Note</p>
<p>While OnlineStats.jl often does not provide the fastest way to compute the given statistics when all the intermediate data can fit in memory, in many cases you don't really need the absolute best performance. However, it may be worth considering other ways to compute statistics if ThreadsX.jl + OnlineStats.jl becomes the bottleneck.</p></div>
<h2 id="manual_reductions"><a href="#manual_reductions">Manual reductions</a></h2>
<p>For non-trivial parallel computations, you need to write a custom reduction.  <a href="https://github.com/JuliaFolds/FLoops.jl">FLoops.jl</a> provides a concise set of syntax for writing custom reductions.  For example, this is how to compute sums of two quantities in one sweep:</p>
<pre><code><span>using</span> FLoops

<span>@floop</span> <span>for</span> (x, y) <span>in</span> zip(<span>1</span>:<span>3</span>, <span>1</span>:<span>2</span>:<span>6</span>)
    a = x + y
    b = x - y
    <span>@reduce</span>(s += a, t += b)
<span>end</span>
(s, t)</code></pre>
<pre><code>(15, -3)</code></pre> <div><div><p>üî¨ Test Code</p>
<pre><code><span>@test</span> (s, t) == (<span>15</span>, -<span>3</span>)</code></pre></div> </div>
<p>In this example, we do not initialize <code>s</code> and <code>t</code>; but it is not a typo.  In parallel sum, the only reasonable value of the initial state of the accumulators like <code>s</code> and <code>t</code> is zero.  So, <code>@reduce(s += a, t
+= b)</code> works as if <code>s</code> and <code>t</code> are initialized to appropriate type of zero.  However, since there are many zeros in Julia (<code>0::Int</code>, <code>0.0::Float64</code>, <code>(0x00 + 0x00im)::Complex{UInt8}</code>, ...), <code>s</code> and <code>t</code> are undefined if the input collection (i.e., the value of <code>xs</code> in <code>for
x in xs</code>) is empty.</p>
<p>To control the type of the accumulators and also to avoid <code>UndefVarError</code> in the empty case, you can set the initial value with <code>accumulator = initial_value op input</code> syntax</p>
<pre><code><span>@floop</span> <span>for</span> (x, y) <span>in</span> zip(<span>1</span>:<span>3</span>, <span>1</span>:<span>2</span>:<span>6</span>)
    a = x + y
    b = x - y
    <span>@reduce</span>(s2 = <span>0.0</span> + a, t2 = <span>0</span><span>im</span> + b)
<span>end</span>
(s2, t2)</code></pre>
<pre><code>(15.0, -3 + 0im)</code></pre> <div><div><p>üî¨ Test Code</p>
<pre><code><span>@test</span> (s2, t2) === (<span>15.0</span>, -<span>3</span> + <span>0</span><span>im</span>)</code></pre></div> </div>
<p>To understand the computation of <code>@floop</code>‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/">https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/</a></em></p>]]>
            </description>
            <link>https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24700436</guid>
            <pubDate>Tue, 06 Oct 2020 17:44:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Gradient Boosted Decision Trees]]>
            </title>
            <description>
<![CDATA[
Score 133 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24700250">thread link</a>) | @simonwardjones
<br/>
October 6, 2020 | https://www.simonwardjones.co.uk/posts/gradient_boosted_decision_trees/ | <a href="https://web.archive.org/web/*/https://www.simonwardjones.co.uk/posts/gradient_boosted_decision_trees/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

<article>
  <div>
    <div>
      
      
      
      <p>What is a <code>gradient boosted decision tree</code>? ü§∑‚Äç‚ôÇÔ∏è</p>
<p>This article is the fifth in a series covering fundamental machine learning algorithms. Each post will be split into two parts</p>
<ol>
<li><a href="#the-idea-and-key-concepts"><strong>The idea and key concepts</strong></a>
- Most people should be able to follow this section and learn how the algorithm works</li>
<li><a href="#the-maths"><strong>The maths</strong></a>
- This is for the interested reader and will include detailed mathematical derivations followed by an implementation in Python</li>
</ol>
<p>Click</p>
<ul>
<li><a href="https://www.simonwardjones.co.uk/posts/linear_regression/">here</a> if you missed <code>From zero to Linear Regression</code></li>
<li><a href="https://www.simonwardjones.co.uk/posts/logistic_regression/">here</a> if you missed <code>From zero to Logistic Regression</code></li>
<li><a href="https://www.simonwardjones.co.uk/posts/decision_trees/">here</a> if you missed <code>From zero to Decision Tree</code></li>
<li><a href="https://www.simonwardjones.co.uk/posts/random_forests/">here</a> if you missed <code>From zero to Random Forest</code></li>
</ul>
<p>Great if you have already read these!</p>
<hr>
<h2 id="the-idea-and-key-concepts">The idea and key concepts</h2>
<p>In the last post we talked about <code>underfitting</code>, <code>overfitting</code>, <code>bias</code> and <code>variance</code>. We explained how a <code>random forest</code> uses the average output of multiple trees to reduce the chance of overfitting without introducing bias by oversimplifying (such as using only one tree but restricting the depth).</p>
<p><code>Gradient boosting</code> is a machine learning technique for regression and classification where multiple models are trained <code>sequentially</code> with each model trying to learn the mistakes from the previous models. The individual models are known as <code>weak learners</code> and in the case of <code>gradient boosted decision trees</code> the individual models are decision trees.</p>
<p>In order to give intuition it is easiest to consider first the case of regression. Imagine we are again trying to predict house prices in a desirable area of north London. With training data that looks like the following</p>





<table <thead="">
<tbody><tr>
<th></th>
<th>House size üè†</th>
<th>Garden size üå≥</th>
<th>Garage? üöô</th>
<th>True House Price üí∞</th>
</tr>

</tbody><tbody>
<tr>
<td>1</td>
<td>1000</td>
<td>700</td>
<td>Garage</td>
<td>¬£1m</td>
</tr>
<tr>
<td>2</td>
<td>770</td>
<td>580</td>
<td>No Garage</td>
<td>¬£0.75m</td>
</tr>
<tr>
<td>3</td>
<td>660</td>
<td>200</td>
<td>Garage</td>
<td>¬£0.72m</td>
</tr>
</tbody>
</table>

<p><strong>Initial prediction $f_0$</strong></p>
<p>We can make an initial prediction for each of the house prices based on an initial model, let‚Äôs call this initial model $f_0$. Often this model is very simple - just using the mean of the target variable in the training data. The following table shows the initial predictions as well as the <code>errors</code> $e_1$ (also known as the <code>residuals</code>) defined for each sample as $e_1 = y - f_0$ where $y$ is the true value and $f_0$ is our initial prediction</p>





<table <thead="">
<tbody><tr>
<th></th>
<th>True House Price üí∞</th>
<th>Initial Prediction $f_0$</th>
<th>Error $e_1$</th>
</tr>

</tbody><tbody>
<tr>
<td>1</td>
<td>¬£1m</td>
<td>¬£0.82m</td>
<td>¬£(1m - 0.82) = ¬£0.18m</td>
</tr>
<tr>
<td>2</td>
<td>¬£0.75m</td>
<td>¬£0.82m</td>
<td>¬£(0.75m - 0.82m) = -¬£0.07m</td>
</tr>
<tr>
<td>3</td>
<td>¬£0.72m</td>
<td>¬£0.82m</td>
<td>¬£(0.72m - 0.82m) = -¬£0.1m</td>
</tr>
</tbody>
</table>

<p><strong>Predicting the error</strong></p>
<p>Our initial prediction isn‚Äôt very accurate as it is just the mean house price of the training data! In order to improve this we introduce another model $f_1$ trying to predict the error $e_1$ from the sample feature values. In gradient boosted decision trees this model is itself a decision tree. So now we can predict what the error $e_1$ will be for each sample using $f_1$</p>





<table <thead="">
<tbody><tr>
<th></th>
<th>True House Price üí∞</th>
<th>Initial Prediction $f_0$</th>
<th>Error $e_1$</th>
<th>Predicted Error $f_1$</th>
</tr>

</tbody><tbody>
<tr>
<td>1</td>
<td>¬£1m</td>
<td>¬£0.82m</td>
<td>¬£(1m - 0.82) = ¬£0.18m</td>
<td>¬£0.17m</td>
</tr>
<tr>
<td>2</td>
<td>¬£0.75m</td>
<td>¬£0.82m</td>
<td>¬£(0.75m - 0.82m) = -¬£0.07m</td>
<td>¬£-0.09m</td>
</tr>
<tr>
<td>3</td>
<td>¬£0.72m</td>
<td>¬£0.82m</td>
<td>¬£(0.72m - 0.82m) = -¬£0.1m</td>
<td>¬£-0.1m</td>
</tr>
</tbody>
</table>

<p><strong>Updating our prediction using the error prediction</strong></p>
<p>For the first house our initial prediction $f_0$ was ¬£0.82m (using the mean) and as we actually know the true value we can see this gave an error $e_1$ of 0.18m. We then trained $f_1$ - a decision tree - to predict the error $e_1$ for each sample. In practise this is only a prediction of the error so it wont be exactly equal, in this toy example our $f_1$ model predicted an error of ¬£0.17m. We could now combine the two models into a new second prediction called $F_1$ by adding the predicted error $f_1$ to the initial prediction $f_0$ as in the table below</p>





<table <thead="">
<tbody><tr>
<th></th>
<th>True House Price üí∞</th>
<th>Initial Prediction $f_0$</th>
<th>Predicted Error $f_1$</th>
<th>Prediction $F_1 =f_0 + f_1$</th>
</tr>

</tbody><tbody>
<tr>
<td>1</td>
<td>¬£1m</td>
<td>¬£0.82m</td>
<td>¬£0.17m</td>
<td>¬£0.99m</td>
</tr>
<tr>
<td>2</td>
<td>¬£0.75m</td>
<td>¬£0.82m</td>
<td>-¬£0.09m</td>
<td>¬£0.73m</td>
</tr>
<tr>
<td>3</td>
<td>¬£0.72m</td>
<td>¬£0.82m</td>
<td>-¬£0.1m</td>
<td>¬£0.71m</td>
</tr>
</tbody>
</table>

<p><strong>Additive model</strong></p>
<p>Now we have a second prediction $F_1$ we can continue in a sequential manner, again calculating the error of our second prediction $e_2$ and training a tree $f_2$ to predict this second error. Then once again we add this second predicted error to the second prediction to get a third prediction $F_2 = F_1 + f_2$ and so on. As the models are summed together this approach is known as an <code>aditive model</code>. In general we have
$$F_m =  F_{m-1} + f_m$$
Where the next prediction $F_m$ is made up of the current prediction $F_{m-1}$ and the prediction of the error $f_m \sim e_m =y - F_{m-1}$ at this stage. In general the number of <code>weak learners</code> is a <code>hyper parameter</code> you have to choose.</p>
<p><strong>learning rate</strong></p>
<p>We can think of each individual <code>weak learner</code> $f_m$ as stepping our predictions closer to the true target values $y$. To reduce the variance and overfitting rather than stepping the whole predicted error we can instead add only a fraction of the step controlled by the learning rate. So rather than
$$F_m =  F_{m-1} + f_m$$
In gradient boosting we use
$$F_m =  F_{m-1} + (\text{learning rate}*f_m)$$
This process requires more steps but reduces the variance and overfitting overall.</p>
<p><strong>Summary of the algorithm</strong></p>
<ol>
<li>Make initial model $f_0$ (often the mean of y)</li>
<li>Train decision tree model $f_1$ on the error $e_1 = y - f_0$ where y is the true value</li>
<li>Calculate new prediction $F_1 = f_0 + \eta * f_1$ where $\eta$ is the learning rate</li>
<li>Repeat 2, 3 as many times as chosen where in general
<ol>
<li>Train model $f_m$ on the error $e_m = y - F_{m-1}$</li>
<li>Calculate new prediction as $F_{m-1} + \eta * f_m$</li>
</ol>
</li>
</ol>
<p>In short gradient boosting uses an initial prediction and then sequentially updates this prediction by fitting a model to the error at that stage.</p>
<p>In the following section we explore the mathematical details and extend the algorithm to the classification setting. We also cover the intuition behind gradient boosting as gradient descent.</p>
<hr>
<h2 id="the-maths">The maths</h2>
<p><strong>Why is it called gradient boosting?</strong></p>
<p>In general in <code>supervised learning</code> we aim to find a model $F$ to fit the data such that the predicted value $\hat{y}_i$ for the $j$th training example $\mathbf{x}_i$ is approximately equal to the $j$th target value $y_i$ or equivalently</p>
<p>
    $$
\hat{y}_i=F(\mathbf{x}_i)\sim y_i \quad\forall j \in {1,\dots,n} 
$$
</p><p>
Where n is the number of training samples.</p>
<p>Equivalently we aim to minimise a loss function $\mathcal{L(y, \hat{y})}$ which tells us how badly the model $\hat{y}$ currently fits the data $y$.</p>
<p>In a <code>parametric</code> setting (e.g. logistic regression) the model can be written as

</p><p>
    $$
\hat{y}_i=F_{\mathbf{\theta}}(\mathbf{x}_i)
$$
</p>
<p>Where the subscript $\mathbf{\theta}$ indicates the models dependence on the parameters. We can also write the loss in terms of $\mathbf{\theta}$ as $\mathcal{L(y, \hat{y}(\mathbf{\theta})})$. In this setting we update the model parameters using gradient descent. That is we iteratively update the model parameters by stepping the parameters in the direction of the negative gradient of the loss function with respect to the parameters (where $\eta$ is the learning rate).</p>

<p>
    $$
\mathbf{\theta}^{m+1} = \mathbf{\theta}^{m} - \eta* \frac{\partial\mathcal{L}}{\partial{\mathbf{\theta}^m}}
$$
</p>
<p>Instead of differentiating the loss with respect to $\mathbf{\theta}$ we can differentiate with respect to the prediction $\hat{y}$ directly. If we think about gradient descent ideally we would update $\hat{y}$ as follows to reduce the cost function</p>

<p>
    $$
\hat{y}_i \to \hat{y}_i - \eta\frac{\partial\mathcal{L}}{\partial{\hat{y}_i}}
$$
</p>
<p>Equivalently we update $F_{m-1}$ by adding another ‚Äúdelta model‚Äù $f_{m+1}$</p>

<p>
    $$
\hat{y}_i = F_m(\mathbf{x}_i) + f_{m+1}(\mathbf{x}_i) \quad\forall j \in {1,\dots,n} 
$$
</p>
<p>Where $\eta$ is the learning rate and

</p><p>
    $$
f_{m+1}(\mathbf{x}_i)= -\eta\frac{\partial\mathcal{L}}{\partial{\hat{y}_i}}
$$
</p>
<p>In practise we cannot set this delta model exactly so we train a model on the data to fit

</p><p>
    $$
 - \eta\frac{\partial\mathcal{L}}{\partial{\hat{y}_i}}$$
</p><p>
In general this gradient can be fitted with any model but gradient boosted decision trees use a decision tree - hence the name! Note each tree will have it‚Äôs own Loss $\mathcal{L}^{f_{m+1}}$ separate to the global loss $\mathcal{L}$.</p>
<p><strong>Key Point</strong></p>
<p>The gradient boosted decision tree is not trained on the residuals at each step. Rather it is trained on the negative gradient of the loss function evaluated using the prediction of the current step - which happens to be the residual for some common cost functions.</p>
<h3 id="regression">Regression</h3>
<p>In the case of regression we define the loss function as the mean square error</p>
<p>$$
\mathcal{L}(\hat{y}) = \frac{1}{2n}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2
$$
hence
$$
-\eta\frac{\partial\mathcal{L}}{\partial{\hat{y}_i}} = \frac{\eta}{n}(y_i-\hat{y}_i)
$$</p>
<p>How the process looks:</p>
<p>We fit $f_0(x)\sim y$ then $F_0(x) = f_0(x)$<br>
We fit $f_1(x)\sim (y-F_0(x))$ then $F_1(x) = F_0(x) + \eta f_1(x)$<br>
We fit $f_2(x)\sim (y-F_1(x))$ then $F_2(x) = F_1(x) + \eta f_2(x)$<br>
We fit $f_3(x)\sim (y-F_2(x))$ then $F_3(x) = F_2(x) + \eta f_3(x)$<br>
‚Ä¶<br>
We fit $f_m(x)\sim (y-F_{m-1}(x))$ then $F_m(x) = F_{m-1}(x) + \eta f_m(x)$</p>
<p>Then predictions $\hat{y} = F_m(x)$</p>
<h4 id="binomial-classification">Binomial Classification</h4>
<p>Suppose our iterative model was $\hat{y}_i = F_m(x_i)$ where the $\hat{y}_i$ directly represented the probability $x_i$ is in class 1. i.e. $P(x_i \in C_1)$ where $C_1$ represents class 1.</p>
<p>In this case the delta model doesn‚Äôt make sense as we would be directly adding to a probability value. As in logistic regression it is often the case to fit the model to a transformation of probability.</p>
<p>We define a model
$$
\hat{y}\sim F(x)
$$
where
$$
\hat{p} = \frac{1}{1+e^{-\hat{y}}}
$$
so
$$
\hat{y} = \log\left(\frac{\hat{p}}{1-\hat{p}}\right)
$$</p>
<p>where $\hat{p}$ represents the probability of being in class 1, $\hat{y}$ is sometimes known as the logit.</p>
<p>Note $\hat{p}\in[0,1],\quad \hat{y}\in(-\infty,\infty),\quad y\in{0,1}$</p>
<p>Hence in the classification setting the gradient boosted decision tree predicts $\hat{y}$ as a sum of multiple delta models. The probability values are then calculated by transforming $\hat{y}$ using the sigmoid function (a.k.a the expit function).</p>
<p>We will use the following fact later on</p>

<p>
    $$
\begin{align}
\hat{p} &amp;= \frac{1}{1+e^{-\hat{y}}} \quad so \\
\hat{p} &amp;= ‚Ä¶</p></div></div></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.simonwardjones.co.uk/posts/gradient_boosted_decision_trees/">https://www.simonwardjones.co.uk/posts/gradient_boosted_decision_trees/</a></em></p>]]>
            </description>
            <link>https://www.simonwardjones.co.uk/posts/gradient_boosted_decision_trees/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24700250</guid>
            <pubDate>Tue, 06 Oct 2020 17:26:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Eliminating Task Processing Outages by Replacing RabbitMQ with Apache Kafka]]>
            </title>
            <description>
<![CDATA[
Score 23 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24699534">thread link</a>) | @sciurus
<br/>
October 6, 2020 | https://doordash.engineering/2020/09/03/eliminating-task-processing-outages-with-kafka/ | <a href="https://web.archive.org/web/*/https://doordash.engineering/2020/09/03/eliminating-task-processing-outages-with-kafka/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
					<p><span>Scaling backend infrastructure to handle hyper-growth is one of the many exciting challenges of working at DoorDash. In mid 2019, we faced significant scaling challenges and frequent outages involving </span><a href="https://en.wikipedia.org/wiki/Celery_(software)"><span>Celery</span></a><span> and </span><a href="https://www.rabbitmq.com/"><span>RabbitMQ</span></a><span>, two technologies powering the system that handles the asynchronous work enabling critical functionalities of our platform, including order checkout and Dasher assignments.&nbsp;</span></p>
<p><span>We quickly solved this problem with a simple, </span><a href="https://kafka.apache.org/"><span>Apache Kafka</span></a><span>-based asynchronous task processing system that stopped our outages while we continued to iterate on a robust solution. Our initial version implemented the smallest set of features needed to accommodate a large portion of existing Celery tasks. Once in production, we continued to add support for more Celery features while addressing novel problems that arose when using Kafka. </span></p>
<h2><span>The problems we faced using Celery and RabbitMQ</span></h2>
<p><span>RabbitMQ and Celery were mission critical pieces of our infrastructure that powered over 900 different asynchronous tasks at DoorDash, including order checkout, merchant order transmission, and Dasher location processing. The problem DoorDash faced was that RabbitMQ was frequently going down due to excessive load. If task processing went down, DoorDash effectively went down and orders could not be completed, resulting in revenue loss for our merchants and Dashers, and a poor experience for our consumers. We faced issues on the following fronts:</span></p>
<ul>
<li><b>Availability:</b><span> Outages caused by demand reduced availability.&nbsp;</span></li>
<li><b>Scalability:</b><span> RabbitMQ could not scale with the growth of our business.&nbsp;</span></li>
<li><b>Observability:</b><span> RabbitMQ offered limited metrics and Celery workers were opaque.&nbsp;</span></li>
<li><b>Operational efficiency:</b><span> Restarting these components was a time-consuming, manual process.&nbsp;</span></li>
</ul>
<h3><span>Why our asynchronous task processing system wasn‚Äôt highly available</span></h3>
<p><span>This biggest problem we faced were outages, and they often came when demand was at its peak. RabbitMQ would go down due to load, </span><a href="https://www.rabbitmq.com/connections.html#high-connection-churn"><span>excessive connection churn</span></a><span>, and other reasons. Orders would be halted, and we‚Äôd have to restart our system or sometimes even bring up an entirely new broker and manually </span><a href="https://en.wikipedia.org/wiki/Failover"><span>failover</span></a><span> in order to recover from the outage.</span></p>
<p><span>On diving deeper into the availability issues, we found the following sub-issues:</span></p>
<ul>
<li><span>Celery allows users to schedule tasks in the future with a countdown or ETA. Our heavy use of&nbsp; these countdowns resulted in noticeable load increases on the broker. Some of our outages were directly related to an increase in tasks with countdowns. We ultimately decided to restrict the use of countdowns in favor of another system we had in place for scheduling work in the future.</span></li>
<li><span>Sudden bursts of traffic would leave RabbitMQ in a degraded state where task consumption was significantly lower than expected. In our experience, this could only be resolved with a RabbitMQ bounce. RabbitMQ has a concept of Flow Control where it will reduce the speed of connections which are publishing too quickly so that queues can keep up. Flow Control was often, but not always, involved in these availability degradations. When Flow Control kicks in, the publishers effectively see it as network latency. Network latency reduces our response times; if latency increases during peak traffic, significant slowdowns can result that cascade as requests pile up upstream.</span></li>
<li><span>Our python </span><a href="https://uwsgi-docs.readthedocs.io/en/latest/"><span>uWSGI</span></a><span> web workers had a feature called harakiri that was enabled to kill any processes that exceeded a timeout. During outages or slowdowns, harakiri resulted in a connection churn to the RabbitMQ brokers as processes were repeatedly killed and restarted. With thousands of web workers running at any given time, any slowness that triggered harakiri would in turn contribute even more to slowness by adding extra load to RabbitMQ.</span></li>
<li><span>In production we experienced several cases where task processing in the Celery consumers&nbsp; stopped, even in the absence of significant load. Our investigation efforts did not yield evidence of any resource constraints that would‚Äôve halted processing, and the workers resumed processing once they were bounced. This problem was never root caused, though we suspect an issue in the Celery workers themselves and not RabbitMQ.</span></li>
</ul>
<p><span>Overall, all of these availability issues were unacceptable for us as high reliability is one of our highest priorities. Since these outages were costing us a lot in terms of missed orders and credibility we needed a solution that would address these problems as soon as possible.</span></p>
<h3><span>Why our legacy solution did not scale&nbsp;</span></h3>
<p><span>The next biggest problem was scale. DoorDash is growing fast and we were quickly reaching the limits of our existing solution. We needed to find something that would keep up with our continued growth since our legacy solution had the following problems:&nbsp;</span></p>
<p><strong>Hitting the vertical scaling limit</strong></p>
<p><span>We were using the largest available single-node RabbitMQ solution that was available to us. There was no path to scale vertically any further and we were already starting to push that node to its limits.</span></p>
<p><strong>The High Availability mode limited our capacity&nbsp;</strong></p>
<p><span>Due to replication, the primary-secondary High Availability (HA) mode reduced throughput compared to the single node option, leaving us with even less headroom than just the single node solution. We could not afford to trade throughput for availability.</span></p>
<p><span>Secondly, the primary-secondary HA mode did not, in practice, reduce the severity of our outages. Failovers took more than 20&nbsp; minutes&nbsp; to complete and would often get stuck requiring manual intervention. Messages were often lost in the process as well.</span></p>
<p><span>We were quickly running out of headroom as DoorDash continued to grow and push our task processing to its limits. We needed a solution that could scale horizontally as our processing needs grew.</span></p>
<h3><span>How Celery and RabbitMQ offered limited observability</span></h3>
<p><span>Knowing what‚Äôs going on in any system is fundamental to ensuring its availability, scalability, and operational integrity.&nbsp;</span></p>
<p><span>As we navigated the issues outlined above, we noticed that :</span></p>
<ul>
<li><span>We were limited to a small set of RabbitMQ metrics available to us.</span></li>
<li><span>We had limited visibility into the Celery workers themselves.</span></li>
</ul>
<p><span>We needed to be able to see real-time metrics of every aspect of our system which meant the observability limitations needed to be addressed as well.&nbsp;</span></p>
<h3><span>The operational efficiency challenges</span></h3>
<p><span>We also faced several issues with operating RabbitMQ:</span></p>
<ul>
<li><span>We often had to failover our RabbitMQ node to a new one to resolve the persistent degradation we observed. This operation was manual and time consuming for the engineers involved and often had to be done late at night, outside of peak times.</span></li>
<li><span>There were no in-house Celery or RabbitMQ experts at DoorDash who we could lean on to help devise a scaling strategy for this technology.</span></li>
</ul>
<p><span>Engineering time spent operating and maintaining RabbitMQ was not sustainable. We needed something that better met our current and future needs.</span></p>
<h2><span>Potential solutions to our problems with Celery and RabbitMQ&nbsp;</span></h2>
<p><span>With the problems outlined above, we considered the following solutions:</span></p>
<ul>
<li><b>Change the Celery broker from RabbitMQ to Redis or Kafka. </b>This would allow us to continue using Celery, with a different and potentially more reliable backing datastore.</li>
</ul>
<ul>
<li><b>Add multi-broker support to our </b><a href="https://www.djangoproject.com/"><b>Django</b></a><b> app so consumers could publish to N different brokers based on whatever logic we wanted. </b>Task processing will get sharded across multiple brokers, so each broker will experience a fraction of the initial load.</li>
</ul>
<ul>
<li><b>Upgrade to newer versions of Celery and RabbitMQ. </b>Newer versions of Celery and RabbitMQ were expected to fix reliability issues, buying us time as we were already extracting components from our Django monolith in parallel.</li>
</ul>
<ul>
<li><b>Migrate to a custom solution backed by Kafka. </b>This solution takes more effort than the other options we listed, but also has more potential to solve every problem we were having with the legacy solution.</li>
</ul>
<p><span>Each option has its pros and cons:</span></p>
<table>
<tbody>
<tr>
<td><b>Option</b></td>
<td><b>Pros</b></td>
<td><b>Cons</b></td>
</tr>
<tr>
<td><span>Redis as broker&nbsp;</span></td>
<td>
<ul>
<li><span>Improved availability with ElasticCache and multi-AZ support</span></li>
<li><span>Improved broker observability with ElasticCache as the broker</span></li>
<li><span>Improved operational efficiency</span></li>
<li><span>In-house operational experience and expertise with Redis</span></li>
<li><span>A broker swap is straight-foward as a supported option in Celery</span></li>
<li><span>Harakiri connection churn does not significantly degrade Redis performance</span></li>
</ul>
</td>
<td>
<ul>
<li><span>Incompatible with Redis clustered mode</span></li>
<li><span>Single node Redis does not scale horizontally</span></li>
<li><span>No Celery observability improvements</span></li>
<li><span>This solution does not address the observed issue where Celery workers stopped processing tasks</span></li>
</ul>
</td>
</tr>
<tr>
<td><span>Kafka as broker</span></td>
<td>
<ul>
<li><span>Kafka can be highly available</span></li>
<li><span>Kafka is horizontally scalable</span></li>
<li><span>Improved observability with Kafka as the broker</span></li>
<li><span>Improved operational efficiency</span></li>
<li><span>DoorDash had in-house Kafka expertise</span></li>
<li><span>A broker swap is straight-foward as a supported option in Celery</span></li>
<li><span>Harakiri connection churn does not significantly degrade Kafka performance</span></li>
</ul>
</td>
<td>
<ul>
<li><span>Kafka is not supported by Celery yet&nbsp;</span></li>
<li><span>Does not address the observed issue where Celery workers stop processing tasks</span></li>
<li><span>No celery observability improvements</span></li>
<li><span>Despite in-house experience, we had not operated Kafka at scale at DoorDash.</span></li>
</ul>
</td>
</tr>
<tr>
<td><span>Multiple brokers</span></td>
<td>
<ul>
<li><span>Improved availability&nbsp;</span></li>
<li><span>Horizontal scalability</span></li>
</ul>
</td>
<td>
<ul>
<li><span>No observability improvements</span></li>
<li><span>No operational efficiency improvements</span></li>
<li><span>Does not address the observed issue where Celery workers stop processing tasks</span></li>
<li><span>Does not address the issue with harakiri-induced connection churn</span></li>
</ul>
</td>
</tr>
<tr>
<td><span>Upgrade versions</span></td>
<td>
<ul>
<li><span>Might improve the issue where RabbitMQ becomes stuck in a degraded state</span></li>
<li><span>Might improve the issue where Celery workers get stuck</span></li>
<li><span>Might buy us headroom to implement a longer term strategy</span></li>
</ul>
</td>
<td>
<ul>
<li><span>Not guaranteed to fix our observed bugs</span></li>
<li><span>Will not immediately fix our issues with availability, scalability, observability, and operational efficiency</span></li>
<li><span>Newer versions of RabbitMQ and Celery required newer versions of Python.</span></li>
<li><span>Does not address the issue with harakiri-induced connection churn</span></li>
</ul>
</td>
</tr>
<tr>
<td><span>Custom Kafka solution</span></td>
<td>
<ul>
<li><span>Kafka can be highly available</span></li>
<li><span>Kafka is horizontally scalable</span></li>
<li><span>Improved observability with Kakfa as the broker</span></li>
<li><span>Improved operational efficiency</span></li>
<li><span>In-house Kafka expertise</span></li>
<li><span>A broker change is ‚Ä¶</span></li></ul></td></tr></tbody></table></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://doordash.engineering/2020/09/03/eliminating-task-processing-outages-with-kafka/">https://doordash.engineering/2020/09/03/eliminating-task-processing-outages-with-kafka/</a></em></p>]]>
            </description>
            <link>https://doordash.engineering/2020/09/03/eliminating-task-processing-outages-with-kafka/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24699534</guid>
            <pubDate>Tue, 06 Oct 2020 16:26:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Teaching my five year old to code by cheating]]>
            </title>
            <description>
<![CDATA[
Score 47 | Comments 25 (<a href="https://news.ycombinator.com/item?id=24699448">thread link</a>) | @gregorymichael
<br/>
October 6, 2020 | https://baugues.com/cheat-code/ | <a href="https://web.archive.org/web/*/https://baugues.com/cheat-code/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<p>My wife and I became <a href="https://baugues.com/homeschool">reluctant homeschoolers</a> this year ‚Äì choosing to teach our five year old daughter without the our school's remote learning. Rachel teaches Reading, Writing, Arts, and Science. My job is Math, Chess, and Technology. </p><p>I started programming on a <a href="https://baugues.com/trs-80">TRS-80</a> when I was six or seven. Back then, the computer booted into BASIC, the most approachable programming language of all time. Hello World in BASIC looks something like:</p><pre><code>10 print "hello world" 
20 goto 10
</code></pre><p>Programming in BASIC was the most instant gratification you could get on a TRS-80. There were few games and no Internet. Had I been introduced to a different programming language at a different age, I'm not sure I would have taken to it.</p><p>That's been a problem I've been wrestling with when introducing our daughter, Emma, to programming. Modern developer environments have a lot of friction and overhead. We've played with Swift Playgrounds, which is great for introducing programming concepts, but feels like you're writing instructions inside a video game as opposed to harnessing the the raw power of code to control the computer.</p><p>A colleague recently introduced me to <a href="https://repl.it/talk/announcements/Announcing-Basic-Language-With-Graphics-Beta/31741">pg-basic on repl.it</a>, which recaptures the simplicity of writing BASIC on a TRS-80.</p><p>Emma and I are working on addition. She likes video games and coding, so I figured we could create a game to practice math. The general idea is: pick two numbers at random, ask her to add them, give her points if she gets it right. We did it in Python, as the code was't that dissimilar to its Basic equivalent. </p><p>Go ahead, run it. (And edit, if you wish.) </p><!--kg-card-begin: html--><!--kg-card-end: html--><p>I composed the code with her sitting next to me, asking for her suggestions along the way.</p><ul><li>"What should we name this variable?"</li><li>"How many points should you get when you get one right?"</li><li>"How many points do you need to win?"</li><li>"What should it say when you win?" </li></ul><p>Then I made her a deal: if she won the game two times, she could cheat and change the code. She loves cheating.</p><p>She quickly figured out she could change the lines that generate numbers to:</p><pre><code>lulu = 0
boonie = random.randrange(11)
</code></pre><p>Math problems got easier. Then she changed it to: </p><pre><code>lulu = 0
boonie = 0
</code></pre><p>Problems got <em>a lot</em> easier.</p><p>She still had to answer a bunch of questions to win the game, and typing zero and enter repeatedly is hard work, so she changed the looping condition to:</p><pre><code>while points &lt; 1:
</code></pre><p>It may be the first time in her short educational career when she's had control over the quiz, instead of the quiz having control over her.</p><p>Thinking back to how I started writing code, it was copying a few dozen lines of BASIC out of the back of <em><a href="http://games.datagrind.com/index.php?pageid=10">3-2-1 Contact</a></em>, getting it to run, and then tweaking it. Today, when I learn a new language or service, it's "copy, paste, edit."</p><p>Composing along side Emma and letting her edit seems to be a winning strategy. Yesterday, after making a modification, she thought for a few seconds, turned to look at me, and said, "... I can use code to do <em>anything</em>."</p><p>She's starting to get it.</p>
			</section></div>]]>
            </description>
            <link>https://baugues.com/cheat-code/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24699448</guid>
            <pubDate>Tue, 06 Oct 2020 16:22:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Try out the new Python 3.9 features in a Python sandbox]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24698705">thread link</a>) | @sunaden
<br/>
October 6, 2020 | https://deepnote.com/project/09e2609b-986b-40fa-9f56-fcbbc60eb61d | <a href="https://web.archive.org/web/*/https://deepnote.com/project/09e2609b-986b-40fa-9f56-fcbbc60eb61d">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><section><p><span>You can view this project and add comments, but can't make any changes.</span><span> <!-- -->You can try to<!-- --> <a>sign in</a> <!-- -->to request additional access.</span></p></section></div></div></div>]]>
            </description>
            <link>https://deepnote.com/project/09e2609b-986b-40fa-9f56-fcbbc60eb61d</link>
            <guid isPermaLink="false">hacker-news-small-sites-24698705</guid>
            <pubDate>Tue, 06 Oct 2020 15:26:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Time for a WTF MySQL Moment]]>
            </title>
            <description>
<![CDATA[
Score 306 | Comments 110 (<a href="https://news.ycombinator.com/item?id=24698660">thread link</a>) | @gbl08ma
<br/>
October 6, 2020 | https://gbl08ma.com/time-for-a-wtf-mysql-moment/ | <a href="https://web.archive.org/web/*/https://gbl08ma.com/time-for-a-wtf-mysql-moment/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content-holder" role="main">
	<div>
		<article id="post-21465">			
			
	<p>
				October 4, 2020 / gbl08ma / 0 Comments	</p>
	<p>Many people have been experiencing strange time perception phenomenon throughout 2020, but certain database management systems have been into time shenanigans for way longer. This came to my attention when a friend received the following exception in one of his projects (his popular Discord bot, <a href="https://accord.abcric.net/">Accord</a>), coming from the MySQL connector being used with EF Core:</p>
<pre>MySqlException: Incorrect TIME value: '960:00:00.000000'</pre>
<p>Not being too experienced with MySQL, as I prefer PostgreSQL for reasons that will soon become self-evident, for a brief moment I assumed the incorrection in this value was the hundreds of hours, as one could reasonably assume that maybe TIME values were capped at 24 hours, or that a different syntax was needed for values spanning multiple days, and that one would need to use, say, ‚Äú40:00:00:00‚Äù to represent 40 days. But reality turned out to be more complex and harder to explain.</p>
<p>With checking the documentation being the most natural next step, the MySQL documentation goes:</p>
<blockquote><p>MySQL retrieves and displays <code>TIME</code> values in <em><code>'hh:mm:ss'</code></em> format (or <em><code>'hhh:mm:ss'</code></em> format for large hours values).</p></blockquote>
<p>So far so good, our problematic TIME value respects this format, but the fact that <code>hh</code> and <code>hhh</code> are explicitly pointed out is already suspect (what about values with over 999 hours?). The next sentence in the documentation explains why, and left me with even more questions of the WTF kind:</p>
<blockquote><p><code>TIME</code> values may range from <code>'-838:59:59'</code> to <code>'838:59:59'</code>.</p></blockquote>
<p>Oooh Kaaay‚Ä¶ that‚Äôs an oddly specific range, but I‚Äôm sure there has to be a technical reason for it. 839 hours is 34.958(3) days, and the whole range spans exactly 6040798 seconds. The documentation also mentions the following:</p>
<blockquote><p>MySQL recognizes <code>TIME</code> values in several formats, some of which can include a trailing fractional seconds part in up to microseconds (6 digits) precision.</p></blockquote>
<p>Therefore, it also makes sense to point out that the whole interval spans <span id="display">6 040 798 </span>000 000 microseconds, but again, these seem like oddly specific numbers. They are not near any power of two, the latter being between 2<sup>42</sup> and 2<sup>43</sup>, so MySQL must be using some awkward internal representation format. But before we dive into that, let me just point out how bad this type is. It is the closest MySQL has to a time interval type, and yet it can‚Äôt deal with intervals that are just a bit over a month long. How much is that ‚Äúbit‚Äù? Not even a nice, rounded number of days, it seems.</p>
<p>To make matters worse, it appears that the most popular EF Core MySQL provider maps .NET‚Äôs <code>TimeSpan</code> to <code>TIME</code> by default, despite the fact that&nbsp;<code>TimeSpan</code> can contain intervals in the dozens of millennia (it uses a 64 bit integer and has 10<sup>-8</sup> s precision) compared to TIME‚Äôs measly ‚Äúa bit over two months‚Äù. This is an <a href="https://github.com/PomeloFoundation/Pomelo.EntityFrameworkCore.MySql/issues/1046">issue other people have run into</a>, and the discussion in that issue includes a ‚ÄúThis mimics the behavior of SQL Server‚Äù remark, which made me go check and, sure enough, SQL Server‚Äôs <code>time</code> is meant to encode a time of day and has a range of 00:00:00.0000000 through 23:59:59.9999999, something which overall makes more sense to me than MySQL‚Äôs odd TIME range.</p>
<p>So let‚Äôs go back to MySQL. What is the reasoning behind such an <em>interesting</em> range? The <a href="https://dev.mysql.com/doc/internals/en/date-and-time-data-type-representation.html">MySQL Internals Manual</a> says that the storage for the TIME type has changed with version 5.6.4, having gained support for fractional seconds in this version. It uses 3 bytes for the non-fractional type. Now, had they just used these 3 bytes to encode a number of seconds, they would have been able to support intervals spanning over 2330 hours, which would already be a considerable improvement over the current 838 hours maximum, even if still a bit useless when it comes to mapping a <code>TimeSpan</code> to it.</p>
<p>This means their encoding must be wasting bits, probably so it is easier to work with‚Ä¶ not sure in what circumstances exactly, but maybe it makes more sense if your database management system (and/or your conception of what the users will do with it) just loves strings, and you really want to speed up the hh:mm:ss representation. So, behold:</p>
<blockquote>
<pre>1 bit sign (1= non-negative, 0= negative)
1 bit unused (reserved for future extensions)
10 bits hour (0-838)
6 bits minute (0-59) 
6 bits second (0-59) 
---------------------
24 bits = 3 bytes</pre>
</blockquote>
<p>This explains everything, right? Well, look closely. 10 bits for the hour‚Ä¶ and a range of 0 to 838. I kindly remind you that 2<sup>10</sup> is 1024, not 838. The plot thickens. I‚Äôm not the first person to wonder about this, of course, <a href="https://stackoverflow.com/questions/39259910/why-is-mysqls-maximum-time-limit-8385959">this was asked on StackOverflow before</a>. The accepted answer in that question explains everything, but <em>it almost didn‚Äôt</em>, as it initially dismisses the odd choice of 838 as ‚Äúbackward compatibility with applications that were written a while ago‚Äù, and only later it is explained that this choice had to do with compatibility with MySQL version‚Ä¶ 3, from the times when, you know, Windows 98 was a fresh operating system and Linux wasn‚Äôt 10 years old yet.</p>
<p>In MySQL 3, the TIME type used 3 bytes as well, but they were used differently. One of the bits was used for the sign as well, but the remaining 23 bits were an integer value produced like this: Hours √ó 10000 + Minutes √ó 100 + Seconds; in other words, the two least significant decimal digits of the number contained the seconds, the next two contained the minutes, and the remaining ones contained the hours. 2<sup>23</sup> is 83888608, i.e. 838:86:08, therefore, the maximum valid time in this format is 838:59:59. This format is even less wieldy than the current one, requiring multiplication and division to do basically anything with it, except string formatting and parsing ‚Äì once again showing that MySQL places too much value on string IO and not so much on having types that are convenient for internal operations and non-string-based protocols.</p>
<p>MySQL developers had ample opportunities to fix this type, or at the very least introduce an alternative one that is free of this reduced range. They changed this type twice from MySQL 3 until now, but decided to retain the range every time, supposedly for compatibility reasons. I am struggling to imagine the circumstances where increasing the value range for a type can break compatibility with an application ‚Äì do types in MySQL have defined overflow behaviors? Is any sane person writing applications where they are relying on a database type‚Äôs intrinsic limits for validation? If yes, who looked at this awkward 838 hours range and thought of it as an appropriate limitation to carry unchanged into their application‚Äôs data model? At this point, I don‚Äôt even want to know.</p>
<p>Despite having changed twice throughout MySQL‚Äôs lifetime, the TIME type is still quite an awkward and limited one. That unused, ‚Äúreserved for future extensions‚Äù bit is, in my opinion, really the <em>pi√®ce de r√©sistance</em> here. Here‚Äôs hoping that one day it will be used to signify a ‚Äúlegacy‚Äù TIME value and that, by then, MySQL and/or MariaDB will have support for a proper type like <a href="https://www.postgresql.org/docs/current/datatype-datetime.html">PostgreSQL‚Äôs INTERVAL</a>, which has a range of +/- 178000000 years and a very reasonable microsecond precision.</p>
	</article>		
	</div>
	</div></div>]]>
            </description>
            <link>https://gbl08ma.com/time-for-a-wtf-mysql-moment/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24698660</guid>
            <pubDate>Tue, 06 Oct 2020 15:23:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hiring for tech jobs has increased more than 100% in these Midwestern cities]]>
            </title>
            <description>
<![CDATA[
Score 118 | Comments 277 (<a href="https://news.ycombinator.com/item?id=24698449">thread link</a>) | @KaiserSanchez
<br/>
October 6, 2020 | https://www.purpose.jobs/blog/hiring-tech-jobs-has-increased-in-midwestern-cities | <a href="https://web.archive.org/web/*/https://www.purpose.jobs/blog/hiring-tech-jobs-has-increased-in-midwestern-cities">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p><img src="https://www.purpose.jobs/hubfs/social-suggested-images/www.michiganbusiness.org49d2d3globalassetsimagesnews1440-bannersdetroit-1440.jpg" alt="Hiring for Tech Jobs has Increased More than 100% in These Midwestern Cities">
</p></div><div>
<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p>When people think tech jobs, they tend to think Silicon Valley or New York City.</p>
<!--more-->
<p>They don‚Äôt think about the Midwest, which is better known for rolling farmland and wide-open spaces than a booming tech scene where startups thrive.</p>
<p>But it‚Äôs time to think again about the Midwest.&nbsp;</p>
<p><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-1-midwest.jpg?width=600&amp;name=img-1-midwest.jpg" alt="img-1-midwest" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-1-midwest.jpg?width=300&amp;name=img-1-midwest.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-1-midwest.jpg?width=600&amp;name=img-1-midwest.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-1-midwest.jpg?width=900&amp;name=img-1-midwest.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-1-midwest.jpg?width=1200&amp;name=img-1-midwest.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-1-midwest.jpg?width=1500&amp;name=img-1-midwest.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-1-midwest.jpg?width=1800&amp;name=img-1-midwest.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>This region comprises 19 percent of the <em>entire U.S. GDP</em>. Twenty-five percent of all computer science grads get their degrees in the Midwest. Forty-five percent of Fortune 500 countries are located here, as is 60 percent of all U.S. manufacturing.</p>
<p>And, as icing on the cake, seven of the top 10 <a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank">most affordable states</a> in the nation are in the Midwest.</p>
<p>What does that have to do with tech jobs? Well, increasingly, startup founders and investors are taking note of all those things the Midwest has to offer, as well as the excellent quality of life and affordable cost of living you can find in so many cities in the Heartland. They‚Äôre realizing you don‚Äôt have to be based in the Golden State or the Big Apple if you want your startup to succeed. You can be based in the Midwest and find just as much success.</p>
<div><p>So tech startups are booming in the Midwest. Don‚Äôt believe us? The proof is in the numbers.</p></div>
<h2><span>In 3 of the Midwest‚Äôs Top 10 Cities, Tech Hiring Is Up More than 100% In the Last 3 Years</span></h2>
<div><p>We‚Äôll let the numbers tell the full story. These are the Midwest‚Äôs top 10 cities in terms of growth and offerings for tech workers.&nbsp;</p></div>
<h3><span>Chicago: 8th in the U.S. for Net Tech Employment</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-2-chicago.jpg?width=600&amp;name=img-2-chicago.jpg" alt="img-2-chicago" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-2-chicago.jpg?width=300&amp;name=img-2-chicago.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-2-chicago.jpg?width=600&amp;name=img-2-chicago.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-2-chicago.jpg?width=900&amp;name=img-2-chicago.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-2-chicago.jpg?width=1200&amp;name=img-2-chicago.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-2-chicago.jpg?width=1500&amp;name=img-2-chicago.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-2-chicago.jpg?width=1800&amp;name=img-2-chicago.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p><a href="https://www.purpose.jobs/chicago" rel="noopener" target="_blank">Chicago</a> is the No. 1 city in the Midwest for growth in the tech sector, and it ranks eighth in the country for net tech employment. Currently, there are 344,146 people in Chicago working in tech jobs. The city saw nearly 18 percent growth in its net tech employment from 2010 to 2018, and from just 2017 to 2018, job posting in the tech sector increased by a whopping 73 percent.</p></div>
<h3><span>Detroit: 11th in the U.S. for Net Tech Employment</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-3-detroit.jpg?width=600&amp;name=img-3-detroit.jpg" alt="img-3-detroit" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-3-detroit.jpg?width=300&amp;name=img-3-detroit.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-3-detroit.jpg?width=600&amp;name=img-3-detroit.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-3-detroit.jpg?width=900&amp;name=img-3-detroit.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-3-detroit.jpg?width=1200&amp;name=img-3-detroit.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-3-detroit.jpg?width=1500&amp;name=img-3-detroit.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-3-detroit.jpg?width=1800&amp;name=img-3-detroit.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<p>Coming in just behind Chicago is <a href="https://www.purpose.jobs/detroit" rel="noopener" target="_blank">Detroit</a>, which ranks 11th in the U.S. for its net tech employment. 241,135 people work in the tech sector in Detroit, where net tech employment increased by 37.2 percent from 2010 to 2018. From 2017 to 2018, job postings in tech rose 41 percent, making Detroit a fantastic spot to look for a startup job.</p>
<p><em>Looking to get connected with top startups? Join the purpose.jobs talent community to start applying for Midwest startup jobs.</em> <!--HubSpot Call-to-Action Code --><span id="hs-cta-wrapper-6fe04a32-7b40-49d0-873c-8f8ae79aa1dd"><span id="hs-cta-6fe04a32-7b40-49d0-873c-8f8ae79aa1dd"><!--[if lte IE 8]><div id="hs-cta-ie-element"></div><![endif]--><a href="https://cta-redirect.hubspot.com/cta/redirect/2873777/6fe04a32-7b40-49d0-873c-8f8ae79aa1dd" target="_blank"><img id="hs-cta-img-6fe04a32-7b40-49d0-873c-8f8ae79aa1dd" src="https://no-cache.hubspot.com/cta/default/2873777/6fe04a32-7b40-49d0-873c-8f8ae79aa1dd.png" alt="Create a free profile."></a></span></span><!-- end HubSpot Call-to-Action Code --></p>

<h3><span>Minneapolis: Nearly 200,000 Tech Jobs and Growing</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-4-minn.jpg?width=600&amp;name=img-4-minn.jpg" alt="img-4-minn" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-4-minn.jpg?width=300&amp;name=img-4-minn.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-4-minn.jpg?width=600&amp;name=img-4-minn.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-4-minn.jpg?width=900&amp;name=img-4-minn.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-4-minn.jpg?width=1200&amp;name=img-4-minn.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-4-minn.jpg?width=1500&amp;name=img-4-minn.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-4-minn.jpg?width=1800&amp;name=img-4-minn.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p>Third in the Midwest for growth is Minneapolis, a thriving city many overlook, despite its tech workforce of 196,151 and growing. Minneapolis is ranked 14th in the U.S. overall for net tech employment, which increased 17 percent in the city from 2010 to 2018. What‚Äôs even more impressive is that job postings in the tech sector increased 76 percent in Minneapolis from 2017 to 2018.</p></div>
<h3><span>Kansas City: Where New Tech Jobs Have Almost Doubled in Recent Years<br></span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-5-kc.jpg?width=600&amp;name=img-5-kc.jpg" alt="img-5-kc" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-5-kc.jpg?width=300&amp;name=img-5-kc.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-5-kc.jpg?width=600&amp;name=img-5-kc.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-5-kc.jpg?width=900&amp;name=img-5-kc.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-5-kc.jpg?width=1200&amp;name=img-5-kc.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-5-kc.jpg?width=1500&amp;name=img-5-kc.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-5-kc.jpg?width=1800&amp;name=img-5-kc.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p>Kansas City has more to boast about than its Super Bowl win. The city is home to 100,782 people who work in the tech sector, making it 24th in the U.S. for net tech employment. Kansas City also saw 17.3 percent growth in its net tech employment from 2010 to 2018, and 82 percent growth in its tech job posting just from 2017 to 2018, indicating that its rate of growth is ramping up even faster in recent years than over the last decade.</p></div>
<h3><span>Cincinnati: Nearly 100,000 Tech Workers and Steady Growth of New Jobs<br></span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-6-cinci.jpg?width=600&amp;name=img-6-cinci.jpg" alt="img-6-cinci" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-6-cinci.jpg?width=300&amp;name=img-6-cinci.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-6-cinci.jpg?width=600&amp;name=img-6-cinci.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-6-cinci.jpg?width=900&amp;name=img-6-cinci.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-6-cinci.jpg?width=1200&amp;name=img-6-cinci.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-6-cinci.jpg?width=1500&amp;name=img-6-cinci.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-6-cinci.jpg?width=1800&amp;name=img-6-cinci.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p>Fifth on the list of growing Midwest cities in <a href="https://www.purpose.jobs/cincinnati" rel="noopener" target="_blank">Cincinnati</a>, where 82,088 workers already have tech jobs. From 2010 to 2018, the city saw a 23.9 percent increase in its net tech employment, and job postings in the tech sector jumped up 41 percent just from 2017 to 2018. That lands Cincinnati 28th in the U.S. for net tech employment, and there‚Äôs plenty of opportunity here as the city continues to grow.</p></div>
<h3><span>Cleveland: Job Growth that Nearly Doubled in Just One Year</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-7-cleveland.jpg?width=600&amp;name=img-7-cleveland.jpg" alt="img-7-cleveland" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-7-cleveland.jpg?width=300&amp;name=img-7-cleveland.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-7-cleveland.jpg?width=600&amp;name=img-7-cleveland.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-7-cleveland.jpg?width=900&amp;name=img-7-cleveland.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-7-cleveland.jpg?width=1200&amp;name=img-7-cleveland.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-7-cleveland.jpg?width=1500&amp;name=img-7-cleveland.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-7-cleveland.jpg?width=1800&amp;name=img-7-cleveland.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p>Sixth in the Midwest is <a href="https://www.purpose.jobs/cleveland" rel="noopener" target="_blank">Cleveland</a>, an oft-overlooked Ohio metropolis that has plenty to offer tech workers ‚Äî&nbsp;just ask the 76,698 workers who have tech jobs there. Cleveland saw 16.3 percent growth in its net tech employment from 2010 to 2018, which led to its 93 percent increase in tech job postings from 2017 to 2018. Of all the cities in the U.S., Cleveland ranks 29th for net tech employment, making it a place well worth considering whether you‚Äôre looking for a tech job or hoping to found a startup in a new home.</p></div>
<h3><span>Indianapolis: More than 100 Percent Job Growth in One Year</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-8-indi.jpg?width=600&amp;name=img-8-indi.jpg" alt="img-8-indi" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-8-indi.jpg?width=300&amp;name=img-8-indi.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-8-indi.jpg?width=600&amp;name=img-8-indi.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-8-indi.jpg?width=900&amp;name=img-8-indi.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-8-indi.jpg?width=1200&amp;name=img-8-indi.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-8-indi.jpg?width=1500&amp;name=img-8-indi.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-8-indi.jpg?width=1800&amp;name=img-8-indi.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p><a href="https://www.purpose.jobs/indianapolis" rel="noopener" target="_blank">Indianapolis</a> is the first of our three Midwestern cities that increased their startup job growth more than 100 percent ‚Äî&nbsp;the city saw a 121 percent increase in new tech job postings from 2017 to 2018, after 24.2 percent growth in net tech employment from 2010 to 2018. As of now, there are 74,615 people employed in the tech sector in Indy, and that number is only going up.</p></div>
<h3><span>Milwaukee: Tied for Highest Increase in New Tech Jobs in the Midwest</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-9-milwak.jpg?width=600&amp;name=img-9-milwak.jpg" alt="img-9-milwak" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-9-milwak.jpg?width=300&amp;name=img-9-milwak.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-9-milwak.jpg?width=600&amp;name=img-9-milwak.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-9-milwak.jpg?width=900&amp;name=img-9-milwak.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-9-milwak.jpg?width=1200&amp;name=img-9-milwak.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-9-milwak.jpg?width=1500&amp;name=img-9-milwak.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-9-milwak.jpg?width=1800&amp;name=img-9-milwak.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p>With an astonishing 137 percent increase in new tech job postings from 2017 to 2018, Milwaukee is one of the most promising spots in the Midwest for anyone looking for a tech position. The city currently boasts 71,755 tech workers after a 9.2 percent increase in net tech employment from 2010 to 2018. Sure, that‚Äôs slower growth over the course of the decade than some of the cities on our list, but the rate of new job postings in Milwaukee show this city is just getting started.</p></div>
<h3><span>Omaha: An Unlikely Hotspot for New Tech Job Postings</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-10-omaha.jpg?width=600&amp;name=img-10-omaha.jpg" alt="img-10-omaha" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-10-omaha.jpg?width=300&amp;name=img-10-omaha.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-10-omaha.jpg?width=600&amp;name=img-10-omaha.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-10-omaha.jpg?width=900&amp;name=img-10-omaha.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-10-omaha.jpg?width=1200&amp;name=img-10-omaha.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-10-omaha.jpg?width=1500&amp;name=img-10-omaha.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-10-omaha.jpg?width=1800&amp;name=img-10-omaha.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p>Like Milwaukee, Omaha also had a stunning 137 percent increase in new tech job postings from 2017 to 2018. While growth in net tech jobs in the city was only 10.7 percent from 2010 to 2018, all that seems to indicate is that tech workers are <em>just</em> starting to realize what Omaha has to offer. 37,508 tech workers live in the city now, but with such a marked increase in new tech jobs, we can only see that number going up.</p></div>
<h3><span>Des Moines: Second-Highest 10-Year Growth in the Midwest</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-11-des-moines.jpg?width=600&amp;name=img-11-des-moines.jpg" alt="img-11-des-moines" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-11-des-moines.jpg?width=300&amp;name=img-11-des-moines.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-11-des-moines.jpg?width=600&amp;name=img-11-des-moines.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-11-des-moines.jpg?width=900&amp;name=img-11-des-moines.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-11-des-moines.jpg?width=1200&amp;name=img-11-des-moines.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-11-des-moines.jpg?width=1500&amp;name=img-11-des-moines.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-11-des-moines.jpg?width=1800&amp;name=img-11-des-moines.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p>While Des Moines is 10th in the top 10 Midwestern cities, it‚Äôs had the second-highest rate of growth in net tech employment from 2010 to 2018: 26.9 percent, behind only Detroit. Des Moines is currently home to 28,693 tech workers, and from 2017 to 2018, saw a 47 percent increase in new tech job postings.&nbsp;</p></div>
<h2><span>Midwestern Companies Are Hiring Tens of Thousands of Tech Workers Right Now</span></h2>
<div><p>In Chicago, Detroit, and Indianapolis alone, there are nearly 31,000 open tech positions at any given time. The Midwest is next for tech workers. Find out more with a free download of the Midwest Salary and Cost of Living Handbook.</p></div>
<p><!--HubSpot Call-to-Action Code --><span id="hs-cta-wrapper-d825b188-3a7f-4d00-ac4a-1ef02e65ec84"><span id="hs-cta-d825b188-3a7f-4d00-ac4a-1ef02e65ec84"><!--[if lte IE 8]><div id="hs-cta-ie-element"></div><![endif]--><a href="https://cta-redirect.hubspot.com/cta/redirect/2873777/d825b188-3a7f-4d00-ac4a-1ef02e65ec84" target="_blank"><img id="hs-cta-img-d825b188-3a7f-4d00-ac4a-1ef02e65ec84" height="709" width="1600" src="https://no-cache.hubspot.com/cta/default/2873777/d825b188-3a7f-4d00-ac4a-1ef02e65ec84.png" alt="New call-to-action"></a></span></span><!-- end HubSpot Call-to-Action Code --></p>

<p><strong><br></strong><strong><img src="https://www.purpose.jobs/hs-fs/hubfs/Christina%20headshot.png?width=120&amp;name=Christina%20headshot.png" alt="Christina headshot" width="120" srcset="https://www.purpose.jobs/hs-fs/hubfs/Christina%20headshot.png?width=60&amp;name=Christina%20headshot.png 60w, https://www.purpose.jobs/hs-fs/hubfs/Christina%20headshot.png?width=120&amp;name=Christina%20headshot.png 120w, https://www.purpose.jobs/hs-fs/hubfs/Christina%20headshot.png?width=180&amp;name=Christina%20headshot.png 180w, https://www.purpose.jobs/hs-fs/hubfs/Christina%20headshot.png?width=240&amp;name=Christina%20headshot.png 240w, https://www.purpose.jobs/hs-fs/hubfs/Christina%20headshot.png?width=300&amp;name=Christina%20headshot.png 300w, https://www.purpose.jobs/hs-fs/hubfs/Christina%20headshot.png?width=360&amp;name=Christina%20headshot.png 360w" sizes="(max-width: 120px) 100vw, 120px"></strong><em><strong>Christina Marfice</strong> is a born and raised Midwesterner who traveled the globe and came right back. She has been a journalist and freelance writer for almost ten years. In addition to her other projects, she explores startup strategies, business operations, and eCommerce topics for&nbsp;<a target="_blank" data-stringify-link="https://www.yesoptimist.com/" delay="150" data-sk="tooltip_parent" href="https://www.yesoptimist.com/" rel="noopener">Optimist</a>. She currently resides in Chicago with her two cats, Dumpling and Doughnut.</em></p></span>
</p>

</div></div>]]>
            </description>
            <link>https://www.purpose.jobs/blog/hiring-tech-jobs-has-increased-in-midwestern-cities</link>
            <guid isPermaLink="false">hacker-news-small-sites-24698449</guid>
            <pubDate>Tue, 06 Oct 2020 15:09:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Deploy to K8s without YAML using ShuttleOps]]>
            </title>
            <description>
<![CDATA[
Score 60 | Comments 50 (<a href="https://news.ycombinator.com/item?id=24698326">thread link</a>) | @gscho
<br/>
October 6, 2020 | https://go.shuttleops.io/no-code-docker-kubernetes | <a href="https://web.archive.org/web/*/https://go.shuttleops.io/no-code-docker-kubernetes">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
<div data-widget-type="custom_widget" data-x="0" data-w="12">
<p><span id="hs_cos_wrapper_module_159802898093264_" data-hs-cos-general-type="widget" data-hs-cos-type="rich_text"><h2><span>See How No-Code </span><span id="5f0fb456-fdf5-4e5b-993a-ec7087c62a86" data-renderer-mark="true" data-mark-type="annotation" data-mark-annotation-type="inlineComment" data-id="5f0fb456-fdf5-4e5b-993a-ec7087c62a86">Continuous Delivery<br></span><span>&nbsp;Can Accelerate Your Business</span></h2>
<h5>The same powerful drag-and-drop interface, true multicloud integration and security and compliance you‚Äôve come to expect from ShuttleOps, now with Docker and Kubernetes support. See how easy it is to onboard your application, your team, and scale your delivery. Get started today for free. No credit card required!&nbsp;</h5>

<p><span><!--HubSpot Call-to-Action Code --><span id="hs-cta-wrapper-fcb53eb1-7328-44f8-b128-f953ffc8bab9"><span id="hs-cta-fcb53eb1-7328-44f8-b128-f953ffc8bab9"><!--[if lte IE 8]><div id="hs-cta-ie-element"></div><![endif]--><a href="https://cta-redirect.hubspot.com/cta/redirect/5669359/fcb53eb1-7328-44f8-b128-f953ffc8bab9"><img id="hs-cta-img-fcb53eb1-7328-44f8-b128-f953ffc8bab9" src="https://no-cache.hubspot.com/cta/default/5669359/fcb53eb1-7328-44f8-b128-f953ffc8bab9.png" alt="Get Started"></a></span></span><!-- end HubSpot Call-to-Action Code --></span></p></span></p>

</div><!--end widget-span -->
</div><!--end row-->
</div></div>]]>
            </description>
            <link>https://go.shuttleops.io/no-code-docker-kubernetes</link>
            <guid isPermaLink="false">hacker-news-small-sites-24698326</guid>
            <pubDate>Tue, 06 Oct 2020 14:58:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[4x4 Macro Pad Kit]]>
            </title>
            <description>
<![CDATA[
Score 73 | Comments 37 (<a href="https://news.ycombinator.com/item?id=24697624">thread link</a>) | @0xC45
<br/>
October 6, 2020 | https://0xc45.com/blog/4x4-macro-pad/ | <a href="https://web.archive.org/web/*/https://0xc45.com/blog/4x4-macro-pad/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    <header>
        
    </header>
    <p>10/6/2020</p>
    <h2>Contents</h2>
    <ul>
        
        <li>
            <a href="https://0xc45.com/blog/4x4-macro-pad/#overview">Overview</a>
            
        </li>
        
        <li>
            <a href="https://0xc45.com/blog/4x4-macro-pad/#build-process">Build Process</a>
            
        </li>
        
        <li>
            <a href="https://0xc45.com/blog/4x4-macro-pad/#flash-firmware">Flash Firmware</a>
            
        </li>
        
        <li>
            <a href="https://0xc45.com/blog/4x4-macro-pad/#design-keycaps">Design Keycaps</a>
            
        </li>
        
        <li>
            <a href="https://0xc45.com/blog/4x4-macro-pad/#conclusion">Conclusion</a>
            
        </li>
        
        <li>
            <a href="https://0xc45.com/blog/4x4-macro-pad/#links">Links</a>
            
        </li>
        
    </ul>
    <section>
<h2 id="overview">Overview</h2>
<p>Last weekend, I built a 4x4 keyboard kit. By this point, many people are familiar with the growing (and outspoken) mechanical keyboard hobbyist community. However, this kit is a bit unique. It's not a full keyboard. Instead, it's a 4x4 "macro pad" intended for sending keyboard shortcut sequences such as muting my microphone, muting my audio, volume up, volume down, etc. Additionally, with some extra software such as AutoHotKey, vastly complex programs could be triggered with the press of a button.</p>
<h2 id="build-process">Build Process</h2>
<p>Overall, building the macro pad was a simple and straightforward process. The <a href="https://www.1upkeyboards.com/instructions-downloads/sweet-16-instructions/">kit's build guide</a> provides a nice set of instructions with pictures to explain things. However, unlike many (some?) keyboard kits, the Sweet 16 kit requires soldering a few smaller components, such as the diodes and microcontroller headers. Additionally, the kit requires soldering one "surface-mount" component, the reset switch.</p>
<p>The parts:
<img src="https://0xc45.com/blog/4x4-macro-pad/sweet16-parts.jpg" alt="Sweet16 Parts"></p>
<p>Completed build:
<img src="https://0xc45.com/blog/4x4-macro-pad/sweet16-solder-joints.jpg" alt="Sweet16 Solder Joints"></p>
<h2 id="flash-firmware">Flash Firmware</h2>
<p>To program my custom keymap (including multiple keypress macros), I used <a href="https://qmk.fm/">QMK firmware</a>, the most popular keyboard firmware project.</p>
<p>Using QMK, it's possible to create custom keycodes that, when pressed, trigger a sequence of inputs. So, by pressing one button on the macro pad (or keyboard), the firmware will submit an entire sequence of keycode presses.</p>
<p>To do this, I defined my custom keycodes in an enum:</p>
<pre><code><span>enum </span><span>macro_keycodes {
  MICMUTE = SAFE_RANGE,
  MACRO1,
  MACRO2,
  MACRO3,
  MACRO4,
  MACRO5,
  MACRO6,
  MACRO7,
  MACRO8
};
</span></code></pre>
<p>Next, I defined a "keymap" array. Each position in the array corresponds to a single button on the 4x4 macro pad:</p>
<pre><code><span>const </span><span>uint16_t PROGMEM keymaps[][MATRIX_ROWS][MATRIX_COLS] = {
  [</span><span>0</span><span>] = </span><span>LAYOUT_ortho_4x4</span><span>( </span><span>/* Base */</span><span>
    MICMUTE, KC_MUTE, KC_VOLD, KC_VOLU,
    XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,
    MACRO1,  MACRO2,  MACRO3,  MACRO4,
    MACRO5,  MACRO6,  MACRO7,  MACRO8
  ),
};
</span></code></pre>
<p>Lastly, I implemented the <code>process_record_user</code> function to define what should happen when each custom keycode is pressed:</p>
<pre><code><span>bool </span><span>process_record_user</span><span>(uint16_t </span><span>keycode</span><span>, keyrecord_t *</span><span>record</span><span>) {
  </span><span>switch </span><span>(keycode) {
  </span><span>case</span><span> MICMUTE:
    </span><span>if </span><span>(record-&gt;event.</span><span>pressed</span><span>) {
      </span><span>SEND_STRING</span><span>(</span><span>SS_LCTL</span><span>(</span><span>SS_LALT</span><span>(</span><span>SS_LSFT</span><span>(</span><span>SS_TAP</span><span>(X_F10)))));
    }
    </span><span>break</span><span>;
  </span><span>case</span><span> MACRO1:
    </span><span>if </span><span>(record-&gt;event.</span><span>pressed</span><span>) {
      </span><span>SEND_STRING</span><span>(</span><span>SS_LCTL</span><span>(</span><span>SS_LALT</span><span>(</span><span>SS_LSFT</span><span>(</span><span>SS_TAP</span><span>(X_F1)))));
    }
    </span><span>break</span><span>;
  </span><span>/*
   * ... etc
   */
  </span><span>}
  </span><span>return </span><span>true
</span><span>}
</span></code></pre>
<p>As you can see, I have configured the <code>MICMUTE</code> button to send the entire sequence <code>CTRL+ALT+SHIFT+F10</code>. However, in practice, any arbitrary sequence could be sent for any button. And, that's only beginning to scratch the surface of the capabilities of the QMK firmware.</p>
<h2 id="design-keycaps">Design Keycaps</h2>
<p>For this "DIY" kit, it felt important to design my own icons. I'm no graphic designer, but it was kinda fun. To do this, I used "re-legendable" keycaps that snap together with a clear top. Then, I printed the icons on plain white paper, cut them out, and sandwiched each icon in the keycaps. Here's a photo of my efforts:</p>
<p><img src="https://0xc45.com/blog/4x4-macro-pad/sweet16-completed.jpg" alt="Sweet16 Completed"></p>
<h2 id="conclusion">Conclusion</h2>
<p>This was a pretty quick project, but I felt like it deserved a writeup nevertheless. As a relative beginner at soldering, this kit was a fantastic way to increase my skills and ability beyond the "absolute beginner" level required for most keyboard kits. Furthermore, the final product is quite useful and extensible. Beyond the specific purpose as a simple macro pad keyboard, this hardware is essentially a microcontroller connected to a set of buttons. There are numerous possible applications. It's ripe for hacking. This device could become a MIDI controller, home automation remote, or anything else my imagination might dream up. Until next time.</p>
<h2 id="links">Links</h2>
<ol>
<li>Sweeet 16 Macro Pad Kit: <a href="https://www.1upkeyboards.com/shop/keyboard-kits/macro-pads/sweet-16-macro-pad-black/">https://www.1upkeyboards.com/shop/keyboard-kits/macro-pads/sweet-16-macro-pad-black/</a></li>
<li>QMK Firmware: <a href="https://qmk.fm/">https://qmk.fm/</a></li>
<li>My Sweet 16 Keymap: <a href="https://github.com/0xC45/qmk-firmware/blob/master/keyboards/1upkeyboards/sweet16/keymaps/0xC45/keymap.c">https://github.com/0xC45/qmk-firmware/blob/master/keyboards/1upkeyboards/sweet16/keymaps/0xC45/keymap.c</a></li>
</ol>

    </section>
</article></div>]]>
            </description>
            <link>https://0xc45.com/blog/4x4-macro-pad/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24697624</guid>
            <pubDate>Tue, 06 Oct 2020 13:49:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On a Typical Day: Daniel Ek]]>
            </title>
            <description>
<![CDATA[
Score 40 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24697009">thread link</a>) | @tosh
<br/>
October 6, 2020 | https://www.theobservereffect.org/daniel.html | <a href="https://web.archive.org/web/*/https://www.theobservereffect.org/daniel.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><em>
									Welcome to the second interview on 'The Observer Effect'. We are lucky to have one
									of the most influential founders/CEOs in technology and media - Daniel Ek, Founder
									and CEO of Spotify. This interview was published on 4th October, 2020.

							</em></p><p><em>Daniel does things very differently from other business leaders and was generous to go
								deep with us on his leadership style, time management, decision making, Spotify's impact
								on the world and much, much more. Enjoy!
								</em>
							</p><p><b><a href="https://sriramk.com/">Sriram Krishnan</a></b><br>
								<em><strong>Let‚Äôs start with the basics. Walk me through a typical day in the life of
										Daniel Ek.</strong></em>
							</p><p><strong>Daniel Ek</strong><br>
								So, this will sound incredibly lazy compared to some leaders. I wake up at around 6:30
								in the morning and spend some time with my kids and wife. At 7:30, I go work out. At
								8:30, I go for a walk ‚Äì even in the winter. I‚Äôve found this is often where I do my best
								thinking. At 9:30, I read for thirty minutes to an hour. Sometimes I read the news, but
								you‚Äôll also find an ever-rotating stack of books in my office, next to my bed, on tables
								around the house. Books on history, leadership, biographies. It‚Äôs a pretty eclectic mix
								‚Äì much like my taste in music. Finally, my ‚Äúwork‚Äù day really starts at 10:30.
							</p><p>
								Many people make big decisions early on in the day, I make them later in the day--at
								least later in the day here in Europe. Ironically, it's not actually because I'm more
								productive then, rather because we have so many of our staff in the US, and as a result,
								I've kind of primed myself to work that way.
							</p><p>
								So the earlier part of my day is focused on coaching, one-on-ones, and planning. Then, I
								typically tackle one topic a day which takes a lot of my time. That's my big thing for
								the day. Before we go into a live team discussion on that particular topic, I invest
								time to prepare beforehand ‚Äì reading and talking to members of the team who are either
								part of the decision-making process or who have insights and context. I sometimes even
								get external perspectives.
							</p><p>
								I also think about what my role is at that meeting. Sometimes I'm the approver. Other
								times, I'm supposed to come with a thoughtful perspective on whether an initiative makes
								sense or not.
							</p><p>
								I‚Äôve found that creating this clarity of role for myself is critical. It‚Äôs something I
								challenge my direct reports to think about as they engage with their own teams. I remind
								them that all meetings are not the same. Even when we are meeting to discuss really,
								really complicated topics I always ask myself: ‚ÄúWhat am I going to do in this meeting?
								What does my involvement really need to be?‚Äù
							</p><p>
								The truth is: it's entirely contextual. I find it crucial to be upfront about everyone‚Äôs
								role in different meetings, I think this is super, super important. Often that's my
								number one thing: to make sure I know what role I'm playing.</p><p>
								<b><i>Wow, okay, there are multiple things in there ranging from how you choose to spend
										your time to how you handle meetings. To work backwards, what makes a good
										meeting in your mind?
									</i></b></p><p>A great meeting has three key elements: the desired outcome of the meeting is clear ahead
								of time; the various options are clear, ideally ahead of time; and the roles of the
								participants are clear at the time.
							</p><p>
								I often find that meetings lack one of those elements. Sometimes they lack all those,
								which is when you have to say, ‚ÄúThis is a horrible meeting, let's end it and regroup so
								it can be more effective for everyone.‚Äù
							</p><p>
								To clarify outcomes, options, and roles ahead of time, we sometimes rely upon a preread.
								Prereads are a great way to share context so that attendees can quickly get into the
								meat of the issue and not waste time getting everyone up to speed. What I find is when
								you use a tool like a Google Doc, you can take in a great deal of information by reading
								comments, assessing options, and understanding how opinions have evolved over time. With
								this uniform background and context, attendees can focus on discussing the matter at
								hand versus getting on the same page. When the latter happens, the meeting becomes an
								incredible waste of time.
							</p><p>
								I think that's the single largest source of optimization for a company: the makeup of
								their meetings. To be clear, it's not about fewer meetings because meetings serve a
								purpose. Rather, it‚Äôs key to improve the meetings, themselves. A lot of my efforts focus
								on teaching people this framework. Ironically, I find that most people are just
								challenged by that stuff.
							</p><p>
								Candidly, that‚Äôs my role as leader: to coach others on how best to make use of their
								limited time. Not only is time the most precious resource the company has, it‚Äôs also the
								most precious resource they have! It‚Äôs crucial that they approach the use of their time
								with a holistic perspective. By way of example, I had a recent call with one of my
								directors who had not taken a vacation in six months. Our conversation delved into why
								this person thought that they could not be away for two weeks, and me arguing for why
								the person had to take two weeks to recharge!
							</p><p>
								There is never enough time ‚Äì for work, for family and friends ‚Äì and it takes work to
								make the best use of it. It's all about fostering a holistic perspective in life.

							</p><p>
								<b><i>
										That‚Äôs fascinating. Let‚Äôs turn to your team.

										Your direct reports are highly accomplished people; what are the common mistakes
										you see executives at that level make when it comes to personal time management?
									</i>
							</b></p><div>
							<div><p>
								I don‚Äôt think most executives dedicate enough time to thinking. They spend too much time
								in meetings. By the way, I will say as a caveat, I do know people who are incredibly
								organized and succeed with a lot of ‚Äúdo time.‚Äù Shishir Mehrotra [Co-founder and CEO of
								Coda] is a great example. If you've seen the docs on how he organizes his time...
								</p><p>

								<b><i>Oh yeah, he has a lot of very well-organized docs! [laughs]</i></b></p></div><p>
								He is a source of inspiration. For a while, I tried to mimic his style because I was so
								impressed with his thinking behind it. But in the end, it just wasn‚Äôt for me. It
								actually drove me nuts. <i>[Sriram laughs]</i>
								But I respect him. I would say he's a highly effective executive. His system works for
								him. It's not one size fits all. Some of my direct reports thrive on lots of meetings.
								But, in general, I would say the largest mistake is that they conflate meetings with
								productivity. Often fewer meetings and better decisions drive the business forward.

							</p>
							<h2 id="opencalendar"><b>On Creating an Open Calendar</b></h2>
							<p>


								<b><i>This dovetails nicely with something that fascinates many of your colleagues: how
										do you have so much open time on your calendar?

										This drastically differs from your typical ‚Äúsuccessful CEO‚Äù who is booked from
										8:30am to 6pm. Walk us through your calendar and how you manage to create this
										open space.
									</i></b>

							</p>

							<p>

								My friends know me well! I do keep a lot of open time. I understand this comes from a
								place of privilege and I‚Äôm very lucky to have this flexibility.
							</p>
							<p>

								I feel like synchronous time is very costly; asynchronous time is better. I know there
								are some leaders who prefer to have all executive decisions travel through them. But
								then, you have to wait until the leader has availability to review things. Sometimes you
								run into delays in that process.
							</p>
							<p>
								I typically don't have more than really three or four meetings per day. There are
								exceptions; when I travel, I book in a lot more and I don't keep to my normal schedule.
								That said, most of the time it's three or four meetings a day.

							</p>
							<p>
								My way is to plan long term and do so ahead of time so that people better understand the
								direction in which they're going. You have to be incredibly crisp and clear when doing
								that. For instance, right now we're finalizing our five year plans and long range
								planning. These are actual, real targets fueled by real insights. They are made up of
								lots of super-detailed quarterly and annual goals. I don‚Äôt spend much time on the
								quarterly goals and instead focus on our so-called ‚Äúbig rocks.‚Äù
							</p>


							<h2 id="bets"><b>On Company Bets</b></h2>
							<p>
								At Spotify, we have something called ‚ÄúCompany Bets.‚Äù These are large-scale initiatives
								that we believe will have a significant impact on the business within a relatively short
								period of time. I find that these bets are a much better use of my time. Our Company
								Bets typically update every six months, so I'm not needed that much in between. This
								way, I can constantly be thinking: ‚ÄúWhere are we headed in the next six months?‚Äù Right
								now, I am thinking more about H2 2021. From a timeline perspective, that's the earliest
								place where I focus most of my time.
							</p>
							<p>
								It‚Äôs also my role to think far beyond that. For instance, I‚Äôm immersing myself in our
								2025 plans. I trust my team to manage the day-to-day, shorter-term initiatives and
								iterate as needed based on data and insights. They‚Äôre the best at that and I appreciate
								that this then frees me up to think about the long term.
							</p>

							<h2 id="decisionmaking"><b>On Delegated Decision Making<br></b></h2>
							<p>
								<b><i>
										Your system reminds me of Jack [Dorsey] at Twitter a ‚Ä¶</i></b></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.theobservereffect.org/daniel.html">https://www.theobservereffect.org/daniel.html</a></em></p>]]>
            </description>
            <link>https://www.theobservereffect.org/daniel.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24697009</guid>
            <pubDate>Tue, 06 Oct 2020 12:32:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Cyclone Scheme]]>
            </title>
            <description>
<![CDATA[
Score 87 | Comments 28 (<a href="https://news.ycombinator.com/item?id=24696939">thread link</a>) | @andrenth
<br/>
October 6, 2020 | https://justinethier.github.io/cyclone/ | <a href="https://web.archive.org/web/*/https://justinethier.github.io/cyclone/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main_content_wrap">
      <section id="main_content">
      <p>Cyclone Scheme is a brand-new compiler that allows real-world application development using the R<sup>7</sup>RS Scheme Language standard. We provide modern features and a stable system capable of generating fast native binaries.</p>

<p><a href="https://github.com/justinethier/cyclone/raw/master/docs/research-papers/CheneyMTA.pdf">Cheney on the MTA</a> is used by Cyclone‚Äôs runtime to implement full tail recursion, continuations, and generational garbage collection. In addition, the Cheney on the MTA concept has been extended to allow execution of multiple native threads. An on-the-fly garbage collector is used to manage the second-generation heap and perform major collections without ‚Äústopping the world‚Äù.</p>



<ul>
  <li>Support for the majority of the Scheme language as specified by the latest <a href="https://justinethier.github.io/cyclone/docs/Scheme-Language-Compliance.html">R<sup>7</sup>RS standard</a>.</li>
  <li>New features from R<sup>7</sup>RS including libraries, exceptions, and record types.</li>
  <li>Built-in support for Unicode strings and characters.</li>
  <li>Hygienic macros based on <code>syntax-rules</code></li>
  <li>Low-level explicit renaming macros</li>
  <li>Guaranteed tail call optimizations</li>
  <li>Native multithreading support</li>
  <li>A foreign function interface that allows easy integration with C</li>
  <li>A concurrent, generational garbage collector based on Cheney on the MTA</li>
  <li>Includes an optimizing Scheme-to-C compiler,</li>
  <li>‚Ä¶ as well as an interpreter for debugging</li>
  <li>A <a href="https://github.com/cyclone-scheme/cyclone-winds">Package Manager</a> and a growing list of packages.</li>
  <li>Support for <a href="https://justinethier.github.io/cyclone/docs/API.html#srfi-libraries">many popular SRFI‚Äôs</a></li>
  <li>Online user manual and API documentation</li>
  <li>Support for Linux, Windows, FreeBSD, and Mac platforms.</li>
  <li>Known to run on x86-64, x86, and Arm (Raspberry Pi) architectures.</li>
</ul>



<p>There are several options available for installing Cyclone:</p>

<h2 id="docker">Docker</h2>
<p><img src="https://justinethier.github.io/cyclone/docs/images/docker-thumb.png" alt="Docker" title="Docker"></p>

<p>Cyclone can be run from a <a href="https://hub.docker.com/r/cyclonescm/cyclone">Docker Image</a>:</p>

<div><div><pre><code>docker run -it cyclonescm/cyclone bash
</code></pre></div></div>

<h2 id="homebrew">Homebrew</h2>
<p><img src="https://justinethier.github.io/cyclone/docs/images/homebrew-thumb.png" alt="Homebrew" title="Homebrew"></p>

<p>Mac (and Linux!) users wanting to use Homebrew can do the following.</p>

<p>Note if Homebrew is not already installed: follow the instructions at <a href="https://brew.sh/">https://brew.sh/</a> to install the homebrew package manager.</p>

<div><div><pre><code>brew tap cyclone-scheme/cyclone
brew install cyclone-scheme/cyclone/cyclone-bootstrap
</code></pre></div></div>

<h2 id="arch-linux">Arch Linux</h2>
<p><img src="https://justinethier.github.io/cyclone/docs/images/arch-linux-thumb.png" alt="Arch Linux" title="Arch Linux"></p>

<p>Arch Linux users can install using the <a href="https://aur.archlinux.org/packages/cyclone-scheme/">AUR</a>:</p>

<div><div><pre><code>git clone https://aur.archlinux.org/cyclone-scheme.git
cd cyclone-scheme
makepkg -si
</code></pre></div></div>

<h2 id="build-from-source">Build from Source</h2>
<p><img src="https://justinethier.github.io/cyclone/docs/images/build-thumb.png" alt="Build from Source" title="Build from Source"></p>

<p>To install Cyclone on your machine for the first time on Linux, Windows, FreeBSD, and for Mac users wanting to install without using Homebrew, use <a href="https://github.com/justinethier/cyclone-bootstrap"><strong>cyclone-bootstrap</strong></a> to build a set of binaries. Instructions are provided for Linux, Mac, Windows (via MSYS), and FreeBSD 12.</p>



<p>After installing you can run the <code>cyclone</code> command to compile a single Scheme file:</p>

<div><div><pre><code>$ cyclone examples/fac.scm
$ examples/fac
3628800
</code></pre></div></div>

<p>And the <code>icyc</code> command to start an interactive interpreter. Note you can use <a href="http://linux.die.net/man/1/rlwrap"><code>rlwrap</code></a> to make the interpreter more friendly, EG: <code>rlwrap icyc</code>:</p>

<div><div><pre><code>$ icyc

              :@
            @@@
          @@@@:
        `@@@@@+
       .@@@+@@@      
       @@     @@     Cyclone Scheme-&gt;C compiler
      ,@             http://justinethier.github.io/cyclone/
      '@
      .@
       @@     #@     (c) 2014-2019 Justin Ethier
       `@@@#@@@.     Version 0.11
        #@@@@@
        +@@@+
        @@#
      `@.
   
cyclone&gt; (write 'hello-world)
hello-world
</code></pre></div></div>

<p>Read the documentation below for more information on how to use Cyclone.</p>



<p><img src="https://justinethier.github.io/cyclone/docs/images/cyclone-winds-small.png" alt="Cyclone Winds" title="Cyclone Winds"></p>

<p>The <code>cyclone-winds</code> package manager provides the ability to install packaged libraries and programs for Cyclone. See the <a href="https://github.com/cyclone-scheme/cyclone-winds#cyclone-winds">cyclone-winds</a> site for more information.</p>



<ul>
  <li>
    <p>The <a href="https://justinethier.github.io/cyclone/docs/User-Manual">User Manual</a> covers in detail how to use Cyclone and provides information on the Scheme language features implemented by Cyclone.</p>
  </li>
  <li>
    <p>An <a href="https://justinethier.github.io/cyclone/docs/API">API Reference</a> is available for all libraries provided by Cyclone, including a complete alphabetical listing.</p>
  </li>
  <li>
    <p>If you need a resource to start learning the Scheme language you may want to try a classic textbook such as <a href="https://mitpress.mit.edu/sicp/full-text/book/book.html">Structure and Interpretation of Computer Programs</a>.</p>
  </li>
  <li>
    <p>Finally, this <a href="http://ecraven.github.io/r7rs-benchmarks/benchmark.html">benchmarks</a> page by <a href="https://github.com/ecraven">ecraven</a> compares the performance of Cyclone with other Schemes.</p>
  </li>
</ul>



<p>Cyclone provides several example programs, including:</p>

<ul>
  <li>
    <p><a href="https://github.com/justinethier/cyclone/blob/master/examples/tail-call-optimization.scm">Tail Call Optimization</a> - A simple example of Scheme tail call optimization; this program runs forever, calling into two mutually recursive functions.</p>
  </li>
  <li>
    <p><a href="https://github.com/justinethier/cyclone/blob/master/examples/threading">Threading</a> - Various examples of multi-threaded programs.</p>
  </li>
  <li>
    <p><a href="https://github.com/justinethier/cyclone/blob/master/examples/game-of-life">Game of Life</a> - The <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Conway‚Äôs game of life</a> example program and libraries from R<sup>7</sup>RS.</p>
  </li>
  <li>
    <p><a href="https://github.com/justinethier/cyclone/blob/master/examples/game-of-life-png">Game of Life PNG Image Generator</a> - A modified version of game of life that uses libpng to create an image of each iteration instead of writing it to console. This example also demonstrates basic usage of the C Foreign Function Interface (FFI).</p>
  </li>
  <li>
    <p>Finally, the largest program is the compiler itself. Most of the code is contained in a series of libraries which are used by <a href="https://github.com/justinethier/cyclone/blob/master/cyclone.scm"><code>cyclone.scm</code></a> and <a href="https://github.com/justinethier/cyclone/blob/master/icyc.scm"><code>icyc.scm</code></a> to create executables for Cyclone‚Äôs compiler and interpreter.</p>
  </li>
</ul>



<ul>
  <li>
    <p><a href="https://justinethier.github.io/cyclone/docs/Writing-the-Cyclone-Scheme-Compiler-Revised-2017">Writing the Cyclone Scheme Compiler</a> provides high-level details on how the compiler was written and how it works.</p>
  </li>
  <li>
    <p>There is a <a href="https://justinethier.github.io/cyclone/docs/Development">Development Guide</a> with instructions for common tasks when hacking on the compiler itself.</p>
  </li>
  <li>
    <p>Cyclone‚Äôs <a href="https://justinethier.github.io/cyclone/docs/Garbage-Collector">Garbage Collector</a> is documented at a high-level. This document includes details on extending Cheney on the MTA to support multiple stacks and fusing that approach with a tri-color marking collector.</p>
  </li>
  <li>
    <p>The garbage collector was subsequently enhanced to support <a href="https://justinethier.github.io/cyclone/docs/Garbage-Collection-Using-Lazy-Sweeping">Lazy Sweeping</a> which improves performance for a wide range of applications.</p>
  </li>
</ul>



<p>Copyright (C) 2014 <a href="http://github.com/justinethier">Justin Ethier</a>.</p>

<p>Cyclone is available under the <a href="http://www.opensource.org/licenses/mit-license.php">MIT license</a>.</p>

            <h2>Recent News</h2>
      
        <h4>
          <a href="https://justinethier.github.io/cyclone//2020/09/17/Released-Cyclone-Scheme-0.21.html">Released Cyclone Scheme 0.21</a>
        </h4>
        <span>September 17, 2020</span>
        <br>
        Various bug fixes and continuous integration support for FreeBSD.
      
        <h4>
          <a href="https://justinethier.github.io/cyclone//2020/08/14/Released-Cyclone-Scheme-0.20.html">Released Cyclone Scheme 0.20</a>
        </h4>
        <span>August 14, 2020</span>
        <br>
        We now have official support for calling Scheme from C.
      
        <h4>
          <a href="https://justinethier.github.io/cyclone//2020/08/03/Released-Cyclone-Scheme-0.19.html">Released Cyclone Scheme 0.19</a>
        </h4>
        <span>August  3, 2020</span>
        <br>
        This release improves error reporting and includes many bug fixes.
      


      </section>
    </div></div>]]>
            </description>
            <link>https://justinethier.github.io/cyclone/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24696939</guid>
            <pubDate>Tue, 06 Oct 2020 12:23:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why I am building permapeople.org]]>
            </title>
            <description>
<![CDATA[
Score 131 | Comments 141 (<a href="https://news.ycombinator.com/item?id=24696688">thread link</a>) | @roboben
<br/>
October 6, 2020 | https://permapeople.org/blog/2020/10/05/why-i-am-building-permapeople-org.html | <a href="https://web.archive.org/web/*/https://permapeople.org/blog/2020/10/05/why-i-am-building-permapeople-org.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><img src="https://permapeople.org/blog/assets/why-permapeople.jpg" alt="Two potatoes in an Ikea bag"></p>

<p>Four years ago my grandfather gave me two potatoes. I had no idea what to do, so I put them in bought soil in a big blue Ikea bag on the balcony and with a bit watering, they turned out great and I got hooked on growing food for my family and me. It is really magical if you think of how much you spare our planet with growing your own food: You need to get a job to make money so that you can spend that money on buying food which was produced and delivered close to you by large, complex and very inefficient industries. This system spends incredible amounts of resources (time, energy, labor) which you can save by simply growing your own food. And it doesn‚Äôt stop with food only: People grow plants for medicinal uses, to help the nature and wildlife around them, or just for their own pleasure.</p>

<h2 id="the-problem">The Problem</h2>

<p>This season I really tried to scale up and created raised beds all over our small urban plot in Berlin, Germany. I wanted to do it sustainable and close to nature, so I read Toby Hemenway‚Äôs Gaia‚Äôs Garden, which is probably the most widely read book on permaculture. While it is a good base to start and there are a lot of resources around online, it is actually pretty hard to make all that info useful for my own garden. Most of the time I found myself random googling just to answer simple questions like</p>

<ul>
  <li>What plants in the herb layer are available in my zone?</li>
  <li>What are the best companion plants for Tomatoes?</li>
  <li>What is the best time to sow peas in the garden in my zone?</li>
</ul>

<p>I fell back to having a spreadsheet, a collection of browser bookmarks, and a few books to look up what plants I can grow and how they could fit together in my garden. This took me a lot of time, I‚Äôd rather spend in the garden.
After the garden was planned, the next challenge was where to find seeds, seedlings and plants to start the garden. Mostly I googled the plant name I wanted to buy and ordered in whatever shop came up but it would be so much easier to buy it directly from other fellow gardeners.
The season started and another thing I did was writing a diary of all my garden activities. The idea was to learn from past mistakes to grow better next year. It worked well for me but true learning comes from sharing experiences with others, which was not possible with that.
While this worked for this year, I wanted to have something better next year so I started building a platform around all these topics.</p>

<h2 id="a-platform-for-everyone">A Platform for Everyone</h2>

<p>Most of the resources about growing plants you find online are either anecdotal or very scientific. There is no place where a gardening enthusiast can share their experiences, see what other enthusiasts learned already, and collaborate on everything related to growing plants. I think to achieve that, we need:</p>

<p>A <strong>permaculture plant database</strong> which everyone can search easily by common permaculture plant attributes like Layer, preferred light and soil conditions, times when to plant and harvest and benefits for animals, human and the environment. In addition everyone can look up advanced topics like companion planting and guild design. To make this info useful, it needs to be verified by others through ratings, comments and linked sources. If someone could see that most people were successful with growing that specific variety of a plant in your area or that a certain guild really works for a lot of others, that would be a huge help for everyone.</p>

<p>A <strong>permaculture marketplace</strong> where people can share/trade/buy/sell seeds, plants and everything else they might need like equipment, books, courses. Others can use it to sell products from their permaculture gardens to make an income for themselves. Everything happens directly between fellow gardeners.</p>

<p>A <strong>permaculture garden log and planner</strong> where everyone can log their past garden activities, learn from each other and plan their next season or project. If this info is combined with all other gardeners, then it becomes citizen science and we can improve everyone‚Äôs gardening results. Imagine you could be notified when all the more advanced gardeners start their tomato seedlings in your area, so you could do that too.</p>

<h2 id="make-the-planet-a-better-place-for-real">Make the planet a better place (for real)</h2>

<p>There are many people who want to grow plants for many reasons but don‚Äôt know how. There are also many people already growing a few plants in their garden and learned it the hard way. I believe we need a platform where they can come together and share their experiences and learn from one another to help improve the life of eveveryone. If we would all start growing a bit of our own food, we could help the planet and ourselves in so many impactful ways.</p>

<p>There is a lot to write about the implications of having such a platform, which I will do in future posts.</p>

<p>In the meantime, you can check out the plant database <a href="https://permapeople.org/database">here</a> and if you are interested, either <a href="https://permapeople.org/users/sign_up">sign up</a>, write me an email to hello at permapeople org or sign up for the newsletter where I am posting regular updates.</p>

<p>Thanks for reading üå±‚úåÔ∏è,</p>

<p>ben</p>

  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://permapeople.org/blog/2020/10/05/why-i-am-building-permapeople-org.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24696688</guid>
            <pubDate>Tue, 06 Oct 2020 11:42:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Chat bot powered by GPT-3]]>
            </title>
            <description>
<![CDATA[
Score 70 | Comments 79 (<a href="https://news.ycombinator.com/item?id=24695710">thread link</a>) | @piotrgrudzien
<br/>
October 6, 2020 | https://blog.quickchat.ai/post/knowledge-base-chat-bot/ | <a href="https://web.archive.org/web/*/https://blog.quickchat.ai/post/knowledge-base-chat-bot/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><figure><img src="https://blog.quickchat.ai/images/blog-post-1-bg.png" alt="Knowledge-base chat bot for SaaS product sales"></figure><section><div><p><em>Brief summary of our GPT-3 chat bot for SaaS product sales.</em></p><p>The most natural way for us to communicate is, well, <em>natural language</em>. Chat bots are nothing new but unless they meet a high-enough quality bar, they tend to be a step backwards rather than forward. We believe huge language models such as <a href="https://openai.com/blog/openai-api/">OpenAI‚Äôs GPT-3</a> will form a foundation for a truly conversational human-computer interface. It is, however, a foundation rather than a solution in and of itself.</p><p>In this new paradigm, the big challenge becomes to ensure the chat bot strictly sticks to the topic it was designed for and provides accurate information - without depriving it of its creativity.</p><p>I will discuss this briefly in the context of what we refer to as <strong>knowledge-base chat bots</strong>. They are built to answer general questions and hold a conversation about a product, service or a topic delineated by a predetermined unstructured knowledge base.</p><p><img src="https://blog.quickchat.ai/images/zeroth_faster.gif" alt="Start a conversation - image" title="Start a conversation"></p><p>Our chat bot implementation approved by the OpenAI team (try it out live at <a href="https://itemsy.com/">itemsy.com</a>) is an expert on Itemsy - a software product for managing the content you read online. It relies on GPT-3 for its conversational capabilities.</p><p>Thanks to our <a href="https://quickchat.ai/">Quickchat</a> engine (on top of GPT-3), it makes full and accurate use of the Itemsy knowledge base it was provided with, focuses on the topic at hand and cannot be maneuvered away from it:</p><p><img src="https://blog.quickchat.ai/images/first_faster.gif" alt="Avoid off-topic conversations - image" title="Avoid off-topic conversations"></p><p>Ultimately, it‚Äôs all about <em>conversation</em>. It requires context, needs to be unscripted, adaptive and creative. You‚Äôre still talking to a machine but this time language feels more like natural language. üôÉ</p><p><img src="https://blog.quickchat.ai/images/second_faster.gif" alt="Creative conversation guided by the user - image" title="Creative conversation guided by the user"></p><p>We‚Äôre ready to work with you and launch conversational chat bots for a wide range of use cases. Reach out to us at <a href="https://quickchat.ai/">quickchat.ai</a>!</p><blockquote>‚Äî Dominik Posmyk (@dominikposmyk) <a href="https://twitter.com/dominikposmyk/status/1309497928810213376?ref_src=twsrc%5Etfw">September 25, 2020</a></blockquote></div></section></article><div><h2>Follow the Quickchat blog for product updates, user stories and technical posts about artificial intelligence.</h2><p>
<span>Please correct your email address</span></p></div></div>]]>
            </description>
            <link>https://blog.quickchat.ai/post/knowledge-base-chat-bot/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24695710</guid>
            <pubDate>Tue, 06 Oct 2020 08:22:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fooling Around with Foveated Rendering]]>
            </title>
            <description>
<![CDATA[
Score 172 | Comments 84 (<a href="https://news.ycombinator.com/item?id=24695275">thread link</a>) | @underanalyzer
<br/>
October 5, 2020 | https://www.peterstefek.me/focused-render.html | <a href="https://web.archive.org/web/*/https://www.peterstefek.me/focused-render.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
 <div>
  
  <p><label>Posted on <strong>28 September 2020</strong></label></p>
<p>Shadertoy is a wonderful tool which lets users create and share a type of program called a fragment shader online. The true magic of shadertoy is its community of very talented graphics programmers who build incredible works of art despite having access to only a sliver of the traditional graphics pipeline.  </p>
<p>Some of these shaders are very computationally intensive and even in a small window, they crawl along well below their intended 60 frames per second on my old laptop. Inspired by a technique in the VR community called Foveated Rendering, I decided to try to optimize these shaders by only rendering a fully detailed image within a small focal region. As you move away from the focal point the image quality decreases.   </p>
<p>This rendering scheme is motivated by biology. It turns out your eye notices more detail in the center of your vision than in the periphery. Some VR graphics programmers realized they could take advantage of this phenomenon to increase the effective resolution of images by increasing image quality towards the center of your vision. An in depth discussion of foveated rendering can be found in the ‚Äúprevious work section‚Äù of this <a href="https://ai.facebook.com/blog/deepfovea-using-deep-learning-for-foveated-reconstruction-in-ar-vr">paper</a>.  </p>
<p>I did not have the time, equipment or the background necessary to implement a full foveated rendering system but it was fun to fool around with the concept.  </p>
<p>Before diving into the technical details let‚Äôs look at a simple shadertoy fragment shader.   </p>
<p><code>
void mainImage(out vec4 fragColor, in vec2 fragCoord)<br>
{
</code></p><p><code>
    // Normalized pixel coordinates (from 0 to 1)<br>
    vec2 uv = fragCoord/iResolution.xy;
<div><pre><span></span><span>//</span> <span>Output</span> <span>the</span> <span>pixel</span> <span>coordinates</span> <span>as</span> <span>a</span> <span>color</span> <span>to</span> <span>screen</span>
<span>//</span> <span>fragColor</span> <span>is</span> <span>a</span> <span>4</span> <span>vector</span> <span>of</span> <span>the</span> <span>form</span>
<span>//</span> <span>(</span><span>red</span><span>,</span> <span>green</span><span>,</span> <span>blue</span><span>,</span> <span>transparency</span><span>)</span>
<span>fragColor</span> <span>=</span> <span>vec4</span><span>(</span><span>uv</span><span>,</span> <span>0</span><span>.</span><span>0</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>);</span>
</pre></div>


</code></p><p><code>
}<br>
</code></p>
<p>This program runs once for each pixel on the screen. Each time it runs, we receive the input variable <code>fragCoord</code>. <code>fragCoord</code> is a 2d vector which contains the x and y coordinates of the pixel being drawn. We normalize those coordinates by dividing by <code>iResolution</code>, another 2d vector, which contains the width and height of the image. Finally we output a color to the screen, whose red and green channels are proportional to the x and y position of the pixel being drawn. The output of this shader looks like this:<br>
</p><p>
  <img src="https://www.peterstefek.me/images/focused-render/simple-shader-out.png" width="50%"> 
</p>
<p>Side note, why do these shader programs require their own language? Shaders are special because they run on the graphics card instead of the cpu. They are highly parallel. A helpful mental model might be imagining that each pixel is colored simultaneously. Therefore a lot of things that we take for granted in normal program languages such as liberally accessing memory and branching become much more difficult.  </p>
<p>In shadertoy shaders the bottleneck is always in the pixel rendering step. So to speed them up we want to only render a subset of the all the pixels on the screen. It seems like selectively rendering pixels should be as simple as adding a branch to the per pixel shader code that looks like:  </p>
<p><code>
void mainImage(out vec4 fragColor, in vec2 fragCoord) <br>
{<br>
</code></p><p><code>
    if (fragCoord is in the subset of pixels to render) {
      <p>
      ... do computationally intensive work 
      </p> 
    } else {
      <p>
      // return a black pixel<br>
      return vec4(0, 0, 0, 1); 
      </p> 
    } 
</code></p><p><code> 
}
</code></p>
<p>Unfortunately we cannot just use an if statement inside of the shader to save us from rendering all the pixels. Unlike normal programming languages, fragment shaders always execute both parts of each branch due to gpu limitations. So while our above code will still have to spend the sample amount of time evaluating compuationally intensive work.  </p>
<p>Luckily, it turns out that graphics drivers can selectively mark which pixels not to shade by writing their location to a special buffer called the stencil buffer. We can use this stencil buffer to only shade the subset of pixels we are interested in.</p>
<p>Once I could efficiently render a subset of the pixels, I needed to come up with a pre-generated sampling pattern. Most foveated rendering techniques seem to use a grid, but I decided to try a non uniform approach. Searching for some kind of optimal sampling pattern seemed like an interesting problem and if I was going to devote more time to this I'd explore options like <a href="https://blog.demofox.org/2018/01/30/what-the-heck-is-blue-noise/">blue noise</a>. However in the interest of time, I just decided fill in a small circle in the center and then use samples drawn from one low variance and one high variance gaussians centered at the middle of the screen to place the rest of the pixels. The final sampling pattern ended up looking like this:
</p><p>
  <img src="https://www.peterstefek.me/images/focused-render/final-sample-pattern.png" width="50%"> 
</p>
<p>Next, I needed a way to fill in all the missing pixels in the final image. The approach I took was pretty simple. I started by mapping each pixel in the final screen to its nearest neighbor. Since my sampling pattern was predetermined, I could create this map beforehand and pass it into the shader as a texture. Here's what this mapping looks like:<br>
</p><p>
  <img src="https://www.peterstefek.me/images/focused-render/nearest-mapping.png" width="50%"> 
</p>
<p>And here‚Äôs a gif of the mapping applied to a <a href="https://www.shadertoy.com/view/3lsSzf">shadertoy</a> created by the extremely talented <a href="https://www.iquilezles.org/">Inigo Quilez</a>:<br>
</p><p>
  <img src="https://www.peterstefek.me/images/focused-render/1-neighbor.gif"> 
</p>
<p>The above screen is 420x236 pixels and only 1/10th of those pixels are actually rendered. The focal point is directly in the center of the screen. Here's what the full resolution version looks like:</p>
<p>
  <img src="https://www.peterstefek.me/images/focused-render/original.gif"> 
</p>

<p>And here's what it looks like with only our sampling pixels:
</p><p>
  <img src="https://www.peterstefek.me/images/focused-render/sample-pixels.gif"> 
</p>
<p>One little improvement I tried was to make 4 different maps. The kth map mapped each pixel in the final image to its kth nearest sampled neighbor. I weighted each of neighbors by the inverse of their distance to the pixel in question. I actually even tried using some gradient descent based optimization to fine tune the weights but ended up seeing little improvement. It also seemed that increasing the number of maps beyond 4 did not improve things much either. Here's what the example from above looks like with weighted interpolation between the four closest neighbors of each pixel (we are still rendering only 1/10th of the total pixels):
</p><p>
  <img src="https://www.peterstefek.me/images/focused-render/4-neighbors.gif"> 
</p>
<p>Finally, here's the shader with 1/5th of the total pixels rendered (as opposed to 1/10th shown above):
</p><p>
  <img src="https://www.peterstefek.me/images/focused-render/1of5pixels.gif"> 
</p>
<p>Okay that's all cool but does this technique actually increase performance? I did not do a rigerous benchmark, but <a href="https://www.shadertoy.com/view/3l23Rh">this shader</a> goes from around 20-25 fps on my plugged in laptop to 60 fps when reduced to 1/5th of the total pixels. <a href="https://www.shadertoy.com/view/Ms2SD1">Another shader</a> went from around 15 fps to 60 fps.  </p>
<p>One last side note is that this method can be used with any 3d scene and is not exclusive to shader toys. I just chose to use them because they are always bottlenecked by the pixel rendering step and they are really pretty!</p>
<p>Further questions:</p>
<ul>
<li>How do we achive better temporal stability? (the <a href="https://ai.facebook.com/blog/deepfovea-using-deep-learning-for-foveated-reconstruction-in-ar-vr">paper</a> I mentioned earlier talks about this)</li>
<li>Can we dynamically change the sampling pattern to give us better results? For example what if we sampled along edges or areas where large amounts of motion is occuring? Of course to do this we would need to compute our nearest neighbor mappings on the fly (there are actually <a href="https://www.shadertoy.com/view/XtlGDS">some</a> <a href="https://www.shadertoy.com/view/ldl3W8">shadertoys</a> which already demonstrate capability).</li>
<li>How could this scheme improve if we had access to the internals of the 3d scene? For example, could we adjust our sampling pattern based on depth information?  </li>
<li>How does this actually look in VR?  </li>
</ul>
<p>Have questions / comments / corrections?<br>
Get in touch: <a href="mailto:pstefek.dev@gmail.com">pstefek.dev@gmail.com</a>   </p>
<p>Discussion on <a href="https://news.ycombinator.com/item?id=24695275">Hacker News</a></p>
 </div>
</div></div>]]>
            </description>
            <link>https://www.peterstefek.me/focused-render.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24695275</guid>
            <pubDate>Tue, 06 Oct 2020 06:44:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[File Corruption Is Attractive]]>
            </title>
            <description>
<![CDATA[
Score 24 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24694017">thread link</a>) | @sergioro
<br/>
October 5, 2020 | https://venam.nixers.net/blog/programming/2020/10/05/corruption-at-the-core.html | <a href="https://web.archive.org/web/*/https://venam.nixers.net/blog/programming/2020/10/05/corruption-at-the-core.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <article>
    <p><img src="https://venam.nixers.net/blog/assets/chaos1.jpg" alt="Chaos, an important theme in hermetism" loading="lazy"></p>

<p>We live in a world that is gradually and incessantly attracted by
over-rationality and order. In this article we‚Äôll burst the enchanted
bubble and embrace corruption and chaos ‚Äî We‚Äôre going to discuss the
topic of image glitch art.</p>

<h2 id="wÃ∏hÃ∏aÃ∑tÃ¥Ã∂sÃ¥-Ã∂aÃ¥-Ã∑gÃ∑lÃ∏iÃ∑tÃ¥cÃµhÃµ">wÃ∏hÃ∏aÃ∑tÃ¥‚ÄôÃ∂sÃ¥ Ã∂aÃ¥ Ã∑gÃ∑lÃ∏iÃ∑tÃ¥cÃµhÃµ</h2>

<p>Welcome to the land of creative destruction: image glitch art. Our story
starts with a simple idea: glitching a wallpaper to create a slideshow
of corrupted pictures.<br>
The unfortunate victim of our crime: The world (Right click <strong>&gt;</strong> View
image, while keeping the <strong>Control</strong> key pressed, to admire it in more
details while its still in its pristine form):</p>

<p><img src="https://venam.nixers.net/blog/assets/glitch_art/world_map.jpg" alt="World Map, nominal case" loading="lazy"></p>

<p>Before we begin, let‚Äôs attempt to define what we‚Äôre trying to do: What
is glitch art?<br>
Like any art movement, words can barely express the essence behind the
meaning, they are but fleeting and nebulous. Regardless, I‚Äôll be an
infidel and valiantly express what I think glitch art is.</p>

<p>A glitch is a perturbation, a minor malfunction, a spurious signal. In
computers, glitches are predominantly accidental events that are
undesirable and could possibly corrupt data.<br>
Glitch art started as people developed a liking for such unusual events
and the effects glitches had on the media they were perturbing. Some started
to collect these glitches that happened naturally in the wild, and others
started to intentionally appropriate the effects by manually performing them.<br>
In the art scene, some started using image processing to ‚Äúfake‚Äù true
glitching effects.</p>

<p>Glitches happen all the time and everywhere, information is never as
durable and reliable as we might like it to be, and living in a physical
world makes it even less so. You‚Äôve probably encountered or heard of the
effect of putting a magnet next to anything electronic that hasn‚Äôt been
rugged to withstand such scenario.<br>
That‚Äôs why many techniques have been put in place to avoid glitches,
at all layers, from the hardware storage, to the software reading
and interpreting it. Be it error correcting codes (ECC) or error detection
algorithms, they are all enemies of glitch art and the chaos we like.</p>

<p>However, this niche aesthetic is more than a fun pass-time for computer
aficionado, there is a bigger picture. Similar to painters with
brushes on a canvas, we are offered a material, an object to work with
‚Äî a material made of bits and formatted in a specific way.<br>
Like any object, our medium has a form and meaning, it can move, it has
a size, it can be transferred, and interpreted ‚Äî information theory
is the field interested in this.<br>
Like any object, our medium can be subject and react to deformations,
forces, and stressors. How it flows is what the field of rheology
is interested in (not to be confused with computational rheology, the
field of fluid simulation.) The medium fluidity can be studied to answer
questions such as: is it elastic, solid, viscous, or oily, how does it
respond, within the bound of information theory, to different types of
applied forces.</p>

<p>Here are some words you may encounter and that you definitely want
to know:</p>

<ul>
  <li>
    <p>Misregistration: Whenever a physical medium misread data because of
damages caused by scratches, dirt, smudges, gamma rays, or any other
treasures the universe throws at us.</p>
  </li>
  <li>
    <p>Datamoshing, Photomosh, Imagemosh: Abusing the format of a medium,
normally compression artefacts, to create glitches. For example, video
compression often use i-frames for fixed images and p-frame for the
movement/transition of pixels on that image. <a href="https://www.reddit.com/r/datamoshing/">Removing i-frames is a
common glitching method</a>.</p>
  </li>
  <li>
    <p>Databending: An idea taken from circuit bending, bending the circuit
board of a toy to generate weird sounds. Databending is about bending
the medium into another unrelated one, reinterpreting it as something
it is not meant to be.</p>
  </li>
</ul>

<p>Let me add that glitch art is vast and fascinating, this article is but a
glimpse into this space. If you‚Äôre captivated as much as I am, please take
a look at <a href="http://gli.tc/h/0nline/">gli.tc</a> and <a href="https://beyondresolution.info/">Rosa Menkman‚Äôs Beyond
Resolution</a>. Images can be pleasantly
destroyed in a great number of ways to create masterpieces.</p>

<h2 id="iÃ∑mÃ∑aÃ∑gÃ¥eÃ¥-Ã∏gÃ∏lÃ¥iÃ¥tÃ¥cÃµhÃ∏-Ã¥aÃ∂rÃµtÃµ">IÃ∑mÃ∑aÃ∑gÃ¥eÃ¥ Ã∏GÃ∏lÃ¥iÃ¥tÃ¥cÃµhÃ∏ Ã¥AÃ∂rÃµtÃµ</h2>

<p>Before starting let‚Äôs give some advices:</p>

<ul>
  <li>Back up your precious files before corrupting them.</li>
  <li>Any glitching techniques can be combined and/or applied multiple times.</li>
  <li>Sometimes too little has no effect, and sometimes too much can destroy
the file.</li>
  <li>It‚Äôs all about trials and errors, especially errors that result in
glitches.</li>
</ul>

<h3 id="Ã∑hÃ∑oÃµwÃ∂-ÃµtÃ∏oÃ¥-Ã∂iÃ∑nÃ∂dÃ∏uÃ∑cÃ∂eÃµ-Ã∂aÃ∏-Ã∂gÃ∏lÃµiÃ∑tÃ∂cÃ∏hÃ¥">Ã∑HÃ∑oÃµwÃ∂ ÃµTÃ∏oÃ¥ Ã∂IÃ∑nÃ∂dÃ∏uÃ∑cÃ∂eÃµ Ã∂AÃ∏ Ã∂GÃ∏lÃµiÃ∑tÃ∂cÃ∏hÃ¥</h3>

<p>Now it‚Äôs time to think about how we can apply our mischievous little
stimuli, its size, the level or layer at which it‚Äôll be applied, and
the methodological recipe we‚Äôll concoct to poison our images.</p>

<p>Glitch artist Benjamin Berg classifies glitches into 3 categories:</p>

<ul>
  <li>Incorrect Editing: Editing a file using a software that
wasn‚Äôt made to edit such file. Like editing an image file as if it
was a text file.</li>
  <li>Reinterpretation aka Databending: Convert or read a file as if it was
another type of medium. Like listening to an image file as if it was
an audio file (aka sonification).</li>
  <li>Forced errors, Datamoshing, and Misregistration: A software or hardware
bug to force specific errors in the file. This can be about the
corruption of specific bytes in the file to induce glitches, or
something happening accidentally like a device turning off when saving
a file.</li>
</ul>

<p>So let‚Äôs get to work!</p>

<h3 id="mÃ∑aÃµsÃµhÃ∏iÃ∂nÃ∂gÃ∑-Ã¥tÃ∂hÃ∑eÃ∑-Ã∑dÃ∂aÃ∏tÃµaÃ∏-Ã∏rÃ∑aÃ∏nÃ∂dÃ∂oÃ∏mÃ¥lÃµyÃ∑">MÃ∑aÃµsÃµhÃ∏iÃ∂nÃ∂gÃ∑ Ã¥TÃ∂hÃ∑eÃ∑ Ã∑DÃ∂aÃ∏tÃµaÃ∏ Ã∏RÃ∑aÃ∏nÃ∂dÃ∂oÃ∏mÃ¥lÃµyÃ∑</h3>

<p>The easiest, but roughest, way to glitch a file is to put on our monkey
suit and overwrite or add random bytes in our image. As you would have
guessed, this isn‚Äôt very efficient but half the time it does the trick
and forces errors.</p>

<p>This technique is better suited for stronger materials like images in
raw format ‚Äî without metadata and headers. We‚Äôll understand why in a bit.<br>
To convert the file to raw format, open it in GIMP, select <strong>Export As</strong>,
select the file by extension, and choose the raw type. For now, it doesn‚Äôt
matter if you pick pixel-ordered or planar, but we‚Äôll come back to this
choice later because it‚Äôs an important one.</p>

<p><img src="https://venam.nixers.net/blog/assets/glitch_art/gimp_saveas_raw.jpg" alt="GIMP process to save image as raw" loading="lazy"></p>

<figure><pre><code data-lang="shell">file world_map.data
<span># world_map.data: Targa image data - Map (771-3) 771 x 259 x 1 - 1-bit alpha "\003\003\003\003\003\003\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001"</span></code></pre></figure>

<p>You should also note the width and height of the image as it now doesn‚Äôt
contain this information anymore, and we‚Äôll need those to reopen it in
GIMP. In our case it is <code>2000x1479</code>.</p>

<p>We now proceed to hand over the file to our least favorite
staff and let them have an anger tantrum at it. So what does
it look like, let‚Äôs take a look at <a href="https://www.youtube.com/watch?v=oj6NMiuU0ys">the result our monkey
did</a>:</p>

<p><img src="https://venam.nixers.net/blog/assets/glitch_art/world_map_random_bytes.jpg" alt="World Map, monkey have been randomly mashing the
world" loading="lazy"></p>

<p>Not bad at all for something random, but we can do better.</p>

<h3 id="cÃ∏oÃ∂mÃ∏pÃ∑rÃ∂eÃ∑sÃ∏sÃµiÃ∏oÃ∂nÃ¥-Ã∂dÃµeÃµfÃ∂oÃ∂rÃ¥mÃ∑aÃ∂tÃ∑iÃµoÃ∏nÃ∑">CÃ∏oÃ∂mÃ∏pÃ∑rÃ∂eÃ∑sÃ∏sÃµiÃ∏oÃ∂nÃ¥ Ã∂DÃµeÃµfÃ∂oÃ∂rÃ¥mÃ∑aÃ∂tÃ∑iÃµoÃ∏nÃ∑</h3>

<p>Some medium are more malleable when squished properly and squished
in different ways. The image sheds a lot of information and only the
essence stays. That‚Äôs a form of databending.<br>
For example, increasing the compression of JPEG images can open the path
for glitches to happen more frequently. This is a key asset, especially
when trying to create errors related to the compression parameters within
the format of the file.</p>

<figure><pre><code data-lang="shell">convert <span>-quality</span> 2 world_map.jpg world_map_compressed.jpg</code></pre></figure>

<p><img src="https://venam.nixers.net/blog/assets/glitch_art/world_map_compressed.jpg" alt="World Map, compressed to extract its
essence" loading="lazy"></p>

<p>Keep this in your toolbox to use along with other techniques.</p>

<h3 id="gÃµeÃ¥tÃµtÃ∂iÃ∏nÃµgÃ∑-Ã¥iÃ∏nÃµtÃ∑iÃµmÃ∑aÃ¥tÃ∏eÃ¥-Ã∏wÃ∂iÃµtÃ∏hÃ∂-Ã¥tÃµhÃ∏eÃ∏-Ã¥fÃ∑oÃ¥rÃ∂mÃ∏aÃ¥tÃ∑">GÃµeÃ¥tÃµtÃ∂iÃ∏nÃµgÃ∑ Ã¥IÃ∏nÃµtÃ∑iÃµmÃ∑aÃ¥tÃ∏eÃ¥ Ã∏WÃ∂iÃµtÃ∏hÃ∂ Ã¥TÃµhÃ∏eÃ∏ Ã¥FÃ∑oÃ¥rÃ∂mÃ∏aÃ¥tÃ∑</h3>

<p>We want to corrupt in the most efficient way possible, to create
attractive chaos from the smallest change possible. To do that we have to
get intimate with the medium, to understand its deepest secrets, tickle
the image in the right places. This is what we previously referred to
as imagemoshing.</p>

<p>There‚Äôs a panoply of image formats, and they all are special in their
own ways. However, there‚Äôs still some commonality:</p>

<ul>
  <li>Header, Footer, and Metadata: If the format contains these extra
information, be it extraneous or essential, what they represent, and
how they affect the rest of the image.</li>
  <li>Compression: The format can either be compressed or not. When it is
compressed, there can be extra bits of information to help other software
uncompress the image data.</li>
  <li>How the data is laid out: Usually, the image color information is
decomposed into its components such as HSL, RGB, or others. These
components then need to be represented in the image data, either in
an interleaved or planar manner. Planar refers to writing components
independently in the data (<em>ex:</em> all R, then all G, then all B),
while interleaved refers to having them joined non-contiguously in an
alternate sequence (<em>ex:</em> RGB, then RGB, then RGB..).</li>
</ul>

<p>Manipulating these to our advantage can lead to wonderful glitches. For
example, in our previous raw image example ‚Äî an image bare of header,
footer, and without compression ‚Äî the pixels were interleaved which
gave rise to the effect we‚Äôve seen, namely shifts and changes in some
colors. Having them in planar form would‚Äôve led to different glitches
in separate color channels.</p>

<h3 id="rÃµeÃ∑iÃ∑nÃ¥tÃ∂eÃ∑rÃ∂pÃ∏rÃ¥eÃ∏tÃ∏aÃ∑tÃ∂iÃ∑oÃµnÃ¥-ÃµaÃ∏sÃµ-ÃµrÃ∏iÃ∑cÃµhÃ∏-Ã∏tÃ∑eÃµxÃµtÃ¥-Ã¥aÃ¥kÃ∑aÃ∏-Ã∑wÃ∂oÃ¥rÃµdÃ¥pÃ¥aÃ∏dÃµ-Ã∑eÃµfÃ∏fÃ¥eÃ∂cÃ∂tÃ∂">RÃµeÃ∑iÃ∑nÃ¥tÃ∂eÃ∑rÃ∂pÃ∏rÃ¥eÃ∏tÃ∏aÃ∑tÃ∂iÃ∑oÃµnÃ¥ ÃµAÃ∏sÃµ ÃµRÃ∏iÃ∑cÃµhÃ∏ Ã∏TÃ∑eÃµxÃµtÃ¥ Ã¥AÃ¥KÃ∑AÃ∏ Ã∑WÃ∂oÃ¥rÃµdÃ¥PÃ¥aÃ∏dÃµ Ã∑EÃµfÃ∏fÃ¥eÃ∂cÃ∂tÃ∂</h3>

<p>Let‚Äôs give this a try with the well-known WordPad effect, which is about
databending an image into rich text: opening the image in WordPad and
saving it.<br>
Keep in mind that this only works with raw images as it‚Äôs highly
destructive and otherwise could break fragile key info in the header
and footer. So let‚Äôs reuse our interleaved raw image of earlier but also
get a planar one.</p>

<p>This is our results for interleaved:</p>

<p><img src="https://venam.nixers.net/blog/assets/glitch_art/world_map_wordpad.interleaved.corrupt.jpg" alt="World Map, WordPad effect interleaved" loading="lazy"></p>

<p>And for planar:</p>

<p><img src="https://venam.nixers.net/blog/assets/glitch_art/world_map_wordpad.planar.corrupt.jpg" alt="World Map, WordPad effect planar" loading="lazy"></p>

<p>Technically, what happens is that during the bending and interpretations
as rich text, some bytes are inserted in some ‚Ä¶</p></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://venam.nixers.net/blog/programming/2020/10/05/corruption-at-the-core.html">https://venam.nixers.net/blog/programming/2020/10/05/corruption-at-the-core.html</a></em></p>]]>
            </description>
            <link>https://venam.nixers.net/blog/programming/2020/10/05/corruption-at-the-core.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24694017</guid>
            <pubDate>Tue, 06 Oct 2020 01:43:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Don‚Äôt Find Mentors. Find Your Future Self]]>
            </title>
            <description>
<![CDATA[
Score 37 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24693763">thread link</a>) | @jdcampolargo
<br/>
October 5, 2020 | https://www.juandavidcampolargo.com/blog/future-self | <a href="https://web.archive.org/web/*/https://www.juandavidcampolargo.com/blog/future-self">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div data-layout-label="Post Body" data-type="item" data-updated-on="1601739309102" id="item-5f789898afa8642a86474931"><div><div><div data-block-type="2" id="block-ba447757312844ef3468"><div><p>Back in 2018, I was fifteen years old, and I wanted to do something over the summer. I could get a job, work on my businesses, sit around, watch Netflix, or work for free at tech startups.&nbsp;</p><p>I chose the latter.</p><p>Why would I work for free in a startup? It wasn‚Äôt for free; I had to pay for the train tickets and food. I didn‚Äôt care. I knew that being out there meeting founders and investors would be the best way to learn.&nbsp;</p><p>I wanted to be involved in the startup world by creating, developing, and investing in them. I believe that‚Äôs the way how we can change the world and have a positive impact.&nbsp;</p><p>But how in the world would I find those startups?</p><p>I sent 500+ emails, called companies, asked family friends, and did everything I could, but I got annoyed. So I was like, ‚ÄúScrew it, I‚Äôm just going to go and ask them.‚Äù</p><p>I went to this startup incubator called Techstars at 1871 and asked if they were hiring. And that I was willing to work for free or ‚Äúvolunteer.‚Äù</p><p>At first, the founders looked at me like, ‚ÄúKid, how old even are you?‚Äù Most founders looked the other way, but two guys were like, ‚ÄúDamn, let‚Äôs find something for this kid to do.‚Äù</p><p>And that‚Äôs how one of my best summers started.</p><p>The founders may have thought a fifteen-year-old won‚Äôt be able to do much, but I literally told them, ‚ÄúI‚Äôm willing to do anything, I just want to learn and spend as much time as possible at the incubator.‚Äù It felt like I was learning by osmosis.&nbsp;&nbsp;</p><p>I would go with the founders to meetings with investors and mentors and take notes, find venture capitalists interested in investing, work on hardware, and anything they told me to do.&nbsp;</p><p>I worked with three startups that developed an AI fitness assistant, solar drones, and a real estate crowdfunding site.&nbsp;</p><p>That summer showed me the world of startups. I met people like the governor, billionaires, and overall people who were heavily involved in the startup world.&nbsp;</p><p>I also experienced the startup world, which I‚Äôm sure will pay off in the future.&nbsp;</p><p>But most importantly, I met Paul, the founder who gave me the opportunity.&nbsp;</p><p>I always kept him posted on my progress in my projects and would ask how his company was doing. And Thanksgiving, Christmas, and Happy New Year messages as well.&nbsp;</p><p>Well, it turns out that Paul studied in the same college and did the same major. I‚Äôve talked to him multiple times in the last few weeks and it reminded me of the <strong>importance of having people who were once where you are.</strong></p><p>Some people called people like Paul ‚Äúmentors,‚Äù but that word is misleading because 1) it‚Äôs overused and over-hyped 2) it‚Äôs transactional and self-centered.&nbsp;</p><p>How do I call people like Paul? I call them, <strong>‚ÄúMy Future Selves.‚Äù</strong></p><p>They often can give that little push you need. And you get to cheat because you get to talk to your future self. What‚Äôs even better is that you can always do things differently and learn from their mistakes.&nbsp;</p><p>Studying engineering isn‚Äôt the easiest thing in the world and when you bomb a test, you can feel like ‚ÄúYou don‚Äôt have what it takes.‚Äù My first chemistry exam? Hmmm. One to forget. I like to do as best as I can and not doing as expected frustrates me, especially when I need to get a good GPA to transfer to the engineering school.&nbsp;</p><p>I felt like I was the only one, but as I talked to ‚ÄúMy Future Self‚Äù or Paul, I realized I wasn‚Äôt the only one and he too didn‚Äôt do great on his first chemistry exam. But he improved and could transfer to the engineering school.&nbsp;</p><p>He advised me on how to approach studying and how to approach the test and my grades have improved.&nbsp;</p><p>Sometimes, you can also lose sight of the big picture of what you want. Future selves remind you to keep focused not by scolding you but by asking you simple questions like, ‚ÄúWhat are your future plans after college?‚Äù or ‚ÄúDo you like what you‚Äôre studying?‚Äù</p><p>In my case, Paul knows I‚Äôm into startups and <a href="https://www.juandavidcampolargo.com/blog/ambition" target="_blank">eating the world</a>, so he helps me with choosing a path that aligns with my interests and with my goals.&nbsp;</p><p>Looking back, that was not a normal thing for a teen to do. Yes, you can say it, ‚ÄúI was weird.‚Äù&nbsp;</p><p>Well, not really. I knew <strong>who</strong> I was and <strong>where</strong> I wanted to go. That‚Äôs how going up to startup founders asking for a job could become your summer.&nbsp;</p><h2>How To Find Your Future Self</h2><p>If I hadn‚Äôt talked to Paul a few times since college started, I‚Äôd honestly be down and not very excited about the possibilities. <strong>Future selves can you show a path that gets you excited to work harder and more ambitiously.&nbsp;</strong></p><p>If you‚Äôre interested in finding a possible future self. I‚Äôve learned a few lessons that can be helpful.&nbsp;</p><p><strong><em>Avoid being artificial. </em></strong>We‚Äôve all heard the talk, ‚ÄúGet a mentor and blah blah blah.‚Äù Sure, but that can‚Äôt the only reason. You should be genuine. And please please, don‚Äôt be asking people to be your mentor.&nbsp;</p><p>How can you be more genuine?</p><ul data-rte-list="default"><li><p>Make yourself useful to them</p></li><li><p>Get a job or internship at that person‚Äôs company or lab</p></li><li><p>Ask unique and interesting questions</p></li><li><p>Just DM or email them</p></li></ul><p><strong><em>Understand their motivations. </em></strong>Keep this question in mind, ‚ÄúWhy would someone want to mentor you?‚Äù</p><p>It usually happens when a high-potential person approaches them, and they can help him/her reach that potential.&nbsp; And if they can see their advice pays off 10x in you. If your future self tells you to walk 5 steps, they want to see you walk 50 steps.&nbsp;&nbsp;</p><p><strong><em>Don‚Äôt find mentors. </em></strong>This one will be the hardest to understand, but the most powerful. I don‚Äôt ask people, ‚ÄúDo you want to be my mentor?‚Äù</p><p>Most people will say ‚ÄúNo.‚Äù Instead, the way you find mentors or future selves is by:</p><p>1) Finding people who are where you want to be.&nbsp;</p><p>2) Asking great questions and becoming genuinely interested in their work, and finally,&nbsp;</p><p>3) Keeping in contact with them and asking questions and/or advice when you need it. Or if you find something useful for them, this is when you help them.</p><p>I‚Äôm grateful to Paul and many of my other future selves because they allow me to learn from their mistakes, pick a unique path for me, and give you the little push and fresh perspective when you most need it.&nbsp;</p><p>Thank you, Paul, and thank you to all my future selves!</p></div></div></div></div></div></div></div>]]>
            </description>
            <link>https://www.juandavidcampolargo.com/blog/future-self</link>
            <guid isPermaLink="false">hacker-news-small-sites-24693763</guid>
            <pubDate>Tue, 06 Oct 2020 00:52:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Oswald Spengler ‚Äì an intellectual life]]>
            </title>
            <description>
<![CDATA[
Score 48 | Comments 34 (<a href="https://news.ycombinator.com/item?id=24693655">thread link</a>) | @objections
<br/>
October 5, 2020 | https://engelsbergideas.com/portraits/oswald-spengler-an-intellectual-life/ | <a href="https://web.archive.org/web/*/https://engelsbergideas.com/portraits/oswald-spengler-an-intellectual-life/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-id="9549bc7" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;,&quot;_animation_delay&quot;:500}" data-widget_type="theme-post-content.default">
				<div>
			
<p>Oswald Spengler was one of the most profound pessimists of modern times but at a glance, his legacy appears to have collected decades of dust since his early death in 1936.&nbsp;Considered unessential by historians and troublesome by philosophers, he nonetheless exerted an extraordinary influence over many powerful figures throughout the twentieth century. Wittgenstein said he was one of his chief inspirations; the Jungian theorist Joseph Campbell claimed Spengler‚Äôs work was his biggest influence; the philosopher Martin Heidegger was profoundly affected by Spengler‚Äôs thinking; and former Secretary of State, Henry Kissinger, wrote favourably about him in his doctoral thesis ‚ÄòThe Meaning of History‚Äô. Kissinger gifted a one-volume edition of Spengler‚Äôs The Decline of the West to President Nixon when he served in his administration to ‚Äòemphasise the manifestation of events‚Äô. But who was this man whose thought has shaped modern philosophy and the perception of some of our top policy gurus, and what did he believe?&nbsp;</p>



<p>Oswald Spengler was born in the Duchy of Brunswick in 1880 to Protestant parents. His father‚Äôs family were traditionally mining engineers and metallurgical inspectors while his mother, Pauline, from whom Spengler received his irascible temperament, hailed from an artistically inclined lineage of ballet dancers and bohemians. Unlike his ancestors, his father worked as a senior postal secretary and severely chastised any hint of intellectualism in his children, a repulsion that must have conditioned the young Spengler to distrust celebrated thinkers later in his life. It was this inherited mixture of two divergent tendencies ‚Äì engineering and science with bohemianism and the arts ‚Äì that afforded Spengler a unique intellectual vantage and prepared him to proffer his special perspective to his readers in the future.&nbsp;</p>



<p>After excelling in Greek, Latin, Mathematics and the Sciences at school, he attended the universities of Munich, Berlin and Halle. In 1903, two years after his father‚Äôs death, he initially failed to obtain his PhD but managed to pass a year later via an oral equivalent of the examination. He then qualified as a schoolmaster and led a relatively uneventful life teaching in Saarbrucken, Dusseldorf and Hamburg. When his mother died in 1910, he returned to Munich where he lived on his modest inheritance as a private scholar. It is said he owned no books and suffered from great loneliness rather like a latter-day Nietzsche. But, like Nietzsche, these years proved to be formative. It was around this time that he conceived of writing a book that would challenge common preconceptions of history and its meaning. He began work on&nbsp;<em>The Decline of the West&nbsp;</em>in 1911 and completed a first draft in 1914, but due to the war, he had to wait until 1918 for it to be published. Burdened with a weak heart, he was exempted from military service yet this didn‚Äôt mean he had an easy time. With much of his inheritance invested abroad, he was forced to persist in a state of serious poverty until the publication of his work.&nbsp;</p>



<p>When it came out, its success made him an instant celebrity and he finally gained the respect and reputation his talents deserved. His explanation of the war as a ‚Äòhistorical change of phase‚Äô which was ‚Äòpreordained for Germany hundreds of years ago‚Äô consoled Germans who were being blamed in the European press for gratuitously starting the greatest military catastrophe of their time. In the early days of the Weimar Republic, he called himself a Socialist, but not a socialist of any obvious orthodoxy. He rejected Marx, whom he saw as the chief critic of the English capitalism he  defined as ‚ÄòGet rich, so you don‚Äôt have to work anymore‚Äô. He believed that the true source of German socialism was not Marx so much as Frederick William I. To succinctly explicate his brand of Prussian Socialism he used the phrase ‚ÄòDo your duty, work‚Äô.&nbsp;</p>



<p>His Prussian subgenre of Socialism naturally appealed to National Socialists and he became an inspirational figure to some early believers of Nazism. His criticism of the Weimar Republic, of Marxism, pacifism and democracy intellectually aided the Nazis in their ideological advancement to the top of German politics, but once they were in power Spengler famously refused to ‚ÄòHeil Hitler‚Äô and was thereafter shunned. He occupied his concluding years by collecting thousands of books and exotic primitive weapons, by reading the comedies of Moliere and Shakespeare and listening to the haunting quartets of his hero, Beethoven. He died three weeks before his fifty-sixth birthday of a sudden heart attack on 8<sup>th</sup>&nbsp;May, exactly nine years before the bloody fall of the Third Reich.</p>



<p>Like Kant, who rarely left his place of birth and was famously not convivial, Spengler lived an externally anodyne existence while his mental interior shone incandescently with original ideas. He worked in an age of Prussian militarism and German nationalism, of extraordinary revelations and dire events, when the tumultuous tide of uncontrollable circumstances invoked an apocalyptic atmosphere. His thinking therefore can be seen as an illuminating response to the blindness of his time. His magnum opus,&nbsp;<em>The Decline of the West</em>, was a rejection of Eurocentric versions of history and a repudiation of traditional structures of scholarship. He deemed the historical phases of ‚Äòclassical, medieval and modern‚Äô as inaccurate, random and unhelpful, as a cracked encasement of human activity. He did not want to appraise the past chronologically and solely link cause to effect. He wanted to examine what was common and unique to cultures across the world, what the nature of mass existence is, and how humans see themselves through history, if at all. He called this new approach a ‚ÄòCopernican overturning‚Äô. For the study of history, it is as seminal a moment as the Newton‚Äôs shift in Physics or Descartes‚Äôs dramatic declaration of ‚ÄòCogito ergo sum‚Äô in Western Philosophy. This drastic development he hoped would forever transform humanity‚Äôs elemental understanding of history. However, his obscurity seems to have precluded his influence on the public and his theories remain widely unread outside the insular circles of academia.&nbsp;Perhaps the pessimism he espoused makes him appear intellectually unprofitable and therefore unattractive to the merely curious reader.&nbsp;</p>



<p>In&nbsp;<em>The Decline of the West</em>, Spengler‚Äôs ‚ÄòCopernican overturning‚Äô led him to see past eras as loose biographies of isolated cultures. He applied the seasonal system of spring, summer, autumn and winter to the evolution and undoing of every prosperous society. They rise, flourish, wither and vanish like distinct wild flowers on a windswept heath. Spring naturally represents an awakening when the force of cultural expression is so strong that it sets a precedent for centuries thereafter. Summer signifies a stage of pleasing artistic production and clever imitations when creation becomes a personal activity. Autumn is when the soul of a culture depicts its happiness and attempts to return to nature, and winter means the solidifying of culture into a civilisation, when great art is mostly extinguished and the creative energy that drove a culture on is almost entirely spent.&nbsp;</p>



<p>Although there have been innumerable births of culture, Spengler argued only eight ‚Äòhigh cultures‚Äô have existed: the Babylonian, Egyptian, Indian, Sino-Japanese, Mesoamerican, Classical (Greek-Roman), Magian (or Arabian) and our own, the Faustian. Classical man was more concerned by the near and the present, whereas we children of the Faustian age are forever looking into infinity, searching for the ultimate end of our speculative powers. Sadly, he claims that the Faustian age is the most tragic because although we enjoy unprecedented technological discoveries and rapidly strive and create, we know deep down that our goals will always elude us and that our efforts will eventually be proven futile.&nbsp; To add to the dark tone he used to describe our time, he believed our culture was now in its winter phase. Lawrence Durrell opened his Alexandrian Quartet with the beautiful line ‚Äòin the midst of winter you feel the inventions of spring‚Äô. In Spengler, this ‚Äòfeeling‚Äô is a remembrance rather than an anticipation. The time of birth and rejuvenation is forever finished, metaphorically speaking. In this present phase, Spengler‚Äôs prophesising limns an authoritative ruler or ‚Äònew emperor‚Äô who emerges in response to the disintegration of culture. He called this the Caesarian age. Many Spenglerian commentators have identified the advent of Hitler and Nazism as an example of a Caesarian instance in cultural history and it would be an easy feat to find comparisons in politics today who fit the description of attributes Spengler so eerily expatiates.&nbsp;</p>



<p>Spengler‚Äôs approach to the philosophy of history ‚Äì to how we see history and what meaning we can derive from an appraisal of past events ‚Äì is informed by a myriad of unorthodox intellectual disciplines. Morphology, the study of the form and structure of organisms and their specific structural features, determines his perception of eras and their attributes. Mathematics is used to reveal the rational divergence of different cultures and to prove the sociocultural relativism of subjects that supposedly seek certainty. Art, economics, politics, literature and architecture are each discussed with a rare authority, but his factual errors and historical inaccuracies have galvanised his critics and stunted his appeal. However, Spengler‚Äôs holistic discernments distinguishes him from other philosophers of history and raises his status to a laudable level of exceptional intellectual skill. His analytical insight penetrates more deeply and disturbingly than Toynbee and his Nietzschean prose and encyclopaedically varied knowledge rewards his readers with a weird and widened perspective. Like most great thinkers, it is not the bleak conclusions he draws which are ‚Ä¶</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://engelsbergideas.com/portraits/oswald-spengler-an-intellectual-life/">https://engelsbergideas.com/portraits/oswald-spengler-an-intellectual-life/</a></em></p>]]>
            </description>
            <link>https://engelsbergideas.com/portraits/oswald-spengler-an-intellectual-life/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24693655</guid>
            <pubDate>Tue, 06 Oct 2020 00:30:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Probability and Statistics with Applications to Computing [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 66 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24693589">thread link</a>) | @ArtWomb
<br/>
October 5, 2020 | https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf | <a href="https://web.archive.org/web/*/https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><i d]g(*h¬™¬Æjc≈°¬ª‚Ä¢√ü√ï]¬ø¬ø}[x√Ø^√Ær)≈†f3@="√ØJ¬ß‚Äúccc‚Äì/_^RRbs‚Äî¬¨m√à" ;√Ñ√ß√èe√°√ö?¬ß‚Äπ‚Ç¨√ë="" p8]x¬±<√Ö<¬≠}√ü√¥‚Ä°√ô‚Ä¶‚Äπ!kim2≈†‚Äπ‚Äπm1c√Ü¬°‚Äì‚Ä∫√è¬∑yyyb¬°'ÔøΩÔøΩa√Ä‚Ä°kd¬°8√ª√ª¬£‚Ä°="" √∂√ò¬Ω+√™√¶ÔøΩd√¥f√î√†:q√†eu√äju;~√ó¬®)?l7¬•="" √Ç="" √ñ¬Æ[bc¬£vciq√≤√ä!√°2√ú√ë√Ér√∏="" √å|√±√ù√™`‚Ä∞-¬ªÔøΩ8&7≈æ¬¢√É√Ω="" √Ω¬≠¬º‚Äû_√∂√á≈í)≈†√á3‚Ñ¢.d¬¢√ßl¬•&n≈ìx√ª√∂√≠'o≈æÀúo)¬≠√äÔøΩ‚Äú?y√â≈†vw-j√æb7rÔøΩ√Ö¬ªs="" ‚Ä†ÔøΩ√ßh<b¬¶i√¨¬≤√ì‚Äûa-√•p√ãm√â¬¨y-g="" "√¶¬≥g√è6n√úh¬°ÔøΩ\]¬∞x√é√Ω√∫zÔøΩ]g√≥‚Ä¢‚Ä†="" m√ä`4√ù√Æ‚Ä°qp;i√¨1‚Ñ¢√¨√¥≈∏√ø|√•√§√§√ë¬´w¬Ø#f,y¬≤√§√¢√Ö‚Äπ.mq√ï¬¥ÀÜÔøΩ¬£`‚Äπ√Ω√É≈Ω√∞¬¢√ü7a√Ø√ã√ô)√ës√î≈íh≈Ω,√öx√ï√ñ√≤(j¬Æa¬¨j√ãk¬≤y≈ìu√´√ñ√ôb√é¬°c-7≈∏o√≥√≥√≥√∑√¨√ô3y√≤√§&m≈°√∏√∏x‚Äú√âd"√ëÔøΩnw‚Äπq√©≈í√°√É¬º¬≤‚Äìn√õ~√∞@√å√ô√ì√ä‚Äπ√ß√ï¬π√ï9√î'ÔøΩ+√∂√Æ≈Ω=""><b n¬∏1¬†¬±√ú¬Æ3√î%√ïe!ÔøΩj¬°¬´≈í√Å¬±ÔøΩ√Ücv‚Ä∞ynglbpaÔøΩk¬Ø‚Äû¬´c√≤,¬¥√•ÔøΩ¬£(≈†√ú√ä"√ç^¬Øi√Æ2¬¢^?√•‚Äò√óue‚Ä∞ÔøΩm‚Äû√â√ù^√ès√è?¬ø}√áv√∏‚Äì.Àú_`6z√û√ë#√ê¬∂√Åo¬∞√Æ≈Ω;√Æp¬©d^¬Ø√¥¬´e√Ö\√ã¬∂√£√õ√ç≈∏|√Ω¬±u√±&√ó‚Ñ¢√ãb9%‚Ä¶n)√ò√§z¬µ(¬∂¬º'¬ß¬†o√Ω√ö√¢√≠w¬¨≈æk="" √ºau¬∏‚Ä∞;√Øpv‚Äù‚ÄúÔøΩ@&√´.‚Äòt‚Ä∞¬∫="" ‚Ä¶¬∞√±√Å√Ø¬¨x¬±√¢\√•¬§e√≠≈í="" n√ºÔøΩ?√æxe‚Äô√ù%bu√î¬•≈Ω¬∞j+r‚Ä¶¬§b_‚Ä∫√Ä√úu√ò.w√™‚Ä∞ÀÜs√¶√êimzua@j√ë√à≈ì¬°≈Ω√™.c!Àú√úÔøΩ√õ√õ‚Ä°∆í√∫=""><s ‚Äòh¬™√∑*^√≠√ï¬™j≈æi[j¬Ø≈∏!¬±¬º≈°gy√§1<`√Ä√ï="" √ú8(¬≤gp!z$_√µ√¥≈í√ªk√Å‚ÄùuqÔøΩ√†√¢t‚Ä°√ëh6:p<‚Ä∞√£√ó√Æ≈íÔøΩ√Ä√û$bx√Ä%¬µ¬®Àú‚Äò}h="" !="" l‚Äò"‚Äû‚Ä°b!q√ÇhÀÜ@b"ÔøΩp¬ºze‚Äö‚Äò≈í‚Ä¶ic"ÔøΩ(¬µ="‚Ñ¢41" ¬°¬§!¬Ω≈†l√Ä√à√å¬´fip7√±¬º√™√æ¬ø√ª√â‚Ä°¬Ωs√¨≈æ≈æÔøΩ¬†k√°uÔøΩ¬§√ë¬ºj9√î3√Æ‚Ä∞‚Äì√Ø√º√ø¬•="" w√º¬§√∞√∫+n≈∏f√≠√ïc√é.√∑≈ì√∑√°√í‚Ä¢‚Äìs‚Äöhh¬Ør‚Äö√éui‚Äôx5‚Äì4¬©¬º√ä¬´ÔøΩ&{¬Ø="" b√ö√ò‚Ñ¢e√∂¬´ÔøΩ‚Äì¬Æ√≤¬Æ6<u¬∫q¬µ√Ωv="" √Ø‚Ä∞o√àco≈ìuo√≠¬∑√®s="" ‚Ä∞√ßu&√ªÔøΩoÔøΩ√¥√™Àúw≈°]tc4gÀúwii¬¥‚Äôgz‚Ä¢'√±√™¬±ÔøΩm√Ø="" ‚Äπ≈∏:fb¬™√∞*="" ;k¬Ø√≤¬¨¬Ω√äÀÜ?√û@≈°¬æv√ß√π√≥*d^s≈Ωi‚Äòf√™√†u¬∏‚Ñ¢√é¬´√™√î√ï¬´∆ísxv¬Æx5ÔøΩ<√≤√àc¬∏√à√í"√è‚Äπ√º√à‚Äù¬™\√°√•p√Ω0√¶y0√£e~¬§|&¬Æ]∆í≈ì.¬™√ì√ª¬§ÔøΩ√®‚Ä∫a%‚Ä†h√∞‚Ä∞="" ≈°d!¬®_<‚Äùf∆í√ª‚Ñ¢="" √Åe‚Ñ¢a`l%¬§:i√Çq√¶h‚Äô√Üh\r√±‚Ä∞≈íb¬¨≈Ω&‚Äû√á‚Äú√Ühhli√Å¬´√é‚Ñ¢3√¢k√ª¬æ5<¬Ωj}¬∫¬©√´ÔøΩ¬ª√§kn√π}q√•√ªi="" ¬Ø√é¬µ≈æu√è|√ü¬∑_√°="" i∆í¬©¬°¬º'ÔøΩÔøΩ^√•="" √¶u‚Ä†q¬§¬©√û¬´b√ùg√Ø‚Ä∫√ñ8√±√Ödk√§g√é√™‚Ç¨ÔøΩ^¬Ωur√•%ÔøΩ'[-8t8!="" f‚Ç¨,¬Ø¬æ√øi√•qÔøΩ√áo√≠¬≤i¬Ø¬™√º√©hr√§√ïp¬Ø*≈ìdzu√çc%√∑t√∫ÔøΩ√ü√µk^¬≠"ÔøΩh0j√™ub≈°*¬´√†¬æk≈∏¬¨¬∞√¥√ÆÔøΩ√Ö‚Äù‚Äû√Ñ√≥¬™≈†$s¬Ø‚Äô‚Äûh7d√©¬øÔøΩ‚Äôx^{d√Æu¬±¬∫aj^="" ¬Ω≈†o2√â#ÔøΩ<√≤√Å8∆í√â√õ_pj="" n="" ‚Äπ¬•,j¬™√•¬†="C√´√•" √Ö="" ‚Ä∞¬®√ódq#√≤¬πz‚Ä°p="√ï¬´¬ºD‚Äö√∫3#¬°√äk!‚ÄôHWa¬≤√ß√í√ê‚Äû&nbsp;≈í√∫{6!" ¬¨y√å.mÀÜ2¬¶√©aÔøΩ?¬™√±*¬´√±¬ºj}¬∂¬•√≠w¬∫g√ô¬≠¬ø¬∫p¬æ¬¨‚Ç¨√≤*=""><i ‚Äπb#√Ö√Ñ(c≈í##e∆í*m√º‚Äúr≈†‚Ä∞‚Äπtx∆í∆í√†@1¬∏√ú‚Ä¢b√©√Æ√â√•√ì√†√∫√†¬∏‚Äπi¬ßr≈í¬§√†pm√Ü]≈†‚Ñ¢√öx="" ‚Ä°√Æ-&ÔøΩ¬©="" b‚Äû19[khy‚Äù3∆í¬ß√á\‚Äπ6‚Äû"'?≈í‚Äút√†¬¥1%‚Ä¢‚Äì√ºejq9‚Ä∫kÔøΩr¬¨∆í#="" √Öx="" ≈Ω¬Ø¬¥98√ó*="" ¬•x="√ó√Çfb&quot;#E√ü¬•" √≥j¬∏√ü√Ç@‚Ç¨√ù20g√å5≈†√Ä√Ç‚Äπ‚Ä∞¬¥√ák&‚Ä†[¬∂√¢¬ß9p="" m8le="v]√ê¬≠‚Ç¨$=ÀÜ√∂+¬§" =""><i √än<y¬æ"+x√ío%bwÔøΩo¬†v!√¶√á="" <a√´√Ç√ìj‚Ä°ÔøΩ¬¥‚Ä¶'¬†`xz(h≈æ,="" b√îa{√±¬¨g‚Ä°¬°‚Ä∞¬£:="" ov‚Ä∞√åco¬∫¬¥√óof‚Ä°!(‚Ä∫√ùxz‚Ç¨¬µ¬µ√èjkk√Ω√Ω√Ω¬±.√¨g¬£i√ìff¬£√°√ß≈∏op¬®√öpz¬∏p¬°√âh"≈∏ÔøΩ`√É√Å√ë¬±¬†¬∞√†√æ∆í√ª¬ØxÔøΩ¬≤√°="" ¬Æ√ècÔøΩjk≈∏<~¬ºl√Æ¬§√î0u=""><i ¬º,<-;l]?hÔøΩ="x√ô‚Ä∞'¬•`√ùÔøΩ¬¶√Ω‚Ä∫√∫∆ív√ì7¬±T≈æ?;¬∫Mb7wE62" g‚Äù√îj<b‚Äî.kwg¬∫+√ö√∫¬©√úcx√öx√ß√∏0e√ß="" c@1√™Àú4‚Ä¢√©o√ä√∞3v√≤√ï¬∑√±s√Ö¬¥‚Äù√∑‚Äò="" rd&ÀÜzhÔøΩpf9ÔøΩe√Ä5√ä‚Äú≈°&√¨)¬¢‚Ç¨k@yp[qh¬§="" √ú‚Ä¶‚Äπ√¨√ç√áf4¬∞ikahh#&¬∂*√Ñ_√Ä$cp√•!√ï¬Æh√ò√Ç}ÀÜ√ñ="" z¬æ√ús√Ø¬†spd;¬®√Çl=""><i √Ç'@√à√¢¬∞ji√∞$*ÀÜ_¬≥z√°p√Ω√õa‚Äö√Ø√Ä‚Äúj0]√Å‚Äú¬†="" cuwa‚Äû¬¢¬†¬≤="" √•r4uj√ü¬ß¬™√Ñsj√∑m√∞¬§(√òu<5j‚Ç¨¬Ω√Å√¥4√ÄÔøΩe√º√Ωf√≤$="¬¨-√±√∑7" ≈æ√è¬Æ√ç√ú√°j√µ¬ª‚Ä¢.√Ü2√ú√åx√£u√¨|y¬£¬¶√™¬π√∂√£xz√≥<√ª¬°√õ4√à;5√ä√∑≈æfw√ä‚Ç¨gj√Ä,c√¢d#‚Ä°5<‚Ä°e\¬∑√æ&.√ñ√º‚Ñ¢¬¶z3="" y3e¬Ø√±¬ß√â&&l√°joj√õ;¬±m√Ω¬π√É‚Ä†h∆í√†|¬≤=""><i √¢√Øa5√Å‚Äú¬¢="" √è√ô√ô√ª¬°i:aaa¬π¬π¬π√øf<u≈í¬¢√ú`¬∫ÀÜ¬ß‚Äùfya‚Ä¢xj¬¶="" ‚Äö√ê(xwww√©√í¬•√•√ã‚Äîc3¬ª√ª√ª√∏√°#¬≠‚Äö]√Ä‚Äú¬®`√ó√±d√àxr%√¢≈°√Öil0√•o‚Äî√∞‚Äù√í√°iuÔøΩp‚Ä¶$<¬øe√º2x=""><s k‚Ñ¢-s√≤≈°√¶s\√øh%=""><i ∆í∆í‚Ä¶="" ¬®="" ÀÜf√Ñ∆í√´≈°√°√øn√´1≈°$‚Ä¢√°,‚Äî√Å√ù≈í‚Ä°√Ä'≈∏nt√ú√ã√†="" √É¬¥3<√¢√πÀÜ¬≤√óÔøΩÔøΩ¬∂√ÇÀÜ¬∫√ó#‚Ç¨e0‚Äô‚ÄôÔøΩcp0‚Ä¢√±="" Àú¬¨¬ª√ç`="√Ü`+≈í√Äv3FÔøΩ¬´√Å0E|√∑2¬∏√ç`√å√∞√Æ√î√πHQ√≤98‚Ç¨‚Ä¶~√òv`¬¶√≠√Ç√Än¬≥¬≠√¨√õ¬ºk4U√ºk¬§w√°¬≥≈í‚Ä¶ÔøΩ¬∑√Ä√∞¬Æ¬π"><i><s ≈í¬¶√ú7√ã√àsÔøΩ\¬†‚Äìx≈†¬µ(`="" √ûÔøΩ√ø√®z√∞√∫~√¨√øtz~t√ô√ãcÔøΩ¬¢¬†¬∫≈ìj%ÔøΩ√π¬æ√Ö‚Ä¶≈í‚Äπ*ÔøΩ√àr√≤‚Äù@ÀÜ|)p√π√Ä√ú√ç√ñ4√≠√à‚Äò#.≈ì<y2¬ø√å√à√à0="" √Éw¬ø√è√Ω¬≠a¬¢="" √ò="" ¬™="" 0="" [ÔøΩo@vr‚Äò√ïj√Å¬£ÔøΩ‚Äì¬´‚Äô√õ√º≈æ¬¥√±¬≥="" √áÔøΩ(¬∏ÔøΩ="" c√Ü√•e{¬µ√Ädfb√ør="" sÀÜ¬•√°√Ö="" ‚Äö="" ‚Äî="" .0√Ñr≈†¬∞}¬≤;‚Äì¬¨¬∂‚Ä¶a$`√ò¬∞j="" √ú√ëy{‚Ñ¢¬™[z¬∑="" -yq√ìpÀÜ√ó√ã‚Ä†rcl*ov≈Ω¬£¬¥;v¬Æhb¬®r¬π√ç√è√â√ây¬ªv√≠√î¬©sg√ç≈°¬µc√á√ì4ub¬†√ä√∑√Çb]2‚Ä∞j`¬¢¬°ro,xp√•‚Ä¢w~√∂√ôg¬π¬π¬πg‚Ä¢√∑√≤¬ß:d√º#√ö¬¶m‚Ä∫¬∂m√õn√ö¬¥√©<{√ª√ï<√ò√≥√∏√øyyy√º√∑¬™=""><b><b ÔøΩb√â√∂vÔøΩj¬ß√Ö¬∫d√ÄÔøΩ8√ág√Ö.k\‚Ä†¬®‚Ä°≈í="" =""><i ¬¨≈Ωq√î¬•w¬¥√©:‚Ä°√µÔøΩ∆í√≥√≤_f√ß:¬ßu√í¬∂≈°¬™7¬Ω¬æ√¢fr√∏¬π√óe√∏≈†¬´√®5√±!_¬¢‚Äòq<d√±√°e√Ö=""><s ‚Äû√ª¬∑tb!a√à√ß√π*`lze;‚Äπ√ó5√ër∆í√∞¬¨¬∫√ê√åz√í√†¬∏gxl¬º√Ädiy√ª-y¬≠;o="" √é‚Ä†i√Ç+j‚Äì¬∂√¢o\^√•√äu√Ç‚Äúo*1`h≈æs√õ‚Ç¨="" ‚Äì¬©e√ävie;¬ø¬§√∑‚Äù√°¬£¬©√ñh‚Ñ¢c|√â√ô¬®b¬£p‚Ä¶edÔøΩ-√≠¬®r9¬π√ú=""><i ‚Äô√Æ√£q≈°∆í√¨}="" ¬Ø5√Ä7-}¬£√∞gzf√®√µ4n¬º¬¶kxÀú‚Ñ¢~≈ìcri¬≥¬Ø√àÔøΩg@√Ö¬¥)√Æ1‚Ä∞√æ_¬ºalsb√∑ÀÜk=""><b ¬≠√ä√´%jtÔøΩ="" √ï√ß¬¨,p1√Å√Ø¬æ√ªn]l="" hj‚Äö‚Äö‚Äö¬†√õy√õ¬∂m¬†_¬Æ√¨¬Ω√±√Ç¬∏="" ="" ¬ØÔøΩ√∂whÔøΩ√Ü√´¬†≈ì√†√∑g√Çnj2}¬≤?√°¬ß](‚Äûp√Üa`‚Ä†c¬™√≤√∞‚Ä∫√±√æ@≈Ωd="" t‚Äì√∏,uty√ä="" √à√∫√¨izzj)√∞‚Äú‚Äô¬ßiy‚Ñ¢yb‚Äì√òluy‚Äì≈†="" ≈∏=""><b><i o*d¬¥,w√ég¬∑¬Æ√Ä+≈ì¬∂*‚Äö‚Äπf\y‚Äû="" ¬´'y√®√è√ø¬•¬®¬ø¬©≈ì√ø‚Ä∫¬¥√é√øro√´‚Ä∫¬Æ¬∑5¬æw√´eq√è√Ωy‚Äù#√å‚Äî√π√Ç‚Äπn√ß√Ö¬¶p√∂√£y√∫ÔøΩ√õ,√í√ú¬≠√ù√ºn¬≥y√ñ√û¬≤≈Ω√ú="" 7√és√§="" ¬£√≠b√Æ&2x¬´√é‚Ä∞zq√∂√ö¬¢q≈í+¬°:√¨‚Ä¶]‚Äû!√î‚Äú√ú{√à≈Ω8^√á=""><s><b √Å7Àú√≥¬∏√£√ë√á√Üa$211y¬ºhqbb‚Ä†√©p√ò1*≈ìÀÜ‚Äúg0^3g√é:t√Ñ¬æ√Ω√ª¬¢¬¢¬¢√äÔøΩ^ÔøΩ¬≥¬©i‚Äú¬¶√§√™ccgggÀú√ù√π‚Äô√º√Ñ√Ñ‚Äû¬∑xq="" ‚Äû√Ö‚Ä∞*)‚Äô≈°¬∑}√ª≈Ωv√ü?z√î√®;w$‚Äô|x‚Äìhp√êk(√ø="" 9√π0≈í√ó√∂m√õ√´7hÔøΩy="" √è#√Ä"r¬©‚Äù√£√¢z¬∑jmhÀÜ="" √Üh√ô¬≤√•¬πs√ßÔøΩ¬•¬¨√¢√ç|¬¶√≤‚Ä°√å,yÔøΩ√¨≈Ω‚Ä°g¬≥√¶√ç!u‚Ä¢¬™u‚Äî√Ω¬µ√¨√ù¬ªw"4√©√∏@√Ä-√ª[-√ø√∑?rim¬ø√æ√Ω√∞√Å="" ¬∫t√ê1b∆í_¬ß√Ü√º√©u√Ä"‚Äî‚Äù‚Ä¢‚Äú~√õ?j√ç√•‚Ç¨√Å‚Ä∫√ú√∫¬Æ√É√ó√ü≈Ω√ª√õ√≥ÀÜk√∞√´√®√¥\‚Ä∞¬¨@¬Æ¬¨¬∞√Ωv√Ä√ú√îcx√è`j="" ¬ª√Ç@¬¥5f¬®b‚Ç¨≈æa="√ï‚Ç¨√æ&nbsp;¬æ" ¬ßh3Àú√ß‚Ä∫‚Ç¨≈†¬Ωlh√ì="" √Äo`√¢‚Ç¨s!¬Ωcq√≥Àúx√ç`√õÀú="√ØÔøΩ¬≤√åP'ÀÜ" ¬°√ïng√¨kvp¬±≈∏‚Äò√∑≈∏_√î√≤‚Äπ√è¬†√å¬≤√Ö√ñ3≈Ω=""><s><u><i s√ùs[≈Ω="" ¬¥‚Äö="" mpn¬¨i¬©√∏8¬¶¬∏_¬™s√∑w4¬¥:¬Æ∆íva√¶="" √í.v‚Ä∫b√≥:!¬Æ‚Ä†?j∆íz√ò√Öyr√¨√Ñ√°√®¬ß√£k√£≈æu√ã√µf√ã¬∏¬†yz√Ø√¢n√ã="" ¬´="" n√©√¨√ä√ÅÔøΩ¬¢‚Äπ¬ø¬ø√ô="" q="" gwh√ö√¨¬£√•z‚Ä¢√†‚Äπ‚Äû¬≤‚Äú¬π¬≠√ù√£^yÀÜ="" t¬≠≈Ω¬ªxhu="" ≈†5¬¶c¬∑‚Ä∫=""><u><i h√¶√®√ô√ü&≈†√™="" ¬¶m¬§‚Äô)‚ÄôÔøΩ#cm‚Ä∫5kÀú√¨√ëkld¬∫¬∞)u+¬°bt‚Ä¢ÔøΩ="" ~yt¬®√ñ="√ê‚Ä∞zUi√®%ÔøΩ¬¥¬∫T‚Ä∞‚Äö≈æ‚Ä¶" ∆ín√¥ht‚Äû)w¬´√åh√Ö√ê¬ªwc¬∑√ë%√Ñ√•ÔøΩ√Ä="" √ï@="" √®√ûb√®√æ√ù√õj√®√ºq4tx‚Ä∞ÔøΩÀÜ√Ø:%.¬®¬¨{√ãb)√Ülyj‚Äò√£¬†w√ë:¬•]u¬∫{i9#n√®√ù¬´b√è.√®‚Äô√çiwa'√™u¬•¬°‚Äî4√í√®r¬•√û≈ìx¬†v√Ø$‚Ä†^on√º¬ª¬∑‚Äπ6'g¬Æ‚Ç¨√µdjrwd≈Ωl‚Ç¨$ÀÜ¬∞c√Ñh√ã¬§¬µ√°*√º≈í¬ßd‚Äòj‚Äúc¬∂j√àx¬•ÔøΩ√∞√í‚Äú|‚Ç¨‚Äù1‚Äö$√å*f4="" |x√°'√¥#rb≈Ω¬∂p#√•¬°‚Äπde√ù+‚Ä†_="" tp‚Äù‚Ä¶n√Ö%≈íh¬µ*kkÀÜ2w¬Ω√òÀÜ:ÀÜ√ê√ê√•√ß"#¬≠‚Äûn`¬≠√§uqs‚Äì√¢u@√∑2√¢‚Äû^√éÀÜz≈æ√∂ty¬ß√∏=""><a √§‚Äπ≈∏¬≠9ÔøΩ1√Ø.‚Äú}(‚Äîx‚Äôd≈Ω|w√é-≈í√ä√É‚Äú4√ød¬¶¬º[b="" ‚Ä†¬©√πo0‚Ä¶≈ìly¬Æh¬ß.√£.√è√∫‚ÄπÔøΩ!√ñ¬©‚Ç¨w\√Ä√Ø√ùv√≤zv√ïc¬∏‚Ä°,j√ß√æ¬ª?√ú√¥%]¬£u√ò√û√≤'v~t√ò¬µÀúm¬¨√∫√çf¬º√Ö="" √ö¬°¬®y8j≈Ω¬ß¬∏.=""><u √Ω‚Ä¶ÔøΩ√Æ√ò¬ø?v9√•$;bii"i√ö√ü√üÔøΩ√¨d¬π√Çe$&&≈†¬§*¬°√ó(‚Ä¢j;;;√¨b√ä√®√ël√ö¬¥i$iy?t√ó¬Æ]√òy√≤√Ü√ô√ô√ô√ë√ëq$u="" √Ω¬•¬≠¬≠md¬¶ad$v="" √•mxx8$√û√ú√ú,‚Ä†¬™√±√±√±√∞√¶'n≈ì√Ä√é‚Äô7√ë√ë√ël6¬∫¬™√∫knn.≈í#g≈Ω`‚Äîp√û¬∞¬©√ú"uÔøΩ¬†}√®√§√§‚ÄûÔøΩ"√¨quÔøΩÀÜ1√∏√∏√∏Àú‚Ñ¢‚Ñ¢uuua‚Äîp√ûtvv≈°≈°≈°≈†√ëujiia_ÔøΩ√™√ª|ÔøΩ]√æ‚Äî¬∂¬≤¬≤¬¢¬Æ1≈°≈æ≈æ¬®!!!≈ì‚Äπ"√òzjj≈†¬∑¬∑√∑√ö¬µk¬ø√∫√™+√∏a√ü¬æ}√º¬∑e√≥√≥√≥∆í√¥√Åp‚Äûu‚Ä¢¬≠¬•\ww√á9="" √Ø‚Äπ="" ¬æx√ó]w-[¬∂≈í√ø√ïccc√Ö√´ÔøΩ√∫√´;p√û√ë#!!√°‚Ñ¢g≈æ¬πk√∞kp≈æ‚Äò√§√§√§@√∫vu="" √Ñn|w¬¨y¬≥‚Ä†i≈†√¢3lf√•√æ√Ω√ª‚Ä¶u‚Ä¢√ê_¬†√ìe¬¢¬£¬£∆í[!t*‚Ä¢cy√¥√ëg_√Ω√µ√ó^{√≠¬ß?√Ω¬©√¶‚Äî<√ó¬£`√∑√Ä√°≈†j:44‚Äû√òi:{√∂√¨¬™u¬´4b¬¢√∏="" ÔøΩ="" √Ä-¬†¬™‚Äû√æ2<<√å6√¢v√º√†‚Ä∫n√Æ√ú¬πp√æ|√∞√Å√ë‚Äπh√Åw0‚Äû√Å^‚ÄöÔøΩ="√ä-$¬∂‚Ä¢H!‚Äù¬™√ê√ªcS5¬π¬•&nbsp;!**j√Å‚Äö¬£‚Ä∫ÀÜX">√àO‚Äô√ër√ö(ÔøΩ√å√ºIY√¶√¶t√≤√Üt√ö¬≤:¬ª¬∑ÔøΩ0√àÔøΩ√æ√™√É-‚Ä¢¬£|√í+e¬©‚Ä∫¬º9V9-‚Äö√ésm5‚Äî'¬´√Ø¬≠S√ØoV¬ø?√á8√ó^6√ê√≠√õ+56√©/√ì√Ø¬¨06‚Ä°‚Ñ¢√ß‚Ä¢√µhk√Ü‚Äô‚Äöu¬æ¬≥√¶√ÜR√ï√ù¬µ√ä√ßÀÜ¬´∆í‚Äù¬§T√Ç√£‚Äö¬™1&gt;√¥ Us‚Ä∞√™f√Ç\g√æ√ÜT√≤√Ö√±¬§¬π¬Æ4
√üQ∆í^√¶H¬©9brvg]√¢L‚Ä°√øL‚Ä°√è¬®√Ñ‚Ä°[≈æ√îX≈ì‚Äû‚Äöjm~,=;‚ÄìU√û√ï`¬≠√¢Xv7Àú√ì¬≤}√ã∆í√±√±√à√Ωa¬¨¬Ø6¬©
√ØC\^≈°¬ØÔøΩ3k≈ìM}L¬ΩO≈∏
&gt;1rb&amp;√∏√Ñl√†‚Ä∞√ô`#¬•¬®√µ¬∑`√ïW¬ª&nbsp;¬¨¬™√´¬∫¬µ¬ºBÔøΩQc¬•]¬¨
4Y√è¬™¬ªk‚Ç¨!¬´j√ê¬º√™¬Ω¬ªw^√í¬Ø√ö√û√ï!√¶Àú¬∂√ù¬Ø√∫E‚Äô√©'‚Ä∞&amp;√Ω√°¬ø√å¬™0v≈†√∂.√Ä¬º√Ä¬™√ö¬º*5C√è√Ü0bkr≈°√ä¬±¬Ω√çQ#‚ÄôhvQF+1¬Ω¬π4¬´¬ø%uY‚Äúry.mu8K√Ω¬∫‚Äπv√ï‚Äù.U&nbsp;y√ïks¬¥9%e≈ìO√ë≈°u≈∏k√ü√é¬®¬Æ¬©!¬®p/
6∆íÀÜ√®o≈∏≈Ω√æ√Ø¬Ø¬°√ö√ü≈æN|√ø√Å√∏√ø√ær√æ~^√Ω¬ø[√π√©√£‚Ñ¢G3√≤Gs√≤‚Ä°¬≥¬≤{√ì√ê√≠√ñ¬®√∞√Ü¬®√†∆íU√π≈∏√Å{√ægÔøΩ:≈æ^‚Ä¢~rS√≤√π√âg‚ÄûÔøΩ¬Ø4_‚Ä°≈ΩL√º√™ÔøΩ√ëjp√∞lA¬´¬•11AU_√ñ√ó√Ω¬∫¬ªÀú‚Ä¶√†√Ä≈†√â‚Ä¶√õ¬¨:3¬•≈í‚Ä¢√íp√¨‚Äô√î&amp;pZK ¬´√¶√ÜCV√Å¬º*`UrZ(!)≈í‚Ç¨∆í¬¨¬∫√≠√ó√£‚Äú‚Ä¶\√µ#¬©U¬¥k5¬∞¬™√Ø√≥Q√à√Å.f‚Ç¨U¬ØD√É‚Äò7boD√û≈Ω2Àú/Oz¬≥yU"¬≤~¬±√∏√µk√µu‚Ç¨¬µ¬±‚Äú√Æ=¬ø≈æU√µ√´√üc√≠¬©√ærW^√µ√∂√≠√õ333√ª√∂¬´ba¬ø√™S¬§_√µ√ÉD^ÔøΩ‚Ä∞‚ÄìUu¬£√¨_h¬∏xÔøΩU∆íVE√≤¬™‚ÄûU√ì√¢¬πPo9	¬º√ä¬∏!Ads,¬ß$ÔøΩ_‚Äò√ûR‚Äì¬•¬¶\z√Ø≈°√≥¬ø_|_cdUÀúW¬•]≈°¬¨‚Ä†¬¨*&nbsp;"¬•¬ø≈íE¬≠√û√∂4h√µ√∂√≤`√≥¬Ω‚Ä¶¬∂?√ª√èo¬¶√ø√æ√ó¬π¬ø&gt;√ø√æ√±√∏√ü¬øYz√ª√ü?,√øh√º√ë√¨≈Ω√û"&amp;7G7√á]j√ø√°√ìÔøΩo&gt;√¨√π√¢A√ª‚Äú√ã√¢Oo¬Ωv_√∞p‚Äú¬ª¬ÆfM	h3√¢j√∞¬Ø?¬º√¥‚Äì¬º6V¬°√°‚Ä¢√∑6‚Äù√µ6√†¬ªYEp≈ì¬£@L*√úf√ïQ‚ÄùU‚Äú√ô%)Me‚Ä∞¬®√û√ís√¢¬´‚Äπ]4¬´‚ÄôRÔøΩ√û‚Ä†¬¢z‚Äπ√û√°¬Ω√ç‚Ä∞√≤√í√û
&amp;¬øp7√µ6√ú√ç&lt;√ê√ô,√ü√É√§¬¢¬∑√óco¬ΩÔøΩ4Àú¬®.|¬≥¬¨¬∫_√É‚Ä¶¬Æ√£√∫¬´.t√µV√óq]¬∑√ûeU+++√Ä¬™g√é≈ì√ë¬≥¬™~√ΩA√ñ/¬≤√™¬•√ΩYu¬º,y=√§√î‚Ä¶√∞S¬´√°¬∞Ar&gt;√¥T¬¥¬ΩÔøΩÔøΩ:G√¨9¬Æ‚Äöx√∏yC\&nbsp;SJ¬∞Kn¬¥sY‚ÄôSv‚Äûgv‚ÄûoI¬ºg
√é√è‚Äô1√åk√≤&lt;√ã‚Äú"H¬©a¬§√î¬®¬¢G‚Äπ=¬≥ ¬¶&amp;/≈ΩU√óP≈ím(Nj&amp;$√∂√≥‚Äö√ß{√ºz4√Ç()5CL√éi¬£√¶√≤√í√é¬´cWG¬±√ß√ïI=√µ¬®V√É)CM‚Äû¬°&amp;√¢0ÔøΩ¬∏√êÔøΩ_≈∏(i¬¶N&nbsp;h_√î¬Æ/‚Ä¢√ú8ÔøΩ√ü≈ì ≈æ‚Ä¢B‚Ç¨ÔøΩ√ï¬Æ‚Äúo≈ì/¬ø¬πR√∂√æ√π√≤√µ√í¬∂√•ÔøΩ≈í&gt;#¬¶√è√â¬©‚Äî¬¶K¬Æ/√•_≈°)X√¨.m¬°≈í¬∂‚ÄôFx‚Ä¢≈°f¬¢≈°‚Ä°≈Ω√µ7.‚Äúd≈∏W√á-k¬¢g:√£√§√ï‚Ä¶rZ√∏√ô$‚Äù,!)KP‚Ñ¢√ûQ3¬©√∞‚Äî¬π√µr¬Ω√™‚Äπpu‚Ä¶√êC&nbsp;6‚Äì√∑P3√Å√ìQ2-ÔøΩ√±√üf‚Äù,||Pq\√Ä&nbsp;¬™√ì‚ÄîÔøΩ‚Äö√™√≥dÔøΩ¬´yÀú¬≥iÔøΩ‚Äú‚Ä∞ÔøΩ¬Ω1√ã√´√îD√†√±¬©√Ä√£√£`√ª4T≈†[eoÀúU√∑√ó¬∏√ó¬≠√•‚Äö√ºR¬∑ √à√®pUWWW√õ√ò√ò√®Yu7¬´&gt;√ª√≤√ô¬£‚Ä°ÀÜ¬∑d√ï‚Ñ¢√ô√ô{w√Æ,-.¬æ‚ÄùU√õw|‚Ç¨¬°&gt;√ÄX√£≈æ¬£Y√ï√∫√•y√ïhmMZ@YR`c¬©√ß√¨Wu√®√°@V¬•V√ç√á√î√Ä√ò)vH:*ÔøΩh¬£'u5&amp;¬∂√ó%k‚Äû¬∏√≥C¬∏√çiAeL¬¥√•*i¬´√ñ‚Äì≈æ√´+¬ª¬µZ√±p¬´√™√™Y√äYe√ï√ª≈æ{¬∞Tk¬ª¬©¬ºÔøΩ=√ê≈°√ük√Ω√¶√É√æ&gt;S√ø√¥l√¥√©Z√á¬£√πWw√ø√£‚Ä∫√â≈∏≈æM√º√∏√π√∏√á¬∫LA¬¥√ªSm0v√ù<zi√ª√∂i√ó¬≥g√ä √â≈∏="l√ª√¢¬æ√∏√£√õ√º‚Ä°[√úK√µ√≥2√Ü≈í‚Äû">√ëJ¬ª¬®¬¶√ú]¬Ø¬∫¬∑A√ú≈ì$≈í‚Äπ√ã{¬´¬≤¬µ¬¨≈°/¬©*kKE√∫U¬£&amp;√§	√á√Ü¬ß5‚Ä¢¬°¬¨√è√àMÔøΩ√ó√∏#√Ω¬™¬≤7√à¬™¬∏√ß¬¨√∫BY√ä¬™√Å√é/e√ïg√ìÔøΩ¬®3¬®P√é¬´gfJ√Å	‚Ä¢¬ΩQV√ù5X
¬≠√ò√õ¬¨¬∫¬∑yj√Ø¬§¬ø√ΩKi√è¬©√∂≈æ√ü√é√éN√è¬™√∫√µ¬ª^¬ªY√µ√ã/&gt;G√ªU√ª√∫√Ä√ô\X\¬º}√≥√¶√ô¬≥g_Z√ú√ü√å√º4≈°¬Ω√Ω$√Å√§A≈ìQs‚Ç¨√±√Æ¬º¬™¬Æ√û‚Ä†¬πi√µV‚Ä∫W-√á√≤*√ù√ï¬≠¬∞_U√â¬¨
¬®-‚Ä†UM&lt;√Ñ√ï1√É√¢P¬µ J√ÜH√™lH√™¬®O√£VGq‚Ä∫3)F24‚Äö√º≈ìM√¢≈∏Uu√π¬Ωu√¢√ÉKU[‚Äú‚ÄùiiÀÜ|√ê6UTo¬´¬¢z¬ª}78√ê|gN√¥√≠√ì√æ&gt;√ó√º√¥√ï√ò¬£%√π‚Äπ≈†√Øk~√æj√≤√ßg‚Äú√Ωp√¥√©r√ß∆í¬©6√Ñp√ë√õQ√°√ç1√°‚Ä°‚Ä∫√≤o&gt;|¬Æ¬∑≈∏√ü}|‚Äπ¬Ωi]S√Ç¬ßi}ZR¬Ω1
'¬°√Ö√ìh√π6¬´6√Æ√å√≤c@¬ΩÔøΩ√©√Ä¬≠¬®c/G!cw‚Äô9√∏T√é6¬´√Ü√ìs¬ª¬Ω4"'¬µ√êAP√•	Y√∑"¬´b^dUDo√∑¬≤j¬Æ¬ª√âf√î‚Ñ¢‚Ä∫√ë‚Äî¬¢√é\‚Ä∞<s-√º√å(¬µp√æ¬∞√™?[o¬∂¬∑y√µ√µf¬®√©:√ê[‚ÄùumllÔøΩ¬´‚Äö√´yu¬ø√æ√´_fu¬π¬¨m¬§4y9√®√§r(√úk√°¬ß¬ß‚Äöod√ò≈æ√∂¬¥>√£og&nbsp;‚Äπ¬´√ö√¥
√êÔøΩ‚Ä°¬§@'B¬≤¬∑√ú23√Ç5-√Ñ¬ª0√éM≈†¬∞¬™ÀÜjA√â√¥(‚Ä∞‚Ä°√æ¬∑√â"¬™ÔøΩ‚ÄöiSÔøΩSÔøΩ‚Ä¶a√§√Ü‚Ç¨√ÄÀú‚Ñ¢√üP¬Ø¬®
≈æV¬∫O¬∑{‚Äπ√ΩEU√â¬≠√ÑAeV;+e¬∫#ri0t¬±?¬≤‚ÄîÔøΩ√ô√ΩR≈†¬∫Y¬∞√∏PU_6√Ä)ÔøΩV√§≈ì√à√®k ¬©!]V.√µlNgl√çd¬Æ≈Ω¬°#√É√∏‚Äù‚Ñ¢6√¢√ÜD√Æ√ñl√ñ¬•¬≥YG√≤gd√Ñ1u\Àú‚Äù2!"^√â√úÀúJ¬æ0‚Äô:-/`W6‚Ä¢√Æg√ÉKB√∞¬Ø√≠√≠¬®)√©√ßf√étD.√∂M)Cd¬¥LQU≈Ω√±f√ß‚Ä¢g√∞‚Ä∞¬∏√∂¬∫√†Q)P]')√ç¬ß6/¬©6^‚Äì¬¢√û√¨√§¬¥HzN&nbsp;≈ìa√öUo√ñYgF√é√∞.≈†√≥/√Ä√∏√¶√áx√Ø‚Ç¨√™vF¬ß√ì¬¶¬∫SUh@5√Ñ√â¬±g7bx≈ì√∂?:√¢l√à√Ø¬®√Ü√ß¬®√Ü√ü@√±√¶XU√∑√≤p¬Ø{√Ä√ûqÔøΩ{√†W¬ª¬µh‚Ä∫UuY√ç√ó≈ì<yr√è¬™;y√ïgh^uzxx,v¬Ω{√ß√é√¢ky¬µ¬£¬´c√Ñ¬ø‚Äúd√øÔøΩ‚Ñ¢√∑√∑0√û√®^≈ìqg¬∞‚Äò‚Äî√É¬∂¬∑√öt√û√ôasy‚Ç¨#x¬Øg¬Ω>√ê¬¶rtrQ\`ib`√ûs√∫√õu¬≥ÔøΩ¬®‚Ñ¢√∞T√ñ√§a9√±r√¥‚Äû&lt;`E¬∞:¬º&gt;¬∂1¬π&gt;sa{q,im<e¬∫=¬´ÔøΩq√î n?√óÔøΩ√ø\√©√ù5√Ç√•√ô√ä¬≥j√≤(ÔøΩ2-ÔøΩ√Ωs="" ¬™y‚Ç¨‚Äìj¬¨√≥;√ç¬™√´¬Ω≈ìÔøΩ¬æ¬¶¬´#√ç_√úw|√Ωa√á√óÔøΩ;√Ø√çi√û|¬∏¬Æ√∏√∂c√ïw√∑~√∑t√µ√©‚Ä¢√∂‚Ä∫√É√Ç‚Ä∫ÀÜ√ª√ôÔøΩ1√ö¬∏zwi√∞√Öc√©g√∑do¬Æ√∞="" ÔøΩ¬≤7√î√µ√ª√´‚Äì‚Äù¬µg%≈í¬≥r:√ï√Ü[¬´‚Äîh¬∑v="" ¬∑="" ‚Äì¬Æo√†√áde¬™¬∫r;u√Ø√åa‚Äìt≈íiq‚Äπ1√ß‚Ä°"f%1b2‚Äôw-√ù√â¬´√¶$√à¬æ="" ‚Ç¨t√¨√õ¬Æ√§√î]v-|}vu1]ÔøΩ8u1@[‚Äπ8¬Ωvj≈∏√∞√õ¬±√™~¬±‚Äú‚Äìuu√ç¬∫_√ù<¬•k√¥¬±‚Äîua√¨¬§gu√Ω√∫¬Ω¬Ø‚Äî√§u!¬´^√Æc¬º‚Ä¢o¬Ω‚Äùu‚Äò√π¬™√Ω\√¶‚Äúh¬£√ªqfw√¢≈í="">@√¶¬´r√º≈í|_n√¢‚Äò√º√¢t¬∞√ò¬Ø√öBt√§A`E¬≠)5¬∞*bÔøΩ‚Ä∫√ê√ÜÀÜÀú√©zxa4d}"√ë[√å‚Ä¶√ë√∏¬µ√±¬§‚Äπc)¬£‚Äô≈ì≈Ω≈°¬¢.VIO=~¬©√ªB√©¬Ω√µ≈†√ç√â√ä))y¬º√®-4@√µ¬µT√í√ï√õ3√º/(¬æ√π¬∞√≥√´√á]¬∑&amp;D√Ø?¬π√ö√æ√≠S¬®¬∑√ü&lt;√Æ√πhSqC#‚Äû√ÄÀÜ√û^√ì¬¥^√¶?\¬Ω√Ω√¥≈Ω√®√ë&amp;oc¬∞qS]¬°‚Äî¬µ √ü√ñ[&nbsp;√≠Sb√™
4≈ì¬º}a¬Ø√¶‚Ä¢∆íx√ç¬´v√Ä‚ÄìU¬®¬∑√ì√≠	√ß¬£‚Äî‚Ä°√Ç√π¬±¬≠‚Ä¢√â≈ìX¬ºÔøΩW√çN√®¬¨√∑TC`¬ªV‚Äô√á/¬≥√™N^5√∂EV√çq7¬æÀÜ√®√≠√Ö√à√ì√´√°¬ß7BO
S~#V√ïÔøΩ¬∂¬∑Yuo¬Ω‚Ñ¢n√ãK√µV¬∑√ûL√è¬™√∫√µ‚Ä°]√ª¬∞√™‚ÄöB"√æEVU‚Äî√†√¶√ΩÔøΩ√è≈∏{9√¨√îh√Ä√±P√´√Æ‚Äì¬ß|lN\
¬¥7v4√û5#√é√á√´√´X≈Ω¬≥√•-3√É√Ω¬Ω
cÔøΩetK√õ¬¢‚Ä¶hMH√∂(√Ñ@!¬∏*cÀú√±ÀÜ6¬§√î¬®¬™¬¥√à√™,‚Ç¨r√ë√¥≈ì√ò√ö√º8&gt;)dD√¨8,v√íÀÜ√ú‚Äû√§√ò√¶¬≤‚Äù√¶¬≤t!9¬•¬ø9b¬Æ√á√∑lÔøΩ√øPk≈í‚Äù≈°'¬´√éS√í√≥@√∞¬©d√Äj‚Äú1q√≤√ô√é√∏¬ÆBo}¬©¬™¬æ|F‚Ñ¢¬æ2≈í]√Å√éu¬•
p∆íMC\ √ê√π√∞√π‚Äû√ïQ√¨¬≤:qZ≈æ&gt;3√Ñ-√´k$¬™‚Ä∫K√ª√¢‚Äî5Q‚Äπ}ÀúQf7¬≥≈Ωv‚Ä¶u,EÔøΩ¬µ¬∞t¬∫'U*k2‚Ä†Z#f:=√áe√û√≤,ÔøΩÔøΩ√Ö¬´Hm!$¬≥K√íxDLg¬Ω√ªP¬´ÔøΩ≈†c√üXF√è≈Ω¬ße∆í‚Ç¨<w¬•‚Ä°s√Ç¬©√ô≈†≈°mv%¬•y√§√á√ÄÔøΩ¬™¬∫¬Ωo {ÔøΩ‚Äù√æ‚Ä†"¬†‚Äû≈Ω¬Ω¬∂1¬†¬∫√Ø√∑92√®s¬¥√ó√ªh≈∏√ß‚Äò="">√ü√ì
√ëf√ï‚Äî≈Ωk√ú√è=√†5`¬≠ ¬£¬¨¬™√ïd=¬´√Ç√µK¬¨zg_V√≠V≈†√∏√Ø'√ò√ù≈†6¬∏‚Ä∫√äo√Ü*
√ë
√†‚Äîg¬ªJ√ü√õ:√û√±√´FZ¬®P{%√î¬Ø‚ÄûO‚Ä¶¬±√æ√∏‚Äû‚Ç¨¬∫b√∑√æf√´ÔøΩ√õ√éz{√î‚ÄûO√¥≈ì81-|T√™¬£√≤C5√¢√∞I√îXf¬æ¬≥¬¢‚Ä∞¬Ω8?√õ‚Ä¢√öQ‚Äúp¬µÔøΩ^2)+¬º:Wps¬•hk¬∫tFQ¬°√°‚Äô'‚ÄûU;¬£`√ó√™¬º¬ºv‚Äù#	√ñ√ï√Æ√ÜÔøΩA√∂G7x≈∏√û|zWxs‚Ä†¬∑√ï√ó|s≈°√ø√ë5√ëÔøΩ$≈∏√ùz[‚Äö¬•+C¬º+C-‚Äî‚Ä°Z¬∂√∫‚Ä∫7¬∏√Ø√èr?¬∫√Ö}z∆ís√Ø4@[ÔøΩ1√ße¬µsmH√†$¬•M≈†¬©c&lt;√ä|g√Ö¬µ¬•‚Äô√∑√è\√ç‚Äìt¬≥J{√™¬∑‚Äú¬™
z≈æ‚Äû≈ì7*‚Ä∞≈∏√Ø‚ÄπX√¨V"√πD√Ä¬™ÀÜY7√¨≈∏≈†√áVJ√∑√†√ô√∞l¬§√ï√é¬§√î¬§_A√ñ√≠W√ù√ù?¬•√õ¬Ø√™j√¨Yu1√¥$ÀÜ≈°‚Ç¨l.‚Ä°≈æ\
99V√≤√¶Yu¬ø¬©√¥¬ø¬æYU;¬°¬¨¬™[‚Äú¬¶gU√Ω√∫¬Ω¬ØÔøΩU¬´¬™¬∏√å{7c&nbsp;√û√û‚Äπ5¬ºm√ê√†k√®√£`¬¢√õ√ã√å¬ΩM~√â|&nbsp;¬∑¬•‚Ä∞√æ\‚Äö√ê√õ√æf[√É¬•2%‚Äì‚Äì√Ö√à√Ö√ês¬∞√àÔøΩ¬∑¬ØZ4√ò:,≈Ω‚ÄòDÔøΩ¬µ√Ö.
`V‚Ä†√£.≈Ω%≈í¬∑e√Äh√û√ï√è*n≈ìz[¬º1Q:!!¬∑ÔøΩ'‚Ä¶U:¬¶√´√õz{I¬∞¬Ω¬Ω2√Ü√π√®&amp;√ø¬≥√ªPo¬ØÔøΩ¬∂l√∂6√ü‚Ñ¢o√Ω√∏¬¶√∏‚Äπ‚Ä°Po≈∏^]QC¬±E√µv√®√≠ √∑√∂√ê√õ¬¶'√ó√ô¬∑√è√ï√èKÀú¬≤√öyY
√ê[h√•!¬≠≈æR'Z¬´‚ÄìT‚Äû+√Ö√Ø≈∏√ã_√ï√§5√£¬ª√´√∞¬®¬∑‚Ñ¢√§4¬®¬∑SJ√åB√∏b_poSL√ë√õ‚Äôm¬Ω¬•e√á¬∑√ó¬π
√≤√≠Zl√∏‚Ä¢√Æ‚Ä¢)!H¬ø¬™v√®√ºs+`8¬πfGo¬µ7√πH¬ø¬™yÔøΩ¬≥Y¬∂¬ª√±R√î√õ√≥;z¬´¬¶¬ºyV}Eo‚Äùn√∞√´4¬´√Æ¬Ω√É√ó:ooo¬ØgU√Ω√∫¬£¬≠ÔøΩU√•¬´'M√ª‚Ä∫&lt;6p√ºl√∞‚Ä∞A¬ø¬£A√áÔøΩ√çOxZ≈æ√¥¬±&gt;√•g{&amp;√Ä¬≠√û≈æ‚Äö
B5  1≈æ¬∂√∏√ë"%√à&gt;√å√ô¬ª$√ÅZYk¬°b‚Ä∫¬≥≈†√¨
0≈æ¬πQ@‚Äö√º√ã‚Äô√º;√´,√π√é¬•	a√â¬°√§¬¥¬∞¬™√¥JF5√É*≈í√®m¬≤√ÆoÔøΩ)≈æVrxCqRc1≈Ω∆í√á‚Ä∞¬®Àú‚Ä∞√´¬∏√úu\√¶!¬•%¬∑√≥E¬§<iu≈æÀÜ\¬§`dÔøΩhbg$‚Äò2*^√â(p√êj‚Ä†‚Ä¶‚Ä∞s=!√≥¬Ω!√£m√±ÔøΩ¬µ¬•√ê¬¥√®'a√ço?√õ¬±√ò¬≤√ê:¬•ÀÜ√´¬©(z√êsw4¬•,0√ï√í√ó‚Äù‚Äö|≈∏<√ô√äh¬πm√àdfiu‚ÄìÀú‚Äù%$e*j1‚Ç¨¬£‚Ä°e≈Ω] √¨√¢¬¨√Ü‚Äô¬¥‚Ä†√¢va*≈∏√ú√ùh√ï√É¬∂ÔøΩt√õu¬•√ák)‚Ä¢‚Äùjl√Ö'‚ÄûsÔøΩ≈æw√ó‚Ä∫u√ï‚Ñ¢‚Ä¢&zdf√∏dfl≈†√Æ2s√í√µ="" u6a^√∫√õ√∏√ö≈ìv¬≥:er="">√ñ√≠u¬§√á√´H‚Ä°√ß√°N√∑C√ù√û¬ße√ÇVEG'‚Ç¨‚Ä¢
‚Ä¶J¬•√∫‚Ä¢¬¨¬∫≈∏{√ä¬™¬ø8√™M¬™¬æ√î√©N+√à(¬´M1¬∞6_s√¢√Ñ‚Ä∞√¶√¶f=¬´√ÆfU¬°p√¶√¨√ô¬ªw√Ø¬æ‚ÄöU/am¬ØF≈æ¬π≈í4‚Ä¢_‚Ä∞2ÔøΩ√∏≈∏√±¬∞‚Ä¶¬≥√©u√Ç¬ß=ck¬ÆFx&nbsp;K√π¬øb¬¨¬≥√êU√Ö¬∂√®m¬≤T0mI¬©‚Äò‚Äù≈íHdlMt[M`_¬≥¬ß√õX≈ì√Ñ-√É5RZ√ä√í‚Ä†√∏¬±¬Ω‚Äò+≈°√®√ô¬Æ‚Äû√é¬∫√¨6j¬°‚ÄöV¬§√¶√•¬ØOd^Y√à√ûÀú√å‚Ä∫V√†√ò√Ñ^%≈ìGÔøΩz&nbsp;‚Ä∞¬™√ßd¬¥‚Ä¢^√örcAÔøΩ¬ªW{X7YÔøΩ¬Ø√ó?¬æ√öpy¬¥q¬µ¬´q¬Ω≈∏}kÔøΩ√Ω√°ÔøΩ√Ü'√ó?|¬ø√±√∫4{¬≠‚Ä°¬≥¬¶b¬Ø√µ6^√®n\√≠¬Æ¬ø4√ézt‚Ä¶√±√∞√≠√¶y√ö¬¢¬¢v[¬Ø√∑E√î	!e¬¨‚Ä¢¬¨i&amp;MJ[√ì9‚Äî√ß2V‚Ä°3‚Ä†5%];&amp;√Ä¬≤√™1)wDs¬∂;h¬∂√ã¬ø¬∑)&lt;#v15Vb√¶√á√í¬≤¬∞b¬™Wo‚ÄúMo‚Äú‚Ä¢Àú√¢X‚Ñ¢\≈°R≈°&nbsp;[¬¨[¬Æ√ø¬¢/%;¬πÀú;‚Ñ¢F9‚Ä∫√å‚Ä†≈ìQ√ì&lt;√ò√Å'O√Ö¬øYV}ÔøΩi5{‚Ä∫U√ë¬£√∫R¬≥√Æ‚Äî‚Äì√í≈æSm√¨¬§gU√Ω√∫¬Ω¬Ø_`√ï¬•¬•[¬∑n√≠√É¬™ÔøΩ=M√å√∑√É√é&nbsp;z{#√Üp+√≤√ì√õ√Ä√ã√éX'
B√µVg√¥|√®√∂√ò&nbsp;¬∑√π&nbsp;¬∑√æ√∏x_N‚Ñ¢S/√áB√Ö¬±‚ÄôT;U√†b¬®‚Ñ¢Po√≥¬¢¬ª√Ω:√´∆í8√∏x√∂s¬ΩM‚Äú√Ü,√∂G¬ÆGO√à‚Äú:Àú92*¬¥¬¢aP9&nbsp;¬∑k√£√πc√¢¬≤!√î√õ¬±√ßzK‚Ä∫S√ê/√ê√éu"e-J√¶√∫√´√°%Do¬Ø4l¬≤ÔøΩ√û‚ÄöÔøΩwW√ò@i≈∏\o√º√†J√£√ï	√∂‚Ä¶N√éZ/{M√Ñ¬∂q¬µ¬ß√æ√ö√≥√ëe√∫√ΩMhApV@R√™¬¥¬∏√ï[&nbsp;√≠√É-‚Ä¢3≈†¬≤+Y‚Äî√é¬¶¬Ø¬®3¬∏ &nbsp;*√ë≈°¬Æ¬∑Q¬°√ûN*"√¶¬∫¬°√°dG}tS)¬ºÔøΩ√ÄaÀÜÔøΩGuf¬º¬¢√ñ¬•ÔøΩk√ù√ã¬±l¬©p%&amp;¬±
F√µ¬∂p¬ß=j√õX{‚Ñ¢√Øg¬ØÔøΩ‚Äò√Ä¬´
√¥6√ê√â4√ì√çh.√§√ÑJ√®I√∞√ï√õr√æd√ï7√í¬¨¬∫kZ√çK√´√ç¬¥¬¨jiiill|√∫√¥i=¬´√™√ód√Ω√´¬¨√ö√ñ√ñ[‚Äù4√™uT√£wT√£¬´√ä√∑ÀÜ¬ø√ôG√£¬£nf√á&lt;,NxYÔøΩ√î&amp;X∆íÔøΩU¬∂≈†t¬≥ÀÜp¬µ*√ÇZ‚Ä∞¬©¬¶√•√âfo¬ßF¬ºi√áRQcFHvL√µ√å≈í√∞√ä≈Ω√≤"¬ß;w√ï[P3ÔøΩ
c√±√±√Å√•IA√â!√Ñ‚Äù0bJ$=;PQc√û√Å2√Øn¬¥h¬≠tc√¶G√ó√¶%¬∞
‚Ç¨¬æI√©~√ÉB[ÔøΩ√ê^√ÖqS1¬º≈†4&gt;1¬≠ÔøΩ√ó√ü√¨?!w√´k√Ö√§&lt;)5¬´¬ø%|\√¶5!√∑R¬∑‚Ä†‚Äôm¬£√à¬™√≥e√ï√ä≈°ÔøΩ lR√°
√∂Àú√î_√Ö√ÜJ)%‚Äô¬™‚Äôvf√ä¬∞√à\√¶&gt;,√∂√™nÀÜWAÔøΩB8√çT¬¶¬∑√ì√∏0≈°√Ç-Kk!$(j¬ºy√ñ√†]¬¶¬µ√í‚ÄπU¬¢¬±K√Ω¬§4√õ√ÆsEÔøΩMm¬æ_e
‚Ä†‚Äù
≈æK(\‚ÄìTT≈í
 g¬∏√ã√®¬¶L√ìv¬¶)1√ï&gt;#√ú-9√ò=%√òe√á≈æ(√∞√ém!√¢&amp;
@#√Ñ√ô¬º¬∞‚Ç¨Rm√è√∏X≈∏√≤¬∞&lt;√©d~¬¢√ú√±ÀÜ√Ç√£ÔøΩ√í√£ÔøΩ√å√ΩÔøΩ√Ç√≠&nbsp;√ú√´¬§B√î¬™√¨√¨z∆í¬¨√∫√è≈Ωk|ÔøΩX¬∑Y√ï√ã√ãK√è¬™/¬¨_b√ï{√ª¬≥j¬ªÀú¬øk¬≥~z=√≤√¥¬•√à3‚Äò¬ß‚Ä¶¬æ¬ß√ùm≈í¬∂ÔøΩ√§&gt;¬∏≈°¬≥¬´nY‚Äòn¬πQ¬πQ&gt; 0¬®√âw¬ø√Ä√†√êIi‚Äì√â√§¬¥pRj#/LV√£√ïQ√Ø~√â‚Ñ¢√π	√µE‚Ä∞l|RCaZg}√åtG√à√í@√àLgTwc≈†ÀúW^'+√∑√ú n}wq<ur‚Äì√ü[_>√à)Ws+4-0ÀÜi¬©≈°‚Äò‚Äò.j*√é√∑‚Äô¬ß‚Äût√≥,(i¬∑‚Äì+√Ø¬Æ‚Äú√Æ^$_¬®YT¬≤‚Äì‚Äù¬¨¬µ¬°√ö√õ¬´√î√ª‚Ä∫‚Äù√ª‚Äù√∑¬´¬°√µ‚Ñ¢‚Äôu¬ÆÔøΩ¬π¬®¬ª√¶√¢√µ√∂√Ç¬≠√ï¬≤√ã¬≥√•¬≥R2M
)√£¬≠U jiÔøΩ√ù√•∆íl√Ç¬∑lE‚Äú¬∫6‚Äòp^ÔøΩ¬®n√çV√í‚Äπ√ê√©	m√ï√ôbr¬∂ÔøΩ‚Äù9,ÔøΩ√©√∞√∫&nbsp;d‚Ä†#‚Äìn	¬®√Ç≈†}JF≈ì‚Ç¨√¨√û√ï
√ê$√ª
2_5~g√û√ü√é√Ä≈°t√≠¬º¬øM‚Ç¨¬≥n√ì@'‚ÄúH'√£‚Ä∞&nbsp;√£ j≈°;√®√∏≈í√ø√±¬°B‚ÄûU√õ√ü¬´¬æ~¬≥¬™√ÆQ√ï¬≠√êmV√ïÔøΩÔøΩt√ç¬∫uc'‚ÄùUA√¨¬§gU√Ω√∫¬Ω¬ØW¬±¬™LX√µ√é¬≠[¬≥/e√ï√∂√é.√≥r√à√©ÔøΩÀÜ√ìk‚Äò¬ß¬ØDÔøΩ¬πq≈°√°y√Ü√ì√®¬≠√±Nd0
√Ö√ï√ªÔøΩ)¬´¬Æ@o¬≥¬°√ûz√¶F√π√á√π¬∞√±≈Ω@d@√îJ¬≤/KÀÜ¬¶d‚Äû‚ÄòR¬£‚Ñ¢‚Ä¶!ÔøΩ
√ÆRz √ê[√≥@¬Ω√Ö√ï¬§√∑5G√év≈∏‚Ñ¢ÔøΩc:√´√í$dÀú¬≤√¨i√à¬π0≈°‚Äî√ÜRGEE¬Ω
√•∆íM√•√™fToI#√çUg‚Ä¢‚Ä¢√´c‚Äû‚Ä¶√ä¬¥ÀÜ&gt;+¬°/¬´¬™o¬≠@¬Ω¬Ωs¬±j¬π¬ª√™¬≠¬¢nk¬¨√¶√é¬®¬∑w√ó¬®√ó√éV/√ä√´√†√Ä¬æv√¶‚Äö≈ì¬π¬®¬¨√ô¬©¬∫}¬°√º√¶r√ô√ñ$a¬≤‚Ä¢&gt;%F¬Ø¬´F√Å√∑o¬©Ts‚Ä∞∆í√Çp+~m¬∑6n0¬±¬Ø)OI/T√íQ√É√âl√î√õ≈í	Y√ê√õ1¬©‚Äî‚Äù√ùP‚ÄûC3¬™TÔøΩ√ûV¬•√áIi≈Ω√ùÔøΩ‚Äì]
√¶M√•.√®‚Äû^√¨¬¶√ìeÔøΩ&amp;U√ë¬™¬≥G‚Äú4W√É√âm¬Ω=√¥v√ñ√Øx√©√ç¬≥√™√´4¬´√™√∂F√Ω¬≥√É¬¥∆í√º√¥¬¨¬™_√Ä√µkX¬µ¬ª0q√Ä√≥H¬ø√è‚Äò&gt;√ü#√É√æG;&lt;{¬≤58√§l|√ò√ï√¥¬®¬ª√π1O‚Äπ√ûV¬ß|Qbu@K‚ÄöMB≈ì√ç¬±¬Ωl‚Äπ≈æF+√ï‚Ç¨j_‚Äú-√ó2-√î1)√ê5=√å¬•"√Ö¬©‚Äúe3√òb√ô√ì`√ã%√ò‚Äì&amp;√Ç*√Ñl@yR¬´√ê¬Ω‚Äúe√ó√õ¬æ√ñB¬ø∆í5ÔøΩ√®√â√à
¬ße√á2rbY‚Ä¶Qb¬™Ko‚Äú√•√ó¬∫‚Ä°√£√òU√Ø√í√ù√®√í√áu√§√ô√µ7;Ji!≈ì√¢<iux√ók-p√íÀÜ√¨‚Ä°√Å‚Äö¬ø√µ√®¬Æh%fh¬≠√Ñ√úv¬®z√†<"‚Ä†‚Äì¬ßjÔøΩ[g]‚Ç¨¬≤&xÔøΩ√Ø¬Æ8 ¬≥¬π√≠yn‚Ç¨‚Ä¶√•t@¬¶√ú2\s≈Ωs≈°√Ñf:,√™‚Äπax√æ)¬©¬∂√´io4√ñ="" ‚Ä†¬Ω≈ì√Æ√ò√Å¬≤√≠i¬∞ÔøΩ√ë¬≠√´≈†√ù="" ¬∏p‚Ñ¢s‚Äòky√Äs¬£f¬πi¬®¬∂√ù√µ¬∂}l‚Äπ^≈Ωe‚ÄúewÔøΩu[¬µ="" !√û¬ªb√Ωt¬ªs‚Äò¬´bws√∞2‚Äö3√ê√û√ê√è√∂≈í¬Ø√µi√∞"∆í‚Äî√ö√ï√¨Àú¬Ω√â¬±b¬ªcb¬∑∆ír¬∑∆íb√ób√ßb√∑="" ¬±√∞ÔøΩ¬∞√™¬´y¬∑y√µ√µ¬ß√ï√¨="" ‚Ç¨qv≈°¬¨ÔøΩÔøΩ√µ¬¨√∫‚Äπ¬¨z√øyu19√Üz5√¨√îr√ò√∂√î√ù√Øs¬Ævv‚Ä†¬Ø√Çu√ÑÔøΩ≈°e‚Äπop√é≈†√∞√äÔøΩ√∂¬¶√ß:u7ÀúÔøΩÀúÔøΩo¬≤(m'¬¶‚Äû‚Äô¬¢jd5n‚Äôj="" jf|m="">√µ@c√•'Kh√ë#b√ø¬πn√ü)eÔøΩ≈†√ã'√§J)Y‚Äô¬™√ú)%√∂¬º:fe8vB‚Äì√ñ]W√úS‚Ä°√Øk√Ñ√∑¬≥√ã¬¥4'$¬•√ã∆ís¬•√É√çU √†‚Ñ¢‚ÄìÔøΩ/≈∏-¬º¬æTpm¬°h¬±‚Äπ&lt;#¬°M‚Äπ√©s
√™√∫(√°√Ür√ë√µ√Ö‚Äô√´K√ÖK]U3:√≤W√ï√ìb√™R√°√™B√é‚Ä¢¬π√¨¬µ√ë√ú¬±V√íp√òDu3q¬®‚Ä∞¬∞√ù]^ÔøΩ√Øf‚Ä¢√év&amp;≈æ≈†Z√¨ÔøΩ√§%ÀÜF
√êD√§~E‚Äì‚Ç¨≈í√∏M√à]√Å‚ÄòQ√ÉÀú√π8≈ìJ√§%‚Ä†‚ÄìEN√É√∞*\:X√†h‚Ä∫√±+√≠K‚Äú‚Äπ√¢√Ä√∂√ùUÔøΩW∆í*x¬©C≈ìL√º≈í√É≈í‚Ä†≈ΩM86√™l√ú√ßXV¬Æ√∏MX√µÔøΩ&gt;^√ù¬¨√∫
¬£4v√í¬≥¬™~√Ω‚Ä∫¬≠W¬≥√™√¢+Y¬µ¬ª‚Ä∞¬πt√ï√õ√µÀÜ√ì√ß√ÇNQ√ùO¬ª[√Ä‚Äì¬®√≠√ª(w√ù√¨√™¬Æ‚Ä†¬∫√î√§√õ√Ñ¬∫f‚Ä†{√ßc¬ºJ√¨ÔøΩ√û¬©i*¬∑-≈Ω‚Äπ"¬•‚Äî'F7‚Äù√∏(Y≈Ω<r@ul;√ñ!z√ã√åo‚Äò√óf≈í√ã√º√¶z|√áe√ÅÔøΩ√µ ‚Äö≈†)%¬ª¬≠:g¬Æ¬≥¬¨‚Ä∞y≈Ω‚Ä¶ÔøΩk¬ª√µ¬∂rj‚Ä†_√ï√§o¬∑fz¬†√û√é)‚Ä∞w√¶¬∂√µvnk‚Ä¶√É.vr¬∂&√ã√û?√µ√∂√™|√â¬º‚Äö:#‚Ä†√µ¬Ωs¬¢√™="" ue¬†="" √®√≠√•¬≥√ô¬´√™m="" ¬∞√™m√Ö¬∂√û6‚Äì¬™√™√±¬ΩÔøΩ√Ö¬Ωq√ã√™√à‚Ä¶¬æ'√®¬≠√ï[r&¬ø"sx‚Ä¢4*√µ≈°ÔøΩ¬ª√¥¬∑¬∏√≥*¬£‚Ñ¢√πiÀÜ√ûb√®9¬∞c≈†‚Äù≈†q√¨:√´`c¬ª√å‚Äûleq@o√∑≈Ω‚Ä∫‚Ä°z√´o¬øk:¬™¬∑~√Ü)√é#ÀÜ√û≈Ω√Æ√®¬≠≈†‚Äù‚Ä°√¥¬ø1v√ï-ÔøΩ√ñmv}ÔøΩ‚Ä†‚Äπ¬Ω√ç¬™h√É√Ö¬Æfu4.¬¨="" 4v√è¬™√∫√µg[√èy√µo√ª√©√´¬Ø√ø)v√≠(h√¨v;√î√•}¬∏√ã√´√∞‚Ç¨√Ø‚Ñ¢√á!√∑3√Øy≈æz√è√é√†¬†¬£√°!'√£#.ÔøΩxÔøΩ#√Ñ√∫<√á`o≈ìhr_d,¬≠6‚Ä¢√êldsr‚Ä†yjÔøΩ="" √ñ√á="">√Å√è√§Xg√á)3√ß‚ÄùY¬¥TX√ñ√¶[`√ús¬£¬º√≥c|≈†√¢¬º¬©Y√é√ºJ¬´‚Äì
√´‚Äì
‚Ä∫¬¢M+√â¬¶¬Æ√à¬≠*=%"E√Ç≈í√ú0N‚Ñ¢¬ßÀúj'¬Ø¬±T√ñ≈°+j,√õh√∂¬≤'¬∑&lt;F√ë¬©-AmtW√ÉE√Üp‚Ä¢3√ú5¬Æ≈†ZW)√ï‚Ä∫∆íO√•‚Äì&amp;q√∞i<bl√ù|rq√æ√ñml√±v√π√ãk\dt‚Äî6√ò√ém4')√ç¬©‚Ä¢√§]wÀútwÀÜeb‚Ñ¢√à5 ¬¥≈°√âÔøΩ¬°√ß`h√ô√ê√î‚Äî[√îff7‚Äî3√å%√ï‚Äì√çz≈Ωwyr`i|pi¬º1√ñ¬Ø0‚Ä†¬∏9q="">‚Ç¨√ç√ôxn¬π%√∏√òXb6¬ß√î‚Äö]bVg‚Ä¶√µ¬±√Öxm¬∑¬¶√Æ¬¢T[X√¥^XO√ã√†Ev5=√™h|√ò√Ü√∞p¬æ√µ¬æ√≥{¬≠√éZ≈ì√ü√£9¬æ√ás=¬Æ√û,¬´√™vd√®v√Ä√≠√ß¬∞wZ
¬øTÔøΩQc¬•]1¬∞≈æU_√Ç¬™√ì√ç/¬≥¬™ÀÜ¬øi¬µ√á√Å¬©¬ª‚Äπ¬°'9≈æ'≈ì,N√ª√ö≈ì	¬∞G=√ê`1p√Ñ≈Ω≈†¬´	√æ≈ΩIN√ç‚Äπ√ä4¬ª√¥P√∑¬¨Z¬ÆCr√è√üÀÜ¬∑*≈†∆íM≈°¬•√±uE≈æ≈°5‚Äî√†IJ√Ö√í‚Äò≈Ω*@¬¨5¬π‚Ä∞√ç‚Äû√®^¬Æ√è¬§√úm¬¨√çS√ï√ä-√çT¬¶√±√äs√∫≈°√£f¬ªBB&amp;d¬±√ù√µY
@√ú√Ö,‚Äö¬£¬µe~√æ\W√ö‚Äû4¬ø¬Ø√êe√Ö0¬ø√º√¢X√ö√ÜT√ä√∫d√ö¬¥¬¨√¶xd@&nbsp;g√õ√±‚Ä∫√ìi√´‚Äú√©[¬≥¬©‚Äπ¬™√º1A√•≈∏4√å¬´{FQ¬∏6‚Äò¬∏6≈æpn y¬∞	√Ä)√ò¬•}H√à‚Äû6Àúw2√•√ï%√™√ñ√∏√ô√Æ√†¬≥=√æ-1
4gÔøΩ`iDSI¬¶ÀÜ5√Äs√ôw√î¬ªs√ä"9	5y1¬¥√¨h¬¥h≈∏Àú√ïLpjg≈°!¬¨jW
0√û/	≈ì√†-√ì√Æ≈íj(√íW√Ækgbg√ê√Øwt√Ñ√Ø√®√ò&gt;G√ï^GT√πqo‚ÄìU_g¬∞√î¬´c¬ß√ó,¬µ√∑≈æ√ü√ñ√ñV√è¬™√∫√µ¬ª^√ª¬∞√™‚Ä¶√°¬æ√û_`U√∞F√ã¬©=√Øb1√™√≠J√ò¬©¬πÔøΩ‚Äú$√ó‚Äú.‚Äì¬ß}aK‚Äù√ÄU¬§√òl√ó√µ √ê[‚Ç¨¬´√íj¬≥‚Äô‚Ä°¬¥√è≈ì(√∑√∫bx√ù¬≠dZ0√≠
b√Ç	¬∏&lt;6≈†S√Æ"¬•Y7‚Äù√∏‚Äú√ì√¢√®9Q5ÀÜ√û2r‚Äúx‚Ä¢C¬≠≈æ‚Äú
7√Ä}u‚Äò√çebzkE√∂ÔøΩ f^√¥vD√ü√â√åU√íÔøΩ‚Äûz[S&gt;"√å[P¬•≈Ω¬¥√∑7‚Ä∫*&amp;$√∏¬µ√±T¬®¬∑i√£√Ç≈†¬Ω¬≠\√¨.z¬ª1‚Ä¢&gt;√éw≈Ω√∞¬™F√π‚Ä¢√É-¬§&gt;q¬Æ+√®√≠√Ö√ë√ÑUZ_}%&nbsp;` ¬∂Z¬∑I√Ñ@¬©H√â(c√¶TÔøΩg¬ª√ΩTl¬¨¬∏*S@√ä‚Äûo‚Äû4ni≈°¬§:\#tz+¬Ø√µl(‚Ä∞a√§√Ñ∆í 
5≈ì¬¨JÔøΩ¬®H≈Ω‚Äôm:X¬´‚Äì:ÔøΩ¬®¬©√£‚Äî=v'wgTa¬∞¬¥m8√©¬≤m8√©cgÀú√§xz√ê√Ø√®¬®√ø√ëTo=ÔøΩtU¬æIV√ï-~ÔøΩ√°{¬ß∆í√©√ñ‚Ä∫¬°z¬´√õp¬±kÔøΩ`U###=¬´√™√óg√≠√è¬™√êxeU¬µZ‚ÄîU√•;¬¨¬™√àKP¬∫Tx‚Äù{√™√∂&gt;,r;√®|√≤m√ìco[ÔøΩ|√á√¶4 √ñÔøΩX¬ªÀú&nbsp;U√Å√áaU¬∞√µ)‚Ä∫3ÔøΩ‚Ä†o√É‚Äù√£≈í√ì‚ÄùS≈í¬ßy¬§P√®√ßc‚Ä∫`‚Äîf‚Ä∫n≈∏e≈∏√©ÔøΩ√™≈°}√ù=‚Ç¨:√•√á¬∏‚Äî$¬∏‚Ä¢$¬∏‚Äî&amp;z‚Äù%y‚Äô√Ω√π‚Äì%‚Äì%pA√Ñ√§ÔøΩ≈†√§√∞√ä√î√ê√™l√ø≈°|_f¬°¬≥√Ä¬Ø&amp;/ÀÜ‚ÄìQÔøΩ‚Ä∞¬°e√Ö√ísQF1√ÇX√°¬¨‚ÄöÀÜ¬∫B¬∏√´‚Äπ√Ç√´‚Äπ"‚Ñ¢√π¬∞√≤‚ÄûYx6¬Æ¬æ(¬™¬°8|¬≤¬æ&lt; ≈°USW√â*G¬æ0≈íYZ‚Ä∫≈Ω`)bÔøΩ¬ÆFiE√ªO‚Äò√ö√àpbrDej05√ì‚Ä°‚Äì√£I√ã√±¬®√ä√∞,√á√π@√∑√åsw√ü√ü$√∑√å‚Äî≈ì(√∞L3√É√≠√ÅO¬µM
¬±I
¬±≈†√∑‚Ä¶¬ØI$R√ä‚Äö
/tJ=∆íP*√å¬•¬∫‚Ä∫‚Äù√™lr√Ñ√ë√®ÔøΩÔøΩ√ÅA√ãS¬≤-√ûmr|‚Äî√´√¥.√õ√°]¬∂√Ω;¬ßc√≤7√á¬™¬ª`TÔøΩu`√ù¬™√ÇWÔøΩk√úoZÔøΩ‚ÄìU√ëX‚Ä∫¬Ø√ë¬≥√™V]≈∏√ñ¬®_√ç¬™2‚ÄûUg√Ç-fÔøΩOÔøΩÔøΩ√ìL√ê‚Ä∞:√∑√£f¬∞D√üw¬¥X¬ª8oG√ë‚Äö‚Äùf∆ípMqc√§√ô¬™√ò√¶ÔøΩu‚Äù,√ª¬ºhh√ÉX√öP√¢*¬§X√ï√§{‚Äôb(a√† √ê¬≤√Å√ß(FQ√´K‚ÄûN=N	≈Ω[≈°√í‚ÄûOS‚Ä†EÔøΩ3ÔøΩ√û√≤√ÄN¬ºÀú≈ì√üF√ç‚Äú√ì√≤≈í|√ü√ü≈ì6)ÔøΩU√≥3:k√ä`
‚Ç¨]¬º√îÔøΩY√ñD≈∏√ÇhZ√∫Q√∂$¬®y%‚Äπ¬Ω‚Ä∞0_&nbsp;‚Ä∞Y√¨√Éj√∏E√†√≥√†oAÀú4*√ä:7~n |¬∂3¬¶¬ª¬Æ¬§‚Äπ	M‚Äú:k√õ√°¬§chÔøΩ$¬ß√ß≈†√âJ&amp;nD√¢B¬ª√æ	%ÔøΩ[≈°√ï\≈æ√íT≈°\_ÔøΩ√ûF√∑√©k¬∂√´√£Z√à√ûÔøΩR¬≥b¬µ¬≠√•‚Ä¢¬©a¬•‚Ä∞M√•≈Ω(¬´√≤*l‚Äò¬ª&amp;√ü√ú¬¥√ß=¬™h√†√§¬≥c‚Ç¨‚Ä†V√¨;∆í√≥√´og√®ms&amp;√à√¶t¬∑74@SyQy√Æ√µ8√ú‚Ñ¢'¬´¬Ω√É√†√ø√±√ó¬∞√™~√çS¬øX¬´√ø√í√¶¬©W√ó√™√Ø¬Ω√ß√ó¬≥¬™~√Ω√û√ó¬æ¬¨√ö√ª√ã¬¨√ö√â¬©ÔøΩ√∑9&gt;t|*√®√ÑB√à√â√â&nbsp;√ß√£≈Ω√¶'¬ΩÔøΩ¬≥X`f√¨d√∂\oa√ò√´c√Ø√´(¬ß‚Ä∫‚Äî√Ñ√õ&amp;√∏¬πgE¬∏4√¢m‚Ç¨√û√ä√®‚Äì¬§t¬ß√ú¬®`|¬º_alxK‚Ä¶C+√â≈°‚Äì√≠WÔøΩ‚Äπ¬¶d‚Äû#zM√ã√Ü¬≤
¬£¬ª√ù√îB¬®¬∑√≠¬¨@&gt;√®-¬∑4]J√á≈Ω√ã|g¬∫¬º√á√ö‚Äö:√´p√¢¬™&lt;&nbsp;¬∑
¬®¬∑m‚ÄùR57¬≠√Ñ√¥7√•v√ï‚Äìv√ó√°‚Ä°Z√≤‚Äî¬∂√µvÔøΩ[√âÔøΩF‚Äû‚Äπ√Ω√±Po√ïÀú‚Ä¶√û√∏√Å¬¶R&nbsp;¬∑¬Ω
@≈†√±m¬©@o√ª#¬¶√òv¬°‚ÄπY‚Äû√®mÔøΩVoe¬¥\IU^G]√Ç¬∏√ú√®¬≠≈†$$√°¬∏e√ú√≤&gt;¬•¬±8YQ√´&gt;√Ä¬≥√©√•Z¬∑T√Ä¬±YÀú√™,J‚Äò√ì√Ç*S√Ç√∞	‚Äò¬≠$√à¬™]uf
%≈Ω√®√ù&gt;4¬Ø√ì5√æ}a√ñ√ºv√©¬Ø≈Ω√ûx√ô≈ì‚Ä∞¬∑?√ï∆í√®m¬™¬∑√Æ‚Ä°;ÀÜ¬πr‚Ä¶R¬Æ√¨P√ºjV√ùo8¬™¬∑¬∫≈Ω√´√ª
G√ò¬´¬∑¬∫≈Ω√´¬∫z¬´:ÔøΩ¬≤√™¬©S¬ß√Ä√£√µ¬¨¬™_‚Äû¬•√ã¬™?¬æ√à¬™√Ω√Ω√Ω[[[‚Äî/]√í¬ºÀúW‚Ä¢√âd
‚ÄûU¬•¬π√±¬ßb¬∑∆í`+&lt;¬µ¬∏¬∞?√∂g¬£√Éo‚Ñ¢√ª¬≥√Ö√±¬øX≈æ|√á√∫√î{vg8t2:x√ä√ï√¥Àú¬ª√πq√ã‚Äú¬ß√ù√ç√èxXz[√ª√òÀú√ö‚Ä∫‚ÄöP9√ú√ïI√ÆXF{XG¬π√õE{8`¬º√¢|Q{√í√§`g M¬©√Ån)√Å≈æ√©¬°^a√ûY√°¬æ√ô‚Äò¬æ¬π√ë¬æ√π√ü√ÇX√ü¬¢8¬ø¬¨?&gt;&gt;√© ∆ínE¬•‚Ä∞√°√•‚Ä∞√°‚Äû¬§`√ÉHi√ê¬£‚Ä∞‚ÄùIJÔøΩ&amp;¬•√ÜÔøΩ√íb√à`¬ßc¬™√í1‚ÄùXpDl&nbsp;√ø‚ÄùLx√áVe√ÑR2√†_ÔøΩ]‚Ä¶&lt;≈í≈í~IZ4√≤"IKC+SB‚Ä∞)¬°√â!\p9.¬®,	√∏√Ä'c∆í
0A√π1ÔøΩy√ë¬πQ√æ9Q~9Q√û9Q ¬¨EmB¬°iR¬∞3√¢\√ß‚Äû√µq≈†√µv≈í√±t≈í√∂¬∞‚Äπt¬≥ÔøΩp¬µ	u¬∂sÔøΩN,√õ‚Ä∞T√Ñ=√â√è√¶4‚Ç¨}@‚Ç¨R√æ¬ªÀúu2&gt;√¨`t√à√û√†&nbsp;√≠√©√∑¬¨N¬Ωkz√¨ÔøΩT¬≥¬∑√´√¨√üi¬∞‚Ä°i√∑√ã√¶m‚Äì√ÉQ¬π√®√ç¬∞√™¬Æ√∏u:√†^=¬Æq?AF^kc`‚ÄùUÔøΩ?√é√•r√µ¬¨√∫√è¬≤¬™B√Ñ¬µ√∑?6pl):¬≠q=jkO¬¢‚Äî√•	√∞√´√§¬´√£ÔøΩ¬∂√£¬∂dÔøΩ¬º√©√õ¬µTXP2-√º≈ì√íBa√™¬øÔøΩc√ëF¬∑(√Ä¬∏fG√∫‚Ç¨sW‚Ç¨√±√ß‚Äù:¬µ‚Äôl+√ì¬ºK¬∞√°√Ñ‚Äù`p√ä√à√©√°√§tG√Ö¬¥‚Äô<z√òv}√ç√ñ] √éÔøΩ%√ë¬¨‚Äö√§√Ü√¢d≈ì(‚Ñ¢aj¬°√´x‚Ä∫√†d15¬©‚Ä¢Àú#"e‚Äπ√à9j√∂="" bt√î√ìÀÜk¬£√âi√π√≠¬µ¬π√ì√≠√Åg¬ªf¬ª‚Äöz‚Ä∫2√ök≈†;k‚Äπ;jk¬∫xe≈°√ñ√Ñyu√Ä√ô¬Æ√†9u¬†≈°‚Ä°cf√µ‚Ä¢v√î="" 4'√èv√∫√ét√∫≈Ωjc√§√¥|¬§="" }√á√Ω≈í‚Äô%&g≈†hb¬ø"¬£¬´!`h√†√ò√ü√¢(¬•ÔøΩ≈∏¬™¬æ(¬π¬Æ0¬©¬°¬£d√ö¬©√ò2‚Ä†m]q="" ¬¢="" ¬∞3¬∑2√∞¬≤√ÑÔøΩ¬¢¬∏pv¬©∆í¬≤√Ün√ç5¬•‚Ä°yi)u√áÔøΩ√çvk‚Ç¨{¬¶ÔøΩ√Äi√áÔøΩ√≠≈í¬ß√ï)¬´‚Äú="" √è√É="≈æ‚Ä°√õ√Å√∂8√î√°vH‚Ñ¢ÔøΩ¬∫;√û¬´√™ÔøΩ√´}√¨7√©oo¬∞¬Æ√ë√á√û√¶)√îX√∑≈æ¬∞*ÀÜÔøΩlll√¥¬¨¬™_¬ø√´√µkX¬µ∆íS;√•ul<ÔøΩ√™√≠T√ê√±√°‚Ç¨cx√á¬£v&amp;√á√ù-Nx√©zN:‚Ä°√™¬∏-√Öx¬π¬∞‚Äú1√å√ãq‚Äì¬±^¬Æ¬π√ë√∂¬ºJ√ò√•√îJ¬≤√äÔøΩq√ã≈Ω√∞√çC√¥V@¬∂k*¬∑¬ØH√∂√É√á‚Äù!¬•‚Ä†√Å√´√±t¬¢Hi√é*√î[E¬≠√ê¬±¬∫B¬¢¬∑)]ÔøΩ¬æ√Éb‚Äî1¬©K'DDI√ë√™¬≠‚Äù‚Äô=," ‚Äòu¬∞√íe√îb9¬≠¬†¬ª!c¬∫#√ï√õ¬Æ√∫√ú¬†¬∑√å¬¢≈Ωz|w}√æ¬®8√ï√õyu√†ÔøΩ7¬≠¬£¬∂√®mgm¬°≈°√¥v¬∫√ùo#≈íÔøΩt‚Ä¢√à¬´√≥¬†√ûrw√ú&¬µzk√åp5y‚Äù√Æ√£:‚Ä∞¬™√Ç≈†‚Äú√´√†|√πdnidw¬Ω‚Ä¢≈†c!¬≠¬∂¬≠√â¬£d@ÔøΩg(5¬¥√µ¬∂0.‚Äù_i√ì√Å‚Äûz[wl≈∏√•¬•¬´¬∑¬∏@¬ß√∫,t="" '¬µ!‚ÄúÔøΩ√çi√ãs¬±¬∂'‚Ç¨√í¬™<+¬Ω√≠t="¬§¬¨‚Ç¨¬¨¬™h√ØP√à√•¬ø≈æUu√ó_s¬≤√™.√á√µ√óiVEM√óQV577√ó¬≥¬™~√Ω¬°√ñ.V√Ωa‚Ä°UA√î√õ√ó√ó¬∑¬π¬µu√•√≤e‚ÄùU√õ√ö√ötYÀÜ¬≥(√ã¬∑ÔøΩ√Ø√≤√è√•‚Ç¨√Ñ√≠" √õ√©="¬ª¬£o√∫‚Äú√â√°?‚Ñ¢y√ã√¥√®≈∏√çÔøΩ√Ω√Ö√≤√∏√õ√ñ'√ü$ef;√ç√™≈í¬§Y=-≈Ω{[¬°√í}√ö√ü√Æ√å√é}#√™?`√©n‚Ä†√å√õ5v√™d√¨√¨qÔøΩ)!≈Ω)!N√©¬°√é√©a√é√°p¬æÔøΩ√Ä≈ìH√è√ú(¬Ø¬ºh√Ø|8h√É¬ª0√ñ¬ª(√é¬ß8√é¬∑√´[√Ø√†¬±√¨√Ñ√Ä¬≤D8¬º" l√ùa√â`w√Ä≈ì,¬∫√Å≈∏∆í‚Ä∞√àgd#√Ä√†‚Ä†_r≈æ;g√ãp&√Ö√á√ª#5¬Ω¬∞√ü¬ø(p¬≥o="">b¬•≈æ√£‚Ä¢
~*ÔøΩ√¨√∑¬¨p¬∑√åp√ó√¥0√®√®‚Äπ√Ü¬¥IH‚Ä¢ √¢
‚Ä¢6√é√õ
qH¬∂≈í√≤0√è=√Ç√ï4√å√ô$√Ñ^√ò√∏#M¬©^V¬ß√êr_H¬©&amp;G ¬•¬≤;s√Ä√¶√î{V'√ü¬µ&lt;√±¬∂√ô¬±¬ø√æs¬¢√±≈∏√©6o√ó√ò¬æM¬≥y‚Ä∫f√Ω¬∫√ùa‚Ñ¢H√ê√û√ô¬•√ºu¬¨¬∫≈∏[√ã?¬øZÔøΩ¬µ√ÜJ√ö√ç√óM√ñ¬≥√™¬ø√à¬™B¬æ:√ò\√£√ãM‚Ä°√Ω√°√áj√ß#VG^¬®v¬∞9√≠gk&nbsp;√£ÔøΩ√°j√¶l√ùB4o¬Æ0√Å'YU¬¶[+j√åTÔøΩ‚Äì√µ√Ö‚Ä∞√æ≈æ√∞¬≤√Ö¬´√ß√öJ¬∂V√ôP2√ú√Ä√Ä√áÔøΩ√É‚Ä¶G¬£f√≥*≈ì√ìu√ñ‚Ñ¢¬∑3¬≠‚Ä∫	≈æ¬¥√¨xX'≈∏‚Ä∫√Ñ√ÜG)‚Ñ¢∆í&lt;√õ≈æ}W∆í‚Ä°ÔøΩ√ìZ‚Ñ¢(¬¢`¬ª√ºG%.C9#NP‚Äò+¬°¬§¬∂¬≥√¢F$cmncR;F√ÅHm¬£fK)yRj¬Æ¬≤6
|r¬º√çcB√¶¬Æ√∏w¬∞‚Äô¬§√î9#¬≠¬∑)lT√™:"qj√µP√îb¬•‚ÄùaeF+‚Äì¬∂√ù√è≈°√ã‚Äú¬πeIÔøΩ√Ö√©¬º≈†√àv√¨mog√ô√≤*¬Ω√´
¬£√´‚Äπ√ÇD‚Ä∫√é:‚Äπ‚Äì¬ª√î¬£*=≈íÀú√õr¬°Z`1xvI¬æ‚Ä¢i^&lt;¬¢M;x^,3√ô¬≤"√Ö1'√ö59√àDM¬®√ñgW≈ìP4?h‚Ç¨v√Ñ¬´¬Æ√¶'¬º-≈Ω‚Äπ√ù*√ù¬µÔøΩ√≠v¬∞√ç√•‚Ç¨,¬£P√à‚Ä¢]¬ø¬´¬æ¬¢=¬™¬∫¬µ√∫¬∫√ìjtk√µu√í¬¥¬±‚Äú≈æU√µ√´√üo√ΩVmg√ó≈ΩxU√ªB¬•¬∏:√†w¬¥√ê√ÆÀÜÔøΩ!√î[xQo¬°S√ê¬≤√£√†ÔøΩ√®-ÔøΩkyÔøΩ9¬ª√î¬§ √é‚Ä†≈æg√ô√Å2BD√è¬µ√ÇxfF@¬Ω%¬¶:¬∑√ë¬¨Z*√¨Hi%√∞N&gt;zN¬ΩM	¬£e‚Ä∞√àvH¬Ω‚Ä°¬π≈ìa√ã)√≥¬£√ß`k√≥√£jrq≈ì¬≤ÀÜ¬Æ‚ÄîA¬æ√≠@‚Äπ};√ãGP√¥VBÔøΩUq|√ÜeN<o)' √¶h¬´s¬∫¬¢¬µz√õ√ï‚Ç¨‚Ä¢√ì√ì!u"z√õy≈∏8√ñ√¶1.√≥ÀúÔøΩ¬ª="" ¬µ*kr¬•‚Äùemrs0√ê√õa‚Ä∞√´ÔøΩ√è¬ßÔøΩ≈Ω‚Äút¬•="" ¬†√û¬¶¬Ω√•√≠√®ms¬†f¬†√Äa]="" √êf¬æ√Ü¬æ‚Ñ¢√†√¥‚ÄìÔøΩ‚Äôt[w7‚Ç¨¬ßiyw√®mjbnl)√á∆í√à¬™$="">¬∞√ßKJ√∑UY‚Äö√ß√¥‚Äì[nU‚Äì√§‚Äù‚Ä∞√™√≠v√∏‚Ä∫¬§ÔøΩt√™√≥¬∫_c]√ÉIO√ã‚Äú.√¶'¬¢¬¨≈ΩI√úJ‚Ä¢¬∫‚Äô¬Ωu&gt; ¬Ø√à2¬´|¬£¬¨√∫≈†i5¬Ø√ôp¬°√ï√õ¬Ω√ÄzV√ï¬Ø?√∏z¬´√∂√∂√µmll\¬æ|yxxx/¬´√ä√õ√ö√ôX≈Ω√≠;¬ßw√Å√¶¬ª`9¬ækwl‚Ä∫U¬∑√∑¬°?‚Ñ¢~√ã√¨(‚Äôf=√±≈Ω√µ¬©w!¬¥‚Äû√êjtÀÜ‚Äì¬µ9‚Äû[¬∑}∆íC ¬∑≈°‚Äö`‚Äû√çQ√Æ√à√ù#:√´X¬¨¬Ø√≠6¬∫√∫;‚Ç¨√§‚Äù‚Äöf]C]√ì√Ç√ú√ía√Å0√ÄC√∑l√Ñ&gt;'√ä372,≈ì¬æA¬≤l‚Ä¢√õ√ô√ò√≠ÔøΩ‚Ä¶∆í9ÔøΩ√≠_‚Äû‚Ä¶√¨‚Ä∞√º~}@a≈ì_√°√éWÔøΩ√Ø¬æ√∏n‚Äú¬¢¬Ω√ê‚Äô.;√í#+√Ç‚Ä¶</o)'></z√∏v}√≠√∂]></bl√Ω|rq√æ√∂ml√±v√π√´k\dt‚Äî6√∏√Æm4')√≠¬©‚Ä¢√§]wÀútwÀÜeb‚Ñ¢√®5></r@ul;√∂!z√´√¨o‚Äò√óf≈ì√´√º√¶z|√ße√°ÔøΩ√µ></iux√ók-p√≤ÀÜ√¨‚Ä°√°‚Äö¬ø√µ√®¬Æh%fh¬≠√§√ºv¬®z√†<"‚Ä†‚Äì¬ßjÔøΩ[g]‚Ç¨¬≤&xÔøΩ√Ø¬Æ8></ur‚Äì√ü[_></iu≈æÀÜ\¬§`dÔøΩhbg$‚Äò2*^√©(p√∞j‚Ä†‚Ä¶‚Ä∞s=!√≥¬Ω!√£m√±ÔøΩ¬µ¬•√∞¬¥√®'a√≠o?√ª¬±√∏¬≤√∞:¬•ÀÜ√´¬©(z√∞sw4¬•,0√µ√≤√ó‚Äù‚Äö|√ø<√π√™h¬πm√®dfiu‚ÄìÀú‚Äù%$e*j1‚Ç¨¬£‚Ä°e≈æ]></w¬•‚Ä°s√¢¬©√π≈°≈°mv%¬•y√§√ß√†ÔøΩ¬™¬∫¬Ωo></e¬∫=¬´ÔøΩq√¥></yr√Ø¬™;y√µgh^uzxx,v¬Ω{√ß√Æ√¢ky¬µ¬£¬´c√§¬ø‚Äúd√øÔøΩ‚Ñ¢√∑√∑0√æ√®^≈ìqg¬∞‚Äò‚Äî√£¬∂¬∑√∫t√æ√πasy‚Ç¨#x¬Øg¬Ω></s-√º√¨(¬µp√æ¬∞√™?[o¬∂¬∑y√µ√µf¬®√©:√∞[‚ÄùumllÔøΩ¬´‚Äö√´yu¬ø√æ√´_fu¬π¬¨m¬§4y9√®√§r(√ºk√°¬ß¬ß‚Äöod√∏≈æ√∂¬¥></zi√ª√∂i√ó¬≥g√™></u></a></i></u></i></u></s></b></s></i></b></b></i></s></i></b></b></s></i></i></s></i></i></i></i></i></s></b></i></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf">https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf</a></em></p>]]>
            </description>
            <link>https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24693589</guid>
            <pubDate>Tue, 06 Oct 2020 00:17:45 GMT</pubDate>
        </item>
    </channel>
</rss>
