<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 10]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 10. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Thu, 11 Feb 2021 20:29:30 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-10.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Thu, 11 Feb 2021 20:29:30 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[How to Get Startup Ideas]]>
            </title>
            <description>
<![CDATA[
Score 55 | Comments 23 (<a href="https://news.ycombinator.com/item?id=26085118">thread link</a>) | @davidkolodny
<br/>
February 9, 2021 | https://www.wilburlabs.com/blueprints/how-to-get-startup-ideas | <a href="https://web.archive.org/web/*/https://www.wilburlabs.com/blueprints/how-to-get-startup-ideas">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>Startups and technology canâ€™t solve <em>all</em> the worldâ€™s problems, but they can solve a good amount of them. Now more than ever, we need people to build the next generation of companies that will tackle some of the worldâ€™s biggest problems. There are a lot of unknowns right now, but at Wilbur Labs, we believe this is the best time to start a company â€“ ever. </p><p>Wilbur Labs is a startup studio turning bold ideas into market-leading companies. Since 2016 we have built and invested in 15 technology companies, including <a href="https://www.vacationrenter.com/">VacationRenter</a>, <a href="https://www.vitabox.com/">Vitabox</a>, <a href="https://www.joblist.com/">Joblist</a>, <a href="https://www.barkbus.com/">Barkbus</a>, and more.</p><p>We plan to launch several companies every year, which requires building a solid pipeline of ambitious ideas. Over the years, we have used three primary approaches to get startup ideas: build for the future, solve existing problems better, or solve personal problems.</p></div><h2>1. Build for the Future</h2><div><div><p>The future is not static. It is built by people who have a vision for what the future should look like, and those who have the willingness to work hard and take the risks necessary to get there. A great way to get startup ideas is to sit down and think about what the future should look like. Ask yourself the following questions: </p><p><em>How are behaviors changing? Which changes will stick? </em><br><em>What long-term problems have been created as a result of the COVID-19 pandemic? </em><br><em>What priorities and values have changed?</em><br><em>What are the biggest problems today?</em><br><em>What will be the biggest problems tomorrow?</em><br><em>What new problems can technology solve today?</em><br><em>How can new technology change the way existing problems are solved?</em></p></div><p>2020 is a year that consumer behaviors are changing and evolving at a faster rate than ever before. Emerging trends that would have taken 5 to 10 years to mature are instead maturing overnight. There is a large opportunity to think about changing trends and build companies ahead of the curve. eCommerce, Employment, Insurance, and Mobile On-Demand are just a few industries where we are boosting investments as a result of new long-term trends.</p><p>When thinking about the future, itâ€™s important to think and plan for the long-term. The best ideas can be built into big, sustainable businesses that can solve problems and create value over the long-term. Avoid short-termism, also known as ideas which may seem solid in the short-term but have a short expiration date.</p><p>Barkbus is a great example of a business that is building for the future. Back in 2017, the world was rapidly transitioning to direct-to-your-door services. Despite that, the way most people got their dogs groomed hadnâ€™t changed. Pet parents still went to the salon, which was a time consuming and stressful experience for both dogs and their parents. Barkbus launched to <a href="https://www.wilburlabs.com/announcements/why-we-acquired-barkbus">make mobile grooming the new norm</a> for pet parents and their dogs. </p></div><h2>2. Solve Existing Problems Better</h2><div><p>We believe the best companies solve real problems. These problems may be completely new problems, or they may be existing problems that have not been solved well by existing companies to date.</p><p>Do not discount existing problems when generating startup ideas. Some of the largest, most impactful companies throughout history were built by solving existing problems better than others. Google did not invent the search engine, search ads, web based email, or maps. Apple did not invent the smartphone, the tablet, or the smart watch. Facebook did not invent social networking. The list goes on and on. What these companies did is provide a better experience than the existing companies. At Wilbur Labs, we refer to this as â€œcreating a 10x experience.â€� </p><p>Next time you have an idea and someone says, â€œdoesnâ€™t X do that?â€� â€” instead of throwing away your idea, first decide if thereâ€™s an opportunity to 10x the experience. Competitors are a good sign as it means there is a real problem people care about. Challenging the status quo is a great way to solve big problems and build a large business. </p><p>When we built Joblist, we did so because despite multiple large job boards, the job search process hasnâ€™t changed in 20 years â€” when job listings first moved online. We saw an opportunity to 10x the job search experience by introducing more personalization and collaboration tools to a traditionally generic and lonely process. Job seekers have embraced this new approach, which allowed Joblist to help millions of job seekers by powering over 500,000 new applications <a href="https://www.joblist.com/news/why-we-launched-joblist">before their public launch</a>.</p></div><h2>3. Solve Personal Problems</h2><div><div><p>Many of the best startup ideas are personal problems that you have faced. What are the biggest problems that you wish were solved? How many others face the same problem? If you experience a problem, then you are uniquely positioned to solve it.</p><p>More often than not, entrepreneurship is not a way to get rich quickly. You will likely need to work harder and longer, with higher stress and more at stake than working a regular job. The journey is absolutely worth it for the right person, but itâ€™s important that you care enough about what you are working on enough to dedicate 5 to 10 years of your life. This will be easier if you care deeply about the problem you are solving.</p></div>
<p>A great example of this is why we launched VacationRenter. At Wilbur Labs, we had quarterly company offsites where we book a vacation rental to work for a few days in a different city. Every time we searched for a rental, it took hours of planning and work. We found ourselves scrolling through pages of search results and jumping from site to site, trying to find the ideal place for the best price. This was a frustrating experience and we knew we could make the search for a vacation rental better for others like us â€” which is why <a href="https://www.vacationrenter.com/news/why-we-launched-vacationrenter">we launched VacationRenter</a>. If you experience a problem, it is likely that others are experiencing it too. </p>
<p>VacationRenter became the fastest growing travel startup ever, generating over $1 billion gross bookings in 2020. This milestone confirms that we werenâ€™t the only travelers who had a frustrating experience finding the perfect rental.</p></div><h2>Next Steps</h2><div><p>At Wilbur Labs, the best startup ideas check a few key boxes:</p><ul><li><strong>Solves a problem.</strong> Could your idea solve a big problem, and have a large positive impact on a significant number of people?</li><li><strong>Builds a sustainable business. </strong>Can the idea be turned into a sustainable business that creates value over the long-term?</li><li><strong>Uniquely positioned.</strong> Are you uniquely positioned to solve the problem and you want to dedicate 5 to 10 years of your life?</li></ul></div><figure><img alt="best-startup-ideas-venn-diagram" src="https://storage.googleapis.com/wl-blog/images/ideas-venn-diagram.png"></figure><div><p>No matter how good an idea is, we believe that ideas alone are worthless unless you take initiative to execute. Execution is everything. Everyone has big ideas â€” no startup ideas are truly unique. Being able to execute on those ideas is what will turn that idea into a business.</p><p>For any idea, the most important next step is thorough research where you pair your initial idea with independent and external information. To read more about how to evaluate and research a startup idea, and how to take the next step, follow our blueprint on <a href="https://www.wilburlabs.com/blueprints/how-to-turn-an-idea-into-a-business">How to Turn An Idea Into A Business</a>.</p><p>Make sure to follow Wilbur Labs on <a href="https://www.linkedin.com/company/wilbur-labs/">LinkedIn</a>, <a href="https://twitter.com/wilburlabs">Twitter</a>, <a href="https://www.facebook.com/wilburlabs/">Facebook</a>, or <a href="http://instagram.com/wilburlabs">Instagram</a>, as we continue to release more blueprints in the future.</p><p>Want to work with us or have an idea? We are always looking for talented people to work with and exciting projects to partner on. Feel free to check out our <a href="https://www.wilburlabs.com/careers">available openings</a> or <a href="https://www.wilburlabs.com/contact">contact us</a>.</p></div></div><div><p>Wilbur Labs is a San Francisco-based startup studio. We turn bold ideas into market-leading companies.</p><p><a href="https://www.wilburlabs.com/about">Learn More â†’</a></p></div></div>]]>
            </description>
            <link>https://www.wilburlabs.com/blueprints/how-to-get-startup-ideas</link>
            <guid isPermaLink="false">hacker-news-small-sites-26085118</guid>
            <pubDate>Wed, 10 Feb 2021 02:25:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Turtle visual cortex is non-retinotopic]]>
            </title>
            <description>
<![CDATA[
Score 98 | Comments 15 (<a href="https://news.ycombinator.com/item?id=26084739">thread link</a>) | @awinter-py
<br/>
February 9, 2021 | https://blog.jordan.matelsky.com/365papers/141/ | <a href="https://web.archive.org/web/*/https://blog.jordan.matelsky.com/365papers/141/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                            <div>
                                <div>
                                    <p>Most of the visual cortex studies I’ve read have been about mammalian vision. But mammals are not the only organisms with cortex, or at least tissue resembling cortex. This paper explores <em>turtle</em> visual cortex, in an area known as dCx. This dorsal region plays the role of the turtle primary vision center — as close as you can find to the homolog of human V1.</p>

<p>But unlike V1, Fournier &amp; Müller et al confirm the existing knowledge that while dCx receives direct inputs from LGN, its layout is <em>not</em> retinotopic. They expand upon this understanding by recording from both individual cells (with electrophysiological recording) and populations of cells. They show that, aside from some slight preferences of caudal dCx for rostral LGN inputs (and vice versa), dCx doesn’t have any clear layout pattern.</p>

<p>This is interesting because the simple, three-layer cortex of a reptile is a much simpler substrate for understanding vision than the complex, deep cortex of mammals. What, if not “pixel-level” information, is dCx responding to?</p>

<p>While the turtles watched movies, the researchers watched the turtles. In particular, they noticed that certain cells of dCx responded to <em>scene-level</em> stimuli: When there was a cut, or the film started or stopped, there was a burst of activity. When the same stimulus was played repeatedly, these responses died away.</p>

<p>This style of scene-level parsing in mammals is reserved for “later”, down-stream brain areas. Does this mean that turtles are using dCx as both early signal processing as well as higher-level object recognition and interpretation? If so, does this mean that mammalian brains, with discrete areas for low-level and high-level vision, evolved from an earlier system where all of these tasks were colocated?</p>

                                </div>
                            </div>
                        </div></div>]]>
            </description>
            <link>https://blog.jordan.matelsky.com/365papers/141/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26084739</guid>
            <pubDate>Wed, 10 Feb 2021 01:15:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[iRacing Telemetry with F#]]>
            </title>
            <description>
<![CDATA[
Score 45 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26083841">thread link</a>) | @todsacerdoti
<br/>
February 9, 2021 | https://markjames.dev/2021-02-09-iracing-telemetry-fsharp/ | <a href="https://web.archive.org/web/*/https://markjames.dev/2021-02-09-iracing-telemetry-fsharp/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article role="main">
        <p>During the pandemic, I’ve been spending quite a bit of time playing <a href="https://iracing.com/">iRacing</a> in VR. I love the realism of iRacing (and how well it supports VR!), and managed to score a win in the Mazda MX-5 this season at the (sadly) now defunct Oran Park Raceway near Sydney.</p>

<p><img src="https://markjames.dev/img/posts/race-result-mazda.png" width="600" height="150" alt="First place at Oran Park Raceway"></p>

<p>In a <a href="https://markjames.dev/2021-01-08-writing-an-iracing-sdk-implementation-fsharp/">previous post</a>, I discussed how I was looking to create an F# library for the iRacing SDK. This proved to be a bigger challenge than I thought as the iRacing API uses a memory mapped file and I’m not yet familiar enough with F# to be tackling the problem yet.</p>

<p>As an alternative (and thanks to F#’s interoperability with C# and the .NET platform), I decided to familiarize myself with this <a href="https://github.com/NickThissen/iRacingSdkWrapper">C# iRacing SDK</a> and build a small app that gathers some basic telemetry data and writes it to a CSV file which I could then plot in a .NET Interactive Notebook using Plotly.NET. The end result of my first few tests looked like this:</p>


    
        <!-- Plotly.js -->
        <meta http-equiv="X-UA-Compatible" content="IE=11">
        
        
    
    
      
     
    


<p>The entire process was refreshingly simple, but there were a few things that had stumped me as I had still been thinking in C# (which I’m more familiar with). For example, here’s the code which listens to an event from the iRacing SDK and then calls a function which parses and then writes telemetry data to a CSV:</p>

<div><div><pre><code><span>// Get the current speed in m/s and convert to a rounded km/h.</span>
<span>let</span> <span>getSpeed</span> <span>(</span><span>evArgs</span><span>:</span> <span>SdkWrapper</span><span>.</span><span>TelemetryUpdatedEventArgs</span><span>)</span> <span>=</span>
    <span>let</span> <span>speed</span> <span>=</span> <span>float</span> <span>evArgs</span><span>.</span><span>TelemetryInfo</span><span>.</span><span>Speed</span><span>.</span><span>Value</span>
    <span>let</span> <span>speedInKMh</span> <span>=</span> <span>speed</span> <span>*</span> <span>3</span><span>.</span><span>6</span>
    <span>let</span> <span>speedRounded</span> <span>=</span> <span>System</span><span>.</span><span>Math</span><span>.</span><span>Round</span> <span>(</span><span>speedInKMh</span><span>,</span> <span>0</span><span>)</span> 
    <span>speedRounded</span>

<span>// Get the throttle input and round it.</span>
<span>let</span> <span>throttleValue</span> <span>(</span><span>evArgs</span><span>:</span> <span>SdkWrapper</span><span>.</span><span>TelemetryUpdatedEventArgs</span><span>)</span> <span>=</span>
    <span>let</span> <span>throttle</span> <span>=</span> <span>float</span> <span>evArgs</span><span>.</span><span>TelemetryInfo</span><span>.</span><span>Throttle</span><span>.</span><span>Value</span>
    <span>let</span> <span>roundedThrottle</span> <span>=</span> <span>System</span><span>.</span><span>Math</span><span>.</span><span>Round</span> <span>(</span><span>throttle</span><span>,</span> <span>2</span><span>)</span>
    <span>roundedThrottle</span>

<span>// Get the lapdistance and round it.</span>
<span>let</span> <span>getLapDistance</span> <span>(</span><span>evArgs</span><span>:</span> <span>SdkWrapper</span><span>.</span><span>TelemetryUpdatedEventArgs</span><span>)</span> <span>=</span>
    <span>let</span> <span>distance</span> <span>=</span> <span>float</span> <span>evArgs</span><span>.</span><span>TelemetryInfo</span><span>.</span><span>LapDistPct</span><span>.</span><span>Value</span>
    <span>let</span> <span>roundedDistance</span> <span>=</span> <span>System</span><span>.</span><span>Math</span><span>.</span><span>Round</span> <span>(</span><span>distance</span><span>,</span> <span>2</span><span>)</span>
    <span>roundedDistance</span>

<span>// Append the output of the current tick to a CSV file.</span>
<span>let</span> <span>writeToCsv</span> <span>(</span><span>evArgs</span><span>:</span> <span>SdkWrapper</span><span>.</span><span>TelemetryUpdatedEventArgs</span><span>)</span> <span>=</span>
    <span>let</span> <span>dataToWrite</span> <span>=</span> <span>$</span><span>"{getSpeed evArgs},{evArgs.TelemetryInfo.Gear.Value},{throttleValue evArgs},{evArgs.TelemetryInfo.Lap.Value},{getLapDistance evArgs}</span><span>\n</span><span>"</span>
    <span>File</span><span>.</span><span>AppendAllText</span> <span>(</span><span>"""C:</span><span>\</span><span>LapTimes.csv"""</span><span>,</span> <span>dataToWrite</span><span>)</span>
    <span>printfn</span> <span>"Wrote to CSV"</span>

<span>let</span> <span>start</span> <span>()</span> <span>=</span>
    <span>// Bind an instance of iRacing SdkWrapper to iRacing</span>
    <span>let</span> <span>iracing</span> <span>=</span> <span>new</span> <span>SdkWrapper</span><span>(</span><span>TelemetryUpdateFrequency</span> <span>=</span> <span>2</span><span>.</span><span>0</span><span>)</span>
    <span>iracing</span><span>.</span><span>TelemetryUpdated</span>
    <span>// Listen to the iRacing telemetry updates</span>
    <span>|&gt;</span> <span>Observable</span><span>.</span><span>subscribe</span> <span>writeToCsv</span> <span>|&gt;</span> <span>ignore</span>
    <span>iracing</span><span>.</span><span>Start</span><span>()</span>
</code></pre></div></div>
<p>The first thing to note is the structure of the code. F# code is structured from the bottom up (the same applies for files in a project), and I find that this helps with readability and reasoning about the code.</p>

<p>Another thing to consider is how to set the update frequency (in updates per second). In C#, we would update a variable to set the telemetry update frequency like so:</p>
<div><div><pre><code><span>iracing</span><span>.</span><span>TelemetryUpdateFrequency</span> <span>=</span> <span>2.0</span><span>;</span> <span>// Updates per second</span>
</code></pre></div></div>
<p>However, In F#, we bind names to expressions as opposed to assigning variables. Then how can we set an update frequency for the C# library without using a mutable? The answer is simple, pass it in as a parameter to the SdkWrapper constructor:</p>

<div><div><pre><code><span>let</span> <span>iracing</span> <span>=</span> <span>new</span> <span>SdkWrapper</span><span>(</span><span>TelemetryUpdateFrequency</span> <span>=</span> <span>2</span><span>.</span><span>0</span><span>)</span> <span>// Receive events at a rate of two updates/sec.</span>
</code></pre></div></div>

<p>Next, note the use of Observable.subscribe to listen to an event. In C#, we would likely do something like this to subscribe to the event using an event handler:</p>

<div><div><pre><code><span>class</span> <span>Program</span> 
<span>{</span>
    <span>private</span> <span>readonly</span> <span>Sdkwrapper</span> <span>iracing</span><span>;</span>

    <span>static</span> <span>void</span> <span>Main</span><span>(</span><span>string</span><span>[]</span> <span>args</span><span>)</span>
    <span>{</span>    
        <span>iracing</span> <span>=</span> <span>new</span> <span>SdkWrapper</span><span>;</span>
        <span>iracing</span><span>.</span><span>TelemetryUpdated</span> <span>+=</span> <span>OnTelemetryUpdated</span><span>;</span>
        <span>iracing</span><span>.</span><span>Start</span><span>();</span>
    <span>}</span>

    <span>private</span> <span>void</span> <span>OnTelemetryUpdated</span><span>(</span><span>object</span> <span>sender</span><span>,</span> <span>SdkWrapper</span><span>.</span><span>TelemetryUpdatedEventArgs</span> <span>e</span><span>)</span>
    <span>{</span>
        <span>// Use live telemetry</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>In F#, we subscribe to the event using Observable.subscribe and call our writeToCsv function each time the event is fired:</p>
<div><div><pre><code>    <span>iracing</span><span>.</span><span>TelemetryUpdated</span>
    <span>|&gt;</span> <span>Observable</span><span>.</span><span>subscribe</span> <span>writeToCsv</span> <span>|&gt;</span> <span>ignore</span>
</code></pre></div></div>

<p>The interop between C# and F# events is nice, and I find this solution to be quite elegant. In addition, you also have Observable.scan which accumulates state each time an event has fired. I haven’t had to do that in this example, but you can see <a href="https://fsharpforfunandprofit.com/posts/concurrency-reactive/">more details here</a>.</p>

<h2 id="plotting-the-telemetry-data">Plotting the Telemetry Data</h2>

<p>Now that we have a backend system logging some telemetry data from the sim, the next step is to plot the graph we saw above. In order to do so, I created a new .NET Interactive notebook in VS Code Insiders. I then imported the libraries I needed and wrote the following initial code:</p>
<div><div><pre><code><span>[&lt;</span><span>Literal</span><span>&gt;]</span>
<span>let</span> <span>FilePath</span> <span>=</span> <span>"""C:</span><span>\</span><span>LapTimes.csv"""</span>

<span>type</span> <span>rawCsv</span> <span>=</span> <span>CsvProvider</span><span>&lt;</span><span>FilePath</span><span>,</span> <span>HasHeaders</span> <span>=</span> <span>true</span><span>&gt;</span>

<span>// CSV File</span>
<span>let</span> <span>lapPerformance</span> <span>=</span> <span>rawCsv</span><span>.</span><span>GetSample</span><span>()</span>
</code></pre></div></div>
<p>The use of [&lt;Literal&gt;] here is because FilePath must be a constant so that the CsvProvider can read the data while we’re developing. As I mentioned in a previous article on <a href="https://markjames.dev/2021-01-23-plotting-csv-files-fsharp/">CSV files in F#</a>, “Type providers are a blessing and curse in F#. On one hand, they’re amazing, because you get compile-time types for your data! But, that also means the data must be available at compile time. You can usually work around this by either:</p>
<ul>
  <li>Including representative data inside your project’s git repo, so you can build the provider based on sample data and then parse any conforming input data</li>
  <li>Using a string literal in source code to define sample data and use that for the provider (which is what I’ve done here).”</li>
</ul>

<p>I also added some column headers into my CSV file (in the future, I plan to write these automatically in the backend code). We also create a new CsvProvider, and then get the data by calling GetSample() on the raw CSV file.</p>

<p>Next, let’s write the createChart function and then pass it out formatted CSV file:</p>

<div><div><pre><code><span>let</span> <span>createChart</span><span>(</span><span>lapPerformance</span><span>:</span> <span>rawCsv</span><span>)</span> <span>=</span> 

    <span>let</span> <span>speed</span> <span>=</span> <span>lapPerformance</span><span>.</span><span>Rows</span> <span>|&gt;</span> <span>Seq</span><span>.</span><span>filter</span> <span>(</span><span>fun</span> <span>row</span> <span>-&gt;</span> <span>row</span><span>.</span><span>Lap</span> <span>=</span> <span>2</span> <span>)</span>
    <span>speed</span>
    <span>// Map Lap Distance and Speed to X and Y on the line chart.</span>
    <span>|&gt;</span> <span>Seq</span><span>.</span><span>map</span> <span>(</span><span>fun</span> <span>row</span> <span>-&gt;</span> <span>row</span><span>.</span><span>LapDistance</span><span>,</span> <span>row</span><span>.</span><span>Speed</span><span>)</span> 
    <span>|&gt;</span> <span>Chart</span><span>.</span><span>Spline</span>
    <span>|&gt;</span> <span>Chart</span><span>.</span><span>withTitle</span> <span>"Laguna Seca (Ferrari 488 GTE)"</span>
    <span>|&gt;</span> <span>Chart</span><span>.</span><span>withX_AxisStyle</span> <span>(</span><span>"Lap Distance (%)"</span><span>,</span> <span>Showgrid</span><span>=</span><span>false</span><span>,</span><span>Position</span><span>=</span><span>200</span><span>.</span><span>0</span><span>)</span>
    <span>|&gt;</span> <span>Chart</span><span>.</span><span>withY_AxisStyle</span> <span>(</span><span>"Speed (Km/h)"</span><span>,</span> <span>Showgrid</span><span>=</span><span>false</span><span>)</span>
    <span>|&gt;</span> <span>Chart</span><span>.</span><span>withMargin</span><span>(</span><span>Margin</span><span>.</span><span>init</span><span>(</span><span>120</span><span>,</span> <span>100</span><span>,</span> <span>50</span><span>,</span> <span>150</span><span>,</span> <span>0</span><span>,</span> <span>true</span><span>))</span>
    <span>|&gt;</span> <span>Chart</span><span>.</span><span>withSize</span> <span>(</span><span>900</span><span>.</span><span>0</span><span>,</span> <span>600</span><span>.</span><span>0</span><span>)</span>
    <span>|&gt;</span> <span>Chart</span><span>.</span><span>Show</span>
    
<span>createChart</span><span>(</span><span>lapPerformance</span><span>:</span> <span>rawCsv</span><span>)</span> 
</code></pre></div></div>
<p>The first thing I do is make use of a higher order function, Seq.filter in order to only include lap two for plotting. Thanks to type providers, we can reference columns (e.g. row.Lap) directly while developing! I find this super handy as I no longer need to reference the file itself to figure out which columns are which.</p>

<p>Next, we construct a pipeline using the forward piping operator (|&gt;) to map the lap distance and speed variables as X and Y values on a line chart, and then define how the chart should be styled. I really like the forward piping operator, and I find that it produces some really clean and concise code.</p>

<h2 id="exploratory-data-with-deedle">Exploratory Data with Deedle</h2>

<p>For a project at work, I’ve been exploring the use of <a href="https://bluemountaincapital.github.io/Deedle/">Deedle</a> as an alternative to Python’s Pandas, and it could be a good library to make use of here to explore telemetry data in greater detail.</p>

<p>The first thing I tried using Deedle, was seeing what my average lap speed was across laps 1-4 at the Laguna Seca Racetrack:</p>

<div><div><pre><code><span>let</span> <span>lapCsv</span> <span>=</span> <span>Frame</span><span>.</span><span>ReadCsv</span><span>(</span><span>"""C:</span><span>\</span><span>LapTimes.csv"""</span><span>,</span> <span>hasHeaders</span> <span>=</span> <span>true</span><span>)</span>

<span>let</span> <span>speed</span> <span>=</span> <span>lapCsv</span><span>.</span><span>GetColumn</span><span>&lt;</span><span>int</span><span>&gt;(</span><span>"Speed"</span><span>)</span>
<span>speed</span> 
<span>|&gt;</span> <span>Stats</span><span>.</span><span>mean</span>
<span>|&gt;</span> <span>printf</span> <span>"Average speed: %A Km/h"</span>
</code></pre></div></div>

<p>The results were:</p>
<div><div><pre><code>Average speed:
121.036 Km/h
</code></pre></div></div>
<p>In this example, we’re loading our CSV file into a Deedle Dataframe, and then binding the Speed column to speed. Next, we make use of pipeline operator to get the mean of the column and print that to the console.</p>

<h2 id="next-steps">Next Steps</h2>

<p>Although there are much better telemetry systems out there, working on my own simple version has allowed me to better see the differences between C# and F#, while at the same time seeing how I can use C# libraries in F# in order to accomplish different tasks. In addition, It would be a good idea to buffer the output as opposed to writing to the file each tick, but since its only two updates per second and doesn’t need to scale, I’m okay with leaving it as is for now.</p>

<p>Moving forward, the next step is to parse the CSV file and then plot several laps together to analyze breaking points and speed on the straights to see if I can improve on any specific areas of the track.</p>

<p>Lastly, thanks to F# great interop with C#, I’m not so sure that there’s a need to port the library to F#, and I might instead look towards some other areas of F# where there’s more of a need for a library.</p>

      </article></div>]]>
            </description>
            <link>https://markjames.dev/2021-02-09-iracing-telemetry-fsharp/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26083841</guid>
            <pubDate>Tue, 09 Feb 2021 23:16:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Finding Covid-19 vaccine availability]]>
            </title>
            <description>
<![CDATA[
Score 19 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26082306">thread link</a>) | @hyolyeng
<br/>
February 9, 2021 | https://infinitus.ai/blog-posts/vaccinateca-covid19-automation | <a href="https://web.archive.org/web/*/https://infinitus.ai/blog-posts/vaccinateca-covid19-automation">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>How Infinitus helped VaccinateCA improve their coverage of Covid-19 vaccination sites by automating calls to 2500 pharmacies every day.</p></div><div><div><p>It all started when our CEO Ankit was tagged on a tweet:</p><figure><p><img src="https://uploads-ssl.webflow.com/5edab43874bee849d9301d27/601a428d57edde4b089996f4_I_MtQLyy7YWzCtfAiaxFhcmMkrcwEnW-98Us4QJYixOqGaFqeq3uTcxZo6pkaw78gO4bKrJR8Fffvwonapk3gpmSk-XzoQWgOP7hZUbqzZvZC2DsT2rrEqNVB17lLpnKQxCCLMz3.png" alt=""></p></figure><p>At Infinitus, we automate repetitive, routine phone calls so our customers can focus on the more creative and empathetic parts of their businesses. Over the past two years, we have built a platform that collects data on behalf of tens of thousands of healthcare providers to help their patients access and afford therapies as efficiently as possible. When we learned that VaccinateCA volunteers were calling hundreds of local pharmacies daily to check for vaccine availability, we asked ourselves: how can our platform help this volunteer effort?</p><figure id="w-node-_5486cc49-4b64-1119-94c9-d7485b84ba76-f67b3b60"><p><img src="https://uploads-ssl.webflow.com/5edab43874bee849d9301d27/601a4a9679f268411d14b00f_Screen%20Shot%202021-02-02%20at%2010.59.30%20PM.png" loading="lazy" alt=""></p><figcaption>Timeline from first contact to production and a transcript of one of our first successful test calls</figcaption></figure><p>The VaccinateCA phone calls can be broken into two parts - first, identifying whether the vaccine is available, and second, how to get the vaccine. The majority of phone calls never make it past the first question. In fact, volunteers were only getting a 10% hit rate over the 200 calls they were able to make in a day. What if we could help the volunteers focus on the more exploratory work of figuring out who can get vaccines and how, and let us figure out where the vaccine is available?&nbsp;&nbsp;</p><p>Here’s how we built it:</p><h2>Phase 1. Adapt Infrastructure for New Use Case</h2><p>The Infinitus VoiceRPA platform is designed to automate phone calls. While our system is currently tuned for a few use cases, one of our goals for 2021 is to make a platform where we can quickly configure and automate any new call type. We planned to leverage our existing system as much as possible so we could get this project out the door fast. We created a new Covid Vaccination call type and a workspace for storing the data.&nbsp;</p><figure><p><img src="https://uploads-ssl.webflow.com/5edab43874bee849d9301d27/601a428dfb556cc89a26ef76_MH7Dyc-9NLpmZIatKQLSvAZsfA_ADFdYaIG7GImrhv2dWWbM-FfMT5Dl0S0rNx3Ua-sinCe-MpmQsHOEmkalMMcoYhIkMkEvABXU8D2CVnOu-XC98Mkg6csDcGPw-4vb3S-mhbKt.png" alt=""></p><figcaption>The Infinitus Technology Stack</figcaption></figure><p>The volunteers at VaccinateCA provided our team with a list of thousands of hospitals, clinics, and pharmacies that they were calling. We limited our scope to big pharmacy chains which have a large number of locations with standardized IVR (<a href="https://en.wikipedia.org/wiki/Interactive_voice_response">Interactive Voice Response</a>) systems. They were also the most likely places to have vaccine availability across the state.&nbsp;</p><p>Within a day, the Covid Vaccination workspace was filled with pharmacies and phone numbers, all displayed in the same internal tools currently used by our data labeling team.</p><h2>Phase 2. Refine the Conversation Model</h2><p>The volunteers at VaccinateCA follow this script for their phone calls:</p><ul role="list"><li>Is the vaccine available for those 65 years and older?</li><li>Where can appointments be made?</li><li>Are walk-ins accepted?&nbsp;</li><li>What documentation is needed?</li></ul><p>We dedicated two members of our data labeling team to prepare the data to call pharmacies. They guided our digital assistant, Eva, through calls to (a) generate training data for our NLP team to automate future calls and (b) validate if pharmacists would be willing to talk to a digital assistant. Here’s what an early call sounded like:</p><p>The results were promising - pharmacists were providing the information we were looking for, but, as expected, we encountered a few challenges:</p><h4>1. Some pharmacists were not sure what to do when a digital assistant called them.</h4><figure id="w-node-b7e45f80-7c74-e54e-32f5-8797a3059363-f67b3b60"><p><img src="https://uploads-ssl.webflow.com/5edab43874bee849d9301d27/601a428d108eb65d7911a316_XPM4l0fV9unhDxCqe-uyBPL5ZRVMBDY2a-_00VDUM1uquJjZNHCVFBhJf9Ra9SclJM7upP4EuUcJ2Rpcmid80O_s2wmZ9XbBMQrBq90ChQCzZ1d1HjTiuxhjVBv9B5CcgjvO96Iu.png" alt=""></p></figure><p>Between 25-30% of pharmacists who heard Eva’s automated voice, hung up without interacting with her.&nbsp;</p><p>To combat these outright hangups, we followed up with those pharmacies to explain who we are calling on behalf of and why, and asking if in the future they would be willing to talk to our digital assistant.&nbsp;</p><p>‍</p><p>‍</p><p>‍</p><h4>2. Difficulty detecting when to start asking questions.<br></h4><p>A person can easily differentiate between a pharmacist, an IVR system, or when they’re on hold, because humans subconsciously use signals like how ‘real’ a voice sounds and background noise. Humans also use signals such as pauses and changing intonations as cues to know when someone is ready for us to ask a question.<br></p><p>Our current Natural Language Understanding model processes incoming voice through a speech-to-text engine first, thereby eliminating some of the audio-based nuances a human can use to guide a conversation. For some pharmacy calls it is hard to tell from just the text if the pharmacist is ready for questions, or if they still need us to wait on hold.<br></p><p>‍</p><figure id="w-node-e8df9944-846d-a16f-b8f6-397c33be13c7-f67b3b60"><p><img src="https://uploads-ssl.webflow.com/5edab43874bee849d9301d27/601a428d57edde3aa69996f5_2R137_qS0jIN5Stj5eP8EdQXxvr6bHXOQmWrNXDXOHJyIi5PtJ0YuYVmvVjsEk8vvep9tcw3rE2nnWCad2lg90OKuDL7xKPKX_HEDvpcHG57GKLMtPeQw866Hx64zSLrRMzdG5FK.png" alt=""></p></figure><p>Consider the following examples of common phrases that can appear in the IVR, but are also commonly used by pharmacists:</p><p><strong>“How can I help you today?”</strong></p><p>In this case, the system expects a button press or a phrase like “speak to a pharmacist” to help direct the call. However, a pharmacist may use the exact same verbiage when picking up the phone, indicating that we should start asking our vaccine related questions.<br></p><p><strong>“Please hold for a moment.”</strong></p><p>If we interpret this as something to respond to, the IVR system can route our call in unexpected ways. However, if a pharmacist asks us to continue to hold and doesn’t hear a response right away, they often hang up.&nbsp;<br></p><p><strong>“Thank you for calling Rite Aid.”</strong></p><p>If we hear this in the IVR, we want to wait and listen to the menu options and press the appropriate number on a keypad. However, pharmacists also say this when picking up the phone. It is critical to ask questions as soon as we are connected to a pharmacist, otherwise they assume no one is on the line and will hang up.</p><p>‍</p><h4>3. Variation in responses from free-form questions</h4><figure id="w-node-_6c6157e9-3177-b5f2-8e36-9aad4be7a260-f67b3b60"><p><img src="https://uploads-ssl.webflow.com/5edab43874bee849d9301d27/601a428d59087960f81bd2d2_q_yj6nvFeOONto65FyIf6brM9LzBd2clk_84qdS71j074E-78rU1ifwmroEQmzGS1IuHoDOPoFOOrMRPaJslDBhMrUzqcTvn1yycIHzXvp0pIC9Be3-Yy5NoKSGtokqrLARF-8HJ.png" alt=""></p></figure><p>Responses to yes or no questions can be understood at a much higher confidence with a smaller set of training data than open-ended questions. For example, the answer to “Where can appointments be made?” results in harder to understand responses like “Do the County of Alameda.”, which was a mistranscription of something like “Through the County of Alameda [website]”.</p><p>We also noticed that the chances of a pharmacist hanging up on us increased with the length of the call. Combined with the insight that the majority of volunteers’ phone calls never made it past the first question in the script, we realized we could streamline volunteer efforts by automating the question, “Is the vaccine available for those 65 and older?” Eva could pre-screen a much larger pool of sites, and have the volunteers follow-up only in cases where the answer was “yes”.&nbsp;</p><p>‍</p><p>‍</p><h2>Phase 3. Full Automation</h2><p>To automatically navigate IVRs, we collected the scripts that the IVR systems follow. As long as a location belongs to a pharmacy chain (e.g. Rite Aid) whose IVR we have automated, we know what the IVR will say, except for some minor variations like the location’s address. Using fuzzy matching, we built a state machine which responds with the appropriate key press or voice response until we are put on hold to connect to a pharmacist.<br></p><figure id="w-node-_8d2d4741-0562-fd84-5c48-0d60d3f721cb-f67b3b60"><p><img src="https://uploads-ssl.webflow.com/5edab43874bee849d9301d27/601a428e4e028a520cbea1bf_-FeYyvb_mX7wjwX-Zk85DYOpRlGnj9JVe12V4GWeqbBfIjLQ3p2ygjhoyr3sirf5yM0RaFTxuCDSJEbIptL1PUw12Kr2WoMee9H02yKPi98ZQurlsjJUtflNq4phrPcLQ7R49Xj2.png" alt=""></p></figure><p>Once on hold, we get a continuous speech-to-text transcript and pass it to our trained model. If someone picks up and says something like “Hi, how can I help you today?”, the model classifies it as a greeting and Eva asks the covid vaccine availability question. We got around the confusion of phrases used in the IVR and by humans picking up by enabling the hold model only once the IVR portion of the call is complete.</p><p>Finally, for the conversation itself, we focused on one question: “Does your location have the covid-19 vaccine?” Because the volunteers use this data to determine which sites to call back, we want it to be as accurate as possible. If the system gets a response that it doesn’t understand yet (it’s still improving), it will ask a follow up question “Could you please answer yes or no. Do you have covid-19 vaccines?” If we still don’t get an understandable answer, we call back another time.<br></p><p>Each day, Eva calls pharmacies that haven’t been confirmed to have vaccines yet (since this information changes daily). These calls are now fully automated, from navigating each pharmacy’s automated IVR, detecting when a pharmacist is on the line, and finally extracting the response from the pharmacist and saying bye before hanging up.<br></p><p>We have been supplying the results of our calls to the volunteers at VaccinateCA, who then prioritize positive cases to confirm additional details (such as who’s eligible, how to make appointments, etc.).&nbsp;</p><h2>Beyond California</h2><p>Last week, we expanded our support to volunteer groups mirroring the efforts of VaccinateCA. We are now supporting groups in Colorado, Florida, Michigan, New Jersey, New York, Pennsylvania and Virginia.&nbsp;</p><h2>Our Impact</h2><figure id="w-node-_9943b8a9-7511-961f-4f76-5d17724f8253-f67b3b60"><p><img src="https://uploads-ssl.webflow.com/5edab43874bee849d9301d27/601a428e10a41b21690f668e_PbhUak00017R-LVSLvuO1md5n7GtubjnigyLepLSVf5Gbi4pIAJ7IdAqOxu2aNEGb0I0i7Hdg61Ejam2BPyhT2F4Az_EFGKC-zLOzmbSCALoQ5M93swtpog5DAJV5n_zolRoZduO.png" alt=""></p><figcaption>Comment about Eva in the VaccinateCA Discord channel</figcaption></figure><p>Over the past two weeks, we have made over 10,000 calls to pharmacies across 8 states. After the initial implementation phase, we’ve mostly been able to sit back and let the machine do its job. The data we’ve provided back to the group has proved immensely valuable, increasing the efficacy of their volunteer callers by an order of magnitude.</p><blockquote>Working with Infinitus has been game-changing for our effort at VaccinateCA! In under a week, their digital assistant automated daily phone calls to 2500 pharmacies across California. Using Eva means our callers, and the medical professionals they speak to, spend more of their time getting the vaccine to Californians and less hunting around for answers.<br>- Patrick McKenzie, CEO&nbsp;VaccinateCA<br></blockquote><p>It’s been fun working on this small side project, adapting and pushing our technology beyond what it was capable prior, while helping our community. We have many more interesting projects like this, and <a href="https://infinitus.ai/careers">we’re hiring</a>!<br></p></div></div></div>]]>
            </description>
            <link>https://infinitus.ai/blog-posts/vaccinateca-covid19-automation</link>
            <guid isPermaLink="false">hacker-news-small-sites-26082306</guid>
            <pubDate>Tue, 09 Feb 2021 20:58:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Python behind the scenes #8: how Python integers work]]>
            </title>
            <description>
<![CDATA[
Score 140 | Comments 12 (<a href="https://news.ycombinator.com/item?id=26081919">thread link</a>) | @rbanffy
<br/>
February 9, 2021 | https://tenthousandmeters.com/blog/python-behind-the-scenes-8-how-python-integers-work/ | <a href="https://web.archive.org/web/*/https://tenthousandmeters.com/blog/python-behind-the-scenes-8-how-python-integers-work/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<!-- /.post-info -->      <p>In the previous parts of this series we studied the core of the CPython interpreter and saw how the most fundamental aspects of Python are implemented. We made an overview of <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-1-how-the-cpython-vm-works/">the CPython VM</a>, took a look at <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-2-how-the-cpython-compiler-works/">the CPython compiler</a>, stepped through <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-3-stepping-through-the-cpython-source-code/">the CPython source code</a>, studied <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-4-how-python-bytecode-is-executed/">how the VM executes the bytecode</a> and learned <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-5-how-variables-are-implemented-in-cpython/">how variables work</a>. In the two most recent posts we focused on <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-6-how-python-object-system-works/">the Python object system</a>. We learned what Python objects and Python types are, how they are defined and what determines their behavior. This discussion gave us a good understanding of how Python objects work in general. What we haven't discussed is how particular objects, such as strings, integers and lists, are implemented. In this and several upcoming posts we'll cover the implementations of the most important and most interesting built-in types. The subject of today's post is <code>int</code>.</p>
<p><strong>Note</strong>: In this post I'm referring to CPython 3.9. Some implementation details will certainly change as CPython evolves. I'll try to keep track of important changes and add update notes.</p>
<h2>Why Python integers are interesting</h2>
<p>Integers require no introduction. They are so ubiquitous and seem so basic that you may doubt whether it's worth discussing how they are implemented at all. Yet, Python integers are interesting because they are not just 32-bit or 64-bit integers that CPUs work with natively. Python integers are <a href="https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic">arbitrary-precision integers</a>, also known as bignums. This means that they can be as large as we want, and their sizes are only limited by the amount of available memory.</p>
<p>Bignums are handy to work with because we don't need to worry about such things as integer overflows and underflows. They are extensively used in fields like cryptography and computer algebra where large numbers arise all the time and must be represented precisely. So, <a href="https://en.wikipedia.org/wiki/List_of_arbitrary-precision_arithmetic_software">many</a> programming languages have bignums built-in. These include Python, JavaScript, Ruby, Haskell, Erlang, Julia, Racket. Others provide bignums as a part of the standard library. These include Go, Java, C#, D, PHP. Numerous third-party libraries implement bignums. The most popular one is <a href="https://en.wikipedia.org/wiki/GNU_Multiple_Precision_Arithmetic_Library">the GNU Multiple Precision Arithmetic Library</a> (GMP). It provides a C API but has bindings for all major languages.</p>
<p>There are a lot of bignum implementations. They're different in detail, but the general approach to implement bignums is the same. Today we'll see what this approach looks like and use CPython's implementation as a reference example. The two main questions we'll have to answer are:</p>
<ul>
<li>how to represent bignums; and</li>
<li>how to performs arithmetic operations, such as addition and multiplication, on bignums.</li>
</ul>
<p>We'll also discuss how CPython's implementation compares to others and what CPython does to make integers more efficient. </p>
<h2>Bignum representation</h2>
<p>Think for a moment how you would represent large integers in your program if you were to implement them yourself. Probably the most obvious way to do that is to store an integer as a sequence of digits, just like we usually write down numbers. For example, the integer <code>51090942171709440000</code> could be represented as <code>[5, 1, 0, 9, 0, 9, 4, 2, 1, 7, 1, 7, 0, 9, 4, 4, 0, 0, 0, 0]</code>. This is essentially how bignums are represented in practice. The only important difference is that instead of base 10, much larger bases are used. For example, CPython uses base 2^15 or base 2^30 depending on the platform. What's wrong with base 10? If we represent each digit in a sequence with a single byte but use only 10 out of 256 possible values, it would be very memory-inefficient. We could solve this memory-efficiency problem if we use base 256, so that each digit takes a value between 0 and 255. But still much larger bases are used in practice. The reason for that is because larger base means that numbers have less digits, and the less digits numbers have, the faster arithmetic operations are performed. The base cannot be arbitrary large. It's typically limited by the size of the integers that the CPU can work with. We'll see why this is the case when we discuss bignum arithmetic in the next section. Now let's take a look at how CPython represents bignums.</p>
<p>Everything related to the representation of Python integers can be found in <a href="https://github.com/python/cpython/blob/3.9/Include/longintrepr.h"><code>Include/longintrepr.h</code></a>. Technically, Python integers are instances of <code>PyLongObject</code>, which is defined in <a href="https://github.com/python/cpython/blob/3.9/Include/longobject.h"><code>Include/longobject.h</code></a>, but <code>PyLongObject</code> is actually a typedef for <code>struct _longobject</code> that is defined in <code>Include/longintrepr.h</code>:</p>
<div><pre><span></span><span>struct</span> <span>_longobject</span> <span>{</span>
    <span>PyVarObject</span> <span>ob_base</span><span>;</span> <span>// expansion of PyObject_VAR_HEAD macro</span>
    <span>digit</span> <span>ob_digit</span><span>[</span><span>1</span><span>];</span>
<span>};</span>
</pre></div>


<p>This struct extends <a href="https://docs.python.org/3/c-api/structures.html#c.PyVarObject"><code>PyVarObject</code></a>, which in turn extends <a href="https://docs.python.org/3/c-api/structures.html#c.PyObject"><code>PyObject</code></a>:</p>
<div><pre><span></span><span>typedef</span> <span>struct</span> <span>{</span>
    <span>PyObject</span> <span>ob_base</span><span>;</span>
    <span>Py_ssize_t</span> <span>ob_size</span><span>;</span> <span>/* Number of items in variable part */</span>
<span>}</span> <span>PyVarObject</span><span>;</span>
</pre></div>


<p>So, besides a reference count and a type that all Python objects have, an integer object has two other members: </p>
<ul>
<li><code>ob_size</code> that comes from <code>PyVarObject</code>; and </li>
<li><code>ob_digit</code> that is defined in <code>struct _longobject</code>.</li>
</ul>
<p>The <code>ob_digit</code> member is a pointer to an array of digits. On 64-bit platforms, each digit is a 30-bit integer that takes values between 0 and 2^30-1 and is stored as an unsigned 32-bit int (<code>digit</code> is a typedef for <code>uint32_t</code>). On 32-bit platforms, each digit is a 15-bit integer that takes values between 0 and 2^15-1 and is stored as an unsigned 16-bit int (<code>digit</code> is a typedef for <code>unsigned short</code>). To make things concrete, in this post we'll assume that digits are 30 bits long.</p>
<p>The <code>ob_size</code> member is a signed int, whose absolute value tells us the number of digits in the <code>ob_digit</code> array. The sign of <code>ob_size</code> indicates the sign of the integer. Negative <code>ob_size</code> means that the integer is negative. If <code>ob_size</code> is 0, then the integer is 0.</p>
<p>Digits are stored in a little-endian order. The first digit (<code>ob_digit[0]</code>) is the least significant, and the last digit (<code>ob_digit[abs(ob_size)-1]</code>) is the most significant.</p>
<p>Finally, the absolute value of an integer is calculated as follows: </p>
<p>$$val = ob\_digit[0] \times (2 ^{30})^0 + ob\_digit[1] \times (2 ^{30})^1 + \cdots + ob\_digit[|ob\_size| - 1] \times (2 ^{30})^{|ob\_size| - 1}$$</p>
<p>Let's see what all of this means with an example. Suppose we have an integer object that has <code>ob_digit = [3, 5, 1]</code> and <code>ob_size = -3</code>. To compute its value, we can do the following:</p>
<div><pre><span></span><span>$ python -q</span>
<span>&gt;&gt;&gt; </span><span>base</span> <span>=</span> <span>2</span><span>**</span><span>30</span>
<span>&gt;&gt;&gt; </span><span>-</span><span>(</span><span>3</span> <span>*</span> <span>base</span><span>**</span><span>0</span> <span>+</span> <span>5</span> <span>*</span> <span>base</span><span>**</span><span>1</span> <span>+</span> <span>1</span> <span>*</span> <span>base</span><span>**</span><span>2</span><span>)</span>
<span>-1152921509975556099</span>
</pre></div>


<p>Now let's do the reverse. Suppose we want to get the bignum representation of the number <code>51090942171709440000</code>. Here's how we can do that:</p>
<div><pre><span></span><span>&gt;&gt;&gt; </span><span>x</span> <span>=</span> <span>51090942171709440000</span>
<span>&gt;&gt;&gt; </span><span>x</span> <span>%</span> <span>base</span>
<span>952369152</span>
<span>&gt;&gt;&gt; </span><span>(</span><span>x</span> <span>//</span> <span>base</span><span>)</span> <span>%</span> <span>base</span>
<span>337507546</span>
<span>&gt;&gt;&gt; </span><span>(</span><span>x</span> <span>//</span> <span>base</span> <span>//</span> <span>base</span><span>)</span> <span>%</span> <span>base</span>
<span>44</span>
<span>&gt;&gt;&gt; </span><span>(</span><span>x</span> <span>//</span> <span>base</span> <span>//</span> <span>base</span> <span>//</span> <span>base</span><span>)</span> <span>%</span> <span>base</span>
<span>0</span>
</pre></div>


<p>So, <code>ob_digit = [952369152, 337507546, 44]</code> and <code>ob_size = 3</code>. Actually, we don't even have to compute the digits, we can get them by inspecting the integer object using the <a href="https://docs.python.org/3/library/ctypes.html#module-ctypes"><code>ctypes</code></a> standard library:</p>
<div><pre><span></span><span>import</span> <span>ctypes</span>


<span>MAX_DIGITS</span> <span>=</span> <span>1000</span>

<span># This is a class to map a C `PyLongObject` struct to a Python object</span>
<span>class</span> <span>PyLongObject</span><span>(</span><span>ctypes</span><span>.</span><span>Structure</span><span>):</span>
    <span>_fields_</span> <span>=</span> <span>[</span>
        <span>(</span><span>"ob_refcnt"</span><span>,</span> <span>ctypes</span><span>.</span><span>c_ssize_t</span><span>),</span>
        <span>(</span><span>"ob_type"</span><span>,</span> <span>ctypes</span><span>.</span><span>c_void_p</span><span>),</span>
        <span>(</span><span>"ob_size"</span><span>,</span> <span>ctypes</span><span>.</span><span>c_ssize_t</span><span>),</span>
        <span>(</span><span>"ob_digit"</span><span>,</span> <span>MAX_DIGITS</span> <span>*</span> <span>ctypes</span><span>.</span><span>c_uint32</span><span>)</span>
    <span>]</span>


<span>def</span> <span>get_digits</span><span>(</span><span>num</span><span>):</span>
    <span>obj</span> <span>=</span> <span>PyLongObject</span><span>.</span><span>from_address</span><span>(</span><span>id</span><span>(</span><span>num</span><span>))</span>
    <span>digits_len</span> <span>=</span> <span>abs</span><span>(</span><span>obj</span><span>.</span><span>ob_size</span><span>)</span>
    <span>return</span> <span>obj</span><span>.</span><span>ob_digit</span><span>[:</span><span>digits_len</span><span>]</span>
</pre></div>


<div><pre><span></span><span>&gt;&gt;&gt; </span><span>from</span> <span>num_digits</span> <span>import</span> <span>get_digits</span>
<span>&gt;&gt;&gt; </span><span>x</span> <span>=</span> <span>51090942171709440000</span>
<span>&gt;&gt;&gt; </span><span>get_digits</span><span>(</span><span>x</span><span>)</span>
<span>[952369152, 337507546, 44]</span>
</pre></div>


<p>As you might guess, the representation of bignums is an easy part. The main challenge is to implement arithmetic operations and to implement them efficiently.</p>
<h2>Bignum arithmetic</h2>
<p>We learned in <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-6-how-python-object-system-works/">part 6</a> that the behavior of a Python object is determined by the object's type. Each member of a type, called slot, is responsible for a particular aspect of the object's behavior. So, to understand how CPython performs arithmetic operations on integers, we need to study the slots of the <code>int</code> type that implement those operations.</p>
<p>In the C code, the <code>int</code> type is called <code>PyLong_Type</code>. It's defined in <code>Objects/longobject.c</code> as follows:</p>
<div><pre><span></span><span>PyTypeObject</span> <span>PyLong_Type</span> <span>=</span> <span>{</span>
    <span>PyVarObject_HEAD_INIT</span><span>(</span><span>&amp;</span><span>PyType_Type</span><span>,</span> <span>0</span><span>)</span>
    <span>"int"</span><span>,</span>                                      <span>/* tp_name */</span>
    <span>offsetof</span><span>(</span><span>PyLongObject</span><span>,</span> <span>ob_digit</span><span>),</span>           <span>/* tp_basicsize */</span>
    <span>sizeof</span><span>(</span><span>digit</span><span>),</span>                              <span>/* tp_itemsize */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_dealloc */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_vectorcall_offset */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_getattr */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_setattr */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_as_async */</span>
    <span>long_to_decimal_string</span><span>,</span>                     <span>/* tp_repr */</span>
    <span>&amp;</span><span>long_as_number</span><span>,</span>                            <span>/* tp_as_number */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_as_sequence */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_as_mapping */</span>
    <span>(</span><span>hashfunc</span><span>)</span><span>long_hash</span><span>,</span>                        <span>/* tp_hash */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_call */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_str */</span>
    <span>PyObject_GenericGetAttr</span><span>,</span>                    <span>/* tp_getattro */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_setattro */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_as_buffer */</span>
    <span>Py_TPFLAGS_DEFAULT</span> <span>|</span> <span>Py_TPFLAGS_BASETYPE</span> <span>|</span>
        <span>Py_TPFLAGS_LONG_SUBCLASS</span><span>,</span>               <span>/* tp_flags */</span>
    <span>long_doc</span><span>,</span>                                   <span>/* tp_doc */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_traverse */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_clear */</span>
    <span>long_richcompare</span><span>,</span>                           <span>/* tp_richcompare */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_weaklistoffset */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_iter */</span>
    <span>0</span><span>,</span>                                          <span>/* tp_iternext */</span>
    <span>long_methods</span><span>,</span>                               <span>/* tp_methods */</span>
    <span>0</span><span>,</span>                                          <span>/* …</span></pre></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-8-how-python-integers-work/">https://tenthousandmeters.com/blog/python-behind-the-scenes-8-how-python-integers-work/</a></em></p>]]>
            </description>
            <link>https://tenthousandmeters.com/blog/python-behind-the-scenes-8-how-python-integers-work/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26081919</guid>
            <pubDate>Tue, 09 Feb 2021 20:14:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Creeping as a Service]]>
            </title>
            <description>
<![CDATA[
Score 315 | Comments 113 (<a href="https://news.ycombinator.com/item?id=26081672">thread link</a>) | @dshipper
<br/>
February 9, 2021 | https://every.to/divinations/creeping-as-a-service-craas | <a href="https://web.archive.org/web/*/https://every.to/divinations/creeping-as-a-service-craas">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <p><em>Hey, Nathan here! Remember a few months ago when we published a wonderful essay on&nbsp;</em><a href="https://every.to/divinations/linkedins-alternate-universe-21780381" rel="noopener noreferrer" target="_blank"><em>LinkedInâ€™s ridiculousness</em></a><em>? It was one of Divinations most popular posts ever, and it was written by </em><a href="https://twitter.com/fadeke_adegbuyi" rel="noopener noreferrer" target="_blank"><em>Fadeke Adegbuyi</em></a><em>, a brilliant observer of internet culture who also is a senior marketing manager at Doist. Today Iâ€™m thrilled to share with you Fadekeâ€™s latest workâ€”this time examining our obsession with identity through the lens of a Twitter bio tracking app called Spoonbill. Enjoy!</em></p><hr><p>Elon Musk updated his Twitter bio 23 times in 2020. He last changed it on February 4, 2021 at 6:31 AM PST. A few versions include â€œBorn 69 days after 4/20,â€� â€œSoundCloud Rockstar,â€� â€œBudgie Smuggler,â€� and â€œ#bitcoin.â€� I didnâ€™t spend months stalking Muskâ€™s page, developing an encyclopedic knowledge of his time spent in the Twitterverse. I looked it up on <a href="http://spoonbill.io/" rel="noopener noreferrer" target="_blank">Spoonbill</a>.&nbsp;</p><p><a href="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_1.png" target="_blank"><img src="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_1.png"></a></p><p>I could use the app to give you similar information on any celebrity or public figure with a Twitter account. Or I could use it on you. If I wanted to, I could see all the changes youâ€™ve made to your profile since you first signed up: your name, your location, your website, your pinned tweets. </p><p><a href="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_2.png" target="_blank"><img src="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_2.png"></a></p><p>Spoonbillâ€™s creator, <a href="https://twitter.com/justinmduke" rel="noopener noreferrer" target="_blank">Justin M. Duke</a>, describes the app, which has nearly 93 thousand users, as a â€œtracking tool for online metadata.â€� Over 45K of those users have signed up to receive daily emails that aggregate updates across all the Twitter profiles they follow. The open rate for these daily emails hovers around 55%, <a href="https://www.campaignmonitor.com/resources/knowledge-base/what-are-the-average-click-and-read-rates-for-email-campaigns/" rel="noopener noreferrer" target="_blank">well above average</a> for email campaigns.</p><p><a href="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_3.png" target="_blank"><img src="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_3.png"></a></p><p>I asked <a href="https://twitter.com/hunterwalk" rel="noopener noreferrer" target="_blank">Hunter Walk</a>, a Partner at the seed-stage venture capital firm Homebrew and an avid Spoonbiller, why he uses the app :</p><blockquote><em>"It's fun to see people wordsmithing their bios, changing the order of the portfolio companies they list based on startup performance, subtly announcing personal life changes...Instead of letting Twitter decide what's worthy of notification, I get to see </em>every<em> change and decide for myself what's interesting, often with context that the algorithm is unaware of."</em></blockquote><p>The tracked changes that Twitterâ€™s algorithm might register as neutral additions and subtractions can indeed be revealing to onlookers. Spoonbill captures what the people we follow are changing their bios to express; an investment exit, a tongue-in-cheek joke, or the end of an internet cult affiliation.</p><p><a href="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_New.png" target="_blank"><img src="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_New.png"></a></p><p>Social apps have made creepers out of all of us. Whether itâ€™s scrolling to the start of someoneâ€™s Instagram feed or <a href="https://sarajbenincasa.medium.com/confessions-of-a-venmo-voyeur-cb0e4c23d04a" rel="noopener noreferrer" target="_blank">looking through Venmo transactions</a>, weâ€™re constantly peering into othersâ€™ online lives. Spoonbill not only satisfies our tendency for online lurking, but pushes it into voyeur territory; surfacing whatâ€™s meant to be hidden is intimate in a way that scrolling a timeline isnâ€™t. The tool provides a glimpse into the specific ways we represent ourselves to the world, the unseen effort with which we express our identities, and how the uncanny feeling of being watched informs our sense of self.</p><p>Twitter is an interesting observation ground for online identity. Snapshots at Big Sur are meant for Instagramâ€™s photogenic feed, while professional milestones belong on <a href="https://divinations.every.to/p/linkedins-alternate-universe" rel="noopener noreferrer" target="_blank">LinkedInâ€™s alternate universe</a>. But in the Twitterverse, life and work converge. People are just as likely to share career news as they are to live-tweet a Netflix binge; professional gripes commingle with criticism of political leaders; snark and sincerity live side-by-side. </p><p><a href="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_4.png" target="_blank"><img src="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_4.png"></a></p><p>Twitterâ€™s bio section, then, is a tall order: <em>compress your entire identity into a single line item</em>. Many use the space to convey their own complexityâ€“â€“smart yet funny, accomplished but approachable. Society has progressed past the need for the phrase â€œworks hard, plays hard,â€� but thatâ€™s what weâ€™re trying to convey in our own personalized way: we contain multitudes. We are real.</p><p>Spoonbill reveals the way that people play with their latest positioning of self, carving out character space for their newly important aspects of identity.</p><p><a href="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_7.png" target="_blank"><img src="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_7.png"></a></p><p>Hustle culture looms large and professional accomplishments convey worth. In turn, notable work, career highlights, and impressive affiliations take center stage in our Twitter bios, the structure and grammar of which are heavily coded. Freelance journalists add and shuffle their bylines, arranged in order of prestige â€“â€“ @nytimes is meant to come first. In my corner of Tech Twitter, where optionality is king, itâ€™s now effectively a meme to be <em>â€œstarting something new,â€�</em> marked increasingly by the addition of â€œSubstack:â€�, â€œScout:â€�, or â€œSide project:â€� to oneâ€™s bio. Users adding â€œ<em>permanent student</em>â€� or â€œ<em>forever learning</em>â€� to their bios arenâ€™t pupils at cruel and unusual institutions, theyâ€™re signalling intellectual curiosity.&nbsp;</p><p><a href="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_8.png" target="_blank"><img src="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_8.png"></a></p><p>The ability to see Twitter bio updates across hundreds or thousands of profiles over time make Spoonbill updates insightful in aggregate. Certain patterns and conventions emerge.&nbsp;</p><p>Spoonbillâ€™s creator has observed interesting trends in his app:</p><blockquote><em>"On an individual/corporate basis, a particularly fun one is seeing angels and investors tinker with the parts of the portfolio they </em><a href="http://spoonbill.io/twitter/data/Jason/" rel="noopener noreferrer" target="_blank"><em>choose to highlight</em></a><em>. A very classic example of this was the sheer number of firms and investors who removed things mentioning WeWork during, well, the period of time that you didnâ€™t want your name associated with WeWork."</em></blockquote><p>WeWork reached a $47 billion valuation in 2019, but cancelled their plans to go public <a href="https://www.businessinsider.com/wework-ipo-timeline-delayed-ceo-adam-neumann-scandals-explained-2019-9" rel="noopener noreferrer" target="_blank">after their IPO filing drew questions and concerns</a>. In a bio block thatâ€™s meant for selling ourselves to a potential follower, a failed investment takes up valuable real estate. But while WeWork investors might wish we would all forget their involvement as quickly as they hit â€œsaveâ€� on their new bio, Spoonbill always knows.</p><p>In that same 160-character space, weâ€™re also meant to declare our commitment to cause: political, social, or economic. Often, emojis will do:</p><p><a href="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_9.png" target="_blank"><img src="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_9.png"></a></p><p>If you watch Spoonbill updates closely enough, they suggest that the labels and descriptors we choose to signal our allegiances can be fleeting, our attention spans short. As COVID-19 spread throughout the world in early 2020, many updated their bios with their own messages about the virus, encouraging their followers to take some form of action:</p><p><a href="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_10.png" target="_blank"><img src="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_10.png"></a></p><p>But almost as quickly as these bio changes were made, they were unmade, individuals returning to their regular profiles. Less charitably, people just got bored; short-term vigilance surrounding the virus eventually led to a return to normal life for many. More charitably, the longer the pandemic wore on, identities once swaddled in concern for the virus eventually needed some wiggle room.&nbsp;</p><p>Last year, as racial tensions bubbled to a boil throughout the United States and set off an international cascade, Spoonbill revealed how Twitter bios were used to show support for what was top of mindâ€“â€“or a stance on what was top of the fold:</p><p><a href="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_11.png" target="_blank"><img src="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_11.png"></a></p><p>As time went on, it was jarring but predictable to see lines through phrases like â€œBlack lives matterâ€� across swaths of profile updates in my inbox. </p><p><a href="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_12.png" target="_blank"><img src="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_12.png"></a></p><p>Politics similarly finds its way into our descriptions of self. With the 2020 U.S. presidential election, many translated their desired vote on the ballot to their bio: </p><p><a href="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_13.png" target="_blank"><img src="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_13.png"></a></p><p>The story has already been written, and you know what comes next: dashed presidential dreams translated to crossed-out candidates. Adding presidential hopefuls or even political parties to how we identify can be an exercise in disappointment. Disappointment is inevitable (after all, there can only be one). But seeing how long people hold on to their hopefuls was interesting; some still signalling long after their candidate had dropped out.</p><p><a href="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_14.png" target="_blank"><img src="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_14.png"></a></p><p>These tracked changes of our online selves provide a glimpse into the specific ways we represent ourselves to the world, how society manages to infect our identities, and, what we valueâ€”or what we want to be seen as valuing.</p><p>Spoonbill uncovers the tacit labor involved in our quest for the perfect personal positioning. In describing the way online influencers take effortless-looking selfies that are actually quite effortful, <a href="https://twitter.com/wishcrys" rel="noopener noreferrer" target="_blank">Dr. Crystal Abidin (aka wishcrys)</a>, a researcher and â€œanthropologist of influencer culturesâ€� (according to her own Twitter bio), <a href="https://journals.sagepub.com/doi/full/10.1177/2056305116641342" rel="noopener noreferrer" target="_blank">coined the term â€œtacit laborâ€�</a> as:&nbsp;</p><blockquote><em>â€œA collective practice of work that is understated and under-visibilized from being so thoroughly rehearsed that it appears as effortless and subconscious.â€�</em></blockquote><p>You donâ€™t need to be an influencer to know the labor involved in taking and selecting a selfie. Getting the right light, angle, and expression are just some of the considerations made while snapping and choosing from the photos that fill our camera rolls. After selecting the winning shot, we leave behind a stream of rejected selfies, never to see the light of day; the tacit labor stays hidden. After a photo has been cropped, filtered, and face-tuned, a refined â€œselfâ€�-ie joins the feed, adding to our curated online imageâ€“â€“polished, carefree, or somewhere in between.</p><p>Just like we strain for the perfect selfie, we tinker to find the perfect bio, treating it as a personal sales pitch and making micro edits to what we represent and â€œwho we are.â€� Spoonbill catalogues every time-stamped keystroke and tracked changeâ€“â€“from the removal of a single stuffy period to self-consciously swapping â€œMarketerâ€� for â€œStorytellerâ€� and reverting back again.&nbsp; Like the photos in your camera roll, your current Twitter bio has leftâ€“â€“for Spoonbill users to seeâ€“â€“a wake of rejected selves. </p><p><a href="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_5.png" target="_blank"><img src="https://d24ovhgu8s7341.cloudfront.net/uploads/editor/posts/1495/optimized_5.png"></a></p><p>Spoonbill reveals the surprising scale and frenzying frequency with which all kinds of people make these online edits. We add in emojis, expand and shrink our past affiliations, and flirt briefly with humor, adding clever one-liners before eventually reverting to something more serious. Itâ€™s effortfulness on display. Seeing people iterate on their identities in Spoonbill, switching too-formal uppercase for carefree lowercase characters, makes our desire to control how …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://every.to/divinations/creeping-as-a-service-craas">https://every.to/divinations/creeping-as-a-service-craas</a></em></p>]]>
            </description>
            <link>https://every.to/divinations/creeping-as-a-service-craas</link>
            <guid isPermaLink="false">hacker-news-small-sites-26081672</guid>
            <pubDate>Tue, 09 Feb 2021 19:45:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Migrating a JavaScript Library from JavaScript to WebAssembly]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26080798">thread link</a>) | @jasperk
<br/>
February 9, 2021 | https://engineering.q42.nl/webassembly/ | <a href="https://web.archive.org/web/*/https://engineering.q42.nl/webassembly/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://engineering.q42.nl/content/images/size/w300/2020/09/Going-From-JavaScript-to-WebAssembly-in-Three-Steps.png 300w,
                            https://engineering.q42.nl/content/images/size/w600/2020/09/Going-From-JavaScript-to-WebAssembly-in-Three-Steps.png 600w,
                            https://engineering.q42.nl/content/images/size/w1000/2020/09/Going-From-JavaScript-to-WebAssembly-in-Three-Steps.png 1000w,
                            https://engineering.q42.nl/content/images/size/w2000/2020/09/Going-From-JavaScript-to-WebAssembly-in-Three-Steps.png 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://engineering.q42.nl/content/images/size/w2000/2020/09/Going-From-JavaScript-to-WebAssembly-in-Three-Steps.png" alt="Going from JavaScript to WebAssembly in three steps">
            </figure>

            <section>
                <div>
                    <!--kg-card-begin: markdown--><p>Hi! I'm Marcel, web developer at <a href="https://www.q42.com/">Q42</a> and creator of the <a href="https://micr.io/">Micrio storytelling platform</a>.</p>
<p>In 2015, I started developing a JavaScript viewer for ultra high resolution 2D and 360° images with added markers, tours, audio, and more. Since then, I've been pushing to find the best balance between hardware performance, minimal CPU and bandwidth use, and compatibility for older browsers to deliver a sharp and high quality viewing experience.</p>
<p>For Micrio, it is vital that the performance on the client's browser is as good as possible. The reason for this is very simple: when you are being told a story, or watching a movie, even <em>a single frameskip</em> immediately takes you out of your experience.</p>
<p>Since Micrio is being used for an <a href="https://micr.io/showcases">ever growing list</a> of awesome projects, the most important thing is that for whoever visits a Micrio project, it must work, and <em>work well</em>: delivering fast load times, and a butter smooth interactive experience.</p>
<p><a href="https://webassembly.org/">WebAssembly</a> (Wasm) is the ability for your browser to run <em>compiled</em> code at (near-) native speeds. It is now recognised by the W3C as the <a href="https://www.w3.org/2019/12/pressrelease-wasm-rec.html.en">4th official web programming language</a>, after HTML, CSS and JavaScript.</p>
<p>With it, you can run compiled code written in a variety of programming languages (C/C++, Rust, Go, AssemblyScript, <a href="https://github.com/appcypher/awesome-wasm-langs">and many more</a>) in your browser, without any need for plugins.</p>
<p>Finding out about Wasm in late 2019 really made me want to try it out. Could I use this tech to make the Micrio client run smoother than the current version does? Would I need to rewrite everything in C++, and if so, how would that work? Would the effort be worth the gains it would give me? And not in the least.. <em>how does it work!?</em></p>
<p>This article describes my journey from upgrading the Micrio <strong>JavaScript-only client to use WebAssembly</strong>, with the hopes of improving performance, and taking my code to the next level.</p>
<h3 id="thecurrentversion">The current version</h3>
<p>To give a rough idea about the tech stack of the current latest JS-only revision of Micrio (<a href="https://b.micr.io/micrio-2.9.min.js">version 2.9</a>): This library as a single JS file works on all semi-modern browsers, including even Internet Explorer 10 for 2D, and IE 11 for 360° images.</p>
<p>It uses Canvas2D for the rendering of 2D images, and <a href="https://threejs.org/">three.js</a>/WebGL rendering for 360° images. Written in <a href="https://caniuse.com/#search=es6">ES6 JavaScript</a> (or ECMAScript 6, the latest version of JavaScript), it still <a href="https://developers.google.com/closure/compiler">compiles</a> to ES5 to support Internet Explorer 11.</p>
<p>As you can imagine, displaying a <a href="https://micr.io/i/xCSYV/">231.250 x 193.750 pixel image</a> in your browser in a matter of milliseconds, allowing the user to freely zoom in and navigate, requires a little bit of processing power.</p>
<p>Now, Micrio 2.9 <em>isn't bad</em>. It runs pretty smoothly on all devices. But with WebAssembly around the corner, allowing all calculations to be done at native CPU speeds, this could potentially make a big difference in making Micrio's performance even better, and could improve the code architecture a lot.</p>
<p>And, perhaps, this could also mark the setup for a new major version, where I will draw a clear line and drop all compatibility and polyfills for older browsers: <strong>Micrio 3.0</strong>.</p>
<h2 id="firstrewritecandemscripten">First Rewrite: C++ and emscripten</h2>
<p>As a first step into the world of WebAssembly, getting to know the ecosystem, I started to play around with <a href="https://emscripten.org/">emscripten</a>. With it, you can take almost any project made in C or C++, and compile it to a binary <code>.wasm</code> file that your browser can natively run.</p>
<p>At this point, I didn't really have a clear image of where WebAssembly starts and ends, and how autonomously it could run inside your browser. So I started a new project from scratch to see if I could make a C++-implementation of the basic Micrio logic: a virtual <em>zoomable</em> and <em>pannable</em> image consisting of a lot of separate tiles, using a virtual camera for displaying only the tiles necessary for what the user is viewing inside your screen.</p>
<p>It turns out, emscripten already had great compatibility for <a href="https://www.libsdl.org/">libsdl</a>: a low-level audio, keyboard/mouse input, and OpenGL library. Which is awesome, because I could write my code using this very well documented library, even including mouse and key inputs and WebGL rendering. Since I was also working with downloading images, I also used the <a href="https://github.com/nothings/stb">stb_image.h</a> image library.</p>
<p><img src="https://raw.githubusercontent.com/marcelduin/Micrio.Blogpost.wasm-v3/master/img/cpp.png" alt="Setting up SDL and OpenGL" title="C++ in the 21st century"></p>
<p>The largest struggle of this was picking up C++ again, never having used it outside of hobby scope many years ago. But after a few days of cursing and second guessing myself, I had a working first version with all of the most important features written with help of the SDL library:</p>
<ul>
<li>A virtual camera and all necessary viewing logic;</li>
<li>Image tiles downloading;</li>
<li>Rendering using WebGL(/OpenGL) using a simple shader;</li>
<li>Mouse event handling for panning and zooming the image;</li>
<li>Resize event handling to fit Micrio to the desired <code>&lt;canvas&gt;</code> HTML element</li>
</ul>
<p>You can see this version running here: <a href="https://b.micr.io/_test/wasm/index.html">https://b.micr.io/_test/wasm/index.html</a> :</p>
<p><a href="https://b.micr.io/_test/wasm/index.html"><img src="https://raw.githubusercontent.com/marcelduin/Micrio.Blogpost.wasm-v3/master/img/emscripten.png" alt="Micrio in native C++"></a></p>
<h3 id="firstresults">First Results</h3>
<p>As incredibly awesome it was to see Micrio in C++ running smoothly in my browser, and even handing all the user's input, there were a few reservations, which left me with an unsatisfied feeling.</p>
<h4 id="1codingcfeltoldfashioned">1. Coding C++ felt old-fashioned</h4>
<p>Writing C++ felt like going back in time. Incredibly powerful and fully proven, but also archaic, especially for me as a web developer. I spent more time fiddling with making an optimized <code>Makefile</code> than I care to admit.</p>
<p><img src="https://raw.githubusercontent.com/marcelduin/Micrio.Blogpost.wasm-v3/master/img/makefile.png" alt="The emscripten C++ Makefile" title="( ͡° ͜ʖ ͡°)"></p>
<h4 id="2thecompiledwasmbinarywasverylarge">2. The compiled <code>.wasm</code> binary was very large</h4>
<p>As great as the help of <code>libsdl</code> and <code>stb_image.h</code> were to let me use OpenGL and JPG image functions, as much did they add to the final compiled binary file. Even with all <code>emcc</code> compiler optimizations (which can even use the awesome <code>closure</code> JS compiler), the resulting WebAssembly binary file was 760KB. Compared to the JavaScript version of Micrio being around 240KB, this was a major setback. These libraries packed a lot of functionalities that were not necessary for Micrio, but were still included in the compiled version.</p>
<h4 id="3tilagluefile">3. TIL: A <em>glue</em> file</h4>
<p>This is the part where I learnt where the limits of WebAssembly start and finish. <strong>WebAssembly is not a magical self-contained binary that lets you run full applications out of the box</strong>. It actually needs to be <em>bound</em> to the browser using JavaScript.</p>
<p><img src="https://raw.githubusercontent.com/marcelduin/Micrio.Blogpost.wasm-v3/master/img/glued.png" alt="glue clipart"><br>
<small><a href="http://www.clker.com/clipart-13445.html">Image source</a></small></p>
<p>Where I thought that all the SDL OpenGL code in C++ would automagically be recognised by the browser: <em>wrong</em>. What <code>emscripten</code> does, is to take all OpenGL operations from C++, and <em>convert them</em> to WebGL operations your browser can understand.</p>
<p>Same with the <code>libsdl</code> mouse and keyboard-inputs: these were <strong>glued</strong> to the browser using an extra JavaScript file that would set event listeners for the specific cases, and send them to the WebAssembly binary. This separate JavaScript file was generated by the emscripten compiler, and had to be included in the HTML alongside the compiled binary <code>.wasm</code> file.</p>
<p>Everything added together, the new total of the <em>base engine</em> of Micrio was a whopping <strong>791KB</strong>; a bit too much for my liking.</p>
<h2 id="secondrewriteassemblyscript">Second Rewrite: AssemblyScript</h2>
<p>Fast forward a few months, to just after having attended the awesome <a href="https://webassembly-summit.org/">WebAssembly Summit</a> in Mountain View in February 2020. With a bundle of fresh energy and inspiration, I decided to see if I could use WebAssembly to improve the Micrio JavaScript client a second time.</p>
<p>During the WebAssembly conference, I was very impressed by a <a href="https://www.youtube.com/watch?v=C8j_ieOm4vE">synth demo</a> written in <strong><a href="https://www.assemblyscript.org/">AssemblyScript</a></strong>, a language created specifically for WebAssembly, using the TypeScript syntax. Basically you can write (near) TypeScript, which compiles to a <code>.wasm</code>-binary. So anyone familiar with either TypeScript or JavaScript ES6 will not have a lot of difficulties using it.</p>
<p>And the great thing-- it's all installed using <code>npm</code>, so <a href="https://www.assemblyscript.org/quick-start.html">getting it up and running</a> and compiling your program is super easy!</p>
<p>There are a few basic <a href="https://www.assemblyscript.org/types.html"><code>types</code> added in AssemblyScript</a>, which are required for compile-time optimizations:</p>
<ul>
<li><code>f64</code> / <code>f32</code> : For 64 or 32-bit floats;</li>
<li><code>i8</code> / <code>i16</code> / <code>i32</code> / <code>i64</code> : For signed <code>int</code>s, ranging in precision</li>
<li><code>u8</code> .. <code>u64</code> : For unsigned <code>int</code>s</li>
<li><a href="https://www.assemblyscript.org/types.html">And a few more</a></li>
</ul>
<h3 id="goingatomic">Going atomic</h3>
<p>This time, I wanted to see if it was possible to only let a small part of Micrio run inside WebAssembly, and still use most of the JavaScript that was already inside the client. <em>How small can we get it?</em> I decided to focus on a subset of camera functions, such as translating screen coordinates to image coordinates and vice versa. So this time no rendering, event handling, or writing shaders.</p>
<p><img src="https://raw.githubusercontent.com/marcelduin/Micrio.Blogpost.wasm-v3/master/img/assemblyscript.png" alt="Simple camera functions in AssemblyScript" title="My First AssemblyScript"><br>
<small><em>Simple camera functions in AssemblyScript</em></small></p>
<p>The result: a 3KB binary containing some basic math functions, that take an input and return an output. AssemblyScript offers some <em>glue-tooling</em> by providing its own <a href="https://www.assemblyscript.org/loader.html">Loader</a>, which will deal with importing the binary file and being able to call these functions.</p>
<p>However, this is optional and I ended up using the JavaScript <a href="https://developer.mozilla.org/en-US/docs/WebAssembly/Using_the_JavaScript_API">WebAssembly API</a>, <em>neat</em>. And it turns out, this is super easy: simply use the <code>fetch</code> API to load your compiled <code>.wasm</code> file, cast it as an <code>ArrayBuffer</code>, and use the <code>WebAssembly.instantiate()</code> function to get it up and running.</p>
<p><img src="https://raw.githubusercontent.com/marcelduin/Micrio.Blogpost.wasm-v3/master/img/instantiate.png" alt="Loading a wasm file" title="Gluing it yourself"></p>
<p>The compiled binary will then offer an <code>exports</code> object, containing the functions that you have exported in the AssemblyScript file, which you can immediately call from JavaScript as if they were normal functions.</p>
<p>Wait.. "<em>which you can immediately call from JavaScript as if they were normal functions</em>"...</p>
<p><strong>WebAssembly is running synchronously to JavaScript!</strong> 🤯</p>
<p>Having worked with WebWorkers before, I honestly thought that WebAssembly would run inside its own CPU thread, and that any function calls would be <code>async</code>. Nope, the Wasm-functions you call will return immediately!</p>
<p><a href="https://www.assemblyscript.org/exports-and-imports.html#exports"><em>This is, like, powerful stuff</em>!</a></p>
<h3 id="bundlingthecompiledwasminsidethejsfile">Bundling the compiled Wasm inside the JS file</h3>
<p>Since I now had some extra performing hands on deck for Micrio that was very easy to integrate, I decided to include this minimal WebAssembly binary in the then-stable release of Micrio (2.9).</p>
<p>However, I didn't want an extra HTTP request for the Wasm binary every time someone loaded the Micrio JS. So I included a <code>base64</code> encoded version of the Wasm-file <em>inside</em> the Micrio JS, and for browsers that support it, auto-loaded that. As a …</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://engineering.q42.nl/webassembly/">https://engineering.q42.nl/webassembly/</a></em></p>]]>
            </description>
            <link>https://engineering.q42.nl/webassembly/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26080798</guid>
            <pubDate>Tue, 09 Feb 2021 18:19:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Vantage has acquired ec2instances.info]]>
            </title>
            <description>
<![CDATA[
Score 84 | Comments 54 (<a href="https://news.ycombinator.com/item?id=26080571">thread link</a>) | @StratusBen
<br/>
February 9, 2021 | https://www.vantage.sh/blog/vantage-has-acquired-ec2instances-info | <a href="https://web.archive.org/web/*/https://www.vantage.sh/blog/vantage-has-acquired-ec2instances-info">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>Two things that aren’t changing are (1) customers are always going to want simpler ways to interact with their cloud infrastructure and (2) they’re always going to want tools to understand where their cloud costs are coming from to make better business decisions. Vantage is aiming to help with both of these challenges but customers also use other tools to help make informed business decisions. One of the most popular tools we found people using is Ec2instances.info and today I’m happy to announce that Ec2instances.info is joining forces with Vantage.</p><p>Ec2instances.info will now reside at instances.vantage.sh and exists in its same form. The site will continue to be completely free and we have plans to add more features and functionality to it, including additional service prices beyond virtual machine pricing. We’ve ensured that the redirects have backwards compatibility with the prior domain so everything should continue to work as expected.&nbsp;</p><p>Garret Heaton, the creator of Ec2instances.info, says:<br></p><blockquote>After running ec2instances.info as a side project for <a href="https://powdahound.com/2011/03/hosting-a-static-site-on-amazon-s3-ec2instances-info/">almost a decade</a> I'm thrilled to hand over the reins to the Vantage team. Watching the site help people year after year has been a real pleasure and I'm very grateful for all the bug reports and improvements submitted by the public. In recent years the amount of choice available in instance types has grown significantly so comparing them is especially important, but I haven't had time to improve the site at the same rate. I'm very excited to see Vantage invest in the site as well as help people control costs across their entire infrastructure.</blockquote><p>‍</p><p>Vantage is built by working backwards from customer feedback and we hope to use the same playbook for instances.vantage.sh - so please join our <a href="https://join.slack.com/t/vantagecommunity/shared_invite/zt-mc723hg0-64opi05ckDf3gE56xapntg">Slack Community</a>, follow us on <a href="https://twitter.com/joinvantage">Twitter</a> or email us at <a href="mailto:support@vantage.sh">support@vantage.sh</a> to let us know your use-cases and how we can help.</p><p><strong>Q&amp;A</strong><br></p><p><strong>What is Vantage?</strong></p><p>Vantage is an alternative AWS console with a focus on developer experience and cost transparency.&nbsp;<br></p><p>‍</p><p><strong>What is EC2Instances.info?</strong></p><p>Ec2Instances.info, now instances.vantage.sh, is a tool for comparing hundreds of EC2 Instance Types across different instance type attributes.&nbsp;</p><p>‍<br></p><p><strong>I use EC2Instances.info - how will this impact me?</strong></p><p>Only the domain is changing and all existing links will redirect in a way that preserves filters. For all intents and purposes, you shouldn’t be impacted.&nbsp;</p><p>‍<br></p><p><strong>Do I need to be a registered Vantage user to use instances.vantage.sh?</strong></p><p>No. That being said, we are offering a one-time coupon code of $25 to all EC2Instances users that you can apply to your Vantage account using the coupon code of INSTANCES. This offer will expire on March 31st, 2021.</p><p><strong>I have a feature request for ec2instances.info - how can I submit it to you?</strong></p><p>You can continue to leave feedback on the existing <a href="https://github.com/powdahound/ec2instances.info">Github repository</a>, email <a href="mailto:support@vantage.sh">support@vantage.sh</a> with the subject line “Product Feedback Request” or join the “instances-vantage-sh” channel in the Vantage <a href="https://join.slack.com/t/vantagecommunity/shared_invite/zt-mc723hg0-64opi05ckDf3gE56xapntg">Slack community</a>.</p></div></div></div>]]>
            </description>
            <link>https://www.vantage.sh/blog/vantage-has-acquired-ec2instances-info</link>
            <guid isPermaLink="false">hacker-news-small-sites-26080571</guid>
            <pubDate>Tue, 09 Feb 2021 18:00:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Don't start a business. Work on a side project instead]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26080321">thread link</a>) | @neilxm
<br/>
February 9, 2021 | https://neilmathew.co/dont-start-a-company-start-a-side-project/ | <a href="https://web.archive.org/web/*/https://neilmathew.co/dont-start-a-company-start-a-side-project/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<p>If I could go back in time and give a younger me one piece of advice when starting my company, it would be - don't start a business, build a side project instead. Build it for fun, see where it goes, let curiosity be your carrot as opposed to the expectation of success being a stick.</p><p>Side projects are simple. In fact they're so simple that you work on them just for the fun of it. You get deep into it, with no expectations, let what you learn evolve what you do next and let the project basically pull itself out of you.</p><p>When you start a business there is an expectation of success attached to it, otherwise the business is a failure. You need to find a business model, figure out what your vision is and what the market size is. "Is it a big enough idea?" is often the question asked by friends, parents and investors. There's nothing more demotivating than talking about your business to a cynical aunt.</p><p>Secondly, you need to be able to sell your ideas to those around you, and do it without fear. The best way to shed the fear of appearing foolish is to actually do something foolish for the fun of it. Strangely, you'll find that everyone around you is a lot more encouraging in this scenario than if you set out claiming you're going to build a big business.</p><p>Big things are accomplished by the compounding effect of many foolish little things done over a long time. If you start off trying to do a big thing, you'll always feel unaccomplished because you're too focused on what you haven't achieved yet. Focus on the small things, work on little projects, make an app for your friends as a joke, file your aunt's taxes, write a blog, <a href="https://www.reddit.com/r/shittyrobots/">build a shitty robot</a>. Keep doing that and just enjoy the process of building and creating things, and keep that child-like sense of curiosity alive. </p><p>The main reason for this is that you need to do the groundwork to prepare yourself to be receptive to opportunities when they come up. You never know when they will, but unless you are knee deep in something you love already, you just won't notice it.</p><blockquote>“You want to know how to paint a perfect painting? It's easy. Make yourself perfect and then just paint naturally.” <p>― Robert M. Pirsig, <a href="https://www.goodreads.com/work/quotes/175720">Zen and the Art of Motorcycle Maintenance</a></p></blockquote><hr><p><em>I write a weekly newsletter about topics like this. If you want to get an update when I publish a the next one, add your email in here.</em></p><!--kg-card-begin: html-->            <form data-members-form="subscribe">
                
                <div>
                    <div>
                        <p><span>Sending email...</span></p><p><strong>Great!</strong> Check your inbox and click the link to confirm your subscription.
                        </p>
                        <p>
                            Please enter a valid email address!
                        </p>
                    </div>
                </div>
            </form><!--kg-card-end: html-->
			</section></div>]]>
            </description>
            <link>https://neilmathew.co/dont-start-a-company-start-a-side-project/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26080321</guid>
            <pubDate>Tue, 09 Feb 2021 17:37:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fedora on the PinePhone: Pipewire Calling]]>
            </title>
            <description>
<![CDATA[
Score 133 | Comments 85 (<a href="https://news.ycombinator.com/item?id=26080207">thread link</a>) | @ashitlerferad
<br/>
February 9, 2021 | https://odysee.com/@linmob:3/fedora-on-the-pinephone-pipewire-calling:1 | <a href="https://web.archive.org/web/*/https://odysee.com/@linmob:3/fedora-on-the-pinephone-pipewire-calling:1">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://odysee.com/@linmob:3/fedora-on-the-pinephone-pipewire-calling:1</link>
            <guid isPermaLink="false">hacker-news-small-sites-26080207</guid>
            <pubDate>Tue, 09 Feb 2021 17:29:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Node.js 14 is over 20x faster than Python3.8 for fib(n)]]>
            </title>
            <description>
<![CDATA[
Score 67 | Comments 103 (<a href="https://news.ycombinator.com/item?id=26079570">thread link</a>) | @brrrrrm
<br/>
February 9, 2021 | https://jott.live/markdown/nodejs_vs_python_ | <a href="https://web.archive.org/web/*/https://jott.live/markdown/nodejs_vs_python_">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://jott.live/markdown/nodejs_vs_python_</link>
            <guid isPermaLink="false">hacker-news-small-sites-26079570</guid>
            <pubDate>Tue, 09 Feb 2021 16:44:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Polar Signals Continuous Profiler – Systematic Performance Profiling]]>
            </title>
            <description>
<![CDATA[
Score 43 | Comments 4 (<a href="https://news.ycombinator.com/item?id=26079108">thread link</a>) | @brancz
<br/>
February 9, 2021 | https://www.polarsignals.com/blog/posts/2021/02/09/announcing-polar-signals-continuous-profiler/ | <a href="https://web.archive.org/web/*/https://www.polarsignals.com/blog/posts/2021/02/09/announcing-polar-signals-continuous-profiler/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><p>Today we are pleased to introduce Polar Signals <strong>Continuous Profiler</strong>, an open source based SaaS for Continuous Profiling. Profiling has long been a tool in the developer's toolbox. It allows CPU, memory, I/O usage and more to be understood down to a line of code. It is an immensely valuable tool for debugging. <em>Continuous profiling</em> is the act of taking profiles of programs in a systematic way. Polar Signals Continuous Profiler, based on the open source <a target="_blank" href="https://github.com/conprof/conprof">Conprof</a> project, collects, stores and makes <a target="_blank" href="https://github.com/google/pprof">pprof</a> profiles available to be queried over time. This is useful in post-mortem analysis, when it is too late to take measurements. In short, Continuous Profiling allows for profiling to be systematic, instead of a search for the needle in the haystack.</p><h3>Traditional Workflow</h3><p>To understand how Polar Signals Continuous Profiler works, and why we built it, let's go through the traditional workflow of what a developer goes through when profiling. A typical scenario starts out with either a production incident or the intent to optimize a piece of code. Optimization is often driven by the intention to make a system faster, or for example to reduce resource usage in order to save cost.</p><p>Incidents caused by running out of memory (OOMKill) can be a very stressful situation, the process has already been killed by the operating system, and so it is not possible to obtain profiles about the situation anymore. Obtaining profiles often requires SSHing onto production machines and capturing profiles manually. This is problematic for security and auditability, besides that, it is also easy to make mistakes when SSHing in production environments. Above all, profiling manually in this fashion is a search for the needle in the haystack. It is impossible to know whether a situation is reproducible in a way that allows capturing the right data.</p><h3>Polar Signals Continuous Profiler Workflow</h3><p>The goal of Polar Signals Continuous Profiler is to turn the search for the right data into a systematic one. With Polar Signals Continuous Profiler, profiles are captured periodically, ensuring that profiles are available for any moment in time, that way the right data is always only a search away. This way, when a process ran out of memory, a latency spike happens or when we want to start optimize code, the data is already available! This saves time obtaining the data manually, but more importantly the <em>right</em> data is already captured - no need to wait or try and reproduce a past situation.</p><img src="https://www.polarsignals.com/blog/posts/2021/02/09/announcing-polar-signals-continuous-profiler/query-over-time.png" alt="Polar Signals Continuous Profiler Query Over Time"><p>Once the data is available over time, new useful workflows are unlocked. Take the scenario of optimizing a piece of code for reducing resource usage for example. A profile of a particular moment of time is useful, but it is only representative of that exact moment in time. When the data is collected over time, it can for example be summarized into a single report, making it not only representative of that moment of time, but the entire time range. We have found this to be particularly useful to summarize performance characteristics of an entire version of code.</p><img src="https://www.polarsignals.com/blog/posts/2021/02/09/announcing-polar-signals-continuous-profiler/merge.png" alt="Polar Signals Continuous Profiler Merge Feature"><p>And last but not least, not only is the data available over time, but also across deployment-rollouts. It allows answering questions that thus far have been notoriously difficult to answer, which boils down to "What changed in my code, that caused a change in performance?" This typically originates from one of two questions. Why did our resource usage increase? And did my optimization changes work?</p><img src="https://www.polarsignals.com/blog/posts/2021/02/09/announcing-polar-signals-continuous-profiler/compare.png" alt="Polar Signals Continuous Profiler Compare Feature"><p>To sum up, using Polar Signals Continuous Profiler an organization can:</p><ol><li>Save money on their cloud bill, by optimizing resource usage.</li><li>Boost conversion rate, by improving application latency.</li><li>Save time and stress, by make systems more reliable.</li></ol><h3>How Polar Signals Continuous Profiler Works</h3><p>Polar Signals Continuous Profiler periodically scrapes <a href="https://github.com/google/pprof">pprof</a> compatible profiles from HTTP endpoints. We are launching with support for <a href="https://golang.org/">Go</a>, which supports profiling in pprof format via its standard library. Many programs written in Go already have these endpoints, so organizations that already use Go are likely to not need any changes to their code to start using Polar Signals Continuous Profiler! Rust, Python, NodeJS and JVM are already on the roadmap, but let us know what languages and runtimes you would like to see!</p><p>The collector is configured to discover targets using service discovery, this happens to be identical to <a href="https://prometheus.io/">Prometheus'</a> service discovery, so organizations already using Prometheus can re-use configuration from it! And if not, we have automated deployment and configuration strategies prepared for you, for example to get started on Kubernetes seamlessly.</p><p>The collector regularly scrapes the HTTP endpoints and sends the results to Polar Signals. Once received by Polar Signals the exploration for improvements can start!</p><h3>Next steps</h3><p>Polar Signals Continuous Profiler is in private beta starting today! We are working hard to bring it to General Availability. We are in the process of onboarding users to our private beta, and if you want to participate, request access:</p></article></div>]]>
            </description>
            <link>https://www.polarsignals.com/blog/posts/2021/02/09/announcing-polar-signals-continuous-profiler/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26079108</guid>
            <pubDate>Tue, 09 Feb 2021 16:06:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[John McWhorter: The Neoracists]]>
            </title>
            <description>
<![CDATA[
Score 189 | Comments 232 (<a href="https://news.ycombinator.com/item?id=26079002">thread link</a>) | @paulpauper
<br/>
February 9, 2021 | https://www.persuasion.community/p/john-mcwhorter-the-neoracists | <a href="https://web.archive.org/web/*/https://www.persuasion.community/p/john-mcwhorter-the-neoracists">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3f575ee-a995-499c-86fe-72680f8d5d12_700x312.jpeg"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3f575ee-a995-499c-86fe-72680f8d5d12_700x312.jpeg" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/d3f575ee-a995-499c-86fe-72680f8d5d12_700x312.jpeg&quot;,&quot;height&quot;:312,&quot;width&quot;:700,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:40221,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null}" alt=""></a></figure></div><p><strong>[</strong>Excerpt from his new book, <em>The Elect: Neoracists Posing as Antiracists and their Threat to a Progressive America</em>]</p><p><strong>One can divide antiracism into three waves</strong>. First Wave Antiracism battled slavery and segregation. Second Wave Antiracism, in the 1970s and 1980s, battled racist attitudes and taught America that being racist was a flaw. Third Wave Antiracism, becoming mainstream in the 2010s, teaches that racism is baked into the structure of society, so whites’ “complicity” in living within it constitutes racism itself, while for black people, grappling with the racism surrounding them is the totality of experience and must condition exquisite sensitivity toward them, including a suspension of standards of achievement and conduct.</p><p>Third Wave Antiracist tenets, stated clearly and placed in simple oppositions, translate into nothing whatsoever:</p><ol><li><p>When black people say you have insulted them, apologize with profound sincerity and guilt. <strong>But </strong>don’t put black people in a position where you expect them to forgive you. They have dealt with too much to be expected to.</p></li><li><p>Black people are a conglomeration of disparate individuals. “Black culture” is code for “pathological, primitive ghetto people.” <strong>But </strong>don’t expect black people to assimilate to “white” social norms because black people have a culture of their own.</p></li><li><p>Silence about racism is violence. <strong>But </strong>elevate the voices of the oppressed over your own.</p></li><li><p>You must strive eternally to understand the experiences of black people. <strong>But </strong>you can never understand what it is to be black, and if you think you do you’re a racist.</p></li><li><p>Show interest in multiculturalism.&nbsp;<strong>But </strong>do not culturally appropriate. What is not your culture is not for you, and you may not try it or do it. But—if you aren’t nevertheless <em>interested</em> in it, you are a racist.</p></li><li><p>Support black people in creating their own spaces and stay out of them. <strong>But </strong>seek to have black friends. If you don’t have any, you’re a racist. And if you claim any, they’d better be <em>good</em> friends—in their private spaces, you aren’t allowed in.</p></li><li><p>When whites move away from&nbsp;black neighborhoods, it’s white flight.&nbsp;<strong>But </strong>when whites move into black neighborhoods, it’s gentrification, even when they pay black residents generously for their houses.</p></li><li><p>If you’re white and only date white people, you’re a racist. <strong>But </strong>if you’re white and date a black person you are, if only deep down, exotifying an “other.”</p></li><li><p>Black people cannot be held accountable for everything every black person does. <strong>But </strong>all whites must acknowledge their personal complicity in the perfidy throughout history of “whiteness.”</p></li><li><p>Black students must be admitted to schools via adjusted grade and test score standards to ensure a representative number of them and foster a diversity of views in classrooms. <strong>But </strong>it is racist to assume a black student was admitted to a school via racial preferences, and racist to expect them to represent the “diverse” view in classroom discussions.</p></li></ol><p>I suspect that deep down, most know that none of this catechism makes any sense. Less obvious is that it was not even composed with logic in mind. The self-contradiction of these tenets is crucial, in revealing that Third Wave Antiracism is not a philosophy but a religion. </p><p>The revelation of racism is, itself and alone, the point, the intention, of this curriculum. As such, the fact that if you think a little, the tenets cancel one another out, is considered trivial. That they serve their true purpose of revealing people as bigots is paramount—sacrosanct, as it were. Third Wave Antiracism’s needlepoint homily <em>par excellence</em> is the following:</p><blockquote><p>Battling power relations and their discriminatory effects must be the central focus of all human endeavor, be it intellectual, moral, civic or artistic. Those who resist this focus, or even evidence insufficient adherence to it, must be sharply condemned, deprived of influence, and ostracized.</p></blockquote><p><strong>Third Wave Antiracism is losing innocent people jobs.</strong> It is coloring, detouring and sometimes strangling academic inquiry. It forces us to render a great deal of our public discussion of urgent issues in doubletalk any 10-year-old can see through. It forces us to start teaching our actual 10-year-olds, in order to hold them off from spoiling the show in that way, to believe in sophistry in the name of enlightenment. On that, Third Wave Antiracism guru Ibram X. Kendi has written a book on how to raise antiracist children called <em>Antiracist Baby</em>. You couldn’t imagine it better: Are we in a Christopher Guest movie? This and so much else is a sign that Third Wave Antiracism forces us to pretend that performance art is politics. It forces us to spend endless amounts of time listening to nonsense presented as wisdom, and pretend to like it.</p><p>I write this viscerally driven by the fact that all of this supposed wisdom is founded in an ideology under which white people calling themselves our saviors make black people look like the dumbest, weakest, most self-indulgent human beings in the history of our species, and teach black people to revel in that status and cherish it as making us special. Talking of <em>Antiracist Baby</em>, I am especially dismayed at the idea of this indoctrination infecting my daughters’ sense of self. I can’t always be with them, and this anti-humanist ideology may seep into their school curriculum. I shudder at the thought: teachers with eyes shining at the prospect of showing their antiracism by teaching my daughters that they are poster children rather than individuals. </p><p>Ta-Nehisi Coates in <em>Between the World and Me</em> wanted to teach his son that America is set against him; I want to teach my kids the reality of their lives in the 21st&nbsp;rather than early-to-mid-20th century. Lord forbid my daughters internalize a pathetic—yes, absolutely pathetic in all of the resonances of that word—sense that what makes them interesting is what other people think of them, or don’t.</p><p>Many will see me as traitorous in writing this as a black person. They will not understand that I see myself as serving my race by writing it. One of the grimmest tragedies of how this perversion of sociopolitics makes us think (or, not think) is that it will bar more than a few black readers from understanding that I am calling for them to be treated with true dignity. However, they and everyone else should also realize: I know quite well that white readers will be more likely to hear out views like this when written by a black person, and consider it nothing less than my duty as a black person to write it.</p><p>A white version of this would be blithely dismissed as racist. I will be dismissed instead as self-hating by a certain crowd. But frankly, they won’t really mean it, and anyone who gets through <a href="https://johnmcwhorter.substack.com/p/the-elect-neoracists-posing-as-antiracists">my new book</a> on this subject, which I am now publishing in serial, will see that whatever traits I harbor, hating myself or being ashamed of being black is not one of them. And we shall move on. As in, to realizing that what I am documenting matters, and matters deeply. Namely, that America’s sense of what it is to be intellectual, moral, or artistic; what it is to educate a child; what it is to foster justice; what is to express oneself properly; what it is to be a nation—all is being refounded upon a religion. </p><p>This is directly antithetical to the very foundations of the American experiment. Religion has no place in the classroom, in the halls of ivy, in our codes of ethics, or in deciding how we express ourselves, and almost all of us spontaneously understand that and see any misunderstanding of the premise as backward. Yet since about 2015, a peculiar contingent has been slowly headlocking us into making an exception, supposing that this new religion is so incontestably good, so gorgeously surpassing millennia of brilliant philosophers’ attempts to identify the ultimate morality, that we can only bow down in humble acquiescence.</p><p>But a new religion in the guise of world progress is not an advance; it is a detour. It is not altruism; it is self-help. It is not sunlight; it is fungus. It’s time it became ordinary to call it for what it is and stop cowering before it, letting it make people so much less than they—black and everything else—could be.</p><p><strong>Third Wave Antiracism exploits modern Americans’ fear</strong> of being thought racist, using this to promulgate an obsessive, self-involved, totalitarian and unnecessary kind of cultural reprogramming. One could be excused for thinking this glowering kabuki is a continuation of the Civil Rights efforts of yore, the only kind of new antiracism there could be. Its adherents preach with such contemptuous indignation, and are now situated in the most prestigious and influential institutions in the land—on their good days they can seem awfully “correct.”</p><p>However, there is nothing correct about the essence of American thought and culture being transplanted into the soil of a religious faith. Some will go as far as to own up to it being a religion, and wonder why we can’t just accept it as our new national creed. The problem is that on matters of societal procedure and priorities, the adherents of this religion—true to the very nature of religion—cannot be reasoned with. They are, in this, medievals with lattes.</p><p>We need not wonder what the basic objections will be: Third Wave Antiracism isn’t really a religion; I am oversimplifying; I shouldn’t write this without being a theologian; it is a religion but it’s a good one; and so on. I will get all of that out of the way as we go on, and then offer some genuine solutions. But first, what this is not.</p><ol><li><p><em>It is not an argument against protest.</em> I am not arguing against the basic premises of Black Lives Matter, although I have had my differences with some of its offshoot developments. I am not arguing that the Civil Rights movement of the 1950s and 1960s would have been better off sticking to quiet negotiations. I am not arguing against the left. I am arguing against a particular strain of the left that has come to exert a grievous influence over American institutions, to the point that we are beginning to …</p></li></ol></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.persuasion.community/p/john-mcwhorter-the-neoracists">https://www.persuasion.community/p/john-mcwhorter-the-neoracists</a></em></p>]]>
            </description>
            <link>https://www.persuasion.community/p/john-mcwhorter-the-neoracists</link>
            <guid isPermaLink="false">hacker-news-small-sites-26079002</guid>
            <pubDate>Tue, 09 Feb 2021 15:59:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Leveraging the Go Type System]]>
            </title>
            <description>
<![CDATA[
Score 92 | Comments 104 (<a href="https://news.ycombinator.com/item?id=26078865">thread link</a>) | @gopherguides
<br/>
February 9, 2021 | https://www.gopherguides.com/articles/leveraging-the-go-type-system | <a href="https://web.archive.org/web/*/https://www.gopherguides.com/articles/leveraging-the-go-type-system">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  
  
  <div>
    <section>  </section> <string></string> <section><p>If you haven't worked in a typed language before, it may not be obvious at first the power that it brings.  This article will show you how to leverage the type system to make your code easier to use and more reusable.</p> <h4>Target Audience</h4> <p>This article is aimed at developers that are new to Go and have little to no Go experience.</p> <h2>The Problem</h2> <p>For this article, we will look at how to handle <code language="plain">categorical</code> data.  In this case, specifically how to handle the <code language="plain">genre</code> category for classifying a book.</p> <p>To start, we'll define a data structure for a <code language="plain">Book</code> , in which we'll want to categorize it via the <code language="plain">genre</code> :</p> <pre><code language="plain" snippet="book" src="./src/v1/books.go">package books

type Book struct {
	ID    int
	Name  string
	Genre string
}</code></pre>   <p>Now that we have the book defined, let's go ahead and define some constants for <code language="plain">genre</code> :</p> <pre><code language="plain" snippet="genre" src="./src/v1/books.go">const (
	Adventure     = "Adventure"
	Comic         = "Comic"
	Crime         = "Crime"
	Fiction       = "Fiction"
	Fantasy       = "Fantasy"
	Historical    = "Historical"
	Horror        = "Horror"
	Magic         = "Magic"
	Mystery       = "Mystery"
	Philosophical = "Philosophical"
	Political     = "Political"
	Romance       = "Romance"
	Science       = "Science"
	Superhero     = "Superhero"
	Thriller      = "Thriller"
	Western       = "Western"
)</code></pre>   <p>So far, this seems fine.  However, the <code language="plain">genre</code> constants are strings.  While this makes for a very "humanized" way of reading the code, it's not very efficient as it pertains to a computer program.  Strings will take up more storage space, and more memory in the program (not to mention if we stored millions of data records to a database).  As such, we really want to use a smaller data type to represent this data.</p> <p>In Go, one way we can do this is to create constants that are based on the <code language="plain">int</code> type.</p> <pre><code language="plain" snippet="genre" src="./src/v2/books.go">const (
	Adventure     = 1
	Comic         = 2
	Crime         = 3
	Fiction       = 4
	Fantasy       = 5
	Historical    = 6
	Horror        = 7
	Magic         = 8
	Mystery       = 9
	Philosophical = 10
	Political     = 11
	Romance       = 12
	Science       = 13
	Superhero     = 14
	Thriller      = 15
	Western       = 16
)</code></pre>   <p>We also need to change the <code language="plain">Book</code> structure to now represent <code language="plain">Genre</code> as an int:</p> <pre><code language="plain" snippet="book" src="./src/v2/books.go">type Book struct {
	ID    int
	Name  string
	Genre int
}</code></pre>   <p>While we now have a more effecient memory model for <code language="plain">Genre</code> , it's not as "human" friendly.  If I print out the value of a <code language="plain">Book</code> , we now just get an integer value.  To show this, we'll write a quick test showing the output:</p> <pre><code language="plain" snippet="test" src="./src/v2/books_test.go">package books

import (
	"testing"
)

func TestGenre(t *testing.T) {
	b := Book{
		ID:    1,
		Name:  "All About Go",
		Genre: Magic,
	}

	t.Logf("%+v\n", b)

	if got, exp := b.Genre, 8; got != exp {
		t.Errorf("unexpected genre.  got %d, exp %d", got, exp)
	}
}</code></pre>   <p>And here is the output.</p> <pre><code language="plain" snippet="output" src="./src/v2/books_test.go">$ go test -v ./...
=== RUN   TestGenre
    books_test.go:14: {ID:1 Name:All About Go Genre:8}
--- PASS: TestGenre (0.00s)
PASS
ok      github.com/gopherguides/corp/_blog/types/leveraging-types/src/v2     (cached)</code></pre>   <p>Notice that the <code language="plain">Genre</code> just shows a value of <code language="plain">8</code> .  Any time we debug the code, or write a report, etc, we now need to figure out what <code language="plain">8</code> actually represents for a human being.</p> <p>To do this, we can write a helper function that takes the <code language="plain">Genre</code> value, and determines what the "human" representation should be:</p> <pre><code language="plain" snippet="string" src="./src/v2/books.go">func GenreToString(i int) string {
	switch i {
	case 1:
		return "Adventure"
	case 2:
		return "Comic"
	case 3:
		return "Crime"
	case 4:
		return "Fiction"
	case 5:
		return "Fantasy"
	case 6:
		return "Historical"
	case 7:
		return "Horror"
	case 8:
		return "Magic"
	case 9:
		return "Mystery"
	case 10:
		return "Philosophical"
	case 11:
		return "Political"
	case 12:
		return "Romance"
	case 13:
		return "Science"
	case 14:
		return "Superhero"
	case 15:
		return "Thriller"
	case 16:
		return "Western"
	default:
		return ""
	}
}</code></pre>   <h2>A Better Way</h2> <p>While all the above code works fine, it's really missing some key points.</p> <ul><li><p>If a value for a <code language="plain">Genre</code> has to change in the future, we not only have to change the constant value, but we also have to update the <code language="plain">GenreToString</code> function.  If we don't, this will create a bug in our code.</p></li> <li><p>We aren't leveraging the type system to encapsulate this behavior for <code language="plain">Genre</code> .  We'll show you what we mean by that shortly.</p></li></ul> <p>The first thing we really need to do is write a more resilient <code language="plain">GenreToString</code> function.  What we mean by resilient is that even if the value of the <code language="plain">Genre</code> constant changes in the future, the <code language="plain">GenreToString</code> function will not need to change.</p> <p>The correct way to do that is no longer use hard coded values, but use the value of the constant themselves:</p> <pre><code language="plain" snippet="string" src="./src/v3/books.go">func GenreToString(i int) string {
	switch i {
	case Adventure:
		return "Adventure"
	case Comic:
		return "Comic"
	case Crime:
		return "Crime"
	case Fiction:
		return "Fiction"
	case Fantasy:
		return "Fantasy"
	case Historical:
		return "Historical"
	case Horror:
		return "Horror"
	case Magic:
		return "Magic"
	case Mystery:
		return "Mystery"
	case Philosophical:
		return "Philosophical"
	case Political:
		return "Political"
	case Romance:
		return "Romance"
	case Science:
		return "Science"
	case Superhero:
		return "Superhero"
	case Thriller:
		return "Thriller"
	case Western:
		return "Western"
	default:
		return ""
	}
}</code></pre>   <p>Ok, that's much cleaner (and readable), but we still haven't solved the fact that when we print it out, it shows a data value ( <code language="plain">int</code> ), and not a "human" readable value.</p> <h2>Types to the Rescue</h2> <p>Instead of using a generic <code language="plain">int</code> type for <code language="plain">Genre</code> , we can create our own type based on an existing type.  In this case, we'll create a new type called <code language="plain">Genre</code> based on the <code language="plain">int</code> type:</p> <pre><code language="plain" snippet="genre" src="./src/v4/books.go">type Genre int</code></pre>   <p>Now, we'll define our constants as <code language="plain">Genre</code> types:</p> <pre><code language="plain" snippet="constants" src="./src/v4/books.go">const (
	Adventure     Genre = 1
	Comic         Genre = 2
	Crime         Genre = 3
	Fiction       Genre = 4
	Fantasy       Genre = 5
	Historical    Genre = 6
	Horror        Genre = 7
	Magic         Genre = 8
	Mystery       Genre = 9
	Philosophical Genre = 10
	Political     Genre = 11
	Romance       Genre = 12
	Science       Genre = 13
	Superhero     Genre = 14
	Thriller      Genre = 15
	Western       Genre = 16
)</code></pre>   <p>So far, the code doesn't really feel different.  However, now that <code language="plain">Genre</code> is it's own type, we can add methods to it.  This allows us to encapsulate the "human" behavior we want to the type, and not as a generic function.</p> <p>To do this, we'll add a <code language="plain">String</code> method to the <code language="plain">Genre</code> type:</p> <pre><code language="plain" snippet="string" src="./src/v4/books.go">func (g Genre) String() string {
	switch g {
	case Adventure:
		return "Adventure"
	case Comic:
		return "Comic"
	case Crime:
		return "Crime"
	case Fiction:
		return "Fiction"
	case Fantasy:
		return "Fantasy"
	case Historical:
		return "Historical"
	case Horror:
		return "Horror"
	case Magic:
		return "Magic"
	case Mystery:
		return "Mystery"
	case Philosophical:
		return "Philosophical"
	case Political:
		return "Political"
	case Romance:
		return "Romance"
	case Science:
		return "Science"
	case Superhero:
		return "Superhero"
	case Thriller:
		return "Thriller"
	case Western:
		return "Western"
	default:
		return ""
	}
}</code></pre>   <p>Now, we'll be able to use the <code language="plain">String</code> method when we want to see what the "human" value of a <code language="plain">Genre</code> is:</p> <pre><code language="go">b := Book{
	ID:    1,
	Name:  "All About Go",
	Genre: Magic,
}
fmt.Println(b.Genre.String())</code></pre> <p>Output:</p> <pre><code language="sh">Magic</code></pre> <h2>Magic Formatting</h2> <p>In Go, if you add a <code language="plain">String</code> method to any type, the <code language="plain">fmt</code> package will now use your <code language="plain">String</code> method to "pretty print" the representation of your type.  Because of this, we will now see that if we print out the <code language="plain">book</code> in our tests, we get a "human-readable" <code language="plain">Genre</code> as well:</p> <pre><code language="plain" snippet="test" src="./src/v4/books_test.go">func TestGenre(t *testing.T) {
	b := Book{
		ID:    1,
		Name:  "All About Go",
		Genre: Magic,
	}

	t.Logf("%+v\n", b)

	if got, exp := b.Genre.String(), "Magic"; got != exp {
		t.Errorf("unexpected genre.  got %q, exp %q", got, exp)
	}
}</code></pre>   <p>Output:</p> <pre><code language="plain" snippet="output" src="./src/v4/books_test.go">$ go test -v -run=TestGenre -count=1 .
=== RUN   TestGenre
    books_test.go:16: {ID:1 Name:All About Go Genre:Magic}
--- PASS: TestGenre (0.00s)
PASS
ok      book    0.059s</code></pre>   <p>We now see the value for <code language="plain">Genre</code> in the printed output is <code language="plain">Magic</code> , and not <code language="plain">8</code> . It's also important to note that our test actually didn't change, only the way in which we leveraged our new type for <code language="plain">Genre</code> .</p> <h2>What about Iota?</h2> <p>For those of you that are familiar with Go already, you might have looked at this problem and asked "Why didn't you just use iota?". <a href="https://github.com/golang/go/wiki/Iota" target="_blank">Iota</a> is an identifier that you can use in Go to also create incrementing number constants.  While there are several reasons I didn't use iota here, I did dedicate an entire article to the topic.  Read all about it in <a href="https://www.gopherguides.com/articles/how-to-use-iota-in-golang" target="_blank">Where and When to use Iota in Go</a> .</p> <h2>Summary</h2> <p>While this example was purposefully basic in nature, it illustrates the power of defining your own type, and leveraging the type system in Go to create more resilient, readable, and reusable code.</p> <h3>Want More?</h3> <p>Check out our previous article, <a href="https://www.gopherguides.com/articles/embracing-the-go-type-system" target="_blank">Embracing the Go Type System</a> and learn how to use the type system to avoid common mistakes in Go.</p></section>
  </div>
  
</div></div>]]>
            </description>
            <link>https://www.gopherguides.com/articles/leveraging-the-go-type-system</link>
            <guid isPermaLink="false">hacker-news-small-sites-26078865</guid>
            <pubDate>Tue, 09 Feb 2021 15:51:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A simple 11.2 GHz radio telescope (Hardware) (2020)]]>
            </title>
            <description>
<![CDATA[
Score 126 | Comments 20 (<a href="https://news.ycombinator.com/item?id=26078761">thread link</a>) | @_Microft
<br/>
February 9, 2021 | https://physicsopenlab.org/2020/10/10/a-simple-11-2-ghz-radiotelescope/ | <a href="https://web.archive.org/web/*/https://physicsopenlab.org/2020/10/10/a-simple-11-2-ghz-radiotelescope/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
					
					
					<p><a href="http://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope1.jpg"><img data-attachment-id="14895" data-permalink="https://physicsopenlab.org/2020/10/10/un-semplice-radiotelescopio-a-11-2-ghz/10ghzradiotelescope1/" data-orig-file="https://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope1.jpg" data-orig-size="1044,890" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="10GHzRadiotelescope1" data-image-description="" data-medium-file="https://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope1-300x256.jpg" data-large-file="https://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope1-1024x873.jpg" loading="lazy" src="http://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope1-1024x873.jpg" alt="" width="550" height="469" srcset="https://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope1-1024x873.jpg 1024w, https://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope1-300x256.jpg 300w, https://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope1-768x655.jpg 768w, https://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope1.jpg 1044w" sizes="(max-width: 550px) 100vw, 550px"></a></p>
<p><em><strong>Abstract : </strong><span lang="en">In this post we describe the construction of a small amateur radio telescope operating at the frequency of 11.2 GHz. The construction of the radio telescope takes advantage of the satellite TV market which has made it easy and cheap to find parabolic reflector antennas with relative illuminator (feed horn) and LNB block (low noise amplifier-frequency converter). The performances of a similar instrument are naturally rather limited, however they still allow to make interesting observations of some of the most intense radio sources.</span></em></p>
<h3>Introduction</h3>
<p>Radio astronomy is a difficult and fascinating science. It requires the use of bulky and expensive antennas, uses sophisticated radio-electronic technologies and sophisticated algorithms for signal processing. At first glance it would seem completely beyond the reach of an “amateur”. In reality it is possible to make interesting radio astronomical observations even at an amateur level.<br>
On our site we have already described some radio astronomy projects for specific applications:</p>
<ul>
<li><a href="http://physicsopenlab.org/2020/05/03/loop-antenna-for-very-low-frequency/">Loop Antenna for Very Low Frequency</a></li>
<li><a href="http://physicsopenlab.org/2020/05/07/vlf-receiver-for-sid-monitoring/">VLF Receiver for SID Monitoring</a></li>
<li><a href="http://physicsopenlab.org/2020/07/20/horn-antenna-for-the-21cm-neutral-hydrogen-line/">Horn Antenna for the 21cm Neutral-Hydrogen Line</a></li>
<li><a href="http://physicsopenlab.org/2020/07/26/sdr-based-receiver-for-the-21-cm-neutral-hydrogen-line/">Low-Noise SDR-Based Receiver for the 21cm Neutral-Hydrogen Line</a></li>
<li><a href="http://physicsopenlab.org/2020/07/26/gnuradio-software-for-the-21-cm-neutral-hydrogen-line/">GNURadio Software for 21cm Neutral-Hydrogen Line</a></li>
</ul>
<p>Now we want to try to make an “amateur” radio telescope based on the principle of the <strong>radiometer</strong>. This is certainly not the place to give detailed information on radio astronomy and radio telescopes (there is a lot of information on the net and specific texts), so we limit ourselves to providing some hints on the main points that guided us in the construction of the radio telescope.</p>
<p>Radio astronomy studies celestial bodies by analyzing the radio waves emitted by objects in the sky: any object emits electromagnetic waves through various physical processes (thermal and non-thermal), these waves are picked up by the antenna and analyzed with appropriate instruments: in general the characteristics of the captured signal are no different from those that characterize a <strong>broad spectrum electrical noise</strong>. The purpose of the radio telescope is to pick up this radiation and measure the signal strength, such an instrument is called a radiometer. To be precise, we speak of power per unit area and per unit of bandwidth and is expressed in Jansky&nbsp;: <strong>1Jy = 10<sup>-26</sup> W/m<sup>2</sup> Hz</strong>.</p>
<p>The range of radio frequencies useful for radio astronomy observations is between <strong>20 MHz</strong> and about <strong>20 GHz</strong>: below 20 MHz there is absorption by the ionosphere, above 20 GHz there is absorption by of the gases present in the atmosphere.</p>
<p id="tw-target-text" dir="ltr" data-placeholder="Traduzione"><span lang="en">To choose the most suitable frequency band for an amateur radio telescope we must make a compromise between the observation possibilities and the cost and feasibility constraints. The frequency spectrum of the radio-source emissions depends on the underlying physical process: for “thermal” emissions such as the sun or the moon, the intensity follows the <strong>law of the black body</strong> with maximums at high frequencies (according to the approximation of Rayleigh-Jeans<strong> I ∝ 1/λ<sup>4</sup></strong>), while for non-thermal emissions (for example synchrotron emission) the maximums are at lower frequencies, as can be seen in the graph below which shows the intensity of some radio sources as a function of frequency.</span><a href="http://physicsopenlab.org/wp-content/uploads/2020/10/radiosorgenti.png"><img data-attachment-id="14927" data-permalink="https://physicsopenlab.org/2020/10/10/un-semplice-radiotelescopio-a-11-2-ghz/radiosorgenti/" data-orig-file="https://physicsopenlab.org/wp-content/uploads/2020/10/radiosorgenti.png" data-orig-size="658,756" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="radiosorgenti" data-image-description="" data-medium-file="https://physicsopenlab.org/wp-content/uploads/2020/10/radiosorgenti-261x300.png" data-large-file="https://physicsopenlab.org/wp-content/uploads/2020/10/radiosorgenti.png" loading="lazy" src="http://physicsopenlab.org/wp-content/uploads/2020/10/radiosorgenti.png" alt="" width="450" height="515"></a></p>
<p>As we know the dimensions of the antenna are related to the wavelength of the radiation to be received, furthermore our antenna must be sufficiently directive, otherwise it would be practically useless: this means that to receive frequencies below 1 GHz the dimensions of the antenna should be significantly greater than 1m: large antennas are expensive and difficult to move.<br>
Another aspect to consider is external radio interference. The ether, especially in the city, is now saturated with transmissions and RF signals from the most heterogeneous origin: radio and TV broadcasting, cellular networks, WiFi networks, disturbances from power lines, etc …. Not having the possibility to install the radio telescope in “quiet” places we must choose a frequency band that is not too disturbed.</p>
<p>For the reasons described above, the choice is almost obligatory: the <strong>10-12 GHz frequency band</strong> is the one that seems most suitable for an amateur project like ours. At these frequencies, parabolic reflector antennas and devices designed for satellite television can be re-used. The costs of the equipment are affordable, the spatial resolution of the antenna is quite good and the interference is low (basically broadcasting satellites) and easily avoidable.<br>
Working at lower frequencies would make it possible to easily receive more radio sources but with a considerable increase in terms of costs, not to mention the problem of interference.</p>
<h3>Parabolic Dish Antenna</h3>
<p id="tw-target-text" dir="ltr" data-placeholder="Traduzione"><span lang="en">The antenna we found on the second-hand market is a <strong>prime focus dish</strong> with a diameter of 120 cm. For radio astronomy applications it is better that the dish is of the prime focus type: in these antennas the feed horn is placed in the focus of the dish. In offset-type dishes, the feed-horn is not placed in the center but on the side, this type has constructive advantages but is more difficult to aim to the source than the prime focus.</span></p>
<p dir="ltr" data-placeholder="Traduzione">For this antenna we can calculate the gain and the directivity intended as half power band width HPBW (half power band width) :</p>
<p><strong>G = η*(π*D/λ) = 40 dB</strong></p>
<p><strong>HPBW = 65*λ/D = 1.45°</strong></p>
<p>Where<br>
<strong>η : efficiency = 0.5</strong><br>
<strong>D : diameter = 120 cm</strong><br>
<strong>λ : wavelength = 2.68 cm (correspond to 11.2 GHz)</strong></p>
<p id="tw-target-text" dir="ltr" data-placeholder="Traduzione"><span lang="en">The images below show the antenna and the metal structure used for manual movement.</span></p>
<p><a href="http://physicsopenlab.org/wp-content/uploads/2020/10/parab1.jpg"><img data-attachment-id="14907" data-permalink="https://physicsopenlab.org/2020/10/10/un-semplice-radiotelescopio-a-11-2-ghz/parab1/" data-orig-file="https://physicsopenlab.org/wp-content/uploads/2020/10/parab1.jpg" data-orig-size="884,1346" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="parab1" data-image-description="" data-medium-file="https://physicsopenlab.org/wp-content/uploads/2020/10/parab1-197x300.jpg" data-large-file="https://physicsopenlab.org/wp-content/uploads/2020/10/parab1-673x1024.jpg" loading="lazy" src="http://physicsopenlab.org/wp-content/uploads/2020/10/parab1-673x1024.jpg" alt="" width="500" height="761" srcset="https://physicsopenlab.org/wp-content/uploads/2020/10/parab1-673x1024.jpg 673w, https://physicsopenlab.org/wp-content/uploads/2020/10/parab1-197x300.jpg 197w, https://physicsopenlab.org/wp-content/uploads/2020/10/parab1-768x1169.jpg 768w, https://physicsopenlab.org/wp-content/uploads/2020/10/parab1.jpg 884w" sizes="(max-width: 500px) 100vw, 500px"></a></p>
<p><a href="http://physicsopenlab.org/wp-content/uploads/2020/10/parab2.jpg"><img data-attachment-id="14908" data-permalink="https://physicsopenlab.org/2020/10/10/un-semplice-radiotelescopio-a-11-2-ghz/parab2/" data-orig-file="https://physicsopenlab.org/wp-content/uploads/2020/10/parab2.jpg" data-orig-size="844,1106" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="parab2" data-image-description="" data-medium-file="https://physicsopenlab.org/wp-content/uploads/2020/10/parab2-229x300.jpg" data-large-file="https://physicsopenlab.org/wp-content/uploads/2020/10/parab2-781x1024.jpg" loading="lazy" src="http://physicsopenlab.org/wp-content/uploads/2020/10/parab2-781x1024.jpg" alt="" width="501" height="657" srcset="https://physicsopenlab.org/wp-content/uploads/2020/10/parab2-781x1024.jpg 781w, https://physicsopenlab.org/wp-content/uploads/2020/10/parab2-229x300.jpg 229w, https://physicsopenlab.org/wp-content/uploads/2020/10/parab2-768x1006.jpg 768w, https://physicsopenlab.org/wp-content/uploads/2020/10/parab2.jpg 844w" sizes="(max-width: 501px) 100vw, 501px"></a></p>
<p><a href="http://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope2.jpg"><img data-attachment-id="14909" data-permalink="https://physicsopenlab.org/2020/10/10/un-semplice-radiotelescopio-a-11-2-ghz/10ghzradiotelescope2/" data-orig-file="https://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope2.jpg" data-orig-size="1062,1416" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="10GHzRadiotelescope2" data-image-description="" data-medium-file="https://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope2-225x300.jpg" data-large-file="https://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope2-768x1024.jpg" loading="lazy" src="http://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope2-768x1024.jpg" alt="" width="500" height="667" srcset="https://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope2-768x1024.jpg 768w, https://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope2-225x300.jpg 225w, https://physicsopenlab.org/wp-content/uploads/2020/10/10GHzRadiotelescope2.jpg 1062w" sizes="(max-width: 500px) 100vw, 500px"></a></p>
<h3>LNB</h3>
<p dir="ltr" data-placeholder="Traduzione"><span lang="en">The first component of the system is the converter-amplifier block, the so-called <strong>LNB</strong>. This is the most important component because system performance largely depends on it. Our system receives in the 10-12 GHz band, at these frequencies the use of cables is problematic, for this reason the LNB block provides for a frequency down conversion in a lower band so that normal coaxial cables can be used.<br>
</span><span lang="en">The following image shows the basic scheme of the LNB block: there is a first <strong>RF amplification stage</strong>, followed by the mixer which multiplies the RF signal with the signal generated by a <strong>local oscillator (LO)</strong>. The resulting signal contains the sum and difference frequencies, the next filter eliminates the high frequency sum components to let pass only the frequencies in the band of interest, called <strong>intermediate frequencies (IF)</strong>, which are further amplified by another amplifier stage. In practice it is a heterodyne scheme, in which the frequency of the local oscillator is fixed.</span></p>
<p id="tw-target-text" dir="ltr" data-placeholder="Traduzione"><span lang="en"><a href="http://physicsopenlab.org/wp-content/uploads/2020/10/LNBScheme.png"><img data-attachment-id="14947" data-permalink="https://physicsopenlab.org/2020/10/10/un-semplice-radiotelescopio-a-11-2-ghz/lnbscheme/" data-orig-file="https://physicsopenlab.org/wp-content/uploads/2020/10/LNBScheme.png" data-orig-size="377,213" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="LNBScheme" data-image-description="" data-medium-file="https://physicsopenlab.org/wp-content/uploads/2020/10/LNBScheme-300x169.png" data-large-file="https://physicsopenlab.org/wp-content/uploads/2020/10/LNBScheme.png" loading="lazy" src="http://physicsopenlab.org/wp-content/uploads/2020/10/LNBScheme.png" alt="" width="377" height="213" srcset="https://physicsopenlab.org/wp-content/uploads/2020/10/LNBScheme.png 377w, https://physicsopenlab.org/wp-content/uploads/2020/10/LNBScheme-300x169.png 300w" sizes="(max-width: 377px) 100vw, 377px"></a></span></p>
<p>The LNB block we use is<strong> Invacom’s SNF-031</strong> model which has<strong> low noise</strong> and <strong>good stability</strong> of the gain parameters with respect to variations in operating temperature. The actual antenna is located inside the waveguide which has a C120 flange on the outside to which the feed horn is fixed, which has the task of collecting the waves reflected by the dish and conveying them to the inside the waveguide.</p>
<p>LNB features:</p>
<ul>
<li>Operating frequency band : 10.7 – 12.75 GHz</li>
<li>Intermediate frequencies (IF) : 950 – 2150 MHz, LO = 9.75 GHz</li>
<li><strong>Noise Figure NF = 0.3 dB</strong></li>
<li><strong>Gain G = 50 – 60 dB</strong></li>
</ul>
<p>The following images show the LNB block with its feed horn fixed to the focus of the dish.</p>
<p><a href="http://physicsopenlab.org/wp-content/uploads/2020/10/LNB0.jpg"><img data-attachment-id="14910" data-permalink="https://physicsopenlab.org/2020/10/10/un-semplice-radiotelescopio-a-11-2-ghz/lnb0/" data-orig-file="https://physicsopenlab.org/wp-content/uploads/2020/10/LNB0.jpg" data-orig-size="466,397" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="LNB0" data-image-description="" data-medium-file="https://physicsopenlab.org/wp-content/uploads/2020/10/LNB0-300x256.jpg" data-large-file="https://physicsopenlab.org/wp-content/uploads/2020/10/LNB0.jpg" loading="lazy" src="http://physicsopenlab.org/wp-content/uploads/2020/10/LNB0.jpg" alt="" width="399" height="340" srcset="https://physicsopenlab.org/wp-content/uploads/2020/10/LNB0.jpg 466w, https://physicsopenlab.org/wp-content/uploads/2020/10/LNB0-300x256.jpg 300w" sizes="(max-width: 399px) 100vw, 399px"></a></p>

<p><a href="http://physicsopenlab.org/wp-content/uploads/2020/10/LNB3.jpg"><img data-attachment-id="15000" data-permalink="https://physicsopenlab.org/2020/10/10/un-semplice-radiotelescopio-a-11-2-ghz/lnb3/" data-orig-file="https://physicsopenlab.org/wp-content/uploads/2020/10/LNB3.jpg" data-orig-size="1128,850" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="LNB3" data-image-description="" data-medium-file="https://physicsopenlab.org/wp-content/uploads/2020/10/LNB3-300x226.jpg" data-large-file="https://physicsopenlab.org/wp-content/uploads/2020/10/LNB3-1024x772.jpg" loading="lazy" src="http://physicsopenlab.org/wp-content/uploads/2020/10/LNB3-1024x772.jpg" alt="" width="550" height="415" srcset="https://physicsopenlab.org/wp-content/uploads/2020/10/LNB3-1024x772.jpg 1024w, https://physicsopenlab.org/wp-content/uploads/2020/10/LNB3-300x226.jpg 300w, https://physicsopenlab.org/wp-content/uploads/2020/10/LNB3-768x579.jpg 768w, https://physicsopenlab.org/wp-content/uploads/2020/10/LNB3.jpg 1128w" sizes="(max-width: 550px) 100vw, 550px"></a></p>
<p><a href="http://physicsopenlab.org/wp-content/uploads/2020/10/LNB4.jpg"><img data-attachment-id="15001" data-permalink="https://physicsopenlab.org/2020/10/10/un-semplice-radiotelescopio-a-11-2-ghz/lnb4/" data-orig-file="https://physicsopenlab.org/wp-content/uploads/2020/10/LNB4.jpg" data-orig-size="1184,902" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="LNB4" data-image-description="" data-medium-file="https://physicsopenlab.org/wp-content/uploads/2020/10/LNB4-300x229.jpg" data-large-file="https://physicsopenlab.org/wp-content/uploads/2020/10/LNB4-1024x780.jpg" loading="lazy" src="http://physicsopenlab.org/wp-content/uploads/2020/10/LNB4-1024x780.jpg" alt="" width="551" height="419" srcset="https://physicsopenlab.org/wp-content/uploads/2020/10/LNB4-1024x780.jpg 1024w, https://physicsopenlab.org/wp-content/uploads/2020/10/LNB4-300x229.jpg 300w, https://physicsopenlab.org/wp-content/uploads/2020/10/LNB4-768x585.jpg 768w, https://physicsopenlab.org/wp-content/uploads/2020/10/LNB4.jpg 1184w" sizes="(max-width: 551px) 100vw, 551px"></a></p>
<h3>The Receiver</h3>
<p id="tw-target-text" dir="ltr" data-placeholder="Traduzione"><span lang="en">The receiver consists of the few components, shown in the following image: there is a bias-T for feeding the LNB block, a bandpass filter centered at 1420 MHZ, a wide-band amplifier and the <strong>Airspy R2 SDR receiver</strong>. The “hardware” part has the function of limiting the receiving band and giving the signal a second amplification after the LNB stage. The signal is then acquired by Airspy and subsequently processed for the determination of the total power using <strong>GNURadio</strong> software. The <strong>radiometer</strong> function is practically realized through software.</span></p>
<p><a href="http://physicsopenlab.org/wp-content/uploads/2020/10/radiometer.png"><img data-attachment-id="14917" data-permalink="https://physicsopenlab.org/2020/10/10/un-semplice-radiotelescopio-a-11-2-ghz/radiometer/" data-orig-file="https://physicsopenlab.org/wp-content/uploads/2020/10/radiometer.png" data-orig-size="1088,668" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="radiometer" data-image-description="" data-medium-file="https://physicsopenlab.org/wp-content/uploads/2020/10/radiometer-300x184.png" data-large-file="https://physicsopenlab.org/wp-content/uploads/2020/10/radiometer-1024x629.png" loading="lazy" src="http://physicsopenlab.org/wp-content/uploads/2020/10/radiometer-1024x629.png" alt="" width="600" height="369" srcset="https://physicsopenlab.org/wp-content/uploads/2020/10/radiometer-1024x629.png 1024w, https://physicsopenlab.org/wp-content/uploads/2020/10/radiometer-300x184.png 300w, https://physicsopenlab.org/wp-content/uploads/2020/10/radiometer-768x472.png 768w, https://physicsopenlab.org/wp-content/uploads/2020/10/radiometer.png 1088w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p><strong>Features of our receiver :</strong><br>
Frequency Band = 80 MHz<br>
G<sub>LNB</sub> = 55 dB ; NF<sub>LNB</sub> = 0.3 dB<br>
G<sub>Filter</sub> = 3.5 dB (insertion loss)<br>
G<sub>Ampli</sub> = 15 dB ; NF<sub>Ampli</sub> = 0.75 dB<br>
<strong>Gain : G<sub>LNB</sub> – G<sub>Filter</sub> + G<sub>Ampli</sub> = 55 -3.5 +15 = 66.5 dB</strong><br>
<strong>Noise Figure : F = F<sub>LNB</sub> + (F<sub>Ampli</sub> – 1)/G<sub>LNB</sub> = 0.3 dB<br>
T<sub>e</sub> = (F – 1) * T<sub>0</sub> = 20.3 °K (Receiver equivalent temperature)</strong></p>
<h4>Bias-T</h4>
<p>The Bias-T has the function of “injecting” the supply voltage to the LNB block along the coaxial cable. In practice it is a simple circuit with a coupling capacitor to filter the DC component towards the RF side and an inductance at the DC input. Obtained on eBay, it can be easily self-built but attention must be paid to the “RF” quality of the components and the shielding.</p>
<p><a href="http://physicsopenlab.org/wp-content/uploads/2020/10/Bias-T.jpg"><img data-attachment-id="14914" data-permalink="https://physicsopenlab.org/2020/10/10/un-semplice-radiotelescopio-a-11-2-ghz/bias-t/" data-orig-file="https://physicsopenlab.org/wp-content/uploads/2020/10/Bias-T.jpg" data-orig-size="709,661" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Bias-T" data-image-description="" data-medium-file="https://physicsopenlab.org/wp-content/uploads/2020/10/Bias-T-300x280.jpg" data-large-file="https://physicsopenlab.org/wp-content/uploads/2020/10/Bias-T.jpg" loading="lazy" src="http://physicsopenlab.org/wp-content/uploads/2020/10/Bias-T.jpg" alt="" width="251" height="234" srcset="https://physicsopenlab.org/wp-content/uploads/2020/10/Bias-T.jpg 709w, https://physicsopenlab.org/wp-content/uploads/2020/10/Bias-T-300x280.jpg 300w" sizes="(max-width: 251px) 100vw, 251px"></a></p>
<h4>1420 MHz Band Pass Filter</h4>
<p>This filter is dedicated to amateur radioastronomers interested in the hydrogen line observations. It uses the TA2494A SAW component and measures only 50 x 10mm. It features edge pads for an easy soldering of a RF shield. Insertion loss is typically less than 3.5dB and bandwith 80MHz.</p>
<p><strong>Technical Data</strong> :<br>
Center Frequency <strong>1420MHz</strong><br>
Usable Bandpass <strong>1380-1460MHz</strong><br>
Insertion Loss, 1380 to 1460 MHz <strong>3.5dB</strong><br>
Amplitude Ripple, 1380 to 1460 MHz 1.0 dBpp<br>
VSWR, 1380 &nbsp;to 1420 MHz 1.9:1<br>
Rejection referenced to 0dB :<br>
DC to 1300 MHz 28dB<br>
1550 to 3000 MHz 30dB<br>
Impedance 50Ω<br>
Maximum Input Power Level 10 dBm</p>
<p>In the images below we show the unit and its frequency response. We have soldered two wires between the SMA female headers and we wrapped the filter with aluminum tape in order to shield the filter.</p>
<p><a href="http://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm.jpg"><img data-attachment-id="14378" data-permalink="https://physicsopenlab.org/2020/07/26/14302/filter21cm/" data-orig-file="https://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm.jpg" data-orig-size="1134,460" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="filter21cm" data-image-description="" data-medium-file="https://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm-300x122.jpg" data-large-file="https://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm-1024x415.jpg" loading="lazy" src="http://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm-1024x415.jpg" alt="" width="601" height="243" srcset="https://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm-1024x415.jpg 1024w, https://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm-300x122.jpg 300w, https://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm-768x312.jpg 768w, https://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm.jpg 1134w" sizes="(max-width: 601px) 100vw, 601px"></a></p>
<p><a href="http://physicsopenlab.org/wp-content/uploads/2020/09/filter21cm1.jpg"><img data-attachment-id="14432" data-permalink="https://physicsopenlab.org/2020/07/26/14302/filter21cm1/" data-orig-file="https://physicsopenlab.org/wp-content/uploads/2020/09/filter21cm1.jpg" data-orig-size="1054,388" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="filter21cm1" data-image-description="" data-medium-file="https://physicsopenlab.org/wp-content/uploads/2020/09/filter21cm1-300x110.jpg" data-large-file="https://physicsopenlab.org/wp-content/uploads/2020/09/filter21cm1-1024x377.jpg" loading="lazy" src="http://physicsopenlab.org/wp-content/uploads/2020/09/filter21cm1-1024x377.jpg" alt="" width="599" height="220" srcset="https://physicsopenlab.org/wp-content/uploads/2020/09/filter21cm1-1024x377.jpg 1024w, https://physicsopenlab.org/wp-content/uploads/2020/09/filter21cm1-300x110.jpg 300w, https://physicsopenlab.org/wp-content/uploads/2020/09/filter21cm1-768x283.jpg 768w, https://physicsopenlab.org/wp-content/uploads/2020/09/filter21cm1-1050x388.jpg 1050w, https://physicsopenlab.org/wp-content/uploads/2020/09/filter21cm1.jpg 1054w" sizes="(max-width: 599px) 100vw, 599px"></a></p>
<table>
<tbody>
<tr>
<td><b><i>Frequency (MHz)</i></b></td>
<td><b><i>Gain (dB)</i></b></td>
</tr>
<tr>
<td>1300</td>
<td><strong>-50</strong></td>
</tr>
<tr>
<td>1420</td>
<td><strong>-3.5</strong></td>
</tr>
<tr>
<td>1500</td>
<td><strong>-50</strong></td>
</tr>
</tbody>
</table>
<p><a href="http://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm2.png"><img data-attachment-id="14379" data-permalink="https://physicsopenlab.org/2020/07/26/14302/filter21cm2/" data-orig-file="https://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm2.png" data-orig-size="873,620" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="filter21cm2" data-image-description="" data-medium-file="https://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm2-300x213.png" data-large-file="https://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm2.png" loading="lazy" src="http://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm2.png" alt="" width="600" height="426" srcset="https://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm2.png 873w, https://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm2-300x213.png 300w, https://physicsopenlab.org/wp-content/uploads/2020/08/filter21cm2-768x545.png 768w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<h4>Wideband Amplifier</h4>
<p>This unit HAB-FLTNOSAW built by UPUTRONICS is a preamp designed to go between a software defined radio receiver and an antenna. The LNA used inside is a MiniCircuits PSA4-5043. This particular model has the SAW filter removed to cover the 0.1MHz to 4GHz. There are 2 options for powering the unit : either by the USB header or via bias-tee. Devices such as the Airspy can enable bias-tee and power the device. Alternatively any mini USB cable can be used to power the device. We chose to power the unit via USB line.</p>
<p><strong>Technical Data</strong> :<br>
Gain 24db @ 100MHz -&gt; <strong>15.2db @ 1415MHz</strong><br>
<strong>NF 0.75dB</strong><br>
Supply Voltage USB or Bias tee 5V</p>
<p>In the images below we …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://physicsopenlab.org/2020/10/10/a-simple-11-2-ghz-radiotelescope/">https://physicsopenlab.org/2020/10/10/a-simple-11-2-ghz-radiotelescope/</a></em></p>]]>
            </description>
            <link>https://physicsopenlab.org/2020/10/10/a-simple-11-2-ghz-radiotelescope/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26078761</guid>
            <pubDate>Tue, 09 Feb 2021 15:44:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Develop Transformative Tools for Thought]]>
            </title>
            <description>
<![CDATA[
Score 29 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26078115">thread link</a>) | @dhruvparamhans
<br/>
February 9, 2021 | https://numinous.productions/ttft/ | <a href="https://web.archive.org/web/*/https://numinous.productions/ttft/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="EssayContents">
      
      <div id="EssayContentsInner">
	
	<div>

      <p>
	Part of the origin myth of modern computing is the story of a
	golden age in the 1960s and 1970s. In this story, visionary
	pioneers pursued a dream in which computers enabled powerful
	tools for thought, that is, tools to augment human
	intelligence<span> E.g., Douglas
	Engelbart, <a href="https://numinous.productions/ttft/assets/Engelbart1962.pdf">Augmenting Human
	Intellect: A Conceptual Framework</a> (1962).</span>. One of
	those pioneers, Alan Kay, summed up the optimism of this dream
	when he wrote of the potential of the personal computer:
	“the very use of it would actually change the thought
	patterns of an entire
	civilization”<span> Alan
	Kay, <a href="https://numinous.productions/ttft/assets/Kay1989.pdf">User Interface: A Personal
	View</a> (1989).</span>.
      </p>

      <p>
	It's an inspiring dream, which helped lead to modern
	interactive graphics, windowing interfaces, word processors,
	and much else. But retrospectively it's difficult not to be
	disappointed, to feel that computers have not yet been nearly
	as transformative as far older tools for thought, such as
	language and writing. Today, it's common in technology circles
	to pay lip service to the pioneering dreams of the past. But
	nostalgia aside there is little determined effort to pursue
	the vision of transformative new tools for thought.
      </p>

      <p>
	We believe now is a good time to work hard on this vision
	again. In this essay we sketch out a set of ideas we believe
	can be used to help develop transformative new tools for
	thought. In the first part of the essay we describe an
	experimental prototype system that we've built, a kind
	of <em>mnemonic medium</em> intended to augment human memory.
	This is a snapshot of an ongoing project, detailing both
	encouraging progress as well as many challenges and
	opportunities.  In the second part of the essay, we broaden
	the focus. We sketch several other prototype systems. And we
	address the question: why is it that the technology industry
	has made comparatively little effort developing this vision of
	transformative tools for thought?
      </p>

      <p>
	In the opening we mentioned some visionaries of the past. To
	those could be added many others – Ivan Sutherland,
	Seymour Papert, Vannevar Bush, and more. Online there is much
	well-deserved veneration for these people. But such veneration
	can veer into an unhealthy reverence for the good old days, a
	belief that giants once roamed the earth, and today's work is
	lesser. Yes, those pioneers did amazing things, and arguably
	had ways of working that modern technologists, in both
	industry and academia, are poorly equipped to carry on. But
	they also made mistakes, and were ignorant of powerful ideas
	that are available today. And so a theme through both parts of
	the essay is to identify powerful ideas that weren't formerly
	known or weren't acted upon. Out of this understanding arises
	a conviction that a remarkable set of opportunities is open
	today.
      </p>

      <p>
	A word on nomenclature: the term “tools for
	thought” rolls off neither the tongue nor the
	keyboard. What's more, the term “tool” implies a
	certain narrowness. Alan Kay has
	argued<span> Again, in Alan
	Kay, <a href="https://numinous.productions/ttft/assets/Kay1989.pdf">User Interface: A Personal
	View</a> (1989), among other places.</span> that a more
	powerful aim is to develop a new <em>medium for
	thought</em>. A medium such as, say,
	Adobe <em>Illustrator</em> is essentially different from any
	of the individual tools <em>Illustrator</em> contains. Such a
	medium creates a powerful immersive context, a context in
	which the user can have new kinds of thought, thoughts that
	were formerly impossible for them. Speaking loosely, the range
	of expressive thoughts possible in such a medium is an
	emergent property of the elementary objects and actions in
	that medium. If those are well chosen, the medium expands the
	possible range of human thought.
      </p>

      <p>
	With that said, the term “tools for thought” has
	been widely used since Iverson's 1950s and 1960s
	work<span> An account may be found in
	Iverson's Turing Award
	lecture, <a href="https://numinous.productions/ttft/assets/Iverson1979.pdf">Notation as a Tool
	of Thought</a> (1979). Incidentally, even Iverson is really
	describing a medium for thought, the APL programming language,
	not a narrow tool.</span> introducing the term. And so we
	shall use “tools for thought” as our catch all
	phrase, while giving ourselves license to explore a broader
	range, and also occasionally preferring the term
	“medium” when it is apt.
      </p>

      <p>
	Let's come back to that phrase from the opening, about
	changing “the thought patterns of an entire
	civilization”. It sounds ludicrous, a kind of tech
	soothsaying. Except, of course, such changes have happened
	multiple times during human history: the development of
	language, of writing, and our other most powerful tools for
	thought. And, for better and worse, computers really have
	affected the thought patterns of our civilization over the
	past 60 years, and those changes seem like just the
	beginning. This essay is a small contribution to understanding
	how such changes happen, and what is still possible.
      </p>

      <p>
	The musician and comedian Martin Mull has observed that
	“writing about music is like dancing about
	architecture”. In a similar way, there's an inherent
	inadequacy in writing about tools for thought. To the extent
	that such a tool succeeds, it expands your thinking beyond
	what can be achieved using existing tools, including
	writing. The more transformative the tool, the larger the gap
	that is opened.  Conversely, the larger the gap, the more
	difficult the new tool is to evoke in writing. But what
	writing can do, and the reason we wrote this essay, is act as
	a bootstrap. It's a way of identifying points of leverage that
	may help develop new tools for thought. So let's get on with
	it.
      </p>


      

      <h2 id="introducing-mnemonic-medium">Introducing the mnemonic medium</h2>
      
      <p>
	Few subjects are more widely regarded as difficult than
	quantum computing and quantum mechanics. Indeed, popular media
	accounts often regale (and intimidate) readers with quotes
	from famous physicists in the vein of: “anyone who
	thinks they’ve understood quantum mechanics has not understood
	quantum mechanics”.
      </p>
      
      <p>
	What makes these subjects difficult? In fact, individually
	many of the underlying ideas are not too complicated for
	people with a technical background. But the ideas come in an
	overwhelming number, a tsunami of unfamiliar concepts and
	notation. People must learn in rapid succession of qubits, the
	bra-ket notation, Hadamard gates, controlled-not gates, and
	many, many other abstract, unfamiliar notions. They're
	imbibing an entire new language. Even if they can follow at
	first, understanding later ideas requires fluency with all the
	earlier ideas. It's overwhelming and eventually disheartening.
      </p>

      <p>
	As an experiment, we have developed a
	website, <a href="https://quantum.country/"><em>Quantum
	Country</em></a>, which explores a new approach to explaining
	quantum computing and quantum
	mechanics. Ostensibly, <em>Quantum Country</em> appears to be
	a conventional essay introduction to these subjects. There is
	text, explanations, and equations, much as in any other
	technical essay. Here's an excerpt:
      </p>
			
      <p><img src="https://numinous.productions/ttft/assets/quantum_country_sample-2x.png" srcset="https://numinous.productions/ttft/assets/quantum_country_sample.png, https://numinous.productions/ttft/assets/quantum_country_sample-2x.png 2x" alt="Screenshot depicting prose and math in Quantum Country, typical of a technical essay."></p><p>
	But it's not a conventional essay. Rather, <em>Quantum
	Country</em> is a prototype for a new type of <em>mnemonic
	medium</em>. Aspirationally, the mnemonic medium makes it
	almost effortless for users to remember what they read. That
	may sound like an impossible aspiration. What makes it
	plausible is that cognitive scientists know a considerable
	amount about how human beings store long-term
	memories. Indeed, what they know can almost be distilled to an
	actionable recipe: follow these steps, and you can remember
	whatever you choose.
      </p>

      <p>
	Unfortunately, those steps are poorly supported by existing
	media.<span>For more on this argument, see
	Andy Matuschak, <a href="https://andymatuschak.org/books/">Why
	books don’t work</a> (2019).</span> Is it possible to
	design a new medium which much more actively supports
	memorization? That is, the medium would build in (and, ideally,
	make almost effortless) the key steps involved in memory. If we
	could do this, then instead of memory being a haphazard event,
	subject to chance, the mnemonic medium would make memory into a
	choice. Of course, on its own this wouldn't make it trivial to
	learn subjects such as quantum mechanics and quantum computing
	– learning those subjects is about much more than memory.
	But it would help in addressing one core difficulty: the
	overwhelming number of new concepts and notation.
      </p>
      
      <p>
	In fact, there are many ways of redesigning the essay medium
	to do that. Before showing you our prototype, please pause for
	a moment and consider the following questions: how could you
	build a medium to better support a person's memory of what
	they read?  What interactions could easily and enjoyably help
	people consolidate memories? And, more broadly: is it possible
	to 2x what people remember? 10x? And would that make any
	long-term difference to their effectiveness?
      </p>

      <p>
	Let's sketch the user experience of <em>Quantum
	Country</em>. At the time of this writing the site contains
	three mnemonic essays (i.e., particular instances of the
	mnemonic medium). We'll focus on the introductory essay,
	<a href="https://quantum.country/qcvc">“Quantum
	  Computing for the Very Curious”</a>. Embedded within
	  the text of the essay are 112 questions about that
	  text. Users are asked to create an account, and quizzed as
	  they read on whether they remember the answers to those
	  questions. Here's what the interaction looks like, as a user
	  answers three questions.
      </p>

			<video autoplay="" loop="" muted="" playsinline="">
				<source src="https://numinous.productions/ttft/assets/qc_interaction.mp4" type="video/mp4">
				<source src="https://numinous.productions/ttft/assets/qc_interaction.webm" type="video/webm">
				<source src="https://numinous.productions/ttft/assets/qc_interaction.ogv" type="video/ogg">
			</video>

      <p>
	Note that this interaction occurs within the text of the essay
	itself. Here's a zoomed-out view, so you can see how such
	questions are surrounded by essay text both above and below:
      </p>

	<p><img src="https://numinous.productions/ttft/assets/qc_zoom_out-2x.png" srcset="https://numinous.productions/ttft/assets/qc_zoom_out.png, https://numinous.productions/ttft/assets/qc_zoom_out-2x.png 2x" alt="Screenshot depicting the interactive quiz interleaved in a few paragraphs of prose."></p><p>
	We use the term <em>cards</em> for these interface elements
	pairing questions and answers.
      </p>

      <p>
	Of course, for long-term memory it's not enough for users to
	be tested just once on their recall. Instead, a few days after
	first reading the essay, the user receives an email asking
	them to sign into a review session. In that review session
	they're …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://numinous.productions/ttft/">https://numinous.productions/ttft/</a></em></p>]]>
            </description>
            <link>https://numinous.productions/ttft/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26078115</guid>
            <pubDate>Tue, 09 Feb 2021 14:51:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Haskell: The Bad Parts, part 2 (2020)]]>
            </title>
            <description>
<![CDATA[
Score 245 | Comments 143 (<a href="https://news.ycombinator.com/item?id=26077823">thread link</a>) | @anuragsoni
<br/>
February 9, 2021 | https://www.snoyman.com/blog/2020/11/haskell-bad-parts-2 | <a href="https://web.archive.org/web/*/https://www.snoyman.com/blog/2020/11/haskell-bad-parts-2">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
      <section>
        <div>
          <div>
            <div>
              <p>
                <a href="https://www.beginrust.com/">New: The "Begin Rust" book</a>
              </p>

              




  






      <p>
        <i>
          See a typo? Have a suggestion?
          <a target="_blank" rel="nofollow" href="https://github.com/snoyberg/snoyman.com/edit/master/content/blog/haskell-bad-parts-2.md">Edit this page on Github</a>
        </i>
      </p>

      



      <p>If you didn't see it, please check out <a href="https://www.snoyman.com/blog/2020/10/haskell-bad-parts-1">part 1 of this series</a> to understand the purpose of this. Now, for more bad parts!</p>
<h2 id="partial-functions-in-general">Partial functions (in general)</h2>
<p>Laziness very likely belongs in this list. My favorite part of criticizing laziness is how quickly people jump to defend it based on edge cases. So let's be a bit more nuanced before I later get far <em>less</em> nuanced. Laziness is <strong>obviously</strong> a good thing. Strictness is <strong>obviously</strong> a good thing. They also both suck. It depends on context and purpose. Each of them introduce different kinds of issues. The real question is: what's a more sensible default? We'll get to that another time.</p>
<p>I called this section partial functions. Am I having a senior moment? Maybe, but I intentionally started with laziness. In a strict language, function calls can result in exceptions being thrown, segfaulting occurring, or panicking. (And if I write a "Rust: The Bad Parts", believe me, I'll be mentioning panicking.) The fact that a function <em>acts</em> like it can successfully perform something, but in fact fails in a predictable way (like failing a <code>HashMap</code> lookup), it should be reflected at the type level. If not, ya dun goofed.</p>
<p>Also, if you have a language that doesn't let you reflect this information at the type level: ya dun goofed.</p>
<p>Partial functions are the antithesis of this concept. They allow you to say "yeah dude, I can <em>totally</em> give you the first value in an empty list." Partial functions are like politicians: you can tell they're lying because their lips are moving. ("But Michael," you say. "Functions don't have lips!" Whatever, I'm waxing poetical.)</p>
<p>Alright, so plenty of languages screw this up. Haskell tells those languages "hold my beer."</p>
<p><img src="https://www.snoyman.com/static/images/holdmybeer.jpg"></p><p>Haskell screws up partial functions way, way worse than other languages:</p>
<ol>
<li>It promotes a whole bunch of them in the standard libraries and <code>Prelude</code>.</li>
<li>Some libraries, like <code>vector</code> (I'm getting to you, don't worry) make it <em>really</em> confusing by providing an <code>index</code> and <code>unsafeIndex</code> function. Hint: <code>index</code> isn't really safe, it's just less unsafe.</li>
<li>There's no obvious way to search for usages of these partial functions.</li>
<li>And, by far, the worst...</li>
</ol>
<h3 id="values-are-partial-too">Values are partial too!</h3>
<p>Only in a lazy language does this exist. You call a function. You get a result. You continue working. In any other non-lazy language, that means you have a value. If I have a <code>u32</code> in Rust, I actually have a <code>u32</code> in Rust. Null pointers in languages like C and Java somewhat muddy this situation, but at least primitive types are really there if they say they're there.</p>
<p>No, not Haskell. <code>x :: Int</code> may in fact not exist. It's a lie. <code>let x = head [] :: [Int]</code> is a box waiting to explode. And you find out <em>much</em> later. And it's even worse than that. <code>let alice = Person { name = "Alice", age = someAge }</code> may give you a valid <code>Person</code> value. You can evaluate it. But Cthulhu help you if you evaluate <code>age alice</code>. Maybe, just maybe, <code>someAge</code> is a bottom value. Boom! You've smuggled a dirty bomb out.</p>
<p>I'm not advocating for removing laziness in Haskell. In fact I'm not really advocating for much of anything in this series. I'm just complaining, because I like complaining.</p>
<p>But <em>if</em> I was to advocate some changes:</p>
<ul>
<li>Deprecate partial functions</li>
<li>Introduce a naming scheme for partial functions to be more obvious</li>
<li>Introduce a compiler warning to note partial function use (with a pragma to turn off specific usages)</li>
<li>Warn by default on partial pattern matches</li>
<li>Advocate strict data fields by default</li>
</ul>
<h3 id="but-ackshualllly-infinite-loops">But ackshualllly, infinite loops</h3>
<p>Someone's gonna say it. So I'll say it. Yes, without major language changes, you can't prevent partial functions. You can't even detect them, unless Turing was wrong (and I have my suspicions.) But Haskell community, please, please learn this lesson:</p>
<p><strong>DON'T LET THE PERFECT BE THE ENEMY OF THE GOOD</strong></p>
<p>We can get rid of many of the most common partial functions trivially. We can detect many common cases by looking for partial pattern matches and usage of <code>throw</code> (again, horribly named function). "But we can't get everything" doesn't mean "don't try to get something."</p>
<h2 id="hubris">Hubris</h2>
<p>Given what I just said, we Haskellers have a lot of hubris. Each time you say "if it compiles it works," a thunk dies and collapses into a blackhole. We've got plenty of messes in Haskell that don't sufficiently protect us from ourselves. The compiler can only do as good a job as our coding standards and our libraries allow.</p>
<p>"But Haskell's at least better than languages like PHP." I mean, obviously I agree with this, or I'd be writing PHP. But since I'm being ridiculously hyperbolic here, let me make a ridiculous claim:</p>
<blockquote>
<p><strong>PHP is better than Haskell, since at least you don't get a false sense of security</strong></p>
<p><em>- Michael Snoyman, totally 100% what he actually believes, you should totally quote this out of context</em></p>
</blockquote>
<p>I've said this so many times. So I'll say it again. Using a great language with safety features is one tiny piece of the puzzle.</p>
<ul>
<li>Did you get the software requirements right?</li>
<li>Did you leverage the type system to prevent the bugs you're trying to prevent?</li>
<li>Do your underlying libraries have bugs?</li>
<li>Did you find a way to implement a function with correct types but incorrect semantics?</li>
<li>Did you host the thing on a dinky server sitting under your desk and forget that you have power outages on a daily basis?</li>
<li>Did you forget to write a single test case?</li>
<li>Do your test cases actually test anything meaningful?</li>
</ul>
<p>There are <em>so many ways</em> for software to fail outside the purview of the type system. We've got to stop thinking that somehow Haskell (or, for that matter, Rust, Scala, and other strongly typed languages) are some kind of panacea. Seriously: the PHP people at least know their languages won't protect them from anything. We should bring some of that humility back to Haskell.</p>
<p>Haskell provides me tools to help prevent certain classes of bugs, so I can spend more of my time catching a bunch of other bugs that I'm absolutely going to write. Because I'm dumb. And we need to remember: we're all dumb.</p>
<h2 id="more-partial-functions">More partial functions!</h2>
<p>You know what's worse than partial functions? Insidiously partial functions. We've all been screaming about <code>head</code> and <code>tail</code> for years. My hackles rise every time I see a <code>read</code> instead of <code>readMaybe</code>. I can't remember the last time I saw the <code>!!</code> operator in production code.</p>
<p>But there are plenty of other functions that are just as dangerous, if not more so. More dangerous because they aren't well known to be partial. They are commonly used. People don't understand why they're dangerous. And they fail only in edge cases that people aren't thinking about.</p>
<p>Exhibit A: I present <code>decodeUtf8</code>. (Thanks <a href="https://twitter.com/kerckhove_ts/status/1321390954172063745?s=20">Syd</a>.)</p>
<p>Go ahead, search your codebase. Be dismayed that you've found it present.</p>
<p>What's wrong with <code>decodeUtf8</code>? As we established last time, character encoding crap breaks stuff in production. UTF-8 works about 99% of the time, especially for people in Western countries. You'll probably forget to even test for it. And that function looks so benign: <code>decodeUtf8 :: ByteString -&gt; Text</code>.</p>
<p><strong>DO NOT BE FOOLED</strong></p>
<p>This function is a ticking time bomb. Use <code>decodeUtf8'</code> (yes, it's named that badly, just like <code>foldl'</code>) and explicitly handle error cases. Or use I/O functions that explicitly handle UTF-8 decoding errors and throw a runtime exception.</p>
<p>"I can't believe Michael still thinks runtime exceptions are a good idea." I'll get to that another time. I don't really believe they're a good idea. I believe they are omnipresent, better than bottom values, and our least-bad-option.</p>
<h2 id="law-abiding-type-classes">Law-abiding type classes</h2>
<p>Now I've truly lost it. What in tarnation could be wrong with law-abiding type classes? They're good, right? Yes, they are! The section heading is complete clickbait. Haha, fooled you!</p>
<p>There's a concept in the Haskell community that all type classes should be law-abiding. I've gone to the really bad extreme opposing this in the past with early versions of <code>classy-prelude</code>. In my defense: it was an experiment. But it was a bad idea. I've mostly come around to the idea of type classes being lawful. (Also, the original namespacing issues that led to <code>classy-prelude</code> really point out a much bigger bad part of Haskell, which I'll get to later. Stay tuned! Hint: Rust beat us again.)</p>
<p>Oh, right. Speaking of Rust: they do <em>not</em> believe in law-abiding type classes. There are plenty of type classes over there (though they call them <code>trait</code>s) that are completely ad-hoc. I'm looking at you, <code>FromIterator</code>. This is Very, Very Bad of course. Or so my Haskell instincts tell me. And yet, it makes code Really, Really Good. So now I'm just confused.</p>
<p>Basically: I think we need much more nuanced on this in the Haskell community. I'm leaning towards my <em>very</em> original instincts having been spot on. So:</p>
<ul>
<li>Law abiding type classes: great</li>
<li>Flippantly non-law-abiding type classes ala the original <code>classy-prelude</code>: bad</li>
<li>"You know what I meant" typeclasses like <code>ToContent</code> in Yesod: also great</li>
</ul>
<p>This isn't exactly in line with a "bad part" of Haskell. Up until now I've been giving a nuanced reflection on my journeys in Haskell. Let me try something better then. Ahem.</p>
<p><strong>DON'T LECTURE ME ON LAW ABIDING TYPE CLASSES AND FLAGRANTLY VIOLATE LAWS</strong></p>
<p>I'm staring at you, <code>Eq Double</code>. No, you cannot do equality on a <code>Double</code>. (And thanks again to Syd for this idea.) Rust, again, Got It Right. See <code>PartialEq</code> vs <code>Eq</code>. Floating point values do not allow for total equality. This makes things like <code>Map Double x</code> dangerous. Like, super dangerous. Though maybe not as dangerous as <code>HashMap Double x</code>, which deserves its own rant later.</p>
<p>So come down from your high horses. We don't have law abiding type classes. We have "if I close my eyes and pretend enough then maybe I have law abiding type classes."</p>
<h2 id="unused-import-warnings">Unused import warnings</h2>
<p>Haskell has a dumb set of default warnings enabled. ("I think you mean GHC, one implementation of Haskell, not …</p></div></div></div></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.snoyman.com/blog/2020/11/haskell-bad-parts-2">https://www.snoyman.com/blog/2020/11/haskell-bad-parts-2</a></em></p>]]>
            </description>
            <link>https://www.snoyman.com/blog/2020/11/haskell-bad-parts-2</link>
            <guid isPermaLink="false">hacker-news-small-sites-26077823</guid>
            <pubDate>Tue, 09 Feb 2021 14:27:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pseudophilosophy encourages confused, self-indulgent thinking]]>
            </title>
            <description>
<![CDATA[
Score 186 | Comments 214 (<a href="https://news.ycombinator.com/item?id=26076731">thread link</a>) | @pseudolus
<br/>
February 9, 2021 | https://psyche.co/ideas/pseudophilosophy-encourages-confused-self-indulgent-thinking | <a href="https://web.archive.org/web/*/https://psyche.co/ideas/pseudophilosophy-encourages-confused-self-indulgent-thinking">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p><strong>There are many kinds</strong> of pseudosciences: astrology, homeopathy, flat-Earthism, anti-vaxx. These â€˜fieldsâ€™ traffic in bizarre claims with scientific pretensions. On a surface level, these claims <em>seem</em> to be scientific and usually appear to comment on the same kind of things that science does. However, upon closer inspection, pseudoscience is revealed to be <a href="https://aeon.co/ideas/why-bullshit-is-no-laughing-matter" rel="noopener">bullshit</a>: it is indifferent to the truth. Analogous to pseudoscience, can there be such a thing as pseudophilosophy, in which one makes claims with philosophical pretensions which on closer inspection turn out to be bullshit? I think there is.</p>
<p>Letâ€™s begin with the concept of pseudophilosophy. If there is something deserving of that name, then it would be deficient with respect to philosophical issues in the same way that pseudoscience is deficient with respect to scientific issues. So, in order to get a grip on pseudophilosophy, we should first <a href="https://doi.org/10.1111/theo.12271" rel="nofollow noreferrer noopener">look</a> more closely at the way in which pseudoscience is deficient, and then see whether we can find something analogous in the philosophical domain.</p>
<p>What makes pseudoscientific beliefs deficient is that theyâ€™re formed in an <em>epistemically unconscientious</em> way. Thatâ€™s to say, these beliefs are made from culpably confused and uninformed reasoning. For example, the belief that the Earth is flat can be sustained only by self-willed disregard of the massive amounts of evidence to the contrary, accumulated over several centuries by several different sciences.</p>
<p>However, such unconscientiousness doesnâ€™t presuppose insincerity or charlatanry. A charlatan is someone who has a hidden, usually profit-seeking, agenda and who is fundamentally indifferent to whether their beliefs are true. Often bullshit is produced without such insincerity, however, since one can <em>care</em> about the truth of oneâ€™s beliefs without <em>taking care</em> with respect to it.</p>
<p>A problem is that most of us are lacking in epistemic conscientiousness, at least sometimes and to some extent. In order for something to count as pseudoscience, some minimal degree of unconscientiousness is therefore required. A good rule of thumb for being conscientious is to keep an eye out for classical fallacies such as <a href="https://aeon.co/ideas/how-ad-hominem-arguments-can-demolish-appeals-to-authority" rel="noopener">ad hominem</a>, straw man, false dilemma and cherry-picking. Such fallacies occur in all kinds of contexts, but in pseudoscience they occur more systematically.</p>
<p>Whether there is a God or whether there are objective moral truths have to be answered largely via a priori reflection, if at all</p>
<p>Epistemic unconscientiousness is an essential but not exhaustive component of pseudoscience. To count as pseudoscientific, a belief must also be about some scientific issue, and this is precisely where pseudoscience and pseudophilosophy differ. Just like pseudoscience, pseudophilosophy is defined by a lack of epistemic conscientiousness, but its subject matter is philosophical rather than scientific.</p>
<p>Roughly speaking, the difference between scientific and philosophical issues is that the latter arenâ€™t in any straightforward way resolvable via empirical investigation. Whether there is a God, for example, or whether there are objective moral truths, are questions that have to be answered largely via a priori reflection, if at all. These questions are thus different from questions such as whether the Earth is flat or spherical, or whether anthrax is caused by bacteria, which do have empirically accessible answers.</p>
<p><strong>There are two kinds of</strong> pseudophilosophy, one mostly harmless and the other insidious. The first variety is usually found in popular scientific contexts. This is where writers, typically with a background in the natural sciences, walk self-confidently into philosophical territory without realising it, and without conscientious attention to relevant philosophical distinctions and arguments. Often implicit empiricist assumptions in epistemology, metaphysics and the philosophy of language are relied upon as if they were self-evident, and without awareness of the threat that those very assumptions pose to the authorâ€™s own reasoning. We can call this phenomenon scientistic pseudophilosophy.</p>
<p>An illustrative example is Sam Harrisâ€™s <a href="https://samharris.org/books/the-moral-landscape/" rel="nofollow noreferrer noopener">book</a> <em>The Moral Landscape</em> (2010), in which straw men are lined up due to Harrisâ€™s failure to grasp the content of many of the philosophical claims and arguments that he criticises, such as Humeâ€™s law (or the is/ought problem) and <span>G E Mooreâ€™s</span> open-question argument (ie, that no moral property is identical to a natural property). Similarly, in <em>A Universe from Nothing</em> (2012), Lawrence Krauss engages with philosophical arguments for theism without understanding them properly. Most saliently, he ends up criticising a caricature version of the so-called cosmological argument about the existence of God.</p>
<p>Usually, the prose is infused with arcane terminology and learned jargon, creating an aura of scholarly profundity</p>
<p>The insidious kind of pseudophilosophy, which I will focus on here, is an academic enterprise, pursued primarily within the humanities and social sciences. I donâ€™t mean to suggest that the disciplines in question are <em>inherently</em> pseudophilosophical, only that, for some reason, a whole lot of pseudophilosophy goes on within them (although this will vary greatly between different universities and departments). Often philosophical issues are raised concerning knowledge, truth, objectivity, rationality and scientific methodology, and, again, without conscientious attention to relevant philosophical distinctions and arguments. A characteristic trait is a deferential attitude toward some supposedly great continental European thinker or thinkers, such as <span>G W F Hegel,</span> Karl Marx, Sigmund Freud, Carl Jung, Martin Heidegger or Jean-Paul Sartre (who might or might not have themselves been guilty of pseudophilosophy). Usually, the prose is infused with arcane terminology and learned jargon, creating an aura of scholarly profundity. We can <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/theo.12047" rel="nofollow noreferrer noopener">call</a> this phenomenon obscurantist pseudophilosophy.</p>
<p>While pseudoscience is particularly prone to causal fallacies and cherry-picking of data, the most common fallacy in obscurantist pseudophilosophy is equivocation. This fallacy <a href="https://journals.sagepub.com/doi/abs/10.1177/0392192112444984" rel="nofollow noreferrer noopener">exploits</a> ambiguities in certain key terms, where plausible but trivial claims lend apparent credibility to interesting but controversial ones. When challenged, the obscurantist will typically retreat to the safe house provided by the trivial interpretation of his claims, only to reoccupy the controversial ground once the critic has left the scene.</p>
<p>Let me <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9973.2005.00370.x" rel="nofollow noreferrer noopener">illustrate</a> how this works, focusing on <a href="https://aeon.co/essays/why-foucaults-work-on-power-is-more-important-than-ever" rel="noopener">Michel Foucault</a>, one of the central figures of French postmodernism. A central theme in Foucaultâ€™s writings is a critique of the notion of objective truth. Although there are controversies about interpretation, at least on the face of it Foucault maintains that truth is socially constructed and subject to ideological influence, and therefore not objective. However, his arguments for this claim focus entirely on the way in which what is assumed or believed to be true is influenced by what he refers to as â€˜powerâ€™. It is, of course, a plausible claim that our assumptions or beliefs are susceptible to ideological influence, especially in emotionally charged areas such as politics, but also in supposedly rational areas such as science. But Foucault doesnâ€™t explain how this rather mundane observation is supposed to imply or support the philosophically controversial claim that what is true, or which facts obtain (concerning the shape of the Earth, for example), is susceptible to ideological influence. Instead, by using the word â€˜truthâ€™ in an impressionistic fashion, the distinction between belief and truth is smudged over, allowing Foucault to make seemingly profound statements such as:</p>
<blockquote>[T]ruth isnâ€™t outside power, or lacking in power: contrary to a myth whose history and functions would repay further study, truth isnâ€™t the reward of free spirits, the child of protracted solitude, nor the privilege of those who have succeeded in liberating themselves. Truth is a thing of this world: it is produced only by virtue of multiple forms of constraint.</blockquote>
<p>I leave it as an exercise to the reader to disambiguate this statement and see what remains.</p>
<p>This kind of fallacious critique of the notion of objective truth is a particularly pernicious aspect of obscurantist pseudophilosophy in general. Often, itâ€™s due to simple misunderstandings (such as confusing truth with belief or knowledge), but sometimes itâ€™s due rather to wilful obscurity (as in the case of Foucault).</p>
<p>Perhaps due to its aura of academic legitimacy and profundity, obscurantist pseudophilosophy is often used to give credence to dogmatic and bellicose political agendas, both on the Left and on the Right. Beyond that, it encourages confused and self-indulgent thinking in university students, and consumes vast resources that could be put to better use.</p>
<p>While pseudoscience can perhaps be counteracted by science education, the cure for pseudophilosophy is not science education but philosophical education. More specifically, it is a matter of developing the kind of basic critical thinking skills that are taught to undergraduates in philosophy. This doesnâ€™t need to be anything fancy. Students should be taught things like learning to distinguish in a disciplined way between central philosophical concepts such as belief, truth, rationality and knowledge. They should be aware of the way ambiguities can be exploited by equivocating arguments, and become adept at how to spot other fallacies such as ad hominem and straw man. With these fundamental tools in hand, there would be a good deal less pseudophilosophy going around.</p></div></div></div>]]>
            </description>
            <link>https://psyche.co/ideas/pseudophilosophy-encourages-confused-self-indulgent-thinking</link>
            <guid isPermaLink="false">hacker-news-small-sites-26076731</guid>
            <pubDate>Tue, 09 Feb 2021 12:34:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: "100 Page Python Intro" eBook]]>
            </title>
            <description>
<![CDATA[
Score 101 | Comments 26 (<a href="https://news.ycombinator.com/item?id=26076721">thread link</a>) | @asicsp
<br/>
February 9, 2021 | https://learnbyexample.github.io/100_page_python_intro/introduction.html | <a href="https://web.archive.org/web/*/https://learnbyexample.github.io/100_page_python_intro/introduction.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><nav id="sidebar" aria-label="Table of contents"></nav><div id="page-wrapper"><div class="page"><div id="content"><main><p><a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Wikipedia</a> does a great job of describing about Python in a few words. So, I'll just copy-paste relevant information here:</p><blockquote><p>Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.</p><p>Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented, and functional programming. Python is often described as a "batteries included" language due to its comprehensive standard library.</p><p>As of December 2020 Python ranked third in TIOBE’s index of most popular programming languages, behind <code>C</code> and <code>Java</code>.</p></blockquote><blockquote><p><img src="https://learnbyexample.github.io/100_page_python_intro/images/info.svg" alt="info"> See also <a href="https://docs.python.org/3/faq/general.html">docs.python: General Python FAQ</a> for answers to questions like "What is Python?", "What is Python good for?", "Why is it called Python?", etc.</p></blockquote><h2><a href="#installation" id="installation">Installation</a></h2><p>On modern Linux distributions, you are likely to find Python already installed. It may be a few versions behind, but should work just fine for most of the topics covered in this book. To get the exact version used here, visit <a href="https://www.python.org/downloads/">Python downloads page</a> and install using the appropriate source for your operating system. Should you face any issues in installing, search online for a solution. Yes, that is something I expect you should be able to do as a prerequisite for this book, i.e. you should have prior experience with basic programming and computer usage.</p><blockquote><p><img src="https://learnbyexample.github.io/100_page_python_intro/images/info.svg" alt="info"> See <a href="https://docs.python.org/3/whatsnew/index.html">docs.python: What's New</a> to track changes across versions. As mentioned in the Preface chapter, <strong>3.9.0</strong> is the version used in this book.</p></blockquote><h2><a href="#online-tools" id="online-tools">Online tools</a></h2><p>In case you are facing installation issues, or do not want to (or cannot) install Python on your computer for some reason, there are plenty of options to execute Python programs using online tools. Some of the popular ones are listed below:</p><ul><li><a href="https://repl.it/languages/python3">Repl.it</a> — Interactive playground. Code, collaborate, compile, run, share, and deploy Python and more online from your browser</li><li><a href="http://www.pythontutor.com/visualize.html#mode=edit">Pythontutor</a> — Visualize code execution, also has example codes and ability to share sessions</li><li><a href="https://www.pythonanywhere.com/">PythonAnywhere</a> — Host, run, and code Python in the cloud</li></ul><p>The <a href="https://www.python.org/">offical Python website</a> also has a <em>Launch Interactive Shell</em> option (<a href="https://www.python.org/shell/">https://www.python.org/shell/</a>), which gives access to a REPL session.</p><h2><a href="#first-program" id="first-program">First program</a></h2><p>It is customary to start learning a new programming language by printing a simple phrase. Create a new directory, say <code>Python/programs</code> for this book. Then, create a plain text file named <code>hello.py</code> with your favorite text editor and type the following piece of code.</p><pre><code># hello.py
print('*************')
print('Hello there!')
print('*************')
</code></pre><p>If you are familiar with using command line on a Unix-like system, run the script as shown below. Other options to execute a Python program will be discussed in the next section.</p><pre><code>$ python3.9 hello.py
*************
Hello there!
*************
</code></pre><p>A few things to note here. The first line is a comment, used here to indicate the name of the Python program. <code>print()</code> is a built-in function, which can be used without having to load some library. A single string argument has been used for each of the three invocations. <code>print()</code> automatically appends a newline character by default. The program ran without a compilation step. As quoted earlier, Python is an <em>interpreted</em> language. More details will be discussed in later chapters.</p><blockquote><p><img src="https://learnbyexample.github.io/100_page_python_intro/images/info.svg" alt="info"> All the Python programs discussed in this book, along with related text files, can be accessed from my GitHub repo <a href="https://github.com/learnbyexample/100_page_python_intro/tree/main/programs">learnbyexample: 100_page_python_intro</a>. However, I highly recommend typing the programs manually by yourself.</p></blockquote><h2><a href="#ide-and-text-editors" id="ide-and-text-editors">IDE and text editors</a></h2><p>An <strong>integrated development environment</strong> (IDE) might suit you better if you are not comfortable with the command line. IDE provides features likes debugging, syntax highlighting, autocompletion, code refactoring and so on. They also help in setting up <strong>virtual environment</strong> to manage different versions of Python and modules (more on that later).</p><p>If you install Python on Windows, it will automatically include <strong>IDLE</strong>, an IDE built using Python's <code>tkinter</code> module. On Linux, see if you already have the program <code>idle3.9</code>. Otherwise you may have to install it separately, for example, <code>sudo apt install idle-python3.9</code> on Ubuntu.</p><p>When you open IDLE, you'll get a Python shell (discussed in the next section). For now, click the <strong>New File</strong> option under <strong>File</strong> menu to open a text editor. Type the short program <code>hello.py</code> discussed in the previous section. After saving the code, press <strong>F5</strong> to run it. You'll see the results in the shell window.</p><p>Screenshots from the text editor and the Python shell are shown below.</p><p><img src="https://learnbyexample.github.io/100_page_python_intro/images/hello.png" alt="hello.py program on IDLE editor"></p><p><img src="https://learnbyexample.github.io/100_page_python_intro/images/Python_shell_run.png" alt="Python shell example with output from an executed program"></p><p>Popular alternatives to IDLE are listed below:</p><ul><li><a href="https://thonny.org/">Thonny</a> — Python IDE for beginners, lots of handy features like viewing variables, debugger, step through, highlight syntax errors, name completion, etc</li><li><a href="https://www.jetbrains.com/pycharm/">Pycharm</a> — smart code completion, code inspections, on-the-fly error highlighting and quick-fixes, automated code refactorings, rich navigation capabilities, support for frameworks, etc</li><li><a href="https://www.spyder-ide.org/">Spyder</a> — typically used for scientific computing</li><li><a href="https://jupyter.org/">Jupyter</a> — web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text</li><li><a href="https://vscodium.com/">VSCodium</a> — community-driven, freely-licensed binary distribution of VSCode</li><li><a href="https://github.com/vim/vim">Vim</a>, <a href="https://www.gnu.org/software/emacs/">Emacs</a>, <a href="https://www.geany.org/">Geany</a>, <a href="https://wiki.gnome.org/Apps/Gedit">Gedit</a> — text editors with support for syntax highlighting and more</li></ul><h2><a href="#repl" id="repl">REPL</a></h2><p>One of the best features of Python is the interactive shell. Such shells are also referred to as REPL, which is an abbreviation for <strong>R</strong>ead <strong>E</strong>valuate <strong>P</strong>rint <strong>L</strong>oop. The Python REPL makes it easy for beginners to try out code snippets for learning purposes. Beyond learning, it is also useful for developing a program in small steps, debugging a large program by trying out few lines of code at a time and so on. REPL will be used frequently in this book to show code snippets.</p><p>When you launch Python from the command line, or open IDLE, you get a shell that is ready for user input after the <code>&gt;&gt;&gt;</code> prompt.</p><pre><code>$ python3.9
Python 3.9.0 (default, Dec  2 2020, 10:42:13) 
[GCC 5.4.0 20160609] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; 
</code></pre><p>Try the below instructions. The first one displays a greeting using the <code>print()</code> function. Then, a user defined variable is used to store a string value. To display the value, you can either use <code>print()</code> again or just type the variable name. Expression results are immediately displayed in the shell. Name of a variable by itself is a valid expression. This behavior is unique to the REPL and an expression by itself won't display anything when used inside a script.</p><pre><code>&gt;&gt;&gt; print('have a nice day')
have a nice day

&gt;&gt;&gt; username = 'learnbyexample'
&gt;&gt;&gt; print(username)
learnbyexample

# use # to start a single line comment
# note that string representation is shown instead of actual value
# details will be discussed later
&gt;&gt;&gt; username
'learnbyexample'

# use exit() to close the shell, can also use Ctrl+D shortcut
&gt;&gt;&gt; exit()
</code></pre><p>I'll stress again the importance of following along the code snippets by manually typing them on your computer. Programming requires hands-on experience too, reading alone isn't enough. As an analogy, can you learn to drive a car by just reading about it? Since one of the prerequisite is that you should already be familiar with programming basics, I'll extend the analogy to learning to drive a different car model. Or, perhaps a different vehicle such as a truck or a bus might be more appropriate here.</p><blockquote><p><img src="https://learnbyexample.github.io/100_page_python_intro/images/info.svg" alt="info"> Depending on the command line shell you are using, you might have the <code>readline</code> library that makes it easier to use the REPL. For example, <code>up</code> and <code>down</code> arrow keys to browse code history, re-execute them (after editing if necessary), search history, autocomplete based on first few characters and so on. See <a href="https://en.wikipedia.org/wiki/GNU_Readline">wikipedia: GNU readline</a> and <a href="https://wiki.archlinux.org/index.php/readline">wiki.archlinux: readline</a> for more information.</p></blockquote><blockquote><p><img src="https://learnbyexample.github.io/100_page_python_intro/images/info.svg" alt="info"> You can use <code>python3.9 -q</code> to avoid <em>version and copyright messages</em> when you start an interactive shell. Use <code>python3.9 -h</code> or visit <a href="https://docs.python.org/3/using/cmdline.html">docs.python: Command line and environment</a> for documentation on cli options.</p></blockquote><h2><a href="#documentation-and-getting-help" id="documentation-and-getting-help">Documentation and getting help</a></h2><p>The offical Python website has an extensive documentation located at <a href="https://docs.python.org/3/">https://docs.python.org/3/</a>. This includes a tutorial, which is much more comprehensive than the contents presented in this book, several guides for specific modules like <code>re</code> and <code>argparse</code> and various other information.</p><p>Here's a couple of annotated screenshots:</p><p><img src="https://learnbyexample.github.io/100_page_python_intro/images/py_docs_1.png" alt="Python documentation: part 1"></p><p><img src="https://learnbyexample.github.io/100_page_python_intro/images/py_docs_2.png" alt="Python documentation: part 2"></p><p>Python provides a <code>help()</code> function, which is quite handy to use from the REPL. If you type <code>help(print)</code> and press the Enter key, you'll get a screen as shown below. If you are using IDLE, the output would be displayed on the same screen. Otherwise, the content might be shown on a different screen depending on your <code>pager</code> settings. Typically, pressing the <code>q</code> key will quit the <code>pager</code> and get you back to the shell.</p><p><img src="https://learnbyexample.github.io/100_page_python_intro/images/help_print.png" alt="help print"></p><blockquote><p><img src="https://learnbyexample.github.io/100_page_python_intro/images/info.svg" alt="info"> Quotes are necessary, for example <code>help('import')</code> and <code>help('del')</code>, if the topic you are looking for isn't an object.</p></blockquote><p>If you get stuck with a problem, there are several ways to get it resolved. For example:</p><ol><li>read/search for that particular topic from documentation/books/tutorials/etc</li><li>reduce the code as much as possible so that you are left with minimal code necessary to reproduce the issue</li><li>talk about the problem with a friend/colleague/inanimate-objects/etc (see <a href="https://rubberduckdebugging.com/">Rubber duck debugging</a>)</li><li>search about the problem online</li></ol><p>You can also ask for help on forums. Make sure to read the instructions provided by the respective forums before asking a question. See also <a href="http://catb.org/%7Eesr/faqs/smart-questions.html#before">how to ask smart-questions</a>. Here's some forums you can use:</p><ul><li><a href="https://www.reddit.com/r/learnpython">/r/learnpython</a> and <a href="https://www.reddit.com/r/learnprogramming/">/r/learnprogramming/</a> — beginner friendly</li><li><a href="https://python-forum.io/">python-forum</a> — dedicated Python forum, encourages back and forth discussions based on the topic of the thread</li><li><a href="https://www.reddit.com/r/Python/">/r/Python/</a> — general Python discussion</li><li><a href="https://stackoverflow.com/tags/python">stackoverflow: python tag</a></li></ul><blockquote><p><img src="https://learnbyexample.github.io/100_page_python_intro/images/info.svg" alt="info"> The <a href="https://learnbyexample.github.io/100_page_python_intro/debugging.html#debugging">Debugging</a> chapter will discuss more on this topic.</p></blockquote></main><nav aria-label="Page navigation"><a rel="prev" href="https://learnbyexample.github.io/100_page_python_intro/preface.html" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left"> <i></i> </a><a rel="next" href="https://learnbyexample.github.io/100_page_python_intro/numeric-data-types.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right"> <i></i> </a></nav></div></div><nav aria-label="Page navigation"><a rel="prev" href="https://learnbyexample.github.io/100_page_python_intro/preface.html" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left"> <i></i> </a><a rel="next" href="https://learnbyexample.github.io/100_page_python_intro/numeric-data-types.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right"> <i></i> </a></nav></div></div>]]>
            </description>
            <link>https://learnbyexample.github.io/100_page_python_intro/introduction.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26076721</guid>
            <pubDate>Tue, 09 Feb 2021 12:33:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[José Valim Reveals “Project Nx” (Numerical Elixir) [audio]]]>
            </title>
            <description>
<![CDATA[
Score 337 | Comments 98 (<a href="https://news.ycombinator.com/item?id=26076680">thread link</a>) | @thibaut_barrere
<br/>
February 9, 2021 | https://thinkingelixir.com/podcast-episodes/034-jose-valim-reveals-project-nx/ | <a href="https://web.archive.org/web/*/https://thinkingelixir.com/podcast-episodes/034-jose-valim-reveals-project-nx/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text">
		<p>
José Valim visits and finally publicly reveals what Project Nx is! He and others have been working on it for 3 months and he’s finally ready to unveil it. José will speak more about it at the LambdaDays conference, demonstrating it with code and announcing the release and availability of the OpenSource code.</p>
<p>Nx stands for “Numerical Elixir”. The Nx project brings a unique numerical mode to Elixir along with GPU support. This important work lays the foundation for a powerful new era with data science in Elixir! Listen in as Jose discusses the details of how this works, how it came about, the goals of the project, what this means for the community, and what comes next.
</p>

<p>
  Show Notes online – <a href="https://thinkingelixir.com/podcast-episodes/034-jose-valim-reveals-project-nx">https://thinkingelixir.com/podcast-episodes/034-jose-valim-reveals-project-nx</a><br>
  
</p>
<p><strong>Elixir Community News</strong></p>

<ul>
<li><a href="https://elixirforum.com/t/introducing-elixirls-the-elixir-language-server/5857/122" target="_blank" rel="noopener noreferrer">https://elixirforum.com/t/introducing-elixirls-the-elixir-language-server/5857/122</a> – ElixirLS version 0.6.3 released</li>
<li><a href="https://github.com/elixir-lsp/elixir-ls/blob/master/CHANGELOG.md" target="_blank" rel="noopener noreferrer">https://github.com/elixir-lsp/elixir-ls/blob/master/CHANGELOG.md</a> – ElixirLS Changelog</li>
<li><a href="https://codesync.global/conferences/code-beam-v-america-2021/" target="_blank" rel="noopener noreferrer">https://codesync.global/conferences/code-beam-v-america-2021/</a> – CodeBEAM American conference dates March 10-12</li>
<li><a href="https://hexdocs.pm/oban/changelog.html#2-4-0-2021-01-26" target="_blank" rel="noopener noreferrer">https://hexdocs.pm/oban/changelog.html#2-4-0-2021-01-26</a> – Oban 2.4.0 released</li>
<li><a href="https://twitter.com/sorentwo/status/1354099475191652352" target="_blank" rel="noopener noreferrer">https://twitter.com/sorentwo/status/1354099475191652352</a> – Oban performance improvement charts</li>
<li><a href="https://github.com/wintermeyer/phx_tailwind_generators" target="_blank" rel="noopener noreferrer">https://github.com/wintermeyer/phx_tailwind_generators</a> – Phoenix TailwindCSS generator</li>
<li><a href="https://github.com/phoenixframework/phoenix/pull/4191" target="_blank" rel="noopener noreferrer">https://github.com/phoenixframework/phoenix/pull/4191</a> – Phoenix JavaScript gets more modern with Phoenix ES modules</li>
<li><a href="https://fullstackphoenix.com/boilerplates/new" target="_blank" rel="noopener noreferrer">https://fullstackphoenix.com/boilerplates/new</a> – Feedback! We learned about the site FullStackPhoenix which helps you start new projects already setup with several options for how to configure it.</li>
</ul>
<p>Do you know some Elixir news we don’t? Tell us at <a href="https://twitter.com/ThinkingElixir">@ThinkingElixir</a></p>
<p><strong>Discussion Resources</strong></p>

<ul>
<li>Nx stands for “Numerical Elixir”</li>
<li><a href="https://elixirforum.com/t/anyone-who-wants-to-speculate-about-this-tweet-from-jose/35772/68" target="_blank" rel="noopener noreferrer">https://elixirforum.com/t/anyone-who-wants-to-speculate-about-this-tweet-from-jose/35772/68</a> – A great text summary of the announcement found on ElixirForum.</li>
<li><a href="https://twitter.com/josevalim/status/1356880707474370560" target="_blank" rel="noopener noreferrer">https://twitter.com/josevalim/status/1356880707474370560</a> – José Valim released the new Nx logo during our recording. The cute animal is a Numbat!</li>
<li><a href="https://en.wikipedia.org/wiki/Numbat" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Numbat</a></li>
<li><a href="https://github.com/erlang/otp/pull/2890" target="_blank" rel="noopener noreferrer">https://github.com/erlang/otp/pull/2890</a> – José’s pull request to add 16-bit floats in bitstrings to OTP</li>
<li><a href="https://numpy.org/" target="_blank" rel="noopener noreferrer">https://numpy.org/</a></li>
<li><a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Softmax_function</a></li>
<li><a href="https://pragprog.com/titles/smgaelixir/genetic-algorithms-in-elixir/" target="_blank" rel="noopener noreferrer">https://pragprog.com/titles/smgaelixir/genetic-algorithms-in-elixir/</a></li>
<li><a href="https://elixirforum.com/t/pelemay-formerly-hastega-challenge-for-gpgpu-on-elixir/22986" target="_blank" rel="noopener noreferrer">https://elixirforum.com/t/pelemay-formerly-hastega-challenge-for-gpgpu-on-elixir/22986</a></li>
<li><a href="https://erlang.org/doc/man/erl_nif.html#dirty_nifs" target="_blank" rel="noopener noreferrer">https://erlang.org/doc/man/erl_nif.html#dirty_nifs</a> – Information on Dirty NIFs</li>
<li><a href="https://twitter.com/bcardarella" target="_blank" rel="noopener noreferrer">https://twitter.com/bcardarella</a></li>
<li><a href="https://www.tensorflow.org/xla" target="_blank" rel="noopener noreferrer">https://www.tensorflow.org/xla</a></li>
<li><a href="https://tvm.apache.org/" target="_blank" rel="noopener noreferrer">https://tvm.apache.org/</a></li>
<li><a href="https://mlir.llvm.org/" target="_blank" rel="noopener noreferrer">https://mlir.llvm.org/</a></li>
<li><a href="https://github.com/google/jax" target="_blank" rel="noopener noreferrer">https://github.com/google/jax</a></li>
<li><a href="https://julialang.org/" target="_blank" rel="noopener noreferrer">https://julialang.org/</a></li>
<li><a href="https://www.lambdadays.org/lambdadays2021/jose-valim" target="_blank" rel="noopener noreferrer">https://www.lambdadays.org/lambdadays2021/jose-valim</a> – LambdaDays speaker page</li>
<li><a href="https://www.lambdadays.org/lambdadays2021#programme" target="_blank" rel="noopener noreferrer">https://www.lambdadays.org/lambdadays2021#programme</a> – José speaks on day 2</li>
<li>Promo code for 15% off LambdaDays tickets! Just use “thinkingelixir15”</li>
<li><a href="https://twitter.com/josevalim" target="_blank" rel="noopener noreferrer">https://twitter.com/josevalim</a></li>
<li><a href="https://twitter.com/sean_moriarity" target="_blank" rel="noopener noreferrer">https://twitter.com/sean_moriarity</a></li>
<li><a href="https://twitter.com/elixirlang" target="_blank" rel="noopener noreferrer">https://twitter.com/elixirlang</a></li>
</ul>
<p><strong>Guest Information</strong></p>

<ul>
<li><a href="https://twitter.com/josevalim" target="_blank" rel="noopener noreferrer">https://twitter.com/josevalim</a> – on Twitter</li>
<li><a href="https://github.com/josevalim" target="_blank" rel="noopener noreferrer">https://github.com/josevalim</a> – on Github</li>
<li><a href="https://dashbit.co/" target="_blank" rel="noopener noreferrer">https://dashbit.co/</a> – Dashbit website and blog</li>
</ul>
<p><strong>Find us online</strong></p>
<ul>
<li>Message the show – <a href="https://twitter.com/ThinkingElixir" target="_blank" rel="noopener noreferrer">@ThinkingElixir</a></li>
<li>Mark Ericksen – <a href="https://twitter.com/brainlid" target="_blank" rel="noopener noreferrer">@brainlid</a></li>
<li>David Bernheisel – <a href="https://twitter.com/bernheisel" target="_blank" rel="noopener noreferrer">@bernheisel</a></li>
<li>Cade Ward – <a href="https://twitter.com/cadebward" target="_blank" rel="noopener noreferrer">@cadebward</a></li>
</ul>
<div itemscope="" itemtype="http://schema.org/AudioObject"><meta itemprop="name" content="#034 José Valim reveals Project Nx"><meta itemprop="uploadDate" content="2021-02-09T04:15:48-07:00"><meta itemprop="encodingFormat" content="audio/mpeg"><meta itemprop="duration" content="PT1H15M59S"><meta itemprop="description" content="José Valim visits and finally publicly reveals what Project Nx is! He and others have been working on it for 3 months and he's finally ready to unveil it. José will speak more about it at the LambdaDays conference, demonstrating it with code and announcing the release and availability of the OpenSource code. This is an exciting development that brings Elixir into areas it hasn't been used before. We also talk about what this means for Elixir and the community going forward. A must listen!



Show Notes online - https://thinkingelixir.com/podcast-episodes/034-jose-valim-reveals-project-nx"><meta itemprop="contentUrl" content="https://media.blubrry.com/thinkingelixir/s/thinkingelixir-podcast.s3.amazonaws.com/034-project-nx-with-jose-valim.mp3"><meta itemprop="contentSize" content="35.0"><div id="powerpress_player_5443"><!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
<p><audio id="audio-6278-1" preload="none" controls="controls"><source type="audio/mpeg" src="https://media.blubrry.com/thinkingelixir/p/thinkingelixir-podcast.s3.amazonaws.com/034-project-nx-with-jose-valim.mp3?_=1"><a href="https://media.blubrry.com/thinkingelixir/p/thinkingelixir-podcast.s3.amazonaws.com/034-project-nx-with-jose-valim.mp3">https://media.blubrry.com/thinkingelixir/p/thinkingelixir-podcast.s3.amazonaws.com/034-project-nx-with-jose-valim.mp3</a></audio></p></div></div><p>Podcast: <a href="https://media.blubrry.com/thinkingelixir/s/thinkingelixir-podcast.s3.amazonaws.com/034-project-nx-with-jose-valim.mp3" title="Download" rel="nofollow" download="034-project-nx-with-jose-valim.mp3">Download</a></p>	</div></div>]]>
            </description>
            <link>https://thinkingelixir.com/podcast-episodes/034-jose-valim-reveals-project-nx/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26076680</guid>
            <pubDate>Tue, 09 Feb 2021 12:26:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Objection to ORM Hatred]]>
            </title>
            <description>
<![CDATA[
Score 50 | Comments 87 (<a href="https://news.ycombinator.com/item?id=26076622">thread link</a>) | @dsego
<br/>
February 9, 2021 | https://www.jakso.me/blog/objection-to-orm-hatred | <a href="https://web.archive.org/web/*/https://www.jakso.me/blog/objection-to-orm-hatred">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-f2fc11fb4d616c1f00b5"><div><p>Whenever someone asks which <a href="https://github.com/sindresorhus/awesome-nodejs#database" target="_blank">Node.js ORM</a> they should use, one of the first answers is always some version of “<em>don’t use an ORM, just write SQL</em>”. This answer is then usually attacked with an opposite opinion along the lines of “<em>you need to use an ORM to hide all that nasty SQL</em>”. People always seem to ignore the third option: using <a href="https://vincit.github.io/objection.js/" target="_blank">an ORM that embraces SQL</a>!</p><p>I completely understand where the hatred comes from though. There are real problems in many ORMs, that <a href="https://medium.com/u/ac187d616e0b" target="_blank">Thomas Hunter II</a> summarizes well in his post <a href="https://blog.logrocket.com/why-you-should-avoid-orms-with-examples-in-node-js-e0baab73fa5" target="_blank">Why you should avoid ORMs</a>. The main points Thomas listed were</p><ol data-rte-list="default"><li><p>You learn the ORM, not SQL and that knowledge is usually not transferable to other tools.</p></li><li><p>Complex ORM calls are inefficient. ORMs have their own object-oriented query language which they try to convert to SQL, and it’s usually really difficult.</p></li><li><p>ORMs can’t do everything. The object oriented approach of most ORMs doesn’t map well to SQL. For many SQL operations, there is no object oriented equivalent.</p></li></ol><p>These problems arise from the fact that most ORMs are designed to abstract away SQL in favor of some object oriented interface. But not all ORMs are like that!</p><p>I created an ORM called <a href="https://vincit.github.io/objection.js/" target="_blank">objection.js</a> for exactly the reasons Thomas listed. The design goal of objection.js is to allow you to use SQL whenever possible and only provide a DSL (Domain Specific Language) or a custom concept when something cannot be easily done using SQL. I’ll get back to objection.js soon. First, I’ll introduce you to the most common argument for avoiding ORMs:</p><blockquote><p>It’s easier to write &lt;something&gt; in plain SQL than using an ORM</p></blockquote><p>This argument is usually followed by a contrived example like this:</p></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1554837408459_4953"><p>Yes, if you only need a flat list of items from one table at a time, by all means, don’t use an ORM! Of course you can use joins, subqueries and multiple queries to access related items in other tables, but that easily becomes tedious. For example, here’s a very basic query that joins a many-to-many relationship (children of a person) and a has-many relationship (pets of a person)</p></div><div data-block-type="2" id="block-yui_3_17_2_1_1554837408459_6074"><div><p>That’s already quite a bit of SQL. What if you want to join more relationships? How about nested relationships? You also need to repeat that every time you want to fetch the related objects. Not to mention the result of that query is a flat list of items and not a nested tree of objects.</p><p>The next thing any ORM hater would do is to use a library that converts that flat result list into a nicely nested object tree and then proceed to writing a bunch of helper functions for adding those joins to queries so that you don’t need to repeat yourself. You quickly realize that most of these helper functions look the same and only the table names and foreign keys are different. Then the obvious choice is to go ahead and define the relationships in a single place as objects and create a generic function that takes a relationship description object and outputs the needed joins.</p><p>Congratulations, you’ve just reinvented the ORM!</p><p>In my opinion, that’s all a good ORM is: a set of tools to make using SQL easier! It allows you to fully use all the features of SQL and the underlying database engine and only provides helpers and custom solutions to things that are difficult to achieve with just plain SQL.</p></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1554837408459_8837"><div><p>With objection.js, you always work with a<a href="https://vincit.github.io/objection.js/api/query-builder/#class-querybuilder" target="_blank"> query builder</a>. You can build any query and use as much SQL as you want but it also helps you to deal with the repetitive stuff like relations if you want it to. The <a href="https://vincit.github.io/objection.js/api/query-builder/join-methods.html#joinrelation" target="_blank">leftJoinRelation</a> method is given the names of the relations you want to join and objection.js uses the <a href="https://vincit.github.io/objection.js/guide/relations.html#examples" target="_blank">relation mappings</a> to create the joins. </p><p>Now you may be thinking, “that’s not much shorter and I still need to define the relationships somewhere”. Well yeah, but you only need to do that once. If your project is so simple that defining the relationships and models takes a significant part of your development time, it doesn’t matter whether you use an ORM or not.</p><p>The <a href="https://vincit.github.io/objection.js/api/query-builder/join-methods.html#joinrelation" target="_blank">joinRelation</a> query above is 100% equivalent to the previous SQL query and would still produce a flat list of result rows. You can easily get a tree of objects using the <a href="https://vincit.github.io/objection.js/guide/query-examples.html#eager-loading" target="_blank">joinEager</a> method:</p></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1554837408459_9918"><p>Loading nested relations is just as easy:</p></div><div data-block-type="2" id="block-yui_3_17_2_1_1554871633687_10767"><p>And in the next code block is the minimal SQL needed to carry out that query. The selects with aliases are needed to be able to build the object tree from the flat list.</p></div><div data-block-type="2" id="block-yui_3_17_2_1_1555046254723_11559"><p>For many ORMs, SQL concepts like subqueries are difficult or impossible to write. Since all objection.js operations return a <a href="https://vincit.github.io/objection.js/api/query-builder/" target="_blank">query builder</a>, writing <a href="https://vincit.github.io/objection.js/recipes/subqueries.html" target="_blank">subqueries</a> is just as easy as using plain SQL. For example, fetching all people that have at least one pet called ‘Fluffy’</p></div><div data-block-type="2" id="block-yui_3_17_2_1_1554837408459_14131"><p>But there’s also an easier way to write this in objection.js if you have defined the <code>pets</code> relationship for the <code>Person</code> model:</p></div><div data-block-type="2" id="block-yui_3_17_2_1_1554837408459_15231"><div><p>But that requires you to learn the ORM (which was one of the arguments against ORMs). You can still start by writing the SQL solution you are familiar with. Once you learn more about objection.js, you find helpers like this that improve your productivity.</p><p>Here’s another example of using the <a href="https://vincit.github.io/objection.js/api/model/static-methods.html#static-relatedquery" target="_blank">relatedQuery</a> helper</p></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1554876097081_10902"><div><p>If your application is simple and you rarely need to deal with nested data and relations, using plain SQL or a query builder like <a href="https://knexjs.org/" target="_blank">knex</a> can be a good option. However, in most applications you want to work with nested data and a lot of relations and an ORM can be a helpful tool. With objection.js, you don’t need to make a compromise. You get the flexibility of a query builder and the relational power of an ORM in the same package.</p><p>If you found any of this interesting, take a closer look at <a href="https://vincit.github.io/objection.js/" target="_blank">objection.js.</a></p><p>Some links to get you started</p><ul data-rte-list="default"><li><p><a href="https://vincit.github.io/objection.js/guide/installation.html" target="_blank">Getting started</a></p></li><li><p><a href="https://vincit.github.io/objection.js/guide/query-examples.html#eager-loading" target="_blank">Eager loading</a></p></li><li><p><a href="https://vincit.github.io/objection.js/guide/query-examples.html#graph-inserts" target="_blank">Graph inserts</a></p></li><li><p><a href="https://github.com/Vincit/objection.js/issues/1069" target="_blank">Who is using objection</a></p></li><li><p><a href="https://vincit.github.io/objection.js/guide/query-examples.html" target="_blank">Query examples</a></p></li><li><p><a href="https://vincit.github.io/objection.js/recipes/relation-subqueries.html" target="_blank">Relation subqueries</a></p></li></ul></div></div></div>]]>
            </description>
            <link>https://www.jakso.me/blog/objection-to-orm-hatred</link>
            <guid isPermaLink="false">hacker-news-small-sites-26076622</guid>
            <pubDate>Tue, 09 Feb 2021 12:18:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pi from High School Maths]]>
            </title>
            <description>
<![CDATA[
Score 56 | Comments 18 (<a href="https://news.ycombinator.com/item?id=26076128">thread link</a>) | @vonadz
<br/>
February 9, 2021 | https://theartofmachinery.com/2020/10/26/pi_from_high_school_maths.html | <a href="https://web.archive.org/web/*/https://theartofmachinery.com/2020/10/26/pi_from_high_school_maths.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
<p>Warning: I don’t think the stuff in this post has any direct practical application by itself (unless you’re a
nuclear war survivor and need to reconstruct maths from scratch or something). Sometimes I like to go back to basics,
though. Here’s a look at <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
π
</mi>
</mrow>
<annotation encoding="application/x-tex">
\pi
</annotation>
</semantics></math> and areas of curved shapes without any calculus or transcendental functions.</p><h2 id="a-simple-algorithm-for-calculating-pi">A simple algorithm for calculating <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
π
</mi>
</mrow>
<annotation encoding="application/x-tex">
\pi
</annotation>
</semantics></math></h2>
<p>This algorithm starts with simple number theoretic musing. Some whole numbers form neat Pythagorean triples
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mo stretchy="false">
(
</mo>
<mi>
x
</mi>
<mo>
,
</mo>
<mi>
y
</mi>
<mo>
,
</mo>
<mi>
z
</mi>
<mo stretchy="false">
)
</mo>
</mrow>
<annotation encoding="application/x-tex">
(x, y, z)
</annotation>
</semantics></math> where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<msup>
<mi>
x
</mi>
<mn>
2
</mn>
</msup>
<mo>
+
</mo>
<msup>
<mi>
y
</mi>
<mn>
2
</mn>
</msup>
<mo>
=
</mo>
<msup>
<mi>
z
</mi>
<mn>
2
</mn>
</msup>
</mrow>
<annotation encoding="application/x-tex">
x^2 + y^2 = z^2
</annotation>
</semantics></math>. E.g., <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<msup>
<mn>
3
</mn>
<mn>
2
</mn>
</msup>
<mo>
+
</mo>
<msup>
<mn>
4
</mn>
<mn>
2
</mn>
</msup>
<mo>
=
</mo>
<msup>
<mn>
5
</mn>
<mn>
2
</mn>
</msup>
</mrow>
<annotation encoding="application/x-tex">
3^2 + 4^2 = 5^2
</annotation>
</semantics></math>. It’s easy to find all the solutions to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<msup>
<mi>
x
</mi>
<mn>
2
</mn>
</msup>
<mo>
+
</mo>
<msup>
<mi>
y
</mi>
<mn>
2
</mn>
</msup>
<mo>
=
</mo>
<msup>
<mn>
5
</mn>
<mn>
2
</mn>
</msup>
</mrow>
<annotation encoding="application/x-tex">
x^2 + y^2 = 5^2
</annotation>
</semantics></math> through brute-force search because we know that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
x
</mi>
</mrow>
<annotation encoding="application/x-tex">
x
</annotation>
</semantics></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
y
</mi>
</mrow>
<annotation encoding="application/x-tex">
y
</annotation>
</semantics></math> can’t be bigger than <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
5
</mn>
</mrow>
<annotation encoding="application/x-tex">
5
</annotation>
</semantics></math>. Here they are:</p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
<semantics>
<mrow>
<mrow>
<mtable displaystyle="true" columnalign="right left right left right left right left right left" columnspacing="0em">
<mtr>
<mtd>
<msup>
<mn>
0
</mn>
<mn>
2
</mn>
</msup>
<mo>
+
</mo>
<msup>
<mn>
5
</mn>
<mn>
2
</mn>
</msup>
</mtd>
<mtd>
<mo>
=
</mo>
<msup>
<mn>
5
</mn>
<mn>
2
</mn>
</msup>
</mtd>
</mtr>
<mtr>
<mtd>
<msup>
<mn>
3
</mn>
<mn>
2
</mn>
</msup>
<mo>
+
</mo>
<msup>
<mn>
4
</mn>
<mn>
2
</mn>
</msup>
</mtd>
<mtd>
<mo>
=
</mo>
<msup>
<mn>
5
</mn>
<mn>
2
</mn>
</msup>
</mtd>
</mtr>
<mtr>
<mtd>
<msup>
<mn>
4
</mn>
<mn>
2
</mn>
</msup>
<mo>
+
</mo>
<msup>
<mn>
3
</mn>
<mn>
2
</mn>
</msup>
</mtd>
<mtd>
<mo>
=
</mo>
<msup>
<mn>
5
</mn>
<mn>
2
</mn>
</msup>
</mtd>
</mtr>
<mtr>
<mtd>
<msup>
<mn>
5
</mn>
<mn>
2
</mn>
</msup>
<mo>
+
</mo>
<msup>
<mn>
0
</mn>
<mn>
2
</mn>
</msup>
</mtd>
<mtd>
<mo>
=
</mo>
<msup>
<mn>
5
</mn>
<mn>
2
</mn>
</msup>
</mtd>
</mtr>
</mtable>
</mrow>
</mrow>
<annotation encoding="application/x-tex">
\begin{aligned} 0^2 + 5^2 &amp;= 5^2 \\ 3^2 + 4^2 &amp;= 5^2 \\ 4^2 + 3^2 &amp;= 5^2 \\ 5^2 + 0^2 &amp;= 5^2 \end{aligned}
</annotation>
</semantics></math>
<p>(Plus all the negative-number combinations, but let’s stick with non-negative integers and just count 4 solutions.)
If we relax the equation, and count solutions to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<msup>
<mi>
x
</mi>
<mn>
2
</mn>
</msup>
<mo>
+
</mo>
<msup>
<mi>
y
</mi>
<mn>
2
</mn>
</msup>
<mo>
≤
</mo>
<msup>
<mn>
5
</mn>
<mn>
2
</mn>
</msup>
</mrow>
<annotation encoding="application/x-tex">
x^2 + y^2 \le 5^2
</annotation>
</semantics></math>, the answer turns out to be 26. Why care? Well, if <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
t
</mi>
</mrow>
<annotation encoding="application/x-tex">
t
</annotation>
</semantics></math> is the total number of solutions to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<msup>
<mi>
x
</mi>
<mn>
2
</mn>
</msup>
<mo>
+
</mo>
<msup>
<mi>
y
</mi>
<mn>
2
</mn>
</msup>
<mo>
≤
</mo>
<msup>
<mi>
n
</mi>
<mn>
2
</mn>
</msup>
</mrow>
<annotation encoding="application/x-tex">
x^2 + y^2 \le n^2
</annotation>
</semantics></math>, then</p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
<semantics>
<mrow>
<munder>
<mi>
lim
</mi>
<mrow>
<mi>
n
</mi>
<mo>
→
</mo>
<mn>
∞
</mn>
</mrow>
</munder>
<mfrac>
<mrow>
<mn>
4
</mn>
<mi>
t
</mi>
</mrow>
<mrow>
<mo stretchy="false">
(
</mo>
<mi>
n
</mi>
<mo>
+
</mo>
<mn>
1
</mn>
<msup>
<mo stretchy="false">
)
</mo>
<mn>
2
</mn>
</msup>
</mrow>
</mfrac>
<mo>
=
</mo>
<mi>
π
</mi>
</mrow>
<annotation encoding="application/x-tex">
\lim_{n \to \infinity} \frac{4t}{(n+1)^2} = \pi
</annotation>
</semantics></math>
<p>Or, in code, here’s a simple program that estimates <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
π
</mi>
</mrow>
<annotation encoding="application/x-tex">
\pi
</annotation>
</semantics></math>, getting more accurate for bigger values of the <code>n</code> variable:</p>
<figure>
<pre><code data-lang="d"><span>import</span> <span>std</span><span>;</span>

<span>ulong</span> <span>sq</span><span>(</span><span>ulong</span> <span>x</span><span>)</span> <span>pure</span>
<span>{</span>
    <span>return</span> <span>x</span> <span>*</span> <span>x</span><span>;</span>
<span>}</span>

<span>void</span> <span>main</span><span>(</span><span>string</span><span>[]</span> <span>args</span><span>)</span>
<span>{</span>
    <span>const</span> <span>n</span> <span>=</span> <span>args</span><span>.</span><span>length</span> <span>&gt;</span> <span>1</span> <span>?</span> <span>args</span><span>[</span><span>1</span><span>].</span><span>to</span><span>!</span><span>ulong</span> <span>:</span> <span>20</span><span>;</span>

    <span>ulong</span> <span>total</span><span>;</span>
    <span>foreach</span> <span>(</span><span>x</span><span>;</span> <span>0.</span><span>.</span><span>n</span><span>+</span><span>1</span><span>)</span>
    <span>{</span>
        <span>foreach</span> <span>(</span><span>y</span><span>;</span> <span>0.</span><span>.</span><span>n</span><span>+</span><span>1</span><span>)</span>
        <span>{</span>
            <span>if</span> <span>(</span><span>sq</span><span>(</span><span>x</span><span>)</span> <span>+</span> <span>sq</span><span>(</span><span>y</span><span>)</span> <span>&lt;=</span> <span>sq</span><span>(</span><span>n</span><span>))</span> <span>total</span><span>++;</span>
        <span>}</span>
    <span>}</span>

    <span>/*
    // Alternatively, for functional programming fans:
    const total =
        cartesianProduct(iota(n+1), iota(n+1))
                .count!(p =&gt; sq(p[0]) + sq(p[1]) &lt;= sq(n));
    */</span>

    <span>writef</span><span>(</span><span>"%.12f\n"</span><span>,</span> <span>4.0</span> <span>*</span> <span>total</span> <span>/</span> <span>sq</span><span>(</span><span>n</span><span>+</span><span>1</span><span>));</span>
<span>}</span></code></pre>
</figure>
<figure>
<pre><code data-lang="shell">
$ ./pi_calc 
3.038548752834
$ ./pi_calc 10000
3.141362256135
</code></pre>
</figure>
<p>Okay, that’s a little bit more accurate than <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mfrac>
<mn>
22
</mn>
<mn>
7
</mn>
</mfrac>
</mrow>
<annotation encoding="application/x-tex">
\frac{22}{7}
</annotation>
</semantics></math>. Unlike most formulae for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
π
</mi>
</mrow>
<annotation encoding="application/x-tex">
\pi
</annotation>
</semantics></math>, though, there’s a simple diagram that shows how it works. Imagine we lay out the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mo stretchy="false">
(
</mo>
<mi>
x
</mi>
<mo>
,
</mo>
<mi>
y
</mi>
<mo stretchy="false">
)
</mo>
</mrow>
<annotation encoding="application/x-tex">
(x, y)
</annotation>
</semantics></math> integer pairs (where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
x
</mi>
</mrow>
<annotation encoding="application/x-tex">
x
</annotation>
</semantics></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
y
</mi>
</mrow>
<annotation encoding="application/x-tex">
y
</annotation>
</semantics></math> range from <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
0
</mn>
</mrow>
<annotation encoding="application/x-tex">
0
</annotation>
</semantics></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
n
</mi>
</mrow>
<annotation encoding="application/x-tex">
n
</annotation>
</semantics></math>) on a 2D grid the obvious way. The figure below shows an example for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
n
</mi>
<mo>
=
</mo>
<mn>
10
</mn>
</mrow>
<annotation encoding="application/x-tex">
n=10
</annotation>
</semantics></math>, with the arrow <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
r
</mi>
</mrow>
<annotation encoding="application/x-tex">
r
</annotation>
</semantics></math> pointing from the origin to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mo stretchy="false">
(
</mo>
<mn>
6
</mn>
<mo>
,
</mo>
<mn>
8
</mn>
<mo stretchy="false">
)
</mo>
</mrow>
<annotation encoding="application/x-tex">
(6, 8)
</annotation>
</semantics></math>. <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
r
</mi>
</mrow>
<annotation encoding="application/x-tex">
r
</annotation>
</semantics></math> and the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
x
</mi>
</mrow>
<annotation encoding="application/x-tex">
x
</annotation>
</semantics></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
y
</mi>
</mrow>
<annotation encoding="application/x-tex">
y
</annotation>
</semantics></math> components make a right-angled triangle, so <a href="https://www.cut-the-knot.org/pythagoras/">Pythagoras’s theorem</a> says that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<msup>
<mi>
x
</mi>
<mn>
2
</mn>
</msup>
<mo>
+
</mo>
<msup>
<mi>
y
</mi>
<mn>
2
</mn>
</msup>
<mo>
=
</mo>
<msup>
<mi>
r
</mi>
<mn>
2
</mn>
</msup>
</mrow>
<annotation encoding="application/x-tex">
x^2 + y^2 = r^2
</annotation>
</semantics></math>. For <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mo stretchy="false">
(
</mo>
<mn>
6
</mn>
<mo>
,
</mo>
<mn>
8
</mn>
<mo stretchy="false">
)
</mo>
</mrow>
<annotation encoding="application/x-tex">
(6, 8)
</annotation>
</semantics></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
r
</mi>
<mo>
=
</mo>
<mn>
10
</mn>
<mo>
=
</mo>
<mi>
n
</mi>
</mrow>
<annotation encoding="application/x-tex">
r = 10 = n
</annotation>
</semantics></math>, so <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mo stretchy="false">
(
</mo>
<mn>
6
</mn>
<mo>
,
</mo>
<mn>
8
</mn>
<mo stretchy="false">
)
</mo>
</mrow>
<annotation encoding="application/x-tex">
(6, 8)
</annotation>
</semantics></math> is on the boundary as a solution to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<msup>
<mi>
x
</mi>
<mn>
2
</mn>
</msup>
<mo>
+
</mo>
<msup>
<mi>
y
</mi>
<mn>
2
</mn>
</msup>
<mo>
≤
</mo>
<msup>
<mn>
10
</mn>
<mn>
2
</mn>
</msup>
</mrow>
<annotation encoding="application/x-tex">
x^2 + y^2 \le 10^2
</annotation>
</semantics></math>. That boundary (the set of real-valued points a constant distance <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
n
</mi>
<mo>
=
</mo>
<mn>
10
</mn>
</mrow>
<annotation encoding="application/x-tex">
n=10
</annotation>
</semantics></math> from the origin) makes a quarter circle.</p>
<figure role="img">
<a href="https://theartofmachinery.com/images/pi_from_high_school_maths/pi_calc_grid.svg"><img src="https://theartofmachinery.com/images/pi_from_high_school_maths/pi_calc_grid.svg" alt="Grid for calculating an estimate of pi"></a>
</figure>
<p>A circle is a simple, convex shape, and the grid points are evenly spaced, so the number of points inside the
quarter circle will be roughly proportional to the area. More specifically, the fraction of all the grid points inside
the quarter circle will be roughly the area of the quarter circle divided by the area of square around all points. The
quarter circle area is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
π
</mi>
<msup>
<mi>
r
</mi>
<mn>
2
</mn>
</msup>
<mo>
÷
</mo>
<mn>
4
</mn>
</mrow>
<annotation encoding="application/x-tex">
\pi r^2 \div 4
</annotation>
</semantics></math>, inside the square of area <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<msup>
<mi>
r
</mi>
<mn>
2
</mn>
</msup>
</mrow>
<annotation encoding="application/x-tex">
r^2
</annotation>
</semantics></math> (remember, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
n
</mi>
<mo>
=
</mo>
<mi>
r
</mi>
</mrow>
<annotation encoding="application/x-tex">
n = r
</annotation>
</semantics></math>), so <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mfrac>
<mi>
π
</mi>
<mn>
4
</mn>
</mfrac>
</mrow>
<annotation encoding="application/x-tex">
\frac{\pi}{4}
</annotation>
</semantics></math> of all points represent solutions. The <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
x
</mi>
</mrow>
<annotation encoding="application/x-tex">
x
</annotation>
</semantics></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
y
</mi>
</mrow>
<annotation encoding="application/x-tex">
y
</annotation>
</semantics></math> values count from <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
0
</mn>
</mrow>
<annotation encoding="application/x-tex">
0
</annotation>
</semantics></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
n
</mi>
</mrow>
<annotation encoding="application/x-tex">
n
</annotation>
</semantics></math>, so there are <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mo stretchy="false">
(
</mo>
<mi>
n
</mi>
<mo>
+
</mo>
<mn>
1
</mn>
<msup>
<mo stretchy="false">
)
</mo>
<mn>
2
</mn>
</msup>
</mrow>
<annotation encoding="application/x-tex">
(n+1)^2
</annotation>
</semantics></math> grid points. Rearrange the equations and you get a formula for estimating <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
π
</mi>
</mrow>
<annotation encoding="application/x-tex">
\pi
</annotation>
</semantics></math> from a solution count. The grid points keep drawing an arbitrarily more accurate circle as
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
n
</mi>
</mrow>
<annotation encoding="application/x-tex">
n
</annotation>
</semantics></math> gets bigger (just like a higher-resolution computer monitor does) so the estimate is exact in the
limit.</p>
<h2 id="a-faster-implementation">A faster implementation</h2>
<p>The code above is simple but slow because it brute-force scans over all <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mo stretchy="false">
(
</mo>
<mi>
n
</mi>
<mo>
+
</mo>
<mn>
1
</mn>
<mo stretchy="false">
)
</mo>
<mo>
×
</mo>
<mo stretchy="false">
(
</mo>
<mi>
n
</mi>
<mo>
+
</mo>
<mn>
1
</mn>
<mo stretchy="false">
)
</mo>
</mrow>
<annotation encoding="application/x-tex">
(n+1) \times (n+1)
</annotation>
</semantics></math>possible <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
x
</mi>
</mrow>
<annotation encoding="application/x-tex">
x
</annotation>
</semantics></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
y
</mi>
</mrow>
<annotation encoding="application/x-tex">
y
</annotation>
</semantics></math> values. But we obviously don’t need to scan <em>all</em> values. If we know that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<msup>
<mi>
x
</mi>
<mn>
2
</mn>
</msup>
<mo>
+
</mo>
<msup>
<mi>
y
</mi>
<mn>
2
</mn>
</msup>
<mo>
≤
</mo>
<msup>
<mi>
n
</mi>
<mn>
2
</mn>
</msup>
</mrow>
<annotation encoding="application/x-tex">
x^2 + y^2 \le n^2
</annotation>
</semantics></math>, then making <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
x
</mi>
</mrow>
<annotation encoding="application/x-tex">
x
</annotation>
</semantics></math> or <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
y
</mi>
</mrow>
<annotation encoding="application/x-tex">
y
</annotation>
</semantics></math> smaller will only give us another solution. We don’t need to keep testing smaller values after we
find a solution. Ultimately, we only need to find the integral points around the boundary. Here’s a faster algorithm
based on that idea.</p>
<p>Imagine we scan along the integral <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
x
</mi>
</mrow>
<annotation encoding="application/x-tex">
x
</annotation>
</semantics></math> values and find the maximum integral <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
y
</mi>
</mrow>
<annotation encoding="application/x-tex">
y
</annotation>
</semantics></math> value that still gives us a solution. This gives us a border line marked in red in the figure
below. If <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
y
</mi>
<mo>
=
</mo>
<mn>
8
</mn>
</mrow>
<annotation encoding="application/x-tex">
y=8
</annotation>
</semantics></math> for a given <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
x
</mi>
</mrow>
<annotation encoding="application/x-tex">
x
</annotation>
</semantics></math> value, we instantly know there are <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
8
</mn>
<mo>
+
</mo>
<mn>
1
</mn>
<mo>
=
</mo>
<mn>
9
</mn>
</mrow>
<annotation encoding="application/x-tex">
8 + 1 = 9
</annotation>
</semantics></math> solutions with that given <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
x
</mi>
</mrow>
<annotation encoding="application/x-tex">
x
</annotation>
</semantics></math> value (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mo lspace="0.11111em" rspace="0em">
+
</mo>
<mn>
1
</mn>
</mrow>
<annotation encoding="application/x-tex">
+ 1
</annotation>
</semantics></math> to count the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
y
</mi>
<mo>
=
</mo>
<mn>
0
</mn>
</mrow>
<annotation encoding="application/x-tex">
y=0
</annotation>
</semantics></math> solution).</p>
<figure role="img">
<a href="https://theartofmachinery.com/images/pi_from_high_school_maths/pi_fast_calc_grid.svg"><img src="https://theartofmachinery.com/images/pi_from_high_school_maths/pi_fast_calc_grid.svg" alt="Estimating pi efficiently by tracing circle boundary"></a>
</figure>
<p>Note that as <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
x
</mi>
</mrow>
<annotation encoding="application/x-tex">
x
</annotation>
</semantics></math> scans from <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
0
</mn>
</mrow>
<annotation encoding="application/x-tex">
0
</annotation>
</semantics></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
n
</mi>
</mrow>
<annotation encoding="application/x-tex">
n
</annotation>
</semantics></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
y
</mi>
</mrow>
<annotation encoding="application/x-tex">
y
</annotation>
</semantics></math> starts at <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
n
</mi>
</mrow>
<annotation encoding="application/x-tex">
n
</annotation>
</semantics></math> and decreases to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
0
</mn>
</mrow>
<annotation encoding="application/x-tex">
0
</annotation>
</semantics></math>. Importantly, it <em>only</em> decreases — it’s monotonic. So if we scan <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
x
</mi>
</mrow>
<annotation encoding="application/x-tex">
x
</annotation>
</semantics></math> from <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
0
</mn>
</mrow>
<annotation encoding="application/x-tex">
0
</annotation>
</semantics></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
n
</mi>
</mrow>
<annotation encoding="application/x-tex">
n
</annotation>
</semantics></math>, we can find the next boundary <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
y
</mi>
</mrow>
<annotation encoding="application/x-tex">
y
</annotation>
</semantics></math> point by starting from the previous boundary point and searching downwards. Here’s some code:</p>
<figure>
<pre><code data-lang="d"><span>ulong</span> <span>y</span> <span>=</span> <span>n</span><span>,</span> <span>total</span><span>;</span>
<span>foreach</span> <span>(</span><span>x</span><span>;</span> <span>0.</span><span>.</span><span>n</span><span>+</span><span>1</span><span>)</span>
<span>{</span>
    <span>while</span> <span>(</span><span>sq</span><span>(</span><span>x</span><span>)</span> <span>+</span> <span>sq</span><span>(</span><span>y</span><span>)</span> <span>&gt;</span> <span>sq</span><span>(</span><span>n</span><span>))</span> <span>y</span><span>--;</span>
    <span>total</span> <span>+=</span> <span>y</span> <span>+</span> <span>1</span><span>;</span>
<span>}</span></code></pre>
</figure>
<p>This version still has nested loops, so it might look like it’s still <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
O
</mi>
<mo stretchy="false">
(
</mo>
<msup>
<mi>
n
</mi>
<mn>
2
</mn>
</msup>
<mo stretchy="false">
)
</mo>
</mrow>
<annotation encoding="application/x-tex">
O(n^2)
</annotation>
</semantics></math>. However, the inner <code>while</code> loop executes a
varying number of times for each <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
x
</mi>
</mrow>
<annotation encoding="application/x-tex">
x
</annotation>
</semantics></math> value. Often the <code>y--</code> doesn’t trigger at
all. In fact, because <code>y</code> starts from <code>n</code> and monotonically decreases to 0, we know the <code>y--</code> will be executed exactly <code>n</code> times in total. There’s no instruction in that code that executes more
than <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
O
</mi>
<mo stretchy="false">
(
</mo>
<mi>
n
</mi>
<mo stretchy="false">
)
</mo>
</mrow>
<annotation encoding="application/x-tex">
O(n)
</annotation>
</semantics></math> times, total, so the whole algorithm is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
O
</mi>
<mo stretchy="false">
(
</mo>
<mi>
n
</mi>
<mo stretchy="false">
)
</mo>
</mrow>
<annotation encoding="application/x-tex">
O(n)
</annotation>
</semantics></math>.</p>
<p>With 64b <code>ulong</code> integers, the largest value of <code>n</code> that works before overflow is 4294967294:</p>
<figure>
<pre><code data-lang="shell">
$ ./pi_calc 4294967294
3.141592653058
</code></pre>
</figure>
<p>There are ways to get faster convergence using numerical integration tricks, but I like the way this algorithm only
uses integer arithmetic (up until the final division), and can be understood directly from simple diagrams.</p>
<h2 id="area-of-a-circle-without-calculus">Area of a circle without calculus</h2>
<p>Perhaps you feel a bit cheated because that algorithm assumes the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
π
</mi>
<msup>
<mi>
r
</mi>
<mn>
2
</mn>
</msup>
</mrow>
<annotation encoding="application/x-tex">
\pi r^2
</annotation>
</semantics></math> formula for the area of a circle. Sure, that’s arguably included in “high school maths”, but it’s
something students just get told to remember, unless they study integral calculus and derive it that way. But if we’re
going to assume <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
π
</mi>
<msup>
<mi>
r
</mi>
<mn>
2
</mn>
</msup>
</mrow>
<annotation encoding="application/x-tex">
\pi r^2
</annotation>
</semantics></math>, why not assume the theory of trigonometric functions as well, and just use <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
π
</mi>
<mo>
=
</mo>
<mn>
4
</mn>
<mo lspace="0em" rspace="0.16667em">
atan
</mo>
<mo stretchy="false">
(
</mo>
<mn>
1
</mn>
<mo stretchy="false">
)
</mo>
</mrow>
<annotation encoding="application/x-tex">
\pi = 4\atan(1)
</annotation>
</semantics></math>?</p>
<p>The great ancient Greek mathematician Archimedes figured out the circle area over two thousand years ago without
integral calculus (or trigonometric functions for that matter). He started with an elegant insight about regular (i.e.,
equal-sided) polygons.</p>
<p>The famous <a href="https://mathcs.clarku.edu/~djoyce/java/elements/bookI/propI37.html">“half base times height”
formula for the area of a triangle already had a well-known proof in the first book of Euclid’s Elements of
Geometry</a> (easily derived from <a href="https://mathcs.clarku.edu/~djoyce/java/elements/bookI/propI35.html">a
theorem about parallelograms</a>). Conveniently, any regular polygon can be split into equal triangles joined to the
centre. For example, a regular hexagon splits into six triangles, as in the figure below. We can take any one of the
triangles (they’re all the same) and call the “base” the side that’s also a side of the polygon. Then the “height” is
the line from the centre of the base to the centre of the polygon.</p>
<figure role="img">
<a href="https://theartofmachinery.com/images/pi_from_high_school_maths/polygon.svg"><img src="https://theartofmachinery.com/images/pi_from_high_school_maths/polygon.svg" alt="Explanation of perimeter-to-area ratio of regular polygons"></a>
</figure>
<p>Now here’s Archimedes’s neat insight: The ratio of the triangle area to the base is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mfrac>
<mi>
h
</mi>
<mn>
2
</mn>
</mfrac>
</mrow>
<annotation encoding="application/x-tex">
\frac{h}{2}
</annotation>
</semantics></math>. If you add up all the areas, you get the area of the polygon. Likewise, if you add up all the
bases, you get the perimeter of the polygon. Because the triangle area/base ratio is a constant <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mfrac>
<mi>
h
</mi>
<mn>
2
</mn>
</mfrac>
</mrow>
<annotation encoding="application/x-tex">
\frac{h}{2}
</annotation>
</semantics></math> for all triangles, the area/perimeter ratio for the whole polygon is the same <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mfrac>
<mi>
h
</mi>
<mn>
2
</mn>
</mfrac>
</mrow>
<annotation encoding="application/x-tex">
\frac{h}{2}
</annotation>
</semantics></math>. As a formula, the area of <em>any</em> regular polygon is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
P
</mi>
<mo>
×
</mo>
<mfrac>
<mi>
h
</mi>
<mn>
2
</mn>
</mfrac>
</mrow>
<annotation encoding="application/x-tex">
P \times \frac{h}{2}
</annotation>
</semantics></math> (where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
P
</mi>
</mrow>
<annotation encoding="application/x-tex">
P
</annotation>
</semantics></math> is the perimeter).</p>
<p>If you think of a circle as a regular polygon with infinitely many sides (so that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
h
</mi>
</mrow>
<annotation encoding="application/x-tex">
h
</annotation>
</semantics></math> becomes the radius of the circle), and use the circle circumference (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
2
</mn>
<mi>
π
</mi>
<mi>
r
</mi>
</mrow>
<annotation encoding="application/x-tex">
2\pi r
</annotation>
</semantics></math>) as your basic definition of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
π
</mi>
</mrow>
<annotation encoding="application/x-tex">
\pi
</annotation>
</semantics></math>, then that implies the area of a circle is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
2
</mn>
<mi>
π
</mi>
<mi>
r
</mi>
<mo>
×
</mo>
<mfrac>
<mi>
r
</mi>
<mn>
2
</mn>
</mfrac>
<mo>
=
</mo>
<mi>
π
</mi>
<msup>
<mi>
r
</mi>
<mn>
2
</mn>
</msup>
</mrow>
<annotation encoding="application/x-tex">
2\pi r \times \frac{r}{2} = \pi r^2
</annotation>
</semantics></math>.</p>
<p>Of course, Archimedes was a respected mathematician who couldn’t get away with just assuming that anything true of a
polygon is true of a circle (counterexample: all polygons have bumpy corners, but circles don’t). He used the kind of
geometric proof by contradiction that was popular in his day. (He even took it further and analysed spheres, cylinders,
parabolas and other curved objects, almost inventing something like modern real analysis a couple of millenia early.)
Sadly, not all of his mathemetical work has survived, but <a href="https://flashman.neocities.org/ARCHCI1set.htm">the
key part of his Measurement of a Circle</a> has.</p>
<p>Here’s the high-level version. Archimedes claimed that the area of a circle is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
2
</mn>
<mi>
π
</mi>
<mi>
r
</mi>
<mo>
×
</mo>
<mfrac>
<mi>
r
</mi>
<mn>
2
</mn>
</mfrac>
</mrow>
<annotation encoding="application/x-tex">
2\pi r \times \frac{r}{2}
</annotation>
</semantics></math>. Suppose you think his value is too small, and the circle is really bigger than <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
2
</mn>
<mi>
π
</mi>
<mi>
r
</mi>
<mo>
×
</mo>
<mfrac>
<mi>
r
</mi>
<mn>
2
</mn>
</mfrac>
</mrow>
<annotation encoding="application/x-tex">
2\pi r \times \frac{r}{2}
</annotation>
</semantics></math>. That means there’s enough room inside the circle to fit a regular polygon that’s also bigger than
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
2
</mn>
<mi>
π
</mi>
<mi>
r
</mi>
<mo>
×
</mo>
<mfrac>
<mi>
r
</mi>
<mn>
2
</mn>
</mfrac>
</mrow>
<annotation encoding="application/x-tex">
2\pi r \times \frac{r}{2}
</annotation>
</semantics></math>. But Archimedes said that’s contradictory because for any such polygon, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
h
</mi>
<mo>
&lt;
</mo>
<mi>
r
</mi>
</mrow>
<annotation encoding="application/x-tex">
h \lt r
</annotation>
</semantics></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
P
</mi>
<mo>
&lt;
</mo>
<mn>
2
</mn>
<mi>
π
</mi>
<mi>
r
</mi>
</mrow>
<annotation encoding="application/x-tex">
P \lt 2\pi r
</annotation>
</semantics></math> (because each side of the polygon is a straight line that’s shorter than the circle arc that
connects the same points), so the area <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
A
</mi>
<mo>
=
</mo>
<mi>
P
</mi>
<mo>
×
</mo>
<mfrac>
<mi>
h
</mi>
<mn>
2
</mn>
</mfrac>
<mo>
&lt;
</mo>
<mn>
2
</mn>
<mi>
π
</mi>
<mi>
r
</mi>
<mo>
×
</mo>
<mfrac>
<mi>
r
</mi>
<mn>
2
</mn>
</mfrac>
</mrow>
<annotation encoding="application/x-tex">
A = P \times \frac{h}{2} \lt 2\pi r \times \frac{r}{2}
</annotation>
</semantics></math>. The polygon’s area can’t be both bigger and smaller than <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
2
</mn>
<mi>
π
</mi>
<mi>
r
</mi>
<mo>
×
</mo>
<mfrac>
<mi>
r
</mi>
<mn>
2
</mn>
</mfrac>
</mrow>
<annotation encoding="application/x-tex">
2\pi r \times \frac{r}{2}
</annotation>
</semantics></math>.</p>
<figure role="img">
<a href="https://theartofmachinery.com/images/pi_from_high_school_maths/polygon_inner.svg"><img src="https://theartofmachinery.com/images/pi_from_high_school_maths/polygon_inner.svg" alt="A regular polygon inside a circle"></a>
</figure>
<p>Archimedes argued that there’s a similar contradiction if you think <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
2
</mn>
<mi>
π
</mi>
<mi>
r
</mi>
<mo>
×
</mo>
<mfrac>
<mi>
r
</mi>
<mn>
2
</mn>
</mfrac>
</mrow>
<annotation encoding="application/x-tex">
2\pi r \times \frac{r}{2}
</annotation>
</semantics></math> is too big, and the circle area is smaller than that. In that case he could make a polygon that’s
also smaller than <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
2
</mn>
<mi>
π
</mi>
<mi>
r
</mi>
<mo>
×
</mo>
<mfrac>
<mi>
r
</mi>
<mn>
2
</mn>
</mfrac>
</mrow>
<annotation encoding="application/x-tex">
2\pi r \times \frac{r}{2}
</annotation>
</semantics></math>, yet still wraps around the circle. For this polygon, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
h
</mi>
<mo>
=
</mo>
<mi>
r
</mi>
</mrow>
<annotation encoding="application/x-tex">
h = r
</annotation>
</semantics></math>, but he said the perimeter of the polygon must be greater than <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
2
</mn>
<mi>
π
</mi>
<mi>
r
</mi>
</mrow>
<annotation encoding="application/x-tex">
2\pi r
</annotation>
</semantics></math><sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup>, so that the
polygon’s area must be bigger than <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mn>
2
</mn>
<mi>
π
</mi>
<mi>
r
</mi>
<mo>
×
</mo>
<mfrac>
<mi>
r
</mi>
<mn>
2
</mn>
</mfrac>
</mrow>
<annotation encoding="application/x-tex">
2\pi r \times \frac{r}{2}
</annotation>
</semantics></math>, even though it’s also meant to be smaller.</p>
<figure role="img">
<a href="https://theartofmachinery.com/images/pi_from_high_school_maths/polygon_outer.svg"><img src="https://theartofmachinery.com/images/pi_from_high_school_maths/polygon_outer.svg" alt="A regular polygon around a circle"></a>
</figure>
<p>If both of those cases lead to contradiction, we’re left with the only alternative that the circle area is
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
<semantics>
<mrow>
<mi>
π
</mi>
<msup>
<mi>
r
</mi>
<mn>
2
</mn>
</msup>
</mrow>
<annotation encoding="application/x-tex">
\pi r^2
</annotation>
</semantics></math>.</p>

</div></div>]]>
            </description>
            <link>https://theartofmachinery.com/2020/10/26/pi_from_high_school_maths.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26076128</guid>
            <pubDate>Tue, 09 Feb 2021 11:11:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Compiler Class]]>
            </title>
            <description>
<![CDATA[
Score 126 | Comments 48 (<a href="https://news.ycombinator.com/item?id=26075930">thread link</a>) | @ingve
<br/>
February 9, 2021 | https://norswap.com/compilers/ | <a href="https://web.archive.org/web/*/https://norswap.com/compilers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    
<p>In 2021, I'm teaching a master-level compiler class at <a href="https://uclouvain.be/">Université catholique de
Louvain</a>.</p>
<p>All the course materials are made available online, for anyone interested to
peruse. I'm also happy to <a href="mailto:norswap+compiler+q@gmail.com">answer</a> your questions.</p>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLOech0kWpH8-njQpmSNGSiQBPUvl8v3IM">Youtube Playlist</a></li>
<li><a href="https://drive.google.com/drive/folders/1cMgLvEiaWsyfip8wJXXilVhvM2XDrS-6">Slides</a></li>
</ul>
<p>The course's project is to create your own programming language. A few libraries
are supplied to assist in this task:</p>
<ul>
<li><a href="https://github.com/norswap/autumn">Autumn</a> (parsing)</li>
<li><a href="https://github.com/norswap/uranium/">Uranium</a> (semantic analysis)</li>
</ul>
  </div></div>]]>
            </description>
            <link>https://norswap.com/compilers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26075930</guid>
            <pubDate>Tue, 09 Feb 2021 10:43:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Securing my personal SSH infrastructure with Yubikeys]]>
            </title>
            <description>
<![CDATA[
Score 41 | Comments 23 (<a href="https://news.ycombinator.com/item?id=26075386">thread link</a>) | @ingve
<br/>
February 9, 2021 | https://www.dzombak.com/blog/2021/02/Securing-my-personal-SSH-infrastructure-with-Yubikeys.html | <a href="https://web.archive.org/web/*/https://www.dzombak.com/blog/2021/02/Securing-my-personal-SSH-infrastructure-with-Yubikeys.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
			<header>
				
				
				<time pubdate="" datetime="2021-02-08T19:08:13-05:00">
					<span>February</span>
					<span>08</span>
					<span>2021</span>
				</time>
				
			</header>
			
			<p>One recently-completed project I mentioned in January’s <a href="https://www.dzombak.com/blog/2021/01/Now-Jan-2021.html">“Now” post</a> was locking down SSH in my personal computing infrastructure using Yubikeys. In this post, I’ll outline my goals, the strategy I took, and the problems and solutions I ran into along the way.</p>

<h2>Goals &amp; Strategy</h2>

<p>Historically, I’ve used a pretty basic SSH setup for my personal projects: my user account on every laptop/desktop/server had its own key in <code>~/.ssh</code>, and I’d try to keep the <a href="https://www.ssh.com/ssh/authorized_keys/"><code>authorized_keys</code></a> lists on all my servers more-or-less up-to-date. This presents a number of obvious security problems.</p>

<p>I wanted to ensure that, should an attacker gain access to one of my servers, they can’t use that access to move onto any other computer I control. To do this, I moved to using a few <a href="https://www.yubico.com/">Yubikeys</a> to store my SSH keys; there’s no longer key material stored on any server for a hypothetical attacker to steal. The Yubikeys require a PIN, so this is an example of two-factor authentication: something I physically have, and something I know.</p>

<p><a href="https://www.cloudsavvyit.com/25/what-is-ssh-agent-forwarding-and-how-do-you-use-it/">SSH agent forwarding</a> is used to allow me to SSH from one server to another or fetch code from GitHub on a remote server. With <a href="https://github.com/FiloSottile/yubikey-agent"><code>yubikey-agent</code></a>, my preferred agent software, every single SSH operation — yes, even those performed via agent forwarding — requires a physical touch to confirm.</p>

<p>I use a private Git repository to synchronize SSH configuration (including <code>authorized_keys</code>, the list of public keys corresponding to my Yubikeys) between machines, with a modular local configuration system allowing me to quickly enable commonly-used SSH configuration blocks which only apply to a subset of my machines.</p>

<h2>Implementation</h2>

<h3>Yubikeys</h3>

<p>I found it easiest by far to use <a href="https://github.com/FiloSottile/yubikey-agent"><code>yubikey-agent</code></a> for this project. It’s pretty straightforward to set this up; the real work was figuring out how to smooth out the various difficulties I encountered later.</p>

<p>(The commands and configuration changes under this heading apply to client machines with attached Yubikeys.)</p>

<p>Install <code>yubikey-agent</code> with <a href="https://brew.sh/">Homebrew</a>:</p>

<pre><code>brew tap filippo.io/yubikey-agent https://filippo.io/yubikey-agent
brew install yubikey-agent
brew services start yubikey-agent
</code></pre>

<p>Run <code>yubikey-agent -setup</code> to generate a new SSH key on your Yubikey.</p>

<p>Set the <code>SSH_AUTH_SOCK</code> environment variable (do this in your <code>.bashrc</code> or <code>.zshrc</code>). In <a href="https://github.com/cdzombak/dotfiles/blob/800ca4e760da36171643e8a116e038cc4c4dcf31/zsh/.zshrc#L24">my dotfiles</a>, I first check that <code>yubikey-agent</code> is installed, then proceed:</p>

<pre><code>command -v yubikey-agent &gt;/dev/null 2&gt;&amp;1 &amp;&amp; export SSH_AUTH_SOCK="/usr/local/var/run/yubikey-agent.sock"
</code></pre>

<p>Enable SSH agent forwarding for a specific, trusted host (don’t enable it for all hosts; that’s a potential security issue) by adding <code>ForwardAgent yes</code> to that host’s block in your <code>~/.ssh/config</code>. A complete example might look like:</p>

<pre><code>Host dzombak.com
    User chris
    HostName dzombak.com
    ForwardAgent yes
</code></pre>

<p>This is a bit of a spoiler for a topic later in this blog post, but we’ll also add this to our <code>~/.ssh/config</code>:</p>

<pre><code>Host *
    IdentityAgent /usr/local/var/run/yubikey-agent.sock
</code></pre>

<p>This allows apps started from outside your terminal — like the GUI Git client, <a href="https://fork.dev/">Fork.app</a> — to find and use <code>yubikey-agent</code>.</p>

<h4>A note: Secretive.app</h4>

<p>I’d like to use the new macOS app <a href="https://github.com/maxgoedjen/secretive">Secretive</a>, which stores SSH keys in the Secure Enclave on newer MacBooks and requires Touch ID to authenticate. Unfortunately, for Reasons™ I’m still using macOS Mojave, and Secretive requires Catalina or Big Sur. I plan to move to Big Sur soon enough, since I want to get an M1 MacBook Pro when the 16” models are released, so I’ll be able to try Secretive soon enough. (Worth noting, this changes the security model somewhat, as the second factor is biometric rather than a PIN, but it’s still two factors.)</p>

<h3>Server Configuration</h3>

<p>Of course, moving to Yubikeys doesn’t solve much if your servers still allow password logins. On every server, in <code>/etc/ssh/sshd_config</code>, I set the following. Make this change only after you’ve set up a Yubikey and added it to <code>authorized_keys</code> for your user account on the server!</p>

<pre><code>ChallengeResponseAuthentication no
PasswordAuthentication no
PermitRootLogin no
</code></pre>

<p>(That last line — <code>PermitRootLogin no</code> — ensures that logins as <code>root</code> via SSH are never allowed, which is a good SSH best practice unrelated to Yubikeys.)</p>

<p><a href="https://www.cyberciti.biz/faq/howto-restart-ssh/">Restart the SSH service</a>, and immediately — before logging out — open a new terminal window and test that you can still login to the server with your Yubikey.</p>

<p>macOS tends to lose changes to <code>sshd_config</code> during OS upgrades, so after installing macOS updates I make sure to check that my SSH server configuration is intact.</p>

<h3>Git repo for SSH configuration</h3>

<p>I keep my <code>~/.ssh</code> directory, with a few important exceptions (keys are never committed!), in a private Git repo. This allows me to sync configuration and <code>authorized_keys</code> changes between systems easily. I’ve created a stripped-down version at <a href="https://github.com/cdzombak/ssh-example">cdzombak/ssh-example</a> which you can use as a basis for your own setup.</p>

<p>I’ll walk through the highlights here:</p>

<ul>
<li><a href="https://github.com/cdzombak/ssh-example/blob/main/README.md"><code>README.md</code></a> covers initial installation &amp; setup.</li>
<li><a href="https://github.com/cdzombak/ssh-example/blob/main/authorized_keys"><code>authorized_keys</code></a> is where you’ll add the public keys associated with the new, private SSH keys on your Yubikeys.</li>
<li><a href="https://github.com/cdzombak/ssh-example/blob/main/config"><code>config</code></a> is where you’ll add <code>Host</code> blocks for your servers. At the top, it sets some SSH best practices that I’ve accumulated over the years.</li>
<li><a href="https://github.com/cdzombak/ssh-example/blob/main/fix-permissions.sh"><code>fix-permissions.sh</code></a> ensures that <code>~/.ssh/authorized_keys</code> and <code>~/.ssh/rc</code> have the correct permissions. I run this after pulling from the repository; if it results in any changes the files’ permissions in the repo should be corrected.</li>
<li><a href="https://github.com/cdzombak/ssh-example/blob/main/rc"><code>rc</code></a> runs after I log into a machine via SSH. In this case, it updates a symlink in <code>~/.ssh/sock</code> to point to the new SSH agent socket. See the “Long-lived <code>screen</code> sessions” section, below, for an explanation on why this is necessary.</li>
<li><a href="https://github.com/cdzombak/ssh-example/tree/main/config.templates"><code>config.templates/</code></a> contains SSH configuration blocks which <em>can</em> be included on a given machine, but shouldn’t be included everywhere. The most important of these are <a href="https://github.com/cdzombak/ssh-example/blob/main/config.templates/yubikey-agent"><code>yubikey-agent</code></a>, which when enabled sets <code>IdentityAgent</code> to the <code>yubikey-agent</code> socket as described above; and <a href="https://github.com/cdzombak/ssh-example/blob/main/config.templates/homedir-ssh-auth-sock"><code>homedir-ssh-auth-sock</code></a>, which sets <code>IdentityAgent</code> to the symlink <code>rc</code> creates after an SSH login. On any given machine, I enable one and only one of these two configurations, depending whether it’s a client machine with Yubikey attached or a server which will rely on agent forwarding for any outgoing SSH connections.</li>
<li><a href="https://github.com/cdzombak/ssh-example/tree/main/config.local"><code>config.local/</code></a> is <a href="https://github.com/cdzombak/ssh-example/blob/main/config#L2">included by</a> <code>config</code> and ignored by Git. I can add symlinks from here to <code>config.templates</code> to enable a specific SSH configuration block on the machine.</li>
</ul>


<p>Those last two — <code>config.local</code> and <code>config.templates</code> — are important, because that’s how I achieve variations in SSH configuration between different machines. <a href="https://github.com/cdzombak/ssh-example/blob/main/README.md#configuration">The README covers</a> how to enable config templates on a given computer.</p>

<h2>Challenges &amp; Solutions</h2>

<h3>Long-lived <code>screen</code> sessions</h3>

<p>I use GNU <code>screen</code> (yes, still; I haven’t bothered to learn <code>tmux</code>) as a terminal multiplexer and to provide persistence between SSH sessions. This was a problem for SSH agent forwarding: when I first SSH in and start a <code>screen</code> session, the <code>SSH_AUTH_SOCK</code> environment variable would be set. But when I logged in from somewhere else and reattached to the <code>screen</code> session, the <code>SSH_AUTH_SOCK</code> environment variable wouldn’t get updated, so SSH agent forwarding was broken until I started a new <code>screen</code> session.</p>

<p><a href="https://gist.github.com/martijnvermaat/8070533">This Gist</a> helped me fix this situation. There are a few parts to this solution. (The commands and configuration changes under this heading apply to servers you’ll SSH into.)</p>

<p>First, we have to have a location for our SSH agent socket that doesn’t change between logins. These few lines in <a href="https://github.com/cdzombak/ssh-example/blob/main/rc#L5-L7"><code>~/.ssh/rc</code></a> achieve this:</p>

<pre><code>if test "$SSH_AUTH_SOCK" ; then
    ln -sf "$SSH_AUTH_SOCK" ~/.ssh/sock/ssh_auth_sock
fi
</code></pre>

<p>Great! Then we just need clients to use this new, always-updated socket. To do this, we configure <a href="https://github.com/cdzombak/dotfiles/blob/800ca4e760da36171643e8a116e038cc4c4dcf31/screen/.screenrc#L31-L32"><code>~/.screenrc</code></a> to set the environment variable <code>SSH_AUTH_SOCK</code>:</p>

<pre><code>setenv SSH_AUTH_SOCK $HOME/.ssh/sock/ssh_auth_sock
</code></pre>

<p>Finally, we’ll also want to include <code>IdentityAgent ~/.ssh/sock/ssh_auth_sock</code> in our SSH configuration. We can do this by including the <code>homedir-ssh-auth-sock</code> configuration block within my <a href="https://github.com/cdzombak/ssh-example">modular SSH configuration</a> setup:</p>

<pre><code>cd ~/.ssh/config.local
ln -s ../config.templates/homedir-ssh-auth-sock .
</code></pre>

<h3>SSH agent forwarding when running commands under sudo</h3>

<p>When running a command with <code>sudo</code>, you’re working in a new environment; your user’s environment variables are not preserved. This will, of course, break SSH agent forwarding.</p>

<p>To solve this, we want to preserve the <code>SSH_AUTH_SOCK</code> environment variable when using <code>sudo</code>. (The commands and configuration changes under this heading apply to servers you’ll SSH into.)</p>

<p>Run <code>visudo</code> (as <code>root</code>) to edit your <code>sudoers</code> file, and add:</p>

<pre><code>Defaults&gt;root    env_keep+=SSH_AUTH_SOCK
</code></pre>

<p>This means that when using <code>sudo</code> to run a command as <code>root</code> (not as any other user), your <code>SSH_AUTH_SOCK</code> variable remains intact, and agent forwarding works as expected.</p>

<h3>SSH agent forwarding via <code>SSH_AUTH_SOCK</code> doesn’t work with GUI macOS apps</h3>

<p><a href="https://github.com/ForkIssues/Tracker/issues/44">This issue in the Fork app’s issue tracker</a> was really helpful here. <code>.bashrc</code> and <code>.zshrc</code> don’t apply to GUI apps (unless you launch them from the terminal), so setting <code>SSH_AUTH_SOCK</code> <em>only</em> in shell configuration files won’t work.</p>

<p>This is why we need <code>IdentityAgent /usr/local/var/run/yubikey-agent.sock</code> in our SSH configuration. To enable this within my <a href="https://github.com/cdzombak/ssh-example">modular SSH configuration</a> setup:</p>

<pre><code>cd ~/.ssh/config.local
ln -s ../config.templates/yubikey-agent .
</code></pre>

<p>(This change applies to macOS client machines with attached Yubikeys.)</p>

<h3>Avoiding repeated mystery Yubikey prompts (using HTTPS for GitHub and Bitbucket repositories)</h3>

<p>After I set this up, my Yubikey would periodically blink as if I were trying to SSH into something, but I hadn’t tried to do anything with SSH! That was worrying, until I realized it was just <a href="https://fork.dev/">Fork</a> trying to update repository info in the background.</p>

<p>I decided to configure Git on my laptops to use HTTPS instead of SSH when communicating with GitHub and Bitbucket, so Fork can work in the background as it desires.</p>

<p>To do this, we’ll add the following to <code>~/.gitconfig</code>. (This …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.dzombak.com/blog/2021/02/Securing-my-personal-SSH-infrastructure-with-Yubikeys.html">https://www.dzombak.com/blog/2021/02/Securing-my-personal-SSH-infrastructure-with-Yubikeys.html</a></em></p>]]>
            </description>
            <link>https://www.dzombak.com/blog/2021/02/Securing-my-personal-SSH-infrastructure-with-Yubikeys.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26075386</guid>
            <pubDate>Tue, 09 Feb 2021 09:11:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Beginner's Guide to NFTs]]>
            </title>
            <description>
<![CDATA[
Score 81 | Comments 47 (<a href="https://news.ycombinator.com/item?id=26073970">thread link</a>) | @donohoe
<br/>
February 8, 2021 | https://linda.mirror.xyz/df649d61efb92c910464a4e74ae213c4cab150b9cbcc4b7fb6090fc77881a95d | <a href="https://web.archive.org/web/*/https://linda.mirror.xyz/df649d61efb92c910464a4e74ae213c4cab150b9cbcc4b7fb6090fc77881a95d">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><h3>What are NFTs?</h3>
<p>Non-fungible token (NFT) is a term used to describe a unique digital asset whose ownership is tracked on a blockchain, such as Ethereum. Assets that can be represented as NFTs range from digital goods, such as items that exist within virtual worlds, to claims on physical assets such as clothing items or real estate. In the coming years, we will see NFTs used to unlock entirely new use cases that are only made possible by crypto.</p>
<figure><p><img src="https://images.mirror-media.xyz/publication-images/9799dae1-c2c9-4712-813b-10b23c279a36.jpeg" alt="Tokens redeemable for real world goods on Zora"></p><figcaption>Tokens redeemable for real world goods on Zora</figcaption></figure>
<p>While the Ethereum ecosystem is where most NFT activity has taken place to date, NFTs can exist on other smart contract platforms too. This is because, at their core, NFTs are just digital abstractions used to represent assets that are one of a kind. Non-fungible token isn’t the most intuitive term since we don’t commonly refer to the fungibility of objects in the physical world, but this is an important technical distinction when it comes to how an asset is represented on a blockchain. The goal for this post is not to detail every project within the NFT space, but to give a high level overview of what NFTs are, why they are interesting, and showcase some of their potential use cases.</p>
<h3>Why are NFTs interesting?</h3>
<p>NFTs are powerful because, when combined with other financial building blocks on Ethereum, they allow anyone to issue, own, and trade them. This makes interacting with NFTs significantly more efficient than in traditional platforms. The same reason why cryptocurrency used in payments is more efficient than traditional payments, that it is borderless and significantly easier to transfer, applies to NFTs. For example, if you want to create tradable in-game items as a game developer, then you can instantly have them be tradable through protocols that allow for decentralized exchange of NFTs. You don’t have to create your own marketplace or go through the onboarding process of a centralized platform in order to have the items be tradable.</p>
<p>NFT activity can go well past trading and include actions like being able to borrow and lend, support fractional ownership (e.g. <a href="https://www.niftex.com/">NIFTEX</a>), or use it as collateral in taking out a loan (e.g. <a href="https://nftfi.com/">NFTfi</a>). The possibilities are endless when you have the ability to combine NFTs with DeFi building blocks. For example, the game <a href="http://aavegotchi.com/">Aavegotchi</a> combines DeFi and NFT gaming where each Aavegotchi character represents a user’s collateral that is deposited within the lending platform <a href="https://aave.com/">Aave</a>, but you can also battle the characters, level them up, and equip wearables that change their traits.</p>
<figure><p><img src="https://images.mirror-media.xyz/publication-images/a1259686-6b61-455a-a204-f6e6da648f10.jpeg" alt="Aavegotchi gaming attributes change based on actions taken in DeFi"></p><figcaption>Aavegotchi gaming attributes change based on actions taken in DeFi</figcaption></figure>
<p>NFTs can cover an extremely wide variety of areas given they are simply digital representations of ownership but there has been significant growth within art and gaming in particular. Note that many digital works of art and gaming items are a subset of a larger category of NFT collectibles. There is also the emerging space of social tokens, which sometimes falls into the NFT category or that is closely related to it.</p>
<h3>Art</h3>
<p>NFTs can make fractionalized ownership more accessible, so if there is a valuable item that otherwise wouldn’t have been accessible for someone to own, now they can own a piece of it. Custody of a physical item still requires a trustworthy custodian, but being able to issue, hold, and trade it as a cryptoasset unlocks more use cases. One can also craft an NFT such that the creator receives a percentage of all secondary sales in a completely automated way. Artists typically don’t receive a cut of secondary sales in the traditional art world.</p>
<p>Programmable art is another interesting concept where a piece of artwork can incorporate on-chain data to dynamically update certain features or characteristics of the work. For example, one could create a piece of programmable art whose background changes if the price of ether goes above a certain dollar-value. There are countless creative possibilities.</p>
<figure><p><img src="https://images.mirror-media.xyz/publication-images/a51670c0-4a1c-4aaf-917d-0e551855b56d.jpeg" alt="Rutger van der Tas artwork that changes based on if it’s day or night"></p><figcaption>Rutger van der Tas artwork that changes based on if it’s day or night</figcaption></figure>
<p><a href="https://async.art/">Async Art</a> is a digital art marketplace known for programmable art. Many artists on Async sell artwork where someone is able to own the “master” copy which consists of a number of individual layers, but other people can own the individual layers and adjust their attributes over time. Imagine groups of people being able to own art collectively, where members of the group manage attributes of the work.</p>
<figure><p><img src="https://images.mirror-media.xyz/publication-images/fea07c75-8b3c-4592-b8f2-f5d6d92e544c.jpeg" alt="Osinachi artwork that includes a master and layers that can be individually adjusted"></p><figcaption>Osinachi artwork that includes a master and layers that can be individually adjusted</figcaption></figure>
<p>A common question about digital artwork is what can you do with it? These works can be displayed in digital frames in a physical setting for people to enjoy. The digital artist Beeple sold physical tokens along with his digital NFTs and made <a href="https://niftygateway.com/itemdetail/primary/0x6e5dc5405baefb8c0166bcc78d2692777f2cbffb/21">$3.5 million</a> from sales in an auction on the Nifty Gateway marketplace.</p>
<figure><p><img src="https://images.mirror-media.xyz/publication-images/8689be31-b33f-46ac-94aa-feea3a1b3dd1.jpeg" alt="Artist Beeple’s physical token that came with the NFT sale"></p><figcaption>Artist Beeple’s physical token that came with the NFT sale</figcaption></figure>
<p>Digital artwork can also be displayed in online collections like a <a href="https://superrare.co/thevault/collection">SuperRare profile</a> as well as in virtual worlds. There are a number of art galleries within <a href="http://cryptovoxels.com/">Cryptovoxels</a>, a virtual world where users can buy and sell land parcels as NFTs. As virtual reality spaces become more popular, having digital art to display will become more common. This wouldn’t be much different than someone spending money on video game items to customize a character’s appearance, already a multi-billion dollar industry.</p>
<figure><p><img src="https://images.mirror-media.xyz/publication-images/7386d358-d43d-4934-9ea5-b51c33932365.jpeg" alt="Digital art gallery in Cryptovoxels"></p><figcaption>Digital art gallery in Cryptovoxels</figcaption></figure>
<p>A common skepticism is that someone can just take a screenshot of the image or get a digital file so it’s not really scarce. However, the same argument could apply to physical items as well. Anyone can take a photo of the Mona Lisa or create a replica of it, but it isn’t the real item from the artist. People are willing to pay a premium for the original work. Another interesting aspect of digital art or collectibles is that you can easily verify the item’s ownership history. Some digital items might be worth more depending on who owned it in the past.</p>
<p>With NFTs, you can also prove that the item is real and tamper proof. This is an issue in the physical collectibles space. For example, a <a href="https://en.wikipedia.org/wiki/T206_Honus_Wagner">T206 Honus Wagner baseball card</a> was sold to Wayne Gretzky for $451k and sold again for several million dollars. One of the sellers of the card later admitted in court to trimming the card’s edges to make it look better. You can ensure NFT supply doesn’t change and there’s no counterfeit or continued printing. For example, there are many <a href="https://www.youtube.com/watch?v=AAvLC3fz068&amp;">counterfeit Black Lotus</a> cards in the popular game Magic: The Gathering. Common verification methods include bending the card to make sure it doesn’t crease or going through centralized grading services whose rating significantly affects the card's value.</p>
<h3>Gaming</h3>
<p>Steam, a popular video game platform, has a <a href="https://steamcommunity.com/market/?">Community Market</a> where in-game items can be bought and sold. Steam’s marketplace is centralized and collects a transaction fee of 5% for each item from the buyer. Games like Team Fortress 2 and Dota 2 take an additional fee of 10% for their items sold.</p>
<figure><p><img src="https://images.mirror-media.xyz/publication-images/4ad85fcf-b986-4b60-9fb5-925200fde3a9.jpeg" alt="Steam Community Market"></p><figcaption>Steam Community Market</figcaption></figure>
<p>Steam also restricts user wallet balances to $2,000 and the price of a single item to $1,800. While these cases are outliers, many in-game items can actually sell for significantly larger amounts such as the <a href="https://www.engadget.com/2013-11-06-dota-2-pink-war-dog-courier-sells-for-38-000.html">DotA 2 pink war dog courier</a> for $38,000. Within the crypto space there have also been high priced sales such as the $170,000 <a href="https://thenextweb.com/hardfork/2018/09/05/most-expensive-cryptokitty/">CryptoKitty</a>. Similarly Magic: The Gathering’s coveted <a href="https://www.polygon.com/2019/3/5/18251623/magic-the-gathering-black-lotus-auction-price">Black Lotus</a> card sold for $166,100. There is certainly demand for valuable in-game items. In decentralized marketplaces, there isn’t a limit imposed on what game items can be sold and for what amount.</p>
<p>Decentralized marketplaces can significantly reduce transaction fees due to the improved efficiency of starting up and operating a marketplace. Marketplaces can also improve overall user experience and increase interest in the game. Hearthstone is a wildly popular digital collectible card game created by Blizzard Entertainment that had over <a href="https://variety.com/2018/gaming/news/hearthstone-has-over-100-million-players-1203019919/">100 million players</a> in 2018. Hearthstone opted to not have a marketplace for their cards which leaves opportunity for other digital collectible card games that allow for an open marketplace. <a href="https://godsunchained.com/">Gods Unchained</a> and <a href="https://www.skyweaver.net/">SkyWeaver</a> are two trading card games (TCGs) that have cards that are freely tradable. You are also able to earn cards as you level up in the game. Players that did not purchase packs of cards will still be able to improve upon the game’s default decks. It’s an exciting feeling knowing that the earned cards have real world value and can be sold or traded for other cards. Gods Unchained also lets users earn tokens for when they win a match or refer their friend. The tokens themselves unlock rare in-game items and are also freely tradable.</p>
<figure><p><img src="https://images.mirror-media.xyz/publication-images/aa82c586-39b6-46b1-ae55-cb8a2e44c8e1.jpeg" alt="Gods Unchained"></p><figcaption>Gods Unchained</figcaption></figure>
<p>One of the most popular games in crypto is <a href="https://axieinfinity.com/">Axie Infinity</a>. You battle with a team of pet Axies and level them up. There's no aspect of the gameplay that feels like a blockchain game but there is the added benefit of being able to openly trade the Axies on NFT marketplaces such as <a href="https://opensea.io/">OpenSea</a>. The game has become popular in the <a href="https://www.coindesk.com/nft-game-filipinos-covid">Philippines</a> and earning tokens from the game has become a viable source of income for players even <a href="https://www.playtoearn.online/2020/07/08/axie-infinity-beats-minimum-wage-in-many-countries/">beating minimum wage</a> in many countries. There is also a decentralized autonomous organization (DAO) called <a href="https://yieldguild.io/">Yield Guild Games</a> which leases Axies to players that want to get started with the game but don’t have the funds to purchase them. Several rare Axies were even <a href="https://www.delphidigital.io/reports/why-we-spent-159k-on-digital-battle-pets-2/">purchased</a> for $159,000 total.</p>
<figure><p><img src="https://images.mirror-media.xyz/publication-images/26353bcc-0d05-47ea-89ac-fcdc9d2dd848.jpeg" alt="Axie Infinity battle"></p><figcaption>Axie Infinity battle</figcaption></figure>
<p>There’s another popular game <a href="https://zkga.me/">Dark Forest</a> where you can explore different planets and collect Artifact NFTs on new planets, which give bonuses when attached to the planet. Being able to discover and collect these items or even win them in a battle and then trade them in decentralized marketplaces can make the game even more fun.</p>
<figure><p><img src="https://images.mirror-media.xyz/publication-images/be66b61a-05f0-46b7-aa8f-f7ed542d5cd3.jpeg" alt="Planet in Dark Forest which has an Artifact NFT"></p><figcaption>Planet in Dark Forest which has an Artifact NFT</figcaption></figure>
<p>One concern with making in-game items tradable is that the ability to trade items can negatively affect the gameplay as people are focused on …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://linda.mirror.xyz/df649d61efb92c910464a4e74ae213c4cab150b9cbcc4b7fb6090fc77881a95d">https://linda.mirror.xyz/df649d61efb92c910464a4e74ae213c4cab150b9cbcc4b7fb6090fc77881a95d</a></em></p>]]>
            </description>
            <link>https://linda.mirror.xyz/df649d61efb92c910464a4e74ae213c4cab150b9cbcc4b7fb6090fc77881a95d</link>
            <guid isPermaLink="false">hacker-news-small-sites-26073970</guid>
            <pubDate>Tue, 09 Feb 2021 04:42:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Turning an old Amazon Kindle into a eInk development platform]]>
            </title>
            <description>
<![CDATA[
Score 362 | Comments 87 (<a href="https://news.ycombinator.com/item?id=26073463">thread link</a>) | @miles
<br/>
February 8, 2021 | https://blog.lidskialf.net/2021/02/08/turning-an-old-kindle-into-a-eink-development-platform/ | <a href="https://web.archive.org/web/*/https://blog.lidskialf.net/2021/02/08/turning-an-old-kindle-into-a-eink-development-platform/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

	<section id="primary">
		<main id="main">

			
<article id="post-1721">

	

	
			<figure>
				<img width="998" height="1331" src="https://adq454703481.files.wordpress.com/2021/02/kindle_serial_magnified.jpg?w=998" alt="" loading="lazy" srcset="https://adq454703481.files.wordpress.com/2021/02/kindle_serial_magnified.jpg 998w, https://adq454703481.files.wordpress.com/2021/02/kindle_serial_magnified.jpg?w=112 112w, https://adq454703481.files.wordpress.com/2021/02/kindle_serial_magnified.jpg?w=225 225w, https://adq454703481.files.wordpress.com/2021/02/kindle_serial_magnified.jpg?w=768 768w" sizes="(max-width: 998px) 100vw, 998px" data-attachment-id="1751" data-permalink="https://blog.lidskialf.net/kindle_serial_magnified/" data-orig-file="https://adq454703481.files.wordpress.com/2021/02/kindle_serial_magnified.jpg" data-orig-size="998,1331" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="kindle_serial_magnified" data-image-description="" data-medium-file="https://adq454703481.files.wordpress.com/2021/02/kindle_serial_magnified.jpg?w=225" data-large-file="https://adq454703481.files.wordpress.com/2021/02/kindle_serial_magnified.jpg?w=768">			</figure><!-- .post-thumbnail -->

		
	<div>
		
<p>I fancied getting an eink screen to use for future projects. I bought a wee one with a raspberry pi “hat” attached. However, I realised later that I could maybe just re-purpose an old Amazon Kindle ebook reader.</p>



<p>I’ve messed with Kindles before, ages ago: I ported an <a href="https://blog.lidskialf.net/2010/10/09/kif-an-infocom-text-adventure-interpreter-for-the-kindle/">Infocom interpreter</a> and a <a href="https://blog.lidskialf.net/2010/10/20/mangle-a-better-manga-reader-for-the-kindle/">Manga</a> reader to it. I managed to get Amazon’s own software to load them as “Kindlets” and show them integrated into their ebook reader. However, now I just want a nice cheap Linux based eink development platform.</p>



<p>Cheap Kindle From Ebay (and why)</p>



<p>So, off to ebay I went! I saw a number of really cheap ones marked “BLOCKED BY AMAZON”; I decided not to go for these since theoretically they might have been stolen. In the end, I went for £7 Kindle 4 “non-touch”.</p>



<p>A few days later, it turned up. And I discovered <em>why</em> it might have been so cheap: its stuck in some sort of unquittable demo mode:</p>



<figure><img data-attachment-id="1725" data-permalink="https://blog.lidskialf.net/kindle_demo_mode/" data-orig-file="https://adq454703481.files.wordpress.com/2021/02/kindle_demo_mode.jpg" data-orig-size="892,1239" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="kindle_demo_mode" data-image-description="" data-medium-file="https://adq454703481.files.wordpress.com/2021/02/kindle_demo_mode.jpg?w=216" data-large-file="https://adq454703481.files.wordpress.com/2021/02/kindle_demo_mode.jpg?w=737" src="https://adq454703481.files.wordpress.com/2021/02/kindle_demo_mode.jpg?w=737" alt="" srcset="https://adq454703481.files.wordpress.com/2021/02/kindle_demo_mode.jpg?w=737 737w, https://adq454703481.files.wordpress.com/2021/02/kindle_demo_mode.jpg?w=108 108w, https://adq454703481.files.wordpress.com/2021/02/kindle_demo_mode.jpg?w=216 216w, https://adq454703481.files.wordpress.com/2021/02/kindle_demo_mode.jpg?w=768 768w, https://adq454703481.files.wordpress.com/2021/02/kindle_demo_mode.jpg 892w" sizes="(max-width: 737px) 100vw, 737px"></figure>



<p>I did some googling and although it seems later Kindle versions can be un-demo-mode-ed, nothing seemed to work for this version. I don’t actually care though; I don’t want to run the original Kindle ebook software on this.</p>



<p>So, the next step is to gain access. Browsing the <a href="https://www.mobileread.com/forums/showthread.php?t=166687">mobileread forums</a> showed it has a debug serial port: time to open the case!</p>



<p>Physical Access Granted!</p>



<p>This was kinda tricky! There are multiple clips all round it and the case is glued onto the battery compartment, so judicious application of a Big Knife was required to persuade it to let go. I cleaned the glue up with some Acetone.</p>



<figure><img data-attachment-id="1729" data-permalink="https://blog.lidskialf.net/kindle_illustrated/" data-orig-file="https://adq454703481.files.wordpress.com/2021/02/kindle_illustrated.jpg" data-orig-size="998,1331" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="kindle_illustrated" data-image-description="" data-medium-file="https://adq454703481.files.wordpress.com/2021/02/kindle_illustrated.jpg?w=225" data-large-file="https://adq454703481.files.wordpress.com/2021/02/kindle_illustrated.jpg?w=768" src="https://adq454703481.files.wordpress.com/2021/02/kindle_illustrated.jpg?w=768" alt="" srcset="https://adq454703481.files.wordpress.com/2021/02/kindle_illustrated.jpg?w=768 768w, https://adq454703481.files.wordpress.com/2021/02/kindle_illustrated.jpg?w=112 112w, https://adq454703481.files.wordpress.com/2021/02/kindle_illustrated.jpg?w=225 225w, https://adq454703481.files.wordpress.com/2021/02/kindle_illustrated.jpg 998w" sizes="(max-width: 768px) 100vw, 768px"></figure>



<ul><li>Red: annoying clips</li><li>Purple: <strong>really</strong> annoying glue.</li><li>Yellow: the serial port!</li></ul>



<p>As usual for this sort of thing, the serial port is missing its socket, so we need to solder onto the tiny contacts on the board. I like to use ~0.2mm wirewrap wire for this sort of thing, and the surface mount rework bit for my soldering iron:</p>







<p>I don’t want to leave any wires flapping about, but I also know at some point I’m going to screw up and need serial port console access, so I came up with a solution and attached it:</p>







<p>I superglued a piece of Veroboard onto the kindle’s PCB, then soldered wirewrap wires from the tiny PCB contacts onto one end. Finally, I soldered a larger, more conventional “Dupont” cable socket on the other end so I could easily attach and detach from it. Oh, the top cable on the Kindle PCB is 0v/GND, the others are TX and RX (I forget the order of those two).</p>



<p>Final hurdle: the kindle serial port runs at 1.8v, so I needed a serial port adaptor which supports that:</p>



<figure><img data-attachment-id="1737" data-permalink="https://blog.lidskialf.net/kindle_usbserial/" data-orig-file="https://adq454703481.files.wordpress.com/2021/02/kindle_usbserial.jpg" data-orig-size="545,828" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="kindle_usbserial" data-image-description="" data-medium-file="https://adq454703481.files.wordpress.com/2021/02/kindle_usbserial.jpg?w=197" data-large-file="https://adq454703481.files.wordpress.com/2021/02/kindle_usbserial.jpg?w=545" src="https://adq454703481.files.wordpress.com/2021/02/kindle_usbserial.jpg?w=545" alt="" srcset="https://adq454703481.files.wordpress.com/2021/02/kindle_usbserial.jpg 545w, https://adq454703481.files.wordpress.com/2021/02/kindle_usbserial.jpg?w=99 99w, https://adq454703481.files.wordpress.com/2021/02/kindle_usbserial.jpg?w=197 197w" sizes="(max-width: 545px) 100vw, 545px"></figure>



<p>The one I bought does 5v. 3.3v, 2.5v, and 1.8v: it’s pretty neat!</p>



<p>Root Access Granted!</p>



<p>Next, I attached the serial adapter to my laptop, ran the <strong>minicom</strong> serial port software and rebooted the Kindle. Then, once I (inevitably) swapped the TX and RX wires around, I was greeted by this!</p>



<pre><code>U-Boot 2009.08-lab126 (Aug 29 2012 - 12:55:24)

CPU:   Freescale i.MX50 family 1.1V at 800 MHz
mx50 pll1: 800MHz
mx50 pll2: 400MHz
mx50 pll3: 216MHz
ipg clock     : 50000000Hz
ipg per clock : 50000000Hz
uart clock    : 24000000Hz
ahb clock     : 100000000Hz
axi_a clock   : 400000000Hz
axi_b clock   : 200000000Hz
weim_clock    : 100000000Hz
ddr clock     : 800000000Hz
esdhc1 clock  : 80000000Hz
esdhc2 clock  : 80000000Hz
esdhc3 clock  : 80000000Hz
esdhc4 clock  : 80000000Hz
MMC:  FSL_ESDHC: 0, FSL_ESDHC: 1
Board: Tequila
Boot Reason: [POR]
Boot Device: MMC
Board Id: 0031701123730Z56
S/N: B02317022392005M
Initing MDDR memory
ZQ calibration complete: 0x128=0xfffe0010 0x12C=0xffffffff
DRAM:  256 MB
Using default environment

In:    serial
Out:   logbuff
Err:   logbuff
Quick Memory Test 0x70000000, 0x10000000
POST done in 13 ms
Hit any key to stop autoboot:  0 
## Booting kernel from Legacy Image at 70800000 ...
   Image Name:   Linux-2.6.31-rt11-lab126
   Image Type:   ARM Linux Kernel Image (uncompressed)
   Data Size:    4777568 Bytes =  4.6 MB
   Load Address: 70008000
   Entry Point:  70008000
   Verifying Checksum ... OK
   Loading Kernel Image ... OK
OK
Starting kernel ...

[snip]

Welcome to Kindle!

kindle login: </code></pre>



<p>Great, so that’s booting the uboot bootloader, then booting into linux and asking me to login.</p>



<p>Trying to login as root prompts for a password: Hmm… However, I already knew from previous Kindle stuff that you can generate the password from the serial number. I found <a href="https://www.sven.de/kindle/">this website</a> which generates a number of possible passwords for a specific device: Mine was the third on the list.</p>



<p>In case that site dies here’s the key snippet of Javascript:</p>



<pre><code>var md5 = hex_md5(serial);
document.getElementById("rootpw").innerHTML = "fiona" + md5.substring(7,11);
document.getElementById("rootpw2").innerHTML = "fiona" + md5.substring(7,10);
document.getElementById("rootpw3").innerHTML = "fiona" + md5.substr(13,3);</code></pre>



<p>Oh: I forgot to mention how I extracted the device’s serial number. Well, plugging it into USB doesn’t really “work”: you can’t mount these demo devices as disks. But, under linux, it still outputs the serial number in linux’s <strong>dmesg</strong> output (you can also get it using <strong>printenv</strong> in uboot if you press enter when it says “Hit any key to stop autoboot”):</p>



<pre><code>[128033.676587] usb 1-2: new high-speed USB device number 51 using xhci_hcd
[128033.829631] usb 1-2: New USB device found, idVendor=1949, idProduct=0004, bcdDevice= 1.00
[128033.829638] usb 1-2: New USB device strings: Mfr=1, Product=2, SerialNumber=3
[128033.829642] usb 1-2: Product: Amazon Kindle
[128033.829645] usb 1-2: Manufacturer: Amazon
[128033.829648] usb 1-2: SerialNumber: XXXXXXXXXXXXXXXX
</code></pre>



<p>Cool! We have root and can login! Now to figure out how to make it a bit easier to do stuff on it.</p>



<p>Dumping The System</p>



<p>First step is usually to dump the disks for analysis on another computer.</p>



<p>Checking <code>/proc/mounts</code> shows multiple partitions of the main disk on <code>/dev/mmcblk0</code>.</p>



<p>Running <code>fdisk /dev/mmcblk0</code> gives the following:</p>



<pre><code>Units = cylinders of 64 * 512 = 32768 bytes

        Device Boot      Start         End      Blocks  Id System
/dev/mmcblk0p1   *        1025       12224      358400  83 Linux
/dev/mmcblk0p2           12225       14272       65536  83 Linux
/dev/mmcblk0p3           14273       15296       32768  83 Linux
/dev/mmcblk0p4           15297       59776     1423360   b Win95 FAT32</code></pre>



<ul><li>So, four partitions, three linux, and one FAT32. </li><li>The first disk starts quite far into the disk: turns out the kernel is stored in that “missing” area. </li><li>Poking about a bit more shows partition 1 is the normal system, 2 is a sort of diagnostic tool partition, 3 is for storing internal private state of the kindle (eg wifi passwords). Finally, 4 is the one you see when you plug a kindle in over USB: its where your books would reside.</li><li>Partition 4 is mounted at /mnt/us.</li></ul>



<p>I dumped the start of the disk and partitions 1-3 onto /mnt/us using dd (I like to take a complete raw image if I can so I can restore it in case something goes wrong):</p>



<pre><code>dd if=/dev/mmcblk0 of=/mnt/us/kindle.img bs=32768 count=15297</code></pre>



<p>Although this Kindle doesn’t show a disk over USB, since I have root, I can simply make it do it:</p>



<pre><code>rmmod g_file_storage
modprobe g_file_storage file=/dev/mmcblk0p4</code></pre>



<p>It popped up on my laptop so I copied everything off.</p>



<p>System Analysis</p>



<p>Finally, I mounted the partitions in kindle.img on my laptop with:</p>



<pre><code>kpartx -v kindle.img</code></pre>



<p>I could then mount the individual partitions on my laptop. I extracted all the files into a folder so I could poke around them and grep them easily. I figured out:</p>



<ul><li>It uses <code>rc.d</code> as its system init system, so there are lots of nice plain text scripts.</li><li>Init level 5 is the “normal” system running the ebook software</li><li>The ebook software is in <code>/opt/amazon</code> and is in Java (I kinda already knew this, but needed a quick refresher).</li><li>There’s a whole load of interesting plain text “diag” scripts for testing.</li><li>There’s a rather nifty <code>wifid</code> daemon for managing the wifi connection: I figured out how to talk to this from the diag scripts.</li><li>You can write to the eink screen from the command line using the <code>/usr/sbin/eips</code> command (<a href="https://wiki.mobileread.com/wiki/Eips">docs here</a>).</li><li>I couldn’t find an obvious “turn off demo mode” switch: it appears the demo mode is a customised build of the Java ebook software.</li><li>The following system services are to do with unsupported features, the ebook software, or talking back to amazon: <code>S50wan S70wand S75phd S81usbnetd S93webreaderd S94browserd S95framework S96boot_finished</code>.</li></ul>



<p>Talking To Wifid</p>



<p>You can use the built in wifid to connect to wifi and manage your wifi profiles. Oh, also, remember many Kindles only support 2.4Ghz wifi when you wonder why it isn’t working 😉</p>



<p><strong>List number of WIFI profiles:</strong></p>



<p><code>lipc-get-prop com.lab126.wifid profileCount</code></p>



<p><strong>Show contents of a WIFI profile:</strong></p>



<p><code>echo "{index=(0)}" | lipc-hash-prop com.lab126.wifid profileData</code></p>



<p><strong>Delete a WIFI profile:</strong></p>



<p><code>lipc-set-prop com.lab126.wifid deleteProfile WIFIESSID</code></p>



<p><strong>Create a WIFI profile:</strong></p>



<p><code>echo '{essid="WIFIESSID", smethod="wpa2", secured="yes", psk="WIFIPSK"}' | lipc-hash-prop com.lab126.wifid createProfile</code></p>



<p>smethod can be one of open,wep,wpa,wpa2 (if you choose open, set secured to “no”)</p>



<p>WIFIPSK is the WIFI PSK as generated with the <code>wpa_passphrase</code> utility (which is actually on the kindle): a normal “wifi passphrase” will not work.</p>



<p><strong>Connect a WIFI profile:</strong></p>



<p><code>lipc-set-prop com.lab126.wifid cmConnect WIFIESSID</code></p>



<p><strong>Show WIFI connection status:</strong></p>



<p><code>echo "{index = (0)}" | lipc-hash-prop -n com.lab126.wifid currentEssid</code></p>



<p>Making Changes To Root</p>



<p>Many of the following instructions need to change the root disk on the kindle. However, by default it is mounted in read only mode, preventing modification. To fix, run this command on the kindle:</p>



<pre><code>mntroot rw</code></pre>



<p>When done, set it back to read only mode to prevent any unwanted changes:</p>



<pre><code>mntroot ro</code></pre>



<p>Installing Dropbear SSH</p>



<p>I wanted to be able to ssh into my kindle, so I needed to install the <a href="https://matt.ucc.asn.au/dropbear/dropbear.html">dropbear</a> ssh daemon. Of course this is an ARM based device, so I either had to compile it myself or find it somewhere. Luckily …</p></div></article></main></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.lidskialf.net/2021/02/08/turning-an-old-kindle-into-a-eink-development-platform/">https://blog.lidskialf.net/2021/02/08/turning-an-old-kindle-into-a-eink-development-platform/</a></em></p>]]>
            </description>
            <link>https://blog.lidskialf.net/2021/02/08/turning-an-old-kindle-into-a-eink-development-platform/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26073463</guid>
            <pubDate>Tue, 09 Feb 2021 03:21:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Deep Learning for Expressive Piano Performances]]>
            </title>
            <description>
<![CDATA[
Score 21 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26072761">thread link</a>) | @angusturner
<br/>
February 8, 2021 | https://popgun-labs.github.io/ml-blog/generative_models/piano/symbolic_music/2020/02/01/beatnet.html | <a href="https://web.archive.org/web/*/https://popgun-labs.github.io/ml-blog/generative_models/piano/symbolic_music/2020/02/01/beatnet.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <!-- Global site tag (gtag.js) - Google Analytics -->




<!-- mathjax support -->


<h2 id="preface">Preface</h2>

<p>For the last four years, a small team at Popgun has been studying the application of deep learning to music analysis
and generation. This research has culminated in the release of <a href="https://splashpro.popgun.ai/">Splash Pro</a> - a free, AI-powered plugin 
for Digital Audio Workstations (DAWs). With the release of this blog, we hope to provide an accessible introduction to 
deep learning with music, by explaining some core projects we have worked on.</p>

<h2 id="introduction">Introduction</h2>

<p>In this post I want to describe the design of a model for expressive piano synthesis. The model, 
which we call ‘BeatNet’, is capable of analysing a piano performance and generating new pieces that imitate the 
original playing style. Before I get into the technical workings, here are a few samples generated by BeatNet:</p>

<p><strong>In the style of “Yiruma - River Flows In You”</strong>
</p>





<p><strong>In the style of “Chopin - Nocturne in E-flat major, Op. 9, No. 2”</strong>
</p>





<p><strong>In the style of “Errol Garner - Misty”*</strong>
</p>





<p>BeatNet is able to replicate many of the characteristics present in human piano performances, albeit with
some weaknesses and limitations that I will address. To explain how this model works, I have split the 
remainder of the blog into two sections. Part I will discuss musical representations, while Part II will explore the 
model design and showcase more of its capabilities.</p>

<p><span>
*Note: BeatNet does not model sustain pedal events. Sustain was manually added to this piece.
</span></p>

<h2 id="part-i---representations-of-music">Part I - Representations of Music</h2>

<p>In designing a model for music generation, it is critical to choose an appropriate representation. This choice
determines which data can be faithfully encoded, the modelling techniques that are available, and the efficacy of the 
overall system. For this reason, I have decided to begin with a brief explanation of some common musical 
representations. If this all feels familiar to you, feel free to skip ahead to part II.</p>

<h3 id="audio">Audio</h3>

<p>When we talk about audio in machine learning, we are typically referring to an array of time-domain samples. These
samples are a digital approximation to the physical sound pressure wave. The quality of this approximation is
determined by the sample rate and the bit-depth (see: <a href="https://en.wikipedia.org/wiki/Pulse-code_modulation">PCM</a>). In 
research, it is common to see sample rates of 16-22kHz. However, to capture the full range of human audible frequencies,
most HiFi applications (e.g. music, podcasts, etc) use rates of around 44kHz 
(see: <a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">Nyquist Theorem</a>).</p>

<p>Because of its high dimensionality, modelling raw audio is extremely challenging. For a generative model to 
reproduce audio structure on a timescale of a few seconds, it must capture the relationships between tens of 
thousands of samples. While it is becoming increasingly feasible to do this (see <a href="#citation-1">[1]</a>), there are more 
efficient ways to encode piano performances.</p>

<p>Note: For an example of music modelling directly in the audio domain, 
see <a href="https://openai.com/blog/jukebox/">OpenAI’s Jukebox</a> <a href="#citation-2">[2]</a>.</p>

<h3 id="midi">MIDI</h3>

<p>MIDI is a technical standard allowing electronic instruments and computers to communicate. It comprises
a sequence of ‘messages’, each describing a particular musical event or instruction. Instructions relating to the 
note timings, pitch and loudness (i.e. velocity) can be transmitted and stored. For example, a piano performance 
encoded as MIDI is a record of which notes were pressed, how forcefully and at what time.</p>

<p>To convert a MIDI file back into listenable audio, a piece of software replays these instructions and
synthesizes the relevant notes. MIDI does not encode acoustic information, such as the instrument sound or the recording
environment. For this reason it is drastically more efficient than audio, and it is a good starting point for our 
exploration.</p>

<p>When working with MIDI we use the terrific <a href="https://github.com/craffel/pretty-midi">PrettyMIDI</a> library by Colin Raffel. This abstracts away many
complexities of the raw format. See Colin Raffel’s <a href="https://colinraffel.com/publications/thesis.pdf">thesis</a> for
a more in-depth explanation of the MIDI file format.</p>

<p><strong>Figure 1 - Für Elise in PrettyMIDI Format</strong></p>
<div><div><pre><code>Note(start=0.000000, end=0.305000, pitch=76, velocity=67)
Note(start=0.300000, end=0.605000, pitch=75, velocity=63)
Note(start=0.600000, end=0.905000, pitch=76, velocity=74)
Note(start=0.900000, end=1.205000, pitch=75, velocity=71)
...
</code></pre></div></div>

<p>Notice how succinctly we can represent Für Elise this way, and consider that the equivalent audio section 
contains approximately 50k samples (for a 44kHz recording).</p>

<h3 id="piano-roll">Piano Roll</h3>

<p>This will look familiar to anyone who has used a Digital Audio Workstation (DAW) such as GarageBand or Ableton.
The vertical axis denotes pitch, and the horizontal axis denotes time. The velocity of each note is encoded
by its magnitude. As well as providing an intuitive way to visualize MIDI, piano roll has some interesting 
properties for machine learning.</p>

<p>For example, consider the effect of raising the song’s pitch by one semitone, or delaying its onset by a few seconds. 
These transpositions preserve the spatial structure of the piano roll. When designing a model we can exploit this 
property by applying 2D Convolutional Neural Networks (CNN). Early experiments at Popgun tried exactly
this, as did <a href="https://youtu.be/nA3YOFUCn4U?t=597">this project</a> using PixelCNN. However, there are some serious drawbacks to this approach.</p>

<p><strong>Figure 2 - Für Elise in Piano Roll Format</strong></p>
<p><img src="https://popgun-labs.github.io/ml-blog/assets/images/piano_roll_fur_elise_crop.png" alt="Für Elise - Piano Roll"></p>

<p>A typical song encoded as piano roll is incredibly sparse: nearly all entries are zero. 
Naively applying a CNN wastes a large amount of computation on entirely empty regions. It is also difficult
to choose a suitable resolution for the time axis. If the original data is quantised, such that each note aligns
perfectly with a uniform musical grid, we can choose the resolution based on the quantisation strength. However,
this breaks down for expressive piano performances, where natural variations in timing are critical to the musicality.</p>

<p>Note: Many works have tried to model piano roll, using a broad range of neural net architectures. For those interested 
to learn more, <a href="https://arxiv.org/abs/1206.6392">this paper</a> is a good starting point <a href="#citation-16">[16]</a>.</p>

<h3 id="time-shift-format">Time-Shift Format</h3>

<p>In 2017 <a href="https://magenta.tensorflow.org/blog/">Google Magenta</a> released a model called Performance RNN <a href="#citation-3">[3]</a>, which demonstrated the ability 
to model expressive piano performances with high fidelity. The key innovation was a new format that is highly 
suited to this task. For reasons that will become apparent, we refer to this “performance representation” simply as 
“time-shift”. Like MIDI, time-shift encodes music with a sequence of discrete musical events. What sets it apart is 
the unique way it encodes the progression of time. Instead of representing time along a specific axis (piano roll), 
or as a property of each individual note (MIDI), in time-shift there is a specific event that indicates the 
advancement of the piece.</p>

<p><strong>Figure 3 - Für Elise in Time-Shift Format</strong></p>
<div><div><pre><code>272 - Set Velocity = 64
76  - Turn On MIDI Note 76 (E5)
317 - Advance Time by 0.3 Seconds
204 - Turn Off MIDI Note 76 (E5)
271 - Set Velocity = 60
75  - Turn On MIDI Note 75 (D#5)
317 - Advance Time by 0.3 Seconds
203 - Turn Off MIDI Note 75 (D#5)
274 - Set Velocity = 72
76  - Turn On MIDI Note 76 (E5)
317 - Advance Time by 0.3 Seconds
204 - Turn Off MIDI Note 76 (E5)
273 - Set Velocity = 68
75  - Turn On MIDI Note 75 (D#5)
317 - Advance Time by 0.3 Seconds
...
</code></pre></div></div>

<p>This one-dimensional sequence of tokens is similar to the encodings used in language models.
This means that modern advances in NLP (e.g. ByteNet <a href="#citation-4">[4]</a>, Transformers <a href="#citation-5">[5]</a>) can be 
readily applied to the time-shift format.</p>

<p><strong>Figure 4 - An Explanation of the Time-Shift Events</strong></p>
<div><div><pre><code>0 - 127: Note on events    
128 - 255: Note off events    
256 - 287: 32 velocity change events   
288 - 387: 100 quantized time-shift values (10ms -&gt; 1000ms)
</code></pre></div></div>

<p>Note: We have not accounted for sustain pedal events, however it might be interesting to incorporate them
in future work.</p>

<h2 id="part-ii---beatnet-a-convolutional-model-for-piano-music">Part II - BeatNet: A Convolutional Model for Piano Music</h2>

<p>In 2016, researchers at Google’s DeepMind lab released the now-famous WaveNet paper <a href="#citation-1">[1]</a>. The core 
insight was that 1D convolutions could produce efficient sequence models, by eliminating the need for expensive 
recurrent computations during training. The efficacy was proven with a 
<a href="https://deepmind.com/blog/article/wavenet-generative-model-raw-audio">demo</a> of raw audio generation. The 
publication of ByteNet <a href="#citation-4">[4]</a> shortly afterwards, extended this idea to the domain of natural language 
processing.</p>

<p>Inspired by this work, Popgun developed its own ByteNet model for symbolic piano music. We refer to this work as
“BeatNet”. The choice to use ByteNet over traditionally dominant RNNs was motivated by a few factors:</p>
<ol>
  <li>The ability to capture long-term dependencies over hundreds or thousands of tokens</li>
  <li>Fast training</li>
  <li>Hype. WaveNet and ByteNet were the first papers to threaten the dominance of RNNs, before Transformer based
architectures <a href="#citation-5">[5]</a> exploded in popularity.</li>
</ol>

<h3 id="data-collection-and-processing">Data Collection and Processing</h3>

<p>We curated a MIDI dataset by drawing from a number of online sources. One key source was the
<a href="http://www.piano-e-competition.com/">Yamaha e-Piano Competition Dataset</a>, consisting of jazz 
and classical music performances. Another source was the <a href="https://colinraffel.com/projects/lmd/">Lakh MIDI Dataset</a>, 
which contains a broader range of genres, including many contemporary pieces.</p>

<p>To create a model which generates highly expressive performances, we processed the data to filter out ‘low quality’ 
items. Anyone who has worked with MIDI datasets will appreciate that many songs sound ‘bad’.
This raises some difficult problems: What is musical quality? How can we account for subjective preferences? 
Are some songs so weird we can confidently deem them ‘not musical’? Suffice to say, these problems are beyond
the scope of this post.</p>

<p>Ultimately, our working solution can be summarised as this: An engineer listened to some data items, in each case 
proclaiming “Yep, this sounds musical” or “Nope, I don’t like it”. In the process, they developed automated heuristics
to filter unwanted items, biasing the dataset towards songs that are subjectively ‘musical’. In the future it would be 
interesting to crowd-source these assessments, or to devise heuristics based on music theory instead.</p>

<h3 id="a-working-prototype">A Working Prototype</h3>

<p>We completed our first working prototype in July 2017. It is …</p></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://popgun-labs.github.io/ml-blog/generative_models/piano/symbolic_music/2020/02/01/beatnet.html">https://popgun-labs.github.io/ml-blog/generative_models/piano/symbolic_music/2020/02/01/beatnet.html</a></em></p>]]>
            </description>
            <link>https://popgun-labs.github.io/ml-blog/generative_models/piano/symbolic_music/2020/02/01/beatnet.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26072761</guid>
            <pubDate>Tue, 09 Feb 2021 01:33:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I don't want to do front-end anymore]]>
            </title>
            <description>
<![CDATA[
Score 387 | Comments 372 (<a href="https://news.ycombinator.com/item?id=26071906">thread link</a>) | @askonomm
<br/>
February 8, 2021 | https://www.askonomm.com/blog/i-dont-want-to-do-frontend-anymore | <a href="https://web.archive.org/web/*/https://www.askonomm.com/blog/i-dont-want-to-do-frontend-anymore">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>I started coding when I was 14, around 2007 or so. The very first thing I wrote was HTML, then CSS. I liked
making stupid little web pages containing youtube embeds and guest books (anyone remembers those?) full of
<a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/marquee">marquees</a>, <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/blink">blinks</a>
and gifs. Lots of gifs. The pages were very simple, but joyful to make and also to use. At the time I had no idea that
you could make any money with making web pages, I simply thought it was a silly hobby people could have.</p>
<p>Fast-forward 3 years I got my first few gigs as a web developer. By then I was pretty good at HTML and CSS already,
had dabbled enough with PHP to know my way around of most sticky problems I would find myself in and while I didn't
really know much of vanilla JavaScript, it was okay, because everyone used almost exclusively <a href="https://jquery.com/">jQuery</a> anyway. Yet still,
the websites were rather simple and joyful to make.</p>
<p>It was easy to get started, too - you just created the files and refreshed the page. HTML? PHP? JavaScript? Just create the
file and save it, then refresh the browser. Rinse and repeat until you are happy with the result. But then, almost
out of nowhere, something changed. At first, it came slowly and before it could even speed up, it was already here.
<strong>The complicated web was here</strong>.</p>
<p>Nowadays I make a living mainly with JavaScript and TypeScript using React.js as a front-end framework. That's right,
front-ends are so complex they now need frameworks to be able to manage their seemingly infinite component hierarchies.
JavaScript no longer is liked by the community, so the community created a poor man's version of a typed language which
duct-tapes around an <a href="https://whydoesitsuck.com/why-does-javascript-suck/">already poorly made language</a>. </p>
<p>Starting a new project? Make sure to write your project idea down because by the time you are finished setting up the vast
boilerplate you have probably forgotten it. Vast boilerplate? Oh yeah, you better set-up your project with <a href="https://www.typescriptlang.org/">TypeScript</a>,
<a href="https://eslint.org/">ESlint</a>, <a href="https://webpack.js.org/">Webpack</a> and <a href="https://babeljs.io/">Babel</a>, because
if you don't then obviously you haven't learned anything since 2005 and suck. Don't have <a href="https://www.npmjs.com/">NPM</a>? Better install that, too,
because nobody installs libraries without a package manager anymore. Oh and while you're at it, install also <a href="https://yarnpkg.com/">Yarn</a>,
because why not make use of two package managers at the same time. <em>Phew</em>, you did all that? Damn, that's commitment!
You can finally write what is essentially just HTML and JavaScript!</p>
<p>Now, don't forget we also want to style our project, but if you thought you could get away with writing good ol' CSS,
you were wrong. We no longer write CSS, because it's lame. We now write <a href="https://sass-lang.com/">SCSS</a> instead,
so that we could write dynamic stylesheets that convert to CSS, because <a href="https://hospodarets.com/you-might-not-need-a-css-preprocessor">of course 99% of the web needs that</a>. </p>
<p>But enough of the hating. If I wanted I could hate on anything most likely, and the web has come a long way since 2007,
in lots of good ways, and I don't want to discredit that. I suppose with the increase of complexity in what we want to
achieve on the web the stack to achieve it with has had to also increase in complexity. For me, personally, it's
too much. I want to have a personal life and not have to spend my nights reading up on some new flavour of *.js in
fear that if I don't I would soon be made irrelevant. I don't want to learn nor use a million different tools. I don't want
to know a bit about everything and a lot about nothing. </p>
<p>Thus, I don't want to do front-end development anymore. The joy is gone. I've given in my resignation at my current place of
employment and will be seeking an exclusively back-end role for my next adventure, starting April. The language doesn't
matter much to me, I know enough of them to know that they are all very similar and thus easy enough to learn. If you know of a
good opportunity, <a href="mailto:asko@askonomm.com">let me know</a>.</p></div></div>]]>
            </description>
            <link>https://www.askonomm.com/blog/i-dont-want-to-do-frontend-anymore</link>
            <guid isPermaLink="false">hacker-news-small-sites-26071906</guid>
            <pubDate>Mon, 08 Feb 2021 23:40:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What's an SPF Record?]]>
            </title>
            <description>
<![CDATA[
Score 118 | Comments 45 (<a href="https://news.ycombinator.com/item?id=26071103">thread link</a>) | @albertgoeswoof
<br/>
February 8, 2021 | https://blog.ohmysmtp.com/blog/whats-an-spf-record/ | <a href="https://web.archive.org/web/*/https://blog.ohmysmtp.com/blog/whats-an-spf-record/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>February 08, 2021</p><p>A Sender Policy Framework (SPF) record is a piece of text that you can add to your Domain Name System (DNS). It tells email clients (such as Gmail or Outlook) who can send email from your domain, and those clients can check each email received against this record to see if it is likely to be spam or forged.</p><h2>Background</h2><p>SPAM has been sloshing around on the internet since forever (well <a href="https://www.themarysue.com/first-spam-email/">since 1978, apparently</a>, and so have methods to fight it. SPF is a neat way to help prevent your domain name from being abused for appearing to send spam or forged emails.</p><p>The thing with email is that anyone can pretend to send emails from anyone else, there’s nothing in the specifications that prevent this. To impersonate someone, all you have to do is change the <code>From</code> email header to say it came from them. To make matters worse, emails are handed between lots of different servers when being sent, and are rarely encrypted to prevent modifications en-route.</p><p>Why is this allowed? Well email is really, really old! No one designing email could have possibly anticipated the kind of spam we see today, and computers just didn’t have the power or resources to do complex cryptography and a gazillion network requests back in the day.</p><p>Fortunately there are some technologies (including SPF) that work together to help validate that an email has come from the person/place it claims to have come from.</p><h2>So how do SPF records help prevent spam?</h2><p>An SPF record says which mail servers are allowed to send email on your behalf. So when an email arrives from a particular server, the receiving server can look up the SPF record of the domain in the <code>From</code> field and validate that the server is allowed to send email for that specific Domain.</p><p>If no match is found, then depending on the SPF policy in the record, the email will be marked as spam, outright rejected or have its spam score lowered. </p><p>Without SPF there would be no way to check that the server that sent an email is actually allowed to send emails for the domain. By adding an SPF record you’re telling the world that this particular IP, service or set of IPs is allowed to send email for that domain.</p><h2>What does an SPF record look like?</h2><p>Here’s a simple record:</p><pre data-language="" data-index="0"><code><span>v=spf1 ip4:192.168.0.1 ~all</span></code></pre><p>Let’s break it down:</p><ul><li><code>v=spf1</code> tells whoever is looking this up that this is an SPF record, and it’s a Version 1 record. There’s only one version at present, so this will always be the same for the forseeable future</li><li><code>ip4:192.168.0.1</code> is an IP address that’s allowed to send emails from this domain. You can list multiple IPs here by including a space between them, add ranges or even full domains</li><li><code>~</code> is one of four <em>qualifiers</em> which tells the email client how to mark an email that matches the term to the right (in this case <code>all</code>). The options are pass (+), fail (-), softfail (~) or neutral (?) - using these allows you to do things like whitelist or blacklist addresses, or completely disable all emails from a given domain</li><li><code>all</code> simply says match all emails</li></ul><p>Email clients will evaluate this record <strong>from left to right</strong>, looking for a match against the IP address that the email actually originated from. So if an email came from 192.168.0.1 it will match this record and pass SPF, but if it didn’t arrive from there it will match <code>all</code> and be marked as a softfail (because of the <code>~</code>). Softfail means that the email should not be rejected for this reason alone, but it will be factored into the spam score.</p><p>To add this to your DNS record, open up your domain registrar or DNS management console, and add a new <code>TXT</code> record with the text as your content. If you’re using a service to send emails on your behalf your email service provider will give you the exact record details you need, if you’re sending emails from your own server you’ll need to craft your own record, you can use <a href="https://tools.ietf.org/html/rfc7208">RFC7208</a> as a reference to get it perfect.</p><p>Easy-peasy lemon squeezy!</p><h2>Why don’t you need to setup an SPF record with OhMySMTP?</h2><p>The good news is that you don’t need to know any of this or do anything to use OhMySMTP and have your emails pass SPF validation.</p><p>We set a specific header in the email called <code>Return-Path</code> with all emails sent through our email service. If you take a peek at a raw email sent over OhMySMTP you’ll find that the return path ends in <code>@mailer.ohmysmtp.com</code>, and if you look this up in the DNS system, you’ll find an SPF record that points to our server IP address.</p><p>Luckily email clients accept the return path header as the source of the email, so SPF checks pass on all our emails. without our users needing to do anything.</p><h2>Other technologies and further reading</h2><p>If you want to learn everything there is to know about SPF, you can read the RFC (“Request For Comments”, basically the SPF specification) here: <a href="https://tools.ietf.org/html/rfc7208">https://tools.ietf.org/html/rfc7208</a></p><p>But SPF alone doesn’t completely solve spam, we also need DKIM, and to a lesser extent DMARC. More on those later!</p><hr><ul><li><a rel="prev" href="https://blog.ohmysmtp.com/blog/send-emails-over-smtp/">← <!-- -->Send emails over SMTP with OhMySMTP.com</a></li><li></li></ul></div></div>]]>
            </description>
            <link>https://blog.ohmysmtp.com/blog/whats-an-spf-record/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26071103</guid>
            <pubDate>Mon, 08 Feb 2021 22:26:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Tesla spent $1.5B in clean car credits on Bitcoin]]>
            </title>
            <description>
<![CDATA[
Score 64 | Comments 153 (<a href="https://news.ycombinator.com/item?id=26070744">thread link</a>) | @ForHackernews
<br/>
February 8, 2021 | https://amycastor.com/2021/02/08/tesla-spent-1-5b-in-clean-car-credits-on-bitcoin-the-filthiest-asset-imaginable/ | <a href="https://web.archive.org/web/*/https://amycastor.com/2021/02/08/tesla-spent-1-5b-in-clean-car-credits-on-bitcoin-the-filthiest-asset-imaginable/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-5855">
		<div>
		
<p>Tesla bought $1.5 billion worth of bitcoin, the company said in a <a rel="noreferrer noopener" href="https://secfilings.nasdaq.com/filingFrameset.asp?FilingID=14678414&amp;RcvdDate=2/8/2021&amp;CoName=TESLA,%20INC.&amp;FormType=10-K&amp;View=html" target="_blank">regulatory filing</a> on Monday, effectively putting nearly all of the money it earned on clean car credits towards the world’s filthiest asset. </p>



<p>Where to begin? Let’s start with the firm’s SEC filing. As of January 2021, the Silicon Valley-based company updated its investment policy to allow it more flexibility in diversifying its returns on cash. Those changes allow Tesla to buy bitcoin and other cryptocurrencies, which it immediately did.</p>



<p>“Thereafter, we invested an aggregate $1.50 billion in bitcoin under this policy and may acquire and hold digital assets from time to time or long-term. Moreover, we expect to begin accepting bitcoin as a form of payment for our products in the near future, subject to applicable laws and initially on a limited basis, which we may or may not liquidate upon receipt.”</p>



<p>The filing does not say how Tesla bought the bitcoin or how they are custodying it. It also does not tell us how many bitcoin it purchased or for what average price. We only know Tesla bought bitcoin sometime between Jan. 1 and early February, when the price was between $30,000 to $41,000.&nbsp;</p>



<p>Tesla says its customers will be able to buy its vehicles with bitcoin. However, “liquidate upon receipt” means that if you purchase a Tesla with bitcoins, the company is likely to sell those bitcoins for cash immediately, something that is usually done by sending the funds through a payment processor first. </p>



<p>This is what most large merchants do when they say they are accepting bitcoin. They convert it to cash, so they don’t have to deal with bitcoin’s wild volatility. So if you buy a Tesla with bitcoin in the future, it will likely be the same as selling your bitcoin for fiat and then handing the cash over to Tesla. </p>



<h2>Clean car credits for bitcoin</h2>



<p>Tesla earns tradable credits under various regulations related to zero-emission vehicles, greenhouse gas, fuel economy, renewable energy, and clean fuel. It then turns around and sells those credits to other automakers when they can’t comply with auto emissions and fuel economy standards.</p>



<p>In 2020, Tesla reported making $1.58 billion in selling these tradable credits it received. And here is the important bit: without those tradeable credits, the company would not have been profitable. <a href="https://apnews.com/article/coronavirus-pandemic-67705113fb4f895bd051a1bee1d23518" target="_blank" rel="noreferrer noopener">Tesla would have lost money.</a> So what does it do with that money? It turns around and buys bitcoin. </p>



<p>Bitcoin is an environmental disaster. The bitcoin network currently burns around 116.87 terawatt-hours per year, according to the <a href="https://cbeci.org/">University of Cambridge’s Centre for Alternative Finance.</a> To give you an idea of how devastating that is to our climate, that is as much energy as a small country or <a href="https://news.bitcoin.com/the-bitcoin-network-now-consumes-7-nuclear-plants-worth-of-power/#:~:text=The%20Cambridge%20Bitcoin%20Electricity%20Consumption%20Index%20(CBECI)%20shows%20the%20estimated,is%207.46%20gigawatts%20(GW).&amp;text=The%20amount%20of%20power%20consumed,photovoltaic%20(PV)%20solar%20panels." target="_blank" rel="noreferrer noopener">seven nuclear power plants. </a></p>



<figure><div>
<div><blockquote data-width="550" data-dnt="true"><p lang="en" dir="ltr">Bitcoin is such an environmental disaster it really is a crime against humanity.  So what does Tesla do with their $1.5B in revenue last year from clean car credits sold to other automakers?  Put it into “Destroy the Planet Inc”</p>— Nicholas Weaver (@ncweaver) <a href="https://twitter.com/ncweaver/status/1358780808144723968?ref_src=twsrc%5Etfw">February 8, 2021</a></blockquote></div>
</div></figure>



<p>Keep in mind, bitcoin’s energy consumption increases right alongside the price of bitcoin. As bitcoin goes up in price, more people want to mine the virtual currency for profit, leading to greater energy consumption as they pile more money into power-hungry ASIC rigs.</p>



<p>Bitcoin is not only filthy for its energy waste but also because it is the currency of choice in underground economies. <a href="https://www.forbes.com/sites/andreatinianow/2020/07/01/bitcoin-demand-drives-14-billion-ransomware-industry-in-the-us/?sh=40346fca32d8" target="_blank" rel="noreferrer noopener">Ransomware</a> would probably not exist if it were not for bitcoin. </p>



<p>And bitcoin fits the very definition of <a rel="noreferrer noopener" href="https://www.ic.unicamp.br/~stolfi/bitcoin/2020-12-31-bitcoin-ponzi.html" target="_blank">a Ponzi scheme.</a> It has no intrinsic value—any money new investors put into the system immediately goes out via bitcoin miners selling their 900 newly minted bitcoin per day. Tesla’s massive influx of cash will fund the bitcoin miners for about a month and a half, at most. </p>



<h2>Elon Musk shilling crypto</h2>



<p>Two years ago, Musk and Tesla paid a combined <a rel="noreferrer noopener" href="https://www.sec.gov/news/press-release/2018-226" target="_blank">$40 million penalty</a> to the SEC after Musk’s cryptic tweets about taking Tesla private led to stock fluctuations. The regulator <a href="https://www.sec.gov/litigation/complaints/2018/comp-pr2018-219.pdf">charged him</a> with securities fraud. As part of the settlement, Musk agreed to step down as chairman of the company, although he continued to hold the title of CEO. </p>



<p>Apparently, Musk has learned nothing from that experience. Last month, presumably around the time Tesla was buying up hoards of bitcoin unbeknownst to the general public, Musk caused the price of bitcoin to go up 20% when he <a rel="noreferrer noopener" href="https://www.cnbc.com/2021/01/29/bitcoin-spikes-20percent-after-elon-musk-adds-bitcoin-to-his-twitter-bio.html" target="_blank">changed his&nbsp;Twitter bio</a>&nbsp;to include the word “bitcoin.”</p>



<p>Soon after changing the bio, Musk said in a&nbsp;<a rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1355068728128516101" target="_blank">tweet:</a> “In retrospect, it was inevitable.” In retrospect, that tweet looks like an early hint that Tesla was funneling money into the digital asset.  </p>



<figure><div>

</div></figure>



<p>Will Musk get into trouble for his bitcoin tweets? </p>



<p>It is unlikely, Columbia University Law Professor John Coffee, Jr., told the <a href="https://www.wsj.com/articles/tesla-buys-1-5-billion-in-bitcoin-11612791688">Wall Street Journal</a>, especially given that a federal judge rebuked the SEC when it sought to hold Musk in contempt in 2019. “I don’t think the commission would dare push it that far,” he said. </p>



<p>The latest Tesla news caused bitcoin to spike 18% this morning, <a rel="noreferrer noopener" href="https://www.cnbc.com/2021/02/08/bitcoin-surges-above-43000-to-a-record-after-elon-musks-tesla-buys-1point5-billion.html" target="_blank">sending the price to over $44,000,</a> and setting a new all-time high. </p>



<p><em>Updates Feb. 8: Bitcoin topped $44,000 on Monday, even higher than the $43,000 I mentioned earlier. I added that in the SEC settlement Musk agreed to step down as chairman of Tesla. And I added the Coffee quote from WSJ.</em></p>



<p><em>If you like my work, please support my writing by subscribing to my&nbsp;<a rel="noreferrer noopener" href="https://www.patreon.com/amycastor" target="_blank">Patreon account</a>&nbsp;for as little as $5 a month.&nbsp; </em></p>
			</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>]]>
            </description>
            <link>https://amycastor.com/2021/02/08/tesla-spent-1-5b-in-clean-car-credits-on-bitcoin-the-filthiest-asset-imaginable/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26070744</guid>
            <pubDate>Mon, 08 Feb 2021 21:57:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Privacy Engineering Event with VP of Engineering at Signal and Engineer at Zoom]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26070394">thread link</a>) | @giacaglia
<br/>
February 8, 2021 | https://transcend-io.zoom.us/webinar/register/1616128158152/WN_5UYAMb_hTwC563sSJOqdlw | <a href="https://web.archive.org/web/*/https://transcend-io.zoom.us/webinar/register/1616128158152/WN_5UYAMb_hTwC563sSJOqdlw">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
<p><label for="timezone">Time Zone:</label>&nbsp;&nbsp;

</p>
</div>
</div></div>]]>
            </description>
            <link>https://transcend-io.zoom.us/webinar/register/1616128158152/WN_5UYAMb_hTwC563sSJOqdlw</link>
            <guid isPermaLink="false">hacker-news-small-sites-26070394</guid>
            <pubDate>Mon, 08 Feb 2021 21:27:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Clerk – all of user management as-a-service, not just authentication]]>
            </title>
            <description>
<![CDATA[
Score 566 | Comments 222 (<a href="https://news.ycombinator.com/item?id=26069621">thread link</a>) | @colinclerk
<br/>
February 8, 2021 | https://clerk.dev/blog/all-of-user-management-not-just-authentication | <a href="https://web.archive.org/web/*/https://clerk.dev/blog/all-of-user-management-not-just-authentication">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><br>While working on side projects, we've always felt that adding <strong>great</strong> user management was too complex and too cumbersome.</p><p>The task came with a sense of helplessness. We knew what "great" looked like, but it was impractical to build all of that functionality.<br></p><figure id="w-node-a41d4bea-17b7-40a7-4d5c-8c6e66f6adf6-12ab58b9"><p><img src="https://uploads-ssl.webflow.com/5fd7bfc6e6f1ce2fd6ab58c5/6019dfd34486031760de5797_blog-google.png" loading="lazy" alt=""></p><figcaption>Google user management</figcaption></figure><p>We thought authentication-as-a-service vendors might ease our pain, but over and over again, we were disappointed by how much extra work was necessary. We never understood why until one friend quipped, <em>"auth-as-a-service really just solves half of 2-factor auth."</em></p><p>Then it clicked. <strong>We needed all of user management, not <em>just</em> authentication. </strong>And we realized if we could solve this problem, countless others could benefit, too.</p><p>Today, we're absolutely thrilled to launch Clerk: <strong>user management as-a-service</strong>. We're solving the whole problem, from frontend to backend, with beautiful UIs and elegant APIs. Our architecture pulls user management out of the way, so developers can focus on what makes their software truly special.</p><figure id="w-node-_139be728-0c3f-7abd-62db-458a61e69a47-12ab58b9"><p><img src="https://uploads-ssl.webflow.com/5fd7bfc6e6f1ce2fd6ab58c5/6019dff6fd671523538092e1_blog-clerk.png" loading="lazy" alt=""></p><figcaption>Clerk user management</figcaption></figure><p>Our launch today includes UI components for Sign Up, Sign In, User Profile, and what we're calling the "User Button." They can be mounted directly in your application, or you can redirect users to a Clerk-hosted page on <strong>accounts.yourdomain.com</strong>.</p><p>Best of all, the components will automatically update as our team optimizes their design, develops new features, and adds support for the latest in account security.</p><p>While today marks an exciting milestone for Clerk, this is truly just a "minimum viable product." The roadmap ahead will bring many features to better support developers and their end users:</p><ul role="list"><li>SDKs for additional languages and frameworks</li><li>Additional OpenID Connect and OAuth providers</li><li>Additional 2-step verification factors like TOTP and WebAuthN</li><li>Session management and revocation in the User Profile</li><li>Team management and enterprise authentication like SAML</li></ul><p>Need help with something you don't see listed? <a href="https://dashboard.clerk.dev/feedback">Make a request.</a></p></div></div>]]>
            </description>
            <link>https://clerk.dev/blog/all-of-user-management-not-just-authentication</link>
            <guid isPermaLink="false">hacker-news-small-sites-26069621</guid>
            <pubDate>Mon, 08 Feb 2021 20:25:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Scientists develop transparent wood that is stronger and lighter than glass]]>
            </title>
            <description>
<![CDATA[
Score 88 | Comments 29 (<a href="https://news.ycombinator.com/item?id=26068435">thread link</a>) | @ooboe
<br/>
February 8, 2021 | https://www.cbc.ca/radio/quirks/scientists-develop-transparent-wood-that-is-stronger-and-lighter-than-glass-1.5902739 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/radio/quirks/scientists-develop-transparent-wood-that-is-stronger-and-lighter-than-glass-1.5902739">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Bob McDonald's blog: A simple backyard procedure results in see-through wood with enormous potential as a building material.</p><div><figure><div><p><img loading="lazy" alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5902788.1612553036!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/transparent-wood-university-of-maryland.jpg"></p></div><figcaption>A flower is visible behind a piece of the transparent material.<!-- --> <!-- -->(Qinqin Xia, University of Maryland/Science Advances)</figcaption></figure><p><span><p>Researchers at the University of Maryland have turned ordinary sheets of wood into <a href="https://advances.sciencemag.org/content/7/5/eabd7342" target="_blank"><u>transparent material</u></a> that is nearly as clear as glass, but stronger and with better insulating properties. It could become an energy efficient building material in the future.</p>  <p>Wood is made of two basic ingredients: cellulose, which are tiny fibres, and lignin, which bonds those fibres together to give it strength.</p>  <p>Tear a paper towel in half and look closely along the edge. You will see the little cellulose fibres sticking up. Lignin is a glue-like material that bonds the fibres together, a little like the plastic resin in fibreglass or carbon fibre. The lignin also contains molecules called chromophores, which give the wood its brown colour and prevent light from passing through.</p>  <p>Early attempts to make transparent wood involved removing the lignin, but this involved hazardous chemicals, high temperatures and a lot of time, making the product expensive and somewhat brittle. The new technique is so cheap and easy it could literally be done in a backyard.</p>  <p>Starting with planks of wood a metre long and one millimetre thick, the scientists simply brushed on a solution of hydrogen peroxide using an ordinary paint brush. When left in the sun, or under a UV lamp for an hour or so, the peroxide bleached out the brown chromophores but left the lignin intact, so the wood turned white.</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5902789.1612553066!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/transparent-wood-university-of-maryland.jpg 300w,https://i.cbc.ca/1.5902789.1612553066!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/transparent-wood-university-of-maryland.jpg 460w,https://i.cbc.ca/1.5902789.1612553066!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/transparent-wood-university-of-maryland.jpg 620w,https://i.cbc.ca/1.5902789.1612553066!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/transparent-wood-university-of-maryland.jpg 780w,https://i.cbc.ca/1.5902789.1612553066!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/transparent-wood-university-of-maryland.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5902789.1612553066!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/transparent-wood-university-of-maryland.jpg"></p></div><figcaption>Researchers demonstrated after brushing a coat of hydrogen peroxide on the opaque wood material, and exposing it to one hour of sunlight, it turns transparent.<!-- --> <!-- -->(Qinqin Xia, University of Maryland/Science Advances)</figcaption></figure></span></p>  <p>Next, they infused the wood with a tough transparent epoxy designed for marine use, which filled in the spaces and pores in the wood and then hardened.&nbsp;This made the white wood transparent.</p>  <p>You can see a similar effect by taking that same piece of paper towel, dip half of it in water and place it on a patterned surface.&nbsp;The white paper towel will become translucent with light passing through the water and cellulose fibres without being scattered by refraction.</p>  <p>The epoxy in the wood does an even better job, allowing 90 per cent of visible light to pass through. The result is a long piece of what looks like glass, with the strength and flexibility of wood.</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5902790.1612552994!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/transparent-wood-usda-forest-service-jpg.jpg 300w,https://i.cbc.ca/1.5902790.1612552994!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/transparent-wood-usda-forest-service-jpg.jpg 460w,https://i.cbc.ca/1.5902790.1612552994!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/transparent-wood-usda-forest-service-jpg.jpg 620w,https://i.cbc.ca/1.5902790.1612552994!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/transparent-wood-usda-forest-service-jpg.jpg 780w,https://i.cbc.ca/1.5902790.1612552994!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/transparent-wood-usda-forest-service-jpg.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5902790.1612552994!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/transparent-wood-usda-forest-service-jpg.jpg"></p></div><figcaption>A researcher holds up a square of transparent wood material against a green leaf.<!-- --> <!-- -->(USDA Forest Service)</figcaption></figure></span></p>  <p>As window material, it would be much more resistant to accidental breakage. The clear wood&nbsp;is lighter than glass, with better insulating properties, which is important because windows are a major source of heat loss in buildings. It also might take less energy to manufacture clear wood because there are no high temperatures involved.</p>  <p>Transparent wood could become an alternative to glass in energy efficient buildings, or perhaps coverings for solar panels in harsh environments. There could be no end of uses.</p>      <p>Many different types of wood, from balsa to oak, can be made transparent, and it doesn't matter if it is cut along the grain or against it. If the transparent wood is made a little thicker, it would be strong enough to become part of the structure of a building, so there could be entire transparent wooden walls.</p>  <p>While this technology has yet to&nbsp; be scaled up to industrial levels, the researchers say it has great potential as a new building material. In fact, they say that theoretically, an entire house could be made transparent. It is not clear why anyone would want to live in a transparent house, but for people who do, it would be OK to throw stones…</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5902787.1612553101!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/transparent-wood-qinqin-xia-university-of-maryland.jpg 300w,https://i.cbc.ca/1.5902787.1612553101!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/transparent-wood-qinqin-xia-university-of-maryland.jpg 460w,https://i.cbc.ca/1.5902787.1612553101!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/transparent-wood-qinqin-xia-university-of-maryland.jpg 620w,https://i.cbc.ca/1.5902787.1612553101!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/transparent-wood-qinqin-xia-university-of-maryland.jpg 780w,https://i.cbc.ca/1.5902787.1612553101!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/transparent-wood-qinqin-xia-university-of-maryland.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5902787.1612553101!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/transparent-wood-qinqin-xia-university-of-maryland.jpg"></p></div><figcaption>A researcher holds up a 10-centimetre long slab of the see-through wood.<!-- --> <!-- -->(Qinqin Xia, University of Maryland/Science Advances)</figcaption></figure></span></p>  <p>Images copyright Xia et al.&nbsp;<a href="https://creativecommons.org/licenses/by-nc/4.0/" rel="license">Creative Commons Attribution-NonCommercial license</a>,&nbsp;</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/radio/quirks/scientists-develop-transparent-wood-that-is-stronger-and-lighter-than-glass-1.5902739</link>
            <guid isPermaLink="false">hacker-news-small-sites-26068435</guid>
            <pubDate>Mon, 08 Feb 2021 18:50:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Automatic differentiation does incur truncation errors (kinda)]]>
            </title>
            <description>
<![CDATA[
Score 83 | Comments 42 (<a href="https://news.ycombinator.com/item?id=26068382">thread link</a>) | @oxinabox
<br/>
February 8, 2021 | https://www.oxinabox.net/2021/02/08/AD-truncation-error.html | <a href="https://web.archive.org/web/*/https://www.oxinabox.net/2021/02/08/AD-truncation-error.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main_content">
      <p>Griewank and Walther’s 0th Rule of algorithmic differentiation (AD) states:</p>

<blockquote>
  <p>Algorithmic differentiation does not incur truncation error.</p>
</blockquote>

<p>(<a href="https://dl.acm.org/doi/book/10.5555/1455489">2008, “Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation”, Andreas Griewank and Andrea Walther.</a>)</p>

<p>In this blog post I will show you a case that looks like it does in fact incur truncation error.
Though this case will arguably be a misinterpretation of that rule.
This blog post will thus highlight why careful interpretation of the rule is necessary.
Further it will motivate why we need to often add more custom sensitivity rules (custom primitives) to our AD systems, even though you can AD anything with just a few basic rules.</p>

<p>Credit to <a href="https://mikeinnes.github.io/">Mike Innes</a> who pointed this out to me at JuliaCon 2018.
<!--more--></p>



<p>We will start by implementing a simple forwards mode AD.
This implementation is based on <a href="https://juliadiff.org/ChainRulesCore.jl/dev/autodiff/operator_overloading.html#ForwardDiffZero">ForwardDiffZero from the ChainRules docs.</a>, but without ChainRules support.
Though it is also the simplest most stock-standard implementation once can conceive of.</p>

<div><div><pre><code><span>struct</span><span> Dual</span> <span>&lt;:</span> <span>Number</span>
    <span>primal</span><span>::</span><span>Float64</span>
    <span>partial</span><span>::</span><span>Float64</span>
<span>end</span>

<span>primal</span><span>(</span><span>d</span><span>::</span><span>Dual</span><span>)</span> <span>=</span> <span>d</span><span>.</span><span>primal</span>
<span>partial</span><span>(</span><span>d</span><span>::</span><span>Dual</span><span>)</span> <span>=</span> <span>d</span><span>.</span><span>partial</span>

<span>primal</span><span>(</span><span>d</span><span>::</span><span>Number</span><span>)</span> <span>=</span> <span>d</span>
<span>partial</span><span>(</span><span>d</span><span>::</span><span>Number</span><span>)</span> <span>=</span> <span>0.0</span>

<span>function</span><span> Base.:+</span><span>(</span><span>a</span><span>::</span><span>Union</span><span>{</span><span>Dual</span><span>,</span> <span>T</span><span>},</span> <span>b</span><span>::</span><span>Union</span><span>{</span><span>Dual</span><span>,</span> <span>T</span><span>})</span> <span>where</span> <span>T</span><span>&lt;:</span><span>Real</span>
    <span>return</span> <span>Dual</span><span>(</span><span>primal</span><span>(</span><span>a</span><span>)</span><span>+</span><span>primal</span><span>(</span><span>b</span><span>),</span> <span>partial</span><span>(</span><span>a</span><span>)</span><span>+</span><span>partial</span><span>(</span><span>b</span><span>))</span>
<span>end</span>

<span>function</span><span> Base.:</span><span>-</span><span>(</span><span>a</span><span>::</span><span>Union</span><span>{</span><span>Dual</span><span>,</span> <span>T</span><span>},</span> <span>b</span><span>::</span><span>Union</span><span>{</span><span>Dual</span><span>,</span> <span>T</span><span>})</span> <span>where</span> <span>T</span><span>&lt;:</span><span>Real</span>
    <span>return</span> <span>Dual</span><span>(</span><span>primal</span><span>(</span><span>a</span><span>)</span><span>-</span><span>primal</span><span>(</span><span>b</span><span>),</span> <span>partial</span><span>(</span><span>a</span><span>)</span><span>-</span><span>partial</span><span>(</span><span>b</span><span>))</span>
<span>end</span>

<span>function</span><span> Base.:*</span><span>(</span><span>a</span><span>::</span><span>Union</span><span>{</span><span>Dual</span><span>,</span> <span>T</span><span>},</span> <span>b</span><span>::</span><span>Union</span><span>{</span><span>Dual</span><span>,</span> <span>T</span><span>})</span> <span>where</span> <span>T</span><span>&lt;:</span><span>Real</span>
    <span>return</span> <span>Dual</span><span>(</span>
        <span>primal</span><span>(</span><span>a</span><span>)</span><span>*</span><span>primal</span><span>(</span><span>b</span><span>),</span>
        <span>partial</span><span>(</span><span>a</span><span>)</span><span>*</span><span>primal</span><span>(</span><span>b</span><span>)</span> <span>+</span> <span>primal</span><span>(</span><span>a</span><span>)</span><span>*</span><span>partial</span><span>(</span><span>b</span><span>)</span>
    <span>)</span>
<span>end</span>

<span>function</span><span> Base.:/</span><span>(</span><span>a</span><span>::</span><span>Union</span><span>{</span><span>Dual</span><span>,</span> <span>T</span><span>},</span> <span>b</span><span>::</span><span>Union</span><span>{</span><span>Dual</span><span>,</span> <span>T</span><span>})</span> <span>where</span> <span>T</span><span>&lt;:</span><span>Real</span>
    <span>return</span> <span>Dual</span><span>(</span>
        <span>primal</span><span>(</span><span>a</span><span>)</span><span>/</span><span>primal</span><span>(</span><span>b</span><span>),</span>
        <span>(</span><span>partial</span><span>(</span><span>a</span><span>)</span><span>*</span><span>primal</span><span>(</span><span>b</span><span>)</span> <span>-</span> <span>primal</span><span>(</span><span>a</span><span>)</span><span>*</span><span>partial</span><span>(</span><span>b</span><span>))</span> <span>/</span> <span>primal</span><span>(</span><span>b</span><span>)</span><span>^</span><span>2</span>
    <span>)</span>
<span>end</span>

<span># needed for `^` to work from having `*` defined</span>
<span>Base</span><span>.</span><span>to_power_type</span><span>(</span><span>x</span><span>::</span><span>Dual</span><span>)</span> <span>=</span> <span>x</span>


<span>"Do a calculus. `f` should have a single input."</span>
<span>function</span><span> derv</span><span>(</span><span>f</span><span>,</span> <span>arg</span><span>)</span>
    <span>duals</span> <span>=</span> <span>Dual</span><span>(</span><span>arg</span><span>,</span> <span>1.0</span><span>)</span>
    <span>return</span> <span>partial</span><span>(</span><span>f</span><span>(</span><span>duals</span><span>...</span><span>))</span>
<span>end</span>
</code></pre></div></div>

<p>We can try out this AD and see that it works.</p>
<div><div><pre><code><span>julia</span><span>&gt;</span> <span>foo</span><span>(</span><span>x</span><span>)</span> <span>=</span> <span>x</span><span>^</span><span>3</span> <span>+</span> <span>x</span><span>^</span><span>2</span> <span>+</span> <span>1</span><span>;</span>

<span>julia</span><span>&gt;</span> <span>derv</span><span>(</span><span>foo</span><span>,</span> <span>20.0</span><span>)</span>
<span>1240.0</span>

<span>julia</span><span>&gt;</span> <span>3</span><span>*</span><span>(</span><span>20.0</span><span>)</span><span>^</span><span>2</span> <span>+</span> <span>2</span><span>*</span><span>(</span><span>20.0</span><span>)</span>
<span>1240.0</span>
</code></pre></div></div>

<h2 id="2-implement-sin-and-cos-for-demonstration-purposes">2. Implement Sin and Cos, for demonstration purposes</h2>

<p>Now we are going to implement the <code>sin</code> and <code>cos</code> functions for demonstration purposes.
<a href="https://github.com/JuliaLang/julia/blob/v1.5.3/base/special/trig.jl">JuliaLang has an implementation of <code>sin</code> and <code>cos</code> in Julia</a>, that could be ADed though by a source to source AD (like <a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a>).
But because it is restricted to <code>Float32</code> and <code>Float64</code> an operator overloading AD like ours can’t be used with it.
That’s Ok we will just code up a simple one using Taylor polynomials.
we know that eventually the code run does have to look something like this, since all operations are implemented in terms of <code>+</code>, <code>*</code> and <code>^</code>, <code>/</code>, bit-shifts and control-flow.
(Technically <a href="https://en.wikipedia.org/wiki/X86_instruction_listings#Added_with_80387">x86 assembly</a> does have a primitive for <code>sin</code> and <code>cos</code> but as far as I know no LibM actually uses them. There is a <a href="https://reviews.llvm.org/D36344">discussion of why LLVM</a> doesn’t ever emit them if you go looking)
The real code would include control flow to wrap around large values and stay close to zero, but we can skip that and just avoid inputting large values.</p>

<p>So using Taylor polynomials of degree 12 for each of them:</p>

<div><div><pre><code><span>my_sin</span><span>(</span><span>x</span><span>)</span> <span>=</span> <span>x</span> <span>-</span> <span>x</span><span>^</span><span>3</span><span>/</span><span>factorial</span><span>(</span><span>3</span><span>)</span> <span>+</span> <span>x</span><span>^</span><span>5</span><span>/</span><span>factorial</span><span>(</span><span>5</span><span>)</span> <span>-</span> <span>x</span><span>^</span><span>7</span><span>/</span><span>factorial</span><span>(</span><span>7</span><span>)</span> <span>+</span> <span>x</span><span>^</span><span>9</span><span>/</span><span>factorial</span><span>(</span><span>9</span><span>)</span> <span>-</span> <span>x</span><span>^</span><span>11</span><span>/</span><span>factorial</span><span>(</span><span>11</span><span>)</span>  <span># + 0*x^12</span>

<span>my_cos</span><span>(</span><span>x</span><span>)</span> <span>=</span> <span>1</span> <span>-</span> <span>x</span><span>^</span><span>2</span><span>/</span><span>factorial</span><span>(</span><span>2</span><span>)</span> <span>+</span> <span>x</span><span>^</span><span>4</span><span>/</span><span>factorial</span><span>(</span><span>4</span><span>)</span> <span>-</span> <span>x</span><span>^</span><span>6</span><span>/</span><span>factorial</span><span>(</span><span>6</span><span>)</span> <span>+</span> <span>x</span><span>^</span><span>8</span><span>/</span><span>factorial</span><span>(</span><span>8</span><span>)</span> <span>-</span> <span>x</span><span>^</span><span>10</span><span>/</span><span>factorial</span><span>(</span><span>10</span><span>)</span> <span>+</span> <span>x</span><span>^</span><span>12</span><span>/</span><span>factorial</span><span>(</span><span>12</span><span>)</span>
</code></pre></div></div>

<p>Check the accuracy
we know that  <code>sin(π/3) == √3/2</code>, and that <code>cos(π/3) == 1/2</code>
(note that yes, <code>π</code> and <code>√3/2</code> are  approximating here but they are very accurate one. And this doesn’t change the result that follows.)</p>
<div><div><pre><code><span>julia</span><span>&gt;</span> <span>my_sin</span><span>(</span><span>π</span><span>/</span><span>3</span><span>)</span>
<span>0.8660254034934827</span>

<span>julia</span><span>&gt;</span> <span>√3</span><span>/</span><span>2</span>
<span>0.8660254037844386</span>

<span>julia</span><span>&gt;</span> <span>abs</span><span>(</span><span>√3</span><span>/</span><span>2</span> <span>-</span> <span>my_sin</span><span>(</span><span>π</span><span>/</span><span>3</span><span>))</span>
<span>2.9095592601890985e-10</span>

<span>julia</span><span>&gt;</span> <span>my_cos</span><span>(</span><span>π</span><span>/</span><span>3</span><span>)</span>
<span>0.5000000000217777</span>

<span>julia</span><span>&gt;</span> <span>abs</span><span>(</span><span>0.5</span> <span>-</span> <span>my_cos</span><span>(</span><span>π</span><span>/</span><span>3</span><span>))</span>
<span>2.177769076183722e-11</span>
</code></pre></div></div>

<p>This is not terrible.
<code>cos</code> is slightly more accurate than <code>sin</code>. 
We have a fairly passable implementation of <code>sin</code> and <code>cos</code>.</p>

<h2 id="3-now-lets-do-ad">3. Now lets do AD.</h2>

<p>We know the derivative of <code>sin(x)</code> is <code>cos(x)</code>.
So if we take the derivative of <code>my_sin(π/3)</code> we should get <code>my_cos(π/3)≈0.5</code>.
<em>It should be as accurate as the original implementation, right?</em>
because Griewank and Walther’s 0th Rule:</p>

<blockquote>
  <p>Algorithmic differentiation does not incur truncation error.</p>
</blockquote>

<div><div><pre><code><span>julia</span><span>&gt;</span> <span>derv</span><span>(</span><span>my_sin</span><span>,</span> <span>π</span><span>/</span><span>3</span><span>)</span>
<span>0.4999999963909431</span>
</code></pre></div></div>
<p>Wait a second.
That doesn’t seem accurate, we expected 0.5, or at least something pretty close to that.
<code>my_cos</code> was accurate to $2 \times 10^{-11}$.
<code>my_sin</code> was accurate to $3 \times 10^{-10}$
How accurate is this:</p>
<pre><code>julia&gt; abs(derv(my_sin, π/3) - 0.5)
3.609056886677564e-9
</code></pre>

<p>What went wrong?</p>

<h2 id="4-verify">4. Verify</h2>

<p>Now, I did implement an AD from scratch there.
So maybe you are thinking that I screwed it up.
Maybe a reverse mode AD would not suffer from this problem; or maybe one that uses source to source?
Lets try some of Julia’s many AD systems then.</p>

<div><div><pre><code><span>julia</span><span>&gt;</span> <span>import</span> <span>ForwardDiff</span><span>,</span> <span>ReverseDiff</span><span>,</span> <span>Nabla</span><span>,</span> <span>Yota</span><span>,</span> <span>Zygote</span><span>,</span> <span>Tracker</span><span>,</span> <span>Enzyme</span><span>;</span>

<span>julia</span><span>&gt;</span> <span>ForwardDiff</span><span>.</span><span>derivative</span><span>(</span><span>my_sin</span><span>,</span> <span>π</span><span>/</span><span>3</span><span>)</span>
<span>0.4999999963909432</span>

<span>julia</span><span>&gt;</span> <span>ReverseDiff</span><span>.</span><span>gradient</span><span>(</span><span>x</span><span>-&gt;</span><span>my_sin</span><span>(</span><span>x</span><span>[</span><span>1</span><span>]),</span> <span>[</span><span>π</span><span>/</span><span>3</span><span>,])</span>
<span>1</span><span>-</span><span>element</span> <span>Vector</span><span>{</span><span>Float64</span><span>}</span><span>:</span>
 <span>0.4999999963909433</span>

<span>julia</span><span>&gt;</span> <span>Nabla</span><span>.</span><span>∇</span><span>(</span><span>my_sin</span><span>)(</span><span>π</span><span>/</span><span>3</span><span>)</span>
<span>(</span><span>0.4999999963909433</span><span>,)</span>

<span>julia</span><span>&gt;</span> <span>Yota</span><span>.</span><span>grad</span><span>(</span><span>my_sin</span><span>,</span> <span>π</span><span>/</span><span>3</span><span>)[</span><span>2</span><span>][</span><span>1</span><span>]</span>
<span>0.4999999963909433</span>

<span>julia</span><span>&gt;</span> <span>Zygote</span><span>.</span><span>gradient</span><span>(</span><span>my_sin</span><span>,</span> <span>π</span><span>/</span><span>3</span><span>)</span>
<span>(</span><span>0.4999999963909433</span><span>,)</span>

<span>julia</span><span>&gt;</span> <span>Tracker</span><span>.</span><span>gradient</span><span>(</span><span>my_sin</span><span>,</span> <span>π</span><span>/</span><span>3</span><span>)</span>
<span>(</span><span>0.4999999963909432</span> <span>(</span><span>tracked</span><span>),)</span>

<span>julia</span><span>&gt;</span> <span>Enzyme</span><span>.</span><span>autodiff</span><span>(</span><span>my_sin</span><span>,</span> <span>Active</span><span>(</span><span>π</span><span>/</span><span>3</span><span>))</span>
<span>0.4999999963909432</span>
</code></pre></div></div>

<p>Ok, I just tried <strong>7</strong> AD systems based on totally different implementations.
I mean <a href="https://github.com/wsmoses/Enzyme.jl/">Enzyme</a> is reverse mode running at the LLVM level.
Totally different from <a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff</a> which is the more mature version of the forward mode operator overloading AD I coded above.
Every single one agreed with my result, up to <a href="https://en.wikipedia.org/wiki/Unit_in_the_last_place">1 ULP</a>.
I think that last digit changing is probably to do with order of addition (IEEE floating point math is funky), but that is another blog-post.
So I think we can reliably say that this is what an AD system will output when asked for the derivative of <code>my_sin</code> at <code>π/3</code>.</p>

<h2 id="5-explanation">5. Explanation</h2>

<p>Why does AD seem to be incurring truncation errors?
Why is the derivative of <code>my_sin</code> much less accurate than <code>my_cos</code>?</p>

<p>The AD system is (as you might have surmised) not incurring truncation errors.
It is giving us exactly what we asked for, which is the derivative of <code>my_sin</code>.
<code>my_sin</code> is a polynomial.
The derivative of the polynomial is:</p>
<div><div><pre><code><span>d_my_sin</span><span>(</span><span>x</span><span>)</span> <span>=</span> <span>1</span> <span>-</span> <span>3</span><span>x</span><span>^</span><span>2</span><span>/</span><span>factorial</span><span>(</span><span>3</span><span>)</span> <span>+</span> <span>5</span><span>x</span><span>^</span><span>4</span><span>/</span><span>factorial</span><span>(</span><span>5</span><span>)</span> <span>-</span> <span>7</span><span>x</span><span>^</span><span>6</span><span>/</span><span>factorial</span><span>(</span><span>7</span><span>)</span> <span>+</span> <span>9</span><span>x</span><span>^</span><span>8</span><span>/</span><span>factorial</span><span>(</span><span>9</span><span>)</span> <span>-</span> <span>11</span><span>x</span><span>^</span><span>10</span><span>/</span><span>factorial</span><span>(</span><span>11</span><span>)</span>
</code></pre></div></div>
<p>which indeed does have</p>
<div><div><pre><code><span>julia</span><span>&gt;</span> <span>d_my_sin</span><span>(</span><span>π</span><span>/</span><span>3</span><span>)</span>
<span>0.4999999963909432</span>
</code></pre></div></div>
<p><code>d_my_sin</code> is a lower degree polynomial approximation to <code>cos</code> than <code>my_cos</code> was, so it is less accurate.
Further, you can see that while n-derivative of <code>sin</code> is always defined as <code>sin(x+n*π/2)</code>, as we keep taking derivatives of the polynomial approximations terms keep getting dropped.
AD is making it smoother and smoother til it is just a flat <code>0</code>.</p>

<p>The key take away here is that <em>the map is not the territory</em>.
Most nontrivial functions on computers are implemented as some function that that approximates (<em>the map</em>) the mathematical ideal (<em>the territory</em>).
Automatic differentiation gives back a completely accurate derivative of the that function (<em>the map</em>) doing the approximation.
<em>Furthermore, the accurate derivative of an approximation to the idea (e.g <code>d_my_sin</code>), is less accurate than and approximation to the (ideal) derivative of the ideal (e.g. <code>my_cos</code>).</em>
There is no truncation error in the work the AD did; but there is a truncation error in the sense that we are now using a more truncated approximation that we would write ourselves.</p>

<p>So what do?
Well-firstly, do you want to do anything?
Maybe the derivative of the approximation is more useful.
(I have been told that this is the case for some optimal control problems).
But if we want to fix it can we?
Yes, the answer is to insert domain knowledge, telling the AD system directly what the approximation to the derivative of the ideal is.
The AD system doesn’t know its working with an approximation, and even if it did, it doesn’t know what the idea it is approximating is.
The way to tell it is with a custom primitive i.e. a custom sensitivity rule.
This is what the ChainRules project in Julia is about, being able to add custom primitives for more things.</p>

<p>Every real AD system already has a primitive for <code>sin</code> built in
(which is one of the reasons I had to define my own above).
but it won’t have one for every novel system you approximate.
E.g. for things defined in terms of differential equation solutions or other iterative methods.</p>

<p>We can define in our toy AD at the start this custom primitive via:</p>
<div><div><pre><code><span>function</span><span> my_sin</span><span>(</span><span>x</span><span>::</span><span>Dual</span><span>)</span>
    <span>return</span> <span>Dual</span><span>(</span><span>my_sin</span><span>(</span><span>primal</span><span>(</span><span>x</span><span>)),</span> <span>partial</span><span>(</span><span>x</span><span>)</span> <span>*</span> <span>my_cos</span><span>(</span><span>primal</span><span>(</span><span>x</span><span>)))</span>
<span>end</span>
</code></pre></div></div>
<p>and it does indeed fix it.</p>
<div><div><pre><code><span>julia</span><span>&gt;</span> <span>derv</span><span>(</span><span>my_sin</span><span>,</span> <span>π</span><span>/</span><span>3</span><span>)</span>
<span>0.5000000000217777</span>

<span>julia</span><span>&gt;</span> <span>abs</span><span>(</span><span>derv</span><span>(</span><span>my_sin</span><span>,</span> <span>π</span><span>/</span><span>3</span><span>)</span> <span>-</span> <span>0.5</span><span>)</span>
<span>2.177769076183722e-11</span>
</code></pre></div></div>

<h3 id="bonus-will-symbolic-differentiation-save-me">Bonus: will symbolic differentiation save me?</h3>

<p>Most symbolic differentiation systems will have a rule just like the custom primitive for <code>sin</code> built in.
It basically has to do something like this, where-as a forward/reverse AD system could do as we did and fall back to <code>+</code> and <code>*</code>.
<em>But</em>, it certainly would <em>not</em> help you to apply symbolic AD to an approximation, that would give exactly the result we derived for <code>d_sin</code> above.</p>

<p>More interestingly, languages where symbolic differentiation is common tend also th have interesting representations of functions in the first place.
This does open up avenues for interesting solutions.
A suitably weird language could be using representation of <code>sin</code> that is a lazily evaluated polynomial of infinite degree underneath.
And in that case there is a rule for its derivative, expressed in terms of changes to its coefficient generating function; which would also give back a lazily evaluated polynomial.
I don’t know if anyone does that though; I suspect it doesn’t generalized well.
Further, for a lot of things you want to solving systems via iterative methods, and these …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.oxinabox.net/2021/02/08/AD-truncation-error.html">https://www.oxinabox.net/2021/02/08/AD-truncation-error.html</a></em></p>]]>
            </description>
            <link>https://www.oxinabox.net/2021/02/08/AD-truncation-error.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26068382</guid>
            <pubDate>Mon, 08 Feb 2021 18:45:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My Opinion on Blockchain]]>
            </title>
            <description>
<![CDATA[
Score 61 | Comments 104 (<a href="https://news.ycombinator.com/item?id=26067770">thread link</a>) | @hoenir
<br/>
February 8, 2021 | https://vladcalin.ro/blog/2021-02-07-about-blockchain | <a href="https://web.archive.org/web/*/https://vladcalin.ro/blog/2021-02-07-about-blockchain">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>This post was inspired by a question on Reddit: How much of your investment portofolio is crypto?</p><p>Well, my answer is 0%, and it will remain 0% forever.</p><p>I don't see the value in it, and the people who advocate for them are hypocrites: they say they get into
crypto as a rebellion act against the current FIAT system that is controlled by governments, while
day-dreaming about selling it for a huge profit (in FIAT currency).
Every advocate promotes holding, which is not what a currency is for: currencies are meant to be
a medium of exchange and to change hands.</p><p>Also, I see rampant misunderstanding of basic economics in people who advocate for it:</p><ul><li><p>they say that the fact that it is deflationary and can not see inflation is a good thing, when it is
actually a bad thing. There is a reason the target inflation is at ~2% almost everywhere.
We need inflation to keep the economy going.</p></li><li><p>they say that it's a currency, when it's not. It is not accessible to everybody
(not everybody has access to internet).</p></li><li><p>if you lose your key, you lose the access to your wallet for good and nobody can help you.
People seem to not understand how bad this is. Imagine people losing everything because of one silly mistake.</p></li><li><p>Volatility is the enemy of any economy market: we need stability and predictability to avoid mayhem:
businesses need predictability to prepare their stocks, factories need predictability to produce goods,
people need predictability to budget and make long-term plans. Sure, long term plans usually don't go as planned,
but you don't want to rethink your entire strategy on a weekly basis because of some wild fluctuation (this week a
car costs 0.5BTC, next week it will be 5BTC). Because of volatility you can't even save for the future.</p></li></ul><p>Also, as a technology, blockchain itself it's not that impressive. At its core, it is a decentralized
slow read-only database which needs as lot of power to run, which do not have many real-life useful applications.
Yeah, some would argue that having a decentralized zero-trust database would be an advantage, but
I am yet to see a system using blockchain, where a normal database is not enough.</p><p>The only real-life application I could see is in audit trailing, where the decentralization part
is useless (only the chaining of blocks by hashing is useful there, to avoid tampering with past entries),
but that use-case is not related to the reason blockchain advocates promote it.</p><p>Every time some company announces that they will build their own blockchain solution, the blockchain
community celebrates, because they consider it to be proof that blockchain is the future, but in fact,
it's a massive counter-argument against the core belief they promote: decentralization.
The fact that a company is building their own, it means that they will control it.</p><h3>Conclusion</h3><p>In my opinion, Bitcoin is a fad and it's value is given by two things: people who have too much faith in an asset that
is not really an asset and has no use except offering a field for speculation, and the FOMO of people who get caught in
the heat of the moment and buy when a friend tells them to buy or mass-media brings it to the public attention
once very few months.</p><p>This article is a personal opinion, and represents no financial advice. I am in not saying "sell" or "buy".
Do your own research (thoroughly).</p></div></div></div>]]>
            </description>
            <link>https://vladcalin.ro/blog/2021-02-07-about-blockchain</link>
            <guid isPermaLink="false">hacker-news-small-sites-26067770</guid>
            <pubDate>Mon, 08 Feb 2021 17:54:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Reminder: Terraria Dev’s Google Story Is Not Unusual at All]]>
            </title>
            <description>
<![CDATA[
Score 314 | Comments 8 (<a href="https://news.ycombinator.com/item?id=26067188">thread link</a>) | @teddyh
<br/>
February 8, 2021 | https://codewriteplay.com/2021/02/08/reminder-terraria-devs-google-story-is-not-unusual-at-all/ | <a href="https://web.archive.org/web/*/https://codewriteplay.com/2021/02/08/reminder-terraria-devs-google-story-is-not-unusual-at-all/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<div>

			
<p> <em>Terraria</em> co-creator Andrew Spinks is the latest high-profile victim of Google’s ban-first, answer-questions-never approach to creators on its platform, but the problem goes back over a decade. </p>






<p>If you’re new, welcome! You might like our companion podcast, <a href="https://codewriteplay.com/the-gamedev-breakdown-podcast/" target="_blank" rel="noreferrer noopener">GameDev Breakdown</a>.</p>



<p>In a recent thread on Twitter, Spinks tagged Google in an explanation of his situation. In short: his accounts have been locked for weeks, and no one will tell him why. </p>



<blockquote data-width="550" data-dnt="true"><p lang="en" dir="ltr"><a href="https://twitter.com/Google?ref_src=twsrc%5Etfw">@Google</a> my account has now been disabled for over 3 weeks. I still have no idea why, and after using every resource I have to get this resolved you have done nothing but given me the runaround.</p>— Andrew Spinks (@Demilogic) <a href="https://twitter.com/Demilogic/status/1358661840402845696?ref_src=twsrc%5Etfw">February 8, 2021</a></blockquote>



<p>Like countless internet users, Spinks had loads of money and data under Google’s control in the form of Google Play apps, Google Drive space, YouTube content, and his Gmail account. </p>



<p>The thread’s replies are packed with users who share similar stories.</p>



<blockquote data-width="550" data-dnt="true"><p lang="en" dir="ltr">The same happened to me a few months ago. Any appeal is met with automatic denials and just "you violated terms of service." <a href="https://t.co/W7Ot6c5Wqt">https://t.co/W7Ot6c5Wqt</a></p>— Cleroth⭐ (@Cleroth) <a href="https://twitter.com/Cleroth/status/1358744907188498433?ref_src=twsrc%5Etfw">February 8, 2021</a></blockquote>



<p>Spinks’ story is frustrating, but it’s far from new. For nearly as long as Google has offered user accounts, users have shared stories of mysterious ban notices and lockouts with vague—if any—reasoning. Although there is an appeals process in the case of bans, submissions are commonly rejected without revealing any additional significant information. </p>



<p>In Google’s earliest days, this phenomenon was tied almost exclusively to webmasters and bloggers using the AdSense system (this is where yours truly was personally <a rel="noreferrer noopener" href="https://codewriteplay.com/2015/12/28/invalid-activity-my-google-adsense-horror-story/" target="_blank">banned for life</a> years ago now). As big as Google is, it seems to consistently do more advertising business than it can apparently keep track of, so it seems to rely on a largely automated system of flagging what it deems “suspicious” or “invalid” activity on its ads, routinely banning the responsible accounts. </p>



<p>But the word “responsible” is interesting in the context of ads accessible to the entire internet. In effect, anyone can go on a clicking spree on anyone’s AdSense ads with a decent chance of getting the responsible account banned indefinitely. This has led to a loss of tons of advertising revenue for creators likely in no way responsible for their removal. This nonsensical system has become so well-known that a cottage industry for <a href="https://krebsonsecurity.com/2020/02/pay-up-or-well-make-google-ban-your-ads/">Adsense Extortion</a> eventually sprung up targeting site owners. The threat: pay us or we’ll get you banned. </p>


		


<p>Instead of improving, Google has taken over more of the internet, and the lazy bans have spread. YouTube creators are now at the mercy of the very same system that once only plagued us webmaster nerds. Anyone is capable of the very same “invalid click activity” on the very same ads, and the <a rel="noreferrer noopener" href="https://www.polygon.com/2015/1/20/7856497/google-youtube-adsense-kafka" target="_blank">results are identical</a>.</p>



<p>Spinks’ lockout seems more security-related. It’s unlikely he does a lot with AdSense, and ad bans rarely result in a full account lockout. However, this doesn’t explain why Google has left him with no idea what happened for most of a month now. He’s absolutely right in saying Google is a risk to his business. </p>



<blockquote data-width="550" data-dnt="true"><p lang="en" dir="ltr">I will not be involved with a corporation that values their customers and partners so little. Doing business with you is a liability.</p>— Andrew Spinks (@Demilogic) <a href="https://twitter.com/Demilogic/status/1358661843192012801?ref_src=twsrc%5Etfw">February 8, 2021</a></blockquote>



<p>While no one is in a position to <em>fully </em>ruin Google’s day, Spinks was in a better position to give them something to think about than the rest of us. </p>



<blockquote data-width="550" data-dnt="true"><p lang="en" dir="ltr">I absolutely have not done anything to violate your terms of service, so I can take this no other way than you deciding to burn this bridge. Consider it burned. <a href="https://twitter.com/hashtag/Terraria?src=hash&amp;ref_src=twsrc%5Etfw">#Terraria</a> for <a href="https://twitter.com/GoogleStadia?ref_src=twsrc%5Etfw">@GoogleStadia</a> is canceled. My company will no longer support any of your platforms moving forward.</p>— Andrew Spinks (@Demilogic) <a href="https://twitter.com/Demilogic/status/1358661842147692549?ref_src=twsrc%5Etfw">February 8, 2021</a></blockquote>



<p>Google has been known to reach out in a very limited number of high-profile cases like this to resolve issues, but it’s frankly a bad look at this point whether they do or don’t. I would never call on a dev to make good on a threat to cancel a game, but I hope he’ll continue to advocate for the countless creators who have no leverage with the internet’s premiere provider for synchronized productivity and advertising revenue solutions. </p>
		
		
		

		</div><!-- .entry-content -->

	</div></div>]]>
            </description>
            <link>https://codewriteplay.com/2021/02/08/reminder-terraria-devs-google-story-is-not-unusual-at-all/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26067188</guid>
            <pubDate>Mon, 08 Feb 2021 17:08:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Using computer vision to help win $1M in Mountain Dew's Super Bowl contest]]>
            </title>
            <description>
<![CDATA[
Score 231 | Comments 92 (<a href="https://news.ycombinator.com/item?id=26066970">thread link</a>) | @rocauc
<br/>
February 8, 2021 | https://blog.roboflow.com/mountain-dew-contest-computer-vision/ | <a href="https://web.archive.org/web/*/https://blog.roboflow.com/mountain-dew-contest-computer-vision/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
              <div>
                <div><p>Last night during Super Bowl LV, Mountain Dew ran an ad featuring John Cena riding through a Mountain Dew-themed amusement park. Bottles are scattered all over the scene: neon signs on buildings, in fun house mirrors, and flying out of the car trunk.</p><p>At the end of the ad, John Cena challenges the audience: <strong>The first person to tweet at Mountain Dew the exact number of bottles that appear in the commercial is eligible to win $1 million.</strong></p><p>Watch the ad for yourself here:</p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/9cEiYQwYLPk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><p>When we heard that a task called for careful visual inspection and counting, we knew <a href="https://blog.roboflow.com/intro-to-computer-vision/">computer vision</a> would be a helpful tool. So, we did what any developer would do: trained an <a href="https://models.roboflow.com/object-detection">object detection model</a> to recognize bottles that appear throughout the scene. </p><figure><img src="https://blog.roboflow.com/content/images/2021/02/Winning_1_Million_with_Computer_Vision_Mountain_Dew_Super_Bowl_Contest.gif" alt="Counting Mountain Dew Contest with Computer Vision"><figcaption>A three-second example of the model finding Mountain Dew bottles in the scene.</figcaption></figure><p>In this case, <strong>we're using a <a href="https://models.roboflow.com/">computer vision model</a> to help us find any bottles we may have otherwise missed.</strong> The viewer should still identify the unique occurrences of each bottle across the scene when tweeting a submission.</p><p>Per the <a href="https://www.lifechangingdew.com/rules">Official Rules</a>, any type of bottle counts – but each bottle should only be counted once. (For example, the bottle in the car John Cena drinks from is present multiple different times, but it should only be counted once towards the tally.) </p><p>Let's dive in.</p><h3 id="preparing-a-dataset">Preparing a Dataset</h3><p>First, we need a dataset of images from the ad. In this case, we can grab the exact video file of the commercial. We'll need to <a href="https://blog.roboflow.com/using-video-computer-vision/">split the video file into individual image frames</a> in order to annotate the images and train a model.</p><p>I created a dataset and dropped the Mountain Dew video into Roboflow, which asks what frame rate I'd like to sample. I decided on doing three frames per second, which creates 92 images from the roughly 30-second Super Bowl spot.</p><figure><img src="https://blog.roboflow.com/content/images/2021/02/Winning_1_Million_with_Computer_Vision_Mountain_Dew_Super_Bowl_Contest-upload.gif" alt="Turning video into images for computer vision."><figcaption>Selecting a three frames per second rate for the commercial.</figcaption></figure><p>Having each of the individual frames from the video is independently helpful: it means we can have a closer look at all of the places where the Mountain Dew bottles may be present.</p><p>Once we have these images, we need to <a href="https://docs.roboflow.com/annotate">annotate</a> all of the bottles we can find in each of the scenes. While this is fairly similar to counting all of the bottles manually, remember we might not be perfect in finding all of the bottles with our own eyes. So, hopefully, in teaching a computer vision model what bottles look like and then asking that same model to find bottles for us, we'll see any we may have missed.</p><figure><img src="https://blog.roboflow.com/content/images/2021/02/annotating-bottles.png" alt="Labeling bottles for computer vision" srcset="https://blog.roboflow.com/content/images/size/w600/2021/02/annotating-bottles.png 600w, https://blog.roboflow.com/content/images/size/w1000/2021/02/annotating-bottles.png 1000w, https://blog.roboflow.com/content/images/2021/02/annotating-bottles.png 1573w" sizes="(min-width: 720px) 720px"><figcaption>Annotating all of the Mountain Dew bottles in each of our images.</figcaption></figure><p>After labeling (and deleting one completely black image from the vid), we have 869 annotations across 91 images. We've open sourced this final <a href="https://public.roboflow.com/object-detection/mountain-dew-commercial">Mountain Dew bottles image dataset</a>:</p><figure><a href="https://public.roboflow.com/object-detection/mountain-dew-commercial"><div><p>Mountain Dew Commercial Object Detection Dataset</p><p># Overview ## Mountain Dew is running a $1,000,000 counting contest. Computer Vision can help you win.:fa-spacer: ### Watch [our video](https://youtu.be/jqVKdMjPXHA) explaining how to use this dataset. ![Mountain Dew](https://i.imgur.com/ED4jpM3.png…</p></div><p><img src="https://storage.googleapis.com/roboflow-platform-sources/Ly2DeBzbwsemGd2ReHk4BFxy8683/WF57VFawbh9kJl5K4fie/original.jpg"></p></a></figure><h3 id="training-an-object-detection-model">Training an Object Detection Model</h3><p>Once we have our images collected and labeled, we can train a model to find bottles for us. Before training, however, we can use <a href="https://blog.roboflow.com/boosting-image-detection-performance-with-data-augmentation/">image augmentation</a> to increase the size of our training dataset.</p><p>By applying random distortions like <a href="https://docs.roboflow.com/image-transformations/image-augmentation">brightness changes</a>, <a href="https://blog.roboflow.com/why-and-how-to-implement-random-rotate-data-augmentation/">perspective changes</a>, <a href="https://blog.roboflow.com/how-flip-augmentation-improves-model-performance/">flips</a>, and more, we can increase the volume and variability of our training dataset so that our model has more examples to learn from.</p><figure><div><div><p><img src="https://blog.roboflow.com/content/images/2021/02/dew-aug1.png" width="760" height="760" alt="" srcset="https://blog.roboflow.com/content/images/size/w600/2021/02/dew-aug1.png 600w, https://blog.roboflow.com/content/images/2021/02/dew-aug1.png 760w" sizes="(min-width: 720px) 720px"></p><p><img src="https://blog.roboflow.com/content/images/2021/02/dew-aug2.png" width="818" height="818" alt="" srcset="https://blog.roboflow.com/content/images/size/w600/2021/02/dew-aug2.png 600w, https://blog.roboflow.com/content/images/2021/02/dew-aug2.png 818w" sizes="(min-width: 720px) 720px"></p></div><div><p><img src="https://blog.roboflow.com/content/images/2021/02/dew-aug3.png" width="820" height="818" alt="" srcset="https://blog.roboflow.com/content/images/size/w600/2021/02/dew-aug3.png 600w, https://blog.roboflow.com/content/images/2021/02/dew-aug3.png 820w" sizes="(min-width: 720px) 720px"></p><p><img src="https://blog.roboflow.com/content/images/2021/02/dew-aug4.png" width="821" height="817" alt="" srcset="https://blog.roboflow.com/content/images/size/w600/2021/02/dew-aug4.png 600w, https://blog.roboflow.com/content/images/2021/02/dew-aug4.png 821w" sizes="(min-width: 720px) 720px"></p></div></div><figcaption>Our <a href="https://docs.roboflow.com/image-transformations/image-augmentation">augmented images</a> have been brightened, sheared, flipped, rotated, and more to increase dataset size and variability.</figcaption></figure><p>We then made use of <a href="https://docs.roboflow.com/train">Roboflow Train</a>, which gives us the option to one-click have a trained model available. Critically, when starting model training, we can start from a previous model checkpoint. This <a href="https://blog.roboflow.com/a-primer-on-transfer-learning/">transfer learning</a> will accelerate model training and improve model accuracy. For this dataset, starting training from the <a href="https://blog.roboflow.com/coco-dataset/">COCO Dataset</a> (a dataset of "Common Objects in Context") will give the model a healthy head start.</p><figure><img src="https://blog.roboflow.com/content/images/2021/02/roboflow-train-results.png" alt="Roboflow Train results"><figcaption>The results of our model training. 89.5% <a href="https://blog.roboflow.com/mean-average-precision/">mean average precision</a>. Not bad!</figcaption></figure><h3 id="using-our-model">Using Our Model</h3><p>Once the model finishes training, we have an <a href="https://blog.roboflow.com/hands-on-with-the-roboflow-infer-web-application-interface/">API</a> we can call to perform detections on our original video. With a little shell scripting, we can passthrough the original commercial video frame-by-frame and reconstruct the result with the bounding boxes present.</p><p>The result? See for yourself:</p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/jqVKdMjPXHA?start=773&amp;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><figcaption>A full video walkthrough of our process. The resulting video is available at the 12:53 mark.</figcaption></figure><p>Good luck! We hope you win that $1 million.</p></div>
                
              </div>
            </div></div>]]>
            </description>
            <link>https://blog.roboflow.com/mountain-dew-contest-computer-vision/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26066970</guid>
            <pubDate>Mon, 08 Feb 2021 16:52:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: SVG Pattern Generator – Geometric seamless patterns for the web]]>
            </title>
            <description>
<![CDATA[
Score 34 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26066335">thread link</a>) | @visiwig
<br/>
February 8, 2021 | https://www.visiwig.com/patterns/ | <a href="https://web.archive.org/web/*/https://www.visiwig.com/patterns/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="CONTENT">
		
		<main id="swup">					
						
			<div>					
	
	


	<section id="PATTERNS">
		<div id="pttnSTAGE">
			
						
			<div id="pttnMENU">
				<div id="pttnINPUT">
					<p><label id="labelPttnScale" for="inputPttnScale">Scale: 1</label>
						
					</p>
					<p><label id="labelPttnRotate" for="inputPttnRotate">Rotation: 0</label>
						
					</p>
					<p><label id="labelPttnStroke" for="inputPttnStroke">Stroke Width: 0</label>
						
					</p>
					<div id="input-pttn-color">
						
						<p>
							MODE: &nbsp;
								
							
						</p>
					</div>
					<div id="input-pttn-preview">
						<p>You can use the patterns in any project, commercial or personal without attribution or any costs, but you can’t replicate Visiwig's patterns, redistribute the patterns in packs, or create integrations for these patterns.</p>
						<p><a href="https://www.visiwig.com/pattern-license">FULL LICENSE</a>
					</p></div>
					
				</div>
				</div>		
		</div>
		<div>	
			
			<p>Customize seamless patterns and export for the web or your favorite vector software</p>
			
						
			<div id="pttnPromo">
				<div><p><a href="https://www.visiwig.com/product/100-essential-geometric-and-minimalist-patterns/">PREVIEW ALL 100 PATTERNS</a></p></div>			</div>			
		</div>		
	</section>

			</div> <!-- </.swup-wrap> -->
		</main>		
		
		</div></div>]]>
            </description>
            <link>https://www.visiwig.com/patterns/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26066335</guid>
            <pubDate>Mon, 08 Feb 2021 16:08:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Just Launch the Damn Thing]]>
            </title>
            <description>
<![CDATA[
Score 39 | Comments 18 (<a href="https://news.ycombinator.com/item?id=26066236">thread link</a>) | @alex_c
<br/>
February 8, 2021 | https://mobilefolk.com/just-launch-the-damn-thing/ | <a href="https://web.archive.org/web/*/https://mobilefolk.com/just-launch-the-damn-thing/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://mobilefolk.com/just-launch-the-damn-thing/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26066236</guid>
            <pubDate>Mon, 08 Feb 2021 16:00:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[State of the Common Lisp ecosystem, 2020]]>
            </title>
            <description>
<![CDATA[
Score 306 | Comments 127 (<a href="https://news.ycombinator.com/item?id=26065511">thread link</a>) | @lelf
<br/>
February 8, 2021 | https://lisp-journey.gitlab.io/blog/state-of-the-common-lisp-ecosystem-2020/ | <a href="https://web.archive.org/web/*/https://lisp-journey.gitlab.io/blog/state-of-the-common-lisp-ecosystem-2020/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p>This is a description of the Common Lisp ecosystem, as of January, 2021,
from the perspective of a user and contributor.</p>

<p>The purpose of this article is both to give an overview of the
ecosystem, and to help drive consolidation in each domain.</p>

<p>Each application domain has recommendations for consolidating that
part of the ecosystem, and pointers for interesting future work.</p>

<p>This article is derived from
Fernando Borretti’s <a href="https://borretti.me/article/common-lisp-sotu-2015">State of the Common Lisp ecosystem from 2015</a>, hence the introduction that sounded familiar.
This new one will be an opportunity to look at what was achieved, or what is
still lacking.</p>

<p><strong>Disclaimer</strong>: This article is not a list of every project or article of interest that came out in the last years. I wrote an overview of 2018 closer to that goal <a href="https://lisp-journey.gitlab.io/blog/these-years-in-common-lisp-2018/">here</a>. More libraries can be discovered on the <a href="https://github.com/CodyReichert/awesome-cl">Awesome-cl</a> list, on GitHub and on <a href="https://www.cliki.net/">Cliki</a>.</p>

<p><strong>Acknowledgements</strong> I would like to thank @borodust, @ambrevar and @digikar for their kind feedback.</p>

<!-- markdown-toc start - Don't edit this section. Run M-x markdown-toc-refresh-toc -->

<p><strong>Table of Contents</strong></p>

<ul>
<li><a href="#application-domains">Application domains</a>

<ul>
<li><a href="#command-line">Command line</a></li>
<li><a href="#databases">Databases</a></li>
<li><a href="#concurrency">Concurrency</a></li>
<li><a href="#file-formats">File formats</a></li>
<li><a href="#gui">GUI</a></li>
<li><a href="#machine-learning">Machine Learning</a></li>
<li><a href="#system">System</a></li>
<li><a href="#web-development">Web Development</a>

<ul>
<li><a href="#backend">Backend</a></li>
<li><a href="#frontend">Frontend</a></li>
<li><a href="#javascript">JavaScript</a></li>
<li><a href="#isomorphic-web-frameworks">Isomorphic web frameworks</a></li>
</ul></li>
</ul></li>
<li><a href="#languages-interop">Languages interop</a>

<ul>
<li><a href="#apl">APL</a></li>
<li><a href="#c-c-objective-c">C, C++, Objective C</a></li>
<li><a href="#clojure">Clojure</a></li>
<li><a href="#python">Python</a></li>
<li><a href="#net-core">.Net Core</a></li>
</ul></li>
<li><a href="#development">Development</a>

<ul>
<li><a href="#implementations">Implementations</a></li>
<li><a href="#editors">Editors</a></li>
<li><a href="#developer-utilities">Developer utilities</a></li>
<li><a href="#package-management">Package Management</a></li>
<li><a href="#build-system">Build System</a></li>
<li><a href="#type-system">Type system</a></li>
<li><a href="#testing-ci">Testing, CI</a></li>
</ul></li>
<li><a href="#community">Community</a>

<ul>
<li><a href="#online-presence">Online presence</a>

<ul>
<li><a href="#new-common-lispnet-website">New common-lisp.net website</a></li>
<li><a href="#cookbook">Cookbook</a></li>
<li><a href="#awesome-cl">awesome-cl</a></li>
<li><a href="#more">More</a></li>
</ul></li>
<li><a href="#new-books">New books</a></li>
<li><a href="#companies">Companies</a></li>
<li><a href="#growth">Growth</a></li>
</ul></li>
<li><a href="#last-words">Last words</a></li>
</ul>

<!-- markdown-toc end -->



<h2 id="command-line">Command line</h2>

<p>There used to be several options to ease building and distribution of command line programs,
but now <a href="https://github.com/snmsts/roswell">Roswell</a> has gained most momentum,
and that’s a good thing. Roswell is an implementation
manager, installer and a script runner, and one of its neat features is support for
very easily compiling tiny scripts into executables.</p>

<p>Now, <a href="https://guix.gnu.org/">GNU Guix</a> has gained many CL libraries, and becomes a contender to Roswell. Guix can be used as a package manager on top of your Unix distribution. It brings reproducible builds, rollbacks, the ability to install exact versions of any library (including system dependencies), contained environments and user profiles. It makes it easy too to install the latest version of a CL implementation and libraries and, to a certain extent, to share scripts. See the article <a href="https://ambrevar.xyz/lisp-repl-shell/index.html">A Lisp REPL as my main shell</a> for insights.</p>

<p>To parse command line arguments, <a href="https://github.com/mrkkrp/unix-opts">unix-opts</a> shows decent activity. As a reminder, the CLI arguments are stored portably in <code>uiop:command-line-arguments</code>.</p>

<p><a href="https://github.com/cl-adams/adams">Adams</a> is a new UNIX system administration tool, not unlike Chef or Ansible.</p>

<p><strong>Consolidation</strong></p>

<p>More features to the <a href="https://github.com/CodyReichert/awesome-cl#scripting">sripting libraries</a>.</p>

<p><strong>Future work</strong></p>

<p>The <a href="https://github.com/cxxxr/lem/">Lem editor</a> has built a great user interface and REPL on top of ncurses, with the cl-charms library. It would be great to re-use its components, so that Lispers could easily build similar rich terminal-based interfaces.</p>

<h2 id="databases">Databases</h2>

<p><a href="https://github.com/fukamachi/mito">Mito</a> is an ORM for Common Lisp
with migrations, relationships and PostgreSQL support. It is based on
cl-dbi (a uniform interface to the various database server-specific
libraries such as cl-postgres and cl-mysql) and SxQL (a DSL for
building safe, automatically parameterized SQL queries).</p>

<p>It also has a tutorial in the Cookbook:
<a href="https://lispcookbook.github.io/cl-cookbook/databases.html">Cookbook/databases</a>.</p>

<!-- On my blog, [an article](https://lisp-journey.gitlab.io/blog/composing-queries-with-mito-aka-replacing-lazy-querysets-and-q-objects/) -->

<!-- on how to compose queries with Mito and SxQL, and on how we only need -->

<!-- lisp knowledge to replace Django functionalities. -->

<p>There are of course more libraries in that field. Some new ones since 2015 are:</p>

<p><a href="https://github.com/ruricolist/cl-yesql">cl-yesql</a> (by the author of
Serapeum, Spinneret and other great libraries) is based on Clojure’s
Yesql.</p>

<p><a href="https://github.com/kraison/vivace-graph-v3">vivace-graph</a> is a <strong>graph
database</strong> and Prolog implementation, taking design and inspiration from
CouchDB, neo4j and AllegroGraph.</p>

<p>Vsevolod Dyomkin, the author of Rutils, the Programming Algorithms
book and other libraries, is writing
<a href="https://github.com/vseloved/cl-agraph">cl-agraph</a>, a minimal client
to Franz Inc’s <a href="https://allegrograph.com/">AllegroGraph</a>. AllegroGraph is a
“horizontally distributed, multi-model (document and graph),
entity-event <strong>knowledge graph</strong> technology”. It is proprietary and has a
free version with a limit of 5 million triples. Surely one of those Lisp hidden gems we should know more about.</p>

<p>A general migration tool was lacking. We now have
<a href="https://github.com/dnaeon/cl-migratum">cl-migratum</a>, a “system which
provides facilities for performing database schema migrations,
designed to work with various databases”.</p>

<p>And of course, <a href="https://github.com/dimitri/pgloader">pgloader</a> is still a Common Lisp success story.</p>

<p><strong>Achievement</strong></p>

<p>Among the emerging ORMs, Mito is the one actively maintained that Lispers seem to have chosen. Good. CLSQL certainly still works, but we don’t hear about it and it looks outdated. So, Mito it is.</p>

<p><strong>Consolidation</strong></p>

<p>Mito has 11 contributors and is actively watched, but it probably should have another(s) core maintainers.</p>

<p><strong>Future work</strong></p>

<p>Bindings for the new databases coming out.</p>

<h2 id="concurrency">Concurrency</h2>

<p>In the last year, Manfred Bergmann developed
<a href="https://github.com/mdbergmann/cl-gserver">cl-gserver</a>. It is a
“message passing” library/framework with <strong>actors</strong> similar to
<strong>Erlang</strong> or <strong>Akka</strong>. It is an important achievement.</p>

<p>Its v1 features:</p>

<ul>
<li>actors can use a shared pool of message dispatchers which effectively allows to create millions of actors.</li>
<li>the possibility to create actor hierarchies. An actor can have child actors. An actor now can also “watch” another actor to get notified about its termination.</li>
</ul>

<p>Many other libraries exist in this area:</p>

<ul>
<li><a href="https://common-lisp.net/project/bordeaux-threads/">BordeauxThreads</a> - Portable, shared-state concurrency

<ul>
<li>the “de-facto” concurrency library.</li>
</ul></li>
<li><a href="https://github.com/lmj/lparallel">lparallel</a> - A library for parallel programming.

<ul>
<li>also solid, battle-tested and popular, aka de-facto.</li>
</ul></li>
<li><a href="https://github.com/hawkir/calispel">calispel</a> - <a href="https://en.wikipedia.org/wiki/Communicating_sequential_processes">CSP</a>-like channels for common lisp. With blocking, optionally buffered channels and a “CSP select” statement. ISC-style.

<ul>
<li>“It is complete, flexible and easy to use. I would recommend Calispel over Lparallel and ChanL.” @Ambrevar. <a href="https://github.com/CodyReichert/awesome-cl/issues/290">discussion</a></li>
</ul></li>
<li><a href="https://github.com/zkat/chanl">ChanL</a> - Portable, channel-based concurrency.</li>
<li><a href="https://github.com/orthecreedence/cl-async">cl-async</a> - A library for general-purpose, non-blocking programming.

<ul>
<li>works atop libuv</li>
</ul></li>
<li><a href="https://github.com/TBRSS/moira">Moira</a> -  Monitor and restart background threads. In-lisp process supervisor.</li>
<li><a href="https://gitlab.com/ediethelm/trivial-monitored-thread">trivial-monitored-thread</a> -
a Common Lisp library offering a way of spawning threads and being
informed when one any of them crash and die.</li>
<li><a href="https://github.com/lmj/lfarm">lfarm</a> - distributing work across machines (on top of lparallel and usocket).</li>
<li><a href="https://github.com/taksatou/cl-gearman">cl-gearman</a> - a library for the <a href="http://gearman.org/">Gearman</a> distributed job system.

<ul>
<li>Alexander Artemenko used it instead of lfarm for Ultralisp:
<a href="https://40ants.com/lisp-project-of-the-day/2020/06/0095-cl-gearman.html">https://40ants.com/lisp-project-of-the-day/2020/06/0095-cl-gearman.html</a>,
because “lfarm is not well suited to environments where worker
hosts can go down and return back later”.</li>
</ul></li>
<li><a href="https://github.com/brown/swank-crew">swank-crew</a> - distributed computation framework implemented using Swank Client.</li>
<li><a href="https://github.com/takagi/cl-coroutine">cl-coroutine</a> - a coroutine library. It uses the CL-CONT continuations library in its implementation.</li>
<li><a href="https://github.com/cosmos72/stmx">CMTX</a>: high performance transactional memory for Common Lisp.

<ul>
<li>In our opinion, a library not well known and under-appreciated.</li>
</ul></li>
</ul>

<p>(see <a href="https://github.com/CodyReichert/awesome-cl#parallelism-and-concurrency">awesome-cl#parallelism-and-concurrency</a>)</p>

<p><strong>Consolidation</strong></p>

<p>Bordeaux-Threads is <em>the</em> “de-facto” library, but there is some choice
paralysis between Lparallel, Calispel, Bordeaux-Threads and SBCL’s
contribs. Use the libraries in the wild and write about them.</p>

<h2 id="file-formats">File formats</h2>

<p>There exist Common Lisp libraries for all the major file formats:</p>

<ul>
<li>XML: <a href="https://github.com/Shinmera/plump">Plump</a> (and <a href="https://github.com/Shinmera/lquery/">Lquery</a>), as well as <a href="https://common-lisp.net/project/cxml/">CXML</a>, which can parse large files incrementally.</li>
<li>JSON: <a href="https://github.com/Rudolph-Miller/jonathan">Jonathan</a>, <a href="https://common-lisp.net/project/cl-json/">cl-json</a> or <a href="https://sabracrolleton.github.io/json-review">more</a>. With utilities:

<ul>
<li><a href="https://github.com/y2q-actionman/cl-json-pointer">json-pointer</a> - A JSON Pointer implementation.</li>
<li><a href="https://github.com/gschjetne/json-mop">json-mop</a> - A metaclass for bridging CLOS and JSON objects (remind that JSON libraries can already serialize your own objects).</li>
<li><a href="https://github.com/fisxoj/json-schema">json-schema</a></li>
</ul></li>
<li>YAML: cl-yaml</li>
<li>CSV: <a href="https://github.com/AccelerationNet/cl-csv">cl-csv</a></li>
</ul>

<p><strong>Achievement</strong></p>

<p>New in 2015, Jonathan is now a good first choice for an easy to use and fast JSON encoder and decoder.</p>

<p><strong>Consolidation</strong></p>

<p>There is not a predominant JSON library. This leads to choice paralysis.</p>

<p>They all represent null values differently. We need a library that
“does the right thing”. See maybe the massive <a href="https://github.com/xh4/web-toolkit#json">web-toolkit</a> for its JSON handling ?</p>

<blockquote>
<p>It distinguishes null, false and [] from Lisp’s NIL thus supports identical transformation between JSON values. It provides object constructor and accessor to build and access nesting JSON objects.</p>
</blockquote>

<p>Give the <a href="https://github.com/sharplispers/xpath">XPath</a> library some love and documentation.</p>

<p><strong>Future Work</strong></p>

<p>Still valid from 2015:</p>

<blockquote>
<p>A YAML parser so that cl-yaml doesn’t depend on the libyaml library would make distribution far simpler.</p>
</blockquote>

<h2 id="gui">GUI</h2>

<p>A usual complain in Common Lisp land is the lack of a complete,
cross-platform GUI solution. Ltk is a very good library, but Tk is
limited. Qtools is great, but is only for Qt4.</p>

<p>A lot has happened, and is still happening (if you watch the right
repositories, you know that a Qt5 wrapper is in the works (ECL already has Qt5 bindings: <a href="https://gitlab.com/eql/EQL5/">EQL5</a>, with an <a href="https://gitlab.com/eql/EQL5-Android">Android port</a>)).</p>

<p>edit: see also <a href="https://gitlab.com/eql/eql5-sailfish">EQL5-sailfish</a> for Sailfish OS. <a href="https://openrepos.net/user/13436/programs">Here</a> are two example apps.</p>

<p>Matthew Kennedy wrote excellent FFI bindings to the IUP Portable User
Interface library: <a href="https://github.com/lispnik/iup/">IUP</a>. IUP is
cross-platform (Windows, macOS, GNU/Linux, with new Android, iOS,
Cocoa and Web Assembly drivers), has many widgets (but less than Qt), has a small API and
is actively developed. IUP was created at the PUC university of Rio de Janeiro.</p>

<p>Nicolas Hafner started <a href="https://github.com/Shirakumo/alloy">Alloy</a>, a
new user interface protocol and toolkit implementation, which he uses in his Kandria game.</p>

<p>Very recently, David Botton released <a href="https://github.com/rabbibotton/clog">CLOG</a>, “the Common Lisp Omnificent GUI”:</p>

<blockquote>
<p>CLOG uses web technology to produce graphical user interfaces for applications locally or remotely. CLOG can take the place, or work alongside, most cross-platform GUI frameworks and website frameworks. The CLOG package starts up the connectivity to the browser or other websocket client (often a browser embedded in a native template application.)</p>

<p>It is complete enough for most uses.</p>
</blockquote>

<p>There are more GUI libraries and frameworks: <a href="https://github.com/CodyReichert/awesome-cl#Gui">https://github.com/CodyReichert/awesome-cl#Gui</a> (and more under the works). In particular, LispWorks’ CAPI is still presented as the best in town by the ones who tried it.</p>

<p><strong>Consolidation</strong></p>

<p>Since roughly October, 2020, Nicolas Hafner works full time on
<a href="https://kandria.com/">Kandria</a>. Supporting his work, through <a href="https://github.com/sponsors/Shinmera">GitHub
sponsors</a> or
<a href="https://ko-fi.com/shinmera">ko-fi</a> would be 1) a great sign of recognition and 2) useful for the ecosystem, especially for Alloy.</p>

<p>I wrote an introduction to these frameworks in the Cookbook:
<a href="https://lispcookbook.github.io/cl-cookbook/gui.html">Cookbook/gui</a>. More
examples or demo projects would be welcome.</p>

<p>There are two actively maintained diverged forks of the GTK bindings. A …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://lisp-journey.gitlab.io/blog/state-of-the-common-lisp-ecosystem-2020/">https://lisp-journey.gitlab.io/blog/state-of-the-common-lisp-ecosystem-2020/</a></em></p>]]>
            </description>
            <link>https://lisp-journey.gitlab.io/blog/state-of-the-common-lisp-ecosystem-2020/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26065511</guid>
            <pubDate>Mon, 08 Feb 2021 15:10:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Looking at GSM security 30 years later]]>
            </title>
            <description>
<![CDATA[
Score 135 | Comments 35 (<a href="https://news.ycombinator.com/item?id=26064725">thread link</a>) | @8sfLes
<br/>
February 8, 2021 | https://harrisonsand.com/gsm-security/ | <a href="https://web.archive.org/web/*/https://harrisonsand.com/gsm-security/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
                <div>
                    <p>Looking at 2G GSM traffic three decades since its inception might not sound terribly interesting, but the protocol is still surprisingly common today. It's often used a fallback for when coverage is limited and more modern protocols aren't available, as well as for legacy IoT devices.</p><p>In Norway, Telia and Telenor are in the process of phasing our their 3G networks. At they same time they are keeping their 2G networks operational <a href="https://www.telenor.no/bedrift/iot/m2m/3g/">until at least 2025</a>. Meaning all devices that don't support VoLTE will use 2G for calls and SMS.</p><p>There are well documented security flaws in GSM, and publicly available tools to exploit them. At the same time, it has become considerably cheaper and easier to analyze GSM traffic over the past few years. Open source tools such as <a href="https://github.com/ptrkrysik/gr-gsm">gr-gsm</a> have matured, and the community has developed methods for capturing the GSM spectrum without the need for expensive SDR radios.</p><p>With less than $100 and a weekend it's possible to capture and analyze GSM traffic. With some extra effort it's possible to decrypt your own traffic, and depending on how your mobile provider has set up their network it may even be possible for somebody else to illegally decrypt traffic they don't own.</p><h3 id="the-gsm-spectrum">The GSM spectrum</h3><figure><img src="https://harrisonsand.com/content/images/2021/02/Screenshot_20210208_122702.png" alt="" srcset="https://harrisonsand.com/content/images/size/w600/2021/02/Screenshot_20210208_122702.png 600w, https://harrisonsand.com/content/images/size/w1000/2021/02/Screenshot_20210208_122702.png 1000w, https://harrisonsand.com/content/images/size/w1600/2021/02/Screenshot_20210208_122702.png 1600w, https://harrisonsand.com/content/images/2021/02/Screenshot_20210208_122702.png 1920w" sizes="(min-width: 1200px) 1200px"><figcaption>GSM spectrum, showing multiple channels</figcaption></figure><p>More often than not traffic will "hop" between different channels during a transmission. Hops occur quite quickly, every few milliseconds or so. In practice, this normally means if you're interested in looking at more than a base station's broadcast control channel (BCCH) you need a way to simultaneously capture multiple channels at once.</p><p>You can visually see the hopping in the screenshot above. Stationary control traffic is on the left, and hopping traffic is on the right. These "slices" are produced as the base stations hop between different channels during transmission.</p><p>A local tower of mine is configured to hop between ARFCN channels 57 and 112. This corresponds to downlink frequencies of 946.4 and 957.4 MHz. Channels have a bandwidth of 0.2 MHz, so that means if I want to look at both channels at once I need a radio with a bandwidth of at least 11.2 MHz (946.3 to 957.5). Keep in mind towers will be configured differently, potentially using more channels spread further apart.</p><p>Inexpensive SDRs tend to have a maximum bandwidth of about 2.4 MHz. You can overcome this limitation by using multiple radios and <a href="https://github.com/ptrkrysik/multi-rtl">multi-rtl</a>, which combines their signals together to get a larger effective bandwidth.</p><p>In my case I had access to a <a href="https://www.ettus.com/product-categories/usrp-bus-series/">USRP radio</a> that can capture up to 56 MHz of bandwidth, and that's what I'll be using for my tests. However, the core concepts are identical no matter what radio you use.</p><h3 id="other-hardware">Other hardware</h3><p>If you want to decrypt your own traffic you'll need a way to get the Kc encryption key from your SIM card. Several guides out there use a series of AT+CRSM commands that access the SIM card through the phone's modem. I have gotten this to work in the past, but many phones don't expose the modem's serial interface by default, and even if you can get access it tends to be a messy process. I prefer to use a smart card reader to pull the key from the SIM directly.</p><p>I purchased a HID OMNIKEY 6121 USB smart card reader, and some adapters from the <a href="https://piswords.aliexpress.com/store/829772">Piswords AliExpress store</a> to make it a bit less fiddly to swap SIMs in and out.</p><h3 id="software">Software</h3><p>Recent versions of Ubuntu have gr-gsm in their repositories by default. Installation on Kubuntu 20.10 was as easy as:</p><pre><code>sudo apt install gr-gsm wireshark gqrx-sdr cardpeek</code></pre><p>You'll need Wireshark for looking at traffic, gqrx if you want to visually see the spectrum, and Cardpeek for pulling keys off the SIM card.</p><p>For the USRP you'll need a bit more setup. First download the latest firmware images and then set the UHD_IMAGES_DIR environment variable.</p><pre><code>sudo uhd_images_downloader
export UHD_IMAGES_DIR=/usr/share/uhd/images</code></pre><p>Next, we configure the system so non-root users can access the radio.</p><pre><code>sudo cp /usr/lib/uhd/utils/uhd-usrp.rules /etc/udev/rules.d/
sudo udevadm control --reload-rules
sudo udevadm trigger</code></pre><p>You can test if everything is working by scanning for nearby base stations with the <code>grgsm_scanner</code> utility. It takes about a minute to start displaying data.</p><figure><img src="https://harrisonsand.com/content/images/2021/02/Screenshot_20210207_160020.png" alt="" srcset="https://harrisonsand.com/content/images/size/w600/2021/02/Screenshot_20210207_160020.png 600w, https://harrisonsand.com/content/images/size/w1000/2021/02/Screenshot_20210207_160020.png 1000w, https://harrisonsand.com/content/images/2021/02/Screenshot_20210207_160020.png 1177w"></figure><h3 id="find-the-base-station">Find the base station</h3><p>Before we can capture traffic we need to know what base station our phone is connected to. This is <em>probably</em> the station with the best signal strength, so in my scan above that would be CID 2106 on ARFCN channel 57. But to be sure lets double check.</p><p>On an iPhone you can dial <code>*3001#12345#*</code> which takes you to field test mode. Go to the "All Metrics" tab and scroll down to GSM Serving Cell Info.</p><figure><img src="https://harrisonsand.com/content/images/2021/02/21-02-07-16-25-45-1531-2.png" alt="" srcset="https://harrisonsand.com/content/images/size/w600/2021/02/21-02-07-16-25-45-1531-2.png 600w, https://harrisonsand.com/content/images/size/w1000/2021/02/21-02-07-16-25-45-1531-2.png 1000w, https://harrisonsand.com/content/images/2021/02/21-02-07-16-25-45-1531-2.png 1125w" sizes="(min-width: 720px) 720px"></figure><p>You can see that the ARFCN is set to 57, as we suspected. You can also look under "GSM Idle Config" to verify that the ci matches the CID from the scan, in this case 2106.</p><p>On Android the <a href="https://play.google.com/store/apps/details?id=com.wilysis.cellinfolite">Network Cell Info Lite</a> app provides similar information.</p><h3 id="base-station-arfcns">Base station ARFCNs</h3><p>Next, we need to find out what channels the base station is configured to use. This is so we know what frequencies to configure our capture for. A list of these channels is periodically broadcast inside the System Information Type 1 packets from the BCCH. </p><p>The <code>grgsm_livemon</code> utility allows us to monitor data from the BCCH. Use the <code>-f</code> flag to set the frequency of the base station. You can find this in the output from <code>grgsm_scanner</code>, or by using the <a href="https://www.cellmapper.net/arfcn">cellmapper.net</a> ARFCN frequency calculator. (use the downlink frequency)</p><pre><code>grgsm_livemon -f 946.4M</code></pre><p>Traffic is piped to the system's loopback interface inside UDP packets on port 4729. You can use this display filter to only show Type 1 packets: <code>gsm_a.dtap.msg_rr_type == 0x19</code>.</p><figure><img src="https://harrisonsand.com/content/images/2021/02/Screenshot_20210207_164557.png" alt="" srcset="https://harrisonsand.com/content/images/size/w600/2021/02/Screenshot_20210207_164557.png 600w, https://harrisonsand.com/content/images/size/w1000/2021/02/Screenshot_20210207_164557.png 1000w, https://harrisonsand.com/content/images/size/w1600/2021/02/Screenshot_20210207_164557.png 1600w, https://harrisonsand.com/content/images/2021/02/Screenshot_20210207_164557.png 1960w" sizes="(min-width: 1200px) 1200px"></figure><p>Looking under Cell Channel Description we can see that this base station is configured to use two ARFCNs: 57 and 112.</p><h3 id="capturing-traffic">Capturing traffic</h3><p>Using the ARFCN calculator we can see that the downlink frequencies for those channels are 946.4 and 957.4 MHz. The minimum frequency range to capture both would be 946.3 to 957.5 MHz. Adding 0.1 MHz to each end to account for the 0.2 MHz channel bandwidth.</p><p>For the capture itself, we'll use the <code>grgsm_capture</code> utility. We can use the <code>-f</code> flag to set the center frequency between our two channels, and the <code>-s</code> flag to set a sufficient sample rate / bandwidth.</p><pre><code>Usage: grgsm_capture [options] output_filename

RTL-SDR capturing app of gr-gsm.

Options:
  -h, --help            show this help message and exit
  -f FREQ, --freq=FREQ  Set frequency [default=none]
  -a ARFCN, --arfcn=ARFCN
                        Set ARFCN instead of frequency (for PCS1900 add0x8000
                        (2**15) to the ARFCN number)
  -g GAIN, --gain=GAIN  Set gain [default=30.0]
  -s SAMP_RATE, --samp-rate=SAMP_RATE
                        Set samp_rate [default=1.0M]
  -T REC_LENGTH, --rec-length=REC_LENGTH
                        Set length of recording in seconds [default=infinity]
  -p FREQ_CORR, --freq-corr=FREQ_CORR
                        Set frequency correction in ppm [default=0]

  Additional osmosdr source options:
    Options specific to a subset of SDR receivers supported by osmosdr
    source.

    -w BANDWIDTH, --bandwidth=BANDWIDTH
                        Set bandwidth [default=samp_rate]
    --bb-gain=BB_GAIN   Set baseband gain [default=20.0]
    --if-gain=IF_GAIN   Set intermediate freque gain [default=20.0]
    --ant=ANTENNA       Set antenna [default=]
    --args=DEVICE_ARGS  Set device arguments [default=]. Use --list-devices
                        the view the available devices
    -l, --list-devices  List available SDR devices, use --args to specify
                        hints
</code></pre><p>The center frequency is set to 951.9 MHz, and the sample rate / bandwidth is set to 12 MHz. Technically we could go down to 11.2 MHz, but 12 is a nice even number so that's what I'll use here. You can also configure the gain, but I found the default of 30 dBi to work fine.</p><p>If you see 0's printed in your console it means your dropping samples in the capture. This basically means your computer can't keep up with the data streaming from the SDR. A common cause of dropped samples are spikes in disk writes from the capture utility, and the disk can't save the data fast enough. Increasing the buffer on the USRP can help, which we do via the <code>--args</code> flag. You could also try capturing data with the <code>uhd_rx_cfile</code> utlity, which may perform slightly better.</p><p>If you still get dropped samples, you could also try saving capture files to the <code>/dev/shm/</code> directory. This will store the capture files in memory instead of disk, which has performance benefits. The downside here is there will be less storage space for the capture.</p><pre><code>grgsm_capture --args="uhd,num_recv_frames=1024" -f 951.9M -s 12.0M call.cfile</code></pre><p><strong>Note: </strong>Before taking the capture you need to make sure your phone is communicating over GSM/2G. Most phones let you set this within the cellular data options, though depending on your exact phone and carrier settings the option may be hidden. If you don't see the option try using a different phone. The lowest I could go on my iPhone was 3G, but Android let me set 2G. Also worth noting is that my iPhone would have automatically downgraded to 2G if I disabled 4G and was in an area without 3G coverage.</p><p>Start the capture, and Ctrl+C out once you're done. Keep in mind at this point we're just capturing the downlink traffic from the base station to the phone. Some good tests would be to either place a call or receive a text message.</p><h3 id="channelize-capture">Channelize capture</h3><p><code>call.cfile</code> is "wideband" capture that contains two ARFCNs. For further processing with gr-gsm we need to split each channel into its own capture file.</p><p>We can use the <code>grgsm_channelize</code> utility for this. Set the frequency and sample rate so they're the same as when we did the capture. At the end of the command append the list of ARFCNs to extract.</p><figure><img src="https://harrisonsand.com/content/images/2021/02/Screenshot_20210207_193136.png" alt="" srcset="https://harrisonsand.com/content/images/size/w600/2021/02/Screenshot_20210207_193136.png 600w, https://harrisonsand.com/content/images/size/w1000/2021/02/Screenshot_20210207_193136.png 1000w, https://harrisonsand.com/content/images/2021/02/Screenshot_20210207_193136.png 1113w"></figure><p>A new folder will be created that contains separate capture files for each ARFCN.</p><h3 id="analyzing-bcch-traffic">Analyzing BCCH traffic</h3><p>Now we can decode the BCCH traffic from our capture. The BCCH, or broadcast control channel is where the phone listens to get told what to do. So if you receive …</p></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://harrisonsand.com/gsm-security/">https://harrisonsand.com/gsm-security/</a></em></p>]]>
            </description>
            <link>https://harrisonsand.com/gsm-security/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26064725</guid>
            <pubDate>Mon, 08 Feb 2021 14:17:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Requests dropped when using Cloudflare’s free tier for a commercial project]]>
            </title>
            <description>
<![CDATA[
Score 176 | Comments 119 (<a href="https://news.ycombinator.com/item?id=26063239">thread link</a>) | @pawurb
<br/>
February 8, 2021 | https://pawelurbanek.com/cloudflare-free-plan | <a href="https://web.archive.org/web/*/https://pawelurbanek.com/cloudflare-free-plan">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<div role="main">
<p>
The claims contained in this article were incorrect. After another contact with Cloudflare support, it turned out that the Bot Fight Mode behaves differently for Free and PRO plans. That's what caused the instant improvement after upgrading. Another way to resolve the issues I was experiencing would have been to disable the bot fight mode altogether or add a custom page rule disabling it. My website never experienced any traffic throttling. I've decided to remove the article, to stop spreading the misinformation about CF services.
</p>
</div>
</div></div>]]>
            </description>
            <link>https://pawelurbanek.com/cloudflare-free-plan</link>
            <guid isPermaLink="false">hacker-news-small-sites-26063239</guid>
            <pubDate>Mon, 08 Feb 2021 11:36:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Online glassboard (like Lightboard) but using just free software]]>
            </title>
            <description>
<![CDATA[
Score 71 | Comments 9 (<a href="https://news.ycombinator.com/item?id=26063177">thread link</a>) | @theBashShell
<br/>
February 8, 2021 | http://blogs.lobsterpot.com.au/2021/01/30/presentation-trickery-online-glassboard-like-lightboard-but-using-just-free-software/ | <a href="https://web.archive.org/web/*/http://blogs.lobsterpot.com.au/2021/01/30/presentation-trickery-online-glassboard-like-lightboard-but-using-just-free-software/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-4134">
	
	<!-- .entry-header -->

	<div>
		
<p>I don't know if you've ever seen me present. I like to use whiteboards or flip charts, and that doesn't necessarily translate well to online presentations.</p>



<p>It was at least ten years ago when I had an idea about giving online presentations with a whiteboard, but where the whiteboard would be between me and the camera. A glassboard rather than a whiteboard, obviously, so that I would still be visible through the glass but not obscuring the text. And the image would be mirrored so that the things I wrote would be readable to the audience, given that I'd be drawing on the other side of it. Recently I've found out this is a real thing called <a rel="noreferrer noopener" aria-label="Lightboard (opens in a new tab)" href="https://library.iated.org/view/KOC2017LIG" target="_blank">Lightboard</a>, using ultra-clear glass and lights to make sure what's drawn glows enough. And considering this seems to have only appeared around 2015 (a good five years after I was musing about the concept), I really should've explored things further. </p>



<figure><img src="https://wiki.nus.edu.sg/download/attachments/135954958/lightboard.jpg?version=1&amp;modificationDate=1458616080983&amp;api=v2" alt=""><figcaption>This picture is from a site belonging to the <a rel="noreferrer noopener" aria-label="National University of Singapore (opens in a new tab)" href="https://wiki.nus.edu.sg/pages/viewpage.action?pageId=135954958" target="_blank">National University of Singapore</a>, but an image search gives plenty of examples</figcaption></figure>



<p>In a world where technical presentations are more online than in-person, I've struggled a bit with how to give my usual style of presentation. I know I could set up something like this, and I've been tempted, but it would be a lot of effort, and it's not exactly portable, and I just haven't (although I know at least one person who has…)</p>



<p>I know what you're thinking – and that's that tools like Teams do have whiteboards in their meetings. That's not what I want though – in those situations the main screen becomes white (or whatever colour the whiteboard background is), and the bit showing me disappears or shrinks to the corner. It's like sharing a screen. And my style isn't just writing on a whiteboard, it's pointing at the whiteboard, it's gesturing, it's all of that stuff that doesn't work if I'm constrained to the corner of the screen. If I had a greenscreen behind me I could do the "weatherperson" trick of moving in front of the screen to point things out, etc, but it's still not quite what I want.</p>



<p>And so I got to thinking about what could be done.</p>



<p>Enter OBS. That free piece of software that many people use now. Plus, from version 26 on, it can act as a Virtual Camera, so that Teams (or Zoom or GoTo or whatever) can show whatever OBS is doing. So if I figured if I could get this to behave the right way, I would be able to use it in live presentations. The button that appeared in version 26 is in the Controls pane in the bottom right of the OBS window. </p>



<figure><img src="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-12.png" alt="" srcset="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-12.png 306w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-12-300x162.png 300w" sizes="(max-width: 306px) 100vw, 306px"></figure>



<p>There is no mode where I can just use my stylus to draw on the screen where my face is. At least I don't believe there is, but I found a workaround.</p>



<p>The basic concept here is that I use my webcam as a source, but overlay a window capture from an application where I can draw with my stylus (PowerPoint will do, and is good for other reasons too). Then I set the application's background to a <a rel="noreferrer noopener" aria-label="chroma key (opens in a new tab)" href="https://en.wikipedia.org/wiki/Chroma_key" target="_blank">chroma key</a> colour (like bright green #00FF00) and filter that out like a traditional greenscreen. I stretch that source in OBS bit so that the main drawing area is over my webcam feed, and all the menus and stuff is outside.</p>



<p>To explain with pictures:</p>



<p>I started with a plain PowerPoint presentation and set the background to bright green. I'm going to leave the PowerPoint application in this mode because I find it behaves better. And as I'm about to draw on it with a stylus, I don't want it to do anything weird by switching into 'presenter mode' or 'annotation mode' or anything like that.</p>



<figure><img src="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-2-1024x598.png" alt="" srcset="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-2-1024x598.png 1024w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-2-300x175.png 300w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-2-768x449.png 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Then in OBS, with the webcam as the bottom layer of the Sources, I add a Window Capture and use the PowerPoint screen.</p>



<figure><img src="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-1.png" alt="" srcset="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-1.png 483w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-1-300x116.png 300w" sizes="(max-width: 483px) 100vw, 483px"></figure>



<p>Now select the WindowCapture and resize it to match the slide to the webcam. This slide is going to disappear though, when we add the filter.</p>



<figure><img src="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/ResizingOBS.gif" alt=""></figure>



<p>Right-clicking on the "Window Capture" source, I can go to Filters and add a Chroma Key filter, using the default Green. You'll notice that the bits that are green in PowerPoint have gone grey, indicating that they'll be invisible.</p>



<figure><img src="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-3.png" alt="" srcset="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-3.png 863w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-3-300x263.png 300w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-3-768x675.png 768w" sizes="(max-width: 863px) 100vw, 863px"></figure>



<p>Now back in OBS, I see myself again..</p>



<figure><img src="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-4.png" alt="" srcset="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-4.png 975w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-4-300x223.png 300w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-4-768x572.png 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p>…but when I write in PowerPoint, it appears in front of me. (Oh, because I want to point at the things I draw, I flip the webcam horizontally. You might have noticed the writing on my shirt is backwards.)</p>



<figure><img src="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-5-1024x593.png" alt="" srcset="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-5-1024x593.png 1024w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-5-300x174.png 300w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-5-768x444.png 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>And it's not just text that works. I can have standard bullet-point text.</p>



<figure><img src="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-6-1024x594.png" alt="" srcset="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-6-1024x594.png 1024w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-6-300x174.png 300w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-6-768x446.png 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>And I can write in other colours too, just not green.</p>



<p>But that brings me to an interesting set of tricks.</p>



<p>The first one is to grab a screenshot of me on the webcam, and make it very saturated, and green. I can do this using just about any image editing tool. So now I have an image that looks like this.</p>



<figure><img src="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-7.png" alt="" srcset="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-7.png 759w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-7-300x169.png 300w" sizes="(max-width: 759px) 100vw, 759px"></figure>



<p>I set this to my PowerPoint background, and I can easily see where I can draw and where I can't (assuming I don't move around too much).</p>



<figure><img src="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-9-1024x614.png" alt="" srcset="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-9-1024x614.png 1024w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-9-300x180.png 300w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-9-768x461.png 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>I can also add gridlines to help my handwriting stay neater, and help me make sure I don't run out of room.</p>



<p>But in many ways the biggest trick is around the flow of my presentation.</p>



<p>Part of why I use whiteboards and flipcharts is because I feel like they help the audience connect with me better. There's something about writing live that means that people feel like I'm doing it on the fly, being more responsive to how the audience is responding, changing tack as I go. </p>



<p>But it's not like that at all. When I present without slides, or even without a computer, I have to know my material really well. I have to know where I'm going. I don't have the crutch of a slide deck of bullet points. I need to know what I'm planning to write. How I'm going to make the various points. Even how the audience is likely to respond to various things. By knowing the narrative of my presentation really well, it gives me the freedom to move around the content if I feel like I need to, but I have to know my anchor points in my head, because I don't have them on the screen.</p>



<p>So… as long as they're still green, and therefore invisible to the audience, I can indeed have them on the screen now. And whether I use consecutive slides with different colours (first green, then white/black/whatever) to make it look like it's building, or whether I trace over the top using my stylus, I can have all the notes I like – even notes to myself that the audience can't see. I can have as many slides as I want and can move around them just like regular folk do.</p>



<figure><img src="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-11-1024x594.png" alt="" srcset="http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-11-1024x594.png 1024w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-11-300x174.png 300w, http://blogs.lobsterpot.com.au/wp-content/uploads/2021/01/image-11-768x445.png 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>When it comes to giving demos, I still need to add a separate screen or do traditional screensharing through Teams or GoTo or Zoom or whatever. But with a bit more effort I could do my demo in the background and still draw over the top. Or get a physical greenscreen behind me so that I can have a solid background or demo screen behind me and still have my drawings in the foreground. This can definitely go a lot further.</p>



<p>But for my whiteboard-based presentations, this should work nicely.</p>



<p><a href="http://twitter.com/rob_farley" target="_blank" rel="noreferrer noopener" aria-label="@rob_farley (opens in a new tab)">@rob_farley</a> </p>



<p>PS: I just made a short video at <a rel="noreferrer noopener" aria-label="https://youtu.be/LdtmEf2XhSU (opens in a new tab)" href="https://youtu.be/LdtmEf2XhSU" target="_blank">https://youtu.be/LdtmEf2XhSU</a> to show drawing on the screen. It would've been better if I had taken a few minutes to sort out my lighting and background, but you can see the rough concept there.</p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article></div>]]>
            </description>
            <link>http://blogs.lobsterpot.com.au/2021/01/30/presentation-trickery-online-glassboard-like-lightboard-but-using-just-free-software/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26063177</guid>
            <pubDate>Mon, 08 Feb 2021 11:25:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Reviving an old VCD dub with Python]]>
            </title>
            <description>
<![CDATA[
Score 41 | Comments 9 (<a href="https://news.ycombinator.com/item?id=26063097">thread link</a>) | @edent
<br/>
February 8, 2021 | https://jsutton.co.uk/sync-dubs-with-python/ | <a href="https://web.archive.org/web/*/https://jsutton.co.uk/sync-dubs-with-python/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://jsutton.co.uk/content/images/size/w300/2021/02/PXL_20210130_192525870-1.jpg 300w,
                            https://jsutton.co.uk/content/images/size/w600/2021/02/PXL_20210130_192525870-1.jpg 600w,
                            https://jsutton.co.uk/content/images/size/w1000/2021/02/PXL_20210130_192525870-1.jpg 1000w,
                            https://jsutton.co.uk/content/images/size/w2000/2021/02/PXL_20210130_192525870-1.jpg 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://jsutton.co.uk/content/images/size/w2000/2021/02/PXL_20210130_192525870-1.jpg" alt="Reviving an old VCD dub with python">
            </figure>

            <section>
                <div>
                    <p>We have children in the family learning both English and Cantonese simultaneously, a great way to help support this will be to show them films that they will love in their mother tongue, and what better films to show children but films by Studio Ghibli? </p><p>But here's the problem: &nbsp;Studio Ghibli films are in Japanese (naturally), but fortunately they've been translated into many languages and distributed world wide. Even better, they're now even available on Netflix. However, for simplicities sake, you tend to only be able to get the translated version for the country or region that you're living in, so for the UK... English.</p><figure><img src="https://jsutton.co.uk/content/images/2021/02/howl_regions.jpg" alt="" srcset="https://jsutton.co.uk/content/images/size/w600/2021/02/howl_regions.jpg 600w, https://jsutton.co.uk/content/images/size/w1000/2021/02/howl_regions.jpg 1000w, https://jsutton.co.uk/content/images/size/w1600/2021/02/howl_regions.jpg 1600w, https://jsutton.co.uk/content/images/size/w2400/2021/02/howl_regions.jpg 2400w" sizes="(min-width: 720px) 720px"><figcaption>The back of the UK Howl's Moving Castle Blu Ray Case, showing the two language options (English and Japanese) along with the disk regions (2/B).</figcaption></figure><!--kg-card-begin: markdown--><p>You can, if you're lucky, find copies of the Cantonese Blu Ray available to import, but then you usually face two problems:</p>
<ol>
<li>High Price. The English copy of the Spirited Away Blu Ray is about £12 on Amazon UK, but the cantonese import is over £45!</li>
<li>Region Locking. Thankfully Blu Ray only has 3 regions compared to DVD's 6! But it's still a problem if you can afford to import a  blu ray from another contintent. You could then import the relevant player, but that's a lot of wasted cash.</li>
</ol>
<!--kg-card-end: markdown--><figure><img src="https://jsutton.co.uk/content/images/2021/02/Blu-ray-regions_with_key.svg" alt=""><figcaption>Blu Ray Regions: https://en.wikipedia.org/wiki/DVD_region_code</figcaption></figure><figure><img src="https://jsutton.co.uk/content/images/2021/02/spirited_away_cant_amazon.png" alt="" srcset="https://jsutton.co.uk/content/images/size/w600/2021/02/spirited_away_cant_amazon.png 600w, https://jsutton.co.uk/content/images/size/w1000/2021/02/spirited_away_cant_amazon.png 1000w, https://jsutton.co.uk/content/images/2021/02/spirited_away_cant_amazon.png 1431w" sizes="(min-width: 720px) 720px"><figcaption>Spirited Away Cantonese listing on amazon UK</figcaption></figure><p>"Ok", I hear you say, "... but you can just stream these films online now, surely the cantonese dub must be available?". Sadly not, as of 2021, the only way to watch Ghibli films online is on Netflix, and you do have some extra dubs available, but only a tiny fraction of the ones in existence.</p><figure><img src="https://jsutton.co.uk/content/images/2021/02/Screenshot-2021-02-06-at-22.10.24.png" alt="" srcset="https://jsutton.co.uk/content/images/size/w600/2021/02/Screenshot-2021-02-06-at-22.10.24.png 600w, https://jsutton.co.uk/content/images/size/w1000/2021/02/Screenshot-2021-02-06-at-22.10.24.png 1000w, https://jsutton.co.uk/content/images/2021/02/Screenshot-2021-02-06-at-22.10.24.png 1146w" sizes="(min-width: 720px) 720px"><figcaption>The audio tracks available for Howl's Moving castle on Netflix: Arabic, English, Hindi, Japanese and Polish</figcaption></figure><h2 id="as-luck-would-have-it-">As luck would have it...</h2><p>.. a family member had been in Hong Kong back in the mid 2000's and brought back a box set of Studio Ghibli Cantonese dubbed VCDs. Not DVDs, <strong>VCDs</strong>. These seemed to come and go pretty quickly in the west, being replaced by DVDs. In Asia however, VCDs were popular for far longer (<a href="https://en.wikipedia.org/wiki/Video_CD#In_Asia">https://en.wikipedia.org/wiki/Video_CD#In_Asia</a>). VCDs have two main problems when using them in 2021: Firstly, the quality can only be accurately summarised as "<a href="https://knowyourmeme.com/memes/recorded-with-a-potato">Potato</a>". Secondly, because a CD is limited to 700MB, the film has to be split in half across two disks. A bit pants in this modern day and age really!</p><p>But let's not look a gift horse in the mouth, there is definitely something we can do with these!</p><p>I already own almost all of the UK releases of the Ghibli films, so now I had the perfect source material; High Quality video from the Blu Ray versions of the films, along with relatively decent Cantonese dubbing of the same film on VCD. All we need to do is find a way to copy the Cantonese Dub onto the High Quality video...</p><figure><img src="https://jsutton.co.uk/content/images/2021/02/PXL_20210130_192525870.jpg" alt="" srcset="https://jsutton.co.uk/content/images/size/w600/2021/02/PXL_20210130_192525870.jpg 600w, https://jsutton.co.uk/content/images/size/w1000/2021/02/PXL_20210130_192525870.jpg 1000w, https://jsutton.co.uk/content/images/size/w1600/2021/02/PXL_20210130_192525870.jpg 1600w, https://jsutton.co.uk/content/images/size/w2400/2021/02/PXL_20210130_192525870.jpg 2400w" sizes="(min-width: 720px) 720px"><figcaption>The UK Blu Ray &amp; Hong Kong VCD of Howl's Moving Castle</figcaption></figure><h2 id="so-how-do-we-do-this">So how do we do this?</h2><!--kg-card-begin: markdown--><p>My original plan had four simple steps:</p>
<ol>
<li>Rip the Blu Ray and VCD versions to my computer.</li>
<li>Extract the Cantonese Dub from the VCD version.</li>
<li>Add it as a new language track on the extracted mp4.</li>
<li>Enjoy a classic film in a new language on my HD TV.</li>
</ol>
<p>Alas, the best made plans, often go awry..</p>
<ul>
<li>The VCD films are split int two, meaning they need to be re-joined (Nothing a little ffmpeg magic can't do!).</li>
<li>The Join in the middle is sometimes, but not always exact, meaning that there may be milliseconds missing in the middle when compared to the full length film.</li>
<li>The VCD film and Blu Ray films are not entirely identical, the VCDs have a black screen with the name of the local distributor at the start for a few seconds (of varying length).</li>
</ul>
<!--kg-card-end: markdown--><h2 id="new-plan-choirless-to-the-rescue-">New Plan - Choirless to the Rescue!</h2><p>Re-winding back to 2020, some colleagues of mine in IBM had a fantastic idea for our Internal Call for Code competition. <a href="https://www.choirless.com/">Choirless</a> is an online web service that allows distanced choir and band members to record their performances separately and to have them automatically synchronised and merged into a fantastic performance video. The summary of Choirless is well worth a watch:</p><figure></figure><p>It struck me that the work Choirless does to synchronise all of the performers could potentially be the magic bullet for solving this, <a href="https://twitter.com/HammerToe">@HammerToe</a> had written some clever python that could take two audio streams and synchronise them and even had streamed a walkthrough of his code that I had managed to catch months ago by chance:</p><figure></figure><p>Anything that I could explain here about my process would just be a re-hashing of the above video, so I highly recommend that you check it out. But the key point is this: Whilst both the Japanese and Cantonese audio tracks will be different because of the language differences, there is still plenty of audio that should be (almost) <em><strong>identical. </strong></em>All of the music and sound effects should be the same across both versions, so that will give us exactly what we need to identify the correct starting points for the Cantonese dub.</p><h2 id="how-it-works">How it works</h2><figure><img src="https://jsutton.co.uk/content/images/2021/02/diagram.png" alt=""><figcaption>Diagram showing the synchronisation process&nbsp;</figcaption></figure><p>The process starts off with extracting the video &amp; audio from the Blu Ray and VCDs onto your computer, I used <a href="https://handbrake.fr/">handbrake</a> for this.</p><p>Then you process the HQ Video (Primary) and the first VCD Video (Secondary). The python script will use the onset strength to look for peaks in both audio streams, it will then do forward and back passes with different offsets to find the best match for where the onset strength matches up.</p><figure><img src="https://jsutton.co.uk/content/images/2021/02/Screenshot-2021-01-31-at-11.05.59.png" alt="" srcset="https://jsutton.co.uk/content/images/size/w600/2021/02/Screenshot-2021-01-31-at-11.05.59.png 600w, https://jsutton.co.uk/content/images/size/w1000/2021/02/Screenshot-2021-01-31-at-11.05.59.png 1000w, https://jsutton.co.uk/content/images/size/w1600/2021/02/Screenshot-2021-01-31-at-11.05.59.png 1600w, https://jsutton.co.uk/content/images/2021/02/Screenshot-2021-01-31-at-11.05.59.png 2354w" sizes="(min-width: 720px) 720px"><figcaption>Two charts showing the onset strength peaks before and after synchronisation.</figcaption></figure><p>Now because the VCD was split in two, we need to work out at which point the audio "should" start, so that we can skip forward on the primary track to the right place to give us our best chance of finding the offset. This is basically the following formula:</p><!--kg-card-begin: markdown--><p><code>hq_offset = part_1_duration - part_1_offset</code></p>
<!--kg-card-end: markdown--><p>Once we've got that, we repeat the synchronisation process for the second half of the audio! Sometimes the difference is positive rather than negative, this means that there is sadly a gap in the middle, so we need to add a slight delay in the middle before carrying on, so keep that in mind when you're doing this yourself.</p><!--kg-card-begin: markdown--><p>At this point, we should have the following:</p>
<ul>
<li>HQ rip of the Blu Ray</li>
<li>LQ Rip Parts 1 and 2 of the VCD</li>
<li>The offset in milliseconds for both parts of the VCD audio.</li>
</ul>
<!--kg-card-end: markdown--><p>Now for the fun part, use <a href="http://sox.sourceforge.net/#:~:text=SoX%20is%20a%20cross%2Dplatform,audio%20files%20on%20most%20platforms.">sox</a> to join the audio streams together, if you need to add some "dead air" in the middle, then use you can also generate some empty audio with sox:</p><!--kg-card-begin: markdown--><p>Generate 510ms of blank audio:<br>
<code>sox -n -r 44100 -c 1 silence510ms.wav trim 0.0 0.510</code></p>
<p>Merge Audio Streams:<br>
<code>sox file1.wav file2.wav merged.wav</code></p>
<p>Add your merged audio into your existing HQ film:<br>
<code>ffmpeg -i high_quality_film.mp4 -i cantonese_merged.aac -map 0 -map 1:a -c copy high_quality_film_Cantonese.mp4</code></p>
<!--kg-card-end: markdown--><p>Tada! You should now see a new audio track on your film in the language of your choice for you to personally enjoy. Remember to only use this process on your own personal copies of the media and don't distribute them to anyone else as that could land you in hot water!</p><h2 id="the-code-">The Code.</h2><p>The code I've thrown together definitely isn't anything near production as I've just had to use it for the handful of disks I own, but you're more than welcome to use it for your own purposes. I've thrown it up on GitHub for you use here:</p><figure><a href="https://gist.github.com/jpwsutton/28883e65292d0bfb8effe8357712e701"><div><p>A very rough python script to synchronise dubbed audio from different videos.</p><p>A very rough python script to synchronise dubbed audio from different videos. - audio_dub_sync.py</p><p><img src="https://github.githubassets.com/favicons/favicon.svg"><span>Gist</span></p></div><p><img src="https://github.githubassets.com/images/modules/gists/gist-og-image.png"></p></a></figure>
                </div>
            </section>



        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://jsutton.co.uk/sync-dubs-with-python/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26063097</guid>
            <pubDate>Mon, 08 Feb 2021 11:13:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Migrate Everything from Linux to BSD]]>
            </title>
            <description>
<![CDATA[
Score 119 | Comments 156 (<a href="https://news.ycombinator.com/item?id=26060307">thread link</a>) | @zdw
<br/>
February 7, 2021 | https://www.unixsheikh.com/articles/why-you-should-migrate-everything-from-linux-to-bsd.html | <a href="https://web.archive.org/web/*/https://www.unixsheikh.com/articles/why-you-should-migrate-everything-from-linux-to-bsd.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>

<p>Published on <span id="pubdate">2020-01-18</span>. Modified on <span id="moddate">2021-02-10</span>.</p>
<p>As an operating system GNU/Linux has become a mess because of the fragmented nature of the project, the bloatware in the kernel, but mainly because of the manipulation by corporate interests. There exist <a href="https://unixsheikh.com/articles/technical-reasons-to-choose-freebsd-over-linux.html">several technical reasons</a> for when a migration from GNU/Linux to BSD make sense, but this article isn't about that, it's an "analyzes" of the current status in Linux-land, and it is an opinionated rant, more than anything else.</p>
<h3>Table of Contents</h3>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#fragmented">Linux is fragmented</a></li>
<li><a href="#hijacked">Linux is being heavily influenced by corporate interests</a></li>
<li><a href="#bsd">BSD is the place to be</a></li>
<li><a href="#license">License problems</a></li>
<li><a href="#migrate">Time to migrate everything to BSD</a></li>
<li><a href="#links">Relevant links</a></li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>In the past I have always been a favorite of choosing operating system and tools based upon technical merit. However, in today's world of companies like Microsoft, Apple, Google, and many others, compromising user privacy, and conducting controversial activities, I don't believe that to be the right cause of action.</p>
<p>Proprietary operating systems like <a href="https://en.wikipedia.org/wiki/Windows_10#Privacy_and_data_collection">Microsoft Windows 10</a>, <a href="https://gist.github.com/iosecure/357e724811fe04167332ef54e736670d">Apple MacOS</a>, and <a href="https://en.wikipedia.org/wiki/Android_%28operating_system%29#Security_and_privacy">Google Android</a> have become famous for their ill conduct, and even companies like Lenovo is using UEFI boot to inject custom Windows components, so that the system can phone home to Lenovo.</p>
<p>I have been a proponent for the open source alternatives, like GNU/Linux and BSD, for a very long time. Not only that, I also believe that the open source alternatives are much better in many technical areas.</p>
<p>I have also always been very much against <a href="https://www.unixsheikh.com/articles/the-typical-discussions-about-bsd-vs-linux.html">The typical discussions about BSD vs Linux</a>, and as I wrote in my article back then, I have always believed that the different open source projects can help each other and cooperate, and that end-users should only debate such issues from a technical stand point rather than personal preference.</p>
<p>Whenever it has been possible, I have proposed people, both private and in the industry, to change the operating systems they use to open source alternatives, and when people have been receptive to my advocacy I have helped them migrate from Microsoft Windows on their workstations to BSD or Linux. And likewise on the server side. This has been a truly successful endeavor and I have honestly never experienced a dissatisfied person or company.</p>
<p>However, things are beginning to change in the GNU/Linux world as more and more corporations want to control the direction of Linux as an operating system. Due to the structure and organization of GNU/Linux as an operating system, it is unfortunately susceptible to these influences, and while it is still open source, and still not anywhere near the bad things that is going on with the proprietary alternatives, some opt-out features have slowly been introduced into both the kernel and systemd.</p>
<p>You can still choose to opt-out of these features and go your merry way, but as an open source enthusiast and proponent, and as a privacy concerned individual, perhaps the better approach is to migrate systems to something where you don't have to concern yourself with "creepware".</p>
<p>As a system administrator I don't want to worry about whether I am going to be surprised the next time I upgrade a system, and I don't want to keep a list of spyware I have to remember to opt-out of whenever I run one of these systems.</p>
<p>Several Linux distributions have decided (not only because of privacy opt-out issues, but other issues as well) to implement <a href="https://en.wikipedia.org/wiki/Category:Linux_distributions_without_systemd">other init solutions than systemd</a>, but with the situation going on in the kernel development, and with many third party applications becoming more and dependent upon systemd, the problems are spreading to other parts of the operating system and I believe this is becoming an uphill battle.</p>
<p>From a community perspective, and from a security perspective, I don't believe the future of GNU/Linux looks very bright, and as a possible alternative solution I suggest migrating everything (when possible) to something a bit more sane, like one of the BSD projects.</p>

<h2 id="fragmented">Linux is fragmented</h2>
<p>In 1983 Richard Stallman announced his intent to start coding the GNU Project in a Usenet message. By June 1987, the project had accumulated and developed free and open source software for an assembler, an almost finished portable optimizing C compiler (GCC), an editor (GNU Emacs), and various Unix utilities, such as ls, grep, awk, make and ld.</p>
<p>In 1991, the Linux kernel appeared, developed outside the GNU project by Linus Torvalds, and in December 1992 it was made available under version 2 of the GNU General Public License. Combined with the operating system utilities already developed by the GNU project, it became the GNU/Linux operating system, better known as just "Linux".</p>
<p>Then came the Linux distributions. Different projects took the Linux kernel, the GNU tools and libraries, additional third party software, documentation, the X Window System, a window manager, and a desktop environment, and combined those components into the distributions. Different distributions focused on different goals, some put focus on the desktop while others put their main focus on servers, and again others tried to provide a multi-purpose operating system.</p>
<p>In the past all these different components and projects where developed by open source enthusiasts and communities and the passion for programming and open source was the driving force.</p>
<p>This is no longer the case! Please see <a href="https://unixsheikh.com/articles/the-real-motivation-behind-systemd.html">The real motivation behind systemd</a>.</p>
<p>Linus Torvalds has many times made it very clear that he doesn't care about what goes on in the "Linux world", all he cares about is the kernel development, and on January 6, 2020 in the "Moderated Discussions" forum at <a href="https://www.realworldtech.com/forum/?threadid=189711&amp;curpostid=189841">realworldtech.com</a>, Linus Torvalds answered a user's question, with an absolute jaw-dropping comment, about a year-old kernel maintenance controversy that heavily impacted the <a href="https://zfsonlinux.org/">ZFS on Linux</a> project.</p>
<p>After answering the user's actual question, Torvalds went on to make very wrong and damaging claims about the ZFS filesystem. Torvalds said:</p>
<blockquote>
It (ZFS) was always more of a buzzword than anything else.
</blockquote>
<p>By that statements Linus Torvalds has just reduced more that 15 years of development of one of the most robust and popular filesystems in the world into a "buzzword"!</p>
<p>ZFS is described as "The last word in filesystems". It is a combined filesystem and logical volume manager originally designed by Sun Microsystems. ZFS is a stable, fast, secure, and future-proof filesystem. It is scalable, and includes extensive protection against data corruption, support for high storage capacities, a maximum 16 Exabyte file size, and a maximum 256 Quadrillion Zettabytes storage with no limit on number of filesystems (datasets) or files, efficient data compression, snapshots and copy-on-write clones, continuous integrity checking and automatic repair, RAID-Z, native NFSv4 ACLs, and can be very precisely configured.</p>
<p>The two main implementations, by Oracle and by the <a href="https://en.wikipedia.org/wiki/OpenZFS">OpenZFS</a> project, are extremely similar, making ZFS widely available within Unix-like systems.</p>
<p>As mentioned in the Wikipedia article, OpenZFS is an umbrella project aimed at bringing together individuals and companies that use the ZFS file system and work on its improvements, aiming as well at making ZFS more widely used and developed in an open-source manner. OpenZFS brings together developers from the illumos, Linux, FreeBSD, and macOS platforms, and a wide range of companies. High-level goals of the project include raising awareness of the quality, utility and availability of open-source implementations of ZFS, encouraging open communication about ongoing efforts toward improving open-source variants of ZFS, and ensuring consistent reliability, functionality and performance of all distributions of ZFS.</p>
<p><a href="https://github.com/zfsonlinux/zfs/commits/master">OpenZFS on Linux</a>, which is the Linux part of the project, has currently 345 active contributors with more that 5.600 commits, and commits are being made on an almost daily basis!</p>
<p>Some of the worlds biggest CDN and data storage services runs ZFS on either FreeBSD or Linux!</p>
<p>In another situation Linus Torvalds gave an interview on <a href="https://www.youtube.com/watch?v=mysM-V5h9z8">TFiR: open source and Emerging Tech YouTube channel</a> about Linux on the desktop in which he makes another amazing statement saying that Linux still isn't ready for the desktop and that perhaps <a href="https://en.wikipedia.org/wiki/Chrome_OS">Chrome OS</a> is the solution to that problem.</p>
<p>These and many other statements by Linus Torvalds show that Linux as an operating system has no real direction and no clear management because the kernel development is performed in isolation from the rest of the Linux world.</p>
<p>Linus Torvalds is generally also very open to the rapid influence by corporate interests and his perspective on security is also worrying.</p>
<p>In 2009 Linus Torvalds admitted that the kernel development is getting out of control.</p>
<blockquote>
We're getting bloated and huge. Yes, it's a problem ... Uh, I'd love to say we have a plan ... I mean, sometimes it's a bit sad that we are definitely not the streamlined, small, hyper-efficient kernel that I envisioned 15 years ago ... The kernel is huge and bloated, and our icache footprint is scary. I mean, there is no question about that. And whenever we add a new feature, it only gets worse.
</blockquote>
<p>At LinuxCon 2014, he said that he thinks the bloat situation is better because modern PCs are a lot faster!</p>
<blockquote>
We've been bloating the kernel over the last 20 years, but hardware has grown faster.
</blockquote>
<p>This is a very problematic attitude.</p>
<p>When software gets bloated it not only becomes more insecure and more error prone, but it also becomes much slower. Thinking that the problem goes away because hardware becomes faster is an immature attitude. In this day and age we need to optimize software so that less power is required, we need to save power and limit pollution.</p>
<p>In a 2007 interview "Why I quit": kernel developer Con Kolivas he stated:</p>
<blockquote>
If there is any one big problem with kernel development and Linux it is the complete disconnection of the development process from normal users. You know, the ones who constitute 99.9% of the Linux user base. The Linux kernel mailing list is the …</blockquote></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.unixsheikh.com/articles/why-you-should-migrate-everything-from-linux-to-bsd.html">https://www.unixsheikh.com/articles/why-you-should-migrate-everything-from-linux-to-bsd.html</a></em></p>]]>
            </description>
            <link>https://www.unixsheikh.com/articles/why-you-should-migrate-everything-from-linux-to-bsd.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26060307</guid>
            <pubDate>Mon, 08 Feb 2021 03:02:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Change your MAC address with a shell script (2019)]]>
            </title>
            <description>
<![CDATA[
Score 188 | Comments 101 (<a href="https://news.ycombinator.com/item?id=26060152">thread link</a>) | @mooreds
<br/>
February 7, 2021 | https://josh.works/shell-script-basics-change-mac-address | <a href="https://web.archive.org/web/*/https://josh.works/shell-script-basics-change-mac-address">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  

  <time datetime="2019-12-18T13:00:00+00:00">December 2019</time>
  <section>
    
    <!-- https://planetjekyll.github.io/snippets/reading-time -->
<p>
  
  
    Reading time: 8 mins
  
</p>

  </section>
  
  
  <h2>Article Table of Contents</h2>
<ul>
  <li><a href="#get-your-current-mac-address">Get your current MAC address</a></li>
  <li><a href="#figure-out-which-adapter-your-machine-is-using-to-connect-to-the-wifi">Figure out which adapter your machine is using to connect to the wifi</a></li>
  <li><a href="#generate-a-random-new-mac-address">Generate a random new MAC address</a></li>
  <li><a href="#set-current-mac-address-to-temporarynew-mac-address">Set current mac address to temporary/new MAC address</a></li>
  <li><a href="#feb-9-2021-update">Feb 9, 2021 update</a></li>
  <li><a href="#related-reading">Related Reading</a></li>
</ul>
  <p>For a while, I’ve had notes from <a href="https://www.online-tech-tips.com/computer-tips/how-to-change-mac-address/">Change or Spoof a MAC Address in Windows or OS X</a> saved, so if I am using a wifi connection that limits me to thirty minutes or an hour, I can “spoof” a new MAC address, and when I re-connect to the wifi, the Access Point thinks I’m on a new, unique device.</p>

<p>For the record - when I’m posted up at a coffee shop for an extended period of time, I make sure to <em>buy products regularly</em> in payment for my time. So, if you’re spoofing your MAC address to use wifi for a longer period of time, maybe make sure to spend $5 or $10 when you run the script.</p>

<p>Now, in case you think that I’m actually saving myself time here, I’m totally not. Here’s why:</p>

<p><img src="https://imgs.xkcd.com/comics/automation.png" alt="XKCD Automation" title="'Automating' comes from the roots 'auto-' meaning 'self-', and 'mating', meaning 'screwing'."></p>

<p><em><a href="https://xkcd.com/1319/">XKCD: Automation</a></em></p>

<p>And:</p>

<p><img src="https://imgs.xkcd.com/comics/is_it_worth_the_time.png" alt="XKCD Is it worth the time?" title="Don't forget the time you spend finding the chart to look up what you save. And the time spent reading this reminder about the time spent. And the time trying to figure out if either of those actually make sense. Remember, every second counts toward your life total, including these right now."></p>

<p><em><a href="https://xkcd.com/1205/">XKCD: Is it worth the time?</a></em></p>

<p>Here’s the steps, for someone who’s on a MAC, to spoof your MAC address:</p>
      <h3 id="get-your-current-mac-address">
        
        
          Get your current MAC address <a href="#get-your-current-mac-address">#</a>
        
        
      </h3>
    

<p>Hold down <code>option</code> key, click your wifi icon:</p>

<p><img src="https://josh.works/images/2019-12-11-bash-basics-01.jpg" alt="wifi details"></p>
      <h3 id="figure-out-which-adapter-your-machine-is-using-to-connect-to-the-wifi">
        
        
          Figure out which adapter your machine is using to connect to the wifi <a href="#figure-out-which-adapter-your-machine-is-using-to-connect-to-the-wifi">#</a>
        
        
      </h3>
    

<div><div><pre><code>ifconfig en0 | <span>grep </span>ether <span># one of these will return a MAC address that matches</span>
ifconfig en1 | <span>grep </span>ether <span># the value you saw when looking for your current</span>
ifconfig en2 | <span>grep </span>ether <span># mac address.</span>
ifconfig en3 | <span>grep </span>ether <span># Keep incrementing the `en0` value until you run out of </span>
                          <span># devices</span>
</code></pre></div></div>

<p>Here’s me working through the list:</p>

<p><img src="https://josh.works/images/2019-12-11-basic-bash-02.jpg" alt="checking all ports"></p>

<p>For me, the very first result matched the MAC address I got from <code>option+click</code>ing the wifi network.</p>

<p>That means I’ll be using <code>en0</code> as the [E]ther[N]et adapter I’ll update shortly.</p>

<!--more-->
      <h3 id="generate-a-random-new-mac-address">
        
        
          Generate a random new MAC address <a href="#generate-a-random-new-mac-address">#</a>
        
        
      </h3>
    

<p>A MAC address has a standard-looking format.</p>

<p>It looks like six blocks of two digits, which happen to be <a href="https://en.wikipedia.org/wiki/Hexadecimal">hexadecimal representations</a> of <code>00000000</code> through <code>11111111</code>.</p>

<p>Here’s some randomly-generated MAC addresses:</p>

<div><div><pre><code>e2:81:f6:f6:f9:e8
1f:24:37:47:d6:25
03:20:3f:48:46:ad
</code></pre></div></div>

<p>To generate a random string of characters that produces a <a href="http://sqa.fyicenter.com/1000208_MAC_Address_Validator.html">valid MAC address</a>, run:</p>

<div><div><pre><code>openssl rand -hex 6 | sed 's/\(..\)/\1:/g; s/.$//'
</code></pre></div></div>

<p>And boom. You’ve got a MAC address.</p>

<p>Now we need to update our current MAC address to this new MAC address.</p>
      <h3 id="set-current-mac-address-to-temporarynew-mac-address">
        
        
          Set current mac address to temporary/new MAC address <a href="#set-current-mac-address-to-temporarynew-mac-address">#</a>
        
        
      </h3>
    

<p>You can use <code>ifconfig</code> to set the ethernet address (is that interchangable with the MAC address?) to the randomly-generated string you got from the <code>openssl</code> command:</p>

<div><div><pre><code><span># you need to sudo it, unfortunately. No quotes around the mac address</span>
<span>sudo </span>ifconfig en0 ether xx:xx:xx:xx:xx:xx

<span># here's the full command</span>
<span>sudo </span>ifconfig en0 ether 8c:85:90:5a:79:56
</code></pre></div></div>

<p>And with that, you can re-connect to the wifi network, and it <em>should</em> recognize you as a new device, and let you onto the network.</p>

<p>Here’s what I first ended up with:</p>

<div><div><pre><code><span>#!/bin/bash</span>

<span>echo</span> <span>"Hi! lets change our mac address."</span>
<span>echo</span> <span>"Step 1: hold down option key, click wifi logo. Note the mac address"</span>
<span>echo</span> <span>"Step 2: click 'disconnect from network' "</span>
<span>echo</span> <span>"Step 3: note which ethernet adapter lines up with the mac address you just saw"</span>

<span>en0</span><span>=</span><span>$(</span> ifconfig en0 | <span>grep </span>ether <span>)</span>
<span>en1</span><span>=</span><span>$(</span> ifconfig en1 | <span>grep </span>ether <span>)</span>
<span>en2</span><span>=</span><span>$(</span> ifconfig en2 | <span>grep </span>ether <span>)</span>
<span>en3</span><span>=</span><span>$(</span> ifconfig en3 | <span>grep </span>ether <span>)</span>
<span>en4</span><span>=</span><span>$(</span> ifconfig en4 | <span>grep </span>ether <span>)</span>
<span>en5</span><span>=</span><span>$(</span> ifconfig en5 | <span>grep </span>ether <span>)</span>

<span>echo </span>en0 is: <span>$en0</span>
<span>echo </span>en1 is: <span>$en1</span>
<span>echo </span>en2 is: <span>$en2</span>
<span>echo </span>en3 is: <span>$en3</span>
<span>echo </span>en4 is: <span>$en4</span>
<span>echo </span>en5 is: <span>$en5</span>

<span>read</span> <span>-p</span> <span>'Step 4: Enter which ethernet device lined up with the given mac address: '</span> ether_adapter
<span>export </span><span>ether_adapter</span><span>=</span><span>$ether_adapter</span>

<span>mac</span><span>=</span><span>$(</span> openssl rand <span>-hex</span> 6 | <span>sed</span> <span>"s/</span><span>\(</span><span>..</span><span>\)</span><span>/</span><span>\1</span><span>:/g; s/.</span><span>$/</span><span>/"</span> <span>)</span>
<span>export </span><span>mac</span><span>=</span><span>$mac</span>

<span>echo</span> <span>"btw, here's the new mac address we're going to use: </span><span>$mac</span><span>"</span>
<span>echo</span> <span>"OK, we will change the mac address associated with: </span><span>$ether_adapter</span><span>"</span>

<span>old_mac</span><span>=</span><span>$(</span> ifconfig <span>$ether_adapter</span> | <span>grep </span>ether <span>)</span>

<span>echo</span> <span>"The old value was: </span><span>$old_mac</span><span>"</span>

<span>sudo </span>ifconfig <span>$ether_adapter</span> ether <span>$mac</span>

<span>new_mac</span><span>=</span><span>$(</span> ifconfig <span>$ether_adapter</span> | <span>grep </span>ether <span>)</span>

<span>echo</span> <span>"The new value is: </span><span>$new_mac</span><span>"</span>
<span>echo</span> <span>"go ahead and re-connect to the wifi. You should be able to join the network."</span>
</code></pre></div></div>

<p>Believe it or not, this script was far from perfect.</p>

<p>First off, not all randomly-generated mac addresses are valid, even though they were passing an online mac address validator I was testing against. <a href="https://askubuntu.com/a/536221">AskUbuntu</a> nicely shared what was going on.</p>

<p>I didn’t want to deal with manually building a valid MAC address; I just noticed that about 2 out of 3 attempts to change my mac address, using the above script, the MAC address didn’t change.</p>

<p>So, my next version of this script pulls the <code>generate mac address</code> and <code>set current mac address to generated mac address</code> steps into a function, and keeps calling the function until the mac address has changed.</p>

<p>Why be elegant and precise when you can brute force a crappy solution?</p>

<p>Here’s my finished “solution”:</p>

<div><div><pre><code><span>#!/bin/bash</span>
<span># skipping some lines</span>
<span>read</span> <span>-p</span> <span>'Step 4: Which device lined up would you like to change? (hit return for en0) '</span> ether_adapter

<span>if</span> <span>[</span> <span>-z</span> <span>$ether_adapter</span> <span>]</span>
<span>then
  </span><span>ether_adapter</span><span>=</span><span>"en0"</span>
<span>fi

</span><span>export </span><span>ether_adapter</span><span>=</span><span>$ether_adapter</span>

generate_and_set_new_mac_address<span>()</span> <span>{</span>
  <span>mac</span><span>=</span><span>$(</span> openssl rand <span>-hex</span> 6 | <span>sed</span> <span>"s/</span><span>\(</span><span>..</span><span>\)</span><span>/</span><span>\1</span><span>:/g; s/./0/2; s/.</span><span>$/</span><span>/"</span><span>)</span>
  <span>export </span><span>mac</span><span>=</span><span>$mac</span>
  <span>echo</span> <span>"OK, we will change the mac address associated with: </span><span>$ether_adapter</span><span>"</span>

  <span>old_mac</span><span>=</span><span>$(</span> ifconfig <span>$ether_adapter</span> | <span>grep </span>ether <span>)</span>

  <span>echo</span> <span>"The old value was: </span><span>$old_mac</span><span>"</span>
  <span>sudo </span>ifconfig <span>$ether_adapter</span> ether <span>$mac</span>

  <span>new_mac</span><span>=</span><span>$(</span> ifconfig <span>$ether_adapter</span> | <span>grep </span>ether <span>)</span>
  <span>echo</span> <span>"The new value is: </span><span>$new_mac</span><span>"</span>
<span>}</span>
<span>echo</span> <span>$new_mac</span>
<span>echo</span> <span>$old_mac</span>

<span>while</span> <span>[</span> <span>"</span><span>$new_mac</span><span>"</span> <span>==</span> <span>"</span><span>$old_mac</span><span>"</span> <span>]</span>
<span>do
  </span><span>echo</span> <span>"not the same"</span>
  generate_and_set_new_mac_address
<span>done

</span><span>echo</span> <span>"go ahead and re-connect to the wifi. You should be able to join the network."</span>
</code></pre></div></div>

<p>and this seems to work pretty well:</p>

<p><img src="https://josh.works/images/2019-12-17-bash_script_success.jpg" alt="success"></p>

<p>I hope that in the near future, I’ll look at this bash script and have many ways to improve it. For now, it’ll do the trick.</p>
      <h3 id="feb-9-2021-update">
        
        
          Feb 9, 2021 update <a href="#feb-9-2021-update">#</a>
        
        
      </h3>
    

<p>Following <a href="https://twitter.com/serent/status/1359208380435361792">this tweet</a> by <code>@serent</code>, I updated the script a little. To generate a valid MAC address:</p>

<div><div><pre><code><span>@@ -160,7 +162,7 @@</span> fi
 export ether_adapter=$ether_adapter

 generate_and_set_new_mac_address() {
<span>-  mac=$( openssl rand -hex 6 | sed "s/\(..\)/\1:/g; s/.$//" )
</span><span>+  mac=$( openssl rand -hex 6 | sed "s/\(..\)/\1:/g; s/./0/2; s/.$//")
</span>   export mac=$mac
   echo "OK, we will change the mac address associated with: $ether_adapter"
</code></pre></div></div>

<p>There’s other pain points in the script. I’ll fix them at some point.</p>
      
    

<ul>
  <li><a href="https://news.ycombinator.com/item?id=26060152#26060229">Hacker News comments from 2021 on this post</a></li>
  <li><a href="https://ryanstutorials.net/bash-scripting-tutorial/bash-script.php">Bash Scripting Tutorial</a></li>
  <li><a href="https://www.online-tech-tips.com/computer-tips/how-to-change-mac-address/">Change or Spoof a MAC Address in Windows or OS X</a></li>
  <li><a href="https://stackoverflow.com/questions/13781216/meaning-of-too-many-arguments-error-from-if-square-brackets">Resolution to <code>[: too many arguments</code> error</a></li>
  <li><a href="https://askubuntu.com/questions/423530/cant-change-my-mac-address-cant-assign-requested-address/536221">AskUbuntu: “Can’t change my mac address - can’t assign requested address”</a></li>
</ul>
  

</article></div>]]>
            </description>
            <link>https://josh.works/shell-script-basics-change-mac-address</link>
            <guid isPermaLink="false">hacker-news-small-sites-26060152</guid>
            <pubDate>Mon, 08 Feb 2021 02:28:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I'm an interviewer at my company and burnt out]]>
            </title>
            <description>
<![CDATA[
Score 53 | Comments 87 (<a href="https://news.ycombinator.com/item?id=26060099">thread link</a>) | @mooreds
<br/>
February 7, 2021 | https://dear.mariechatfield.com/interviewer-burn-out/ | <a href="https://web.archive.org/web/*/https://dear.mariechatfield.com/interviewer-burn-out/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><blockquote>
<p>Dear Marie,</p>
<p>I am one of a handful of women engineers at my company and I have been interviewing 1-2 candidates per week for the past several months. Since I started interviewing, we have only hired white or Asian men.</p>
<p>Bringing in underrepresented identities to my workplace is important to me, and leadership at my company constantly says they value ‘Diversity and Inclusion’. I’ve offered ideas on how to attract a wider group of candidates, but I don’t feel like I am heard or taken seriously. I’m exhausted being part of the interview team with the results we have. At this point, I only want to interview underrepresented candidates, but that seems inappropriate to ask for.</p>
<p>Am I complicit in a racist, sexist system? How can I push for change and preserve my energy?</p>
</blockquote>
<!-- question -->

<p>Hey there,</p>
<p>Interviewing one or two candidates per week for months sounds absolutely exhausting. It’s no wonder that you’re tired and burnt out—especially if it seems like your company isn’t actually hiring anyone outside their established pattern, despite their avowed dedication to diversity and inclusion.</p>
<p><strong>I have found conducting interviews to be difficult and energy-consuming work, particularly if you are invested in treating candidates equitably and with empathy.</strong> It’s not just that the hour-long calendar block in the middle of the day, it’s also:</p>
<ul>
<li>preparing for the interview ahead of time, making sure you have all the resources you need and your environment is set up.</li>
<li>being deeply focused during the interview, paying attention to the smallest of interactions to discern signals (of varying reliability) about a person’s areas of technical expertise and approach to work and collaboration.</li>
<li>responding to candidates on the fly, figuring out the right balance of giving them a hint when they’re stuck and seeing how they solve a problem themselves, and ensuring they have an excellent experience with your company.</li>
<li>writing up feedback afterwards and deliberating your evaluation and whether you’ve let unconscious bias influence you and if your feedback is well-calibrated with the rest of the hiring group so that you’re not consistently scoring everyone higher or lower than the average.</li>
<li>participating in decisions and conversations around interviewing as a whole, and how to make it more equitable and inclusive.</li>
</ul>
<p>It sounds like you aren’t hiring specifically for roles on your immediate team, but rather part of a group of interviewers who support hiring across the whole company. This structure makes sense for larger organizations who have a steady stream of open roles. If everyone helps out a little bit all the time, then no one team’s productivity is completely sunk when they need to hire.</p>
<p>The downside, of course, is that the hiring never really stops. <strong>You don’t get to celebrate by welcoming your new teammate; you just keep interviewing. And interviewing. And interviewing.</strong></p>
<p>At one company, I completed over 100 technical interviews in about two years. I had prided myself on being a very good interviewer, in part because of all the practice. By the end, I was completely miserable and I hated everything about it. I couldn’t convince myself to care about candidates anymore and I was having trouble focusing and giving them my full attention. So, I stopped interviewing. And it was <em>such</em> a relief. It wasn’t even necessarily the time back that I cared about; it was <strong>letting go of the immense responsibility of contributing to hiring decisions, but with so little authority to change anything.</strong></p>
<p>Consider a few of these options, and what each one feels like:</p>
<ul>
<li>What if you reduced your availability to one interview a week?</li>
<li>What if you reduced your availability to two interviews a month?</li>
<li>What if you stopped interviewing for a month?</li>
<li>What if you stopped interviewing for a quarter?</li>
</ul>
<p><strong>Did you feel a sense of relief or joy when you imagined those? Which ones feels the most like what you need right now?</strong></p>
<p>You say that you’ve offered ideas about how to improve your hiring practices and that you <em>“don’t feel like I am heard or taken seriously.”</em> I am guessing that you are not a manager, or any other person who makes the final call on hiring decisions. If you were, my advice would be very different<sup id="fnref-1"><a href="#fn-1">1</a></sup>.</p>
<p>But if I’m right, your main job responsibility is probably to write, release, and maintain software—<em>not</em> to build a team or manage their collective effectiveness. A manager <em>must</em> hire at some point if they want to ensure their team has the right balance of expertise and experience. You probably help out managers at your company by giving them quality feedback about the people they are considering hiring; but it’s unlikely that your annual evaluation centers on your participation in the hiring process or the hiring outcomes at your company.</p>
<p>So… take a break. Don’t stop entirely or forever; you are doing the important work of advocating for inclusion and equity, I would hate for you to give that up entirely. But it sounds like what you’re doing now isn’t working the way you had hoped, and you’re too tired to imagine a bigger and bolder approach right now.</p>
<p><strong>I have quipped that my main career goal is to do such outstanding work that I can be a complete and total pain in the ass about the things that truly matter, like accessibility and inclusion and equity.</strong> That doesn’t mean that I’m waiting until I’ve “arrived” to advocate for what I know is right and to put my privilege to use. It means that I don’t waste my battles on inconsequential things and I try to make myself valuable enough that I’m harder to push out when I do dig in my heels.</p>
<p>But in order to spend social capital, you have to build it up first—and for you, that might mean giving up interviewing for a bit, focusing on your main job responsibility and building relationships with your coworkers, then coming back with a fresh perspective, renewed energy, and a group of like-minded peers with a common goal of improving the hiring process.</p>
<p>Assuming that you don’t have the authority to make a hire/no-hire decision or to revamp your recruiting process, you’re a bit limited in what you can do on your own. <strong>But there is strength in numbers, and it’s harder to ignore a group.</strong> See if you can find or organize a few people who <em>also</em> care about diversity and inclusion in recruiting and hiring. It doesn’t have to be anything formal<sup id="fnref-2"><a href="#fn-2">2</a></sup>, it can just be three people who all +1 each other’s ideas and bring up the same important questions again and again.</p>
<p><strong>You also asked: <em>“Am I complicit in a racist, sexist system?”</em> I’m not sure that I can answer that one for you.</strong> You know your situation much better than I do; if that’s a concern that keeps coming up for you, sit with it. It may be that there’s something you need to do that you haven’t yet.</p>
<p>I think you would definitely be complicit if you stopped raising concerns about inclusion entirely; if you ignored or minimized the concerns raised by other marginalized people because it makes you feel uncomfortable about your own choices or privilege; or if you had the authority to make a change for the better and you didn’t use it.</p>
<p><strong>If you do come to the conclusion that you have not been living up to your values and have instead prioritized your comfort or advancement over the equity of all—welcome to the club.</strong> We mess up a lot, and we learn with empathy and gratitude for the people who corrected us. That’s another benefit to finding some peers who can advocate alongside you—you’ll each have areas of insight the others won’t and you can teach each other.</p>
<p>I hope that you’re able to take a break from interviewing. I hope that when you’re ready to resume, it’s because you’re genuinely excited to have some influence over hiring at your company and you feel equipped to push for practices that you know are worthwhile<sup id="fnref-3"><a href="#fn-3">3</a></sup>. I hope you find support from other engineers who care about inclusion. I hope your company figures out a way to stop using whatever metrics they love that so consistently undervalue Black and Indigenous people, Hispanic/Latinx people, women and people with other minoritized genders, disabled people, LGTBQ+ people, and those who span more than one of those communities.</p>
<p>In short: I hope you get good rest, and that rest fuels you for another round. There is much work to be done. Let’s do it together. 💖</p>
<p>With so much warmth,<br>
Marie</p>
</div></div>]]>
            </description>
            <link>https://dear.mariechatfield.com/interviewer-burn-out/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26060099</guid>
            <pubDate>Mon, 08 Feb 2021 02:16:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Kill a Unicorn]]>
            </title>
            <description>
<![CDATA[
Score 270 | Comments 83 (<a href="https://news.ycombinator.com/item?id=26060038">thread link</a>) | @chrisfrantz
<br/>
February 7, 2021 | https://www.chrisfrantz.com/how-to-kill-a-unicorn/ | <a href="https://web.archive.org/web/*/https://www.chrisfrantz.com/how-to-kill-a-unicorn/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-179"><div><p>I took a deep dive on freemium pricing and decided to document it in this post. Why freemium? I think it's how the little guys can win against market leaders.</p><p>Basically, it's how you kill a unicorn.</p><p>Notion, Figma, Canva, Mailchimp, and Segment. What do all of these platforms have in common, besides being category leaders? Each one has unseated the previous category leaders with less money and smaller teams. Each has also opted for a product led growth strategy that is fueled by product virality and an innovative pricing strategy.</p><p>But let's back up. How did we get here?</p><p>The year is 2012 and the preeminent name in design isn't Sketch or Figma - it's Adobe. 2012 era Adobe is a much different beast to where we are now. There was no real SaaS model and licenses were hundreds of dollars each, locked to a machine, and didn't get upgrades. Still, Adobe is unequivocally the category leader. But in just a few years, that would all change due to an enterprising young entrepreneur and her new business model.</p><p>Canva launched in 2012 and spread like wildfire. In 3 years, the company was able to accumulate accumulated 10 million users, with almost 1 million coming in the first year alone. By the next year, they had 17 thousand videos about their product.</p><p>This kind of fervent user adoption is the stuff of legend. I started hearing about Canva around that time. My then-girlfriend (now wife) had put together a project and it looked much better than anything I could knock out in Photoshop or Illustrator. She mentioned it was not only really easy to use, but it was free! At the time I think I was still bouncing from Adobe license to Adobe license, each of which was half a month's rent, so I was blown away.</p><p>Then I started seeing it everywhere. It was being used for school projects, large advertising campaigns, and was the first recommendation in online groups whenever someone needed a design tool.</p><p>That space used to be occupied by Adobe, but in a matter of a few years, they were being overtaken in the B2C and even a bit in the B2B space by a brand new company with a limited amount of funding.</p><figure><img width="1024" height="570" src="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled1-1024x570.png" alt="" srcset="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled1-1024x570.png 1024w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled1-300x167.png 300w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled1-768x427.png 768w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled1.png 1215w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled1-1024x570.png" data-srcset="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled1-1024x570.png 1024w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled1-300x167.png 300w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled1-768x427.png 768w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled1.png 1215w"></figure><figure><img width="1024" height="572" src="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled2-1024x572.png" alt="" srcset="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled2-1024x572.png 1024w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled2-300x167.png 300w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled2-768x429.png 768w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled2.png 1227w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled2-1024x572.png" data-srcset="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled2-1024x572.png 1024w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled2-300x167.png 300w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled2-768x429.png 768w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled2.png 1227w"></figure><p>Meanwhile, another company, Sketch, was eating away at their market share by offering design-focused tools for modern internet companies. Adobe had Photoshop and Illustrator, neither of which was purpose-built for the internet and both of which were expensive af. Adobe XD wasn't quite production-ready at that point, so Sketch was able to make inroads by offering a lower-priced license for its software.</p><p>Out of left-field comes the Canva of professional design tools: Figma.</p><figure><img width="1024" height="565" src="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled3-1024x565.png" alt="" srcset="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled3-1024x565.png 1024w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled3-300x165.png 300w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled3-768x423.png 768w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled3.png 1255w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled3-1024x565.png" data-srcset="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled3-1024x565.png 1024w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled3-300x165.png 300w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled3-768x423.png 768w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled3.png 1255w"></figure><p>Figma quickly overtakes Sketch by offering a few unique features and an unbeatable pricing model. Freemium, with a twist.</p><p>Figma, like Canva before it, quickly discovered the power that a free product with shareable outputs offers. Users create content with the tool, share the content and then inevitably get asked how it was made. Of course, the user is both happy to share and give a positive recommendation since the output content is being appreciated, and the platform gains another user.</p><p>A couple points to understand about the shiny world of marketing:</p><ul><li>Recommendations from trusted people always lead to higher conversion rates. It's easy to recommend free things since the only exchange is time.</li><li>Backlinks are everything, still. Google hasn't cracked how to build page authority without backlinks and having them everywhere, attached with positive recommendations and high user dwell time once a user clicks a link, is seen as a positive signal.</li><li>Creating content in the app means content can be shared outside the app. That's a simple growth mechanism that can be leveraged mightily once a certain user volume has been hit.</li><li>Finally, email is the best marketing channel still. Freemium platforms exchange email addresses for access and make lead generation a snap.</li></ul><p>So Adobe now is taking fire from all sides and they eventually pivot to a cloud-based subscription model while also focusing all their efforts upmarket on the professional landscape, effectively ceding their B2C audience to Canva.</p><p><em>Quick note: There are many other factors that go into making a platform successful, from the founder to tech to the support. Pricing is just one of the pieces.</em></p><h2><strong>How Freemium really works</strong></h2><p>The common misconception with freemium is that it’s an easy way to not generate any revenue. How many stories have you heard about independent projects that have been worked on for years, only to launch with only a few dollars in revenue, followed by months of slogging before eventually shutting down. That’s very likely&nbsp;<em>not</em>&nbsp;a pricing problem, but a positioning problem. You need to position your product in front of the right audiences and the price becomes a natural fit for your expansion strategy.</p><p>In reality, there are a number of different ingredients in the recipe you need to nail before you can actually execute a freemium strategy effectively. You don’t need a team of hundreds of people, but it does help to lay out the strategy ahead of time.</p><p>Let's break out the different factors involved in setting up a freemium plan.</p><ul><li>Pricing that scales with usage</li><li>Shareable content</li><li>In-app upsells</li><li>Massive market opportunity</li><li>Pricing innovation on the primary value</li></ul><p>First, if you are missing even one of these, it might not be the pricing strategy for you. Each part is key in building out a converting freemium funnel.</p><p><strong>Pricing that scales with usage</strong></p><p>If a user wants to dip a toe in and play around with your platform, they can do that at zero cost (outside of an email address). That’s the basic premise of a freemium plan, but what happens when a user is converted and wants to expand their usage?</p><p>Your pricing should accommodate that natural urge and your product should gracefully allow them to expand their usage with as few barriers as possible.</p><p><strong>In-app upsells aka feature gating</strong></p><p>You need to rope off some of your app so there’s a reason to pay! However, you should charge based on either&nbsp;<strong>secondary user value or</strong>&nbsp;expanding the primary user value, but ideally both.</p><p>Let me explain. Google Analytics made analytics free and accessible to everyone. They charge based on primary user value, but only if you want to expand to a much higher tier of usage. For 99% of people, it will always be free.</p><p>That’s great, but really they only offer expanding their primary user value (analytics) as an upsell. Profitwell, one of my favorite companies utilizing this technique, charges entirely on secondary user value. They offer Profitwell Retain, a dunning service, on top of their analytics. Analytics are totally free. After all, you already have all your financial information integrated into it and onboarded your team.</p><p>Canva however offers all that and more. They let users expand their primary usage through additional templates, but also access other tools like scheduling posts on social media once you upgrade. Why pay 12K a year for Hootsuite when you can just upgrade Canva?</p><figure><img width="1024" height="673" src="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled4-1024x673.jpg" alt="" srcset="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled4-1024x673.jpg 1024w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled4-300x197.jpg 300w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled4-768x505.jpg 768w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled4-1536x1009.jpg 1536w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled4-2048x1346.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled4-1024x673.jpg" data-srcset="https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled4-1024x673.jpg 1024w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled4-300x197.jpg 300w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled4-768x505.jpg 768w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled4-1536x1009.jpg 1536w, https://www.chrisfrantz.com/wp-content/uploads/2021/01/Untitled4-2048x1346.jpg 2048w"></figure><p>With each of these product incentives, users are motioned into the next stage of the tool.</p><p><strong>Shareable content</strong></p><p>This could be considered a growth loop, since we’re building in some type of publishing into the platform so users can share links. Our goal with shareable content is to increase the k factor of a user, effectively turning a free user into a marketing machine. Every time they create content in the app and share it externally, you have another potential opportunity for user acquisition which helps balance out the slightly lower conversion rate you’ll see with a freemium plan.</p><p>You can do this with reports, tables, images, videos, etc. If it can be shared inside a team, you should enable it to be published from your platform.</p><p><strong>Massive Market Opportunity</strong></p><p>Freemium works best when you can boil the ocean. No really, let it rip. The ocean should be a massive existing user base or the early signs of a large user base. You will need the volume to make it work with freemium pricing. Here are the TAMs for a few of the examples.</p><ul><li>Canva: Anyone that needs to create images.</li><li>Profitwell: Anyone that wants to know their businesses metrics</li><li>Google Optimize: Anyone that wants to test anything on their site.</li><li>Microsoft Teams: Anyone that uses Slack. 😉</li></ul><p>Each of these audience segments is huge and represents uniquely large opportunities. Freemium pricing becomes much more difficult to pull off once you have to compete for a small audience with specific needs. You want to create a product that will work with 90% of users.</p><p><strong>Pricing Innovation on Primary Value</strong></p><p>What does this mean?</p><p>A competitive advantage that can’t be innovated against. This is the most important point. How does Slack compete with Teams when users have Teams included in their subscription? How does Baremetrics compete with Profitwell when most of what Baremetrics offered is free with Profitwell?</p><p>Well...they don’t. Baremetrics sold. Slack sold. You can’t compete against freemium when you’re able to offer the primary value for free. The existing market leaders can’t match your pricing without giving up millions in recurring revenue and they’re not going to do that.</p><p>That’s how you beat Adobe or fill-in-the-blank industry behemoth.</p><p>Create a great competitor (the hardest part), offer the primary value for free, then monetize on expanding that primary value or offer a secondary value.</p><h2><strong>I want to be free</strong></h2><p>So I'm putting my money where my mouth is. I'm switching over all my projects to freemium.</p><p>As of today:</p><ul><li><a href="http://snazzy.ai/">Snazzy.ai</a>, content marketing powered by GPT-3, now has a free tier</li><li><a href="http://ga-insights.com/">GA-Insights.com</a>, all your ad networks into Slack, has a free tier being built out</li><li><a href="http://weld.ai/">Weld.ai</a>, in beta as a Supermetrics alternative, will now be completely free at launch.</li></ul><p>The riskiest part of a freemium pricing tier as a bootstrapped founder is getting the volume you need to make up any initial drop in conversion rate.</p><h2><strong>Reference: The market leaders (and the victims)</strong></h2><ul><li>Profitwell: Baremetrics, Chartmogul</li><li>Canva: Photoshop, Illustrator, Word</li><li>Google Optimize: Optimizely, VWO</li><li>Microsoft Teams: Slack</li><li>Figma: Photoshop, Illustrator, Sketch Invision</li></ul><p>Early …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.chrisfrantz.com/how-to-kill-a-unicorn/">https://www.chrisfrantz.com/how-to-kill-a-unicorn/</a></em></p>]]>
            </description>
            <link>https://www.chrisfrantz.com/how-to-kill-a-unicorn/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26060038</guid>
            <pubDate>Mon, 08 Feb 2021 02:03:23 GMT</pubDate>
        </item>
    </channel>
</rss>
