<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 10]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 10. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Sun, 11 Oct 2020 12:33:52 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-10.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Sun, 11 Oct 2020 12:33:52 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Iron, How Did They Make It, Part IVa: Steel Yourself]]>
            </title>
            <description>
<![CDATA[
Score 111 | Comments 16 (<a href="https://news.ycombinator.com/item?id=24726793">thread link</a>) | @parsecs
<br/>
October 8, 2020 | https://acoup.blog/2020/10/09/collections-iron-how-did-they-make-it-part-iva-steel-yourself/ | <a href="https://web.archive.org/web/*/https://acoup.blog/2020/10/09/collections-iron-how-did-they-make-it-part-iva-steel-yourself/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>This week, we continue our four(and a half)-part (<a href="https://acoup.blog/2020/09/18/collections-iron-how-did-they-make-it-part-i-mining/">I</a>, <a href="https://acoup.blog/2020/09/25/collections-iron-how-did-they-make-it-part-ii-trees-for-blooms/">II</a>, <a href="https://acoup.blog/2020/10/02/collections-iron-how-did-they-make-it-part-iii-hammer-time/">III</a>, IVa, IVb) look at pre-modern iron and steel production.  Last week, we looked at how a blacksmith reshapes our iron from a spongy mass called a bloom first into a more workable shape and then finally into some final useful object like a tool.  But as we noted last week, the blacksmith doesn’t just need to manage the shape of the iron, but also its hardness and ductility.</p>



<p>As we’ll see this week, those factors – hardness and ductility (and a bunch of other more complex characteristics of metals which we’re going to leave out for simplicity’s sake) – can be manipulated by changing the chemical composition of the metal itself by <em>alloying</em> the iron with another element, carbon.  And because writing this post has run long and time has run short, <em>next</em> week, we’ll finish up by looking at how those same factors also respond to mechanical effects (work hardening) and heat treatment.</p>



<p>As always, if you like what you are reading here, please share it; if you really like it, you can support me on <a href="https://www.patreon.com/user?u=20122096">Patreon</a>. And if you want updates whenever a new post appears, you can click below for email updates or follow me on twitter (@BretDevereaux) for updates as to new posts as well as my occasional ancient history, foreign policy or military history musings.</p>





<h2>What Is Steel?</h2>



<p>Let’s start with the absolute basics: <em>what is steel</em>?  Fundamentally, <strong>steel is an alloy of iron and carbon</strong>.  We can, for the most part, dispense with many modern varieties of steel that involve more complex alloys; things like stainless steel (which add chromium to the mix) were unknown to pre-modern smiths and produced only by accident.  Natural alloys of this sort (particularly with manganese) might have been produced by accident where local ores had trace amounts of other metals.  This may have led to the common belief among ancient and medieval writers that iron from certain areas was superior to others (steel from <a href="https://en.wikipedia.org/wiki/Noricum">Noricum </a>in the Roman period, for instance, had this reputation, note Buchwald, <em>op. cit.</em> for the evidence of this), though I have not seen this proved with chemical studies.</p>



<p>So we are going to limit ourselves here to just carbon and iron.  Now in video-game logic, that means you take one ‘unit’ of carbon and one ‘unit’ of iron and bash them together in a fire to make steel.  As we’ll see, the process is at least moderately more complicated than that.  But more to the point: <strong>those proportions are totally wrong</strong>.  Steel is a combination of iron and carbon, <em>but not equal parts or anything close to it</em>.  Instead, the general division goes this way (there are several classification systems but they all have the same general grades):</p>



<p>Below 0.05% carbon or so, we just refer to that as iron.  There is going to be some small amount of carbon in most iron objects, picked up in the smelting or forging process.<br>From 0.05% carbon to 0.25% carbon is mild or low carbon steel.<br>From about 0.3% to about 0.6%, we might call medium carbon steel, although I see this classification only infrequently.<br>From <strong>0.6% to around 1.25%</strong> carbon is <em>high-carbon steel</em>, also known as <strong>spring steel</strong>.  For most armor, weapons and tools, this is the ‘good stuff’ (but see below on pattern welding).<br>From <strong>1.25% to 2%</strong> are ‘ultra-high-carbon steels’ which, as far as I can tell didn’t see much use in the ancient or medieval world.<br><strong>Above 2%</strong>, you have <strong>cast iron</strong> or <strong>pig iron</strong>; excessive carbon makes the steel much too hard and brittle, making it unsuitable for most purposes.</p>



<figure><img data-attachment-id="4764" data-permalink="https://acoup.blog/360074001/" data-orig-file="https://acoupdotblog.files.wordpress.com/2020/10/360074001.jpg" data-orig-size="2200,2431" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="360074001" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2020/10/360074001.jpg?w=271" data-large-file="https://acoupdotblog.files.wordpress.com/2020/10/360074001.jpg?w=927" src="https://acoupdotblog.files.wordpress.com/2020/10/360074001.jpg?w=927" alt="" srcset="https://acoupdotblog.files.wordpress.com/2020/10/360074001.jpg?w=927 927w, https://acoupdotblog.files.wordpress.com/2020/10/360074001.jpg?w=1854 1854w, https://acoupdotblog.files.wordpress.com/2020/10/360074001.jpg?w=136 136w, https://acoupdotblog.files.wordpress.com/2020/10/360074001.jpg?w=271 271w, https://acoupdotblog.files.wordpress.com/2020/10/360074001.jpg?w=768 768w" sizes="(max-width: 927px) 100vw, 927px"><figcaption>This is a difficult topic to illustrate so, since the internet is for cat pictures,<a href="https://www.britishmuseum.org/collection/object/A_1993-0714-2"> via the British Museum</a>, here is a Ming Dynasty cast-iron statuette of a cat, 15th or 16th century.  Cast iron production was discovered much earlier in China than in most of the rest of the world, but cast iron products were brittle and not generally suitable for demanding use.</figcaption></figure>



<p>I don’t want to get too bogged down in the exact chemistry of how the introduction of carbon changes the metallic matrix of the iron; <a href="https://en.wikipedia.org/wiki/Steel#Properties">you are welcome to read about it</a>.  <strong>As the carbon content of the iron increases, the iron’s basic characteristics – it’s ductility and hardness (among others) – changes</strong>.  Pure iron, when it takes a heavy impact, tends to deform (bend) to absorb that impact (it is ductile and soft).  Increasing the carbon-content makes the iron harder, causing it to both resist bending more and also to hold an edge better (hardness is the key characteristic for holding an edge through use).  In the right amount, the steel is springy, bending to absorb impacts but rapidly returning to its original shape.  But <em>too much</em> carbon and the steel becomes <em>too</em> hard and not ductile enough, causing it to become brittle.</p>



<p>Compared to the other materials available for tools and weapons, high carbon ‘spring steel’ was essentially the super-material of the pre-modern world.  High carbon steel is <em>dramatically</em> harder than iron, such that a good steel blade will bite – often surprisingly deeply – into an iron blade without much damage to itself.  Moreover, good steel can take fairly high energy impacts and simply bend to absorb the energy before springing back into its original shape (rather than, as with iron, having <em>plastic</em> deformation, where it bends, but doesn’t bend back – which is still better than <em>breaking</em>, but not much).  And for armor, <a href="https://acoup.blog/2019/07/04/collections-archery-distance-and-kiting/">you may recall from our previous</a> look at arrow penetration, a steel plate’s ability to resist puncture is <em>much</em> higher than the same plate made of iron (bronze, by the by, performs about as well as iron, assuming both are work hardened).  of course, different applications still prefer different carbon contents; armor, for instance, tended to benefit from somewhat lower carbon content than a sword blade.</p>



<p>It is sometimes contended that the ancients did not know the difference between iron and steel.  This is mostly a philological argument based on the infrequency of a technical distinction between the two in ancient languages.  Latin authors will frequently use <em>ferrum</em> (iron) to mean both iron and steel; Greek will use <a href="http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.04.0057%3Aentry%3Dsi%2Fdhros&amp;highlight=iron">σίδηρος </a>(sideros, “iron”) much the same way.  The problem here is that high literature in the ancient world – which is almost all of the literature we have – has a strong aversion to technical terms <em>in general</em>; it would do no good for an elite writer to display knowledge more becoming to a tradesman than a senator.  That said in a handful of spots, Latin authors use <a href="http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.04.0059%3Aentry%3Dchalybs1&amp;highlight=steel"><em>chalybs</em> </a>(from the Greek χάλυψ) to mean steel, as distinct from iron.</p>



<p>More to the point, while our elite authors – who are, at most dilettantish observers of metallurgy, never active participants – may or may not know the difference,<strong> ancient artisans clearly did</strong>.  As Tylecote (<em>op. cit.</em>) notes, we see surface carburization on tools as clearly as 1000 B.C. in the Levant and Egypt, although the extent of its use and intentionality is hard to gauge to due rust and damage. There is no such problem with Gallic metallurgy from at least the La Tène period (450 BCE – 50 B.C.) or Roman metallurgy from c. 200 B.C., because we see evidence of smiths quite deliberately varying carbon content over the different parts of sword-blades (more carbon in the edges, less in the core) through pattern welding, which itself can leave a tell-tale ‘streaky’ appearance to the blade (these streaks can be faked, but there’s little point in faking them if they are not already understood to signify a better weapon).  There can be little doubt that the smith who welds a steel edge to an iron core to make a sword blade understands that there is something <em>different</em> about that edge (especially since he cannot, as we can, precisely test the hardness of the two every time – he must know a method that <em>generally</em> produces harder metal and be working from that assumption; high carbon steel, properly produced, can be much harder than iron, as we’ll see).</p>



<figure><img data-attachment-id="4760" data-permalink="https://acoup.blog/34632001/" data-orig-file="https://acoupdotblog.files.wordpress.com/2020/10/34632001.jpg" data-orig-size="2500,1692" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="34632001" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2020/10/34632001.jpg?w=300" data-large-file="https://acoupdotblog.files.wordpress.com/2020/10/34632001.jpg?w=1024" src="https://acoupdotblog.files.wordpress.com/2020/10/34632001.jpg?w=1024" alt="" srcset="https://acoupdotblog.files.wordpress.com/2020/10/34632001.jpg?w=1024 1024w, https://acoupdotblog.files.wordpress.com/2020/10/34632001.jpg?w=2048 2048w, https://acoupdotblog.files.wordpress.com/2020/10/34632001.jpg?w=150 150w, https://acoupdotblog.files.wordpress.com/2020/10/34632001.jpg?w=300 300w, https://acoupdotblog.files.wordpress.com/2020/10/34632001.jpg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><a href="https://www.britishmuseum.org/collection/object/G_1866-0806-1">Via the British Museum</a>, the so-called ‘Sword of Tiberius,’ a Mainz-type Roman gladius from the early imperial period (c. 15 AD).  The sword itself has a mild steel core with high carbon steel edges and a thin coating of high-carbon steel along the flat.  Almost certainly the higher carbon edge was welded on to the mild steel core during manufacture, an example of a blacksmith quite intentionally using different grades of steel.</figcaption></figure>



<p>That said, our ancient – or even medieval – smiths do not understand the chemistry of all of this, of course.  Understanding the effects of carbuzation and how to harness that to make better tools must have been something learned through experience and experimentation, not from theoretical knowledge – a thing passed from master to apprentice, with only slight modification in each generation (though it is equally clear that techniques could move quite quickly over cultural boundaries, since smiths with an inferior technique need only imitate a superior one).</p>



<h2>Making Steel</h2>



<p>Now, in modern steel-making, the main problem is an excess of carbon.  Steel, when smelted in a blast furnace, tends to have far too much carbon.  Consequently a lot of modern iron-working is about walking the steel down to a usefully low amount of carbon <a href="https://en.wikipedia.org/wiki/Steelmaking#Modern_processes">by getting excess carbon out of it</a>.  But ancient iron-working approaches the steeling problem from exactly the opposite direction, likely beginning with something close to a pure mass of iron and having to find ways to get more carbon into that iron to produce steel.</p>



<p><strong>So how do we take our carbon and get it into our iron?</strong>  Well, the good news is that the basic principle is actually very simple: <strong>when hot, iron will absorb carbon from the environment around it, although the process is quite slow</strong> if the iron is not molten (which it never is in these processes).  There are a few stages where that can happen and thus a few different ways of making steel out of our iron.</p>



<p>The popular assumption – in part because it was the working scholarly assumption for quite some time – is that iron can be at least partially …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://acoup.blog/2020/10/09/collections-iron-how-did-they-make-it-part-iva-steel-yourself/">https://acoup.blog/2020/10/09/collections-iron-how-did-they-make-it-part-iva-steel-yourself/</a></em></p>]]>
            </description>
            <link>https://acoup.blog/2020/10/09/collections-iron-how-did-they-make-it-part-iva-steel-yourself/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24726793</guid>
            <pubDate>Fri, 09 Oct 2020 04:20:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bye-Bye, Apple]]>
            </title>
            <description>
<![CDATA[
Score 25 | Comments 40 (<a href="https://news.ycombinator.com/item?id=24726241">thread link</a>) | @rauhl
<br/>
October 8, 2020 | http://blog.cretaria.com/posts/bye-bye-apple.html | <a href="https://web.archive.org/web/*/http://blog.cretaria.com/posts/bye-bye-apple.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <header>
                
                <p>Sync’ up! … without getting drained</p>
            </header>
	    <section>
<article>
<p><abbr>oct 8</abbr></p>
<h2>Bye-bye, Apple</h2>
<p>The days of Apple products are behind me.
I had been developing on a Macbook for over
twelve years, but now, I’ve switched to an
ever trending setup: OpenBSD on a Thinkpad.</p>

<p>The new platform is a winner. Everything is
clean, quick, and configurable. When I 
<code>ps uaxww</code>, I’m not hogging ‘gigs’ of <abbr>RAM</abbr>
just to have things up and running. There’s
no black magic that derails me at every turn.
In short, my sanity has been long restored.</p>

<h3>What I miss</h3>


<p>Nothing is better than a fast web browser.
In Mac, this ‘<abbr>OS</abbr> within the <abbr>OS</abbr>’ was 
a mean beast. It certainly ran fast, but
the Chromium package for OpenBSD isn’t all
that bad.</p>

<p>That magnet power interface was a real win
with the Apple laptops. I miss that, in 
addition to speakers that could be maxed
out to their potential.</p>

<h3>On the other hand…</h3>


<p>There’s a healthy list of things I will
forever be glad to never have to deal 
with again:</p>

<ul>
<li>Xcode</li>
<li>the omnipresent ‘Dock’ (never used it once)</li>
<li>the omnipresent ‘Finder’</li>
<li>‘.DS_Store’ files</li>
<li>black magic in the ‘Terminal.app’</li>
<li>Notifications (and its omnipresent menu hamburger icon)</li>
<li>App store</li>
<li>start-up chord</li>
</ul>
<p>I’ve noticed that with every passing year, the
peripheral interface ports are dwindling. On
an older Macbook, I still had <em>some</em> options (<abbr>SD</abbr>
card reader, <abbr>USB2</abbr>, etc.). But lately, it’s out of
control.</p>

<p>On this middle-of-the-road Thinkpad, I have
an <abbr>SD</abbr> card reader,
<abbr>HDMI</abbr>, scads of <abbr>USB</abbr> ports, <abbr>RJ-45</abbr> —
I’m never going to need a dongle, or say the
word dongle, ever again now that Apple is 
out of my life.</p>

<h3>Home again</h3>


<p>My memory is pretty good. And I recall when
I got my first Mac product: it was because
there was no other decent option for
having a development laptop, but one
where Microsoft Windows wasn’t a requirement.</p>

<p>Many times I tried duct-taping a Linux
install on my various Macs, but things 
were ‘just not there.’ There was always
an issue with this or that, and it was
truly painful.</p>

<p>I think I lost the scent of the trail. 
OpenBSD works so well, I wonder how many
years I could have been using this great
<abbr>OS</abbr> outside of just the server world.</p>

<p>Of course, this setup isn’t for all. If
you’re green on the <abbr>UNIX</abbr> front, or
can’t read a manual, you’d be foolish 
to do it. For the others, it certainly
is a viable solution, to say the least.</p>

<p>I can honestly predict that I can see 
myself using this setup for twenty-five
more years. It’s like coming home to a
quiet, orderly house.</p>

<p>Open your heart to OpenBSD on Thinkpad
at your first opportunity.</p><p><svg xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" viewBox="200 0 400 800" width="400" height="800"><rect width="400" height="800" fill="#b33a3a"></rect><polygon points="299.5 150 254.5 247 351.5 247 "></polygon><path d="m321.9 548.9l-243.8 0"></path><path d="m303 247.1l0 243.8"></path><polygon points="94.5 150 49.5 247 146.5 247 "></polygon><path d="m98 247.1l0 243.8"></path><text font-family="Helvetica, Arial, sans-serif" font-size="60" y="112" x="10" style="fill-opacity:null">THIS END UP</text><a xlink:href="//cretaria.com"><text id="link-cretaria" font-family="Helvetica, Arial, sans-serif" font-size="50" y="680" x="10" fill-opacity="null">What’s Cretaria?</text></a><path d="m175 700l200 0" style="fill-opacity:null;fill:none;stroke-linejoin:null;stroke-opacity:null;stroke-width:4;stroke:#000"></path></svg></p></article>
            </section>
            
        </div></div>]]>
            </description>
            <link>http://blog.cretaria.com/posts/bye-bye-apple.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24726241</guid>
            <pubDate>Fri, 09 Oct 2020 02:49:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Report may suggest that login requirement for Oculus Quest 2 is anticompetitive]]>
            </title>
            <description>
<![CDATA[
Score 300 | Comments 144 (<a href="https://news.ycombinator.com/item?id=24725515">thread link</a>) | @vrfinal
<br/>
October 8, 2020 | https://www.vrfinal.com/report-from-the-house-of-representatives-may-suggest-that-the-facebook-login-requirement-for-the-oculus-quest-2-is-anticompetitive/ | <a href="https://web.archive.org/web/*/https://www.vrfinal.com/report-from-the-house-of-representatives-may-suggest-that-the-facebook-login-requirement-for-the-oculus-quest-2-is-anticompetitive/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <main>
            <article>
                            <p><time datetime="2020-10-08">
                  Oct 08, 2020
                </time>
                <span>2 min read</span>
              </p>
                <div>
    <p><a href="https://www.vrfinal.com/report-from-the-house-of-representatives-may-suggest-that-the-facebook-login-requirement-for-the-oculus-quest-2-is-anticompetitive/">
        <img data-srcset="/content/images/size/w400/2020/10/Oculus.png 400w, /content/images/size/w750/2020/10/Oculus.png 750w, /content/images/size/w960/2020/10/Oculus.png 960w" data-sizes="auto" alt="Report from the House of Representatives may suggest that the Facebook login requirement for the Oculus Quest 2 is anticompetitive." srcset="https://www.vrfinal.com/content/images/size/w400/2020/10/Oculus.png 400w, https://www.vrfinal.com/content/images/size/w750/2020/10/Oculus.png 750w, https://www.vrfinal.com/content/images/size/w960/2020/10/Oculus.png 960w">
      </a>
    </p>
  </div>
              <div>
                <div>
                  <p><a href="https://www.documentcloud.org/documents/7222836-Investigation-of-Competition-in-Digital-Markets.html">A recent report</a> from the US House of Representatives subcommittee on antitrust laws suggests that the requirement for all Quest 2 users to login via a Facebook account may be anticompetitive.</p><figure><img src="https://www.vrfinal.com/content/images/2020/10/ts_oculus-quest-2.png" alt="" srcset="https://www.vrfinal.com/content/images/size/w600/2020/10/ts_oculus-quest-2.png 600w, https://www.vrfinal.com/content/images/2020/10/ts_oculus-quest-2.png 960w" sizes="(min-width: 720px) 720px"></figure><p>The Oculus Quest 2 is the first headset produced by Facebook that requires users to create an account on their social media site in order to set it up. The report states that, <em>“conditioning access to a product or service in which a firm has market power to the use of a separate product or service is anticompetitive.”</em></p><p>The report, which clocks in at a terrify 449 pages, investigates the issues of competition in the digital market. The report looks at companies like Amazon, Google, Apple, and yes, Facebook. The report only mentions VR a small number of times, but it does go into detail about the large acquisitions of that each company has made. This includes Facebook’s purchase of Oculus in 2014.</p><p>The report states,</p><p><em>“Facebook has also maintained and expanded its dominance through a series of acquisitions of companies it viewed as competitive threats, and selectively excluded competitors from using its platform to insulate itself from competitive pressure.</em></p><p><em>Facebook has also maintained its monopoly through a series of anticompetitive business practices. The company used its data advantage to create superior market intelligence to identify nascent competitive threats and then acquire, copy, or kill these firms. Once dominant, Facebook selectively enforced its platform policies based on whether it perceived other companies as competitive threats. In doing so, it advantaged its own services while weakening other firms.”</em></p><figure><img src="https://www.vrfinal.com/content/images/2020/10/Sidequest-New-Logo-1.jpg" alt=""></figure><p>This has major implications for the future of the Oculus and developers, we have seen Facebook flex their considerable power over smaller developers. We have previously reported on the issues that the <a href="https://www.vrfinal.com/unofficial-oculus-quest-appstore-receives-650-000-in-funding/">developer focused app store, Sidequest</a>, has had in gaining purchase in the Oculus ecosystem, not to mention the <a href="https://www.vrfinal.com/vr-developers-are-concerned-about-facebooks-walled-garden/">side-lining of the VR steaming service, Bigscreen</a>, by giving favourable terms to large companies like Fandango. With Facebook offering the most affordable headset on the market, we may see even more developers become disillusioned with the Oculus ecosystem and move on to greener pastures.</p>
                </div>
                  
                              </div>
            </article>
              <section>
    <p><img data-src="/content/images/size/w150/2020/09/IMG_20200812_155324.jpg" alt="Andrew Boggs" src="https://www.vrfinal.com/content/images/size/w150/2020/09/IMG_20200812_155324.jpg">
    </p>
    <div>
      
      <p>Andrew is a Northern Ireland based journalist with a passion for video games. His latest hobby is watching people speedrun Super Mario 64 and realising how bad he is at platformers.</p>
    </div>
  </section>
            <div>
      <div>
        <p><img data-srcset="/content/images/size/w400/2020/10/jump-vr-headset-1021x580.jpg 400w, /content/images/size/w750/2020/10/jump-vr-headset-1021x580.jpg 750w, /content/images/size/w960/2020/10/jump-vr-headset-1021x580.jpg 960w" data-sizes="auto" alt="Co-Founder of The Void announces his new VR attraction: Skydiving" srcset="https://www.vrfinal.com/content/images/size/w400/2020/10/jump-vr-headset-1021x580.jpg 400w, https://www.vrfinal.com/content/images/size/w750/2020/10/jump-vr-headset-1021x580.jpg 750w, https://www.vrfinal.com/content/images/size/w960/2020/10/jump-vr-headset-1021x580.jpg 960w">
        <span>Previous Post</span></p><h4>Co-Founder of The Void announces his new VR attraction: Skydiving</h4>
        </div>

    <div>
      <p><img data-srcset="/content/images/size/w400/2020/10/all-new-zapbox-1.png 400w, /content/images/size/w750/2020/10/all-new-zapbox-1.png 750w, /content/images/size/w960/2020/10/all-new-zapbox-1.png 960w" data-sizes="auto" alt="The All-New ZapBox revealed on Kickstarter, MR headset for only $40" srcset="https://www.vrfinal.com/content/images/size/w400/2020/10/all-new-zapbox-1.png 400w, https://www.vrfinal.com/content/images/size/w750/2020/10/all-new-zapbox-1.png 750w, https://www.vrfinal.com/content/images/size/w960/2020/10/all-new-zapbox-1.png 960w">
      <span>Next Post</span></p><h4>The All-New ZapBox revealed on Kickstarter, MR headset for only $40</h4>
      </div>
</div>            


        </main>
      </div>
    </div></div>]]>
            </description>
            <link>https://www.vrfinal.com/report-from-the-house-of-representatives-may-suggest-that-the-facebook-login-requirement-for-the-oculus-quest-2-is-anticompetitive/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24725515</guid>
            <pubDate>Fri, 09 Oct 2020 00:35:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Density launches Open Area radar system for buildings]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24720736">thread link</a>) | @afar
<br/>
October 8, 2020 | https://www.density.io/blog/introducing-open-area | <a href="https://web.archive.org/web/*/https://www.density.io/blog/introducing-open-area">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><blockquote>"Any sufficiently advanced technology is indistinguishable from magic."</blockquote><h6>— Arthur C. Clarke, <em>Profiles of the Future</em> <em>(1962)</em></h6><h4>Open Area</h4><p>In 2017, we had a customer tell us she spent $700,000 every year for each building in a 4 million square foot office portfolio. $700,000 bought her human consultants who would visit her offices and do 7-day observational studies of how busy different spaces were (once per quarter).</p><p>As it turns out, she was not alone. Over the years, hundreds of customers have asked if they could use the infrared technology in our Entry sensors for open space detection (to measure desk availability, lounge use, how people use an amenity, and so on). Our answer has always had to be, no. </p><p>Not yet, anyway.</p><p>Since then we have been thinking about and working to solve the thorny problem of counting people in unbounded space and making it affordable to scale to tens of thousands of business and hundreds of millions of square feet.</p><p>Today, we're proud to introduce the latest addition to our platform — Density Open Area.</p><figure id="w-node-5d0de9b0c67f-665b10f0"><p><img src="https://assets-global.website-files.com/5f4a004f01308268d80d6e85/5f7cd78f8d47e339ff7661b4_Open%20Area%20in%20hand.png" loading="lazy" alt=""></p><figcaption>Open Area, a radar based sensor</figcaption></figure><h4>Technical Leaps</h4><p>Once, every few years, you get a glimpse of the future and how it might work. The famous ones are well known: the internet growing <a href="https://www.cnbc.com/2020/01/17/at-age-30-jeff-bezos-thought-this-would-be-his-one-big-regret-in-life.html">at 2,300%</a>, Englebart's <a href="https://www.youtube.com/watch?v=yJDv-zdhzMY&amp;t=153s">mother of all demos</a>, Steve's <a href="https://web.stanford.edu/dept/SUL/sites/mac/parc.html">visit to Xerox PARC</a>.</p><p>The importance of a novel observation or technical leap is obvious in retrospect but it's easy to disregard in the moment:&nbsp;hypertext, vaccines, the cambered wing, luggage with wheels, even the steam engine was not of any immediate consequence. It often takes decades even centuries of maturing before any given innovation's future is assured. But every now and then, if you squint, you sometimes get a chance to make out the rough profile of the future.</p><p>This is what we saw:</p><figure id="w-node-7ff48cc6176d-665b10f0"></figure><p>‍</p><h4>The Power of Radar</h4><p>Open Area leverages a radar system of our own design. </p><p>Each dot is a depth value generated from thousands of small movements in three dimensional space. We use these clustered data points to count people and observe movement anonymously.</p><p>Open Area's range and ability is extraordinary. The sensor is accurate up to 20 feet off the ground, can handle 1,325 square feet, and has a dynamic field of view configurable through a web app. </p><p>The technology fits in the palm of your hand, is unaffected by sunlight or reflectivity, and mounts in minutes. It is more accurate than a camera, anonymous at source, and made in America.</p><figure id="w-node-dc27d77422eb-665b10f0"><p><img src="https://assets-global.website-files.com/5f4a004f01308268d80d6e85/5f7ec280f461fdee990537d7_looping%20animation.gif" loading="lazy" alt=""></p></figure><h4>‍</h4><h4>Features &amp; Benefits</h4><p>The sensor comes with a suite of new applications designed to take advantage of Open Area's unique aerial dataset. Users will be able to access:</p><ul role="list"><li>60% reduction in cost to deploy (vs. camera / optical alternatives).</li><li>20 foot range, 40 foot effective diameter (4x coverage of alternatives).</li><li>1,325 square feet of coverage</li><li>Measure up to 20 desks (early Alpha)</li><li>Historical occupant pathing and heatmaps</li><li>Desk and room availability (+&nbsp;release)</li><li>Touchdowns and dwell time</li></ul><figure id="w-node-3dc275f7db55-665b10f0"><p><img src="https://assets-global.website-files.com/5f4a004f01308268d80d6e85/5f7ec65d2e837f523f524063_oa%20software.png" loading="lazy" alt=""></p><figcaption>Density web application</figcaption></figure><h4>Availability &amp;&nbsp;Price</h4><p>Available today in limited quantity. Large scale production starts early 2021.</p><ul role="list"><li>Hardware: $399 / sensor*</li><li>Software:&nbsp;$199 / sensor / year*</li></ul><p>‍<em>*Introductory pricing is available on orders through 2020.</em></p><p>‍</p><h4>One more thing ...</h4><p>In the process of exploring Open Area's unique capabilities, we realized something novel – a way of looking at people in space we'd never seen before. </p><p>Synchronize your floorplan and turn on Density Live. It will feel like a fragment of the future.</p><p>‍</p><figure id="w-node-735909241e83-665b10f0"></figure><p>‍</p><p>You can register to see a product demo <a href="https://density.webflow.io/people-counting-resources-webinars/introducing-open-area-densitys-newest-sensor-offering">October 20th</a> or send us an email to learn more – sales@density.io. </p><p>We can't wait to see what you do with the tech.</p><p>Andrew, Density CEO<br></p></div></div></div>]]>
            </description>
            <link>https://www.density.io/blog/introducing-open-area</link>
            <guid isPermaLink="false">hacker-news-small-sites-24720736</guid>
            <pubDate>Thu, 08 Oct 2020 16:31:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: I reverse engineered my cable modem and turned it into an SDR]]>
            </title>
            <description>
<![CDATA[
Score 311 | Comments 64 (<a href="https://news.ycombinator.com/item?id=24719680">thread link</a>) | @0x00000000
<br/>
October 8, 2020 | https://stdw.github.io/cm-sdr/ | <a href="https://web.archive.org/web/*/https://stdw.github.io/cm-sdr/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

      <section>
        <div id="title">
          
          
          <hr>
          <p><span>Project maintained by <a href="https://github.com/stdw">stdw</a></span>
          <span>Hosted on GitHub Pages — Theme by <a href="https://twitter.com/michigangraham">mattgraham</a></span>
        </p></div>

        

<p><img src="https://stdw.github.io/cm-sdr/img/modem.jpg" alt="cable modem"></p>

<h2 id="introduction">Introduction</h2>
<p>A few weeks ago I got curious about an old cable modem sitting in my closet,
a Motorola MB7220. Initially I was interested in what kind of hardware it had
and if it was running Linux. Some quick searching brought me to a thread on
a web forum where people were discussing the built in spectrum analyzer feature
used for diagnostics. Someone mentioned that they could see spikes
corresponding to FM radio stations. This sparked a thought: if a cable modem 
and a digital TV tuner dongle are fundamentally doing the same thing (receiving 
and demodulating QAM signals), could a modem be turned into an 
<a href="https://en.wikipedia.org/wiki/Software-defined_radio">SDR (software-defined radio)</a>
a la <a href="https://www.rtl-sdr.com/">RTL-SDR</a>?</p>

<p>Going into this project, I knew next to nothing about RF and had no idea if
this goal was even feasible at all for the hardware. I found 
<a href="http://www.hermeslite.com/">an SDR project</a> based on an Analog Devices 
cable modem chip, as well as a <a href="https://forums.qrz.com/index.php?threads/cable-modem-to-software-defined-radio-modification-projects.512433/">forum thread</a>
where someone else was wondering about the same thing a few years ago.</p>

<p>The last post in the thread from user VK4HAT states:</p>

<blockquote>
  <p>I say if you have the skills, time and desire, give it a go and see where you end up. If google shows nothing, then its likely not been tried. With so few firsts available in life, take those that present themselves and have a crack, even if failure is always an option.</p>
</blockquote>

<p>So that is exactly what I did.</p>

<h2 id="gaining-access">Gaining Access</h2>
<p>My first goal was to look for an access vector or a way to communicate with the
device. I knew that there wasn’t much to see on the web interface and telnet
was disabled, so I skipped ahead to opening it up.</p>

<p>After removing a few screws from the plastic housing to get access to the
board, my first thought was to look for <a href="https://en.wikipedia.org/wiki/Universal_asynchronous_receiver-transmitter">UART</a> headers to take a peek at the serial console. 
After identifying two candidates consisting of four vias surrounded by a 
rectangle near the edge of the PCB, it was time to identify the pins. 
Using a multimeter, the ground pin can be easily identified by checking 
the continuity with one of the metal shields on board. The VCC pin can be 
identified by measuring the voltage of each pin when powering on the board. 
It should be a steady 3.3v, or in some cases 1.8v or 5v. This pin is not 
needed, but is still useful to identify the operating voltage and eliminate 
one candidate for the Tx and Rx pins.
While booting, the Tx pin will sit on average a little lower than the VCC pin
and drop much lower when a lot of data is being output. This leaves the last 
pin as Rx.</p>

<p>One of the UARTs identified earlier did not seem to be transmitting anything
while the other did. After soldering some wires to the active UART, I connected
the Tx to UART Rx GPIO pin on a Raspberry Pi, the Rx to the Pi’s Tx, and the 
ground to the ground pin. Note that this can only be done because both systems
are 3.3v. Had that not been the case, a USB TTL adapter with an adjustable 
voltage level could be used just as easily, and is probably a better idea most
of the time anyway.</p>

<p>There are a few reasons why the Raspberry Pi is not the best serial interface
such as if you need parity or other features, but in this case I had it on hand
and it works. The serial console of the Pi must also be disabled so that it can 
be freed up for other purposes. There is another reason I chose to use the 
Raspberry Pi which I will get to later.</p>

<p>Finally, to actually see the data I used the <code>cu</code> utility:<br>
<code>cu -l /dev/serial0 -s 115200</code><br>
The baud rate was a lucky guess, but 115200 is very common on such devices.
If the baud rate is wrong you will quickly know when you see a bunch of garbage
on the screen. A logic analyzer could be used to definitively find the baud 
rate and other parameters, but guessing is sometimes quicker and always 
cheaper.</p>

<p>After powering on the device, the terminal filled with output:</p>

<div><div><pre><code>pi@raspberrypi:~/modem $ cu -l /dev/serial0 -s 115200
Connected.
�
B3312inim S C 84(9 m
ose_VS 8
STesldlo rh 83 rs 10
STesldhi: _h 8, _s 13
Sync: 0 
MemSize:            128 M
Chip ID:     BCM3383D-B0

BootLoader Version: 2.4.0 fyl spiboot reduced DDR drive avs
Build Date: Nov 12 2015
Build Time: 14:31:43
SPI flash ID 0xef4016, size 4MB, block size 64KB, write buffer 256, flags 0x0
Cust key size 128

Signature/PID: 3383


Image 1 Program Header:
   Signature: 3383
     Control: 0005
   Major Rev: 0003
   Minor Rev: 0000
  Build Time: 2015/11/26 08:47:57 Z
 File Length: 1692841 bytes
Load Address: 80004000
    Filename: ecram_sto.bin
         HCS: e749
         CRC: 175b753f

Found image 1 at offset 20000

Enter '1', '2', or 'p' within 2 seconds or take default...


Performing CRC on Image 1...
CRC time = 282177012
Detected LZMA compressed image... decompressing... 
Target Address: 0x80004000
decompressSpace is 0x8000000
Elapsed time 736066500

Decompressed length: 8091524

Executing Image 1...


 eCos - hal_diag_init
Ecos memory map:
BLOCK    OWNER        MIPS      SIZE      MEM
Block 0: Owner: 0 - 0x00000000 0x07e00000 0x00000000
Block 0: Owner: 0 - 0 MB 126 MB 0 MB
Block 1: Owner: 3 - 0x07e00000 0x00200000 0x07e00000
Block 1: Owner: 3 - 126 MB 2 MB 126 MB
126MB (129024KB) remaining for eCos
Init device '/dev/BrcmTelnetIoDriver'
Init device '/dev/ttydiag'
Init tty channel: 807bb020
Init device '/dev/tty0'
Init tty channel: 807bb040
Init device '/dev/haldiag'
HAL/diag SERIAL init
Init device '/dev/ser0'
BCM 33XX SERIAL init - dev: b4e00500.2
Set output buffer - buf: 0x80852408 len: 4096
Set input buffer - buf: 0x80853408 len: 4096
BCM 33XX SERIAL config
Init device '/dev/ser1'
BCM 33XX SERIAL init - dev: b4e00520.3
Set output buffer - buf: 0x80854408 len: 4096
Set input buffer - buf: 0x80855408 len: 4096
BCM 33XX SERIAL config

Init device '/dev/ser2'
InitBoard: MIPS frequency 637200000

...

Reading Permanent settings from non-vol...
Checksum for permanent settings:  0xe9d88f65
Setting downstream calibration signature to '5.7.1mp1|die temperature:70.775degC'
Settings were read and verified.


Reading Dynamic settings from non-vol...
Checksum for dynamic settings:  0x6e4a329
Settings were read and verified.

Console input has been disabled in non-vol.
Console output has been disabled in non-vol!  Goodbye...
[00:00:00 01/01/1970] [Reset/Standby Switch Thread] BcmResetStandbySwitchThread::ProcessResetSwitchEvent:  (Reset/Standby Switch Thread) Reset switch released; resetting...
[00:00:00 01/01/1970] [Reset/Standby Switch Thread] BcmResetStandbySwitchThread::ProcessResetSwitchEvent:  (Reset/Standby Switch Thread) Cant Reset pfCmDocsisCtlThread==NULL...
</code></pre></div></div>

<p>This output contains a wealth of information. The device is 
running <a href="https://en.wikipedia.org/wiki/ECos">eCos</a> on a MIPS processor 
which is part of a Broadcom BCM3383 SoC. It turns out there are actually
two MIPS processors on this SoC although one of them is not used on this
modem, explaining the other UART. On some devices, the second processor
will run Linux for additional features.</p>

<p>Also, this seems like the end of the line for serial because shortly after 
booting the actual OS, it disables the serial console. Hitting “p” at the 
bootloader prompt does not lead to much except a way to download new OS 
images via tftp and a utility to read and write memory addresses. This could
be used to bypass the check, but a much greater understanding of the OS and
memory layout would be required.</p>

<h2 id="dumping-the-flash">Dumping the flash</h2>

<p>My goal now was to enable the serial console. Examination of the board reveals
a single <a href="https://en.wikipedia.org/wiki/Serial_Peripheral_Interface">SPI</a> flash
chip which likely contains the bootloader, OS, and configuration as it is the
only non-volatile storage visible on the board.</p>

<p>This is where the Raspberry Pi comes in handy once again. The GPIO header also
conveniently contains a SPI interface which can be used to read the data off
of the flash chip.</p>

<p>Searching the number on the chip, “winbond 25Q32JV”, yields the datasheet
containing the pinout. The important ones are VCC, Chip Select (CS), Clock
(CLK), Data Out (DO), Data In (DI), and ground.</p>

<p>One common issue with dumping a SPI chip on a board is that the chip requires
power, but this will also usually power the board and cause it to start booting
and using the chip. I chose to overcome this by heating the VCC pin with my
soldering iron and very carefully lifting it off the pad. This is a convenient,
but rather crude solution which may result in snapped off leads so use at your
own risk! I also soldered a jumper wire to the pad and another to the floating
leg so that I could easily connect and disconnect them and allow the device to
boot again.</p>

<p>Another note, on some boards the Chip Select pin is assumed to always be 
enabled so it is directly tied to VCC. This means when you power the CS 
pin, the board also starts booting. This can be solved in a similar way
to the VCC pin.</p>

<p>Now, wires can be soldered to the rest of the pins and the they can be
connected to the Raspberry Pi. The ground goes to ground (the UART ground
from earlier can also be used), the VCC to the Pi’s 3.3v pin. (Again, it is
critical to verify with the datasheet that this is a 3.3v chip because the Pi
only supports 3.3v). The DO pin is connected to the Pi’s SPI <code>MISO</code> (master in 
slave out) pin and DI to the <code>MOSI</code> pin (master out slave in). Lastly, the 
Clock is connected to the <code>SCLK</code> GPIO pin and the Chip Select to the <code>CE0</code> pin.</p>

<table>
  <thead>
    <tr>
      <th><img src="https://stdw.github.io/cm-sdr/img/chip.jpg" alt="flash chip"></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Not the best soldering job but it will work</em></td>
    </tr>
  </tbody>
</table>

<p>To actually read the chip, there is a fantastic tool called 
<a href="https://flashrom.org/Flashrom">flashrom</a> which supports an enormous number of
chips. <code>flashrom</code> is present in the repos of many distributions including that
of the Raspberry Pi OS (formerly known as Raspbian).</p>

<p>Luckily the W25Q32JV is supported, under the name “W25Q32.V”. A quick check on
the flashrom wiki shows the size and voltage match what is expected and that
the chip is fully supported.</p>

<p>Before proceeding, ensure that the SPI interface on the Pi is enabled by
using the <code>raspi-config</code> utility and checking under “Interfacing Options”.</p>

<p>At last we can read the chip. First verify that it is …</p></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://stdw.github.io/cm-sdr/">https://stdw.github.io/cm-sdr/</a></em></p>]]>
            </description>
            <link>https://stdw.github.io/cm-sdr/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24719680</guid>
            <pubDate>Thu, 08 Oct 2020 15:00:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Guide to Deep Learning and Neural Networks]]>
            </title>
            <description>
<![CDATA[
Score 61 | Comments 8 (<a href="https://news.ycombinator.com/item?id=24719670">thread link</a>) | @NaeosPsy
<br/>
October 8, 2020 | https://serokell.io/blog/deep-learning-and-neural-network-guide | <a href="https://web.archive.org/web/*/https://serokell.io/blog/deep-learning-and-neural-network-guide">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>As a subset of artificial intelligence, deep learning lies at the heart of various innovations: self-driving cars, natural language processing, image recognition and so on. Companies that deliver DL solutions (such as Amazon, Tesla, Salesforce) are at the forefront of stock markets and attract impressive investments. According to <a href="https://www.statista.com/statistics/621468/worldwide-artificial-intelligence-startup-company-funding-by-year/">Statista</a>, the total funding of artificial intelligence startup companies worldwide in 2014–2019 is equal to more than $26 billion. This high interest can be explained by the amazing benefits of deep learning and its architectures — artificial neural networks.</p><p><img src="https://serokell.io/files/3s/3slpcvqe.1_(32)_(1).jpg" alt="AI startup funding graph"></p><h2 id="what-is-deep-learning%3F">What is deep learning?</h2><p><img src="https://serokell.io/files/yc/yctimg60.deviator-1_(1).jpg" alt="what is deep learning"></p><p>Deep learning is one of the subsets of machine learning that uses deep learning algorithms to implicitly come up with important conclusions based on input data.</p><p>Usually, deep learning is unsupervised or semi-supervised. Deep learning is based on <a href="https://en.wikipedia.org/wiki/Feature_learning#:~:text=In%20machine%20learning%2C%20feature%20learning,or%20classification%20from%20raw%20data.">representation learning</a>. Instead of using task-specific algorithms, it learns from representative examples. For example, if you want to build a model that recognizes cats by species, you need to prepare a database that includes a lot of different cat images.</p><p>The main architectures of deep learning are:</p><ul>
<li>Convolutional neural networks</li>
<li>Recurrent neural networks</li>
<li>Generative adversarial networks</li>
<li>Recursive neural networks</li>
</ul><p>We are going to talk about them more in detail later in this text.</p><h3 id="difference-between-machine-learning-and-deep-learning">Difference between machine learning and deep learning</h3><p>Machine learning attempts to extract new knowledge from a large set of pre-processed data loaded into the system. Programmers need to formulate the rules for the machine, and it learns based on them. Sometimes, a human might intervene to correct its errors.</p><p>However, deep learning is a bit different:</p><table>
  <tbody><tr>
   <th>Deep learning
   </th>
   <th>Machine learning
   </th>
  </tr>
  <tr>
   <td>large amounts of data
   </td>
   <td>small datasets as long as they are high-quality
   </td>
  </tr>
  <tr>
   <td>computation-heavy
   </td>
   <td>not always
   </td>
  </tr>
  <tr>
   <td>an draw accurate conclusions from raw data
   </td>
   <td>carefully pre-processed data
   </td>
  </tr>
  <tr>
   <td>take much longer to train
   </td>
   <td>can be trained in a reduced amount of time
   </td>
  </tr>
  <tr>
   <td>you can't know what are the particular features that the neurons  represent
   </td>
   <td>logic behind the machine’s decision is clear
   </td>
  </tr>
  <tr>
   <td>can be used in unexpected ways
   </td>
   <td>algorithm is built to solve a specific problem
   </td>
  </tr>
</tbody></table><h2 id="advantages-of-deep-learning">Advantages of deep learning</h2><p>Now that you know what the difference between DL and ML is, let us look at some advantages of deep learning.</p><ul>
<li>In 2015, a group of Google engineers was conducting research about <a href="https://ai.googleblog.com/2015/07/deepdream-code-example-for-visualizing.html">how NN carry out classification tasks</a>. By chance, they also noticed that neural networks can hallucinate and <a href="https://www.youtube.com/watch?v=uSUOdu_5MPc&amp;t=932s">produce rather interesting art</a>.</li>
<li>The ability to identify patterns and anomalies in large volumes of raw data enables deep learning to efficiently deliver accurate and reliable analysis results to professionals. For example, Amazon has more than <a href="https://www.digitalcommerce360.com/article/amazon-sales/">560 million items on the website and 300+ million users</a>. No human accountant or even a whole army of accountants would be able to track that many transactions without an AI tool.</li>
<li>Deep learning doesn’t rely on human expertise as much as traditional machine learning. DL allows us to make discoveries in data even when the developers are not sure what they are trying to find. For example, you want your algorithms to be able to <a href="https://www.digitalocean.com/community/tutorials/how-to-build-a-deep-learning-model-to-predict-employee-retention-using-keras-and-tensorflow">predict customer retention</a>, but you’re not sure which characteristics of a customer will enable the system to make this prediction.</li>
</ul><p><img src="https://serokell.io/files/b3/b37v6nzo.2_(24)_(1).jpg" alt="Deep learning advantages"></p><h2 id="problems-of-deep-learning">Problems of deep learning</h2><ul>
<li>Large amounts of quality data are resource-consuming to collect. For many years, the largest and best-prepared collection of samples was <a href="https://www.zdnet.com/article/worlds-largest-image-database-to-help-computers-learn-to-see/#:~:text=To%20develop%20a%20system%20that,holds%2014%20million%20labeled%20images.">ImageNet with 14 million different images</a> and more than 20,000 categories. It was founded in 2012, and only last year, <a href="https://neurohive.io/en/datasets/tencent-dataset/">Tencent released a database</a> that is larger and more versatile.</li>
<li>Another difficulty with deep learning technology is that it cannot provide reasons for its conclusions. Therefore, it is difficult to assess the performance of the model if you are not aware of what the output is supposed to be. Unlike in traditional machine learning, you will not be able to test the algorithm and find out why your system decided that, for example, it is a cat in the picture and not a dog.</li>
<li>It is very costly to build deep learning algorithms. It is impossible without qualified staff who are trained to work with sophisticated maths. Moreover, deep learning is a resource-intensive technology. It requires powerful GPUs and a lot of memory to train the models. A lot of memory is needed to store input data, weight parameters, and activation functions as an input propagates through the network. Sometimes deep learning algorithms become so power-hungry that researchers prefer to use <a href="https://serokell.io/blog/how-to-choose-ml-technique">other algorithms</a>, even sacrificing the accuracy of predictions.</li>
</ul><p>However, in many cases, deep learning cannot be substituted.</p><iframe width="560" height="315" src="https://www.youtube.com/embed/0VH1Lim8gL8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe><h2 id="how-can-you-apply-dl-to-real-life-problems%3F">How can you apply DL to real-life problems?</h2><p><img src="https://serokell.io/files/66/66a4xqmg.4_(18)_(1).jpg" alt="Deep learning applications"></p><p>Today, deep learning is applied across different industries for various use cases:</p><ul>
<li><strong>Speech recognition.</strong> All major commercial speech recognition systems (like Microsoft Cortana, Alexa, Google assistant, Apple Siri) are based on deep learning-based.</li>
<li><strong>Pattern recognition.</strong> Pattern recognition systems are already able to give more accurate results than the human eye in <a href="https://www.bbc.com/news/health-50857759#:~:text=Artificial%20intelligence%20is%20more%20accurate,images%20from%20nearly%2029%2C000%20women.">medical diagnosis</a>.</li>
<li><strong>Natural language processing.</strong> Neural networks have been used to implement language models since the early 2000s. The invention of <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM</a> helped improve machine translation and language modeling.</li>
<li><strong>Discovery of new drugs.</strong> For example, the <a href="https://arxiv.org/abs/1510.02855">AtomNet neural network</a> has been used to predict new biomolecules that can potentially cure diseases such as Ebola and multiple sclerosis.</li>
<li><strong>Recommender systems.</strong> Today, deep learning is being used to study user preferences across many domains. <a href="https://www.netflix.com/">Netflix</a> is one of the brightest examples in this field.</li>
</ul><h2 id="what-are-artificial-neural-networks%3F">What are artificial neural networks?</h2><p><img src="https://serokell.io/files/vd/vd78l0x8.deviator-2_(1).jpg" alt="What are artificial neural networks"></p><p>“Artificial neural networks” and “deep learning” are often used interchangeably, which isn’t really correct. Not all neural networks are “deep”, meaning “with many hidden layers”, and not all deep learning architectures are neural networks. There are also <a href="https://en.wikipedia.org/wiki/Deep_belief_network#:~:text=In%20machine%20learning%2C%20a%20deep,between%20units%20within%20each%20layer.">deep belief networks</a>, for example.</p><p><img src="https://serokell.io/files/vk/vkpzrxrf.5_(12)_(1).jpg" alt="neural networks"></p><p>However, since neural networks are the most hyped algorithms right now and are, in fact, very useful for solving complex tasks, we are going to talk about them in this post.</p><h3 id="definition-of-an-ann">Definition of an ANN</h3><p>An artificial neural network is heavily inspired by the structure of a human brain. Simply put, an ANN represents a sequence of neurons connected by synapses. Those sequences are often organized into layers.</p><p>Having many (sometimes millions) of input neurons in the system, the machine learns to analyze and even memorize various information. Due to this structure, a neural network can process monstrous amounts of information very fast.</p><p>Here is a video for those who want to dive deeper into the technical details of how artificial neural networks work.</p><iframe width="560" height="315" src="https://www.youtube.com/embed/njKP3FqW3Sk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe><p>Artificial neural networs are incredibly valuable not only to analyze incoming information but also to reproduce it from their memory.</p><h2 id="components-of-neural-networks">Components of Neural Networks</h2><p>Every neural network consists of neurons, synapses, weights, biases, and functions.</p><h3 id="neurons">Neurons</h3><p>A neuron or a node of a neural network is a computing unit that receives information, performs simple calculations with it, and passes it further.</p><p>All neurons in a net are divided into three groups:</p><ul>
<li>Input neurons that receive information from the outside world;</li>
<li>Hidden neurons that process that information;</li>
<li>Output neurons that produce a conclusion.</li>
</ul><p><img src="https://serokell.io/files/yt/ytl4jey2.6_(8)_(1).jpg" alt="ML architecture"></p><p>In a large neural network with many neurons and connections between them, neurons are organized in layers. An input layer receives information, n hidden layers (at least 3+) process it, and an output layer provides some result.</p><p>Each of the neurons inputs and outputs some data. If this is the first layer, input = output. In other cases, the information that the neurons have received from the previous layer is passed to input. Then, it uses an activation function to get a new output, which is passed to the next layer of neurons in the system.</p><p>Neurons only operate numbers in the range [0,1] or [-1,1]. In order to turn data into something that a neuron can work with, we need normalization. We talked about what it is in the <a href="https://serokell.io/blog/regression-analysis-overview">post about regression analysis</a>.</p><p>Wait, but how do neurons communicate? Through synapses.</p><h3 id="synapses-and-weights">Synapses and weights</h3><p>If we didn’t have synapses, we would be stuck with a bunch of inactive useless neurons. A synapse is a connection between two neurons. Every synapse has a weight. It is the weight that changes the input information while it is transmitted from one neuron to another. The neuron with the greater weight will be dominant in the next neuron. One can say that the <a href="https://en.wikipedia.org/wiki/Weighing_matrix">matrix of weights</a> is the brain of the whole neural system.</p><p><img src="https://serokell.io/files/b9/b92z8vod.7_(9)_(1).jpg" alt="Neuron weights"></p><p>It is thanks to these weights that the input information is processed and converted into a result. During the initialization (first launch of the NN), the weights are randomly assigned. Later on, they are optimized.</p><h3 id="bias">Bias</h3><p>A bias neuron allows for more variations of weights to be stored. Biases add richer representation of the input space to the model’s weights.</p><p>In the case of neural networks, a bias neuron is added to every layer. It plays a vital role by making it possible to move the activation function to the left or right on the graph.</p><p><img src="https://serokell.io/files/ey/eyarbo1y.8_(5)_(1).jpg" alt="bias neurons"></p><p>It is true that ANNs can work without bias neurons. However, they are almost always added and counted as an indispensable part of the overall model.</p><h2 id="how-anns-work">How ANNs work</h2><p>Every neuron processes input data to extract a feature. Let’s imagine that we have features x1, x2, x3, and three neurons, each of which is connected with all these features.</p><p>Each of the neurons has its own weights that are used to weight the features. During the training of the network, you need to select such weights for each of the neurons that the output provided by the whole network would be true-to-life.</p><p>To perform transformations and get an output, every neuron has an activation function. It allows us to get some new feature space. This combination of functions performs a transformation that is described by a common function F — this describes the formula behind the NN’s magic.</p><p><img src="https://serokell.io/files/ly/ly9z5sh4.9_(3)_(1).jpg" alt="ANN: activation function"></p><p>There are a lot of activation functions, …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://serokell.io/blog/deep-learning-and-neural-network-guide">https://serokell.io/blog/deep-learning-and-neural-network-guide</a></em></p>]]>
            </description>
            <link>https://serokell.io/blog/deep-learning-and-neural-network-guide</link>
            <guid isPermaLink="false">hacker-news-small-sites-24719670</guid>
            <pubDate>Thu, 08 Oct 2020 14:59:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Here's how Russia could track your every move – without even hacking your phone]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 20 (<a href="https://news.ycombinator.com/item?id=24719602">thread link</a>) | @geek_slop
<br/>
October 8, 2020 | https://www.geekslop.com/features/technology-articles/hacking-and-security-technology-articles/2020/if-you-have-this-popular-app-installed-on-your-phone-consider-this-heres-how-russia-could-track-your-every-move-without-even-hacking-your-phone | <a href="https://web.archive.org/web/*/https://www.geekslop.com/features/technology-articles/hacking-and-security-technology-articles/2020/if-you-have-this-popular-app-installed-on-your-phone-consider-this-heres-how-russia-could-track-your-every-move-without-even-hacking-your-phone">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<p><img width="550" height="366" src="https://d9j7f8i6.rocketcdn.me/wp-content/uploads/2015/07/image_thumb141.png" alt="image thumb141" loading="lazy" srcset="https://d9j7f8i6.rocketcdn.me/wp-content/uploads/2015/07/image_thumb141.png 550w, https://d9j7f8i6.rocketcdn.me/wp-content/uploads/2015/07/image_thumb141-416x277.png 416w, https://d9j7f8i6.rocketcdn.me/wp-content/uploads/2015/07/image_thumb141-300x200.png 300w" sizes="(max-width: 550px) 100vw, 550px" data-attachment-id="13840" data-permalink="https://www.geekslop.com/news/technology-news/hacking-and-security/2015/interesting-geographic-attack-vector-from-a-russian-launched-cyber-counter-attack/attachment/russian-and-american-flags" data-orig-file="https://d9j7f8i6.rocketcdn.me/wp-content/uploads/2015/07/image_thumb141.png" data-orig-size="550,366" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Russian and American flags" data-image-description="" data-medium-file="https://d9j7f8i6.rocketcdn.me/wp-content/uploads/2015/07/image_thumb141-300x200.png" data-large-file="https://d9j7f8i6.rocketcdn.me/wp-content/uploads/2015/07/image_thumb141.png" title="If you have this popular app installed on your phone, consider this: Russia could track your every move - without even hacking your phone. 1"></p><p>What geek doesn’t like a good conspiracy theory? Well, I’ve got one for you – a follow-the-money chain that leads from the head of Russian state to an app that is installed on millions of personal phones – one that you probably use every day.</p>



<h2>The back-channel links between American media outlets and Russian propaganda</h2>



<p>Russia’s most used back-channel outlet to the Western World is Russia Today (RT or rt.com), a commonly known propaganda outlet for the Russian government. The RT media outlet is directly funded by the Russian federal tax budget and under the <em>Foreign Agents Registration Act</em>, is registered as a “foreign agent” with the <em>United States Department of Justice</em>. There’s no argument – RT is a propaganda machine for the Russian government.</p>



<p>In October 2020, the <a aria-label="Wall Street Journal noted how many Americans are unwittingly directed to RT (opens in a new tab)" rel="noreferrer noopener external" href="https://www.wsj.com/articles/how-russia-today-skirts-high-tech-blockade-to-reach-u-s-readers-11602078094?mod=hp_featst_pos3" target="_blank" data-wpel-link="external">Wall Street Journal noted how many Americans are unwittingly directed to RT</a> from right-leaning websites such as RealClearPolitics, The Blaze, 245WallSt, Newser, The Daily Caller, Newsmax, the National Review, and others. The Journal investigated the bizarre relationship and found that the outlets were a part of a distribution network known as <em>Mixi Media</em> – a company with a privately registered domain and no About page on their website. They also discovered that included in the Mixi Media family was another Russian state-backed outlet, Sputnik – and the that the owner and founder of Mixi Media is a man named Alex Baron. When they contacted Baron about the revelatory article they were going to publish, Mixi Media immediately began dropping partners from the network.</p>



<h2>Alex Baron and ties to the Russian government?</h2>



<p>Alex Baron is not a name known to many. According to WSJ, he is an associate of Russian private-equity magnate Victor Remsha. The Wall Street Journal also says Mixi “has other ties to Russia” and that there are some “technical connections between Mixi and properties owned by Remsha”. </p>



<p>Baron denies all ties with Remsha, his companies, and RT. However, he does not deny that he is the tech director of a piece of software found on millions of phones around the country – an app that in 2017 was scandalously found to be sending user location data to a third-party using WiFi tracking even when GPS location sharing was turned off. The app is one of the most popular and highly-rated apps on Andriod and iPhones – the weather app, AccuWeather.</p>



<h2>AccuWeather</h2>



<p>All it takes is a look at AccuWeather’s permissions to see how easily a foreign country could use an app to track a person. The AccuWeather app has access to and is allowed a terrifying degree of freedom on your smartphone device. As of October 8, 2020, the weather app was allowed:</p>



<h3>Storage</h3>



<ul><li>modify or delete the contents of your USB storage</li><li>read the contents of your USB storage</li></ul><h3>Wi-Fi connection information</h3>



<ul><li>view Wi-Fi connections (this is how they were able to track and send location data even when GPS was turned off)</li></ul><h3>Device ID &amp; call information</h3>



<ul><li>read phone status and identity</li></ul><h3>Location</h3>



<ul><li>precise location (GPS and network-based)</li><li>approximate location (network-based)</li></ul><h3>Microphone</h3>



<ul><li>record audio</li></ul><h3>Other</h3>



<ul><li>receive data from Internet</li><li>pair with Bluetooth devices, including microphones</li><li>read Google service configuration</li><li>draw over other apps, a permission that lets an app cover up warnings or change content of other apps</li><li>run at startup</li><li>connect and disconnect from Wi-Fi</li><li>prevent device from sleeping</li><li>access Bluetooth settings</li><li>disable your screen lock</li><li>control vibration</li><li>change system display settings</li><li>view network connections</li><li>and yes, full network access</li></ul><h2>Could Russia use AccuWeather to track the movements of Americans?</h2>



<p>It’s an indirect link from the Russian state-owned RT media outlet and the AccuWeather app but there is certainly an interesting chain of relationships that could be concerning to most people. Could Russia use an app like AccuWeather to track Americans movements? At this point, nobody knows. But I can tell you that right before I clicked “Publish” for this article, I uninstalled AccuWeather from my phone.</p>
		</div></div>]]>
            </description>
            <link>https://www.geekslop.com/features/technology-articles/hacking-and-security-technology-articles/2020/if-you-have-this-popular-app-installed-on-your-phone-consider-this-heres-how-russia-could-track-your-every-move-without-even-hacking-your-phone</link>
            <guid isPermaLink="false">hacker-news-small-sites-24719602</guid>
            <pubDate>Thu, 08 Oct 2020 14:52:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A city with a thousand eyes: mass surveillance in Belgrade]]>
            </title>
            <description>
<![CDATA[
Score 126 | Comments 43 (<a href="https://news.ycombinator.com/item?id=24718535">thread link</a>) | @milankragujevic
<br/>
October 8, 2020 | https://aboutintel.eu/mass-surveillance-serbia/ | <a href="https://web.archive.org/web/*/https://aboutintel.eu/mass-surveillance-serbia/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<pre><strong><a href="https://aboutintel.eu/automated-video-surveillance">Discussion Prompt</a><a href="https://aboutintel.eu/predictive-policing">:</a> </strong>Should we ban the use of automated 
video-surveillance?

<a href="https://aboutintel.eu/automated-video-surveillance">See all contributions to this question.</a></pre>







<p><em>Modern video surveillance is a far cry from its clumsy predecessor. As technology has improved, and camera prices plummeted, surveillance en masse is likely coming to a city near you. Belgrade is one such city, experiencing the roll-out of thousands of cameras as part of a so-called “Safe Society” project. Installed without any public debate, nor a strong legal framework protecting digital and civil rights, concern by local civil society is high. Facial recognition in public spaces is one tool to fight crime, but residents must ask themselves, should this highly intrusive measure trump the privacy of all citizens</em>?</p>



<hr>



<p>Mass biometric surveillance can adversely affect a society — especially if in the case of Serbia, it is one with an already weak democratic tradition — and can cause serious violations of human rights. This is why it should be banned.</p>



<p>The digital age has brought about the idea that technology can exclusively be a force for good, helping us achieve nearly crime-free societies and peace among nations. Whether it’s preventing terrorist attacks or combating organised crime, one answer has been more surveillance of communications and the movements of citizens – most of whom are law-abiding. Now, entering the 2020s, the next targets of surveillance are our <em>faces</em>.&nbsp;</p>



<p><br><strong>Thousands of new eyes for Belgrade</strong></p>



<p>Citizens of the Serbian capital Belgrade learned in early 2019 that their city will be covered with a thousand<a href="https://www.sharefoundation.info/en/new-surveillance-cameras-in-belgrade-location-and-human-rights-impact-analysis-withheld/"> <span>cutting-edge surveillance cameras</span></a> in the following two years as part of the so-called “Safe Society” project. The project was unveiled without any prior public debate.<em> </em>What especially caught the public’s attention was the fact that these cameras — supplied by<a href="https://www.sharefoundation.info/en/huawei-knows-everything-about-cameras-in-belgrade-and-they-are-glad-to-share/"> <span>Chinese tech giant Huawei</span></a> — will have facial and vehicle license plate recognition capabilities. The news was declared by high-ranking officials in internal affairs, the Police Director of Serbia and the Minister of Interior. The latter is one of the key figures of the ruling party and a close associate of President Vučić, which gave the announcement particular ‘weight’ in public. Since then, a citizen initiative known as<a href="https://hiljade.kamera.rs/en/home/"> <span>“Thousands of Cameras” (“Hiljade kamera”)</span></a>, led by SHARE Foundation — a non-profit organisation dedicated to protecting digital rights, which I work for — has been actively challenging this surveillance system and demanding that such an intrusive technology be discussed in an open and inclusive setting before it is introduced.</p>



<p>Serbia does not have a long democratic tradition and features a <a href="https://www.hrw.org/world-report/2019/country-chapters/serbia/kosovo"><span>problematic human rights record</span></a> to this day. As a remnant of socialist Yugoslavia, which prioritised safety and security, privacy awareness is very low for most of the population. The country’s recent democratic backslide is also quite alarming. Earlier this year,<a href="https://freedomhouse.org/report/nations-transit/2020/dropping-democratic-facade"> <span>Freedom House</span></a> downgraded Serbia to a Transitional/Hybrid regime for the first time since 2003. On the<a href="https://rsf.org/en/serbia"> <span>World Press Freedom Index</span></a> for 2020, Serbia is ranked 93rd out of 180 countries – another 3 places down from the previous year. In its report, Reporters Without Borders <a href="https://rsf.org/en/serbia"><span>states</span></a> that “Serbia has become a country where it is often dangerous to be a journalist”. Data protection and privacy do not rest on a long political tradition. A data protection law has existed in Serbia for just over a decade and the Commissioner for Information of Public Importance and Personal Data Protection (the national data protection authority) was established only 16 years ago, at first as a freedom of information complaints body. Furthermore, video surveillance in Serbia — a country currently negotiating EU membership — has seen its fair share of controversy, with camera footage often being abused, leaked in the pro-government media, or the cameras ‘conveniently’ not working at key moments (i.e. when powerful individuals could have been compromised by the footage).&nbsp;</p>



<p>As the city administration’s infrastructure strategy is quite unpopular, the citizens of Belgrade, despite a general privacy lethargy, have paid attention to the new surveillance system; citizens and “Thousands of Cameras” activists<a href="https://hiljade.kamera.rs/map/"> <span>have mapped hundreds of locations</span></a> across Belgrade where cameras have been installed. This form of crowdsourcing provides more information about surveilled locations than the police itself has published. Photos of cameras from various Belgrade neighbourhoods are regularly posted on the<a href="https://twitter.com/hiljadekamera"> <span>“Thousands of Cameras” Twitter feed</span></a>. With this alarming spread of cameras, we have to ask what the deeper implications of such surveillance are? And can it cause or cement irreversible changes in a world of declining democratic values, particularly in a country like Serbia?</p>



<p><br><strong>The legal framework</strong></p>



<p>Serbia has modernised its data protection legal framework by adopting the new Law on Personal Data Protection (LPDP) in late 2018; its full application began in November 2019. Drafted from a confusing mix of translated GDPR regulations and the EU Law Enforcement Directive, the text of Serbia’s new LPDP was controversial from the start, but at least it provided a more modern approach to data protection. Among its main flaws however, is the fact that the new LPDP does not specifically regulate two important aspects of data processing: biometric data and video-surveillance. Despite this, the law’s general provisions and principles should still apply to any data processing, such as a massive video surveillance system across Belgrade.</p>



<p>Before deploying a public space surveillance system, the LPDP obliges the data controller to conduct a Data Protection Impact Assessment (DPIA) and ask for the Commissioner’s opinion. However the Ministry of Interior of Serbia, which is the implementing body for the “Safe Society” project, failed to comply with the LPDP. It issued two DPIAs, both of which did not satisfy the Commissioner, who refused to approve. Despite this, the cameras were installed anyway.&nbsp;</p>



<p>The latest information gathered from the second DPIA of the Ministry suggests that there will be<a href="https://www.sharefoundation.info/wp-content/uploads/Mini1000.png"> </a>more than 8000 different <a href="https://www.sharefoundation.info/wp-content/uploads/Mini1000.png"><span>cameras and other devices</span></a> in use, such as body cams, mobile cameras and vehicle-mounted cameras. Although facial recognition, i.e. automated detection of people’s faces from a video feed, is not yet rolled out by the Ministry, this feature is expected to be implemented by the end of the project. While little is known about the project’s timeline, this can be expected to be in the next two years.</p>



<p>Apart from the data protection issues related to facial recognition, we also need to ask whether these technologies are necessary and proportionate, particularly from the perspective of the European human rights framework and its underlying values. Is facial recognition in public spaces the only available measure that can be used to prevent serious crime and protect citizens? Can this highly intrusive measure trump the privacy of all citizens, effectively turning whole cities into mass surveillance zones?</p>



<p>All in all, the Belgrade surveillance camera system is of <a href="https://hiljade.kamera.rs/en/law-society/">dubious legality</a>, to say the least, because:&nbsp;</p>



<ol><li>the actual purpose of introducing this system has not been clearly defined;&nbsp;</li><li>it has not been confirmed that the use of this system is necessary for the operations of state bodies;&nbsp;</li><li>its positive influence on the reduction of criminal offences has been overrated and its use is not proportionate to the risks related to the rights and freedom of citizens;&nbsp;</li><li>there is no law to begin with that defines that the police have the right to use smart surveillance in public places; and&nbsp;</li><li>the Data Protection Impact Assessment (DPIA) of the Ministry of Interior does not meet formal and material conditions defined by the law and was not approved by the Commissioner.&nbsp;</li></ol>



<p><br><strong>Point of no return for human rights</strong></p>



<p>Automated biometric video-surveillance may be the pinnacle of today’s<a href="https://www.publicbooks.org/the-folly-of-technological-solutionism-an-interview-with-evgeny-morozov/"> <span>techno-solutionism</span></a> – trying to solve deep and complex social problems with often non-critical use of technology. Sadly, decision makers are usually blind to issues of ethical and legal nature, and far-reaching consequences, if a society sets public safety as its ultimate value. It is all the more troubling if they believe it can be achieved with technology such as mass video-surveillance. Once governments get a hold of such powerful technology, it might be impossible to completely remove it from their arsenal, even after successful legal challenges. In that regard, we can draw a parallel to blanket communications metadata retention — a highly controversial measure in terms of proportionality which is<a href="https://edri.org/eu-member-states-willing-to-retain-illegal-data-retention/"> <span>still alive and kicking in the EU</span></a>, despite two CJEU judgements against it.<sup>1 </sup>Not to mention lucrative infrastructure deals required to install a massive video surveillance network, possibly in every larger city. This is particularly worrisome for countries such as Serbia, which are currently experiencing democratic backslides.</p>



<p>In addition to privacy, other associated human rights and freedoms, such as freedom of expression and the rights to protest and peaceful gathering, would<a href="https://edri.org/facial-recognition-and-fundamental-rights-101/"> <span>also be affected</span></a> in areas covered with automated video-surveillance. Imagine a scenario where a government keeps a biometric database of all citizens who attended anti-government protests; the ways in which this could affect their work, families, social relationships, and other aspects of everyday life are vast. Facial recognition also<a href="https://www.washingtonpost.com/technology/2019/12/19/federal-study-confirms-racial-bias-many-facial-recognition-systems-casts-doubt-their-expanding-use/"> <span>discriminates against minorities</span></a>, further entrenching bias against disadvantaged communities and making them more vulnerable.</p>



<p>With its high risks and numerous adverse effects, especially once reaching a “point-of-no-return”, automated biometric video surveillance does not uphold the values of respect for human rights, equality and social justice of the EU and the Council of Europe. Therefore, banning automated video surveillance is the right step forward, especially when we take into account other worrying trends, such as <a href="https://www.laquadrature.net/en/2020/02/04/technopolice-resisting-the-total-surveillance-of-our-cities-and-of-our-lives/"><span>the total …</span></a></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://aboutintel.eu/mass-surveillance-serbia/">https://aboutintel.eu/mass-surveillance-serbia/</a></em></p>]]>
            </description>
            <link>https://aboutintel.eu/mass-surveillance-serbia/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24718535</guid>
            <pubDate>Thu, 08 Oct 2020 12:33:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Machine Learning Engineer Guide: Feature Store vs. Data Warehouse]]>
            </title>
            <description>
<![CDATA[
Score 164 | Comments 54 (<a href="https://news.ycombinator.com/item?id=24718301">thread link</a>) | @nathaliaariza
<br/>
October 8, 2020 | https://www.logicalclocks.com/blog/feature-store-vs-data-warehouse | <a href="https://web.archive.org/web/*/https://www.logicalclocks.com/blog/feature-store-vs-data-warehouse">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><div><div><p>TLDR; The feature store is a data warehouse of features for machine learning (ML). Architecturally, it differs from the traditional data warehouse in that it is a dual-database, with one database (row-oriented) serving features at low latency to online applications and the other database (column-oriented) storing large volumes of features, used by Data Scientists to create train/test datasets and by batch applications doing offline model scoring.</p><h2>Features Store: Data Warehouse Déjà Vu</h2><p>Data warehouses democratized access to Enterprise data by centralizing data in a single platform and then empowering business analysts with visual tools, such as Tableau and Power BI. No longer did they need to know what data resides where and how to query that data in that platform. They could derive historical insights into the business using BI tools.&nbsp;<br></p><p>Data scientists, in contrast, build predictive models to derive business insights. The feature store is the data warehouse for Data Science - it is a central vault for storing documented, curated, and access-controlled features that can be used across many different models. The feature store ingests data from the Enterprise’s many different sources after transforming, aggregating, and validating the data.&nbsp;<br></p><p>Feature pipelines need to be written to ensure that data reliably flows from existing sources and is available in a format ready to be consumed by ML training pipelines and models.</p><p>Most Data Scientists currently do not have a feature store. They spend most of their time looking for, cleaning, and featurizing data. Hence, the (very real) cliché that 80% of data science is data wrangling. Data Scientists without a feature store work in an era akin to how business analysts worked before the advent of data warehouses, with low individual and organizational productivity.</p><h2>The Data Warehouse is an input <br>to the Feature Store&nbsp;</h2><p>Both platforms are a central store of curated data used to generate insights into the data. Both platforms have pipelines (ETL and feature pipelines, respectively) to ingest data from one or more disparate sources (operational databases, data lakes, etc).</p><p>Both benefit from metadata catalogs to organize data sets and access control to share data with only authorized actors.&nbsp;</p><p>Both platforms can be designed to scale-out on commodity hardware and store large volumes of data, although typically a data warehouse stores only relevant to analysis (modern <a href="#">data lakehouses</a> are designed to store large volumes of data more cost efficiently).<em>‍</em></p><figure id="w-node-d225d8bb42d9-b88dcb71"><p><img src="https://uploads-ssl.webflow.com/5e6f7cd3ee7f51ec3ea4da0f/5f7eed8f6ff9277909bd9c6e_visual_blog5.jpg" loading="lazy" alt=""></p></figure><h2>Feature Store as a Dual Database</h2><p>The main architectural difference between a data warehouse and a feature store is that the data warehouse is typically a single columnar database, while the feature store is typically implemented as two databases:</p><ul role="list"><li>an <strong>offline feature store</strong> for serving large batches of features to (1) create train/test datasets and (2) batch applications scoring models using those batches of features, and</li><li>an <strong>online feature store</strong> for serving a single row of features (a <em>feature vector</em>) to be used as input features for an online model for an individual prediction.<br></li></ul><p><strong>The offline feature store</strong> is typically required to efficiently serve and store large amounts of feature data, <strong>while the online feature store</strong> is required to return feature vectors in very low latency (e.g., &lt; 10ms). Examples of databases used for the offline feature store are Apache Hive and BigQuery and examples of online feature stores include MySQL Cluster, Redis, and DynamoDB.&nbsp;</p><p>Note that if you want to reuse features in different train/test datasets for different models, your database or application will need to join features together. This is true for both the offline and online feature stores. If your feature store does not support joining features, that is, you do not reuse features across different models, you (or some system) will need to create a new ingestion pipeline for every new model you support in production.</p><figure id="w-node-fbcf667874fd-b88dcb71"><p><img src="https://uploads-ssl.webflow.com/5e6f7cd3ee7f51ec3ea4da0f/5f7eee044a3a1d737610fc25_visual_blog4.jpg" loading="lazy" alt=""></p></figure><h2>Detailed Comparison</h2><p>In the table below, we see an overview of the main architectural differences between feature stores and data warehouses.<strong> Data warehouses</strong> are used primarily by business analysts for interactive querying and for generating historical reports/dashboards on the business.<strong> Feature stores</strong> are used by both data scientists and by the online/batch applications, and they are fed data by feature pipelines, typically written in Python or Scala/Java.&nbsp;</p><p>Data scientists typically use Python programs to create train/test datasets by joining existing features in the feature store together and materializing the train/test datasets in a <a href="#">file format best suited to the framework</a> they are going to train their model in (e.g., TFRecord for TensorFlow, NPY for PyTorch). Data warehouses and SQL currently lack this capability to create train/test datasets in ML file formats.</p><figure id="w-node-3cbc549719ba-b88dcb71"><p><img src="https://uploads-ssl.webflow.com/5e6f7cd3ee7f51ec3ea4da0f/5f7ef334aaaf9d602868fc36_table_comparison_04.jpg" loading="lazy" alt=""></p></figure><h2>Feature Data should be Validated <br>before Ingestion</h2><p>The table also shows the differences in the types of data stored, as well as how the data is stored, validated, and queried. A data warehouse stores data in tables along with schemas for describing the type of data and constraints for columns. Similarly, the feature store stores typed data (typically in tables), but as features are typically stored as ready-to-consume numerical values or vectors (embeddings) or tensors, there is less need for a richer set of column types compared to a data warehouse.&nbsp; Foreign key constraints are typically not supported in feature stores, due to the difficulty in enforcing such constraints between online and offline stores.</p><p>As model training is very sensitive to bad data (null values, outliers cause numerical instability, missing values), feature data should be validated before ingestion. Data validation frameworks, such as <a href="#">Great Expectations</a> and <a href="#">Deequ</a>, have appeared to help implement feature pipelines that apply predicates (data validation rules) on all the features ingested into the feature store, ensuring high data quality in the feature store.&nbsp;</p><p>Domain specific languages (DSL) are sometimes used to define the feature transformations, aggregations, and data validation rules in feature pipelines, but general purpose languages (Python, Scala) are commonly used when non-trivial feature engineering is required.&nbsp;</p><h2>Using the feature store to create train/test data</h2><p>Data scientists are one of the main users of the feature store. They use a feature repository to perform exploratory data analysis (EDA) - searching/browsing for available features and inspecting feature values/schemas/statistics. Data Scientists mainly use Python to select features to create train/test datasets. This typically involves joining features together to create a&nbsp; train/test dataset in their file format of choice (.tfrecord, .csv, .npy, .petastorm, etc). Sometimes feature stores support a DSL (domain specific language) to create train/test datasets or other languages such as Scala/Java.&nbsp;</p><h2>Online feature store</h2><p>Online applications use the online feature store to retrieve feature values with low latency to build feature vectors that are sent to models for predictions. In contrast to higher latency data warehouses, feature stores may be required to return feature vectors in single millisecond latency - only really achievable in row-oriented or key-value stores.&nbsp;</p><p>The typical access pattern for retrieving features is a key-value lookup, but if features are to be reused in the online feature store, then joins are again required (either in the database or in the application). In some databases (such as <a href="#">MySQL Cluster</a>), a small number of joins can be performed at very low latency.<br></p><h2>Feature statistics to monitor for feature <br>drift and data drift</h2><p>Descriptive statistics (e.g., mean, standard deviation) for features are also useful when identifying data drift in online models. Your monitoring infrastructure can calculate statistics on live prediction traffic, and compare those statistics with the values in the feature store to <a href="#">identify data drift</a> for the live traffic, potentially required retraining of the model.</p><h2>Time-Travel&nbsp;</h2><p>Temporal databases support <em>time-travel</em>: the ability to query data as it was at a given point-in-time or data changes in a given time-interval. The “AS OF SYSTEM TIME” syntax was introduced to <a href="#">SQL 2011</a> to standardize point-in-time queries, while the “VERSIONS BETWEEN SYSTEM TIME ... AND ... “ syntax was introduced to identify the versioned changes to data in a time interval. Time-travel is supported in some data warehouses, but does not have universal support across all vendors.</p><p>For a feature store time-travel has several important uses: when creating train/test data (e.g., training data is data from the years 2010-2018, while test data is data from the range 2019-2020). Time-travel is also useful to make changes to a dataset (e.g., rollback a bad commit of data to the dataset) or to compare metadata (statistics) for features and how they change over time. We rarely require time-travel for features used in serving. Time-travel is also important when performing point-in-time joins, where we ensure that there is no data leakage from the future when we create train/test datasets from historical data.</p><h2>Feature Pipelines&nbsp;</h2><p>Data warehouses typically have timed triggers for running ETL jobs (or data pipelines) to ingest the latest data from operational databases, message queues, and data lakes. Similarly, feature pipelines can timed triggers to transform and aggregate the latest data from different sources before storing it in both the online and offline feature store for scoring by online and offline applications. However, additional pipelines can also feed features to the feature store.&nbsp;</p><p>Predictions made by models can be stored in the feature store along with the outcomes for those predictions. There can be long lags of even days or months or years before outcomes become available - e.g., a prediction on whether a loan will be repaid or not), but as they arrive new training data becomes available that can be used to trigger re-training of models.</p><figure id="w-node-74c5777df82a-b88dcb71"><p><img src="https://uploads-ssl.webflow.com/5e6f7cd3ee7f51ec3ea4da0f/5f7ef3219e6baa58c301c833_table_comparison_03.jpg" loading="lazy" alt=""></p></figure><h2>Conclusion</h2><p>Data …</p></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.logicalclocks.com/blog/feature-store-vs-data-warehouse">https://www.logicalclocks.com/blog/feature-store-vs-data-warehouse</a></em></p>]]>
            </description>
            <link>https://www.logicalclocks.com/blog/feature-store-vs-data-warehouse</link>
            <guid isPermaLink="false">hacker-news-small-sites-24718301</guid>
            <pubDate>Thu, 08 Oct 2020 12:02:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Using a Piece of Paper as a Display Terminal – Ed vs. Vim]]>
            </title>
            <description>
<![CDATA[
Score 111 | Comments 60 (<a href="https://news.ycombinator.com/item?id=24716218">thread link</a>) | @rhabarba
<br/>
October 7, 2020 | https://blog.robertelder.org/paper-display-terminal-ed-vim/ | <a href="https://web.archive.org/web/*/https://blog.robertelder.org/paper-display-terminal-ed-vim/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
						<div>


<h5>2020-10-05 - By Robert Elder</h5>




<iframe src="https://www.youtube.com/embed/8vmOTvRXZ0E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>



<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This article will focus on discussing why the ancient text editor <a href="https://en.wikipedia.org/wiki/Ed_(text_editor)">'ed'</a> works the way it does.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Despite having its roots in the late 1960s, the 'ed' editor is still installed by default on most modern Linux distributions. &nbsp;Although there are few practical use cases for this editor today, it can still be meaningful to learn how 'ed' works since other Unix tools like vim, grep or sed have features that are significantly influenced from 'ed'.</p>



<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If you try running the 'ed' command with or without a file argument, you'll see something that looks like this:</p>

<p><img src="https://blog.robertelder.org/images/paper-display-terminal-ed-vim_prompt-wait_811x349_q92.png" width="811" height="349"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If you're waiting for something to happen then you'll be waiting a long time. &nbsp;That's because 'ed' is waiting for you to do something! &nbsp;The 'ed' program doesn't work like other command-line text editors such as vim or nano. &nbsp;The luxury of being able to print the contents of the current file to the terminal is something that 'ed' takes very seriously. &nbsp;That's why you need to explicitly give 'ed' a command to tell it to do so!</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For example, if you want to print out an individual line in the file, you can just type the line number and press enter. &nbsp;If you want to append text, use the single-letter command 'a' on a line by itself to enter 'append' mode. &nbsp;Once you're done adding text, write a '.' character on a line by itself and press enter to stop adding text to the file. &nbsp;To review all the lines in the file, you can use the command '1,$p'. &nbsp;Finally, once you're done editing the file, you can use 'wq' to exit:</p>

<p><code><pre>1
Hello World.
a
Here is some more text.
.
1,$p
Hello World.
Here is some more text.
wq
</pre></code></p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To make sense of why this editor is so hard to use, it make sense to think about the way in which people interacted with computers in the early days of computing. &nbsp;Around the time when 'ed' was created, it was still common for computers to print their output on <em>paper</em> instead of electronic screens! &nbsp;These early output devices were called <a href="https://en.wikipedia.org/wiki/Teleprinter">'teleprinters'</a>, often abbreviated as TTY. &nbsp;The term TTY is still used on most *nix systems to this day, and if you run this command, you can probably see some of the virtual TTY devices on your system:</p>

<p><code><pre><span>ls</span> /dev/tty*
</pre></code></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This ancient model of sending an infinite stream of characters (sent serially) to a 'terminal' or 'teleprinter' device that 'prints' or 'renders' them is still used today. &nbsp;It is even used by more modern terminal programs like vim or nano! &nbsp;You might not believe that vim works this way because it displays all kinds of information at the top and bottom of the terminal. &nbsp;Vim also lets you scroll up and down or open up screen splits etc. &nbsp;You can't possibly send the output of vim to a printer, right? &nbsp;Yes, you can and that's exactly what we're about to try.</p>



<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The 'script' command lets you capture all output during a terminal session and save it to a file. &nbsp;This is a great way to log the output when you're running through a sequences of commands that you need to keep track of, but you can also use it to capture everything that gets output to the terminal during a vim session:</p>

<p><code><pre>script
</pre></code></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;After running the 'script' command try opening a vim session. &nbsp;After doing a few things in vim, quit and then run the 'exit' command in the shell to exit the 'script' session to finish logging:</p>

<p><code><pre><span>exit</span>
</pre></code></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;All of the output from the terminal session is now saved in a file called 'typescript'. &nbsp;Here's an image of what some of the output in the script looks like:</p>

<p><img src="https://blog.robertelder.org/images/paper-display-terminal-ed-vim_vim-script-output_718x292_q92.png" width="718" height="292"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Many of these seemingly gibberish symbols are actually <a href="https://en.wikipedia.org/wiki/ANSI_escape_code">ANSI Escape codes</a>. &nbsp;These are the secret to how vim (and all other terminal applications) can use a serial output to print all sorts of interesting user interfaces. &nbsp;Most importantly, some of these escape sequences allow you to move the printing cursor around to arbitrary positions. &nbsp;That's how vim can keep some text pinned at the bottom or top of the terminal while also giving the illusion that you're scrolling 'up' or 'down' in the file. &nbsp;Some escape codes also control the foreground and background colours.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the old days, these escape codes were not actually processed by the CPU. &nbsp;The were instead interpreted by the 'terminal' monitor device itself. &nbsp;In other words, the oldest 'terminals' can be thought of as physically separate devices that received a serial stream of text, cursor movement instructions, and color changing instructions.  &nbsp;On a 'modern' computer, every 'terminal' window that you open is basically a software emulation of an ancient physical device that you can imagine to look like a small and bulky CRT monitor. &nbsp;Today, these escape codes are processed by the CPU of your laptop or desktop computer inside these software emulated 'terminals' in your graphical desktop environment.</p>



<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;So, what's stopping us from trying to render the output of vim on a piece of paper to pretend that we're in the year 1969? &nbsp;Nothing! &nbsp;Here's is what the output of vim looks like when I try to render it using my laser printer:</p>

<p><img src="https://blog.robertelder.org/images/paper-display-terminal-ed-vim_vim-page_1920x1080_q30.jpeg" width="1920" height="1080"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The reason that this output doesn't look very useful is because my printer isn't expecting to be used as a display terminal for vim. &nbsp;It doesn't know how to deal with all the ANSI escape sequences and we end up with this weird looking mess.  &nbsp;Do you see the '?2004h' part near the start of the output? &nbsp;You can look that up and see that it's an ANSI escape sequence to 'Turn on bracketed paste mode'. &nbsp;It is an exercise left to the reader to look up the rest of the ANSI escape sequences shown on the page above.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Interestingly, my printer seemed to choke when I printed this and got stuck saying 'data remaining' until I printed a blank test page.</p>

<p><img src="https://blog.robertelder.org/images/paper-display-terminal-ed-vim_printer-stuck_1920x894_q50.jpeg" width="1920" height="894"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I didn't bother investigating, but I assume one of the control sequences confused the printer and made it think it was still waiting on data from the computer.</p>



<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;And here is what using ed would look like if you were only able to render its output on a piece of paper:</p>

<p><img src="https://blog.robertelder.org/images/paper-display-terminal-ed-vim_ed-page_1920x1080_q30.jpeg" width="1920" height="1080"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Since 'ed' doesn't print any ANSI escape sequences, my printer prints this with no problems!</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Based on the two experiences described above, which editor do you think you'd prefer if you had to print all the output on paper?</p>



<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If you decide to try and learn 'ed', you'll find that the man pages and the '-h' flag are not very helpful. &nbsp;Instead, you should check out the 'info' pages since that's where you'll find out all the details of different editor modes and single-letter commands are:</p>

<p><code><pre>info ed
</pre></code></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;After you play around with 'ed' for a while, you'll realize that it acts almost like a command-line shell in itself. &nbsp;The only difference is that the environment in which you're working is a file instead of the user space of your operating system. &nbsp;Every little i/o operation on the file is implemented as a command that requires as little information as possible. &nbsp;This makes complete sense when you have to print everything to paper!</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Imagine working on a file with modern Vim where your output always goes to a printer. &nbsp;You now decide to open a log file to check what's on the last line and immediately, your printer starts churning a full page of material. &nbsp;Oops, you opened the wrong file. &nbsp;Try another file, and again, oops! &nbsp;Wrong file again! &nbsp;That's a lot of wasted paper. &nbsp;What is it with these millennials and their fancy text editors that just print every line automatically! &nbsp;I remember the good old days when the users had control over their systems, and programs wouldn't just do whatever they want without asking!</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In conclusion, the reason why the 'ed' command works the way it does was due to the higher resources constraints that existed at the time. &nbsp;Features like electronic display terminals and ANSI escape sequences were not in common enough use at the time when 'ed' was created, so there was no reason to consider using them. &nbsp;Instead, a shell-like command-line interface for editing files made way more sense.</p>




<table>
<tbody>
<tr>


	
		<td><a href="https://blog.robertelder.org/recording-660-fps-on-raspberry-pi-camera/"><img src="https://blog.robertelder.org/images/recording-660-fps-on-raspberry-pi-camera-thumb_250x150_q85.jpeg" alt="A Guide to Recording 660FPS Video On A $6 Raspberry Pi Camera" width="250" height="150"></a><div><p><a href="https://blog.robertelder.org/recording-660-fps-on-raspberry-pi-camera/"><strong>A Guide to Recording 660FPS Video On A $6 Raspberry Pi Camera</strong></a></p><p>Published 2019-08-01</p></div>
		</td>
	
	
	
	
	
	



	
	
	
	
		<td><a href="https://www.kickstarter.com/projects/2034896774/regular-expression-sticker-collection-and-video-guide?utm_source=blog&amp;utm_medium=link&amp;utm_campaign=k7&amp;utm_content=paper-display-terminal-ed-vim"><img src="https://blog.robertelder.org/images/k7_250x150_q85.png" alt="Regular Expression Laptop Stickers &amp; Video Guide" width="250" height="150"></a><div><p><a href="https://www.kickstarter.com/projects/2034896774/regular-expression-sticker-collection-and-video-guide?utm_source=blog&amp;utm_medium=link&amp;utm_campaign=k7&amp;utm_content=paper-display-terminal-ed-vim"><strong>Regular Expression Laptop Stickers &amp; Video Guide</strong></a></p></div>
		</td>
	
	
	



	
		<td><a href="https://blog.robertelder.org/don-libes-expect-unix-automation-tool/"><img src="https://blog.robertelder.org/images/automation-methods-thumb_250x150_q85.png" alt="Don Libes' Expect:  A Surprisingly Underappreciated Unix Automation Tool" width="250" height="150"></a><div><p><a href="https://blog.robertelder.org/don-libes-expect-unix-automation-tool/"><strong>Don Libes' Expect:  A Surprisingly Underappreciated Unix Automation Tool</strong></a></p><p>Published 2016-12-08</p></div>
		</td>
	
	
	
	
	
	



	
	
	
		<td><a href="https://twitter.com/RobertElderSoft">@RobertElderSoft On Twitter</a>
		</td>
	
	
	
	

</tr>
<tr>


	
		<td><a href="https://blog.robertelder.org/virtual-memory-with-256-bytes-of-ram/"><img src="https://blog.robertelder.org/images/256-bytes-virtual-memory-thumb_250x150_q85.png" alt="Virtual Memory With 256 Bytes of RAM - Interactive Demo" width="250" height="150"></a><div><p><a href="https://blog.robertelder.org/virtual-memory-with-256-bytes-of-ram/"><strong>Virtual Memory With 256 Bytes of RAM - Interactive Demo</strong></a></p><p>Published 2016-01-10</p></div>
		</td>
	
	
	
	
	
	



	
	
		<td><h2>Subscribe to Updates</h2><form method="post" action="https://api.robertelder.org/v1/message/">Email: </form><br><a href="https://www.robertelder.org/privacy-policy/">Privacy Policy</a>
		</td>
	
	
	
	
	



	
		<td><a href="https://blog.robertelder.org/what-is-ssh/"><img src="https://blog.robertelder.org/images/what-is-ssh-thumb_250x150_q85.png" alt="What is SSH?  Linux Commands For Beginners" width="250" height="150"></a><div><p><a href="https://blog.robertelder.org/what-is-ssh/"><strong>What is SSH?  Linux Commands For Beginners</strong></a></p><p>Published 2017-04-30</p></div>
		</td>
	
	
	
	
	
	



	
		<td><a href="https://blog.robertelder.org/installing-ubuntu-16-linux-ge62-6qd-apache-pro-msi-notebook/"><img src="https://blog.robertelder.org/images/msi-ge62-6qd-apache-pro_250x150_q85.jpeg" alt="Installing Ubuntu 16 Linux On A GE62 6QD Apache Pro MSI Notebook" width="250" height="150"></a><div><p><a href="https://blog.robertelder.org/installing-ubuntu-16-linux-ge62-6qd-apache-pro-msi-notebook/"><strong>Installing Ubuntu 16 Linux On A GE62 6QD Apache Pro MSI Notebook</strong></a></p><p>Published 2016-08-02</p></div>
		</td>
	
	
	
	
	
	

</tr>
<tr>


	
		<td><a href="https://blog.robertelder.org/data-science-linux-command-line/"><img src="https://blog.robertelder.org/images/data-science-linux-commands-thumb_250x150_q85.jpeg" alt="An Introduction To Data Science On The Linux Command Line" width="250" height="150"></a><div><p><a href="https://blog.robertelder.org/data-science-linux-command-line/"><strong>An Introduction To Data Science On The Linux Command Line</strong></a></p><p>Published 2019-10-16</p></div>
		</td>
	
	
	
	
	
	



	
		<td><a href="https://blog.robertelder.org/robert-elder-software-linux-operating-system/"><img src="https://blog.robertelder.org/images/robert-elder-software-linux-operating-system-thumb_250x150_q85.png" alt="Introducing The Robert Elder Software Linux Operating System" width="250" height="150"></a><div><p><a href="https://blog.robertelder.org/robert-elder-software-linux-operating-system/"><strong>Introducing The Robert Elder Software Linux Operating System</strong></a></p><p>Published 2016-09-27</p></div>
		</td>
	
	
	
	
	
	



	
		<td><a href="https://blog.robertelder.org/overlap-add-overlap-save/"><img src="https://blog.robertelder.org/images/overlap-add-overlap-save-thumb_250x150_q85.png" alt="Overlap Add, Overlap Save Visual Explanation" width="250" height="150"></a><div><p><a href="https://blog.robertelder.org/overlap-add-overlap-save/"><strong>Overlap Add, Overlap Save Visual Explanation</strong></a></p><p>Published 2018-02-10</p></div>
		</td>
	
	
	
	
	
	



	
		<td><a href="https://blog.robertelder.org/fast-meme-transform/"><img src="https://blog.robertelder.org/images/fast-meme-transform-thumb_250x150_q85.jpeg" alt="The Fast Meme Transform: Convert Audio Into Linux Commands" width="250" height="150"></a><div><p><a href="https://blog.robertelder.org/fast-meme-transform/"><strong>The Fast Meme Transform: Convert Audio Into Linux Commands</strong></a></p><p>Published 2018-02-10</p></div>
		</td>
	
	
	
	
	
	

</tr>
</tbody>
</table>


				</div>
			</div></div>]]>
            </description>
            <link>https://blog.robertelder.org/paper-display-terminal-ed-vim/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24716218</guid>
            <pubDate>Thu, 08 Oct 2020 05:45:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[NYU DS-GA 1008 – Deep Learning]]>
            </title>
            <description>
<![CDATA[
Score 115 | Comments 25 (<a href="https://news.ycombinator.com/item?id=24715307">thread link</a>) | @eugenhotaj
<br/>
October 7, 2020 | https://atcold.github.io/pytorch-Deep-Learning/ | <a href="https://web.archive.org/web/*/https://atcold.github.io/pytorch-Deep-Learning/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    <!-- Provide site root to javascript -->
    

    <!-- Work around some values being stored in localStorage wrapped in quotes -->
    

    <!-- Set the theme before any content is loaded, prevents flash -->
    

    <!-- Hide / unhide sidebar before it is displayed -->
    

    <nav id="sidebar" aria-label="Table of contents" aria-hidden="false">
        
        
    </nav>

    <div id="page-wrapper">
        <div class="page">
            

            <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
            

            <div id="content">
                <main>
                    <div class="page">
                      
                      <p><strong>DS-GA 1008 · SPRING 2020 · <a href="http://cds.nyu.edu/">NYU CENTER FOR DATA SCIENCE</a></strong></p>



<h2 id="description">Description</h2>

<p>This course concerns the latest techniques in deep learning and representation learning, focusing on supervised and unsupervised deep learning, embedding methods, metric learning, convolutional and recurrent nets, with applications to computer vision, natural language understanding, and speech recognition. The prerequisites include: <a href="https://cds.nyu.edu/academics/ms-curriculum/">DS-GA 1001 Intro to Data Science</a> or a graduate-level machine learning course.</p>

<h2 id="lectures">Lectures</h2>

<p><strong>Legend</strong>: 🖥 slides, 📓 Jupyter notebook, 🎥 YouTube video.</p>

<table>
<!-- =============================== HEADER ================================ -->
  <thead>
    <tr>
      <th>Week</th>
      <th>Format</th>
      <th>Title</th>
      <th>Resources</th>
    </tr>
  </thead>
  <tbody>
<!-- =============================== WEEK 1 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week01/01">①</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week01/01-1">History and motivation</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1Q7LtZyIS1f3TfeTGll3aDtWygh3GAfCb">🖥️</a>
        <a href="https://www.youtube.com/watch?v=0bMe_vCZo30">🎥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week01/01-2">Evolution and DL</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week01/01-3">Neural nets (NN)</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/01-tensor_tutorial.ipynb">📓</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/02-space_stretching.ipynb">📓</a>
        <a href="https://www.youtube.com/watch?v=5_qrxVq1kvc">🎥</a>
      </td>
    </tr>
<!-- =============================== WEEK 2 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week02/02">②</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week02/02-1">SGD and backprop</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1w2jV_BT2hWzfOKBR02x_rB4-dfVUI6SR">🖥️</a>
        <a href="https://www.youtube.com/watch?v=d9vdh3b787Y">🎥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week02/02-2">Backprop in practice</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week02/02-3">NN training</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/01%20-%20Spiral%20classification.pdf">🖥</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/04-spiral_classification.ipynb">📓</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/05-regression.ipynb">📓</a>
        <a href="https://www.youtube.com/watch?v=WAn6lip5oWk">🎥</a>
      </td>
    </tr>
<!-- =============================== WEEK 3 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week03/03">③</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-1">Parameter transformation</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=18UFaOGNKKKO5TYnSxr2b8dryI-PgZQmC">🖥️</a>
        <a href="https://youtu.be/FW5gFiJb-ig">🎥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-2">CNN</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3">Natural signals' properties</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/02%20-%20CNN.pdf">🖥</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/06-convnet.ipynb">📓</a>
        <a href="https://youtu.be/kwPWpVverkw">🎥</a>
      </td>
    </tr>
<!-- =============================== WEEK 4 ================================ -->
    <tr>
      <td rowspan="1"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week04/04">④</a></td>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week04/04-1">1D convolutions</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/07-listening_to_kernels.ipynb">📓</a>
        <a href="https://youtu.be/OrBEon3VlQg">🎥</a>
      </td>
    </tr>
<!-- =============================== WEEK 5 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week05/05">⑤</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week05/05-1">Optimisation I</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1pwlGN6hDFfEYQqBqcMjWbe4yfBDTxsab">🖥️</a>
        <a href="https://youtu.be/--NZb480zlg">🎥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week05/05-2">Optimisation II</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week05/05-3">CNN, autograd</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/03-autograd_tutorial.ipynb">📓</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/extra/b-custom_grads.ipynb">📓</a>
        <a href="https://youtu.be/eEzCZnOFU1w">🎥</a>
      </td>
    </tr>
<!-- =============================== WEEK 6 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week06/06">⑥</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week06/06-1">CNN applications</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1opT7lV0IRYJegtZjuHsKhlsM5L7GpGL1">🖥️</a>
        <a href="https://drive.google.com/open?id=1sdeVBC3nuh5Zkm2sqzdScEicRvLc_v-F">🖥️</a>
        <a href="https://youtu.be/ycbMGyCPzvE">🎥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week06/06-2">RNNs and attention</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week06/06-3">Training RNNs</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/08-seq_classification.ipynb">📓</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/09-echo_data.ipynb">📓</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/04%20-%20RNN.pdf">🖥️</a>
        <a href="https://youtu.be/8cAffg2jaT0">🎥</a>
      </td>
    </tr>
<!-- =============================== WEEK 7 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week07/07">⑦</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1">Energy-Based Models</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa">🖥️</a>
        <a href="https://youtu.be/tVwV14YkbYs">🎥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-2">SSL, EBM</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-3">Autoencoders</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/05%20-%20Generative%20models.pdf">🖥️</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/10-autoencoder.ipynb">📓</a>
        <a href="https://youtu.be/bggWQ14DD9M">🎥</a>
      </td>
    </tr>
<!-- =============================== WEEK 8 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week08/08">⑧</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week08/08-1">Contrastive methods</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1Zo_PyBEO6aNt0GV74kj8MQL7kfHdIHYO">🖥️</a>
        <a href="https://youtu.be/ZaVP2SY23nc">🎥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week08/08-2">Regularised latent</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week08/08-3">Training VAEs</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/05%20-%20Generative%20models.pdf">🖥️</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/11-VAE.ipynb">📓</a>
        <a href="https://youtu.be/7Rb4s9wNOmc">🎥</a>
      </td>
    </tr>
<!-- =============================== WEEK 9 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week09/09">⑨</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week09/09-1">Sparsity</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1wJRzhjSqlrSqEpX4Omagb_gdIkQ5f-6K">🖥️</a>
        <a href="https://youtu.be/Pgct8PKV7iw">🎥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week09/09-2">World model, GANs</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week09/09-3">Training GANs</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/05%20-%20Generative%20models.pdf">🖥️</a>
        <a href="https://github.com/pytorch/examples/tree/master/dcgan">📓</a>
        <a href="https://youtu.be/xYc11zyZ26M">🎥</a>
      </td>
    </tr>
<!-- =============================== WEEK 10 =============================== -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week10/10">⑩</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week10/10-1">CV SSL I</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=16lsnDN2HIBTcRucbVKY5B_U16c0tNQhR">🖥️</a>
        <a href="https://youtu.be/0KeR6i1_56g">🎥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week10/10-2">CV SSL II</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week10/10-3">Predictive Control</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/09%20-%20Controller%20learning.pdf">🖥️</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/14-truck_backer-upper.ipynb">📓</a>
        <a href="https://youtu.be/A3klBqEWR-I">🎥</a>
      </td>
    </tr>
<!-- =============================== WEEK 11 =============================== -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week11/11">⑪</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week11/11-1">Activations</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/file/d/1AzFVLG7D4NK6ugh60f0cJQGYF5OL2sUB">🖥️</a>
        <a href="https://drive.google.com/file/d/1rkiZy0vjZqE2w7baVWvxwfAGae0Eh1Wm">🖥️</a>
        <a href="https://drive.google.com/file/d/1tryOlVAFmazLLZusD2-UfReFMkPk5hPk">🖥️</a>
        <a href="https://youtu.be/bj1fh3BvqSU">🎥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week11/11-2">Losses</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week11/11-3">PPUU</a></td>
      <td>
        <a href="http://bit.ly/PPUU-slides">🖥️</a>
        <a href="http://bit.ly/PPUU-code">📓</a>
        <a href="https://youtu.be/VcrCr-KNBHc">🎥</a>
      </td>
    </tr>
<!-- =============================== WEEK 12 =============================== -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week12/12">⑫</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-1">DL for NLP I</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/file/d/149m3wRavTp4DQZ6RJTej8KP8gv4jnkPW/">🖥️</a>
        <a href="https://youtu.be/6D4EWKJgNn0">🎥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-2">DL for NLP II</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-3">Attention &amp; transformer</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/10%20-%20Attention%20%26%20transformer.pdf">🖥️</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/15-transformer.ipynb">📓</a>
        <a href="https://youtu.be/f01J0Dri-6k">🎥</a>
      </td>
    </tr>
<!-- =============================== WEEK 13 =============================== -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week13/13">⑬</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week13/13-1">GCNs I</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/file/d/1oq-nZE2bEiQjqBlmk5_N_rFC8LQY0jQr/">🖥️</a>
        <a href="https://youtu.be/Iiv9R6BjxHM">🎥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week13/13-2">GCNs II</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week13/13-3">GCNs III</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/11%20-%20GCN.pdf">🖥️</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/16-gated_GCN.ipynb">📓</a>
        <a href="https://youtu.be/2aKXWqkbpWg">🎥</a>
      </td>
    </tr>
<!-- =============================== WEEK 14 =============================== -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week14/14">⑭</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week14/14-1">Structured Prediction</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/file/d/1qBu-2hYWaGYEXeX7kAU8O4S2RZ1hMjsk/">🖥️</a>
        <a href="https://youtu.be/gYayCG6YyO8">🎥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week14/14-2">Graphical methods</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week14/14-3">Regularisation and Bayesian</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/07%20-%20Regularisation.pdf">🖥️</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/12-regularization.ipynb">📓</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/08%20-%20Bayesian%20NN.pdf">🖥️</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/13-bayesian_nn.ipynb">📓</a>
        <a href="https://youtu.be/DL7iew823c0">🎥</a>
      </td>
    </tr>
  </tbody>
</table>

<h2 id="people">People</h2>

<table>
  <thead>
    <tr>
      <th>Role</th>
      <th>Photo</th>
      <th>Contact</th>
      <th>About</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Instructor</td>
      <td><img src="https://atcold.github.io/pytorch-Deep-Learning/images/Yann.png" width="100" height="100"></td>
      <td><a href="https://twitter.com/ylecun">Yann LeCun</a><br>yann@cs.nyu.edu</td>
      <td>Silver Professor in CS at NYU<br>and Turing Award winner</td>
    </tr>
    <tr>
      <td>Instructor</td>
      <td><img src="https://avatars1.githubusercontent.com/u/2119355" width="100" height="100"></td>
      <td><a href="https://twitter.com/alfcnz">Alfredo Canziani</a><br>canziani@nyu.edu</td>
      <td>Asst. Prof. in CS at NYU</td>
    </tr>
    <tr>
      <td>Assistant</td>
      <td><img src="https://pbs.twimg.com/profile_images/1186879808845860864/czRv3g1G_400x400.jpg" width="100" height="100"></td>
      <td><a href="https://twitter.com/marikgoldstein">Mark Goldstein</a><br>goldstein@nyu.edu</td>
      <td>PhD student in CS at NYU</td>
    </tr>
    <tr>
      <td>Webmaster</td>
      <td><img src="https://pbs.twimg.com/profile_images/673997980370927616/vMXf545j_400x400.jpg" width="100" height="100"></td>
      <td><a href="https://twitter.com/ebetica">Zeming Lin</a><br>zl2799@nyu.edu</td>
      <td>PhD student in CS at NYU</td>
    </tr>
  </tbody>
</table>

<!--
|Grader|<img src="https://st3.depositphotos.com/13159112/17145/v/450/depositphotos_171453724-stock-illustration-default-avatar-profile-icon-grey.jpg" width="100" height="100">|Serkan Karakulak <br>sk7685@nyu.edu|
|Grader|<img src="https://st3.depositphotos.com/13159112/17145/v/450/depositphotos_171453724-stock-illustration-default-avatar-profile-icon-grey.jpg" width="100" height="100">|Raghav Jajodia <br>rj1408@nyu.edu|
|Grader|<img src="https://st3.depositphotos.com/13159112/17145/v/450/depositphotos_171453724-stock-illustration-default-avatar-profile-icon-grey.jpg" width="100" height="100">|Priyank Pathak <br>pp1953@nyu.edu|
|Grader|<img src="https://st3.depositphotos.com/13159112/17145/v/450/depositphotos_171453724-stock-illustration-default-avatar-profile-icon-grey.jpg" width="100" height="100">|Chiao-Hsun Wang <br>chw371@nyu.edu|
|Grader|<img src="https://st3.depositphotos.com/13159112/17145/v/450/depositphotos_171453724-stock-illustration-default-avatar-profile-icon-grey.jpg" width="100" height="100">|Pedro Vidal<br>pmh314@nyu.edu|
|Grader|<img src="https://st3.depositphotos.com/13159112/17145/v/450/depositphotos_171453724-stock-illustration-default-avatar-profile-icon-grey.jpg" width="100" height="100">|Bixing Yan <br>by783@nyu.edu|
-->

<h2 id="disclaimer">Disclaimer</h2>

<p>All other texts found on this site are lecture notes taken by students of the New York University during lectures given by Yann Le Cun, Alfredo Canziani, Ishan Misra, Mike Lewis and Xavier Bresson.
Thus the texts in English were written by about 130 people, which has an impact on the homogeneity of the texts (some write in the past tense, others in the present tense; the abbreviations used are not always the same; some write short sentences, while others write sentences of up to 5 or 6 lines, etc.).
It is possible that there may be some omissions: typing errors, spelling mistakes, etc. If you notice any, we invite you to submit a PR on the <a href="https://github.com/Atcold/pytorch-Deep-Learning/pulls">GitHub directory of the site</a> specifying with an <code>[EN]</code> that it concerns the English translation.</p>

<p>Wishing you a deep reading !</p>

                    </div>
                </main>

                <nav aria-label="Page navigation">
                    <!-- Mobile navigation buttons -->
                    <a rel="prev" href="https://atcold.github.io/pytorch-Deep-Learning" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i></i>
                    </a>

                    <a rel="next" href="https://atcold.github.io/pytorch-Deep-Learning/en/about/" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>

                    
                </nav>
            </div>
        </div>

        <nav aria-label="Page navigation">
                <a href="https://atcold.github.io/pytorch-Deep-Learning" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                    <i></i>
                </a>

                <a href="https://atcold.github.io/pytorch-Deep-Learning/en/about/" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                    <i></i>
                </a>
        </nav>

    </div>

    
    
    

    

    

</div>]]>
            </description>
            <link>https://atcold.github.io/pytorch-Deep-Learning/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24715307</guid>
            <pubDate>Thu, 08 Oct 2020 03:13:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[New York City thinks up to half of restaurants will close permanently [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 83 | Comments 135 (<a href="https://news.ycombinator.com/item?id=24715150">thread link</a>) | @bookofjoe
<br/>
October 7, 2020 | https://www.osc.state.ny.us/files/reports/osdc/pdf/nyc-restaurant-industry-final.pdf | <a href="https://web.archive.org/web/*/https://www.osc.state.ny.us/files/reports/osdc/pdf/nyc-restaurant-industry-final.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.osc.state.ny.us/files/reports/osdc/pdf/nyc-restaurant-industry-final.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24715150</guid>
            <pubDate>Thu, 08 Oct 2020 02:47:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I built an app to fix my depression]]>
            </title>
            <description>
<![CDATA[
Score 64 | Comments 40 (<a href="https://news.ycombinator.com/item?id=24715148">thread link</a>) | @zoozla
<br/>
October 7, 2020 | https://blog.elifiner.com/i-built-an-app-to-fix-my-depression/ | <a href="https://web.archive.org/web/*/https://blog.elifiner.com/i-built-an-app-to-fix-my-depression/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://blog.elifiner.com/i-built-an-app-to-fix-my-depression/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24715148</guid>
            <pubDate>Thu, 08 Oct 2020 02:47:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Recycling was a lie to sell more plastic, recycling industry veteran says]]>
            </title>
            <description>
<![CDATA[
Score 969 | Comments 407 (<a href="https://news.ycombinator.com/item?id=24714880">thread link</a>) | @vivekd
<br/>
October 7, 2020 | https://www.cbc.ca/documentaries/the-passionate-eye/recycling-was-a-lie-a-big-lie-to-sell-more-plastic-industry-experts-say-1.5735618 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/documentaries/the-passionate-eye/recycling-was-a-lie-a-big-lie-to-sell-more-plastic-industry-experts-say-1.5735618">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Less than 10 per cent of the plastics we’ve used have been recycled. A new documentary reveals why</p><div><figure><div><p><img alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5755241.1602170985!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/157672506.jpg"></p></div><figcaption>trash on the beach<!-- --> <!-- -->(Getty Images)</figcaption></figure><p><span><p>Although our landfills and oceans are full of it, we are as dependent as ever on plastic. And since COVID-19, it's gotten worse.&nbsp;</p>  <p>Last year, Canada announced it was working on a ban of single-use plastics, which was initally&nbsp;<a href="https://www.cbc.ca/news/canada/toronto/single-use-plastics-covid-1.5683617">sidelined by the pandemic</a>. Recently, the government announced that <a href="https://www.cbc.ca/news/politics/single-use-plastics-1.5753327">many single-use plastics will be banned</a> by the end of 2021. At the same time, <a href="https://www.cbc.ca/news/canada/toronto/single-use-plastics-covid-1.5683617">CBC News reports</a> our single-use plastic use increased by 250 to 300 per cent as people tossed their personal protective equipment and stopped using reusable bags and containers over fears they would spread the virus.</p>  <p>What makes our lives convenient is also burying us. <a href="https://www.cbc.ca/passionateeye/episodes/plastic-wars"><strong><em>Plastic Wars</em></strong></a>, presented by <em>The Passionate Eye</em>, looks at the mounting crisis and how the industry has spent millions promoting recycling — just to sell more plastic.</p>  <h2>Less than 10% of the plastics we've used have been recycled</h2>  <p>Although activists sounded the alarm about plastic waste in the 1970s, the documentary claims from 1990 to 2010, plastic production more than doubled. We've been sorting our trash for decades, believing it would be recycled. But the truth is the vast majority of the plastic we use won't be. Over the last seven decades, <a href="https://www.oecd.org/environment/waste/policy-highlights-improving-plastics-management.pdf">less than 10 per cent of plastic waste has been recycled</a>.&nbsp;</p>  <p>That's because, says David Allaway, from the Oregon Department of Environmental Quality, the conversation has been almost exclusively about recycling and not reducing and reusing.</p>  <p><span><span><div><div role="button" tabindex="0" title="Plastic Wars: Recycling"><div><p><img src="https://thumbnails.cbc.ca/maven_legacy/thumbnails/1002/935/PlasticWars_Recycling_2500kbps_620x350_1755680835594.jpg" alt=""></p></div></div></div><span>Even as the plastic crisis worsens, the demand for plastic grows and plastic production is rapidly expanding. One issue? Only focusing on recycling, and not reducing the amount of plastic that we use.<!-- --> <!-- -->1:06</span></span></span></p>  <h2>Recycling logo was used as a green marketing tool, says industry expert</h2>  <p>In the '80s, the industry was at the centre of an environmental backlash. Fearing an outright ban on plastics, manufacturers looked for ways to get ahead of the problem. They looked at recycling as a way to improve the image of their product and started labeling plastics with the now ubiquitous chasing-arrows symbol with a number inside.&nbsp;</p>  <p>According to Ronald Liesemer, an industry veteran who was tasked with overseeing the new initiative, "Making recycling work was a way to keep their products in the marketplace."&nbsp;</p>  <p>Most consumers might have assumed the symbol meant the product was recyclable. But according to experts in the film, there was no economically viable way to recycle most plastics, and they have ultimately ended up in a landfill. This included plastic films, bags and the wrapping around packaged goods, as well as containers like margarine tubs.<br> "Our own customers … they would flat out say, 'It says it's recyclable right on it,'" says Coy Smith, former board member of the National Recycling Coalition. "And I'd be like, 'I can tell you, I can't give this away. There's no one that would even take it if I paid them to take it.'" He believes manufacturers used the symbol as a green marketing tool.</p>  <p>"If the public thinks that recycling is working, then they're not going to be as concerned about the environment," says Larry Thomas, another top industry official interviewed in <a href="https://www.cbc.ca/passionateeye/episodes/plastic-wars"><em><strong>Plastic Wars</strong></em></a>.</p>  <p>According to Lewis Freeman, a former vice-president with the Society of the Plastics Industry, many in the industry had doubts about recycling from the start. "There was never an enthusiastic belief that recycling was ultimately going to work in a significant way," he says.</p>  <p>Yet the plastic industry spent millions on ads selling plastics and recycling to consumers.</p>  <h2>Lots of our plastic was shipped to China, then Southeast Asia, for 'recycling'</h2>  <p>To solve the plastic waste problem, many recyclers started selling their product to China in the 1990s. According to recycling broker Sunil Bagaria, China took waste that North American recyclers couldn't use. "As long as it remotely resembled plastic, they wanted it," he says.</p>  <p>But they used the good stuff and disposed of the rest. And because of a growing plastic waste problem in that country, China finally stopped taking most imported plastic waste in 2018.</p>  <p>"We never asked the question, 'Are they doing it the right way? Are we damaging the environment more in the name of recycling?'" says Bagaria.</p>  <p>Now, Southeast Asian countries like Indonesia have picked up the plastic waste market. And although some North American plastics recyclers are following up to ensure their products are in fact being recycled, plastic waste is now a growing problem there, too.&nbsp;</p>  <p>In <a href="https://www.cbc.ca/passionateeye/episodes/plastic-wars"><em><strong>Plastic Wars</strong></em></a>, local activist Yuyan Ismawati visits a rural community where locals scour through a huge field of plastic waste for items of value and burn the rest. This creates health problems for the residents in addition to destroying the surrounding environment. "We are struggling to clean up the modern debris and modern litter in Indonesia, the additional burden of waste from overseas — I don't know how we are going to handle it," says Ismawati. "Americans need to know that your waste ended up here."</p>  <h2>Production of plastics expected to triple by 2050</h2>  <p>In 2020, roughly 60 years after concerns about plastic waste were first raised, the focus is still on the consumer to recycle, says Allaway, and not on the environmental impact of the product and overproduction by the industry.</p>  <p><span><span><div><div role="button" tabindex="0" title="Plastic Wars: Full Impact"><div><p><img src="https://thumbnails.cbc.ca/maven_legacy/thumbnails/1004/887/PlasticWars_FullImpact_2500kbps_620x350_1755684419745.jpg" alt=""></p></div></div></div><span>Consumers are constantly told that they should do their part to reduce plastic waste, but in reality, consumers have the lowest amount of leverage in reducing waste - it's plastic producers that should be reporting their full environmental impacts.<!-- --> <!-- -->1:56</span></span></span></p>  <p>According to <a href="https://www.cbc.ca/passionateeye/episodes/plastic-wars"><em><strong>Plastic Wars</strong></em></a> the problem is only going to get worse. By 2050, it's estimated the global production of plastic will triple. As the oil and gas industry — which provides the source materials for plastics — &nbsp;faces a future of declining demand for fuel, it has turned to other markets.&nbsp;</p>  <p>The stakes are high, says Annie Leonard, executive director of Greenpeace USA. "This is their lifeline," she says. "They are going to double down on single-use plastic like we have never seen. So we're heading towards a real battle.... This is the big war."&nbsp;</p>  <p>Watch <a href="https://www.cbc.ca/passionateeye/episodes/plastic-wars"><em><strong>Plastic Wars</strong></em></a> on <em>The Passionate Eye</em>.</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/documentaries/the-passionate-eye/recycling-was-a-lie-a-big-lie-to-sell-more-plastic-industry-experts-say-1.5735618</link>
            <guid isPermaLink="false">hacker-news-small-sites-24714880</guid>
            <pubDate>Thu, 08 Oct 2020 02:01:04 GMT</pubDate>
        </item>
    </channel>
</rss>
