<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 10]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 10. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Fri, 30 Oct 2020 20:16:20 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-10.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Fri, 30 Oct 2020 20:16:20 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[  You will need a subscription license to access Qt 6 (non-LPGL)]]>
            </title>
            <description>
<![CDATA[
Score 26 | Comments 10 (<a href="https://news.ycombinator.com/item?id=24928720">thread link</a>) | @deng
<br/>
October 29, 2020 | https://www.qt.io/faq/what-happens-to-my-perpetual-licenses-once-i-convert-to-subscription | <a href="https://web.archive.org/web/*/https://www.qt.io/faq/what-happens-to-my-perpetual-licenses-once-i-convert-to-subscription">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p><span><span>Yes. If your </span><span>Qt licenses are perpetual, you may continue to use the product in perpetuity after your maintenance expires.&nbsp; Access to product and technical support will only be available via the purchase of an Extended Maintenance Contract for software releases that are end-of-life. If you opt not to renew, please note that Qt will not guarantee to support software versions acquired with a perpetual license.</span></span></p>
<p><span><span>Please note, you will need a subscription license to access Qt 6.</span></span></p>
<!--more-->
<p><span><span>Qt versions can be viewed <a href="https://wiki.qt.io/QtReleasing" rel="noopener">here</a>.</span></span></p>
</span></p></div>]]>
            </description>
            <link>https://www.qt.io/faq/what-happens-to-my-perpetual-licenses-once-i-convert-to-subscription</link>
            <guid isPermaLink="false">hacker-news-small-sites-24928720</guid>
            <pubDate>Thu, 29 Oct 2020 09:44:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Image Scaling Attacks]]>
            </title>
            <description>
<![CDATA[
Score 192 | Comments 39 (<a href="https://news.ycombinator.com/item?id=24927655">thread link</a>) | @wendythehacker
<br/>
October 28, 2020 | https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/ | <a href="https://web.archive.org/web/*/https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
    <p>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag “huskyai” to see related posts.</p>
<ul>
<li><a href="https://embracethered.com/blog/posts/2020/husky-ai-walkthrough/">Overview</a>: How Husky AI was built, threat modeled and operationalized</li>
<li><a href="https://embracethered.com/blog/posts/2020/husky-ai-threat-modeling-machine-learning/">Attacks</a>: Some of the attacks I want to investigate, learn about, and try out</li>
</ul>
<p>A few weeks ago while preparing demos for my GrayHat 2020 - Red Team Village presentation I ran across “Image Scaling Attacks” in <a href="https://www.usenix.org/system/files/sec20-quiring.pdf">Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning</a> by Erwin Quiring, et al.</p>
<p>I thought that was so cool!</p>
<h2 id="what-is-an-image-scaling-attack">What is an image scaling attack?</h2>
<p>The basic idea is to hide a smaller image inside a larger image (it should be about 5-10x the size). The attack is easy to explain actually:</p>
<ol>
<li>Attacker crafts a malicious input image by hiding the desired target image inside a benign image</li>
<li>The image is loaded by the server</li>
<li>Pre-processing resizes the image</li>
<li>The server acts and makes decision based on a different image then intended</li>
</ol>
<p>My goal was to hide a husky image inside another image:</p>
<p><a href="https://embracethered.com/blog/images/2020/image-rescale-attack.gif"><img src="https://embracethered.com/blog/images/2020/image-rescale-attack.gif" alt="Image Rescaling Attack"></a></p>
<p>Here are the two images I used - before and after the modification:
<a href="https://embracethered.com/blog/images/2020/image-rescaling-attack-schoenbrunn.png"><img src="https://embracethered.com/blog/images/2020/image-rescaling-attack-schoenbrunn.png" alt="Image Rescaling Attack"></a></p>
<p>If you look closely, you can see that the second image does have some strange dots all around. But this is not noticable when viewed in smaller version.</p>
<p>You can find the code on <a href="https://github.com/EQuiw/2019-scalingattack">Github</a>. I used Google Colab to run it, and there were some errors initialy but it worked - let me know if interested and I can clean up and share the Notebook also.</p>
<h2 id="rescaling-and-magic-happens">Rescaling and magic happens!</h2>
<p>Now, look what happens when the image is loaded and resized with <code>OpenCV</code> using default settings:</p>
<p><a href="https://embracethered.com/blog/images/2020/image-rescaling-attack.png"><img src="https://embracethered.com/blog/images/2020/image-rescaling-attack.png" alt="Image Rescaling Attack"></a></p>
<p>On the left you can see the original sized image, and on the left the same image downsized to 128x128 pixels.</p>
<p><strong>That’s amazing!</strong></p>
<p>The downsized image is an entirely different picture now! Of course I picked a husky, since I wanted to attack “Husky AI” and find another bypass.</p>
<h2 id="implications">Implications</h2>
<p>This can have a set of implications:</p>
<ol>
<li><strong>Training process:</strong> Images that poisen the training data (as pre-processing rescales images)</li>
<li><strong>Model queries:</strong> The model might predict on a different image than the one the user uploaded</li>
<li><strong>Non ML related attacks:</strong> This can also be an issue in other, non machine learning areas.</li>
</ol>
<p>I guess security never gets boring, there is always something new to learn.</p>
<h2 id="mitigations">Mitigations</h2>
<p>Turns out that Husky AI uses PIL and that was not vulnerable to this attack by default.</p>
<p>I got lucky, because initially Husky AI did use <code>OpenCV</code> and it’s default settings to resize images. But for some reason I changed that early on (not knowing it would also mitigate this attack).</p>
<p>If you use <code>OpenCV</code> the issue can be fixed by using the <code>interpolation</code> argument when calling the <code>resize</code> API to not have it use the default.</p>
<p>Hope that was useful and interesting.</p>
<p>Cheers,
Johann.</p>
<p><a href="https://twitter.com/wunderwuzzi23">@wunderwuzzi23</a></p>
<h2 id="references">References</h2>
<ul>
<li>Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning (<a href="https://www.usenix.org/system/files/sec20-quiring.pdf">https://www.usenix.org/system/files/sec20-quiring.pdf</a>) (Erwin Quiring, TU Braunschweig)</li>
<li><a href="https://github.com/EQuiw/2019-scalingattack">https://github.com/EQuiw/2019-scalingattack</a></li>
</ul>

  </section></div>]]>
            </description>
            <link>https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24927655</guid>
            <pubDate>Thu, 29 Oct 2020 05:48:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Sale of Amateur Radio AMPRnet TCP/IP Addresses Raised $108M]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24927037">thread link</a>) | @todsacerdoti
<br/>
October 28, 2020 | https://www.icqpodcast.com/news/2020/10/25/sale-of-amateur-radio-amprnet-tcpip-addresses-raised-108m | <a href="https://web.archive.org/web/*/https://www.icqpodcast.com/news/2020/10/25/sale-of-amateur-radio-amprnet-tcpip-addresses-raised-108m">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-7b3dd6c3950208794ea7"><div><p>President of Amateur Radio Digital Communications (ARDC) has confirmed they received $108 million from Amazon for 4 million amateur radio TCP/IP addresses </p><p>Since its allocation to Amateur Radio in the mid-1980s, Internet network 44 (44.0.0.0/8), known as the AMPRNet™, has been used by amateur radio operators to conduct scientific research and to experiment with digital communications over the radio with a goal of advancing the state of the art of Amateur Radio networking, and to educate amateur radio operators in these techniques.</p><p>Amateur Radio Digital Communications (ARDC) is a non-profit California corporation formed to further these goals.</p><p>In mid-2019 a block (44.192.0.0/10) of approximately four million AMPRNet™ IP addresses, out of the 16 million available, was sold to Amazon by ARDC but it is only now that the sale price has been released. Amazon paid $27 for each IPv4 address.</p></div></div></div>]]>
            </description>
            <link>https://www.icqpodcast.com/news/2020/10/25/sale-of-amateur-radio-amprnet-tcpip-addresses-raised-108m</link>
            <guid isPermaLink="false">hacker-news-small-sites-24927037</guid>
            <pubDate>Thu, 29 Oct 2020 03:57:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to walk upright and stop living in a cave]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24927008">thread link</a>) | @taylorlunt
<br/>
October 28, 2020 | https://taylor.gl/blog/9/ | <a href="https://web.archive.org/web/*/https://taylor.gl/blog/9/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
    
    <div>
      
<p>
<a href="https://taylor.gl/">Home</a>

  
  
  <a href="https://taylor.gl/"> </a>

  
  
  »
  
  <a href="https://taylor.gl/blog/"> Blog</a>

</p>

<p><span>
  Reading time: 9 minutes.

  Written in 2020.
</span></p>

<p>Call me arrogant, but I’d rather optimize my indoor environment than try to spend more time in the capricious outdoors. I think it’s defeatism to give up on improving our indoor spaces and resign ourselves to the fickle weather and seasons. </p>
<p>If I was going to create an ideal environment for a human, I think there are several things I would include that we routinely fail to include in our homes and offices.</p>
<h3 id="lighting">Lighting</h3>
<p>Our indoor lighting situation usually sucks. The fact that “natural lighting” is a selling point in real estate shows how terrible a job we are doing in this department. We rely on the sun naturally providing us with sufficient light, and if it’s an overcast day or the days have grown shorter in the winter, then I guess we’re shit out of luck. </p>
<p>Usually, indoor areas are around 50-500 lux. This is hundreds of times dimmer than the sunlight. Clearly, we weren’t designed to thrive in such dim environments, and science does verify a connection between brighter light and alertness. If we don’t want to be sleepy like it’s nighttime, we shouldn’t light our rooms like it’s nighttime. For some, the effects of dim lighting go beyond simple lethargy and, especially in the winter, cause serious mood problems like seasonal affective disorder or the winter blues. This is common, but it’s not necessary. Bright light, particularly blue light, can also generally boost mood and may be a comparable stimulant to caffeine. (Those who are prone to mania should be careful, as intense light can trigger mania or hypomania in those predisposed.) Brighter lighting can also help circadian rhythm issues (which I, for example, have struggled with for years), both by entraining your circadian rhythm so your body better knows when it’s day, and by shortening it if it’s too long. </p>
<p>Lighting isn’t as expensive as it used to be, so we can do better than we have in the past. The cost of electricity for LED lighting is now negligible, and the only real factor is the cost of the bulbs themselves. Reaching for the full 100,000 lux of sunlight would still be prohibitively expensive, but going for at least 10,000 lux is doable with only a few hundred dollars. I won’t go into specifics here, but you can get more information on specific lighting setups <a href="https://www.lesswrong.com/posts/hC2NFsuf5anuGadFm/how-to-build-a-lumenator">here</a> or <a href="https://meaningness.com/metablog/sad-light-lumens">here</a>. In particular, get bulbs with a color temperature close to sunlight (5600k), but make sure the bulbs have a <span>good<span>good means 90+</span></span> Color Rendering Index (CRI), otherwise the light will feel harsh.</p>
<p>I recommend putting any bright lighting you buy for your home on electrical timers so you don’t accidentally leave them on during the evening and screw up your sleep. You may also want to set your phone/computer brightness on a timer, if you can. The goal is to mimic the natural day/night cycle of our evolutionary environment, but without all the pesky volatility of nature. You can get programs like f.lux too, which reduce the amount of blue light emitted by your device in the evening, but in my experience this isn’t good enough and reducing the actual brightness of the device at night is also important.</p>
<p>“But what about vitamin D? Just go outside!” This is terrible advice, and I hear it too often. Sunlight is a powerful carcinogen, and vitamin D supplements are not, and they’re cheap. </p>
<h3 id="carbon-dioxide">Carbon dioxide</h3>
<p>Carbon MON-oxide is the deadly one you probably already have a monitor for in your house. Carbon DI-oxide is the feeble cousin of carbon monoxide, but it still has a negative effect on human health: <span>high (but common)<span>1,000 ppm or higher</span></span> levels impairs our ability to think. Just what you don’t want in an office. High levels may also have a negative long-term impact in other areas of our health. </p>
<p>Hold your breath. When it sucks and you decide to start breathing again, it’s carbon dioxide buildup, not lack of oxygen, causing you to feel panic and the need to breath. Carbon dioxide is a toxin. And we breath it out into poorly ventilated rooms, where the levels can rise to double or triple what they are <span>outdoors<span>around 400 ppm</span></span>.</p>
<p>Several studies have shown significant (temporary) cognitive impairments due to carbon dioxide levels over 1,000 ppm, but such levels are <span>common<span>I recently bought a carbon dioxide meter and found such levels in my home.</span></span> in poorly ventilated shared spaces. Fortunately, the solution is simple: open a window. Unfortunately, this doesn’t work when it’s raining, or when it’s too hot outside, or when it’s too cold outside… In particular, I have to contend with Canadian winters, which means opening the window is a valid strategy for a minority of the year unless I buy an expensive heat recovery ventilator. I don’t have a good solution for mitigating carbon dioxide buildup in the winter. Let me know if you do.</p>
<p>And, by the way, plants won’t work. They won’t suck up nearly enough carbon dioxide. You would need hundreds of plants per person, or roughly a dozen full-size trees per person, to offset the carbon dioxide exhaled by humans in a room.</p>
<p>A fun fact: if we don’t stop pumping carbon dioxide into the atmosphere, then in about a century, carbon dioxide <em>outdoors</em> may reach cognitively impairing levels. Then what do we do? </p>
<h3 id="temperature-and-humidity">Temperature and Humidity</h3>
<p>High/low humidity and high/low temperature both lead to discomfort and lower scores on concentration measures. People generally have temperature under control, or at least it’s something they’re aware of. Humidity is less common to measure, but a $10 hygrometer should help you get your indoor space to the ideal 30-50% humidity range if it isn’t already. Air conditioners also tend to reduce humidity as well as temperature, so air-condition in the summer and use a humidifier in the winter.</p>
<p>At night, drop the temperature a few degrees if you can; It’s easier to sleep in a cool room. I wonder how many hours of sleep have been reclaimed already due to the advent of smart thermostats.</p>
<h3 id="background-noise">Background Noise</h3>
<p>I imagine this factor is more subjective than the others, but too loud is distracting, even aggrivating; too quiet makes your sniffles and sighs painfully audible to others, and so is distracting. Uneven background noise like traffic is worse than the uniform background noise of white noise or trickling water. Bad background noise leads to poorer cognition and focus.</p>
<p>It’s easy to be bothered by noise and not realize it until the noise stops and a wave of relief finally makes you aware of how annoyed you were by the sound. Noise issues are happily easy to control: earplugs or noise-cancelling headphones will generally do the trick. It would be utopic to eliminate bothersome noise from the environment altogether, but it’s not necessary. </p>
<h3 id="segregation-of-activities">Segregation of Activities</h3>
<p>A heroin addict who normally takes their dose in their car decides one day to inject in their bathroom. They die of an overdose, even though they took the same amount they normally do. Why? Our brains maintain associations with different environments. If you normally inject heroin when you get in your car, then your body starts to prepare you for the drug as soon as you get in the car. Drug tolerance, then, is partly environmental. (This <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1196296/">actually happened</a> and happens regularly.) Your mind and body are affected by your environment due to Pavlovian conditioning. When the bell rings, the dog salivates. When the lunch bell rings, so do you. </p>
<p>One common piece of advice given by doctors to insomniacs is to only use your bed for sleeping and for sex, and it’s good advice. If you use your bed for reading, studying, and watching TV, then your mind will not form a strong association between the bed and sleep, and you will have a harder time falling asleep. </p>
<p>Likewise, if you do all your slacking off at the same desk you do your work at, you will probably have a harder time focusing. Even having your smartphone within your field of view while you work has been shown to reduce focus. So it wouldn’t hurt to have different areas for work and play, and to not eat at your desk. (And even different user accounts on your computer for work and non-work, if you don’t find that idea to be a pain in the ass like I do.)</p>
<p>We also form associations not just with space, but with time. Hence another piece of common sleep hygeine advice: go to sleep at the same time every night. Your body will learn to expect sleep at that time. Likewise, people who eat at the same time every day eat with their bodies prepared to receive food, and so are less likely to become obese. Studies have shown this. Unfortunately, setting every aspect of your life to a clock can make you feel like a robot, so I usually don’t tolerate such rigidity in my life. But it’s worth thinking about.</p>
<h3 id="conclusion">Conclusion</h3>
<ul>
<li>Brighter lights for your poor eyes</li>
<li>Better ventilation for your poor lungs</li>
<li>Optimal temperature and humidity for your poor skin</li>
<li>Less distracting background noise for your poor ears</li>
<li>Activity-specific areas for your poor brain</li>
</ul>
<p>I also think the aesthetics of most of our indoor environments could use an upgrade, but I don’t have much to say on the subject besides simply saying so. (Though I would bet: green lush &gt; grey drab.)</p>
<p>We sometimes act like we are just <span>machines<span>caffeine in ⟶ code out</span></span>, but we are not. We’re mushy creatures with delicate bodies and delicate minds, too. And we evolved for one specific environment. There is no guarantee that the indoor environment which is cheapest to produce is going to be just as good for us as a bespoke imitation of our evolutionary environment, and in fact it is not. I think life would be more pleasant if people took these factors more serously when designing indoor environments, and our work would be more efficient and less prone to mistakes.</p>


    </div>
  </section></div>]]>
            </description>
            <link>https://taylor.gl/blog/9/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24927008</guid>
            <pubDate>Thu, 29 Oct 2020 03:52:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Falsehoods programmers believe about addresses (2013)]]>
            </title>
            <description>
<![CDATA[
Score 33 | Comments 24 (<a href="https://news.ycombinator.com/item?id=24926417">thread link</a>) | @gk1
<br/>
October 28, 2020 | https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/ | <a href="https://web.archive.org/web/*/https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>Perhaps you've read posts like <a href="http://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/">Falsehoods Programmers Believe About Names</a>
and <a href="http://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time">Falsehoods programmers believe about time</a>.
Maybe you've also read <a href="http://wiesmann.codiferes.net/wordpress/?p=15187&amp;lang=en">Falsehoods programmers believe about geography</a>.</p>

<p>Addressing is a fertile ground for incorrect assumptions, because everyone's used to dealing with addresses and 99% of the time they seem so simple.
Below are some incorrect assumptions I've seen made, or made myself, or had reported to me.
(If you want to look up an address for a UK postcode or vice-versa to confirm what I'm telling you, try the <a href="http://www.royalmail.com/postcode-finder/">Royal Mail Postcode Finder</a>)</p>

<!-- Composition of building numbers -->

<ul>
<li><p><strong>An address will start with, or at least include, a building number.</strong></p>

<p>Counterexample: Royal Opera House, Covent Garden, London, WC2E 9DD, United Kingdom.</p></li>
<li><p><strong>When there is a building number, it will be all-numeric.</strong></p>

<p>Counterexample: 1A Egmont Road, Middlesbrough, TS4 2HT</p>

<p>4-5 Bonhill Street, London, EC2A 4BX</p></li>
<li><p><strong>No buildings are numbered zero</strong></p>

<p>Counterexample: 0 Egmont Road, Middlesbrough, TS4 2HT</p></li>
<li><p><strong>Well, at the very least no buildings have negative numbers</strong></p>

<p>Guy Chisholm provided this counterexample: Minusone Priory Road, Newbury, RG14 7QS</p>

<p>(none of the databases I've checked render this as -1)</p></li>
<li><p><strong>We can put those funny numbers into the building name field, as no buildings have both a name and a funny number</strong></p>

<p>Counterexample: Idas Court, 4-6 Princes Road, Hull, HU5 2RD</p></li>
<li><p><strong>When there's a building name, there won't be a building number (or vice-versa)</strong></p>

<p>Counterexample: Flat 1.4, Ziggurat Building, 60-66 Saffron Hill, London, EC1N 8QX, United Kingdom</p></li>
<li><p><strong>A building number will only be used once per street</strong></p>

<p>The difference between 50 Ammanford Road, Tycroes, Ammanford, SA18 3QJ and 50 Ammanford Road, Llandybie, Ammanford, SA18 3YF is about 4 miles (<a href="https://maps.google.co.uk/maps?q=SA18+3QJ+to+SA18+3YF">Google Maps</a>).</p></li>
<li><p><strong>When there's line with a number in an address, it's the building number.</strong></p>

<p>Counterexample: Flat 18, Da Vinci House, 44 Saffron Hill, London, EC1N 8FH, United Kingdom</p>

<p>You also get suite numbers, floor numbers, unit numbers, and organisations with numbers in their names.</p>

<p>Adrien Piérard contributes an address from Japan with fifteen digits in six separate numbers (five if you count the zip code as a single number). The format is: 980-0804 (zip code), Miyagi-ken (prefecture) Sendai-shi (city) Aoba-ku (ward) Kokubuncho (district) 4-10-20 (sub-district-number block-number lot-number) Sendai (building name) 401 (flat number).</p></li>
<li><p><strong>OK, the first line starting with a number then</strong></p>

<p>Counterexample: 3 Store, 311-318 High Holborn, London, WC1V 7BN</p></li>
<li><p><strong>A building will only have one number</strong></p>

<p>Benton Lam offers this address from the Hong Kong Special Administrative Region - it has both a number on its road (14) and in its group of buildings (3): 15/F, Cityplaza 3, 14 TaiKoo Wan Road, Island East, HKSAR</p></li>
<li><p><strong>The number of buildings is the difference between the highest and lowest building numbers</strong></p>

<p>Tibor Schütz points out building numbers may be skipped - for example, on a street where even-numbered buildings are on one side, odd numbers on the other; multiple buildings sharing the same number (such as where a new house has been built) and buildings with more than one number.</p>

<p>Cyrille Chépélov and Sami Lehtinen tell me in Antibes, France and rural Finland some buildings are numbered based on the distance from the start of the road - such as Longroad 65 for the building 750m from the start of longroad.</p></li>
<li><p><strong>If the addresses on the left of the road are even, the addresses on the right must be odd</strong></p>

<p>Cyrille Chépélov points out that in places, <a href="https://maps.google.fr/maps?q=48.857415,2.467167">Boulevard Théophile Sueur, Montreuil, Seine-Saint-Denis, France</a> has evens-only on both sides. The two sides are also in different cities and Départements.</p></li>
<li><p><strong>A building name won't also be a number</strong></p>

<p>Ben Tilly reports on Ten Post Office Sq, Boston MA 02109 USA - which is not, reportedly, the same as 10 Post Office Sq, Boston MA 02109 USA.</p></li>
<li><p><strong>Well, at least you can omit leading zeros</strong></p>

<p>Shaun Crampton reports living at 101 Alma St, Apartment 001, Palo Alto - where apartments 1 and 001 were on different floors.</p></li>
<li><p><strong>A street with a building A will not also have a building Alpha</strong></p>

<p>Douglas Perreault reports he lived in a block within a condo association; it was a large association, with blocks A through Z then Alpha, Beta, Gamma, Delta, and Theta. Mail and deliveries were often misrouted from block Alpha to block A and vice-versa. His address at the time was: 14100 N 46th St., Alpha 39, Tampa, FL 33613</p></li>
</ul>

<!-- Composition of street names -->

<ul>
<li><p><strong>A street name won't include a number</strong></p>

<p>8 Seven Gardens Burgh, WOODBRIDGE, IP13 6SU (pointed out by Raphael Mankin)</p></li>
<li><p><strong>OK, but numbers in street names are expressed as words, not digits</strong></p>

<p>Jan Jongboom reports streets can be numbered in the Netherlands - for example, Plein 1944 in Nijmegen.</p></li>
<li><p><strong>When there's a numbered street and a house number, there will be a separator between them</strong></p>

<p>Another from Jan Jongboom: Gondel 2695, Lelystad, means area Gondel, street 26, number 95</p></li>
<li><p><strong>Street names always end in descriptors like 'street', 'avenue', 'drive', 'square', 'hill' or 'view'</strong></p>

<p>They don't always - for example: Piccadilly, London, W1J 9PN</p></li>
<li><p><strong>OK, but when they do have a descriptor there will only be one</strong></p>

<p>A street name can be entirely descriptors: 17 Hill Street, London, W1J 5LJ or <a href="https://en.wikipedia.org/wiki/Avenue_Road">Avenue Road, Toronto, Ontario</a>.</p></li>
<li><p><strong>OK, but when they do have a descriptor it will be at the end</strong></p>

<p>French addresses use prefix descriptors like 'rue', 'avenue', 'place' and 'allee'.</p></li>
<li><p><strong>OK, but if there's a descriptor it'll be at the start or end of the street name.</strong></p>

<p>Or the middle, like 3 Bishops Square Business Park, Hatfield, AL10 9NA</p></li>
<li><p><strong>OK, but at the very least you wouldn't name a town Street</strong></p>

<p><a href="https://maps.google.co.uk/maps?q=Street,+Somerset">Actually there's a town called Street in Somerset, UK</a>.</p></li>
<li><p><strong>Street numbers (and building numbers) don't contain fractions</strong></p>

<p>Dan, Fred Kroon, David Underwood and Daniel Dickison submitted examples of fractional street numbers like <a href="https://maps.google.com/maps?q=43rd%20%C2%BD%20st,%20Pittsburgh,%20PA">43rd ½ St, Pittsburgh, PA</a>, and of fractional building numbers. These can be written in unicode (43rd ½ St), as a fraction with a slash (43 1/2) or as a decimal (43.5)</p>

<p>Gene Wirchenko reports a fractional building number: 1313 1/2 Railroad Ave Bellingham WA 98225-4729</p></li>
<li><p><strong>Street names don't recurr in the same city</strong></p>

<p><a href="https://maps.google.co.uk/maps?q=from:W3+6LJ+to:W5+5DB+to:N8+7PB+to:SE25+6EP+to:E13+0AJ+to:E17+7LD+to:NW10+4LX+to:N1+9TR+to:E1+6PG+to:NW1+0JH+to:W14+8NL+to:SE13+6AD+to:SW19+5DX+to:E11+2AJ+to:SW19+2AE+to:E6+2HJ+&amp;saddr=W3+6LJ&amp;daddr=W5+5DB+to:N8+7PB+to:SE25+6EP+to:E13+0AJ+to:E17+7LD+to:NW10+4LX+to:N1+9TR+to:E1+6PG+to:NW1+0JH+to:W14+8NL+to:SE13+6AD+to:SW19+5DX+to:E11+2AJ+to:SW19+2AE+to:E6+2HJ">Here's a map of the following addresses:</a></p>

<ul>
<li>High Street, London, W3 6LJ</li>
<li>High Street, London, W5 5DB</li>
<li>High Street, London, N8 7PB</li>
<li>High Street, London, SE25 6EP</li>
<li>High Street, London, E13 0AJ</li>
<li>High Street, London, E17 7LD</li>
<li>High Street, London, NW10 4LX</li>
<li>Islington High Street, London, N1 9TR</li>
<li>Shoreditch High Street, London, E1 6PG</li>
<li>Camden High Street, London, NW1 0JH</li>
<li>Kensington High Street, London, W14 8NL</li>
<li>Lewisham High Street, London, SE13 6AD</li>
<li>High Street Wimbledon, London, SW19 5DX</li>
<li>High Street Wanstead, London, E11 2AJ</li>
<li>High Street Colliers Wood, London, SW19 2AE</li>
<li>High Street North, London, E6 2HJ </li>
</ul></li>
<li><p><strong>But street names don't recurr in close proximity</strong></p>

<p>Julian Fleischer provides an example from Bocholt in Germany showing several roads in close proximity all called <a href="https://maps.google.com/maps?q=51.853945,6.615334">Up de Welle</a>.</p></li>
<li><p><strong>An address will be comprised of road names</strong></p>

<p>Kirk Kerekes spent several years using an address of the form "2 mi N then 3 mi W of Jennings, OK 74038" which regularly got successful deliveries. Mike Riley used to mail the Very Large Array radio telescope at "50 miles (80 km) West of Socorro, New Mexico, USA"</p>

<p>Sam pointed me to <a href="http://www.menomoneefallsnow.com/news/99857214.html">Menomonee Falls</a> where houses are addressed using Milwaukee County's grid system instead of house numbers - giving addresses like N88 W16541 Foobar St.</p>

<p>Andy Monat sent the following address example, from a <a href="http://ciapa.tulane.edu/uploads/1_EE_2012_Acceptance_Packet_INFORMATION-1340749206.pdf">semester abroad program at Tulane University </a>: CIAPA, 50 meters north of the Hypermas/Walmart of Curridabat, San Jose, Costa Rica. Adrien Piérard and Luke Allardyce point out street names are seldom used in Japan - instead, districts and blocks and lot numbers are used (more info on the <a href="https://en.wikipedia.org/wiki/Japanese_addressing_system">Wikipedia entry for the Japanese addressing system</a>).  A <a href="http://www.worldpress.org/Americas/592.cfm">2002 World Press Review report</a> gave this sample address: From where the Chinese restaurant used to be, two blocks down, half a block toward the lake, next door to the house where the yellow car is parked, Managua, Nicaragua. Shaun Crampton sent <a href="https://vianica.com/nicaragua/practical-info/14-addresses.html">an article with more details and examples of the Nicaraguan system</a>. Stig Brautaset pointed out <a href="http://www.bbc.co.uk/news/magazine-14806350">a BBC article about post in Kabul</a> gives this example: "Hamid Jaan, behind Darul-Aman palace". Nathan Fellman reports similar addressing is used in Nicaragua and Costa Rica.</p>

<p>Paul Puschmann and Tibor Schütz pointed out the city of <a href="http://de.wikipedia.org/wiki/Quadratestadt">Mannheim in Germany is sometimes called Quadratestadt (City of Squares)</a> as the city centre is arranged in a grid, with blocks assigned a letter (along the north-south axis) and a number (along the east-west axis) then buildings numbered by block number. So an example address at numbers 6 to 13 on block R 5 would be: Institut für Deutsche Sprache, R 5, 6-13, D-68161 Mannheim </p>

<p>Leoni Lubbinge gives an example of a South African address: Part 84, Strydfontein 306 JR, Pretoria which means the 84th plot of the farm Strydfontein 306 JR.</p></li>
</ul>

<!-- Elements being present or absent -->

<ul>
<li><p><strong>A road will have a name</strong></p>

<p>Plenty of roads like driveways, onramps and the aisles of carparks don't have names. Some roads in Japan also don't have names, as <a href="https://en.wikipedia.org/wiki/Japanese_addressing_system">the prevalent addressing system works on districts, subdistricts, blocks, lots and lot numbers</a>.</p>

<p>Peter Kenway points out in America some homes are addressed as Rural Routes, where numbers are allocated to boxes on a route covering multiple roads. For example: Box 1234, R.R. 1, Winthrop, ME 04364.</p></li>
<li><p><strong>A road will only have one name</strong></p>

<p>Many different roads, from Goswell Road in London to Regent Road in Edinburgh, make up the 410 mile <a href="https://en.wikipedia.org/wiki/A1_road_%28Great_Britain%29">A1</a>. And while there may only be one "1 Goswell Road" and only one "1 Regent Road" there are multiple buildings numbered 1 on the road designated A1.</p>

<p>Roads may also be named in multiple languages. For example, in Ireland roads may be named in both English and Irish</p></li>
<li><p><strong>Addresses will only have one street</strong></p>

<p>The Royal Mail have what they call a 'dependent street' - for example: 6 Elm Avenue, Runcorn Road, Birmingham, B12 8QX, United Kingdom (Runcorn Road is the street, Elm Avenue is the stubby 'dependent street' and isn't unique within the city. <a href="http://maps.google.co.uk/maps?q=B12+8QX">Google Maps</a> )</p>

<p>Another counterexample: Rogue Hair, 1 Hopton Parade, Streatham High Road, London, SW16 …</p></li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/">https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/</a></em></p>]]>
            </description>
            <link>https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24926417</guid>
            <pubDate>Thu, 29 Oct 2020 02:25:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I Violated a Code of Conduct]]>
            </title>
            <description>
<![CDATA[
Score 755 | Comments 514 (<a href="https://news.ycombinator.com/item?id=24926214">thread link</a>) | @tosh
<br/>
October 28, 2020 | https://www.fast.ai/2020/10/28/code-of-conduct/ | <a href="https://web.archive.org/web/*/https://www.fast.ai/2020/10/28/code-of-conduct/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>

<p><span>Written: 28 Oct 2020 by <i>Jeremy Howard</i></span></p><blockquote>
<p>Summary: NumFOCUS found I violated their Code of Conduct (CoC) at JupyterCon because my talk was not “kind”, because I said Joel Grus was “wrong” regarding his opinion that Jupyter Notebook is not a good software development environment. Joel (who I greatly respect, and consider an asset to the data science community) was not involved in NumFOCUS’s action, was not told about it, and did not support it. NumFOCUS did not follow their own enforcement procedure and violated their own CoC, left me hanging for over a week not even knowing what I was accused of, and did not give me an opportunity to provide input before concluding their investigation. I repeatedly told their committee that my emotional resilience was low at the moment due to medical issues, which they laughed about and ignored, as I tried (unsuccessfully) to hold back tears. The process has left me shattered, and I won’t be able to accept any speaking requests for the foreseeable future. I support the thoughtful enforcement of Code of Conducts to address sexist, racist, and harassing behavior, but that is not what happened in this case.</p>
</blockquote>
<h2 id="overview">Overview</h2>
<p>In my recent JupyterCon keynote, “I Like Jupyter Notebooks” (re-recording provided at the bottom of this post, if you’re interested in seeing it for yourself), I sought to offer a rebuttal to Joel Grus’ highly influential JupyterCon presentation “<a href="https://www.youtube.com/watch?v=7jiPeIFXb6U">I Don’t Like Notebooks</a>”. Joel claimed in his talk that Jupyter is a poor choice for software development and teaching, and I claimed in my talk that it is a good choice. The NumFOCUS committee found me guilty of violating their code of conduct for having not been “kind” in my disagreement with Joel, and for “insulting” him. The specific reasons given were that:</p>
<ul>
<li>I said that Joel Grus was “wrong”</li>
<li>I used some of his slides (properly attributed) and a brief clip from one of his videos to explain why I thought he was wrong</li>
<li>That I made “a negative reference” to his prior talk</li>
<li>I was also told that “as a keynote speaker” I would “be held to a higher standard than others” (although this was not communicated to me prior to my talk, nor what that higher standard is)</li>
</ul>
<p>Code of Conducts can be a useful tool, when thoughtfully created and thoughtfully enforced, to address sexism, racism, and harassment, all of which have been problems at tech conferences. Given the <a href="https://medium.com/tech-diversity-files/if-you-think-women-in-tech-is-just-a-pipeline-problem-you-haven-t-been-paying-attention-cb7a2073b996">diversity issues in the tech industry</a>, it is important that we continue the work of making conferences more inclusive, particularly to those from marginalized backgrounds. Having a code of conduct with explicit rules against violent threats, unwelcome sexual attention, repeated harassment, sexually explicit pictures, and other harmful behavior is the first step towards addressing and stopping those behaviors. The JupyterCon code provides the following examples of unacceptable behavior, none of which are at all similar to what I did (i.e. saying that someone was wrong on a technical topic, and explaining how and why):</p>
<ul>
<li>Violent threats or violent language directed against another person</li>
<li>Discriminatory jokes and language</li>
<li>Posting sexually explicit or violent material</li>
<li>Posting (or threatening to post) other people’s personally identifying information (“doxing”)</li>
<li>Personal insults, especially those using racist or sexist terms</li>
<li>Unwelcome sexual attention</li>
<li>Advocating for, or encouraging, any of the above behavior</li>
<li>Repeated harassment of others. In general, if someone asks you to stop, then stop</li>
</ul>
<p>My experience with the NumFOCUS code of conduct raises a few key issues:</p>
<ul>
<li>The CoC enforcement process involved conflicting &amp; changing information, no opportunity for me to give input, the stress of a long wait of unknown duration with no information about what I was accused of or what would happen next, and the committee members violated their own CoC during the process</li>
<li>There were two totally different Codes of Conduct with different requirements linked in different places</li>
<li>I was held to a different, undocumented and uncommunicated standard</li>
<li>The existence of, or details about, the CoC were not communicated prior to confirmation of the engagement</li>
<li>CoC experts recommend avoiding requirements of politeness or other forms of “proper” behavior, but should focus on a specific list of unacceptable behaviors. The JupyterCon CoC, however, is nearly entirely a list of “proper” behaviors (such as “Be welcoming”, “Be considerate”, and “Be friendly”) that are vaguely defined</li>
<li>CoC experts recommend using a CoC that focuses on a list of unacceptable behaviors. Both the codes linked to JupyterCon have such a link, and none of the unacceptable behavior examples are in any way related or close to what happened in this case. But NumFOCUS nonetheless found me in violation.</li>
</ul>
<p>I would rather not have to write this post at all. However I know that people will ask about why my talk isn’t available on the JupyterCon site, so I felt that I should explain exactly what happened. In particular, I was concerned that if only partial information became available, the anti-CoC crowd might jump on this as an example of problems with codes of conduct more generally, or might point at this as part of “cancel culture” (a concept I vehemently disagree with, since what is referred to as “cancellation” is often just “facing consequences”). Finally, I found that being on the “other side” of a code of conduct issue gave me additional insights into the process, and that it’s important that I should share those insights to help the community in the future.</p>
<h2 id="details">Details</h2>
<p>The rest of this post is a fairly detailed account of what happened, for those that are interested.</p>
<h3 id="my-talk-at-jupytercon">My talk at JupyterCon</h3>
<p>I recently gave a talk at <a href="https://jupytercon.com/">JupyterCon</a>. My partner Rachel gave a <a href="https://www.youtube.com/watch?v=frc7FgheUj4">talk at JupyterCon</a> a couple of years ago, and had a wonderful experience, and I’m a huge fan of Jupyter, so I wanted to support the project. The conference used to be organized by O’Reilly, who have always done a wonderful job of conferences I’ve attended, but this year the conference was instead handled by <a href="https://numfocus.org/">NumFOCUS</a>.</p>
<p>For my talk, I decided to focus on Jupyter as a literate and <a href="https://www.fast.ai/2019/12/02/nbdev/">exploratory programming environment</a>, using <a href="https://nbdev.fast.ai/">nbdev</a>. One challenge, however, is that two years earlier Joel Grus had given a brilliant presentation called <a href="https://www.youtube.com/watch?v=7jiPeIFXb6U">I Don’t Like Notebooks</a> which had been so compelling that I have found it nearly impossible to talk about programming in Jupyter without being told “you should watch this talk which explains why programming in Jupyter is a terrible idea”.</p>
<p>Joel opened and closed his presentation with some light-hearted digs at me, since I’d asked him ahead of time <em>not</em> to do such a presentation. So I thought I’d kill two birds with one stone, and take the opportunity to respond directly to him. Not only was his presentation brilliant, but his slides were hilarious, so I decided to directly parody his talk by using (with full credit of course) some of his slides directly. That way people that hadn’t seen his talk could both get to enjoy the fantastic content, and also understand just what I was responding to. For instance, here’s how Joel illustrated the challenge of running cells in the right order:</p>
<figure>
<img srcset="https://www.fast.ai/images/numfocus/joel-order.png 2w" sizes="1px" src="https://www.fast.ai/images/numfocus/joel-order.png">
</figure>
<p>I showed that slide, explaining that it’s Joel’s take on the issue, and then followed up with a slide showing how easy it actually is to run all cells in order:</p>
<figure>
<img srcset="https://www.fast.ai/images/numfocus/jeremy-order.png 2w" sizes="1px" src="https://www.fast.ai/images/numfocus/jeremy-order.png">
</figure>
<p>Every slide included a snippet from Joel’s title slide, which, I explained, showed which slides were directly taken from his presentation. I was careful to ensure I did not modify any of his slides in any way. When first introducing his presentation, I described Joel as “a brilliant communicator, really funny, and wrong”. I didn’t make any other comments about Joel (although, for the record, I think he’s awesome, and highly recommend <a href="https://www.amazon.com/Data-Science-Scratch-Principles-Python/dp/1492041130">his book</a>.</p>
<h3 id="the-code-of-conduct-violation-notice">The Code of Conduct violation notice</h3>
<p>A week later, I received an email telling me that two CoC reports were filed regarding my JupyterCon keynote presentation. I was told that “The Code of Conduct Enforcement Team is meeting tomorrow to review the incident and will be contacting you to inform you of the nature of the report and to understand your perspective”.</p>
<p>The CoC wasn’t mentioned at all until after I’d been invited to speak, had accepted, and had completed the online registration. I had reviewed it at that time, and had been a bit confused. The email I received linked to a <a href="https://jupytercon.com/codeofconduct/">JupyterCon Code of Conduct</a>, but that in turn didn’t provide much detail about what is and isn’t OK, and that in turn linked to a different <a href="https://numfocus.org/code-of-conduct">NumFOCUS Code of Conduct</a>. A link was also provided to <a href="https://numfocus.typeform.com/to/ynjGdT">report violations</a>, which also linked to and named the NumFOCUS CoC.</p>
<p>I was concerned that I had done something which might be viewed as a violation, and looked forward to hearing about the nature of the report and having a chance to share my perspective. I was heartened that JupyterCon documented that they follow the <a href="https://numfocus.org/code-of-conduct/response-and-enforcement-events-meetups">NumFOCUS Enforcement Manual</a>. I was also heartened that the manual has a section “Communicate with the Reported Person about the Incident” which says they will “Let the reported person tell someone on the CoC response team their side of the story; the person who receives their side of the story should be prepared to convey it at the response team meeting”. I was also pleased to see that much of the manual and code of conduct followed the advice (and even used some wording from) the brilliant folks at the <a href="https://adainitiative.org/">Ada Initiative</a>, who are extremely thoughtful about how to develop and apply codes of conduct.</p>
<p>One challenge is that the JupyterCon CoC is based on Django’s, which has very general guidelines such as “Be welcoming” and “Be considerate”, which can be taken by different people in different ways. The NumFOCUS code is much clearer, with a specific list of “Unacceptable behaviors”, although that list includes “Other unethical or unprofessional conduct”, which is troublesome, since “unprofessional” can be catch-all gate-keeping mechanism for whatever those in the “profession” deem to be against their particular norms, and which those outside the in-group …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.fast.ai/2020/10/28/code-of-conduct/">https://www.fast.ai/2020/10/28/code-of-conduct/</a></em></p>]]>
            </description>
            <link>https://www.fast.ai/2020/10/28/code-of-conduct/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24926214</guid>
            <pubDate>Thu, 29 Oct 2020 01:47:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How I used GPT-3 to hit Hacker News front page 5 times in 3 weeks]]>
            </title>
            <description>
<![CDATA[
Score 20 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24924035">thread link</a>) | @masterrr
<br/>
October 28, 2020 | https://vasilishynkarenka.com/gpt-3/ | <a href="https://web.archive.org/web/*/https://vasilishynkarenka.com/gpt-3/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://vasilishynkarenka.com/content/images/size/w300/2020/10/Screen-Shot-2020-10-28-at-8.24.48-AM.png 300w,
                            https://vasilishynkarenka.com/content/images/size/w600/2020/10/Screen-Shot-2020-10-28-at-8.24.48-AM.png 600w,
                            https://vasilishynkarenka.com/content/images/size/w1000/2020/10/Screen-Shot-2020-10-28-at-8.24.48-AM.png 1000w,
                            https://vasilishynkarenka.com/content/images/size/w2000/2020/10/Screen-Shot-2020-10-28-at-8.24.48-AM.png 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://vasilishynkarenka.com/content/images/size/w2000/2020/10/Screen-Shot-2020-10-28-at-8.24.48-AM.png" alt="How I used GPT-3 to hit Hacker News front page 5 times in 3 weeks">
            </figure>

            <section>
                <div>
                    <p>Posting on Hacker News is like a box of chocolates – you never know what you’re gonna get. I’ve been submitting since 2017 and never had more than one point. So I stopped trying.</p><p>A month ago, I got access to OpenAI API. After ten minutes of tinkering, I’ve got an idea: what if I could make it write good Hacker News titles for my blog posts? I quickly looked up the most favorited posts, designed the prompt in plain English, and generated my first title. It looked weird. I even doubted for a few minutes if it’s worth sharing. But I was curious – so I closed my eyes and hit submit.</p><p>The post went to the moon with <a href="https://news.ycombinator.com/item?id=24547098">229 points and 126 comments</a> in one day. Fascinated, I continued generating titles (and what would you do?). In three weeks, I got to the front page five times, received 1054 upvotes, and had 37k people come to my site.</p><p>Below is everything I’ve learned building a Hacker News post titles generator with OpenAI API, designing GPT-3 prompts, and figuring out how to apply GPT-3 to problems ranging from sales emails to SEO-optimized blog posts. At the end of the post, I cover the broader implications of GPT-3 that became obvious only after a month of working with the tool. If you’re an entrepreneur or an investor who wants to understand the change this tech will drive, you can read my speculations there.</p><p>If you have no idea what I’m talking about, read more about GPT-3 in <a href="https://www.gwern.net/newsletter/2020/05">Gwern’s post</a> first, or check <a href="https://beta.openai.com/">OpenAI’s website</a> with demo videos. In my work, I assume you’re already familiar with the API on some basic level.</p><p><em>Oct 28 update: After I published the draft, 23 people asked how to apply GPT-3 to their problems. To help them get started with the OpenAI API, I started building the first GPT-3 course that covers everything I learned – from use cases to prompt design. If you’re interested, </em><a href="mailto:vasilishynkarenka@gmail.com?subject=GPT-3%20course"><em>email me here</em></a><em>.</em></p><hr><p>After I got an idea for an HN titles app, I needed to understand how to do it with GPT-3. As there were no tutorials on approaching the problem, I went to the playground and began experimenting.</p><h2 id="generating-new-titles">Generating new titles</h2><h3 id="1-finding-the-data">1. Finding the data</h3><p>First, I wanted to see if I could make GPT-3 generate engaging post titles at all. To do that, I needed two things:</p><ol><li>Write a prompt that concisely describes the problem that I want to solve.</li><li>Feed the API some sample data to stimulate the completion.</li></ol><p>I went ahead and searched for the most upvoted HN posts of all time. In a few minutes, I’ve got <a href="https://hn.algolia.com/?query=&amp;sort=byPopularity&amp;prefix&amp;page=0&amp;dateRange=all&amp;type=story">an Algolia page</a> with a list of links. But after skimming through them, I figured out that upvotes wouldn’t work. They are mostly news and poorly reflect what kind of content the community values.</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/10/Screen-Shot-2020-10-28-at-9.18.27-AM.png" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/10/Screen-Shot-2020-10-28-at-9.18.27-AM.png 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/10/Screen-Shot-2020-10-28-at-9.18.27-AM.png 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/10/Screen-Shot-2020-10-28-at-9.18.27-AM.png 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/10/Screen-Shot-2020-10-28-at-9.18.27-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><a href="https://hn.algolia.com/?query=&amp;sort=byPopularity&amp;prefix&amp;page=0&amp;dateRange=all&amp;type=story">Most upvoted HN posts of all time.</a></figcaption></figure><p>Disappointed, I discarded upvotes as a metric. I needed something that would describe the value people get from the post. Something like... bookmarks?</p><p>I quickly looked up <a href="https://news.ycombinator.com/item?id=24351073">the most favorited HN posts</a>. The idea was simple: people don’t bookmark news. They favorite things they want to explore later.</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/10/Screen-Shot-2020-10-28-at-9.21.11-AM.png" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/10/Screen-Shot-2020-10-28-at-9.21.11-AM.png 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/10/Screen-Shot-2020-10-28-at-9.21.11-AM.png 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/10/Screen-Shot-2020-10-28-at-9.21.11-AM.png 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/10/Screen-Shot-2020-10-28-at-9.21.11-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><a href="https://observablehq.com/@tomlarkworthy/hacker-favourites-analysis">Most favorited HN posts of all time.</a></figcaption></figure><p>The next step was to grab the data from the list, insert it into the playground, and write a clear and concise prompt.</p><p><em>I actually grab data from the <a href="https://news.ycombinator.com/item?id=24351073">dang’s comment</a> rather than the original post; his list was a global one.</em></p><h3 id="2-designing-a-prompt">2. Designing a prompt</h3><p>The best way to program the API is to write a direct and straightforward task description, as you would do if you were delegating this problem to a human assistant. </p><p>Here’s how my first prompt looked like:</p><blockquote>Generate viral titles for Hacker News (<a href="https://news.ycombinator.com/">https://news.ycombinator.com/</a>) posts. The titles have to be provocative and incentivize users to click on them.</blockquote><p>Data-wise, I needed to clean up the list slightly, get rid of irrelevant stuff like IDs, and choose up to five titles to use as a sample – OpenAI team suggests that selecting three to five titles works best for text generation. If you feed the API more examples, it picks up wrong intents and generates irrelevant completions.</p><p>In a few minutes of Google Sheets work, the cleanup was done, and I had a data set of the most favorited HN post titles of all time. I put together my first prompt and clicked “Generate.”</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/10/Screen-Shot-2020-10-28-at-9.29.16-AM.png" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/10/Screen-Shot-2020-10-28-at-9.29.16-AM.png 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/10/Screen-Shot-2020-10-28-at-9.29.16-AM.png 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/10/Screen-Shot-2020-10-28-at-9.29.16-AM.png 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/10/Screen-Shot-2020-10-28-at-9.29.16-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Cleaned up titles of most favorited HN posts of all time.</figcaption></figure><h3 id="3-tinkering-with-completions">3. Tinkering with completions</h3><p>The first completion was unpromising. The list of titles had too many Ask HNs, and GPT-3 picked up questions as a pattern:</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/10/Screen-Shot-2020-10-28-at-9.40.59-AM.png" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/10/Screen-Shot-2020-10-28-at-9.40.59-AM.png 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/10/Screen-Shot-2020-10-28-at-9.40.59-AM.png 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/10/Screen-Shot-2020-10-28-at-9.40.59-AM.png 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/10/Screen-Shot-2020-10-28-at-9.40.59-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>My first attempt to generate HN titles.</figcaption></figure><p>To fix that, I cut out half Ask HNs from the dataset and began tinkering.</p><p>If there was one thing I could tell someone about GPT-3, it’d be that getting a perfect completion from the first try is a dream. The best stuff comes after dozens of experiments, sometimes after generating completions with the same parameters many times with the Best Of parameter and writing another GPT-3 classifier to discover a good one. Moreover, you need to test the prompt’s quality, data samples, and temperature (“creativity” of responses) individually to understand what you need to improve. </p><p><em>If you’re looking for prompt design tips, head on to chapter 2 of the post.</em></p><p>Here’s a list of experiments I’ve done:</p><ul><li>Edited the prompt many times, including and excluding adjectives from the task description. I tried “catchy,” “provocative,” “thought-provoking,” and many others. The best configuration I’ve got was “Write a short, thought-provoking, and eye-catching post title for a Hacker News (https://news.ycombinator.com/) submission.”</li><li>Split the prompt into two sentences, separating the task and its description. Discovered that one-sentence long prompts work best for simple tasks.</li><li>Played with data samples. I added and removed Ask HNs, randomly sampled from the list of most favorited posts, and tried picking more subjectively thoughtful titles.</li><li>Changed the temperature. The best results came at .9, while anything less than .7 was repetitive and very similar to the samples. Titles generated with temperature 1 were too random and didn’t look like good HN titles at all.</li></ul><p>To judge the quality of completions, I’ve come up with a question: “If I saw this on HN, would I click on it?” This helped me move quickly through experiments because I knew what bad results looked like.</p><p>After half an hour of tinkering, I’ve got the following completion: &nbsp; &nbsp; &nbsp;</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/10/Screen-Shot-2020-10-28-at-9.54.17-AM.png" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/10/Screen-Shot-2020-10-28-at-9.54.17-AM.png 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/10/Screen-Shot-2020-10-28-at-9.54.17-AM.png 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/10/Screen-Shot-2020-10-28-at-9.54.17-AM.png 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/10/Screen-Shot-2020-10-28-at-9.54.17-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>My first good GPT-3 completions for HN titles.</figcaption></figure><p>That’s when I realized that I’m onto something. From the list of generated titles above, I’d click on at least three links just out of curiosity. Especially on “A developer’s guide to getting in shape.”</p><p>What’s even more interesting, the completion above was a result of fine-tuning the API. Title8, What You Love Is Not Your Life’s Work, was originally a part of another completion that was lame. So I cut out the bad stuff, added Title8 to my data sample, and continued generating from there.</p><p>The next step was to see if I could make GPT-3 create a custom title for my blog post.</p><h2 id="generating-custom-titles">Generating custom titles</h2><h3 id="1-changing-the-method">1. Changing the method</h3><p>To make GPT-3 generate custom titles, I needed to change my approach. I was no longer exploring new, potentially interesting headlines but figuring out how to make a good one for a post that was already written. To do that, I couldn’t just tell the API, “hey, generate me a good one.” I needed to show what a good one actually is and give GPT-3 some idea of what the post is about.</p><p>The first thing I changed is the prompt. This was relatively easy because I applied the same model of thinking again – “What would I tell a human assistant if I had to delegate this problem?”</p><p>Here’s a prompt that I used:</p><blockquote>Write a short, thought-provoking, and eye-catching post title for Hacker News (https://news.ycombinator.com/) submission based on a blog post’s provided description.</blockquote><p>One unexpected benefit from tinkering with the prompt is the clarity of thought. When you’re dealing with GPT-3, there’s no way to make it work if you don’t know what you’re doing. You need to express your thought in clear terms. It’s a brilliant natural constraint and one of the most underrated benefits of GPT-3: when you must state clearly what you want to do, you begin to better understand the problem.</p><h3 id="2-updating-the-data">2. Updating the data</h3><p>After I designed a new prompt, I had to update my data set. I went back to my previous experiment, selected five titles from the list of most favorited posts of all time, and wrote a clear and concise description for each of them. Lastly, I wrote a description of my own work that I wanted to generate the title for. I left the title field blank and hit “Generate.”</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/10/Screen-Shot-2020-10-28-at-10.35.59-AM.png" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/10/Screen-Shot-2020-10-28-at-10.35.59-AM.png 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/10/Screen-Shot-2020-10-28-at-10.35.59-AM.png 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/10/Screen-Shot-2020-10-28-at-10.35.59-AM.png 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/10/Screen-Shot-2020-10-28-at-10.35.59-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Updated data set for generating custom HN titles.</figcaption></figure><h3 id="3-experimenting-with-titles">3. Experimenting with titles</h3><p>This time, the completions were shockingly good. Because I’ve already learned how to design a prompt and get the data right, I didn’t have to spend much time tuning the parameters. The only thing I changed was the temperature – the best results came at .92.</p><p>Here’s a list of titles I generated for my post on <a href="https://vasilishynkarenka.com/3-weeks-of-shaolin-kung-fu-in-china/">studying kung fu in China</a>:</p><ul><li>How a Shaolin monk trains Western students</li><li>I went to a Kung Fu school for a month, and I learned 3 lessons that I will never forget</li><li>How I got a Shaolin monk to teach me Kung Fu in China</li><li>What it’s like to study Kung Fu from a Shaolin monk</li><li>21 days in the Shaolin Temple</li><li>An edited transcript of my conversation with my Kung Fu master</li><li>How I learned Kung Fu from a monk in China: The full story</li><li>I Spent a Month with a Kung Fu Master in China</li><li>The Shaolin warrior in the cubicle</li><li>Kamikaze training – A brief study in Kung Fu</li><li>Why I traveled to China to study Kung Fu</li><li>I went to a Shaolin Monastery for 6 months to learn Kung Fu: a true story</li><li>I went to the Shaolin Temple to study Kung Fu, and all I got was this lousy blog post </li><li>The monk and the engineer</li><li>What it’s like studying Kung Fu from Shaolin monks for 6 months</li><li>I went to Shaolin and all I got were Kung Fu lessons</li></ul><p>And, of course, my all-time favorite one:</p><blockquote>When a Shaolin monk met a programmer from the Silicon Valley</blockquote><p>But the best titles didn’t come from completions. The most interesting headlines were the ones I ended up thinking of <em>after</em> I saw the …</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://vasilishynkarenka.com/gpt-3/">https://vasilishynkarenka.com/gpt-3/</a></em></p>]]>
            </description>
            <link>https://vasilishynkarenka.com/gpt-3/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24924035</guid>
            <pubDate>Wed, 28 Oct 2020 20:55:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building a Functional Email Server]]>
            </title>
            <description>
<![CDATA[
Score 22 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24923242">thread link</a>) | @panic
<br/>
October 28, 2020 | https://signalsandthreads.com/building-a-functional-email-server/ | <a href="https://web.archive.org/web/*/https://signalsandthreads.com/building-a-functional-email-server/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<h2 id="000004">00:00:04</h2>

<div><p>Welcome to Signals and Threads, in-depth conversations about every layer of the tech stack from Jane Street. I’m Ron Minsky. </p><p> All right, so, it’s my pleasure today to sit down and have a conversation with Dominick LoBraico about email. In particular, we’re going to talk about a system that Dominick architected and led the development of called Mailcore, which is Jane Street’s own homegrown mail server. </p><p> And I think this is interesting on its own because email is an interesting topic and the whole architecture behind it, but I think it’s also a lens into some interesting questions about software design and how you manage infrastructure, some questions about how you make this choice of when you build your own thing and when you use standard, existing tools, and also some interesting questions about how programming language plays a role in systems design.</p></div>

<h2 id="000048">00:00:48</h2>

<p>Hi, Ron.</p>

<h2 id="000049">00:00:49</h2>

<p>Hey, DLo. So, to get started, can you tell us a little bit about how email works?</p>

<h2 id="000054">00:00:54</h2>

<div><p>Sure. Yeah. So email is based on an old and venerable protocol on the Internet called the Simple Mail Transfer Protocol, SMTP, and SMTP… you can kind of think of it as playing the role that the Postal Service plays in delivering regular mail. It is a way for one server that wants to deliver a message somewhere, to hand that message off to another party, who can get it to its final destination, whether that is the eventual destination server itself or some intermediary who can help you get a little bit closer.
</p><p>Email itself came into fruition, as we know today, in the early days of the Internet, and the protocol itself is very simple. You basically have the actual body of the message itself, which has its own separate format and specification, and then you have a set of instructions for expressing who that message is destined for and who it’s coming from, and so one server connects to another.
</p><p>And it says, “I’ve got a message. It’s coming from so and so, and it’s meant to be delivered to some other person. Here’s the body of the message,” and the receiving server can do with that what it will. It can either say, “Great, I’ll take that, and I’ll be responsible for it from here on out.” It can say, “No, I don’t know anything about that person. You have to find somebody else to deliver that to” or reject it for any number of other reasons, like, “This looks like it has a virus,” or “You’re not allowed to connect to me,” or “I’m not available for receiving mail right now.”</p></div>

<h2 id="000217">00:02:17</h2>

<p>And one thing that always strikes me about email is it’s this kind of wondrous artifact from the early Internet, which is a <em>truly</em> open social network. There’s lots of things that people talk about, right? Could we make existing social networks better and more open and all of that, and email, just <em>is</em> from its initial design; and its complete history has been this very open thing, and as you point out, the core protocols and transports are relatively simple, although there is actually a surprising amount of complexity in the RFCs that tell you how to parse a particular email. The overall system is pretty simple, but there’s a lot of complexity in all of the different players who build systems that actually manage and transfer email around and how they deal with the various problems that happen, like spam and people attacking systems via email and all of that. So the foundations are relatively simple, but the emergent complexity of the system is actually pretty high.</p>

<h2 id="000310">00:03:10</h2>

<p>Like with many protocols of the old Internet, it was designed in a time where the world was much simpler than it is today, especially the Internet-connected world. You know, there were probably 50 institutions that had Internet connections or ARPANET connections at the time, and you didn’t really have to worry that anybody was going to be spamming because barely anybody even know what email was in the first place.</p>

<h2 id="000330">00:03:30</h2>

<div><p>When you start and build a new thing, the early properties of the thing that you build can often be really sticky and really matter in a way that’s kind of hard to predict. So this one early property of being open has stayed there. Email is a thing that anyone can participate in. Organizations can build their own infrastructure to connect to it, and through all the rather large transformations that the email system has gone through, that openness remains as a core property.
</p><p>This is the horrible thing about designing to build a new thing,  when you want to design something new, you have to make a bunch of choices, and clearly, you shouldn’t worry about them that much, because probably the thing you build is going to fail and isn’t going to work out, and even if it does, you’re going to learn more about the problem later, and so you shouldn’t worry too much about the early decisions. But also, some of the early decisions, you don’t know which ones are going to turn out to be very hard to change.</p></div>

<h2 id="000413">00:04:13</h2>

<p>That’s right.</p>

<h2 id="000414">00:04:14</h2>

<p>And you’ll be stuck with them until the end of time.</p>

<h2 id="000416">00:04:16</h2>

<p>And in fact, you know, the big players in email today – obviously Google and Gmail, are a really large percentage of the email sending and receiving on the Internet – but they’re still wrestling with some of those early decisions and some of that openness that are architected in, as they try to figure out how they can make email more secure and how they can protect their users and rein in some of the malicious actors on the Internet, and that’s just a hard thing to do while trying to maintain the existing openness that email has; it cuts both ways I guess.</p>

<h2 id="000445">00:04:45</h2>

<p>That openness, in the end, has a lot of value.</p>

<h2 id="000446">00:04:46</h2>

<p>Absolutely. Yeah.</p>

<h2 id="000448">00:04:48</h2>

<p>So the story here is about how you ended up building the system called Mailcore. What did email at Jane Street look like when you first ran into the problem?</p>

<h2 id="000456">00:04:56</h2>

<div><p>So you might think that there’s really not much special about the way Jane Street uses email compared to any other company, and largely, that’s true. I think we have a few special requirements by dint of the fact that we are in a regulated industry, so we have some requirements around logging for compliance purposes every message that is sent or received by somebody at Jane Street.
</p><p>But other than that, our email system looks pretty similar, or has looked, in the past, pretty similar, to the way an email system in any organization might look, and the rough summary is we have some mail gateways that sit on the outside of our network for receiving email from foreign servers, you know, from external parties, and then we have some mail server, or a set of servers, inside of our network that handle all of the complicated business logic around what to do with those messages.
</p><p>So, in some cases, it’s as simple as receive the message and deliver it into the mailbox of the user if we are the intended recipient. In other cases, it is apply filtering for things like spam and viruses and other things that we might want to extract from messages before we deliver them, do expansion for mailing lists. So if you send an email to some group at Jane Street, you want to be able to expand that group name to the actual list of recipient mailboxes to make sure that it actually ends up in the inboxes of the recipients who it’s destined for. And then this extra compliance implication of making sure that we’re logging all of the right messages with all of the right metadata. </p><p>And at the time that I started, the mail infrastructure here was all based on an open source mail server that has its own config language and is pretty widely used on the Internet at large, and we had about 400 or 500 lines of configuration in the most complex case, I think, for this system to get it to do all of these different things that we wanted it to be able to do.</p></div>

<h2 id="000644">00:06:44</h2>

<p>Great. So that sounds like a reasonable approach in terms of how to build oneself a mail system. What problems did we run into with it?</p>

<h2 id="000650">00:06:50</h2>

<div><p>Yeah, so the biggest problem here, at the end of the day, was the complexity required for configuring this system to do all of the things that we needed it to do. So, now, I said 400 or 500 lines of configuration – that probably doesn’t sound like a huge number, but when it’s in a kind of bespoke configuration language that’s unlike the configuration of any other system and unlike any programming language that a developer or engineer at Jane Street would be familiar with, the complexity of 400 or 500 lines in a foreign language is pretty large and can be a little bit imposing to deal with.
</p><p>In particular, we had some scary near-misses where we realized that we had done the wrong thing in terms of archiving some email for compliance purposes that we were supposed to archive, and luckily, in each of those cases, there were mitigating factors such that it didn’t end up being a big deal, but that near-miss gave us a little bit of a scare because we went and looked at the configuration and wanted to understand how we had gotten ourselves into this position, and it was harder than it felt like it should be to understand what had gone wrong and how to fix it.</p></div>

<h2 id="000750">00:07:50</h2>

<p>It’s maybe also worth mentioning that the problem of logging all of your messages for compliance purposes may sound easy, but it’s made more complicated by the fact that Jane Street is a company that operates in lots of different regulatory regimes and has actually different rules for some of the different places it operates. So even the sort of seemingly simple, “Let’s just write everything down” is more complicated than it might appear at first.</p>

<h2 id="000811">00:08:11</h2>

<p>That’s right, yeah. We have different requirements in terms of what has to be written down and what kinds of metadata we need to store and where the extra copies need to be physically located around the world and things like that, which are reasonable sounding when you think about the human aspects of it, you know, when you reason about, okay, yeah, you need a copy for this and a …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://signalsandthreads.com/building-a-functional-email-server/">https://signalsandthreads.com/building-a-functional-email-server/</a></em></p>]]>
            </description>
            <link>https://signalsandthreads.com/building-a-functional-email-server/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24923242</guid>
            <pubDate>Wed, 28 Oct 2020 19:40:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Learnings from Running a Longevity Startup]]>
            </title>
            <description>
<![CDATA[
Score 36 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24923116">thread link</a>) | @sarthakjshetty
<br/>
October 28, 2020 | https://www.celinehh.com/year-1-learnings | <a href="https://web.archive.org/web/*/https://www.celinehh.com/year-1-learnings">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main" role="main">

      

        

        <div data-content-field="main-content">
          <div data-type="page" data-updated-on="1603911174701" id="page-5f98dba740e38c75e7b2c208"><div><div><div data-block-type="2" id="block-5f98dba740e38c75e7b2c209"><div><p>Oct 27, 2020</p><p>I incorporated Celevity in October 2019. Since then, we’ve gone from a deck and a provisional patent to a team of 12+, a lead drug, and a head brimming with new knowledge and experiences. These are some of my biggest learnings in the last year. </p><h3>Small decisions compound exponentially, especially around team &amp; culture</h3><p>One of the most important things I ever did for our culture was a mistake. The week before a key proof-of-concept study, I decided to completely rework the study design and re-question all of our assumptions. This delayed our timelines and made most of our previous work moot. It also saved the study. </p><p>That was a show not tell moment with our values - it showed the team that truth seeking is the most important thing, and no one has to be scared to tell me if they want to change up something last minute (because I did it myself!). This paved the way to many other close saves. </p><h3>For key relationships, make sure motivations are aligned</h3><p>From investors and advisors to contractors and employees, taking time to understand to fully empathize with their priorities and worries, and not automatically assuming that their motivations are the same as yours, was a hard lesson to embody. As a stereotypical bombastic founder, it was hard for me to comprehend someone <em>not </em>being mission-motivated and not wanting this in the world as badly as I do.</p><p>In hiring, I’ve found that even the most talented person will not perform well if they don’t deeply and truly care about your mission (at least at this stage of the company). In professional relationships, it’s unlikely their motivation is the same as yours, but often motivations can be complimentary. Everything is so much easier when they are. </p><h3>Most catchy startup advice has layers of nuance and conditionals that you wont realize until you’re in the thick of it</h3><p>I hit this one a lot this year, a likely consequence from listening to a few too many startup podcasts. A good example is ‘hire fast, fire fast’. Generally good advice, in my opinion. But no one tells you how difficult it is to execute on that advice, the accusations you’ll face from the person or others, the emotional toll of making a decision that affects someone’s wellbeing, and the ripple effects a firing, even if necessary, can have on the team and how to deal with that. Correct advice? Totally. But far from the full picture. One of my biggest learnings this year was becoming aware of and starting to learn how to predict the nuance and conditionals around things that seem simple from the outside. </p><h3>The line between optimism and naïveté is a wide as a hair </h3><p>To do novel hard things requires optimism that you have some key insight or ability that means you can achieve what others haven’t. You need to be optimistic to trudge through the sludge and to recruit others to your crazy idea long before it’s proven. Naive optimism is so important for building a company and telling a story. But you also need to be paranoid and see around corners. </p><p>Balancing these is difficult - too optimistic and you’ll fall into a trap, too paranoid and you’ll never make a decision. Over the last year, I’ve gained a healthy respect for the challenges ahead of us, moving my natural tendency of optimism a few inches closer to paranoia. I think this will serve us well on this next stage of our journey. </p><p>For me, the hardest part of this was realizing that the optimal balance between optimism and realism differs depending on the situation; what works when facing investors doesn’t work when recruiting, and these two situations differ significantly to the balance you need to hold internally. </p></div></div></div></div></div>
        </div>
      
    </div></div>]]>
            </description>
            <link>https://www.celinehh.com/year-1-learnings</link>
            <guid isPermaLink="false">hacker-news-small-sites-24923116</guid>
            <pubDate>Wed, 28 Oct 2020 19:31:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Porsche Classifier – Identify Porsche models with 95% accuracy]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24922472">thread link</a>) | @archa
<br/>
October 28, 2020 | https://www.rkpblog.tech/2020/04/porsche-classifier/ | <a href="https://web.archive.org/web/*/https://www.rkpblog.tech/2020/04/porsche-classifier/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				

<h2 id="porsche-classifier-https-porsche-classifier-azurewebsites-net"><a href="https://porsche-classifier.azurewebsites.net/">Porsche Classifier</a></h2>



<p><img src="https://d33wubrfki0l68.cloudfront.net/675336d96deabd0007621a71180bcfb57edb6b11/805fd/images/2020-04-04-porsche_classifier/718.jpg" alt="porsche-718"></p>

<p>Trained using <a href="https://fast.ai/">fastai-v3</a>, <a href="https://pytorch.org/">pytorch</a> and <a href="https://gradient.paperspace.com/">Gradient</a>.
Uses <a href="https://www.mathworks.com/help/deeplearning/ref/resnet50.html">resnet50</a> and trained on a <a href="https://images.nvidia.com/content/pdf/quadro/data-sheets/192195-DS-NV-Quadro-P5000-US-12Sept-NV-FNL-WEB.pdf">Nvidia Quadro P5000</a>.
Built on <a href="https://www.docker.com/">docker</a> and is hosted on <a href="https://azure.microsoft.com/en-in/services/app-service/web/">Microsoft Azure Web Services</a>.
Trained on a dataset of publicly sourced images containing 30000 Porsche car models of varying degree of quality.
Porsche cars, specially the latest generations of the Panamera/Taycan, Macan/Cayenne &amp; 911 / 718 can be pretty tricky to tell apart for a layman who isn’t paying very close attention, which is why I wanted to test out what kind of features this deep learning model would pick up.</p>

<h3 id="creating-your-own-dataset-from-google-images">Creating your own dataset from Google Images</h3>

<p>In this tutorial we will see how to easily create an image dataset through Google Images. Note: You will have to repeat these steps for any new category you want to Google (e.g once for dogs and once for cats).</p>
<div><pre>```python
from fastai.vision import *
```</pre></div>
<h4 id="get-a-list-of-urls">Get a list of URLs</h4>

<h5 id="search-and-scroll">Search and scroll</h5>

<p>Go to Google Images and search for the images you are interested in. The more specific you are in your Google Search, the better the results and the less manual pruning you will have to do.</p>

<p>Scroll down until you’ve seen all the images you want to download, or until you see a button that says ‘Show more results’. All the images you scrolled past are now available to download. To get more, click on the button, and continue scrolling. The maximum number of images Google Images shows is 700.</p>

<p>It is a good idea to put things you want to exclude into the search query, for instance if you are searching for the Eurasian wolf, “canis lupus lupus”, it might be a good idea to exclude other variants:</p>

<p>“canis lupus lupus” -dog -arctos -familiaris -baileyi -occidentalis</p>

<p>You can also limit your results to show only photos by clicking on Tools and selecting Photos from the Type dropdown.</p>

<h3 id="download-into-file">Download into file</h3>

<p>Now you must run some Javascript code in your browser which will save the URLs of all the images you want for you dataset.</p>

<p>In Google Chrome press CtrlShiftj on Windows/Linux and CmdOptj on macOS, and a small window the javascript ‘Console’ will appear. In Firefox press CtrlShiftk on Windows/Linux or CmdOptk on macOS. That is where you will paste the JavaScript commands.</p>

<p>You will need to get the urls of each of the images. Before running the following commands, you may want to disable ad blocking extensions (uBlock, AdBlockPlus etc.) in Chrome. Otherwise the window.open() command doesn’t work. Then you can run the following commands:</p>
<div><pre><code data-lang="python">urls<span>=</span>Array<span>.</span>from(document<span>.</span>querySelectorAll(<span>'.rg_i'</span>))<span>.</span><span>map</span>(el<span>=&gt;</span> el<span>.</span>hasAttribute(<span>'data-src'</span>)?el<span>.</span>getAttribute(<span>'data-src'</span>):el<span>.</span>getAttribute(<span>'data-iurl'</span>));
window<span>.</span><span>open</span>(<span>'data:text/csv;charset=utf-8,'</span> <span>+</span> escape(urls<span>.</span>join(<span>'</span><span>\n</span><span>'</span>)));</code></pre></div>
<h3 id="create-directory-and-upload-urls-file-into-your-server">Create directory and upload urls file into your server</h3>

<p>Choose an appropriate name for your labeled images. You can run these steps multiple times to create different labels.</p>
<div><pre><code data-lang="python">    In [<span>0</span>]:
    folder <span>=</span> <span>'718'</span>
    <span>file</span> <span>=</span> <span>'718.csv'</span>
    In [<span>0</span>]:
    folder <span>=</span> <span>'911'</span>
    <span>file</span> <span>=</span> <span>'911.csv'</span>
    In [<span>0</span>]:
    folder <span>=</span> <span>'cayenne'</span>
    <span>file</span> <span>=</span> <span>'cayenne.csv'</span>
    In [<span>0</span>]:
    folder <span>=</span> <span>'macan'</span>
    <span>file</span> <span>=</span> <span>'macan.csv'</span>
    In [<span>0</span>]:
    folder <span>=</span> <span>'taycan'</span>
    <span>file</span> <span>=</span> <span>'taycan.csv'</span>
    In [<span>0</span>]:
    folder <span>=</span> <span>'panamera'</span>
    <span>file</span> <span>=</span> <span>'panamera.csv'</span></code></pre></div>
<p>You will need to run this cell once per each category.</p>
<div><pre><code data-lang="python">    In [<span>0</span>]:
    path <span>=</span> Path(<span>'data/porsche'</span>)
    dest <span>=</span> path<span>/</span>folder
    dest<span>.</span>mkdir(parents<span>=</span><span>True</span>, exist_ok<span>=</span><span>True</span>)
    In [<span>0</span>]:
    path<span>.</span>ls()
    Out[<span>0</span>]:
    [PosixPath(<span>'data/porsche/cayenne'</span>),
     PosixPath(<span>'data/porsche/panamera.csv'</span>),
     PosixPath(<span>'data/porsche/cayenne.csv'</span>),
     PosixPath(<span>'data/porsche/panamera'</span>),
     PosixPath(<span>'data/porsche/taycan'</span>),
     PosixPath(<span>'data/porsche/911.csv'</span>),
     PosixPath(<span>'data/porsche/taycan.csv'</span>),
     PosixPath(<span>'data/porsche/911'</span>),
     PosixPath(<span>'data/porsche/macan.csv'</span>),
     PosixPath(<span>'data/porsche/718'</span>),
     PosixPath(<span>'data/porsche/718.csv'</span>),
     PosixPath(<span>'data/porsche/macan'</span>)]</code></pre></div>
<p>Finally, upload your urls file. You just need to press ‘Upload’ in your working directory and select your file, then click ‘Upload’ for each of the displayed files.</p>

<p><code>uploaded file</code></p>

<h3 id="download-images">Download images</h3>

<p>Now you will need to download your images from their respective urls.</p>

<p>fast.ai has a function that allows you to do just that. You just have to specify the urls filename as well as the destination folder and this function will download and save all images that can be opened. If they have some problem in being opened, they will not be saved.</p>

<p>Let’s download our images! Notice you can choose a maximum number of images to be downloaded. In this case we will not download all the urls.</p>

<p>You will need to run this line once for every category.</p>
<div><pre><code data-lang="python">In [<span>0</span>]:
classes <span>=</span> [<span>'taycan'</span>,<span>'panamera'</span>,<span>'macan'</span>,<span>'cayenne'</span>,<span>'718'</span>,<span>'911'</span>]
In [<span>0</span>]:
download_images(path<span>/</span><span>file</span>, dest, max_pics<span>=</span><span>500</span>)

    In [<span>0</span>]:
    <span># If you have problems download, try with `max_workers=0` to see exceptions:</span>
    download_images(path<span>/</span><span>file</span>, dest, max_pics<span>=</span><span>20</span>, max_workers<span>=</span><span>0</span>)
    <span>``</span>`

Then we can remove <span>any</span> images that can<span>'t be opened:</span>
<span>`python In [0]: for c in classes: print(c) verify_images(path/c, delete=True, max_size=500) taycan panamera macan cayenne 718 911`</span>

<span>### View data</span>

<span>    ```python</span>
<span>    In [0]:</span>
<span>    np.random.seed(42)</span>
<span>    data = ImageDataBunch.from_folder(path, train=".", valid_pct=0.2,</span>
<span>            ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)</span>

<span>    In [0]:</span>
<span>    #If you already cleaned your data, run this cell instead of the one before</span>
<span>     np.random.seed(42)</span>
<span>     data = ImageDataBunch.from_csv(path, folder=".", valid_pct=0.2, csv_labels='</span>cleaned<span>.</span>csv<span>',</span>
<span>             ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)</span>
<span>    ```</span>

<span>Good! Let'</span>s take a look at some of our pictures then<span>.</span>
<span>``</span>`python
In [<span>0</span>]:
data<span>.</span>classes
Out[<span>0</span>]:
[<span>'718'</span>, <span>'911'</span>, <span>'cayenne'</span>, <span>'macan'</span>, <span>'panamera'</span>, <span>'taycan'</span>]
In [<span>0</span>]:
data<span>.</span>show_batch(rows<span>=</span><span>6</span>, figsize<span>=</span>(<span>7</span>,<span>8</span>))
In [<span>0</span>]:
data<span>.</span>classes, data<span>.</span>c, <span>len</span>(data<span>.</span>train_ds), <span>len</span>(data<span>.</span>valid_ds)
Out[<span>0</span>]:
([<span>'718'</span>, <span>'911'</span>, <span>'cayenne'</span>, <span>'macan'</span>, <span>'panamera'</span>, <span>'taycan'</span>], <span>6</span>, <span>1920</span>, <span>480</span>)</code></pre></div>
<h3 id="train-model">Train model</h3>
<div><pre>```python
In [0]:
learn = cnn_learner(data, models.resnet50, metrics=error_rate)
In [0]:
learn.fit_one_cycle(40)
```</pre></div>
<table>
<thead>
<tr>
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>error_rate</th>
<th>time</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>2.608914</td>
<td>1.610778</td>
<td>0.606250</td>
<td>00:09</td>
</tr>

<tr>
<td>1</td>
<td>2.229989</td>
<td>1.467350</td>
<td>0.531250</td>
<td>00:09</td>
</tr>

<tr>
<td>2</td>
<td>1.992984</td>
<td>1.457949</td>
<td>0.495833</td>
<td>00:10</td>
</tr>

<tr>
<td>3</td>
<td>1.800289</td>
<td>1.435493</td>
<td>0.481250</td>
<td>00:09</td>
</tr>

<tr>
<td>4</td>
<td>1.639752</td>
<td>1.454458</td>
<td>0.479167</td>
<td>00:09</td>
</tr>

<tr>
<td>5</td>
<td>1.502409</td>
<td>1.400133</td>
<td>0.464583</td>
<td>00:09</td>
</tr>

<tr>
<td>6</td>
<td>1.371669</td>
<td>1.300878</td>
<td>0.450000</td>
<td>00:10</td>
</tr>

<tr>
<td>7</td>
<td>1.258696</td>
<td>1.236995</td>
<td>0.418750</td>
<td>00:09</td>
</tr>

<tr>
<td>8</td>
<td>1.138708</td>
<td>1.241532</td>
<td>0.418750</td>
<td>00:09</td>
</tr>

<tr>
<td>9</td>
<td>1.043424</td>
<td>1.175137</td>
<td>0.406250</td>
<td>00:10</td>
</tr>

<tr>
<td>10</td>
<td>0.984921</td>
<td>1.146469</td>
<td>0.385417</td>
<td>00:10</td>
</tr>

<tr>
<td>11</td>
<td>0.927935</td>
<td>1.169490</td>
<td>0.379167</td>
<td>00:10</td>
</tr>

<tr>
<td>12</td>
<td>0.876125</td>
<td>1.170498</td>
<td>0.391667</td>
<td>00:10</td>
</tr>

<tr>
<td>13</td>
<td>0.825623</td>
<td>1.195051</td>
<td>0.375000</td>
<td>00:10</td>
</tr>

<tr>
<td>14</td>
<td>0.765536</td>
<td>1.155461</td>
<td>0.364583</td>
<td>00:10</td>
</tr>

<tr>
<td>15</td>
<td>0.727942</td>
<td>1.145015</td>
<td>0.381250</td>
<td>00:10</td>
</tr>

<tr>
<td>16</td>
<td>0.688683</td>
<td>1.260339</td>
<td>0.387500</td>
<td>00:10</td>
</tr>

<tr>
<td>17</td>
<td>0.637377</td>
<td>1.175742</td>
<td>0.366667</td>
<td>00:10</td>
</tr>

<tr>
<td>18</td>
<td>0.608817</td>
<td>1.228916</td>
<td>0.385417</td>
<td>00:10</td>
</tr>

<tr>
<td>19</td>
<td>0.572675</td>
<td>1.248361</td>
<td>0.379167</td>
<td>00:09</td>
</tr>

<tr>
<td>20</td>
<td>0.555676</td>
<td>1.256141</td>
<td>0.364583</td>
<td>00:10</td>
</tr>

<tr>
<td>21</td>
<td>0.511557</td>
<td>1.273524</td>
<td>0.375000</td>
<td>00:10</td>
</tr>

<tr>
<td>22</td>
<td>0.483267</td>
<td>1.251337</td>
<td>0.362500</td>
<td>00:10</td>
</tr>

<tr>
<td>23</td>
<td>0.436271</td>
<td>1.288411</td>
<td>0.354167</td>
<td>00:10</td>
</tr>

<tr>
<td>24</td>
<td>0.415738</td>
<td>1.234846</td>
<td>0.364583</td>
<td>00:10</td>
</tr>

<tr>
<td>25</td>
<td>0.397631</td>
<td>1.279648</td>
<td>0.354167</td>
<td>00:10</td>
</tr>

<tr>
<td>26</td>
<td>0.377773</td>
<td>1.224547</td>
<td>0.347917</td>
<td>00:10</td>
</tr>

<tr>
<td>27</td>
<td>0.352112</td>
<td>1.226564</td>
<td>0.339583</td>
<td>00:09</td>
</tr>

<tr>
<td>28</td>
<td>0.338672</td>
<td>1.195467</td>
<td>0.341667</td>
<td>00:10</td>
</tr>

<tr>
<td>29</td>
<td>0.328226</td>
<td>1.212193</td>
<td>0.347917</td>
<td>00:10</td>
</tr>

<tr>
<td>30</td>
<td>0.296725</td>
<td>1.213175</td>
<td>0.339583</td>
<td>00:10</td>
</tr>

<tr>
<td>31</td>
<td>0.290399</td>
<td>1.222019</td>
<td>0.327083</td>
<td>00:10</td>
</tr>

<tr>
<td>32</td>
<td>0.263940</td>
<td>1.222777</td>
<td>0.327083</td>
<td>00:10</td>
</tr>

<tr>
<td>33</td>
<td>0.258763</td>
<td>1.207108</td>
<td>0.322917</td>
<td>00:10</td>
</tr>

<tr>
<td>34</td>
<td>0.253563</td>
<td>1.217487</td>
<td>0.329167</td>
<td>00:10</td>
</tr>

<tr>
<td>35</td>
<td>0.236343</td>
<td>1.217709</td>
<td>0.322917</td>
<td>00:10</td>
</tr>

<tr>
<td>36</td>
<td>0.221070</td>
<td>1.228969</td>
<td>0.325000</td>
<td>00:10</td>
</tr>

<tr>
<td>37</td>
<td>0.230405</td>
<td>1.240643</td>
<td>0.327083</td>
<td>00:10</td>
</tr>

<tr>
<td>38</td>
<td>0.217511</td>
<td>1.230480</td>
<td>0.318750</td>
<td>00:10</td>
</tr>

<tr>
<td>39</td>
<td>0.209594</td>
<td>1.243002</td>
<td>0.318750</td>
<td>00:10</td>
</tr>
</tbody>
</table>
<div><pre><code data-lang="python">    In [<span>0</span>]:
    learn<span>.</span>save(<span>'stage-1'</span>)
    In [<span>0</span>]:
    learn<span>.</span>unfreeze()
    In [<span>0</span>]:
    learn<span>.</span>lr_find()
    <span>25.00</span><span>%</span> [<span>1</span><span>/</span><span>4</span> <span>00</span>:<span>12</span><span>&lt;</span><span>00</span>:<span>37</span>]

<span>``</span>`
epoch <span>|</span> train_loss <span>|</span> valid_loss <span>|</span> error_rate <span>|</span> time
<span>-----</span> <span>|</span> <span>----------</span> <span>|</span> <span>----------</span> <span>|</span> <span>----------</span> <span>|</span> <span>----</span>
<span>0</span> <span>|</span> <span>0.196860</span> <span>|</span> <span>#na# | 00:12</span>

<span>83.33</span><span>%</span> [<span>25</span><span>/</span><span>30</span> <span>00</span>:<span>10</span><span>&lt;</span><span>00</span>:<span>02</span> <span>0.6416</span>]
LR Finder <span>is</span> complete, <span>type</span> {learner_name}<span>.</span>recorder<span>.</span>plot() to see the graph<span>.</span>
<span>``</span>`

In [<span>0</span>]:

<span># If the plot is not showing try to give a start and end learning rate</span>

<span># learn.lr_find(start_lr=1e-5, end_lr=1e-1)</span>

learn<span>.</span>recorder<span>.</span>plot()

In [<span>0</span>]:
learn<span>.</span>fit_one_cycle(<span>2</span>, max_lr<span>=</span><span>slice</span>(<span>3e-5</span>,<span>3e-4</span>))
epoch train_loss valid_loss error_rate time
<span>0</span> <span>0.280386</span> <span>1.524914</span> <span>0.372917</span> <span>00</span>:<span>13</span>
<span>1</span> <span>0.276844</span> <span>1.312542</span> <span>0.345833</span> <span>00</span>:<span>13</span>
In [<span>0</span>]:
learn<span>.</span>save(<span>'stage-2'</span>)

<span>``</span>`
<span>### Interpretation</span>
In [<span>0</span>]:
learn<span>.</span>load(<span>'stage-2'</span>);
In [<span>0</span>]:
interp <span>=</span> ClassificationInterpretation<span>.</span>from_learner(learn)
In [<span>0</span>]:
interp<span>.</span>plot_confusion_matrix()

<span>### Cleaning Up</span>
Some of our top losses aren<span>'t due to bad performance by our model. There are images in our data set that shouldn'</span>t be<span>.</span>

Using the ImageCleaner widget <span>from</span> fastai.widgets we can prune our top losses, removing photos that don<span>'t belong.</span>

<span>```</span>

<span>In [0]:</span>
<span>from fastai.widgets import \*</span>

<span>```</span>
<span>First we need to get the file paths from our top_losses. We can do this with .from_toplosses. We then feed the top losses indexes and corresponding dataset to ImageCleaner.</span>

<span>Notice that the widget will not delete images directly from disk but it will create a new csv file cleaned.csv from where you can create a new ImageDataBunch with the corrected labels to continue training your model.</span>
<span>```</span>

<span>In [0]:</span>
<span>db = (ImageList.from_folder(path)</span>
<span>.split_none()</span>
<span>.label_from_folder()</span>
<span>.transform(get_transforms(), size=224)</span>
<span>.databunch()</span>
<span>)</span>
<span>In [0]:</span>

<span># If you already cleaned your data using indexes from `from_toplosses`,</span>

<span># run this cell instead of the one before to proceed with removing duplicates.</span>

<span># Otherwise all the results of the previous step would be overwritten by</span>

<span># the new run of `ImageCleaner`.</span>

<span>db = (ImageList.from_csv(path, '</span>cleaned<span>.</span>csv<span>', folder='</span><span>.</span><span>')</span>
<span>.split_none()</span>
<span>.label_from_df()</span>
<span>.transform(get_transforms(), size=224)</span>
<span>.databunch()</span>
<span>)</span>

<span>```</span>
<span>Then we create a new learner to use our new databunch with all the images.</span>
<span>```</span>

<span>In [0]:</span>
<span>learn_cln = cnn_learner(db, models.resnet50, metrics=error_rate)</span>

<span>learn_cln.load('</span>stage<span>-</span><span>2</span><span>');</span>
<span>In [0]:</span>
<span>ds, idxs = DatasetFormatter().from_toplosses(learn_cln)</span>
<span>In [0]:</span>

<span># Don'</span>t run this <span>in</span> google colab <span>or</span> <span>any</span> other instances running jupyter lab<span>.</span>

<span># If you do run this on Jupyter Lab, you need to restart your runtime and</span>

<span># runtime state including all local variables will be …</span></code></pre></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.rkpblog.tech/2020/04/porsche-classifier/">https://www.rkpblog.tech/2020/04/porsche-classifier/</a></em></p>]]>
            </description>
            <link>https://www.rkpblog.tech/2020/04/porsche-classifier/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24922472</guid>
            <pubDate>Wed, 28 Oct 2020 18:36:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Transhumanism and Augmented Reality – A brave new augmented world]]>
            </title>
            <description>
<![CDATA[
Score 28 | Comments 12 (<a href="https://news.ycombinator.com/item?id=24922309">thread link</a>) | @brna
<br/>
October 28, 2020 | https://qaautomation.dev/a-brave-new-agumented-world/ | <a href="https://web.archive.org/web/*/https://qaautomation.dev/a-brave-new-agumented-world/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
    <article>

        

        <section>
            <!--kg-card-begin: markdown--><h4 id="preface">Preface</h4>
<p>This post is a bundle of thoughts on the near future of Augmented Reality implementations, ways of integration, and introducing new layers of cognition. We are going to talk about AR as a way of enabling transhumanism by broadening the human cognitive and sensory toolbox.<br>
We are not going to be concerned with what is and what is not technologically possible at the moment.</p>
<h4 id="introduction">Introduction</h4>
<p>Have you ever taken a walk and thought about all the accessible information on animals, plants, objects, or even people? How much information is available on the internet? How many facts have you encountered and already forgotten?</p>
<p>Ancient Greeks put an active effort into training and maintaining their memorizing capabilities, and already at this stage of progress, human memory is shifting – or has shifted from remembering facts to remembering how and where to find facts. We are keeping less and less information hardcoded in our permanent memory and are in turn learning how to<br>
consume, process, and find useful data in an endless stream of information and misinformation.</p>
<p>A technologically proficient user has access to a vast volume and spectrum of data at his/her disposal, sitting right in the pocket. The human approach to holding knowledge is evolving as we speak. Heck, it started evolving with the invention of speech. But another significant<br>
change is on the cusp of happening. Our mobile phone interfaces, while serving all the information we need, are still detached from us, having considerable latency when we need to find, show, or input data. With the introduction of AR, AI, and human-computer interfaces, everything is about to change. AR or human augmentation in general will make that data<br>
more accessible and drastically lessen the latency needed for the interaction, making the tools feel more and more as part of ourselves.</p>
<p>Imagine walking down the street. You look around to observe people on the street and notifications start popping up alongside people’s heads. For one person the message says:<br>
"You have seen this person three times this week already"; for another one it states:<br>
"You have five mutual connections and common interests".<br>
Those notifications could also tell you if the person is willing to make new connections, based on previously shared data or facial microexpression analysis. Maybe we have gone too far down the rabbit hole for now, but you<br>
get the idea.</p>
<h3 id="definingarandtranshumanism">Defining AR and Transhumanism</h3>
<p>Merriam-Webster defines Augmented Reality as "an enhanced version of reality created by the use of technology to overlay digital information on an image of something being viewed through a device (such as a smartphone camera)". For the purposes of this post, I would like us not to limit reality to human vision alone.</p>
<p>Merriam-Webster has no definition for Transhumanism, but Wikipedia defines it as a philosophical movement that advocates for the transformation of the human condition by developing and making widely available sophisticated technologies to greatly enhance human intellect and physiology.</p>
<p>The two are getting more and more intertwined as technology advances, so I would like us to redefine AR, if only for this post, as "any technology that enhances or expands the human experience of reality", or maybe even as "practical transhumanism".</p>
<h3 id="augmentationtechnologymilestones">Augmentation technology milestones</h3>
<p>These are some freely set milestones for AR technology.</p>
<ul>
<li>LVL 0 – environment emulation via VR – for developmental fast-tracking</li>
<li>LVL 1 – external IO devices – headsets, glasses, gloves</li>
<li>LVL 2 – embedded IO devices – lenses, external neural signal readers</li>
<li>LVL 3 – tapping into existing sensory and neural networks</li>
<li>LVL 4 – registering new input/output devices in neural networks</li>
</ul>
<h3 id="degreesofaugmentation">Degrees of augmentation</h3>
<p>For the time being, and at least until we master the LVL 4 milestone described above, we are not creating new sensory or actuation systems, we are only hijacking the existing ones. Hi, Neuralink ;). This means that with every bit of information we gain, we are blocking the bandwidth and use for our existing IO mechanisms.In all fairness, people recovering mobility after a severe head trauma are evidence that the brain could possess enough plasticity to use whatever is thrown at it, given we connect the right dots, have enough time and practice. User age could also be a part of the equation.<br>
Sounds easier than it is, I know.</p>
<p>So, the more we augment our existing senses, the more we obstruct our basic physical-world data flow, the need to set some design restriction guidelines will grow. We still have accidents while using mobile phones or headphones, so the question arises: how could we manage a society full of people that overlap and fully cover their field of vision and hearing?<br>
Some safety rules integrated into the tech would be quite handy. Here is an example of defining degrees of augmentation:</p>
<ul>
<li>No augmentation</li>
<li>Info - can be an overlay, but should not block much sensory bandwidth</li>
<li>Safe communication – headset equivalent</li>
<li>Immersive experience – not safe for driving, safe for outdoors</li>
<li>Fully overlaying sensory input with enabled alerts – public safe zones</li>
<li>Fully overlaying sensory input – only for safe zones at home</li>
</ul>
<h3 id="augmentationusagecategories">Augmentation usage categories</h3>
<h4 id="knowledgemanagement">Knowledge management</h4>
<p>At some point, AR should enable access to data from the internet, which should be retrieved and presented without too much user effort, based on surrounding situations and context. Also, the useful data should be categorized and stored for retrieval at a later time.<br>
The interface should be able to combine relevant data and present solutions, statistics, and probabilities.</p>
<p>This could also enable saving verifiable event recordings, snapshots, and transcripts, real-time language translation, or seeing blueprints and instructions while fixing devices.</p>
<h4 id="extendingsensoryreach">Extending sensory reach</h4>
<p>The human sensory reach is already drastically extended with the use of IoT and the internet in general, but there is so much more of existing tech to be integrated into an AR system to extend human senses. Users could see and feel if their home or possessions are safe, or more generally, sense physical-world information streams in real-time.<br>
For example, physical-world data can be processed in parallel with the user experiencing the immediate surroundings. Thus, the user would, for instance, return home and detect certain objects have been moved, etc.</p>
<h4 id="extendingphysicalactuationactionreach">Extending physical actuation/action reach</h4>
<p>Besides our physical reach, most of us have already experienced controlling or affecting the physical-world at a distance. AR could help make the world around us feel more like an extension of our physical bodies.<br>
We could run automated processes upon visual triggers, i.e. unlock the front door without thinking, or control devices, drones, and robotics with ease.</p>
<h4 id="communicationandindividuality">Communication and individuality</h4>
<p>Communication could also be streamlined. We could have an instant connection with anyone around the world, almost as if standing side by side. Communication could play out on several levels, depending on the level of technological advancement, and could be:</p>
<ul>
<li>Restrained – messages and recordings with a delay and opportunity for curation</li>
<li>Unrestrained – conservative – real-time audio-visual communication</li>
<li>Unrestrained – progressive – direct thought transfer in a Neuralink-like brain-computer interface manner</li>
</ul>
<p>We could socialize, play games, or work like never before:<br>
imagine coding while sharing your thought process with a colleague, and also having an "AI" assisted IDE.</p>
<h4 id="extendingthecognitiontoolbox">Extending the cognition toolbox</h4>
<p>Given having a portable AR system and a smart enough underlying OS that replicates our surroundings and anticipates our needs OR an efficient enough way of selecting and using computational tools, we could:</p>
<ul>
<li>see object trajectories</li>
<li>have perfect math at our disposal</li>
<li>know objects, plant life, animals, etc.</li>
</ul>
<p>AI or General AI could, if available, also be used as an intermediary to the available cognition toolbox, to serve the necessary tools and results to the end-user.</p>
<h4 id="pastimeandentertainment">Pastime and entertainment</h4>
<p>I won’t go into great detail here, VR games are already immersive and fun, but try to imagine what they would be like if some freedom is added to the mix. As games and entertainment are presumably most likely to require as much immersion as possible, it will be necessary to consider immersion safety.</p>
<h5 id="vrgamesandapsfullyoverlayingsensoryinput">VR games and APS - fully overlaying sensory input</h5>
<p>Cars, stairs, sudden drops, and obstacles are here to stay, so for any device that can write over your sensory input, a way to risk mitigation must be in place.</p>
<h5 id="endusersafetyzonecalibration">End user safety zone calibration</h5>
<p>Imagine you want to play a full-on VR game somewhere. How could you stay safe and not trip over something or fall over some curb? It is simple, you could just walk around your playground to define its contours and limits. A certain amount of space can even be taken out of the outer limits of the playground, any obstacles are reproduced in-game so you can navigate past them. You start the game and play, and in case you move to the outer limits, the game simply starts to fade away.</p>
<h5 id="safesurfaces">Safe surfaces</h5>
<p>OK, but where else could you use 100% augmentation opacity in the real world? The first thing that comes to mind as safe would be walls. You can project stuff in front of almost any wall and be sure that nothing you can’t see will hit you from that direction, or at least as sure as you are now.</p>
<h5 id="contentkillswitch">Content killswitch</h5>
<p>Another useful feature would be to have the immersive content stop when quiet time or rest is needed, as well as if stress levels are critical.</p>
<h3 id="pitfallsofarandhumanaugmentation">Pitfalls of AR and Human augmentation</h3>
<ul>
<li>Having no signal or battery</li>
<li>Developing a dependence on AR in everyday life</li>
<li>Losing sense of the physical self</li>
<li>Losing individuality</li>
<li>Identifying the AR OS as a part of the personality</li>
</ul>
<h3 id="wecanworkondevelopingartoday">We can work on developing AR today</h3>
<p>I guess we are all well aware of the current limitations of the AR systems, but looking past the non-existing or low tech ways of input and output that we nowadays possess, by emulating non-existing augmentation pathways and …</p></section></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://qaautomation.dev/a-brave-new-agumented-world/">https://qaautomation.dev/a-brave-new-agumented-world/</a></em></p>]]>
            </description>
            <link>https://qaautomation.dev/a-brave-new-agumented-world/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24922309</guid>
            <pubDate>Wed, 28 Oct 2020 18:24:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pyston v2: Faster Python]]>
            </title>
            <description>
<![CDATA[
Score 250 | Comments 178 (<a href="https://news.ycombinator.com/item?id=24921790">thread link</a>) | @kmod
<br/>
October 28, 2020 | https://blog.pyston.org/2020/10/28/pyston-v2-20-faster-python/ | <a href="https://web.archive.org/web/*/https://blog.pyston.org/2020/10/28/pyston-v2-20-faster-python/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
			
		<div>
		<main id="main" role="main">

					<div>
				
<article id="post-895">
	<!-- .entry-header -->

	<div>
		
<p>We’re very excited to release Pyston v2, a faster and highly compatible implementation of the Python programming language.  Version 2 is 20% faster than stock Python 3.8 on our macrobenchmarks.  More importantly, it is likely to be faster on your code.  Pyston v2 can reduce server costs, reduce user latencies, and improve developer productivity.</p>



<p>Pyston v2 is easy to deploy, so if you’re looking for better Python performance, we encourage you to take five minutes and <a href="#availability">try Pyston</a>. Doing so is one of the easiest ways to speed up your project.</p>



<h2>Performance</h2>



<p>Pyston v2 provides a noticeable speedup on many workloads while having few drawbacks.  Our focus has been on web serving workloads, but Pyston v2 is also faster on other workloads and popular benchmarks.</p>



<p>Our team put together a new <a href="https://github.com/pyston/python-macrobenchmarks/">public Python macrobenchmark suite</a> that measures the performance of several commonly-used Python projects.  The benchmarks in this suite are larger than those found in other Python suites, making them more likely to be representative of real-world applications.  Even though this gives us a lower headline number than other projects, we believe it translates to better speedups for real use cases.  Pyston v2 still shows sped-up performance on microbenchmarks, being twice as fast as standard Python on tests like chaos.py and nbody.py.</p>



<p>Here are our performance results:</p>



<figure><table><tbody><tr><td></td><td>CPython 3.8.5</td><td>Pyston 2.0</td><td>PyPy 7.3.2</td></tr><tr><td>flaskblogging warmup time [1]</td><td>n/a</td><td>n/a</td><td>85s</td></tr><tr><td>flaskblogging mean latency</td><td>5.1ms</td><td>4.1ms</td><td>2.5ms</td></tr><tr><td>flaskblogging p99 latency</td><td>6.3ms</td><td>5.2ms</td><td>5.8ms</td></tr><tr><td>flaskblogging memory usage</td><td>47MB</td><td>54MB</td><td>228MB</td></tr><tr><td>djangocms warmup time [1]</td><td>n/a</td><td>n/a</td><td>105s</td></tr><tr><td>djangocms mean latency</td><td>14.1ms</td><td>11.8ms</td><td>15.9ms</td></tr><tr><td>djangocms p99 latency</td><td>15.0ms</td><td>12.8ms</td><td>179ms</td></tr><tr><td>djangocms memory usage</td><td>84MB</td><td>91MB</td><td>279MB</td></tr><tr><td>Pylint speedup</td><td>1x</td><td>1.16x</td><td>0.50x</td></tr><tr><td>mypy speedup</td><td>1x</td><td>1.07x [2]</td><td>unsupported</td></tr><tr><td>PyTorch speedup</td><td>1x</td><td>1.00x [2]</td><td>unsupported</td></tr><tr><td>PyPy benchmark suite [3]</td><td>1x</td><td>1.36x</td><td>2.48x</td></tr></tbody></table><figcaption>Results were collected on an m5.large EC2 instance running Ubuntu 20.04</figcaption></figure>



<p>[1] Warmup time is defined as time until the benchmark reached 95% of peak performance; if it was not distinguishable from noise it is marked “n/a”.  Only post-warmup behavior is considered for latency measurement.<br>[2] mypy and PyTorch don’t support automatically building their C extensions from source, so these Pyston numbers use our unsafe compatibility mode<br>[3] The PyPy benchmark suite was modified to only run the benchmarks that are compatible with Python 3.8</p>



<h2>Results analysis</h2>



<p>In our targeted benchmarks (djangocms + flaskblogging), Pyston v2 provides an average 1.22x speedup for mean latency and an 1.18x improvement for p99 latency while using a just few more megabytes per process.  We have not yet invested time in optimizing the other benchmarks.</p>



<p>“p99 latency” is the upper 99th percentile of the response-time distribution, and is a common metric used in web serving contexts since it can provide insight into user experience that is lost by taking an average.  PyPy’s high p99 latency on djangocms comes from periodic latency spikes, presumably from garbage collection pauses.  CPython and Pyston both exhibit periodic spikes, presumably from their cycle collectors, but they are both less frequent and much smaller in magnitude.</p>



<p>The mypy and PyTorch benchmarks show a natural boundary of Pyston v2. These benchmarks both do the bulk of their work in C extensions which are unaffected by our Python speedups.  We natively support the C API and do not have an emulation layer, so we are still able to provide a small boost to mypy performance and do not degrade pytorch or numpy performance.  Your benefit will depend on your mix of Python and C extension work.</p>



<h2>Technical approach</h2>



<p>We’re planning on going into more detail in future blog posts, but some of the techniques we use in Pyston v2 include:</p>



<ul><li>A very-low-overhead JIT using <a href="https://luajit.org/dynasm.html">DynASM</a></li><li><a href="https://bugs.python.org/issue14757">Quickening</a></li><li>General CPython optimizations</li><li>Build process improvements</li></ul>



<h2>Compatibility</h2>



<p>Since Pyston is a fork of CPython, we believe it is one of the most compatible alternative Python implementations available today.  It supports all the same features and C API that CPython does.</p>



<p>While Pyston is identically functional in theory, in practice there are some temporary compatibility hurdles for any new Python implementation.  Please see <a href="https://github.com/pyston/pyston/wiki">our wiki</a> for details.</p>



<h2 id="availability">Availability</h2>



<p>Pyston v2.0 is <a href="https://github.com/pyston/pyston/releases">immediately available</a> as a pre-built package.  Currently, we have packages for Ubuntu 18.04 and 20.04 x86_64.  If you would like support for a different OS, let us know by filing an issue in our <a href="https://github.com/pyston/pyston/issues">issue tracker</a>.</p>



<p>Trying out Pyston is as simple as installing our package, replacing <code>python3</code> with <code>pyston3</code>, and reinstalling your dependencies with <code>pip-pyston3 install</code> (though see our <a href="https://github.com/pyston/pyston/wiki">wiki</a> for a known issue about setuptools). If you already have an automated build set up, the change should be just a few lines.</p>



<p>Our plan is to open-source the code in the future, but since compiler projects are expensive and we no longer have benevolent corporate sponsorship, it is currently closed-source while we iron out our business model.</p>



<h2>Reaching us</h2>



<p>We are designing Pyston for developers and love to hear about your needs and experiences.  So, we’ve set up a <a href="https://discord.gg/S7gsqnb">Discord server</a> where you can chat with us.  If you’d like a commercially-supported version of Pyston, please <a href="mailto:business@pyston.org">send us an email</a>.</p>



<p>We’ve optimized Pyston for several use cases but are eager to hear about new ones so that we can make it even more beneficial.  If you run into any problems or instances where Pyston does not help as much as expected, please let us know!</p>



<h2>Background</h2>



<p>We designed Pyston v1 at Dropbox to speed up Python for its web serving workloads.  After the project ended, some of us from the team brainstormed how we would do it differently if we were to do it again.  In early 2020, enough pieces were in place for us to start a company and work on Pyston full-time.</p>



<p>Pyston v2 is inspired by but is technically unrelated to the original Pyston v1 effort.</p>



<h2>Moving forward</h2>



<p>We’re on a mission to make Python faster and have plenty of ideas to do so.  That means we’re actively looking for people to join the team.  <a href="https://discord.gg/S7gsqnb">Let us know</a> if you’d like to get involved.  Otherwise stay tuned for future releases and reach out if you have any questions!</p>
			</div><!-- .entry-content -->

	<!-- .entry-meta -->
</article><!-- #post-## -->
			</div>

				<nav role="navigation" id="nav-below">
		

	
				
	
	</nav><!-- #nav-below -->
	
			
<!-- #comments -->

		
		</main><!-- #main -->
	</div><!-- #primary -->

					<!-- #secondary -->
	
	</div></div>]]>
            </description>
            <link>https://blog.pyston.org/2020/10/28/pyston-v2-20-faster-python/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24921790</guid>
            <pubDate>Wed, 28 Oct 2020 17:42:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Uncover how your funds spend money in politics]]>
            </title>
            <description>
<![CDATA[
Score 41 | Comments 14 (<a href="https://news.ycombinator.com/item?id=24921199">thread link</a>) | @mushufasa
<br/>
October 28, 2020 | https://www.yourstake.org/politics/ | <a href="https://web.archive.org/web/*/https://www.yourstake.org/politics/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

<section>
  <p>
    <span>Are your Investments</span>
    <br>
    <span>Democrat or Republican?</span>
  </p>
  <p><img src="https://stake-assets.s3.amazonaws.com/static/sillyimages/Donkey-.svg">
    <img src="https://stake-assets.s3.amazonaws.com/static/sillyimages/Indian-elephant.svg">
  </p>
  <br>


  <!--
    to implement when we setup plaid
    <div class="tabs is-centered is-toggle">
    <ul>
      <li class="is-active">
        <a>
          <span>Select</span>
        </a>
      </li>
      <li>
        <a>
          <span>Sync</span>
        </a>
      </li>
    </ul>
  </div> -->
  
  <p>
    Lookup your Fund
  </p>
  <!-- field -->
<!-- myform-->

<p>
  <em>Find out how much money the companies in your mutual fund donate to Democratic and Republican candidates.</em>
</p>

<p><a href="https://www.yourstake.org/yourimpact/">
    <img src="https://stake-assets.s3.amazonaws.com/static/images/yourstakelogo.png" alt="Stake: Your Voice through Your Investments">
  </a>
</p>






</section></div>]]>
            </description>
            <link>https://www.yourstake.org/politics/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24921199</guid>
            <pubDate>Wed, 28 Oct 2020 17:01:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Chess Captcha]]>
            </title>
            <description>
<![CDATA[
Score 40 | Comments 28 (<a href="https://news.ycombinator.com/item?id=24920945">thread link</a>) | @chadash
<br/>
October 28, 2020 | https://elioair.github.io/chesscaptcha/ | <a href="https://web.archive.org/web/*/https://elioair.github.io/chesscaptcha/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section id="main_content">
          <h3>
ChessCaptcha.</h3>

<p>This is a captcha system where the user either recreates the position of the pieces on the board - <em>non chess savvy users</em> - or she solves a mate-in-one puzzle by putting the piece on the square where it gives the checkmate - <em>chess savvy users only</em>. There is also a no-js fallback that exists mostly as a placeholder for future iterations; don't use it.</p>
        </section><section id="usage">
          <h3>
Usage.</h3>

<h4>Copy the position mode. - Default</h4>
<p>
  In this mode which happens to be the one where the user needs no chess knowledge at all to use, he simply drags the pieces
  into the board trying to replicate the position shown in the image.
</p>
<p>
  <img src="https://elioair.github.io/chesscaptcha/images/chesscaptchausage.gif">
</p>
<h4>Mate in One mode</h4>
<p>
  Here the user is given a position and where he has to move a piece to create a mating position on the board.
  Only chess players -and maybe computer engines. cough..!- will be able to solve this. It can be used for example in chess sites
  to determine if the user is really a chess player or a simple spammer.
</p>
<p>
  <img src="https://elioair.github.io/chesscaptcha/images/chesscaptchamatemode.gif">
</p>
<h4>Color Tolerance</h4>
<p>
  If needed you can turn on color tolerance. In this case the validation will be color agnostic and for example the white king will be 
  considered equal to the black king.
</p>
<p>
  <img src="https://elioair.github.io/chesscaptcha/images/chesscaptchacolortolerance.gif">
</p>
        </section></div>]]>
            </description>
            <link>https://elioair.github.io/chesscaptcha/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24920945</guid>
            <pubDate>Wed, 28 Oct 2020 16:44:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[If Not SPAs, What?]]>
            </title>
            <description>
<![CDATA[
Score 280 | Comments 315 (<a href="https://news.ycombinator.com/item?id=24920702">thread link</a>) | @todsacerdoti
<br/>
October 28, 2020 | https://macwright.com/2020/10/28/if-not-spas.html | <a href="https://web.archive.org/web/*/https://macwright.com/2020/10/28/if-not-spas.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>A few months ago, I <a href="https://macwright.com/2020/05/10/spa-fatigue.html">wrote an article about how the SPA pattern has failed to simplify web development</a>. The <em>SPA pattern</em> (Single-Page Apps), I tried to define, was about the React model, which also covers, to a large extent, the model of Vue, Angular, and other frontend frameworks.</p><p>Like any critique, it begs for a prescription and I didn’t give one, other than gesturing toward server-side frameworks like Rails and Django. But I think there are some trends starting to form. I had queued up some time to <em>really dive into the frameworks</em>, but things like <a href="https://macwright.com/2020/10/12/holly-park.html">walking in parks</a> have taken priority, so here’s just a grand tour.</p><h3 id="opinionated-full-stack-javascript-frameworks">Opinionated full-stack JavaScript frameworks</h3><p>Primarily I’m talking about <a href="https://remix.run/features">Remix</a>, <a href="https://redwoodjs.com/">RedwoodJS</a>, and <a href="https://blitzjs.com/">Blitz.js</a>, though I’m sure there are similar efforts in the non-React world that are relevant. <a href="https://nextjs.org/">Next.js</a> <em>almost</em> falls into this category, but as far as I can tell, it’s still unopinionated about the data layer and most sites that use Next.js are still going to use a separate API stack. But that’s subject to change, because all of these are moving fast.</p><p>It’s interesting to note that Remix, Redwood, and Next are all backed by companies or foundations, and that Blitz is aiming early on to be a <a href="https://github.com/sponsors/blitz-js">sponsor-funded project</a>. These projects, I think, are trying to sidestep the “tragedy of the commons” failures of earlier open source, wherein overworked and unpaid maintainers service a large userbase and eventually burn out and abandon the project.</p><p>To take Remix as an example, it re-ties data loading with routes, and then gives the pretty amazing promise of <em>no client side data fetching by default</em>. These frameworks are also <em>opinionated about status codes and caching strategies</em>. RedwoodJS automatically creates an <a href="https://redwoodjs.com/tutorial/side-quest-how-redwood-works-with-data">ORM-like interface using GraphQL and Prisma</a>.</p><p>As context, Remix is backed by the folks from <a href="https://reacttraining.com/">React Training</a>, who are also the folks from <a href="https://reactrouter.com/">React Router</a>, which is as much React pedigree as you can get without joining the team at Facebook. Redwood is run by <a href="https://prestonwernerventures.com/">Preston-Werner Ventures</a>, of <a href="https://en.wikipedia.org/wiki/Tom_Preston-Werner">Tom Preston-Werner</a>, a GitHub founder. Next.js is sponsored and heavily promoted by <a href="https://vercel.com/">Vercel</a>, née Zeit.</p><h3 id="turbolinks">Turbolinks</h3><p>It’s worthwhile to just mention <a href="https://github.com/turbolinks/turbolinks">Turbolinks</a>. I didn’t use it until this year, and apparently there were issues with it before, but the pitch for Turbolinks 5 is: <em>what is the bare minimum you need to do to get the SPA experience without any cooperation from your application?</em></p><p>So it’s a tiny JavaScript library that sits on top of an existing server-rendered application and replaces full pageloads with SPA-like partial pageloads. Instead of loading a page from scratch, pages are loaded with AJAX, page contents are replaced, and client-side navigation updates your URLs. Basically, it prevents the ‘blink’ of real page transitions and saves on all othe sorts of costs of fully loading a new page. Turbolinks was spawned from the <a href="https://rubyonrails.org/">Ruby on Rails</a> project, and works great with Rails but doesn’t require it.</p><p>In terms of power-to-weight for user experience improvements, Turbolinks is a standout: it adds very little complexity and a tiny size impact for a big user experience improvement.</p><h3 id="server-side-state-frameworks">Server-side-state frameworks</h3><p>These are the spiciest new solution. The main contenders are <a href="https://laravel-livewire.com/">Laravel Livewire</a> (in PHP), <a href="https://docs.stimulusreflex.com/">Stimulus Reflex</a> (for Ruby on Rails), and <a href="https://github.com/phoenixframework/phoenix_live_view">Phoenix LiveView</a> (on Phoenix, in Elixir).</p><p>The pitch here is: <em>what if you didn’t have to write any JavaScript?</em> It sort of hearkens back to the critique of JavaScript in <a href="https://vimeo.com/5047563">_why’s ART &amp;&amp; CODE talk</a>, that web development is the only kind where you normally have to write in three (or more) languages. These languages also most remnants of “client-side” logic, putting it all on the server side.</p><p>How do they do this? Well, a lot of WebSockets, in the case of Reflex and LiveView, as well as very tightly coupled server interactions. As you can see in the <a href="https://www.phoenixframework.org/blog/build-a-real-time-twitter-clone-in-15-minutes-with-live-view-and-phoenix-1-5">LiveView demo</a>, which I highly recommend, these frameworks tend to operate sort of like reactive DOM libraries on the front end – in which the framework figures out minimal steps to transform from one state to another - except those steps are computed on the server side and then generically applied on the client side. They also do a lot more data storage &amp; state management on the server-side, because a lot of those interactions which wouldn’t be persisted to the server are now at least communicated to the server.</p><p>These frameworks are exciting, and also extremely contrarian, because they are the polar opposite of the “frontend plus agnostic API layer” pattern, and they also wholeheartedly embrace the thing everyone tries to avoid: mutable state on the server.</p><h3 id="modest-progressive-enhancement-javascript-frameworks">Modest progressive-enhancement JavaScript frameworks</h3><p>These are typically used “in addition” to the above, but they certainly deserve a shout-out because I think a wide swath of frontend-programming concerns actually only need a tiny hint of JavaScript. But the main caveat is that <em>they assume that you know JavaScript and the DOM</em>, which are not necessarily universal skills anymore. A lot of developers growing up on React have acquired a real blind spot for native browser APIs.</p><p>The main ones I’ve looked at are <a href="https://stimulusjs.org/">Stimulus</a> (out of the Ruby on Rails camp), <a href="https://github.com/alpinejs/alpine/">Alpine</a>, and <a href="https://htmx.org/">htmlx</a>. They’re all tiny, and work great in <em>existing pages</em>. I think – and here come the flames – <a href="https://developer.mozilla.org/en-US/docs/Web/Web_Components">Web Components</a> also fit into this sphere of progressive enhancement! If you just use good web components - <a href="https://github.com/search?q=topic%3Aweb-components+org%3Agithub&amp;type=Repositories">only ones that GitHub writes is a good rule of thumb</a> - then they can fit the role of just improving an existing static UI. It’s where you start to use Web Components as an apples-to-apples replacement for full-fledged frontend frameworks is where things seem to get dicey.</p><p>These frameworks have the luxury of operating on a deeply improved web stack, one with fundamental components like <a href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API">fetch()</a> and <a href="https://developer.mozilla.org/en-US/docs/Web/API/MutationObserver">MutationObserver</a>. These things were previously at the core of the utility of progressive enhancement frameworks, and now they can just be the utilities that those frameworks build on.</p><hr><p>I’m sure that there are additional patterns out there! But these currents all seem strong right now, and it’s fascinating to see some really divergent and adventurous – and common-sense – approaches start to crop up.</p></div></div>]]>
            </description>
            <link>https://macwright.com/2020/10/28/if-not-spas.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24920702</guid>
            <pubDate>Wed, 28 Oct 2020 16:27:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bizarre Design Choices in Zoom's End-to-End Encryption]]>
            </title>
            <description>
<![CDATA[
Score 41 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24920125">thread link</a>) | @some_furry
<br/>
October 28, 2020 | https://soatok.blog/2020/10/28/bizarre-design-choices-in-zooms-end-to-end-encryption/ | <a href="https://web.archive.org/web/*/https://soatok.blog/2020/10/28/bizarre-design-choices-in-zooms-end-to-end-encryption/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<div>

			
<p>Zoom recently announced that they were going to make end-to-end encryption available to all of their users–not just customers.</p>



<figure><div>

</div></figure>



<p>This is a good move, especially for people living in countries with <a href="https://soatok.blog/2020/07/02/how-and-why-america-was-hit-so-hard-by-covid-19/">inept leadership that failed to address the COVID-19 pandemic</a> and therefore need to conduct their work and schooling remotely through software like Zoom. I enthusiastically applaud them for making this change.</p>



<div><figure><img data-attachment-id="1333" data-permalink="https://soatok.blog/soatoktelegrams2020-08/" data-orig-file="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png" data-orig-size="512,512" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SoatokTelegrams2020-08" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=512" src="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=512" alt="" srcset="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png 512w, https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=150 150w, https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=300 300w" sizes="(max-width: 512px) 100vw, 512px"><figcaption>End-to-end encryption, on by default, is a huge win for everyone who uses Zoom. (Art by <a href="https://twitter.com/lynxvsjackalope">Khia</a>.)</figcaption></figure></div>



<p>The end-to-end encryption capability arrives on the heels of their acquisition of <a href="https://keybase.io/">Keybase</a> in earlier this year. Hiring a team of security experts and cryptography engineers seems like a good move overall.</p>



<p>Upon hearing this news, I decided to be a good neighbor and take a look at their source code, with the reasoning, “If so many people’s privacy is going to be dependent on Zoom’s security, I might as well make sure they’re not doing something ridiculously bad.”</p>



<p>Except I couldn’t find their source code anywhere online. But they did publish <a href="https://github.com/zoom/zoom-e2e-whitepaper">a white paper on Github</a>…</p>







<h2>Disclaimers</h2>



<p>What follows is the opinion of some guy on the Internet with a fursona–so whether or not you choose to take it seriously should be informed by this context. It is not the opinion of anyone’s employer, nor is it endorsed by Zoom, etc. Tell your lawyers to calm their nips.</p>



<p>More importantly, I’m not here to hate on Zoom for doing a good thing, nor on the security experts that worked hard on making Zoom better for their users. The responsibility of security professionals is to the users, after all.</p>



<p>Also, these aren’t zero-days, so don’t try to lecture me about “responsible” disclosure. (That term is also <a href="https://adamcaudill.com/2015/11/19/responsible-disclosure-is-wrong/">problematic</a>, by the way.)</p>



<p>Got it? Good. Let’s move on.</p>







<h2>Bizarre Design Choices in Version 2.3 of Zoom’s E2E White Paper</h2>



<p>Note: I’ve altered the screenshots to be white text on a black background, since my blog’s color scheme is darker than a typical academic PDF. You can find the source <a href="https://github.com/zoom/zoom-e2e-whitepaper/blob/d3be2a5a3e16be04f1199b92630f180ba79cb51c/zoom_e2e.pdf">here</a>.</p>



<h3>Cryptographic Algorithms</h3>



<div><figure><img data-attachment-id="1744" data-permalink="https://soatok.blog/zoom-e2e-02/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png" data-orig-size="784,652" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-02" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png" alt=""></figure></div>



<p>It’s a little weird that they’re calculating a signature over SHA256(Context) || SHA256(M), considering Ed25519 uses SHA512 internally.</p>



<p>It would make just as much sense to sign Context || M directly–or, if pre-hashing large streams is needed, SHA512(Context || M).</p>



<div><figure><img data-attachment-id="1740" data-permalink="https://soatok.blog/zoom-e2e-01/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png" data-orig-size="1039,788" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-01" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png" alt=""></figure></div>



<p>At the top of this section, it says it uses libsodium’s <code>crypto_box</code> interface. But then they go onto… not actually use it.</p>



<p>Instead, they wrote their own protocol using HKDF, two SHA256 hashes, and XChaCha20-Poly1305.</p>



<p>While secure, this isn’t <em>really</em> using the crypto_box interface.</p>



<p>The only part of the libsodium interface that’s being used is <code><a href="https://github.com/jedisct1/libsodium/blob/927dfe8e2eaa86160d3ba12a7e3258fbc322909c/src/libsodium/crypto_box/curve25519xsalsa20poly1305/box_curve25519xsalsa20poly1305.c#L35-L46">crypto_box_beforenm()</a></code>, which could easily have been a call to <code>crypto_scalarmult()</code>instead (since they’re passing the output of the scalar multiplication to HKDF anyway).</p>







<p>Also, the SHA256(a) || SHA256(b) pattern returns. Zoom’s engineers must love SHA256 for some reason.</p>



<p>This time, it’s in the additional associated data for the XChaCha20-Poly1305. </p>



<p>Binding the ciphertext and the signature to the same context string is a sensible thing to do, it’s just the concatenation of SHA256 hashes is a bit weird when SHA512 exists.</p>



<h3>Meeting Leader Security Code</h3>



<div><figure><img data-attachment-id="1746" data-permalink="https://soatok.blog/zoom-e2e-03/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png" data-orig-size="760,733" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-03" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png" alt=""></figure></div>



<p>Here we see Zoom using the a SHA256 of a constant string (“<code>Zoombase-1-ClientOnly-MAC-SecurityCode</code>“) in a construction that tries but fails to be HMAC.</p>



<p>And then they concatenate it with the SHA256 hash of the public key (which is already a 256-bit value), and then they hash the whole thing again.</p>



<p>It’s redundant SHA256 all the way down. The redundancy of “MAC” and “SecurityCode” in their constant string is, at least, consistent with the rest of their design philosophy.</p>



<p>It would be a real shame if double-hashing carried the risk of <a href="https://eprint.iacr.org/2013/382">invalidating security proofs</a>, or if <a href="https://cseweb.ucsd.edu/~mihir/papers/kmd5.pdf">the security proof for HMAC</a> required a high Hamming distance of padding constants and this design decision also later <a href="https://eprint.iacr.org/2012/684.pdf">saved HMAC from related-key attacks</a>.</p>



<h3>Hiding Personal Details</h3>



<figure><img data-attachment-id="1750" data-permalink="https://soatok.blog/zoom-e2e-04/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png" data-orig-size="739,603" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-04" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=739" alt="" srcset="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png 739w, https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=150 150w, https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=300 300w" sizes="(max-width: 739px) 100vw, 739px"></figure>



<p>Wait, you’re telling me Zoom was aware of HMAC’s existence this whole time?</p>



<div><figure><img data-attachment-id="1202" data-permalink="https://soatok.blog/soatoktelegrams2020-02/" data-orig-file="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png" data-orig-size="512,512" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SoatokTelegrams2020-02" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=512" src="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=512" alt="" srcset="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png 512w, https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=150 150w, https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=300 300w" sizes="(max-width: 512px) 100vw, 512px"><figcaption>I give up!</figcaption></figure></div>



<h2>Enough Pointless Dunking, What’s the Takeaway?</h2>



<p>None of the design decisions Zoom made that I’ve criticized here are security vulnerabilities, but they do demonstrate an early lack of cryptography expertise in their product design.</p>



<p>After all, the weirdness is almost entirely contained in section 3 of their white paper, which describes the “Phase I” of their rollout. So what I’ve pointed out here appears to be mostly legacy cruft that wasn’t risky enough to bother changing in their final design.</p>



<p>The rest of their paper is pretty straightforward and pleasant to read. Their design makes sense in general, and each phase includes an “Areas to Improve” section.</p>



<p>All in all, if you’re worried about the security of Zoom’s E2EE feature, the only thing they can really do better is to publish the source code (and link to it from the whitepaper repository for ease-of-discovery) for this feature so independent experts can publicly review it.</p>



<p>However, they seem to be getting a lot of mileage out of the experts on their payroll, so I wouldn’t count on that happening.</p>

		</div><!-- .entry-content -->

	</div></div>]]>
            </description>
            <link>https://soatok.blog/2020/10/28/bizarre-design-choices-in-zooms-end-to-end-encryption/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24920125</guid>
            <pubDate>Wed, 28 Oct 2020 15:45:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[20 Predictions for Community in the 2020s]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24918006">thread link</a>) | @teaguns
<br/>
October 28, 2020 | https://the.community.club/mac/20-predictions-for-community-in-the-2020s-2ec3 | <a href="https://web.archive.org/web/*/https://the.community.club/mac/20-predictions-for-community-in-the-2020s-2ec3">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main-title">
            <p><img src="https://the.community.club/images/PcDObvAWNr8SuZtdzYc-K9JFgEm2j9yanWgap0z1z4Q/s:1000:420/aHR0cHM6Ly90aGUu/Y29tbXVuaXR5LmNs/dWIvcmVtb3RlaW1h/Z2VzL2kvY3h5aGJj/OHF3ZmQzc2p1NGww/MDYucG5n" width="1000" height="420" alt="Cover image for 20 Predictions for Community in the 2020s">
            </p>

          
        </div><div>

          <div data-article-id="38" id="article-body">
            <p>Software is eating the world, and community is eating software</p>

<p>In no particular order, a few predictions for community in the 2020s.</p>

<p>💪 <strong>Community will continue to be a broad term</strong>, but finally people and companies will fully grasp that an audience does not mean the same thing as a community.</p>

<p>🪑 <strong>Community will finally get a seat at the table</strong>, following in the footsteps of customer success from a decade ago. Community will become its own department within orgs, leading to more Heads of Community, Chief Community Officers, and other community leadership roles.</p>

<p>✨ <strong>Continued specialization of roles in community.</strong> Community Manager will turn into Community Engagement, Community Marketing, Community Support, Community Success, etc.</p>

<p>🙋 <strong>And then we’ll see other roles start to merge under community.</strong> Community, customer support, and customer success will overlap and start to blend together. Traditional CS roles will still exist, but more ‘low-level’ support will be offloaded to communities and community teams.</p>

<p>📈 <strong>More and more companies will invest in community early</strong>, hiring their first community roles in their first 10 or 20 employees. Community will be standard for startups started in the 2020s.</p>

<p>💼 <strong>The unbundling of LinkedIn.</strong> What Dribbble is to designers, we’ll see for nurses, marketers, community managers, and every other career vertical, providing community, ‘portfolios’, and better job hunting experiences tailored to the specific type.</p>

<p>🎁 <strong>The unbundling of Reddit.</strong> Every subreddit will get its own dedicated community app, built specifically to serve the unique needs of that vertical. Vertical specificity will win out over general community platforms.</p>

<p>🚗 <strong>Community-driven versions of existing products</strong> and services will continue to be built, eventually taking over those who built without community. See Public vs Robinhood.</p>

<p>📣 <strong>Interactive audio and audio communities</strong> will continue to grow. Spotify will enter the market with their own offering, or acquire an existing player similarly to when they initially entered podcasting.</p>

<p>🕺 <strong>We’ll see innovation in the community platform space.</strong> Everything right now looks like some form of forum or ‘Slack for community’. We’ll see platforms that rethink the idea of community from the ground up, in less structured ways than ‘forums’.</p>

<p>🌍 The explosion of remote work will continue to drive <strong>a greater need for community at every level, online and offline</strong>, locally and globally. Expect more companies to build verticals in Nextdoor’s space.</p>

<p>🤑 <strong>Companies will continue to acquire communities</strong> as starting points for their own. See Outreach + Saleshacker, Stripe + Indiehackers, and DigitalOcean + Scotch.</p>

<p>💡 <strong>More and more companies will build communities of interest</strong>, rather than just customer &amp; support communities. This will enable their communities to reach a wider audience, and ultimately be seen as revenue generators rather than cost centers.</p>

<p>💬 <strong>A number of independent communities will turn into full-fledged businesses</strong> and media ‘empires’ as they grow beyond the community they started with.</p>

<p>🛠 <strong>Consolidation of tools.</strong> We’re currently in a golden age of community, engagement and event tools, but over the next decade we’ll see major players start to lead, acquire competitors, and ultimately ‘win’. We’ll also see existing companies (Salesforce, Slack, etc) invest more in community by acquiring companies that are being built right now.</p>

<p>🙊 <strong>Increased innovation will occur in the moderation space.</strong> As community gets elevated in organizations, companies will finally want to invest in tools to help make moderation easier and less dependent on pure manpower.</p>

<p>🏢 <strong>Companies will treat their internal teams more like communities</strong>, and there will be an expansion of internal community teams, especially at cos with 1,000+ employees. Remote-first companies will set this trend, needing to be more intentional about fostering internal community.</p>

<p>👋 <strong>Continued normalization of internet friends</strong>, people that you met in online communities rather than in-person. This has been normal for a decade or more in the gaming world, but it will bleed over to professional friends and relationships.</p>

<p>🏃‍♀️ <strong>In-person events will come back quickly post-Covid</strong>, but they will be smaller and more intimate, focusing on genuine connection. It will be a while before we see large 500+ person events and conferences again.</p>

<p>📆 <strong>Companies will continue to invest in online events</strong> and engagement as they realize how much larger of a reach they can achieve for lower cost and overhead. In-person conferences will adopt hybrid models, offering virtual access in addition to their physical locations.</p>


<hr>

<p>Have any predictions of your own for how community (especially within organizations) will evolve over the next decade? Share them in the thread below! 👇</p>


          </div>

        </div></div>]]>
            </description>
            <link>https://the.community.club/mac/20-predictions-for-community-in-the-2020s-2ec3</link>
            <guid isPermaLink="false">hacker-news-small-sites-24918006</guid>
            <pubDate>Wed, 28 Oct 2020 12:39:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Dutch flying car gets permission to drive on European roads]]>
            </title>
            <description>
<![CDATA[
Score 176 | Comments 105 (<a href="https://news.ycombinator.com/item?id=24917841">thread link</a>) | @Bologo
<br/>
October 28, 2020 | https://www.psychnewsdaily.com/dutch-flying-car-pal-v-gets-permission-to-drive-on-european-roads/ | <a href="https://web.archive.org/web/*/https://www.psychnewsdaily.com/dutch-flying-car-pal-v-gets-permission-to-drive-on-european-roads/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-4802" role="main"><div><div><div><p>The Dutch company <a rel="noreferrer noopener" href="https://www.pal-v.com/en/" target="_blank">PAL-V</a> today announced that its Liberty flying car <a href="https://www.pal-v.com/en/press/worlds-first-flying-car-hits-the-road" target="_blank" rel="noreferrer noopener">has received permission</a> from the <a rel="noreferrer noopener" href="https://www.rdw.nl/over-rdw/information-in-english" target="_blank">Netherlands Vehicle Authority</a>&nbsp;to drive on public roads.<span data-ez-name="psychnewsdaily_com-medrectangle-3"></span></p><p>The PAL-V Liberty, the flying car’s full name, is a gyro-copter.&nbsp;The rotor can be folded so that the vehicle can also drive on the road like a regular car.&nbsp;</p><p>It needs a runway of between 180 – 330 meters for takeoff, but only 30 meters for landings.&nbsp;Both in the air and on the road, its maximum speed is 180 km/hour (112 mph). Converting from road to air mode (or vice versa) takes between five and ten minutes.</p><p>The Liberty runs on normal gasoline, and has a range of 1315 km (817 miles) on the road. In the air, it can fly 400 – 500 km (250 – 310 milles), and can remain airborne for 4.3 hours.</p><p>The PAL-V Liberty weighs 664 kg (1464 lbs) when empty. Its fuel tank holds 100 liters. The tank of a Honda Accord, just by way of comparison, holds about 53 liters.<span data-ez-name="psychnewsdaily_com-medrectangle-4"></span></p><h2>Already 30 orders for this Dutch flying car</h2><p>According to the company, about thirty Dutch residents have already ordered and paid for the Liberty. The list price is just under €500,000 ($587,000).</p><p>At the moment, the granted permission is only for a single vehicle. That means PAL-V cannot yet put their car into full production.&nbsp;The Netherlands Vehicle Authority first needs to ensure that the company can produce every vehicle according to the same quality standards.</p><p>PAL-V has been working on the Liberty since 2007. The European Aviation Safety Agency is still examining the company’s request to have the vehicles certified to fly. The company expects this permission to arrive in 2022.</p><p>The US state of New Hampshire <a href="https://www.timesnownews.com/auto/features/article/this-is-the-first-state-in-us-to-allow-flying-cars-on-public-roads/634621" target="_blank" rel="noreferrer noopener">made it legal to drive flying cars on public roads</a> in August of this year.<span data-ez-name="psychnewsdaily_com-box-4"></span></p><p>See a video of the PAL-V Liberty driving <a rel="noreferrer noopener" href="https://youtu.be/yIjSaEeO2l0" target="_blank">here</a>, and flying (briefly) <a rel="noreferrer noopener" href="https://youtu.be/fFW_0C7yFCI" target="_blank">here</a>.</p><figure><img loading="lazy" width="1024" height="512" src="https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty-1024x512.jpg" alt="" srcset="https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty-1024x512.jpg 1024w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty-300x150.jpg 300w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty-768x384.jpg 768w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty-1536x768.jpg 1536w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty-360x180.jpg 360w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty-1320x660.jpg 1320w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty.jpg 1560w" sizes="(max-width: 1024px) 100vw, 1024px"></figure><figure><img loading="lazy" width="1024" height="537" src="https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-flying-car-1024x537.jpg" alt="" srcset="https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-flying-car-1024x537.jpg 1024w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-flying-car-300x157.jpg 300w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-flying-car-768x403.jpg 768w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-flying-car-360x189.jpg 360w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-flying-car-1320x693.jpg 1320w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-flying-car.jpg 1408w" sizes="(max-width: 1024px) 100vw, 1024px"></figure><figure><img loading="lazy" width="1024" height="538" src="https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-fying-car-interior-1-1024x538.jpg" alt="" srcset="https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-fying-car-interior-1-1024x538.jpg 1024w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-fying-car-interior-1-300x158.jpg 300w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-fying-car-interior-1-768x403.jpg 768w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-fying-car-interior-1-360x189.jpg 360w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-fying-car-interior-1.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px"></figure><p>For a weekly summary of the latest psychology news, subscribe to our <a href="https://www.psychnewsdaily.com/the-psych-news-weekly-newsletter/" target="_blank" rel="noreferrer noopener">Psych News Weekly newsletter</a>.</p></div></div></div></article></div>]]>
            </description>
            <link>https://www.psychnewsdaily.com/dutch-flying-car-pal-v-gets-permission-to-drive-on-european-roads/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24917841</guid>
            <pubDate>Wed, 28 Oct 2020 12:17:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Are we losing our ability to remember?]]>
            </title>
            <description>
<![CDATA[
Score 232 | Comments 154 (<a href="https://news.ycombinator.com/item?id=24917721">thread link</a>) | @scotthtaylor
<br/>
October 28, 2020 | https://st.im/are-we-losing-our-ability-to-remember/ | <a href="https://web.archive.org/web/*/https://st.im/are-we-losing-our-ability-to-remember/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <p>The other day I had a bit of a crisis, I was worried that I was starting to have trouble with my memory. Something had to be wrong! I started to notice (increasingly!) my inability to recall trivial things; for example, the action points from a Zoom call, or a quote from a book that I had read a couple of months ago. Surely this can’t be normal?</p><p>Before calling the doctor’s office I did what any decent hypochondriac would do, and started googling. After clicking through a few pages, I began to feel a bit better. It was normal. Short term (or working) memory <em>is</em> inefficient, and unless I revisit the thing I’m trying to remember a few times, I’m most likely going to forget it. And no, it’s not a side effect of turning thirty. Phew.</p><p>It’s a “feature, not a bug” of how our memory systems are designed. </p><p>Our memory is made up of not one, nor two, but three components: 1) a sensory register, 2) working memory, and 3) long-term memory.</p><p>When I look back at my childhood or I remember some basic words from French, I'm drawing on portions of my brain involved in long-term memory. But when I'm trying to hold a few ideas in mind to connect them together so I can understand a concept or solve a problem, I'm using my working memory.</p><p>With my health crisis averted, I got thinking about technology and its impact (positive and negative) on the way the brain, and memory, function. With all the knowledge I could ever need at my fingertips, alongside note taking apps and the smartphones that are now an extension of our physical being -- am I being lazy or efficient, or a mixture of the two? Much like our overactive fight-or-flight response, has evolution not had a chance to adapt or catch-up with the mind of today versus our ancestors’? </p><h3 id="four-chunks-of-information">Four chunks of information</h3><p>We ‘can’t remember’ things because there is a limit to what we can hold in our working memory. Researchers used to think that it could hold around seven items or chunks, but now it’s widely believed that the working memory only holds about four chunks of information.</p><p>If you’re anything like me you’ll have to repeat something to yourself until you have a chance to write it down. Repetitions are needed so that natural dissipating processes don’t suck the memories away. How many times have you found yourself shutting your eyes to keep other things from intruding into the limited slots of your working memory as you concentrate?</p><p>I believe that we need to offload from our working memory as soon as possible.</p><p>With the goal historically being, before computers, to move it to long-term memory. If this didn’t happen -- you’d essentially be waving goodbye to that memory. </p><p>Moving a memory from ‘working’ to ‘long-term’ takes time and practice. </p><p>There’s a steep drop in what you remember, anyway. The ‘forgetting curve’, as it’s called, is steepest during the first twenty-four hours after you learn something. Exactly how much you forget, percentage-wise, varies, but unless you review the material, much of it slips down the drain. What you remember after day one has a good chance of still being retained after thirty. </p><h3 id="spaced-repetition">Spaced repetition</h3><p>To improve retention, spaced repetition is typically used. This technique involves repeating what you're trying to retain, ensuring to space the repetition out. Repeating a new vocabulary word or a problem solving technique for example over a number of days.</p><p>The good news is, our long-term memory has room for billions of items. In fact there can be so many items they can bury each other. It can be difficult for you to find the information you need unless you practice and repeat at least a few times. This allows the synoptic connections in the brain to form and strengthen into a lasting structure.</p><p>Long-term memory is important because it's where you store fundamental concepts and techniques that are often involved in whatever you're learning about.</p><p>Having strong foundations in your long-term memory also makes the working memory more efficient, and able to connect dots from wider, more abstract, fields. It gives our thinking ‘richness’ and ‘associative access’. </p><p>Richness refers to the theory that a large number of things we have apparently forgotten all about are still there, somewhere, and add depth to our thinking. Associative access means that your thoughts can be accessed in a number of different ways by semantic or perceptual associations  — memories can be triggered by related words, by category names, by a smell, an old song or photograph, or even seemingly random neural firings that bring them up to consciousness.</p><h3 id="offloading-memory">Offloading memory</h3><p>But now, of course, we don’t bother to do all of the hard work of committing many things to our long-term memory. We have devices -- and the internet -- to remember stuff for us.</p><p>When I think back to when I started journaling on my iPad and laptop, as well as using apps like <a href="https://obsidian.md/">Obsidian</a> that are focused on ‘<a href="https://st.im/ive-become-obsessed-with-networked-thought/">networked thought</a>’, it is interesting<strong> </strong>to hypothesise how they have potentially impacted the fundamental chemistry or feedback loops in my brain. </p><p>For me, they have helped reduce my cognitive load by letting me off-load much of what goes on in my brain to an external entity. In this day and age, this recall memory has become less necessary. Recognition memory is more important (i.e. the ability to judge that a currently present object, person, place, or event, has previously been encountered or experienced).</p><p>Research has shown that the internet functions as a sort of externalised memory. “When people expect to have future access to information, they have lower rates of recall of the information itself,” <a href="https://www.ncbi.nlm.nih.gov/pubmed/21764755">as one study puts it</a>. </p><p>If you know that you ‘know’ something, and you know how to retrieve it (thank you Google) that performs pretty much the same function as having a brain stuffed with lots of long-term memories. And the new ‘networked thought’ apps allow us to make interesting connections between these various bits of stored knowledge in much the same way that a well-stocked memory does.</p><h3 id="using-our-second-brain">Using our second brain </h3><p>So I wouldn’t say we are losing our ability to remember, as I posed at the start of this post. I think people (me included) just don’t do enough work to move stuff from our working memory into our long-term memory. </p><p>Our decreased reliance on recall memory and our &nbsp;ever-decreasing attention spans may not be the disaster that I first feared.</p><p>I think that the internet and apps focused on network thinking do assist us. They do act as a second brain. And I think that &nbsp;makes us more efficient.</p><hr><p>Cover photo by <a href="https://unsplash.com/@sarandywestfall_photo?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Sarandy Westfall</a> on <a href="https://unsplash.com/s/photos/memory?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a><br>Thanks to <a href="https://jonathangifford.com/">Jonathan</a> for reading through early drafts of this post.<br>For discussion, check out this post on <a href="https://news.ycombinator.com/item?id=24917721">Hacker News</a>. &nbsp;</p>
              <section>
                <h2>Enjoying these posts? Subscribe for more</h2>
                <a href="https://st.im/subscribe/">Subscribe now</a>
                <br>
                <a href="https://st.im/signin/">Already have an account? Sign in</a>
              </section>
  </div></div>]]>
            </description>
            <link>https://st.im/are-we-losing-our-ability-to-remember/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24917721</guid>
            <pubDate>Wed, 28 Oct 2020 11:58:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What SOFIA’s discovery of water on the Moon is, and isn’t]]>
            </title>
            <description>
<![CDATA[
Score 20 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24917107">thread link</a>) | @uncertainquark
<br/>
October 28, 2020 | https://jatan.space/on-sofias-discovery-of-water-on-the-moon/ | <a href="https://web.archive.org/web/*/https://jatan.space/on-sofias-discovery-of-water-on-the-moon/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
		
	<main id="content" role="main">

	<div>
		<div>
						<article id="post-5589">
				<div>
<p>NASA and the Germany space agency’s airborne SOFIA telescope has&nbsp;<a href="https://www.nasa.gov/press-release/nasa-s-sofia-discovers-water-on-sunlit-surface-of-moon/">detected water</a>&nbsp;on the Moon’s surface. There are many misconceptions floating around this discovery so I’d like to clarify the nature of the findings and what it means for lunar science and exploration.</p>



<div><figure><img loading="lazy" src="https://i1.wp.com/jatan.space/wp-content/uploads/2020/10/nasa-sofia-water-cover.jpg?resize=819%2C819&amp;ssl=1" alt="" width="819" height="819" srcset="https://i1.wp.com/jatan.space/wp-content/uploads/2020/10/nasa-sofia-water-cover.jpg?w=985&amp;ssl=1 985w, https://i1.wp.com/jatan.space/wp-content/uploads/2020/10/nasa-sofia-water-cover.jpg?resize=200%2C200&amp;ssl=1 200w, https://i1.wp.com/jatan.space/wp-content/uploads/2020/10/nasa-sofia-water-cover.jpg?resize=768%2C768&amp;ssl=1 768w, https://i1.wp.com/jatan.space/wp-content/uploads/2020/10/nasa-sofia-water-cover.jpg?resize=800%2C800&amp;ssl=1 800w, https://i1.wp.com/jatan.space/wp-content/uploads/2020/10/nasa-sofia-water-cover.jpg?resize=400%2C400&amp;ssl=1 400w" sizes="(max-width: 819px) 100vw, 819px" data-recalc-dims="1"><figcaption>This illustration highlights the Moon’s Clavius crater with an illustration depicting water trapped in its lunar soil, along with an image of NASA’s airborne SOFIA telescope that found such water. <a href="https://www.nasa.gov/press-release/nasa-s-sofia-discovers-water-on-sunlit-surface-of-moon/">Credits: NASA/Daniel Rutter</a></figcaption></figure></div>



<h2>Different from Chandrayaan 1</h2>



<p>SOFIA detected water on the Moon’s non-polar regions, and in areas of sunlight, making it different from ISRO Chandrayaan 1’s&nbsp;<a href="https://jatan.space/how-nasa-and-chandrayaan-discovered-water-on-the-moon/">discovery of water ice</a>&nbsp;inside dark, cold craters on the lunar poles.</p>



<p>The water SOFIA found is locked in the lunar soil and rocks. Interestingly, NASA’s M3 instrument on Chandrayaan 1 had&nbsp;<a href="https://web.archive.org/web/20120929095219/http://lunarscience.nasa.gov/wp-content/uploads/2012/08/38_Klima_NLSI_2012.pdf">seen hints</a>&nbsp;of such trapped water in non-polar regions, <a href="https://www.nasa.gov/topics/moonmars/features/clark1.html">as did Cassini</a>. But unlike SOFIA, those missions couldn’t tell if what they had detected was water or just hydroxyl groups (H2O vs OH).</p>



<p>Chandrayaan 1 dropped an impact probe on the Moon 12 years ago and did&nbsp;<a href="http://www.planetary.org/blogs/emily-lakdawalla/2010/2430.html">find water</a>, as in H2O. But it too is different from SOFIA’s findings because the probe detected water in the Moon’s thin atmosphere, not on the surface.</p>



<h2>Scientific not exploratory importance</h2>



<p>NASA PR spinned the announcement as the water being a promising source for future missions in creating sustainable habitats. But that’s not the case since this trapped water is in trace amounts, less than even the driest deserts on Earth. However, finding out where this water comes from or how it’s created has implications for understanding&nbsp;<a href="https://jatan.space/apollo-moon-origin/">the Moon’s origin</a>, which itself is tied to Earth’s.</p>



<p>The one area where the results have some exploratory relevance is understanding how water is transported on the Moon, so as to get a better handle on where on the Moon such resources are deposited. But the substantially more water ice present on the lunar poles continue to be the prime target for enabling sustainable human presence on the Moon.</p>



<div><figure><img loading="lazy" width="1200" height="675" src="https://i0.wp.com/jatan.space/wp-content/uploads/2020/01/3cc0f-water-ice-moon-map-m3.jpg?resize=1200%2C675&amp;ssl=1" alt="" data-recalc-dims="1"><figcaption>Illustration showing water ice in eternally dark craters on the Moon’s south pole (left) and north pole (right) as discovered by ISRO’s Chandrayaan 1 spacecraft.&nbsp;<a href="https://www.jpl.nasa.gov/news/news.php?feature=7218">Credit: NASA</a></figcaption></figure></div>



<p>Alongside the SOFIA findings, NASA mentioned&nbsp;<a href="https://www.nature.com/articles/s41550-020-1198-9#_blank">another result</a>&nbsp;that does have exploratory significance. Researchers using data from NASA’s Lunar Reconnaissance Orbiter have identified several craters smaller than a kilometer which are eternally dark polar and can thus host water ice. The sheer number of such small craters increases the expected amount of water ice on the Moon.</p>



<p>As per previous observations by the Chandrayaan 1 orbiter and the Lunar Reconnaissance Orbiter, scientists estimated the Moon’s poles to host&nbsp;<a href="https://www.nasa.gov/mission_pages/Mini-RF/multimedia/feature_ice_like_deposits.html">more than 600 billion kg</a>&nbsp;of water ice, enough to fill at least 240,000 Olympic-sized swimming pools. As the next logical step, Chandrayaan 2 orbiter is&nbsp;<a href="https://jatan.space/chandrayaan-2-is-creating-the-highest-resolution-map-of-the-moon/">quantifying the amount of water ice</a>&nbsp;on the lunar poles and mapping it as we speak, and its findings should include these smaller craters.</p>



<p>Once we have such data, these smaller, dark craters will likely be better targets for future missions as they’d be easier to explore and extract water from than large craters like Shackleton.</p>



<p><strong>Like my work?</strong><br>I could write this article thanks to my readers. To keep me going, <a href="https://jatan.space/support">support me</a> and get exclusive benefits in return.&nbsp;🚀</p>

</div>

			</article>
					</div>
	</div>
</main><!--/.neve-main-->




</div></div>]]>
            </description>
            <link>https://jatan.space/on-sofias-discovery-of-water-on-the-moon/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24917107</guid>
            <pubDate>Wed, 28 Oct 2020 10:12:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Markov Chain Monte Carlo (MCMC) Sampling, Part 1: The Basics (2019)]]>
            </title>
            <description>
<![CDATA[
Score 107 | Comments 16 (<a href="https://news.ycombinator.com/item?id=24917011">thread link</a>) | @vonadz
<br/>
October 28, 2020 | https://www.tweag.io/blog/2019-10-25-mcmc-intro1/ | <a href="https://web.archive.org/web/*/https://www.tweag.io/blog/2019-10-25-mcmc-intro1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><p>This is part 1 of a series of blog posts about MCMC techniques:</p>
<ul>
<li><a href="https://www.tweag.io/posts/2020-01-09-mcmc-intro2.html">Part II: Gibbs sampling</a></li>
<li><a href="https://www.tweag.io/blog/2020-08-06-mcmc-intro3/">Part III: Hamiltonian Monte Carlo</a></li>
<li><a href="https://www.tweag.io/blog/2020-10-28-mcmc-intro-4/">Part IV: Replica Exchange</a></li>
</ul>
<p><a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo</a> (MCMC) is a powerful class of methods to sample from probability distributions known only up to an (unknown) normalization constant. But before we dive into MCMC, let’s consider why you might want to do sampling in the first place.</p>
<p>The answer to that is: whenever you’re either interested in the samples themselves (for example, inferring unknown parameters in Bayesian inference) or you need them to approximate expected values of functions w.r.t. to a probability distribution (for example, calculating thermodynamic quantities from the distribution of microstates in statistical physics).
Sometimes, only the mode of a probability distribution is of primary interest. In this case, it’s obtained by numerical optimization so full sampling is not necessary.</p>
<p>It turns out that sampling from any but the most basic probability distributions is a difficult task.
<a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">Inverse transform sampling</a> is an elementary method to sample from probability distributions, but requires the cumulative distribution function, which in turn requires knowledge of the, generally unknown, normalization constant.
Now in principle, you could just obtain the normalization constant by numerical integration, but this quickly gets infeasible with an increasing number of dimensions.
<a href="https://en.wikipedia.org/wiki/Rejection_sampling">Rejection sampling</a> does not require a normalized distribution, but efficiently implementing it requires a good deal of knowledge about the distribution of interest, and it suffers strongly from the curse of dimension, meaning that its efficiency decreases rapidly with an increasing number of variables.
That’s when you need a smart way to obtain representative samples from your distribution which doesn’t require knowledge of the normalization constant.</p>
<p>MCMC algorithms are a class of methods which do exactly that.
These methods date back to a <a href="https://pdfs.semanticscholar.org/7b3d/c9438227f747e770a6fb6d7d7c01d98725d6.pdf">seminal paper by Metropolis et al.</a>, who developed the first MCMC algorithm, correspondingly called <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis algorithm</a>, to calculate the equation of state of a two-dimensional system of hard spheres. In reality, they were looking for a general method to calculate expected values occurring in statistical physics.</p>
<p>In this blog post, I introduce the basics of MCMC sampling; in subsequent posts I’ll cover several important, increasingly complex and powerful MCMC algorithms, which all address different difficulties one frequently faces when using the Metropolis-Hastings algorithm. Along the way, you will gain a solid understanding of these challenges and how to address them.
Also, this serves as a reference for MCMC methods in the context of the <a href="https://www.tweag.io/posts/2019-09-20-monad-bayes-1.html">monad-bayes</a> series.
Furthermore, I hope the provided notebooks will not only spark your interest in exploring the behavior of MCMC algorithms for various parameters/probability distributions, but also serve as a basis for implementing and understanding useful extensions of the basic versions of the algorithms I present.</p>
<h2>Markov chains</h2>
<p>Now that we know why we want to sample, let’s get to the heart of MCMC: Markov chains.
What is a Markov chain? Without all the technical details, a Markov chain is a random sequence of states in some state space in which the probability of picking a certain state next depends only on the current state in the chain and not on the previous history: it is memory-less.
Under certain conditions, a Markov chain has a unique stationary distribution of states to which it converges after a certain number of states. From that number on, states in the Markov chain are distributed according to the invariant distribution.</p>
<p>In order to sample from a distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi(x)</annotation></semantics></math></span></span>, a MCMC algorithm constructs and simulates a Markov chain whose stationary distribution is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi(x)</annotation></semantics></math></span></span>, meaning that, after an initial “burn-in” phase, the states of that Markov chain are distributed according to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi(x)</annotation></semantics></math></span></span>.
We thus just have to store the states to obtain samples from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi(x)</annotation></semantics></math></span></span>.</p>
<p>For didactic purposes, let’s for now consider both a discrete state space and discrete “time”.
The key quantity characterizing a Markov chain is the transition operator <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">T(x_{i+1}|x_i)</annotation></semantics></math></span></span> which gives you the probability of being in state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{i+1}</annotation></semantics></math></span></span> at time <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">i+1</annotation></semantics></math></span></span> given that the chain is in state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span></span> at time <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></span>.</p>
<p>Now just for fun (and for illustration), let’s quickly whip up a Markov chain which has a unique stationary distribution.
We’ll start with some imports and settings for the plots:</p>
<div data-language="python"><pre><code><span>%</span>matplotlib notebook
<span>%</span>matplotlib inline
<span>import</span> numpy <span>as</span> np
<span>import</span> matplotlib<span>.</span>pyplot <span>as</span> plt
plt<span>.</span>rcParams<span>[</span><span>'figure.figsize'</span><span>]</span> <span>=</span> <span>[</span><span>10</span><span>,</span> <span>6</span><span>]</span>
np<span>.</span>random<span>.</span>seed<span>(</span><span>42</span><span>)</span></code></pre></div>
<p>The Markov chain will hop around on a discrete state space which is made up from three weather states:</p>
<div data-language="python"><pre><code>state_space <span>=</span> <span>(</span><span>"sunny"</span><span>,</span> <span>"cloudy"</span><span>,</span> <span>"rainy"</span><span>)</span></code></pre></div>
<p>In a discrete state space, the transition operator is just a matrix.
Columns and rows correspond, in our case, to sunny, cloudy, and rainy weather.
We pick more or less sensible values for all transition probabilities:</p>
<div data-language="python"><pre><code>transition_matrix <span>=</span> np<span>.</span>array<span>(</span><span>(</span><span>(</span><span>0.6</span><span>,</span> <span>0.3</span><span>,</span> <span>0.1</span><span>)</span><span>,</span>
                              <span>(</span><span>0.3</span><span>,</span> <span>0.4</span><span>,</span> <span>0.3</span><span>)</span><span>,</span>
                              <span>(</span><span>0.2</span><span>,</span> <span>0.3</span><span>,</span> <span>0.5</span><span>)</span><span>)</span><span>)</span></code></pre></div>
<p>The rows indicate the states the chain might currently be in and the columns the states the chains might transition to.
If we take one “time” step of the Markov chain as one hour, then, if it’s sunny, there’s a 60% chance it stays sunny in the next hour, a 30% chance that in the next hour we will have cloudy weather, and only a 10% chance of rain immediately after it had been sunny before.
This also means that each row has to sum up to one.</p>
<p>Let’s run our Markov chain for a while:</p>
<div data-language="python"><pre><code>n_steps <span>=</span> <span>20000</span>
states <span>=</span> <span>[</span><span>0</span><span>]</span>
<span>for</span> i <span>in</span> <span>range</span><span>(</span>n_steps<span>)</span><span>:</span>
    states<span>.</span>append<span>(</span>np<span>.</span>random<span>.</span>choice<span>(</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>)</span><span>,</span> p<span>=</span>transition_matrix<span>[</span>states<span>[</span><span>-</span><span>1</span><span>]</span><span>]</span><span>)</span><span>)</span>
states <span>=</span> np<span>.</span>array<span>(</span>states<span>)</span></code></pre></div>
<p>We can monitor the convergence of our Markov chain to its stationary distribution by calculating the empirical probability for each of the states as a function of chain length:</p>
<div data-language="python"><pre><code><span>def</span> <span>despine</span><span>(</span>ax<span>,</span> spines<span>=</span><span>(</span><span>'top'</span><span>,</span> <span>'left'</span><span>,</span> <span>'right'</span><span>)</span><span>)</span><span>:</span>
    <span>for</span> spine <span>in</span> spines<span>:</span>
        ax<span>.</span>spines<span>[</span>spine<span>]</span><span>.</span>set_visible<span>(</span><span>False</span><span>)</span>

fig<span>,</span> ax <span>=</span> plt<span>.</span>subplots<span>(</span><span>)</span>
width <span>=</span> <span>1000</span>
offsets <span>=</span> <span>range</span><span>(</span><span>1</span><span>,</span> n_steps<span>,</span> <span>5</span><span>)</span>
<span>for</span> i<span>,</span> label <span>in</span> <span>enumerate</span><span>(</span>state_space<span>)</span><span>:</span>
    ax<span>.</span>plot<span>(</span>offsets<span>,</span> <span>[</span>np<span>.</span><span>sum</span><span>(</span>states<span>[</span><span>:</span>offset<span>]</span> <span>==</span> i<span>)</span> <span>/</span> offset
            <span>for</span> offset <span>in</span> offsets<span>]</span><span>,</span> label<span>=</span>label<span>)</span>
ax<span>.</span>set_xlabel<span>(</span><span>"number of steps"</span><span>)</span>
ax<span>.</span>set_ylabel<span>(</span><span>"likelihood"</span><span>)</span>
ax<span>.</span>legend<span>(</span>frameon<span>=</span><span>False</span><span>)</span>
despine<span>(</span>ax<span>,</span> <span>(</span><span>'top'</span><span>,</span> <span>'right'</span><span>)</span><span>)</span>
plt<span>.</span>show<span>(</span><span>)</span></code></pre></div>
<p><span>
      <a href="https://www.tweag.io/static/970ffb50b65a63406121b8ed07f5e4b1/d0d8c/mcmc-intro1-weatherchain.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="png" title="png" src="https://www.tweag.io/static/970ffb50b65a63406121b8ed07f5e4b1/fcda8/mcmc-intro1-weatherchain.png" srcset="https://www.tweag.io/static/970ffb50b65a63406121b8ed07f5e4b1/12f09/mcmc-intro1-weatherchain.png 148w,
https://www.tweag.io/static/970ffb50b65a63406121b8ed07f5e4b1/e4a3f/mcmc-intro1-weatherchain.png 295w,
https://www.tweag.io/static/970ffb50b65a63406121b8ed07f5e4b1/fcda8/mcmc-intro1-weatherchain.png 590w,
https://www.tweag.io/static/970ffb50b65a63406121b8ed07f5e4b1/d0d8c/mcmc-intro1-weatherchain.png 609w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<h2>The mother of all MCMC algorithms: Metropolis-Hastings</h2>
<p>So that’s lots of fun, but back to sampling an arbitrary probability distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span></span>.
It could either be discrete, in which case we would keep talking about a transition matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span></span>, or be continuous, in which case <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span></span> would be a transition <em>kernel</em>.
From now on, we’re considering continuous distributions, but all concepts presented here transfer to the discrete case.</p>
<p>If we could design the transition kernel in such a way that the next state is already drawn from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span></span>, we would be done, as our Markov chain would… well… immediately sample from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span></span>.
Unfortunately, to do this, we need to be able to sample from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span></span>, which we can’t.
Otherwise you wouldn’t be reading this, right?</p>
<p>A way around this is to split the transition kernel <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">T(x_{i+1}|x_i)</annotation></semantics></math></span></span> into two parts:
a proposal step and an acceptance/rejection step.
The proposal step features a proposal distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q(x_{i+1}|x_i)</annotation></semantics></math></span></span>, from which we can sample possible next states of the chain.
In addition to being able to sample from it, we can choose this distribution arbitrarily. But, one should strive to design it such that samples from it are both as little correlated with the current state as possible and have a good chance of being accepted in the acceptance step.
Said acceptance/rejection step is the second part of the transition kernel and corrects for the error introduced by proposal states drawn from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo mathvariant="normal">≠</mo><mi>π</mi></mrow><annotation encoding="application/x-tex">q \neq \pi</annotation></semantics></math></span></span>.
It involves calculating an acceptance probability <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_\mathrm{acc}(x_{i+1}|x_i)</annotation></semantics></math></span></span> and accepting the proposal <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{i+1}</annotation></semantics></math></span></span> with that probability as the next state in the chain.
Drawing the next state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{i+1}</annotation></semantics></math></span></span> from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">T(x_{i+1}|x_i)</annotation></semantics></math></span></span> is then done as follows:
first, a proposal state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{i+1}</annotation></semantics></math></span></span> is drawn from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q(x_{i+1}|x_i)</annotation></semantics></math></span></span>.
It is then accepted as the next state with probability <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_\mathrm{acc}(x_{i+1}|x_i)</annotation></semantics></math></span></span> or rejected with probability <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">1-p_\mathrm{acc}(x\_{i+1}|x_i)</annotation></semantics></math></span></span>, in which case the current state is copied as the next state.</p>
<p>We thus have</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>×</mo><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mtext>&nbsp;.</mtext></mrow><annotation encoding="application/x-tex">T(x_{i+1}|x_i)=q(x_{i+1} | x_i) \times p_\mathrm{acc}(x_{i+1}|x_i) \ \text .</annotation></semantics></math></span></span></span></p><p>A sufficient condition for a Markov chain to have <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span></span> as its stationary distribution is the transition kernel obeying <em>detailed balance</em> or, in the physics literature, <em>microscopic reversibility</em>:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi>T</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>π</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mi>T</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi(x_i) T(x_{i+1}|x_i) = \pi(x_{i+1}) T(x_i|x_{i+1})</annotation></semantics></math></span></span></span></p><p>This means that the probability of being in a state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span></span> and transitioning to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{i+1}</annotation></semantics></math></span></span> must be equal to the probability of the reverse process, namely, being in state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">x\_{i+1}</annotation></semantics></math></span></span> and transitioning to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span></span>.
Transition kernels of most MCMC algorithms satisfy this condition.</p>
<p>For the two-part transition kernel to obey detailed balance, we need to choose <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub></mrow><annotation encoding="application/x-tex">p_\mathrm{acc}</annotation></semantics></math></span></span> correctly, meaning that is has to correct for any asymmetries in probability flow from / to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{i+1}</annotation></semantics></math></span></span> or <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span></span>.
Metropolis-Hastings uses the Metropolis acceptance criterion:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mrow><mrow><mo fence="true">{</mo><mn>1</mn><mo separator="true">,</mo><mfrac><mrow><mi>π</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>×</mo><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><mi>π</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>×</mo><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mo fence="true">}</mo></mrow><mtext>&nbsp;.</mtext></mrow><annotation encoding="application/x-tex">p_\mathrm{acc}(x_{i+1}|x_i) = \mathrm{min} \left\{1, \frac{\pi(x_{i+1}) \times q(x_i|x_{i+1})}{\pi(x_i) \times q(x_{i+1}|x_i)} \right\} \ \text .</annotation></semantics></math></span></span></span></p><p>Now here’s where the magic happens:
we know <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span></span> only up to a constant, but it doesn’t matter, because that unknown constant cancels out in the expression for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub></mrow><annotation encoding="application/x-tex">p_\mathrm{acc}</annotation></semantics></math></span></span>!
It is this property of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub></mrow><annotation encoding="application/x-tex">p_\mathrm{acc}</annotation></semantics></math></span></span> which makes algorithms based on Metropolis-Hastings work for unnormalized distributions.
Often, symmetric proposal distributions with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mi>q</mi><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q(x_i|x_{i+1})=q(x\_{i+1}|x_i)</annotation></semantics></math></span></span> are used, in which case the Metropolis-Hastings algorithm reduces to the original, but less general Metropolis algorithm developed in 1953 and for which</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mrow><mrow><mo fence="true">{</mo><mn>1</mn><mo separator="true">,</mo><mfrac><mrow><mi>π</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><mi>π</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mo fence="true">}</mo></mrow><mtext>&nbsp;.</mtext></mrow><annotation encoding="application/x-tex">p_\mathrm{acc}(x_{i+1}|x_i) = \mathrm{min} \left\{1, \frac{\pi(x_{i+1})}{\pi(x_i)} \right\} \ \text .</annotation></semantics></math></span></span></span></p><p>We can then write the complete Metropolis-Hastings transition kernel as</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>×</mo><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>:</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo mathvariant="normal">≠</mo><msub><mi>x</mi><mi>i</mi></msub><mtext>;</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>1</mn><mo>−</mo><mo>∫</mo><mi mathvariant="normal">d</mi><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mtext>&nbsp;</mtext><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>×</mo><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>:</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>x</mi><mi>i</mi></msub><mtext>.</mtext></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">T(x_{i+1}|x_i) = \begin{cases}
                   q(x_{i+1}|x_i) \times p_\mathrm{acc}(x_{i+1}|x_i) &amp;: x_{i+1} \neq x_i \text ; \\\\
                   1 - \int \mathrm{d}x_{i+1} \ q(x_{i+1}|x_i) \times p_\mathrm{acc}(x_{i+1}|x_i) &amp;: x_{i+1} = x_i\text .
                 \end{cases}</annotation></semantics></math></span></span></span></p><h2>Implementing the Metropolis-Hastings algorithm in Python</h2>
<p>All right, now that we know how Metropolis-Hastings works, let’s go ahead and implement it.
First, we set the log-probability of the distribution we want to sample from—without normalization constants, as we pretend we don’t know them. Let’s work for now with a standard normal distribution:</p>
<div data-language="python"><pre><code><span>def</span> <span>log_prob</span><span>(</span>x<span>)</span><span>:</span>
     <span>return</span> <span>-</span><span>0.5</span> <span>*</span> np<span>.</span><span>sum</span><span>(</span>x <span>**</span> <span>2</span><span>)</span></code></pre></div>
<p>Next, we choose a symmetric proposal distribution. Generally, including information you have about the distribution …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.tweag.io/blog/2019-10-25-mcmc-intro1/">https://www.tweag.io/blog/2019-10-25-mcmc-intro1/</a></em></p>]]>
            </description>
            <link>https://www.tweag.io/blog/2019-10-25-mcmc-intro1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24917011</guid>
            <pubDate>Wed, 28 Oct 2020 09:55:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Draw on a PDF Online]]>
            </title>
            <description>
<![CDATA[
Score 40 | Comments 24 (<a href="https://news.ycombinator.com/item?id=24916721">thread link</a>) | @perrys
<br/>
October 28, 2020 | https://www.goodannotations.com/tools/draw-on-pdf | <a href="https://web.archive.org/web/*/https://www.goodannotations.com/tools/draw-on-pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><h2>Replace pen and paper by drawing on PDF online</h2><p>Pen and papers are slow and inefficient. Leave the slow school tools in the past, quickly jot down, circle, and draw pointers on PDF documents. Draw your thoughts and markups on the screen as fast as they come to mind. The free pdf editor lets you use any stylus to draw on PDF online better and more effectively. Simply pick the right tool and the right color and draw online. </p><h2>Avoid friction and draw directly on your PDF document</h2><p>Less friction means more accomplishment. Taking your PDFs online and drawing with the right tools is fast, convenient, and easy to use. Don’t waste time with bulky software and confusing tool manuals. Focus on the message, not the technicals, and draw on PDF like a professional. </p><h2>Draw with a better markup and annotation PDF editor </h2><p>Take the lead on your project and stay in charge. Indicate and suggest what should be changed, improved, and deleted from any PDF document. Circle the problematic text, draw a pointer, and explain why that part of the PDF document should get more attention than the rest. Your team members will appreciate better digital communication.</p><h2>Deliver product guides and how-to material in PDF</h2><p>Create product guides that look and feel professional and trustworthy. Nothing says we’re a serious project better than a nicely done PDF catalog of how-to product guides. Take screenshots of your digital product, and draw on PDF online. Simply choose the PDF option when you download the file or share it with a link.</p><h2>Most PDF tools don’t let you draw in PDF documents </h2><p>Regular PDF software lets you only open and read the file. And even when you find an inexpensive and reasonably understandable pdf editor, you probably can’t add pointers, circles, squares, and other elements. Avoid the trouble, and upload your PDF to the best online PDF editor and simply jot the details down on PDF.</p><h2>Delete, change or reverse any PDF drawing online</h2><p>Never lose track of your work by having to start all over again after a careless mistake. Click on the bent arrow to undo any drawings on your document, and try again without damaging your design. Backspace and the trash bin icon will help you delete a specific element. Use the pointer to select a troublesome drawing and remove it with one click on the trash bin icon.</p><h2>Draw on PDF from any internet devices and gadget</h2><p>Tablets, mobile phones, and MacBooks are all fantastic devices to draw on PDF online. As long as you have an internet connection, you can draw your stylus, fingers, trackpad, and mouse on any PDF file. Jot down your ideas on the move, and share them digitally from anywhere and at any time. </p><h2>Draw for free with no trial accounts and data retention</h2><div><p>Don’t start a free month because you don’t need to pay to edit PDF documents. Enjoy smooth and frictionless PDF drawings wholly free and online. You can transform your PDF files into professional-looking project guides and give classy feedback to other project members with Good Annotations.</p><p>We’re thinking about premium features for different tools. Come back soon to discover optimized teamwork, collaborative editing, and organized file libraries. </p></div></div></div></div></div>]]>
            </description>
            <link>https://www.goodannotations.com/tools/draw-on-pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24916721</guid>
            <pubDate>Wed, 28 Oct 2020 09:04:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Emerging JavaScript pattern: multiple return values]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24916683">thread link</a>) | @loige
<br/>
October 28, 2020 | https://loige.co/emerging-javascript-pattern-multiple-return-values | <a href="https://web.archive.org/web/*/https://loige.co/emerging-javascript-pattern-multiple-return-values">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><p>In this article, I want to explore an interesting pattern that I am seeing more and more in JavaScript code which allows you to return multiple values from a function.</p>
<p>You probably know already that JavaScript does not support multiple return values natively, so this article will actually explore some ways to “simulate” this behavior.</p>
<p>One of the most famous usages of this pattern I have seen recently is within <a href="https://reactjs.org/docs/hooks-overview.html">React Hooks</a>, but before delving into that, let’s see what I mean with “multiple return values” by exploring this concept in other languages.</p>
<h2 id="multiple-return-values-in-other-languages"><a href="#multiple-return-values-in-other-languages" aria-label="multiple return values in other languages permalink"></a>Multiple return values in other languages</h2>
<p>Two languages that come to my mind which natively support multiple return values are Lua and Go. Let’s implement a simple <em>integer division</em> function that returns both the <em>quotient</em> and the <em>remainder</em>.</p>
<h3 id="lua"><a href="#lua" aria-label="lua permalink"></a>Lua</h3>
<p>Let’s start with a simple implementation in Lua. It’s definitely worth mentioning that <a href="https://www.lua.org/pil/5.1.html">Lua’s official documentation</a> defines multiple return values as <em>“An unconventional, but quite convenient feature”</em>:</p>
<div data-language="lua"><pre><code><span>function</span> <span>intDiv</span> <span>(</span>dividend<span>,</span> divisor<span>)</span>
  <span>local</span> quotient <span>=</span> math<span>.</span><span>floor</span><span>(</span>dividend <span>/</span> divisor<span>)</span>
  <span>local</span> remainder <span>=</span> dividend <span>%</span> divisor
  <span>return</span> quotient<span>,</span> remainder
<span>end</span>

<span>print</span><span>(</span><span>intDiv</span><span>(</span><span>10</span><span>,</span><span>3</span><span>)</span><span>)</span> </code></pre></div>
<h3 id="go"><a href="#go" aria-label="go permalink"></a>Go</h3>
<p>Here’s some equivalent code in Go:</p>
<div data-language="go"><pre><code><span>package</span> main

<span>import</span> <span>"fmt"</span>

<span>func</span> <span>intDiv</span><span>(</span>dividend<span>,</span> divisor <span>int</span><span>)</span> <span>(</span><span>int</span><span>,</span> <span>int</span><span>)</span> <span>{</span>
  quotient <span>:=</span> dividend <span>/</span> divisor
  remainder <span>:=</span> dividend <span>%</span> divisor
  <span>return</span> quotient<span>,</span> remainder
<span>}</span>

<span>func</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  fmt<span>.</span><span>Println</span><span>(</span><span>intDiv</span><span>(</span><span>10</span><span>,</span><span>3</span><span>)</span><span>)</span> 
<span>}</span></code></pre></div>
<p>As you can see in these 2 code snippets, functions can return more than 1 value and this can be very convenient in cases where you logically have produce multiple outputs in a computation.</p>
<p><strong>Note</strong>: a more realistic implementation in Go, would take into account errors (e.g. division by 0) and add an extra return value to propagate potential errors. We shouldn’t worry too much about this for the sake of this article, but it is definitely worth mentioning that multiple return values in Go shine when it comes to error propagation and error handling. We will touch a bit more on this later in this article to see how this idea can be applied to JavaScript as well, especially in the context of Async/Await.</p>
<h2 id="simulating-multiple-return-values-in-javascript"><a href="#simulating-multiple-return-values-in-javascript" aria-label="simulating multiple return values in javascript permalink"></a>Simulating multiple return values in JavaScript</h2>
<p>So, as we said early on, JavaScript does not natively support a syntax to return more than one value from a function. We can workaround this limitation by using <em>composite values</em> like arrays or objects.</p>
<h3 id="multiple-return-values-with-arrays"><a href="#multiple-return-values-with-arrays" aria-label="multiple return values with arrays permalink"></a>Multiple return values with arrays</h3>
<p>Let’s implement our <code>intDiv</code> in JavaScript by using arrays as return types:</p>
<div data-language="javascript"><pre><code><span>intDiv</span> <span>=</span> <span>(</span><span>dividend<span>,</span> divisor</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> quotient <span>=</span> Math<span>.</span><span>floor</span><span>(</span>dividend <span>/</span> divisor<span>)</span>
  <span>const</span> remainder <span>=</span> dividend <span>%</span> divisor
  <span>return</span> <span>[</span>quotient<span>,</span> remainder<span>]</span>
<span>}</span>

console<span>.</span><span>log</span><span>(</span><span>intDiv</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>)</span><span>)</span> </code></pre></div>
<p>Here we are just printing the result of a division, but let’s assume we want to handle the two return values individually, how do we <em>reference</em> those?</p>
<p>Well, the return value is an array so we can simply access the two elements in the array using the indices <code>0</code> and <code>1</code>:</p>
<div data-language="javascript"><pre><code><span>const</span> result <span>=</span> <span>intDiv</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>)</span>
<span>const</span> quotient <span>=</span> result<span>[</span><span>0</span><span>]</span>
<span>const</span> remainder <span>=</span> result<span>[</span><span>1</span><span>]</span>
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Quotient = </span><span><span>${</span>quotient<span>}</span></span><span>`</span></span><span>)</span> 
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Remainder = </span><span><span>${</span>remainder<span>}</span></span><span>`</span></span><span>)</span> </code></pre></div>
<p>This syntax is arguably verbose and definitely not very elegant. Thankfully, <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment">ES2015 array destructuring assignment</a> can help us here:</p>
<div data-language="javascript"><pre><code><span>const</span> <span>[</span>quotient<span>,</span> remainder<span>]</span> <span>=</span> <span>intDiv</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>)</span>
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Quotient = </span><span><span>${</span>quotient<span>}</span></span><span>`</span></span><span>)</span> 
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Remainder = </span><span><span>${</span>remainder<span>}</span></span><span>`</span></span><span>)</span> </code></pre></div>
<p>This is much nicer to read and we also trimmed away 2 out 3 lines of code, big win!</p>
<p>As nice as it is, this implementation has an important shortcoming: return values are positional, so you need to be careful and respect the order while destructuring.</p>
<h3 id="multiple-return-values-with-objects"><a href="#multiple-return-values-with-objects" aria-label="multiple return values with objects permalink"></a>Multiple return values with objects</h3>
<p>An alternative implementation could use objects as return value, let’s see how:</p>
<div data-language="javascript"><pre><code><span>intDiv</span> <span>=</span> <span>(</span><span>dividend<span>,</span> divisor</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> quotient <span>=</span> Math<span>.</span><span>floor</span><span>(</span>dividend <span>/</span> divisor<span>)</span>
  <span>const</span> remainder <span>=</span> dividend <span>%</span> divisor
  <span>return</span> <span>{</span> quotient<span>,</span> remainder <span>}</span>
<span>}</span></code></pre></div>
<p>Note that here we are using another syntactic sugar from ES2015 (Enhanced object literal syntax) that allows us to define objects very concisely. Prior to ES2015, we would have defined the return statement as <code>{quotient: quotient, remainder: remainder}</code>.</p>
<p>With this approach we will be able to use our <code>intDiv</code> function as follows:</p>
<div data-language="javascript"><pre><code><span>const</span> result <span>=</span> <span>intDiv</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>)</span>
<span>const</span> quotient <span>=</span> result<span>.</span>quotient
<span>const</span> remainder <span>=</span> result<span>.</span>remainder
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Quotient = </span><span><span>${</span>quotient<span>}</span></span><span>`</span></span><span>)</span> 
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Remainder = </span><span><span>${</span>remainder<span>}</span></span><span>`</span></span><span>)</span> </code></pre></div>
<p>Again, this is a bit too verbose and ES2015 has another fantastic syntactic sugar to make this nicer:</p>
<div data-language="javascript"><pre><code><span>const</span> <span>{</span> quotient<span>,</span> remainder <span>}</span> <span>=</span> <span>intDiv</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>)</span>
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Quotient = </span><span><span>${</span>quotient<span>}</span></span><span>`</span></span><span>)</span> 
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Remainder = </span><span><span>${</span>remainder<span>}</span></span><span>`</span></span><span>)</span> </code></pre></div>
<p>This syntactic sugar is called <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment#Object_destructuring">Object Destructuring Assignment</a>. With this approach we are now independent from the position of return values (we can swap the position of <code>quotient</code> and <code>remainder</code> without side effects). This syntax also lets you rename the destructured variables, which can very useful to avoid name collisions with other local variables, or just to make variable names shorter or more descriptive as we please. Let’s see how this works:</p>
<div data-language="javascript"><pre><code><span>const</span> <span>{</span> remainder<span>:</span> r<span>,</span> quotient<span>:</span> q <span>}</span> <span>=</span> <span>intDiv</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>)</span>
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Quotient = </span><span><span>${</span>q<span>}</span></span><span>`</span></span><span>)</span> 
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Remainder = </span><span><span>${</span>r<span>}</span></span><span>`</span></span><span>)</span> </code></pre></div>
<p>Here we are not dependent by values position, but by their names in the returned object. If you are designing an API with multiple return values, it’s up to you to figure out which trade off will be the best to guarantee a proper developer experience.</p>
<p>Ok, now you should have a good idea on how to simulate multiple return values in JavaScript. In the next section we will see some more realistic examples that take advantage of this pattern.</p>
<h2 id="some-more-realistic-use-cases"><a href="#some-more-realistic-use-cases" aria-label="some more realistic use cases permalink"></a>Some more realistic use cases</h2>
<p>As mentioned early on, this technique has been recently popularized by React hooks, so we are gonna explore this use case first. Later we will see other two cases related to Async/Await.</p>
<h3 id="react-hooks"><a href="#react-hooks" aria-label="react hooks permalink"></a>React Hooks</h3>
<p>React hooks are a <a href="https://reactjs.org/docs/hooks-overview.html">new feature proposal</a> available from <em>React v16.7.0-alpha</em> that lets you use state and other React features without having to write a class.</p>
<p>The first and most famous React hook present is called <strong>State Hook</strong>.</p>
<p>Let’s see how it works with an example, let’s build a CSS color viewer component.</p>
<p>Here’s how our component is going to look like:</p>
<p><img src="https://loige.co/content/4c8fe2a31135971902611822ca2f3df2/css-color-viewer-demo.gif" alt="CssColorViewer React component demo"></p>
<p>And here’s the code used to implement this:</p>
<div data-language="javascript"><pre><code><span>import</span> <span>{</span> useState <span>}</span> <span>from</span> <span>'react'</span>

<span>function</span> <span>CssColorViewer</span><span>(</span><span>)</span> <span>{</span>
  <span>const</span> <span>[</span>cssColor<span>,</span> setCssColor<span>]</span> <span>=</span> <span>useState</span><span>(</span><span>'Blue'</span><span>)</span>
  

  <span>const</span> <span>onCssColorChange</span> <span>=</span> <span>e</span> <span>=&gt;</span> <span>{</span>
    <span>setCssColor</span><span>(</span>e<span>.</span>target<span>.</span>value<span>)</span>
  <span>}</span>

  <span>return</span> <span>(</span>
    <span>&lt;</span>div<span>&gt;</span>
      <span>&lt;</span>input value<span>=</span><span>{</span>cssColor<span>}</span> onChange<span>=</span><span>{</span>onCssColorChange<span>}</span> <span>/</span><span>&gt;</span>
      <span>&lt;</span>div
        style<span>=</span><span>{</span><span>{</span>
          width<span>:</span> <span>100</span><span>,</span>
          height<span>:</span> <span>100</span><span>,</span>
          background<span>:</span> cssColor<span>,</span>
        <span>}</span><span>}</span>
      <span>/</span><span>&gt;</span>
    <span>&lt;</span><span>/</span>div<span>&gt;</span>
  <span>)</span>
<span>}</span></code></pre></div>
<p>You can see this component in action and play with the code on <a href="https://codesandbox.io/s/9lzyov54lr">CodeSandbox</a>.</p>
<p>For the sake of this article, we are going to focus only on the <code>useState</code> call, but if you are curious to understand better how the hook itself works internally I really recommend you read the <a href="https://reactjs.org/docs/hooks-state.html">official State Hook documentation</a>. I was personally curious to understand how multiple <code>useState</code> calls could maintain the relationship with the specific state attribute (since there’s no explicit labelling or reference). If you are curious about that too, well you should read the <a href="https://reactjs.org/docs/hooks-faq.html#how-does-react-associate-hook-calls-with-components">Hooks FAQ</a> and <a href="https://medium.com/@dan_abramov/making-sense-of-react-hooks-fdbde8803889">Dan Abramov’s recent article about Hooks</a>.</p>
<p>Back to the <code>useState</code> call in our example, now!</p>
<p>The <code>useState</code> hook acts like a factory: given a default value for the state property (<code>'Blue'</code> in our case), it will need to instantiate for you 2 things:</p>
<ul>
<li>the current value for the specific state property (<code>cssColor</code> in our case)</li>
<li>a function that allows you to alter the specific property (<code>setCssColor</code> in our case)</li>
</ul>
<p>React developers decided to handle this requirement by simulating multiple return values with an array.</p>
<p>Combining this with array destructuring and proper variable naming, the result is an API that is very nice to read and to use.</p>
<p>This React feature is still very experimental and subject to change at the time of writing, but it already sounds like a big deal for the React community to make the code more expressive and reduce the barrier to entry to start adopting React.</p>
<p>The point I want to make is that, in this specific case, the multiple return values pattern plays a big role towards this goal.</p>
<h3 id="converting-callbacks-api-to-asyncawait"><a href="#converting-callbacks-api-to-asyncawait" aria-label="converting callbacks api to asyncawait permalink"></a>Converting callbacks API to Async/Await</h3>
<p>Recently I found another great use case for the multiple return values pattern while trying to convert a callback oriented API into an equivalent Async/Await API.</p>
<p>To make this part clear, I am going to explain very quickly an approach I use to convert callback based APIs into functions that I can use with Async/Await.</p>
<p>Let’s take this generic example:</p>
<div data-language="javascript"><pre><code><span>function</span> <span>doSomething</span><span>(</span><span>input<span>,</span> callback</span><span>)</span> <span>{</span>
  
  
  
  <span>callback</span><span>(</span>error<span>,</span> response<span>)</span>
<span>}</span></code></pre></div>
<p>To convert this function into something that can be used with Async/Await we have to essentially <a href="https://loige.co/to-promise-or-to-callback-that-is-the-question/"><em>promisify</em></a> it. There are libraries to do it and, if you are using Node.js you can even use the builtin <a href="https://nodejs.org/api/util.html#util_util_promisify_original"><code>util.promisify</code></a>, but that’s something we can do ourselves by just creating a <em>wrapper</em> function like the following one:</p>
<div data-language="javascript"><pre><code><span>const</span> <span>doSomethingPromise</span> <span>=</span> <span>(</span><span>input</span><span>)</span> <span>=&gt;</span> <span>new</span> <span>Promise</span><span>(</span><span>(</span><span>resolve<span>,</span> reject</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>doSomething</span><span>(</span>input<span>,</span> <span>(</span><span>error response</span><span>)</span> <span>=&gt;</span> <span>{</span>
    <span>if</span> <span>(</span>error<span>)</span> <span>{</span>
      <span>return</span> <span>reject</span><span>(</span>error<span>)</span>
    <span>}</span>

    <span>return</span> <span>resolve</span><span>(</span>response<span>)</span>
  <span>}</span><span>)</span>
<span>}</span><span>)</span></code></pre></div>
<p>In short, our wrapper function <code>doSomethingPromise</code> is immediately returning a <code>Promise</code>. Inside the body of the promise we are invoking the original <code>doSomething</code> function with a callback that will be resolving or rejecting the promise based on whether there’s an <code>error</code> or not.</p>
<p>Now we can finally take advantage of Async/Await:</p>
<div data-language="javascript"><pre><code>
<span>const</span> response <span>=</span> <span>await</span> <span>doSomethingPromise</span><span>(</span>input<span>)</span></code></pre></div>
<p><strong>Note</strong>: this will throw in case of error, so make sure you have it in a <code>try/catch</code> block to handle the error correctly.</p>
<blockquote>
<p>If you are curious about <em>promisifying</em> callback-based functions, I have <a href="https://loige.co/to-promise-or-to-callback-that-is-the-question/">an entire article</a> dedicated to this topic.</p>
</blockquote>
<p>In my specific use case, I was using a <a href="https://www.npmjs.com/package/twitter">twitter client</a> library that follows this conventions:</p>
<div data-language="javascript"><pre><code>
client<span>.</span><span>get</span><span>(</span><span>'statuses/user_timeline'</span><span>,</span> params<span>,</span> <span>function</span> <span>callback</span>…</code></pre></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://loige.co/emerging-javascript-pattern-multiple-return-values">https://loige.co/emerging-javascript-pattern-multiple-return-values</a></em></p>]]>
            </description>
            <link>https://loige.co/emerging-javascript-pattern-multiple-return-values</link>
            <guid isPermaLink="false">hacker-news-small-sites-24916683</guid>
            <pubDate>Wed, 28 Oct 2020 08:56:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Qt 6 will provide additional libraries via Conan package manager]]>
            </title>
            <description>
<![CDATA[
Score 42 | Comments 34 (<a href="https://news.ycombinator.com/item?id=24916321">thread link</a>) | @alaenix
<br/>
October 28, 2020 | https://www.qt.io/blog/qt-6-additional-libraries-via-package-manager | <a href="https://web.archive.org/web/*/https://www.qt.io/blog/qt-6-additional-libraries-via-package-manager">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                            

                            <p>
                                Tuesday October 27, 2020 by <a href="https://www.qt.io/blog/author/iikka-eklund">Iikka Eklund</a> | <a href="#commento">Comments</a>
                            </p>
                            
                            <p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p><span data-contrast="auto">With Qt 6 we want to provide more flexibility via </span><span data-contrast="auto">leveraging</span><span data-contrast="auto"> a package manager in addition to Qt </span><span data-contrast="auto">Online </span><span data-contrast="auto">Installer. The new package manager functionality, based on conan.io (</span><a href="https://conan.io/"><span data-contrast="none">https://conan.io</span></a><span data-contrast="auto">), allows provi</span><span data-contrast="auto">ding more </span><span data-contrast="auto">packages </span><span data-contrast="auto">to the users without increasing the complexity of the baseline Qt. In addition to the </span><span data-contrast="auto">packages </span><span data-contrast="auto">provided by Qt, the package manager can be used for getting content from other sources.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>
<!--more-->
<p><span data-contrast="auto">Initially</span><span>,</span><span data-contrast="auto"> we have three </span><span data-contrast="auto">Additional Li</span><span data-contrast="auto">b</span><span data-contrast="auto">raries </span><span data-contrast="auto">provided via the package manager: Qt Network Authorization, Qt Image Formats</span><span>,</span><span data-contrast="auto"> and Qt 3D. More </span><span data-contrast="auto">Additional Libraries </span><span data-contrast="auto">will be available </span><span data-contrast="auto">in forthcoming</span><span data-contrast="auto"> Qt 6 releases. We are currently leveraging the exis</span><span data-contrast="auto">ting Qt delivery system as</span><span data-contrast="auto"> the</span> <span data-contrast="auto">backend for the </span><span data-contrast="auto">Additional Libraries</span><span data-contrast="auto"> available via the package manager.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>
<p><strong>How the packages are managed?&nbsp;</strong></p>
<p><span data-contrast="auto">The required </span><span data-contrast="auto">tools, Conan, </span><span data-contrast="auto">CMake</span><span>,</span><span data-contrast="auto"> and Ninja, can be easily installed u</span><span data-contrast="auto">sing</span><span data-contrast="auto"> the</span> <span><strong>Qt </strong></span><strong><span data-contrast="auto">O</span></strong><strong><span data-contrast="auto">nline installer </span></strong><strong><span data-contrast="auto">4.0</span></strong><span data-contrast="auto">, which is going to be released </span><span data-contrast="auto">soon.</span> <span data-contrast="auto">The</span><span data-contrast="auto"> Conan</span><span data-contrast="auto"> build </span><span data-contrast="auto">recipes</span> <span data-contrast="auto">for </span><span data-contrast="auto">Additional Libraries</span><span data-contrast="auto"> require </span><span data-contrast="auto">CMake</span><span data-contrast="auto"> and Ninja to build the module</span><span data-contrast="auto">.</span> <span data-contrast="auto">The project linking to the module</span> <span data-contrast="auto">can be </span><span data-contrast="auto">qmake</span><span data-contrast="auto">-</span><span data-contrast="auto">based as well.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>
<p><span data-contrast="auto">Once installed</span><span data-contrast="auto">, </span><span data-contrast="auto">the selected </span><span data-contrast="auto">Add</span><span data-contrast="auto">itional Libraries</span> <span data-contrast="auto">can be built </span><span data-contrast="auto">once </span><span data-contrast="auto">by </span><span data-contrast="auto">using</span><span data-contrast="auto"> Conan</span><span data-contrast="auto"> per selected target configuration</span><span data-contrast="auto">. After the build</span><span data-contrast="auto">,</span><span data-contrast="auto"> the binary package is available in </span><span data-contrast="auto">user’s</span><span data-contrast="auto"> local</span><span data-contrast="auto"> Conan</span><span data-contrast="auto"> cache</span><span data-contrast="auto">, and can be </span><span data-contrast="auto">linked to any other project</span><span data-contrast="auto">.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;<br></span><span></span></p>
<p><span><strong>How to get and build the packages?&nbsp;</strong><br></span><span data-contrast="auto"></span></p>
<p><span data-contrast="auto">An example build call</span> <span data-contrast="auto">look</span><span data-contrast="auto">s</span><span data-contrast="auto"> like this:</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>
<pre>$conan.exe install qtnetworkauth/6.0.0@qt/beta --build=missing <br>--profile=&lt;QtSdk&gt;/Tools/Conan/profiles/qt-6.0.0-msvc2019_64 -s <br>build_type=Release -g cmake_paths -g=cmake -g deploy&nbsp;</pre>

<p><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">Now, let's look what that contains:</span></p>
<ul>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="auto">qtnetworkauth</span><span data-contrast="auto">/6.0.0@qt/beta</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span>
<ul>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="auto">This is the</span><span data-contrast="auto"> Conan</span><span data-contrast="auto"> reference for the package</span></li>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="auto">You can search available packages in your </span><span data-contrast="auto">C</span><span data-contrast="auto">onan cache by:</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;&nbsp;</span><span data-contrast="auto">$conan</span><span data-contrast="auto">.exe </span><span data-contrast="auto">search</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></li>
</ul>
</li>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="none">--profile</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span>
<ul>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="none">This file is installed by the Qt installer. Each </span><span data-contrast="none">Qt 6 E</span><span data-contrast="none">ssential package installed by the Qt </span><span data-contrast="none">I</span><span data-contrast="none">nstaller</span><span data-contrast="none"> installs also a matching profile file. This tells</span><span data-contrast="none"> Conan</span><span data-contrast="none"> the target build configuration.</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></li>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="none">The user needs to select</span><span data-contrast="none"> a </span><span data-contrast="none">suitable profile</span><span data-contrast="none">, that is </span><span data-contrast="none">the target build configuration.</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></li>
</ul>
</li>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="none">-g</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span>
<ul>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="none">If your consuming project is a </span><span data-contrast="none">CMake</span> <span data-contrast="none">project</span><span data-contrast="none"> the</span><span data-contrast="none">n</span><span data-contrast="none"> use the</span> <span data-contrast="none">CMake</span><span data-contrast="none"> generators</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></li>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="none">If your consuming project is</span><span data-contrast="none"> a</span> <span data-contrast="none">qmake</span> <span data-contrast="none">project</span><span data-contrast="none"> then you can pass: -g </span><span data-contrast="none">qmake</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></li>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="none">The “deploy” generator deploys the buil</span><span data-contrast="none">t</span><span data-contrast="none"> A</span><span data-contrast="none">dditional Library</span><span data-contrast="none"> from </span><span data-contrast="none">the</span><span data-contrast="none"> Conan</span><span data-contrast="none"> cache to your working environment</span><span data-contrast="none">. This is useful if you </span><span data-contrast="none">want to bundle your application files together.</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></li>
</ul>
</li>
</ul>
<p><span data-contrast="none">For detailed steps see the <a data-insert="true" href="https://wiki.qt.io/Qt6_Add-on_src_package_build_using_Conan_package_manager" rel="noopener">instructions</a>.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>
<p><span data-contrast="none">Currently</span><span data-contrast="none">,</span><span data-contrast="none"> Qt </span><span data-contrast="none">Online </span><span data-contrast="none">Installer exports the A</span><span data-contrast="none">dditional Library packages (sources and build recipes)</span><span data-contrast="none"> into the</span><span data-contrast="none"> Conan</span><span data-contrast="none"> cache. There is no </span><span data-contrast="none">C</span><span data-contrast="none">onan remote that hosts the Add</span><span data-contrast="none">itional Library</span> <span data-contrast="none">C</span><span data-contrast="none">onan packages</span><span data-contrast="none">.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>
<p aria-level="2"><strong>Next steps&nbsp;</strong></p>
<p><span data-contrast="none">Like Qt 6.0, the current work is still in beta phase and </span><span data-contrast="none">all </span><span data-contrast="none">feedback is welcome</span><span data-contrast="none">.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;Note that currently the Conan profile files and build recipies for&nbsp;</span><span data-contrast="none">Android and iOS targets are being worked on</span><span data-contrast="none">.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;Also, t</span><span data-contrast="none">he build recipes of the </span><span data-contrast="none">Additional Libraries</span><span data-contrast="none"> are not part of the repositories yet. </span><span>&nbsp;</span><span data-contrast="none">Once the build recipes</span><span data-contrast="none"> are mature</span><span data-contrast="none"> the plan is to move those into module repositories.</span><span>&nbsp;</span></p>
<p><span></span><span data-contrast="none">If you want to have a look already now</span><span data-contrast="none">,</span><span data-contrast="none"> how</span><span data-contrast="none"> the conanfile.py recipes </span><span data-contrast="none">look like</span><span data-contrast="none">,</span> <span data-contrast="none">those </span><span data-contrast="none">can be found </span><span data-contrast="none">in</span> <span data-contrast="none">the </span><span data-contrast="none">Qt installation, under each module in “</span><span data-contrast="none">AdditionalLibraries</span><span data-contrast="none">/Qt</span><span data-contrast="none">/”</span><span data-contrast="none">.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>
&nbsp;</span></p>
                            
                            
                                <hr>
                          
                                <h6>Blog Topics:</h6>        
                                
                            


                        </div></div>]]>
            </description>
            <link>https://www.qt.io/blog/qt-6-additional-libraries-via-package-manager</link>
            <guid isPermaLink="false">hacker-news-small-sites-24916321</guid>
            <pubDate>Wed, 28 Oct 2020 07:55:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Tracing Kernel Functions: FBT stack() and arg]]>
            </title>
            <description>
<![CDATA[
Score 25 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24915942">thread link</a>) | @moks
<br/>
October 27, 2020 | https://zinascii.com/2020/fbt-args-and-stack.html | <a href="https://web.archive.org/web/*/https://zinascii.com/2020/fbt-args-and-stack.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

      
      <p>Oct 27, 2020</p>

      <p>
	In my <a href="https://zinascii.com/2020/the-amd64-fbt-handler.html">previous
	post</a> I described how FBT intercepts function calls and
	vectors them into the DTrace framework. That laid the
	foundation for what I want to dicuss in this post: the
	implementation of the <code>stack()</code> action and
	built-in <code>arg</code> variables. These features rely on
	the precise layout of the stack, the details of which I
	touched on previously. In this post I hope to illuminate those
	details a bit more with the help of some visuals, and then
	guide you through the implentation of these two DTrace
	features as they relate to the FBT provider.
      </p>

      <h2>A Correction</h2>

      <p>
	But first I must make a correction to my last post. It turns
	out the FBT handler <b>does not</b> execute on the IST stack.
	It runs on either the thread’s stack or the CPU’s high-level
	interrupt stack depending on the context of the kernel
	function call, but never on the IST.
	Rather, <a href="https://en.wikipedia.org/wiki/Kernel_page-table_isolation">KPTI</a>
	uses the IST stack as a scratch space to perform its
	trampoline into the real handler. This little detail is
	important. Functions like <code>dtrace_getpcstack()</code>
	have zero chance of working if run with the IST stack, for
	reasons which become obvious later. This also explains why the
	AMD64 handler pulls down the stack
	during <code>pushq&nbsp;%RBP</code> emulation: if it’s working
	on the same stack as the thread/interrupt, then it must make
	room for <code>RBP</code>. I can explain better with a visual.
	First, the diagram from the last post.
      </p>

      <figure>
	  
	  <figcaption><a name="fig1-int3-pre-handler">Figure 1. INT3 thread/interrupt state pre-handler</a></figcaption>
      </figure>

      <p>
	On the left we have a kernel thread, interrupt thread, or
	high-level interrupt running on CPU. On the right we have the
	“interrupt context” of the breakpoint exception, using the
	IST. The image is correct in that there are two different
	stacks in play, but what’s running on the right-hand side is
	not the <code>brktrap</code> handler. The right-hand side is
	running the KPTI trampoline, ensuring a CR3 switch when moving
	between the user/kernel boundary. The trampoline also provides
	a facsimile of the processor frame to the interrupted thread’s
	stack, making it none the wiser that KPTI was ever on the
	scene. So all the action happens on the left side, but what
	does the stack look like as we transition through the #BP
	handler on our way to <code>dtrace_invop()</code>?
      </p>

      <figure>
	  
	  <figcaption><a name="fig2-pre-fbt-stack">Figure 2. stack state from #BP to pre dtrace_invop()</a></figcaption>
      </figure>

      <p>
	In phase ① <code>mac_provider_tx()</code> is
	calling <code>mac_ring_tx()</code> while it is under FBT entry
	instrumentation. The last thing on the thread’s stack is the
	return address, and the CPU is about to execute
	the <code>int3</code> instruction.
      </p>

      <p>
	Phase ② is immediately after the CPU has finished execution of
	the <code>int3</code> instruction. The processor (via the
	spectre of the KPTI trampoline) has pushed a 16-byte aligned
	processor frame on the stack and has vectored into
	the <code>brktrap()</code> handler.
      </p>

      <p>
	Phase ③ is after some amount of execution of
	the <code>brktrp()</code> and <code>invoptrap()</code>
	handlers—remember, the #BP handler for DTrace mimics a #UD.
	This last phase shows the state just before the call
	to <code>dtrace_invop()</code>. At this point we’ve grown an
	entire <code>regs</code> structure on the stack and stashed a
	copy of the return address on top of this. The later used to
	populate <code>cpu_dtrace_caller</code>, a variable which
	becomes important later.
      </p>

      <h2>The stack() Action</h2>

      <p>
	The separation of probes and actions is a vital aspect of
	DTrace’s architecture. A firm boundary between these two makes
	DTrace more powerful than it ever could be if they were
	tightly coupled. Think about it, I can ask for the call stack
	in any probe, not just the probes that deem that information
	useful. The probes give you access to a context, and the
	actions give you access to data in that context. To limit the
	execution of actions to specific probes would limit the
	questions you can ask about the system. With this design the
	number of questions you can ask is virtually endless. And it
	turns out one of the more useful questions to ask is: “what
	the hell is running on my CPU”?
      </p>

      <p>
	The <code>stack()</code> action allows you to record the call
	stack that lead to the probe site. In the context of FBT this
	will record the call stack of the kernel thread or interrupt
	executing an entry or return from this kernel function. You
	can also access the userland stack of a thread
	via <code>ustack()</code>, but I don’t cover that here.
      </p>

      <p>
	The <code>stack()</code> action is implemented by
	the <code>dtrace_getpcstack()</code> function. To get there
	from <code>dtrace_invop()</code> requires a couple of more
	calls in the DTrace framework. Ultimately, the call stack to
	get there looks like this.
      </p>

      <figure>
	<div>
	  <pre><code>dtrace_getpcstack()
dtrace_probe()
fbt_invop()
dtrace_invop()
dtrace_invop_callsite() &lt;aka invoptrap&gt;
&lt;rest of call stack that lead here&gt;</code></pre>
	</div>
	<figcaption>call stack between dtrace_getpcstack() and dtrace_invop()</figcaption>
      </figure>

      <p>
	The implementation of <code>stack()</code> really starts
	with <code>DTRACEACT_STACK</code> inside
	of <code>dtrace_probe()</code>.
      </p>

      <figure>
	<figcaption><a href="https://github.com/illumos/illumos-gate/blob/007ca33219ffdc49281657f5f8a9ee1bbfc367ab/usr/src/uts/common/dtrace/dtrace.c#L7184-L7191">usr/src/uts/common/dtrace/dtrace.c</a></figcaption>
	<figure>
	  <div>
<pre><span><span></span><code>			case DTRACEACT_STACK:</code></span>
<span><span></span><code>				if (!dtrace_priv_kernel(state))</code></span>
<span><span></span><code>					continue;</code></span>
<span><span></span><code></code></span>
<span><span></span><code>				dtrace_getpcstack((pc_t *)(tomax + valoffs),</code></span>
<span><span></span><code>				    size / sizeof (pc_t), probe-&gt;dtpr_aframes,</code></span>
<span><span></span><code>				    DTRACE_ANCHORED(probe) ? NULL :</code></span>
<span><span></span><code>				    (uint32_t *)arg0);</code></span></pre>
	  </div>
	  <figcaption>stack() action implementation found in dtrace_probe()</figcaption>
	</figure>
      </figure>

      <p>
	The first argument is the address of the array used to store
	program counter values (aka function pointers). This array
	starts at some offset into the current DTrace buffer. The
	second argument if the size of that array. The third argument
	is the number of “artificial frames” on the stack, more on
	this later. The fourth argument is used to determine if the
	first (topmost) program counter in the call stack is the value
	passed in <code>arg0</code> to <code>dtrace_probe()</code>. An
	“anchored” probe is one that has a function name specified
	when calling <code>dtrace_probe_create()</code>. For example,
	the FBT provider uses the name of the kernel function as the
	probe’s function name, thus it is anchored on the kernel
	function. The profile provider, however, specifies no probe
	function name; it is not anchored and is a bit of a special
	case. I address this at the end of the post.
      </p>

      <p>
	This brings us to the <code>dtrace_getpcstack()</code>
	function. But first I’ll expand
	on <a href="#fig2-pre-fbt-stack">figure 2</a> to show our
	stack state as of source line 60 of the function.
      </p>

      <figure>
	
	<figcaption>
	  <a name="fig3-start-of-dtrace_getpcstack">Figure 3. start of dtrace_getpcstack()</a>
	</figcaption>
      </figure>

      <figure>
	<figcaption><a href="https://github.com/illumos/illumos-gate/blob/007ca33219ffdc49281657f5f8a9ee1bbfc367ab/usr/src/uts/intel/dtrace/dtrace_isa.c#L43-L60">usr/src/uts/intel/dtrace/dtrace_isa.c</a></figcaption>
	<figure>
	  <div>
<pre><span><span></span><code>void</code></span>
<span><span></span><code>dtrace_getpcstack(pc_t *pcstack, int pcstack_limit, int aframes,</code></span>
<span><span></span><code>    uint32_t *intrpc)</code></span>
<span><span></span><code>{</code></span>
<span><span></span><code>	struct frame *fp = (struct frame *)dtrace_getfp();</code></span>
<span><span></span><code>	struct frame *nextfp, *minfp, *stacktop;</code></span>
<span><span></span><code>	int depth = 0;</code></span>
<span><span></span><code>	int on_intr, last = 0;</code></span>
<span><span></span><code>	uintptr_t pc;</code></span>
<span><span></span><code>	uintptr_t caller = CPU-&gt;cpu_dtrace_caller;</code></span>
<span><span></span><code></code></span>
<span><span></span><code>	if ((on_intr = CPU_ON_INTR(CPU)) != 0)</code></span>
<span><span></span><code>		stacktop = (struct frame *)(CPU-&gt;cpu_intr_stack + SA(MINFRAME));</code></span>
<span><span></span><code>	else</code></span>
<span><span></span><code>		stacktop = (struct frame *)curthread-&gt;t_stk;</code></span>
<span><span></span><code>	minfp = fp;</code></span>
<span><span></span><code></code></span>
<span><span></span><code>	aframes++;</code></span></pre>
	  </div>
	  <figcaption>dtrace_getpcstack()</figcaption>
	</figure>
      </figure>

      <p>
	To build the call stack we first need to be able to walk the
	stack. Luckily, illumos keeps frame pointers in the kernel,
	making this easy. But in this particular situation there is
	more to consider. First, we might have two stacks in play: the
	high-level interrupt’s stack as well as the stack of the
	thread it interrupted. Second, the DTrace framework and FBT
	provider have put their own frames between this code and the
	function that tripped this probe; we must exclude these
	“artificial” frames from the result. Finally, we need to make
	sure not to walk off the stack and into space, both for
	correctness and safety. Speaking of the stack,
	the <code>stacktop</code> variable is pointing to the “top” of
	the stack in terms of memory (on x86 stacks grow downwards).
	Logically speaking, <code>stacktop</code> is the bottom of the
	stack and the <code>dtrace_getpcstack()</code> frame is the
	top.
      </p>

      <figure>
	<figcaption><a href="https://github.com/illumos/illumos-gate/blob/007ca33219ffdc49281657f5f8a9ee1bbfc367ab/usr/src/uts/intel/dtrace/dtrace_isa.c#L62-L63">usr/src/uts/intel/dtrace/dtrace_isa.c</a></figcaption>
	<figure>
	  <div>
<pre><span><span></span><code>	if (intrpc != NULL &amp;&amp; depth &lt; pcstack_limit)</code></span>
<span><span></span><code>		pcstack[depth++] = (pc_t)intrpc;</code></span></pre>
	  </div>
	  <figcaption>dtrace_getpcstack()</figcaption>
	</figure>
      </figure>

      <p>
	If <code>intrpc</code> is set, then that’s our first program counter.
      </p>

      <figure>
	<figcaption><a href="https://github.com/illumos/illumos-gate/blob/007ca33219ffdc49281657f5f8a9ee1bbfc367ab/usr/src/uts/intel/dtrace/dtrace_isa.c#L65-L85">usr/src/uts/intel/dtrace/dtrace_isa.c</a></figcaption>
	<figure>
	  <div>
<pre><span><span></span><code>	while (depth &lt; pcstack_limit) {</code></span>
<span><span></span><code>		nextfp = (struct frame *)fp-&gt;fr_savfp;</code></span>
<span><span></span><code>		pc = fp-&gt;fr_savpc;</code></span>
<span><span></span><code></code></span>
<span><span></span><code>		if (nextfp &lt;= minfp || nextfp &gt;= stacktop) {</code></span>
<span><span></span><code>			if (on_intr) {</code></span>
<span><span></span><code>				/*</code></span>
<span><span></span><code>				 * Hop from interrupt stack to thread stack.</code></span>
<span><span></span><code>				 */</code></span>
<span><span></span><code>				stacktop = (struct frame *)curthread-&gt;t_stk;</code></span>
<span><span></span><code>				minfp = (struct frame *)curthread-&gt;t_stkbase;</code></span>
<span><span></span><code>				on_intr = 0;</code></span>
<span><span></span><code>				continue;</code></span>
<span><span></span><code>			}</code></span>
<span><span></span><code></code></span>
<span><span></span><code>			/*</code></span>
<span><span></span><code>			 * This is the last frame we can process; indicate</code></span>
<span><span></span><code>			 * that we should return after processing this frame.</code></span>
<span><span></span><code>			 */</code></span>
<span><span></span><code>			last = 1;</code></span>
<span><span></span><code>		}</code></span></pre>

	  </div>
	  <figcaption>dtrace_getpcstack()</figcaption>
	</figure>
      </figure>

      <p>
	The main loop walks the call stack and fills in program
	counters as long as there are slots remaining in
	<code>pcstack</code>. If we were in the context of a
	high-level interrupt and we’ve walked off its stack, then hop
	to the thread stack. Otherwise, we’ve walked off the thread
	stack, leaving just this last frame to record.
      </p>

      <figure>
	<figcaption><a href="https://github.com/illumos/illumos-gate/blob/007ca33219ffdc49281657f5f8a9ee1bbfc367ab/usr/src/uts/intel/dtrace/dtrace_isa.c#L87-L98">usr/src/uts/intel/dtrace/dtrace_isa.c</a></figcaption>
	<figure>
	  <div>
<pre><span><span></span><code>		if (aframes &gt; 0) {</code></span>
<span><span></span><code>			if (--aframes == 0 &amp;&amp; caller != 0) {</code></span>
<span><span></span><code>				/*</code></span>
<span><span></span><code>				 * We've just run out of artificial frames,</code></span>
<span><span></span><code>				 * and we have a valid caller -- fill it in</code></span>
<span><span></span><code>				 * now.</code></span>
<span><span></span><code>				 */</code></span>
<span><span></span><code>				ASSERT(depth &lt; pcstack_limit);</code></span>
<span><span></span><code>				pcstack[depth++] = (pc_t)caller;</code></span>
<span><span></span><code>				caller = 0;</code></span>
<span><span></span><code>			}</code></span>
<span><span></span><code>		} else {</code></span></pre>
	  </div>
	  <figcaption>dtrace_getpcstack()</figcaption>
	</figure>
      </figure>

      <p>
	Make sure to skip over any artificial frames.
	The <code>aframes</code> value is based on information given
	by the provider at probe creation time
	(<code>dtrace_probe_create()</code>/<code>dtpr_aframes</code>)
	as well as knowledge inherent to the DTrace framework. These
	two know how many frames they have each injected between
	the <code>stack()</code> action and the first real frame; we
	sum the values to know how many total frames to skip.
      </p>

      <p>
	The <code>caller</code> variable is a bit more subtle; and
	this is another thing I got wrong in
	my <a href="https://zinascii.com/2020/the-amd64-fbt-handler.html">last post</a> while
	discussing the return probe. The <code>caller</code> value
	…</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://zinascii.com/2020/fbt-args-and-stack.html">https://zinascii.com/2020/fbt-args-and-stack.html</a></em></p>]]>
            </description>
            <link>https://zinascii.com/2020/fbt-args-and-stack.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24915942</guid>
            <pubDate>Wed, 28 Oct 2020 06:39:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Two Paradigms of Personal Computing]]>
            </title>
            <description>
<![CDATA[
Score 39 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24915683">thread link</a>) | @riverlong
<br/>
October 27, 2020 | https://jayriverlong.github.io/2020/10/27/machines.html | <a href="https://web.archive.org/web/*/https://jayriverlong.github.io/2020/10/27/machines.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main" role="main"> <article role="article">  <p>A frequently recurring staple of Hacker News is the <em>personal computing rant</em>: modern computers are black boxes, far too locked down.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup> We long for our desktop computers, the early-2000s computing paradigm, and we want to take back control of our data.</p> <p>I sympathize greatly with this view. For the past five years, I have exclusively run Arch Linux. I love the early-2000s style of personal computing: text-heavy interfaces, words rather than icons, uniform keyboard shortcuts everywhere. I do nearly all my computing in emacs. It is feature rich, and no product manager is ever going to change the experience, interface, or shortcuts. My workspace allows me to be <em>fast</em>. Contrary to much modern software, emacs responds instantaneously to my keystrokes. Every action has an immediate effect, clear on the page, with no background process ambiguity. This is enormously satisfying. I would characterize this computing environment as a spiritual descendant of the typewriter: on its own, it is a dumb machine, but when you command it, it becomes a powerful extension of you. The user experience is given by speed, smoothness, reliability, and nothing else.</p> <p>On the other hand, you’ve got iPhones and iPads. These are not machines that you command – on the contrary, you might argue that all their notifications command you. With touch interfaces, facial recognition, and voice control, these are machines that are meant to merge with you. Instead of becoming an <em>extension</em> of you, it becomes <em>part</em> of you. Instead of you commanding it, it is meant to anticipate your commands. Where is the data? How does it work? Who knows. They’re total black boxes, effectively indistinguishable from magic, whereas your Linux Desktop is a DIYable simple machine.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup></p> <p>Many contemporary writers think of personal computing as one paradigm that is gradually evolving. I disagree: I think there are two, deeply different paradigms of personal computing, reflecting deeply different desires.</p> <ul> <li>Machine as extension of the self (your Windows XP Desktop).</li> <li>Machine as part of the self (your iPhone).</li> </ul> <p>Plenty of devices exist in the space in-between. Android phones try to give you some control back over your device, by which they necessarily preclude themselves from truly being <em>part</em> of you. Modern versions of OS X, with the touchbar, Siri, and integrations into all your other Apple devices, are starting to make the leap from extending the self to being part of the self.</p> <p>As a genre, the personal computing rant comes out of the correct view that these are all forms of personal computing, but it fails to recognize that within this broad umbrella, we’ve seen paradigms arise that differ in both intent and the human desire they’re meant to meet. It’s not that one class of devices is replacing the other. They are fundamentally different, and one paradigm succeeds where the other falls short of meeting the consumers’ needs. The terminal never met the needs of those who want Amazon Alexa, and vice-versa.</p> <p>I live in the in-between. I happily imagine a world where my Apple Watch monitors my glucose and blood oxygen, my Eight Sleep quantifies my rest, I mark up documents on giant touchscreens, but I sit on a clacky IBM keyboard to write code and blog posts in a terminal that hasn’t changed in twenty years. While your iPhone might meld with your mind by quantifying and anticipating your every need, my terminal melds with my mind by being fast, constant, and <em>always</em> correct. Each has its place, and I would never trade one for the other.</p>  <hr>  <br> </article> </div></div>]]>
            </description>
            <link>https://jayriverlong.github.io/2020/10/27/machines.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24915683</guid>
            <pubDate>Wed, 28 Oct 2020 05:53:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Bayesian Perspective on Q-Learning]]>
            </title>
            <description>
<![CDATA[
Score 52 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24915662">thread link</a>) | @jonbaer
<br/>
October 27, 2020 | https://brandinho.github.io/bayesian-perspective-q-learning/ | <a href="https://web.archive.org/web/*/https://brandinho.github.io/bayesian-perspective-q-learning/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <d-contents>
    <nav>
      <h3>Contents</h3>
      
      
      
      
      
      
    </nav>
  </d-contents>

  <p>
    Recent work by Dabney et al. suggests that the brain represents reward predictions as probability distributions
    <d-footnote>
      Experiments were conducted on mice using single-unit recordings from the ventral tegmental area.
    </d-footnote><d-cite key="dabney2020distribtionmice"></d-cite>.
    This contrasts against the widely adopted approach in reinforcement learning (RL) of modelling single scalar
    quantities (expected values).
    In fact, by using distributions we are able to quantify uncertainty in the decision-making process.
    Uncertainty is especially important in domains where making a mistake can result in the inability to recover
    <d-footnote>
      Examples of such domains include autonomous vehicles, healthcare, and the financial markets.
    </d-footnote>. Research in risk-aware reinforcement learning has emerged to address such problems
    <d-cite key="morimura2010risksensitive,chow2018riskconstrainedrl"></d-cite>.
    However, another important application of uncertainty, which we focus on in this article, is efficient exploration
    of the state-action space.
  </p>

  <h2>Introduction</h2>

  <p>
    The purpose of this article is to clearly explain Q-Learning from the perspective of a Bayesian.
    As such, we use a small grid world and a simple extension of tabular Q-Learning to illustrate the fundamentals.
    Specifically, we show how to extend the deterministic Q-Learning algorithm to model
    the variance of Q-values with Bayes' rule. We focus on a sub-class of problems where it is reasonable to assume that Q-values
    are normally distributed
    and derive insights when this assumption holds true. Lastly, we demonstrate that applying Bayes' rule to update
    Q-values comes with a challenge: it is vulnerable to early exploitation of suboptimal policies.
  </p>

  <p>
    This article is largely based on the seminal work from Dearden et al. <d-cite key="dearden1998bayesianqlearning"></d-cite>.
    Specifically, we expand on the assumption that Q-values are normally distributed and evaluate various Bayesian exploration
    policies. One key distinction is that we model $$\mu$$ and $$\sigma^2$$, while the
    authors of the original Bayesian Q-Learning paper model a distribution over these parameters. This allows them to quantify
    uncertainty in their parameters as well as the expected return - we only focus on the latter.
  </p>

  <div><h4>Epistemic vs Aleatoric Uncertainty</h4></div>
  <div><p>
    Since Dearden et al. model a distribution over the parameters, they can sample from this distribution and the resulting
    dispersion in Q-values is known as <b>epistemic</b> uncertainty. Essentially, this uncertainty is representative of the
    "knowledge gap" that results from limited data (i.e. limited observations). If we close this gap, then we are left with
    irreducible uncertainty (i.e. inherent randomness in the environment), which is known as <b>aleatoric</b> uncertainty
    </p><d-cite key="kiureghian2007aleatoric"></d-cite><p>.

    </p><p>
    One can argue that the line between epistemic and aleatoric uncertainty is rather blurry. The information that
    you feed into your model will determine how much uncertainty can be reduced. The more information you incorporate about
    the underlying mechanics of how the environment operates (i.e. more features), the less aleatoric uncertainty there will be.

    </p><p>

    It is important to note that inductive bias also plays an important role in determining what is categorized as
    epistemic vs aleatoric uncertainty for your model.
    </p><p>

    <b>Important Note about Our Simplified Approach:</b></p><p>

    Since we only use $$\sigma^2$$ to represent uncertainty, our approach does not distinguish between epistemic and aleatoric uncertainty.

    Given enough interactions, the agent will close the knowledge gap and $$\sigma^2$$ will only represent aleatoric uncertainty. However, the agent still
    uses this uncertainty to explore.

    This is problematic because the whole point of exploration is to gain
    knowledge, which indicates that we should only explore using epistemic uncertainty.
    </p></div>


  

  <p>
    Since we are modelling $$\mu$$ and $$\sigma^2$$, we begin by evaluating the conditions under which it is appropriate
    to assume Q-values are normally distributed.
  </p>

  <a href="#section-1" id="section-1"></a>
  <h2>When Are Q-Values Normally Distributed?</h2>

  <p>
    The readers who are familiar with Q-Learning can skip over the collapsible box below.
  </p>

  <div><h4>Temporal Difference Learning</h4></div>
  <div>
    <p>
      Temporal Difference (TD) learning is the dominant paradigm used to learn value functions in reinforcement learning
      <d-cite key="sutton1988tempdiff"></d-cite>.
      Below we will quickly summarize a TD learning algorithm for Q-values,
      which is called Q-Learning. First, we will write Q-values as follows <d-cite key="Sutton2017ReinforcementIntroduction"></d-cite>:
    </p>

    <d-math block="">
      \overbrace{Q_\pi(s,a)}^\text{current Q-value} =
      \overbrace{R_s^a}^\text{expected reward for (s,a)} +
      \overbrace{\gamma Q_\pi(s^{\prime},a^{\prime})}^\text{discounted Q-value at next timestep}
    </d-math>

    <p>
      We will precisely define Q-value as the expected value of the total return from taking action $$a$$ in state $$s$$ and following
      policy $$\pi$$ thereafter. The part about $$\pi$$ is important because the agent's view on how good an action is
      depends on the actions it will take in subsequent states. We will discuss this further when analyzing our agent in
      the game environment.
    </p>

    <p>
      For the Q-Learning algorithm, we sample a reward $$r$$ from the environment, and estimate the Q-value for the current
      state-action pair $$q(s,a)$$ and the next state-action pair $$q(s^{\prime},a^{\prime})$$
      <d-footnote>
        For Q-Learning, the next action $$a^{\prime}$$ is the action with the largest Q-value in that state:
        $$\max_{a^{\prime}} q(s^{\prime}, a^{\prime})$$.
      </d-footnote>. We can represent the sample as:
    </p>

    <d-math block="">
      q(s,a) = r + \gamma q{(s^\prime,a^\prime)}
    </d-math>

    <p>
      The important thing to realize is that the left side of the equation is an estimate (current Q-value), and the right side
      of the equation is a combination of information gathered from the environment (the sampled reward) and another estimate
      (next Q-value). Since the right side of the equation contains more information about the true Q-value than the left side,
      we want to move the value of the left side closer to that of the right side. We accomplish this by minimizing the squared
      Temporal Difference error ($$\delta^2_{TD}$$), where $$\delta_{TD}$$ is defined as:
    </p>

    <d-math block="">
      \delta_{TD} = r + \gamma q(s^\prime,a^\prime) - q(s,a)
    </d-math>

    <p>
      The way we do this in a tabular environment, where $$\alpha$$ is the learning rate, is with the following update rule:
    </p>

    <d-math block="">
      q(s,a) \leftarrow \alpha(r_{t+1} + \gamma q(s^\prime,a^\prime)) + (1 - \alpha) q(s,a)
    </d-math>

    <p>
      Updating in this manner is called bootstrapping because we are using one Q-value to update another Q-value.
    </p>
  </div>

  

  <p>
    We will use the Central Limit Theorem (CLT) as the foundation to understand when Q-values are normally
    distributed. Since Q-values are sample sums, then they should look more and more normally distributed as the sample size
    increases <d-cite key="lecam1986clt"></d-cite>.
    However, the first nuance that we will point out is that rewards must be sampled from distributions with finite variance.
    Thus, if rewards are sampled distributions such as Cauchy or Lévy, then we cannot assume Q-values are normally distributed.
  </p>

  

  <p>
    Otherwise, Q-values are approximately normally distributed when the number of <b><i>effective timesteps</i></b>
    $$\widetilde{N}$$ is large
    <d-footnote>
      We can think of effective timesteps as the number of <b>full</b> samples.
    </d-footnote>.
    This metric is comprised of three factors:
  </p>

  <ul>
    <li>
      $$N$$ - <b>Number of timesteps</b>: As $$N$$ increases, so does $$\widetilde{N}$$.
    </li>

    <li>
      $$\xi$$ - <b>Sparsity</b>: We define sparsity as the number of timesteps,
      on average, a reward of zero is deterministically received in between receiving non-zero rewards
      <d-footnote>
        In the Google Colab notebook, we ran simulations to show that $$\xi$$ reduces the effective number of timesteps by $$\frac{1}{\xi + 1}$$:
        <a href="https://colab.research.google.com/github/brandinho/bayesian-perspective-q-learning/blob/main/Sparsity.ipynb">
          Experiment in a <span>Notebook</span>
        </a>
      </d-footnote>.
      When sparsity is present, we lose samples (since they are always zero).
      <!-- As sparsity increases, we essentially lose samples since they are always zero. -->
      Therefore, as $$\xi$$ increases, $$\widetilde{N}$$ decreases.
    </li>

    <li>
      $$\gamma$$ - <b>Discount Factor</b>:
      As $$\gamma$$ gets smaller, the agent places more weight on immediate rewards relative to distant ones, which means
      that we cannot treat distant rewards as full samples. Therefore, as $$\gamma$$ increases, so does $$\widetilde{N}$$.
    </li>

    <div><h4>Discount Factor and Mixture Distributions</h4></div>
    <div>
      <p>
        We will define the total return as the sum of discounted future
        rewards, where the discount factor $$\gamma$$ can take on any value between $$0$$ (myopic) and $$1$$ (far-sighted).
        It helps to think of the resulting distribution $$G_t$$ as a weighted mixture distribution.
      </p>

      <d-math block="">
        G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ... + \gamma^{N-1} r_{t+N}
      </d-math>

      <p>
        When we set $$\gamma \lt 1$$, the mixture weights for the underlying distributions change from equal weight
        to time-weighted, where immediate timesteps have a higher weight. When $$\gamma = 0$$, then this is
        equivalent to sampling from only one timestep and CLT would not hold. Use the slider
        to see the effect $$\gamma$$ has on the mixture weights, and ultimately the mixture distribution.
      </p>

      <div>
        <div>
          <p>
              $$$$<br>
              $$$$
          </p>
          

          <p>
              $$$$<br>
              $$$$
          </p>
          

          <p>
              $$$$<br>
              $$$$
          </p>

          
          
        </div>

        
      </div>

      <p><label for="barGammaMixture">$$\gamma$$ = <span></span></label></p>
    </div>
  </ul>

  We combine the factors above to formally define the number of effective timesteps:

  <d-math block="">
    \widetilde{N} = \frac{1}{\xi + 1}\sum_{i=0}^{N-1}\gamma^{i}
  </d-math>

  <p>
    Below we visually demonstrate how each factor affects the normality of Q-values
    <d-footnote>
      We scale the Q-values by $$\widetilde{N}$$ because otherwise the distribution of Q-values
      moves farther and farther to the right as the number of effective timesteps increases, which distorts the visual.
    </d-footnote>:
  </p>

  <p>
    <label for="skew">Skew-Normal</label>
    
    <label for="bernoulli">Bernoulli</label>
  </p>

  

  

  <figcaption>
    Select whether the underlying distribution follows a skew-normal or a Bernoulli distribution.
    In the Google Colab notebook we also include three statistical tests of normality for the Q-value distribution.
    </figcaption>
 …</div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://brandinho.github.io/bayesian-perspective-q-learning/">https://brandinho.github.io/bayesian-perspective-q-learning/</a></em></p>]]>
            </description>
            <link>https://brandinho.github.io/bayesian-perspective-q-learning/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24915662</guid>
            <pubDate>Wed, 28 Oct 2020 05:49:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Theory of Software Architecture]]>
            </title>
            <description>
<![CDATA[
Score 469 | Comments 233 (<a href="https://news.ycombinator.com/item?id=24915497">thread link</a>) | @nreece
<br/>
October 27, 2020 | https://danuker.go.ro/the-grand-unified-theory-of-software-architecture.html | <a href="https://web.archive.org/web/*/https://danuker.go.ro/the-grand-unified-theory-of-software-architecture.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div>
        <div>
    <section id="content">
        <article>
            
            <div>
                
                <p>Take <strong>Uncle Bob's</strong> Clean Architecture and map its correspondences with <strong>Gary Bernhardt's</strong> thin imperative shell around a functional core, and you get an understanding of how to cheaply maintain and scale software!</p>
<p>This is what <a href="https://rhodesmill.org/brandon/">Mr. Brandon Rhodes</a> did. It's not every day that I find such clear insight.</p>
<p>I am honored to have found his <a href="https://rhodesmill.org/brandon/talks/#clean-architecture-python">presentation</a> and <a href="https://rhodesmill.org/brandon/slides/2014-07-pyohio/clean-architecture/">slides</a> explaining  <a href="https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html">Uncle Bob's Clean Architecture</a> and Gary Bernhardt's PyCon talks of <a href="https://archive.org/details/pyvideo_422___units-need-testing-too">2011</a>, <a href="https://pycon-2012-notes.readthedocs.io/en/latest/fast_tests_slow_tests.html">2012</a>, and <a href="https://www.destroyallsoftware.com/talks/boundaries">2013</a>.</p>
<p>Mr. Rhodes offers such a distilled view, that he can show you these crucial concepts in 3 slides of code. I will go ahead and summarize what he said and add a tiny bit of my insight.</p>
<p>Copyright of all Python code on this page belongs to <a href="https://rhodesmill.org/brandon/">Mr. Brandon Rhodes</a>, and copyright of the diagram belongs to <a href="https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html">Robert C. Martin (Uncle Bob)</a>. I use these under (hopefully) fair use (nonprofit and educational).</p>


<p>First of all, we need to be on the same page, in order to be able to understand each other. Here are the words I'll use:</p>
<ul>
<li>Function: I use "function" or "pure function" to refer to a Python "function" that only uses its parameters for input, returns a result as output, and does not cause any other side-effects (such as I/O). <ul>
<li>A pure function returns the same output given the same inputs.</li>
<li>A pure function may be called any number of times without changing the system state - it should have no influence on DB, UI, other functions or classes.</li>
<li>This is very similar to a mathematical function: takes you from <em>x</em> to <em>y</em> and nothing else happens.</li>
<li>Sadly we can't have only pure functions; software has a <strong>purpose</strong> of causing side-effects.</li>
</ul>
</li>
<li>Procedure, Routine, or Subroutine: A piece of code that executes, that may or may not have side effects. This is a "function" in Python, but might not be a "pure function".</li>
<li>Tests: automated unit tests. By "unit" I mean not necessarily just a class, but a behavior. If you want, see more details in <a href="https://danuker.go.ro/tdd-revisited-pytest-updated-2020-09-03.html#update-2020-09-03-keep-coupling-low">the coupling chapter of my previous post</a>.</li>
</ul>

<div><pre><span></span><code><span>import</span> <span>requests</span>                      <span># Listing 1</span>
<span>from</span> <span>urllib</span> <span>import</span> <span>urlencode</span>

<span>def</span> <span>find_definition</span><span>(</span><span>word</span><span>):</span>
    <span>q</span> <span>=</span> <span>'define '</span> <span>+</span> <span>word</span>
    <span>url</span> <span>=</span> <span>'http://api.duckduckgo.com/?'</span>
    <span>url</span> <span>+=</span> <span>urlencode</span><span>({</span><span>'q'</span><span>:</span> <span>q</span><span>,</span> <span>'format'</span><span>:</span> <span>'json'</span><span>})</span>
    <span>response</span> <span>=</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>url</span><span>)</span>     <span># I/O</span>
    <span>data</span> <span>=</span> <span>response</span><span>.</span><span>json</span><span>()</span>           <span># I/O</span>
    <span>definition</span> <span>=</span> <span>data</span><span>[</span><span>u</span><span>'Definition'</span><span>]</span>
    <span>if</span> <span>definition</span> <span>==</span> <span>u</span><span>''</span><span>:</span>
        <span>raise</span> <span>ValueError</span><span>(</span><span>'that is not a word'</span><span>)</span>
    <span>return</span> <span>definition</span>
</code></pre></div>
<p>Here, we have a piece of code that prepares a URL, then gets some data over the network (I/O), then validates the result (a word definition) and returns it.</p>
<p>This is a bit much: a procedure should ideally do one thing only. While this small-ish procedure is quite readable still, it is a metaphor for a more developed system - where it could be arbitrarily long.</p>
<p>The current knee-jerk reaction is to <em>hide</em> the I/O operations somewhere far away. Here is the same code after extracting the I/O lines:</p>

<div><pre><span></span><code><span>def</span> <span>find_definition</span><span>(</span><span>word</span><span>):</span>           <span># Listing 2</span>
    <span>q</span> <span>=</span> <span>'define '</span> <span>+</span> <span>word</span>
    <span>url</span> <span>=</span> <span>'http://api.duckduckgo.com/?'</span>
    <span>url</span> <span>+=</span> <span>urlencode</span><span>({</span><span>'q'</span><span>:</span> <span>q</span><span>,</span> <span>'format'</span><span>:</span> <span>'json'</span><span>})</span>
    <span>data</span> <span>=</span> <span>call_json_api</span><span>(</span><span>url</span><span>)</span>
    <span>definition</span> <span>=</span> <span>data</span><span>[</span><span>u</span><span>'Definition'</span><span>]</span>
    <span>if</span> <span>definition</span> <span>==</span> <span>u</span><span>''</span><span>:</span>
        <span>raise</span> <span>ValueError</span><span>(</span><span>'that is not a word'</span><span>)</span>
    <span>return</span> <span>definition</span>

<span>def</span> <span>call_json_api</span><span>(</span><span>url</span><span>):</span>
    <span>response</span> <span>=</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>url</span><span>)</span>     <span># I/O</span>
    <span>data</span> <span>=</span> <span>response</span><span>.</span><span>json</span><span>()</span>           <span># I/O</span>
    <span>return</span> <span>data</span>
</code></pre></div>
<p>In Listing #2, the I/O is extracted from the top-level procedure. </p>
<p>The problem is, the code is still <strong>coupled</strong> - <code>call_json_api</code> is called whenever you want to test anything - even the building of the URL or the parsing of the result.</p>
<p><strong>Coupling kills software.</strong></p>
<p>A good rule of thumb to spot coupling is this: Can you test a piece of code without having to mock or dependency inject like Frankenstein?</p>
<p>Here, we can't test <code>find_definition</code> without somehow replacing <code>call_json_api</code> from inside it, in order to avoid making HTTP requests.</p>
<p>Let's find out what a better solution looks like.</p>

<div><pre><span></span><code><span>def</span> <span>find_definition</span><span>(</span><span>word</span><span>):</span>           <span># Listing 3</span>
    <span>url</span> <span>=</span> <span>build_url</span><span>(</span><span>word</span><span>)</span>
    <span>data</span> <span>=</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>url</span><span>)</span><span>.</span><span>json</span><span>()</span>  <span># I/O</span>
    <span>return</span> <span>pluck_definition</span><span>(</span><span>data</span><span>)</span>

<span>def</span> <span>build_url</span><span>(</span><span>word</span><span>):</span>
    <span>q</span> <span>=</span> <span>'define '</span> <span>+</span> <span>word</span>
    <span>url</span> <span>=</span> <span>'http://api.duckduckgo.com/?'</span>
    <span>url</span> <span>+=</span> <span>urlencode</span><span>({</span><span>'q'</span><span>:</span> <span>q</span><span>,</span> <span>'format'</span><span>:</span> <span>'json'</span><span>})</span>
    <span>return</span> <span>url</span>

<span>def</span> <span>pluck_definition</span><span>(</span><span>data</span><span>):</span>
    <span>definition</span> <span>=</span> <span>data</span><span>[</span><span>u</span><span>'Definition'</span><span>]</span>
    <span>if</span> <span>definition</span> <span>==</span> <span>u</span><span>''</span><span>:</span>
        <span>raise</span> <span>ValueError</span><span>(</span><span>'that is not a word'</span><span>)</span>
    <span>return</span> <span>definition</span>
</code></pre></div>
<p>Here, the procedure at the top (aka. the <span><strong>imperative shell</strong></span> of the program) is handling the I/O, and everything else is moved to <span><strong>pure functions</strong></span> (<code>build_url</code>, <code>pluck_definition</code>). The <span><strong>pure functions</strong></span> are easily testable by just calling them on made-up data structures; no Frankenstein needed.</p>
<p>This separation into an <span><strong>imperative shell</strong></span> and <span><strong>functional core</strong></span> is an encouraged idea by Functional Programming.</p>
<p>Ideally, though, in a real system, you wouldn't test elements as small as these routines, but integrate more of the system. See <a href="https://danuker.go.ro/tdd-revisited-pytest-updated-2020-09-03.html#update-2020-09-03-keep-coupling-low">the coupling chapter of my previous post</a> to understand the trade-offs.</p>

<p>Look at <a href="https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html">Uncle Bob's Clean Architecture chart</a> (Copyright Robert C. Martin aka. Uncle Bob) :
<img alt="The Clean Architecture" src="https://danuker.go.ro/images/CleanArchitecture.jpg"></p>
<p>Uncle Bob's <span><strong>Use Cases</strong></span> and <span><strong>Entities</strong></span> (red and yellow circles of the chart) map to the <span><strong>pure functions</strong></span> we saw earlier - <code>build_url</code> and <code>pluck_definition</code> from Listing 3, and the <span><strong>plain objects</strong></span> they receive as parameters and send as outputs. <em>(updated 2020-10-28)</em></p>
<p>Uncle Bob's <span><strong>Interface Adapters</strong></span> (green circle) map to the top-level <span><strong>imperative shell</strong></span>  from earlier - <code>find_definition</code> from Listing 3, handling only I/O to the outside (Web, DB, UI, other frameworks).</p>
<p><a href="https://www.reddit.com/r/programming/comments/jj7ave/the_grand_unified_theory_of_software_architecture/gabst6z/?context=3">Update 2020-10-28</a>: A "Model" object in today's MVC frameworks is a poisoned apple: it is not a <a href="https://khanlou.com/2014/12/pure-objects/">"pure" object</a> or <a href="http://xunitpatterns.com/Humble%20Object.html">"humble" object</a>, but one that can produce side effects like saving or loading from the database. Their "save" and "read" methods litter your code with untestable side-effects all over. Avoid them, or confine them to the periphery of your system and reduce their influence accordingly (they are actually a hidden <span><strong>Interface Adapter</strong></span>) due to interacting with the DB.</p>
<p>Notice the arrows on the left side of the circles, pointing inwards to more and more abstract parts. These are procedure or function calls. Our code is called by the outside. <strong>This has some exceptions. Whatever you do, the database won't call your app. But the web can, a user can through a UI, the OS can through STDIN, and a timer can, at regular intervals (such as in a game).</strong> <em>(updated 2020-10-28)</em></p>
<p>The top-level procedure:</p>
<ol>
<li>gets the input, </li>
<li>adapts it to simple objects acceptable to the system,</li>
<li>pushes it through the functional core,</li>
<li>gets the returned value from the functional core,</li>
<li>adapts it for the output device,</li>
<li>and pushes it out to the output device.</li>
</ol>
<p>This lets us easily test the functional core. Ideally, most of a production system should be pure-functional.</p>

<p>If you reduce the <span><strong>imperative shell</strong></span> and move code into the <span><strong>functional core</strong></span>, each test can verify almost the entire (now-functional) stack, but stopping short of actually performing external actions.</p>
<p>You can then test the imperative shell using <strong>fewer integration tests</strong>: you only need to check that it is <strong>correctly connected</strong> to the functional core.</p>
<p>Having two users for the system - the real user and the unit tests - and listening to both, lets you guide your architecture so as to <strong>minimize coupling</strong> and build a more <strong>flexible system</strong>.</p>
<p>Having a flexible system lets you implement new features and change existing ones <strong>quickly and cheaply</strong>, in order to <strong>stay competitive as a business</strong>.</p>
<p>Comments are much appreciated. I am yet to apply these insights, and I may be missing something!</p>
<p><strong>Edit 2020-10-28:</strong> I have tried out this methodology in some small TDD Katas, and together with TDD, it works great. But I am not employed right now, so I can't say I've <em>really</em> tried it.</p>
            </div>
            <!-- /.entry-content -->


        </article>
    </section>

        </div>
        
    </div>
</div></div>]]>
            </description>
            <link>https://danuker.go.ro/the-grand-unified-theory-of-software-architecture.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24915497</guid>
            <pubDate>Wed, 28 Oct 2020 05:19:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Dual-Boot Ubuntu 20.04 and Windows 10 with Encryption]]>
            </title>
            <description>
<![CDATA[
Score 141 | Comments 143 (<a href="https://news.ycombinator.com/item?id=24914573">thread link</a>) | @Fiveplus
<br/>
October 27, 2020 | https://www.mikekasberg.com/blog/2020/04/08/dual-boot-ubuntu-and-windows-with-encryption.html | <a href="https://web.archive.org/web/*/https://www.mikekasberg.com/blog/2020/04/08/dual-boot-ubuntu-and-windows-with-encryption.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  <p><img src="https://www.mikekasberg.com/images/posts/dual-boot-encryption-full.jpg" alt="Image for "></p>
  <p><span>08 Apr 2020</span></p><p>When you run the Ubuntu installer, there’s an option to dual-boot Ubuntu with an
existing Windows installation. There’s also an option to encrypt your Ubuntu
installation, but <em>only if you erase everything and install ubuntu</em>. There’s no
automatic way to install Ubuntu alongside Windows 10 with encryption. And while
there are plenty of tutorials for dual-booting Ubuntu and Windows, many of them
are outdated – often referencing an MBR partition table – and almost none of
them seem to address encrypting your Ubuntu partition.</p>

<blockquote>
  <p>Dual-booting with encrypted storage should not be this hard in 2020.</p>

  <p>–Me, while figuring out how to do this.</p>
</blockquote>

<p>In reality, once you figure it out, it’s not that hard. The tricky thing is that
this isn’t well-documented <strong>anywhere</strong>! So I’m hoping to fix that with this
tutorial blog post. Honestly, if you know enough about Ubuntu to set up a
dual-boot with Windows, it’s only a little bit harder to do it with encryption.
I prepared this tutorial on a Dell Latitude e7450, and I fine-tuned it when I
tested it on my Dell Precision 5510. So it should work with almost no
modification on most Dell systems, and with only minor modifications
(particularly around BIOS setup) on most other types of computers.</p>

<h2 id="references">References</h2>

<p>To write this guide, I compiled information from several sources. Here are some
of the most useful references I found:</p>

<ul>
  <li><a href="https://gist.github.com/luispabon/db2c9e5f6cc73bb37812a19a40e137bc">XPS 15 9560 Dual-Boot with Encryption Notes</a>,
by <a href="https://gist.github.com/luispabon">luispabon</a>. I followed these notes pretty
closely, but modified some partition sizes and names based on other guides.</li>
  <li><a href="https://gist.github.com/mdziekon/221bdb597cf32b46c50ffab96dbec08a">XPS 15 9570 Dual-Boot with Encryption Notes</a>,
by <a href="https://gist.github.com/mdziekon">mdziekon</a>, upon which the above is based.</li>
  <li><a href="https://help.ubuntu.com/community/Full_Disk_Encryption_Howto_2019">Full Disk Encryption HowTo 2019</a>,
from the Ubuntu Community Wiki. This is a great resource, but deals with
encryption without dual-booting.</li>
  <li><a href="https://help.ubuntu.com/community/ManualFullSystemEncryption">Manual Full System Encryption</a>,
from the Ubuntu Community Wiki. This is longer, and isn’t focused on
dual-booting, but provides great details on the way certain things work.</li>
</ul>

<p>It is worth noting that this method doesn’t encrypt <code>/boot</code>. While there are
valid reasons for encrypting /boot, the graphical installer does not encrypt it
when you do a graphical install with LUKS. As such, I’m matching that precedent,
and keeping the simplicity of an unencrypted /boot partition. Thus, the guide
I’ve compiled below is just about the <strong>simplest way to have a LUKS encryption
with dual-boot.</strong></p>

<h2 id="why-encryption-is-important">Why encryption is important</h2>

<p>I began using encrypted storage on all my personal computers 5 or 6 years ago
after noticing that all the companies I’d worked for required it, and had good
reason to. Laptops get lost and stolen all the time. They’re high-value items
that are small and easy to carry. And when a thief gets your laptop, there’s
tons of valuable information on it that they can use or sell. Even if you use a
password to log in, it’s easy for an attacker to gain access to your data if
your disk isn’t encrypted – for example, by using a live USB stick. And once
they have that data, they might get access to online accounts, bank statements,
emails, and tons of other data. For me, an encrypted hard disk isn’t optional
anymore – its a necessity.</p>

<h2 id="an-overview">An Overview</h2>

<p>So what are we going to do? This tutorial will help you set up a system to
<strong>dual-boot Ubuntu 20.04 and Windows 10</strong>. (I haven’t tested it, but it should
work with most other modern versions (~16.04+) of Ubuntu or Windows.) The system
will use a GPT hard disk with UEFI (your BIOS must support UEFI). The Ubuntu
partition will be encrypted with LUKS.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup> The Windows partition can optionally
be encrypted with BitLocker. I’m going to keep the Ubuntu installation as close
to a “default” installation as possible – no fancy tricks like a separate
<code>/home</code> partition, but it should be somewhat easy to add that yourself if you
really want to.</p>

<p>I’m going to start with a blank hard disk, installing both Windows 10 and Ubuntu
from scratch. If you already have Windows installed and you want to keep it, you
should be able to shrink your windows partition and join us in phase 3 (though
you might want to skim phases 1 and 2 to understand what we did).</p>

<p>To give you a broad overview of where we’re headed, here’s what we’re going to
do:</p>

<ol>
  <li>Prepare the installation media and computer</li>
  <li>Install Windows 10</li>
  <li>Create an encrypted partition for Ubuntu</li>
  <li>Install Ubuntu</li>
</ol>

<p>Of course, as with any new OS installation, you should back up any important
data before proceeding. <strong>The instructions below will erase all the data on your
hard disk.</strong> Proceed at your own risk; I’m not responsible for any damage or
data loss.</p>



<p>Since we’re installing both Windows 10 and Ubuntu from scratch, we’ll need a USB
stick for each. If you don’t already have a computer running Ubuntu or Windows,
making the installation media will be a little harder – but there are tutorials
for that and I’ll let you figure it out on your own.</p>

<ol>
  <li>Create a Windows Installer USB stick.  The easiest way is to use the <a href="https://www.microsoft.com/software-download/">Windows
10 Media Creation Tool</a> from a
computer that’s already running Windows.</li>
  <li>Create an Ubuntu 20.04 USB stick. The easiest way is to <a href="https://ubuntu.com/download/desktop">download the
ISO</a> and use the Startup Disk Creator on a
computer that’s already running Ubuntu.</li>
</ol>

<p>Great! We’ve got our USB sticks ready to go! One final thing before we get
started – we need to make sure our BIOS is set up correctly. In particular, we
want to make sure we’re using UEFI to boot our OS.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup></p>

<p><img src="https://www.mikekasberg.com/images/dual-boot-encryption/dell-bios.jpg" alt="The Dell BIOS"></p>

<ol start="3">
  <li>Ensure your computer is running the latest BIOS available. This is important
because an out-of-date BIOS can have bugs, and those bugs sometimes affect
things like UEFI, non-Windows operating systems, or other components we’ll be
touching.</li>
  <li>Edit your BIOS settings. The following names are probably specific to Dell
BIOS, but other manufacturers will have similar settings.
    <ol>
      <li>Under <code>General</code> and <code>Boot Sequence</code>, make sure your <code>Boot
List Option</code> is set to <code>UEFI</code>.</li>
      <li>Under <code>General</code> and <code>Advanced Boot Options</code>, I disabled
<code>Legacy Option ROMs</code>. It’s important that both OSes install in UEFI mode.
(You can probably enable this when installation is complete if you care).</li>
      <li>Under <code>Security</code>, <code>TPM Security</code> must be enabled if you
want to easily set up BitLocker in Windows.</li>
      <li>I disabled <code>Secure Boot</code>. I’m not sure if this is absolutely required, and
you can try leaving it on or re-enabling it when you’re done if you want.</li>
    </ol>
  </li>
</ol>

<p>Now that our BIOS is configured for UEFI, we’re going to set up our hard disk.</p>

<div>
<p><b>For this tutorial, your BIOS must support UEFI!</b></p>
<p>Most modern computers support this, but if yours doesn't this tutorial won't
work for you. You should consider:</p>

<ul>
  <li>Installing only Linux with encryption using the graphical installer.</li>
  <li>OR Installing only Windows with encryption.</li>
  <li>OR Dual-booting Linux and Windows without encryption using Ubuntu's graphical installer.</li>
  <li>OR Finding another tutorial or figuring out how to do this with an MBR disk.</li>
</ul>
</div>

<ol start="5">
  <li><strong>Completely erase</strong> your hard disk and set it up for UEFI by doing the
following.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3">3</a></sup>
    <ol>
      <li>Boot your Ubuntu USB stick and use <code>Try without installing</code>.</li>
      <li>Open a terminal. Make it fullscreen while you’re at it.</li>
      <li>Figure out what your primary hard disk is called. It will probably be
either <code>/dev/sda</code> or <code>/dev/nvme0n1</code>. Importantly, it’s <strong>not</strong> <code>/dev/sda1</code> or
<code>/dev/nvme0n1p1</code> – those are partitions of the disk. One way to figure out what
yours is called is to run <code>lsblk</code> and look at the disk size. Throughout the rest
of this guide, I’m going to refer to <code>/dev/sda</code>. <strong>If yours is not
<code>/dev/sda</code>, replace <code>/dev/sda</code> with your own (perhaps <code>/dev/sdb</code> or
<code>/dev/nvme0n1</code>) for the rest of this guide.</strong></li>
      <li>
        <p>Run the following commands. This will initialize the drive as a GPT drive
and create a 550M EFI system partition formatted as FAT32.</p>

        <div><div><pre><code>$ sudo su
# sgdisk --zap-all /dev/sda
# sgdisk --new=1:0:+550M /dev/sda
# sgdisk --change-name=1:EFI /dev/sda
# sgdisk --typecode=1:ef00 /dev/sda
# mkfs.fat -F 32 /dev/sda1
</code></pre></div>        </div>
      </li>
    </ol>
  </li>
</ol>

<p>OK, phase 1’s complete. We have our installation media ready to go and the
computer’s BIOS and hard drive is set up correctly. Next, we’ll install Windows.</p>

<h2 id="phase-2-install-windows">Phase 2: Install Windows</h2>

<p>In this phase, we’re going to install Windows. Note that when we do this, we’re
going to leave some unallocated space to install Linux later. This is a good
approach because the Windows installer will mess with our partitions a little
bit, and its easier to let it do so before finalizing our Linux partitions.</p>

<p><img src="https://www.mikekasberg.com/images/dual-boot-encryption/windows-installer.jpg" alt="The Windows installer"></p>

<ol>
  <li>Boot from your Windows Installer USB stick.</li>
  <li>Choose a <code>Custom (advanced)</code> install to get to the Windows partitioning tool.</li>
  <li>Create a new partition. The size of this partition should be the amount of
disk space you want to use for Windows. In this example, I did 80G since the SSD
on my computer is relatively small. If unsure, do about half of your hard
disk.</li>
  <li>Windows will warn you that it is going to create an extra system partition.
This is good.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4">4</a></sup></li>
  <li>Install Windows onto the partition you just made. There’s no need to format
any partitions – the Windows installer will take care of that for you.</li>
  <li>When the Windows installation is finished, log in and enable BitLocker on
drive <code>C:</code>. This will automatically create yet another partition on your disk
(a Windows recovery partition) - which is why we’re doing it before
partitioning for Ubuntu.</li>
</ol>

<p>At this point, you can start using Windows. But I’d avoid doing too much setup
or personalization yet so you don’t have to do it again if something goes wrong
below. If you want to double check your partitions, this is what you’ll be left
with after installing Windows and enabling BitLocker:</p>

<div><div><pre><code>ubuntu@ubuntu:~$ sudo sgdisk --print /dev/sda
Disk /dev/sda: 500118192 sectors, 238.5 GiB

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048         1128447   550.0 MiB   EF00  EFI
   2         1128448         1161215   16.0 MiB    0C01  Microsoft reserved ...
   3         1161216       167825076   79.5 GiB    0700  Basic data partition
   4       167825408       168900607   525.0 MiB   2700
</code></pre></div></div>

<h2 id="phase-3-partition-the-drive-for-ubuntu">Phase 3: Partition the drive for Ubuntu</h2>

<p>This is the trickiest phase since this is where we need to manually set up our
encrypted disks for Ubuntu. We’re going to make it work very similar to …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.mikekasberg.com/blog/2020/04/08/dual-boot-ubuntu-and-windows-with-encryption.html">https://www.mikekasberg.com/blog/2020/04/08/dual-boot-ubuntu-and-windows-with-encryption.html</a></em></p>]]>
            </description>
            <link>https://www.mikekasberg.com/blog/2020/04/08/dual-boot-ubuntu-and-windows-with-encryption.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24914573</guid>
            <pubDate>Wed, 28 Oct 2020 02:56:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[When is no-code useful?]]>
            </title>
            <description>
<![CDATA[
Score 85 | Comments 79 (<a href="https://news.ycombinator.com/item?id=24914062">thread link</a>) | @thesephist
<br/>
October 27, 2020 | https://linus.coffee/note/no-code/ | <a href="https://web.archive.org/web/*/https://linus.coffee/note/no-code/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
        <p>To talk about what no-code is good for, we need to first take a digression on what makes no-code fundamentally different from “yes-code” software.</p>
<h2 id="the-grain-of-abstractions">The grain of abstractions</h2>
<p>Software – yes-code software – has been around for a while. One of the things we’ve learned as an industry is how to write software <em>that gracefully evolves</em>. We’re not perfect – sad, legacy systems still proliferate – but we as a technical industry have learned how to build and evolve software systems against changing requirements and constraints that span years and decades.</p>
<p>When we first solve a problem with software, we write some code against the constraints of that particular day. We don’t necessarily know how the problem is going to change. Maybe there will be different customers or stakeholders tomorrow, or maybe the product will expand to serve a related, but different, problem space. We need to be able to change software to accommodate changing circumstances without rewriting it, and that is fundamentally what <em>software engineering</em> is: how to change software systems. <em>Change</em> is the name of the game.</p>
<p>We’ve gotten decent at change. We’ve built tools like Git and patterns like continuous integration and code autoformatting to make it easier to change code and remain stable. We’ve learned how to operate large software teams, especially in open source. We’ve also learned to use better abstractions. Abstractions are conceptual wrappers that isolate different parts of a codebase – say, a data source from a user interface – so that one part may change while another doesn’t. In general we have started to figure out how to make the DNA of software systems resilient against the changing tides of time.</p>
<p>No-code seems to reject a lot of those learnings, for better or worse. I haven’t seen any no-code company or product that allows source control (and I’ve seen many no-code companies, but you’re welcome to prove me wrong.) I have yet to find no-code products that allow for natural construction of abstraction between layers of a no-code workflow. No-code software is also scarily ill-prepared for large scale development: we have software systems being worked on by tens of thousands of engineers – what does it look like for a team of 1000 engineers to be working on a set of thousands of no-code workflows? Chaos.</p>
<blockquote>
<p>Traditional software has learned the abstractions and patterns that make software resilient and adaptable to change and scale. No-code software is not ready for changing constraints nor development scale.</p>
</blockquote>
<p>Despite these limitations, I think no-code has a few niches where making tradeoffs in adaptability and scale allows no-code tools to be much, much better. So, given this, <em>when is no-code useful?</em></p>
<h2 id="1-transitionary-ephemeral-software">1. Transitionary, ephemeral software</h2>
<p>The obvious answer, and one I had before our conversation, was <em>transitionary</em> software, software with <em>a defined lifetime</em>. If your software system has a finite, pre-defined lifetime and team, it doesn’t need to worry about changing constraints or team growth. It just needs to worry about solving a problem well, now.</p>
<p>Lots of software has predictably finite lifetime: a product prototype for an early-stage company, a game or app used as a part of an interactive online ad, a quick sketch or solution to patch a particularly urgent problem in a product, an app built for an event or a conference or a recruiting cycle or a quarterly goal tracker… all of these are projects with a pre-defined, maximum lifetime. They don’t need to last or grow or change – they just need to work now, and by giving up some of the adaptability of software abstractions of code, no-code software benefits from way faster prototyping speed. This is a plus.</p>
<p>I think we see lots of finite-time software in transitions. Transitions from having no product to having a product, in a prototype. Transitions in the process of brainstorming a solution and trying multiple designs. Software with a finite shelf life is a good fit for no-code tools.</p>
<h2 id="2-high-churn-code">2. High-churn code</h2>
<p>There’s another category of software for which long-term maintainability matters little – code with high churn.</p>
<p>By high churn, I mean that requirements are changing almost daily, and very little of the code written today will exist in a month or a quarter’s time. If the code you write today doesn’t have to last and evolve, because something new is going to take its place tomorrow, what matters is the speed to build, not resilience to change.</p>
<p>There’s lots of high-churn code in businesses. Marketing websites and landing pages, data pipelines for analytics, e-commerce storefronts, marketing campaigns, payment portals – requirements for these kinds of solutions change quickly enough that code is constantly being rewritten, and if code needs to <em>be replaced</em> more than it needs to <em>last</em>, no-code tools are a great fit.</p>
<h2 id="avoiding-the-same-mistakes">Avoiding the same mistakes</h2>
<p>I think “no-code” is a misnomer. It leads us to think that no-code software is the start of a trend in which general software will involve less coding, and software engineering will become easier. This is not the case. Software engineering is not about building solutions, it’s about evolving them. But change resiliency over time is not the focus of no-code tools, and I think that’s ok.</p>
<p>I think no-code tools are instead an extension of a different trend: <a href="https://thesephist.com/posts/text/">reifying workflows</a>. Business processes and workflows used to be documented in Word docs strewn about the office or on a shared folder, or even just passed down by oral tradition in companies. Now, we have tools that allow us to build these workflows, talk about them, edit them, and share them more concretely. This is a huge boon for more repeatable business processes and for getting things done quickly! I think this is the true win of no-code tools: concretizing workflows.</p>
<p>If no-code wants to be a serious competitor against “traditional” software – though I don’t think it should try – no-code needs to learn from the mistakes of early software. No-code tools need to understand that products and software systems need to live on for decades against changing teams and requirements, and against products and companies and standards that die out and get replaced. This requires a cultural shift, a tooling shift, and a new class of abstractions in our toolbelt as no-code engineers. Anytime we try to introduce more tooling and abstraction to no-code, I think no-code gets just a little more “code” in it. And perhaps that’ll bring us right back to where we started, discovering that code is good.</p>
<p>After all, the world is complex. And when we build software against the complexity of the world, that <a href="https://thesephist.com/posts/complexity-conservation/">complexity needs to go somewhere</a>. Software is complex, but only as much as the world it attempts to make sense of.</p>
<p>It feels like we’re getting off the edge of a discovery phase of no-code, and into a time when we’re starting to understand what problems no-code tools are great for. I think it’s important that no-code tool builders focus on those strengths, or risk falling into the trap of repeating the software industry’s mistakes from the ground up.</p>

        <hr>
        <p>
            
            Next:
            <a href="https://linus.coffee/note/scannability/"><em>Scannability is king</em></a>
            
        </p>
    </article></div>]]>
            </description>
            <link>https://linus.coffee/note/no-code/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24914062</guid>
            <pubDate>Wed, 28 Oct 2020 01:36:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Sega Saturn Homebrew with Game Basic]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24913598">thread link</a>) | @lostgame
<br/>
October 27, 2020 | https://flybacklabs.com/sega-saturn-homebrew-with-game-basic/ | <a href="https://web.archive.org/web/*/https://flybacklabs.com/sega-saturn-homebrew-with-game-basic/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-6">
		<!-- .entry-header -->

	
	<div>
		
	
<figure><img data-attachment-id="123" data-permalink="https://flybacklabs.com/sega-saturn-homebrew-with-game-basic/game-basic-for-sega-saturn-2/" data-orig-file="https://flybacklabs.com/wp-content/uploads/2020/10/game-basic-for-sega-saturn.jpg" data-orig-size="2560,1920" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone SE (2nd generation)&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1602104707&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;3.99&quot;,&quot;iso&quot;:&quot;500&quot;,&quot;shutter_speed&quot;:&quot;0.033333333333333&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Game BASIC for Sega Saturn" data-image-description="" data-medium-file="https://flybacklabs.com/wp-content/uploads/2020/10/game-basic-for-sega-saturn-300x225.jpg" data-large-file="https://flybacklabs.com/wp-content/uploads/2020/10/game-basic-for-sega-saturn-1024x768.jpg" loading="lazy" width="1024" height="768" src="https://flybacklabs.com/wp-content/uploads/2020/10/game-basic-for-sega-saturn-1024x768.jpg" alt="Sega Saturn for Game BASIC - Complete in Box" srcset="https://flybacklabs.com/wp-content/uploads/2020/10/game-basic-for-sega-saturn-1024x768.jpg 1024w, https://flybacklabs.com/wp-content/uploads/2020/10/game-basic-for-sega-saturn-300x225.jpg 300w, https://flybacklabs.com/wp-content/uploads/2020/10/game-basic-for-sega-saturn-768x576.jpg 768w, https://flybacklabs.com/wp-content/uploads/2020/10/game-basic-for-sega-saturn-1536x1152.jpg 1536w, https://flybacklabs.com/wp-content/uploads/2020/10/game-basic-for-sega-saturn-2048x1536.jpg 2048w" sizes="(max-width: 767px) 89vw, (max-width: 1000px) 54vw, (max-width: 1071px) 543px, 580px"><figcaption>Game BASIC for Sega Saturn – Complete in Box</figcaption></figure>



<h2>Part 1: Introduction</h2>



<h3>Summary</h3>



<p>Game BASIC for Sega Saturn is a homebrew development kit that allows you to program games for the Sega Saturn using the BASIC programming language.&nbsp; If you’re familiar with the PlayStation’s Net Yaroze platform, think of this as the Saturn’s answer to it – just cheaper and easier to get started with.</p>



<p>Game BASIC’s use of the BASIC language makes for a very low barrier to entry in terms of programming skill.&nbsp; Though the Saturn is notoriously difficult to program for, Game BASIC makes it easy to get started and is surprisingly powerful, allowing very easy sprite manipulation and straightforward 3D polygon implementation.&nbsp; It even includes an adapter cable that allows you to communicate with the Saturn from your PC to transfer or save programs and streamline development.  For example, here’s a Pilotwings-esque demo, but in Game BASIC:</p>



<figure><p>
<iframe title="Game Basic for Sega Saturn - GBSS CD - Jump Multi Controller" width="525" height="394" src="https://www.youtube.com/embed/jMPksluhvlE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p><figcaption>“Jump” demo, provided with Game BASIC (video courtesy <a href="https://www.satakore.com/sega-saturn-game-basic,,GBSSCD_G_JUMP_A,,GBSS-CD-Jump-Multi-Controller-Version-Bits-Laboratory.html" target="_blank" rel="noreferrer noopener">Satakore.com</a>)</figcaption></figure>



<p>The caveat?&nbsp; Game BASIC was released only in Japan, so this means a complete setup can be difficult to obtain and all documentation is in Japanese!&nbsp; Moreover, the supporting software that allows you to use your PC for streamlined development was intended for the Windows 95 era and flat out does not install on modern systems. Oh, and the adapter cable that allows you to connect your Saturn to your PC is a 25-pin serial connection!</p>



<p>Who in the world still has both Game BASIC and a Windows 95 PC with a physical serial port? Nobody!&nbsp; (Well, unless you’re <a href="https://www.youtube.com/watch?v=O_QU8eaMymo" target="_blank" rel="noreferrer noopener">Modern Vintage Gamer</a>) But if you’re a brave experimenter who’s not afraid to tinker a bit, there are still multiple options to get everything working, even today!&nbsp; You can even do a lot just via emulation.&nbsp; So, let’s head to the Lab and get started…</p>



<h3>What You’ll Need</h3>



<p>There are several options for working with Game BASIC, ranging from quite simple but clunky to work with, to quite powerful and streamlined.&nbsp; Here are the three options:</p>



<h4>The Simple Saturn-only setup</h4>



<h5>Required Tools</h5>



<ul><li>A copy of the <a href="https://archive.org/details/game-basic-for-sega-saturn" target="_blank" rel="noreferrer noopener">Game BASIC for Sega Saturn disc</a></li><li>The ability to play Japanese Saturn games (A Japanese or modded console, a Pro Action Replay/<a href="https://ppcenter.webou.net/pskai/" target="_blank" rel="noreferrer noopener">Pseudo Saturn Kai</a> cartridge, or a Saturn emulator)</li><li>Any Saturn controller</li><li>Plenty of room in the Saturn’s internal memory (if you want to save your programs)</li></ul>



<p>For this option, you’ll run Game BASIC on the Saturn with no PC connection, using only standard Saturn accessories.&nbsp; This is a reasonable choice if you just want to write “Hello, World!”-style programs or play around with the neat games and demos that come with the kit.&nbsp; Theoretically, you can write even the most complex programs this way, but you’ll run into limitations on the size of games you can save to the Saturn’s internal memory.&nbsp; Plus, programming with a virtual keyboard is an absolute pain.&nbsp; Start here if you don’t have the necessary hardware for the other options, or if you just want to poke around a bit and see what this is all about.</p>



<h4>The Enhanced Saturn-only setup</h4>



<h5>Required Tools</h5>



<ul><li>All of the tools from Option 1, PLUS</li><li>Some kind of external expanded memory, such as:<ul><li>A direct-save memory cartridge (e.g., the official Saturn Backup Memory)</li></ul><ul><li>A Sega Saturn Floppy Disk Drive and some 3.5″ floppy disks</li></ul></li><li>A Sega Saturn keyboard OR the NetLink keyboard adapter with a PS/2 keyboard</li><li>Fun peripherals, like the Stunner light gun, 3D Control Pad, multi-tap, and Shuttle Mouse</li></ul>



<p>One of the great things about Game BASIC is how easy it makes it to access the Saturn’s peripherals, including the internal backup RAM, external memory cartridges, and even the Saturn Floppy Disk Drive.&nbsp; With a setup like this, you’ll have plenty of space to save your programs and you can use a real keyboard for text entry.&nbsp; You can even start experimenting with different forms of input, like analog controls and light guns!&nbsp; But without access to the tools a PC provides, it will be difficult to make nice-looking sprites, textures, and 3D models.&nbsp; So, this will still limit what you’re capable of.&nbsp; Regardless, this is a great option for the sheer fun factor of “Hey look! I’m programming with my Saturn!” or if you have no ability to connect your Saturn to a PC.</p>



<p>You can even go this route with a Saturn emulator, giving you easy access to improved keyboard, mouse, and storage options.&nbsp; I’ve confirmed that Mednafen successfully emulates Game BASIC and allows for keyboard and mouse pass-through input, meaning you can do a whole lot of Saturn development with very little barrier to entry.</p>



<h4>The Full Saturn plus PC Setup</h4>



<h5>Required Tools</h5>



<ul><li>A complete Game BASIC for Sega Saturn kit, including:<ul><li>A copy of the <a rel="noreferrer noopener" href="https://archive.org/details/game-basic-for-sega-saturn" target="_blank">Game BASIC for Sega Saturn disc</a></li></ul><ul><li>A copy of the <a rel="noreferrer noopener" href="https://archive.org/details/game-basic-for-sega-saturn" target="_blank">Windows 95 Tools disc</a></li></ul><ul><li>The special Saturn-to-PC serial cable adapter</li></ul></li><li>A modern PC with a USB port, capable of running a Virtual Machine (I use VirtualBox)</li><li>A copy of Windows XP SP3 32-bit to install on a VM</li><li>A <a href="https://www.amazon.com/USB-Serial-Adapter-Prolific-PL-2303/dp/B003WOWBBW" target="_blank" rel="noreferrer noopener">USB-to-Serial adapter</a> (Must support RS232 with a DB25 connector)</li><li>The ability to play Japanese Saturn games on original hardware (Japanese or modded console, or a Pro Action Replay/Pseudo Saturn Kai cartridge)</li><li>A Saturn controller</li><li>Optionally, any fun Saturn accessories you may want to experiment with (I especially recommend a keyboard or keyboard adapter)</li></ul>



<p>This is the Cadillac option!&nbsp; This is the setup I use, is how Game BASIC was really intended to be used (well, except nobody expected it to be run on a VM, I suppose), and is what the rest of this guide will focus on.&nbsp; With this setup, writing a game is as simple as writing BASIC in a text editor and hitting a couple of buttons to send it to your Saturn, where it immediately shows up on your TV and responds to controller and keyboard input!&nbsp; Seriously, it’s super cool once you get it working…</p>



<p>The Simple and Enhanced Saturn-only setups are extremely straightforward.&nbsp; You just boot Game BASIC like any other Saturn game and get started, so there’s not much configuration to discuss.&nbsp; Regardless of the setup you choose, continue on to Part 2 for a few test programs.&nbsp; But if you want the Full setup, it’s quite a project to get going, so read on to Part 3 for the complete How-To!</p>


<nav role="navigation"><!-- .nav-links --></nav><!-- .mpp-post-navigation -->	</div><!-- .entry-content -->

	 <!-- .entry-footer -->
</article></div>]]>
            </description>
            <link>https://flybacklabs.com/sega-saturn-homebrew-with-game-basic/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24913598</guid>
            <pubDate>Wed, 28 Oct 2020 00:22:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Five practices for serverless and distributed systems productivity]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24912370">thread link</a>) | @wfaler
<br/>
October 27, 2020 | https://chaordic.io/blog/serverless-distributed-system-productivity/ | <a href="https://web.archive.org/web/*/https://chaordic.io/blog/serverless-distributed-system-productivity/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                    
                  <p><img src="https://chaordic-public.s3.eu-central-1.amazonaws.com/images/productivity-1995786_960_720.jpg" alt=""></p>
<p>To productively build serverless &amp; distributed systems, we need to adopt new practices, some which may seem counterintuitive at first. This post will suggest five concrete practices to up your game.</p>
<p>Let’s jump in, roughly in order of importance:</p>
<h4 id="1-prefer-running-your-system-in-the-cloud-over-local-emulation">1. Prefer running your system in the cloud over local emulation</h4>
<p>Local development is sacred cow for many developers, being able to deploy, run and test an application or system on your laptop. But the reality of modern distributed systems is that some things cannot easily be emulated locally, if at all.</p>
<p>It is not for a lack of trying: systems running on Kubernetes are often emulated with Docker Compose or Minikube, with varying levels of success. For AWS Serverless, we have AWS SAM and LocalStack.</p>
<p>There are however a few issues with this type of emulation:</p>
<ul>
<li>The cost of maintaining emulation is high, since you effectively maintain two stacks - one for local development, and one for “real” deployment.</li>
<li>Emulation isn’t the real thing: you will find configuration drift, small or large differences in the behaviour of your stack.</li>
</ul>
<p>In summary, our experience of trying to maintain emulation is that the costs far outweigh the benefits. That effort should instead be put towards the ability to deploy local code quickly to the cloud environment, or where possible, connect a locally running process to a real environment in the cloud.</p>
<p>One of the great benefits of Serverless in particular, is that the cost of creating and provisioning <em>Feature Environments</em> is approaching zero. Moving local development to the cloud is a low-cost proposition.</p>
<h4 id="2-cicd-pipelines-are-not-enough-local-deployment-automation-is-crucial">2. CI/CD Pipelines are not enough, local deployment automation is crucial</h4>
<p>Given our stance on local emulation, the next instinct to suppress is that of relying on CI/CD to manage deployments to feature environments. If we rely entirely on CI pipelines for our ongoing development work, we end up wasting large amounts of time waiting for CI pipelines to build and deploy.</p>
<p>If instead, CI &amp; local development workflows can share as much as possible of deployment infrastructure, we should be able to reduce/remove this bottleneck on development, while also minimizing the duplicated effort of maintaining two types of automation.</p>
<p>In summary, any developer should trivially be able to:</p>
<ul>
<li>Deploy or connect locally built resources to their own feature environment in seconds with a single command from their laptop.</li>
<li>Deploy only the unit of deployment which has changed.</li>
<li>Have a development experience that is practically indistinguishable from “local development”.</li>
</ul>
<h4 id="3-for-aws-serverless--function-as-a-service---monolithic-functions-are-ok">3. For AWS Serverless &amp; “Function-as-a-Service” - monolithic functions are OK</h4>
<p>The first thing many teams do when they first start using AWS Lambda, is that they create separate deployable functions for every type of function invocation. For instance, a REST API gets a function for every single endpoint on the API. However, if you think in terms of <em>Domain Driven Design</em>, this might mean you end up with a number of functions that logically make up a single <em>Bounded Context</em>.</p>
<p>There are also practical considerations: as an example, AWS CloudFormation has a default limit of maximum 200 resources per stack. When you start adding up the multiplicative effect of the resources required for a Serverless application, you quickly realise that this is a limit that can be easily reached. Sticking to a service per Bounded Context, that acts as the target for multiple types of Lambda invocations is a sensible thing to do. Doing this will also reduce the automation and coordination overhead.</p>
<h4 id="4-consider-adopting-a-monorepo-with-tooling-appropriate-for-monorepos">4. Consider adopting a monorepo, with tooling appropriate for monorepos</h4>
<p>Let us start with a caveat emptor: monorepos without using appropriate tooling can be a disaster of slow CI pipelines &amp; low productivity. The upside is that some amazing and battle-tested tooling for monorepos exist these days (a personal favourite is <a href="https://bazel.build/">Bazel</a>, which originated from Google).</p>
<p>Without a monorepo, building distributed systems can be painful.</p>
<p>With dozens of repositories, code navigation for larger changes spanning multiple services become painful. Another thing that quickly becomes painful is dependency management, code sharing &amp; reuse. It is not uncommon to find that different components have different, mutually incompatible dependencies, which become painful to upgrade.</p>
<p>A monorepo negates these pains, while also making things such as security audits easier to conduct and address.</p>
<p>However, to avoid the “rebuild the universe” problem with monorepos, you need tooling that solves three problems:</p>
<ul>
<li>Change detection &amp; dependency-graph tracking.</li>
<li>Dependency-graph based rebuilds (rebuild only what has changed and what is invalidated by the change).</li>
<li>Build caching.</li>
</ul>
<h4 id="5-implement-all-three-pillars-of-observability">5. Implement all three pillars of observability</h4>
<p>Observability in control theory is defined as the ability to infer the internal state of a system by from knowledge about its external outputs. In practice, this is the triumvirate of event logs (log aggregation &amp; analytics), metrics (for alerting) &amp; tracing (driving visualization). Most organizations today skew heavily towards event logs only, with maybe some metrics, but very few do all three well.</p>
<p>In a distributed systems world, doing all three pillars of observability well means that the time to detect, find and address bugs and other issues can be cut down to a fraction of what it would otherwise be. Furthermore, great observability will also improve developer productivity, since we can better understand the state of our entire system.</p>
<p>This is perhaps an area where AWS Serverless stands out as a leader. While the Kubernetes eco-system is filled with many options of various complexity and quality, AWS gives us an easy way to achieve a high level of observability at low effort and cost. <em>CloudWatch Logs, CloudWatch Metrics</em> and <em>AWS X-Ray</em> can provide observability to a Serverless architecture at a relatively low threshold of effort and learning.</p>
<h4 id="conclusion">Conclusion</h4>
<p>It should be obvious by now that Serverless &amp; distributed systems require great discipline in deployment automation. It is unfortunate that many still make the distinction between deployment being an “ops” concern, separate from development.</p>
<p>Dev &amp; Ops in modern systems are intertwined, the level and quality of automation has a great impact on DevEx (Developer Experience), and thus developer productivity. It is not a concern that can be postponed or thought about after the fact: it requires effort initially, and disciplined refinement throughout. If this is done well, you will be able to develop with a speed, reliability and level of productivity that will run rings around the competition.</p>
<p>We will return to this subject in the future to show what a practical Serverless toolchain and reference architecture could look like. Feel free to sign up to our email list below to get notified when we do!</p>
                  
                  </div></div>]]>
            </description>
            <link>https://chaordic.io/blog/serverless-distributed-system-productivity/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24912370</guid>
            <pubDate>Tue, 27 Oct 2020 21:52:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[10 years, 8 months and 12 days]]>
            </title>
            <description>
<![CDATA[
Score 37 | Comments 14 (<a href="https://news.ycombinator.com/item?id=24911784">thread link</a>) | @app4soft
<br/>
October 27, 2020 | https://www.prototypo.io/blog/news/10-years-8-months-and-12-days/ | <a href="https://web.archive.org/web/*/https://www.prototypo.io/blog/news/10-years-8-months-and-12-days/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section id="lesanimals-page-wrapper" role="main">
        <section id="template-post-single" data-view-name="template-post-single" data-id="2969">

        <canvas data-text="News"></canvas>

        <span></span>

        

        <div>

            
            <div>
                <p>This article could have been entitled&nbsp;337,737,600 seconds, but hiding the fact that Prototypo started more than 10 years ago would have been a missed opportunity to show the dedication the team and I, as a founder, put in this project over the years.</p>
<p>Prototypo started as a student project when I studied Graphic Design at H.E.A.R Strasbourg in France. As a non-savvy Type Designer I was frustrated to not be able to complete the typeface projects I had in mind. Typefaces are made of rules and systems, right? Catcha! We can code something that follows rules, so we should succeed in coding fonts. That was the starting point of the next 10 years, and the beginning of the Roller Coaster, a.k.a creating a startup company.</p>
<p>Prototypo is a startup like many others: before having a stable and reliable business model, we put a lot of energy into developing innovative and useful technologies for our users.</p>
<p>But before being a startup, Prototypo is a company. Today we have reached the end of our resources without having found the expected Product Market Fit, and so the Break-even.</p>
<p>After several years of a strong dedication and passion for what we’ve built, we decided to shutdown the company.</p>
<p>Since the first second, I knew that it would be a hard journey with many obstacles on the path. But I regret nothing. The next 337,737,599 seconds were full of great experiences, shared with amazing people.</p>
<p>Thank you for those who supported us along the road, it was a great adventure.</p>
<p><a href="https://www.linkedin.com/in/yannick-mathey/">Yannick,</a> (former) CEO</p>

            </div>

            

        </div>

                    
        
    </section>
</section></div>]]>
            </description>
            <link>https://www.prototypo.io/blog/news/10-years-8-months-and-12-days/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24911784</guid>
            <pubDate>Tue, 27 Oct 2020 20:56:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hiring Myths Common in Hacker News Discussions]]>
            </title>
            <description>
<![CDATA[
Score 116 | Comments 159 (<a href="https://news.ycombinator.com/item?id=24911758">thread link</a>) | @Ozzie_osman
<br/>
October 27, 2020 | https://somehowmanage.com/2020/10/27/4-hiring-myths-common-in-hackernews-discussions/ | <a href="https://web.archive.org/web/*/https://somehowmanage.com/2020/10/27/4-hiring-myths-common-in-hackernews-discussions/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

	<section id="primary">
		<main id="main">

			
<article id="post-222">

	

	
	<div>
		
<p>Another day, another HackerNews discussion about hiring being broken. The most recent one I saw was triggered by <a href="https://blog.alinelerner.com/ive-been-an-engineer-and-a-recruiter-hiring-is-broken-heres-why-and-heres-what-it-should-be-like-instead/">a blog post</a> by the formidable Aline Lerner (disclaimer: Aline is a friend and we collaborated on a <a href="https://www.holloway.com/g/technical-recruiting-hiring/about">hiring book</a> last year). Now, I 100% agree that hiring is broken, and Aline’s post is really thoughtful. In fact, a lot of “hiring is broken” articles are thoughtful.</p>



<p>But the discussion threads are something else—they miss the point of the article. The discussion threads are even more broken than hiring. And they’re really repetitive. They always do contain grains of truth, but inevitably have us reaching conclusions that are simplistic, and in my opinion, create a pretty bad attitude in the tech industry.</p>



<p><strong>Conclusion #1: “Hiring sucks for candidates, but hiring managers can do what they want</strong>“</p>



<p>The truth is that hiring is hard for everyone. There’s no question about it. It’s hard for both candidates and for hiring managers. Sure, FAANGs and the startup-du-jour might have a leg up, but most people who are hiring are trying to hire at a non-FAANG, non-sexy company. If you’ve never done it, you should try it at some point in your career. It’s an <em>incredibly </em>humbling experience. Or, at the very least, find a friend who’s spent time on hiring, and ask them for their favorite battle story. They’ve been ghosted by candidates. They’ve spent hours trying to convince people to talk to them. They’ve spent even more time getting candidates to the offer stage, only to lose out to the FAANG / startup-du-jour.</p>



<p>And yes, on the balance, power and information asymmetry work out in favor of the companies hiring. And that asymmetry is much larger with FAANGs. But even FAANGs have to invest a tremendous amount of time and energy into hiring. It’s not really easy for anyone. </p>



<p>Especially if you want to do it <em>well</em>. Ask any successful leader (entrepreneur, manager) what they spend most of their time on, and it’ll either involve a large chunk spent on hiring (if they appreciate the problem and give it the attention it deserves) or dealing with the consequences of bad hiring (if they don’t).</p>



<p><strong>Conclusion #2: “Hiring is a crap-shoot—it’s a roll of the dice</strong>“</p>



<p>I strongly disagree with this one. When writing the <a href="https://www.holloway.com/g/technical-recruiting-hiring">Holloway Guide to Technical Hiring and Recruiting</a>, I got to interview dozens of really thoughtful hiring managers and recruiters. They were really good at their jobs. And there were some common themes. They were thoughtful about every step of their process. They kept their process balanced and fair, holding a high bar but respecting candidates and their time. They didn’t chase the same pool of candidates everyone else was chasing—instead, they found non-traditional ways to discover really talented and motivated people who weren’t in the pool of usual suspects. They were thoughtful about what signals they were looking for and how best to assess them. And, they deeply understood their team’s needs, and candidates’ needs, and were really good at deciding when there was or wasn’t a fit. But most of all, they were effective: they built really talented teams.</p>



<p>There are a handful of companies that have built amazing hiring engines, and the proof is that they’ve been able to put together really strong teams. You can generally tell that if a person worked at a certain company at a certain time, that person is probably incredibly intelligent and incredibly motivated (some examples are Google, Facebook, Stripe, Dropbox at different points in time). There will always be noise. Even the best hiring managers will sometimes make hiring mistakes. And of course, even the best engineers may not be a fit for every role or every company. </p>



<p>Again, hiring is hard. But there is not a shred of doubt in my mind that if you are thoughtful about it, you can hire well. And really, you don’t need to be perfect at it. You just need to be better than the rest.</p>



<p><strong>Conclusion #3: “FAANGs suck at hiring”</strong></p>



<p>This one has some truth to it, but it’s a lot more subtle than “FAANGs suck at hiring”. Because let’s face it, they do hire really smart people. Some of the smartest people I know are at FAANGs right now. So let’s decouple that statement a little more.</p>



<p>FAANGs <em>do </em>suck at parts of hiring, like their candidate experience. They can be really slow at making hiring decisions. Their hiring process might be tedious and seem arbitrary. But <em>they usually can get away with it</em>, <em>and you probably can’t!</em> They’ve got a strong brand, interesting technical challenges (interesting for some people, at least), and a lot of money. In fact, one FAANG VP of Engineering told me: “our process is what we can get away with”. To the point that they can even play it off as a positive: “our process is slow and long because we are <em>very</em> selective”.</p>



<p>And look, I’m sure FAANGs lose some talented candidates who get turned off by their “you’d-be-blessed-to-work-with-us” attitude. They definitely have a lot of room for improvement. But at the end of the day, they’re operating a process that’s delivering large quantities of really smart people at scale. In fact, I’d argue their internal processes around strategy, performance management / promotions, etc cause incredibly <em>more</em> damage to them than broken hiring—if you lose out on hiring one talented person when you have thousands applying to work for you, that’s one story, but if you hire someone really talented and driven, and they work for you for 6 to 12 months but don’t meet their potential and leave in bitter frustration… well, that’s a subject for another post)</p>



<p>“But”, people go on, “FAANGs <em>also</em> don’t know how to interview!” Which brings me to trope #4.</p>



<p><strong>Conclusion #4: “Whiteboard</strong> <strong>and algo/coding interviews suck”</strong></p>



<p>Again, this one has some truth to it, but if you just stop at the above statement, you miss the point.</p>



<p>Algo/coding interviews are one of the primary hiring mechanisms used by FAANG companies. And they are incredibly unpopular—at least in discussion threads. But big companies have spent years looking at their hiring data and feeding that back into their hiring process (coining the term “<a href="https://rework.withgoogle.com/subjects/people-analytics/">people analytics</a>” along the way).</p>



<p>The argument against them is usually a combination of:</p>



<ul><li>they really only assess pattern-matching skills (map a problem to something you’ve seen before)</li><li>they only assess willingness to spend time preparing for these types of interviews</li></ul>



<p>These are fair criticisms, but that doesn’t mean these interviews are actually terrible. I mean, they might be terrible for you if you’re interviewing and you don’t get the job. You’re probably a brilliant engineer, and I agree, these interviews certainly don’t fully assess your ability (or maybe you’re a shit engineer, I don’t know you personally). In any case, the leap from “this interview sucked for me” to “this interview sucks” is still pretty big.</p>



<p>If you’re a large tech co with a big brand and a salary scale that ranks at the top of&nbsp;<a href="https://www.levels.fyi/">Levels.fyi</a>, you probably get a lot of applications. So a good interview process is one that weeds out people who wouldn’t do well at your company. To do well at a large tech company, you need to (and I’m painting with a really broad brush, but this is true for 90% of roles at these companies):</p>



<ol><li>Some sort of problem-solving skill that’s a mix of raw intelligence and/or ability to solve problems by pattern-matching to things you’ve seen before.</li><li>Ability/commitment to work on something that may not&nbsp;<em>always&nbsp;</em>be that intrinsically motivating, in the context of getting/maintaining a well-paying job at a large, known company.</li></ol>



<p>Hopefully you can see where I’m going with this. Basically, the very criticisms thrown at these types of interviews are the reason they work well for these companies. They’re a good proxy for the work you’d be doing there and how willing you are to do it. If you’re good at pattern matching, and are willing to invest effort into practicing to get one of these jobs, you’ll probably do well at the job.</p>



<p>Not that there’s anything wrong with that type of work. I spent several years at big tech co’s, and the work was intellectually stimulating most of the time. But a lot of times it wasn’t. It was a lot of pattern-matching. Looking at how someone else had solved a problem in a different part of the code-base, and adapting that to my use-case.</p>



<p>On the other hand, if you’re an engineer (no matter how brilliant) who struggles with being told what to do or doing work that you can’t immediately connect to something intrinsically motivating to you, that FAANG interview just did both you and the company a favor by weeding you out of the process.</p>



<p>So the truth is, there is no single “best interview technique”. In our book, we wrote several chapters about different interviewing techniques and their pros and cons. In-person algo/coding interviews on a whiteboard, in-person interviews where you work in an existing code base, <a href="https://www.holloway.com/g/technical-recruiting-hiring/sections/take-homes">take-home interviews</a>, pairing together, having a trial period, etc all have pros and cons. The trick is finding a technique that works for both the company and the candidate. </p>



<p>And that can really differ from company to company and candidate to candidate. A VP at Netflix told me about how they had a really strong candidate come in, but when asked to do a whiteboard-type interview, informed them (politely) that they might as well just reject him then. He was no good at whiteboard interviews… But if they allowed him to go home and write some code, he’d be happy to talk through it. And since then, many Netflix teams have offered candidates the choice of doing a take home.</p>



<p>And really, any interview format can suck. It can fail to assess a candidate for the things a company needs and it can be a negative candidate experience. Which would you rather have:</p>



<ul><li>A whiteboard interview with heavy algorithms for a role where that knowledge (or ability to develop that knowledge) isn’t critical, delivered by an apathetic engineer who doesn’t care about their job.</li><li>A …</li></ul></div></article></main></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://somehowmanage.com/2020/10/27/4-hiring-myths-common-in-hackernews-discussions/">https://somehowmanage.com/2020/10/27/4-hiring-myths-common-in-hackernews-discussions/</a></em></p>]]>
            </description>
            <link>https://somehowmanage.com/2020/10/27/4-hiring-myths-common-in-hackernews-discussions/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24911758</guid>
            <pubDate>Tue, 27 Oct 2020 20:53:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Get started with 2-minute rule]]>
            </title>
            <description>
<![CDATA[
Score 204 | Comments 64 (<a href="https://news.ycombinator.com/item?id=24911312">thread link</a>) | @hoanhan101
<br/>
October 27, 2020 | https://hoanhan.co/2-minute-rule | <a href="https://web.archive.org/web/*/https://hoanhan.co/2-minute-rule">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main" role="main"><article role="article"><p>Scale any task down into a 2-minute version to make it easier to get started.</p><time datetime="2020-10-27T00:00:00-04:00"> October 27, 2020 · 1 min read · <a href="https://hoanhan.co/category/Motion">Motion</a><hr> </time><p>Whenever you find it hard to get started on a task, consider scaling it down into a 2-minute version. For example,</p><ul><li>Read a book → Read one page</li><li>Write an essay → Write one sentence</li><li>Run 10 miles → Wear my running shoes</li><li>Do 100 push-ups → Do 1 push up</li><li>Eat more vegetables → Eat an apple</li><li>Study for interview → Skim through my notes</li><li>Build a program → Code a function</li></ul><p>The idea is to make it super easy to get started. Once you pass the starting point, which is arguably the hardest step, you start to gain momentum to keep doing the task itself:</p><ul><li>Read one page → Read 10 pages → Finish the first chapter</li><li>Write one sentence → Write an opening paragraph → Write the body</li><li>Wear my running shoes → Walk for 5 minutes → Run for 5 minutes</li></ul><p>As you can see, once you start, it is much easier to continue doing it. Sometimes, you’ll find yourself completing the task even before you even notice it.</p><blockquote><p>For more insights on system planning and goal setting, feel free to check out <a href="https://hoanhan.co/motion">this guide</a>. If you’re curious about how I apply it on a daily basis, <a href="https://motion.hoanhan.co/goals/hoanhan/">check this out →</a></p></blockquote><hr><p><strong>References:</strong></p><ul><li><a href="https://jamesclear.com/how-to-stop-procrastinating">https://jamesclear.com/how-to-stop-procrastinating</a></li><li><a href="https://www.lifehack.org/articles/productivity/how-stop-procrastinating-and-stick-good-habits-using-the-2-minute-rule.html">https://www.lifehack.org/articles/productivity/how-stop-procrastinating-and-stick-good-habits-using-the-2-minute-rule.html</a></li></ul><hr><hr><p> Tagged: <a href="https://hoanhan.co/tag/motion.hoanhan.co">#motion.hoanhan.co</a>, <a href="https://hoanhan.co/tag/consistency">#consistency</a>, <a href="https://hoanhan.co/tag/start">#start</a></p><br> </article></div></div>]]>
            </description>
            <link>https://hoanhan.co/2-minute-rule</link>
            <guid isPermaLink="false">hacker-news-small-sites-24911312</guid>
            <pubDate>Tue, 27 Oct 2020 20:13:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pants 2.0.0 released – Concurrently cache and orchestrate modern Python builds]]>
            </title>
            <description>
<![CDATA[
Score 19 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24911148">thread link</a>) | @stuhood
<br/>
October 27, 2020 | https://blog.pantsbuild.org/introducing-pants-v2/ | <a href="https://web.archive.org/web/*/https://blog.pantsbuild.org/introducing-pants-v2/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<h3 id="pants-2-0-0-the-first-stable-release-of-the-pants-v2-open-source-build-system-is-out-now-">Pants 2.0.0, the first stable release of the Pants v2 open-source build system, is out now!</h3><p>There are so many tools in the Python development ecosystem. You might use <a href="https://pip.pypa.io/en/stable/">pip</a> to resolve dependencies, <a href="https://docs.pytest.org/en/stable/">pytest</a> to run tests, <a href="https://flake8.pycqa.org/en/latest/">flake8</a> and <a href="https://www.pylint.org/">pylint</a> for lint checks, <a href="https://black.readthedocs.io/en/stable/">black</a> and <a href="https://pycqa.github.io/isort/">isort</a> for auto-formatting, <a href="http://mypy-lang.org/">mypy</a> for type checking, <a href="https://ipython.org/">IPython</a> or <a href="https://jupyter.org/">Jupyter</a> for interactive sessions, <a href="https://setuptools.readthedocs.io/en/latest/">setuptools</a>, <a href="https://pex.readthedocs.io/en/latest/">pex</a> or <a href="https://www.docker.com/">docker</a> for packaging, <a href="https://developers.google.com/protocol-buffers">protocol buffers</a> for code generation, and many more. Not to mention any custom tooling you've built for your repo. </p><p>Installing, configuring and orchestrating the invocation of these tools<strong>—</strong>all while not re-executing work unnecessarily<strong>—</strong>is a hard problem, especially as your codebase grows. The lack of a robust, scalable build system for Python has been a problem for a long time, and this has become even more acute in recent years, with Python codebases increasing in size and complexity. </p><p>Fortunately, there is now a tailor-made (pun intended) solution: <strong>Pants v2</strong>!</p><p><a href="https://www.pantsbuild.org/">Pants v2</a> is designed from the ground-up for fast, consistent builds. Some noteworthy features include:</p><ul><li>Minimal metadata and boilerplate</li><li>Fine-grained workflow</li><li>Shared result caching</li><li>Concurrent execution</li><li>A responsive, scalable UI</li><li>Unified interface for multiple tools and languages</li><li>Extensibility and customizability via a plugin API</li></ul><figure><img src="https://blog.pantsbuild.org/content/images/2020/10/render1603750306032.gif" alt=""><figcaption>Pants running multiple linters in parallel</figcaption></figure><p>Read on to learn more about Pants v2, and what it means for your Python codebase.</p><hr><h2 id="a-little-history">A little history</h2><p>We started the original open-source Pants project back in 2011. At the time, we were frustrated by slow, flaky Scala builds. The leading strategy for scaling was to hand each developer a RAM stick and a screwdriver... Surely this was a problem we could tackle with software! Thus Pants v1 was born. </p><p>Pants v1 was quite successful, and was adopted at cutting-edge tech companies such as Twitter, Foursquare, Square and others. But we still weren't satisfied: The APIs were clunkier than we would have liked, the UI was overly chatty, caching was hard to get right, and concurrent execution had to be special-cased. We knew there were plenty of performance and stability improvements to be had, if we could only unlock them. </p><p>We learned a lot from our years of work on Pants v1, and knew that we could design something new and better, leaning on our experience with v1 while addressing the drawbacks of that system. Luckily, at the same time as we began thinking about this hypothetical next system, a new motivating problem emerged: Python builds.</p><h2 id="python-builds-today">Python builds today</h2><p>As you probably know, Python has skyrocketed in popularity in recent years. Not only is it used to build a wide variety of server applications, via frameworks such as <a href="https://www.djangoproject.com/">Django</a> and <a href="https://flask.palletsprojects.com/en/1.1.x/">Flask</a>, but it's also the language of choice for data scientists, thanks to powerful libraries and tools such as <a href="https://numpy.org/">NumPy</a>, <a href="https://www.scipy.org/">SciPy</a>, <a href="https://pandas.pydata.org/">Pandas</a> and <a href="https://jupyter.org/">Jupyter</a>. </p><p>Python hits a sweet spot of simplicity and power, but there is a big problem - there is no truly great scalable build tool for Python, and this is becoming a real pain point as Python repos grow like never before. &nbsp;</p><p>Python builds today involve manually invoking a wide variety of tools. Each tool has to be installed, configured and invoked in just the right way, often while sequencing the output of one tool into input of another. Knowing how to use each tool in a given scenario is complicated and burdensome. </p><p>Sure, you can hack around the problem for a while with some combination of shell scripts, <a href="https://www.gnu.org/software/make/manual/make.html">Makefiles</a>, <a href="https://tox.readthedocs.io/en/latest/">tox</a>, and <a href="https://python-poetry.org/">poetry</a>. But even a small code change might require you to run a huge amount of sequential build work. Re-executing the same processes with the same inputs over and over again is a frustrating waste of time and resources. &nbsp;And these solutions start to break down as your codebase grows.</p><p>Perhaps you experimented with more complex build systems, such as <a href="https://bazel.build/">Bazel</a> or <a href="https://v1.pantsbuild.org/">Pants v1</a>. &nbsp;But it's laborious to maintain all that BUILD metadata, all for a sub-par experience not optimized for Python. Not to mention the difficulty of implementing your own custom build logic. </p><p>Alternatively, maybe you've been tempted to split up your codebase into multiple interdependent repos, each with their own "smaller" builds. But that creates an even thornier problem, namely how to manage those interdependencies. Having to propagate changes across codebase boundaries can slow development down to a crawl, and leave you with the worst of both worlds - slower processes and a fragmented, unmanageable codebase.</p><p>A great build system for repos - of all sizes - that include Python code would support fine-grained invalidation and caching, so that it only executes the build work actually affected by a change. It would support concurrent local and even remote execution, to greatly speed up work by using all available CPU. It would be easy to adopt in a small repo, but would scale up as your codebase grows. It wouldn't require huge amounts of boilerplate metadata, and it would be easy to extend with custom build logic. </p><p>Well, Pants v2 is that system! </p><h2 id="introducing-pants-v2">Introducing Pants v2</h2><p><a href="https://www.pantsbuild.org/">Pants v2</a> is a completely new open-source build system, inspired by our work on Pants v1. &nbsp;We've been developing and testing it for the last couple of years, and it's finally ready for prime time!</p><p>A key factor in the design of Pants v2 was a set of lessons we learned from Pants v1 and other existing systems, such as Bazel. Among them: that ease of use and performance matter, boilerplate is annoying, concurrency and caching require hard design work, and most people will need custom logic at some point.</p><h3 id="lesson-1-ease-of-use-and-performance-both-matter">Lesson #1: Ease of use and performance both matter</h3><p>When designing software you often find yourself making tradeoffs between ease of use and performance. But in a build system, both are vital. The Pants v2 execution engine - which is the performance-critical heart of the system - is written in <a href="https://www.rust-lang.org/">Rust</a>, for raw speed. And the domain-specific build logic is written in familiar, easy to work with, type-annotated Python 3. This helps make Pants v2 easy to extend, without compromising performance. </p><p>Pants v2 also runs a daemon that memoizes fine-grained build state in memory, for even faster performance. This daemon watches for changes to your source files and precisely invalidates its state on the fly to ensure that the minimum amount of work happens the next time you build.</p><h3 id="lesson-2-writing-build-metadata-is-a-real-drag">Lesson #2: Writing build metadata is a real drag</h3><p>Some build tools are slow because they don't have enough information about the structure of your code to intelligently perform incremental work. Others have gone too far in the other direction, requiring a huge amount of metadata and boilerplate in BUILD files, especially relating to your code's dependencies. </p><p>Pants v2 offers the best of both worlds - intelligent, fine-grained incremental work, without the boilerplate. It does so by assuming sensible, magic-free defaults, inferring dependencies from the import statements in your code, and supporting plugins for custom inference logic. Stay tuned for an upcoming post on exactly how Pants achieves this!</p><h3 id="lesson-3-design-for-caching-concurrency-and-remoting">Lesson #3: Design for caching, concurrency and remoting </h3><p>Writing build logic that can be cached and executed concurrently and remotely is very hard. You have to be very careful about not producing or consuming side-effects, and it's extremely difficult to tack that on later. And unless you design your APIs with care, supporting these kinds of features often places severe restrictions on what your build logic may safely do. </p><p>In Pants v2, build logic is composed of <a href="https://en.wikipedia.org/wiki/Pure_function">pure</a> Python 3 <a href="https://docs.python.org/3/library/asyncio-task.html">async coroutines</a>. So a build rule can depend not only on its inputs, but can also await on new data at runtime - all of which is precisely tracked for invalidation and caching. This gives us the best of both worlds: logic that is properly isolated from side-effects, and is therefore amenable to caching, concurrent execution and remoting, while still allowing the use of natural control flow.</p><figure><img src="https://blog.pantsbuild.org/content/images/2020/10/caching.gif" alt=""><figcaption>We run both tests, then add a syntax error to one test and rerun; the unmodified test uses the cache and is isolated from the syntax error.</figcaption></figure><h3 id="lesson-4-almost-everyone-needs-to-customize-their-builds">Lesson #4: Almost everyone needs to customize their builds</h3><p>Most teams have custom build steps, so extensibility is a key feature in any build system. Pants v2 is built around a <a href="https://www.pantsbuild.org/docs/plugins-overview">plugin architecture</a>. You can write your own rules using the same API as the built-in functionality. So your custom build logic will enjoy the same fine-grained invalidation, caching, concurrency and remote execution abilities as the core Pants code.</p><h2 id="pants-2-0-0-is-out-now-">Pants 2.0.0 is out now!</h2><p>All this leads me to the happy announcement that <a href="https://pypi.org/project/pantsbuild.pants/2.0.0/">Pants 2.0.0</a>, the first stable release of Pants v2, is out now! 2.0.0 is the culmination of years of design and development work, and many months of beta testing at several organizations. So we're really happy, proud (and relieved…) to finally have it ready for general use. </p><p>You can see what Python tools Pants currently supports <a href="https://www.pantsbuild.org/docs/python">here</a>. There are also commands for querying and understanding your dependency graph, and a robust help system. &nbsp;We're adding support for additional tools and features all the time, and it's straightforward to implement your own. Beta users have already written their own logic for Cython and docker, for example. </p><p>Now is a great time to adopt Pants 2.0.0! The team that developed Pants v2 is <a href="https://www.pantsbuild.org/docs/community">ready to help you</a> onboard, answer any questions, and even pair with you to help you write any custom build logic. We're also eager to get feedback, bug reports and suggestions for what features we should focus on in the next weeks and months of development.</p><p>Pants v2 is developed by a helpful open source community, is funded by a 501(c)6 non-profit, and has excellent support available. If you have a growing Python codebase, and want to take Pants 2.0.0 for a spin, <a href="https://www.pantsbuild.org/docs/community">let us know</a>. We'd love to fit you with some new Pants today!</p>
			</section></div>]]>
            </description>
            <link>https://blog.pantsbuild.org/introducing-pants-v2/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24911148</guid>
            <pubDate>Tue, 27 Oct 2020 19:58:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[We Will Never Have Enough Software Developers]]>
            </title>
            <description>
<![CDATA[
Score 34 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24910949">thread link</a>) | @bartdegoede
<br/>
October 27, 2020 | https://whoisnnamdi.com/never-enough-developers/ | <a href="https://web.archive.org/web/*/https://whoisnnamdi.com/never-enough-developers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://whoisnnamdi.com/content/images/size/w300/2020/10/header-v2-resized.png 300w,
                            https://whoisnnamdi.com/content/images/size/w600/2020/10/header-v2-resized.png 600w,
                            https://whoisnnamdi.com/content/images/size/w1000/2020/10/header-v2-resized.png 1000w,
                            https://whoisnnamdi.com/content/images/size/w2000/2020/10/header-v2-resized.png 2000w" sizes="(max-width: 800px) 400px,
                            (max-width: 1170px) 700px,
                            1400px" src="https://whoisnnamdi.com/content/images/size/w2000/2020/10/header-v2-resized.png" alt="Why We Will Never Have Enough Software Developers">
            </figure>

            <section>
                <div>
                    <p>We will never have enough software developers.</p><p>Developers are dropping out of the profession in large numbers despite efforts to grow the number of computer science graduates and software engineers.</p><p>Here's why.</p><h2 id="developer-dropout-is-real">Developer dropout is real</h2><p>Software development has a <em>serious</em> retention problem:</p><ul><li>At age 26, 59% of engineering and computer science grads work in occupations <em>related</em> to their field of study. By age 50, only 41% work in the same domain, meaning a full <strong>~30% drop out of the field by mid-career</strong></li><li>In contrast, engineering and computer science majors who join <em>unrelated</em> fields upon graduation retain at much higher rates, with only 10-15% switching out after the age of 26:</li></ul><figure><img src="https://whoisnnamdi.com/content/images/2020/10/W-kECKz1nw.png"></figure><p>Engineers often leave engineering for non-STEM management roles. Graduation into management is not surprising. What's surprising is that these are <strong>non-STEM</strong> positions. Engineers swap technical roles for <em>non-technical</em> roles over time.</p><p>This phenomenon, which I'll call "<strong>developer dropout</strong>," is a real problem. What's behind it?</p><!--kg-card-begin: html--><section>
    <h3>Receive my next long-form post</h3><p>Thoughtful analysis of the business and economics of tech</p>
                

</section><!--kg-card-end: html--><h2 id="out-with-the-old-skills-in-with-the-new-skills">Out with the old skills, in with the new skills</h2><p>Programming-related jobs have high rates of skill turnover. Over time, the types of skills required by companies hiring software developers change more rapidly than any other profession.</p><p>To demonstrate this, <a href="https://academic.oup.com/qje/article/135/4/1965/5858010">researchers</a> analyzed job postings on more than 40,000 online job boards and company websites between 2007 and 2019, controlling for employer, location, and occupation. They defined "new" skills as those that were rare or non-existent in 2007 but prevalent in 2019 and "old" skills as those that were prevalent in 2007 but rare or extinct in 2019.</p><ul><li>While only 30% of all job vacancies required at least one new skill by 2019, <strong>47% of computer and mathematical jobs required at least one new skill</strong> (i.e. a skill that was not common back in 2007)</li><li>This compares to <em>less than 20%</em> of jobs in fields like education, law, and community and social services</li><li>In addition, <strong>16% of jobs in computer and mathematical fields in 2007 required a skill that was obsolete by 2019</strong> (i.e. a skill that was common in 2007 but relatively rare in 2019), more than double any other job category:</li></ul><figure><img src="https://whoisnnamdi.com/content/images/2020/10/azDsA9n3rx.png"></figure><p>About a third of the change in required skills in computer-related occupations is due to specific new software:</p><ul><li>The fastest-growing software skills between 2007 and 2019 include <strong>Python, R, and Apache Hadoop</strong></li><li>Software that was popular in 2007 but effectively obsolete by 2019 includes QuarkXpress, ActionScript, Solaris, IBM Websphere, and Adobe Flash (ah, finally a name I recognize)</li></ul><p>Data science, machine learning, and AI saw big increases among technology-intensive jobs as well. For example, the number of STEM-related jobs requiring skills in machine learning and AI grew more than 4x from 2007-2017, touching more than 15% of STEM jobs:</p><figure><img src="https://whoisnnamdi.com/content/images/2020/10/Pc3AjVflhW.png"></figure><p>To better compare rates of skill change across occupations, the researchers came up with a measure of skill change that tracks the absolute growth or decline of various skills within each profession from 2007 to 2019. Occupations whose required skills change rapidly in prevalence among job postings receive a high score, while jobs whose skills do not change much receive a lower score:</p><figure><img src="https://whoisnnamdi.com/content/images/2020/10/xfeKvOWxo-.png"></figure><ul><li><strong>Computer-related occupations receive the highest score by far, 4.8</strong>. Note that the mean and standard deviation of this measure are ~3 and ~1 respectively, so computer-related jobs are <strong>nearly two standard deviations away from the typical job in America</strong></li><li>Meanwhile, jobs in education and and those involving manual labor have very low skill change scores, typically less than 2.</li></ul><p>We can get even more granular and look at specific job roles. This level of detail makes the difference even more stark (only showing the fastest changing roles):</p><figure><img src="https://whoisnnamdi.com/content/images/2020/10/OwoB5xsSdO.png"></figure><p>Web development has the highest rate of skill change <em>among all jobs in the country</em>. Next up are sales engineers, another often technical role. Database administrators, computer network architects, sysadmins, and application developers all make the top 10, and we see many other technical roles among the top 30. The mean and standard deviation are similar here, placing web development <strong>more than 3 standard deviations away from the typical job in America</strong> in terms of skill change over time.</p><p>Suffice to say, <strong>software development is a rapidly changing profession</strong>.</p><p>You might think, however, that skill change would eventually settle down as one becomes more experienced.</p><p><em>You'd be wrong.</em> The skills for software engineering jobs change rapidly throughout the entire career lifecycle:</p><figure><img src="https://whoisnnamdi.com/content/images/2020/10/7ItjXMaTE-.png"></figure><ul><li>In entry-level roles in computer and engineering occupations all the way through those requiring 12+ years of experience, <strong>the proportion of job postings requiring at least one new skill in 2019 was effectively the same, 40-45%</strong></li><li>In contrast, <strong>29% of entry-level non-computing and engineering roles in 2019 required at least one new skill, but this proportion declines to 24%</strong> for jobs requiring more than four years of experience</li></ul><blockquote>This means that experienced STEM workers seeking employment in 2019 are often required to possess skills that <strong>were not required</strong> when they entered the labor market in 2007 or earlier.</blockquote><p>Software engineers <strong>never</strong> escape the skill-change vortex, even many years into their careers. Experienced engineers must learn and adopt technologies that didn't even exist when they started out. Developers must constantly retool themselves, even well after their <a href="https://whoisnnamdi.com/college-degrees-software-engineers/">formal education</a> ends.</p><h2 id="nothing-s-changed-but-my-change"><a href="https://youtu.be/m1ERvlxgCD8?t=166">Nothing's changed but my change</a></h2><p><strong>College majors associated with faster changing jobs pay more early on.</strong></p><ul><li>In professions with one standard deviation increased skill change, pay is <strong>~30%</strong> higher in the first few years of one's career</li><li>If we exclude both the fastest and slowest-changing fields (Engineering/Computer Science at the high end, Health/Education at the low end), the early earnings premium for faster-changing roles increases to <strong>~60%</strong>:</li></ul><figure><img src="https://whoisnnamdi.com/content/images/2020/10/heMo13OsG1.png"></figure><p><strong>Fast-changing fields pay better.</strong></p><p>Notice however that the pay advantage declines over time. By the time one approaches the age of 50, the pay premium for working in rapidly changing fields falls dramatically to only 20-30% vs slower changing professions.</p><p>Here's another way to see the eroding pay advantage. The below chart simulates the earnings of the average worker by category of college degree from ages 23 to 50 in 2016 dollars.</p><ul><li>Computer science and engineering grads start off with sizable advantage vs any other major</li><li>However, this premium <em>falls</em> over time as the earnings of CS and engineering graduates plateau over time while the earnings of their peers grow <em>faster</em> for <em>longer</em></li><li>In fact, <strong>life and physical science graduates' earnings surpass their computer and engineering classmates by the age of 40</strong>:</li></ul><figure><img src="https://whoisnnamdi.com/content/images/2020/10/Lifecyle-Earnings-by-Degree-Category.png"></figure><p>Excluding business majors, the earnings premium of software engineering declines over time in both percentage <em>and</em> absolute dollar terms, to the point where engineers barely out-earn social science majors:</p><figure><img src="https://whoisnnamdi.com/content/images/2020/10/Engineering-_-Computer-Science-Earnings-Premium.png"></figure><p>But the focus on college major is somewhat misleading. This phenomenon has less to do with one's field of study and more to do with <em>choice of occupation</em>.</p><p>To show this, researchers plotted the earnings premium of various categories workers relative to those with a non-Engineering/Computer Science major working in a non-Engineering/Computer Science job.</p><ul><li>Workers who major in Engineering or Computer Science but work in unrelated fields actually see their earnings advantage <em>compound</em> over time, rather than decline</li><li>On the other hand, regardless of major, individuals who work in Engineering or Computer Science jobs see their earnings advantage erode over the years:</li></ul><figure><img src="https://whoisnnamdi.com/content/images/2020/10/OFffH9kBKA.png"></figure><blockquote><strong>Declining relative returns is a feature of STEM jobs, not majors.</strong> The earnings premium for non-STEM majors in STEM occupations starts off near 40%, but declines to 20% within a decade. In contrast, the relative earnings advantage grows over time for computer science and engineering majors working in non-STEM occupations.</blockquote><p>The <strong>profession</strong> of software development drives the declining earnings premium, <strong>not the college major</strong>.</p><p>In fact, computer science majors who work in non-CS fields experience the <em>opposite</em> dynamic of their non-developer peers — their relative earnings premium rises as they advance. A CS major who eschews the profession doesn't earn much more than otherwise similar non-CS majors early on, but eventually out-earns their peers by nearly 20%.</p><p>OK, that's enough about <em>what</em> is happening. Now let's see <em>why</em> it's happening.</p><h2 id="human-capital-depreciates-too"><em>Human</em> capital depreciates too</h2><p>Imagine a simple model where workers choose their profession in order to maximize income, which is a derivative of their own skill or human capital. Over time, workers gain new skills, while the value of their existing skills depreciates somewhat due to changing times.</p><p>Some workers, endowed with superior ability, learn faster than others, picking up skills at a quicker pace. Those workers will tend to sort into high-skilled, fast-changing professions initially, maximizing their early career earnings. Less impressive workers will sort into low-skilled, slower-changing professions.</p><p>In a world where human capital never depreciated, we could imagine that high-skilled individuals like software developers would maintain a relative human capital (and earnings) advantage over other professionals, leading to consistently increasing pay and a stable relative premium:</p><figure><img src="https://whoisnnamdi.com/content/images/2020/10/Human-Capital--w_o-Depreciation-.png"></figure><figure><img src="https://whoisnnamdi.com/content/images/2020/10/Software-Engineering-Human-Capital-Premium--w_o-Depreciation-.png"></figure><p>But, if human capital depreciates over time and that rate of depreciation is higher in rapidly-changing fields like software development, then developers' initial advantage would erode over time, narrowing the gap vs. non-developers:</p><figure><img src="https://whoisnnamdi.com/content/images/2020/10/Human-Capital--w_-Depreciation-.png"></figure><figure><img src="https://whoisnnamdi.com/content/images/2020/10/Software-Engineering-Human-Capital-Premium--w_-Depreciation-.png"></figure><p>This simple model helps explain what we see in the data — the software engineering earnings advantage disappears as the <em>effective</em> human capital gap narrows.</p><blockquote>Applied majors such as computer science, engineering, and business teach vintage-specific skills that become less valuable as new skills are introduced to the workplace …</blockquote></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://whoisnnamdi.com/never-enough-developers/">https://whoisnnamdi.com/never-enough-developers/</a></em></p>]]>
            </description>
            <link>https://whoisnnamdi.com/never-enough-developers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24910949</guid>
            <pubDate>Tue, 27 Oct 2020 19:36:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Apple’s A14 Packs 134M Transistors/mm²]]>
            </title>
            <description>
<![CDATA[
Score 312 | Comments 171 (<a href="https://news.ycombinator.com/item?id=24910778">thread link</a>) | @jonbaer
<br/>
October 27, 2020 | https://semianalysis.com/apples-a14-packs-134-million-transistors-mm2-but-falls-far-short-of-tsmcs-density-claims/ | <a href="https://web.archive.org/web/*/https://semianalysis.com/apples-a14-packs-134-million-transistors-mm2-but-falls-far-short-of-tsmcs-density-claims/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

			
				
<article id="post-604">
	
   
   
   <div>

   
   
      

   	
   	<div>
   		
<figure><amp-img width="1024" height="852" src="https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/a.png?resize=1024%2C852&amp;ssl=1" alt="" srcset="https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/a.png?resize=1024%2C852&amp;ssl=1 1024w, https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/a.png?resize=300%2C250&amp;ssl=1 300w, https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/a.png?resize=768%2C639&amp;ssl=1 768w, https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/a.png?w=1200&amp;ssl=1 1200w" sizes="(max-width: 1024px) 100vw, 1024px" layout="intrinsic" disable-inline-width="" i-amphtml-layout="intrinsic"><img loading="lazy" width="1024" height="852" src="https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/a.png?resize=1024%2C852&amp;ssl=1" alt="" srcset="https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/a.png?resize=1024%2C852&amp;ssl=1 1024w, https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/a.png?resize=300%2C250&amp;ssl=1 300w, https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/a.png?resize=768%2C639&amp;ssl=1 768w, https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/a.png?w=1200&amp;ssl=1 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9Jzg1Micgd2lkdGg9JzEwMjQnIHhtbG5zPSdodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZycgdmVyc2lvbj0nMS4xJy8+"></amp-img></figure>



<p>Our friends over at ICmasters have delved into the package of the Apple A14 Bionic. The die size has been unmasked, and it stands in at 88mm<sup>2</sup>. Despite cramming in 11.8 billion transistors, the die size is incredibly small thanks to utilization of TSMC’s 5nm process node.</p>



<figure><amp-img width="1024" height="573" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/0.png?resize=1024%2C573&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/0.png?resize=1024%2C573&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/0.png?resize=300%2C168&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/0.png?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/0.png?w=1200&amp;ssl=1 1200w" sizes="(max-width: 1024px) 100vw, 1024px" layout="intrinsic" disable-inline-width="" i-amphtml-layout="intrinsic"><img loading="lazy" width="1024" height="573" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/0.png?resize=1024%2C573&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/0.png?resize=1024%2C573&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/0.png?resize=300%2C168&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/0.png?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/0.png?w=1200&amp;ssl=1 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9JzU3Mycgd2lkdGg9JzEwMjQnIHhtbG5zPSdodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZycgdmVyc2lvbj0nMS4xJy8+"></amp-img></figure>



<p>The march of progress is not all rosy. Apple’s chips have historically achieved 90%+ of the process node’s theoretical density in their processors. This generation stands out by missing that mark by a large amount. A14 comes in at a cool 78% effective transistor density when compared to theoretical density. Despite TSMC claiming a 1.8x shrink for N5, Apple only achieves a 1.49x shrink.</p>



<figure><amp-img width="1024" height="263" src="https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?resize=1024%2C263&amp;ssl=1" alt="" srcset="https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?resize=1024%2C263&amp;ssl=1 1024w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?resize=300%2C77&amp;ssl=1 300w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?resize=768%2C197&amp;ssl=1 768w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?resize=1536%2C394&amp;ssl=1 1536w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?resize=2048%2C526&amp;ssl=1 2048w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?resize=1200%2C308&amp;ssl=1 1200w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?w=2280&amp;ssl=1 2280w" sizes="(max-width: 1024px) 100vw, 1024px" layout="intrinsic" disable-inline-width="" i-amphtml-layout="intrinsic"><img loading="lazy" width="1024" height="263" src="https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?resize=1024%2C263&amp;ssl=1" alt="" srcset="https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?resize=1024%2C263&amp;ssl=1 1024w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?resize=300%2C77&amp;ssl=1 300w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?resize=768%2C197&amp;ssl=1 768w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?resize=1536%2C394&amp;ssl=1 1536w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?resize=2048%2C526&amp;ssl=1 2048w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?resize=1200%2C308&amp;ssl=1 1200w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2-1.png?w=2280&amp;ssl=1 2280w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9JzI2Mycgd2lkdGg9JzEwMjQnIHhtbG5zPSdodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZycgdmVyc2lvbj0nMS4xJy8+"></amp-img></figure>



<p>This is not due to a failure of TSMC or Apple. These companies are clear leaders for the manufacturing and design of semiconductors respectively. Instead, this failure to convert theoretical to effective density stems from the slow death of SRAM scaling. SRAM is extensively used throughout a processor from registers to caches. Geoffrey Yeap of TSMC claims that the typical mobile SoC which consists of 60% logic, 30% SRAM, and 10% analog/IO.</p>



<figure><amp-img width="1024" height="576" src="https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2.5.jpg?resize=1024%2C576&amp;ssl=1" alt="" srcset="https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2.5.jpg?resize=1024%2C576&amp;ssl=1 1024w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2.5.jpg?resize=300%2C169&amp;ssl=1 300w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2.5.jpg?resize=768%2C432&amp;ssl=1 768w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2.5.jpg?w=1200&amp;ssl=1 1200w" sizes="(max-width: 1024px) 100vw, 1024px" layout="intrinsic" disable-inline-width="" i-amphtml-layout="intrinsic"><img loading="lazy" width="1024" height="576" src="https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2.5.jpg?resize=1024%2C576&amp;ssl=1" alt="" srcset="https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2.5.jpg?resize=1024%2C576&amp;ssl=1 1024w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2.5.jpg?resize=300%2C169&amp;ssl=1 300w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2.5.jpg?resize=768%2C432&amp;ssl=1 768w, https://i1.wp.com/semianalysis.com/wp-content/uploads/2020/10/2.5.jpg?w=1200&amp;ssl=1 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9JzU3Nicgd2lkdGg9JzEwMjQnIHhtbG5zPSdodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZycgdmVyc2lvbj0nMS4xJy8+"></amp-img></figure>



<p>TSMC’s N5 node diverges from prior shrinks by showing signs of slowing SRAM scaling. Despite being a full shrink with logic, the SRAM is a 1.35x shrink. This figure is overstated as it will end up being even lower once other the other assist circuitry is accounted for. Hence TSMC’s guidance of chip area reduction at 35%-40% with N5. SemiAnalysis expects this to be a trend that will persist with new nodes. TSMC and Samsung are already demonstrating 3D stacked SRAM which will help alleviate the issue of density.</p>



<figure><amp-img width="1024" height="576" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/3-1.jpg?resize=1024%2C576&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/3-1.jpg?resize=1024%2C576&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/3-1.jpg?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/3-1.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/3-1.jpg?resize=1200%2C675&amp;ssl=1 1200w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/3-1.jpg?w=1280&amp;ssl=1 1280w" sizes="(max-width: 1024px) 100vw, 1024px" layout="intrinsic" disable-inline-width="" i-amphtml-layout="intrinsic"><img loading="lazy" width="1024" height="576" src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/3-1.jpg?resize=1024%2C576&amp;ssl=1" alt="" srcset="https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/3-1.jpg?resize=1024%2C576&amp;ssl=1 1024w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/3-1.jpg?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/3-1.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/3-1.jpg?resize=1200%2C675&amp;ssl=1 1200w, https://i0.wp.com/semianalysis.com/wp-content/uploads/2020/10/3-1.jpg?w=1280&amp;ssl=1 1280w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9JzU3Nicgd2lkdGg9JzEwMjQnIHhtbG5zPSdodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZycgdmVyc2lvbj0nMS4xJy8+"></amp-img></figure>



<p>3D Stacking is not the silver bullet. Cost scaling has begun slowing dramatically. With TSMC N5 wafer pricing in the ~$17k range, it is clear cost per transistor has not fallen. Even if SRAM scaling kept up, the cost per transistor would still have remained flat from N7 to N5.</p>



<figure><amp-img width="1024" height="592" src="https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/4.jpg?resize=1024%2C592&amp;ssl=1" alt="" srcset="https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/4.jpg?resize=1024%2C592&amp;ssl=1 1024w, https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/4.jpg?resize=300%2C173&amp;ssl=1 300w, https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/4.jpg?resize=768%2C444&amp;ssl=1 768w, https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/4.jpg?w=1147&amp;ssl=1 1147w" sizes="(max-width: 1024px) 100vw, 1024px" layout="intrinsic" disable-inline-width="" i-amphtml-layout="intrinsic"><img loading="lazy" width="1024" height="592" src="https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/4.jpg?resize=1024%2C592&amp;ssl=1" alt="" srcset="https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/4.jpg?resize=1024%2C592&amp;ssl=1 1024w, https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/4.jpg?resize=300%2C173&amp;ssl=1 300w, https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/4.jpg?resize=768%2C444&amp;ssl=1 768w, https://i2.wp.com/semianalysis.com/wp-content/uploads/2020/10/4.jpg?w=1147&amp;ssl=1 1147w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9JzU5Micgd2lkdGg9JzEwMjQnIHhtbG5zPSdodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZycgdmVyc2lvbj0nMS4xJy8+"></amp-img></figure>




   	</div>

   </div>

	</article>

			
		</div></div>]]>
            </description>
            <link>https://semianalysis.com/apples-a14-packs-134-million-transistors-mm2-but-falls-far-short-of-tsmcs-density-claims/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24910778</guid>
            <pubDate>Tue, 27 Oct 2020 19:17:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Mozilla's Fix the Internet Showcase]]>
            </title>
            <description>
<![CDATA[
Score 29 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24910188">thread link</a>) | @lightninglu10
<br/>
October 27, 2020 | https://talium.co/doc/xboZza/s/ | <a href="https://web.archive.org/web/*/https://talium.co/doc/xboZza/s/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p dir="ltr">Our mission is to support founders and companies building amazing internet products that care about the health of the internet. Grow, but not at all costs. Build delightful products and retain your users, but not because you've exploited someone's addictive triggers. <span>Privacy, sustainability, inclusivity, are not something to "figure out later", but are core in the company culture. </span></p><p dir="ltr"><span>We want all of our founders building big, massive products and companies. Hopefully as big as Mozilla Firefox. We just want everyone to do it ethically.</span></p><p dir="ltr">Since our launch this Spring, weâ€™ve funded 50 amazing teams in our Incubator, and weâ€™ve mentored over 300 different projects in our Open Lab. All of these teams are working on building a better internet.</p><p dir="ltr"><span size="5">We're <b>thrilled</b> to announce the <a href="https://hopin.to/events/mozilla-builders-fix-the-internet" target="_blank">Inaugural Mozilla Builders Fix The Internet Showcase</a> this Thursday, October 29 from 11:00am-12:30pm PDT.</span></p><p dir="ltr"><span>Our Showcase will feature:</span></p><ul><li dir="ltr"><p dir="ltr" role="presentation"><span>ðŸ‘©â€�ðŸ�« Mozillaâ€™s CEO Mitchell Baker kicking us off with her thoughts on the state of the internet and What Needs Building</span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span>ðŸ”¥ A fireside chat with founders on "Conscious Capitalism"</span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span><span>ðŸ”¥ </span>Hot takes from Mozilla builders and mentors (incl. Rotten Tomatoes founder Patrick Lee, and others) on â€œHow To Build A Better Internetâ€�!  </span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span>ðŸ‘¥ Weâ€™ll be showing off the top projects weâ€™ve funded through our Incubator and helped along through our Open Lab.</span></p></li></ul><p dir="ltr"><span>Featured companies are building </span><a href="https://shopneutral.io/" target="_blank">Honey for carbon offsets</a>,<span> </span><a href="https://www.thekanary.com/" target="_blank">Swiffer for personal data</a><span>, high interest savings through crypto, </span><a href="https://www.inmotion.app/" target="_blank">Superhuman for the browser</a><span>, </span><a href="https://www.bravedns.com/" target="_blank">next-gen DNS resolvers</a>, top tech publication <a href="https://hackernoon.com/" target="_blank">Hacker Noon</a>, decentralized farming networks, and <a href="https://builders.mozilla.community/alumni.html" target="_blank">so much more</a>.</p><p dir="ltr"><span>Some of the problems our teams are solving include:</span></p><ul><li dir="ltr"><p dir="ltr" role="presentation"><span>How do we shift the balance of power from centralized forces back towards individuals, citizens and communities?</span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span>Can we build a new way to communicate online that favors privacy and people? How do we make platforms safe for usersâ€™ voices while protecting their personal and professional interests? What needs to evolve?</span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span>Can we build new business models for messaging, social networking, news and information that donâ€™t rely on excessive data mining or hijacking our attention?</span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span>What will the next evolution of the internet networkâ€™s architecture and infrastructure look like?  How can decentralized technologies move the internet further towards the edge and the people?</span></p></li></ul><p dir="ltr"><span>With missions that large, it may feel like we need to rewire everything... but getting started doesnâ€™t have to be overwhelming.</span></p><p dir="ltr"><span>So if you want to learn from our </span><a href="https://builders.mozilla.community/?utm_source=www.mozilla.org&amp;utm_medium=referral&amp;utm_campaign=builders-redirect" target="_blank">amazing mentors</a> and founders, or if you're a startup and want to learn more about our $75k and $16k funding opportunities, <a href="https://hopin.to/events/mozilla-builders-fix-the-internet" target="_blank">come join us at the event</a>!</p><p dir="ltr"><span>If you're also a concerned internet citizen, help us spread the word by sharing the event with one or two of your friends. </span></p><p dir="ltr"><span>We're also doing a giveaway on Twitter of our </span><a href="https://twitter.com/mozillabuilders/status/1319380829303238656" target="_blank">Mozilla Builders Fix-The-Internet swag box</a><span>. All you need to do is </span><a href="https://talium.co/doc/xboZza/s/I'm%20going,%20are%20you?%20%20On%20October%2029th,%20join%20@mozillabuilders%20and%20@mozilla%20CEO%20and%20Founder%20@MitchellBaker%20for%20their%20inaugural%20Fix%20the%20Internet%20Showcase%20to%20learn%20from%20entrepreneurs%20and%20mentors%20on%20how%20to%20build%20a%20better%20net%20for%20all!%20RSVP:%20https://hopin.to/events/mozilla-builders-fix-the-internet" target="_blank">tweet this tweet</a>, and <a href="https://hopin.to/events/mozilla-builders-fix-the-internet" target="_blank">RSVP for the showcase</a> <span>to be eligible to win.</span></p><p dir="ltr">See you at our Showcase!</p><p dir="ltr">p.s. this post was written on&nbsp;<a href="http://talium.co/" target="_blank">talium.co</a>, one of the awesome products from our Summer batch!</p></div></div>]]>
            </description>
            <link>https://talium.co/doc/xboZza/s/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24910188</guid>
            <pubDate>Tue, 27 Oct 2020 18:16:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[OberonScript: a safe scripting language and runtime for web apps (2007)]]>
            </title>
            <description>
<![CDATA[
Score 45 | Comments 33 (<a href="https://news.ycombinator.com/item?id=24909114">thread link</a>) | @lproven
<br/>
October 27, 2020 | http://www.ralphsommerer.com/obn.htm | <a href="https://web.archive.org/web/*/http://www.ralphsommerer.com/obn.htm">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="flowhor">
  
  <div id="flowvert">
   
   <div id="mainbody">
       
<h2>OberonScript</h2>
<p>Oberon Script is a scripting language and runtime system for building interactive Web Client applications. It consists of a compiler that translates the full Oberon language into JavaScript code, and a small runtime system that detects and compiles at load-time script sections written in Oberon Script.</p>
<p>It is a complete re-implementation from scratch of an earlier compiler that I built
while being with <a href="http://research.microsoft.com/">Microsoft Research</a>
in <a href="http://en.wikipedia.org/wiki/Cambridge">Cambridge</a>. For legal reasons I was unable to take the code with me
but I cleared the code by MSR's legal department for publication via <a href="http://research.microsoft.com/research/downloads/default.aspx">Microsoft Research's code posting tool</a>.
However, I left MSR before completing the process, hence the complete rewrite.</p>
<h3>Code</h3>
<p>The code of the compiler is available in its current, far from final version in source code (obviously...) for personal,
non-commercial and non-governmental use. The usual disclaimers apply.</p>
<p>A very minimal documentation is also available. I may merge it some time with the prettyprinted HTML version below.</p>

<p>V2.0beta [<a href="http://www.ralphsommerer.com/oberon.js">JavaScript</a>] [<a href="http://www.ralphsommerer.com/oberon.js.htm">HTML</a> <small>color coded</small>] [<a href="http://www.ralphsommerer.com/obndoc.htm">Docu</a>] <small>July 6, 2007</small></p>

<h3>Presentations</h3>
<ul>
<li><a href="http://www.oberon-industry.ethz.ch/">Oberon Day 2007</a>, ETH Zürich, Switzerland, June 29, 2007</li>
<li>Joint Modular Languages Conference (<a href="http://cms.brookes.ac.uk/computing/JMLC2006/">JMLC 2006</a>), Oxford, UK, September 13-15, 2006</li>
</ul>
<h3>Publications</h3>
<p>Ralph Sommerer: 
<i>Oberon Script: A Lightweight Compiler and Runtime System for the 
Web</i>, Proceedings of the 7th Joint Modular Languages Conference, <a href="http://cms.brookes.ac.uk/computing/JMLC2006/">JMLC 2006</a>, Oxford, UK, September 13-15, 2006,
<a href="http://www.springer.com/dal/home/generic/search/results?SGWID=1-40109-22-173677107-0">LNCS Vol 4228</a>, Springer, 2006 
<small>[also available as <a href="http://research.microsoft.com/research/pubs/view.aspx?tr_id=1094">MSR 
Technical Report 2006-50</a>]</small> </p>

</div>
   
  </div>
 </div></div>]]>
            </description>
            <link>http://www.ralphsommerer.com/obn.htm</link>
            <guid isPermaLink="false">hacker-news-small-sites-24909114</guid>
            <pubDate>Tue, 27 Oct 2020 16:39:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Abusing Teams client protocol to bypass Teams security policies]]>
            </title>
            <description>
<![CDATA[
Score 235 | Comments 108 (<a href="https://news.ycombinator.com/item?id=24908776">thread link</a>) | @tommoor
<br/>
October 27, 2020 | https://o365blog.com/post/teams-policies/ | <a href="https://web.archive.org/web/*/https://o365blog.com/post/teams-policies/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			<figure>
				<img src="https://o365blog.com/images/posts/teams-policies.png" alt="Abusing Teams client protocol to bypass Teams security policies">
			</figure>
				<nav id="TableOfContents">
<ul>
<li><a href="#what-are-teams-policies">What are Teams policies?</a></li>
<li><a href="#bypassing-teams-policies">Bypassing Teams policies</a>
<ul>
<li><a href="#initial-discovery">Initial discovery</a></li>
<li><a href="#observing-teams-client-behaviour">Observing Teams client behaviour</a></li>
<li><a href="#testing-in-action">Testing in action</a></li>
</ul></li>
<li><a href="#detecting-and-protecting">Detecting and protecting</a></li>
<li><a href="#summary">Summary</a></li>
<li><a href="#references">References:</a></li>
</ul>
</nav>
			<p>Administrators can use teams policies for controlling what users can do in Microsoft Teams.</p>

<p>In this blog, I’ll show that these policies are applied only in client and thus can be easily bypassed.</p>





<p>Policies are used in Microsoft Office 365 and Azure AD for securing access to services and data. Besides the <a href="https://docs.microsoft.com/en-us/microsoft-365/security/office-365-security/identity-access-policies?view=o365-worldwide" target="_blank">common identity and device access policies</a>,
Microsoft has provided a set of <a href="https://docs.microsoft.com/en-us/microsoft-365/security/office-365-security/teams-access-policies?view=o365-worldwide" target="_blank">Teams specific policies</a>:</p>

<ul>
<li>Teams and channel policies</li>
<li>Messaging policies</li>
<li>Meeting policies</li>
<li>App permission policies</li>
</ul>

<p>For example, administrators can configure Teams so that external users are not able to edit or delete any messages they’ve sent. Or, an owner of a Teams site can disable message editing for members of a certain channel.</p>



<h2 id="initial-discovery">Initial discovery</h2>

<p>While I was working with the previous version (v0.4.4) of <a href="https://o365blog.com/aadinternals" target="_blank">AADInternals</a> Teams functions I noticed an interesting thing: I was able to edit and delete chat messages using AADInternals as a guest
even when it was not allowed.</p>

<p>This led to a question that <strong>what if the policies are applied only at the client end?</strong> In practice this would mean that the Teams service tells to your Teams client that “Though shall not edit messages!” but the client
could still do so.</p>

<h2 id="observing-teams-client-behaviour">Observing Teams client behaviour</h2>

<p>I started by watching what was going on between the client and cloud when the Teams client started. The first observation was that the client made about 120 http requests to the cloud.
While browsing through those requests, I spotted one that caught my interest (headers stripped):</p>

<pre><code>POST https://teams.microsoft.com/api/mt/part/emea-02/beta/users/useraggregatesettings HTTP/1.1

{
    "tenantSettingsV2": true,
    "userResourcesSettings": true,
    "messagingPolicy": true,
    "clientSettings": true,
    "targetingPolicy": true,
    "tenantSiteUrl": true,
    "userPropertiesSettings": true,
    "callingPolicy": true,
    "meetingPolicy": true,
    "educationAssignmentsAppPolicy": true
}
</code></pre>

<p>The response contained all the settings and policies the Teams client is allowed to do as the logged in user. Below can be seen the <strong>messagingPolicy</strong> section:
</p><div><pre><code data-lang="json"><span></span><span>"messagingPolicy"</span><span>:</span> <span>{</span>
	<span>"value"</span><span>:</span> <span>{</span>
		<span>"allowUserEditMessage"</span><span>:</span> <span>true</span><span>,</span>
		<span>"allowUserDeleteMessage"</span><span>:</span> <span>true</span><span>,</span>
		<span>"allowUserChat"</span><span>:</span> <span>true</span><span>,</span>
		<span>"allowGiphy"</span><span>:</span> <span>true</span><span>,</span>
		<span>"giphyRatingType"</span><span>:</span> <span>"Moderate"</span><span>,</span>
		<span>"allowGiphyDisplay"</span><span>:</span> <span>true</span><span>,</span>
		<span>"allowPasteInternetImage"</span><span>:</span> <span>true</span><span>,</span>
		<span>"allowMemes"</span><span>:</span> <span>true</span><span>,</span>
		<span>"allowStickers"</span><span>:</span> <span>true</span><span>,</span>
		<span>"allowUserTranslation"</span><span>:</span> <span>true</span><span>,</span>
		<span>"allowUrlPreviews"</span><span>:</span> <span>true</span><span>,</span>
		<span>"readReceiptsEnabledType"</span><span>:</span> <span>"UserPreference"</span><span>,</span>
		<span>"allowImmersiveReader"</span><span>:</span> <span>true</span><span>,</span>
		<span>"allowPriorityMessages"</span><span>:</span> <span>true</span><span>,</span>
		<span>"audioMessageEnabledType"</span><span>:</span> <span>"ChatsAndChannels"</span><span>,</span>
		<span>"channelsInChatListEnabledType"</span><span>:</span> <span>"DisabledUserOverride"</span><span>,</span>
		<span>"allowRemoveUser"</span><span>:</span> <span>true</span><span>,</span>
		<span>"allowSmartReply"</span><span>:</span> <span>true</span>
	<span>}</span>
<span>}</span>
</code></pre></div>


<p>What we can learn here is that the Teams client asks from the cloud what the current user is allowed to do, which was the expected behaviour.</p>

<h2 id="testing-in-action">Testing in action</h2>

<p>Next I decided to try whether I could lie to Teams client:</p>

<ol>
<li><p>I saved the response from above to be used as a baseline.</p></li>

<li><div><p>I created a new Messaging policy to disable editing and deleting of sent messages.</p><p>
I applied the policy to a single demo user:</p><p>
<img src="https://o365blog.com/images/posts/teams-policies1.png" alt="Custom policy"></p><p>Now I had two policies, the default organisation wide and the restricted one for demo user:</p><p>
<img src="https://o365blog.com/images/posts/teams-policies2.png" alt="Policies"></p></div></li>

<li><p>I restarted the Teams client and noticed that the editing and deleting were correctly disabled (didn’t exists).</p></li>

<li><div><p>I compared the returned policies from the <strong>useraggregatesettings</strong> requests<br>
and as we can see, the request was missing two lines:</p><p>
<img src="https://o365blog.com/images/posts/teams-policies3.png" alt="Policy comparison"></p></div></li>

<li><div><p>I closed the client and configured Fiddler to do an autoresponse using the saved http response from above:</p><p>
<img src="https://o365blog.com/images/posts/teams-policies4.png" alt="Fiddler autoresponse"></p><p>
Now, when the client is requesting the settings file, it will be served the one that allows editing and deleting.</p></div></li>

<li><p>I started the Teams client and <strong>the editing and deleting were again allowed</strong> and I was able to edit and delete (my own) messages!</p></li>
</ol>

<p>What we can lean here is that <strong>we can lie to Teams client</strong> and change its behaviour 😂 <br>
Moreover, we learnt that <strong>Teams policies are applied only on the client</strong> 🤦‍♂</p>

<p>Here is the video demonstrating this with <strong>AADInternals</strong> and <strong>Fiddler</strong> (sorry for the bad audio after 03:20):</p>

<div><iframe width="560" height="315" src="https://www.youtube.com/embed/Zcqig-OyUMY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div>

<p>Below is a video that shows in action that this works also with <strong>cloud file storage restrictions</strong>:<br></p>

<p><iframe width="560" height="315" src="https://www.youtube.com/embed/a32TkLIBwS4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<br><strong>Note:</strong> Although not seen on the video, I was able to add my Google Drive account to Teams so this is not just a UI thing.</p>



<p>As far as I know, the “uncompliant” Teams client behaviour can not be detected.</p>

<p>Same verdict with protecting. Well, one could try to use Conditional Access (CA) with device ownership and compliance restrictions, but that doesn’t cover all scenarios.</p>



<p>Our little test here proves that <strong>Teams policies are applied ONLY on the client!</strong>.</p>

<p>If the user (or guest) is utilising Teams APIs directly, using for instance AADInternals <a href="https://o365blog.com/aadinternals/#teams-functions" target="_blank">Teams functionality</a>, he or she can bypass the restrictions set by the policies.
However, this is not a bug or vulnerability as such, but a (very very bad) design choice by Microsoft.</p>

<p>Users can do at least the following:</p>

<ul>
<li>Bypass messaging policies</li>
<li>Bypass cloud file storage restrictions</li>
<li>Bypass meetings policies</li>
</ul>

<p>⚠️ <strong>Teams policies are NOT a security measure and organisations should not rely on them!</strong> ⚠️</p>



<ul>
<li>Microsoft: <a href="https://docs.microsoft.com/en-us/microsoft-365/security/office-365-security/identity-access-policies?view=o365-worldwide" target="_blank">Common identity and device access policies</a></li>
<li>Microsoft: <a href="https://docs.microsoft.com/en-us/microsoft-365/security/office-365-security/teams-access-policies?view=o365-worldwide" target="_blank">Policy recommendations for securing Teams chats, groups, and files</a></li>
</ul>
		</div></div>]]>
            </description>
            <link>https://o365blog.com/post/teams-policies/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24908776</guid>
            <pubDate>Tue, 27 Oct 2020 16:07:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Study shows how exercise stalls cancer growth through the immune system]]>
            </title>
            <description>
<![CDATA[
Score 17 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24908425">thread link</a>) | @gmays
<br/>
October 27, 2020 | https://news.ki.se/study-shows-how-exercise-stalls-cancer-growth-through-the-immune-system | <a href="https://web.archive.org/web/*/https://news.ki.se/study-shows-how-exercise-stalls-cancer-growth-through-the-immune-system">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <p><img src="https://news.ki.se/sites/default/files/styles/article_full_width/public/qbank/ZIM_6255ZIM_6255-custom20201026104950.jpg" alt="Randall Johnson and Helene Rundqvist, researchers at Karolinska Institutet. Credit: Stefan Zimmerman."></p><p>Randall Johnson and Helene Rundqvist, researchers at Karolinska Institutet. Credit: Stefan Zimmerman.</p>
                  </div><div>
        
            <p>Prior research has shown that physical activity can prevent unhealth as well as improve the prognosis of several diseases including various forms of cancer. Exactly how exercise exerts its protective effects against cancer is, however, still unknown, especially when it comes to the biological mechanisms. One plausible explanation is that physical activity activates the immune system and thereby bolsters the body’s ability to prevent and inhibit cancer growth.</p>

<p>In this study, researchers at Karolinska Institutet expanded on this hypothesis by examining how the immune system’s cytotoxic T cells, that is white blood cells specialized in killing cancer cells, respond to exercise.</p>

<h2>Cancer growth slowed in trained animals</h2>

<p>They divided mice with cancer into two groups and let one group exercise regularly in a spinning wheel while the other remained inactive. The result showed that cancer growth slowed and mortality decreased in the trained animals compared with the untrained.</p>

<p>Next, the researchers examined the importance of cytotoxic T cells by injecting antibodies that remove these T cells in both trained and untrained mice. The antibodies knocked out the positive effect of exercise on both cancer growth and survival, which according to the researchers demonstrates the significance of these T cells for exercise-induced suppression of cancer.</p>

<p>The researchers also transferred cytotoxic T cells from trained to untrained mice with tumors, which improved their prospects compared with those who got cells from untrained animals.</p>

<h2>Exercise altered T cell metabolism</h2>

<p>To examine how exercise influenced cancer growth, the researchers isolated T cells, blood and tissue samples after a training sessions and measured levels of common metabolites that are produced in muscle and excreted into plasma at high levels during exertion. Some of these metabolites, such as lactate, altered the metabolism of the T cells and increased their activity. The researchers also found that T cells isolated from an exercised animal showed an altered metabolism compared to T cells from resting animals.</p>

<p>In addition, the researchers examined how these metabolites change in response to exercise in humans. They took blood samples from eight healthy men after 30 minutes of intense cycling and noticed that the same training-induced metabolites were released in humans.</p>

<p>“Our research shows that exercise affects the production of several molecules and metabolites that activate cancer-fighting immune cells and thereby inhibit cancer growth,” says <a href="https://staff.ki.se/people/helame">Helene Rundqvist</a>, senior researcher at the <a href="https://ki.se/en/labmed/department-of-laboratory-medicine">Department of Laboratory Medicine</a>, Karolinska Institutet, and the study’s first author. “We hope these results may contribute to a deeper understanding of how our lifestyle impacts our immune system and inform the development of new immunotherapies against cancer.”</p>

<p>The researchers have received financing from the Knut and Alice Wallenberg Foundation, the Swedish Research Council, the Swedish Cancer Society, the Swedish Childhood Cancer Foundation, the Swedish Society of Medicine, Cancer Research UK and the Wellcome Trust.</p>

<h2>Publication</h2>

<p>“<a href="https://elifesciences.org/articles/59996">Cytotoxic T-cells mediate an exercise-induced reduction in tumor growth</a>,”<strong> </strong>Helene Rundqvist, Pedro Veliça, Laura Barbieri, Paulo A. Gameiro, David Bargiela, Milos Gojkovic, Sara Mijwel, Stefan Reitzner, David Wullimann, Emil Ahlstedt, Jernej Ule, Arne Östman and Randall S. Johnson, <em>eLife</em>, online October 23, 2020, doi: 10.7554/eLife.59996</p>
      
      </div></div>]]>
            </description>
            <link>https://news.ki.se/study-shows-how-exercise-stalls-cancer-growth-through-the-immune-system</link>
            <guid isPermaLink="false">hacker-news-small-sites-24908425</guid>
            <pubDate>Tue, 27 Oct 2020 15:36:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Guide to Notion Landing Pages]]>
            </title>
            <description>
<![CDATA[
Score 54 | Comments 27 (<a href="https://news.ycombinator.com/item?id=24906774">thread link</a>) | @saviorand
<br/>
October 27, 2020 | https://optemization.com/notion-landing-page-guide | <a href="https://web.archive.org/web/*/https://optemization.com/notion-landing-page-guide">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><article id="notion-landing-page-guide"><div id="4ef8369dd07944578af5ecae07585f8f"><div id="ec8a50571edc41dea01826949daf52b2"><p><span><span>We at Optemization know a thing or two about  Notion landing pages. In fact, Tem published the first Notion website back in March 2020 (oh, how the world has changed). </span></span></p><p><span><span>Thanks in large to awesome projects like Super and Fruition, building websites on Notion became easier and faster. </span></span></p><p><span><span>Given the surge in popularity, I deciced to pen this comprehensive guide and create some duplicable blocks, so you can create your own Notion website in no time — enjoy 🙌</span></span></p></div></div><h2><span id="cc149452e88b42e8b29622443a589d9c"></span><span><span>🔑 An overview of the guide</span></span></h2><div id="c496b9ce8126498e85222feb210e850b"><div id="4e7f6e020650473a8d103736b9068090"><p><span><span>Making a landing page with Notion is easy to the point of enjoyable — you don't need any coding skills at all. It's also flexible — you can mix, match, and style various blocks to get the look and feel needed to present your idea (or product) just the right way.</span></span></p><p><span><span>This guide will walk you through every step of setting up your landing with Notion, publishing it to the web, adding analytics and custom styling. As a cherry on top, we've also curated 10 ready-made components that you can use to get your Notion landing page out in no time.</span></span></p></div></div><h2><span id="4b158c7e538e49518051b4d9b3d1f780"></span><span><span>🎯 Setting your landing page goals</span></span></h2><p><span><span>Landing pages are the best way to "sell" something, to tell people about a specific product, service or a resource, and make them do a specific action. </span></span></p><p><span><span>When user does an action, this is called "conversion", and usually landing pages are fine-tuned to get as much conversions as possible. Conversions could be anything: from subscribing to a newsletter or joining a community, to buying a product or a service. Landing pages can be purely informational, too. </span></span></p><p><span><span>Think what's the purpose of your page, and what the user needs to do to contribute.</span></span></p><h2><span id="cbfc301b7d274e27a06afc720d7eb511"></span><span><span>🤔 Deciding on what to show on the landing page</span></span></h2><p><span><span>Defining your goal was the hard part — after you know what action you want the user to make, it's easy to define what to show on your landing. </span></span></p><p><span><span>If they're subscribing to a newsletter, tell what it's about. If the goal is to grow a community, tell about the people already there and show what's the purpose of this community. If you're selling a product, focus on the value it offers and on core functionality. Don't forget call-to-actions to let the user actually realize their interest when they're convinced.</span></span></p><p><span><span>Take a look at </span><span><a target="_blank" rel="noopener noreferrer" href="https://demandcurve.com/#1opac8uusrjldfqjb39wpp">Demand Curve</a></span><span>'s landing page, for example. </span></span></p><div id="54fa9f431f9a45fc970d6e7fc90bf338"><picture><source srcset="https://api.super.so/asset/optemization.com/32e404a1-908e-4e9d-a0ca-dcce9c1e5edf.png?w=750&amp;f=webp" media="(max-width: 414px)" type="image/webp"><source src="https://api.super.so/asset/optemization.com/32e404a1-908e-4e9d-a0ca-dcce9c1e5edf.png?w=750" media="(max-width: 414px)"><source srcset="https://api.super.so/asset/optemization.com/32e404a1-908e-4e9d-a0ca-dcce9c1e5edf.png?w=1500&amp;f=webp" type="image/webp"><img src="https://api.super.so/asset/optemization.com/32e404a1-908e-4e9d-a0ca-dcce9c1e5edf.png?w=1500" alt="image" loading="lazy"></picture></div><p><span><span>They're advertising their start-up program, focusing on what the program is about, how it's structured and why people learn valuable things during that program. It's all there — right on the first screen you can see the bullet points describing what the program is about (Growth Strategy, Ads, etc.).</span></span></p><p><span><span>Demand Curve's team also put a special emphasis on social proof — there's a ton of different testimonials and stories from people on the page.</span></span></p><p><span><span>Sometimes short landing pages that span 1-2 screens have higher conversion that longer ones, that span 3 and more screens. When you need your user to perform a simple action, like sharing their email, a short page works better. Short landings also make more sense for "warm" clients who already know what you are offering.</span></span></p><p><span><span>Longer landings with lots of information work better for "cold" clients, because they need more context. After you have an idea on what size is appropriate in your case, you can start experimenting with content.</span></span></p><h2><span id="3812d49b74904f9d961961ff3c8b07b9"></span><span><span>🖋️ Adding some content</span></span></h2><p><span><span>On a typical landing page, the information is arranged into standard "blocks" with valuable information:  there's an eye-catching Hero image, simple text blocks describing your value proposition, a call-to-action that lets you collect interest, and a footer with terms. </span></span></p><p><span><span>Like on </span><span><a target="_blank" rel="noopener noreferrer" href="https://www.refactoringgrowth.com/">Refactoring Growth</a></span><span>'s landing page, you have an elaborate introduction, several sections with a value proposition (simple text) block and a picture or a graphic, a pricing block and a Call-to-Action.</span></span></p><div id="c32142c387d44b52b7995b22ae6c17c9"><picture><source srcset="https://api.super.so/asset/optemization.com/381f703e-7f47-4032-8bcb-2105ba4ea2b0.png?w=750&amp;f=webp" media="(max-width: 414px)" type="image/webp"><source src="https://api.super.so/asset/optemization.com/381f703e-7f47-4032-8bcb-2105ba4ea2b0.png?w=750" media="(max-width: 414px)"><source srcset="https://api.super.so/asset/optemization.com/381f703e-7f47-4032-8bcb-2105ba4ea2b0.png?w=1500&amp;f=webp" type="image/webp"><img src="https://api.super.so/asset/optemization.com/381f703e-7f47-4032-8bcb-2105ba4ea2b0.png?w=1500" alt="image" loading="lazy"></picture></div><p><span><span>Optionally, you can add more useful stuff. It's a good idea to include social proof (which is very important for conversions and creates a sense of community around your value prop), juicy product shots or screenshots and a blog section linking to your posts somewhere else. </span></span></p><div id="d3e8989030c247cba938e6e513c95e1d"><picture><source srcset="https://api.super.so/asset/optemization.com/a9051f9d-917c-4013-bde2-6b219dbfaa5d.png?w=750&amp;f=webp" media="(max-width: 414px)" type="image/webp"><source src="https://api.super.so/asset/optemization.com/a9051f9d-917c-4013-bde2-6b219dbfaa5d.png?w=750" media="(max-width: 414px)"><source srcset="https://api.super.so/asset/optemization.com/a9051f9d-917c-4013-bde2-6b219dbfaa5d.png?w=1500&amp;f=webp" type="image/webp"><img src="https://api.super.so/asset/optemization.com/a9051f9d-917c-4013-bde2-6b219dbfaa5d.png?w=1500" alt="image" loading="lazy"></picture></div><h2><span id="f07e89f0db6c4c5188083b57316e7685"></span><span><span>🎁 Ready-made landing page blocks</span></span></h2><p><span><span>We've assembled some common sections in Notion so you can borrow them for your website.</span></span></p><p><span><span>Just open the component you like below, click on the bookmark, then click "Duplicate" and drag your component's page into any page you like. Select "Turn into", then "text". You've got your landing page! Add some space between the blocks, change the content and delete the toggle. Then customize it as you like.</span></span></p><p><span><span>Here's a 53-second demo: </span></span></p><p><span><span>Mix and match the blocks and add your own section to make your own converting landing page in 10 minutes.</span></span></p><h2><span id="dbfa3c0dca5f4aada94f5376166f8281"></span><span><span>🌐 Publish your page to the web </span></span></h2><p><span><span>It's easy to publish your page and make it accessible from a custom domain. 
We like two services: </span><span><a target="_blank" rel="noopener noreferrer" href="https://fruitionsite.com/">Fruition</a></span><span> is free and open-source, while </span><span><a target="_blank" rel="noopener noreferrer" href="https://super.so/">Super</a></span><span> offers more functionality and better performance. </span></span></p><p><span><span>Fruition has a </span><span><a target="_blank" rel="noopener noreferrer" href="https://fruitionsite.com/">very elaborate guide</a></span><span> right on their home page, and a </span><span><a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=aw0x54PzCaI">video tutorial</a></span><span>, and Super gives a good onboarding when you sign up, leading you through all the necessary steps. 

</span><span><span>Here's a step-by-step to get you going on Super:</span></span></span></p><ol><li id="30fa5f0ef68b4ad79354c951f1b9aba1"><span><span>Sign up, select a plan (essentially boils down to how many sites you need, keep in mind that one site can have many subpages, like any website on the Web)
</span></span></li><li id="787193987a0b4c7c95437a027e1bd716"><span><span>Select whether you want to make a static website out of your page, or go with a default Notion-based method. 

First option offers great performance, but doesn't allow for filtered views and calendars on your page. 

Default Notion page is relatively poor in terms of performance and SEO, but all Notion functionality will work and it's still enough for simple personal websites or pages where you don't need fast loading.
</span></span></li><ol></ol><li id="e44a993f1fce466193429d780f87fd90"><span><span>Select your site name used in Super, a custom domain from a domain provider, and the URL to your original Notion page with the page set to public via the "Share" menu at the top ("Share" → "Share to the Web")
</span></span></li><li id="2aa6fd185ae9449084ed3674d150ac83"><span><span>Add one or more pretty URLs if you need them. By default, all the sub-pages you add inside your home page will have ugly Notion URLs. You can change that by providing links to sub-pages you want to add slugs to and specifying the slug (e.g. </span><span><a target="_blank" rel="noopener noreferrer" href="http://optemization.com/preconceived">optemization.com/preconceived</a></span><span>)
</span></span></li><li id="ff4df464b80145149fe1675f38702e2b"><span><span>Add A and CNAME records to point Super to your domain name (Super provides these and you need to enter them in your domain provider's control panel). You can also provide an API Key if it's GoDaddy.</span></span></li><ol></ol><li id="52d2dce988b94c798bc7fafc30303552"><span><span>Enter your site's description, attach an image and an icon for social sharing. You can also select a custom font for your page's contents at this point.
</span></span></li></ol><p><span><span>Voila! The site should now be public. You're awesome.</span></span></p><h2><span id="728b3a07a7f3429b818b7992351d8dfa"></span><span><span>🔢 Add analytics</span></span></h2><p><span><span>With both Super and Fruition, you can inject your own Javascript into your page. This means you can use most of analytics solutions available.</span></span></p><p><span><span>For example, you can get your Fathom Analytics script to inject by going to Settings → Site → Site ID. Here's their own Fathom's </span><span><a target="_blank" rel="noopener noreferrer" href="https://usefathom.com/support/tracking">instruction</a></span><span>. Google Analytics also has a global site tag you can inject. </span></span></p><p><span><span>Both look like this (from Super's landing):</span></span></p><pre id="c777ed719650472ea7f2d1bb17322dd3"><code><span><pre><code><span>&lt;</span><span>script src</span><span>=</span><span>"https://cdn.analytics.com"</span><span>&gt;</span><span>&lt;</span><span>/</span><span>script</span><span>&gt;</span></code></pre></span></code></pre><p><span><span>After you copy and paste the script into the right field in Super or with Fruition, it should should auto-magically start collecting your stats and you'll be able to see them in your analytics dashboard. These are statistics we measure for </span><span><a target="_blank" rel="noopener noreferrer" href="http://optemization.com/">optemization.com</a></span><span>:</span></span></p><h2><span id="4b1780ab79904f75abb4ecda48f54fd6"></span><span><span>✨Add styling </span></span></h2><p><span><span>In theory, with Super or Fruition you can style practically any part of your page. One of the simple things to do is to change default Notion colors. To change any color on your page just add a script (the same way you add an analytics script) that replaces Notion's CSS values.</span></span></p><p><span><span><a target="_blank" rel="noopener noreferrer" href="https://demo.super.so/guides/colors">Super</a></span><span> has a great mini-guide on doing that, below is the script with every default Notion color. Just replace the color you want to change with a HEX value (#000 for black, #fff for white) and delete the rest to change colors for a site hosted with Super (Fruition works the same way).</span></span></p><details id="cd15c584098c4a26b2e6aeadc95bd312"><summary><span><span>Notion's core colors (check the full list here: </span><span><span><a id="/fed8e0f6059d469fadaeeac47812b6e7" href="https://optemization.com/fed8e0f6059d469fadaeeac47812b6e7"><div><p><img src="https://super.so/icon/dark/hexagon.svg" alt="Notion Colors"></p><p><span><span>Notion Colors</span></span></p></div></a></span></span><span>)</span></span></summary><div><pre id="675898bed12c4045ac7bdd3c20ed3ae3"><code><span><pre><code><span>&lt;</span><span>style</span><span>&gt;</span><span>
</span><span>  </span><span>:</span><span>root </span><span>{</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>text</span><span>-</span><span>default</span><span>:</span><span> #</span><span>37352</span><span>f</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>text</span><span>-</span><span>default</span><span>-</span><span>light</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>55</span><span>,</span><span>53</span><span>,</span><span>47</span><span>,</span><span>0.6</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>text</span><span>-</span><span>gray</span><span>:</span><span> #</span><span>9</span><span>b9a97</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>text</span><span>-</span><span>brown</span><span>:</span><span> #</span><span>64473</span><span>a</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>text</span><span>-</span><span>orange</span><span>:</span><span> #d9730d</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>text</span><span>-</span><span>yellow</span><span>:</span><span> #dfab01</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>text</span><span>-</span><span>green</span><span>:</span><span> #</span><span>0</span><span>f7b6c</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>text</span><span>-</span><span>blue</span><span>:</span><span> #</span><span>0</span><span>b6e99</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>text</span><span>-</span><span>purple</span><span>:</span><span> #</span><span>6940</span><span>a5</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>text</span><span>-</span><span>pink</span><span>:</span><span> #ad1a72</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>text</span><span>-</span><span>red</span><span>:</span><span> #e03e3e</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>default</span><span>:</span><span> #fff</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>gray</span><span>:</span><span> #ebeced</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>brown</span><span>:</span><span> #e9e5e3</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>orange</span><span>:</span><span> #faebdd</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>yellow</span><span>:</span><span> #fbf3db</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>green</span><span>:</span><span> #ddedea</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>blue</span><span>:</span><span> #ddebf1</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>purple</span><span>:</span><span> #eae4f2</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>pink</span><span>:</span><span> #f4dfeb</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>red</span><span>:</span><span> #fbe4e4</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>gray</span><span>-</span><span>light</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>235</span><span>,</span><span>236</span><span>,</span><span>237</span><span>,</span><span>0.3</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>brown</span><span>-</span><span>light</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>233</span><span>,</span><span>229</span><span>,</span><span>227</span><span>,</span><span>0.3</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>orange</span><span>-</span><span>light</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>250</span><span>,</span><span>235</span><span>,</span><span>221</span><span>,</span><span>0.3</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>yellow</span><span>-</span><span>light</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>251</span><span>,</span><span>243</span><span>,</span><span>219</span><span>,</span><span>0.3</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>green</span><span>-</span><span>light</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>221</span><span>,</span><span>237</span><span>,</span><span>234</span><span>,</span><span>0.3</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>blue</span><span>-</span><span>light</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>221</span><span>,</span><span>235</span><span>,</span><span>241</span><span>,</span><span>0.3</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>purple</span><span>-</span><span>light</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>234</span><span>,</span><span>228</span><span>,</span><span>242</span><span>,</span><span>0.3</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>pink</span><span>-</span><span>light</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>244</span><span>,</span><span>223</span><span>,</span><span>235</span><span>,</span><span>0.3</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>bg</span><span>-</span><span>red</span><span>-</span><span>light</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>251</span><span>,</span><span>228</span><span>,</span><span>228</span><span>,</span><span>0.3</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>pill</span><span>-</span><span>default</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>206</span><span>,</span><span>205</span><span>,</span><span>202</span><span>,</span><span>0.5</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>pill</span><span>-</span><span>gray</span><span>:</span><span> </span><span>hsla</span><span>(</span><span>45</span><span>,</span><span>2</span><span>%</span><span>,</span><span>60</span><span>%</span><span>,</span><span>0.4</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>pill</span><span>-</span><span>brown</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>140</span><span>,</span><span>46</span><span>,</span><span>0</span><span>,</span><span>0.2</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>pill</span><span>-</span><span>orange</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>245</span><span>,</span><span>93</span><span>,</span><span>0</span><span>,</span><span>0.2</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>pill</span><span>-</span><span>yellow</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>233</span><span>,</span><span>168</span><span>,</span><span>0</span><span>,</span><span>0.2</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>pill</span><span>-</span><span>green</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>0</span><span>,</span><span>135</span><span>,</span><span>107</span><span>,</span><span>0.2</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>pill</span><span>-</span><span>blue</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>0</span><span>,</span><span>120</span><span>,</span><span>223</span><span>,</span><span>0.2</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>pill</span><span>-</span><span>purple</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>103</span><span>,</span><span>36</span><span>,</span><span>222</span><span>,</span><span>0.2</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>pill</span><span>-</span><span>pink</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>221</span><span>,</span><span>0</span><span>,</span><span>129</span><span>,</span><span>0.2</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>pill</span><span>-</span><span>red</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>255</span><span>,</span><span>0</span><span>,</span><span>26</span><span>,</span><span>0.2</span><span>)</span><span>;</span><span>
</span><span>    </span><span>--</span><span>color</span><span>-</span><span>ui</span><span>-</span><span>hover</span><span>-</span><span>bg</span><span>:</span><span> </span><span>rgba</span><span>(</span><span>55</span><span>,</span><span>53</span><span>,</span><span>47</span><span>,</span><span>0.08</span><span>)</span><span>;</span><span>
</span><span>  </span><span>}</span><span>
</span><span></span><span>&lt;</span><span>/</span><span>style</span><span>&gt;</span></code></pre></span></code></pre></div></details><h2><span id="86c16edd25c04182b7d14ae872bed16d"></span><span><span>🍾 Add a pop-up Call-to-Action block</span></span></h2><p><span><span>We've made an opinionated CTA block you can use to ask your user to sign up at some point when they engage with the page. This one is shown 3 seconds after the page is opened (change the "3000" value inside the window.onload to adjust the duration).</span></span></p><div id="735cc70d16a944f19b754c5ed6b73d83"><picture><source srcset="https://api.super.so/asset/optemization.com/4ec1e65e-e001-4fb2-84f0-b9446ec5b07f.gif?w=750&amp;f=webp" media="(max-width: 414px)" type="image/webp"><source src="https://api.super.so/asset/optemization.com/4ec1e65e-e001-4fb2-84f0-b9446ec5b07f.gif?w=750" media="(max-width: 414px)"><source srcset="https://api.super.so/asset/optemization.com/4ec1e65e-e001-4fb2-84f0-b9446ec5b07f.gif?w=1500&amp;f=webp" type="image/webp"><img src="https://api.super.so/asset/optemization.com/4ec1e65e-e001-4fb2-84f0-b9446ec5b07f.gif?w=1500" alt="image" loading="lazy"></picture></div><p><span><span>Simply include the following script in your Super or Fruition "Inject scripts" section, similar to how you included analytics:</span></span></p><pre id="567243414569454490cb895f08584391"><code><span><pre><code><span>&lt;</span><span>script src</span><span>=</span><span>"https://unpkg.com/sweetalert/dist/sweetalert.min.js"</span><span>&gt;</span><span>&lt;</span><span>/</span><span>script</span><span>&gt;</span><span>
</span><span></span><span>// Thanks Sweetalert for the alert! </span><span>
</span><span></span><span>&lt;</span><span>script</span><span>&gt;</span><span>
</span><span></span><span>window</span><span>.</span><span>on…</span></code></pre></span></code></pre></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://optemization.com/notion-landing-page-guide">https://optemization.com/notion-landing-page-guide</a></em></p>]]>
            </description>
            <link>https://optemization.com/notion-landing-page-guide</link>
            <guid isPermaLink="false">hacker-news-small-sites-24906774</guid>
            <pubDate>Tue, 27 Oct 2020 12:34:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Research team discovers breakthrough with potential to reverse Alzheimer's]]>
            </title>
            <description>
<![CDATA[
Score 231 | Comments 57 (<a href="https://news.ycombinator.com/item?id=24906758">thread link</a>) | @elorant
<br/>
October 27, 2020 | https://news.ucalgary.ca/news/research-team-discovers-breakthrough-potential-prevent-reverse-alzheimers | <a href="https://web.archive.org/web/*/https://news.ucalgary.ca/news/research-team-discovers-breakthrough-potential-prevent-reverse-alzheimers">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <div>
            <div>
              <div>

                
                                
                                                  <div>
                                                                                        <p><span><span>A research team at the University of Calgary’s <a href="https://cumming.ucalgary.ca/">Cumming School of Medicine</a> (CSM) led by Dr. S.R. Wayne Chen, PhD, has made an exciting breakthrough with the potential to prevent and reverse the effects of Alzheimer’s disease.</span></span></p>

<p><span><span>The team discovered that limiting the open time of a channel called the ryanodine receptor, which acts like a gateway to cells located in the heart and brain, reverses and prevents progression of Alzheimer’s disease in animal models. They also identified a drug that interrupts the disease process.</span></span></p>

<p><span><span>The effect of giving the drug to animal models was remarkable: After one month of treatment, the memory loss and cognitive impairments in these models disappeared. </span></span></p>

<p><span><span>“The significance of identifying a clinically used drug that acts on a defined target to provide anti-Alzheimer’s disease benefits can’t be overstated,” says Chen, a member of the <a href="https://libin.ucalgary.ca/">Libin Cardiovascular Institute</a> and the <a href="https://hbi.ucalgary.ca/">Hotchkiss Brain Institute</a> at the CSM.&nbsp;</span></span><span lang="EN-US"><span><span><span>Dr. Jinjing Yao, PhD, a student of Chen, is the first author of the study.</span></span></span></span></p>

<p><span><span>The results of this groundbreaking study were recently published in the peer-reviewed journal, <a href="https://www.cell.com/cell-reports/fulltext/S2211-1247(20)31158-X"><em>Cell Reports</em></a>.&nbsp;</span></span></p>

<p><span><span>This work is potentially highly impactful as more than half a million Canadians live with Alzheimer’s disease and other dementias, suffering memory loss and other cognitive impairments with a negative impact on quality of life. </span></span></p>

<h3><strong><span><span><span><span>The science behind the findings</span></span></span></span></strong></h3>

<p><span><span>Previous research has shown that the progression of Alzheimer’s disease is driven by a vicious cycle of the protein amyloid β (Aβ) inducing hyperactivity at the neuron level. However, the mechanism behind this wasn’t fully understood nor were there effective treatments to stop the cycle. &nbsp;</span></span></p>

<p><span><span>Chen’s team used a portion of an existing drug used for heart patients, carvedilol, to treat mice models with Alzheimer’s symptoms. After a month of treatment, researchers tested animal models with very promising results. </span></span></p>

<p><span><span>“We treated them for a month and the effect was quite amazing,” says Chen, explaining the drug was successful in reversing major symptoms of Alzheimer’s disease. “We couldn’t tell the drug-treated disease models and the healthy models apart.” </span></span></p>

<p><span><span>Chen, a Clarivate Highly Cited Researcher, is optimistic about the future of this research, noting the next step will be clinical trials in people.</span></span></p>

<p><em><span><span>Wayne Chen is a professor in the Department&nbsp;of Physiology and Pharmacology, Biochemistry and </span></span><span><span>Molecular Biology at the CSM.&nbsp;</span></span></em><span><span><em><span lang="EN-US"><span><span>Led by the&nbsp;</span></span></span></em><a href="http://www.hbi.ucalgary.ca/"><em><span><span><span><span><span>Hotchkiss Brain Institute</span></span></span></span></span></em></a><em><span lang="EN-US"><span><span>,&nbsp;</span></span></span></em><a href="http://www.ucalgary.ca/research/brain-and-mental-health"><em><span><span><span><span><span>Brain and Mental Health</span></span></span></span></span></em></a><em><span lang="EN-US"><span><span>&nbsp;is one of six research strategies guiding the University of Calgary toward its&nbsp;Eyes High&nbsp;goals. The strategy provides a unifying direction for brain and mental health research at the university.</span></span></span></em></span></span></p>



                                                                                                                                                                                                                                      

  
    

    
  <div data-history-node-id="23525" role="article" about="/news/research-team-discovers-breakthrough-potential-prevent-reverse-alzheimers" typeof="schema:Article">
    <div>
      <div>
                          <div>
            <div>
              <div>
                <div>
                                        <picture>
                <!--[if IE 9]><video style="display: none;"><![endif]-->
              <source srcset="https://news.ucalgary.ca/news/sites/default/files/styles/ucws_image_desktop/public/2020-09/20190911Libin%20Portraits-%20Dr.%20Chen%20-117.jpg?itok=ByB_f0m8 1x" media="all and (min-width: 992px)" type="image/jpeg">
              <source srcset="https://news.ucalgary.ca/news/sites/default/files/styles/ucws_image_tablet/public/2020-09/20190911Libin%20Portraits-%20Dr.%20Chen%20-117.jpg?itok=5tYigcJ8 1x" media="all and (min-width: 768px)" type="image/jpeg">
              <source srcset="https://news.ucalgary.ca/news/sites/default/files/styles/ucws_image_mobile/public/2020-09/20190911Libin%20Portraits-%20Dr.%20Chen%20-117.jpg?itok=-CoV0v5E 1x" media="all and (max-width: 767px)" type="image/jpeg">
            <!--[if IE 9]></video><![endif]-->
            <img src="https://news.ucalgary.ca/news/sites/default/files/styles/ucws_image_desktop/public/2020-09/20190911Libin%20Portraits-%20Dr.%20Chen%20-117.jpg?itok=ByB_f0m8" alt="Dr. Wayne Chen, PhD" title="Dr. Wayne Chen, PhD" typeof="foaf:Image">

  </picture>

                                    </div>
                                  <p>Dr. Wayne Chen, PhD</p>
                                                  <p>Britton Ledingham for the Libin Cardiovascular Institute</p>
                              </div>
            </div>
          </div>
              </div>
          </div>
  </div>


                                                                                    </div>
                
                
              </div>
              
            </div>

          </div>
        </div></div>]]>
            </description>
            <link>https://news.ucalgary.ca/news/research-team-discovers-breakthrough-potential-prevent-reverse-alzheimers</link>
            <guid isPermaLink="false">hacker-news-small-sites-24906758</guid>
            <pubDate>Tue, 27 Oct 2020 12:32:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Key Points of Working Effectively with Legacy Code]]>
            </title>
            <description>
<![CDATA[
Score 53 | Comments 21 (<a href="https://news.ycombinator.com/item?id=24906739">thread link</a>) | @nicoespeon
<br/>
October 27, 2020 | https://understandlegacycode.com/blog/key-points-of-working-effectively-with-legacy-code/ | <a href="https://web.archive.org/web/*/https://understandlegacycode.com/blog/key-points-of-working-effectively-with-legacy-code/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><blockquote><p>“Legacy Code is code without tests”</p></blockquote><p>If you’ve come across that definition, it’s from Michael Feathers’ book: <a href="https://www.google.com/search?q=working+effectively+with+legacy+code">Working Effectively with Legacy Code</a>.</p><p>While I have <a href="https://understandlegacycode.com/blog/what-is-legacy-code-is-it-code-without-tests/">a slightly extended definition</a>, this is a very valid and useful one!</p><p>Feathers’ book is from 2004. Yet, its content doesn’t get outdated. There is a reason for that and <a href="http://www.commitstrip.com/en/2019/03/13/like-a-good-wine">this CommitStrip</a> puts it best:</p><p><img src="https://understandlegacycode.com/assets/legacy-code-commitstrip.png" alt="Working Effectively with Legacy Code is like a good wine: it gets better with age"></p><p>This book is a reference. Probably THE reference.</p><p>When there’s a thread about Legacy Code, it doesn’t take long for someone to drop a comment suggesting you read it.</p><blockquote><p>I didn’t read it. I’ve seen it’s recommended. But what are the key points of that book?</p></blockquote><p>If that’s you, I got your back!</p><p>Here’s my summary of the salient points of the book and how they can help you deal with your existing codebase.</p><h2 id="first-add-tests-then-do-your-changes"><a href="#first-add-tests-then-do-your-changes" aria-label="first add tests then do your changes permalink"></a>First, add tests, then do your changes</h2><p>The challenge with changing existing code is to preserve the existing behavior.</p><p>When code is not tested, how do you know you didn’t break anything?</p><p>You need <strong>feedback</strong>. Automated feedback is the best. Thus, this is the first thing you need to do: write the tests.</p><p>Only then you’ll be safe to change the code and refactor.</p><p>Your goal is to get there. The point of the book is to show you <em>how</em> you can get there when you have to deal with an impossibly convoluted codebase. Which leads us to the next point…</p><h2 id="adding-tests-the-legacy-code-dilemma"><a href="#adding-tests-the-legacy-code-dilemma" aria-label="adding tests the legacy code dilemma permalink"></a>Adding tests: the Legacy Code dilemma</h2><p>Before you change code, you should have tests in place. But to put tests in place, you have to change code.</p><p>This is the paradox of Legacy Code!</p><p>So, how do you go about it? Are you doomed?</p><p>You’re not. But you should be extra careful until you got tests in place. You should perform minimal, safe refactorings.</p><p><strong>Change as little code as possible to get tests in place.</strong></p><p>The recipe is:</p><ol><li>Identify change points (Seams)</li><li>Break dependencies</li><li>Write the tests</li><li>Make your changes</li><li>Refactor</li></ol><p>Once you get to the tests, you know how to proceed. The first two points are the difficult ones.</p><h2 id="identify-seams-to-break-your-code-dependencies"><a href="#identify-seams-to-break-your-code-dependencies" aria-label="identify seams to break your code dependencies permalink"></a>Identify Seams to break your code dependencies</h2><p>Adding tests on the existing code can be challenging.</p><p>Hell, it’s usually a nightmare!</p><p>That’s because the code was not written to be <em>testable</em> in the first place. 99% of the time, this is a dependency problem: the code you want to test can’t run because it needs <em>something</em> hard to put in the test.</p><p>Sometimes it’s a database connection. Sometimes it’s a call to a third-party server. Sometimes it’s a parameter that’s complex to instantiate. Usually, it’s a complex mix of all that.</p><p>To test your code, you need to <strong>break these dependencies</strong> in the tests.</p><p>Therefore, you need to identify <strong>Seams</strong>.</p><blockquote><p>“A Seam is a place to alter program behavior, without changing the code.”</p></blockquote><p>There are different types of Seams. The gist of it is to identify how you can change the code behavior without touching the source code.</p><p>If your language is Object-Oriented, the most common and convenient Seam is an object.</p><p>Consider this piece of JavaScript code:</p><pre data-language="js" data-index="0"><p><code><span><span>export</span><span> </span><span>class</span><span> </span><span>DatabaseConnector</span><span> {</span></span>
<span><span>  </span><span>// A lot of code…</span></span>
<span></span>
<span><span>  </span><span>connect</span><span>() {</span></span>
<span><span>    </span><span>// Perform some calls to connect to the DB.</span></span>
<span><span>  }</span></span>
<span><span>}</span></span></code></p></pre><p>Say the <code>connect()</code> method is causing you problems when you try to put code into tests. Well, the whole class is a Seam you can alter.</p><p>You can extend this class in tests to prevent it from connecting to an actual DB:</p><pre data-language="js" data-index="1"><p><code><span><span>class</span><span> </span><span>FakeDatabaseConnector</span><span> </span><span>extends</span><span> </span><span>DatabaseConnector</span><span> {</span></span>
<span><span>  </span><span>connect</span><span>() {</span></span>
<span><span>    </span><span>// Override the problematic calls to the DB</span></span>
<span><span>    </span><span>console</span><span>.</span><span>log</span><span>(</span><span>"Connect to the DB"</span><span>)</span></span>
<span><span>  }</span></span>
<span><span>}</span></span></code></p></pre><p>There are other kinds of Seams.</p><p>If your language allows you to change code behavior without altering the source code, you have an entry point to writing the tests.</p><p>Speaking about tests…</p><h2 id="unit-tests-are-fast-and-reliable"><a href="#unit-tests-are-fast-and-reliable" aria-label="unit tests are fast and reliable permalink"></a>Unit tests are fast and reliable</h2><p>Discussions about testing best practices usually turn into heated debates. Should you apply the <a href="https://martinfowler.com/articles/practical-test-pyramid.html">Pyramid of Tests</a> principle and write a maximum of unit tests? Or should you embrace <a href="https://kentcdodds.com/blog/write-tests/">the Testing Trophy</a> instead and write mostly integration tests?</p><blockquote><p>Why are people giving contradictory advice?</p></blockquote><p>Because <strong>they don’t have the same definition of what a “unit” is</strong>. Thus, some people talk about “integration tests” when others talk about “unit tests”.</p><p>To avoid any confusion, Michael Feathers gives a clear definition of <strong>what is NOT</strong> a unit test.</p><p>In short, your test is not unit if:</p><ol><li>it doesn’t run fast (&lt; 100ms / test)</li><li>it talks to the Infrastructure (e.g. a database, the network, the file system, environment variables…)</li></ol><p>Write a maximum of tests that have these 2 qualities. How you call them doesn’t matter.</p><p>Now, sometimes it’s really hard to write such tests because you don’t even <em>understand</em> what the code is supposed to do. There’s a technique for that…</p><h2 id="characterization-tests"><a href="#characterization-tests" aria-label="characterization tests permalink"></a>Characterization tests</h2><p>Before you can refactor the code, you need tests. But writing these tests can be challenging. Especially when code is hard to understand.</p><blockquote><p>“A characterization test is a test that characterizes the actual behavior of a piece of code.”</p></blockquote><p>Instead of writing comprehensive unit tests, you capture the current behavior of the code. You take a snapshot of what it does.</p><p>The test ensures that this behavior doesn’t change!</p><p>This is very powerful because:</p><ol><li>With most systems, what the code <em>actually</em> does is more important than what it <em>should</em> do.</li><li>You can quickly cover Legacy Code with these tests, giving you a safety net to refactor.</li></ol><p>This technique is also called ”<em>Approval Testing</em>”, ”<em>Snapshot Testing</em>” or ”<em>Golden Master</em>” in the wild. Same stuff.</p><p>And I also blogged about it: <a href="https://understandlegacycode.com/blog/3-steps-to-add-tests-on-existing-code-when-you-have-short-deadlines">the 3 steps to add tests on Legacy Code when you have short deadlines</a>.</p><p>Having short deadlines is a very common situation. When you are in a hurry, it’s hard to take the time not to make things worse. Hopefully, there’s something you can do…</p><h2 id="use-sprout--wrap-techniques-to-add-code-when-you-dont-have-time-to-refactor"><a href="#use-sprout--wrap-techniques-to-add-code-when-you-dont-have-time-to-refactor" aria-label="use sprout  wrap techniques to add code when you dont have time to refactor permalink"></a>Use Sprout &amp; Wrap techniques to add code when you don’t have time to refactor</h2><p>Big lumps of code have a gravitational force. They <em>attract</em> more code.</p><p>It’s the Broken Window theory: a little disorder calls for more serious crimes. If the class is already 2,000 lines-long, who cares that you add 3 more <code>if</code> statements?</p><p>Well, you should. Now you have to maintain a 2,0<strong>10</strong> lines-long class!</p><p>But what if you really, <em>really</em> don’t have time to write tests for that class? That’s just 3 <code>if</code> statements and you might not feel like you can justify taking 2 days for that — although you should.</p><p><img src="https://understandlegacycode.com/assets/time-for-dat.jpg"></p><p>In such a tricky position, you can still make the right call with these 2 techniques.</p><h3 id="1-the-sprout-technique"><a href="#1-the-sprout-technique" aria-label="1 the sprout technique permalink"></a>1. The Sprout technique</h3><ol><li>Create your code somewhere else.</li><li>Unit test it.</li><li>Identify where you should call that code from the existing code: the <em>insertion point</em>.</li><li>Call your code from the Legacy Code.</li></ol><p>Consider the following example:</p><pre data-language="js" data-index="2"><p><code><span><span>class</span><span> </span><span>TransactionGate</span><span> {</span></span>
<span><span>  </span><span>// … a lot of code</span></span>
<span></span>
<span><span>  </span><span>postEntries</span><span>(</span><span>entries</span><span>) {</span></span>
<span><span>    </span><span>for</span><span> (</span><span>let</span><span> </span><span>entry</span><span> </span><span>of</span><span> </span><span>entries</span><span>) {</span></span>
<span><span>      </span><span>entry</span><span>.</span><span>postDate</span><span>()</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    </span><span>// … a lot of code</span></span>
<span></span>
<span><span>    </span><span>transactionBundle</span><span>.</span><span>getListManager</span><span>().</span><span>add</span><span>(</span><span>entries</span><span>)</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  </span><span>// … a lot of code</span></span>
<span><span>}</span></span></code></p></pre><p>Say you need to deduplicate the <code>entries</code>, but <code>postEntries()</code> is hard to test and you really don’t have time for that.</p><p>You can <em>sprout</em> the code somewhere else, like in a new method <code>uniqueEntries()</code>.</p><p>This new method, you can test easily, because it’s isolated.</p><p>Then, insert a call to that method in the existing, non-tested code. Minimal change, minimal risk.</p><pre data-language="js" data-index="3"><p><code><span><span>class</span><span> </span><span>TransactionGate</span><span> {</span></span>
<span><span>  </span><span>// … a lot of code</span></span>
<span></span>
<span><span>  </span><span>uniqueEntries</span><span>(</span><span>entries</span><span>) {</span></span>
<span><span>    </span><span>// Some clever logic to dedupe entries, fully tested!</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  </span><span>postEntries</span><span>(</span><span>entries</span><span>) {</span></span>
<span><span>    </span><span>const</span><span> </span><span>uniqueEntries</span><span> = </span><span>this</span><span>.</span><span>uniqueEntries</span><span>(</span><span>entries</span><span>)</span></span>
<span></span>
<span><span>    </span><span>for</span><span> (</span><span>let</span><span> </span><span>entry</span><span> </span><span>of</span><span> </span><span>uniqueEntries</span><span>) {</span></span>
<span><span>      </span><span>entry</span><span>.</span><span>postDate</span><span>()</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    </span><span>// … a lot of code</span></span>
<span></span>
<span><span>    </span><span>transactionBundle</span><span>.</span><span>getListManager</span><span>().</span><span>add</span><span>(</span><span>uniqueEntries</span><span>)</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  </span><span>// … a lot of code</span></span>
<span><span>}</span></span></code></p></pre><p>The diff might be clearer:</p><pre data-language="diff" data-index="4"><p><code><span><span>class TransactionGate {</span></span>
<span><span>  // … a lot of code</span></span>
<span></span>
<span><span>+  uniqueEntries(entries) {</span></span>
<span><span>+    // Some clever logic to dedupe entries, fully tested!</span></span>
<span><span>+  }</span></span>
<span></span>
<span><span>  postEntries(entries) {</span></span>
<span><span>+    const uniqueEntries = this.uniqueEntries(entries)</span></span>
<span><span>+</span></span>
<span><span>+    for (let entry of uniqueEntries) {</span></span>
<span><span>-    for (let entry of entries) {</span></span>
<span><span>      entry.postDate()</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    // … a lot of code</span></span>
<span></span>
<span><span>+    transactionBundle.getListManager().add(uniqueEntries)</span></span>
<span><span>-    transactionBundle.getListManager().add(entries)</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  // … a lot of code</span></span>
<span><span>}</span></span></code></p></pre><p>You can Sprout a single method, a whole class or anything that will isolate your new code.</p><h3 id="2-the-wrap-technique"><a href="#2-the-wrap-technique" aria-label="2 the wrap technique permalink"></a>2. The Wrap technique</h3><p>When the change you need to do should happen before or after the existing code, you can also <em>wrap</em> it.</p><ol><li>Rename the old method you want to wrap.</li><li>Create a new method with the same name and signature as the old method.</li><li>Call the old method from the new method.</li><li>Put the new logic before/after the other method call.</li></ol><p>That new logic, you can test.</p><p>Why? Because the old method is a Seam you can alter in the tests.</p><p>Remember the previous code?</p><pre data-language="js" data-index="5"><p><code><span><span>class</span><span> </span><span>TransactionGate</span><span> {</span></span>
<span><span>  </span><span>// … a lot of code</span></span>
<span></span>
<span><span>  </span><span>postEntries</span><span>(</span><span>entries</span><span>) {</span></span>
<span><span>    </span><span>for</span><span> (</span><span>let</span><span> </span><span>entry</span><span> </span><span>of</span><span> </span><span>entries</span><span>) {</span></span>
<span><span>      </span><span>entry</span><span>.</span><span>postDate</span><span>()</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    </span><span>// … a lot of code</span></span>
<span></span>
<span><span>    </span><span>transactionBundle</span><span>.</span><span>getListManager</span><span>().</span><span>add</span><span>(</span><span>entries</span><span>)</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  </span><span>// … a lot of code</span></span>
<span><span>}</span></span></code></p></pre><p>Another way to tackle the problem would be to wrap it, so we pass to <code>postEntries()</code> the list of deduped entries:</p><pre data-language="js" data-index="6"><p><code><span><span>class</span><span> </span><span>TransactionGate</span><span> {</span></span>
<span><span>  </span><span>// … a lot of code</span></span>
<span></span>
<span><span>  </span><span>postEntries</span><span>(</span><span>entries</span><span>) {</span></span>
<span><span>    </span><span>// Some clever logic to retrieve unique entries</span></span>
<span><span>    </span><span>this</span><span>.</span><span>postEntriesThatAreUnique</span><span>(</span><span>uniqueEntries</span><span>)</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  </span><span>postEntriesThatAreUnique</span><span>(</span><span>entries</span><span>) {</span></span>
<span><span>    </span><span>for</span><span> (</span><span>let</span><span> </span><span>entry</span><span> </span><span>of</span><span> </span><span>entries</span><span>) {</span></span>
<span><span>      </span><span>entry</span><span>.</span><span>postDate</span><span>()</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    </span><span>// … a lot of code</span></span>
<span></span>
<span><span>    </span><span>transactionBundle</span><span>.</span><span>getListManager</span><span>().</span><span>add</span><span>(</span><span>entries</span><span>)</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  </span><span>// … a lot of code</span></span>
<span><span>}</span></span></code></p></pre><p>In tests, you’d alter the problematic <code>postEntriesThatAreUnique()</code>, so you can test the dedupe logic works.</p><p>The diff might be clearer:</p><pre data-language="diff" data-index="7"><p><code><span><span>class TransactionGate {</span></span>
<span><span>  // … a lot of code</span></span>
<span></span>
<span><span>+  postEntries(entries) {</span></span>
<span><span>+    // Some clever logic to retrieve unique entries</span></span>
<span><span>+    this.postEntriesThatAreUnique(uniqueEntries)</span></span>
<span><span>+  }</span></span>
<span></span>
<span><span>+  postEntriesThatAreUnique(entries) {</span></span>
<span><span>-  postEntries(entries) {</span></span>
<span><span>    for (let entry of entries) {</span></span>
<span><span>      entry.postDate()</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    // … a lot of code</span></span>
<span></span>
<span><span>    transactionBundle.getListManager().add(entries)</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  // … a lot of code</span></span>
<span><span>}</span></span></code></p></pre><p>These techniques are not ideal and they have pitfalls. But they are useful tools to have when addressing Legacy Code.</p><p>And when necessary, you can even bend the rules a bit…</p><h2 id="use-scratch-refactoring-to-get-familiar-with-the-code"><a href="#use-scratch-refactoring-to-get-familiar-with-the-code" aria-label="use scratch refactoring to get familiar with the code permalink"></a>Use scratch refactoring to get familiar with the code</h2><p>When you’ve to work with a code that you didn’t write, that is not tested and that …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://understandlegacycode.com/blog/key-points-of-working-effectively-with-legacy-code/">https://understandlegacycode.com/blog/key-points-of-working-effectively-with-legacy-code/</a></em></p>]]>
            </description>
            <link>https://understandlegacycode.com/blog/key-points-of-working-effectively-with-legacy-code/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24906739</guid>
            <pubDate>Tue, 27 Oct 2020 12:28:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How you could have come up with Paxos yourself]]>
            </title>
            <description>
<![CDATA[
Score 222 | Comments 53 (<a href="https://news.ycombinator.com/item?id=24906225">thread link</a>) | @todsacerdoti
<br/>
October 27, 2020 | https://explain.yshui.dev/distributed%20system/2020/09/20/paxos.html | <a href="https://web.archive.org/web/*/https://explain.yshui.dev/distributed%20system/2020/09/20/paxos.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>In the field of computer science, the Paxos algorithm is notorious for how difficult it is to understand. I had to learn the Paxos algorithm in my distributed systems class. I even have "implemented" it by translating Leslie Lamport's TLA+ to Python. But I didn't understand it until much much later.</p>

<p>Now I have a better understanding of Paxos than I used to, I want to explain it to other people. Not because I'd like to help people, rather, I find that explaining things is a very good way to find blind spots in my own understanding.</p>

<p>So, where do we start? Personally, I dislike explanations that start with a step-by-step breakdown of the algorithm, followed by a proof of why those steps do what they claim to do. Instead, I much prefer to start with the problem the algorithm tries to solve, then iteratively come up with a solution together with the reader. So that's what I am going to do. And now you understand the title.</p>

<p><em>Small disclaimer:</em> The glossaries used in this article is different from what is commonly used for Paxos. I just picked the ones that made the most sense for my narrative.</p>

<h2 id="the-problem">The problem</h2>

<p>The distributed consensus problem is widely useful, so the reader probably doesn't need to be motivated. Here I will just simply state the problem.</p>

<p>There is a group of agents (let's call them $\sc{CLIENT}s$), who want to choose a number among their selections. Any number is fine, as long as everyone agrees on the same number.</p>

<p>Here, there are a few assumptions we will make to make this problem meaningful:</p>

<ul>
  <li>All the agents - including but not limited to the $\sc{CLIENT}s$, as we will add more types of agents later - are well-behaved. Meaning they all execute the prescribed algorithms faithfully, and don't maliciously try to trick other agents. (If you like jargons: Byzantine failures don't occur.)</li>
  <li>Agents can talk to each other by sending each other messages, but the messages they send to each other could take arbitrarily long before reaching their destination, and might get lost (but never altered).</li>
</ul>

<p>The agents could also "fail". However, failing is equivalent to all messages sent to/from that agent being lost forever. So whether we have this assumption or not won't change the algorithm we come up with.</p>

<p>Also, to not complicate things, we are only solving the "single-round" consensus problem in this article, meaning as the output of this algorithm, all of the $\sc{CLIENT}s$ will get a single number which they agree on.</p>

<h2 id="solution-searching-adventure">Solution searching adventure</h2>

<h3 id="iteration-0">Iteration 0</h3>

<p>When trying to solve a complex problem such as this one, it's usually a good idea to start by simplifying the problem.
As a start, let's just ignore the need to be reliable entirely.</p>

<p>If we throw reliability out the window, it should be easy to come up with a very simple solution: we add an agent (let's call it $\sc{COORDINATOR}$).
The $\sc{CLIENTS}$ send whatever number they pick to the $\sc{COORDINATOR}$ in a $\sc{PROPOSAL}(client_i, x)$ message, where $x$ is the number
proposed by the $i$-th $\sc{CLIENT}$. The $\sc{COORDINATOR}$ picks an arbitrary proposal (say, $x'$),
and informs the other $\sc{CLIENT}s$ about this decision.
Specifically, the $\sc{COORDINATOR}$ will just reply with a $\sc{CHOSEN}(x')$ message to all the $\sc{PROPOSAL}(\ldots)$ messages it has
received and will receive.</p>

<p>If we assume no messages ever get lost, it is quite easy to see that every $\sc{CLIENT}$ will get a number. And because only one number is ever chosen, they will all get the same number.</p>

<p>It is also easy to see why this solution is impractical: it has a single point of failure. Once the singular $\sc{COORDINATOR}$ fails, no further progress can be made.</p>

<h3 id="iteration-1">Iteration 1</h3>

<p>To improve this almost looks easy at first glance: just add more $\sc{COORDINATOR}s$!</p>

<p>Sure, more $\sc{COORDINATOR}s$ would remove the single point of failure. However, if there are more than one $\sc{COORDINATOR}s$, they might individually make different decisions, which results in the $\sc{CLIENT}s$ having disagreement.</p>

<p>What if we let the $\sc{COORDINATOR}s$ reach an agreement among themselves before responding? But wait, doesn't that sound familiar? Having a group of agents reaching an agreement, that's exactly what we added the $\sc{COORDINATOR}s$ to solve. We just made the problem cyclic.</p>

<p>Let's take a step back. Is there a way for the clients to reach an agreement without having the $\sc{COORDINATOR}s$ communicate with each other?</p>

<p>In other words, among the decisions of the $\sc{COORDINATOR}s$, is there an deterministic algorithm to pick out a specific one that is robust against message losses?</p>

<p>This might sound hard, but it's actually quite simple: pick the decision that is backed by more than half of the $\sc{COORDINATOR}s$.</p>

<p>There can't be two decisions both with more than half of the $\sc{COORDINATOR}s$ backing them; and if a decision doesn't have that many $\sc{COORDINATOR}s$ backing it, it won't appear to have more backing $\sc{COORDINATOR}s$ through message losses.</p>

<p>Since this approach resembles a majority vote, let's call $\sc{COORDINATOR}$ decisions $\sc{VOTE}(coord_i, x)$ from now on, where $x$ is the number picked by the $i$-th $\sc{COORDINATOR}$. Each $\sc{COORDINATOR}$ has a single vote, because each of them only makes a single decision.</p>

<p>Obviously, our solution cannot be infinitely reliable. If more than half of the $\sc{COORDINATOR}s$ went down, there will never be a majority reached. But this is already vastly better than our first solution, and the reliability scales with the number of $\sc{COORDINATOR}s$. So we will call it good enough.</p>

<p>Sadly, this solution doesn't actually work: there might not be a majority at all! For example, it's possible that three of the proposals each get a third of the votes. We would have a stalemate in that case.</p>

<h3 id="iteration-2">Iteration 2</h3>

<p>Again, a solution seems straightforward: just try again in case of a stalemate.</p>

<p>But then again, things aren't that simple.</p>

<p>First of all, the $\sc{COORDINATOR}s$ need to be made aware of a retry. Otherwise, because each $\sc{COORDINATOR}$ only has one vote, they won't be able to vote again even if the $\sc{CLIENT}s$ retry.</p>

<p>To do that, we attach an attempt id to all the messages sent. i.e. $\sc{PROPOSAL}(client_i, x)$ becomes $\sc{PROPOSAL}(\#attempt, client_i, x)$, and so forth. Each time a $\sc{CLIENT}$ retries, it bumps $\#attempt$ to the maximum $\#attempt$ it knows of plus 1. And the $\sc{COORDINATOR}s$ should only responds to messages with the most recent $\#attempt$.</p>

<p>Hopefully the intent of the $\#attempt$ number is clear. (<a href="https://github.com/yshui/explain-algorithms/issues/new">Let me know</a> if not.)</p>

<p>Are we good now? Unfortunately, no. Consider this scenario:</p>

<p>There were 2 clients. They proposed their numbers, the $\sc{COORDINATOR}$ voted on them and all agreed on a single number, $x_1$, all is good. But, all of the $\sc{VOTE}(\ldots)$ messages got lost on the way to $client_2$, while $client_1$ received all of the messages just fine. At this point, $client_1$ thought $x_1$ is the number, but $client_2$ went on to retry. The $\sc{COORDINATOR}s$ voted again, and got $x_2$. This time, all the messages sent to $client_1$ got lost.</p>

<p>And behold, we got the two clients to disagree.</p>

<p>There is an important insight to be had here. Whenever a $\sc{COORDINATOR}$, say $coord_i$, sends out a $\sc{VOTE}(\ldots, coord_i, x)$, there is a chance that some $\sc{CLIENT}$ would adopt $x$. If $coord_i$ ever sends out two votes with different $x$, there is a chance that some of the $\sc{CLIENT}s$ would disagree.</p>

<p>In other words, once a $\sc{COORDINATOR}$ has revealed its vote, it has to stick to it.</p>

<p>This seems to run contrary to our attempt: if the $\sc{COORDINATOR}s$ cannot change their votes, what's the point of retrying? A stalemate will be a stalemate forever.</p>

<p>Looks like we reached a dead end with this type of voting. It appears the problem stems from the fact that the $\sc{COORDINATOR}s$ have to commit to their votes.</p>

<p>So, what if we introduce a form of non-commitment voting?</p>

<h3 id="iteration-3">Iteration 3</h3>

<p>Let's explore this idea. Say, the $\sc{COORDINATOR}s$ could now send a $\sc{TENTATIVE}\sc{VOTE}(\#attempt, coord_i, x)$ message, to tentatively vote for $x$.</p>

<p>Obviously, the $\sc{CLIENT}s$ couldn't adopt $x$ right away. So what's this vote good for?</p>

<p>Ah, right, it could get us to a majority.</p>

<p>It is correct that tentative votes don't lead directly to an agreement among $\sc{CLIENT}s$, but it can show us when a majority has formed among the $\sc{COORDINATOR}s$.</p>

<p>Once a $\sc{CLIENT}$ sees a majority tentative vote, it can then message the $\sc{COORDINATOR}s$ to ask for an actual vote. (Let's call this message $\sc{PLEASE}\sc{VOTE}(\#attempt, client_i)$). Intuitively, the $\sc{COORDINATOR}s$ have to make the same vote in the actual vote as their tentative votes.</p>

<p>If all goes well, we would get a majority and an agreement. If there is no majority, the $\sc{COORDINATOR}s$ won't even start a vote, so they are free to change their mind. So the $\sc{CLIENT}s$ could start another attempt which might have a different outcome.</p>

<p>What if things don't go well? What if the $\sc{PLEASE}\sc{VOTE}$ messages weren't received by some of the $\sc{COORDINATOR}s$?
In that case, some of the $\sc{COORDINATOR}s$ would have voted, and their decisions cannot be changed. That is to say, in all subsequent attempts, these $\sc{COORDINATOR}s$ will always vote for what they have voted for, whether it's a tentative vote, or the actual vote. But that doesn't create a problem for us. There was a majority in the tentative votes, and now we solidified part of the tentative votes. There is at least one way we can still reach a majority in the next round: everyone votes the same as they did in this round. And we can prove this inductively for all future rounds.</p>

<p>From this, we can have a rough image of how the algorithm functions: as attempts are being made, more and more $\sc{COORDINATOR}s$ start to make up their mind which number they will commit to, while making sure a majority could still be reached. Eventually, …</p></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://explain.yshui.dev/distributed%20system/2020/09/20/paxos.html">https://explain.yshui.dev/distributed%20system/2020/09/20/paxos.html</a></em></p>]]>
            </description>
            <link>https://explain.yshui.dev/distributed%20system/2020/09/20/paxos.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24906225</guid>
            <pubDate>Tue, 27 Oct 2020 11:06:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Text layout is a loose hierarchy of segmentation]]>
            </title>
            <description>
<![CDATA[
Score 130 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24906010">thread link</a>) | @adamnemecek
<br/>
October 27, 2020 | https://raphlinus.github.io/text/2020/10/26/text-layout.html | <a href="https://web.archive.org/web/*/https://raphlinus.github.io/text/2020/10/26/text-layout.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>I love text layout, and have been working with it in one form or other for over 35 years. Yet, knowledge about it is quite arcane. I don’t believe there is a single place where it’s all properly written down. I have some explanation for that: while basic text layout is very important for UI, games, and other contexts, a lot of the “professional” needs around text layout are embedded in <em>much</em> more complicated systems such as Microsoft Word or a modern Web browser.</p>

<p>A complete account of text layout would be at least a small book. Since there’s no way I can write that now, this blog post is a small step towards that – in particular, an attempt to describe the “big picture,” using the conceptual framework of a “loose hierarchy.” Essentially, a text layout engine breaks the input into finer and finer grains, then reassembles the results into a text layout object suitable for drawing, measurement, and hit testing.</p>

<p>The main hierarchy is concerned with laying out the entire paragraph as a single line of text. Line breaking is also important, but has a separate, parallel hierarchy.</p>

<h2 id="the-main-text-layout-hierarchy">The main text layout hierarchy</h2>

<p>The hierarchy is: paragraph segmentation as the coarsest granularity, followed by rich text style and BiDi analysis, then itemization (coverage by font), then Unicode script, and shaping clusters as the finest.</p>

<p><img src="https://raphlinus.github.io/assets/layout_pyramid.svg" alt="diagram of layout hierarchy"></p>

<h3 id="paragraph-segmentation">Paragraph segmentation</h3>

<p>The coarsest, and also simplest, segmentation task is paragraph segmentation. Most of the time, paragraphs are simply separated by newline (U+000A) characters, though Unicode in its infinite wisdom specifies a number of code point sequences that function as paragraph separators in plain text:</p>

<ul>
  <li>U+000A LINE FEED</li>
  <li>U+000B VERTICAL TAB</li>
  <li>U+000C FORM FEED</li>
  <li>U+000D CARRIAGE RETURN</li>
  <li>U+000D U+000A (CR + LF)</li>
  <li>U+0085 NEXT LINE</li>
  <li>U+2008 LINE SEPARATOR</li>
  <li>U+2009 PARAGRAPH SEPARATOR</li>
</ul>

<p>In rich text, paragraphs are usually indicated through markup rather than special characters, for example <code>&lt;p&gt;</code> or <code>&lt;br&gt;</code> in HTML. But in this post, as in most text layout APIs, we’ll treat rich text as plain text + attribute spans.</p>

<h3 id="rich-text-style">Rich text style</h3>

<p>A paragraph of rich text may contain <em>spans</em> that can affect formatting. In particular, choice of font, font weight, italic or no, and a number of other attributes can affect text layout. Thus, each paragraph is typically broken into a some number of <em>style runs,</em> so that within a run the style is consistent.</p>

<p>Note that some style changes don’t <em>necessarily</em> affect text layout. A classic example is color. Firefox, rather famously, does <em>not</em> define segmentation boundaries here for color changes. If a color boundary cuts a ligature, it uses fancy graphics techiques to render parts of the ligature in different color. But this is a subtle refinement and I think not required for basic text rendering. For more details, see <a href="https://gankra.github.io/blah/text-hates-you/">Text Rendering Hates You</a>.</p>

<h3 id="bidirectional-analysis">Bidirectional analysis</h3>

<p>Completely separate from the style spans, a paragraph may in general contain both left-to-right and right-to-left text. The need for bidirectional (BiDi) text is certainly one of the things that makes text layout more complicated.</p>

<p>Fortunately, this part of the stack is defined by a standard (<a href="http://www.unicode.org/reports/tr9/">UAX #9</a>), and there are a number of good implementations. The interested reader is referred to <a href="https://www.w3.org/International/articles/inline-bidi-markup/uba-basics">Unicode Bidirectional Algorithm basics</a>. The key takeaway here is that BiDi analysis is done on the plain text of the entire paragraph, and the result is a sequence of <em>level runs,</em> where the level of each run defines whether it is LTR or RTL.</p>

<p>The level runs and the style runs are then merged, so that in subsequent stages each run is of a consistent style and directionality. As such, for the purpose of defining the hierarchy, the result of BiDi analysis could alternatively be considered an implicit or derived rich text span.</p>

<p>In addition to BiDi, which I consider a basic requirement, a more sophisticated text layout engine will also be able to handle vertical <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/writing-mode">writing modes</a>, including mixed cases where short strings are horizontal within the vertical primary direction. Extremely sophisticated layout engines will also be able to handle ruby text and other ways of annotating the main text flow with intercalated strings. See <a href="https://www.w3.org/TR/jlreq/">Requirements for Japanese Text Layout</a> for many examples of sophisticated layout requirements; the scope of this blog post really is basic text layout of the kind needed in user interfaces.</p>

<h3 id="itemization-font-coverage">Itemization (font coverage)</h3>

<p>Itemization is the trickiest and least well specified part of the hierarchy. There is no standard for it, and no common implementation. Rather, each text layout engine deals with it in its own special way.</p>

<p>Essentially, the result of itemization is to choose a single concrete font for a run, from a <em>font collection.</em> Generally a font collection consists of a main font (selected by font name from system fonts, or loaded as a custom asset), backed by a <em>fallback stack,</em> which are usually system fonts, but thanks to <a href="https://www.google.com/get/noto/">Noto</a> it is possible to bundle a fallback font stack with an application, if you don’t mind spending a few hundred megabytes for the assets.</p>

<p>Why is it so tricky? A few reasons, which I’ll touch on.</p>

<p>First, it’s not so easy to determine whether a font can render a particular string of text. One reason is <a href="https://unicode.org/reports/tr15/">Unicode normalization</a>. For example, the string “é” can be encoded as U+00E9 (in NFC encoding) or as U+0065 U+0301 (in NFD encoding). Due to the principle of <a href="https://en.wikipedia.org/wiki/Unicode_equivalence">Unicode equivalence</a>, these should be rendered identically, but a font may have coverage for only one or the other in its <a href="https://docs.microsoft.com/en-us/typography/opentype/spec/cmap">Character to Glyph Index Mapping</a> (cmap) table. The shaping engine has all the Unicode logic to handle these cases.</p>

<p>Of course, realistic fonts with Latin coverage will have both of these particular sequences covered in the cmap table, but edge cases certainly do happen, both in extended Latin ranges, and other scripts such as Hangul, which has complex normalization rules (thanks in part to a Korean standard for normalization which is somewhat at odds with Unicode). It’s worth noting that <a href="https://devblogs.microsoft.com/oldnewthing/20201009-00/?p=104351">DirectWrite gets Hangul normalization quite wrong</a>.</p>

<p>I believe a similar situation exists with the Arabic presentation forms; see <a href="https://www.arabeyes.org/Developing_Arabic_fonts">Developing Arabic fonts</a> for more detail on that.</p>

<p>Because of these tricky normalization and presentation issues, the most robust way to determine whether a font can render a string is to try it. This is how LibreOffice has worked for a while, and in 2015 <a href="https://lists.freedesktop.org/archives/harfbuzz/2015-October/005168.html">Chromium followed</a>. See also <a href="https://www.chromium.org/teams/layout-team/eliminating-simple-text">Eliminating Simple Text</a> for more background on the Chromium text layout changes.</p>

<p><em>Another</em> whole class of complexity is emoji. A lot of emoji can be rendered with either <a href="https://en.wikipedia.org/wiki/Emoji#Emoji_versus_text_presentation">text or emoji presentation</a>, and there are no hard and fast rules to pick one or the other. Generally the text presentation is in a symbol font, and the emoji presentation is in a separate color font. A particularly tough example is the smiling emoji, which began its encoding life as 0x01 in <a href="https://en.wikipedia.org/wiki/Code_page_437">Code page 437</a>, the standard 8-bit character encoding of the original IBM PC, and is now U+263A in Unicode. However, the suggested default presentation is text, which won’t do in a world which expects color. Apple on iOS unilaterally chose an emoji presentation, so many text stacks follow Apple’s lead. (Incidentally, the most robust way to encode such emoji is to append a <a href="https://en.wikipedia.org/wiki/Variation_Selectors_(Unicode_block)">variation selector</a> to pin down the presentation.)</p>

<p>Another source of complexity when trying to write a cross-platform text layout engine is querying the system fonts. See <a href="https://raphlinus.github.io/rust/skribo/text/2019/04/04/font-fallback.html">Font fallback deep dive</a> for more information about that.</p>

<p>I should note one thing, which might help people doing archaeology of legacy text stacks: it used to be pretty common for text layout to resolve “compatibility” forms such as NFKC and NFKD, and this can lead to various problems. But today it is more common to solve that particular problem by providing a font stack with <em>massive</em> Unicode coverage, including all the code points in the relevant compatibility ranges.</p>

<h3 id="script">Script</h3>

<p>The <em>shaping</em> of text, or the transformation of a sequence of code points into a sequence of positioned glyphs, depends on the script. Some scripts, such as Arabic and Devanagari, have extremely elaborate shaping rules, while others, such as Chinese, are a fairly straightforward mapping from code point into glyph. Latin is somewhere in the middle, starting with a straightforward mapping, but ligatures and kerning are also required for high quality text layout.</p>

<p>Determining script runs is reasonably straightforward - many characters have a Unicode script property which uniquely identifies which script they belong to. However, some characters, such as space, are “common,” so the assigned script just continues the previous run.</p>

<p>A simple example is “hello мир”. This string is broken into two script runs: “hello “ is <code>Latn</code>, and “мир” is <code>Cyrl</code>.</p>

<h3 id="shaping-cluster">Shaping (cluster)</h3>

<p>At this point, we have a run of constant style, font, direction, and script. It is ready for <em>shaping.</em> Shaping is a complicated process that converts a string (sequence of Unicode code points) into positioned glyphs. For the purpose of this blog post, we can generally treat it as a black box. Fortunately, a very high quality open source implementation exists, in the form of HarfBuzz.</p>

<p>We’re not <em>quite</em> done with segmentation, though, as shaping assigns substrings in the input to <a href="https://harfbuzz.github.io/clusters.html">clusters</a> of glyphs. The correspondence depends a lot on the font. In Latin, the string “fi” is often shaped to a single glyph (a ligature). For complex scripts such as Devanagari, a cluster is most often a syllable in the source text, and complex reordering can happen within the cluster.</p>

<p>Clusters are important for <em>hit testing,</em> or determining the correspondence between a physical cursor position in the text layout and the offset within the text. Generally, they can be ignored if the text will only be rendered, not edited (or selected).</p>

<p>Note that these shaping clusters are distinct from grapheme clusters. The “fi” example has two grapheme clusters but a single shaping cluster, so a grapheme cluster boundary can cut a shaping cluster. Since …</p></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://raphlinus.github.io/text/2020/10/26/text-layout.html">https://raphlinus.github.io/text/2020/10/26/text-layout.html</a></em></p>]]>
            </description>
            <link>https://raphlinus.github.io/text/2020/10/26/text-layout.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24906010</guid>
            <pubDate>Tue, 27 Oct 2020 10:25:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Graphics in Qt 6.0: QRhi, Qt Quick, Qt Quick 3D]]>
            </title>
            <description>
<![CDATA[
Score 116 | Comments 54 (<a href="https://news.ycombinator.com/item?id=24905634">thread link</a>) | @MikusR
<br/>
October 27, 2020 | https://www.qt.io/blog/graphics-in-qt-6.0-qrhi-qt-quick-qt-quick-3d | <a href="https://web.archive.org/web/*/https://www.qt.io/blog/graphics-in-qt-6.0-qrhi-qt-quick-qt-quick-3d">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                            

                            <p>
                                Monday October 26, 2020 by <a href="https://www.qt.io/blog/author/laszlo-agocs">Laszlo Agocs</a> | <a href="#commento">Comments</a>
                            </p>
                            
                            <p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p>Last year we had a three part blog series about Qt's new approach to working with 3D graphics APIs and shading languages: <a href="https://www.qt.io/blog/qt-quick-on-vulkan-metal-direct3d">part 1</a>, <a href="https://www.qt.io/blog/qt-quick-on-vulkan-metal-and-direct3d-part-2">part 2</a>, <a href="https://www.qt.io/blog/qt-quick-on-vulkan-metal-and-direct3d-part-3">part 3</a>. For <a href="https://doc-snapshots.qt.io/qt6-dev/qtquick-index.html">Qt Quick</a>, an early, opt-in preview of the new rendering architecture was shipped in Qt 5.14, with some improvements in Qt 5.15. With the release of Qt 6.0 upcoming, let's see what has happened since Qt 5.15. It will not be possible to cover every detail of the graphics stack improvements for Qt Quick here, let alone dive into the vast amount of Qt Quick 3D features, many of which are new or improved in Qt 6.0. Rather, the aim is just to give an overview of what can be expected from the graphics stack perspective when Qt 6.0 ships later this year.</p>
<p>Note that the documentation links refer to the Qt 6 snapshot documentation. This allows seeing the latest C++ and QML API pages, including all changed and new functions, but the content is also not final. These links may also break later on.</p>
<!--more-->
<h2>QRhi improvements</h2>
<p>QRhi, the Qt Rendering Hardware Interface, is Qt's internal graphics abstraction when 3D APIs, such as OpenGL, Vulkan, Metal, and Direct 3D, are involved. Compared to 5.15, the main improvements in 6.0 are a lot of polishing fixes here and there, and, most importantly, a large set of performance optimizations. While benefitting Qt Quick as well, these become especially important with Qt Quick 3D when complex scenes with many renderable objects are involved.</p>
<p>With some simplifications, the main layers of the Qt 6.0 graphics stack can be visualized like this:</p>
<p><img src="https://www.qt.io/hs-fs/hubfs/rhiarch-3.png?width=800&amp;name=rhiarch-3.png" alt="rhiarch-3" width="800" srcset="https://www.qt.io/hs-fs/hubfs/rhiarch-3.png?width=400&amp;name=rhiarch-3.png 400w, https://www.qt.io/hs-fs/hubfs/rhiarch-3.png?width=800&amp;name=rhiarch-3.png 800w, https://www.qt.io/hs-fs/hubfs/rhiarch-3.png?width=1200&amp;name=rhiarch-3.png 1200w, https://www.qt.io/hs-fs/hubfs/rhiarch-3.png?width=1600&amp;name=rhiarch-3.png 1600w, https://www.qt.io/hs-fs/hubfs/rhiarch-3.png?width=2000&amp;name=rhiarch-3.png 2000w, https://www.qt.io/hs-fs/hubfs/rhiarch-3.png?width=2400&amp;name=rhiarch-3.png 2400w" sizes="(max-width: 800px) 100vw, 800px"></p>
<h2>Shader management</h2>
<p>The Qt Shader Tools module is now a selectable module in the installer. For applications this can be relevant because this is the module that provides the <em>qsb</em> command-line tool (not to be confused with <em>qbs</em>) and its associated CMake build system integration. In addition, the module is a mandatory dependency for Qt Quick 3D at the moment.</p>
<p>Qt 6 no longer uses OpenGL-compatible GLSL source snippets directly. Rather, shaders are all written in Vulkan-style GLSL, then reflected and translated to other shading languages, and finally packaged up into a serializable QShader object that can be consumed by QRhi. The shader preparation pipeline in Qt 6 is the following:</p>
<p><img src="https://www.qt.io/hs-fs/hubfs/shaderconditioning.png?width=1280&amp;name=shaderconditioning.png" alt="shaderconditioning" width="1280" srcset="https://www.qt.io/hs-fs/hubfs/shaderconditioning.png?width=640&amp;name=shaderconditioning.png 640w, https://www.qt.io/hs-fs/hubfs/shaderconditioning.png?width=1280&amp;name=shaderconditioning.png 1280w, https://www.qt.io/hs-fs/hubfs/shaderconditioning.png?width=1920&amp;name=shaderconditioning.png 1920w, https://www.qt.io/hs-fs/hubfs/shaderconditioning.png?width=2560&amp;name=shaderconditioning.png 2560w, https://www.qt.io/hs-fs/hubfs/shaderconditioning.png?width=3200&amp;name=shaderconditioning.png 3200w, https://www.qt.io/hs-fs/hubfs/shaderconditioning.png?width=3840&amp;name=shaderconditioning.png 3840w" sizes="(max-width: 1280px) 100vw, 1280px"></p>
<p>In QML applications using Qt Quick, whenever working with ShaderEffect, or subclassing QSGMaterialShader, the application will need to provide a baked shader pack in form of a .qsb file. These are generated by the <em>qsb</em> tool. This does not however mean that developers have to start dealing with a new tool directly: with the CMake integration one can easily list the vertex, fragment, and compute shaders in CMakeLists.txt via the qt6_add_shaders() CMake function. Invoking qsb and packing the resulting .qsb files into the Qt resource system is then taken care of by the build system.</p>
<p>See <a href="https://doc-snapshots.qt.io/qt6-dev/qtshadertools-index.html">the shadertools documentation</a> for an overview of how graphics and compute shaders are handled in Qt 6 and the details of the qsb tool and its CMake integration.</p>
<h2>Direct OpenGL is no more for Qt Quick</h2>
<p>In Qt 5.14 and 5.15, Qt Quick shipped with an optional QRhi-based rendering path that could be enabled by setting the environment variable <em>QSG_RHI</em>. This allowed painless experimenting with the new stack, while keeping the traditional, battle tested direct OpenGL code path the default.</p>
<p>In Qt 6.0 all such switches are gone. There is no way get rendering go directly to OpenGL with Qt Quick scenes. Rather, the new default is the QRhi-based rendering path of the Qt Quick scene graph. Other than the defaults changing, the ways to configure what QRhi backend, and so which graphics API to use are mostly unchanged compared to Qt 5.15. See <a href="https://doc-snapshots.qt.io/qt6-dev/qtquick-visualcanvas-scenegraph-renderer.html#rendering-via-the-qt-rendering-hardware-interface">the documentation</a> for details. One difference is better API naming: in C++ code to request, and so effectively tie the application to, a given QRhi backend (and by extension graphics API) is now done through the <a href="https://doc-snapshots.qt.io/qt6-dev/qquickwindow.html#setGraphicsApi">QQuickWindow::setGraphicsApi()</a> function, whereas in 5.15 this task used to be pushed onto an overload of setSceneGraphBackend(), leading to fairly inaccurate naming.</p>
<p>There are a number of implications, although many applications will not notice any of these. If an application uses neither shader code (ShaderEffect, QSGMaterial) nor does it perform its own rendering with OpenGL directly, there is a very high chance that it will need no migration steps at all. (at least not because of graphics)</p>
<h4>Applications using OpenGL directly</h4>
<p>What about applications that use OpenGL directly in one way or another, and are not interested in functioning with other graphics APIs? For example, applications that use <a href="https://doc-snapshots.qt.io/qt6-dev/qquickframebufferobject.html">QQuickFramebufferObject</a>, or connect to signals like <a href="https://doc-snapshots.qt.io/qt6-dev/qquickwindow.html#beforeRendering">QQuickWindow::beforeRendering()</a> to inject their own OpenGL rendering under or above the Qt Quick scene. This is when the setGraphicsApi() function mentioned above comes into play for real: if an application wishes, it can always state that it wants OpenGL (or Vulkan, or Metal, or D3D) only, and nothing else. That way it is guaranteed that Qt Quick is going to use the corresponding QRhi backend (or else it will fail to initialize), so the application can safely assume that going directly to OpenGL is safe, because Qt Quick will also end up rendering through OpenGL. Note that this does not exempt the application from having to do other type of porting steps: for example, if it in addition uses ShaderEffect or creates its own custom materials, it will still need to migrate to the new ways of handling shaders and materials.</p>
<h4>QSG* and QQuick* API changes</h4>
<p>The API changes mainly fall into 3 categories. This is not going to be an exhaustive list, but rather just a peek at some of the important changes. Detailed change lists and porting guides are expected to be available with the final Qt 6.0 release.</p>
<ul>
<li>
<div><p>Different approach to shaders and materials: <a href="https://doc-snapshots.qt.io/qt6-dev/qsgmaterialshader.html">QSGMaterialShader</a> received a full revamp (matching more or less what the now-removed QSGMaterialRhiShader used to be in 5.14 and 5.15). <a href="https://doc-snapshots.qt.io/qt6-dev/qml-qtquick-shadereffect.html">ShaderEffect</a> no longer allows inline shader strings. Rather, the vertexShader and fragmentShader properties are URLs, similarly to <span>Image.source</span> and others. They can refer to a local .qsb file, or a .qsb file embedded via the Qt resource system (qrc).</p></div>
</li>
<li>
<p>Removing OpenGL-specifics from QQuickWindow, QSGTexture, and elsewhere. It should come as no surprise that functions like <em>GLuint textureId()</em>, <em>createTextureFromId(GLuint textureId, ...)</em>, or <em>setRenderTarget(GLuint fboId)</em> are now gone. Adopting (wrapping) an existing OpenGL texture, Vulkan image, Metal texture, or D3D11 texture, or accessing the underlying native texture for a QSGTexture is still perfectly possible, but now is done via a different set of APIs, such as <a href="https://doc-snapshots.qt.io/qt6-dev/qnativeinterface-qsgvulkantexture.html">QSGVulkanTexture</a> and the <a href="https://doc-snapshots.qt.io/qt6-dev/qnativeinterface-sub-qtquick.html">other similar classes</a>, instances of which are <a href="https://doc-snapshots.qt.io/qt6-dev/qsgtexture.html?__hstc=233546881.8510e053e4fb66e1a58543a6e9886427.1603454017210.1603454017210.1603454017210.1&amp;__hssc=233546881.1.1603454017210&amp;__hsfp=1285229618#nativeInterface" rel="noopener">queryable from QSGTexure</a>.</p>
<ul>
<li>
<div><p>Integrating the application's own custom rendering with the graphics API that Qt Quick renders with is fully supported, not just for OpenGL, but also Vulkan, Metal, and D3D11. Due to their nature however, some of these APIs will need more than connecting to one single signal like beforeRendering() or afterRendering(). For example, we now also have <a href="https://doc-snapshots.qt.io/qt6-dev/qquickwindow.html#beforeRenderPassRecording">beforeRenderPassRecording()</a>. See the relevant section in the <a href="https://doc-snapshots.qt.io/qt6-dev/qtquick-visualcanvas-scenegraph.html#mixing-scene-graph-and-the-native-graphics-api">scenegraph overview docs</a> for more details and links to examples. Finally, the number of native graphics resources queryable via <a href="https://doc-snapshots.qt.io/qt6-dev/qsgrendererinterface.html">QSGRendererInterface</a> has been extended, now covering Vulkan, Metal, and Direct 3D too.</p></div>
</li>
</ul>
</li>
<li>
<p>Extending support for redirecting the Qt Quick scene into an offscreen render target. <a href="https://www.qt.io/blog/%3Ehttps://doc-snapshots.qt.io/qt6-dev/qquickrendercontrol.html">QQuickRenderControl</a> and the related infrastructure has been heavily enhanced. This was done not just to enable working with graphics APIs other than OpenGL the same way as in Qt 5 (for example, to render a Qt Quick scene into a Vulkan VkImage without an on-screen window), but also to enable integration with AR/VR frameworks and APIs such as <a href="https://www.khronos.org/openxr/">OpenXR</a> (in combination with any of Vulkan, D3D11, or OpenGL). Besides the slightly changed QQuickRenderControl interface, we now have a number of helper classes that improve the configurability of a QQuickWindow: <a href="https://doc-snapshots.qt.io/qt6-dev/qquickrendertarget.html">QQuickRenderTarget</a>, <a href="https://doc-snapshots.qt.io/qt6-dev/qquickgraphicsdevice.html">QQuickGraphicsDevice</a>, and <a href="https://doc-snapshots.qt.io/qt6-dev/qquickgraphicsconfiguration.html">QQuickGraphicsConfiguration</a>. These are essential in scenarios where a more fine grained control is needed: integrating with APIs like OpenXR is not always straightforward when an existing rendering engine is involved, with a number of potential chicken-egg problems when it comes to the creation, initialization, and ownership of instance, device, and other graphics objects: Which Vulkan instance should Qt Quick use, or should it create a new one upon initializing the scenegraph for the first time? Which Vulkan physical device or DXGI adapter should Qt Quick pick, or just stay with the default? Which VkDevice extensions should be enabled in addition to what Qt itself needs? What 2D image/texture should rendering target, who creates that and when? The expectation is that Qt 6.0 will be well-prepared and providing the foundations for further exploring the world of AR/VR during the rest of the Qt 6.x series.</p>
</li>
</ul>
<h4>New approach to handling shader code in ShaderEffect</h4>
<p>A comprehensive example of the new approach to shader code in ShaderEffect is the Qt 6 port of the classic Qt 5 Cinematic Experience demo. <a href="https://github.com/alpqr/qt5-cinematic-experience" rel="noopener">(GitHub repo)</a> This version is ported to CMake and is fully functional with all graphics APIs, including all shader and particle effects.</p>
<p><img src="https://www.qt.io/hs-fs/hubfs/cinematic.png?width=702&amp;name=cinematic.png" alt="cinematic" width="702" srcset="https://www.qt.io/hs-fs/hubfs/cinematic.png?width=351&amp;name=cinematic.png 351w, https://www.qt.io/hs-fs/hubfs/cinematic.png?width=702&amp;name=cinematic.png 702w, https://www.qt.io/hs-fs/hubfs/cinematic.png?width=1053&amp;name=cinematic.png 1053w, https://www.qt.io/hs-fs/hubfs/cinematic.png?width=1404&amp;name=cinematic.png 1404w, https://www.qt.io/hs-fs/hubfs/cinematic.png?width=1755&amp;name=cinematic.png 1755w, https://www.qt.io/hs-fs/hubfs/cinematic.png?width=2106&amp;name=cinematic.png 2106w" sizes="(max-width: 702px) 100vw, 702px"></p>
<p>Looking at the QML source code, for example the code for the <a href="https://github.com/alpqr/qt5-cinematic-experience/blob/master/content/CurtainEffect.qml" rel="noopener">curtain effect </a>shows that indeed it has all inline GLSL strings removed.</p>
<p><img src="https://www.qt.io/hs-fs/hubfs/image-png-Oct-19-2020-12-22-47-14-PM.png?width=354&amp;name=image-png-Oct-19-2020-12-22-47-14-PM.png" width="354" srcset="https://www.qt.io/hs-fs/hubfs/image-png-Oct-19-2020-12-22-47-14-PM.png?width=177&amp;name=image-png-Oct-19-2020-12-22-47-14-PM.png 177w, https://www.qt.io/hs-fs/hubfs/image-png-Oct-19-2020-12-22-47-14-PM.png?width=354&amp;name=image-png-Oct-19-2020-12-22-47-14-PM.png 354w, https://www.qt.io/hs-fs/hubfs/image-png-Oct-19-2020-12-22-47-14-PM.png?width=531&amp;name=image-png-Oct-19-2020-12-22-47-14-PM.png 531w, https://www.qt.io/hs-fs/hubfs/image-png-Oct-19-2020-12-22-47-14-PM.png?width=708&amp;name=image-png-Oct-19-2020-12-22-47-14-PM.png 708w, https://www.qt.io/hs-fs/hubfs/image-png-Oct-19-2020-12-22-47-14-PM.png?width=885&amp;name=image-png-Oct-19-2020-12-22-47-14-PM.png 885w, https://www.qt.io/hs-fs/hubfs/image-png-Oct-19-2020-12-22-47-14-PM.png?width=1062&amp;name=image-png-Oct-19-2020-12-22-47-14-PM.png 1062w" sizes="(max-width: 354px) 100vw, 354px"></p>
<p>Instead, the vertex and fragment shaders now live as <a href="https://github.com/alpqr/qt5-cinematic-experience/tree/master/shaders" rel="noopener">ordinary files in the source tree</a>, not bundled with the application executable anymore.</p>
<p><a href="https://github.com/alpqr/qt5-cinematic-experience/tree/master/shaders" rel="noopener"><img src="https://www.qt.io/hs-fs/hubfs/image-png-Oct-19-2020-12-24-06-07-PM.png?width=300&amp;name=image-png-Oct-19-2020-12-24-06-07-PM.png" width="300" srcset="https://www.qt.io/hs-fs/hubfs/image-png-Oct-19-2020-12-24-06-07-PM.png?width=150&amp;name=image-png-Oct-19-2020-12-24-06-07-PM.png 150w, https://www.qt.io/hs-fs/hubfs/image-png-Oct-19-2020-12-24-06-07-PM.png?width=300&amp;name=image-png-Oct-19-2020-12-24-06-07-PM.png 300w, https://www.qt.io/hs-fs/hubfs/image-png-Oct-19-2020-12-24-06-07-PM.png?width=450&amp;name=image-png-Oct-19-2020-12-24-06-07-PM.png 450w, https://www.qt.io/hs-fs/hubfs/image-png-Oct-19-2020-12-24-06-07-PM.png?width=600&amp;name=image-png-Oct-19-2020-12-24-06-07-PM.png 600w, https://www.qt.io/hs-fs/hubfs/image-png-Oct-19-2020-12-24-06-07-PM.png?width=750&amp;name=image-png-Oct-19-2020-12-24-06-07-PM.png 750w, https://www.qt.io/hs-fs/hubfs/image-png-Oct-19-2020-12-24-06-07-PM.png?width=900&amp;name=image-png-Oct-19-2020-12-24-06-07-PM.png 900w" sizes="(max-width: 300px) 100vw, 300px"></a></p>
<p>It is now up to the build system and Qt Shader Tools to compile, reflect, and translate at build time - with the added benefit of shader compilation errors becoming proper build errors instead of …</p></span></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.qt.io/blog/graphics-in-qt-6.0-qrhi-qt-quick-qt-quick-3d">https://www.qt.io/blog/graphics-in-qt-6.0-qrhi-qt-quick-qt-quick-3d</a></em></p>]]>
            </description>
            <link>https://www.qt.io/blog/graphics-in-qt-6.0-qrhi-qt-quick-qt-quick-3d</link>
            <guid isPermaLink="false">hacker-news-small-sites-24905634</guid>
            <pubDate>Tue, 27 Oct 2020 09:09:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Amiga 1000 Phoenix Project]]>
            </title>
            <description>
<![CDATA[
Score 93 | Comments 24 (<a href="https://news.ycombinator.com/item?id=24905247">thread link</a>) | @retrohax
<br/>
October 27, 2020 | https://retrohax.net/amiga-1000-project-phoenix-motherborad/ | <a href="https://web.archive.org/web/*/https://retrohax.net/amiga-1000-project-phoenix-motherborad/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-11666">

<div>
<p>… or failures are your friends :&gt;</p>
<p>&lt;INTRO&gt;</p>
<figure><img src="https://i0.wp.com/imgur.com/xzJXBgG.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/xzJXBgG.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>&lt;/INTRO&gt;</p>
<p>The story behind this whole post is a bit lengthy but I’ll try to be brief 🙂</p>
<p>In August of 2019, I’ve received an email from MrTrinsic. Back then, I didn’t yet know what is coming lol.</p>
<p>It turned out that MrTrinsic is a great Amiga enthusiast and he’d asked me to work on his Amiga 1000 … but no on a standard A1000 but with an Amiga 1000 Phoenix motherboard!</p>
<p>Amiga 1000 Phoenix Enhanced mobo is an extremely rare motherboard replacement for Amiga 1000. Some people think there were no more than 200 units manufactured, others say it was no more than 2000. I’ve no idea either but still, it is very rare so a magic smoke is not an option lol.</p>
<p>This motherboard is an awesome hack in itself and that is why MrTrinsic refers to it as DIVA 😀</p>
<p>Let me quote an excerpt from one of emails.</p>
<p><em>You should mention or point out more clearly that the Phoenix Board is a … DIVA!<br>It is a hack. Just look at the manual what kind of things you can modify and what kind of headers there are to change stuff.<br>The price is that it has an extremely bad signal quality. Plus, it lacks the Buster-Chip that the Amiga 2000 has.<br>The Phoenix is a bad version of the original A2000 from Braunschweig, which in itself was a hacked and beefed-up version of the A1000.<br>Plus, the Phoenix only has two layers. It’s a nightmare as we have seen.</em></p>
<p>It simply always has some problems like stability and compatibility issues which I’ve tried to sort out.</p>
<p>Phoenix mobo was developed in 1989/1990 by our fellow friends from Australia and was one of the very first crowd-funding campaigns! You can read/watch more on one of my fav websites -&gt; <a rel="noreferrer noopener" href="https://www.amigalove.com/viewtopic.php?t=476" target="_blank">www.amigalove.com</a></p>
<p>Hardware specs are available here -&gt;<a rel="noreferrer noopener" href="https://amiga.resource.cx/exp/phoenix" target="_blank"> amiga.resource.cx</a></p>
<h4>The plan</h4>
<p>Initially, MrTrinsic asked me to work on some external floppy replacements by Dell which I will cover in another post. Once I’ve figured out that floppy drive issue he’d decided we should start working on The Phoenix project.</p>
<p>At first, he’d send me a large box with gear that he wanted to have in this Amiga. I was like OMG! Not only Phoenix but the whole large project was about to begin!</p>
<p>The plan was to run lots of modern hardware add-ons with Amiga 1000 Phoenix and later try to squeeze it into a nice looking case, plus make it alive and stable.</p>
<p>The first package arrived and I was really excited by what I’ve seen.</p>
<p>Amiga 1000 Phoenix in an A1000 case with lots of mods and hacks already installed, plus, tons of other hardware mods still in boxes … and that was only for starters …</p>
<figure><img src="https://i0.wp.com/i.imgur.com/eTKSUuT.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/i.imgur.com/eTKSUuT.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/IIqmyaR.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/IIqmyaR.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/KYwUvJJ.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/KYwUvJJ.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i0.wp.com/imgur.com/dRdZdns.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/dRdZdns.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/650o3wz.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/650o3wz.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/2lHWTqy.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/2lHWTqy.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i0.wp.com/imgur.com/x8f24dT.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/x8f24dT.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/bw37Za0.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/bw37Za0.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/SQd4oOX.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/SQd4oOX.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>Obviously, the original plan was to MAKE Amiga 1000 Phoenix GREAT AGAIN!</p>
<p>Jokes aside, the main goal was to run a graphics card along with <a href="http://wiki.icomp.de/wiki/ACA500plus" target="_blank" rel="noreferrer noopener">ACA500plus</a> + <a href="http://wiki.icomp.de/wiki/ACA1233n" target="_blank" rel="noreferrer noopener">ACA1233n</a> accelerator card by iComp</p>
<p>On top of tons of other minor hardware mods, He’d also sent me two graphic cards – <a rel="noreferrer noopener" href="https://shop.mntmn.com/products/zz9000-for-amiga-preorder" target="_blank">ZZ9000 by MNT</a> and GBAPII++ by KryoFlux.</p>
<figure><img src="https://i1.wp.com/imgur.com/fvKi0VG.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/fvKi0VG.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/dg6QzfV.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/dg6QzfV.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<h4>Running it</h4>
<p>First things first. Phoenix motherboard is so rare that I first had to learn how it works and how it is all connected etc.</p>
<p>As it gave me a black screen at the very first run, I had to start learning about jumper settings and the board in general</p>
<p>Below, you can see a block diagram of particular parts location to give you an idea of what is where.</p>
<figure><img src="https://i0.wp.com/imgur.com/1Dz8Jbp.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/1Dz8Jbp.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>Of course, I would not move on quickly without MrTrinsics support. He’d pointed me to several sites and sent over some more info about the board itself. One of the most important documents I’ve received was <a rel="noreferrer noopener" href="https://retrohax.net/wp-content/uploads/2020/10/phoenix_jumpers_english.pdf" target="_blank">the jumper settings file</a> along with the <a rel="noreferrer noopener" href="https://retrohax.net/wp-content/uploads/2020/10/Phoenix.pdf" target="_blank">original manual</a>.</p>
<p>The above documents gave me the general idea of how things should work. The very first thing that I did was the removal of all added mods. I’ve then tried to run the A1K but still no luck – black screen. MrTrinsic then pointed me to jumper L35 which could cause such behavior if set incorrectly and bingo! It worked! </p>
<p>Since Phoenix has slots for more than one ROM chip, it is possible to install three KickStarts – 1.3. and 3.1 and third as a custom option. That was already done, along with a switch hack.</p>
<figure><img src="https://i2.wp.com/imgur.com/AtNPAf9.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/AtNPAf9.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/LIOJAzo.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/LIOJAzo.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>The problem was that Amiga wasn’t starting every single time. Instead, it booted every couple of times. My next move was to take it out and try running it outside of the case. This is where I’ve started noticing all the awesome texts on the PCB itself. I took PCB out started shooting pics of those texts and greetz for various hackers of that era.</p>
<figure><img src="https://i1.wp.com/imgur.com/SdWiBVf.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/SdWiBVf.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/5vhFvEe.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/5vhFvEe.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/JSecjDN.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/JSecjDN.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/ema2Gj3.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/ema2Gj3.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/iPLyCXN.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/iPLyCXN.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i0.wp.com/imgur.com/dvlyOz0.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/dvlyOz0.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/dNT86JL.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/dNT86JL.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/JrX8CQy.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/JrX8CQy.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i0.wp.com/imgur.com/fpooFLQ.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/fpooFLQ.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/Infqadj.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/Infqadj.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>Next, I’ve checked for any obvious problems and when I was happy with this inspection, I’ve put it back to a case to simply avoid any accidental short circuits caused by beer-drinking ;P</p>
<p>I’ve decided that I will try to run it with only Indivision ECS2, and KryoFlux GBAPII++ inserted.</p>
<p>I’ve then located the switch setting for the first ROM and put an awesome<a href="http://www.diagrom.com/" target="_blank" rel="noreferrer noopener"> DiagROM by John “<em>Chucky</em>” Hertell</a> in a socket. Yeah, I know, it is a quite a large resistor ;P</p>
<figure><img src="https://i2.wp.com/imgur.com/1Xs3Ren.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/1Xs3Ren.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>To my surprise, it worked flawlessly and I was greeted by a known diag info and a menu a bit later.</p>
<figure><img src="https://i0.wp.com/imgur.com/OLe47z8.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/OLe47z8.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i0.wp.com/imgur.com/gxM4wyI.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/gxM4wyI.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/DevqDvV.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/DevqDvV.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/zu2QPUL.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/zu2QPUL.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>Once it worked, I’ve figured that it might be as simple as a flaky ROM socket problem. I’ve put a 2.0 ROM in the place of DiagROM and Viola! It works!</p>
<figure><img src="https://i1.wp.com/imgur.com/CVzvAYd.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/CVzvAYd.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>When I’ve figured that part out, I could move on and start working on the alternative power supply which was …</p>
<h4>HDPLEX + Uber nice Amiga adapter</h4>
<p>MrTrinsic sent me this HQ HDPLEX Pico PSU but he’d also sent me a very cool DIY KIT – ATX2.0d-Amiga adapter which has super cool features like over-voltage/current protection outputs all needed voltages, and has additional floppy power outputs. Moreover, it generates a TICK signal which is good to have for testing.</p>
<p>However, it was a DIY KIT so I had to solder it all up first.</p>
<figure><img src="https://i1.wp.com/imgur.com/vjNBj6x.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/vjNBj6x.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/wWorjpy.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/wWorjpy.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/zLxusZT.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/zLxusZT.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/hQ4rfGh.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/hQ4rfGh.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/IQSyKPj.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/IQSyKPj.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/ksYAoZz.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/ksYAoZz.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/iOkMKIS.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/iOkMKIS.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/kAsBdka.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/kAsBdka.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>Combined together, it created an awesome and stable power source for this Amiga project.</p>
<figure><img src="https://i2.wp.com/imgur.com/AeB0dya.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/AeB0dya.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>All I needed to do next was to prepare all the wiring. That was a trivial job after taking some measurements. I’ve used wires from my Nissan Patrol spare wiring kit as these are thick (copper) and nice, hence the color mismatch ;P</p>
<figure><img src="https://i1.wp.com/imgur.com/XoLJW6m.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/XoLJW6m.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/X9KQOAo.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/X9KQOAo.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/wzAdwin.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/wzAdwin.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/B2j2Lvt.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/B2j2Lvt.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>It worked like a charm so now I had two working power supplies – original and superior to it HDPLEX with a kickass adapter.</p>
<h4>030 cards</h4>
<p>The next step was about adding 68030 CPU to the system. I had two options as MrTrinsic sent me two different solutions.</p>
<p>The First solution was an <a rel="noreferrer noopener" href="https://icomp.de/shop-icomp/en/produkt-details/product/ACA500plus.html" target="_blank">ACA500plus</a> card along with <a rel="noreferrer noopener" href="http://wiki.icomp.de/wiki/ACA1233n" target="_blank">an ACA1233n</a> accelerator card by Individual Computers. ACA500plus also had an Ethernet add-on – X-Surf 500</p>
<p>These two make a great solution but for AMIGA 500. There are not many folks out there who played it with it in an A1000 and especially with a Phoenix mobo!</p>
<figure><img src="https://i2.wp.com/imgur.com/bTlloDp.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/bTlloDp.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i0.wp.com/imgur.com/sbzmxTx.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/sbzmxTx.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>The first run was promising …</p>
<figure><img src="https://i2.wp.com/imgur.com/LFm6kCo.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/LFm6kCo.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/FlttUWD.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/FlttUWD.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>Then I’ve added ACA1233n on an EXTREMELY WANTED DURING PANDEMIC stand 😀 😀 😀</p>
<figure><img src="https://i0.wp.com/imgur.com/oU3w4xv.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/oU3w4xv.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/BMKsRiL.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/BMKsRiL.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/pMIcpbV.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/pMIcpbV.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>To my surprise, it worked!</p>
<figure><img src="https://i1.wp.com/imgur.com/GtUMP6a.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/GtUMP6a.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i0.wp.com/imgur.com/pWIyOlg.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/pWIyOlg.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/imgur.com/GbOnzwF.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/imgur.com/GbOnzwF.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>Below, a demo running on this setup</p>
<figure><p><span><iframe width="900" height="507" src="https://www.youtube.com/embed/PJYyBRBnhu8?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;wmode=transparent" allowfullscreen="true"></iframe></span>
</p></figure>
<p>The second setup was a bit different. It is made of four devices.</p>
<ul><li>Open 68000 relocator card</li><li>68030 accelerator card </li><li>SDRam + IDE interface</li><li>IDE2CF interface</li></ul>
<p>This setup also appeared to be working nicely after some tests, however as MrTrinsic pointed out, it has some stability issues and will not allow running some software so it was a backup card in case ACA failed. I don’t have a video of it running though.</p>
<figure><img src="https://i0.wp.com/imgur.com/ll5k4FQ.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/ll5k4FQ.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i0.wp.com/imgur.com/hNND0rK.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/hNND0rK.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/6dDVLSJ.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/6dDVLSJ.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/9m0vnry.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/9m0vnry.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<h4>Other mods and add-ons</h4>
<p>Once accel-cards were tested, I’ve started installing OS and testing other mods. To name the few:</p>
<ul><li>X-surf 500 Ethernet card</li><li>RapidRoad USB</li><li>Indivision ECS v2</li><li>SCSI2SD </li><li>KryoFlux GBAPII++</li></ul>
<figure><img src="https://i0.wp.com/imgur.com/EBLwZFR.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/EBLwZFR.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i0.wp.com/i.imgur.com/x7HIGH1.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/i.imgur.com/x7HIGH1.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/i.imgur.com/khhqFtb.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/i.imgur.com/khhqFtb.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/i.imgur.com/n0qbkbP.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/i.imgur.com/n0qbkbP.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>Indivision ECS v2 and SCSI2SD worked flawlessly so I started playing with other gear.</p>
<figure><img src="https://i1.wp.com/i.imgur.com/p2dSIad.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/i.imgur.com/p2dSIad.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>After installing all the needed software I’ve finally managed to get an IP addr from my local DHCP server</p>
<figure><img src="https://i1.wp.com/i.imgur.com/DS3Vz0m.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/i.imgur.com/DS3Vz0m.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/i.imgur.com/m0IKjsS.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/i.imgur.com/m0IKjsS.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/i.imgur.com/XGyz7F0.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/i.imgur.com/XGyz7F0.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/i.imgur.com/OMaE81e.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/i.imgur.com/OMaE81e.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>Obviously, I wouldn’t be myself if I didn’t destroy something. I’ve accidentally connected the RapidRoad USB module to a clock port the other way around. The magic smoke appeared and…</p>
<figure><img src="https://i0.wp.com/imgur.com/LHLR7WY.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/LHLR7WY.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>Of course, I had to fix it. After a while, it turned out that only 3R3 resistor was fried.</p>
<figure><img src="https://i2.wp.com/i.imgur.com/GdJIXWt.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/i.imgur.com/GdJIXWt.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>I’ve quickly replaced it and started testing USB functionality.</p>
<figure><img src="https://i2.wp.com/i.imgur.com/OXAhtvf.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/i.imgur.com/OXAhtvf.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/i.imgur.com/5tzhgLD.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/i.imgur.com/5tzhgLD.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<h4>GFX cards</h4>
<p>Once all other major mods were working more or less correctly, I could start testing GFX cards high-res modes.</p>
<p>This is where it all started to go wrong …</p>
<p>I had two cards to test with this setup – GBAPII++ by Kryoflux and ZZ9000 by MNT. </p>
<p>GBAPII++ worked nicely only with green 030 cards, but in such config, there would be no Ethernet card.</p>
<figure><img src="https://i0.wp.com/i.imgur.com/GMNmuZ5.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/i.imgur.com/GMNmuZ5.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/i.imgur.com/RaE0AYH.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/i.imgur.com/RaE0AYH.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/x5aY8Sy.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/x5aY8Sy.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/i.imgur.com/rxF5KEt.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/i.imgur.com/rxF5KEt.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>Then I’ve tried running ZZ9000 along with green 030 and ACA cards but I’ve encountered autoconfig problems.</p>
<figure><img src="https://i2.wp.com/i.imgur.com/olqS18u.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/i.imgur.com/olqS18u.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/i.imgur.com/JWDIXJ4.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/i.imgur.com/JWDIXJ4.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i0.wp.com/i.imgur.com/G5sFp80.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/i.imgur.com/G5sFp80.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i1.wp.com/imgur.com/j5tkOVP.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/imgur.com/j5tkOVP.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>Finally, I’ve focused on ACA500plus with ACA1233n and I just couldn’t make it work. </p>
<p>When ACA was inserted then GBAPII++ was completely invisible to the system.</p>
<p>After updating tons of libraries, firmware and reinstalling OS a few times without any luck, we’ve figured out that it might be a power issue. MrTrinsic ordered an adapter for A500 which would allow pumping in more power.</p>
<p>I’ve first tested it with a stock A500.</p>
<figure><img src="https://i1.wp.com/i.imgur.com/AFRHzjv.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/i.imgur.com/AFRHzjv.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<figure><img src="https://i2.wp.com/i.imgur.com/P3Luh0r.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/i.imgur.com/P3Luh0r.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<p>Same story, GBAPII++ was invisible but I’ve checked it with A1K and power injector adapter just to be sure … unfortunately no luck again.</p>
<figure><img src="https://i0.wp.com/imgur.com/1hkTi44.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/1hkTi44.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<h4>Unfinished 🙁</h4>
<p>I’ve invested weeks of time into this GFX problem research but finally, I had to give up on this project for now. It is still in an unfinished state until we will find a solution to all problems. The project is partially done but it requires more work and I hope to cover it someday in one of the future posts making Amiga 1000 Phoenix great again!</p>
<p>But worry not, this gave birth to a new project which is even more awesome.</p>
<p>Currently, it is a work-in-progress but that is a story for another blog post 🙂</p>
<figure><img src="https://i0.wp.com/imgur.com/puO0E7q.png?w=900&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/imgur.com/puO0E7q.png?w=900&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
<h4>OUTRO</h4>
<p>If any of my readers know any solution, hints, or knows where I did mistakes, then please leave a comment here or on FB and Twitter pages.</p>
<p>If you want to get retro gear or hardware modules, please visit <a href="https://retrohax.net/shop/">our shop</a> -&gt; https://retrohax.net/shop/</p>
<p>Please support our work by commenting here and on our <a href="https://www.facebook.com/Retrohax.net">Facebook</a> and <a href="https://twitter.com/RetrohaxN">Twitter</a> pages.</p>
<p>If you want to donate a dead computer then <a href="https://retrohax.net/contact/">drop me an email</a>. Extreme cases are welcome 🙂</p>

 </div>

</article></div>]]>
            </description>
            <link>https://retrohax.net/amiga-1000-project-phoenix-motherborad/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24905247</guid>
            <pubDate>Tue, 27 Oct 2020 07:52:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Telemelt: Web-Based Multiplayer Multiemulator]]>
            </title>
            <description>
<![CDATA[
Score 53 | Comments 10 (<a href="https://news.ycombinator.com/item?id=24905193">thread link</a>) | @polm23
<br/>
October 27, 2020 | https://www.andrewreitano.com/posts/telemelt/ | <a href="https://web.archive.org/web/*/https://www.andrewreitano.com/posts/telemelt/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      

<p><img src="https://cdn.glitch.com/33ee6b19-10fa-4d51-8898-d35eb670b3fe%2Ftelemelt-FINAL.png?v=1601250432872" alt="Telemelt Logo"></p>
<p><a href="https://telemelt.com/">Telemelt</a> is a web-based multi-emulator (<a href="https://www.retroarch.com/">RetroArch</a>/<a href="https://www.libretro.com/">libretro</a>) designed to recreate the experience of playing console games with a single controller in a room full of friends.</p>
<p>Gathering around a TV and collectively attempting to beat a game has been a consistent joy throughout my life, and is one of the rituals I miss the most with the ones we're separated from right now. I wanted to create a way to play single player/hotseat games with friends remotely that was free and required minimal setup. Netplay has always been unsatisfying for this, since only the host had low enough latency for games that require tight timing. I started this project in March, and have been pouring my heart into it every day possible since then.<br>
<img src="https://cdn.glitch.com/33ee6b19-10fa-4d51-8898-d35eb670b3fe%2F6dfcbe63-2f80-4bb3-a780-a48a51dc0a19.image.png?v=1602011998888" alt="collage"></p>
<p>The system skips traditional netplay to allow a single player to assume host responsibility and experience <strong>zero network latency</strong> for games with demanding response times, with the viewers processing frames as broadcasted by the player. The virtual controller/host can be instantly passed to any user in the room.</p>
<p><strong>Designed to be lowest hassle way to play emulated games with a friend or loved one, even if they are not tech savvy!</strong></p>
<ul>
<li>No port forwarding, no IP addresses thanks to auto-generated URL</li>
<li>No installation or downloads, everything works in browser</li>
<li>Mappings for common/inexpensive controllers (PS3/4, XB360/1, Switch JC/Pro etc..) on both USB/BT thanks to RetroArch bindings</li>
<li>Always free! No tracking, no logins, no paid advertisements - personal budget allocated for Azure costs to serve thousands of users for months</li>
</ul>
<p><img src="https://cdn.glitch.com/33ee6b19-10fa-4d51-8898-d35eb670b3fe%2Fgiphy.gif?v=1602018705346" alt="Become Player"></p>
<p><strong>Focus on the experience of passing <em>a single controller</em> between people</strong></p>
<ul>
<li>This allows for a unique and efficient architecture that provides a consistent experience with minimal bandwidth</li>
<li>No AV is transmitted! Only controller and serialized state data, made even smaller by <a href="https://github.com/phretaddin/schemapack">schemapack</a></li>
<li>User state error checked per frame and reconciled via <a href="http://socket.io/">socket.io</a> server if needed</li>
<li>Takes advantage of deterministic nature of certain libretro cores - like a broadcasted <a href="https://en.wikipedia.org/wiki/Tool-assisted_speedrun">TAS</a></li>
<li>Rethink the kinds of content you would play (painting, music collaboration, RPGs, game shows)</li>
<li>Everyone working towards the same goal keeps people engaged</li>
</ul>
<p><img src="https://cdn.glitch.com/33ee6b19-10fa-4d51-8898-d35eb670b3fe%2Fimage90.gif?v=1602020845682" alt=""><br>
<strong>Quick start - Open up a Discord/Zoom/Meet etc.. with friends and try some of these!</strong></p>
<ul>
<li>Minigame collections (player transfer system is fast enough to keep up with switching on every stage)</li>
<li>Practice for a week then hold live olympic games (world/winter/summer/california games etc)</li>
<li>Live sound test or NSF/VGM listening party</li>
<li>Homebrews, hacks, and live demoparties (with live reactions)</li>
<li>Golf / bowling / fishing games to chill</li>
<li>Create music collaboratively in LSDJ/NTRQ</li>
<li>TV Gameshows</li>
<li>Painting games (mouse support!)<br>
<img src="https://cdn.glitch.com/33ee6b19-10fa-4d51-8898-d35eb670b3fe%2Fgiphy%20(1).gif?v=1602020614286" alt=""></li>
<li>Grind through an rpg over a few weeks (savestates work great for this!)</li>
<li>Alternate copiloting an game no one has played before on gamefaqs</li>
<li>Hotseat tactical / turn based games</li>
<li>Teach a friend to speedrun with the ability to demonstrate and practice together</li>
</ul>
<p><strong>A platform to experiment with multi-user experiences</strong></p>
<ul>
<li><a href="https://www.andrewreitano.com/posts/nespectre/">NESpectre support</a> – crowd control of a game by manipulating memory on the fly<br>
<img src="https://cdn.glitch.com/33ee6b19-10fa-4d51-8898-d35eb670b3fe%2Fgiphy%20(2).gif?v=1602020758322" alt="smb-ns"></li>
<li>Reaction system with live sfx</li>
<li>Frame accurate split timers</li>
</ul>
<p><strong>Coming soon</strong></p>
<ul>
<li><a href="https://www.andrewreitano.com/posts/super-russian-roulette/">Super Russian Roulette</a> Online</li>
<li>Homebrew and demo content library built in to showcase personal work</li>
<li>Double Ferrari, music ROM albums</li>
<li>Live read for updating CSS elements using game RAM</li>
</ul>
<!-- ![](https://cdn.glitch.com/33ee6b19-10fa-4d51-8898-d35eb670b3fe%2Ftes0.gif?v=1601915361226) -->
<p>Telemelt is built on an Azure DevOps build and release pipeline, and takes full advantage of Azure Container Instances, Functions, Container Registry and AppInsights/Metrics to scale, handle matchmaking, and serve static assets</p>
<p><img src="https://cdn.glitch.com/33ee6b19-10fa-4d51-8898-d35eb670b3fe%2F6d15440e-8815-483b-95e6-159140492018.image.png?v=1601949797025" alt="Azure Backend"></p>
<!-- |![](https://cdn.glitch.com/33ee6b19-10fa-4d51-8898-d35eb670b3fe%2Ftes0.gif?v=1601915361226)<br>|![](https://cdn.glitch.com/33ee6b19-10fa-4d51-8898-d35eb670b3fe%2Fana-endless.gif?v=1600290723632)<br>|
|---|---| -->
<!-- ![Telemelt Screenshot](https://cdn.glitch.com/33ee6b19-10fa-4d51-8898-d35eb670b3fe%2Ftn-telemelt.png?v=1600196583685) -->


<p><a href="https://www.andrewreitano.com/">← Home</a></p>

    </div></div>]]>
            </description>
            <link>https://www.andrewreitano.com/posts/telemelt/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24905193</guid>
            <pubDate>Tue, 27 Oct 2020 07:40:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Rachel Whiteread’s House: why was this Bow landmark demolished? (2015)]]>
            </title>
            <description>
<![CDATA[
Score 50 | Comments 24 (<a href="https://news.ycombinator.com/item?id=24905103">thread link</a>) | @BerislavLopac
<br/>
October 27, 2020 | https://romanroadlondon.com/rachel-whitereads-house-bows-legacy/ | <a href="https://web.archive.org/web/*/https://romanroadlondon.com/rachel-whitereads-house-bows-legacy/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
			
<p>Every day people walk past Wennington Green, on the corner of Grove Road and Roman Road, without realising this was the spot on which stood Rachel Whiteread’s controversial inside-out concrete cast of an East End terraced house. Why was this Bow landmark demolished?</p>



<figure><img loading="lazy" width="1024" height="690" src="https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-front-credit-David-Hoffman-1024x690.jpg" alt="Rachel Whiteread's house For Sale signs" srcset="https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-front-credit-David-Hoffman-1024x690.jpg 1024w, https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-front-credit-David-Hoffman-300x202.jpg 300w, https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-front-credit-David-Hoffman-768x518.jpg 768w, https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-front-credit-David-Hoffman.jpg 1500w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Rachel Whiteread’s house For Sale © David Hoffman</figcaption></figure>



<h2>Wennington Green, Bow, London</h2>



<p>At the intersection of Roman Road and Grove Road lies Wennington Green – a patch of land in <a href="https://romanroadlondon.com/mile-end-park-history/" target="_blank" rel="noreferrer noopener">Mile End Park </a>principally sculpted by flying bombs. In fact, an English Heritage plaque on the railway bridge a little further down Grove Road commemorates the first one to strike London.</p>



<p>The blitz largely levelled the hundred or so turn-of-the-century terraces, common stock of the working class East End, on what is now the Green but up until the early ‘90s a row of dilapidated dwellings persisted. These residences were mostly derelict, abandoned and occasionally squatted, but for one, ex-docker Mr Sidney Gale of No. 193, a condemned house remained a home.</p>



<p>Indeed, in 1993, the final row was fated to be demolished as part of Tower Hamlet’s council’s plan to create a Green Corridor, unifying the broken line of parkland between the Isle of Dogs and the established lung of <a href="https://romanroadlondon.com/victoria-park-east-london-bow/">Victoria Park</a>. It was a means to cleanse the area of the legacy of prefabs that had homed the dislocated population post-war and a redemptive space for those in the high-rise accommodation which had replaced the traditional terraces. ‘What people who live in tower blocks want is parkland,’ declared Councillor Eric Flounders.</p>



<p>Yet psychogeographer Iain Sinclair was sceptical of the council’s motives, calling it ‘an Arcadia for the underclass… the whole scheme was a disinterested attempt at municipal aesthetics’. Pointedly, the area was in direct sight of Mrs Thatcher’s commerce baby, Canary Wharf, and the call for a Green was not a far cry from the original motivations to create Victoria Park, opened in 1845. Vicky Park is the oldest purpose built recreation ground in London, conceived to curb disease contracted in damp and cramped conditions and as a means harness working-class wildness. Indeed, Bow is an area rich with connotations of a strong working-class community, but also radicalism and revolt.</p>



<p>Mr Gale forcefully resisted eviction from No. 193 by the council for a number of years, even festooning the property with banners to affirm his unrelenting presence, but, realising his campaign was ultimately futile, he eventually yielded and was re-homed nearby. Gale’s loss was artist Rachel Whiteread’s gain. The practitioner had been seeking a condemned property in London for over two years to realise a project which was essentially a development upon her Turner Prize nominated sculpture, Ghost (1990); a room-sized cast of a bedsit contained in a Victorian property in Archway.</p>



<p>Whiteread, supported by art commissioners Artangel, approached the London Borough of Tower Hamlets&nbsp;council about utilising the property and the authorities duly consented. In the beginning, the council were of the opinion that, ‘It won’t cost the neighbourhood a penny and will provide an unusual landmark for the area,’ however, by the time the bricks were removed and the sculpture was exposed, Councillor Flounders, Chair of Bow Parks Board, denounced it as ‘excrescent.’</p>



<h2>Building Rachel Whiteread’s House</h2>



<figure><img loading="lazy" width="1024" height="692" src="https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-house-credit-David-Hoffman-1024x692.jpg" alt="Rachel Whiteread's House on Grove Road in Bow, photo by David Hoffman" srcset="https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-house-credit-David-Hoffman-1024x692.jpg 1024w, https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-house-credit-David-Hoffman-300x203.jpg 300w, https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-house-credit-David-Hoffman-768x519.jpg 768w, https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-house-credit-David-Hoffman.jpg 1500w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Rachel Whiteread’s house view of Grove Road © David Hoffman</figcaption></figure>



<p>To make House, Whiteread used the physical house as&nbsp;a&nbsp;mould, making a cast from the interior by spraying a skin of liquid concrete around a metal armature constructed to support the weight of the work. Coating the whole house took over a month and an additional ten days were needed for the concrete to cure and set. Once solid, scaffolding was erected and Whiteread and her assistants began to remove the exterior brick structure.</p>



<p>What was revealed was an uncanny sight – the concrete impressed with the idiosyncrasies of over a century of domestic habitation. Depressions translated into protrusions; the industrial material betraying past human intimacies: soot marking the fire; yellow paint from a top-floor bedroom. The floors in-between stories could not be cast so, as local Markham Hall recalls, it resembled a ‘wedding cake.’ But the marriage at hand, between the art world and the East End, was to be short-lived and volatile.</p>



<h2>Reactions to Rachel Whiteread’s House</h2>



<p>It was essentially a neutral process, with no specific moral agenda, but Whiteread conceded ‘I knew of course, while I was making House, it had a political dimension. You can’t make a cast of a house in a poor area of London and not be political.’ Yet it became ‘far more political than I could have predicted.’ Seemingly, the council had also underestimated its resonance, as it quickly became front-page news, attracting scores of art-pilgrims and causing traffic chaos. Its status was even brought to debate in the House of Commons. The council couldn’t wait to get rid of the work fast enough, coming to regard it as a politically embarrassing monument to an impoverished history and standing in the way of the construction of a less threatening green space.</p>



<figure><img loading="lazy" width="1024" height="690" src="https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-house-graffiti-%C2%A9David-Hoffman-1024x690.jpg" alt="Rachel Whiteread's house with Wot For Why Not graffiti © David Hoffman" srcset="https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-house-graffiti-%C2%A9David-Hoffman-1024x690.jpg 1024w, https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-house-graffiti-%C2%A9David-Hoffman-300x202.jpg 300w, https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-house-graffiti-%C2%A9David-Hoffman-768x518.jpg 768w, https://romanroadlondon.com/wp-content/uploads/2019/09/Rachel-Whiteread-house-graffiti-%C2%A9David-Hoffman.jpg 1500w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Wot For? Why Not? © David Hoffman</figcaption></figure>



<p>People reacted to House so strongly as it transcended the individual and became an archetype; an emblem for the area’s time-honoured mode of living. Indeed, it raised pressing questions regarding degrading housing stock and what should be done with it, the creeping gentrification of a historically tight working class community and scepticism towards the authority of those instigating change for the supposed ‘greater good’. In whose name was this change really for? And poignantly, it confronted these questions on its own material terms – concrete being the material used to fix the original Victorian House’s bricks; the embossed surface betraying the umbilical cords of pipes and power-lines which linked the individual house with the local, national and global public.</p>



<p>At base, Whiteread made an empty space, the negative void of the vacated property, into a positive object but insists ‘the work is to do with absence not presence.’ Implicitly, House was defined by the object it was not. When one refers to a ‘house’ we generally mean the façade as publicly viewed from the street, the interior is intimate – that is the home. Whiteread’s sculpture was resolutely an interior; an unsettling mass of inside, out.</p>



<p>The form elicited unease as when we have an intimate relationship with a space, we start to ignore its intricacies, instead handling navigating through the general form unconsciously. Thus, the sculpture was uncanny as it solidified the overlooked, demanding a long-looking, deep contemplation, as one attempted to decipher the inverted forms. Whiteread neglected to furnish her House with meaning but it was predominantly envisioned as what the populace of Bow would soon lose sight of. The interior, a family home, was left out in the cold and significantly, Whiteread chose not to cast the attic space, as if riffing on the idiom ‘left with no roof over their heads.’</p>



<p>It’s understandable then, that there was some resentment from locals towards House as they perceived it to be adding insult to injury over the demolition of such homes in the area, and a crassness in exposing working-class abode for the ‘arty’ leisure classes. Yet, this wasn’t just a case of Art World vs. East Enders, the opinions were split within both camps: in the Art World, Andrew Graham-Dixon proclaimed it ‘a strange and fantastical object which also amounts to one of the most extraordinary and imaginative public sculptures created by an English artist this century.’ To Brian&nbsp;Sewell, it was a ‘meritless gigantism.’ Sewell’s scorn found resonance in one local’s assertion that ‘an engineer could have done it; I don’t see it as creative.’ Whereas, another neighbour to the site regarded it as ‘brilliant,’ reckoning it to be ‘a new way of looking at traditional things.’</p>



<p>House’s evicted resident, Mr Gale, protested, ‘They’re taking the wee-wee.’ He questioned, ‘How can they get grants for arts projects when we can’t get grants for homes? I could have bought a new home for my family with this money.’ His sentiment was echoed in graffiti scrawled on the sculpture: ‘WOT FOR?’ Another renegade scribe rebuffing ‘Why not!’</p>



<p>House’s economics became even more contentious when Whiteread won the £20,000 Turner Prize for the work (the first woman to receive the honour), and then £40,000 from the rebel K Foundation (composed of members of the defunct pop group KLF) for the ‘worst artist of the year.’ Whiteread split the latter money between Shelter, a charity for the London’s homeless, and a fund to supplement young artists. On the same day, the&nbsp;Council made the decision to refuse House a stay of execution. By this time, many people (philanthropists, dealers, galleries) had offered to purchase House, but money offered to the Council to retain its presence was blasted by the authorities as ‘bribes.’ In any case, Whiteread was adamant that the sculpture was ‘absolutely specific to the site’ thus preferred its destruction to relocation.</p>



<p>The bulldozers came on January 11th 1994. The <em>East London Advertiser</em> reports that ‘art lovers’ chained themselves to the railings in attempt to save the work, quoting one as protesting, ‘We’re doing this because House represents the destruction of not only homes but whole communities in East London.’ Another suggested its removal was a sacrilegious act, perceiving House as ‘a headstone to the houses that were here.’ But the sculpture’s fate was sealed. As Whiteread succinctly mused ‘it took three and a half years to develop, four months to make, and thirty minutes to demolish.’</p>



<h2>The legacy of Rachel Whiteread’s House</h2>



<p>The Houseless Park has now had over 20 years to bed in to the community’s psyche and flourish naturally. In all of the articles and art history books I have trawled to substantiate …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://romanroadlondon.com/rachel-whitereads-house-bows-legacy/">https://romanroadlondon.com/rachel-whitereads-house-bows-legacy/</a></em></p>]]>
            </description>
            <link>https://romanroadlondon.com/rachel-whitereads-house-bows-legacy/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24905103</guid>
            <pubDate>Tue, 27 Oct 2020 07:21:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[MIOS: IBM 5150 BIOS Replacement Project]]>
            </title>
            <description>
<![CDATA[
Score 30 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24904148">thread link</a>) | @userbinator
<br/>
October 26, 2020 | http://www.mtmscientific.com/mios.html | <a href="https://web.archive.org/web/*/http://www.mtmscientific.com/mios.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://www.mtmscientific.com/mios.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24904148</guid>
            <pubDate>Tue, 27 Oct 2020 03:09:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An App to Measure Your Coffee Grind Size Distribution]]>
            </title>
            <description>
<![CDATA[
Score 19 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24903834">thread link</a>) | @occupy_paul_st
<br/>
October 26, 2020 | https://coffeeadastra.com/2019/04/07/an-app-to-measure-your-coffee-grind-size-distribution-2/ | <a href="https://web.archive.org/web/*/https://coffeeadastra.com/2019/04/07/an-app-to-measure-your-coffee-grind-size-distribution-2/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

	<section id="primary">
		<main id="main">

			
<article id="post-1314">

	

	
			<figure>
				<img width="1568" height="1068" src="https://coffeeadastra.files.wordpress.com/2019/01/screenshot_coffee_grind_size.png?w=1568" alt="" loading="lazy" srcset="https://coffeeadastra.files.wordpress.com/2019/01/screenshot_coffee_grind_size.png?w=1568 1568w, https://coffeeadastra.files.wordpress.com/2019/01/screenshot_coffee_grind_size.png?w=150 150w, https://coffeeadastra.files.wordpress.com/2019/01/screenshot_coffee_grind_size.png?w=300 300w, https://coffeeadastra.files.wordpress.com/2019/01/screenshot_coffee_grind_size.png?w=768 768w, https://coffeeadastra.files.wordpress.com/2019/01/screenshot_coffee_grind_size.png?w=1024 1024w, https://coffeeadastra.files.wordpress.com/2019/01/screenshot_coffee_grind_size.png 2836w" sizes="(max-width: 1568px) 100vw, 1568px" data-attachment-id="977" data-permalink="https://coffeeadastra.com/screenshot_coffee_grind_size/" data-orig-file="https://coffeeadastra.files.wordpress.com/2019/01/screenshot_coffee_grind_size.png" data-orig-size="2836,1932" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot_coffee_grind_size" data-image-description="" data-medium-file="https://coffeeadastra.files.wordpress.com/2019/01/screenshot_coffee_grind_size.png?w=300" data-large-file="https://coffeeadastra.files.wordpress.com/2019/01/screenshot_coffee_grind_size.png?w=750">			</figure><!-- .post-thumbnail -->

		
	<div>
		
<p>[Edit April 25 2019: Please note <strong>this is not an iPhone or Android app</strong>, and I have no plans to release it as such. You can use your phone or any other camera to take pictures of your ground coffee, but then you need to install the application on either OS X or through Python (on any operating system) to analyze the data. <a href="https://github.com/jgagneastro/coffeegrindsize/archive/master.zip">Download the application package here</a>.]</p>



<p>Today I would like to present an OS X application I have been developing for a few months. It turns out writing Python software for coffee is a great way to relax after a day of writing Python software for astrophysics.</p>



<p>When I started being interested in brewing specialty coffee a few years ago, one of the first things that irritated me was our inability to recommend grind sizes for different coffee brewing methods, or to compare the quality of different grinders in an objective way. Sure, some laboratories have laser diffraction equipment that can measure the size of all particles coming out of a grinder, but rare are the coffee geeks that have access to these multi-hundred thousands of dollars kinds of equipment.</p>



<p>At first, I decided to take pictures of my coffee grounds spread on a white sheet, and to use an old piece of software called <a href="https://imagej.nih.gov/ij/">ImageJ</a>, developed by the National Institutes of Health mainly to analyze microscope images, to obtain a distribution of the sizes of my coffee grounds. This worked decently well, and allowed me to start comparing different grinders. Then <a href="https://www.scottrao.com/">Scott Rao</a> made me realize that a stand-alone application that doesn’t need a complicated installation and that is dedicated to coffee would be of interest to many people in the coffee industry. Probably just the 10% geekiest of them, but that’s cool.</p>



<p>I’m hoping that this application will help us understand the effects of particle size distributions on the taste of coffee. I don’t think the industry really kept us in the loop with all the laser diffraction experiments, so hopefully we can help ourselves as a community.</p>



<p>If you are interested in measuring the particle size distribution of your grinder, then this app is for you ‒ and it’s free. I placed it as “open source” on <a href="https://github.com/jgagneastro/coffeegrindsize">GitHub</a>, so if you are a developer, you are welcome to send me suggestions in the form of <em>push requests</em> (the developers will know what that means).</p>



<p>If you would like to get started, I suggest you read this quick <a href="https://github.com/jgagneastro/coffeegrindsize/blob/master/Help/coffee_grind_size_installation.pdf">installation guide</a>, which will explain how to download the app and run it even though I am not a registered Apple Developer. Then, you can choose to either read this <a href="https://github.com/jgagneastro/coffeegrindsize/blob/master/Help/coffee_grind_size_summarized_manual.pdf">quick summary</a> that will get you running with the basics, or this <strong><em>very</em></strong> detailed and wordy <a href="https://github.com/jgagneastro/coffeegrindsize/blob/master/Help/coffee_grind_size_manual.pdf">user manual</a> that will guide you through all the detailed options the application offers you.</p>



<p>I would like to show you an example of what can be done with the software. Below, I am comparing the particle size distribution of the Baratza Forté grinder, which uses 54 mm flat steel burrs, with that of the Lido 3 hand grinder, which uses 48 mm conical steel burrs. I set both grinders in a way that produces a similar peak of average-sized particles with diameters around 1 mm, but as you can see, the particle size distributions are very different ! The Forté generates way less fines (with diameters below 0.5 mm) and slightly less boulders (with diameters of approximately 2 mm), which is indicative of a better quality grinder.</p>



<figure><img data-attachment-id="976" data-permalink="https://coffeeadastra.com/lido3_size9_file10_hist_mass_diam/" data-orig-file="https://coffeeadastra.files.wordpress.com/2019/01/lido3_size9_file10_hist_mass_diam.png" data-orig-size="1000,780" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lido3_size9_file10_hist_mass_diam" data-image-description="" data-medium-file="https://coffeeadastra.files.wordpress.com/2019/01/lido3_size9_file10_hist_mass_diam.png?w=300" data-large-file="https://coffeeadastra.files.wordpress.com/2019/01/lido3_size9_file10_hist_mass_diam.png?w=750" src="https://coffeeadastra.files.wordpress.com/2019/01/lido3_size9_file10_hist_mass_diam.png" alt=""><figcaption>An example of figure that can be generated with the coffee grind size software. Each red bar corresponds to one kind of particle diameter generated by the Lido 3 grinder (smallest particles correspond to the leftmost bar, largest ones correspond to the rightmost bar), and their heights correspond to the total contribution of these kinds of particles, by mass. As you can see, the particles that contribute to the largest amount of mass have sizes just above 1 mm. The red circle shows the average particle diameter for the Lido 3, and the horizontal bars show the “characteristic width” of the distribution – this corresponds to the width that covers approximately 68% of all the distribution. Similarly, the blue line and the blue circle describe the distribution of the Forté grinder.</figcaption></figure>



<p>For now, the app is only intended to be used on OS X computers. But if you are running any other kind of system and know your way around Python, you can always download it directly from <a href="https://github.com/jgagneastro/coffeegrindsize">GitHub</a> and run it with your own installation of Python 3.</p>



<figure><video controls="" src="https://www.dropbox.com/s/66ug5i9bkhrghef/screencap_coffeegrindsize.mov?raw=1"></video><figcaption>This is an example of how to use the coffee grind size application.</figcaption></figure>



<p>I would like to thank Scott Rao for his excitement when I shared this project idea with him, and for beta testing the software. I would also like to thank Alex Levitt, Mitch Hale, Caleb Fischer, Francisco Quijano and Victor Malherbe for beta testing the software.</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

			<div>
	
	<p>
		I’m a researcher in astrophysics at the Rio Tinto Alcan planetarium of Espace pour la Vie, in Montreal.		<a href="https://coffeeadastra.com/author/jgagneastro/" rel="author">
			View more posts		</a>
	</p><!-- .author-description -->
</div><!-- .author-bio -->
	
</article><!-- #post-${ID} -->

	<nav role="navigation" aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
<!-- #comments -->

		</main><!-- #main -->
	</section><!-- #primary -->


	</div></div>]]>
            </description>
            <link>https://coffeeadastra.com/2019/04/07/an-app-to-measure-your-coffee-grind-size-distribution-2/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24903834</guid>
            <pubDate>Tue, 27 Oct 2020 02:02:10 GMT</pubDate>
        </item>
    </channel>
</rss>
