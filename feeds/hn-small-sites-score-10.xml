<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 10]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 10. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Mon, 03 Aug 2020 08:19:41 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-10.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Mon, 03 Aug 2020 08:19:41 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Gimp is working on its own version of “smart objects”]]>
            </title>
            <description>
<![CDATA[
Score 19 | Comments 24 (<a href="https://news.ycombinator.com/item?id=24018269">thread link</a>) | @pmoriarty
<br/>
July 31, 2020 | https://daviesmediadesign.com/gimp-is-quietly-working-on-its-own-version-of-smart-objects-and-its-just-as-good-as-photoshops/ | <a href="https://web.archive.org/web/*/https://daviesmediadesign.com/gimp-is-quietly-working-on-its-own-version-of-smart-objects-and-its-just-as-good-as-photoshops/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
					
<p>The GIMP team recently informed me that the next upcoming version of GIMP will be GIMP 2.10.22 (no GIMP 3.0 just yet – womp). Though there has been no official announcements of specific features to come in this next version, there has been one feature announcement in particular that has caught my eye – the “<strong>linked layers</strong>” feature.</p>



<h2>Linked Layers – GIMP’s Answer to Photoshop’s Smart Objects?</h2>



<p><a href="https://twitter.com/zemarmot/status/1286952087558004738?s=20" target="_blank" rel="noreferrer noopener">GIMP quietly announced (via a retweet)</a> that the <a href="https://patreon.com/zemarmot?utm_medium=social&amp;utm_source=twitter&amp;utm_campaign=creatorshare" target="_blank" rel="noreferrer noopener">ZeMarmot team</a> was working on a new feature they are calling the “Linked Layers” feature (note: this has nothing to do with the “transform link” feature already found in GIMP).</p>



<p>In fact, work for this feature actually began in July 2019 (about a year ago) but, due to limited developers working on GIMP, this feature was shelved until recently.&nbsp;</p>



<p>Jehan from ZeMarmot, who recently revisited development on this feature, described the application of Linked Layers in this way: “What if you wanted to use the same background in several images? Instead of duplicating it, you could just link it from all images using it. And if you edit a bit your common background, it would automatically update the render of all images.”</p>



<p>Sound familiar?&nbsp;</p>



<p>To me, this sounds like GIMP’s version of “Smart Objects” – the popular Photoshop feature that allows you to “link” a different file to your composition, then have that file live update in your current composition any time you make changes to it. This has many applications – especially in the world of creating universal templates that can be updated with your own design (by simply replacing the “linked image” with your own design, then re-saving that image). <strong>This is huge.&nbsp;</strong></p>



<figure><img data-attachment-id="13511" data-permalink="https://daviesmediadesign.com/gimp-is-quietly-working-on-its-own-version-of-smart-objects-and-its-just-as-good-as-photoshops/linked-layers-test-gimp-smart-objects-2/" data-orig-file="https://i0.wp.com/daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects-2.jpg?fit=820%2C511&amp;ssl=1" data-orig-size="820,511" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Linked-Layers-Test-GIMP-Smart-Objects-2" data-image-description="" data-medium-file="https://i0.wp.com/daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects-2.jpg?fit=300%2C187&amp;ssl=1" data-large-file="https://i0.wp.com/daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects-2.jpg?fit=820%2C511&amp;ssl=1" src="https://daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects-2.jpg" alt="" srcset="https://daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects-2.jpg 820w, https://daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects-2-480x299.jpg 480w" sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 820px, 100vw" data-recalc-dims="1" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects-2.jpg" data-srcset="https://daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects-2.jpg 820w, https://daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects-2-480x299.jpg 480w"><figcaption><em>How linked layers would work (image from <a href="https://youtu.be/N5oyqbD7zyQ">ZeMarmot test video</a>)</em></figcaption></figure>



<p>It would work like this in GIMP: you would create a new layer, and under “Fill With” you choose “Image Link” (red arrow in the image above). Choosing this option would bring up an option to select an image to link the layer to on your computer (blue arrow). You then select your image, and click “OK” to create your new layer. The layer is now linked to the image on your computer. If you were to then open that image in GIMP and make edits to it, the edits would automatically update in the linked layer inside your other composition.</p>



<h2>Why GIMP’s Linked Layers Will Be Just As Good As Photoshop’s Smart Objects</h2>



<figure><img data-attachment-id="13510" data-permalink="https://daviesmediadesign.com/gimp-is-quietly-working-on-its-own-version-of-smart-objects-and-its-just-as-good-as-photoshops/linked-layers-test-gimp-smart-objects/" data-orig-file="https://i2.wp.com/daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects.jpg?fit=850%2C346&amp;ssl=1" data-orig-size="850,346" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Linked-Layers-Test-GIMP-Smart-Objects" data-image-description="" data-medium-file="https://i2.wp.com/daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects.jpg?fit=300%2C122&amp;ssl=1" data-large-file="https://i2.wp.com/daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects.jpg?fit=850%2C346&amp;ssl=1" src="https://daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects.jpg" alt="" srcset="https://daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects.jpg 850w, https://daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects-480x195.jpg 480w" sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 850px, 100vw" data-recalc-dims="1" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects.jpg" data-srcset="https://daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects.jpg 850w, https://daviesmediadesign.com/wp-content/uploads/2020/07/Linked-Layers-Test-GIMP-Smart-Objects-480x195.jpg 480w"><figcaption><em>In this photo, which is from <a href="https://youtu.be/N5oyqbD7zyQ">ZeMarmot’s video demo-ing the linked layers feature</a>, you can see a new layer thumbnail/icon (red arrow) indicating that the layer is linked to another image.</em></figcaption></figure>



<p>Some of you are wondering how this feature would keep pace with Photoshop’s well-established “Smart Objects” feature. In Photoshop, not only can you link a layer from one composition to a layer or image from another composition, but you can also link vector objects from Adobe Illustrator as a layer in Photoshop, and edit that vector object essentially in real-time in either program and have the quality of the vector object remain intact.</p>



<p>Well, GIMP’s “linked layers” will do the exact same thing – using Open Source alternatives, of course. This feature will be integrated with Inkscape so that, for example, you can link a vector composition to a GIMP layer, and have the GIMP layer scale up without quality loss the way any vector object would in Inskcape. <strong>This is mind-blowing.</strong> You could also make updates to the vector file in Inkscape, and those changes would then refresh inside of the linked layer in GIMP. It would essentially create a dynamic link between the two programs.</p>



<p>Here is a <a href="https://youtu.be/N5oyqbD7zyQ">video ZeMarmot released showing this new “linked layers” feature in action.</a> This isn’t just a pipe-dream or concept yet to be implemented – it’s a feature they’ve demonstrated to work in a test-version of GIMP. </p>



<p>If GIMP can successfully get linked layers to work in a stable release version of GIMP, the internet, myself included, will lose its mind. And, once again, GIMP will have successfully taken a bite out of Adobe’s photo editing software lead (still without charging its users a dime).</p>



<p>In my opinion, some form of “smart object” functionality is one of the top 3 features missing in GIMP (and one of the top 3 things I get comments about on my videos – especially when doing PS vs. GIMP comparisons). Stay tuned – I will provide more information on this feature as it comes out (hopefully via a video tutorial if they get the feature implemented in one of the new release-versions of GIMP).</p>



<p>If you want to help speed up the development of this and other cool new features in GIMP, I recommend supporting the <a href="https://patreon.com/zemarmot?utm_medium=social&amp;utm_source=twitter&amp;utm_campaign=creatorshare">ZeMarmot team on Patreon</a> so they can spend more time on GIMP development and less time on other jobs to pay their bills.</p>




</div></div>]]>
            </description>
            <link>https://daviesmediadesign.com/gimp-is-quietly-working-on-its-own-version-of-smart-objects-and-its-just-as-good-as-photoshops/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24018269</guid>
            <pubDate>Sat, 01 Aug 2020 06:49:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What I learned from doing over 60 technical interviews in 30 days]]>
            </title>
            <description>
<![CDATA[
Score 181 | Comments 111 (<a href="https://news.ycombinator.com/item?id=24017555">thread link</a>) | @bolajiayodeji
<br/>
July 31, 2020 | https://meekg33k.dev/what-i-learned-from-doing-60-technical-interviews-in-30-days-ckda9sn7s00iftss13b0wd0ky | <a href="https://web.archive.org/web/*/https://meekg33k.dev/what-i-learned-from-doing-60-technical-interviews-in-30-days-ckda9sn7s00iftss13b0wd0ky">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text"><p>In this article, I’ll share my motivation for doing 60+ technical interviews in 30 days. More importantly, I’ll share 13 lessons I learned from my failures and my successes.</p>
<p>I’ve grouped the lessons into three categories to match the phases of a typical recruitment process.</p>
<p>While most of the lessons apply directly to software engineers and technical professionals, the principles behind these lessons can be applied to all careers. I hope you find something useful that you can apply to your professional lives.</p>
<h2 id="how-did-i-get-started-">How did I get started?</h2>
<blockquote>
<p> “If you’re going to fail, do it fast.” — Unknown</p>
</blockquote>
<p>Like any other software engineer, I’ve had different types of technical interviews - from the dreaded whiteboard coding interview to the unreal 45-minute coding challenge on platforms like HackerRank. While some of my experiences in these interviews were great, others were bad. Really bad.</p>
<p>But I wanted to get really good at interviewing. I wanted to learn to overcome the interviewing phobia and exude confidence at interviews. Like a skilled surfer, I wanted to learn to ride the high pressure waves that came with interviews. I was also looking to change jobs at the time.</p>
<p>So from January through early March 2020, I applied to and was contacted by companies based in the US and Europe. From early-stage startups like Coda to later stage startups like Crunchbase, from mid-size companies like Affirm, to bigger companies like Amazon and even remote companies like Webflow.</p>
<p>109+ applications later, I landed myself more than 60 interviews. These comprised more than 60 introductory phone interviews, 50+ technical phone screen interviews, 18 take-home coding projects, 11 coding challenges and 8 on-site interviews including 3 virtual ones.</p>
<h2 id="what-did-i-learn-">What did I learn?</h2>
<p>For better appreciation, I have grouped the lessons into three categories to match the different phases of a typical recruitment process.</p>
<h3 id="pre-interview-phase">Pre-Interview Phase</h3>
<p>This covers everything from the initial contact with a company to the point where the first interview happens.</p>
<h4 id="1-what-i-learned-about-applications">1. What I learned about applications</h4>
<p>When I started applying to companies, I imagined that the more applications I submitted, the higher my chances of getting an interview would be. Seems logical, huh? So I set a target of 5 applications a day, aiming for 1 interview for every 5 applications.</p>
<p>But my strategy didn’t work as I hoped it would. The number of interview requests I got often fell short of my target. It was almost a 1:12 ratio - 1 interview for every 12 applications.</p>
<p>I was faced with the question: do I need to increase my daily target to, say, 10 companies? Or was there something else I needed to change?</p>
<p>With every unsuccessful application, I saw that something needed to change.</p>
<p>That change came when I took a break from meeting my daily numbers and began to think of my applications differently. I began to see each application as a sales pitch to the hiring manager or whoever was going to be reading my application, but here the product being sold was me.</p>
<p>If a company needed to fill a talent gap and I say I had the skills, I needed to find a way to convince them that I did.</p>
<p>My new task then became to find a way to effectively pitch my unique skills, experience and personality in a way that convinced the hiring manager that I was the right fit for the job.</p>
<p>Here is an example of one of such <em>pitches</em> I came up with:</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1596030331456/A8nDIn-ih.png?auto=format&amp;q=60" alt="body image.png"></p>
<p>Backed with my resume, this cover letter had a 95% success rate. The one time this didn’t work, the hiring manager still replied to let me know that the position was no longer available but he would like to connect in the future.</p>
<p>The lesson here is, be very intentional about the application you put forward – quality over quantity. Better still do both. Know your unique competencies and experience and present them in a way that matches the company’s needs without sacrificing your personality.</p>
<p>It is also important to understand the peculiarity of the company you are applying to and its specific needs. A startup or a smaller-sized company may have different needs from a bigger company, thus requiring a different skill-set.</p>
<p>Sell yourself and be sure to back your sales pitch during the interview.</p>
<h4 id="2-what-i-learned-about-recruiter-in-mails">2. What I learned about recruiter in-mails</h4>
<p>During this period, I received a number of in-mails from recruiters (mostly unsolicited) for open roles, the majority of which were roles I wasn’t interested in.</p>
<p>Granted, it was sometimes a lot given my busy schedule but I learned to be empathetic, understanding that these recruiters were only trying to do their jobs.</p>
<p>I stopped seeing these in-mails as noise in my inbox and started making the effort to reply to all recruiter in-mails, even for positions I was not interested in. By doing this, I succeeded in building a network of recruiters that have become a rich resource if I have to switch roles in the future.</p>
<p>Now I don’t expect you may want to start replying to every in-mail you receive. But it might interest you to know that some of the interview requests I got were from recruiters I had replied to before for roles I wasn’t interested in. It never hurts to reply.</p>
<h3 id="the-interview-phase">The Interview Phase</h3>
<p>This covers everything about the interview itself, cutting across the different interview types.</p>
<h4 id="3-how-to-handle-introductory-phone-calls">3. How to handle introductory phone calls</h4>
<p>Yes I get it, you’re busy and many things are competing for your time. But hey, you are also an excellent professional, and that means you never get on a phone call without knowing at least these two things:</p>
<ul>
<li>the first name of your interviewer, and</li>
<li>at least one tangible thing about the company — what they do, where they are located, any recent news, something, anything!</li>
</ul>
<p>I noticed that for interviews where I put in the effort to make these findings, I always came across as being genuinely interested in the company. That’s something recruiters typically look for in these kinds of interviews.</p>
<h4 id="4-how-to-handle-technical-phone-screens">4. How to handle technical phone screens</h4>
<p>The one thing that can almost single-handedly decide how well you do in a technical phone screen interview is your ability to communicate your thoughts clearly.</p>
<p>You may have heard stuff like this before:
“<em>The interviewers care about your thought process. Yes they can see your code but importantly, they want to know why you are doing what you’re doing</em>.”</p>
<p>The interviewer isn’t there with you and so does not have the luxury of seeing other non-verbal cues like your hand gestures or nuances. All the interviewer has is your voice as a means of understanding your thought process.</p>
<p>Now you know how you should lead this conversation, the next question is how do you become good at this? Because the truth is, while expressing your thoughts may come naturally to some people, it doesn’t to others – including me.</p>
<p>So – Practice! Practice!! Practice!!!</p>
<p>Practice doing a lot of mock interviews. Doing these mock interviews with friends made me better and more confident in explaining my thought process. But more interestingly, it helped me develop a new mindset about interviews.</p>
<p>I began to see interviews as a conversation with a friend or a team member. I visualized the interviewer on the other end as one of my friends (I sometimes gave the interviewer a name in my head). So what would have been a high-pressure interview I now saw as a friendly ‘chat’ about a technical problem.</p>
<p>This new mindset, aided by the many practice interviews, helped me grow in confidence so much so that I started enjoying interviews, sorry, technical chats.
How to get started on a problem</p>
<p>Never start solving a problem without fully understanding the problem statement. You are almost never wrong if you start by asking clarifying questions. It’s also a good sign to your interviewer when you ask those questions rather than run with your assumptions.</p>
<h4 id="5-how-to-solve-the-problem">5. How to solve the problem</h4>
<p>Good candidates know how to solve a problem (e.g. a sorting problem), but the best candidates know multiple solutions to a problem and understand the trade-offs of one solution versus the other.</p>
<p>The interviews where I performed the best (Cruise comes to mind) are the ones where I didn’t just solve the algorithmic challenge – I was also able to provide alternative solutions and discuss the trade-offs.</p>
<p>Aim to provide multiple solutions to a problem, be willing to discuss the trade-offs, and be able to implement at least one of them.</p>
<p>For technical interviews, write clean code. Most interviewers care about your code quality as well as the correctness of your solution. Aim for modular code, separate reusable logic into utility functions, name variables and methods properly, and just be a boss!</p>
<h4 id="6-what-to-do-when-you-re-stuck-on-a-problem">6. What to do when you’re stuck on a problem</h4>
<p>There will be times when you’re stuck. And this could be caused by a number of reasons: you don’t have the requisite knowledge, incorrect assumptions, missing details, and so on.</p>
<p>I used to think that at such times I was being judged by how fast I could come up with a solution. So I would be quiet, thinking, not communicating with the interviewer, just thinking.</p>
<p>And this is where a lot of us get it wrong. I get it, you need some alone time to think. But sorry to burst your bubble, that alone time is not when you’re being interviewed by a person.</p>
<p>Yes, your interviewer wants to see that you can come up with a solution, but one thing you must not forget is that they also want to see that <strong>you can collaborate with other team-mates to come up with a solution</strong>. While companies want rock-stars, they also want team-players.</p>
<p>Since your interviewer is a friend, a buddy, a team member who’s on your side and means well for you (Refer to 4), talk to them while you're figuring it out.</p>
<p>Share your thought process up till the point you got stuck and do it confidently, not like some cry for help. By doing so you just may uncover the solution, as was the case during my interview with Coda.</p>
<h4 id="7-how-to-handle-coding-challenges">7. How to handle coding challenges</h4>
<p>The lessons here apply to interviews that take the form of coding challenges on platforms like Hackerrank, Codility, and so on. Typically these are timed challenges, say 45 minutes or …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://meekg33k.dev/what-i-learned-from-doing-60-technical-interviews-in-30-days-ckda9sn7s00iftss13b0wd0ky">https://meekg33k.dev/what-i-learned-from-doing-60-technical-interviews-in-30-days-ckda9sn7s00iftss13b0wd0ky</a></em></p>]]>
            </description>
            <link>https://meekg33k.dev/what-i-learned-from-doing-60-technical-interviews-in-30-days-ckda9sn7s00iftss13b0wd0ky</link>
            <guid isPermaLink="false">hacker-news-small-sites-24017555</guid>
            <pubDate>Sat, 01 Aug 2020 04:19:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Sizle.io – React Presentation Builder]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24016998">thread link</a>) | @sizleio
<br/>
July 31, 2020 | https://sizle.io/presentations/ | <a href="https://web.archive.org/web/*/https://sizle.io/presentations/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><div id="home" data-column-margin="default" data-midnight="dark" data-top-percent="6%" data-bottom-percent="7%" data-bg-mobile-hidden="true" data-using-ctc="true"><div><div data-t-w-inherits="default" data-bg-cover="" data-padding-pos="right" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-hover-bg="" data-hover-bg-opacity="1" data-animation="" data-delay="0"><div><div><div id="fws_5f27c8a4bf7da" data-midnight="" data-column-margin="default"><div><div data-t-w-inherits="default" data-bg-cover="" data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-hover-bg="" data-hover-bg-opacity="1" data-animation="" data-delay="0"><div><div><div><p><h3>Make your content shine on all devices with presentations and proposals that convert more leads</h3></p></div></div></div></div></div></div></div></div></div></div></div><div id="fws_5f27c8a4c4360" data-column-margin="default" data-midnight="dark" data-top-percent="5%" data-bottom-percent="5%"><div><div data-t-w-inherits="default" data-bg-cover="" data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-hover-bg="" data-hover-bg-opacity="1" data-animation="" data-delay="0"><div><div><div data-max-width="100%" data-max-width-mobile="default" data-border-radius="none" data-shadow="none" data-animation="none"><div><div data-hover-animation="none"><p><img data-delay="0" height="775" width="933" data-animation="none" src="https://sizle.io/wp-content/uploads/2020/04/Mobile-friendly-online-presentations-min-2.png" alt="Mobile friendly online presentations" srcset="https://sizle.io/wp-content/uploads/2020/04/Mobile-friendly-online-presentations-min-2.png 933w, https://sizle.io/wp-content/uploads/2020/04/Mobile-friendly-online-presentations-min-2-300x249.png 300w, https://sizle.io/wp-content/uploads/2020/04/Mobile-friendly-online-presentations-min-2-768x638.png 768w" sizes="(min-width: 1450px) 75vw, (min-width: 1000px) 85vw, 100vw"></p></div></div></div></div></div></div><div data-t-w-inherits="default" data-bg-cover="" data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-hover-bg="" data-hover-bg-opacity="1" data-animation="" data-delay="0"><div><div><h2>Mobile friendly</h2><div><p>Give your viewer the best experience no matter what device they’re using.</p></div><h2>Distraction-free</h2><div><p>Display your presentations in a minimal interface that makes your content shine.</p></div><h2>Viewer analytics</h2><div><p>Measure the performance of your presentations across multiple devices.</p></div></div></div></div></div></div><div id="fws_5f27c8a4cb725" data-column-margin="default" data-midnight="dark" data-top-percent="5%" data-bottom-percent="4%" data-using-ctc="true"><div><div data-t-w-inherits="default" data-bg-cover="" data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-hover-bg="" data-hover-bg-opacity="1" data-animation="" data-delay="0"><div><div><div><p>Ensure that your presentations remain commercial in confidence. Share securely with password protection.</p></div></div></div></div><div data-t-w-inherits="default" data-bg-cover="" data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-hover-bg="" data-hover-bg-opacity="1" data-animation="" data-delay="0"><div><div><div data-max-width="100%" data-max-width-mobile="default" data-border-radius="none" data-shadow="none" data-animation="none"><div><div data-hover-animation="none"><p><img data-delay="0" height="964" width="1634" data-animation="none" src="https://sizle.io/wp-content/uploads/2020/02/Sizle-Presentation-Password-Protection.svg" alt="Sizle Presentation Password Protection"></p></div></div></div></div></div></div></div></div><div id="howitworks" data-column-margin="default" data-midnight="dark" data-top-percent="3%" data-bottom-percent="3%" data-using-ctc="true"><div><div data-t-w-inherits="default" data-bg-cover="" data-padding-pos="left" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-hover-bg="" data-hover-bg-opacity="1" data-animation="" data-delay="0"><div><div><div><p><h2>Viewer insights<br> and alerts</h2></p></div><div><p>See how people engage with your<br> presentation slide-by-slide.</p></div></div></div></div></div></div><div id="fws_5f27c8a4d474b" data-column-margin="default" data-midnight="dark" data-top-percent="8%" data-bottom-percent="8%" data-using-ctc="true"><div><div data-t-w-inherits="default" data-bg-cover="" data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-hover-bg="" data-hover-bg-opacity="1" data-animation="" data-delay="0"><div><div><div><p>Create a or upload a presentation to explore a suite of dynamic features.</p></div></div></div></div></div></div></div></div></div></div>]]>
            </description>
            <link>https://sizle.io/presentations/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24016998</guid>
            <pubDate>Sat, 01 Aug 2020 03:02:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Alacritty Version 0.5.0 Release]]>
            </title>
            <description>
<![CDATA[
Score 50 | Comments 55 (<a href="https://news.ycombinator.com/item?id=24016977">thread link</a>) | @sbt567
<br/>
July 31, 2020 | https://blog.christianduerr.com/alacritty_0_5_0_announcement.html | <a href="https://web.archive.org/web/*/https://blog.christianduerr.com/alacritty_0_5_0_announcement.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    <p>
        
        <h4>July 31, 2020</h4>
    </p>

    <h2>Table of Contents</h2>

    <ul>
      <li><a href="#about">About</a></li>
      <li><a href="#added">Added</a></li>
      <li><a href="#changed">Changed</a></li>
      <li><a href="#fixed">Fixed</a></li>
      <li><a href="#removed">Removed</a></li>
      <li><a href="#additional-information">Additional Information</a></li>
    </ul>

    <h2><a name="about" href="#about">About</a></h2>

    <p>
      <a href="https://github.com/alacritty/alacritty" target="_blank">Alacritty</a>
      is a terminal emulator with a strong focus on simplicity and performance.
      With such a strong focus on performance, included features are carefully
      considered and you can always expect Alacritty to be blazingly fast. By
      making sane choices for defaults, Alacritty requires no additional setup.
      However, it does allow configuration of many aspects of the terminal.
    </p>

    <h2>
      <a name="added" href="#added">Added</a>
    </h2>
    <ul>
      <li>Default Command+N keybinding for SpawnNewInstance on macOS</li>
      <li>Vi mode for regex search, copying text, and opening links</li>
        
        
      <li><code>CopySelection</code> action which copies into selection buffer on Linux/BSD</li>
      <li>Option <code>cursor.thickness</code> to set terminal cursor thickness</li>
        <p><img src="https://blog.christianduerr.com/img/cursor_thickness.png">
        </p>
      <li>Font fallback on Windows</li>
      <li>Support for Fontconfig embolden and matrix options</li>
      <li>Opt-out compilation flag <code>winpty</code> to disable WinPTY support</li>
      <li>Scrolling during selection when mouse is at top/bottom of window</li>
        
      <li>Expanding existing selections using single, double and triple click with the right mouse button</li>
        
      <li>Support for <code>gopher</code> and <code>gemini</code> URLs</li>
      <li>Unicode 13 support</li>
      <li>Option to run command on bell which can be set in <code>bell.command</code></li>
      <li>Fallback to program specified in <code>$SHELL</code> variable on Linux/BSD if it is present</li>
    </ul>

    <h2>
      <a name="changed" href="#changed">Changed</a>
    </h2>
    <ul>
      <li>Block cursor is no longer inverted at the start/end of a selection</li>
      <li>Preserve selection on non-LMB or mouse mode clicks</li>
      <li>Wayland client side decorations are now based on config colorscheme</li>
      <li>Low resolution window decoration icon on Windows</li>
      <li>Mouse bindings for additional buttons need to be specified as a number not a string</li>
      <li>Don't hide cursor on modifier press with <code>mouse.hide_when_typing</code> enabled</li>
      <li><code>Shift + Backspace</code> now sends <code>^?</code> instead of <code>^H</code></li>
      <li>Default color scheme is now <code>Tomorrow Night</code> with the bright colors of <code>Tomorrow Night Bright</code></li>
        <p><img src="https://blog.christianduerr.com/img/new_colorscheme.png">
        </p>
      <li>Set IUTF8 termios flag for improved UTF8 input support</li>
      <li>Dragging files into terminal now adds a space after each path</li>
      <li>Default binding replacement conditions</li>
      <li>Adjusted selection clearing granularity to more accurately match content</li>
      <li>To use the cell's text color for selection with a modified background, the <code>color.selection.text</code>
      variable must now be set to <code>CellForeground</code> instead of omitting it</li>
      <li>URLs are no longer highlighted without a clearly delimited scheme</li>
      <li>Renamed config option <code>visual_bell</code> to <code>bell</code></li>
      <li>Moved config option <code>dynamic_title</code> to <code>window.dynamic_title</code></li>
    </ul>

    <h2>
      <a name="fixed" href="#fixed">Fixed</a>
    </h2>
    <ul>
      <li>Selection not cleared when switching between main and alt grid</li>
      <li>Freeze when application is invisible on Wayland</li>
      <li>Paste from some apps on Wayland</li>
      <li>Slow startup with Nvidia binary drivers on some X11 systems</li>
      <li>Display not scrolling when printing new lines while scrolled in history</li>
      <li>Regression in font rendering on macOS</li>
      <li>Scroll down escape (<code>CSI Ps T</code>) incorrectly pulling lines from history</li>
      <li>Dim escape (<code>CSI 2 m</code>) support for truecolor text</li>
      <li>Incorrectly deleted lines when increasing width with a prompt wrapped using spaces</li>
      <li>Documentation for class in <code>--help</code> missing information on setting general class</li>
      <li>Linewrap tracking when switching between primary and alternate screen buffer</li>
      <li>Preservation of the alternate screen's saved cursor when swapping to primary screen and back</li>
      <li>Reflow of cursor during resize</li>
      <li>Cursor color escape ignored when its color is set to inverted in the config</li>
      <li>Fontconfig's <code>autohint</code> and <code>hinting</code> options being ignored</li>
      <li>Ingoring of default FreeType properties</li>
      <li>Alacritty crashing at startup when the configured font does not exist</li>
      <li>Font size rounding error</li>
    </ul>

    <h2>
      <a name="removed" href="#removed">Removed</a>
    </h2>
    <ul>
      <li>Environment variable <code>RUST_LOG</code> for selecting the log level</li>
      <li>Deprecated <code>window.start_maximized</code> config field</li>
      <li>Deprecated <code>render_timer</code> config field</li>
      <li>Deprecated <code>persistent_logging</code> config field</li>
    </ul>

    <h2><a name="additional-information" href="#additional-information">Additional Information</a></h2>

    <p>
      Detailed installation instructions can be found in
      <a href="https://github.com/alacritty/alacritty#installation">
        Alacritty's GitHub README.
      </a>
    </p>

    <p>
      The full changelog including all previous versions can be found
      <a href="https://github.com/alacritty/alacritty/blob/v0.5.0/CHANGELOG.md">here</a>.
    </p>
  

</div>]]>
            </description>
            <link>https://blog.christianduerr.com/alacritty_0_5_0_announcement.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24016977</guid>
            <pubDate>Sat, 01 Aug 2020 02:59:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Designing a Physics Engine]]>
            </title>
            <description>
<![CDATA[
Score 74 | Comments 12 (<a href="https://news.ycombinator.com/item?id=24016718">thread link</a>) | @todsacerdoti
<br/>
July 31, 2020 | https://blog.winter.dev/2020/designing-a-physics-engine/ | <a href="https://web.archive.org/web/*/https://blog.winter.dev/2020/designing-a-physics-engine/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>By coincidence, right when <a href="https://www.youtube.com/c/TheChernoProject/videos" target="_blank" rel="noreferrer noopener">The Cherno</a> announced his game engine series I was just starting to get going on my own engine. I couldn’t wait to finally have a professional opinion on how to make one. With self-taught programming it’s hard to not doubt yourself constantly, wondering if you are doing things right or just think you are.</p>



<p>Recently, he has been posting videos about huge aspects of his engine like physics and entity systems, which were what I really wanted to learn about by making myself, but he ended up using libraries instead of going through the internals! I am not against using libraries, but to use them for the fun stuff? I felt like it defeated the point of making a <em>custom</em> engine series.</p>



<p>There is an argument to be made about saving time, but this was the first C++ project that I was making and the goal from the start was to go through all the major pillars of an engine: input, graphics, physics, entities, and audio. I wanted to learn how those things worked along with C++ and code design in general.</p>



<p>I bet that some other people are interested in the details of how these systems work, and I want to learn how to explain code better, so I am going to try and make some videos going over the internals of these systems. They end up being much simpler than at first glance.</p>



<p><strong>Let’s start with the physics engine…</strong></p>



<p>Physics engines are responsible for figuring out where each object in a scene is over time. Objects can collide with one another, then choose to respond in several ways. It’s a generic problem that the user can configure at several different levels. Do they want a collider? Do they want to respond to collisions? Do they want to simulate dynamics? They could want dynamics, but not gravity. It’s a problem that calls for good planning and robust design.</p>



<p>I looked at how <a href="https://github.com/bulletphysics/bullet3" target="_blank" rel="noreferrer noopener">bullet</a> and <a href="https://github.com/erincatto/box2d" target="_blank" rel="noreferrer noopener">box2d</a> went about sorting their engines and concluded that the way bullet went about it was solid. I boiled it down to just what was needed, and based my design around that. There are already some <a href="https://www.toptal.com/game/video-game-physics-part-i-an-introduction-to-rigid-body-dynamics" target="_blank" rel="noreferrer noopener">great articles</a> going over the hard math involved, so I am going to focus on the design aspect instead because I haven’t seen anyone do that, and it’s also a real headache.</p>



<p>At the current moment, this physics engine is not fully featured, but in future articles I plan to build it out further. This article will not cover rotation, multiple contact point collisions, or constrained simulation. I think it will work out for the best as it’s easy to get overwhelmed, and I want to ease into those topics. With that out of the way, let’s dive into the different parts of a physics engine.</p>



<p>The problem can be split into 2 or 3 pieces, dynamics, collision detection, and collision response. I’ll start with dynamics because it is by far the simplest.</p>



<p><strong>Dynamics</strong></p>



<p>Dynamics is all about calculating where the new positions of objects are based on their velocity and acceleration. In high school you learn about the four kinematic equations along with Newton's three laws which describe the motion of objects. We’ll only be using the first and third kinematic equations, the others are more useful for analysis of situations, not simulation. That leaves us with:</p>



<p><span data-katex-display="true">v = v_0+at</span>



<span data-katex-display="true">\Delta x = v_0t + \frac{1}{2}at^2</span></p><p>We can give ourselves more control by using Newtons 2<sup>nd</sup> law, subbing out acceleration giving us:</p>



<p><span data-katex-display="true">v = v_0+\frac{F}{m}t</span>



<span data-katex-display="true">x = x_0+vt</span></p><p>Each object needs to store these three properties: velocity, mass, and net force. Here we find the first decision we can make towards the design, net force could either be a list or a single vector. In school you make force diagrams and sum up the forces, implying that we should store a list. This would make it so you could set a force, but you would need to remove it later which could get annoying for the user. If we think about it further, net force is really the total force applied in a <em>single</em> frame, so we can use a vector and clear it at the end of each update. This allows the user to apply a force by adding it, but removing it is automatic. This shortens our code and gives a performance bump because there is no summation of forces, it’s a running total.</p>



<p>We’ll use this struct to store the object info for now.</p>



<pre><span>struct</span>&nbsp;<span>Object</span>&nbsp;<span>{</span>
	<span>vector3</span>&nbsp;<span>Position</span><span>;</span>&nbsp;<span>//&nbsp;struct&nbsp;with&nbsp;3&nbsp;floats&nbsp;for&nbsp;x,&nbsp;y,&nbsp;z&nbsp;or&nbsp;i&nbsp;+&nbsp;j&nbsp;+&nbsp;k</span>
	<span>vector3</span>&nbsp;<span>Velocity</span><span>;</span>
	<span>vector3</span>&nbsp;<span>Force</span><span>;</span>
	<span>float</span>&nbsp;<span>Mass</span><span>;</span>
<span>};</span>
</pre>



<p>We need a way to keep track of the objects we want to update. A classic approach is to have a <em>physics world</em> that has list of objects and a <em>step</em> function that loops over each one. Let’s see how that might look; I’ll omit header/cpp files for brevity.</p>



<pre><span>class</span>&nbsp;<span>PhysicsWorld</span>&nbsp;<span>{</span>
<span>private</span><span>:</span>
	<span>std</span><span>::</span><span>vector</span><span>&lt;</span><span>Object</span><span>*&gt;</span>&nbsp;<span>m_objects</span><span>;</span>
	<span>vector3</span>&nbsp;<span>m_gravity</span>&nbsp;<span>=</span>&nbsp;<span>vector3</span><span>(</span><span>0</span><span>,</span>&nbsp;<span>-</span><span>9.81f</span><span>,</span>&nbsp;<span>0</span><span>);</span>
 
<span>public</span><span>:</span>
	<span>void</span>&nbsp;<span>AddObject</span>&nbsp;&nbsp;&nbsp;<span>(</span><span>Object</span><span>*</span>&nbsp;<span>object</span><span>)</span>&nbsp;<span>{</span>&nbsp;<span>/*&nbsp;...&nbsp;*/</span>&nbsp;<span>}</span>
	<span>void</span>&nbsp;<span>RemoveObject</span><span>(</span><span>Object</span><span>*</span>&nbsp;<span>object</span><span>)</span>&nbsp;<span>{</span>&nbsp;<span>/*&nbsp;...&nbsp;*/</span>&nbsp;<span>}</span>
 
	<span>void</span>&nbsp;<span>Step</span><span>(</span>
		<span>float</span>&nbsp;<span>dt</span><span>)</span>
	<span>{</span>
		<span>for</span>&nbsp;<span>(</span><span>Object</span><span>*</span>&nbsp;<span>obj</span>&nbsp;<span>:</span>&nbsp;<span>m_objects</span><span>)</span>&nbsp;<span>{</span>
			<span>obj</span><span>-&gt;</span><span>Force</span>&nbsp;<span>+=</span>&nbsp;<span>obj</span><span>-&gt;</span><span>Mass</span>&nbsp;<span>*</span>&nbsp;<span>m_gravity</span><span>;</span>&nbsp;<span>//&nbsp;apply&nbsp;a&nbsp;force</span>
 
			<span>obj</span><span>-&gt;</span><span>Velocity</span>&nbsp;<span>+=</span>&nbsp;<span>obj</span><span>-&gt;</span><span>Force</span>&nbsp;<span>/</span>&nbsp;<span>obj</span><span>-&gt;</span><span>Mass</span>&nbsp;<span>*</span>&nbsp;<span>dt</span><span>;</span>
			<span>obj</span><span>-&gt;</span><span>Position</span>&nbsp;<span>+=</span>&nbsp;<span>obj</span><span>-&gt;</span><span>Velocity</span>&nbsp;<span>*</span>&nbsp;<span>dt</span><span>;</span>
 
			<span>obj</span><span>-&gt;</span><span>Force</span>&nbsp;<span>=</span>&nbsp;<span>vector3</span><span>(</span><span>0</span><span>,</span>&nbsp;<span>0</span><span>,</span>&nbsp;<span>0</span><span>);</span>&nbsp;<span>//&nbsp;reset&nbsp;net&nbsp;force&nbsp;at&nbsp;the&nbsp;end</span>
		<span>}</span>
	<span>}</span>
<span>};</span>
</pre>



<p>Note the use of pointers, this forces other systems to take care of the actual storing of objects, leaving the physics engine to worry about physics, not memory allocation.</p>



<p>With this you can simulate all sorts of stuff from objects flying through the sky to solar systems.</p>



<iframe src="https://www.youtube.com/embed/crKrkn-RIOU?rel=0" allowfullscreen=""></iframe>



<iframe src="https://www.youtube.com/embed/sHZEs-oQTI4?rel=0" allowfullscreen=""></iframe>



<p>You can do a lot with this, but it’s the easy part to be honest, and that’s not what you came for…</p>



<p><strong>Collision detection</strong></p>



<p>Collision detection is more involved, but we can lighten the load by using some clever tricks. Let’s think about what needs to be found first. If we look at some examples of objects colliding, we notice that in most cases there is a point on each shape that is furthest inside the other.</p>



<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 364.19 219.82"><defs></defs><g id="Layer_2" data-name="Layer 2"><g id="Layer_1-2" data-name="Layer 1"><path id="a7vJywMoip" d="M250.74,108.59A108.34,108.34,0,1,1,142.4.25,108.39,108.39,0,0,1,250.74,108.59Z"></path><path id="aojGvlR1o" d="M364.19,149.47H0"></path><path id="b13RQfe45N" d="M145,216.93a2.64,2.64,0,1,1-2.64-2.64A2.64,2.64,0,0,1,145,216.93Z"></path><path id="a3jwpJe9bd" d="M145,149.47a2.64,2.64,0,1,1-2.64-2.64A2.64,2.64,0,0,1,145,149.47Z"></path><g id="epQfmNKBq"><text transform="translate(147.06 211.83)">A</text></g><g id="g3JoLaPokP"><text transform="translate(147.06 143.56)">B</text></g><path id="bnhzn14P" d="M145,157.62l-2.64-3.82-2.64,3.82h1.68v54.21h1.93V157.62Z"></path><g id="a5EIiwgkw"><g id="aeJzDwQJn"><text transform="translate(269.3 140.4)">Plane B</text></g></g><g id="lWR9tvULn"><g id="giK7zCiBZ"><text transform="translate(209.6 209.41)">Sphere A</text></g></g></g></g></svg>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 319.48 293.52"><defs></defs><g id="Layer_2" data-name="Layer 2"><g id="Layer_1-2" data-name="Layer 1"><path id="circleB" d="M216.92,108.58A108.34,108.34,0,1,1,108.58.25,108.38,108.38,0,0,1,216.92,108.58Z"></path><path id="pointA" d="M201.59,169.27a2.64,2.64,0,1,1-2.64-2.64A2.64,2.64,0,0,1,201.59,169.27Z"></path><path id="bvujLY3Pv" d="M319.23,184.93A108.34,108.34,0,1,1,210.89,76.59,108.4,108.4,0,0,1,319.23,184.93Z"></path><path id="b2sM6mmV3Y" d="M138.53,105.94a2.64,2.64,0,1,1-2.64-2.64A2.64,2.64,0,0,1,138.53,105.94Z"></path><g id="g7IjQmpjFM"><g id="a5ffFLN0Lk"><text transform="translate(206.96 169.27)">A</text></g></g><g id="f5t39XVL9"><g id="a3HVPWIRk6"><text transform="translate(125.66 100.13)">B</text></g></g><path id="a6B9Ro3d2Q" d="M145.73,112.32l-5.51-1.77,1.78,5.5,1.18-1.18,51.76,51.76,1.37-1.37L144.55,113.5Z"></path><g id="b34R6jrlf6"><g id="azoQGzEb0"><text transform="translate(254.28 162.37)">Sphere B</text></g></g><g id="c1Pseh1RfC"><g id="j1Q686eol"><text transform="translate(10.62 84.31)">Sphere A</text></g></g></g></g></svg>



<p>This turns out to be all we need to respond to a collision. From those two points we can find the normal, and how deep the objects are inside one another. This is huge because it means that we can abstract the idea of different shapes away, and only worry about the points in the response.</p>



<p>Let’s jump into the code, we’ll need some helper structs that I’ll note first.</p>



<pre><span>struct</span>&nbsp;<span>CollisionPoints</span>&nbsp;<span>{</span>
	<span>vector3</span>&nbsp;<span>A</span><span>;</span>&nbsp;<span>//&nbsp;Furthest&nbsp;point&nbsp;of&nbsp;A&nbsp;into&nbsp;B</span>
	<span>vector3</span>&nbsp;<span>B</span><span>;</span>&nbsp;<span>//&nbsp;Furthest&nbsp;point&nbsp;of&nbsp;B&nbsp;into&nbsp;A</span>
	<span>vector3</span>&nbsp;<span>Normal</span><span>;</span>&nbsp;<span>//&nbsp;B&nbsp;–&nbsp;A&nbsp;normalized</span>
	<span>float</span>&nbsp;<span>Depth</span><span>;</span>&nbsp;&nbsp;&nbsp;&nbsp;<span>//&nbsp;Length&nbsp;of&nbsp;B&nbsp;–&nbsp;A</span>
	<span>bool</span>&nbsp;<span>HasCollision</span><span>;</span>
<span>};</span>
 
<span>struct</span>&nbsp;<span>Transform</span>&nbsp;<span>{</span>&nbsp;<span>//&nbsp;Describes&nbsp;an&nbsp;objects&nbsp;location</span>
	<span>vector3</span>&nbsp;<span>Position</span><span>;</span>
	<span>vector3</span>&nbsp;<span>Scale</span><span>;</span>
	<span>quaternion</span>&nbsp;<span>Rotation</span><span>;</span>
<span>};</span>
</pre>



<p>Each shape will have a different type of collider to hold its properties and a base to allow them to be stored. Any type of collider should be able to test for a collision with any other type, so we’ll add functions in the base for each one. These functions will take <em>Transforms</em>, so the colliders can use relative coordinates. I’ll only demonstrate spheres and planes, but the code is repeatable for any number of colliders.</p>



<pre><span>struct</span>&nbsp;<span>Collider</span>&nbsp;<span>{</span>
	<span>virtual</span>&nbsp;<span>CollisionPoints</span>&nbsp;<span>TestCollision</span><span>(</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>transform</span><span>,</span>
		<span>const</span>&nbsp;<span>Collider</span><span>*</span>&nbsp;<span>collider</span><span>,</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>colliderTransform</span><span>)</span>&nbsp;<span>const</span>&nbsp;<span>=</span>&nbsp;<span>0</span><span>;</span>
 
	<span>virtual</span>&nbsp;<span>CollisionPoints</span>&nbsp;<span>TestCollision</span><span>(</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>transform</span><span>,</span>
		<span>const</span>&nbsp;<span>SphereCollider</span><span>*</span>&nbsp;<span>sphere</span><span>,</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>sphereTransform</span><span>)</span>&nbsp;<span>const</span>&nbsp;<span>=</span>&nbsp;<span>0</span><span>;</span>
 
	<span>virtual</span>&nbsp;<span>CollisionPoints</span>&nbsp;<span>TestCollision</span><span>(</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>transform</span><span>,</span>
		<span>const</span>&nbsp;<span>PlaneCollider</span><span>*</span>&nbsp;<span>plane</span><span>,</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>planeTransform</span><span>)</span>&nbsp;<span>const</span>&nbsp;<span>=</span>&nbsp;<span>0</span><span>;</span>
<span>};</span>
</pre>



<p>Let’s make both types of colliders at the same time too see how they interact. A sphere is defined as a point and a radius, and a plane is defined as a vector and a distance. We’ll override the functions from <em>Collider</em>, but won’t worry about the work for now.</p>



<p>We can choose per collider which other colliders it will detect by filling, or not filling, in these functions. In this case, we don’t want Plane v Plane collisions, so we return an empty <em>CollisionPoints</em>.</p>



<pre><span>struct</span>&nbsp;<span>SphereCollider</span>
	<span>:</span>&nbsp;<span>Collider</span>
<span>{</span>
	<span>vector3</span>&nbsp;<span>Center</span><span>;</span>
	<span>float</span>&nbsp;<span>Radius</span><span>;</span>
 
	<span>CollisionPoints</span>&nbsp;<span>TestCollision</span><span>(</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>transform</span><span>,</span>
		<span>const</span>&nbsp;<span>Collider</span><span>*</span>&nbsp;<span>collider</span><span>,</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>colliderTransform</span><span>)</span>&nbsp;<span>const</span>&nbsp;<span>override</span>
	<span>{</span>
		<span>return</span>&nbsp;<span>collider</span><span>-&gt;</span><span>TestCollision</span><span>(</span><span>colliderTransform</span><span>,</span>&nbsp;<span>this</span><span>,</span>&nbsp;<span>transform</span><span>);</span>
	<span>}</span>
 
	<span>CollisionPoints</span>&nbsp;<span>TestCollision</span><span>(</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>transform</span><span>,</span>
		<span>const</span>&nbsp;<span>SphereCollider</span><span>*</span>&nbsp;<span>sphere</span><span>,</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>sphereTransform</span><span>)</span>&nbsp;<span>const</span>&nbsp;<span>override</span>
	<span>{</span>
		<span>return</span>&nbsp;<span>algo</span><span>::</span><span>FindSphereSphereCollisionPoints</span><span>(</span>
			<span>this</span><span>,</span>&nbsp;<span>transform</span><span>,</span>&nbsp;<span>sphere</span><span>,</span>&nbsp;<span>sphereTransform</span><span>);</span>
	<span>}</span>
 
	<span>CollisionPoints</span>&nbsp;<span>TestCollision</span><span>(</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>transform</span><span>,</span>
		<span>const</span>&nbsp;<span>PlaneCollider</span><span>*</span>&nbsp;<span>plane</span><span>,</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>planeTransform</span><span>)</span>&nbsp;<span>const</span>&nbsp;<span>override</span>
	<span>{</span>
		<span>return</span>&nbsp;<span>algo</span><span>::</span><span>FindSpherePlaneCollisionPoints</span><span>(</span>
			<span>this</span><span>,</span>&nbsp;<span>transform</span><span>,</span>&nbsp;<span>plane</span><span>,</span>&nbsp;<span>planeTransform</span><span>);</span>
	<span>}</span>
<span>};</span></pre>



<p>We can add a function for testing the base and use a technique called <a href="https://en.wikipedia.org/wiki/Double_dispatch" target="_blank" rel="noreferrer noopener">double dispatch</a>. This takes advantage of the type system to determine both types of colliders for us by swapping the arguments, determining the first, then the second type through two calls of TestCollision. This saves us needing to know what type of colliders we are checking, which means we’ve fully abstracted away the notion of different shapes outside the collision detection.</p>



<pre><span>struct</span>&nbsp;<span>PlaneCollider</span>
	<span>:</span>&nbsp;<span>Collider</span>
<span>{</span>
	<span>vector3</span>&nbsp;<span>Plane</span><span>;</span>
	<span>float</span>&nbsp;<span>Distance</span><span>;</span>
 
	<span>CollisionPoints</span>&nbsp;<span>TestCollision</span><span>(</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>transform</span><span>,</span>
		<span>const</span>&nbsp;<span>Collider</span><span>*</span>&nbsp;<span>collider</span><span>,</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>colliderTransform</span><span>)</span>&nbsp;<span>const</span>&nbsp;<span>override</span>
	<span>{</span>
		<span>return</span>&nbsp;<span>collider</span><span>-&gt;</span><span>TestCollision</span><span>(</span><span>colliderTransform</span><span>,</span>&nbsp;<span>this</span><span>,</span>&nbsp;<span>transform</span><span>);</span>
	<span>}</span>
 
	<span>CollisionPoints</span>&nbsp;<span>TestCollision</span><span>(</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>transform</span><span>,</span>
		<span>const</span>&nbsp;<span>SphereCollider</span><span>*</span>&nbsp;<span>sphere</span><span>,</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>sphereTransform</span><span>)</span>&nbsp;<span>const</span>&nbsp;<span>override</span>
	<span>{</span>
		<span>//&nbsp;reuse&nbsp;sphere&nbsp;code</span>
		<span>return</span>&nbsp;<span>sphere</span><span>-&gt;</span><span>TestCollision</span><span>(</span><span>sphereTransform</span><span>,</span>&nbsp;<span>this</span><span>,</span>&nbsp;<span>transform</span><span>);</span>
	<span>}</span>
 
	<span>CollisionPoints</span>&nbsp;<span>TestCollision</span><span>(</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>transform</span><span>,</span>
		<span>const</span>&nbsp;<span>PlaneCollider</span><span>*</span>&nbsp;<span>plane</span><span>,</span>
		<span>const</span>&nbsp;<span>Transform</span><span>*</span>&nbsp;<span>planeTransform</span><span>)</span>&nbsp;<span>const</span>&nbsp;<span>override</span>
	<span>{</span>
		<span>return</span>&nbsp;<span>{};</span>&nbsp;<span>//&nbsp;No&nbsp;plane&nbsp;v&nbsp;plane</span>
	<span>}</span>
<span>};</span>
</pre>



<p>In cases like this, where there are many classes with a web of similar functions, it can be confusing as to where the actual code is located. <em>Sphere</em> v …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.winter.dev/2020/designing-a-physics-engine/">https://blog.winter.dev/2020/designing-a-physics-engine/</a></em></p>]]>
            </description>
            <link>https://blog.winter.dev/2020/designing-a-physics-engine/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24016718</guid>
            <pubDate>Sat, 01 Aug 2020 02:07:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Glucosamine Supplementation Reduces All-Cause Mortality: Study]]>
            </title>
            <description>
<![CDATA[
Score 254 | Comments 134 (<a href="https://news.ycombinator.com/item?id=24012587">thread link</a>) | @mrfusion
<br/>
July 31, 2020 | https://www.lifespan.io/news/glucosamine-supplementation-reduces-all-cause-mortality/ | <a href="https://web.archive.org/web/*/https://www.lifespan.io/news/glucosamine-supplementation-reduces-all-cause-mortality/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
																
								
								
<p>It is one of the most commonly used supplements frequently taken to address joint pain, but there might be more to this dietary supplement than first meets the eye.</p>
<p>Glucosamine was originally discovered during the 1960s in Italy by pharmacologist Professor Luigi Rovati. Glucosamine is one of the most commonly used dietary supplements and is typically taken to help with the joint pain and inflammation associated with aging.</p>
<p>Glucosamine is a polysaccharide that is found naturally in cartilaginous joint tissues, bones, skin, ligaments, and nails, and it is involved in protein and lipid synthesis. In the context of joints, synovial fluid contains glucosamine and occupies the space between joints, helping to reduce the friction of joint surfaces.</p>
<p>Despite it being frequently taken for arthritis, the evidence for its effectiveness is limited, although, there is data for it being anti-inflammatory, as suggested by the results of a randomized clinical trial in 2015 [1].</p>
<p>However, glucosamine supplementation seems to correlate with lower all-cause mortality and other mortality risks, such as cardiovascular disease (CVD), cancer, respiratory and digestive diseases. A recent <a href="https://ard.bmj.com/content/79/6/829" target="_blank" rel="noopener noreferrer">analysis</a> published in the journal BMJ showed that glucosamine supplementation conveys around a 15% reduction of all-cause mortality [2]. This is a considerable amount when compared to other lifestyle interventions as well as other supplements. The data gathered is from a large number of people, and the trend of reduced mortality is unmistakable.</p>


<blockquote>
<p>This population-based prospective cohort study included 495 077 women and men (mean (SD) age, 56.6 (8.1) years) from the UK Biobank study. Participants were recruited from 2006 to 2010 and were followed up through 2018. We evaluated all-cause mortality and mortality due to cardiovascular disease (CVD), cancer, respiratory and digestive disease. HRs and 95% CIs for all-cause and cause-specific mortality were calculated using Cox proportional hazards models with adjustment for potential confounding variables.</p>
<p>Regular glucosamine supplementation was associated with lower mortality due to all causes, cancer, CVD, respiratory and digestive diseases.</p>
</blockquote>
<p><strong>Conclusion</strong></p>
<p>The exact reasons for this correlation with the reduction of various mortality risks is as yet unknown, but given the large patient group in this and in other analyses along with the popularity of this supplement, it is impossible to deny that there is a definite trend here.</p>
<p>We are not suggesting that you take this supplement, but given that it is cheap and freely available with an excellent safety profile, it may be worth your consideration and further research to evaluate if you wish to take it or not.</p>
			
		
<p><strong>Literature</strong></p>
<p>[1] Navarro, S. L., White, E., Kantor, E. D., Zhang, Y., Rho, J., Song, X., … &amp; Lampe, J. W. (2015). Randomized trial of glucosamine and chondroitin supplementation on inflammation and oxidative stress biomarkers and plasma proteomics profiles in healthy humans. PloS one, 10(2), e0117534.</p>
<p>[2] Li, Z. H., Gao, X., Chung, V. C., Zhong, W. F., Fu, Q., Lv, Y. B., … &amp; Li, F. R. (2020). Associations of regular glucosamine use with all-cause and cause-specific mortality: a large prospective cohort study. Annals of the Rheumatic Diseases, 79(6), 829-836.</p>
																	
															</div></div>]]>
            </description>
            <link>https://www.lifespan.io/news/glucosamine-supplementation-reduces-all-cause-mortality/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24012587</guid>
            <pubDate>Fri, 31 Jul 2020 19:07:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Slack Is Fumbling Developers]]>
            </title>
            <description>
<![CDATA[
Score 28 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24012496">thread link</a>) | @gregdoesit
<br/>
July 31, 2020 | https://www.swyx.io/writing/slack-fumble/ | <a href="https://web.archive.org/web/*/https://www.swyx.io/writing/slack-fumble/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="sapper"> <div>  <main> <div>  <h2 id="postSubtitle">and the Rise of Developer Discords</h2> <p>A few days ago <a href="https://twitter.com/swyx/status/1260929975454056449">I had a sudden realization</a>: I hadn't been active in Slack in the 2 months since <a href="https://www.swyx.io/writing/farewell-netlify/">I left Netlify</a>. Those of you who live and work in Slack know how big this is - I personally went from opening Slack practically <em>every day</em> from 2015 - 2020, to zero meaningful usage whatsoever.</p> <p>In its place, I am now active in a ton of Developer Discord channels. I feel like this is a <em>meaningful</em> shift this year, and based on responses, <a href="https://twitter.com/jkup/status/1260936175172419590?s=20">I'm</a> <a href="https://twitter.com/1Marc/status/1261960701666623488?s=20">not</a> <a href="https://twitter.com/stolinski/status/1260943494635352066?s=20">alone</a>.</p> <section> <h2 id="but-they-didnt-want-developers">But They Didn't Want Developers?</h2> <p>The obvious proximate cause of course, is that I am professionally and personally highly attuned to developer communities, and Slack has actively pushed away developer communities. <a href="https://twitter.com/harry_hedger/status/1261967137989513216?s=20">Quote Harry Hedger</a>:</p> <blockquote> <p>Slack lost <a href="https://www.reactiflux.com/">Reactiflux</a> bc they wanted $70k/month to support them.</p> </blockquote> <p>Sure, I get that this was a deliberate decision. Storage costs money. But <em>how much</em> are we talking? Further, every company understands the logic of running "loss leaders" in order to seed stickiness and future growth. <strong>While literally every other company on Earth is throwing all sorts of free benefits to attract developers</strong>, Slack found developer communities growing like a weed on it, and flicked them away like so much dandruff. Stewart Butterfield is like the anti-<a href="https://www.youtube.com/watch?v=VM-2OVNt-eQ">Ballmer</a>.</p> <p>I'm not here to argue that Slack lost developers because it didn't want developers. That's a tautology. I'm here to argue that <strong>this is a strategic fumble</strong> that opened up the field to whatever will eventually replace Slack for startups (Microsoft Teams' success is difficult to assess so I will ignore it here).</p> </section> <section> <h2 id="slack-losing-its-way">Slack Losing Its Way</h2> <p><strong>Developers are your canary in the coalmine</strong> for user experience - because they create the damn things!</p> <p>Slack's original appeal was that it had a much better user experience than prior work chat/email/collaboration tools including the now-dead HipChat and Campfire. Reams of VC pitch decks were made about the "Consumerization of the Workplace" thanks to Slack's approachable design (as <a href="https://medium.com/@awilkinson/slack-s-2-8-billion-dollar-secret-sauce-5c5ec7117908">MetaLab never fails to remind you</a>) and early touches like emoji reactions and bots. The theory was that we demand the UX polish that we see in our personal lives, in our professional lives as well.</p> <p>Fast forward to today: "I’ve been using Discord a ton lately. Slack for work and Discord for hobbies." That's from <a href="https://twitter.com/1Marc/status/1261960701666623488">Marc Grabanski</a>, who knows a thing or two about developer trends.</p> <p>I'll raise you another point: In 2016, Slack was originally backronym'ed into the <a href="https://www.businessinsider.com/where-did-slack-get-its-name-2016-9">Searchable Log of All Conversation and Knowledge</a>.</p> <p>Fast forward to today and advice from the curiously high amount of "how to work from home" content you've undoubtedly been receiving is overwhelmingly <a href="https://about.gitlab.com/blog/2015/04/08/the-remote-manifesto/#2-communicate-asynchronously">Communicate Asynchronously</a> and <a href="https://knowyourteam.com/m/lessons/161-managing-remote-teams/topics/1331-process-and-tools-how-to-collaborate-effectively-when-your-team-is-remote#but-are-there-tools-you-just-cant-live-without">Use Anything But Slack for Long Lived Knowledge</a>.</p> </section> <section> <h2 id="the-game-is-changing">The Game is Changing</h2> <p>Meanwhile the UX bar has risen in the past 5 years:</p> <ul> <li>Slack makes you create a email and password anew every time you join a new Slack. Clicking a Discord invite immediately lands you in the channel as long as you're logged in.</li> <li>Slack walks you through the full onboarding experience every time you join a new Slack. I've joined probably 80 Slacks, I'm tired of the "👋 Hi, Slackbot here!" welcome messages on how to use Slack, or the downright <em>buggy</em> step through guide that prompts me to put in my profile picture even though Slack clearly figured out that I use the same damn picture every time and is already displaying it. Discord does exactly none of this.</li> <li>You need to repeat the whole email and signup song and dance when you move to your phone. Discord is cross-platform-first - sign in on desktop and you're also signed in on mobile.</li> <li>(Developer specific) The new WYSIWYG editor rolled out last year <a href="https://www.vice.com/en_us/article/pa7nbn/slacks-new-rich-text-editor-shows-why-markdown-still-scares-people">made it annoying</a> for developers accustomed to What We Type Is What We Get Thank You Very Much. Not only does Discord not get in the way of your typing, it offers <strong>SYNTAX HIGHLIGHTING</strong> if you add <a href="https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting">language identifiers</a> to your code fenced blocks!</li> </ul> <p>The past year or so in Office Productivity has been a free-for-all as other companies moved into Slack's open flanks. Notion has torn it up as the Searchable Knowledge Base to beat today. Zoom has overwhelmingly captured video chat, despite Slack also offering it natively. Even a bootstrapped startup like <a href="http://tuple.app/">Tuple</a> has become successful despite Slack buying the literal <a href="https://techcrunch.com/2015/01/28/slack-buys-screenhero-to-add-screen-sharing-and-voice-chat-to-its-work-messaging-platform/">category maker</a> in the space. And of course Discord and even Telegram have made inroads in real time chat.</p> <p>My intuition is that <a href="https://stratechery.com/2017/the-great-unbundling/">the Great Unbundling</a> has come for Slack. Whether or not it survives will probably depend on this: Is text chat really <a href="https://jtbd.info/feature-vs-product-42bf2dad2764">a feature or a product</a>? Either way, having to answer that question is an undesirable strategic position for Slack to be in. There is no victory in this fight.</p> <blockquote> <p>Note: if you feel lost right about here, I released the <a href="https://www.swyx.io/writing/dev-guide-to-tech-strategy/">Developer's Guide to Tech Strategy</a> chapter of <a href="https://twitter.com/coding_career">my upcoming book</a> recently, feel free to pause and check it out.</p> </blockquote> </section> <section> <h2 id="not-over-yet">Not Over Yet</h2> <p>Slack has fumbled a ball, but it still has a strong lead. Two features still make Slack a strong fit for realtime workplace communication: <a href="https://www.theverge.com/2017/1/18/14305528/slack-threads-threaded-messages">threaded messages</a>, which Discord refuses to build, and <a href="https://slackhq.com/shared-channels-growth-innovation">Shared Channels</a>, which <a href="https://twitter.com/swyx/status/1096347638696267778?s=20">I've gone on record</a> as saying will add years' worth and billions of dollars of revenue. Both were introduced in 2017, probably the peak year of innovation for Slack before creating <a href="https://www.google.com/search?q=slack+fund&amp;oq=slack+fund&amp;aqs=chrome..69i57l2j69i59l2j69i65l2j69i61j69i60.843j0j7&amp;sourceid=chrome&amp;ie=UTF-8">the Slack fund</a> in 2018 and then its 2019 IPO.</p> <p>From my limited anecdata, the no-code <a href="https://venturebeat.com/2019/04/24/slack-launches-workflow-builder-for-businesses-to-make-apps-without-code/">Workflow Builder</a> hasn't yet had significant adoption, but I would be happy to see this change over time. All great platforms eventually add low/no-code automation.</p> <p>Slack has also made heavy investments for Enterprise Adoption, with things like <a href="https://slackhq.com/introducing-slack-enterprise-grid">Enterprise Grid</a> (also in 2017! hmm...). I of course don't have any insight into Slack's growth in the Enterprise. I'm fully aware of the economic incentives of going upmarket and forsaking low end customers.</p> </section> <section> <h2 id="communities-over-teams">Communities Over Teams</h2> <p>The net result of all this is that Slack is now very much not <a href="https://www.bloomberg.com/features/2016-design/a/stewart-butterfield/">the operating system for your team</a> that it set out to be. However its entire user experience is tied to teams.</p> <p>More specifically - having one single home team, interacting with that team on one device, and the assumption that you don't join or change teams very often. Perhaps this is the disconnect we are sensing.</p> <p> Slack's focus on teams may be becoming outdated in this new world we live in, where Deep Work and organized knowledge bases come at a premium, and our professional work crosses professional and personal boundaries, and spans across multiple devices, modalities and even multiple communities we both lead and participate in. Discord's focus on communities may win hearts and eventually wallets. Here's <a href="https://twitter.com/kurtkemple/status/1260947289436303367?s=20">Kurt Kemple on Discord's community features</a>, for thought: </p> <blockquote> <p>Discord is more community friendly IMO. Things like moderation, roles, boosting, group A/V, all make it a great place for a more democratized and scaleable platform.</p> </blockquote> <p>By focusing on communities instead of siloed teams, Discord becomes a friend of community leaders. In other words, <strong>Discord aggregates aggregators</strong>. In a world of <a href="https://stratechery.com/aggregation-theory/">Aggregation Theory</a>, this is a very good thing.</p> <p>I thought it was Paul Graham or Benedict Evans who said this, but am unable to find the source: "What hackers do for fun today, we will do at work tomorrow".</p> <p><strong>Hackers are deserting Slack in droves</strong>. Slack should be on high alert.</p> <hr> <p>Recommended reads:</p> <ul> <li>Mule's Dark Horse Discord: How a gaming chat platform is secretly connecting the internet, and defining the future of work: <a href="https://mule.substack.com/p/dark-horse-discord">https://mule.substack.com/p/dark-horse-discord</a></li> <li>Kevin Kwok's The Arc of Collaboration: <a href="https://kwokchain.com/2019/08/16/the-arc-of-collaboration/">https://kwokchain.com/2019/08/16/the-arc-of-collaboration/</a></li> </ul> </section> <hr>  </div> </main>  </div></div></div>]]>
            </description>
            <link>https://www.swyx.io/writing/slack-fumble/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24012496</guid>
            <pubDate>Fri, 31 Jul 2020 19:01:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Does Having an Anime Profile Picture Make You a Better Programmer?]]>
            </title>
            <description>
<![CDATA[
Score 48 | Comments 48 (<a href="https://news.ycombinator.com/item?id=24011583">thread link</a>) | @wh313
<br/>
July 31, 2020 | https://h313.info/blog/github/anime/google-cloud/2020/07/31/does-having-an-anime-profile-picture-make-you-a-better-programmer.html | <a href="https://web.archive.org/web/*/https://h313.info/blog/github/anime/google-cloud/2020/07/31/does-having-an-anime-profile-picture-make-you-a-better-programmer.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
<div>
<article itemscope="" itemtype="http://schema.org/BlogPosting">

<div itemprop="articleBody">
<p>In her 2001 book <em>Anime from Akira to Princess Mononoke</em>, Professor Napier showed that many fans of
anime work in computer science and its related fields. The survey also happened to show that “over
70 percent had a grade point average of 3.0 or higher, which is especially impressive when one
considers the academic rigor of scientific fields.”</p>
<p>Anime has a pretty well-known reputation for creating <a href="https://youtu.be/755BDwzxv5c?t=3">men of culture</a>. That’s a clear
indication that anime fans can be profoundly affected by the medium. In addition, many prolific
open source contributors have anime characters as their profile picture. So that got me to thinking,
does being a fan of anime also make you a more intelligent person?</p>
<p><img src="https://raw.githubusercontent.com/laynH/Anime-Girls-Holding-Programming-Books/master/C%2B%2B/Sakura_Nene_CPP.jpg" alt="sakura nene cpp"></p>
<p>Of course, a question like that is nearly impossible to answer directly. After all, there’s
countless ways to measure intelligence, and anime fandom is so broad that no one definition can fit
all cases. For example, should we consider someone who has only watched <em>Spirited Away</em>, and liked
it very much, but has no exposure to other forms of anime, to be an anime fan? What about people who
only read manga? Or those who exclusively watch <a href="https://en.wikipedia.org/wiki/The_Leader_(web_series)">whatever this is supposed to be</a>?</p>
<p>A smaller question that’s easily answerable would be to see if having an anime profile picture
correlates with you being a better programmer. After all, if someone takes the effort to set their
profile picture to a waifu, they clearly have some fondness for anime. As for being a “better
programmer,” we’ll just equate being better with having more activity on GitHub. And being good at
programming does require an amount of critical reasoning at logic skill, which should equate to a
higher intelligence. Of course, this metric could be easily abused by having a <code>cron</code> job making a
ton of commits, but it’s a measure of programming activity that should be Good Enough™.</p>
<p>Luckily, Google provides their <a href="https://cloud.google.com/vision/">image labelling API</a> for very cheap (or free, if you
have GCP credit). As an example, putting in an image of best girl Mai Sakurajima from <em>Rascal Does
Not Dream of Bunny Girl Senpai</em> into the demo provided, I’ll get this list of labels back from it:</p>
<p><img src="https://h313.info/blog/assets/img/mai_google_vision.png" alt="mai"></p>
<p>Notice how one of the labels is “Anime”? That’s a surprise tool that will help us later :) Google
also provides a Python API, which makes it even easier to check images, since all you have to do now
is check if “Anime” is one of the tags:</p>
<div><div><pre><code><span>anime_or_not</span><span>(</span><span>image</span><span>):</span>
    <span>response</span> <span>=</span> <span>client</span><span>.</span><span>label_detection</span><span>(</span><span>image</span><span>=</span><span>image</span><span>)</span>
    <span>labels</span> <span>=</span> <span>response</span><span>.</span><span>label_annotations</span>

    <span>for</span> <span>item</span> <span>in</span> <span>labels</span><span>:</span>
        <span>if</span> <span>item</span><span>.</span><span>description</span> <span>==</span> <span>"Anime"</span><span>:</span>
            <span>return</span> <span>True</span>
</code></pre></div></div>
<p>As for GitHub commits, we can use the <a href="https://docs.github.com/v3/activity/event_types/">events API</a> that’s roughly analogous to the
contribution history graph of a user. We’ll be measuring user activity just by the number of events
for each user, so each event (opening a PR, creating a repo, etc.) is given equal weight. That’s
roughly analogous to how green a user’s contribution heatmap is.</p>
<p><img src="https://h313.info/blog/assets/img/github_contribution_graph.png" alt="contribution map"></p>
<p><a href="https://pygithub.readthedocs.io/en/latest/">PyGitHub</a> wraps the GitHub API into an easy to use library, so getting the number of
events for a user, as well as their profile picture’s URL, is pretty simple:</p>
<div><div><pre><code><span>users</span> <span>=</span> <span>g</span><span>.</span><span>get_users</span><span>()</span>

<span>for</span> <span>user</span> <span>in</span> <span>users</span><span>:</span>
    <span>event_count</span> <span>=</span> <span>0</span>
    <span>for</span> <span>event</span> <span>in</span> <span>user</span><span>.</span><span>get_events</span><span>():</span>
        <span>event_count</span> <span>+=</span> <span>1</span>

    <span>is_anime_image</span> <span>=</span> <span>check_if_weeb</span><span>(</span><span>user</span><span>.</span><span>avatar_url</span><span>)</span>
</code></pre></div></div>
<p>GitHub does rate limit the API to 5000 requests per hour for authenticated users. That’s enough to
run about 2000 requests per hour. To get around that, we can take advantage of how GitHub profile
IDs are numbered sequentially and process profiles in batches of 1000:</p>
<div><div><pre><code><span>for</span> <span>github_id</span> <span>in</span> <span>range</span><span>(</span><span>1200000</span><span>,</span> <span>1201000</span><span>):</span>
    <span>try</span><span>:</span>
        <span>user</span> <span>=</span> <span>g</span><span>.</span><span>get_user</span><span>(</span><span>github_id</span><span>)</span>
    <span>except</span> <span>Exception</span><span>:</span>
        <span>continue</span>

    <span># do user stuff here
</span></code></pre></div></div>
<p>I’ve modified the <code>get_user</code> function here to use the undocumented <code>/user/:id</code> endpoint. This hasn’t
been implemented in PyGitHub yet, but <a href="https://github.com/PyGithub/PyGithub/issues/1615">this issue</a> seems to be tracking it.</p>
<p>All that’s left is to link these APIs up and save the data. It’s trivial to just loop through all
users using the <code>/users</code> GitHub API endpoint, send their image over to the Google Vision API, note
down whether they had an anime profile picture and the number of events for that user, and finally
log it into a CSV for analysis later. That’s exactly what I did, and you can see my code
<a href="https://github.com/h313/anime-face">here</a>. It’s very research quality, so don’t expect much.</p>
<p>So now I’ve got a table of 3497 GitHub profiles, of which only 23 have anime profile pictures.
Here’s a box plot that displays the distribution of user activity by profile picture type:</p>
<p><img src="https://h313.info/blog/assets/img/github_boxplot.png" alt="box plot"></p>
<p>Hmm, the users with an anime profile picture do seem to have a higher average number of
activities. But we can’t stop here. Keep in mind that there’s way more samples of users without
anime profile pictures compared to those with, as well as the comparatively high amount of
outliers in both groups. To be sure that the difference here is statistically significant,
we’ll need to do a T-test:</p>
<div><div><pre><code><span>from</span> <span>scipy.stats</span> <span>import</span> <span>ttest_ind</span>

<span>cat1</span> <span>=</span> <span>df</span><span>[</span><span>df</span><span>[</span><span>'is_anime_face'</span><span>]</span> <span>==</span> <span>True</span><span>]</span>
<span>cat2</span> <span>=</span> <span>df</span><span>[</span><span>df</span><span>[</span><span>'is_anime_face'</span><span>]</span> <span>==</span> <span>False</span><span>]</span>

<span>ttest_ind</span><span>(</span><span>cat1</span><span>[</span><span>'contribs'</span><span>],</span> <span>cat2</span><span>[</span><span>'contribs'</span><span>])</span>
</code></pre></div></div>
<p>That provides a p-value of <code>0.2371</code>. We now have to conclude that the higher average we got isn’t
statistically significant, since our p-value of 23.7% doesn’t meet the traditional 5% cutoff.
Therefore, we must once again acquiesce to <a href="https://en.wikipedia.org/wiki/Betteridge's_law_of_headlines">Betteridge’s law</a>, and adopt our null
hypothesis, that having an anime profile picture does not necessarily correlate with your abilities
as a programmer.</p>
<p>Further work into this topic can be done, however. Since this project only looked at a small
number of users, who were among the first to register, it is not a representative slice of the
GitHub user population. In addition, it may also be enlightening to include the inactive users
skipped in this experiment.</p>
</div>

</article>
</div>
</div></div>]]>
            </description>
            <link>https://h313.info/blog/github/anime/google-cloud/2020/07/31/does-having-an-anime-profile-picture-make-you-a-better-programmer.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24011583</guid>
            <pubDate>Fri, 31 Jul 2020 18:04:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Niklaus Wirth was right and that is a problem]]>
            </title>
            <description>
<![CDATA[
Score 185 | Comments 169 (<a href="https://news.ycombinator.com/item?id=24011573">thread link</a>) | @bowero
<br/>
July 31, 2020 | https://bowero.nl/blog/2020/07/31/niklaus-wirth-was-right-and-that-is-a-problem/ | <a href="https://web.archive.org/web/*/https://bowero.nl/blog/2020/07/31/niklaus-wirth-was-right-and-that-is-a-problem/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-226">
	

	<div>
		
<p>Wirth’s law is not really a law. Actually, none of them ever are laws. They are adages:</p>



<blockquote><p><em>a proverb or short statement expressing a general truth.</em></p><cite><a href="https://www.lexico.com/en/definition/adage">https://www.lexico.com/en/definition/adage</a></cite></blockquote>



<p>Here is another law that is not a real law:&nbsp;<em>Moore’s law is the observation that the number of transistors in a dense integrated circuit (IC) doubles about every two years.</em></p>



<p>This means that we can expect the speed and capability of computers to increase while lowering the costs. Sadly, this is where Wirth’s law comes in:</p>



<blockquote><p><em>Wirth’s law is an adage on computer performance which states that software is getting slower more rapidly than hardware is becoming faster.</em></p><cite><a href="https://en.wikipedia.org/wiki/Wirth%27s_law">https://en.wikipedia.org/wiki/Wirth%27s_law</a></cite></blockquote>



<p>And while Moore’s law has proven to be true since 1975, Wirth’s law seems to be true as well. Niklaus Wirth, the designer of Pascal, wrote an article in 1995:</p>



<blockquote><p>About 25 years ago, an interactive text editor could be designed with as little as 8,000 bytes of storage. (Modern program editors request 100 times that much!) An operating system had to manage with 8,000 bytes, and a compiler had to fit into 32 Kbytes, whereas their modern descendants require megabytes. Has all this inflated software become any faster? On the contrary. Were it not for a thousand times faster hardware, modern software would be utterly unusable.</p><cite>Niklaus Wirth – A Plea for Lean Software</cite></blockquote>



<p>The problem of modern software development is manyfold. Wirth points out one crucial aspect: time.</p>



<blockquote><p>Time pressure is probably the foremost reason behind the emergence of bulky software.</p><cite>Niklaus Wirth – A Plea for Lean Software</cite></blockquote>



<p>And while that was true back in 1995, that is no longer the most important factor. We now have to deal with a much bigger problem: abstraction. Developers never built things from scratch, and that has never been a problem, but now they have also become lazy.</p>



<p>It was Edsger W. Dijkstra who tried to improve the quality of code and coined the concept of <em>structured programming</em>. He tried to get programming out of the state of crisis it was in, and he found support in programmers like Harlan D. Mills, Richard C. Linger and Bernard I. Witt. For a short period of time, programming was seen as a real craftmanship. Programmers cared about the quality of their programs, and that included clarity and efficiency.</p>



<p>Those times have passed. With the introduction of higher-level languages such as Java, Ruby, PHP and Javascript all in 1995, the same year in which Wirth wrote his article, programming became more abstract. </p>



<p>Languages like these made programming a lot easier and took many things out of the programmer’s hands. They were object-oriented and came with things as an IDE and garbage collection.</p>



<p>This meant that programmers had fewer things to worry about, which is of course great. Sadly, everything comes with a price. Having fewer things to worry about, also means having fewer things to think about. 1995 was the year in which programmers stopped thinking about the quality of their programs.</p>



<p>It also marked the beginning of the widespread use of libraries, probably one of the bigger problems. Don’t get me wrong, I love libraries. They are the only reason I am able to get things done. However, a library never comes with the exact things that you need.</p>



<p>Because a library is not made for one specific project, it probably has a bit more functionalities than you really needed. No problem, you would say. However, things pile up pretty quickly. Even the people who like libraries, don’t want to reinvent the wheel. This results in what we call dependency hell. <a href="https://blog.appsignal.com/2020/04/09/ride-down-the-javascript-dependency-hell.html">Nikola Duza wrote a post about that issue in Javascript</a>.</p>



<p>The problem does not seem that big, but try to grasp what is happening here. In another tutorial that Nikola wrote, he built a simple todo-list. It works in your browser with HTML and Javascript. How many dependencies did he use? <a href="https://blog.appsignal.com/2020/05/14/javascript-growing-pains-from-0-to-13000-dependencies.html">13,000.</a></p>



<p>These numbers are insane, but this problem will only keep increasing. As new, very useful libraries keep being built, the number of dependencies per project will keep growing as well.</p>



<p>That means that the problem Niklaus was warning us about in 1995, only gets bigger over time. </p>



<p>And no, you don’t have to learn assembly and start writing your web application in that. A good way to start would be to split up libraries. Instead of creating one big library that does everything you could ever possibly need, just create many libraries. Your god-like library could still exist, but solely as a wrapper. </p>



<p>This way a programmer only has to select the libraries he really requires, while ignoring the functionalities he is not going to use in his application. Not only are his dependencies smaller, but they will also use less of their dependencies because the dependencies of the unused functionalities do not have to be installed.</p>



<blockquote><p><strong>Note: </strong>The proposed solution obviously is not <em>the</em> solution. It would for example require a good way of versioning software to avoid a new dependency hell.</p></blockquote>
	</div>

	
</article></div>]]>
            </description>
            <link>https://bowero.nl/blog/2020/07/31/niklaus-wirth-was-right-and-that-is-a-problem/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24011573</guid>
            <pubDate>Fri, 31 Jul 2020 18:03:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Rogue Wave of Enterprise SaaS]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24008717">thread link</a>) | @gmays
<br/>
July 31, 2020 | https://staysaasy.com/scaling/2020/07/29/the-rogue-wave-of-enterprise-saas.html | <a href="https://web.archive.org/web/*/https://staysaasy.com/scaling/2020/07/29/the-rogue-wave-of-enterprise-saas.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p><img src="https://staysaasy.com/assets/rogue-wave/freakwave.jpg" alt="Freak wave"></p>

<p>During the pandemic, I’ve done my civic duty and spent lockdown reading a ton of random stuff on the internet. In the midst of a Wikipedia binge, I went down a rabbit hole on <a href="https://en.wikipedia.org/wiki/Rogue_wave">rogue waves</a>: massive waves that occur in the middle of the ocean, endangering ships and generating <a href="https://www.youtube.com/watch?v=pu4ogCy5d4k">kickass Youtube content</a>.</p>

<p>The concept of a rogue wave reminded me of a particularly challenging stage in the life of many venture-backed enterprise SaaS startups. Similar to how a rogue wave forms, during this phase many Small Problems coincide and create a Big Problem. Also like rogue waves, this moment in the life of a high-growth enterprise SaaS business is uncommon – it seems to occur around $5-20 million ARR, which relatively few companies reach, and can seem almost mythical. And finally, the effects of both rogue waves and this challenging startup stage are predictable: if you spend long enough on the high seas, they’re waiting for you.</p>

<p>I’ve seen this phenomenon both in-person as an operator and as an observer of other companies, and I do believe it’s real. I’m writing this essay in order to:</p>

<ul>
  <li>Share what I’ve seen</li>
  <li>Provide some opinions on what you can do about it</li>
</ul>

<p>Hopefully I can be the crusty old mariner bringing back tales that others find useful.</p>

<h2 id="hitting-the-wall">Hitting the Wall</h2>

<p><img src="https://staysaasy.com/assets/rogue-wave/boat.jpg" alt="Big boat, bigger wave">
Startups can be stressful</p>

<p>The rogue wave typically seems to hit between $5M and $20M in ARR. If you’re operating with a common venture-backed SaaS model of <a href="https://www.saastr.com/how-to-figure-out-your-competitors-revenues-in-about-70-seconds/">$1-200k ARR per employee</a>, this is around where you’ll hit the <a href="https://www.bbc.com/future/article/20191001-dunbars-number-why-we-can-only-maintain-150-relationships">Dunbar number</a> of ~150 people: the point where you can no longer operate as a large family, and need to start acting like a corporation. <em>(Note that the post linked is from 2012 – SaaS has since exploded, and more companies are raising huge rounds and hiring more on less revenue)</em></p>

<p>Crossing the 150-person barrier is both operationally and emotionally difficult:</p>

<ul>
  <li>Operational: We have so many teams, we need regular status reports!</li>
  <li>Emotional: Why do I need to send <em>you</em> a status report all of a sudden, I thought we were all friends here?!</li>
</ul>

<p>This transition point is also when many leaders who excelled in scrappy startup mode <a href="https://staysaasy.com/scaling/2020/06/27/hardest-part-of-startup-scale-yourself.html">struggle to level up</a> in a larger organization. Some successfully scale themselves (slow, painful) and others will leave the company (fast, but often even more painful).</p>

<p>Many of the first contracts that you closed post-traction come up for renewal around this point (in the enterprise – SMB SaaS has less predictable growth profiles). If you’re following a <a href="https://www.battery.com/powered/helping-entrepreneurs-triple-triple-double-double-double-to-a-billion-dollar-company/">triple-triple-double-double-double</a> growth path, you’ll be at roughly your second triple. There will be too many customers for the founding team to personally visit and retain everyone.</p>

<p>At this point you’ll need to renew “real” customers, not just friends who took a flyer on your infant product. These aren’t the earliest adopters who will be with you through thick and thin and are really closer to partners. These are real live paying customers who will leave if your product is screwed up. As these renewals approach, a mature post-sales motion becomes essential.</p>

<p>The expectations around your revenue also become more real at this stage. Under about $10M ARR, your revenue projections can be a mild shitshow. But the expectation that you’ll have at least some semblance of predictability steadily increases.</p>

<p>And through it all you need to continue to scale the product. Your code is no longer a series of never-ending green fields rolling off into the distance, and non-trivial parts of your product will need to be meaningfully restructured. You’re burdened by decisions made years ago, often by people who are no longer on the team.</p>

<p>This is especially true for enterprise SaaS. Enterprise products are much more unwieldy than jewel-box consumer products, as they have to support many more users and workflows. Consumer products are like Chipotle: a small, carefully curated set of menu items, built for elegance and efficiency. Enterprise products are like the Cheesecake Factory: you can get steak, pasta, a milkshake and 4 kinds of margarita in the same meal. You need to level up how your team builds products.</p>

<h2 id="cresting-the-wave">Cresting the Wave</h2>

<p>I’m writing about this phenomenon because if you haven’t seen or heard about it, there isn’t really a good way to realize that it might be coming. Even if it isn’t preventable, it’s better to know what’s on the horizon. After all, being able to see around corners is why many companies hire experienced operators.</p>

<p>I won’t claim to be an expert on how to react, but I can share a few things that I think work well and a few that don’t, and what I would do if I had to go through this phase again.</p>

<h3 id="the-crew">The Crew</h3>

<p>First, I would do my best to get the right team in place in advance. In particular, I would make sure that I had very strong functional heads for the functions that will be strained the most: Product, Engineering, Marketing, Sales, Support, and Customer Success. You don’t need every role covered, but it saves headaches to know that some parts of the team are bulletproof. This isn’t the time for unforced errors.</p>

<p><img src="https://staysaasy.com/assets/rogue-wave/pirate-crew.jpg" alt="Pirates of the Caribbean crew">
You want a senior crew. You can tell that Sharkman here has experience and won’t freak out when the database goes down or a large account churns.</p>

<p>I also recommend hiring senior team members who can help see around corners and anticipate issues. This is a taxing time period because it’s so damn busy, and raw, well-directed horsepower tends to carry the day in those situations. More importantly, seasoned operators have typically seen challenging times before, and have the composure to handle them calmly because they know that things are always on fire.</p>

<p>It’s tempting during these busy times to make very junior hires such as new college graduates or coding bootcamp grads just to put butts in seats. These folks can be excellent hires in calmer times, but the chaos caused by too many inexperienced employees is very difficult while you’re cresting the wave.</p>

<h3 id="steering-the-ship">Steering the Ship</h3>

<p>Startups are generally fairly stressful, and that’s heightened in this time period. When operational problems strike in stressful times, it’s common to blame the people involved rather than processes – in reality, bad processes or incentives are usually the root cause.</p>

<p>Generally speaking, the faster you’re growing, the more lightweight your processes should be. When you’re in rapid scaling mode your operational tempo is constantly changing, so there’s no point boiling the ocean to create a perfect process when next month so much will have changed.</p>

<p>For example: when confronting a startup rogue wave, I would not choose to reinvent a completely new system for launching new features. Instead, use 20% of the time to set up a simple, predictable process that gives you 80% of the value. Example: set up a recurring check-in meeting where upcoming releases are discussed by PMs and Tech Leads + a Slack thread where all new releases are announced when they go-live. It won’t be perfect, but you can get this up and running in 15 minutes.</p>

<h3 id="commit">Commit</h3>

<p>This is not the time to hedge your decisions or waffle on strategy.</p>

<p>For example: in general, I prefer to give people a generous window of time to grow into stretch roles. Promoting from within builds continuity, leads to a more invested team, and motivates others by demonstrating that you’re creating strong career paths. But in this phase it’s especially important that you commit to keeping or replacing leaders fast, as there’s just too much going on.</p>

<p>If you’re 30% of the way up a 100 foot wall of water and decide to adjust course, you’re going to get slammed.</p>

<h3 id="keeping-calm">Keeping Calm</h3>

<p>Whatever you do, <em>don’t freak out</em>. When it feels like shit is hitting the fan everywhere, it’s easy to want to react and search for magic solutions. Hire a new Head of X! Spend less! No wait, spend <em>more</em>! Pause development and focus on tech debt! Actually our largest customer needs more features, cancel all tech debt projects!</p>

<p>In reality, to take a line from The Sopranos, <a href="https://www.youtube.com/watch?v=_po7So0MKq4">it won’t be cinematic</a>. Life isn’t a movie. There are no magic fixes – this challenging phase gets resolved by showing up and executing in an unflashy way for months. Magic fixes never really exist, and that’s especially true since this crucible stems from several medium-sized problems amplifying one another.</p>

<p>Searching for a unified solution to your troubles risks distraction. The road is reasonably straightforward and the challenges are tractable: there are just a lot of them. Just like an actual rogue wave, you can’t flee or dodge the factors that make crossing these rogue waves so difficult. You succeed by pointing your boat right at the problem and hitting the gas. Keeping calm doesn’t mean being stubborn or refusing to change, but it does mean that you can’t second guess your decisions – especially when it feels like you’re climbing a wall of water.</p>

<h2 id="takeaways">Takeaways</h2>

<p>Not everyone makes it over the wave – of the roughly 10 cases that I’ve observed, roughly half had their growth stunted for at least several years after hitting this stage. In my opinion this SaaS rogue wave should be taken seriously.</p>

<p>But the good news is that once you’ve crossed the wave, you can speed up again. As you’re rising up the wall everything goes into slow motion; but once you’ve crested the top, your acceleration can increase again as you’ve set yourself up with a more stable operating model and a hardened team.</p>

<p>This is a wonderful time, as there’s so much opportunity. Your team is still in place. The market opportunity is there, and people are buying. And once you’re over the top, the water is much calmer towards the horizon.</p>

<p>In conclusion:</p>

<ul>
  <li>Many challenging situations tend to arise simultaneously when high growth enterprise SaaS companies hit roughly $5-20 million ARR.</li>
  <li>The good news is that after this phase things get easier – the bad news is that it’s not easily avoidable.</li>
  <li>The best way to move forward is to focus on executing – and don’t be overly reactive.</li>
</ul>

    




  </div></div>]]>
            </description>
            <link>https://staysaasy.com/scaling/2020/07/29/the-rogue-wave-of-enterprise-saas.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24008717</guid>
            <pubDate>Fri, 31 Jul 2020 14:09:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Starting a Business Around GPT-3 Is a Bad Idea]]>
            </title>
            <description>
<![CDATA[
Score 111 | Comments 65 (<a href="https://news.ycombinator.com/item?id=24007929">thread link</a>) | @paraschopra
<br/>
July 31, 2020 | https://www.allencheng.com/starting-a-business-around-gpt-3-is-a-bad-idea/ | <a href="https://web.archive.org/web/*/https://www.allencheng.com/starting-a-business-around-gpt-3-is-a-bad-idea/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>GPT-3 is an amazing technology. Within a few weeks after beta API access opened, a <a href="https://twitter.com/xuenay/status/1283312640199196673" target="_blank">host of jaw-dropping demos</a> popped up, from <a href="https://twitter.com/sharifshameem/status/1282676454690451457" target="_blank">automatic code generation</a> to <a href="https://twitter.com/nicklovescode/status/1283326066338062337" target="_blank">automated therapy bots</a> to <a href="https://www.gwern.net/GPT-3" target="_blank">writing original poetry and Navy SEAL copypasta memes</a>. It does things that would have been science fiction 10 years ago.&nbsp;GPT-3 and its successor algorithms are going to change entire industries.</p><p>Naturally, tech founders and VC investors are salivating at the possibilities of turning GPT-3 applications into businesses.</p><p>But a good technology doesn’t necessarily make for a good business. <strong>The fact that GPT-3 works so well out of the box should be terrifying to founders.</strong> Here’s why:</p><ul><li>If it’s easy to make a good-enough app out of the box, the barriers to entry are mercilessly low. Dozens of competitors for your idea will spring up literally overnight, as they already have in these Twitter demos.</li><li>It’s not just about new entrants. If GPT-3 is so easy to adopt and build products with, incumbents will do it too. Thus, in Clayton Christensen’s framework, GPT-3 looks more like a <strong>sustaining innovation</strong> than a disruptive innovation. This will strengthen existing winners more than it creates openings for new startups.</li><li>If the baseline GPT-3 performance cannot be substantially improved to create a substantial (10x) proprietary edge, the competition will shift away from technology to other dimensions of competition—particularly in marketing and distribution. This is where incumbents beat startups.</li><li>Meanwhile, the profits will accrue to the true beneficiaries: 1) the algorithm owners,&nbsp; OpenAI (and, by extension, Azure), 2) to marketing platforms, particularly Google and Facebook. Both can raise pricing to the point where companies built on each are barely profitable.</li></ul><p>Let’s dive more into these ideas.</p><h2><span id="Low_Barrier_to_Entry_Fierce_Competition"><strong>Low Barrier to Entry = Fierce Competition</strong></span></h2><div><figure><img src="https://i1.wp.com/www.allencheng.com/wp-content/uploads/2020/07/image.png?w=750&amp;ssl=1" alt="" srcset="https://i1.wp.com/www.allencheng.com/wp-content/uploads/2020/07/image.png?w=471&amp;ssl=1 471w, https://i1.wp.com/www.allencheng.com/wp-content/uploads/2020/07/image.png?resize=300%2C174&amp;ssl=1 300w" sizes="(max-width: 471px) 100vw, 471px" data-recalc-dims="1"></figure></div><p><a href="https://a16z.com/2020/05/28/moats-before-gross-margins/" target="_blank">Moats provide an enduring competitive advantage</a>. The wider and deeper the moat, the higher the barrier to entry to compete with your business, and the less capably a hotshot teenager can start a new company to compete.</p><p>If a moat is shallow, hundreds of competitors can pop up overnight and provide a good-enough competing product. If you can’t build a meaningful product advantage, then the grounds of competition shift to other dimensions of competition—namely, marketing and distribution.</p><h3><strong>The Parable of Online Mattresses</strong></h3><p>The online mattress industry had shallow moats and played out predictably. A few years ago, if you wanted to start a new mattress company, you only needed to cobble together a few components:</p><ul><li>A manufacturer</li><li>A website</li><li>A marketing campaign</li></ul><p>At the peak of the industry, there were <a href="https://www.cnbc.com/2019/08/18/there-are-now-175-online-mattress-companiesand-you-cant-tell-them-apart.html" target="_blank">175 online mattress companies</a>. None had a meaningful product advantage—many used the same mattress manufacturers. (And having personally tried several of these mattresses in stores, they really did feel about the same.)</p><p>The grounds of competition thus shifted to marketing and branding, which is why a year ago you heard so many podcast ads for Casper, Purple, and the like. When product 17 and product 130 are identical, then the only way to win is to get buyers more familiar with product 17. But <strong>when competition meets a narrow dimension of competition, profits evaporate</strong>—companies bid up marketing prices to the highest that they can endure. It became a game of chicken—how much are <em>you </em>willing to spend to acquire a customer? The best-funded startups “won” this branding battle.</p><p>But this was a short-lived victory. <strong>The ultimate result: perpetually unprofitable businesses</strong>, with <a href="https://www.marketwatch.com/investing/stock/cspr/financials" target="_blank">companies spending most of their revenue on COGS and user acquisition</a>. Casper now has a market value of just $350 million, down from its peak private valuation of $1.1 billion.&nbsp;</p><p>Meanwhile, the real beneficiaries of all this funding are:&nbsp;</p><p>1) the companies selling shovels in the gold rush, including mattress manufacturers and marketing platforms (particularly Google and Facebook)</p><p>2) customers, who enjoyed rock-bottom mattress prices subsidized by venture capital.</p><p>This situation played out in meal kit companies too, and one could argue the same is true of Uber vs Lyft and the food delivery war going on now.</p><h2><span id="Can_You_Differentiate_Yourself_with_GPT-3"><strong>Can You Differentiate Yourself with GPT-3?</strong></span></h2><p>Because GPT-3 works so well out of the box, I see this playing out a similar way. To start a new AI company, you need only cobble together a few components:</p><ul><li>A product built on GPT-3’s (very easy to use) API</li><li>A website</li><li>A marketing campaign</li></ul><p>As we see with Twitter demos, the products you can make off the shelf will frankly be pretty good.</p><p>Unfortunately, most of the products built on GPT-3 will be identical to each other, and few will have a meaningful edge. We’ll dive more into this next.</p><h3><strong>Differentiated Technology</strong></h3><p><strong>To win a market with technology, your product needs to be <em>obviously</em> better to the user.</strong> When Google first came out, it delivered such better search results than other engines—possibly by a subjective factor of 10 or more—that there was little reason to use any competitor. When Dropbox first came out, its syncing and ease of use was unbeatable. Right now SpaceX has a rocket that can do what no other company’s can, at an unbeatable price. These companies have real technology moats.</p><p>In contrast, if an industry has products that are are more or less identical—like online mattresses—the grounds of competition shift elsewhere other than product (like marketing and distribution), and there is no technology moat.</p><p>So imagine: If 100 companies build an AI therapy bot with GPT-3, how different will their user experiences be? How far ahead will company #1 be above company #2, and then over company #10? <strong>Will there be a 10x difference? Or more like a 5% difference?</strong></p><p>I argue: <strong>The better GPT-3 works out of the box, the harder it will be for any single company to build meaningful differentiation.</strong> Frankly, <a href="https://twitter.com/nicklovescode/status/1283326066338062337" target="_blank">early demos already look very good</a>.</p><figure><div><blockquote data-width="550" data-dnt="true"><div lang="en" dir="ltr"><p>I thought you made a good point so I did another run being as depressive as I could. I sampled most “John” lines only once, I think I ran one twice. I’m really impressed</p><p>The prefix makes a difference but that could be automatically prepended by an end-user therapy app <a href="https://t.co/YZmQL9EXqW" target="_blank">pic.twitter.com/YZmQL9EXqW</a></p></div>— Nick Cammarata (@nicklovescode) <a href="https://twitter.com/nicklovescode/status/1283326066338062337?ref_src=twsrc%5Etfw" target="_blank">July 15, 2020</a></blockquote> </div></figure><p>Imagine how much better it can get with just a <em>little</em> more work—a modicum of fine tuning on therapy conversations and user safeguards. This is the new baseline, the “table stakes” for any new entrant, which is going to be relatively easy to achieve.</p><p>And then from this baseline, how much better could any one company get?</p><p>Here’s one way to think about it. The pinnacle of AI therapy would be a bot that rivals the <em>best</em> human therapist. Call this the 100% experience. The <em>average</em> human therapist might be somewhere at the 85% bar. And then a human user might be willing to tolerate a “good enough” AI performing at 70%—it’s clearly worse than a human therapist, but usable enough that it can keep up a coherent conversation and remembers that you were bullied in 5th grade.</p><figure><img src="https://lh6.googleusercontent.com/d-9mr1Bo8EgQaIlWnKd4EQuSrjjJuoBmjOkpcbPwJ0h9xnyIGz832tT133eTopDQZQWr2pXAJ-lS9mhaP9KBInUCnGtSqJKAtdZV-acn55hQ25c-p7bwdHgjyl-zFJ54I9Hg4aVX" alt=""></figure><p><strong>Before GPT-3, building anything </strong><strong><em>close</em></strong><strong> to the Good Enough AI was really hard.</strong> A company would have needed to invest many millions into its own algorithms and data cleanup to get anywhere close to the human user bar.&nbsp;</p><p>In this environment, the leading company had a few advantages:</p><ul><li>The barrier to entry was high, so it faced less competition.</li><li>The spread between the #1 and #3 companies was high, meaning the leading company had a sizable moat.</li><li>It still had a lot of room to grow to get to 85%+ level—this is room in which it could carve out a meaningful edge over competition.</li><li>It owned its own technology, so it could iterate to continuously improve its experience.</li></ul><p>Here’s how the landscape might have looked:</p><figure><img src="https://lh3.googleusercontent.com/Fz675xiZWljHeS4HWUGNcrO8qbaLtjJ9zfqJNCa2HhthN84Nt7JCQpMMhC4SzO9dWfSkGpWxOwIkaz745S-XJijAd91g0JDS2A7nDXrv6ku2fckVbW5kCl-UEyZci3tRgrb4aNwl" alt=""></figure><p>(In practice, I don’t know of any <a href="https://www.healthline.com/health/mental-health/chatbots-reviews#7" target="_blank">pre-GPT-3 AI therapy bot</a> that comes anywhere close to a human therapist. So competitive lead notwithstanding, I don’t think there have been any good AI therapy businesses to date.)</p><p>Then comes GPT-3. <strong>Now <em>anyone</em> can produce a “good enough AI therapy bot,”</strong> with a modicum of fine-tuning and UX design. This outcome wouldn’t be shocking—people were producing <a href="https://twitter.com/nicklovescode/status/1283326066338062337" target="_blank">plausible demos</a> within days of accessing the private beta. Imagine what can be done with just a few weeks of work and $500,000 of investment.</p><p>The result: the range of competition narrows:</p><figure><img src="https://lh6.googleusercontent.com/BiYRcsL4EKmlGKyc5gFDCCkQ_79ijr-jBknzfVb2MVuPj-Rp-D56fLeNhVnV9F46I3tWmUgzJLfV_hh5Q-oOZuR531DWJwI11JKQWgRfFt0fDDDGybrQX7yuTvAGp0TDWmem8_K5" alt=""></figure><p>Compared to the previous situation:</p><ul><li>There are many more competitors who can offer a “good enough” product.</li><li>The range of competition is compressed into a narrower band. It becomes much harder for any single product to stand out from the median product, which is already quite good.</li><li>Because the companies don’t own the core technology behind GPT-3, their ability to improve beyond the baseline performance is limited. Yes, they can fine-tune GPT-3 with their proprietary data, but how much is this going to improve on the core algorithm, which <a href="https://venturebeat.com/2020/06/11/openai-launches-an-api-to-commercialize-its-research/" target="_blank">cost $12 million to train</a> on 40GB of Internet text? <strong>And won’t everyone else be fine-tuning like this too?</strong></li></ul><p>Again, this really gives credit to OpenAI for how well GPT-3 works—out of the gate, you can produce a pretty usable product. But for startups, this merely <em>raises the table stakes</em> instead of offering a competitive advantage, and it <strong>neutralizes the technology edge</strong> any single company can have over another. Great for OpenAI, not great for new startups.</p><p><strong>Then imagine how much worse this will get when GPT-4 comes out. And again with GPT-5. </strong>The minimum bar of quality will keep inching up inexorably, and the range of competition will be compressed further and further.</p><p>Plus, it’s likely that <strong>any proprietary progress you make on GPT-3 will be totally wiped out by GPT-4</strong>; the same way your tweaks on GPT-2 would have been wiped out by GPT-3.</p><p>We’ve talked about AI therapy here, but this argument extends easily to automated code generation, creative writing tools, chatbots, Q&amp;A services, games, and so on.</p><h3><strong>Other Avenues of Differentiation</strong></h3><p>Yes, the algorithm isn’t everything. Products can still differentiate with their user experience, product design, customer support, adding on human services, and so on.&nbsp;</p><p>But as far as AI companies typically go, this is not really the <em>hard</em> …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.allencheng.com/starting-a-business-around-gpt-3-is-a-bad-idea/">https://www.allencheng.com/starting-a-business-around-gpt-3-is-a-bad-idea/</a></em></p>]]>
            </description>
            <link>https://www.allencheng.com/starting-a-business-around-gpt-3-is-a-bad-idea/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24007929</guid>
            <pubDate>Fri, 31 Jul 2020 12:19:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Google removes all Danish music from YouTube]]>
            </title>
            <description>
<![CDATA[
Score 169 | Comments 174 (<a href="https://news.ycombinator.com/item?id=24006932">thread link</a>) | @erk__
<br/>
July 31, 2020 | https://www.koda.dk/about-us/press-release-google-removes-all-danish-music-from-youtube | <a href="https://web.archive.org/web/*/https://www.koda.dk/about-us/press-release-google-removes-all-danish-music-from-youtube">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> <div>   
    <p>Press Release<br>July 30, 2020</p>

<h3><strong>Google removes all Danish music from YouTube </strong></h3>
<h5>While the negotiations on a new joint Nordic agreement are in full swing, Google have chosen to leverage their total dominance in the market in the strongest way possible. On the evening of Thursday 30 July, Google announced that they will soon remove all Danish music content on YouTube.</h5>

<p>Under the auspices of the Nordic alliance of collecting societies, Polaris, negotiations on a joint Nordic agreement on the use of music on YouTube are currently in full swing. The agreement will replace the local agreements of the Norwegian, Finnish and Danish composers and songwriters’ societies, combining them in a single, joint agreement with Google. In the case of Koda, the national agreement for Denmark expired in April, after which it was temporarily extended – as is standard practice in the industry while negotiating a new agreement.</p>
<p>Now, however, Google have issued a new demand: if the agreement is to be temporarily extended, Koda must agree to reduce the payment provided to composers and songwriters for YouTube’s use of music by almost 70% – despite the fact that YouTube’s use of music has increased significantly since Koda entered into its last agreement with Google.</p>
<p>Of course, Koda cannot accept these terms, and Google have now unilaterally decided that Koda’s members cannot have their content shown on YouTube and that their fans and users on YouTube will be unable to listen to Koda members’ music until a new agreement is in place.</p>
<p>Although the parties involved in the negotiations on the new joint agreement are by no means in concord yet, progress has been made in recent weeks, and Koda is puzzled by the extremely aggressive approach taken by Google in the negotiations this time.</p>
<h5><strong>Koda’s media director, Kaare Struve, says:</strong></h5>
<p>‘Google have always taken an “our way or the highway” approach, but even for Google, this is a low point. Of course, Google know that they can create enormous frustration among our members by denying them access to YouTube – and among the many Danes who use YouTube every day. We can only suppose that by doing so, YouTube hope to be able to push through an agreement, one where they alone dictate all terms’.</p>
<p>Ever since the first agreement was signed in 2013, the level of payments received from YouTube has been significantly lower than the level of payment agreed to by subscription-based services.</p>
<h5><strong>Koda’s CEO, Gorm Arildsen, says:</strong></h5>
<p>‘It is no secret that our members have been very dissatisfied with the level of payment received for the use of their music on YouTube for many years now. And it’s no secret that we at Koda have actively advocated putting an end to the tech giants’ free-ride approach and underpayment for artistic content in connection with the EU’s new Copyright Directive. The fact that Google now demands that the payments due from them should be reduced by almost 70% in connection with a temporary contract extension seems quite bizarre’.</p>
<p><strong>Media contact</strong>&nbsp;<br>Head of Communications Eva Hein /<span>&nbsp;</span><a href="mailto:eh@koda.dk">eh@koda.dk</a><span>&nbsp;</span>/ (+45) 61893233</p>
</div>
 </div></div>]]>
            </description>
            <link>https://www.koda.dk/about-us/press-release-google-removes-all-danish-music-from-youtube</link>
            <guid isPermaLink="false">hacker-news-small-sites-24006932</guid>
            <pubDate>Fri, 31 Jul 2020 08:59:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Anachro-PC – The Anachronistic Personal Computer]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24006236">thread link</a>) | @MindGods
<br/>
July 30, 2020 | https://jamesmunns.com/blog/anachro-pc-001/ | <a href="https://web.archive.org/web/*/https://jamesmunns.com/blog/anachro-pc-001/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            
<div>
  
  <p><span>2020-07-30</span></p><hr>
  <blockquote>
<p>TL;DR - These are my notes for a potential computer hobbyist personal computer architecture. Someone called it "Minix for motherboards".</p>
</blockquote>
<p>This roughly describes an architecture of off the shelf microcontroller components that can be used to create a basic standalone PC, somewhere in the neighborhood of performance of an old 286/386 DOS style PC.</p>
<p>If I build this, it will almost certainly be done in Rust. I do hope to specify the protocol/operational semantics well enough that you could create a component that was written in Micro/Circuit Python, C/C++, Ada, or maybe even as an embedded linux PC.</p>
<p>I currently am working on other projects, but I feel like this would be a fun project to stream, or maybe even write a book about learning embedded systems, or learning Rust with embedded systems?</p>
<p><strong>If you'd be interested in taking a class building/using a system like this, or would be interested in seeing this happen, <a href="mailto:james.munns@ferrous-systems.com">send me a message</a>.</strong></p>

<h2 id="project-goals">Project Goals</h2>
<p>The goal of this project is to design a PC-ish architecture that makes it easy for people learning embedded systems to build a single component. This includes:</p>
<ol>
<li>Making the system resistant to failure of a single component</li>
<li>Use only protocols/components that can be easily purchased</li>
<li>Be adaptive to allow for variety of components used</li>
<li>Value simplicity over performance</li>
<li>Make the steps to initial success very short</li>
<li>Make this fun for me and other collaborators to work on</li>
</ol>
<h2 id="project-anti-goals">Project Anti-Goals</h2>
<ol>
<li>Designing something that is wholly useful in commercial deployments</li>
</ol>
<h2 id="interesting-ideas">Interesting Ideas</h2>
<p>These are ideas that I think will help make the project successful.</p>
<h3 id="every-component-is-a-microcontroller-any-microcontroller">Every Component is a Microcontroller - Any Microcontroller.</h3>
<p>The idea is to have every part (or <strong>Component</strong>) of the system be its own standalone microcontroller system. Honestly, this is how most computers work today anyway, but the idea is to be able to use almost any microcontroller, from 8051, to Arm Cortex-M, to RISC-V parts, for any piece of the PC. At some point, I can see people even making parts of the PC out of FPGAs as well.</p>
<p>This includes all of the following PC parts:</p>
<ul>
<li>The Main CPU/Processor</li>
<li>An input controller (Keyboard/Mouse/Touch)</li>
<li>A network interface (Ethernet/Wi-Fi/LPWAN)</li>
<li>A storage controller (SSD/SD/HDD/RAMFS/USB)</li>
<li>A Sound Card</li>
<li>A graphics card (framebuffer, shaders)</li>
</ul>
<p>The system should use a protocol that almost every device has support for, and is forgiving. For that reason, I plan to use SPI for system communication.</p>
<h3 id="the-bus-protocol-spi-but-weird">The Bus Protocol - SPI, but weird.</h3>
<p>So, SPI is a simple, fast, and reliable protocol, but the trick is that it is WAY easier to write a SPI Controller, rather than a SPI peripheral. Since I want things to be simplest for the people writing the Components, they should be the SPI Controllers. But this leaves us with a problem, how do we make a PC with N SPI Controllers work with a single SPI peripheral? Here is where it gets weird.</p>
<ol>
<li>Every Component is a SPI Controller, with SCK, COPI, and CIPO pins wired to a central <strong>Arbitrator</strong>.</li>
<li>Every Component also has two GPIOs:
<ul>
<li>An output that is a "Request" line. This signals to the Arbitrator that the Component would like to talk</li>
<li>An input that is a "Go-Ahead" line. This signals to the Component to begin talking to the Arbitrator</li>
</ul>
</li>
</ol>
<p>In this system, the Arbitrator will either support being a SPI Peripheral for a lot of lines, or will handle MUXing the SCK/COPI/CIPO pins as necessary.
The Arbitrator will decide which component it wants to talk to, and for how long.
When a Component pulls its Request line low, the arbitrator will eventually acknowledge this by pulling the Component's Go-Ahead line low.
The Component can begin clocking SPI data at whatever speed it feels like, up to the maximum speed supported by the Arbitrator.
When the Component is done sending/receiving data, it checks whether the Go-Ahead line is still low.
If so, then the message was "accepted" by the arbitrator.
If the Go-Ahead line is released high before the Component releases the Request line, this means that the arbitrator has "hung up" on the Component, meaning either an error or timeout has occurred.</p>
<h3 id="the-arbitrator-basically-a-northbridge-chip">The Arbitrator - Basically a Northbridge chip</h3>
<p>The Arbitrator has two primary tasks:</p>
<ol>
<li>Arbitrating the Bus Protocol as described above, servicing and discovering components that have been connected</li>
<li>Managing memory allocations used to communicate between devices</li>
</ol>
<p>I've already described the lowest level of the protocol above - we'll use an awkward SPI communication. However I haven't described how the higher protocol layers work.</p>
<p>All devices will communicate through a mailbox system.
The arbitrator will take data from the Components, and place them in an in-memory object store.
Each created item will be assigned a UUID, which will be returned to the Component on creation.
This UUID can then be placed in a mailbox to another component, sending access to the data to that component.
Each item is read-only.</p>
<p>Example (psuedocode):</p>
<pre><span>// A binary RPC messages are sent from Component to the Arbitrator
// The format will probably be binary, but maybe with an alternative
// command to use the following REPL format?
//
// Components are also assigned a UUID on boot. This Component has
// the own-address of 9ea8b6a7-2967-4db8-98b8-d4577548ed04.

// Data can be stored to the Arbitrator memory space. Data can then be
// referenced as inter-device storage using UUIDs as a reference. Data
// is allocated on a FIFO basis, with oldest memory items becoming
// dropped.
//
// The arbitrator may choose to limit the rate, maximum size or other
// parameters of creating memory items based on configuration.
CREATE(
    // number of bytes to write, can be set to zero for dynamic
    // length using something like COBS or when the Request
    // line is de-asserted
    13,

    // The payload of the data
    "Hello, world!",

    // No UUID provided to send to another component, just return
    // the newly created UUID. If provided, this would place
    // the created UUID into the mailbox of another component
    None,
)
// The following UUID is returned to the Component on success
-&gt; Result&lt;3b2fd5d1-ae16-46af-afc1-f60241d0a5b6, CreateError&gt;

// You can send data by reference to another component you know's
// mailbox. Mailboxes are provided as a Component's address. Mailboxes
// are a FIFO queue of UUIDs that can be loaded by that Component
SEND_MAILBOX(
    // The data reference to send
    3b2fd5d1-ae16-46af-afc1-f60241d0a5b6,

    // The destination address
    56c3dbda-c762-4107-be58-855dc8e5aa92,
)

// The other device can receive messages on a FIFO basis, and can view
// the item at the top of the stack without removing it
PEEK_MAILBOX(
    // You can provide Some(usize) as a max message size, or None for
    // anymessage size. Messages larger than the usize value will
    // instead return an Error
    Some(32),
)
-&gt; Result&lt;(13, "Hello, world!"), GetError&gt;

// The other device can receive messages on a FIFO basis, and can view
// and remove the item at the top of the stack. If the Controller
// ends the message before all bytes are received, the item is still
// removed from the FIFO. Thiscan be used to simply drop the item on
// the top of the FIFO.
POP_MAILBOX(
    // You can provide Some(usize) as a max message size, or None for
    // any message size. Messages larger than the usize value will
    // instead return an Error
    Some(32),
)
-&gt; Result&lt;(13, "Hello, world!"), GetError&gt;

// TODO: How to have a "clone and modify" operation that isn't hard
// because of re-allocs? Insertions would suck unless I used some kind
//  of rope structure, which might be too complex to implement
</span></pre>
<p>You can also use the SPI interface for certain communications directly to the arbitrator</p>
<pre><span>// Register ourself using an enumerated type ID
//
// TODO: What to do when calling this more than once?
REGISTER_TYPE(
    IO_CONTROLLER,
) -&gt; Result&lt;9ea8b6a7-2967-4db8-98b8-d4577548ed04, GetError&gt;

// Get the ID of ourself. Matches ID given on registration
GET_OWN_ID()
-&gt; Result&lt;9ea8b6a7-2967-4db8-98b8-d4577548ed04, GetError&gt;

// Get limits of the arbitrator interface
GET_ARBITRATOR_LIMITS()
-&gt; {
    min_speed_hz: 125_000,     // What is the minimum SPI clock rate?
    max_speed_hz: 8_000_000,   // What is the maximum SPI clock rate?
    exp_polls_per_sec: 100,    // What is the expected poll frequency?
    request_line_shared: false,// Is the request line open drain?
    total_memory: 524288,      // 512KiB
    max_files: 512,            // Max number of records alive at once
}

// Get limits for message creation
GET_CREATE_LIMITS()
-&gt; {
    max_bytes: 512,             // Largest message can be 512 bytes
    max_bytes_per_second: 4096, // Running resource limits
    messages_per_second: 10,    // Running resource limits
}
</span></pre>
<blockquote>
<p>TODO: How to register multiple addresses/logical addressing? e.g. for keyboard that has a mouse and touchpad?
Could add a "from" field to every message, but then I'm adding a routing layer?</p>
</blockquote>
<h3 id="an-easy-bootloader">An easy bootloader</h3>
<p>Write a simple bootloader that can enumerate on the bus, take an image, and reboot? PXII boot style? Would need to enumerate type
and maybe even serial number or something to load the right firmware to the right place.</p>
<h2 id="the-first-implementation">The First Implementation</h2>
<p>These are ideas for possible first implementation of the PC architecture described above</p>
<h3 id="a-simulator-with-real-world-hooks">A Simulator with real-world hooks</h3>
<p>I could create a simulated environment for these components, using no_std libraries and using TCP over localhost to emulate the SPI target environment, basically providing an I/O library or a "HAL" for simulating each component inside of a system. Maybe even port RTIC to the simulated environment, or even just use QEMU, using Semihosting for simulated SPI/GPIOs?</p>
<p>With a little work, I could also probably have an arbitrator or even just a simple SPI controller sit outside of the PC, bridging one or more simulated components to a shared network with physical components.</p>
<p>This would allow me to bring up multiple devices quickly, both from a Component software …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://jamesmunns.com/blog/anachro-pc-001/">https://jamesmunns.com/blog/anachro-pc-001/</a></em></p>]]>
            </description>
            <link>https://jamesmunns.com/blog/anachro-pc-001/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24006236</guid>
            <pubDate>Fri, 31 Jul 2020 06:30:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Reverse Engineering the PLA Chip in the Commodore 128]]>
            </title>
            <description>
<![CDATA[
Score 167 | Comments 26 (<a href="https://news.ycombinator.com/item?id=24004640">thread link</a>) | @segfaultbuserr
<br/>
July 30, 2020 | https://c128.se/posts/silicon-adventures/ | <a href="https://web.archive.org/web/*/https://c128.se/posts/silicon-adventures/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <h2 id="backstory-and-first-attempts">Backstory and first attempts</h2>
<p>As I mentioned in my last post I’ve been working on reverse engineering the PLA chip in the C128.
I’m now mostly done with this process so I think it’s time to share some of the findings.</p>
<p>This has been a very interesting project as I did not really know much about semiconductor design
and manufacturing. My existing knowledge extended to having seen some die shots and admiring the pretty
looking pictures.</p>
<p>It all started with me buying a cheap microscope to help with soldering surface mount components.</p>

<p><a href="https://c128.se/posts/silicon-adventures/cheapo-microscope.jpg" data-mediabox="gallery" data-title="Cheap microscope">
    <img src="https://c128.se/posts/silicon-adventures/cheapo-microscope.jpg" alt="Cheap microscope">
</a>
</p>
<p>Some time later I ended up watching a video on youtube showing a simpler way of getting silicon dies out
of the packaging.</p>


<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/ZQeHHYJYWXo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>This looked simple enough to try at home as I already had all the equipment needed. Decapping an IC usually involves heated sulphuric acid or
other similar nasty chemicals which I do not really want to play with at home. So I dug out a couple of broken MOS chips I had lying around
(never throw things away, could come in handy). After fiddling around a bit I ended up with two 8521R0 dies and one 8721 PLA.
The first real photo was the one show in the last post.</p>

<p><a href="https://c128.se/posts/now-available/pla-die.png" data-mediabox="gallery" data-title="Full 8721 PLA die photo">
    <img src="https://c128.se/posts/now-available/pla-die.png" alt="Full 8721 PLA die photo">
</a>
</p>
<h2 id="better-microscope">Better microscope</h2>
<p>While this was a success I quickly realised that this microscope did not have enough resolution for me to be able to capture images of high
enough quality. The maximum magnification is 90x, when using a 2x barlow lens. Additionally it does not have a stage so I had to place the die
on a table and then move the whole microscope, which is very unstable and makes it hard to capture the bits you want to see.</p>
<p>As such a better microscope was sourced and purchased, still within reasonable money.</p>

<p><a href="https://c128.se/posts/silicon-adventures/amscope-me580t.jpg" data-mediabox="gallery" data-title="AmScope ME580-T microscope">
    <img src="https://c128.se/posts/silicon-adventures/amscope-me580t.jpg" alt="AmScope ME580-T microscope">
</a>
</p>
<p>This proved to be much better at taking pictures of decent quality, I was however not happy with the camera I bought with it. It’s a cheap camera with no capability for me to manage it remotely except using the AmScope application. Around this time, the Raspberry Pi foundation
released the new High Quality camera for the Raspberry Pi. This camera has a C-mount on it which matches the microscope so I quickly bought
one and put in on the microscope. This camera is fantastic for this job, full control of the whole process from the comfort of linux.</p>

<p><a href="https://c128.se/posts/silicon-adventures/8521r0-detail.jpg" data-mediabox="gallery" data-title="Silicon die detail from 8521R0">
    <img src="https://c128.se/posts/silicon-adventures/8521r0-detail.jpg" alt="Silicon die detail from 8521R0">
</a>
</p>
<p>All in all a vast improvement to the previous setup. With the higher resolution I started needing to stitch the photos together to make
larger pictures. This sounded simple at the start and ended up being (as you can probably guess) not very easy at all. I’m still struggling
with the stitching, but slowly improving. One of the key things for a successful panorama stitch is to have consistent photos when it comes
to panning, focus, white balance etc. etc. The more even the photos, the easier and better the stitching becomes.</p>
<h2 id="motorizing">Motorizing</h2>
<p>This led me to start working on motorizing the table. It was getting really tiresome having to manually move the table around and the photos
ended up being moved in multiple axis etc.</p>
<p>Lots of design work, 3D printing and research into CNC firmwares later I ended up with the following setup:</p>

<p><a href="https://c128.se/posts/silicon-adventures/modded-me580t.jpg" data-mediabox="gallery" data-title="Modified AmScope ME580-T microscope">
    <img src="https://c128.se/posts/silicon-adventures/modded-me580t.jpg" alt="Modified AmScope ME580-T microscope">
</a>
</p>
<p>Going from the top we have a Raspberry 7” display, with a Raspberry Pi4 mounted on the back. Not see is also the RPi HiQ camera mounted on
the microscope. The RPi takes photos, displays previews on the display and also runs the python code that controls the CNC board.</p>
<p>The stage and the levelling table is motorized with 28BYJ-48 steppers controlled using a small board with an ESP32 running <a href="https://github.com/bdring/Grbl_Esp32">Grbl_Esp32</a> and four AD4498 stepper motor controllers.</p>
<p>This whole setup has some issues both in software and hardware but it works well enough for now to enable me to do some work.</p>

<p>With the logistics now sorted out I returned to the work of reverse engineering the chip itself. My initial focus was on the PLA chip
as this should be one of the simpler ones to figure out. PLA stands for <a href="https://en.wikipedia.org/wiki/Programmable_logic_array">Programmable Logic Array</a> and is a very common structure in designs from this era.</p>
<p>Looking at the schematic diagram from wikipedia we should expect to see two main arrays, AND and OR. Inputs are connected to AND and
outputs from the OR array.</p>
<p>Going back to the previous die shot with can improve it with some annotations for the pins and the general areas of the chip. Once we
have established the pins we can see that all the inputs are connected to one array and all the outputs to the other array, just as
expected. This also helps us establish which array is which.</p>

<p><a href="https://c128.se/posts/silicon-adventures/pla-annotated.png" data-mediabox="gallery" data-title="Annotated die shot of 8721 PLA">
    <img src="https://c128.se/posts/silicon-adventures/pla-annotated.png" alt="Annotated die shot of 8721 PLA">
</a>
</p>
<p>Here we see the I/O pins marked up with how they are connected to the lead frame and the pins on the DIP itself. We can also see the two main
areas that make up a PLA structure, the AND array and the OR array. Additionally there is some extra logic at the bottom marked with a question
mark. The function of this was unknown to me but as all the output pins are passing through it I was guessing that it was an output stage of
some kind.</p>
<h2 id="and-array">AND array</h2>
<p>So, if we take a closer look at the AND matrix to start with we will see the following. The colours are a bit off as this was still done
using the AmScope camera and I didn’t figure out how to set the white balance on it.</p>

<p><a href="https://c128.se/posts/silicon-adventures/and-matrix-metal.jpg" data-mediabox="gallery" data-title="AND matrix with metal layer">
    <img src="https://c128.se/posts/silicon-adventures/and-matrix-metal.jpg" alt="AND matrix with metal layer">
</a>
</p>
<p>This was not very helpful to understand what was going on as all the interesting bits are covered up by the top-most metal layer. This was
early on when I was still learning a lot so to remove the metal I took a very brute force approach. I use heavy mechanical scrubbing to
remove the metal which I also learned once I put it back in the microscope had removed everything but the substrate itself. Oops.</p>
<p>Fortunately, the details that I needed were in the diffusion embedded into the substrate:</p>

<p><a href="https://c128.se/posts/silicon-adventures/and-matrix-delayer.jpg" data-mediabox="gallery" data-title="AND matrix substrate">
    <img src="https://c128.se/posts/silicon-adventures/and-matrix-delayer.jpg" alt="AND matrix substrate">
</a>
</p>
<p>Looking closely at this image we can see little squiggly lines where a transistor is located to create a connection within the matrix.</p>
<h2 id="or-array">OR array</h2>
<p>Moving on to the OR array we see the exact same pattern. Hard to tell with the metal layer in place, though easier compared to the AND
matrix. Much easier with just the substrate and diffusion left.</p>


<h2 id="full-matrix-decode">Full matrix decode</h2>
<p>Armed with this knowledge we can now proceed with extracting the full PLA logic matrix from the images.</p>
<p>I marked all transistors in each matrix with a dot and got the following picture:</p>

<p><a href="https://c128.se/posts/silicon-adventures/decoded-matrix.jpg" data-mediabox="gallery" data-title="AND matrix with metal layer">
    <img src="https://c128.se/posts/silicon-adventures/decoded-matrix.jpg" alt="AND matrix with metal layer">
</a>
</p>
<p>All inputs are horizontal in the AND matrix, with each line having a normal and an inverted signal being fed in.
All outputs are horizontal in the OR matrix and they are connected with vertical lines called product terms.</p>
<p>By looking at the dots, we can decode the product terms by doing logic and for all vertical lines in the AND matrix, for example</p>
<pre><code>    p0 = CHAREN &amp; HIRAM &amp; BA &amp; !MS3 &amp; GAME &amp; RW &amp; AEC &amp; A12 &amp; !A13 &amp; A14 &amp; A15
</code></pre><p>For the outputs we instead look horizontal for each output and combine with or, for example</p>
<pre><code>    SDEN = p42 | p43 | p66 | p69
</code></pre><p>So now we have the entire set of logic equations. Hooray!</p>
<h2 id="output-stage">Output stage</h2>
<p>Going back to the full die picture, we now have everything but the box marked with a question mark in the output path.</p>
<p>Looking at higher resolution photos of this we can see similar structures for each output. In all cases except two the structure
is bypassed and the output from the OR matrix goes directly to the output pin. This is however not the case for the two pins <code>DWE</code> and <code>CASENB</code>.</p>
<p><code>DWE</code> is the Write Enable signal going to the main system DRAM chips, CASENB is gating the CAS signal towards the RAM. These two signals are
processed in some form using these output gate structures, so I had to reverse engineer this block.</p>


<p>After quite some time reading up on silicon chip design and manufacturing and a lot of attempts I managed to come up with a schematic
for this that makes sense. I’m not going to go into the whole process here but I will document and post it later. Here I would also like
to thank <a href="https://www.patreon.com/androSID">Frank Wolf</a> for his help, please support his project if you can!</p>

<p><a href="https://c128.se/posts/silicon-adventures/output-schematic.png" data-mediabox="gallery" data-title="Output block schematic">
    <img src="https://c128.se/posts/silicon-adventures/output-schematic.png" alt="Output block schematic">
</a>
</p>
<p>Going a bit further, the way this is used in the <code>DWE</code> and <code>CASENB</code> outputs makes it a normal D-latch. The latch enable for this also comes
for the PLA matrix in a pair of lines in the OR matrix.</p>
<h2 id="result">Result</h2>
<p>So as a final result we can now write down the full HDL code for the C128 PLA chip. I’m using verilog for this. Mind you this is the first
verilog I’ve ever written so it’s probably suboptimal. Using a D-latch for the output in verilog is normally seen as a bad thing, however
in this case I am doing it to replicate the logic and function of the existing chip.</p>
<p>I have validated this to the best of my knowledge, but if I’ve missed anything please let me know!</p>
<p>In difference to the C64 PLA the C128 PLA can not be replaced with just an EPROM or similar due to the presence of the output latches.</p>
<div><pre><code data-lang="verilog"><span>module</span> pla_8721(
    <span>input</span> rom_256,
    <span>input</span> va14,
    <span>input</span> charen,
    <span>input</span> hiram,
    <span>input</span> loram,
    <span>input</span> ba,
    <span>input</span> vma5,
    <span>input</span> vma4,
    <span>input</span> ms0,
    <span>input</span> ms1,
    <span>input</span> ms2,
    <span>input</span> ms3,
    <span>input</span> z80io,
    <span>input</span> z80en,
    <span>input</span> exrom,
    <span>input</span> game,
    <span>input</span> rw,
    <span>input</span> aec,
    <span>input</span> dmaack,
    <span>input</span> vicfix,
    <span>input</span> a10,
    <span>input</span> a11,
    <span>input</span> a12,
    <span>input</span> a13,
    <span>input</span> a14,
    <span>input</span> a15,
    <span>input</span> clk,

    <span>output</span> sden,
    <span>output</span> roml,
    <span>output</span> romh,
    <span>output</span> clrbnk,
    <span>output</span> from,
    <span>output</span> rom4,
    <span>output</span> rom3,
    <span>output</span> rom2,
    <span>output</span> rom1,
    <span>output</span> iocs,
    <span>output</span> dir,
    <span>output</span> <span>reg</span> dwe,
    <span>output</span> <span>reg</span> casenb,
    <span>output</span> vic,
    <span>output</span> ioacc,
    <span>output</span> gwe,
    <span>output</span> colram,
    <span>output</span> charom);

<span>wire</span> p0;
<span>wire</span> p1;
<span>wire</span> p2;
<span>wire</span> p3;
<span>wire</span> p4;
<span>wire</span> p5;
<span>wire</span> p6;
<span>wire</span> p7;
<span>wire</span> p8;
<span>wire</span> p9;
<span>wire</span> p10;
<span>wire</span> p11;
<span>wire</span> p12;
<span>wire</span> p13;
<span>wire</span> p14;
<span>wire</span> p15;
<span>wire</span> p16;
<span>wire</span> p17;
<span>wire</span> p18;
<span>wire</span> p19;
<span>wire</span> p20;
<span>wire</span> p21;
<span>wire</span> p22;
<span>wire</span> p23;
<span>wire</span> p24;
<span>wire</span> p25;
<span>wire</span> p26;
<span>wire</span> p27;
<span>wire</span> p28;
<span>wire</span> p29;
<span>wire</span> p30;
<span>wire</span> p31;
<span>wire</span> p32;
<span>wire</span> p33;
<span>wire</span> p34;
<span>wire</span> p35;
<span>wire</span> p36;
<span>wire</span> p37;
<span>wire</span> p38;
<span>wire</span> p39;
<span>wire</span> p40;
<span>wire</span> p41;
<span>wire</span> p42;
<span>wire</span> p43;
<span>wire</span> p44;
<span>wire</span> p45;
<span>wire</span> p46;
<span>wire</span> p47;
<span>wire</span> p48;
<span>wire</span> p49;
<span>wire</span> p50;
<span>wire</span> p51;
<span>wire</span> p52;
<span>wire</span> p53;
<span>wire</span> p54;
<span>wire</span> p55;
<span>wire</span> p56;
<span>wire</span> p57;
<span>wire</span> p58;
<span>wire</span> p59;
<span>wire</span> p60;
<span>wire</span> p61;
<span>wire</span> p62;
<span>wire</span> p63;</code></pre></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://c128.se/posts/silicon-adventures/">https://c128.se/posts/silicon-adventures/</a></em></p>]]>
            </description>
            <link>https://c128.se/posts/silicon-adventures/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24004640</guid>
            <pubDate>Fri, 31 Jul 2020 01:11:55 GMT</pubDate>
        </item>
    </channel>
</rss>
