<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 10]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 10. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Fri, 11 Sep 2020 00:56:10 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-10.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Fri, 11 Sep 2020 00:56:10 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Why Bayesian Stats Needs Monte-Carlo Methods]]>
            </title>
            <description>
<![CDATA[
Score 107 | Comments 46 (<a href="https://news.ycombinator.com/item?id=24416908">thread link</a>) | @laplacesdemon48
<br/>
September 8, 2020 | https://www.countbayesie.com/blog/2020/8/16/why-bayesian-stats-need-monte-carlo-methods | <a href="https://web.archive.org/web/*/https://www.countbayesie.com/blog/2020/8/16/why-bayesian-stats-need-monte-carlo-methods">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site">
		<div id="canvas">

			<!-- / headerWrapper -->

			<div id="pageWrapper" role="main">
				<section id="page" data-content-field="main-content">
					<article id="article-5f3999dd551f15457784cec9" data-item-id="5f3999dd551f15457784cec9">

	<div>
  <!--SPECIAL CONTENT-->

    

    <div>

    <!--POST HEADER-->

			<header>
				
				
			</header>

    <!--POST BODY-->

      <div><div data-layout-label="Post Body" data-type="item" data-updated-on="1597610547002" id="item-5f3999dd551f15457784cec9"><div><div><div data-block-type="2" id="block-ce72701dbc5d55959e37"><div><div><p>This post emerged from a series of question surrounding a Twitter comment that brought up some very interesting points about how Bayesian Hypothesis testing works and the inability of analytic solutions to solve even some seemingly trivial problems in Bayesian statistics. </p><p>Comparing Beta distributed random variables is something that comes up pretty frequently on this blog (and in my book as well). The set up is fairly straight forward: model an A/B test as sampling from two beta distributions, sample from each distribution a lot, then compare the results.</p><p>This simulation approach often first appears as a clever little trick to solve a more complex math problem, but in fact is a primative form of Monte-Carlo Integration and turns out to one of the only ways to really solve this problem. By exploring this topic deeper in this post we'll see some of the myths that many people have about analytic solutions as well as demonstrating why Monte-carlo methods are so essential to Bayesian statistics.</p></div><h2>Background: A conversation about election results</h2><div><p>An interesting conversation happened on Twitter recently. It started with a retweet of mine regarding Nate Silver (well know author and election forecaster) posting his latest predictions for the 2020 presidential election showing that Biden has a 71% probability of winning versus Trump's 29%</p></div></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1597610689295_4659"><div><div><p>After the last election there was quite a lot of criticism about Nate Silver's forecasting since his company (538) predicted that <a href="https://projects.fivethirtyeight.com/2016-election-forecast/">Hilary Clinton would win with a probability of 71%</a> in the 2016 presidential election.</p><p>This criticism has always annoyed me personally since, in statistical terms, 71% is generally not considered a strong belief in anything. So it is not inconsistent, nor suprising for someone to believe a candidate has 71% chance of success and they still lose. Even when looking at typical p-values, we wait for 95% percent certainty before making claims (and many feel this is a pretty weak belief). But for some reason whenever election polls come up, it seems even very statistically minded people suddenly think that 51% chance is a high probability.</p><p>I retweeted Nate Silver's forecast, mentioned my annoyance and provided an example of another case with a similar probability of winning:</p></div></div></div><div data-block-json="{&quot;cache_age&quot;:&quot;3153600000&quot;,&quot;authorUrl&quot;:&quot;https://twitter.com/willkurt&quot;,&quot;width&quot;:550,&quot;height&quot;:null,&quot;hSize&quot;:null,&quot;resolveObject&quot;:&quot;Tweet&quot;,&quot;html&quot;:&quot;<blockquote class=\&quot;twitter-tweet\&quot;><p dir=\&quot;ltr\&quot; lang=\&quot;en\&quot;>This time can we all remember that rarely in statistics would we judge P(H|D) = 0.71 as a strong belief in anything. <br><br>For comparison if in an A/B test we had these results:<br><br>A has 2 successes in 15 trials<br>B has 3 successes in 14 trials<br><br>This roughly how strong our belief in B is <a href=\&quot;https://t.co/bB4PiB5Tao\&quot;>https://t.co/bB4PiB5Tao</a></p>\u2014 Will Kurt (@willkurt) <a href=\&quot;https://twitter.com/willkurt/status/1293575032975884288?ref_src=twsrc%5Etfw\&quot;>August 12, 2020</a></blockquote>\n<script async=\&quot;\&quot; src=\&quot;https://platform.twitter.com/widgets.js\&quot; charset=\&quot;utf-8\&quot;></script>&quot;,&quot;url&quot;:&quot;https://twitter.com/willkurt/status/1293575032975884288&quot;,&quot;resolvedBy&quot;:&quot;twitter&quot;,&quot;floatDir&quot;:null,&quot;authorName&quot;:&quot;Will Kurt&quot;,&quot;version&quot;:&quot;1.0&quot;,&quot;resolved&quot;:true,&quot;type&quot;:&quot;rich&quot;,&quot;providerName&quot;:&quot;Twitter&quot;,&quot;providerUrl&quot;:&quot;https://twitter.com&quot;}" data-block-type="22" id="block-yui_3_17_2_1_1597610689295_6372"><div><blockquote><div dir="ltr" lang="en"><p>This time can we all remember that rarely in statistics would we judge P(H|D) = 0.71 as a strong belief in anything. </p><p>For comparison if in an A/B test we had these results:</p><p>A has 2 successes in 15 trials<br>B has 3 successes in 14 trials</p><p>This roughly how strong our belief in B is <a href="https://t.co/bB4PiB5Tao">https://t.co/bB4PiB5Tao</a></p></div>— Will Kurt (@willkurt) <a href="https://twitter.com/willkurt/status/1293575032975884288?ref_src=twsrc%5Etfw">August 12, 2020</a></blockquote>
</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1597610689295_6436"><div><div><p>If you were running an A/B test and your A variant had 2 success in 15 trials and your B variant had 3 successes in 14 trails, you would be roughly 71% confident that B was the superior variant.</p><p>Even someone without much statistical training would likely be very skeptical of such a claim, but somehow during election forecasts even experience statisticians can look at Silver's post and think that Biden winning is a sure thing.</p></div><h2> How do we arrive at P(B &gt; A)?</h2><p><br>Twitter user <a href="https://twitter.com/mbarras_ing">@mbarras_ing</a> ask a really important follow up question, asking to explain this result:</p></div></div><div data-block-json="{&quot;cache_age&quot;:&quot;3153600000&quot;,&quot;authorUrl&quot;:&quot;https://twitter.com/mbarras_ing&quot;,&quot;width&quot;:550,&quot;height&quot;:null,&quot;hSize&quot;:null,&quot;resolveObject&quot;:&quot;Tweet&quot;,&quot;html&quot;:&quot;<blockquote class=\&quot;twitter-tweet\&quot;><p dir=\&quot;ltr\&quot; lang=\&quot;en\&quot;>I may be a bit slow but could you elaborate on how that is? How would one compute 0.71, from the info \&quot;A has 2 successes in 15 trials, B has 3 successes in 14 trials\&quot;?</p>\u2014 Matthew Rhys Barras (@mbarras_ing) <a href=\&quot;https://twitter.com/mbarras_ing/status/1293928326579589121?ref_src=twsrc%5Etfw\&quot;>August 13, 2020</a></blockquote>\n<script async=\&quot;\&quot; src=\&quot;https://platform.twitter.com/widgets.js\&quot; charset=\&quot;utf-8\&quot;></script>&quot;,&quot;url&quot;:&quot;https://twitter.com/mbarras_ing/status/1293928326579589121&quot;,&quot;resolvedBy&quot;:&quot;twitter&quot;,&quot;floatDir&quot;:null,&quot;authorName&quot;:&quot;Matthew Rhys Barras&quot;,&quot;version&quot;:&quot;1.0&quot;,&quot;resolved&quot;:true,&quot;type&quot;:&quot;rich&quot;,&quot;providerName&quot;:&quot;Twitter&quot;,&quot;providerUrl&quot;:&quot;https://twitter.com&quot;}" data-block-type="22" id="block-yui_3_17_2_1_1597610689295_7791"><div><blockquote><p dir="ltr" lang="en">I may be a bit slow but could you elaborate on how that is? How would one compute 0.71, from the info "A has 2 successes in 15 trials, B has 3 successes in 14 trials"?</p>— Matthew Rhys Barras (@mbarras_ing) <a href="https://twitter.com/mbarras_ing/status/1293928326579589121?ref_src=twsrc%5Etfw">August 13, 2020</a></blockquote>
</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1597610689295_7855"><div><div><p>I very strongly believe that all statistics, even quick off the cuff estimates, should be reproducible and explainable.</p><p>I've written a fair bit about approaching similar problems both <a href="https://www.countbayesie.com/blog/2015/4/25/bayesian-ab-testing">on this blog</a> and <a href="https://www.amazon.com/Bayesian-Statistics-Fun-Will-Kurt/dp/1593279566">in my book</a>. The big picture is that we're going to come up with parameter estimates for the rate that A and B convert users and then compute the probability that B is greater than A. </p><p>Since we're estimating conversion rates we're going to use <a href="https://www.countbayesie.com/blog/2015/3/17/interrogating-probability-distributions">the Beta distribution</a> as the distribution of our parameter estimate. In this example I'm also assume a \(\text{Beta}(1,1)\) prior for our A and B variants.</p><p>The likelihood for A is \(\text{Beta}(2,13)\) and for B is \(\text{Beta}(3,11)\) so we can represent A and B as two random variables samples form these posteriors:</p><p>$$A \sim \text{Beta}(2+1, 13+1)$$<br>$$B \sim \text{Beta}(3+1,11+1)$$</p></div><p>We can now represent this in R, and sample from these distributions:</p></div></div><div data-block-type="23" id="block-yui_3_17_2_1_1597610689295_9136"><div><pre><span>N</span> <span>&lt;</span><span>-</span> <span>10000</span>
<span>a_samples</span> <span>&lt;</span><span>-</span> <span>rbeta</span>(<span>N</span>,<span>2</span><span>+</span><span>1</span>,<span>13</span><span>+</span><span>1</span>)
<span>b_samples</span> <span>&lt;</span><span>-</span> <span>rbeta</span>(<span>N</span>,<span>3</span><span>+</span><span>1</span>,<span>11</span><span>+</span><span>1</span>)</pre></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1597610689295_9202"><p>And finally we can look at the results of this to compute the probability that B is greater than A:</p></div><div data-block-type="23" id="block-yui_3_17_2_1_1597610689295_10612"><div><pre><span>sum</span>(<span>b_samples</span> <span>&gt;</span> <span>a_samples</span>)<span>/</span><span>N</span></pre></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1597610689295_10678"><div><p>In this case we get 0.7028 pretty close to 71% for Nate Silver’s problem.</p><h2><p>Can we solve this without R?</p></h2><p><br>This explains where we get our probabilities from, but there is an obvious question that comes up when you see this result, one raised by <a href="https://twitter.com/little_rocko">@little_rocko</a></p></div></div><div data-block-json="{&quot;cache_age&quot;:&quot;3153600000&quot;,&quot;authorUrl&quot;:&quot;https://twitter.com/little_rocko&quot;,&quot;width&quot;:550,&quot;height&quot;:null,&quot;hSize&quot;:null,&quot;resolveObject&quot;:&quot;Tweet&quot;,&quot;html&quot;:&quot;<blockquote class=\&quot;twitter-tweet\&quot;><p dir=\&quot;ltr\&quot; lang=\&quot;en\&quot;>this is an awesome example. is there an easy way (as in non-brute force) to finding the beta parameters that'll match a probability?</p>\u2014 Rocko (@little_rocko) <a href=\&quot;https://twitter.com/little_rocko/status/1294938572299018242?ref_src=twsrc%5Etfw\&quot;>August 16, 2020</a></blockquote>\n<script async=\&quot;\&quot; src=\&quot;https://platform.twitter.com/widgets.js\&quot; charset=\&quot;utf-8\&quot;></script>&quot;,&quot;url&quot;:&quot;https://twitter.com/little_rocko/status/1294938572299018242&quot;,&quot;resolvedBy&quot;:&quot;twitter&quot;,&quot;floatDir&quot;:null,&quot;authorName&quot;:&quot;Rocko&quot;,&quot;version&quot;:&quot;1.0&quot;,&quot;resolved&quot;:true,&quot;type&quot;:&quot;rich&quot;,&quot;providerName&quot;:&quot;Twitter&quot;,&quot;providerUrl&quot;:&quot;https://twitter.com&quot;}" data-block-type="22" id="block-yui_3_17_2_1_1597610689295_12117"><div><blockquote><p dir="ltr" lang="en">this is an awesome example. is there an easy way (as in non-brute force) to finding the beta parameters that'll match a probability?</p>— Rocko (@little_rocko) <a href="https://twitter.com/little_rocko/status/1294938572299018242?ref_src=twsrc%5Etfw">August 16, 2020</a></blockquote>
</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1597610689295_12181"><div><div><p>This question is interesting because it asks two questions that don't necessarily have to be related:</p><p>- is there an easy way to solve this<br>- is there a non-"brute force" way solve this</p><p>Before moving on I want to mention that this little snippet of R involves a lot more abstraction then it seems at first glance. What we are really doing here is equalivant to using a Monte-Carlo simulation to integrate over the distribution of the difference between two Beta distributed random variables. After the next two sections it will be more clear that what's happening here is a surprisingly sophisticated operation that is, in my opinion, the easiest method of solving this problem as well as not truly a brute force solution.</p></div><h2><br>Analytic versus Easy</h2><div><p>When we see computational solutions to mathematical problems our first instinct is typically to feel that we are avoiding solving the problem <em>analytically.</em> An analytical solution is one that uses mathematical analysis to find a closed form solution.</p><p>A strivial example, suppose I wanted to find the value that minimized \(f(x) = (x+3)^2\)</p><p>In R I could brute force this by looking over a range of answers like this:</p></div></div></div><div data-block-type="23" id="block-yui_3_17_2_1_1597610689295_13626"><div><pre><span>f</span> <span>&lt;</span><span>-</span> <span>function</span>(<span>x</span>){  
  (<span>x</span><span>+</span><span>3</span>)<span>^</span><span>2</span>
}
<span>xs</span> <span>&lt;</span><span>-</span> <span>seq</span>(<span>-</span><span>6</span>,<span>6</span>,<span>by</span><span>=</span><span>0.031</span>)
<span>xs</span>[<span>which</span>.<span>min</span>(<span>f</span>(<span>xs</span>))]</pre></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1597610689295_13692"><div><div><p>As expected we get our answer of -3, but this solution takes a bit of work: we need to know how to code and we need a computer. It's also a bit messy because if we had iterated by an incredment that didn't include -3 exactly (say by 0.031) we would not get the exact answer.</p><p>If we know some basic calculus we know that our minimum has to be where the derivative is at 0. We can very easily work out that</p><p>$$f'(x) = 2(x+3)$$<br>And that</p><p>$$2(x + 3) = 0 $$</p><p>When</p><p>$$ x = -3 $$</p><p>Knowning basic calculus this later solution becomes much easier. </p><p>But even with the calculus is part is hard, often solving it once makes future solutions much easier. Take for example if you wanted to find the maximum likelihood for a normal distribution with a mean of \(\mu\) and standard deviation of \(\sigma\)</p><p>To solve this we start with our PDF for the normal distribution \(\varphi\):</p><p>$$\varphi(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$</p><p>Now computing the derivative of this is not necessarily "easy" but it's certainly something we can do. All we really care about is when</p><p>$$\varphi'(x) = 0$$</p><p>Which we can find out happens when (computing the deriviative of course is left as an exercise for the reader):</p><p>$$\frac{\mu-x}{\sigma}$$</p></div><div><p>This allows us to reallize the amazing fact that for any normal distribution we come across, we know that the maximum likelihood estimate is the sample is when \(x = \mu\)!</p><p>Even though our calculus might take us a bit of work, once this is done the problem of doing maximum likelihood estimation for any Normal distribution truly does become easy!</p></div><h3><br>Proposing an Analytic solution to our problem</h3><div><p>Let's revisit our original problem this time attempting to find an analytic solution. This is a very interesting case because arguably this is the simplest Bayesian hypothesis test you can imagine.</p><p>Recall that we have two random variable representing our beliefs in each test. These are distributed according the the posterior which we described earlier.</p><p>$$A \sim \text{Beta}(2+1, 13+1)$$<br>$$B \sim \text{Beta}(3+1,11+1)$$</p><p>Here is where I skipped some steps in reasoning. What we want to know is:</p><p>$$P(B &gt;A)$$</p></div><div><p>Which is not expressed in a particularly useful mathematical way. A better way to solve this is to consider this as the sum (or difference in this case) of two random variables. What we really want to know is:</p><p>$$P(B - A &gt; 0)$$</p><p>In order to solve this problem we can think of a new random variable \(X\) which is going to be the difference between B and A:</p><p>$$X = B - A$$</p><p>Finally we'll suppose we have a probability density function for \(X\) we'll call \(\text{pdf}_X\). If we know \(\text{pdf}_X\) our solution is pretty close, we just need to integrate between 0 and the max domain of this distribution:</p><p>$$P(B &gt; A) = P(B - A &gt; 0) = \int_{x=0}^{\text{max}}\text{pdf}_{X}(x)$$</p><p>Already this is starting to look a bit complicated, but there's one big problem ahead. Unlike Normally distributed random variables, we have no equivalent of the Normal sum theorem (we'll cover this in a bit) for Beta distributed random variables. </p><p>What does \(\text{pdf}_X\) look like? For starters we know it's not a Beta distribution itself. We can see this because we know the domain (or support) of this distribution is not \([0,1]\). Because they are Beta distributed, A and B can both take on values from 0 to 1, which means the maximum result of this difference is 1 but the minimum is -1. So whatever this distribution is, its domain is \([-1,1]\) meaning it cannot be a Beta distribution.</p><p>We can use various rules about sum of random variables to determine the mean and variance of this distribution, but without knowing the exact form of this distribution we are unable to solve the integral analytically. </p><p>Here we can see that even in this profoundly simple problem the analytical solution is frustratingly …</p></div></div></div></div></div></div></div></div></div></article></section></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.countbayesie.com/blog/2020/8/16/why-bayesian-stats-need-monte-carlo-methods">https://www.countbayesie.com/blog/2020/8/16/why-bayesian-stats-need-monte-carlo-methods</a></em></p>]]>
            </description>
            <link>https://www.countbayesie.com/blog/2020/8/16/why-bayesian-stats-need-monte-carlo-methods</link>
            <guid isPermaLink="false">hacker-news-small-sites-24416908</guid>
            <pubDate>Wed, 09 Sep 2020 03:54:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AMD PSB Vendor Locks EPYC CPUs for Enhanced Security at a Cost]]>
            </title>
            <description>
<![CDATA[
Score 273 | Comments 166 (<a href="https://news.ycombinator.com/item?id=24416005">thread link</a>) | @virgulino
<br/>
September 8, 2020 | https://www.servethehome.com/amd-psb-vendor-locks-epyc-cpus-for-enhanced-security-at-a-cost/ | <a href="https://web.archive.org/web/*/https://www.servethehome.com/amd-psb-vendor-locks-epyc-cpus-for-enhanced-security-at-a-cost/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <!-- image --><div><figure><a href="https://www.servethehome.com/wp-content/uploads/2020/09/AMD-Platform-Secure-Boot-Feature-Cover.jpg" data-caption="AMD Platform Secure Boot Feature Cover"><img width="696" height="465" src="https://www.servethehome.com/wp-content/uploads/2020/09/AMD-Platform-Secure-Boot-Feature-Cover-696x465.jpg" srcset="https://www.servethehome.com/wp-content/uploads/2020/09/AMD-Platform-Secure-Boot-Feature-Cover-696x465.jpg 696w, https://www.servethehome.com/wp-content/uploads/2020/09/AMD-Platform-Secure-Boot-Feature-Cover-400x268.jpg 400w, https://www.servethehome.com/wp-content/uploads/2020/09/AMD-Platform-Secure-Boot-Feature-Cover-628x420.jpg 628w, https://www.servethehome.com/wp-content/uploads/2020/09/AMD-Platform-Secure-Boot-Feature-Cover.jpg 800w" sizes="(max-width: 696px) 100vw, 696px" alt="AMD Platform Secure Boot Feature Cover" title="AMD Platform Secure Boot Feature Cover"></a><figcaption>AMD Platform Secure Boot Feature Cover</figcaption></figure></div>
            <!-- content --><p>Today we are going to discuss a change to server security that is going to make waves in the home lab and secondary markets for servers and components in the future. During our recent <a href="https://www.servethehome.com/dell-emc-poweredge-c6525-review-2u4n-amd-epyc-kilo-thread-server/">Dell EMC PowerEdge C6525 review</a> we briefly mentioned that AMD EPYC CPUs in the system are vendor locked to Dell EMC systems. This is not a Dell-specific concern. We have confirmed that other vendors are supporting the feature behind this. For the large vendors, their platform security teams are pushing to build more secure platforms for their customers, and that is going to have future impacts on the secondary server market and home labs.</p>
<p>In this article, we are going to cover the basics of what is happening. We are going to discuss the motivations, and why this is going to be more common in the future. Finally, we are going to discuss what those in the industry can do to keep the secondary server market operating well. If you work with partners or resellers who dip into used parts bins or even have the potential to purchase grey market CPUs, send them this article or accompanying video. The current market has a large disconnect between what some large customers are asking for, and large vendors are delivering on and what others in the market know is happening.<span id="more-46716"></span></p>
<h2>Accompanying Video</h2>
<p>This is an important topic. To ensure that we can cover those who like to read/ skim and those who like to get information via audio, we have an accompanying video:</p>
<p><iframe title="Vendor Locking AMD EPYC CPUs Great for Security at a Cost" width="696" height="392" src="https://www.youtube.com/embed/kNVuTAVYxpM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p>
<p>Feel free to pop open that video on YouTube and check it out or to send to those who prefer not to read.</p>
<h2>Background: How we learned this was a “thing”</h2>
<p>In 2018 we did a <a href="https://www.servethehome.com/dell-emc-poweredge-r7415-review/">Dell EMC PowerEdge R7415 review</a> and as part of that review, we started our normal process of trying different CPUs in the system. Early in that process, we used an AMD EPYC 7251 CPU, the low-end 8-core model, and noticed something curious. It would not work in our other test systems after.</p>
<p>After a bit of research, we found it was because Dell EMC was vendor locking the chips to Dell systems. We did not know exactly why, but we were told was a security feature. At this point, and even to this day two years later, not every vendor takes advantage of all of the AMD EPYC security features. What that practically means is that what we saw with the Dell EMC system is not what we saw with other systems. For example, we were able to interchangeably use CPUs in Supermicro and Tyan systems, but we could not use those systems once they went into a Dell EMC server.</p>
<figure id="attachment_24514" aria-describedby="caption-attachment-24514"><a href="https://www.servethehome.com/dual-amd-epyc-7251-linux-benchmarks-least-expensive-2p-epyc/amd-epyc-7251-in-socket-and-carrier/" rel="attachment wp-att-24514"><img src="https://www.servethehome.com/wp-content/uploads/2017/09/AMD-EPYC-7251-in-Socket-and-Carrier.jpg" alt="AMD EPYC In Socket And Carrier" width="800" height="533" srcset="https://www.servethehome.com/wp-content/uploads/2017/09/AMD-EPYC-7251-in-Socket-and-Carrier.jpg 800w, https://www.servethehome.com/wp-content/uploads/2017/09/AMD-EPYC-7251-in-Socket-and-Carrier-400x267.jpg 400w, https://www.servethehome.com/wp-content/uploads/2017/09/AMD-EPYC-7251-in-Socket-and-Carrier-696x464.jpg 696w, https://www.servethehome.com/wp-content/uploads/2017/09/AMD-EPYC-7251-in-Socket-and-Carrier-630x420.jpg 630w" sizes="(max-width: 800px) 100vw, 800px"></a><figcaption id="caption-attachment-24514">AMD EPYC In Socket And Carrier</figcaption></figure>
<p>We found we were not alone. Laboratories, VARs, and other organizations were finding that transferring AMD EPYC CPUs from one vendor’s system to another, was not the simple process it was on the Intel Xeon side. It did not always work.</p>
<p>We knew it was a security feature and thought that most who are buying servers would be informed of this by their sales reps or channel partners. After I personally got a lot of texts, e-mails, instant messaging, and comments on our C6525 video and article, I realized that this actually may be a situation where many people do not know what is going on.</p>
<figure id="attachment_46531" aria-describedby="caption-attachment-46531"><a href="https://www.servethehome.com/dell-emc-poweredge-c6525-review-2u4n-amd-epyc-kilo-thread-server/dell-emc-poweredge-c6525-internal-view-nodes-partially-out/" rel="attachment wp-att-46531"><img src="https://www.servethehome.com/wp-content/uploads/2020/08/Dell-EMC-PowerEdge-C6525-Internal-View-Nodes-Partially-Out.jpg" alt="Dell EMC PowerEdge C6525 Internal View Nodes Partially Out" width="800" height="519" srcset="https://www.servethehome.com/wp-content/uploads/2020/08/Dell-EMC-PowerEdge-C6525-Internal-View-Nodes-Partially-Out.jpg 800w, https://www.servethehome.com/wp-content/uploads/2020/08/Dell-EMC-PowerEdge-C6525-Internal-View-Nodes-Partially-Out-400x260.jpg 400w, https://www.servethehome.com/wp-content/uploads/2020/08/Dell-EMC-PowerEdge-C6525-Internal-View-Nodes-Partially-Out-696x452.jpg 696w, https://www.servethehome.com/wp-content/uploads/2020/08/Dell-EMC-PowerEdge-C6525-Internal-View-Nodes-Partially-Out-647x420.jpg 647w" sizes="(max-width: 800px) 100vw, 800px"></a><figcaption id="caption-attachment-46531">Dell EMC PowerEdge C6525 Internal View Nodes Partially Out</figcaption></figure>
<p>This experience that we had, apparently is one that is not overly common yet. That makes sense because the systems that utilize the enhanced security levels are still largely new, and being used by their first buyers. Also, AMD still has a smaller market share than Intel. A big reason, by the way, that Intel Xeon does not have this issue is that they do not have the security feature that AMD has. Vendors have come out and stated that their AMD EPYC systems are more secure than their Intel Xeon systems, and this behavior is a byproduct of that enhanced security.</p>
<p>Next, we are going to dive into the feature of AMD processors (and what will be more common in future CPUs from other vendors.)</p>
<h2>AMD EPYC Secure Processor Platform Secure Boot (PSB)</h2>
<p>Let us start with the high-level slide. This is effectively the same slide on the AMD Secure Processor that we saw with the AMD EPYC 7001 series launch, but this is from the EPYC 7002 series. AMD EPYC CPUs may be x86, but they have an embedded Arm Cortex-A5 microcontroller that runs its own OS that is independent of the main system. This AMD Secure Processor is the backbone of AMD’s security push as it provides features such as key management and hardware root of trust for the platform.</p>
<figure id="attachment_36705" aria-describedby="caption-attachment-36705"><a href="https://www.servethehome.com/amd-epyc-7002-series-rome-delivers-a-knockout/amd-epyc-7002-platform-secure-processor/" rel="attachment wp-att-36705"><img src="https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Processor.jpg" alt="AMD EPYC 7002 Platform Secure Processor" width="1792" height="918" srcset="https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Processor.jpg 1792w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Processor-400x205.jpg 400w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Processor-800x410.jpg 800w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Processor-696x357.jpg 696w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Processor-1068x547.jpg 1068w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Processor-820x420.jpg 820w" sizes="(max-width: 1792px) 100vw, 1792px"></a><figcaption id="caption-attachment-36705">AMD EPYC 7002 Platform Secure Processor</figcaption></figure>
<p>AMD spends time <a href="https://www.servethehome.com/amd-confirms-cts-labs-exploits-requiring-admin-access/">patching this solution</a>&nbsp;to make it more secure, but it is generally fairly hard to reach without some extremely low-level access in a system. We are going to come back to the “Enables hardware validated boot” line shortly, but it is important to understand that this secure processor underpins many of AMD’s best security features.</p>
<p>For example, at STH we use EPYC’s Secure Memory Encryption and Secure Encrypted Virtualization heavily. With AMD EPYC, we do not have to manually manage keys. Instead, the ephemeral keys are managed for us by the AMD Secure Processor. This is the basis for what is really the building wave of confidential computing offerings such as&nbsp;<a href="https://www.servethehome.com/google-cloud-confidential-computing-enabled-by-amd-epyc-sev/">Google Cloud Confidential Computing Enabled by AMD EPYC SEV</a>. Intel has its secure boot features and SGX that will be enhanced greatly with <a href="https://www.servethehome.com/the-2021-intel-ice-pickle-how-2021-will-be-crunch-time/">Ice Lake Xeons</a>, but for now, AMD has this capability while Intel does not. When big vendors say AMD is more secure, the AMD Secure Processor is a cornerstone of those offerings.</p>
<figure id="attachment_36704" aria-describedby="caption-attachment-36704"><a href="https://www.servethehome.com/amd-epyc-7002-series-rome-delivers-a-knockout/amd-epyc-7002-platform-secure-memory-encryption/" rel="attachment wp-att-36704"><img src="https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Memory-Encryption.jpg" alt="AMD EPYC 7002 Platform Secure Memory Encryption" width="1769" height="890" srcset="https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Memory-Encryption.jpg 1769w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Memory-Encryption-400x201.jpg 400w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Memory-Encryption-800x402.jpg 800w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Memory-Encryption-696x350.jpg 696w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Memory-Encryption-1068x537.jpg 1068w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Memory-Encryption-835x420.jpg 835w" sizes="(max-width: 1769px) 100vw, 1769px"></a><figcaption id="caption-attachment-36704">AMD EPYC 7002 Platform Secure Memory Encryption</figcaption></figure>
<p>Let us discuss that “Enables hardware validated boot” line. While traditionally CPUs just fire up in whatever platform they are in, AMD has intelligence in their CPU due to the Arm-based AMD Secure Processor. EPYC CPUs are designed to be a bit more intelligent about the platforms they are in, and interact with server platform security to act as this root of trust that would not be possible if they effectively just booted up in any system.</p>
<p>Here is a statement from AMD describing the AMD Platform Secure Boot.</p>
<p><em>The AMD Platform Secure Boot Feature (PSB) is a mitigation for firmware Advanced Persistent Threats. It is a defense-in-depth feature. PSB extends AMD’s silicon root of trust to protect the OEM’s BIOS.&nbsp; This allows the OEM to establish an unbroken chain of trust from AMD’s silicon root of trust to the OEM’s BIOS using PSB, and then from the OEM’s BIOS to the OS Bootloader using UEFI secure boot. This provides a very powerful defense against remote attackers seeking to embed malware into a platform’s firmware.</em></p>
<p><em>An OEM who trusts only their own cryptographically signed BIOS code to run on their platforms will use a PSB enabled motherboard and set one-time-programmable fuses in the processor to bind the processor to the OEM’s firmware code signing key. AMD processors are shipped unlocked from the factory, and can initially be used with any OEM’s motherboard. But once they are used with a motherboard with PSB enabled, the security fuses will be set, and from that point on, that processor can only be used with motherboards that use the same code signing key. (<strong>Source</strong>: AMD statement to STH)</em></p>
<p>That is a lot to take in. We asked HPE about this. Their response mirrored what the above was describing. HPE firmware, when a system is first turned on, performs this binding process where the AMD EPYC CPU expects to see HPE signed firmware. If you alter the HPE firmware on the system, the check fails and the system will not work. That means if your HPE motherboard fails, you can replace it and put your CPU in another HPE motherboard with signed HPE firmware. It also means if the server platform’s firmware is not signed by HPE, the processor will see it as evidence of tampering and not work.</p>
<p><strong>Edit: 2020-09-09</strong> – HPE clarified that they are doing this in a different manner than Dell after initially confirming that they were using the AMD PSB feature. After this went live, HPE sent us the following:</p>
<p><em>HPE does not use the same security technique that Dell is using for a BIOS hardware root of trust. HPE does not burn, fuse, or permanently store our public key into AMD processors which ship with our products. HPE uses a unique approach to authenticate our BIOS and BMC firmware: HPE fuses our hardware – or silicon – root of trust into our own BMC silicon to ensure only authenticated firmware is executed.&nbsp; Thus, while we implement a hardware root of trust for our BIOS and BMC firmware, the processors that ship with our servers are not locked to our platforms. (<strong>Source</strong>: HPE)</em></p>
<p>What is at least interesting there is that HPE was initially claiming feature parity with Dell to us, and from the comments on this article were saying they used this feature in sales pitches, but now are saying they are not blowing the eFuses.</p>
<figure id="attachment_39258" aria-describedby="caption-attachment-39258"><a href="https://www.servethehome.com/pcie-gen4-hpe-proliant-dl325-gen10-plus-and-dl385-gen10-plus-amd-epyc-7002/hpe-proliant-dl325-gen10-plus-at-sc19-cpu-cover/" rel="attachment wp-att-39258"><img src="https://www.servethehome.com/wp-content/uploads/2019/11/HPE-ProLiant-DL325-Gen10-Plus-at-SC19-CPU-Cover.jpg" alt="HPE ProLiant DL325 Gen10 Plus At SC19 CPU Cover" width="800" height="600" srcset="https://www.servethehome.com/wp-content/uploads/2019/11/HPE-ProLiant-DL325-Gen10-Plus-at-SC19-CPU-Cover.jpg 800w, https://www.servethehome.com/wp-content/uploads/2019/11/HPE-ProLiant-DL325-Gen10-Plus-at-SC19-CPU-Cover-400x300.jpg 400w, https://www.servethehome.com/wp-content/uploads/2019/11/HPE-ProLiant-DL325-Gen10-Plus-at-SC19-CPU-Cover-80x60.jpg 80w, https://www.servethehome.com/wp-content/uploads/2019/11/HPE-ProLiant-DL325-Gen10-Plus-at-SC19-CPU-Cover-265x198.jpg 265w, https://www.servethehome.com/wp-content/uploads/2019/11/HPE-ProLiant-DL325-Gen10-Plus-at-SC19-CPU-Cover-696x522.jpg 696w, https://www.servethehome.com/wp-content/uploads/2019/11/HPE-ProLiant-DL325-Gen10-Plus-at-SC19-CPU-Cover-560x420.jpg 560w" sizes="(max-width: 800px) 100vw, 800px"></a><figcaption id="caption-attachment-39258">HPE ProLiant DL325 Gen10 Plus At SC19 CPU Cover</figcaption></figure>
<p>Here is where the concern develops, and not necessarily for AMD, the OEM, or most of the initial customer base. Customers want more security. The OEMs want to create a secure hardware environment because that is what their customers want. AMD is implementing an advanced security solution beyond what Intel Xeons have giving the OEMs and end-customers what they want. Effectively, when these are sold as new systems, this is exactly what everyone involved wants.</p>
<p>If everyone is getting what they want, then where is the concern, that is what we are going to cover next.</p>
        </div></div>]]>
            </description>
            <link>https://www.servethehome.com/amd-psb-vendor-locks-epyc-cpus-for-enhanced-security-at-a-cost/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24416005</guid>
            <pubDate>Wed, 09 Sep 2020 02:02:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Is the web getting slower?]]>
            </title>
            <description>
<![CDATA[
Score 223 | Comments 202 (<a href="https://news.ycombinator.com/item?id=24413705">thread link</a>) | @oedmarap
<br/>
September 8, 2020 | https://www.debugbear.com/blog/is-the-web-getting-slower | <a href="https://web.archive.org/web/*/https://www.debugbear.com/blog/is-the-web-getting-slower">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  <div>
    <div>
    
      
      

      

      <div>
        
        

        <p>A story on Hacker News recently argued that webpage speeds haven't improved, even as internet speeds have gone up.</p>
<p>This article explains why that conclusion can't be drawn from the original data.</p>
<p>We'll also look at how devices and the web have changed over the past 10 years, and what those changes have meant for web performance.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/hn-article.png" alt="Webpage speeds article on Hacker News"></p>
<ol>
<li><a href="#interpreting-the-http-archive-data">Interpreting the HTTP Archive data</a></li>
<li><a href="#how-have-mobile-networks-and-devices-changed-over-the-last-10-years">How have mobile networks and devices changed over the last 10 years?</a></li>
<li><a href="#how-have-websites-changed">How have websites changed?</a></li>
<li><a href="#data-from-the-chrome-user-experience-report">Data from the Chrome User Experience Report</a></li>
<li><a href="#modelling-page-load-times">Modelling page load times</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>
<h2 id="interpreting-the-http-archive-data">Interpreting the HTTP Archive data</h2>
<p>This chart from the <a href="https://www.nngroup.com/articles/the-need-for-speed/">Nielsen Norman Group article</a> suggested that increasing mobile network bandwidth hasn't resulted in faster page load times.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/nngroup-mobile-webperf-chart.png" alt="Chart showing increasing bandwidth along increasing page load times"></p>
<p>However, <strong>the connection speed used by HTTP Archive has not actually increased over time.</strong></p>
<p>Instead it went down in 2013, <a href="https://httparchive.org/faq#what-changes-have-been-made-to-the-test-environment-that-might-affect-the-data">switching from wifi to an emulated 3G connection</a>.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/nngroup-mobile-webperf-chart-annotated.png" alt="Annotation for page loads time showing when methodology changed"></p>
<p>The onLoad metric has increased 55% since 2013, from 12.7s to 19.7s. If you bought a phone in 2013 and have been on a 3G connection ever since, then the web has become slower for you.</p>
<p>Before looking at how devices and the web have changed over the last 10 years, here are a few notes on how to think about this data.</p>
<h3 id="why-look-at-on-load">Why look at onLoad?</h3>
<p>The <code>load</code> event is emitted by the page when all page resources like scripts or images have been downloaded.</p>
<p>If the top of a page renders quickly, but the page also loads 20 images further down, then the onLoad metric will suggest that the page is slow.</p>
<p>A different page might not initially render anything useful at all, and only start loading additional resources and rendering content long after the onLoad event. Yet this page will appear fast.</p>
<p>As a result, onLoad doesn't do a good job measuring whether a user experiences the page as fast.</p>
<p>So why do we even look at this metric? <strong>Because it's been around for a long time</strong>, and HTTP Archive has been tracking it since 2010. Newer metrics like <a href="https://www.debugbear.com/docs/metrics/first-contentful-paint">First Contentful Paint</a> or Time to Interactive were only added to HTTP Archive in 2017.</p>
<h3 id="should-we-expect-increasing-bandwidth-to-result-in-faster-page-load-times">Should we expect increasing bandwidth to result in faster page load times?</h3>
<p>Increasing bandwidth will only make a page load faster if bandwidth is the bottleneck at some point. It won't help if you're on a Gigabit connection with a 1s network roundtrip time.</p>
<p>However, the 1.6Mbps 3G connection emulated by HTTP Archive is very slow, so we should expect significant performance improvements as bandwidth improves. The average website downloads 1.7MB of data in 2020, which will take at least 9s to download on the HTTP Archive connection.</p>
<h3 id="some-more-http-archive-caveats">Some more HTTP Archive caveats</h3>
<p>I'll talk a lot about "the average website" in this article. It's worth noting that HTTP Archive only collects data on homepages, not pages deeper down in the site. The corpus of tested domains has also grown over time.</p>
<p>The tests weren't always run on the same device. Initially a physical iPhone 4 was used, today the tests are run on an emulated Android device.</p>
<p>We'll look at median metric values in this article. If most websites are fast but one in five websites freeze your phone for 20s we won't be able to pick this up.</p>
<h3 id="performance-on-desktop">Performance on desktop</h3>
<p>This article will focus on mobile performance in the US. However, if you're looking at the desktop data from the original article, it's worth noting that the test bandwidth was increased and latency was reduced in 2013.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/desktop-performance.png" alt="Chart showing desktop connection speeds and when emulated connection speed changed"></p>
<h2 id="how-have-mobile-networks-and-devices-changed-over-the-last-10-years">How have mobile networks and devices changed over the last 10 years?</h2>
<p>Let's look at 4 factors:</p>
<ul>
<li>network bandwidth</li>
<li>network latency</li>
<li>processor speeds</li>
<li>browser performance</li>
</ul>
<h3 id="mobile-network-bandwidth-in-the-us">Mobile network bandwidth in the US</h3>
<p>This chart shows average mobile bandwidth in the US by year, according to different sources. It increased from 1 Mbps to around 30 Mbps.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/us-mobile-bandwidth.png" alt="Mobile bandwidth in the US by year"></p>
<p>(I've not been very careful when collecting this data. For example, I didn't consistently distinguish when data was collected from when it was published. <a href="https://docs.google.com/spreadsheets/d/1ifZ_ngADpT3YzezNQLpXKCsr74-BvaBJUkYm6PGpv1g/edit?usp=sharing">You can find my sources here</a>.)</p>
<h3 id="mobile-network-latency-in-the-us">Mobile network latency in the US</h3>
<p>This was harder to find data on, but the results indicate that latency dropped from around 200ms in 2011 to 50ms in 2020.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/us-mobile-latency.png" alt="Mobile latency in the US by year, going down from 200ms in 2011 to 50ms in 2020"></p>
<h3 id="mobile-device-cpu-speeds">Mobile device CPU speeds</h3>
<p>I've not been able to find data on average mobile device speeds in the US. But <a href="https://infrequently.org/">Alex Russel</a> and <a href="https://surma.dev/">Surma</a> have published a <a href="https://twitter.com/slightlylate/status/1233275220275818498">chart showing GeekBench 4 scores alongside the release years of different phones</a>.</p>
<p>Even budget phones have become 4x faster, with iPhones now being up to 20x more powerful.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/mobile-cpu-benchmark.jpeg" alt="Mobile CPU performance over time"></p>
<h3 id="how-have-browsers-changed">How have browsers changed?</h3>
<p>A lot of work has been done on browsers over the last 10 years. JavaScript has become a larger part of the web, so many improvements have focussed here.</p>
<p>Looking at <a href="https://v8.dev/blog/10-years">this chart from the V8 blog</a>, page CPU usage for gone down by a factor of 4.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/v8-performance.png" alt="V8 Speedometer 1 benchmark results 2013 to 2018"></p>
<h4 id="networking">Networking</h4>
<p>Browser networking has also improved, for example with the introduction of HTTP/2 in 2015. 64% of requests are now served over HTTP/2.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/http2.png" alt="HTTP/2 adoption over time"></p>
<h2 id="how-have-websites-changed">How have websites changed?</h2>
<p>Let's look at some data from HTTP Archive to see how websites have changed.</p>
<h3 id="page-weight">Page weight</h3>
<p><a href="https://httparchive.org/reports/page-weight">Mobile page weight</a> increased by 337% between 2013 and 2020. This is primarily driven by an increase in images and JavaScript code.</p>
<p>Other resources also increased a lot –&nbsp;I suspect these are mostly videos.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/page-weight.png" alt="Page weight by resource type over time"></p>
<p>The chart starts in 2013 as HTTP Archive changed its methodology in October 2012. Before page weight was undercounted, as the test stopped when the page load event was triggered, even if more data was still being loaded.</p>
<h3 id="java-script-execution-time">JavaScript execution time</h3>
<p>JavaScript would be the most likely culprit if pages are getting slower despite faster mobile networks. Unfortunately, HTTP Archive only started collecting this data in late 2017, and it seems to have been mostly stable since then.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/javascript-execution-time.png" alt="HTTP Archive JavaScript execution time chart"></p>
<p>The drop in mid-2018 can probably be attributed to a URL corpus change.</p>
<p>Note that the absolute run duration (0.5s) is less than what you'd normally see in a tool like Lighthouse. These tools normally slow down JavaScript execution to emulate a mobile device, but <a href="https://almanac.httparchive.org/en/2019/methodology#lighthouse">this was broken for the HTTP Archive tests</a>. So while this number might be realistic for mid-range phones, a common assumption is that budget phones are around 4x slower.</p>
<h2 id="answering-whether-the-web-has-become-slower">Answering whether the web has become slower</h2>
<p>Has the web become slower? Well, it depends on what your device, network connection, and most-used websites are.</p>
<p>We'd need to weigh real-world performance data to get a distribution that shows how different users experienced the web over time. And should the experience of someone opening thousands of pages a day count as much as someone who only visits Facebook once a week?</p>
<p>I don't have detailed per-user data, but we can take a look at the question in a few different ways:</p>
<ol>
<li>Real-user data from the <a href="https://developers.google.com/web/tools/chrome-user-experience-report">Chrome UX Report (CrUX)</a></li>
<li>Naive modelling based on how websites and devices have changed</li>
</ol>
<p>I also tried downloading old page versions from archive.org and testing them with Lighthouse, but wasn't able to get meaningful results in the time I had available. For example, often some images are missing from the page archive.</p>
<h2 id="data-from-the-chrome-user-experience-report">Data from the Chrome User Experience Report</h2>
<p>The big limitation of CrUX data is that it's only been collected since late 2017. But we can still use it to see if the web has become slower in the last two and a half years.</p>
<p>Note that, unlike HTTP Archive, CrUX looks at the whole domain instead of just homepages.</p>
<p>The data we'll look at is the 75th percentile, meaning pages load at least this fast for 75% of users.</p>
<p>(I'm taking the average across websites rather than the median, which is not great.)</p>
<h3 id="us-page-load-times">US page load times</h3>
<p>CrUX data for the US does not show page performance getting worse.</p>
<p>The onLoad metric shows a slight improvement, maybe due to an increase in bandwidth. Or maybe more activity is now happening after the initial page load.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/crux-us.png" alt="Page load speeds in the US"></p>
<p>The paint metrics seem fairly stable. Largest Contentful Paint is a new metric that has only been collected since mid-2019.</p>
<h3 id="the-rest-of-the-world">The rest of the world</h3>
<p>The downward trend in the US onLoad metric is matched by the global data. There are however signifianct differences in page load times across countries, with onLoad timings in India being almost twice those in South Korea.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/crux-global.png" alt="Page load speeds globally, in the US, UK, Korea, and India"></p>
<p>We can use CrUX data to put HTTP Archive data into perspective. In January 2020 HTTP Archive reported a median (50% percentile) load time of 18.7s, based on its synthetic data.</p>
<p>In contrast, CrUX suggests a load time of just 5.8s –&nbsp;and this is the 75th percentile.</p>
<p>(Note that the Global values here just take an average and are not weighed by population.)</p>
<h2 id="modelling-page-load-times">Modelling page load times</h2>
<p>We can create a theoretical model of how changes in devices, networks, and websites might affect overall performance.</p>
<p>This won't be a great model, but hopefully it will still provide some insight.</p>
<h3 id="theoretical-page-download-time">Theoretical page download time</h3>
<p>Page weight has increased over time, but so has bandwidth. Round-trip latency has also gone down.</p>
<p>Downloading a file the size of the median mobile website would have taken 1.7s in 2013. If your connection hasn't improved since then downloading this much data would now take 4.4s. But with an average connection today it would only take 0.9s.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/minimum-page-download-time.png" alt="TCP download time"></p>
<p>In practice, a website wouldn't consist of just a single request, and other factors like CPU processing or server latency would also affect how quickly the page loads. The onLoad times reported by HTTP Archive are 2-3 times this lower bound.</p>
<p>But we can still use this as an indicator that reduced latency and increased bandwidth have helped make websites load faster overall.</p>
<p>(I'm starting in 2013 rather than 2011, as the HTTP Archive page weight metric has only been measured consistently since then.)</p>
<h3 id="cpu">CPU</h3>
<p>I'm not quite sure how to think about this, but I'll make some guesses anyway.</p>
<p>Someone who used a Galaxy S4 in 2013 and now uses a Galaxy S10 will have seen their CPU processing power go up by a factor of 5. Let's assume that browsers have become 4x more efficient since then. If we naively multiply these numbers we get an overall 20x improvement.</p>
<p>Since 2013, JavaScript page weight has increased 3.7x from 107KB to 392KB. Maybe minification and compression have improved a bit …</p></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.debugbear.com/blog/is-the-web-getting-slower">https://www.debugbear.com/blog/is-the-web-getting-slower</a></em></p>]]>
            </description>
            <link>https://www.debugbear.com/blog/is-the-web-getting-slower</link>
            <guid isPermaLink="false">hacker-news-small-sites-24413705</guid>
            <pubDate>Tue, 08 Sep 2020 21:33:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Non-Posix File Systems]]>
            </title>
            <description>
<![CDATA[
Score 342 | Comments 126 (<a href="https://news.ycombinator.com/item?id=24412970">thread link</a>) | @nsm
<br/>
September 8, 2020 | https://weinholt.se/articles/non-posix-filesystems/ | <a href="https://web.archive.org/web/*/https://weinholt.se/articles/non-posix-filesystems/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
      <div>
        <article>
          <section><p>Operating systems and file systems have traditionally been developed
hand in hand. They impose mutual constraints on each other. Today we
have two major leaders in file system semantics: Windows and <span>POSIX</span>.
They are very close to each other when compared to the full set of
possibilities. Interesting things happened before POSIX monopolized
file system&nbsp;semantics.</p>

<p>When you use a file system through a library instead of going through
the operating system there are some extra possibilities. You are no
longer required to obey the host operating system’s semantics for
filenames. You get to decide if you use <code>/</code> or <code>\</code> to separate
directory components (or something else altogether). Maybe you don’t
even use strings for filenames. The <a href="https://gitlab.com/weinholt/fs-fatfs">fs-fatfs</a> library uses
a list of strings, so it’s up to the caller to define a directory
separator for themselves. While working on that library, I was driven
to write down some ideas that I’ve previously run across and found&nbsp;inspirational.</p>

<p>The very first hierarchical file system was developed for Multics. It
is described in the
paper
<a href="https://www.multicians.org/fjcc4.html">A General-Purpose File System For Secondary Storage</a> (1965)
by Robert C. Daley and Peter G. Neumann. There are several things that I find
astounding about this&nbsp;paper:</p>
<ul>
<li>There were apparently no hierarchical file systems before Multics.
The references do no cite any previous work on this and I haven’t
found&nbsp;any.</li>
<li>Privacy in computing was a big consideration even back&nbsp;then.</li>
<li>Modern hierarchical file system are essentially unchanged from 1965,
but some features disappeared along the&nbsp;way.</li>
<li>There’s a <em>very</em> interesting backup system described in this&nbsp;paper.</li>
</ul>
<p>The paper describes these concepts that we still use&nbsp;today:</p>
<ul>
<li>Files.</li>
<li>Directories.</li>
<li>Root&nbsp;directory.</li>
<li>Working&nbsp;directory.</li>
<li>Tree name, i.e. the name of a file or directory in the tree
structure. Akin to&nbsp;realpath(3).</li>
<li>Path name, which is like a tree name, but it is conceptually
different because components can be links. The paper used <code>":"</code>
instead of <code>"/"</code> to separate&nbsp;directories.</li>
<li>Relative paths. They used an initial <code>":"</code> in place of <code>"./"</code> (or
empty) and <code>"*"</code> in place of <code>".."</code>. Instead of <code>"cd .."</code> you used
<code>"CHANGEDIRECTORY :*"</code>.</li>
<li>Symbolic&nbsp;links.</li>
<li>Access control using access control lists (showed up relatively
recently in&nbsp;<span>POSIX</span>).</li>
<li>Read, execute and write modes on files and directories. Execute
on a directory means permission to search it, just as in&nbsp;<span>POSIX</span>.</li>
<li>Segments and segment numbers (file descriptions and file
descriptors, respectively, if I’m not&nbsp;mistaken).</li>
</ul>
<p>It is said that Unix took inspiration from Multics and simplified
things. What was lost? Whoever implemented the file system for Unix
stopped reading the paper somewhere around subsection 2.3. There are
at least these features we’re&nbsp;missing:</p>
<ul>
<li><p>Access control lists can define a <span>TRAP</span> for a user. Users can refuse
to use TRAPs and instead get an error. Otherwise TRAPs call a named
function at the time of the access. They can be used to implement
smart files that can e.g. monitor file usage or apply password-based
file&nbsp;locks.</p>
<p>(Someone will undoubtedly insert eBPF into the Linux <span>VFS</span> soon and we
will have invented TRAPs&nbsp;again).</p>
</li>
<li><p>Multics fully separated the append mode and the write mode and made
it available to users. You could actually have files and
<em>directories</em> that were writable but not appendable, or appendable
but not writable (or&nbsp;both).</p>
<p>(Append-only <em>files</em> exist on Linux through file attributes settable
by the super user or processes with <code>CAP_LINUX_IMMUTABLE</code>. This is
very weak sauce. Immutable directories exist, but not append-only
directories. And of course there is no way to have a writable file
or directory that can’t&nbsp;grow).</p>
</li>
<li>Directory entries that point to secondary storage. <em>This is a game
changer for file system management.</em> More on this&nbsp;below.</li>
</ul>
<h2 id="multiple-level-storage-and-backups">Multiple-level storage and&nbsp;backups</h2>
<p>Files in the Multics file system could have directory entries that
point to a secondary storage medium. At the time the secondary media
were tapes, but today they could be mechanical hard drives, <span>USB</span> drives
or network servers. But what is the point of having a file that isn’t
there? It’s really quite clever and it only gets more clever the more
you look at&nbsp;it.</p>
<p><strong>When a Multics system was running out of disk space it could remove
unused files from the primary medium, but keep the directory entries.</strong>
If you then tried to open such a file it would ask you to wait while
the file is being made available. It would issue instructions to the
operator (perhaps later a tape robot) to have them retrieve the
required tapes for restoring the file. Very useful when disks were
small and tapes were&nbsp;plentiful.</p>
<p>But it gets even better when you consider backups. Today if you need
to restore a Linux system from backups you do it this way: find the
oldest full backup and restore it, then restore each incremental
backup, going from oldest to newest. This means that the system can’t
be used until you’ve completely restored it from the backups. With
today’s disk sizes that could take a very, very long&nbsp;time.</p>
<p>Multics backups are done differently. To restore a completely hosed
system you first restore the system files, which will allow you to
boot. Then you restore the <em>latest</em> incremental backup. After this the
file system will contain the most current directory entries. <strong>Just
restore one incremental backup tape and all the files are already in
their right places. This is <em>much</em> faster than waiting for everything
to be&nbsp;restored!</strong></p>
<p>Why is only the latest incremental backup required to get everything
back in its place and working? The clever part is that the files might
not yet be on the disk. In fact, most files will probably be on
another backup medium. But the most recently used files have been
restored, so you can most likely do useful work already, and all other
files have their directory entries. If you try to open a file that
hasn’t been restored then you’re asked to wait and the operator is
told which tape to put in to get the file&nbsp;back.</p>
<p>Is something like this offered on <em>any</em> <span>POSIX</span>-compatible file&nbsp;system?</p>

<p>The Xerox Alto was also a system of many firsts. It is most famous for
having the first windowing <span>GUI</span>, which inspired Apple’s design, which
then inspired Microsoft’s design. Of course it had a hierarchical file
system with version numbers, and even a network file&nbsp;system.</p>
<p>I would like to highlight the interesting disk structure of the Alto
file system, which is described in the
paper
<a href="https://www.microsoft.com/en-us/research/publication/an-open-operating-system-for-a-single-user-machine/">An Open Operating System for a Single-User Machine</a> (1979)
by Butler Lampson and Robert F.&nbsp;Sproull.</p>
<p>The designers of the system were concerned about data loss caused by
hardware issues and buggy software. They came up with a feature to
protect the user’s data against loss of any block on the disk. (Unless
the user data was in that lost block, of&nbsp;course).</p>
<p>All blocks on the disk have a label that contains these&nbsp;fields:</p>
<ul>
<li>A file&nbsp;identifier</li>
<li>A version&nbsp;number</li>
<li>A page number (block&nbsp;index)</li>
<li>A&nbsp;length</li>
<li>A next link (pointer to the next block in the&nbsp;file)</li>
<li>A previous&nbsp;link</li>
</ul>
<p>The label means that each block is self-identifying. By my count this
type of disk had an additional 14 bytes of label per each 512 byte&nbsp;block.</p>
<p><strong>If the disk structure is damaged then a special program called the
<em>scavenger</em> will iterate over all blocks and recover the structure.</strong>
Contrast this to a file system like ext4 where a damaged structure can
mean that huge swathes of data are rendered&nbsp;inaccessible.</p>
<p><strong>The scavenger was also used to upgrade the disk structure.</strong> Imagine
converting in-place between ext4, btrfs or <span>ZFS</span> by just running a
scavenger for the new disk structure. It would e.g. regard the
existing ext4 metadata as nonsense and build a new disk structure
based on btrfs by using the labels. (It will not work with these
specific file systems, but it demonstrates the&nbsp;principle).</p>
<p>Why don’t we have this today? Nobody wants to build a <span>POSIX</span> file
system that uses labels because the disks we’re using have 512-byte or
4096-byte blocks and no room for a label. A label would have a very
small overhead for each block, but even a very small overhead like
this is not acceptable to file system designers. More seriously, it
would mess up mmap(2). There are disks with slightly larger blocks
meant to store a label, but they are more expensive and not very
common. So we’re left with fragile file system structures and the sage
advice that you’re supposed to have backups anyway (for which, see
previous&nbsp;section).</p>
<p>The best modern alternative is something like Btrfs or <span>ZFS</span>. Blocks are
checksummed and those checksums are stored next to the pointers in the
disk structure. A modern version of the Alto file system should
certainly have checksums, but not using labels gives weaker
protections than what the Alto file system provided. If you want to
see how relevant the Alto file system design is today, just read what
using ZFS protects against, but then remember that it doesn’t use&nbsp;labels.</p>

<p>Hydra is another historical operating system with innovations in the
file system. It is described in the
paper
<a href="https://research.cs.wisc.edu/areas/os/Qual/papers/hydra.pdf"><span>HYDRA</span>: The Kernel of a Multiprocessor Operating System</a> (1974)
by W. Wulf, E. Cohen, W. Corwin, A. Jones, R. Levin, C. Pierson, and
F. Pollack at Carnegie-Mellon&nbsp;University.</p>
<p>Hydra does away with the idea of ownership and a hierarchy of ever
more privileged components. It uses a fundamentally different concept
of protection and security than what <span>POSIX</span> is based&nbsp;on.</p>
<blockquote>
<p>“Technologists like hierarchical structures – they are elegant; but
experience from the real world shows they are not viable security
structures. The problem, then, which <span>HYDRA</span> attempts to face
squarely, is to maintain order in a nonhierarchical&nbsp;environment.”</p>
<p>– <span>HYDRA</span>: The Kernel of a Multiprocessor Operating System&nbsp;(1974)</p>
</blockquote>
<p>Resources that need protection, such as files, are called <em>objects</em>.
You can apply an <em>operation</em> to an object, such as reading or writing.
But to do so you need a reference to that object, which is called a
<em>capability</em>. You can even know a file’s identity, but without the
reference you can’t do&nbsp;anything.</p>
<p>This is similar, but not identical, to a safe programming …</p></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://weinholt.se/articles/non-posix-filesystems/">https://weinholt.se/articles/non-posix-filesystems/</a></em></p>]]>
            </description>
            <link>https://weinholt.se/articles/non-posix-filesystems/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24412970</guid>
            <pubDate>Tue, 08 Sep 2020 20:32:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Recipe nutrition calculator focusing on micronutrients]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24411315">thread link</a>) | @scxyz42
<br/>
September 8, 2020 | https://www.soupersage.com/recipe-nutrition-calculator | <a href="https://web.archive.org/web/*/https://www.soupersage.com/recipe-nutrition-calculator">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    
    <p>
      Calculate detailed nutrition for your favorite recipes, including calories, carbs, protein, and dozens of micronutrients (vitamins and minerals).<br>
      Adjust the amount of each ingredient and see how the nutrition changes.
    </p>
    <br>
    <div>
      <div>
        <div>
          <p>
            Use a recipe link
          </p>
          <div>
                        
            <form method="POST" action="/recipe-nutrition-calculator" name="parse_recipe_form" id="parse_recipe_form">
                            <label><strong>Recipe Link</strong></label>
              <div>
                
              </div>
            </form>
            <br>
          </div>
        </div>
        <br>
        <div>
          <p><span>OR</span> Copy and paste ingredients
          </p>
          <div>
                        
            <form method="POST" action="https://www.soupersage.com/recipe-nutrition-calculator/pasta/new" name="update_recipe_form" id="update_recipe_form">
        <div>
        
    </div>
    <div>
        <p><label><strong>Ingredients</strong></label>
            
        </p>
         
            <div>
                <br>
                </div>     
            </div>
</form>          </div>
        </div>
      </div>
      <div>
        <div>
          <div>
                        <p>Sample Recipe - Nutrition Analysis</p>
            <p><img src="https://www.soupersage.com/css/img/sample-recipe-nutrition-calculator.jpg" alt="Sample recipe nutrition calculator results">
          </p></div>
        </div>
      </div>
    </div>
    <div>
      <div>
        
        <p><small>
          Food nutritional data is sourced from USDA and NIH <sup><a href="https://fdc.nal.usda.gov/" target="_blank">[1]</a></sup><sup><a href="https://data.nal.usda.gov/dataset/usda-database-flavonoid-content-selected-foods-release-32-november-2015" target="_blank">[2]</a></sup>. The guest version for RDA is based Harvard Medical's adult female on a 2000 calorie diet.<sup><a href="https://www.health.harvard.edu/staying-healthy/listing_of_vitamins" target="_blank">[3]</a></sup> 
        </small></p></div>
    </div>
  </div></div>]]>
            </description>
            <link>https://www.soupersage.com/recipe-nutrition-calculator</link>
            <guid isPermaLink="false">hacker-news-small-sites-24411315</guid>
            <pubDate>Tue, 08 Sep 2020 18:28:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Axial Brought DevOps into Slack Saving 1000s of Hours of Developer Time]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24410267">thread link</a>) | @slajax
<br/>
September 8, 2020 | https://cto.ai/blog/how-axial-brought-devops-into-slack-saving-thousands-of-hours-of-developer-time-and-costs/ | <a href="https://web.archive.org/web/*/https://cto.ai/blog/how-axial-brought-devops-into-slack-saving-thousands-of-hours-of-developer-time-and-costs/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
			<!-- .cover -->
			<div>
				<p>Axial operates a confidential online marketplace connecting buyers and sellers of privately held small and medium sized businesses. The platform is 100% software enabled and includes tools and recommendation engines to empower both buyers and sellers to discover, connect and transact with one another. By expanding the reach of both buyers and sellers and eliminating hundreds of hours of manual time spent pursuing an offline transaction process allows deals with the most merit to rise to the top. The platform goes beyond matching the correct individuals and focuses on facilitating success at many levels. A key example of this is the platform's ability to facilitate real human to human interactions with communication on the platform itself.</p><blockquote><em>“Axial at its core is a specialized matching engine and communication platform.”</em></blockquote><p>The ability for the platform to connect people on a deeper level is what has made it a successful medium for completed deals and a major contributor to democratizing capital markets. As Zach Werheim, Director of Engineering at Axial, puts it, “one big goal is to be able to help people where they need it — for those who, otherwise, would never be able to get it."</p><p>Zach is a prominent software engineer and tech leader who has led a number of engineering teams. He, along with the rest of the Axial team, were quick to recognize the rapidly evolving market conditions in the way people communicate. The mindset of their users was changing quickly, and it was critical to provide them with industry best practices and features.</p><p>The Axial platform has served their customers for over 10 years. In that time they’ve endeavored to maintain best practices while evolving with emerging technologies. Their philosophy of pushing innovation results in a couple of issues arising that the team must address.</p><p><em><strong>1. Ensuring institutional knowledge isn’t lost over time.</strong></em></p><p><em><strong>2. Making it easy to adopt the newest best technologies.</strong></em></p><p><em><strong>3. Making their tooling accessible to their developers.</strong></em></p><p>Applications being built and improved over the course of a decade are always at risk of institutional knowledge being lost. It is paramount that developer and operations workflows and processes are common knowledge and not siloed. Additionally, over the course of the last decade, there are a new set of tools and processes which represent best practices. A core facet of engineering at Axial is to adopt the latest best practices to empower their development team to perform at their highest.</p><blockquote><br><em>Zach was on a mission to make it easier for their development team to adopt optimal tooling without silos and individual points of failure. This would empower their dev team to make the best technology choices while ensuring that the tool knowledge was collaborative and distributed. The solution was creating their own workflows meshed with the optimal tools for their day-to-day operations.</em><br></blockquote><p>After evaluating multiple tools with limited success, the team came across <a href="https://cto.ai/platform">The Ops Platform by CTO.ai</a>. The Ops Platform allowed them to start building solutions immediately by creating and automating developer workflows. These workflows were sharable with the entire team and could plug into any specific developer tooling that was necessary. They did this via Slack workflows and the CTO.ai SlackOps capability. Axial built 3 initial Ops to empower their engineering teams right away: The Dev Op, The Vault Op, and The Cluster Op.</p><!--kg-card-begin: html--><!--kg-card-end: html--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: html--><br><!--kg-card-end: html--><h3 id="1-the-dev-op">1. The Dev Op</h3><p>The engineering team leverages VMs which run in AWS, which also run most of their services for development. Their Dev Command controls the state of their DevBox for AWS cost savings. When the development team isn’t active, it can easily be turned off or turn on, on demand. The team also implemented an automatic shut off. Because it’s built as an Op, the team can run these suites of commands directly from Slack and avoid having to interface with the AWS Console or other specific services.</p><!--kg-card-begin: html--><br><!--kg-card-end: html--><!--kg-card-begin: image--><figure><img src="https://lh5.googleusercontent.com/WJ21lVtgdBCuga-WYA2xJc8h3LQUojaozdTE7cuiVgfVYtDC80LbgJRN0D1_gMul3ekbXDjS3zMwLXszGVPEGzwMOWVcRYSO6nXbTRbYIfw90_7zeRCAnBtiEZhIjThFDvNPGC3A"></figure><!--kg-card-end: image--><!--kg-card-begin: html--><!--kg-card-end: html--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: html--><br>
<!--kg-card-end: html--><h3 id="2-the-vault-op">2. The Vault Op<br></h3><p>This simple Op provides a temporary Vault token for verified members of the team. These Vault tokens are used to run various commands to update Vault-based variables in test, staging, or production clusters, as well as various scripts that touch production.<br></p><!--kg-card-begin: html--><br><!--kg-card-end: html--><!--kg-card-begin: image--><figure><img src="https://lh6.googleusercontent.com/L9L6iKa4kz1Mtn-lkENV8xS3oKSi8CkoDSYFkDUSgmiNNR9ENU0akezIFmJdOTCZDZe2_V5d8fnQfScKgBr9STyWk-GSinHo1UD9VgVX8uPg6V6y4XEYAXmxKFeiIc5219oYyQCI"></figure><!--kg-card-end: image--><!--kg-card-begin: html--><!--kg-card-end: html--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: html--><br><!--kg-card-end: html--><h3 id="3-the-cluster-op">3. The Cluster Op<br></h3><p>When the team has code that is ready for more rigorous testing, the Cluster Op can control the serverless namespaces in Kubernetes and deploy to a cluster. This Op will:</p><ol><li>Turn clusters on and off.</li><li>Deploy various code and services to the cluster.</li><li>Refresh the database to test on appropriate data.<br></li></ol><!--kg-card-begin: html--><br><!--kg-card-end: html--><!--kg-card-begin: image--><figure><img src="https://lh3.googleusercontent.com/R1naNXCB4SLHx5G_LCCMHFAyMKRTrPj_8Af2bW3lIgkcx1qjw-Qg0-d3vmxE1niF_2JhJ2uYsSaIiuSdWm_Am9hlFR8YqBshYroV8CaoiJGL3wdRHitwn79Ifuyip7vgpRqEwKNx"></figure><!--kg-card-end: image--><!--kg-card-begin: html--><!--kg-card-end: html--><p>All 3 Ops were published to the Axial engineering team's Slack workspace <em>and </em>their CLI, allowing everyone to share in the benefit of automated workflows. The <em>Dev </em>Op specifically helped save on their AWS bill by making it easy for the team to activate development services only when necessary. Across the 5 DevBoxes, Zach is able to save over 12 hours of uptime per day, which equates to a savings of over $5,900 annually. The Ops Platform made it simple to build—not only to save money, but also to streamline development for the rest of the engineering team and save developer hours.</p><blockquote><em>“Without CTO.ai, we would have to adopt the bare minimum tooling. Anything new would have to be written in a language our team was familiar with and we’d be running scripts manually. Having this technology available is going to make it easier to develop dev operations around new systems. It’s definitely going to be a big advantage for us going forward”</em></blockquote><div><p>Zach plans to continue to build key Ops to empower the rest of his team. Specifically, a production release pipeline. In addition, there are a lot of fragmented utility scripts their team uses to run various operations. These scripts currently sit in a folder and eventually become outdated as infrastructure changes. “Opifying” these would allow them to be version controlled and give the entire team more visibility. Zach has also thought about ways to empower other departments of the organization with Ops. One of their next plans for Ops is to build an “extremely simple and impossible to screw up” SlackOp to empower their marketing department to perform an operation they would usually need the assistance of engineering for. This Op would be trackable, so the engineering team would have visibility into what is being run, but the marketing team would be unblocked to continue their tasks. </p><p>Adopting Ops has allowed Axial to immediately reduce their cloud spend, enable easier adoption of new technologies, and even open the door to empowering non-engineers to run operations. Ultimately, this allows the engineering team to focus on their core competency in the pursuit of their own market innovation instead of spending time and resources on infrastructure and tooling.</p></div><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><p><em>Contact us to understand how much time and money workflows can save your team:</em></p><!--kg-card-begin: html--><!--[if lte IE 8]>
<script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/v2-legacy.js"></script>
<![endif]-->

<!--kg-card-end: html-->
			</div><!-- .post-content -->
			<!-- .post-footer -->
		</article></div>]]>
            </description>
            <link>https://cto.ai/blog/how-axial-brought-devops-into-slack-saving-thousands-of-hours-of-developer-time-and-costs/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24410267</guid>
            <pubDate>Tue, 08 Sep 2020 16:55:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Verne Edquist – Glenn Gould’s Piano Man]]>
            </title>
            <description>
<![CDATA[
Score 72 | Comments 24 (<a href="https://news.ycombinator.com/item?id=24410113">thread link</a>) | @bookofjoe
<br/>
September 8, 2020 | https://www.glenngould.ca/verne-edquist/ | <a href="https://web.archive.org/web/*/https://www.glenngould.ca/verne-edquist/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				 <!-- .et_pb_column --><div>
				
				
				<div>
				
				
				<div><p><span>In the winter of 1938, deep into the Depression,&nbsp;eight-year-old Verne was put on a train and sent 2,000 miles east, to the Ontario School for the Blind. It was when Verne was in the school’s hospital for six weeks with scarlet fever that he first heard the sound of a piano being tuned – the intervals being stretched and shortened. It was a pinging sound, not quite musical, and hardly melodic.</span></p>
<p><span>Once he had recovered,Verne sought out the school’s tuning shop. As it happened, he had a near perfect ear, and he studied the craft of piano tuning with rigor and determination.</span></p>
<p><span>Years later, Verne often took to quoting his tuning teacher, J. D. Ansell, whose favorite aphorism was “The only place where success comes before work is in the dictionary.” To give Verne experience, Ansell started taking his young protégé into town to tune pianos in private homes. Verne was allowed to keep the money – $2.50 per piano, and sometimes, when he got lucky, $3.00 – which he put toward some basic tools: a tuning wrench, a tuning fork, needle-nose pliers, gauges for measuring the diameter of piano wire, and rubber wedges for muting strings.</span></p>
<p><span>After graduating from high school at age 19, Verne knew his best chance at a job was to find work as a chipper in one of the many piano factories in Toronto (in the early 20th century the city directory listed no fewer than sixteen piano manufacturers). The chippers were the first tuners to work on a newly strung piano, so incomplete in construction that it still had no keys. Those initial tunings are done, first with a pitch fork, then by ear, by plucking the strings with a small chip of wood.</span></p>
<p><span>Verne started out as an apprentice chipper in the factory of the Winter Company, one of piano manufacturers.</span></p>
<p><span>He had a fishing tackle box that he had converted to a tuner’s toolbox. Over the years Verne collected dozens of tools. Some he bought from old-timers, and others he adapted from other trades. He had surgical forceps and dental explorers, which made dandy hooks, opticians’ screwdrivers for adjusting harpsichords, barber scissors for trimming felt, and shoemaker pegs for plugging holes. From the welding trade he took soapstone, a dry lubricant for the buckskin that can squeak in the action of older pianos.</span></p>
<p><span>After being laid off, Verne decided to go door-to-door, tuning pianos. He was systematic, choosing a sequence of different neighborhoods around Toronto that he could reach by street car. Verne soon grew bolder and began cold-calling at sergeants’ messes,&nbsp;asylums, and prisons. He would walk a mile and a half in a snowstorm to tune a piano, and charge $3 for a tuning. He was lucky to get one tuning a day. “That’s how things were,” he recalled years later. “I was glad to get the work. During that time you did what you could.”</span></p>
<p><span>In 1952, at the age of 21, Verne was hired as a fine-tuner in the Heintzman factory, then the largest piano manufacturer in Canada.&nbsp;Verne’s tuning always stood out, even when he was tuning pianos for the first time. To trained ears, the quality of Verne’s tuning was always superior.</span></p>
<p><span>In 1961, he moved to the T. Eaton Co., to take over as the concert tuner. Verne liked to think he could take a piano beyond mere sound, into realms of color. And he liked to think he was giving people a glimpse of that color every time he tuned a piano. Muriel Mussen, who ran the concert department, sat in a small office off the showroom floor,&nbsp;and&nbsp;came to recognize Verne’s spare style of tuning whenever she heard it. Unlike many tuners, who banged a key as hard as they could, he kept it gentle, careful not to hit the key any harder than he had to. Halfway into a piano, Verne would often hear Miss Mussen call out, “I know it’s you tuning out there, Verne.”</span></p>
<p><span>One afternoon about a year after Verne started at Eaton’s,&nbsp;Miss Mussen sent him across town to Glenn Gould’s apartment to tune Gould’s old Chickering. All Gould wanted, he told Verne, was for the tuner to do what had been done hundreds of times before: get the piano into playable condition, if only for the time being. But Verne refused, telling Gould that the tuning pins were so loose they needed to be replaced.</span></p>
<p><span>Verne’s stubborn insistence on doing things his way had endeared him to Gould, and the encounter galvanized what was to become a decades-long association between a pianist and his technician.</span></p>
<p><span>Verne tuned for many famous musicians over the years, including Duke Ellington, Arthur Rubinstein, Rudolf Serkin, Victor Borge, and Liberace. But it was the business he got from Gould that eventually enabled him to quit Eaton’s employ and sustain his family for two decades.</span></p>
<p><span>Each tolerated the other’s idiosyncracies, which were in ample evidence in both men. Gould’s quirks, of course, were legion and legendary. One of their earliest conversations was about Verne’s physical limitations. “I can’t see very well, but I get the job done,” Verne told Gould. And Gould replied that of this he had no doubt. Nothing further on the topic was ever said.</span></p>
</div>
			</div> <!-- .et_pb_text -->
			</div> <!-- .et_pb_column --> <!-- .et_pb_column -->
				
				
			</div></div>]]>
            </description>
            <link>https://www.glenngould.ca/verne-edquist/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24410113</guid>
            <pubDate>Tue, 08 Sep 2020 16:40:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: TerminusHub, Distributed Revision Control for Structured Data]]>
            </title>
            <description>
<![CDATA[
Score 74 | Comments 17 (<a href="https://news.ycombinator.com/item?id=24409334">thread link</a>) | @LukeEF
<br/>
September 8, 2020 | https://terminusdb.com/hub/ | <a href="https://web.archive.org/web/*/https://terminusdb.com/hub/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://terminusdb.com/hub/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24409334</guid>
            <pubDate>Tue, 08 Sep 2020 15:25:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Vitamin D, part 3 – The Evidence]]>
            </title>
            <description>
<![CDATA[
Score 283 | Comments 198 (<a href="https://news.ycombinator.com/item?id=24408511">thread link</a>) | @usefulcat
<br/>
September 8, 2020 | https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3 | <a href="https://web.archive.org/web/*/https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-hook="post-description"><article><div><div data-rce-version="7.19.2"><div dir="ltr"><div><div id="viewer-26bnd"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img"><p><img data-pin-url="https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3" data-pin-media="https://static.wixstatic.com/media/3e0600_9d834b06083a45db9984155f34157ea8~mv2.jpg/v1/fit/w_814,h_363,al_c,q_80/file.png" src="https://static.wixstatic.com/media/3e0600_9d834b06083a45db9984155f34157ea8~mv2.jpg/v1/fit/w_300,h_300,al_c,q_5/file.jpg"></p></div></div></div></div><h3 id="viewer-4bpvv">Vitamin D deficiency is linked to cancer, heart disease, respiratory infection, stroke, diabetes, and death. </h3><p id="viewer-4ds7a">It might seem reasonable that we should all be taking this supplement. But hold on. The argument for taking Vitamin D assumes a few things. First, that low Vitamin D actually causes all of the conditions it is associated with. This is not necessarily true. Low Vitamin D could cause (or contribute to) heart disease, but it could also be that heart disease causes low Vitamin D. Another possibility is that some third factor causes both low Vitamin D and heart disease. </p><p id="viewer-9g65f">Even if we can prove some causation, we then need to assume that correcting the Vitamin D deficiency will prevent the disease. This may not be true either. Low Vitamin D may be an indicator of overall poor health, which increases the risk for cardiac disease. It could appear that low Vitamin D contributes to the condition. But raising the Vitamin D will not help because it will just fix the symptom, not the underlying problem. </p><h3 id="viewer-399rj"><strong>Randomized Controlled Trials</strong></h3><p id="viewer-4p68a">Fortunately we have studies that can help figure this out. Currently the best way to answer a question of clinical relevance is with a large, well-executed <strong>randomized controlled trial (RCT).</strong> An RCT works like this: enroll the participants you want to study and randomly assign half of them to receive a treatment and the other half to receive a placebo. Those receiving the treatment are called the intervention group. The other group is the control group. Ideally, the two groups will be similar in every way except the treatment – they will have the same distribution in age, income, race, gender, and health status. They will even receive pills that look identical, though half will be placebos. If, at the end of the study, there are differences between the two groups, we can attribute those differences to the treatment. </p><div id="viewer-3aoj5"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img" aria-label="Simple randomized controlled trial "><p><img data-pin-url="https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3" data-pin-media="https://static.wixstatic.com/media/3e0600_70919e7b7a7d480c815f451bdce6cb09~mv2.png/v1/fit/w_1674,h_920,al_c,q_80/file.png" src="https://static.wixstatic.com/media/3e0600_70919e7b7a7d480c815f451bdce6cb09~mv2.png/v1/fit/w_300,h_300,al_c,q_5/file.png" alt="Simple randomized controlled trial "></p></div><p><span dir="auto">Simple randomized controlled trial</span></p></div></div></div><p id="viewer-fvrr">RCTs are costly and time intensive, so generally we will only be able to study the interventions and outcomes that have the highest chance of showing a benefit. We base this on prior studies; if several large observational studies show a correlation between Vitamin D and heart disease, then this is something we should study with an RCT. </p><p id="viewer-1ofc5">
We now have results for over 1500 RCTs on Vitamin D supplementation. Some of the largest and most significant trials are listed and summarized at the end of this article. When tested on its ability to prevent disease, <strong>Vitamin D has failed to live up to expectations</strong>. </p><div id="viewer-b7fav"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img" aria-label="Results of Vitamin D Trials "><p><img data-pin-url="https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3" data-pin-media="https://static.wixstatic.com/media/3e0600_a25ba8bbf26b47a096d2dc2ee7e41fcd~mv2.png/v1/fit/w_1206,h_878,al_c,q_80/file.png" src="https://static.wixstatic.com/media/3e0600_a25ba8bbf26b47a096d2dc2ee7e41fcd~mv2.png/v1/fit/w_300,h_300,al_c,q_5/file.png" alt="Results of Vitamin D Trials "></p></div><p><span dir="auto">For more details, see the list of high-quality RCTs at the bottom of this page</span></p></div></div></div><p id="viewer-en75e">One of the largest and best studies we have is called <a href="https://www.vitalstudy.org/" target="_blank" rel="noopener"><u>VITAL</u></a>, led by Dr. JoAnn Manson at Brigham and Women's Hospital in Boston. Over 25,000 adults (men age 50 and older, women age 55 and older) with no history of cardiovascular disease or cancer participated in the study. They received either 2,000 IUs of Vitamin D3 or placebo daily for five years. Researchers evaluated the rates of cancer and major cardiovascular events including stroke, heart attack, and cardiovascular death. </p><p id="viewer-f6k24"><strong>The result: Vitamin D failed to prevent cancer, stroke, heart attack, or cardiovascular death. </strong>These conditions occurred at the same rate in those receiving Vitamin D and those receiving placebo. There was one positive finding, though: in a post hoc analysis, it seemed that Vitamin D decreased the mortality from cancer when the initial two years were excluded. Post hoc (“after the event”) analyses can be quite useful, but need to be viewed with some skepticism. In high-quality RCTs (like VITAL), the outcome measures and statistical analyses are specified prior to data collection. Sometimes, as in VITAL’s case, an unexpected outcome may appear at the time the data are evaluated, prompting a post hoc analysis. In this case, when results from the first two years were excluded, the rate of cancer death was 1.2% in the placebo group, and 0.9% in the Vitamin D group - 25% reduction in cancer mortality, though the absolute numbers were low. It is not possible to draw too many conclusions from this, but it is something to note for future studies. </p><p id="viewer-3cooc"><strong>Even in studies of bone health, Vitamin D has been disappointing.</strong> We already know that Vitamin D is essential in calcium metabolism, and Vitamin D deficiency can lead to bone disorders like rickets and osteomalacia. Taking Vitamin D to treat severe Vitamin D deficiency is necessary for the treatment of these conditions. But in the large trials looking at Vitamin D's ability to prevent bone loss, the participants were taken from the general population, many of whom had normal Vitamin D levels. For severely deficient individuals (only a small percentage of the population), there are clear benefits to supplementation. These benefits do not appear to extend to those with normal levels or mild deficiencies.</p><h3 id="viewer-ct6r5"><strong>Meta-analyses of randomized controlled trials</strong></h3><p id="viewer-ek5t0">Large randomized controlled trials are currently the best way to evaluate a treatment, but they are difficult to do. If done well, an RCT is expensive, time intensive, and challenging to coordinate. It is worth it if we can answer our question adequately, but sometimes we realize after the trial that we needed to account for something else, or that we need more people in the study before we can really answer our question. In addition, RCTs will not always give definitive answers. Sometimes two or more small trials of the same treatment will show opposite results, and we need to reconcile those. Simply redoing an RCT is usually not an option. Instead, scientists have developed a method of combining similar RCTs and analyzing them together, in what is called a meta-analysis. </p><p id="viewer-ab05u">Meta-analyses have exploded in popularity over the last few decades.<span> If done well, a meta-analysis of a group of trials can reveal insights that cannot be seen in the individual trials. Unfortunately there are potential problems. The quality of the meta-analysis depends on the quality of the studies it includes, which can lead to erroneous results when poor quality studies are present (garbage in, garbage out).</span></p><p id="viewer-9nlvf"><span>In addition, <strong>publication bias</strong> may incorrectly amplify a treatment effect. Publication bias occurs when researchers decide to publish only “positive” findings – studies that show a treatment effect. Suppose ten small trials of Vitamin D are performed. Five show a benefit to Vitamin D and five do not. Researchers and journals are much more inclined to publish positive results than negative, so it is possible that only the five positive studies will make it into the medical literature. A meta-analysis on the five published positive studies will show a much stronger effect of Vitamin D than a meta-analysis that included all ten studies. </span></p><div id="viewer-9ul9t"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img"><p><img data-pin-url="https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3" data-pin-media="https://static.wixstatic.com/media/3e0600_1afb8b809c0446029010b53b0b7cdf14~mv2.jpg/v1/fit/w_900,h_373,al_c,q_80/file.png" src="https://static.wixstatic.com/media/3e0600_1afb8b809c0446029010b53b0b7cdf14~mv2.jpg/v1/fit/w_300,h_300,al_c,q_5/file.jpg"></p></div></div></div></div><p id="viewer-a6n1n"> <em>Where negative studies go to die. </em></p><p id="viewer-a6bj6"><a href="https://en.wikipedia.org/wiki/Robert_Rosenthal_(psychologist)" target="_blank" rel="noopener"><em><u>Robert Rosenthal </u></em></a><em>called publication bias "the file drawer problem" because important negative studies often end up here. Photo by </em><a href="https://www.bigstockphoto.com/search/?contributor=nirat" target="_blank" rel="noopener"><em><u>nirat</u></em></a><em> on </em><a href="http://bigstockphoto.com/" target="_blank" rel="noopener"><em><u>bigstockphoto.com.</u></em></a><em> </em></p><p id="viewer-7700d"><span>Not every related trial should be included in a meta-analysis, though. One of the major challenges in performing these analyses is deciding which studies to include or exclude. Ideally only high-quality, well-designed, well-executed studies will be included. Researchers do not always agree on which studies to include, and differences in inclusion criteria have led to similar meta-analyses producing opposite results. </span><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0115934" target="_blank" rel="noopener"><u>This has occurred</u></a> <span>in studies of Vitamin D and fracture risk in older adults, with some meta-analyses showing zero benefit to Vitamin D and others showing a decreased risk of fracture, even though they included many of the same studies. </span></p><p id="viewer-donuf"><span>In addition to being high quality, the included studies should all try to answer the same clinical question. Many meta-analyses can only be done by combining trials that differ in important details, like the age of participants, treatment dosages, or definition of endpoints. Combining these may be feasible, but the clinical relevance may change when disparate groups are lumped together. For example, suppose we want to know if Vitamin D prevents asthma attacks in children. If a meta-analysis shows a small benefit to Vitamin D when all ages are combined, but the individual small studies in children did not show a benefit, what do I advise a 10 year old with asthma?</span></p><p id="viewer-9le66">Despite these challenges, meta-analyses of RCTs can provide meaningful insights. Unfortunately, most high-quality meta-analyses show similar results to our individual randomized controlled trials: <strong>Vitamin D does not appear to prevent disease in healthy adults.</strong> </p><p id="viewer-9hssp">But there are a few promising areas identified in meta-analyses. Note that some of these findings are not consistent with the large RCTs and will need to be studied further before definitive recommendations can be made. </p><p id="viewer-25ike">The areas in which meta-analyses have identified benefits from Vitamin D:</p><ol><li id="viewer-f3qjq"><p><strong>Fracture prevention in elderly nursing home residents </strong>(when also given with calcium):   This is not surprising. We know that Vitamin D and calcium can prevent bone loss in severe Vitamin D deficiency. Elderly adults who are not getting outside are more likely to be severely deficient in Vitamin D, and supplements likely help.</p></li><li id="viewer-fa0a9"><p><strong>Asthma and respiratory infection</strong>: Vitamin D seems to reduce asthma attacks in adults with mild to moderate disease, and daily or weekly Vitamin D seems to prevent acute respiratory infection in those with Vitamin D less than 10 ng/ml (25 nmol/l). There is considerable excitement around Vitamin D's potential role in Covid treatment, though we do not yet have enough evidence to make a definitive conclusion. </p></li><li id="viewer-d7fj2"><p><strong>Cancer mortality</strong>: Vitamin D does not appear to prevent cancer, but may reduce death rates from cancer overall (when all cancers are combined) when Vitamin D is taken for several years. It is not known whether Vitamin D itself fights cancer. It could also be that individuals with cancer are more prone to developing severe Vitamin D deficiency, which leads to bone loss, fragility, and fractures, which increase mortality.</p></li><li id="viewer-cvias"><p><strong>Atopic dermatitis (eczema)</strong>: Vitamin D …</p></li></ol></div></div></div></div></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3">https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3</a></em></p>]]>
            </description>
            <link>https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3</link>
            <guid isPermaLink="false">hacker-news-small-sites-24408511</guid>
            <pubDate>Tue, 08 Sep 2020 14:10:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Treachery of Image Files]]>
            </title>
            <description>
<![CDATA[
Score 49 | Comments 14 (<a href="https://news.ycombinator.com/item?id=24408509">thread link</a>) | @kick
<br/>
September 8, 2020 | http://beyondloom.com/blog/images.html | <a href="https://web.archive.org/web/*/http://beyondloom.com/blog/images.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

<p>I have recently come into possession of a series of exquisite miniatures which I pray are worthy of your esteemed attention. While my employers are familiar with your <em>unusual tastes</em>, they also appreciate your desire for privacy- this showing was arranged with the utmost discretion, I assure you.</p>

<p>To the untrained eye, the pieces might <em>appear</em> remarkably similar. Each canvas is a digitally-reproduced composition measuring ten pixels in height and ten pixels in width, illuminated in a strikingly intense blue.</p>

<p>To an, ahem, <em>avid collector</em> such as yourself, it is no doubt obvious that each print possesses unique <em>qualia</em>, as expounded upon by the artist’s extensive footnotes. Please, take as long as you need…</p>



<figure>
<img src="http://beyondloom.com/blog/images-figures/i.png" alt="Becoming and Unbecoming. Macintosh Operating System 10.14.6, Preview.App 10.1">
<figcaption>Becoming and Unbecoming. Macintosh Operating System 10.14.6, Preview.App 10.1</figcaption>
</figure>

<p>In this piece, the artist used a carefully selected vintage of the <em>Macintosh</em> operating system. The desktop wallpaper color was customized to consist entirely of the desired blue, and then the <em>Preview</em> application was employed to take a screenshot and perform the necessary trimming. Tragically, <em>Preview</em> cannot save files in the <em>Graphics Interchange Format</em> (GIF).</p>

<figure>
<img src="http://beyondloom.com/blog/images-figures/ii.gif" alt="Pointillism I. Sublime Text 3.2.2, ImageMagick 7.0.8-59">
<figcaption>Pointillism I. Sublime Text 3.2.2, ImageMagick 7.0.8–59</figcaption>
</figure>

<p>In this piece, the artist manually constructed the image in the <em>Portable PixMap</em> (PPM) format using the <em>Sublime</em> text editor, as follows:</p>

<pre><code>P3
# width, height, depth
10 10 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
</code></pre>

<p>The <em>ImageMagick</em> application was then used to convert it into a GIF file:</p>

<pre><code>$ convert ii.ppm ii.gif
</code></pre>

<figure>
<img src="http://beyondloom.com/blog/images-figures/iii.gif" alt="Pointillism II. Vim 8.0.1365, FFmpeg 4.1.4">
<figcaption>Pointillism II. Vim 8.0.1365, FFmpeg 4.1.4</figcaption>
</figure>

<p>For this piece, the artist once again created a PPM image, this time using the <em>Vi, Improved</em> text editor, giving the source code a subtly crisper affect:</p>

<pre><code>P3
# horizontal, vertical, perchannel
10 10 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1  0 0 1
</code></pre>

<p>The <em>Fast Forward Moving Pictures Expert Group</em> application was then used to generate a GIF:</p>

<pre><code>$ ffmpeg -i iii.ppm iii.gif
</code></pre>

<figure>
<img src="http://beyondloom.com/blog/images-figures/iv.png" alt="Searing Light. Nano 2.0.6, POV-Ray 3.7.0">
<figcaption>Searing Light. Nano 2.0.6, POV-Ray 3.7.0</figcaption>
</figure>

<p>The artist composed this piece using the <em>Persistence of Vision Raytracer</em> language, composing the following script in the <em>Nano’s Another</em> text editor:</p>

<pre><code>background { rgb &lt;0, 0, 1&gt; }
</code></pre>

<p>The scene was then raytraced as follows:</p>

<pre><code>$ povray +Iiv.pov +W10 +H10 +Oiv
</code></pre>

<p>As with <em>Preview</em> above, the <em>Persistence of Vision Raytracer</em> is intentionally designed not to support the creation of GIF files; output is a <em>Portable Network Graphics</em> file instead.</p>

<figure>
<img src="http://beyondloom.com/blog/images-figures/v.gif" alt="The Cobbler. Sublime Text 3.2.2, as 10.0.1">
<figcaption>The Cobbler. Sublime Text 3.2.2, as 10.0.1</figcaption>
</figure>

<p>This image was composed by the <em>Macintosh Operating System X Mach-O Gnu’s Not Unix-based assembler</em>, a utility for the creation of binary files. The <em>Sublime</em> text editor was once again employed for its neutral tone:</p>

<pre><code>.ascii "GIF89a" # magic number
.short 10       # width
.short 10       # height
.byte  0xF0     # 2-entry global color table
.byte  0        # background color index
.byte  0        # 1:1 pixel aspect ratio

.byte  0x00     # color 0: blue
.byte  0x00
.byte  0xFF
.byte  0x00     # color 1: black
.byte  0x00
.byte  0x00

.ascii ","      # image descriptor
.long  0        # x/y offsets
.short 10       # width
.short 10       # height
.byte  0        # no local colortable
.byte  0        # minimum LZW code size
.byte  0        # end of frame

.ascii ";"      # finish
</code></pre>

<p>Curiously, the output of this assembler is prefaced with an entirely useless header, which must then be trimmed. (Hopefully future revisions of the tool will correct this obvious usability gaffe!) The composition was thus carried out as follows:</p>

<pre><code>$ as v.s -o assembled.o
$ dd bs=1 skip=208 if=assembled.o of=v.gif
</code></pre>

<figure>
<img src="http://beyondloom.com/blog/images-figures/vi.gif" alt="Camera Obscura. oK, iKe, Safari 12.1.2">
<figcaption>Camera Obscura. oK, iKe, Safari 12.1.2</figcaption>
</figure>

<p>Here, the artist employed the <em>Interactive K Environment</em> (iKe), a tool based around the unfathomably esoteric programming language <em>K</em>. The tool can be used to save animated GIF images of program output; in this case, a single frame as indicated by <code>fc:1</code>. Note that <em>iKe</em> automatically doubles the size of pixels within images when rendering.</p>

<pre><code>w:h:5
fc:1
draw:{,(;,"blue";5 5#0)}
</code></pre>

<figure>
<img src="http://beyondloom.com/blog/images-figures/vii.gif" alt="Mu. English, Homo Sapiens">
<figcaption>Mu. English, Homo Sapiens</figcaption>
</figure>

<p>For this work, the artist composed an electronic mail message to a colleague as follows:</p>

<pre><code>Dear [REDACTED],
I pray this letter finds you well.

I am in grave need of your assistance. I require an opaque image file in the .GIF format,
measuring precisely 10 pixels square, consisting entirely of the hexadecimal RGB color #0000FF.
If you could provide me with such a file as an attachment, I would be immensely grateful.

Warmest Regards,
[REDACTED]

(P.S. This will make more sense later.)
</code></pre>

<p>Some time later, the artist used their electronic mail client to save the final print.</p>

<!-- For the record, this GIF was created in the Windows version of Krita 4.30. Thanks, Michal! -->

<figure>
<img src="http://beyondloom.com/blog/images-figures/viii.gif" alt="A Desperate Agony. VirtualBox 6.0.6, Ubuntu 19.04, GIMP 2.10.20, Shouting">
<figcaption>A Desperate Agony. VirtualBox 6.0.6, Ubuntu 19.04, GIMP 2.10.20, Shouting</figcaption>
</figure>

<p>Here, the artist constructed a virtual machine- a sort of computing matryoshka, if you will- running the <em>Ubuntu</em> distribution of the <em>Linux</em> operating system. They then proceeded to install the <em>GIMP</em> application. Despite frequent threats by the developers to remove them, the <em>GIMP</em> application had (at time of composition) some obscure and infrequently-used features permitting the creation and manipulation of image files. After its painstaking creation, the file was then transferred between the virtual machine and its host by way of a physical Universal Serial Bus mass-storage device. The mass-storage device was prepared in advance by formatting it with the 32-bit <em>File Allocation Table</em> filesystem and labeling it <em>Charon</em>, in recognition of its role in transporting the soul of the artwork between worlds.</p>

<figure>
<img src="http://beyondloom.com/blog/images-figures/ix.gif" alt="Composition in Blue and Blue. Gnuplot 5.2, Sublime Text 3.2.2">
<figcaption>Composition in Blue and Blue. Gnuplot 5.2, Sublime Text 3.2.2</figcaption>
</figure>

<p>The artist generated this work using the <em>Gnu’s Not Unix Plotter</em>, based on the following description:</p>

<pre><code>set terminal gif size 10,10
set output 'ix.gif'
set margins 0,0,0,0
unset key
unset tics
unset border
unset label
set style rectangle fs border lc rgb 'blue'
set object 1 rectangle from screen 0,0 to screen 1,1 fillcolor rgb 'blue' behind
plot 1 lt rgb 'blue'
</code></pre>

<p>Note in particular how easily a user of the <em>Gnu’s Not Unix Plotter</em> may customize the background color of a plot. The plot description may then be rendered as follows:</p>

<pre><code>$ gnuplot ix.gp 
</code></pre>

<p><a href="http://beyondloom.com/blog/index.html">back</a></p>
</div>]]>
            </description>
            <link>http://beyondloom.com/blog/images.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24408509</guid>
            <pubDate>Tue, 08 Sep 2020 14:10:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Pick the Right Postgres for Your Application – A Build vs. Buy Analysis]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24408486">thread link</a>) | @ya_RIV
<br/>
September 8, 2020 | https://lakefs.io/2020/09/07/how-to-pick-the-right-postgres-for-your-application/ | <a href="https://web.archive.org/web/*/https://lakefs.io/2020/09/07/how-to-pick-the-right-postgres-for-your-application/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-382" itemtype="https://schema.org/CreativeWork" itemscope="itemscope"><div><div itemprop="text"><p>Lots of applications require a Postgres database. Before you can install them, you will need a Postgres database. How do you pick the right Postgres for your application? There are a bewildering variety of possible ways to acquire a database running on a Postgres instance, but the biggest choice is “build or buy”: whether to install a Postgres version on your own or to purchase it as a service.</p><p>lakeFS is yet another Postgres application, so it requires a Postgres database. At Treeverse we decided for now to buy Postgres as a service. I’ll explain the factors why we decided, and how you might decide to whether or not to follow us.</p><h2><a href="https://github.com/treeverse/blog-internal/blob/master/20200907-picking-postgres/20200907-picking-postgres.md#got-a-postgres"></a>Got a Postgres?</h2><p>If your organization already runs Postgres instances, they may well be the best choice: there can be extensive in-house experience and expertise already present. And you probably don’t need this guide.</p><h2><a href="https://github.com/treeverse/blog-internal/blob/master/20200907-picking-postgres/20200907-picking-postgres.md#expensive-as-a-service"></a>Expensive as a Service?</h2><p>The most common objection to buying a database as a service is sheer price: a DB instance from your cloud provider can cost more than&nbsp;<em>twice</em>&nbsp;the cost of the virtual machine instance that it runs on. Obviously you pay extra to get much more than just the virtualized resources: you get expertise, much of the ops, easier backup and clustering, sometimes even some features.</p><p>For pricing I believe we should prefer to&nbsp;<em>buy</em>&nbsp;what is not a core part of our business. Pricing can actually make this decision easy. If the cost of buying Postgres as a service is a significant expenditure, then necessarily it is a core part of business — and clearly we need to develop the requisite in-house capability to support it. Otherwise cost is not a driving factor in the decision.</p><p>Even if you decide Postgres is a core capability for your organization, you may wish buy Postgres as a service for an initial ramp-up period, planning to move away later. While migrating database installations generally requires some downtime, this too is often a good choice.</p><h2><a href="https://github.com/treeverse/blog-internal/blob/master/20200907-picking-postgres/20200907-picking-postgres.md#choices"></a>Choices</h2><p>Every cloud provider provides managed Postgres as a service:</p><ul><li>AWS provide RDS as well as clustered Postgres-compatible Aurora;</li><li>Azure provide Azure Database for PostgreSQL;</li><li>GCP provide Postgres on Cloud SQL</li></ul><p>Similarly, some hosting providers also have a Postgres as a service offering. If you are already using a hosting provider that offers Postgres — Heroku is one example — this can be a convenient choice.</p><p>Postgres maintain a page&nbsp;<a href="https://www.postgresql.org/support/professional_hosting/">PostgreSQL: Hosting Providers</a>&nbsp;that list a wide range of options for purchasing hosted solutions along with support. Additionally, many hosted application platforms offer Postgres.</p><h2><a href="https://github.com/treeverse/blog-internal/blob/master/20200907-picking-postgres/20200907-picking-postgres.md#expertise-and-ops"></a>Expertise and Ops</h2><p>Developing in-house expertise is often desirable, and not only as a means to save costs. For instance, by allowing rapid resolution of issues without having to go outside the organization, expertise reduces risk.</p><p>On the flip side, managing your in-house database carries an ops burden, which will include monitoring, backup, clustering, and versioning. The last can be particularly troubling: occasionally you&nbsp;<em>will</em>&nbsp;need to upgrade Postgres on a running system.</p><p>A middle ground between buying a service and building it yourself is to contract the setup but manage ops in-house. I believe that unless you do database-related development, databases are generally a poor fit for this middle ground alternative. Contracting only the setup means you will take longer to build expertise in the database. Meanwhile you already start carrying the ops burden of the database.</p><h2><a href="https://github.com/treeverse/blog-internal/blob/master/20200907-picking-postgres/20200907-picking-postgres.md#versions-and-extensions"></a>Versions and Extensions</h2><h3><a href="https://github.com/treeverse/blog-internal/blob/master/20200907-picking-postgres/20200907-picking-postgres.md#versions"></a>Versions</h3><p>Buying from a large service provider appears to limit available versions. Currently, for example, the latest supported Postgres version is 12.4; the major release Postgres 12 was on 2019-10-03. Service providers have been slow to provide version 12:</p><ul><li>AWS RDS started supporting Postgres version 12 on 2020-03-31, and supports Postgres versions 9.4, 9.5, 9.6, 10, and 11; however Aurora clusters do not support version 12 yet;</li><li>Azure still supports only versions 9.5, 9.6, 10.11, and 11.6;</li><li>GCP started providing Postgres version 12 on 2020-05-21;</li><li>ElephantSQL supports Postgres version 12, but the free tier provides version 11.9.</li></ul><p>At the same time service providers can upgrade Postgres (and underlying OS) versions more transparently. This can be particularly useful for receiving patch releases. So low availability of newer Postgres versions is&nbsp;<em>not</em>&nbsp;a significant detriment. Of course, certain applications may require an absolutely latest version of Postgres, which may make it impossible to buy Postgres service.</p><h3><a href="https://github.com/treeverse/blog-internal/blob/master/20200907-picking-postgres/20200907-picking-postgres.md#extensions"></a>Extensions</h3><p>Numerous extensions are available for Postgres. Service providers provide the most popular of these. But they cannot provide all of them, and typically it is not possible to install new extensions that require compilation.</p><h3><a href="https://github.com/treeverse/blog-internal/blob/master/20200907-picking-postgres/20200907-picking-postgres.md#applicability-to-lakefs"></a>Applicability to lakeFS</h3><p>To allow users flexibility, lakeFS requires only Postgres 11. So availability of versions and extensions does not influence choice of provider for a database intended to run only lakeFS.</p><h2><a href="https://github.com/treeverse/blog-internal/blob/master/20200907-picking-postgres/20200907-picking-postgres.md#provisioning"></a>Provisioning</h2><p>Creating a new database instance with a service provider requires plenty of time. For example, on AWS RDS creating a small empty database instance can take 10 minutes. This is very reasonable for one-time setup of long-lived production and even staging environments.</p><h3><a href="https://github.com/treeverse/blog-internal/blob/master/20200907-picking-postgres/20200907-picking-postgres.md#fast-setup-of-small-databases"></a>Fast setup of small databases</h3><p>For short-lived environments or for testing, it can take too long to set up a hosted database. Luckily Postgres is an isolated component and one installation may easily be replaced with another installation. If the database is small and the generated data is not particularly valuable then there are good alternatives with quick setup times. These include creating a new database on an existing Postgres database instance or even — when database instance performance is not critical — running a database instance inside a container. If you have permission to create a new database, both of these alternatives have equivalent setup times that are typically less than a second.</p><p>Using an existing database instance allows you to take advantage of existing setup and ops on that database instance. For example, it will be easy to backup your new database if the database already has backups, and clustering exists at the instance level. By the same token, an existing database instance will offer significantly lower isolation than a container.</p><h3><a href="https://github.com/treeverse/blog-internal/blob/master/20200907-picking-postgres/20200907-picking-postgres.md#examples"></a>Examples</h3><p>An example of creating new databases inside an existing database instance is the ElephantSQL free tier: you are allocated a database inside an existing instance and can see that other databases exist inside that instance. It uses Postgres roles to prevent data leakage (a side effect is that some operations are impossible, for instance any application that requires multiple roles).</p><p>An example of running a database instance inside a container is when running tests. Every test can create its own blank container running Postgres. This is probably the best solution for component testing: it offers complete isolation between tests, the database container is small, and it scales directly with the number of running tests.</p><h2><a href="https://github.com/treeverse/blog-internal/blob/master/20200907-picking-postgres/20200907-picking-postgres.md#what-did-we-pick"></a>What did we pick?</h2><p>All our lakeFS instances currently use plain or Aurora Postgres on RDS:</p><ul><li>Our database usage is currently not high enough to justify the cost savings of running our own instances..</li><li>We build lakeFS, and supporting Postgres as a service is important to the product. So by design, lakeFS does not require latest-version features or esoteric extensions.</li><li>Our core business is currently to develop rather than to operate lakeFS; this is essentially the “ramp-up” time above.</li></ul><p>For isolation and speed of setup, our unit and component testing use a containerized Postgres instance.</p><figure><img src="https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-1024x768.jpg" alt="" width="542" height="407" srcset="https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-1024x768.jpg 1024w, https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-300x225.jpg 300w, https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-768x576.jpg 768w, https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-1536x1152.jpg 1536w, https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-2048x1535.jpg 2048w, https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-24x18.jpg 24w, https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-36x27.jpg 36w, https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-48x36.jpg 48w" sizes="(max-width: 542px) 100vw, 542px" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-1024x768.jpg" data-srcset="https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-1024x768.jpg 1024w, https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-300x225.jpg 300w, https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-768x576.jpg 768w, https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-1536x1152.jpg 1536w, https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-2048x1535.jpg 2048w, https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-24x18.jpg 24w, https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-36x27.jpg 36w, https://lakefs.io/wp-content/uploads/2020/09/img_20200904_234025-48x36.jpg 48w"><figcaption>Picking a database, like picking an elephant, can be hard but need not be intimidating</figcaption></figure></div><!-- .entry-content .clear --></div></article></div>]]>
            </description>
            <link>https://lakefs.io/2020/09/07/how-to-pick-the-right-postgres-for-your-application/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24408486</guid>
            <pubDate>Tue, 08 Sep 2020 14:08:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Balalaika IT newsletter for web developers (Java, Node.js, React)]]>
            </title>
            <description>
<![CDATA[
Score 36 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24407761">thread link</a>) | @keenondrums
<br/>
September 8, 2020 | https://blog.balalaikait.com/balalaika-it-newsletter-9 | <a href="https://web.archive.org/web/*/https://blog.balalaikait.com/balalaika-it-newsletter-9">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1599317301627/XYncjZMBx.jpeg?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div itemprop="text"><p><em>If you like this newsletter subscribe to our new issues at <a href="http://balalaikait.com/" target="_blank">balalaikait.com</a></em></p>

<ul>
<li>CPU caching 101 from Nick Evanson. A <a target="_blank" href="https://www.techspot.com/article/2066-cpu-l1-l2-l3-cache/">concise read</a> about the fundamentals to get you started on how it works and why we need it. </li>
<li>Over the last two years, Uber has attempted to reduce microservice complexity while still maintaining the benefits of a microservice architecture. With <a target="_blank" href="https://eng.uber.com/microservice-architecture/">this blog post</a> we hope to introduce our generalized approach to microservice architectures, which we refer to as “Domain-Oriented Microservice Architecture” (DOMA).</li>
<li>Distributed locks are infamous for being hard to use right. Some time ago Hazelcast team introduced locks as a part of CP Subsystem based on Raft, a well-known consensus algorithm. This <a target="_blank" href="https://hazelcast.com/blog/long-live-distributed-locks/">blog post</a> describes potential pitfalls of distributed lock and how FencedLock solves them.</li>
<li>Great engineers of the past have a lot to learn from. In <a target="_blank" href="https://youtu.be/bo5WL5IQAd0">this talk</a> explains how Erlang was designed to scale well on multicore machines.</li>
</ul>

<ul>
<li>ES6 introduced many nice built-in collections, such as Map, Set, WeakMap, and WeakSet. Unfortunately, the spec does not put many requirements for concrete implementations. If you want to learn practical details of Maps and Sets implementation in V8, read this <a target="_blank" href="https://itnext.io/v8-deep-dives-understanding-map-internals-45eb94a183df">blog post</a> by Andrey P.</li>
<li>JIT compiler is what makes V8 and other JS engines blazing fast. But exactly does it do? Watch this <a target="_blank" href="https://youtu.be/p-iiEDtpy6I">talk</a> by Franziska Hinkelmann to learn the answer.</li>
</ul>

<ul>
<li>Is it possible to build SPAs purely in Rust and without writing a single line of JavaScript? The short answer is YES! Read about the experiment <a target="_blank" href="http://www.sheshbabu.com/posts/rust-wasm-yew-single-page-application/">here</a>.</li>
</ul>

<ul>
<li>A <a target="_blank" href="https://httptoolkit.tech/blog/how-to-debug-node-segfaults">concise guide</a> on how to debug segmentation faults in Node.js.</li>
</ul>

<ul>
<li>Did you know that <code>java.security.SecureRandom</code> may be more or less secure depending on the configuration? This <a target="_blank" href="https://tersesystems.com/blog/2015/12/17/the-right-way-to-use-securerandom/">blog post</a> explains how to use it in the right way.</li>
</ul>
</div></div></section></div></div>]]>
            </description>
            <link>https://blog.balalaikait.com/balalaika-it-newsletter-9</link>
            <guid isPermaLink="false">hacker-news-small-sites-24407761</guid>
            <pubDate>Tue, 08 Sep 2020 12:35:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Free hosted open-source alternative to Zapier/Airflow]]>
            </title>
            <description>
<![CDATA[
Score 263 | Comments 74 (<a href="https://news.ycombinator.com/item?id=24407706">thread link</a>) | @newcrobuzon
<br/>
September 8, 2020 | https://cloud.titanoboa.io/index.html | <a href="https://web.archive.org/web/*/https://cloud.titanoboa.io/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div>
        <div>
            <div>
                <p><span>public βeta</span></p><h2><b>Try a <u>free</u> hosted instance of Titanoboa!</b></h2>


                <p>You don't have to install Titanoboa on your computer - just try it straight away in your browser!</p>
                
                <dev>
                     <br>
                    <b>We are sorry, but Titanoboa server's GUI is not fully optimized for mobile devices yet. Feel free to watch the demo below or read our <a href="https://github.com/mikub/titanoboa/wiki">wiki</a> to learn more!</b>
                </dev>
                <p><br>
                If you are not familiar with Titanoboa you can watch a short demo here:<br>

                <video controls="">
                    <source src="https://www.titanoboa.io/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                </p>

 </div>
        </div>
    </div>
</div></div>]]>
            </description>
            <link>https://cloud.titanoboa.io/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24407706</guid>
            <pubDate>Tue, 08 Sep 2020 12:26:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ad Fraud on LinkedIn]]>
            </title>
            <description>
<![CDATA[
Score 460 | Comments 265 (<a href="https://news.ycombinator.com/item?id=24407432">thread link</a>) | @sbachman
<br/>
September 8, 2020 | https://www.samueljscott.com/2020/09/08/linkedin-ad-fraud/ | <a href="https://web.archive.org/web/*/https://www.samueljscott.com/2020/09/08/linkedin-ad-fraud/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
					<p>The LinkedIn Ads network is likely awash in mistaken clicks and bot traffic that make the platform’s value extremely dubious, according to a new study from global digital agency RMG that the company shared exclusively with me.</p>
<p>Three weeks ago, RMG founding partner Ryan Gellis <a href="https://www.linkedin.com/posts/ryan-gellis_theory-the-linkedin-ad-network-is-full-of-activity-6700077824360910848-LysO/" target="_blank" rel="noopener noreferrer">posted a thread</a> wondering if LinkedIn “is full of fraudulent accounts and unchecked misclicks that artificially inflate the cost of advertising and result in poor performance compared to other ad networks.” The ensuing discussion inspired him to look into the issue further.</p>
<center><iframe src="https://www.youtube.com/embed/jKuyxgWuiRM" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></center><p>Gellis goes through his findings <a href="https://www.youtube.com/watch?v=jKuyxgWuiRM" target="_blank" rel="noopener noreferrer">in this YouTube video</a>. For the test, RMG promoted a webinar by targeting c-certain c-suite executives at large companies in the United States based on their job titles. The ad encouraged them to click to sign up for for event for free.</p>
<p>The first issue the agency found was the projected versus actual cost per click. LinkedIn estimated a $25 average CPC. The actual campaign CPC was nearly $45.</p>
<p>“What’s the point of forecasting if you project one cost and then the actual cost ends up being 200% the projection?” Gellis asks in the video.</p>
<p>But the real issue was the source of the clicks themselves. LinkedIn’s ad platform reported 11 clicks. RMG’s logs showed ten clicks. <a href="https://www.fullstory.com/" target="_blank" rel="noopener noreferrer">FullStory</a>, the digital experience analytics platform that RMG uses, stated that eight people registered for the webinar. However, the true number of registrations was zero.</p>
<p>“Zero people signed up for the webinar from that specific campaign,” Gellis told me in an interview. “That’s kind of the punch line and why I believe this is newsworthy. Nobody signed up. In fact, nobody even exhibited what I believe is real user behavior on the site after clicking the ad. That is also true of all the other ads we ran on LinkedIn. We had a total of 0 people ever “sign up” on any ad we ran during our testing on LinkedIn — regardless of the ad format or messaging.”</p>
<p><a href="https://www.rmgmedia.com/" target="_blank" rel="noopener noreferrer">RMG</a> suspects bot activity or misattributed misclicks had been the cause because visitors would bounce away from the webinar’s landing page before it even had a chance to render — in other words, they would click the LinkedIn advertisement and then leave the website in less than 1.3 seconds. In addition, FullStory reported mouse movements that were nothing like what an actual human would do.</p>
<p>“As far as we’re concerned, the LinkedIn ad network basically has fraudulent clicks and/or misclicks that don’t get attributed to the users that are being passed through as leads in campaigns,” Gellis says in the video.</p>
<p>Of course, the simple size in RMG’s one-day test was extremely small. The target audience size was 16,000. The number of impressions would be between 680 and 2,000. The cost was $250. But Gellis told me that the results match a prior, larger test.</p>
<p><img src="https://www.samueljscott.com/wp-content/uploads/2020/09/linkedin-ads.png" alt="linkedin ads" width="660" height="197" data-eio="l" data-old-src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 660 197'%3E%3C/svg%3E" data-src="https://www.samueljscott.com/wp-content/uploads/2020/09/linkedin-ads.png"></p>
<p>During a $3,000 campaign in March, LinkedIn reported 256 clicks with an average CPC of $11. The company received zero leads from all of the clicks.</p>
<p>“You’ll see the call to action is ‘Request Demo,'” Gellis said. “So, arguably, a user who sees this ad and clicks legitimately on it will be looking to get a demo of the analytics tool we were marketing. If the ads were giving us behavior like this on a $3,000-per-month budget, it’s unlikely they would have performed any better with additional budget.”</p>
<p>RMG charges that LinkedIn’s advertising is on a closed network and does not follow the Interactive Advertising Bureau’s <a href="https://www.iab.com/guidelines/iab-measurement-guidelines/" target="_blank" rel="noopener noreferrer">standards for digital advertising</a>. (The IAB did not respond to a request for comment.)</p>
<p>After reviewing RMG’s findings, a LinkedIn spokesperson sent me the following company statement.</p>
<p>“LinkedIn is a members-first organization and our&nbsp;<u><a title="https://www.linkedin.com/legal/ads-policy" href="https://www.linkedin.com/legal/ads-policy" target="_blank" rel="noreferrer noopener" data-saferedirecturl="https://www.google.com/url?q=https://www.linkedin.com/legal/ads-policy&amp;source=gmail&amp;ust=1599566456486000&amp;usg=AFQjCNFFsmfrYAHwHhV6tM-nG54kogFWEw">Ads program</a></u>&nbsp;is designed to ensure that only high-quality, relevant ads are served to our members. We prohibit the use of bots or other automated fraudulent methods to access our services, as they are in violation of the&nbsp;<u><a title="https://www.linkedin.com/legal/user-agreement" href="https://www.linkedin.com/legal/user-agreement" target="_blank" rel="noreferrer noopener" data-saferedirecturl="https://www.google.com/url?q=https://www.linkedin.com/legal/user-agreement&amp;source=gmail&amp;ust=1599566456486000&amp;usg=AFQjCNErEgCF6J2mC2B2u6Rnd32Ialafiw">User Agreement.</a></u>&nbsp;Additionally, LinkedIn is a&nbsp;member of the IAB&nbsp;and works closely with the organization and its members to help develop standards for the digital advertising industry.”</p>
<p>But when summarizing his findings in the video, Gellis is skeptical of LinkedIn.</p>
<p>“As far as we can tell, LinkedIn is basically a money pit,” he says. “You’re not going to get the performance that you would expect out of the network.”</p>
<p><strong>(Note: For more, <a href="https://www.samueljscott.com/2019/02/26/fake-online-fake-internet/" target="_blank" rel="noopener noreferrer">see my recent talk on how much of the Internet is fake</a>.)</strong></p>
<p><b><i>Thanks for reading! Follow me on <a href="http://twitter.com/samueljscott">Twitter</a> and see my <a href="https://www.samueljscott.com/marketing-speaker/">marketing speaker</a> page to have me visit your conference or company.</i></b></p>
					</div></div>]]>
            </description>
            <link>https://www.samueljscott.com/2020/09/08/linkedin-ad-fraud/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24407432</guid>
            <pubDate>Tue, 08 Sep 2020 11:44:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Chef to be acquired by Progress]]>
            </title>
            <description>
<![CDATA[
Score 158 | Comments 126 (<a href="https://news.ycombinator.com/item?id=24407323">thread link</a>) | @snorlaxhugsy
<br/>
September 8, 2020 | https://blog.chef.io/the-fourth-chapter-of-chef-has-arrived-progress-to-purchase-chef/ | <a href="https://web.archive.org/web/*/https://blog.chef.io/the-fourth-chapter-of-chef-has-arrived-progress-to-purchase-chef/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

	<div id="primary">
		<main id="main" role="main">

		
<article id="post-24840">
	<!-- .entry-header -->

	<div>
					
<p><span>Chefs,</span></p>
<p><span>We are excited to announce that Chef has signed a definitive agreement to be acquired by </span><a href="http://www.progress.com/"><span>Progress</span></a><span>. </span><span>We expect the transaction&nbsp;to be finalized over the next 30 days&nbsp;or so and, once completed,&nbsp;Chef&nbsp;will become an integral part of Progress.&nbsp;</span></p>
<p><span>Progress is a trusted provider of some of the best products to develop, deploy and manage high-impact business applications. Chef will bolster that position by providing continuous delivery, application delivery, dependable compliance, remediation automation, desktop management and more.</span> <span>You, our customers and community, will benefit from working with an established, publicly traded company with significantly greater resources that will provide extensive support for our open source projects and product roadmap.&nbsp;</span></p>
<p><span>Until the deal closes, Chef will remain independent and continue its normal business operations.&nbsp; </span><span>Once the deal is complete, Progress will be focused on continuing and executing on Chef’s business model and product roadmaps and supporting your business</span><span> and our combined communities. Our unwavering commitment to your success will continue and remains a top priority, as Chef enters this new phase as part of the global Progress Software family.&nbsp;</span></p>
<p><span>We look forward to the opportunities that lie ahead and to the benefits the acquisition will provide.</span></p>
<p><span>For more information on the transaction, please see <a href="https://www.progress.com/blogs/progress-to-acquire-chef">this great post</a></span><span>&nbsp;from Progress CEO Yogesh Gupta.</span></p>
<p><span>Barry Crist<br></span><span>Chief Executive Officer, Chef</span></p>
			</div><!-- .entry-content -->

	
	
				
	
		
<div>
							<div>
				<p><a href="#"><img src="https://secure.gravatar.com/avatar/4d50999463b9ae09ae36afd69f82397d?s=98&amp;d=mm&amp;r=g" width="98" height="98" alt="Avatar"></a></p><div>
					<p><strong>Barry Crist</strong></p><p>				
						Barry Crist has more than 20 years of experience in driving enterprise customer success with open source and DevOps software solutions and is a recognized leader in driving a culture of innovation. Barry joined Chef as CEO in 2013 and has been a leading force behind the company’s business operations, culture and technology innovation.					</p>
				</div>
			</div>
			</div>

		<a id="merch-banner" href="https://pages.chef.io/cloudmigrationwp.html" target="_blank">
			<img src="https://blog.chef.io/wp-content/uploads/2019/09/Blog-Merch-Bottom-Banner.jpg">
		</a>
	
	
	

</article><!-- #post-## -->

		</main><!-- #main -->
	</div><!-- #primary -->


<!-- #secondary -->

	</div></div>]]>
            </description>
            <link>https://blog.chef.io/the-fourth-chapter-of-chef-has-arrived-progress-to-purchase-chef/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24407323</guid>
            <pubDate>Tue, 08 Sep 2020 11:25:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Using Nvidia Jetson and OpenDataCam to Explore Computer Vision and IoT Analytics]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24407272">thread link</a>) | @Dwolb
<br/>
September 8, 2020 | https://www.hologram.io/blog/using-nvidia-jetson-and-opendatacam | <a href="https://web.archive.org/web/*/https://www.hologram.io/blog/using-nvidia-jetson-and-opendatacam">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>Computer vision is an incredibly fast growing field, and recent developments have made it possible to quickly start experimenting with almost no previous experience. In this post we’ll show you how to set up a practical computer vision analytic system using the $99 Nvidia Jetson Nano Developers Kit, running OpenDataCam. With OpenDataCam we can recognise, track and count people and a variety of vehicles from a USB webcam feed. We will send the collected data to InfluxDB for visualization and analysis, and set up the Jetson Nano for remote operation and management using a cellular modem and a Hologram SIM.<br></p><p>The video below shows a recording of a live field test I did outside a filling station, counting vehicles and pedestrians travelling in different directions.<br></p><p><a href="https://youtu.be/bjrMs8JFYdo">https://youtu.be/bjrMs8</a><br></p><p>To get started, you will need the following hardware:</p><ul role="list"><li>Jetson Nano Developers kit</li><li>5V 4A Power Supply with barrel connector</li><li>Micro SD Card, class 10 or better, 64 GB recommended</li><li>Wi-Fi adaptor (<a href="https://www.sparkfun.com/products/15449">Edimax N150</a> or <a href="https://www.sparkfun.com/products/15841">Intel 8265.NGWMG M.2 card</a> recommended) or Ethernet cable</li><li>USB Webcam</li><li>4G Modem - USB or Raspberry Pi Hat (Tested with D-Link DWM-222)</li><li>Hologram SIM Card</li><li>HDMI monitor</li><li>USB Mouse and Keyboard</li></ul><figure><p><img src="https://assets-global.website-files.com/5d716c2c4df04f586b7912e3/5f2c5dacd16d7c48f0c455f9_20200724_143026.jpg" alt=""></p></figure><h2>How to set up the Nvidia Jetson Nano<br></h2><p>The Nvidia Jetson Nano is a powerful little single board computer with a GPU for running neural networks to do things like image classification, object detection or speech processing. It’s size and low cost means that it’s perfect for edge computing applications like retail analytics or various industrial uses. Combined with a cellular modem and USB webcam, it allows for quick and easy remote deployments, with the only field requirement being a power supply.<br></p><p>The Jetson Nano has no shortage of documentation and online resources, with Nvidia really having put effort into making machine learning approachable for almost anyone. It’s possible to get a basic object demo running in <a href="https://news.developer.nvidia.com/realtime-object-detection-in-10-lines-of-python-on-jetson-nano/">10 lines of python code</a>.</p><ol role="list"><li>Download the latest <a href="https://developer.nvidia.com/embedded/downloads">SD card image</a> for the Jetson Nano</li><li>Flash the image to the SD Card from your computer. On a Windows PC I prefer <a href="https://www.balena.io/etcher/">BalenaEtcher</a>. If you haven’t done this before, complete instructions are on the <a href="https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#write">Nvidia website</a>.</li><li>If you're using the barrel socket power supply, place a jumper on the J48 pins, just behind the barrel socket. This deactivates the micro USB port as a power supply.</li><li>Insert the micro SD card into the slot on the Nano</li><li>Plug in the power supply, mouse, keyboard, and Wi-Fi adaptor or network cable<br></li></ol><p>The Jetson should now start up, and direct you through the system and network configuration, and login credentials set up. On the user setup page select the “Log in automatically” to allow all services to start without user intervention.<br></p><p>Once done, you should see the desktop. Open a terminal window and do the following:</p><p>Check for updates, and update all packages. This might take a while.</p><p><code>
sudo apt update &amp;&amp; sudo apt upgrade
</code></p><p>‍</p><p>Install the Nano text editor and curl</p><p><code>
sudo apt install nano curl
</code></p><p>‍</p><p>For deployment a cellular connection is really convenient, but for testing and initial setup and ethernet or WiFi connection is quicker to set up. If you are using a Wi-Fi adaptor for testing, disable Wi-Fi power management to improve connection stability.</p><p><code>
sudo nano /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf
</code></p><p>‍</p><p>It will open a file with the following file</p><p><code>
[connection]
wifi.powersave = 3
</code></p><p>Disable Wi-Fi power saving by changing <strong>3</strong> to <strong>2</strong>.<br></p><p>‍</p><p>Get the IP address of your Jetson Nano by running</p><p><code>
ifconfig
</code></p><p>Restart the Jetson before continuing. Any command line operation from this point forward can be done either directly on the Nano with the keyboard, mouse and monitor, or you can use SSH from any computer on the same network.&nbsp;</p><p>Now that you have a running Nano, it’s time to jump into the computer vision software.<br></p><h2>How to Install OpenDataCam</h2><p><a href="https://github.com/opendatacam/opendatacam">OpenDataCam</a> is an open source tool for computer vision analytics, that can track and count objects in almost any video feed. It is probably the easiest to use and setup tool that I have seen for this purpose. It is licensed under the permissive MIT license, which allows for use in commercial products.</p><p>‍<br>OpenDataCam runs on the Docker platform, and requires access to CUDA, Nvidia’s tool for running parallel processing tasks on GPUs. First we need to make sure CUDA is defined in the PATH on the Nano, by editing the <em>.bashrc</em> file</p><p><code>
sudo nano .bashrc
</code></p><p>‍<br></p><p>Add the following two lines to the end of the file, then save and close it.</p><p><code>
export PATH=${PATH}:/usr/local/cuda/bin
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda/lib64
</code></p><p>Increase swap partition size of the Jetson to 6 GB to improve performance and reliability</p><p><code>
git clone https://github.com/JetsonHacksNano/installSwapfile
cd installSwapfile
chmod 777 installSwapfile.sh
./installSwapfile.sh
</code></p><p>‍<br></p><p>Install Docker-compose and dependencies</p><p><code>
sudo apt install -y python3-pip libssl-dev libffi-dev python-openssl
sudo pip3 install docker-compose
</code></p><p>Allow docker to run on startup</p><p><code>
sudo systemctl enable docker
</code></p><p>Install OpenDataCam. You will be asked for your sudo password during this process, and it may take a while.</p><p><code>
mkdir ~/opendatacam
cd ~/opendatacam
wget -N https://raw.githubusercontent.com/opendatacam/opendatacam/v3.0.1/docker/install-opendatacam.sh
chmod 777 install-opendatacam.sh
./install-opendatacam.sh --platform nano
</code></p><h2>How to configure OpenDataCam</h2><p>Once the installation is complete, open Chromium and go to <em>localhost:1880</em> from the Jetson, or <em>*JetsonIP*:1880</em> from any computer on the same network. Once OpenDataCam has started, you will see the following video feed. It is a demo file included in OpenDataCam and demonstrates its object detection capabilities. We’ll use this as an example to get familiar with the interface before changing the video feed for our specific use case.</p><figure><p><img src="https://assets-global.website-files.com/5d716c2c4df04f586b7912e3/5f29889973b49b0b09f6e922_UFzR3FTem-uv1i--ef_6ImhdEpBDpt2KfhSRH-nKZV0pb-fzcXfRg8O0PQY3TbOj2PztqHk-4w3Vd2TxSfclRoaW-4s2Y4LzfuUgjATLtWqQF4bczSNDX8J6xo4QGwdqlm1tRpqq.png" alt=""></p></figure><p>Click on the “Pathfinder” button in the upper left corner, and you will see the “tracks” generated by each car as it’s identified and tracked.<br></p><p>To count the vehicles, click the “Counter” button to add counting lines as shown below. These lines act as checkpoints, counting objects that pass over them. In the example below, I’ve added lines for oncoming, leaving, and crossing traffic. You can also toggle the direction of travel for objects to be counted, by clicking on the arrow in the centre of the line. To start counting, click the “Start recording” button.</p><figure><p><img src="https://assets-global.website-files.com/5d716c2c4df04f586b7912e3/5f298899354fcf9d9df68f0d_GLWKyNi1Xs8Jq7lWGDHkCKCNnvh9syNBk2Lk-1gC1OEQEvHgJEs21VJ8oPYtk1Wc-nbns8Kj_GdcqaA25lKE2UaPU82ZRAQzUcFRCuBuUh23XW0RZsZaK3c3CIqIKH6-UhMRzuhd.png" alt=""></p></figure><p>To increase the reliability of the counters, it is important to place them in areas with high detection confidence. By clicking the hamburger menu in the upper right-hand corner, you can activate the tracker accuracy heatmap, which will highlight the areas with the <strong>lowest</strong> detection confidence levels.</p><figure><p><img src="https://assets-global.website-files.com/5d716c2c4df04f586b7912e3/5f2988983b96a20a4ae37c21_D6BXJhyf_Dj-V7Ee6Xfa5q2Np7CTB_E5WITOkrOwuLSi73JBZjK2gWLf5Iz36E1Bn71D6-om6ao_-lE2HugxczT_NT_y0ojc_2DHNJGRQnvQ3adJOsHddTUM8LhB3z7tXt8GVYzL.png" alt=""></p></figure><figure><p><img src="https://assets-global.website-files.com/5d716c2c4df04f586b7912e3/5f29889972d197251894544b_BuLB_wQ_upuv3Vg3rZMNuyXJmQ0GNx0UyaJB5cmIJ-uJbQEcKPVdKE56PmnNvJBW_ft_Dl2oio5QcaScWLRZoK506Yx-L4BfFnNc8uAo4cmMGXlW1jBLDMBAA_D5DD02V-ZifngK.png" alt=""></p></figure><p>This means that you should avoid these areas when placing counting lines. It might also be a good idea to move the camera to a different perspective, or even improve the detection model using <a href="https://towardsdatascience.com/training-yolo-for-object-detection-in-pytorch-with-your-custom-dataset-the-simple-way-1aa6f56cf7d9">transfer learning</a>.<br></p><p>If you have a sample video file that you want to test, you can simply drag and drop it into the OpenDataCam window and it will start playing the new file.<br></p><p>To use a webcam or IP camera stream, you need to <a href="https://github.com/opendatacam/opendatacam/blob/master/documentation/CONFIG.md#video-input">edit the config.json</a> file in ~/opendatacam/ to specify the desired video source. Complete details are available on the <a href="https://github.com/opendatacam/opendatacam/blob/master/documentation/CONFIG.md#video-input">OpenDataCam Github page</a>, including all the other settings you can change. For now will stick to the demo file while we link all the different parts of the project together, and I will describe the final setup I did for deployment at the end of the post.</p><p>To load and updated config file, restart the Docker container</p><p><code>
sudo docker-compose restart
</code></p><h2>How to Install Node-RED for Cloud Data Collection and Analysis</h2><p>I want to collect traffic data in set intervals and store it for analysis. InfluxDB is a database solution built specifically for time series data, which is exactly what we get from OpenDataCam. It also has some built-in visualisation tools. You can install and run InfluxDB locally on the Jetson, but in a production environment we will likely have multiple sources of data, so sending it to the cloud for analysis makes more sense.<br></p><p>OpenDataCam provides a simple but effective <a href="https://opendatacam.github.io/opendatacam/apidoc/">API </a>for interacting with it and extracting data, but we need to create a simple app to do this. I’ll be using Node-RED, flow-based GUI wrapper for Node.js, which also allows us to see at a glance how data flows through the app, and quickly make changes</p><figure><p><img src="https://assets-global.website-files.com/5d716c2c4df04f586b7912e3/5f298898cf4cb2839161f831_wUmPP-WAF2sNZpv2qt-pTAv6j4twJzsVmUR3MPQSDpX9roRV8JLq8PxcGqSejBnJc7z-RTxFWb7XIFEDAJNGIBgcPjBmHh4DDmMNuapQ3jL2agcDopb5UghwI4yRnsSB_TFLrEIp.png" alt=""></p></figure><p>The above screenshot gives you a good idea of how the flow works. First it checks the status of OpenDataCam (ODC), and starts it if it is not running yet, and if no recording is active, it starts one. If a recording is running, it retrieves the recording and then stop it, and immediately starts a new recording. The data from the completed recording is retrieved, and sent to InfluxDB.<br></p><p>This process is repeated at intervals set in the blue <em>timestamp</em> inject node. The dark green nodes provide output for debugging purposes.<br></p><p>To install Node-RED, open a terminal on your Jetson, and run:</p><p><code>
bash (curl -sL https://raw.githubusercontent.com/node-red/linux-installers/master/deb/update-nodejs-and-nodered)
</code></p><p>Install additional nodes for easy interaction with InfluxDB</p><p><code>
cd ~/.node-red &amp;&amp; npm install node-red-contrib-stackhero-influxdb-v2
</code></p><p>We want to let Node-RED start automatically when the Jetson starts</p><p><code>
sudo systemctl enable nodered.service
</code></p><p>Start Node-RED as a background service</p><p><code>
node-red-start
</code></p><p>Open the Node-RED UI by going to <em>localhost:1880</em> from the Jetson, or <em>*JetsonIP*:1880</em> from any computer on the same network.</p><p>To import the flow, click the menu icon in the upper right corner, and select <em>Import</em>.</p><p>Copy the flow code from this <a href="https://github.com/CuriousMongoose/opendatacam-nodered-influxdb/blob/master/nodered-flow.json">GitHub repo</a>, paste it into the import window, and click <em>Import</em>. The flow will now show in your Node-RED editor. Before we can deploy it, we need to set up InfluxDB to receive data, and get its authentication details.</p><h2>How to configure InfluxDB Cloud</h2><p>Now we need to set up a InfluxDB instance to receive the data. First go to the <a href="https://www.influxdata.com/products/influxdb-cloud/">InfluxDB Cloud page</a>, register for a free account, and create an instance on the cloud platform of your choice. I used AWS.&nbsp;</p><figure><p><img src="https://assets-global.website-files.com/5d716c2c4df04f586b7912e3/5f29889817f6f13ef9861283_iy66hKVJonggHuEE97zGCmEVwrBnvAR0HzJKug2-sTenFZKkAqmzCzpErKJZ_FLEBe6JGaS8v3kr9Kg05pUCwDJKJtK-cwoFw27Jr4ULD90M90_tiYSn5X5kffZhbMXp1udSCsFR.png" alt=""></p></figure><p>Once the instance is created and you are logged in, go to the <em>Buckets</em> tab on the <em>Dat…</em></p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.hologram.io/blog/using-nvidia-jetson-and-opendatacam">https://www.hologram.io/blog/using-nvidia-jetson-and-opendatacam</a></em></p>]]>
            </description>
            <link>https://www.hologram.io/blog/using-nvidia-jetson-and-opendatacam</link>
            <guid isPermaLink="false">hacker-news-small-sites-24407272</guid>
            <pubDate>Tue, 08 Sep 2020 11:15:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why secrets like API keys in Git are such a problem]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24407232">thread link</a>) | @mackenzie-gg
<br/>
September 8, 2020 | https://blog.gitguardian.com/secrets-credentials-api-git/ | <a href="https://web.archive.org/web/*/https://blog.gitguardian.com/secrets-credentials-api-git/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
                <div>
                    <p>Secrets in version control systems (VCS) like git is the current state of the world <strong>despite widely being recognized as a bad practice</strong>. Once source code enters a git repository, it can organically spread into multiple locations. This includes any secrets that may be included within. But why then are secrets in git repositories so common? </p><!--kg-card-begin: html--><!--kg-card-end: html--><p><em>This is the second in a series of articles about secrets within source code and will look specifically at why secrets within git repositories is such a plague, why it is so dangerous and how to prevent it. </em></p><!--kg-card-begin: html--><hr id="1"><!--kg-card-end: html--><h2 id="why-secrets-end-up-in-git">Why secrets end up in git</h2><p>A seasoned developer may be scratching their heads wondering why anyone may put secrets inside a git repository. <strong>But the fact is, secrets inside git repositories is the current state of the world. </strong></p><p><a href="https://blog.gitguardian.com/secret-sprawl/">Last article</a> we talked about how it is common to choose the path of least resistance when it comes to accessing and distributing secrets. <strong>Git acts as the central point of truth for a project</strong>, so it makes sense, at least from a convenience point of view, that secrets are stored inside a private git repository to make distribution and access easy. </p><blockquote>But storing secrets like this is playing with fire, it only takes a very small incident to get burnt. </blockquote><p>In addition to intentionally storing secrets in git, <strong>when secrets are not managed properly, it is very easy to lose track of them</strong>. Secrets may be hardcoded into source code, stored as text file, shared over Slack or buried inside a debug application log. In addition, developers can be in large distributed teams with access to a plethora of secrets while being faced with reduced release cycles and an ever growing number of technologies to master. </p><!--kg-card-begin: html--><hr id="2"><!--kg-card-end: html--><h2 id="why-secrets-in-git-is-dangerous">Why secrets in git is dangerous</h2><p>Source code, we have to remember, is very leaky. Code is copied and transferred everywhere. Git is designed in a way that allows, even promotes, code to be freely distributed. </p><p>Projects can be cloned onto multiple machines, forked into new projects, &nbsp;distributed to customers, made public so on and so forth. <strong>Each time it’s duplicated on git, the entire history of that project is also duplicated.</strong></p><p>Why storing secrets in public repositories is bad will be obvious. They are freely available to everyone on the internet and it is very easy to monitor public repositories, GitHub has a public API to fetch all public commits for example. <br></p><p><strong>But what about private git repositories?</strong><br></p><p>Private repositories don’t publish your source code to the internet openly, but it doesn’t have adequate protection to store such sensitive information either. <strong>Imagine if there was a plain text file with all your credit card numbers within it, you hopefully wouldn’t put this into the companies git repository, secrets are just as sensitive. </strong></p><p><strong>A few things to consider when storing secrets in private repositories:</strong></p><ul><li>Everyone in the organization with access to the repo has access to the secrets within (one compromised account can provide an attacker access to a trove of secrets).</li><li>Repositories can be cloned onto multiple machines or forked into new projects.</li><li>Private repositories can be made public which can have secrets buried in the git history.</li></ul><p>Another important consideration is that <em><strong>code removed from a git repository is never actually gone.</strong></em></p><p>Git keeps track of all changes that are made. Code that is removed - or more technically correct: code that is committed over - still exists within the git history.</p><p>Interestingly enough, code is removed from a project at near equal volume that is added. This means that the code within repositories are much deeper than the first layer and secrets could be buried deep within the git history under a mass of commits that have been long forgotten. </p><figure><img src="https://lh6.googleusercontent.com/iPPNU4jrj_CRg3G1zdZ8dFranXUHL7uDUvUk3c3IoLgp-Ub4iNPrpxe8JGjXBl1ntarRIy1Izu5FByy8OFzOOQr8gQA5OCnnvaG_p20IOPQQmaDpHX2LyLJ6IRI0laPFlIs-Ez0O" alt="git history added files and deleted files"><figcaption>git contributions graph</figcaption></figure><p><a href="https://github.com/hashicorp/vault/graphs/code-frequency"><em>https://github.com/hashicorp/vault/graphs/code-frequency</em></a><em> </em><br></p><blockquote>Comment: The contributions graph that you see above from HashiCorp Vault repository is a typical view of a project's history. The regularity you find in project contribution graphs is both surprising and interesting (check out some projects graphs, it seems to be a rule of nature). <br></blockquote><!--kg-card-begin: html--><hr id="3"><!--kg-card-end: html--><h2 id="real-world-examples-recent-data-breaches">Real world examples: recent data breaches</h2><p>Secrets being leaked into public places happens with surprising regularity.</p><p>If you perform a search on GitHub for the commit message <em>‘removed aws key’</em>, you will find thousands of results. And that's just within public repositories.</p><figure><img src="https://lh4.googleusercontent.com/5WbkGwTV2zn3HJ-qtHQu8nJFOkjyd5zSdh2CRAemcZLaxw7N7KILSMhpjmdCKyxpwAjVokYUWKooWKJ4CYdK3_GmAgieNFx-5I7qDTXUcNCTsfwemi9Qum9-4qkKd5IV0bqEhqkZ" alt="Removed AWS keys search on GitHub"><figcaption>GitHub commit remove credential</figcaption></figure><p><a href="https://github.com/search?q=removed+aws+key&amp;type=Commits">https://github.com/search?q=removed+aws+key&amp;type=Commits</a></p><p>GitGuardian detects over 3,000 leaked secrets each day within public GitHub alone, there are thousands of examples for this but below are a couple recent or noteworthy examples.</p><p><em>Publicly disclosed examples of recent data breaches through leaked credentials.</em></p><blockquote><strong>Starbucks Data Breach - January 2020 </strong><br><a href="https://hackerone.com/reports/716292">JumpCloud API key found in GitHub repository</a></blockquote><blockquote><strong>Equifax Data Breach - April 2020</strong><br><a href="https://hackerone.com/reports/694931">leaked secrets in personal GitHub account granted access to sensitive data for equifax customers</a></blockquote><blockquote><strong>Uber Data Breach - October 2016</strong><br><a href="https://www.ftc.gov/system/files/documents/federal_register_notices/2018/04/152_3054_uber_revised_consent_analysis_pub_frn.pdf">Poor password hygiene allowed intruders to access Uber’s Amazon S3 Datastore using an AWS access key posted in a private GitHub repository.</a></blockquote><p>If this seems like an issue for only large companies to worry about, it’s not. Attackers are constantly exploiting personal services through secret keys too. In one example, <strong>bad actors scanned GitHub for AWS keys and used them to mine cryptocurrency, leaving <a href="https://www.theregister.com/2015/01/06/dev_blunder_shows_github_crawling_with_keyslurping_bots/">developers with thousands of dollars in debt</a></strong>.</p><figure><blockquote data-width="550"><p lang="en" dir="ltr">[RESEARCH IN PROGRESS] We "forgot" to hide AWS keys in the code of a public repo on <a href="https://twitter.com/github?ref_src=twsrc%5Etfw">@github</a> . In 10 minutes keys were exploited from two different IPs.</p>— Bob Diachenko (@MayhemDayOne) <a href="https://twitter.com/MayhemDayOne/status/1286230135885238273?ref_src=twsrc%5Etfw">July 23, 2020</a></blockquote>

</figure><!--kg-card-begin: html--><hr id="4"><!--kg-card-end: html--><h2 id="detecting-secrets-in-reviews-or-not-">Detecting secrets in reviews (or not)</h2><p>One great advantage of git is to be able to quickly and clearly see changes made and compare previous and proposed code states. It’s therefore common to believe that if secrets are leaked in source code, they of course will be detected within a code review or in a pull request.</p><blockquote>Code reviews are great for detecting logic flaws, maintaining good coding practices and keeping code quality high. But they are not adequate protection for detecting secrets. </blockquote><p>This is because reviews generally only consider the net difference between the current and proposed state. Not the entire history of a branch. Branches are commonly cleaned before being merged into the master branch, temporary code is added then deleted, unnecessary files added then removed.. But now these files, which are high risk candidates for containing secrets, are not visible to the reviewer (unless they want to go through the entire history of a branch). <br></p><figure><img src="https://lh6.googleusercontent.com/XZOq-SU3EwpmQM6YEo52h7jk1vUM99PL-IJSs5wbRfioPpiAqIl8Ss2zm6MhZPWGLQ8bv_52YMcMB-qPGhb98e0b52cYMNJQYJB8-FXTE1URxxa9g5_2gpwmqT2iqySjDYXhrfCe" alt="Looking for secrets in git history"><figcaption>Git code review diagram</figcaption></figure><p>Let's walk through the example above. While this is over simplified it tells a familiar story. </p><p><br>Commit B a file named main.py is added. A new branch is created to add a new function to main.py in commit C, this feature uses an API key so to save time for testing this is hardcoded. Once the feature is working the hardcoded API key is replaced with an env variable and the file is cleaned. Finally a pull request is made and accepted because the reviewer looks at the net difference between commit B and D, ignoring commit C. Now undetected secrets are buried in the git history of the project.</p><p>While this scenario is very basic, add in hundreds of commits and files between master and a development branch and you can see how easy it is to miss secrets in code reviews. </p><figure><blockquote data-width="550"><p lang="en" dir="ltr">Wow. Open sourced a private repo. Soon after I got notified from <a href="https://twitter.com/GitGuardian?ref_src=twsrc%5Etfw">@GitGuardian</a> that there is an old commit (where I removed a Terraform tfstate file). Thanks good people. You've got my back GG! 🙏 <a href="https://t.co/2rpvjjuXcI">https://t.co/2rpvjjuXcI</a></p>— Stefan Scherer (@stefscherer) <a href="https://twitter.com/stefscherer/status/1096353637540999169?ref_src=twsrc%5Etfw">February 15, 2019</a></blockquote>

</figure><!--kg-card-begin: html--><hr id="5"><!--kg-card-end: html--><h2 id="using-automated-detection-to-find-secrets-in-git">Using automated detection to find secrets in git</h2><p>Taking in consideration all we have just discussed about secrets inside git, it is clear that this is a problem that will persist and one we cannot solve with human code reviews. While automation is not always the answer, detecting secrets, in particular secrets inside git, automated secrets detection is a clear solution to this widespread problem. </p><p>Unfortunately detecting secrets in git is not quite as easy as it first seems because of the probabilistic nature of secrets. This makes it hard to distinguish between a true secret and other random-looking strings like database IDs or other hashes.</p><p>The good news however, is that <a href="https://dashboard.gitguardian.com/">GitGuardian</a> has built powerful tools for developers to detect secrets in git. A great dashboard with native GitHub and GitLab integrations, a CLI tool called <a href="https://github.com/GitGuardian/gg-shield">GG-Shield</a> or you can even build custom your own git secrets scanner using the <a href="https://api.gitguardian.com/">GitGuardian API</a>. </p><!--kg-card-begin: html--><!--kg-card-end: html--><h2 id="wrap-up">Wrap up</h2><p>Let’s have a quick review of what we have gone through. Git repositories are very common places to find secrets and they remain the perfect incubator for secrets to sprawl into multiple locations. Git keeps a track of a project's history which can be deep making finding secrets difficult. Because of the workflow git creates, it is common for any secrets to be missed during manual checking procedures and automated secrets detection should be introduced into the SDLC.</p><p>Curious of how secrets detection works? Next episode in the secrets in source code series we are going to dive into the mechanics of secrets detection including why probabilistic algorithms are so tricky and the secret sauce behind making them work.</p><!--kg-card-begin: html--><!-- Begin Mailchimp Signup Form -->





<!--End mc_embed_signup--><!--kg-card-end: html-->
                </div>
            </section></div>]]>
            </description>
            <link>https://blog.gitguardian.com/secrets-credentials-api-git/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24407232</guid>
            <pubDate>Tue, 08 Sep 2020 11:08:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[PandaDoc employees arrested in Belarus after founders protest against violence]]>
            </title>
            <description>
<![CDATA[
Score 435 | Comments 184 (<a href="https://news.ycombinator.com/item?id=24406366">thread link</a>) | @perch56
<br/>
September 8, 2020 | https://savepandadoc.org/en/ | <a href="https://web.archive.org/web/*/https://savepandadoc.org/en/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://savepandadoc.org/en/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24406366</guid>
            <pubDate>Tue, 08 Sep 2020 08:29:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why I link to Wayback Machine instead of original web content]]>
            </title>
            <description>
<![CDATA[
Score 548 | Comments 234 (<a href="https://news.ycombinator.com/item?id=24406193">thread link</a>) | @puggo
<br/>
September 8, 2020 | https://hawaiigentech.com/post/commentary/why-i-link-to-waybackmachine-instead/ | <a href="https://web.archive.org/web/*/https://hawaiigentech.com/post/commentary/why-i-link-to-waybackmachine-instead/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main">
      <div>
        <div id="content">
          <article>
    
    

    
    <div>
      <p><img src="https://hawaiigentech.com/post/commentary/why-i-link-to-waybackmachine-instead/wayback.jpg" alt="WayBackMachine"></p>
<hr>
<p>When linking to a page for the <strong>purpose of reference</strong>, it seems better to me to <em><strong>link to the <a href="https://archive.org/">archive</a> of a given page</strong></em>, rather than to the original site itself.</p>
<p>This ensures that after some years have gone by, <strong>my article is guaranteed to be consistent</strong>. Due the  changing nature of the web, there is a chance that after some years, the link could lead to a:</p>
<ul>
<li>404 /  Not Found (most common)</li>
<li>Changed or edited content, or entirely replaced content</li>
<li>Content that, due to a rise in popularity, is now shielded, demanding the user to make an account to read the entire article.</li>
</ul>
<hr>

<p>Take defensive measures. To future-proof your content, rather than reference the general web, its far more reliable to link to an archive.</p>
<h2 id="example">Example:</h2>
<p>The Epoch Times wrote an article on Smartphones data-mining their users. This is the archived article here:</p>
<p><a href="https://web.archive.org/web/20190214015500/https://www.theepochtimes.com/smartphone-app-users-are-data-mined-even-when-not-using-the-apps_2787202.html">The Archive Version (fully readable)</a></p>
<h3 id="article-content-before">Article Content Before</h3>
<p>You can see its perfectly “normal” readable useful  content.</p>
<p><img src="https://hawaiigentech.com/post/commentary/why-i-link-to-waybackmachine-instead/link-content-before.png" alt="Article Link Content Before"></p>
<h3 id="article-content-after">Article Content After</h3>
<p>Now it’s spam from a site suffering financial need.</p>
<p><img src="https://hawaiigentech.com/post/commentary/why-i-link-to-waybackmachine-instead/link-content-after.png" alt="Article Link Content Before"></p>
<p>So in Feb 14 2019 your users would have seen the content you intended. However in Sep 07 2020, your users are being asked to support independent Journalism instead.</p>
<h2 id="if-an-archive-record-doesnt-exist-make-one">If an Archive Record Doesn't Exist, Make One</h2>
<p>Its worth the extra moment, in referencing a site, to make an archive of the page you wish to reference, if one does not exist. After that, immediately use the link from the archive.org entry, rather than the blog, news, info, or forum site you wish to refer to.</p>
<h2 id="in-unstable-times-take-measures-for-stability">In Unstable Times, Take Measures for Stability</h2>
<p>The web is a fast changing place. Even more during the Covid pandemic and suffering financial markets. Since times are financially harder, websites are disappearing, heaping up advertising, demanding user response, and things like this.</p>
<p>To avoid your content losing quality due to these things, linking to a solid, unchanging static copy of the page is far more reliable.</p>

    </div>

    <div>
  <p>
    <span>Author</span>
    <span>Leo Blanchette</span>
  </p>
  <p>
    <span>LastMod</span>
    <span>
        2020-09-07
        
    </span>
  </p>
  
  
</div>

  </article>
        </div>
        

  

  

      </div>
    </div></div>]]>
            </description>
            <link>https://hawaiigentech.com/post/commentary/why-i-link-to-waybackmachine-instead/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24406193</guid>
            <pubDate>Tue, 08 Sep 2020 08:03:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why Past Attempts of Software Metrics Have Failed Us]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24406030">thread link</a>) | @abyx
<br/>
September 8, 2020 | https://www.usehaystack.io/blog/software-development-metrics-pros-cons-and-why-past-attempts-have-failed | <a href="https://web.archive.org/web/*/https://www.usehaystack.io/blog/software-development-metrics-pros-cons-and-why-past-attempts-have-failed">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-w-id="0ea0c8ae-8333-a516-8908-f86a85ef9373"><p>TLDR; Software engineering is a black box. It's incredibly complex and nuanced. The industry makes attempts to stack rank and measure via simplified metrics. This is the wrong approach and hurts engineering culture. Software is complex, previous attempts don't take that into account. There is a better solution.</p><p>‍</p><p><strong>Software engineering is a black box because</strong></p><ol role="list"><li><a href="https://www.notion.so/usehaystack/Articles-f56dc18398104dca8534fc1e1ddd4ff2#990eff2e60e346b282c47abcb5687b96" target="_blank">Software engineering is complex and invisible</a></li><li><a href="https://www.notion.so/usehaystack/Articles-f56dc18398104dca8534fc1e1ddd4ff2#d32d356b92594f6cbb8a4993cdf1c2d0" target="_blank">The output of software development varies</a></li></ol><p>‍</p><p><strong>There have been several attempts at quantifying developer productivity. The limited content you'll find fits into a few categories.</strong></p><ol role="list"><li><a href="https://www.notion.so/usehaystack/Articles-f56dc18398104dca8534fc1e1ddd4ff2#128c578b6c9c489d8c53525166e0f932" target="_blank">KPIs used to measure software engineer performance</a></li><li><a href="https://www.notion.so/usehaystack/Articles-f56dc18398104dca8534fc1e1ddd4ff2#8d31dd02f20a42a6b1b586b955207ce4" target="_blank">Methods of measuring developer performance and their fall backs</a></li><li><a href="https://www.notion.so/usehaystack/Articles-f56dc18398104dca8534fc1e1ddd4ff2#dbf2216d7f134396bcfe69185e96c1a9" target="_blank">Opinion pieces on if measurement is even possible</a></li></ol><p>‍</p><p><strong>You'll notice a pattern prevalent in the space. The industry is obsessed with finding</strong></p><ol role="list"><li>Golden metrics to stack rank and compare teams and individuals.</li><li>KPIs to objectively determine an engineer's performance.</li></ol><p>‍</p><p><strong>The result?</strong></p><p>Attempts to measure software engineer and team productivity using the same, simplified metrics and KPIs.</p><p>‍</p><p><strong>But that's the wrong approach..</strong></p><ol role="list"><li><a href="https://www.notion.so/usehaystack/Articles-f56dc18398104dca8534fc1e1ddd4ff2#5f58624ed1f6432087fa3ef21378cf0b" target="_blank">Attempting to stack rank is fundamentally flawed</a></li><li><a href="https://www.notion.so/usehaystack/Articles-f56dc18398104dca8534fc1e1ddd4ff2#447680dd5e204f8a8bb526db55ebbe5c" target="_blank">'Successful outcomes' are situational</a></li><li><a href="https://www.notion.so/usehaystack/Articles-f56dc18398104dca8534fc1e1ddd4ff2#ea18251c7c0a4d50a1eb57e1e34f7ff8" target="_blank">Simplified metrics hurt engineering culture</a></li></ol><p>‍</p><p><strong>Focus on productivity, not yard sticks</strong></p><ol role="list"><li><a href="https://www.notion.so/usehaystack/Articles-f56dc18398104dca8534fc1e1ddd4ff2#c685ee9fc42c4c678d86b85f01006df8" target="_blank">Measure engineers and teams independently</a></li><li><a href="https://www.notion.so/usehaystack/Articles-f56dc18398104dca8534fc1e1ddd4ff2#dcd2a669df6a4e97afa9ec24e25fbc14" target="_blank">Compare relative to individual history</a></li><li><a href="https://www.notion.so/usehaystack/Articles-f56dc18398104dca8534fc1e1ddd4ff2#b0b75fb866ee44d29f9a5862be9e2579" target="_blank">Identify the main risks and bottlenecks of productivity</a></li></ol><p>‍</p><p><strong>Why is this better?</strong></p><ol role="list"><li><a href="https://www.notion.so/usehaystack/Footnotes-f56dc18398104dca8534fc1e1ddd4ff2#95433ef2f1cb46c5ac5b116b3221ac69" target="_blank">Accurately measures productivity</a></li><li><a href="https://www.notion.so/usehaystack/Articles-f56dc18398104dca8534fc1e1ddd4ff2#f20c40f07837457ea2dc431fbbeb4c6c" target="_blank">Handles situational outcomes</a></li><li><a href="https://www.notion.so/usehaystack/Articles-f56dc18398104dca8534fc1e1ddd4ff2#0f37d98374d64ea3a76d2b179cc16a74" target="_blank">Supports engineering culture</a></li><li><a href="https://www.notion.so/usehaystack/Footnotes-f56dc18398104dca8534fc1e1ddd4ff2#d09f4a9a554643139c1c3d9f0987e1a4" target="_blank">Enables early risk identification</a></li></ol><p>‍</p><p><strong>Main takeaway:</strong></p><p>From the nature of the work to the culture that supports it, engineering is incredibly complex and nuanced. Previous attempts at measuring engineering performance have done a poor job of taking this complexity into account. Making attempts to measure each engineer and team against the same yard stick not only hurts culture, but gives no true indication of productivity to begin with.</p><p>‍</p><p>To date, the industry has been attempting this 'holy grail' of software development metrics from the wrong perspective. By focusing on drivers and blockers of productivity rather than KPIs and stack ranking; we can start to measure engineers and teams in a healthy way. More importantly, this new perspective enables engineering leaders to introduce a software development metrics to help improve while maintaining the highest impact driver of productivity (culture).</p><p>‍</p><p>Check out Haystack's approach <a href="https://usehaystack.io/">here</a>!</p><figure><p><img src="https://uploads-ssl.webflow.com/5ed57622ee14fb96d022d544/5ef130052901e5cad8432b81_Haystack_Designed_Presentation.png" alt=""></p></figure><p><a href="https://usehaystack.io/?utm_source=blog&amp;utm_medium=3%20git%20signals%20to%20identify%20blockers&amp;utm_campaign=inside%20blog">Haystack</a> helps engineering leaders identify blockers and trends. Directly from Github. Instead of guessing if you're improving, or constantly bothering your team for progress updates, simply use Haystack to get alerts in your inbox every morning. Plus a dashboard to track improvements over time.</p><p><a href="https://usehaystack.io/?utm_source=blog&amp;utm_medium=3%20git%20signals%20to%20identify%20blockers&amp;utm_campaign=inside%20blog">Try it for free</a></p><p>‍</p><p>‍</p></div></div>]]>
            </description>
            <link>https://www.usehaystack.io/blog/software-development-metrics-pros-cons-and-why-past-attempts-have-failed</link>
            <guid isPermaLink="false">hacker-news-small-sites-24406030</guid>
            <pubDate>Tue, 08 Sep 2020 07:32:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Escape from Creek Fire]]>
            </title>
            <description>
<![CDATA[
Score 127 | Comments 66 (<a href="https://news.ycombinator.com/item?id=24405981">thread link</a>) | @twohey
<br/>
September 8, 2020 | https://www.jmeshe.co/escape-from-creek-fire | <a href="https://web.archive.org/web/*/https://www.jmeshe.co/escape-from-creek-fire">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
<p><span>
By subscribing to the mailing list of
<strong>
Jaymie Shearer
</strong>
your email address is stored securely, opted into new post notifications and related communications. We respect your inbox and privacy, you may unsubscribe at any time.
</span></p><div>
<p><a href="https://www.exposure.co/privacy" rel="noopener" target="_blank" title="Link to Jaymie Shearer Privacy Policy">
Privacy Policy
</a>
<a href="https://www.exposure.co/terms" rel="noopener" target="_blank" title="Link to Jaymie Shearer Terms of Service">
Terms of Service
</a>
<a href="https://www.exposure.co/report" rel="noopener" target="_blank" title="Report a story or story">
Report
</a>
</p></div>

</div>
</div></div>]]>
            </description>
            <link>https://www.jmeshe.co/escape-from-creek-fire</link>
            <guid isPermaLink="false">hacker-news-small-sites-24405981</guid>
            <pubDate>Tue, 08 Sep 2020 07:22:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[URL query parameters and how laxness creates de facto requirements on the web]]>
            </title>
            <description>
<![CDATA[
Score 95 | Comments 72 (<a href="https://news.ycombinator.com/item?id=24404814">thread link</a>) | @oftenwrong
<br/>
September 7, 2020 | https://utcc.utoronto.ca/~cks/space/blog/web/DeFactoQueryParameters | <a href="https://web.archive.org/web/*/https://utcc.utoronto.ca/~cks/space/blog/web/DeFactoQueryParameters">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2>URL query parameters and how laxness creates de facto requirements on the web</h2>

	<p><small>September  7, 2020</small></p>
</div><div><p>One of the ways that <a href="https://utcc.utoronto.ca/~cks/space/dwiki/DWiki">DWiki</a> (the code behind <a href="https://utcc.utoronto.ca/~cks/space/blog/">Wandering Thoughts</a>) is unusual is that it strictly validates the query parameters
it receives on URLs, including on HTTP <code>GET</code> requests for ordinary
pages. If a HTTP request has unexpected and unsupported query
parameters, such a <code>GET</code> request will normally fail. When I made
this decision it seemed the cautious and conservative approach, but
<a href="https://utcc.utoronto.ca/~cks/space/blog/web/CautionIsAMistakeToday">this caution has turned out to be a mistake on the modern web</a>. In practice, all sorts of sites will
generate versions of your URLs with all sorts of extra query
parameters tacked on, give them to people, and expect them to work.
If your website refuses to play along, (some) people won't get to
see your content. <strong>On today's web, you need to accept (and then
ignore) arbitrary query parameters on your URLs</strong>.</p>

<p>(Today's new query parameter is 's=NN', for various values of NN
like '04' and '09'. I'm not sure what's generating these URLs, but
it may be Slack.)</p>

<p>You might wonder how we got here, and that is a story of lax behavior
(or, if you prefer, being liberal in what you accept). In the
beginning, both Apache (for static web pages) and early web
applications often ignored extra query parameters on URLs, at least
on <code>GET</code> requests. I suspect that other early web servers also
imitated Apache here, but I have less exposure to their behavior
than Apache's. My guess is that this behavior wasn't deliberate,
it was just the simplest way to implement both Apache and early
web applications; you paid attention to what you cared about and
didn't bother to explicitly check that nothing else was supplied.</p>

<p>When people noticed that this behavior was commonplace and widespread,
they began using it. I believe that one of the early uses was for
embedding 'where this link was shared' information for your own web
analytics (<a href="https://utcc.utoronto.ca/~cks/space/blog/web/AnalyticsVsSecurity">cf</a>), either based on your logs
or using JavaScript embedded in the page. In the way of things,
once this was common enough other people began helpfully tagging
the links that were shared through them for you, which is why I
began to see various 'utm_*' query parameters on inbound
requests to <a href="https://utcc.utoronto.ca/~cks/space/blog/">Wandering Thoughts</a> even though I never
published such URLs.
Web developers don't leave attractive nuisances alone for long, so
soon enough people were sticking on extra query parameters to your
URLs that were mostly for them and not so much for you. Facebook
may have been one of the early pioneers here with their 'fbclid'
parameter, but other websites have hopped on this particular train
since then (as I saw recently with these 's=NN' parameters).</p>

<p>At this point, the practice of other websites and services adding
random query parameters to your URLs that pass through them is so
wide spread and common that accepting random query parameters is
pretty much a practical requirement for any web content serving
software that wants to see wide use and not be irritating to the
people operating it. If, like <a href="https://utcc.utoronto.ca/~cks/space/dwiki/DWiki">DWiki</a>, you stick to your guns and
refuse to accept some or all of them, you will drop some amount of
your incoming requests from real people, disappointing would be
readers.</p>

<p>This practical requirement for URL handling is not documented in
any specification, and it's probably not in most 'best practices'
documentation. People writing new web serving systems that are
tempted to be strict and safe and cautious get to learn about it
the hard way.</p>

<p>In general, any laxness in actual implementations of a system can
create a similar spiral of de facto requirements. Something that
is permitted and is useful to people will be used, and then supporting
that becomes a requirement. This is especially the case in a
distributed system like the web, where any attempt to tighten the
rules would only be initially supported by a minority of websites.
These websites would be 'outvoted' by the vast majority of websites
that allow the lax behavior and support it, because that's what
happens when the vast majority work and the minority don't.</p>
</div></div>]]>
            </description>
            <link>https://utcc.utoronto.ca/~cks/space/blog/web/DeFactoQueryParameters</link>
            <guid isPermaLink="false">hacker-news-small-sites-24404814</guid>
            <pubDate>Tue, 08 Sep 2020 02:50:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On writing and selling science fiction stories (2018)]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24404555">thread link</a>) | @forrestbrazeal
<br/>
September 7, 2020 | https://forrestbrazeal.com/2018/11/08/on-writing-and-selling-science-fiction-stories/ | <a href="https://web.archive.org/web/*/https://forrestbrazeal.com/2018/11/08/on-writing-and-selling-science-fiction-stories/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <section>
          <section>
              

              
              
              <article>
                  

<p>Each year, I try to set a few personal goals that stretch me in some way.</p>

<p>In 2017, for example, I <a href="https://forrestbrazeal.com/2017/12/03/how-to-read-100-books-in-a-year-and-still-have-a-life/">read one hundred books</a>, which turned out to be more a test of endurance than skill.</p>

<p>This year, I decided to see if I could write a fictional short story and get it published somewhere. Though I do a fair amount of technical writing in the course of my work, I’d never seriously attempted to write and sell a short story before, or even taken a creative writing class.</p>

<p>This meant I had no knowledge of the market, no connections, and generally no idea what I was doing.</p>

<p>So, a challenge!</p>

<h2 id="goals">Goals</h2>

<p>I decided to focus on the <a href="https://en.wikipedia.org/wiki/Speculative_fiction">speculative fiction genre</a>, partly because it seemed more accessible and mostly because unlike literary journals, the best speculative fiction magazines still pay their authors.</p>

<p>My goals, in descending order of likelihood, were as follows:</p>

<ol>
<li><p>Get something published somewhere, even if unpaid</p></li>

<li><p>Get something published and get paid something for it, even a token amount</p></li>

<li><p>Get something published at <a href="https://www.sfwa.org/about/join-us/sfwa-membership-requirements/#short">Science Fiction Writers of America (SFWA) professional rates</a> (currently 6 cents a word)</p></li>

<li><p>Get at least 1000 words published at SFWA-qualifying markets (the standard for <a href="https://www.sfwa.org/about/join-us/sfwa-membership-requirements/#associate">SFWA associate member status</a>)</p></li>

<li><p>Get at least three stories published, totaling more than 10,000 words, at SFWA-qualifying professional markets (<a href="https://www.sfwa.org/about/join-us/sfwa-membership-requirements/#active">SFWA Active Member status</a>, my self-imposed standard for a legit “professional” science fiction writer)</p></li>
</ol>

<h2 id="results-through-nov-8">Results (through Nov. 8)</h2>

<p>After many form rejections on several terrible stories, I sold my <a href="https://dailysciencefiction.com/science-fiction/biotech/forrest-brazeal/memory-foam">first story</a> on March 23rd, 2018, to Daily Science Fiction (an SWFA pro market). At 1010 words, this sale actually crossed the first four goals off my list at once.</p>

<p>However, I continued to write and improve over the spring and summer of 2018, selling several more stories along the way, eventually totaling more than 16,000 words of fiction. Some publications have long wait times, but by the time “Empathy Bee” <a href="http://www.diabolicalplots.com/the-diabolical-plots-year-five-fiction-lineup/">is published</a> in March 2019, I should have the wordcount needed for active SFWA membership status, my most insane stretch goal.</p>

<h3 id="by-the-numbers">By the numbers</h3>

<ul>
<li><p>Stories Written: 22</p></li>

<li><p>Total Wordcount: ~60,000, the length of a medium-sized novel</p></li>

<li><p>Total Submissions: 70</p></li>

<li><p>Stories Sold: 8</p></li>

<li><p>SFWA-qualifying professional sales: 6</p></li>

<li><p>Rejections: 50 (15 personal)</p></li>

<li><p>Withdrawals: 2</p></li>

<li><p>Still Pending: 10</p></li>

<li><p>Other Results: <a href="https://www.writersofthefuture.com/writers-of-the-future-3rd-quarter-standings-for-year-35/">Honorable Mention, Writers of the Future</a></p></li>
</ul>

<h2 id="what-i-learned">What I Learned</h2>

<h3 id="have-no-ego">Have no ego</h3>

<p>It turns out writing short stories that sell is really, really hard. It’s hard to think of good ideas, it’s hard to write them down in a form that anyone would want to read, and it’s even harder when those readers are magazine editors who get literally hundreds of unsolicited submissions every month. Don’t attempt it if you have an easily bruised sense of self-worth, or you will be depressed a lot.</p>

<p>I’m grateful to have sold some stories this year, but my identity is not tied up in “being a writer”. It can’t be, because I have to…</p>

<h3 id="embrace-failure">Embrace failure</h3>

<p>I learned to view rejections as a badge of honor, which is a helpful mental trick to keep from going insane after a few dozen forms. As I told Jason Bougger in an <a href="http://www.themeofabsence.com/2018/11/author-interview-forrest-brazeal/">author interview</a> over at Theme of Absence, rejections are like the good soreness you feel after working out – it means you’re growing.</p>

<h3 id="keep-your-feedback-loop-short">Keep your feedback loop short</h3>

<p>Early on, when I was getting form rejections from big magazines with no accompanying feedback, I got really frustrated because I knew my work was obviously not up to par – I just didn’t know why. A submission to a smaller publication, Abyss and Apex (who I later ended up selling a different story to!) brought back a personalized rejection with a helpful piece of advice: try signing up for the <a href="https://sff.onlinewritingworkshop.com/">Online Writing Workshop</a>.</p>

<p>At the time I had never heard of writers’ workshops and didn’t really understand why they would be helpful. But I paid a few dollars and signed up. The other writers there provided generous feedback on how to improve my work, and I learned just as much from critiquing their pieces. The OWW membership more than paid for itself when several readers pointed out an obvious plot hole in my story “Empathy Bee”, which subsequently sold in revised form to Diabolical Plots.</p>

<p>Later, once I had professional credits, I was able to join the wonderful <a href="http://www.codexwriters.com/">Codex</a> writers’ community, which has hugely expanded my horizons and understanding of the industry. The more I write, the less I trust myself to be a good judge of my work’s quality, and the more I seek out and appreciate feedback from others.</p>

<h3 id="write-smart-not-just-hard">Write smart, not just hard</h3>

<p>Some writers recommend keeping insane writing regimens, cranking out thousands of words a day, saying it’s the only way to improve. And I wrote a fair amount this year. But stepping back and getting feedback on my work was just as important.</p>

<p>I had a music teacher who used to ask: “Did you practice ten hours, or just the same hour ten times?” When I took time to evaluate my work and deliberately build on it, I improved faster than just by vomiting words indiscriminately onto the page.</p>

<h3 id="keep-reading-good-prose">Keep reading good prose</h3>

<p>No, I didn’t read a hundred books again this year. But I did try to keep my ear filled with good prose. For example, this summer I got on a southern realist kick: Flannery O’Connor, Eudora Welty, Carson McCullers. Studying how those writers crafted characters and situations helped me nail down a couple of southern-set stories that ended up selling. My science fiction writing improved more from reading good writers, period, than from reading science fiction.</p>

<h3 id="just-because-you-wrote-something-good-enough-to-get-published-somewhere-that-doesn-t-mean-the-next-thing-you-write-won-t-be-terrible">Just because you wrote something good enough to get published somewhere, that doesn’t mean the next thing you write won’t be terrible</h3>

<p>This sounds stupid in hindsight, but for a long time I had it in my head that once I sold a story, I’d have figured out what it took to get published, and I wouldn’t have any trouble after that. Instead, I still get tons of rejections, and often my writing seems just as lifeless and terrible to me as it did before I sold my first story.</p>

<p>The good news is that with practice, the overall trend appears to be upward. At least, when I look back at my work from the beginning of the year, I can’t believe how bad it is. So I must be improving, right?</p>

<h2 id="what-s-next">What’s next?</h2>

<p>Like many people who start out in the short fiction game, I would love to publish a novel. So I think that might be a 2019 goal. But I’m sure I’ll continue to write short stories, too. There’s real satisfaction in creating something that you can hold in your head all at once, knowing how it will turn out and why it’s effective.</p>

<p><em>Some of the stories I sold this year are free to read online. You can check them out in my <a href="https://forrestbrazeal.com/bibliography/">bibliography</a></em></p>

              </article>
              <center>
              <p>
                      If you enjoy my articles, comics, and stories, why not sign up for the mailing list?
                  </p>
              
            </center>
              
          </section>
          <br>
          

      </section>

    </div></div>]]>
            </description>
            <link>https://forrestbrazeal.com/2018/11/08/on-writing-and-selling-science-fiction-stories/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24404555</guid>
            <pubDate>Tue, 08 Sep 2020 01:53:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fast.ai and why Python is not the future of ML with Jeremy Howard]]>
            </title>
            <description>
<![CDATA[
Score 63 | Comments 94 (<a href="https://news.ycombinator.com/item?id=24404002">thread link</a>) | @tosh
<br/>
September 7, 2020 | https://www.wandb.com/podcast/jeremy-howard | <a href="https://web.archive.org/web/*/https://www.wandb.com/podcast/jeremy-howard">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Jeremy Howard is a founding researcher at fast.ai, a research institute dedicated to making Deep Learning more accessible. Previously, he was the CEO and Founder at Enlitic, an advanced machine learning company in San Francisco, California. </p><p>Howard is a faculty member at Singularity University, where he teaches data science. He is also a Young Global Leader with the World Economic Forum, and spoke at the World Economic Forum Annual Meeting 2014 on "Jobs For The Machines." </p><p>Howard advised Khosla Ventures as their Data Strategist, identifying the biggest opportunities for investing in data-driven startups and mentoring their portfolio companies to build data-driven businesses. Howard was the founding CEO of two successful Australian startups, FastMail and Optimal Decisions Group. Before that, he spent eight years in management consulting, at McKinsey &amp; Company and AT Kearney.</p><p><strong>TOPICS COVERED:</strong></p><p>0:00 Introduction</p><p>0:52 Dad things</p><p>2:40 The story of Fast.ai</p><p>4:57 How the courses have evolved over time</p><p>9:24 Jeremy’s top down approach to teaching</p><p>13:02 From Fast.ai the course to Fast.ai the library</p><p>15:08 Designing V2 of the library from the ground up</p><p>21:44 The ingenious type dispatch system that powers Fast.ai</p><p>25:52 Were you able to realize the vision behind v2 of the library</p><p>28:05 Is it important to you that Fast.ai is used by everyone in the world, beyond the context of learning</p><p>29:37 Real world applications of Fast.ai, including animal husbandry</p><p>35:08 Staying ahead of the new developments in the field</p><p>38:50 A bias towards learning by doing</p><p>40:02 What’s next for Fast.ai</p><p>40.35 Python is not the future of Machine Learning</p><p>43:58 One underrated aspect of machine learning</p><p>45:25 Biggest challenge of machine learning in the real world</p><p>Follow Jeremy on Twitter:</p><p><a href="https://twitter.com/jeremyphoward" target="_blank">https://twitter.com/jeremyphoward</a><br></p><p>Links:</p><p>Deep learning R&amp;D &amp; education: <a target="_blank" href="https://t.co/ZvDGNlehRt?amp=1">http://fast.ai</a></p><p>Software: <a target="_blank" href="https://t.co/GMYkPDXNW3?amp=1">http://docs.fast.ai</a></p><p>Book: <a target="_blank" href="https://t.co/1YSqXvWW87?amp=1">http://up.fm/book</a></p><p>Course: <a target="_blank" href="https://t.co/Q2qMl59EfH?amp=1">http://course.fast.ai</a></p><p>Papers:</p><p><a target="_blank" href="https://dl.acm.org/doi/10.1145/2487575.2491127"><strong>The business impact of deep learning</strong></a></p><p><a target="_blank" href="https://dl.acm.org/doi/10.1145/2487575.2491127">https://dl.acm.org/doi/10.1145/2487575.2491127</a></p><p><a target="_blank" href="https://www.linkedin.com/redir/redirect?url=http%3A%2F%2Fwww%2Ejmir%2Eorg%2F2012%2F1%2Fe33%2F&amp;amp;urlhash=gLU-&amp;trk=public_profile_publication-title"><strong>De-identification Methods for Open Health Data</strong></a></p><p><a href="https://www.jmir.org/2012/1/e33/">https://www.jmir.org/2012/1/e33/</a><br></p><p>Visit our podcasts homepage for transcripts and more episodes!</p><p><a target="_blank" href="https://www.wandb.com/podcast">www.wandb.com/podcast</a></p><p> Get our podcast on Soundcloud, Apple, and Spotify!</p><p>Soundcloud: <a target="_blank" href="https://bit.ly/2YnGjIq">https://bit.ly/2YnGjIq</a></p><p>Apple Podcasts: <a target="_blank" href="https://bit.ly/2WdrUvI">https://bit.ly/2WdrUvI</a></p><p>Spotify: <a target="_blank" href="https://bit.ly/2SqtadF">https://bit.ly/2SqtadF</a></p><p>We started Weights and Biases to build tools for Machine Learning practitioners because we care a lot about the impact that Machine Learning can have in the world and we love working in the trenches with the people building these models. One of the most fun things about these building tools has been the conversations with these ML practitioners and learning about the interesting things they’re working on. This process has been so fun that we wanted to open it up to the world in the form of our new podcast called Gradient Dissent. We hope you have as much fun listening to it as we had making it!</p><p>Weights and Biases:</p><p>We’re always free for academics and open source projects. Email carey@wandb.com with any questions or feature suggestions.</p><ul role="list"><li>Blog: <a target="_blank" href="https://www.wandb.com/articles">https://www.wandb.com/articles</a></li><li>Gallery: See what you can create with W&amp;B - <a target="_blank" href="https://app.wandb.ai/gallery">https://app.wandb.ai/gallery</a></li><li>Continue the conversation on our slack community - <a href="http://bit.ly/wandb-forum" target="_blank">http://bit.ly/wandb-forum</a><br></li></ul><p>Host: Lukas Biewald - <a href="https://twitter.com/l2k" target="_blank">https://twitter.com/l2k</a></p><p>Producer: Lavanya Shukla - <a href="https://twitter.com/lavanyaai" target="_blank">https://twitter.com/lavanyaai</a></p><p>TRANSCRIPT:</p><p><strong>Lukas: </strong>You're listening to Gradient Dissent, a show where we learn about making machine learning models work in the real world. I'm your host Lukas Biewald. Jeremy Howard created the Fast.ai course, which is maybe the most popular course to learn machine learning and there are a lot out there. He's also the author of the book Deep Learning for Coders with Fast.ai and PyTorch and in that process, he made the Fast.ai library which lots of people use independently to write deep learning. Before that, he was the CEO and co-founder of Enlitic, an exciting startup that applies deep learning to health care applications. And before that, he was the president of Kaggle, one of the most exciting earliest machine learning companies. I'm super excited to talk to him. So Jeremy, it's nice to talk to you. And in preparing the questions, I realized that every time I've talked to you there have been a few gems that I've remembered that I would never think to ask about. Like one time you told me about how you learned Chinese and another time you gave me Dad parenting advice, very specific advice and it's been actually super helpful. </p><p><strong>Jeremy: </strong>Oh great. Tell me what Dad parenting advice worked out?</p><p><strong>Lukas: </strong>Well, what you told me was when you change diapers, use a blow dryer to change a really frustrating experience to a really joyful experience and it's like such good advice. I don't know how you.. I guess I can imagine how you thought of it, but it's...</p><p><strong>Jeremy: </strong>Yeah, yeah, I know they love the whooshing sound, they love the warmth. I'm kind of obsessed about Dad things. So I'm always happy to talk about Dad things. That is this podcast.</p><p><strong>Lukas: </strong>Can we start with that? Now that my daughter is eight months old. Do you have any suggestions for her?</p><p><strong>Jeremy: </strong>Oh my goodness! Eight months old. You know, it's like the same with any kind of learning. It's all about consistency. So I think that the main thing we did right with Claire was just, you know, this delightful child now is we were just super consistent. Like if we said you can't have X unless you do Y, we would never give her X if she didn't do Y. If you want to take your scooter down to the bottom of the road, you have to carry it back up again. We read this great book that was saying if you're not consistent, it becomes like this thing, it's like a gambler. It's like sometimes you get the thing you want, so you just have to keep trying so that's my number one piece of advice. It's the same with teaching machine learning. We always tell people that tenacity is the most important thing for students. To stick with it, do it every day.</p><p><strong>Lukas: </strong>I guess just in the spirit of questions, I'm genuinely curious about, you know, you've built this amazing framework and teaching thing that I think is maybe the most popular and most appreciated framework. I was wondering if you could start by telling me the story of what inspired you to do that and what was the journey to making Fast.ai, the curriculum and Fast.ai, the ML framework.</p><p><strong>Jeremy: </strong>So it was something that my wife Rachel and I started together. Rachel has a math PhD, super technical background, early data scientist and engineer, Uber. I don't. I have just scraped by a philosophy undergrad and have no technical background. But from both of our different directions, we both had this frustration that neural networks in 2012 were super important, clearly going to change the world, but super inaccessible and so we would go to meetups and try to figure out like how do we... Like I knew the basic idea, I'd coded neural networks 20 years ago, but how do you make them really good? There wasn't any open source software at the time for running on GPUs. You know, Dan Seresen's thing was available, but you had to pay for it. There was no source code and we just thought, oh, we've got to change this, because the history of technology leaps has been that it generally increases inequality because the people with resources can access the new technology and then that leads to societal upheaval and a lot of unhappiness. So we thought, well, we should just do what we can. So we thought how are we going to fix this? Basically the goal was, and still is, to be able to use deep learning without requiring any code so that, you know, because the vast majority of the world can't code, we kind of thought, well, to get there, we should, first of all, see what exists right now? Learn how to use it as best as we can ourselves, teach people how to best use it as we can and then make it better, which requires doing research and then turning that into software and then changing the course to teach the hopefully slightly easier version and repeat that again and again for a few years. And so we're kind of in that process.</p><p><strong>Lukas: </strong>That's so interesting. Do you worry that the stuff you're teaching, you're sort of trying to make it obsolete, right? Because you're trying to build higher level abstractions? Like I think one of the things that people really appreciate your course is that it's really clear, in-depth explanations of how these things work. Do you think that that's eventually going to be not necessary or how do you think about that?</p><p><strong>Jeremy: </strong>Yeah, to some extent. I mean, so if you look at the new book and the new course, chapter one starts with really, really foundational stuff around what is a machine learning algorithm? What do we mean to learn an algorithm? What's the difference between traditional programming and machine learning to solve the same problem? And those kinds of basic foundations I think will always be useful, even at the point you're not using any code. I feel like even right now, if somebody is using like PlatformAI or some kind of code-free framework, you still need to understand these basics of an algorithm can only learn based on the data you provide. It's generally not going to be able to extrapolate to patterns it's not seen yet, stuff like that. Um, but yeah, I mean, we have so far released two new courses every year, you know, a part one and part two every year because every year, it's totally out of date. And we always say to our students at the start of part one, Look, you know, none of the details you're learning are going to be of any use in a year or two's time. There's a time when we're doing Piano and then TensorFlow and Keras, and then playing PyTorch. We always say, look, don't worry too much about the software we're using because none of it's still any good, you know, it's goal changing rapidly, you know, faster than JavaScript frameworks, but the concepts are important and yeah, you can pick up a new library and I don't know by weekend, I …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.wandb.com/podcast/jeremy-howard">https://www.wandb.com/podcast/jeremy-howard</a></em></p>]]>
            </description>
            <link>https://www.wandb.com/podcast/jeremy-howard</link>
            <guid isPermaLink="false">hacker-news-small-sites-24404002</guid>
            <pubDate>Tue, 08 Sep 2020 00:10:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Optimize Onboarding]]>
            </title>
            <description>
<![CDATA[
Score 137 | Comments 56 (<a href="https://news.ycombinator.com/item?id=24402419">thread link</a>) | @tekdude
<br/>
September 7, 2020 | https://staysaasy.com/management/2020/08/28/Optimize-Onboarding.html | <a href="https://web.archive.org/web/*/https://staysaasy.com/management/2020/08/28/Optimize-Onboarding.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>It takes roughly 2 weeks to form a habit; it takes roughly two weeks to get comfortable in a new environment. A common mistake is to treat a new report’s first couple weeks like college orientation - social, light hearted, get-to-know-you stuff. If your report spends the first two weeks reading C# documentation and having lunch out on the town with the team, guess what, they’ve just normalized that behavior as what the role is.</p>

<p>The answer: start your report doing real work as soon as possible. If they’re a software engineer, they should be committing code week one (ideally day 2 or 3); If they’re a product manager, they should be attending meetings and picking up supporting activities on a similar time frame. Then the tone has been set - working here means getting things done.</p>

<p>This means as manager you need to give your report tractable work in the first week. Managers not being ready for a report to start is the number one reason people end up normalizing underperformance, because they had nothing meaningful to do. If a manager describes a report as not having initiative in the first couple weeks it’s a red flag - it’s the manager’s job to provide new hires with clear paths to contribute immediately.</p>

<p>Another main reason people fall into onboarding traps is because your organization has painfully slow onboarding. Endless HR videos, slow security processes, a mountain of fragile technology setup - these all make for a shitty and counterproductive start at a company. Optimize your onboarding to get people doing what you hired them to do. Look to formalize performance indicators of your onboarding. For example, for engineers this could be time-to-first-commit and time-to-joining-sprints.</p>

<p>This approach holds for managers and executives as well. In that cohort it’s also common and a common red flag for people to spend the first couple weeks putzing around under the guise of “getting to know what’s going on”. Managers and executives should be involved in the decisions of the team from day 1 and developing artifacts - plans, TODO lists, strategy documents - in the first couple weeks. With good managers and executives you see them day 1 adding perspective and insight into conversations, and everyone collectively appreciates that the right person has been hired.</p>

<p>Onboarding is one of the most critical periods in a person’s time at a company. It’s one of the highest ROI periods when managing someone. Take that opportunity to set expectations properly and measure success quantiatively.</p>


    




  </div></div>]]>
            </description>
            <link>https://staysaasy.com/management/2020/08/28/Optimize-Onboarding.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24402419</guid>
            <pubDate>Mon, 07 Sep 2020 20:18:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Doom Eternal – Graphics Study]]>
            </title>
            <description>
<![CDATA[
Score 161 | Comments 52 (<a href="https://news.ycombinator.com/item?id=24401805">thread link</a>) | @todsacerdoti
<br/>
September 7, 2020 | https://www.simoncoenen.com/blog/programming/graphics/DoomEternalStudy.html | <a href="https://web.archive.org/web/*/https://www.simoncoenen.com/blog/programming/graphics/DoomEternalStudy.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="markdown-content">
            


<p>30 Aug 2020 - Simon Coenen - Reading time: <span title="Estimated read time">
  
  
    23 mins
  
</span><span></span> - <a href="#comment-section">Comments</a>
</p>
<p><img src="https://www.simoncoenen.com/images/blog/010_doom_eternal_study/image1.jpeg" alt=""></p>

<h2 id="background">Background</h2>

<p>Doom Eternal is the successor of Doom 2016. It’s developed using the 7th iteration of id Tech, id Software’s in-house game engine. Doom 2016 has inspired me greatly on a technologic level due to its simplicity and elegance while still having a high visual quality. For Doom Eternal, this is no different. Doom Eternal has improved in many areas of which a few are worth investigating which I will try to cover in this frame breakdown.</p>

<!--more-->

<p>This frame breakdown is inspired by <a href="http://www.adriancourreges.com/blog/2016/09/09/doom-2016-graphics-study/">Adrian Courreges’s study on Doom 2016</a>. I believe these graphics studies give a lot of insight into how certain rendering problems are solved in a AAA game and are greatly educational. In this breakdown I aim to stay at a high level and not go too in-depth of each rendering technique/pass. Some passes might not be covered here because they are very similar to Doom 2016 and are well covered in Adrian Courreges’s study.</p>

<p>I do want to stress here that these studies are absolutely nothing more than <strong>educational</strong>. I do not in any way support the reverse engineering for malicious purposes and stealing intellectual property. If you haven’t played the game yet, don’t worry about spoilers! The section I used for this study is in the beginning of the game which doesn’t give away any of the details.</p>

<p>Now, let’s get down to business.</p>

<p>With Id Tech 7, the engine has moved away from OpenGL and is entirely built with a <strong>Vulkan</strong> backend allowing them to make better use of current generation GPU features, bindless resources in particular.</p>

<hr>



<p><img src="https://www.simoncoenen.com/images/blog/010_doom_eternal_study/image2.png" alt=""></p>

<p>We’re looking at a section in the game close to the start. It’s an interior with a few enemies and a large portion of volumetric lighting. Just like its predecessor, Doom Eternal is using a <strong>forward rendering</strong> pipeline. Doom 2016 was mostly forward rendered with a thin G-Buffer for screen space reflections. However this time, everything is fully forward rendered omitting the G-Buffer.</p>

<hr>

<h2 id="step-away-from-mega-texture">Step away from Mega-Texture</h2>

<p>With id Tech 5 used in <a href="https://en.wikipedia.org/wiki/Rage_(video_game)">Rage</a>, there was a texture streaming concept introduced called ‘Mega-Texture’ which was also used in the previous Doom installment. This system works by rendering a so called ‘feedback texture’ each frame that contains the information of what texture data was visible, that texture is analysed next frame to determine which textures get streamed in from disk. This has an obvious flaw because once a texture is on screen, it’s basically already too late to load it and this causes blurry textures the first few frames it is on screen. In id Tech 7, id Software has stepped away from this approach.</p>

<hr>

<h2 id="gpu-skinning">GPU Skinning</h2>

<p>The first thing that happens even before anything gets drawn to a texture, is evaluating skinning. This is commonly done in a vertex shader before shading. An alternative approach used here, is to do skinning beforehand in a compute shader which writes out skinned vertices to a buffer. This has a couple of advantages mainly not having to do skinning in the vertex shader for every geometry pass. This results in having less shader permutations because the vertex shader doesn’t have to know about skinning.</p>

<p>Skinning in a compute shader is not much different from in a vertex shader except that the output gets written to an intermediate buffer which can then be consumed in a vertex shader that can treat it as a regular static mesh. Just like in a vertex shader, for each vertex, a compute shader thread retrieves the transform of each bone affecting the vertex, transforms its position with each bone transform and adds up these positions based on the skin weights stored on the vertex.</p>

<p>János Turánszki wrote a wonderful write-up of how it can be implemented using a compute shader:
<a href="https://wickedengine.net/2017/09/09/skinning-in-compute-shader/">https://wickedengine.net/2017/09/09/skinning-in-compute-shader/</a>.</p>

<p>Another thing that is worth noting here is the use of <strong>Alembic Caches</strong> in Doom Eternal. These caches contain baked animation which get streamed and decompressed at runtime. As <a href="https://www.youtube.com/watch?v=UsmqWSZpgJY">Digital Foundry described in their tech breakdown</a>, this is used for a wide range of animations going from large cinematic pieces to small tentacles on the floor. This is especially useful for animations that are hard to achieve using skinned animation like organics and cloth simulation. You can compare an Alembic Cache with a video that can be played back and is highly compressed by looking ahead. I suggest watching <a href="https://www.youtube.com/watch?v=zlz-7V_XiUA">Axel Gneiting’s talk at Siggraph 2014</a> if you’re interested in learning more.</p>

<hr>

<h2 id="shadow-mapping">Shadow Mapping</h2>

<p>Next up is shadow rendering. There doesn’t seem to be any large changes in how shadow maps are approached in id Tech 7 compared to its predecessor.</p>

<p>As seen below, shadows get rendered in a large 4096x8196px 24-bit depth texture which may vary across quality levels. The texture is persistent across frames and as described in “Devil is in the Details” at Siggraph 2016, the static geometry in the shadow map is cached to save having to redraw the shadow maps each frame. The technique is fairly simple: as long as nothing in the view of the light moves, there is no need to update the shadows. If a dynamic object in the frustum moves, a ‘cached’ shadow map is copied into the actual shadow map and the dynamic geometry is re-drawn on top. This cached shadow map is the same shadow map but only with static geometry because you can make the assumption that these will never change. This saves having to draw the entire scene in the frustum every time it needs to update. Of course, when the light moves, the entire scene has to be redrawn from scratch.</p>

<p>When sampling the shadow map during lighting, a 3x3 PCF sampling approach is used to smoothen the shadow edges. For the sun light, <strong>cascaded shadow maps</strong> are used to distribute the quality better as it covers such a large portion of the environment.</p>

<p>Here is a closer look at the shadow atlas. A light with higher importance, larger screen area or that is closer to the camera, will get a larger portion of the atlas assigned for better resolution. These heuristics are evaluated dynamically.</p>

<p><img src="https://www.simoncoenen.com/images/blog/010_doom_eternal_study/image3.jpeg" alt=""></p>

<hr>

<h2 id="depth-pre-pass-and-velocity">Depth Pre-pass and Velocity</h2>

<p>Opaque geometry gets rendered to a depth-only target starting with the player’s gun, then static geometry, and finally dynamic geometry. A depth pre-pass is common to avoid unnecessary pixel shader calculations later down the pipeline where geometry overlaps. A depth pre-pass is especially important in a forward renderer where redundant pixel calculations are extremely wasteful due to pixel overdraw. With a depth pre-pass, the actual forward lighting pixel shader can reject pixels by comparing with the depth buffer before execution, saving a lot of performance.</p>

<div id="prepassCarousel" data-ride="carousel">
  <ol>
    <li data-target="#prepassCarousel" data-slide-to="0"></li>
    <li data-target="#prepassCarousel" data-slide-to="1"></li>
    <li data-target="#prepassCarousel" data-slide-to="2"></li>
  </ol>
  <div>
    <div>
      <p><img src="https://www.simoncoenen.com/images/blog/010_doom_eternal_study/Prepass0.png"></p><p>First person gun</p>
    </div>
    <div>
      <p><img src="https://www.simoncoenen.com/images/blog/010_doom_eternal_study/Prepass1.png"></p><p>Static objects</p>
    </div>
    <div>
      <p><img src="https://www.simoncoenen.com/images/blog/010_doom_eternal_study/Prepass2.png"></p><p>Dynamic objects</p>
    </div>
  </div>
  <p><a href="#prepassCarousel" role="button" data-slide="prev">
    
    <span>Previous</span>
  </a>
  <a href="#prepassCarousel" role="button" data-slide="next">
    
    <span>Next</span>
  </a>
</p></div>

<p>Besides rendering depth, the pre-pass also renders to another color target. For dynamic geometry, the velocity is rendered using motion vectors which is the position of the current position subtracted from the position of the pixel in the previous frame. We only need the motion on the X and Y axis so the motion is stored in the red and green channel of a 16-bit floating point render target. This information is later used in post processing for applying motion blur and reprojection for temporal anti-aliasing. The image below is exaggerated because this snapshot doesn’t have a lot of motion. Static geometry does not need motion vectors as their motion can be derived from the camera motion because they have only “moved” relative to the camera.</p>

<p><img src="https://www.simoncoenen.com/images/blog/010_doom_eternal_study/image6.jpeg" alt=""></p>

<hr>

<h2 id="hierarchical-z-depth">Hierarchical-Z Depth</h2>

<p>Next up, a hierarchical mip chain of the depth buffer is generated which is similar to a mip map but instead of averaging 4 neighboring pixels, the maximum is taken. This is commonly done in graphics for various purposes like accelerating screen space reflections and occlusion culling. In this case, this mip chain is used to accelerate the light and decal culling which is covered later. More recently, mip generation is done in a single pass by writing into multiple mips at once. In Doom Eternal, it still traditionally does a dispatch for every mip separately.</p>

<p><img src="https://www.simoncoenen.com/images/blog/010_doom_eternal_study/image7.gif" alt=""></p>

<hr>

<h2 id="mesh-decals">Mesh Decals</h2>

<p>Up until what I’ve covered so far, there haven’t been many noticeable changes compared to Doom 2016. However, “mesh decals” is an addition to the mesh rendering pipeline introduced in Doom Eternal. Unlike the common decal workflow - which are placed freely in the environment - a mesh decal is placed during the mesh authoring pipeline by artists and so belong to the mesh. Before, Doom heavily relied on decals and stepped it up with the addition of so called “mesh decals” in this game for even better detailing and flexibility. “Mesh decals” are small decals like bolts, grills, bumps, stickers, … Just like a traditional decal, it can modify any property of the underlying surface like the normal, roughness, base color, …</p>

<p>To achieve this, the following geometry pass renders each of the decals’s ID into an 8-bit render target. Later during shading, this texture is sampled to retrieve the ID which is used to retrieve a projection matrix bound with each draw call. The matrix projects the pixel’s position from world space into texture space. These coordinates are then used to sample the decal and blend with the underlying material. This is extremely fast and allows artists to go crazy with massive amounts of decals. Because the IDs are rendered to an 8-bit texture, the maximum amount of decals per mesh would theoretically be 255.</p>

<p>One requirement for this, is that all decals are bound to the pipeline when drawing meshes. Doom Eternal uses a fully bindless render pipeline which allows them to bind all decal textures at once and dynamically index them in the shader. More on this bindless pipeline later as this is important to pull off other tricks they’ve done in this game.</p>

<p>Below, the mesh decal texture. The different IDs are coloured to visualize it better.</p>

<p><img src="https://www.simoncoenen.com/images/blog/010_doom_eternal_study/image8.jpeg" alt=""></p>

<hr>

<h2 id="light-and-decal-culling">Light and …</h2></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.simoncoenen.com/blog/programming/graphics/DoomEternalStudy.html">https://www.simoncoenen.com/blog/programming/graphics/DoomEternalStudy.html</a></em></p>]]>
            </description>
            <link>https://www.simoncoenen.com/blog/programming/graphics/DoomEternalStudy.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24401805</guid>
            <pubDate>Mon, 07 Sep 2020 19:10:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Soft Skills for Managers]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24400840">thread link</a>) | @hackitup7
<br/>
September 7, 2020 | https://staysaasy.com/product/2020/09/06/soft-skills-for-managers.html | <a href="https://web.archive.org/web/*/https://staysaasy.com/product/2020/09/06/soft-skills-for-managers.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>Many of the hard technical skills that make a great manager are testable. You can easily evaluate technical expertise – for example, an engineering manager should demonstrate their ability to produce or at least understand code. Similarly, in-depth discussions will quickly reveal whether someone can clearly and concisely communicate nuanced concepts.</p>

<p>The softer skills of management are more difficult to assess but just as important. Below are a few of the soft skills that I value most highly in managers, some ideas on how to assess them, and common traps.</p>

<h2 id="soft-skill-1-initiative">Soft Skill #1: Initiative</h2>

<p>Managers set the tempo for their teams – if they aren’t actively looking for ways to help the business, they will set an example that being proactive is optional. The sad truth is that once you’re a manager you can survive a long time by simply maintaining existing processes, reacting to personnel issues, or (worse) creating busywork and taking credit. Many managers collect fine paychecks drifting like an inert gas within a broader bureaucratic cloud. To scale a company effectively, managers need to be proactive rather than existing simply to push virtual papers across their desks.</p>

<p>Initiative tends to be pretty obvious when evaluating internal management candidates. For external candidates, I usually ask deep-dive questions about a meaty project from their past. The goal: learn about what they did, their role, and most importantly the “why” behind their decisions, diving into the details of what really went down. A sample prompt: “Tell me about a significant project that you led, and I’ll ask a bunch of questions to dive deeper.” This is usually fun to ask and I expect to learn something new from strong candidates – after all, they know much more than I do about the project they worked on, so if they can’t teach me something that’s a red flag.</p>

<p>Searching for the “why” behind their decisions is a good test for initiative – being able to explain why certain decisions were made indicates that a candidate can think critically about what a project actually needs, rather than just executing on someone else’s plan.</p>

<h2 id="soft-skill-2-emotional-control">Soft Skill #2: Emotional Control</h2>

<p>Management can be stressful, <a href="https://staysaasy.com/scaling/2020/05/07/startup-is-this-normal.html">especially at a high growth startup company</a>. You’re the first point of escalation for all manner of problems: inter- or intra-team conflict, critical last-minute blockers, HR issues, and other corporate delights. You don’t have a safety net to fall back on if a situation gets too heated – you are the safety net.</p>

<p>Managers can’t be emotionally reactive. They need to remain rational even in the face of severe stressors, allowing them to remain unflappable when shit goes down (more calm also comes with experience). Sometimes, the downsides of reacting are minor: if someone says something dumb during a presentation and you visibly scowl, you can cause them to get flustered. Others are much more important: for example, if someone comes into a 1:1 very upset over their compensation, you need to remain calm as it can be disastrous to make promises or comments that could be misconstrued.</p>

<p>This skill is challenging to assess when hiring managers. Unlike in, say, the Marine Corps, it’s frowned upon in the tech industry to throw someone into a stressful situation and assess their emotional control. References are imperfect but can help, and this skillset is one of the reasons that internal manager candidates can be such tempting known quantities.</p>

<p>The very best managers take this a step further and demonstrate durability – the ability to not only exercise control, but to do so repeatedly over months and emerge on the other side energized and positive. High growth companies are challenging, and more significantly, they’re challenging <a href="https://staysaasy.com/scaling/2020/07/29/the-rogue-wave-of-enterprise-saas.html">over a very long time</a> – typically years. People who are a consistent force for optimistic calm are the pillars around which you should build your team.</p>

<h2 id="soft-skill-3-dispassionate-empathy">Soft Skill #3: Dispassionate Empathy</h2>

<p>Managers need dispassionate empathy: the ability to logically breakdown how their teams will react to new situations by viewing things from multiple perspectives – without allowing viewpoints that they’re sympathetic to cloud their judgment. If someone on your team is passed over for a promotion that goes to their teammate, how will they react? If someone gets really tough feedback, how will they handle it? Managers need to be able to assess factors such as incentives and egos as they chart a course for their entire team.</p>

<p>Promotions provide a common example: when you promote someone you’re affirming the behaviors that you will reward (as well as other behaviors that you tolerate…), and everyone else on the team will observe, calculate, and react differently according to their own interpretation. If you promote Alice, will her peer Bob freak out that it was unfair, become apathetic, or work harder? What message will it send to their more junior team member Charlie?</p>

<p>Empathizing with others’ viewpoints dispassionately is surprisingly difficult as there’s little in day-to-day life that prepares you for it. When a friend or family member is in a difficult situation, they’re typically looking for empathy without reservations, and we’re trained as primates to deliver it. Mirroring the emotions of people we’re close to is often a feature, not a bug. Many managers need to fight the urge to over-empathize.</p>

<p>I assess this trait in interviews by asking about challenging situations that occurred on a prior team. Examples include:</p>

<ul>
  <li>Tell me about a time that someone asked for something (eg a raise or promotion), and you didn’t give it to them.</li>
  <li>Tell me about a time when a member of your team was in conflict with a member of another team, and you needed to help out.</li>
  <li>Tell me about a time that someone on your team was underperforming.</li>
</ul>

<p>In asking about these scenarios, I’m first looking for the candidate’s ability to articulate the different motivations or incentives involved: do they understand the situation? Did they view it through others’ eyes rather than just their own? I also assess their ability to discuss the situation in a calm way: are they getting emotionally biased?</p>

<p>While emotional life isn’t as cut and dried as a Python script, I find that most great managers can break down situations like an algorithm: look at all of the different incentives and personalities on the team, add a new decision or situation, and guess at what will happen next.</p>

<h2 id="the-soft-skill-trap">The Soft Skill Trap</h2>

<p>Especially for internal candidates for management positions, beware the most common soft skill trap: mistaking a likeable personality for leadership or management aptitude.</p>

<p>It’s easy to misinterpret a gregarious, likeable personality for innate management skill. “Timmy is a ‘people person’ – of course he’ll be able to manage a team!” This is simply not how life works. Timmy might be great as a manager, or he might suck, but in my experience his likeability is unlikely to correlate strongly with the soft skills that matter.</p>

<p>I blame this phenomenon on Hollywood’s stereotypical portrayal of what it means to be a “leader.” Many great managers are gregarious, sociable, and charismatic, but many others are introverted, quiet in group settings, or somewhat socially awkward. I’ve met managers whom I actually somewhat disliked in a social capacity but who were extremely effective at their jobs. However, I have yet to meet a great manager who didn’t take initiative, couldn’t regulate their own emotions, and didn’t understand the incentives that would motivate their team.</p>

<p><img src="https://staysaasy.com/assets/soft_skills/gladiator.jpg" alt="Gladiator">
Good leader overall, but perhaps too much shouting for your average tech company</p>

<p>There’s also a potential element of bias here: does someone seem like they should be in a leadership position just because they’re really confident? Do they feel like they should be in charge because they’re often the loudest voice in the room? These sorts of subjective judgements are also minefields for bias. For a random example, some of the behaviors that say “leader” in American culture scream “jerk” in East Asian culture. I highly recommend assessing management potential according to the dimensions listed above and trying to remove likeability or sociability from your decision calculus.</p>

<h2 id="takeaways">Takeaways</h2>
<ul>
  <li>Great management requires both hard and soft skills. Soft skills are often harder to assess.</li>
  <li>When assessing a manager’s soft skills, look for the following traits: initiative, emotional control, and an ability to understand how others will perceive and react to situations.</li>
  <li>Promoting from within is a great way to remove manager hiring risk.</li>
  <li>Don’t mistake sociability for management skill.</li>
</ul>

    




  </div></div>]]>
            </description>
            <link>https://staysaasy.com/product/2020/09/06/soft-skills-for-managers.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24400840</guid>
            <pubDate>Mon, 07 Sep 2020 17:13:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Heroku terminates Ruqqus site and account without a warning or an explanation]]>
            </title>
            <description>
<![CDATA[
Score 35 | Comments 29 (<a href="https://news.ycombinator.com/item?id=24400102">thread link</a>) | @ecmascript
<br/>
September 7, 2020 | https://ruqqus.com/post/301l/you-cant-cancel-freedom-that-easily | <a href="https://web.archive.org/web/*/https://ruqqus.com/post/301l/you-cant-cancel-freedom-that-easily">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-body">
<p>At 16:09 EDT on 04 Sep 2020 (about 26 hours ago), SalesForce (the owner of web hosting company Heroku) notified us that they had suspended the Ruqqus live and test server environments, with account termination to follow twenty-four hours later.</p>
<p>At the time of this writing, we have not been given a reason as to why we were being kicked. I do not believe we violated Heroku ToS (though Heroku may think otherwise) and we did not exceed Heroku usage limits. Our account manager was friendly but was unfortunately unable to provide us with any information. It is my opinion that by not warning us first (ex. "please remove X which violates Heroku's acceptable use policy") that Heroku actually violated their own terms of service.</p>
<p>We immediately downloaded all of the Ruqqus data, and began working on setting up shop at another host.</p>
<p>I'd like to give a massive shoutout to <a href="https://ruqqus.com/@p2hang"><img src="https://ruqqus.com/@p2hang/pic/profile">@p2hang</a> for lending his considerable expertise in support of getting Ruqqus operational as quickly as possible. He is the third recipient of the super-rare Fire Extinguisher badge.</p>
<p>All (or at least, almost all) functionality is restored. There are a few things which aren't quite yet back to normal, which I hope to fix over the course of tonight and tomorrow.</p>
<p>Known issues:</p>
<ul>
<li>
<p><del>hCaptcha verification issues on attempted signups</del></p>
</li>
<li>
<p><del>Custom guild colors are currently disabled. This was an intentional decision by me, as there were issues loading the style files needed for custom coloring, so it was easier to just disable that for the sake of getting up and running.</del></p>
</li>
<li>
<p><del>Notification bell stuck on.</del></p>
</li>
<li>
<p><del>Lastly, and most importantly, we have no idea how well the current server will be able to handle user load. For that reason, I will be continuing to monitor and make adjustments as needed. Performance may be spotty and unreliable as load resumes.~~ ~~Doesn't seem to be an issue, but we'll have to see how it goes with everyone trying to get on.</del> There are some load/efficiency problems that I'm trying to sort out.</p>
</li>
<li>
<p><del>hCaptcha on signup is malfunctioning.</del></p>
</li>
<li>
<p><del><a href="http://i.ruqqus.com/">i.ruqqus.com</a> image uploads don't save.</del></p>
</li>
<li>
<p>The most recent 30k or so comments didn't make it into the new host. Also possibly some post votes. I <em>think</em> all the comment votes made it in. The missing data is something I'm still working on, but you will probably see rep fluctuating unpredictably as vanished stuff is no longer counted.</p>
</li>
</ul>
<hr>
<p>If you'd like to contribute to the future of freedom, please consider donating via <a href="https://patreon.com/ruqqus" rel="nofollow noopener" target="_blank">Patreon</a> or <a href="https://paypal.me/ruqqus" rel="nofollow noopener" target="_blank">Paypal</a> to help cover current and future server costs. <a href="https://ruqqus.com/help/donate">Cryptocurrency is also accepted</a>.</p>
</div></div>]]>
            </description>
            <link>https://ruqqus.com/post/301l/you-cant-cancel-freedom-that-easily</link>
            <guid isPermaLink="false">hacker-news-small-sites-24400102</guid>
            <pubDate>Mon, 07 Sep 2020 15:35:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Named Parameters in C++20]]>
            </title>
            <description>
<![CDATA[
Score 84 | Comments 82 (<a href="https://news.ycombinator.com/item?id=24398756">thread link</a>) | @ibobev
<br/>
September 7, 2020 | https://pdimov.github.io/blog/2020/09/07/named-parameters-in-c20/ | <a href="https://web.archive.org/web/*/https://pdimov.github.io/blog/2020/09/07/named-parameters-in-c20/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div>
  
  <p><span>07 Sep 2020</span></p><p>A programming language supports <em>named parameters</em> when one
can call a function supplying the parameters by name, as in
the following hypothetical example (using C++ syntax):</p>

<div><div><pre><code>void f( int x, int y );

int main()
{
    f( x = 1, y = 2 );
}
</code></pre></div></div>

<p>C++ is obviously not such a language and there have been
numerous proposals to rectify this omission, unfortunately none
of them successful. The latest attempt is Axel Naumann’s paper
<a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0671r2.html">Self-explanatory Function Arguments</a>,
which tries to attack the problem from another angle by just
allowing normal function calls to be tagged with the parameter
name, as in</p>



<p>enabling compilers to issue helpful warnings when a name doesn’t
match, but not allowing one to omit, or reorder, arguments.</p>

<p>Even in this limited form, named parameters would still be immensely
useful, but this is not what this post is about. What this post is
about is that we can already achieve something very close to named
parameters in C++20, by using a C99 feature called <em>designated
initializers</em>.</p>

<p>Designated initializers allow one to initialize structures by
member name, as in the following example:</p>

<div><div><pre><code>struct A
{
    int x;
    int y;
};

A a1 = { .x = 1, .y = 2 };
A a2 = { .x = 3 }; // a2.y == 0
A a3 = { .y = 4 }; // a3.x == 0
A a4 = { .y = 5, .x = 6 }; // valid C, invalid C++ (reorder)
</code></pre></div></div>

<p>C++ introduces a restriction C doesn’t have: the initializers
must follow the declaration order, similarly to how class member
initalizers are executed in member declaration order. But in
exchange, it allows us to supply default values:</p>

<div><div><pre><code>struct A
{
    int x = 0;
    int y = 0;
};

A a3 = { .y = 4 }; // a3.x == 0, no warning
</code></pre></div></div>

<p>You can already see where this is going. Instead of</p>



<p>we declare</p>



<p>and then call it like this:</p>

<div><div><pre><code>int main()
{
    f({ .x = 1, .y = 2 });
}
</code></pre></div></div>

<p>This works under <a href="https://godbolt.org/z/YfWj3W">GCC</a> and
<a href="https://godbolt.org/z/vbnz4T">Clang</a> even without <code>-std=c++20</code> because
they support designated initializers in earlier language modes as
an extension, and it works under <a href="https://godbolt.org/z/bKozaW">MSVC</a>
with <code>-std:c++latest</code>.</p>

<p>For a more realistic example, consider this snippet, taken from real
code, that sets a 10 second
<a href="https://www.boost.org/doc/libs/1_74_0/libs/beast/doc/html/beast/using_websocket/timeouts.html">timeout</a>
on a <a href="https://boost.org/libs/beast">Boost.Beast</a> websocket:</p>

<div><div><pre><code>#include &lt;boost/beast/websocket/stream.hpp&gt;
#include &lt;boost/beast/core/tcp_stream.hpp&gt;
#include &lt;chrono&gt;

void f1(boost::beast::websocket::stream&lt;boost::beast::tcp_stream&gt;&amp; ws)
{
    auto opt = boost::beast::websocket::stream_base::timeout();

    opt.keep_alive_pings = true;
    opt.idle_timeout = std::chrono::seconds(10);

    ws.set_option(opt);
}
</code></pre></div></div>

<p>Here’s how we can reformulate it by using the above idiom and <code>&lt;chrono&gt;</code>
literals:</p>

<div><div><pre><code>#include &lt;boost/beast/websocket/stream.hpp&gt;
#include &lt;boost/beast/core/tcp_stream.hpp&gt;
#include &lt;chrono&gt;

using namespace std::chrono_literals;

void f2(boost::beast::websocket::stream&lt;boost::beast::tcp_stream&gt;&amp; ws)
{
    ws.set_option({ .idle_timeout = 10s, .keep_alive_pings = true });
}
</code></pre></div></div>

<p>Apart from the slightly awkward <code>({ ... })</code> syntax and the need to observe
the right parameter order, that’s not that far from the ideal; and it’s
considerably better than <code>f1</code>.</p>

<p>This also works for constructors. Consider this hypothetical <code>vector</code> class
that is like <code>std::vector</code>, except with its various constructor overloads
replaced with one taking named parameters:</p>

<div><div><pre><code>template&lt;class T, class A = std::allocator&lt;T&gt;&gt; class vector
{
private:

    struct params
    {
        std::size_t size = 0;
        T element{};
        std::size_t capacity = 0;
        A allocator{};
    };

public:

    explicit vector( params p );
};
</code></pre></div></div>

<p>This is <a href="https://godbolt.org/z/x17fdY">how it’s used</a>:</p>

<div><div><pre><code>auto f()
{
    vector&lt;int&gt; v{{ .size = 4, .element = 11, .capacity = 64 }};
    return v;
}
</code></pre></div></div>

<p>Again, apart from the odd <code>{{ ... }}</code> syntax, not that bad.</p>


</div>

<!--

<div class="related">
  <h2>Related posts</h2>
  <ul class="related-posts">
    
    
      <li>
        <h3>
          <a href="/blog/2020/09/06/why-use-the-boost-license/">
            Why You Should Use the Boost Software License
            <small>06 Sep 2020</small>
          </a>
        </h3>
      </li>
    
    
    
      <li>
        <h3>
          <a href="/blog/2020/09/05/from-simd-to-ast-extraction/">
            From SIMD to AST Extraction
            <small>05 Sep 2020</small>
          </a>
        </h3>
      </li>
    
    
    
      <li>
        <h3>
          <a href="/blog/2020/07/24/compilers-do-static-analysis/">
            Compilers Do Static Analysis, They Just Don't Tell You
            <small>24 Jul 2020</small>
          </a>
        </h3>
      </li>
    
    
  </ul>
</div>

-->

      </div></div>]]>
            </description>
            <link>https://pdimov.github.io/blog/2020/09/07/named-parameters-in-c20/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24398756</guid>
            <pubDate>Mon, 07 Sep 2020 11:51:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Raspberry Pi 3B+ Hackable Linux Handheld]]>
            </title>
            <description>
<![CDATA[
Score 140 | Comments 93 (<a href="https://news.ycombinator.com/item?id=24398485">thread link</a>) | @mmerlin
<br/>
September 7, 2020 | http://yarh.io/yarh-io-mki.html | <a href="https://web.archive.org/web/*/http://yarh.io/yarh-io-mki.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    <section id="top-section">
        <div id="top-section-top">
            <nav>
                
            </nav>
            <p id="heading">
                
                <h2>Raspberry Pi 3B+ Hackable Linux Handheld<br></h2>
            </p>
        </div>
    </section>
    <section id="yarh-io-mki">
        <div>
            <div>
                <div>
                    <p><a href="http://yarh.io/assets/img/yarh-white-hand-large-t-001.png" target="_blank" data-lightbox="yarh-io-mki"><img src="http://yarh.io/assets/img/yarh-white-hand-large-t-001.png"></a></p>
                    <div>
                        <p>YARH.IO is a fully hackable and custamizable Raspberry Pi based handheld, running Raspberry Pi OS and supporting all other Operating Systems available for Raspberry Pi.</p>
                        <p>The dream of a hackable&nbsp;Linux powered handheld has been around for many years, and many attempts have been made to create a working device. While some of the devices have reached the market, none of them withstood the test
                            of real world user experience.<br></p>
                        <p>YARH.IO project has taken on the challenge of building a fully functioning device by combining the best of Raspberry Pi design and 3D printing technology.&nbsp;<br></p>
                        <p>This project takes hackability to the next level by ensuring that every single component needed to build YARH.IO can be easily sourced, with no custom PCBs and just a bit of wire soldering required.&nbsp;</p>
                    </div>
                </div>
                <div>
                    <div>
                        <p>YARH.IO handheld and its unique modular design with exposed interfaces offers unprecedented connectivity with unlimited platforms and devices.<br></p>
                        <p>This model is powered by&nbsp;Raspberry Pi 3B+ and&nbsp;has the best ratio of functionality and computing power requirements for a mobile device. When it comes to the battery powered devices, the overall power consumption and longer
                            battery are more important then the extra processing power or memory.</p>
                        <p>For this model, the width of the main module is based on the keyboard dimensions for improved handling. The 5" Resistive Touch Screen 800x480 HDMI TFT LCD Display&nbsp;is a good option for YARH.IO as it&nbsp;will allow you to perform
                            all of the usual tasks with ease.&nbsp;<br></p>
                    </div>
                    <p><a href="http://yarh.io/assets/img/yarh-red-right-large-t-001.png" target="_blank" data-lightbox="yarh-io-mki"><img src="http://yarh.io/assets/img/yarh-red-right-large-t-001.png"></a></p>
                </div>
                <div>
                    <p><a href="http://yarh.io/assets/img/yarh-white-left-large-t-001.png" target="_blank" data-lightbox="yarh-io-mki"><img src="http://yarh.io/assets/img/yarh-white-left-large-t-001.png"></a></p>
                    <div>
                        <p>Fosmon Portable Lightweight Mini Wireless Bluetooth Keyboard Controller has been selected as the best option for YARH.IO's multifunctional design. If you are using Vim, tmux, and Emacs everyday, the available modifier keys will
                            help you get the most out of the device.</p>
                        <p>In addition to the keyboard, YARH.IO's pointing device will allow you to effortlessly browse the internet and engage with other applications with graphical interface.&nbsp;&nbsp;<br></p>
                        <p>A single removable rechargeable battery will allow you to quickly replace an empty battery with a fully charged one. A high capacity Fenix ARB-L21-5000 5000mAh Li-ion Rechargeable Battery was chosen as the best option as it fits
                            the main module perfectly. For the charger/5v power supply the internals of a Fenix ARE-D1 Smart Charger were used.</p>
                    </div>
                </div>
                <div>
                    <div>
                        <p>The RTC is a must have for a handheld device that is not continuously connected to the internet. YARH.IO includes a DS3231 High Precision RTC Clock Module.</p>
                        <p>Raspberry Pi GPIO connectors have been made available on the bottom sides of the main module and extension module for connecting all of your Raspberry Pi add-on boards.</p>
                        
                    </div>
                    <p><a href="http://yarh.io/assets/img/yarh-black-top-large-t-001.png" target="_blank" data-lightbox="yarh-io-mki"><img src="http://yarh.io/assets/img/yarh-black-top-large-t-001.png"></a></p>
                </div>
                <div>
                    <p><a href="http://yarh.io/assets/img/yarh-black-back-open-large-t-001.png" target="_blank" data-lightbox="yarh-io-mki"><img src="http://yarh.io/assets/img/yarh-black-back-open-large-t-001.png"></a></p>
                    <div>
                        <p>YARH.IO features modular design, where the main module includes Raspberry Pi board, screen, power supply, battery, and RTC and GPIO connector with cables.&nbsp;</p>
                        <p>The main module can be used as a fully functioning handheld computer&nbsp;with the touch screen and onscreen keyboard available for the performance of your basic tasks.</p>
                        <p>One shared USB connector is available on the bottom of the main module, allows connecting USB devices like&nbsp;hard drives or Cellular/WiFi adaptors and mounted inside the extension module.</p>
                        <p>Raspberry Pi GPIO connector is also available on the bottom of the main module, allowing for different add-on modules,&nbsp;<br>including Lora radios, RFID readers/writers, and IR transivers, to&nbsp;be mounted inside the extension
                            module. This configuration works extremely well with Pi-DAC+, creating a great handheld audio player.</p>
                    </div>
                </div>
                <div>
                    <p><a href="http://yarh.io/assets/img/yarh-black-keyboard-large-t-001.png" target="_blank" data-lightbox="yarh-io-mki"><img src="http://yarh.io/assets/img/yarh-black-keyboard-large-t-001.png"></a></p>
                    <div>
                        <p>No 'click' assembly here. This is a fully hackable device with stainless steel socket cap screw used&nbsp;throughout to allow for multiple assembly and disassembly cycles.&nbsp;The Military/Industrial aesthetic can be felt throughout
                            the YARH.IO project design.&nbsp;<br></p>
                        <p>The sturdy housing parts are 3D printed using PLA, ABS, and ASA plastic.&nbsp;<br></p>
                        
                    </div>
                </div>
                <div>
                    <p><a href="http://yarh.io/assets/img/yarh-black-back-keyboard-open-large-t-001.png" target="_blank" data-lightbox="yarh-io-mki"><img src="http://yarh.io/assets/img/yarh-black-back-keyboard-open-large-t-001.png"></a></p>
                    <p>The list of parts used for the YARH.IO project can be purchased from Amazon and other online stores.</p>
                </div>
                <div>
                    
                    <p><a href="http://yarh.io/assets/img/yarh-black-large-t-001.png" target="_blank" data-lightbox="yarh-io-mki"><img src="http://yarh.io/assets/img/yarh-black-large-t-001.png"></a></p>
                </div>
                <div>
                    <p><a href="http://yarh.io/assets/img/yarh-white-front-large-t-001.png" target="_blank" data-lightbox="yarh-io-mki"><img src="http://yarh.io/assets/img/yarh-white-front-large-t-001.png"></a></p>
                    <div>
                        <p>YARH.IO MKI Project at a Glance.&nbsp;</p>
                        <p>The outcome of this YARH.IO model is a successful handheld device with the potential to connect an unlimited range of extension devices and modules. It is a ruggedly designed and fully hackable device that can be 3D printed and
                            assembled in the field.<br></p>
                        <p>As an experimental model YARH.IO's future development will continue to increase usability and functionality of the device. One of the areas of improvement is the adaptation of IPS type screen for wider viewing angles.&nbsp;<br></p>
                        <p>Other future development will focus on adding new compartment modules to accommodate additional I/O, storage and communication devices.&nbsp;<br></p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section id="yarh-io-mki-gallery">
        
    </section>
    <section id="yarh-io-mki-downloads">
        
    </section>
    <section id="footer">
        <div>
            <div>
                <div>
                    <div>
                        <p>© 2019-2020 YARH.IO | info@yarh.io</p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    
    
    
    


</div>]]>
            </description>
            <link>http://yarh.io/yarh-io-mki.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24398485</guid>
            <pubDate>Mon, 07 Sep 2020 10:56:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Many Ways to Start an Xserver]]>
            </title>
            <description>
<![CDATA[
Score 75 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24398154">thread link</a>) | @gbrown_
<br/>
September 7, 2020 | https://nixers.net/Thread-How-Many-Ways-To-Start-An-Xserver | <a href="https://web.archive.org/web/*/https://nixers.net/Thread-How-Many-Ways-To-Start-An-Xserver">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://nixers.net/Thread-How-Many-Ways-To-Start-An-Xserver</link>
            <guid isPermaLink="false">hacker-news-small-sites-24398154</guid>
            <pubDate>Mon, 07 Sep 2020 09:49:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Open Transclude for Networked Writing]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24398014">thread link</a>) | @Frodo478
<br/>
September 7, 2020 | http://subpixel.space/entries/open-transclude/ | <a href="https://web.archive.org/web/*/http://subpixel.space/entries/open-transclude/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>tl;dr: If you follow this blog you’ve seen me experiment with iframe-based citations; this post is about open-sourcing that tooling. <a href="#tutorial-start" target="_self">Skip</a> to demo, implementation tutorial, and GitHub link.</p>

<hr>

<p>Knowledge tooling is happily becoming a hot topic again. With this trend is coming revived interest in <a href="https://en.wikipedia.org/wiki/Project_Xanadu">Xanadu</a>, bi-directional hyperlinking, knowledge databases, visualizing knowledge graphs, and so on. At this moment, I see most of the emphasis being put on tooling for the research side, with Notion, Workflowy, and the new and hyped Roam Research leading the way.</p>

<p><img src="http://subpixel.space/uploads/xanadu-shot.png" alt="Screenshot of the OpenXanadu prototpye"></p>

<p>Where I see less focus is the <em>writing</em> part of the knowledge production process, where older apps like Scrivener are still the thing to beat. And almost nobody at all is working on the reader’s experience. As a blogger who largely caters to a wide audience, I’m especially interested in these areas.</p>

<p>Written information is largely still presented as a single document, and writing tools are geared toward the production of long pages. But before I say what’s wrong with this, let me sing the praises of documents for a moment.</p>

<p>People often get carried away when they discover the original vision of hypertext, which involves a network of documents, portions of which are “transcluded” (included via hypertext) into one another. The implication is that readers could follow any reference and see the source material—and granted, this would be transformative. However, there’s a limit to the effectiveness of the knowledge network as a reading experience. “Hypertext books,” online books which are made up of an abundance of interlinked HTML pages, are mostly unpopular. The failure of this experiment is, in my opinion, very revealing.</p>

<p><img src="http://subpixel.space/uploads/sprawlingplaces-shot.jpg" alt="Screenshot of tinderbox map of hypertext book Sprawling Places by David Kolb">
<span>Tinderbox map of a portion of David Kolb’s hypertext book Sprawling Places</span></p>

<p>Knowledge is not an accumulation of facts, nor is it even a set of facts and their relations. Facts are only rendered meaningful within narratives, and the single-page document is a format very conducive to narrative structure. The hypertext books that have gained popularity (I’m thinking here of <a href="http://meaningness.com/">Meaningness.com</a>) have largely conformed to this in two ways: 1) there is an intended reading order, and 2) the longer essays within the project do most of the heavy lifting in terms of imparting the author’s perspective to readers.</p>

<p>On the other hand, the notion of the “document” that is intrinsic to web development today is overdetermined by the legacy of print media. The web document is a static, <em>finished</em> artifact that does not bring in dynamic data. This is strange because it lives on a medium that is alive, networked, and dynamic, a medium which we increasingly understand more as a <em>space</em> than a thing.</p>

<p>For example, consider how silly it is to include MLA-style citations at the bottom of a text when we have the vast capabilities of linked documents on the web. Why should the reader have to read every citation or trust that an author is not taking a citation out of context, when hyperlinks are available?</p>

<p>This all suggests that a compromise must be struck between the coherence of a text and the new opportunities for knowledge work afforded by the fundamental capabilities of the medium: the internet’s connectivity, the screen’s frame rate.</p>

<hr>

<p>My own blogging is one context in which I’ve seen this tension play out, and have been working to explore ways of making my texts richer. A lot of the ideas I talk about in various pieces of writing are connected to one another. When I publish an essay, I’m not done with it. The ideas live on and get renewed, reused, and recycled in later works. Some sentences contain definitions that are core to my mental models, and there are whole paragraphs that might be useful out of context. I’m building my knowledge network in mind maps and behind various SaaS APIs, but how can I publicly show my thinking to be part a cohesive worldview?</p>

<p>Normally people solve this by simply block quoting themselves, but this is a waste of an opportunity. The indented block quote is a print medium invention <a href="https://en.wikipedia.org/wiki/Block_quotation#Origins">almost as old as typesetting</a>. The block quote is <em>plaintext</em>, it is not actually linked to the original text or its context.</p>

<p>I’ve been experimenting with one idea for a solution, and if you’ve read the last couple blog posts you’ll have seen it there. My stab at an answer is an iframe which shows the quote within its original context and gives a hint at its surroundings. Effectively, it’s a transclusion within my own blog. I’m currently satisfied with what I have as a v1, and am interested to see if others find it useful, so I’m open sourcing it here and including a tutorial.</p>



<p>Open Transclude is a UX pattern, a spec for networked writing within your own blog. Here’s how it looks:</p>



<!-- <script src="/portal.js"></script> -->



<p>What you are looking at is an scroll-locked iframe that links to a quote I picked out of my blog post “Notes on Comparative Psychology.” You can use Open Transclude anywhere you can drop an <code>&lt;a&gt;</code> tag on your own site.</p>

<p>Open Transclude:</p>
<ul>
  <li>Works anywhere on your own domain</li>
  <li>Compatible with most static site generators / templating engines</li>
  <li>12 lines of HTML, 80 lines of SCSS, 22 lines of JS (4.5 kb total)</li>
  <li>Has 0 dependencies&nbsp;— this is native web technology</li>
</ul>

<p>Open Transclude is extremely simple, and the heaviest part of the code is the CSS, which you can simplify at your whim. That’s why I am referring to it as a UX pattern. This is not a protocol. The code is really a commodity. What’s interesting about it is the idea and the design, and this is just one viable implementation! Feel free to adapt it however you like.</p>

<p>The principal improvement over a block quotation is <em>sense of context</em>.</p>

<p>Over on GitHub you’ll find the <a href="https://github.com/tobyshorin/Open-Transclude/">reference implementation for Jekyll</a>. Below is a tutorial for implementing it yourself, by way of also explaining some of the technical design decisions.</p>

<hr>

<h2 id="implementation-recipe">Implementation Recipe</h2>

<p>Here’s what you need to do to get Open Transclude up and running.</p>

<ol>
  <li>Create an anchor tag in the blog post where you want to cite yourself.</li>
  <li>Create the HTML for the reusable transclusion component.</li>
  <li>Call the portal into any document and passing it Jekyll variables.</li>
  <li>A small piece of Javascript which populates your transclusion into the document.</li>
  <li>Create the SCSS file with the component’s styles.</li>
</ol>

<h3 id="1-create-the-anchor-tag-where-you-want-to-cite-yourself">1. <strong>Create the anchor tag where you want to cite yourself</strong></h3>

<p>To quote yourself, you’ll need to create an <code>&lt;a&gt;</code> anchor tag in the markdown file for the post you want to quote. If you wish to highlight a specific piece of text, instead create a <code>&lt;span&gt;&lt;/span&gt;</code> around the section you want to quote. Note that this can <em>only be on your own website</em>—it doesn’t work cross domain.</p>

<p>Here’s what it looks like for the example iframe above.</p>

<figure><pre><code data-lang="markdown">It will, for one thing, become newly conscious of itself, and, to the degree that it is, <span>**it will tend to undermine its own experiential integrity**</span>" (emphasis mine).

<span>&lt;span</span> <span>name=</span><span>"mainstream-magic"</span><span>&gt;</span>Ironically, psychology remains one of the closest things we have to a mainstream magic or a mystical art today. Not only is it plainly the direct descendent of medieval magic, as I learned when I read Ioan Coulianou's <span>*Eros and Magic in the Renaissance*</span> earlier this year. <span>**It is a theory of the self that is phenomenologically accurate, objectively wrong, and is based on magical thinking even as it deconstructs itself**</span>.<span>&lt;/span&gt;</span> Some magical thinking processes that happen in psychotherapy, such as <span>[</span><span>transference to the psychologist</span><span>](</span><span>https://en.wikipedia.org/wiki/Transference#Transference_and_countertransference_during_psychotherapy</span><span>)</span>, are even intended to stay unmentioned to the patient in order to be utilized most effectively by the therapist!</code></pre></figure>

<h3 id="2-create-your-iframe-component">2. Create your iframe component</h3>

<p>This is most useful as a standardized component which can be used across the site, so we are going to take advantage of Jekyll’s templating features. Jekyll and other static site generators like Kirby and Zola support HTML “partials” or “includes” so that you can create reusable components.</p>

<p>In your <code>/_scss</code> or <code>/_sass</code> folder make a new file, <code>portal.scss</code>. I called it “portal” because it’s shorter than “transclusion” and less prone to spelling errors.</p>

<p>Here’s our component:</p>

<figure><pre><code data-lang="html"><span>&lt;div</span> <span>class=</span><span>"portal-container"</span><span>&gt;</span>
    <span>&lt;div</span> <span>class=</span><span>"portal-head"</span><span>&gt;</span>
        <span>&lt;div</span> <span>class=</span><span>"portal-backlink"</span> <span>&gt;</span>
            <span>&lt;div</span> <span>class=</span><span>"portal-title"</span><span>&gt;</span>From <span>&lt;span</span> <span>class=</span><span>"portal-text-title"</span><span>&gt;</span>{{ include.title }}<span>&lt;/span&gt;&lt;/div&gt;</span>
            <span>&lt;a</span> <span>href=</span><span>"{{ include.link }}"</span> <span>class=</span><span>"portal-arrow"</span><span>&gt;</span>Go to text <span>&lt;span</span> <span>class=</span><span>"right-arrow"</span><span>&gt;</span>→<span>&lt;/span&gt;&lt;/a&gt;</span>
        <span>&lt;/div&gt;</span>
    <span>&lt;/div&gt;</span>
    <span>&lt;div</span> <span>id=</span><span>"portal-parent-{{include.anchor}}"</span> <span>class=</span><span>"portal-parent"</span><span>&gt;</span>
        <span>&lt;div</span> <span>class=</span><span>"portal-parent-fader-top"</span><span>&gt;&lt;/div&gt;</span>
        <span>&lt;div</span> <span>class=</span><span>"portal-parent-fader-bottom"</span><span>&gt;&lt;/div&gt;</span>        
        <span>&lt;!-- We'll use Javascript to populate the iframe right here --&gt;</span>
    <span>&lt;/div&gt;</span>    
<span>&lt;/div&gt;</span></code></pre></figure>

<p>You’ll notice immediately that the iframe isn’t there yet. Like I mentioned above, we’re going to be populating it with Javascript.</p>

<p>You’ll also see that in various places we’re using <code>{{ include.___}}</code>. A cool thing about Jekyll includes its that it’s possible to define variables and pass them to our include, so we can create reusable components across our site. Dave Rupert has a <a href="https://daverupert.com/2017/07/jekyll-includes-are-cool/">nice blog post about this</a> called if you want to see more advanced examples!</p>

<h3 id="3-calling-the-component">3. Calling the component</h3>

<p>Anytime you want to pull this component into a blog post, all you have to do is <code>include</code> it in the markdown of another blog post, like this:</p>

<figure><pre><code data-lang="html">  {% include portal.html title="Notes On Comparative Psychology" link="/entries/notes-on-comparative-psychology/#mainstream-magic" anchor="emotional-deficit" %} </code></pre></figure>

<p>When you include it, you’ll need to pass in those three variables - title, link, and anchor, that fill in the includes above. If you’re following along now and making a build in Jekyll, you’ll see an empty, unstyled component with the link. So good so far!</p>

<h3 id="4-populating-with-javascript">4. Populating with Javascript</h3>

<p>This is a good time to address why we need Javascript. Web developers reading this are probably asking why we don’t simply put the full <code>/link#with-anchor</code> into the iframe src and be done with it. …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://subpixel.space/entries/open-transclude/">http://subpixel.space/entries/open-transclude/</a></em></p>]]>
            </description>
            <link>http://subpixel.space/entries/open-transclude/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24398014</guid>
            <pubDate>Mon, 07 Sep 2020 09:23:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Threat modelling case study: bicycles]]>
            </title>
            <description>
<![CDATA[
Score 71 | Comments 117 (<a href="https://news.ycombinator.com/item?id=24397852">thread link</a>) | @calpaterson
<br/>
September 7, 2020 | http://calpaterson.com/bicycle-threat-model.html | <a href="https://web.archive.org/web/*/http://calpaterson.com/bicycle-threat-model.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
        
        <p>August 2020</p>
        <p id="article-description">How to avoid buying your bike again every 6-12
        months and tips for how to apply the same reasoning to other things, like
        computers</p>
        <hr>
        <figure>
            <img src="http://calpaterson.com/assets/solitary-wheel.jpeg" alt="a solitary wheel left behind after the rest of the bike was nicked">
            <figcaption>
                Rucksack Rupert strikes again
            </figcaption>
        </figure>
        <p>Some very commonly repeated advice on preventing someone from nicking your
        bike:</p>
        <blockquote>
            <p>Buy a good [~10% of bicycle value] lock, ideally one with a high
            "SoldSecure" rating and lock your bicycle somewhere inside the rear
            triangle and the rear wheel.</p>
        </blockquote>
        <p>I've seen this advice repeated in cycling magazines, in quality newspapers,
        sometimes even by the police and of course on internet forums.</p>
        <p>The above would imply that if you own a £400 bicycle (<a href="https://www.statista.com/statistics/395884/bicycle-average-prices-in-the-european-union-eu-by-country/">a
        typical price in the UK</a>) you'd buy a £40 lock and put it in the right
        place.</p>
        <p>However, it is a cold fact that a cordless angle grinder can defeat any
        bicycle lock, no matter how expensive (<a href="https://www.youtube.com/watch?v=pywN558dJaU&amp;t=198">see this video for a
        demonstration</a>). You can buy a used cordless angle grinder on eBay for less
        than £100. Even U-locks - traditionally thought to be the strongest type of
        lock - are opened in seconds with a sub-£100 angle grinder.</p>
        <p>There is other advice floating around of doubtful value, for example:</p>
        <blockquote>
            <p>Add your bicycle to a national register</p>
        </blockquote>
        <p>But cases where BikeRadar manage to reunite bicycles with their owners seem
        to be the exception rather than the rule. Probably this is because, as with
        cars, the first thing you do with a stolen bicycle is take it across a border
        to somewhere different.</p>
        <blockquote>
            <p>Lock your bicycle inside a secure building or place, away from sight</p>
        </blockquote>
        <p>This prevents opportunistic theft but if this space is shared with others
        (apartment blocks and offices) it in fact serves to increase the economies of
        scale for prepared thieves who break into your storage area late at night with
        a van. It's not uncommon for office bicycle stores (with many fancy, expensive
        bikes inside) to be emptied out completely overnight by professional
        thieves.</p>
        <h2>A threat model, by user persona</h2>
        <p>I think the advice above is poor because it doesn't come from a systematic
        consideration of the problem <em>from the point of view of thieves</em>.</p>
        <p>To come up with better advice requires a threat model, which is a piece of
        jargon for taking a holistic view of the danger posed by attackers. I think one
        of the simplest and most straightforward ways to do threat modelling is by
        <em>user persona</em>, whereby you consider each kind of attacker in turn,
        making some reasonable assumptions about their level of motivation and
        methods.</p>
        <p>As far as bicycle theft is concerned there are three basic types of
        thief.</p>
        <h3>"No-tools Nigel", the rank opportunist</h3>
        <p>Nigel has just his two hands and is simply looking for a ride home or maybe
        something he can sell to a friend for some quick cash.</p>
        <p>Nigel will steal any unlocked bicycle.</p>
        <p>Nigel is also able to take any bicycle parts that can be removed without
        tools. That means any quick-release wheels or thumbscrew saddles. In an urban
        area your parked bicycle may be passed by a Nigel as often as a few times an
        hour so anything that is not bolted down won't last very long.</p>
        <h3>"Rucksack Rupert", the thief with a few hand tools</h3>
        <p>Rupert has a small pair of shears; 4, 5 and 6mm Allen keys and a 15mm
        spanner for wheel nuts.</p>
        <p>Rupert will make his way through cable locks with his shears. If there is a
        valuable part that can be removed with hand tools he will take it. He is
        particularly keen on premium saddles and name brand wheels.</p>
        <h3>"Powertool Percy", the professional with a complete set of tools</h3>
        <p>Percy has a small collection of electric and air tools including an angle
        grinder as well as bolt-cutters and an air-jack. He has access to <a href="https://en.wikipedia.org/wiki/Fence_(criminal)">criminal fences</a> which he
        can use to sell stolen bicycles quickly. Percy often arrives in his van and
        this allows him to steal multiple bikes at once.</p>
        <p>No bicycle is safe from Percy. No lock can hold against his angle grinder.
        Often he finds if he's wearing a hi-vis jacket he can even get away with using
        his power tools in broad daylight. He's willing to chance that if the bicycle
        seems valuable enough.</p>
        <h2>Coming up with better advice based on Nigel, Rupert and Percy</h2>
        <p>In order to keep your bicycle safe you need to take steps against all three
        levels of imaginary thieves.</p>
        <p>"No-tools Nigel" will be warded off simply by:</p>
        <ul>
            <li>Locking your bicycle whenever you leave it - even if just for a
            minute</li>
            <li>Ensuring you leave nothing on your bicycle that can be removed without
            tools
                <ul>
                    <li>replace quick-release wheel skewers with bolts</li>
                    <li>take your lights with you when you park in public</li>
                    <li>make sure your saddle is not on a thumbscrew</li>
                </ul>
            </li>
        </ul>
        <p>"Rucksack Rupert" will be deterred by:</p>
        <ul>
            <li>Not using a cable lock!</li>
            <li>Making sure that nothing good can be removed from your bicycle with
            hand tools
                <ul>
                    <li>Lock both wheels <em>and the frame</em> to the bike stand -
                    don't rely on bolts</li>
                </ul>
            </li>
        </ul>
        <p>"Powertool Percy" will be kept at bay by:</p>
        <ul>
            <li>Nothing, save ensuring that your bicycle doesn't look valuable enough
            to be worth his time
                <ul>
                    <li>this probably means keeping its value down below a few hundred
                    pounds</li>
                </ul>
            </li>
        </ul>
        <h2>The virtue of the "bicycle shaped object"</h2>
        <p>Valuable bicycles (&gt;£1000) have an extremely short half-life in urban
        areas. The sad truth is that the Percys of the world are common enough and
        resourceful enough that a bicycle worth over a thousand pounds isn't really
        safe anywhere in a large town. <strong>This includes most e-bikes.</strong> You
        might notice that cycle couriers who have e-bikes tend to eat lunch while
        looking directly at their locked e-bike, so that it never goes out of their
        sight. Few people in other lines of work can do the same.</p>
        <p>If the tyres are inflated, my own commuter bicycle is probably worth £30 to
        the right buyer. My bicycle is so low-end that cycling snobs refer to it as a
        mere "bicycle shaped object". Rather selfishly I am glad that such snobs exist
        as having a lot of more valuable bicycles around provides me with good ambient
        security. No thief is going to bother cutting my locks when there is a
        Campagnolo on the next rack.</p>
        <p>One father I know had his primary-school-age daughter "decorate" his
        commuting bicycle with girly stickers and pink glitter. If anyone examines his
        bicycle closely he looks like a complete loon but I think his motivation is
        right: it's going to be much less appealing to steal when it's covered in Miffy
        stickers.</p>
        <h3>Insurance - not usually worth it</h3>
        <p>What about bicycle insurance? It's fairly expensive here in the UK, usually
        10-15% of the bicycle's value annually and insurers typically only pay out when
        the whole bicycle is taken (so if if your front wheel is nicked, you're on your
        own) and when you can demonstrate that it was locked to their standards. Often
        these standards require that it is locked up indoors which means you're
        chancing it whenever you park away from your home or office.</p>
        <h2>Lists of "best practices" vs having your own threat model</h2>
        <p>The same thing goes for securing your bicycle as for securing other things:
        pat, concrete pieces of security advice are something to treat with a bit of
        doubt.</p>
        <p>In bicycles the common mantra is "spend 10% on a lock" but in computing the
        mantras are slogans such as "use a strong password", "back up your important
        data" or "use encryption" but these can all be just as vapid.</p>
        <p>"Strong passwords!" as a slogan fails to address the fact that the average
        internet user has hundreds of logins for various sites (my own password manager
        has over 700 sites recorded). The majority of internet users decide on a single
        "strong password" and then use it everywhere. They are only a single bad
        sysadmin or javascript injection away from losing access to every account on
        every website they have.</p>
        <p>Backing up your important data is only an aid to your security if the backup
        is stored as securely as the original. Much user data is stolen or exposed
        through poorly secured backups on shared fileservers. A huge number of people
        have passport and utility bill scans in their Dropbox - again, behind the same
        email and password they use everywhere. Companies can be surprisingly sloppy
        with backups too: often dumped into cloud storage somewhere once before the Big
        Migration and never removed.</p>
        <p>Encryption is troublesome as it can give undue confidence that can backfire
        spectacularly: a quarter of a million American diplomatic cables were
        inadvertantly published in unredacted form when a Guardian journalist <a href="https://en.wikipedia.org/wiki/WikiLeaks:_Inside_Julian_Assange%27s_War_on_Secrecy">
        included the password for an widely-distributed encrypted file in his book</a>.
        Apparently he thought the file's password was somehow temporary. It wasn't.</p>
        <p>Instead of following such "best practices" it's much more intellectually
        robust to <strong>come up with your own threat model</strong> - then you can
        decide your own concrete steps instead of just following the security steps of
        others which might be inapplicable or even wrong.</p>
        <h2>Some hints on coming up with your own …</h2></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://calpaterson.com/bicycle-threat-model.html">http://calpaterson.com/bicycle-threat-model.html</a></em></p>]]>
            </description>
            <link>http://calpaterson.com/bicycle-threat-model.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24397852</guid>
            <pubDate>Mon, 07 Sep 2020 08:58:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Post-Open Source]]>
            </title>
            <description>
<![CDATA[
Score 44 | Comments 40 (<a href="https://news.ycombinator.com/item?id=24397552">thread link</a>) | @luu
<br/>
September 7, 2020 | https://www.boringcactus.com/2020/08/13/post-open-source.html | <a href="https://web.archive.org/web/*/https://www.boringcactus.com/2020/08/13/post-open-source.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    
    <p>i’m writing this like a day after <a href="https://www.fastcompany.com/90539632/mozilla-vows-mdn-isnt-going-anywhere-as-layoffs-cause-panic-among-developers">big mozilla layoffs</a> that included a lot of people working on cool and important shit.
the consensus i’m seeing is that it reflects mozilla’s search for profit over impact, mismanagement, and disproportionate executive compensation.
this is taking place in a larger trend of corporatization of open source over the past several years, an ongoing open source sustainability crisis, and of course COVID-19, the all-consuming crisis that makes all our other crises worse.
all of this was summed up most concisely by <a href="https://twitter.com/zkat__/status/1293626135142477825">Kat Marchán</a>:</p>

<blockquote>
  <p>Imo, open source as a community endeavor is falling apart right before our eyes, and being replaced by open source as Big Corp entrenchment strategy.</p>

  <p>I mean it’s been happening for a while, but seeing Mozilla sinking like this is just driving the point home for me.</p>

  <p>FOSS is dead</p>
</blockquote>

<p>how did we get here?
where even are we?
what happens next?</p>

<p>i am incredibly unqualified to answer any of this - i didn’t show up until right around the peak of SourceForge, i wasn’t there for most of this - but i’m not gonna let that stop me.</p>

<h2 id="names">names</h2>

<p>to start this funeral service for FOSS, we have to unpack the term itself.
“free and open source software” as a term already contains multitudes.
on one hand, “free software”, an explicitly political movement with a decidedly anti-charismatic leader.
on the other hand, “open source software”, defanged and corporate-friendly by design.
the free software people (correctly) criticize “open source” as milquetoast centrism.
the open source people (correctly) criticize “free software” as stubborn idealism fighting tooth and nail to reject the real world as it actually exists.
they have as much in common as leftists and liberals (but they’re more prepared to work together), and although their short-term goals were similar enough that it made sense to lump them together (hence the cooperation), now that the movement is dead i think there’s more to gain from considering them separately.
most software licenses that i’m going to bring up technically qualify as both, but they’re popular with one or the other, so i’ll refer to “free software licenses” and “open source licenses” as licenses that are more directly tied to those movements, even though any given license likely meets both definitions.</p>

<p>i’d say free software died a while ago, and open source went horribly right.</p>

<h2 id="freedom">freedom</h2>

<p>the free software movement, for all its faults, has always known <a href="https://www.gnu.org/philosophy/free-sw.html.en">what it’s about</a>:</p>

<blockquote>
  <ol>
    <li>The freedom to run the program for any purpose.</li>
    <li>The freedom to study how the program works, and change it to make it do what you wish.</li>
    <li>The freedom to redistribute and make copies so you can help your neighbour.</li>
    <li>The freedom to improve the program, and release your improvements (and modified versions in general) to the public, so that the whole community benefits.</li>
  </ol>
</blockquote>

<p>it’s concise, it’s understandable, and it’s… kinda useless.
this point was <a href="https://lu.is/blog/2016/03/23/free-as-in-my-libreplanet-2016-talk/">raised better by actual lawyer Luis Villa</a> (Karl Marx slander notwithstanding), but those freedoms don’t actually mean shit to the average end user.
only programmers care if they have access to the source code, and most people aren’t programmers.
and i <em>am</em> a programmer, and i don’t give a shit.
the freedom to not think about my operating system and just get work done overrules all of those for me, so i use windows.
like, yeah, those things are all in principle nice to have, and between two otherwise equally good programs i’d take the free one.
but they’re really fuckin specific things, and even if i have the freedom to do them i’m not likely to have the ability or desire to do them, so there’s no good reason for me as a user to use software that’s worse in other ways because it gives me freedoms i don’t need.</p>

<p>the free software movement is explicitly political, but its politics suck.
it’s a movement by and for ideological diehards but the ideology is extremely esoteric.
theirs was a losing battle from day one.
so what was it that actually killed them?
i think in a very real way it was the GPLv3.</p>

<h2 id="losing">losing</h2>

<p>the flagship projects of the free software movement are probably Linux and the GNU pile of tools.
the Linux kernel being released under a free software license doesn’t directly create more free software, though, since even things that tie closely to the kernel aren’t obligated to also be free software, and of course user-level applications can have whatever license they want.
and also most of the people using Linux right now are using it by accident, distributed as ChromeOS or Android, neither of which is free software.
so Linux is a win for the free software movement but a useless one.</p>

<p>the GNU userland tools are, for the most part, even more underwhelming.
it may be technically more accurate to call it GNU/Linux, but the only time i remember my linux userland tools are GNU or free software at all is when there’s <a href="https://twitter.com/boring_cactus/status/1166408436386430976">some weird inconsistency between a GNU tool and its BSD equivalent</a>, and that’s not exactly ideal.
gcc had, as far as i can tell, been basically <em>the</em> C compiler for a while, if you weren’t stuck with MSVC or something worse.
the free software movement were stubborn ideologues with weird priorities, but they still had one big technical advantage.
then the GPLv3 happened.</p>

<p>the GPLv2 was pretty popular at the time, but there were a couple notable loopholes some big corporations had been taking advantage of, which the free software people wanted to close.
a whole bunch of people thought the GPLv2 was fine the way it was, though - closing the loopholes as aggressively as the GPLv3 did cut off some justifiable security measures, and some people said that it could do more harm than good.
the linux kernel, along with a lot more stuff, declared it was sticking with the GPLv2 and not moving to the GPLv3.
when your movement says “here is the new version of The Right Way To Do Things” and several of your largest adherents say “nah fuck you we’re going with the old version” that is not a good sign.
around the same time, free software organizations were starting to successfully sue companies who were using free software but not complying with the license.
so big companies, like Apple, saw new restrictions coming in at the same time as more aggressive enforcement, and said “well shit, we want to base our software on these handy convenient tools like GCC but we can’t use GPLv3 software while keeping our hardware and software as locked together as we’d like.”
so they started pouring money into a new C compiler, LLVM, that was instead open source.</p>

<p>and LLVM became at least as good as GCC, and a less risky decision for big companies, and easier to use to build new languages.
so the free software movement’s last technical advantage was gone.
its social advantages also kinda went up in flames with the GPLv3, too: the software that was the foundation for the GPL enforcement lawsuits stuck with the GPLv2.
the discourse over that decision was so nasty that the lead maintainer (Rob Landley; he’ll come up later) started an identical project which he wound up relicensing under an open source license because the lawsuits had completely backfired: instead of complying with the terms of the GPL, companies were just avoiding GPL software.</p>

<p>the free software movement, in the end, burned itself out, by fighting for a tiny crumb of success and then turning around and lighting that success on fire.
the death of free software tells us that we can’t use a license to trick corporations into sharing our values: they want to profit, and if good software has a license that puts a limit on how much they can do that, they’ll put more resources into writing their own alternative than they would spend complying with the license in the first place.</p>

<h2 id="openness">openness</h2>

<p>the open source movement manages to share the same short term goals as the free software movement but be bad in almost entirely disjoint ways.
the <a href="https://opensource.org/about">mission of the Open Source Initiative</a> says</p>

<blockquote>
  <p>Open source enables a development method for software that harnesses the power of distributed peer review and transparency of process.
The promise of open source is higher quality, better reliability, greater flexibility, lower cost, and an end to predatory vendor lock-in.</p>
</blockquote>

<p>this is so profoundly different from the free software definition that it’s almost comical.
where free software says “we value freedom, which we define in these ways,” open source says “your code will get better.”
the free software movement was prepared to start fights with corporations that used their work but didn’t play by their rules.
the open source movement was invented to be a friendly, apolitical, pro-corporate alternative to the free software movement.</p>

<p>the contrast between “use free software because it preserves your freedom” and “use open source software because it’s better” is profound and honestly a little disappointing to revisit this explicitly.
free software preserves freedoms i don’t need or care about as a user, but it does at least do that.
open source software is frequently not in fact better than closed source alternatives, and “use open source software because on rare occasions it manages to be almost as good” is an even more underwhelming sales pitch than anything free software can give.</p>

<p>where free software is misguided and quixotic, open source is spineless and centrist.
and as tends to happen with spineless centrism, it has eaten the world.</p>

<h2 id="winning">winning</h2>

<p>if there’s anything corporations love more than rewriting software so it lets them make all the money they can dream of, it’s letting other people do that work for them.
it took a while to take off, because the conservative approach of “keep things closed source” was pretty solidly entrenched in a lot of places, but now even the once conservative holdouts have accepted the gospel of centrism.
corporations have little to nothing to lose by publishing existing source code, and can gain all sorts of unpaid volunteer labor.
if they start a new internal project, important enough that they’re …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.boringcactus.com/2020/08/13/post-open-source.html">https://www.boringcactus.com/2020/08/13/post-open-source.html</a></em></p>]]>
            </description>
            <link>https://www.boringcactus.com/2020/08/13/post-open-source.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24397552</guid>
            <pubDate>Mon, 07 Sep 2020 08:06:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Things I Learned to Become a Senior Software Engineer]]>
            </title>
            <description>
<![CDATA[
Score 138 | Comments 118 (<a href="https://news.ycombinator.com/item?id=24397269">thread link</a>) | @janvdberg
<br/>
September 7, 2020 | https://neilkakkar.com/things-I-learned-to-become-a-senior-software-engineer.html | <a href="https://web.archive.org/web/*/https://neilkakkar.com/things-I-learned-to-become-a-senior-software-engineer.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>In 2018, I started working at Bloomberg. Things have changed a lot since. I’m not the most junior member in the company anymore and I’ve mentored quite a few new engineers, which has been amazing. It helped me observe how others differ from me, absorb their best practices, and figure out things I’ve unconsciously been doing pretty well.</p>

<p>Yearly work reviews are a good way to condense these lessons I’ve learned.  They’re valuable for pattern matching, too. Only when I zoom out do certain patterns become visible. I can then <a href="https://neilkakkar.com/the-human-log.html">start tracking these patterns consciously</a>.</p>

<p>The broad theme for this year is zooming out and challenging the boundaries. It’s also about zooming in, and adding nuance to the sections from last year. It’s more fun if you’ve <a href="https://neilkakkar.com/year-in-review-2019.html">read last year’s review first</a>: You can then diff my growth.<sup id="fnref:2"><a href="#fn:2">1</a></sup></p>

<p>It all began with a question: How do I grow further?</p>





<nav>

  <h4>Table of Contents</h4>

<ul id="markdown-toc">
  <li><a href="#growing-using-different-ladders-of-abstraction" id="markdown-toc-growing-using-different-ladders-of-abstraction">Growing using different ladders of abstraction</a></li>
  <li><a href="#learning-what-people-around-me-are-doing" id="markdown-toc-learning-what-people-around-me-are-doing">Learning what people around me are doing</a></li>
  <li>
<a href="#learning-good-habits-of-mind" id="markdown-toc-learning-good-habits-of-mind">Learning good habits of mind</a>    <ul>
      <li><a href="#thinking-well" id="markdown-toc-thinking-well">Thinking Well</a></li>
      <li><a href="#strategies-for-making-day-to-day-more-effective" id="markdown-toc-strategies-for-making-day-to-day-more-effective">Strategies for making day-to-day more effective</a></li>
    </ul>
  </li>
  <li><a href="#acquiring-new-tools-for-thought--mental-models" id="markdown-toc-acquiring-new-tools-for-thought--mental-models">Acquiring new tools for thought &amp; mental models</a></li>
  <li><a href="#protect-your-slack" id="markdown-toc-protect-your-slack">Protect your slack</a></li>
  <li><a href="#ask-questions" id="markdown-toc-ask-questions">Ask Questions</a></li>
  <li><a href="#noticing-confusion" id="markdown-toc-noticing-confusion">Noticing Confusion</a></li>
  <li><a href="#force-multipliers" id="markdown-toc-force-multipliers">Force multipliers</a></li>
  <li><a href="#on-ownership" id="markdown-toc-on-ownership">On Ownership</a></li>
  <li><a href="#embrace-fear" id="markdown-toc-embrace-fear">Embrace fear</a></li>
  <li>
<a href="#adding-nuance" id="markdown-toc-adding-nuance">Adding nuance</a>    <ul>
      <li><a href="#writing-code" id="markdown-toc-writing-code">Writing Code</a></li>
      <li><a href="#testing" id="markdown-toc-testing">Testing</a></li>
      <li>
<a href="#design" id="markdown-toc-design">Design</a>        <ul>
          <li><a href="#gathering-requirements" id="markdown-toc-gathering-requirements">Gathering Requirements</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#some-hacks-that-have-worked-very-well-for-me" id="markdown-toc-some-hacks-that-have-worked-very-well-for-me">Some hacks that have worked very well for me</a></li>
  <li><a href="#super-powers" id="markdown-toc-super-powers">Super powers</a></li>
  <li>
<a href="#some-gotchas-with-growing" id="markdown-toc-some-gotchas-with-growing">Some gotchas with growing</a>    <ul>
      <li><a href="#sometimes-i-feel-i-need-to-know-the-answer-to-everything" id="markdown-toc-sometimes-i-feel-i-need-to-know-the-answer-to-everything">Sometimes, I feel I need to know the answer to everything</a></li>
      <li><a href="#sometimes-i-lose-my-cool" id="markdown-toc-sometimes-i-lose-my-cool">Sometimes, I lose my cool</a></li>
      <li><a href="#neophilia" id="markdown-toc-neophilia">Neophilia</a></li>
    </ul>
  </li>
  <li><a href="#questions" id="markdown-toc-questions">Questions</a></li>
</ul>

</nav>

<!-- works only once jQuery is loaded -->


<h2 id="growing-using-different-ladders-of-abstraction">Growing using different ladders of abstraction</h2>

<p>Entering my second year, I had all the basics in place. I had picked all the low hanging fruit, and my rate of growth slowed down. Not good. The big question in my mind was “How do I grow further?”</p>

<p>There was only so much I could do to improve my coding skills. Most blogs epousing techniques to write cleaner code, repeating yourself, not repeating yourself, etc. are micro-optimisations. Almost none of them would make me instantly impactful.<sup id="fnref:3"><a href="#fn:3">2</a></sup></p>

<p>However, I did figure out something insightful. I’m working inside the software development lifecycle, but this lifecycle is part of a bigger lifecycle: the product and infrastructure development lifecycle. I decided to go broader instead of deeper. Surprisingly, the breadth provided more depth to what I knew.</p>

<p>I zoomed out in 3 broad directions: learning what people around me are doing, learning good habits of mind, and acquiring new tools for thought.</p>

<h2 id="learning-what-people-around-me-are-doing">Learning what people around me are doing</h2>

<p>Since we’re not in a closed system, it makes sense to better understand the job of the product managers, the sales people, and the analysts. In the end it’s a business making money through products. The goal isn’t to write code, it’s to be a profitable business.<sup id="fnref:15"><a href="#fn:15">3</a></sup></p>

<p>Most big companies aren’t doing just one thing, which means there are different paths to making money in the same company. Everyone is on at least one path - if they weren’t, they wouldn’t be here.<sup id="fnref:1"><a href="#fn:1">4</a></sup> Tracking these paths, and the path I’m on was pretty valuable. It helped me see how I matter, and what levers I can pull to become more effective. Sometimes, it’s about making the sales jobs easier, so they can make more sales. Other times, it’s about building a new feature for clients. And some other times, it’s about improving a feature that keeps breaking.</p>

<p>Product managers are the best source for this. They know how the business makes money, who are the clients, and what do clients need.</p>

<p>Over the year, I setup quite a few meetings with everyone on my path. A second benefit this gave me was the context of other’s jobs. It helped me communicate better. Framing things in the right way is powerful.</p>

<p>For example, one conversation helped me appreciate why Sarah in Sales wants a bulk update tool. Some companies have lots of employees, and  updating them one by one is a pain. The code I write would literally ease Sarah’s pain.</p>

<!-- More recently, I got a chance to sit in on a few product scoping meetings between teams. This gave me a lot more appreciation for the job my PM and TLs do. Communication is surprisingly hard, and aligning different teams takes skill. -->

<!-- Force multiplier in team vs force multiplier in the entire chain. -->

<h2 id="learning-good-habits-of-mind">Learning good habits of mind</h2>

<p>Software engineering entails thinking well and making the right decisions. Programming is implementing those decisions.</p>

<p>A habit of mind is something your brain does regularly. This could be thinking of X whenever you see Y happen, or applying thinking tool X to problem Y. In short, habits of mind facilitate better thinking.</p>

<p>I suspected if I learn the general skill, I should be able to apply it better to software engineering.</p>

<h3 id="thinking-well">Thinking Well</h3>

<p>Software engineering is an excellent field to practice thinking well in. The feedback loops are shorter, and gauging correctness doesn’t take too long.</p>

<p>I dived into cognitive science studies. It’s a permanent skill that’s worth exploring - a force multiplier for whatever I end up doing, and pays dividends throughout my life. One output was <a href="https://neilkakkar.com/Bayes-Theorem-Framework-for-Critical-Thinking.html">a framework for critical thinking</a>. It’s compounding, <a href="https://neilkakkar.com/year-in-review-2019.html#compounding-is-powerful-building-intuition-for-compounding-even-more-so">and compounding is powerful</a>.</p>

<p>There’s lots of good things that came out of this, which I’ll talk about in a bit. They deserve their own section.</p>

<h3 id="strategies-for-making-day-to-day-more-effective">Strategies for making day-to-day more effective</h3>

<p>The other side of the coin is habits that allow you to think well. It starts with noticing little irritations during the day, inefficiencies in meetings, and then figuring out strategies to avoid them. These strategic improvements are underrated.</p>

<p>You decide what to do, and then let it run on automatic, freeing up the brain to think of more fun stuff. Of course, that’s what a habit is, too.</p>

<p>Some good habits I’ve noticed:</p>

<ul>
  <li>Never leave a meeting without making the decision / having a next action</li>
  <li>Decide who is going to get it done. Things without an owner rarely get done.</li>
  <li>Document design decisions made during a project</li>
</ul>

<p>This pattern became visible during the review, so I’m keen to pay attention and collect more strategies next year. Having an excellent scrum master who holds me accountable has helped me get better at following these strategies.</p>



<p>New tools for thought are related to thinking well, but more specific to software engineering. Tools for thought help me think better about specific engineering problems.</p>

<p>I’ve adopted a just-in-time approach to this. I look for new tools only when I get stuck on something, or when I find out my abstractions and design decisions aren’t working well.</p>

<p>For example, I was recently struggling with a domain with lots of complex business logic. Edge cases were the norm, and we wanted to design a system that handles this cleanly. That’s when I read about <a href="https://amzn.to/2FdCYUQ" target="_blank" rel="noopener">Domain Driven Design</a><sup id="fnref:25"><a href="#fn:25">5</a></sup>. I could instantly put it to practice and make a big difference. Subsequently, I grasped these concepts better. I acquired a new mental model of how to create enterprise software.</p>

<p>The second way I keep learning and acquiring new mental models is via reading what surfaces on Hacker News. They are interesting ideas, some of which I’ve put to practice, but this is a lot less effective than the technique above. The only reason I still do this is to <a href="https://neilkakkar.com/rationality.html#map-and-the-territory">map the territory</a> - it keeps me aware of techniques that exist, so when I face a problem, I know there’s a method that might help.</p>

<p>The final way I acquire better mental models is by learning new diverse languages. The diversity bit is important. Learning yet another dialect of lisp has a lot less benefit than say, learning C++03, a functional programming language, a dynamic typed language, and a lisp. Today, <a href="https://www.hillelwayne.com/post/j-notation/" target="_blank" rel="noopener">J seems interesting</a>, and one I might consider learning. It’s a thinking model I haven’t used before.</p>

<p>I’ve gotten lots of value from doing this. Each language has its own vocabulary and grammar, and <a href="https://neilkakkar.com/vocabulary-mental-model.html">vocabulary is a meta-mental model</a>. It’s a new lens to look at how to do things.</p>

<p>When memory management is in your control, you understand how pointers and allocators work. When Python then abstracts this away, you appreciate the complexity reduction. When maps and filters in a functional language show up, you appreciate how Python’s for loops can be improved. Indeed, that’s what list comprehensions are. And then you notice how some things are easier with object oriented programming. There’s no one magic tool that fits everything well. And then you understand that despite this, you don’t have to switch tools. You can adapt best practices from one into another to solve your problems: like writing functional javascript. It’s the principles that matter more than their expression.</p>

<figure>
    
    <img src="https://neilkakkar.com/assets/images/divider.jpg" alt="">
    
    
    
</figure>

<p>Broadly, that’s all I did this year. What follow are insights that sprang forth thanks to zooming out.</p>

<h2 id="protect-your-slack">Protect your slack</h2>

<p>When I say slack, I don’t mean the company, but the adjective.</p>

<p>One thing that gives me high output and productivity gains is to “slow down”. Want to get more done? Slow down.</p>

<p>Caveats apply, but here’s what I mean:</p>

<p>I’ve noticed people rush to solve problems. It can be something they’ve done before, or something we have a template for. It feels pretty good to smash through things. I’ve done that before, too! However, there’s very specific cases where this makes sense.<sup id="fnref:11"><a href="#fn:11">6</a></sup></p>

<p>Whenever I’m working on something new, I take the time to learn things about the system I’m working on, and things closely related to it. If it’s too massive, I optimise for learning as much as I can. Every time I revisit the system, I aim to learn more.</p>

<p>When <a href="https://www.lesswrong.com/posts/yLLkWMDbC9ZNKbjDG/slack" target="_blank" rel="noopener">there is slack</a>, you get a chance to experiment, learn, and think things through. This means you get enough time to get things done.</p>

<p>When there is no slack, deadlines are tight, and all your focus goes into getting shit done.</p>

<p>Protecting your slack means not letting deadlines wrap tight around you. Usually, this is as simple (or hard) as communicating.<sup id="fnref:16"><a href="#fn:16">7</a></sup></p>

<p>Slack might have a negative connotation with “slackers”, but protecting slack is a super power. It’s a long term investment into building yourself up at the cost of short term efficiency.</p>

<p>When I’m quickly dishing out stories, I also have a much harder time fixing bugs. I don’t take the time to create proper mental models of the system, which means my assumptions don’t match the code, and this mismatch is where most bugs lie.</p>

<p>I protect my slack, so I can take the time out to prioritise …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://neilkakkar.com/things-I-learned-to-become-a-senior-software-engineer.html">https://neilkakkar.com/things-I-learned-to-become-a-senior-software-engineer.html</a></em></p>]]>
            </description>
            <link>https://neilkakkar.com/things-I-learned-to-become-a-senior-software-engineer.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24397269</guid>
            <pubDate>Mon, 07 Sep 2020 07:11:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Never Lose a Part Again, with the Ultimate Component Storage System]]>
            </title>
            <description>
<![CDATA[
Score 24 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24396611">thread link</a>) | @kevinqqsam
<br/>
September 6, 2020 | https://www.hackster.io/news/never-lose-a-part-again-with-the-ultimate-component-storage-system-00987cde6744 | <a href="https://web.archive.org/web/*/https://www.hackster.io/news/never-lose-a-part-again-with-the-ultimate-component-storage-system-00987cde6744">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section><div><p><span>The creator of </span><a href="https://www.instructables.com/id/The-Ultimate-Component-Storage-System/" rel="nofollow">this project</a><span>, APTechnologies, came up with the idea for the storage cabinet by observing that he would run into trouble when organizing his component library. Rather than having to go across the workshop to get each part, he wanted a way to consolidate everything into one place for easy finding. The system works by first having the user search for a part on an adjacent screen, and then the associated bin will light up if the component is found in the system.</span></p><h3 id="toc-designing-the-cabinet-0"><a href="#toc-designing-the-cabinet-0"><i></i></a><span>Designing the Cabinet</span></h3><p>The storage solution is comprised of a 35 x 12 grid of drawers, which gives ample space for parts. The drawers are laid out according to the spacing between the LEDs in the strip (30 LEDs/meter). They are meant to be printed individually and then slotted in, plus they come in several different sizes.</p><h3 id="toc-fabrication-1"><a href="#toc-fabrication-1"><i></i></a><span>Fabrication</span></h3><p>Each drawer was 3D-printed on a Prusa Mk2S with PLA filament at a .2mm layer height. In order to minimize filament consumption, wall thicknesses are kept very thin, but even with this savings, 5kg of filament was required to fabricate the parts. There is also a small articulating arm that juts out from the side for the HDMI display.</p><h3 id="toc-software-2"><a href="#toc-software-2"><i></i></a><span>Software</span></h3><p>The primary way to interact with the system is through the terminal, which runs a Python 3 script. It begins by checking the text file for data integrity, where it then gets parsed and loaded into an object. Data is stored in CSV-style fashion, with the ID being the first column, the associated LEDs in the second, and finally the quantity in the third column. All subsequent values are optional and simply loaded by the user when the parts is located. A regex is used to parse the user's request, including searching for a component, changing its quantity, adding a new part, and removing a component.</p><h3 id="toc-electronics-3"><a href="#toc-electronics-3"><i></i></a><span>Electronics</span></h3><p>According to the project's creator, the choice for hardware components was quite straightforward. He used a Raspberry Pi 4 Model B in conjunction with a generic HDMI monitor to show the command line interface. For power delivery, he opted for a simple 5V adapter to power both the Raspberry Pi 4 and NeoPixel strip. Since the Pi outputs 3.3v signals from its GPIO pins, a level-shifter is needed, in this case a 74AHCT125. There is an option to use an Arduino Uno via UART if the NeoPixel strip is too unreliable, since the Arduino Uno can supply stricter timings.</p><h3 id="toc-usage-4"><a href="#toc-usage-4"><i></i></a><span>Usage</span></h3><p><span>To add a new part, the command </span><code>PI&lt;ledn&gt;:PI&lt;ledn+k&gt;, &lt;quantity&gt;[, optional parameters]:add</code><span> is sent, which adds a new component to the library. Other commands such as ID&lt;id number&gt; (search for part by ID) and ID&lt;id number&gt;:rm (remove part with that ID) exist to manage added components in the library.</span></p><h3 id="toc-possible-additions-5"><a href="#toc-possible-additions-5"><i></i></a><span>Possible Additions</span></h3><p>Although the system works well for one place and a small collection of components, it tends to scale poorly when there's a large library or multiple users want to interact with it. One way to solve this issue is to replace the single text file with a relational database such as MySQL, where parts can be stored and indexed in a single table, and other data can be referenced. This gives a very powerful interface for the software to easily search, add, and modify parts without having to read and rewrite the text file constantly.</p></div></section><section></section><div><div><a href="https://www.hackster.io/gatoninja236"></a><div><p><span><a href="https://www.hackster.io/gatoninja236">Arduino “having11” Guy</a><span></span></span></p><p>18 year-old IoT and embedded systems enthusiast. Also an interned at Hackster.io and love working on projects and sharing knowledge.</p></div></div></div></div></div>]]>
            </description>
            <link>https://www.hackster.io/news/never-lose-a-part-again-with-the-ultimate-component-storage-system-00987cde6744</link>
            <guid isPermaLink="false">hacker-news-small-sites-24396611</guid>
            <pubDate>Mon, 07 Sep 2020 04:21:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My thoughts about editors in 2020]]>
            </title>
            <description>
<![CDATA[
Score 48 | Comments 64 (<a href="https://news.ycombinator.com/item?id=24395863">thread link</a>) | @todsacerdoti
<br/>
September 6, 2020 | https://phaazon.net/blog/editors-in-2020 | <a href="https://web.archive.org/web/*/https://phaazon.net/blog/editors-in-2020">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><h2><em>editor, vim, neovim, atom, vs-code, emacs, intellij-idea</em></h2><h2>2020-09-07 00:03:00 UTC, by Dimitri Sabadie — <a href="https://phaazon.net/blog/feed">feed</a></h2><hr><div><p>I have been trying a lot of editors lately. When I say “trying”, I really meant I spent some time configuring and using those editors. The ones I spent time using are:</p>
<ul>
<li><a href="https://neovim.io/">neovim</a>, my main, daily editor I use for pretty much almost all projects I work on.</li>
<li><a href="https://www.jetbrains.com/idea">IntelliJ IDEA</a>, that I currently use at work to hack on Java codebases.</li>
<li><a href="https://code.visualstudio.com/">VS Code</a>, that I have been trying mainly in Rust, TOML and Markdown.</li>
<li><a href="https://www.gnu.org/software/emacs">emacs</a>, that I highly hacked around to play on my Haskell and Rust codebases (and YAML / Markdown / TOML too).</li>
<li><a href="https://github.com/hlissner/doom-emacs">DOOM Emacs</a>, that I tried when I saw a colleague in a previous company I worked in (I was impressed by the “united” feeling of the UI and by how everything looked so slick). So I gave it a try.</li>
<li><a href="https://atom.io/">atom</a>, <a href="https://github.com/">GitHub</a>’s take on editors. Same thing, mostly Rust, Haskell, etc.</li>
<li>A bunch of others that I won’t talk about because I quickly stopped using.</li>
</ul>
<p>The goal of this article is to create a temporal “snapshot” of my views on editors and what I think about the current situation. I have been using vim and especially neovim for more than a decade. I need to explain about my current workflow and what I cherish in editing before talking about each editors. Expect a point of view from a neovimer which is looking around at lots of editors.</p>
<blockquote>
<p>This is only a personal workflow / point of view that works well <em>for me right now</em>. It doesn’t mean it will for you and it doesn’t mean a different workflow would be worse.</p>
</blockquote>
<!-- vim-markdown-toc GFM -->
<ul>
<li><a href="#what-i-think-powerful-editing-should-be">What I think powerful editing should be</a>
<ul>
<li><a href="#keyboard-layout">Keyboard layout</a></li>
<li><a href="#modal-editors">Modal editors</a></li>
<li><a href="#how-i-like-to-move-around">How I like to move around</a></li>
<li><a href="#all-the-other-modal-candies">All the other modal candies</a></li>
</ul></li>
<li><a href="#the-editors">The editors</a>
<ul>
<li><a href="#neovim">neovim</a>
<ul>
<li><a href="#my-neovim-setup">My neovim setup</a></li>
<li><a href="#what-i-like-about-neovim">What I like about neovim</a></li>
<li><a href="#what-i-dislike-about-neovim">What I dislike about neovim</a></li>
</ul></li>
<li><a href="#intellij-idea">IntelliJ IDEA</a>
<ul>
<li><a href="#what-i-like-about-intellij-idea">What I like about IntelliJ IDEA</a></li>
<li><a href="#what-i-dislike-about-intellij-idea">What I dislike about IntelliJ IDEA</a></li>
</ul></li>
<li><a href="#vs-code">VS Code</a>
<ul>
<li><a href="#what-i-like-about-vs-code">What I like about VS Code</a></li>
<li><a href="#what-i-dislike-about-vs-code">What I dislike about VS Code</a></li>
</ul></li>
<li><a href="#emacs-and-doom-emacs">emacs and DOOM emacs</a>
<ul>
<li><a href="#what-i-like-about-emacs--doom-emacs">What I like about emacs / DOOM emacs</a></li>
<li><a href="#what-i-dislike-about-emacs--doom-emacs">What I dislike about emacs / DOOM Emacs</a></li>
</ul></li>
<li><a href="#atom">atom</a>
<ul>
<li><a href="#what-i-like-about-atom">What I like about atom</a></li>
<li><a href="#what-i-dislike-about-atom">What I dislike about atom</a></li>
</ul></li>
</ul></li>
<li><a href="#wrap-it-up">Wrap it up</a></li>
</ul>
<!-- vim-markdown-toc -->

<h2 id="keyboard-layout">Keyboard layout</h2>
<p>I am French and I’m using a keyboard layout that is made to type very quickly in French and to code. With hindsight, since I type more often in English than in French, maybe I should have picked another keyboard layout, but the coding experience in my keyboard layout is really great, so I stick around.</p>
<p>The keyboard layout is <strong>bépo</strong>. I learned bépo the “recommended” way — i.e.&nbsp;you have to practice <em>typing</em> (« dactylographie » in French). It means that I use all my fingers to type on a keyboard, and that each key on the keyboard is assigned <em>a single finger</em> that will press it. That helps a lot with muscle memory and to reduce wrist pain (my wrists barely move when I type on a keyboard), among other things. The typing speed is a side-effect of being accurate and having good comfort (if you are curious, I’m pretty fast but there are faster people — I type at around 120 to 130 words per minute). Because I think the speed doesn’t matter when programming, I think the most important part to remember here is the comfort: the wrists don’t move and my fingers fly around the keyboard, whatever the speed.</p>
<h2 id="modal-editors">Modal editors</h2>
<p>I think a modal editor is superior, for various reasons. The first one is that I truly <strong>hate</strong> having to use a <em>mouse</em> for something that can be done without having to move around hundred of pixels with your cursor and clicking on buttons. For instance, running an application, on my current machines, is simply typing <code>alt d</code>, the name of the program (I typically use completion, so I never type fully the name of the program) and hit enter. All this without moving my hands from the keyboard. And I do that for programs like <code>firefox</code>, <code>kdenlive</code>, etc. but for terminal applications, I simply type and complete them in my terminal, which I open simply with <code>alt return</code>.</p>
<p>So, using a mouse to move around a cursor in an editor feels like completely suboptimal to me, especially because we write code (i.e.&nbsp;we type on a keyboard) most of the time, so moving around with the mouse implies several back and forth movements between the keyboard and the mouse. Maybe you don’t care and it’s cool to you, but to me, this is truly <em>horror land</em>. I feel <em>very</em> uncomfortable when doing this.</p>
<p>Also, <em>modern editors</em> that are not modal typically make people move by using the arrows keys, which are either far on your keyboard, or — like my keyboard, a 60% home made one — not in direct access and then require a function key to enable them.</p>
<p>So that’s the first reason why I like modal editors. They make a smarter use of your keyboard for simple yet recurrent features, like moving around — e.g.&nbsp;<code>h j k l</code>. The second reason why I like them is because of the facts they have a completely new mode for non-modal editor (i.e.&nbsp;the <em>normal</em> mode), you have a whole keyboard / lots of keys to bind actions to and do lots of things people usually do with the mouse. Being able to split an editor into several buffers, move around the buffers, go at the beginning of the paragraph, search and replace, register actions into macros and replay them, etc. All this without even moving the wrists. The learning curve is steep if you’re used to the mouse, but once you’ve passed the mental barrier, really, and this is a personal opinion, but I truly think that using the mouse again after that feels like handicap to me.</p>
<h2 id="how-i-like-to-move-around">How I like to move around</h2>
<p>When I look at people coding, I see several kind of programmers:</p>
<ul>
<li>The ones who move with the arrows or with <code>h j k l</code> in modal editors. You can spot them very easily at how the cursor moves in a document. It typically implies keeping a key pressed until the cursor reach a given row, then pressing another key until the cursor reach a given column and then adjust if they went passed the location they had in mind.</li>
<li>People using the mouse, by clicking at the place they want to put the cursor at.</li>
<li>People using <em>relative numbers</em>. That’s an enhancement of the first group: they typically use relative numbers to know which lines they want to jump to very quickly, so that they don’t have to press the up / down keys for ages until reaching the line they want to. Instead, they look at the number on the lines, press that number, and the direction, and bam, they are on the lines. They then use typical motions from vim like <code>$</code> (go to end of line), <code>f</code> (go to the first occurrence of the next character typed after <code>f</code>, like <code>f(</code> will make your cursor go to the next <code>(</code>), <code>%</code> (go to the matching delimiter) or <code>w</code> (go to the beginning of the next word) / <code>b</code> (go to the beginning of the previous word), etc. to move faster on the same line (it works across lines too).</li>
</ul>
<p>I think that represents 99,9% of what I see people do. Obviously, you will get that I don’t belong to the second set of people… but I don’t really belong to any, actually. How I move is, I guess, convoluted for most people and I know some people won’t understand how it can feel. I use <code>h j k l</code> and all the motions from vim described in the third group (and even more; I full lecture of four hours would be required to explain all of them :D), but it all depends on the <em>distance</em> I need to travel. If my cursor is on a word and I want to move to the beginning of a word located very closely to my cursor on the same line, I’ll simply type <code>www</code> if it’s three words apart (or <code>3w</code> if I’m feeling funny). If the distance is higher, I use a tool called [EasyMotion].</p>
<p>Easymotion really is a wonderful tool. The idea is that it has several modes, depending on the kind of move you want to perform:</p>
<ul>
<li><em>By lines</em>: this mode allows you to jump to any line in the current (or all open) buffers.</li>
<li><em>By words</em>: tihs mode allows you to jump to any “word” in the current (or all open) buffers.</li>
<li><em>By characters</em>: when the <em>word</em> mode doesn’t help with jumping to a special operator or character (because it’s not recognized as a word), this mode allows you to jump to any character in the current or (or all open) buffers.</li>
<li>It has other modes but I have never really found useful cases for them.</li>
</ul>
<p>The way I typically do it is by mapping the three modes to <code>&lt;leader&gt;l</code>, <code>&lt;leader&gt;w&gt;</code> and <code>&lt;leader&gt;c</code> (in my case, <code>&lt;leader&gt;</code> is the <em>space</em> key).</p>
<p>Typing <code>SPC l</code> in my current buffer results in this:</p>
<figure>
<img src="https://phaazon.net/media/uploads/easymotion_lines.png" alt=""><figcaption>EasyMotion lines</figcaption>
</figure>
<p>Typing any highlighted character will make my cursor jump to it. The same applies for words, with <code>SPC w</code>:</p>
<figure>
<img src="https://phaazon.net/media/uploads/easymotion_words.png" alt=""><figcaption>EasyMotion words</figcaption>
</figure>
<p>For the <em>character</em> mode, after pressing <code>SPC c</code>, I have to press another character (the one I want to jump to). Let’s say we want to jump to a <code>#</code> (which is not part of words): <code>SPC c #</code>:</p>
<figure>
<img src="https://phaazon.net/media/uploads/easymotion_chars.png" alt=""><figcaption>EasyMotion characters</figcaption>
</figure>
<p>This way of moving is not intuitive at first, but once you get used to it… it’s a <em>must have</em>.</p>
<h2 id="all-the-other-modal-candies">All the other modal candies</h2>
<p>Among all the things that I like about modal editing, here is a non-exhaustive list of features I expect to have around my fingers:</p>
<ul>
<li><code>C-i</code> and <code>C-o</code>: those allows me to jump to something / a file / a place in a buffer and then go back to where I was right before with <code>C-o</code> (read it like <em>out</em>) or go back again with <code>C-i</code> (read it like <em>in</em>).</li>
<li>Macros and registers: those allow me to yank content into different registers (like clipboards) by assigning a single key to paste their content. For instance, I can put a few lines in my <code>t</code> register with <code>"tyi(</code> (“put in the <code>t</code> register the <code>y</code>ank <code>i</code>nside matching <code>(</code>), and paste that content later with <code>"tp</code>. Macros allow more powerful editing control by assigning a key to set of actions with the <code>q</code> keyword (<code>qa</code> will register all the next keystrokes and actions into the <code>a</code> macro). Then simply replay the macro with <code>@a</code>, for instance.</li>
<li>Obviously, all the basic vim motions, like <code>d</code> to delete, <code>y</code> to yank, <code>c</code> to change, <code>t</code> to go to the character right before the one you search, <code>%</code> to go to the other delimiter, etc. And more complex text manipulation, such as “Let’s change what’s inside this function parameter list, delimited by <code>(</code>”: <code>ci(</code>.</li>
</ul>
<p>It would take too much time to list everything, but the main idea is: I need the modal features when editing code.</p>

<p>So let’s talk about the list of editors I mentioned in the preface. The idea is to give <em>my own opinion</em> on those …</p></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://phaazon.net/blog/editors-in-2020">https://phaazon.net/blog/editors-in-2020</a></em></p>]]>
            </description>
            <link>https://phaazon.net/blog/editors-in-2020</link>
            <guid isPermaLink="false">hacker-news-small-sites-24395863</guid>
            <pubDate>Mon, 07 Sep 2020 00:50:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[PostgreSQL B-Tree index deduplication]]>
            </title>
            <description>
<![CDATA[
Score 184 | Comments 25 (<a href="https://news.ycombinator.com/item?id=24395825">thread link</a>) | @petergeoghegan
<br/>
September 6, 2020 | https://blog.rustprooflabs.com/2020/09/postgres-beta3-btree-dedup | <a href="https://web.archive.org/web/*/https://blog.rustprooflabs.com/2020/09/postgres-beta3-btree-dedup">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                    <p>By Ryan Lambert -- Published September 06, 2020</p><p>PostgreSQL 13 development is coming along nicely, Postgres 13 Beta3 was
<a href="https://www.postgresql.org/about/news/2060/">released on 8/13/2020</a>.
The Postgres Beta 1 and 2 releases were released in May and June 2020.
One of the features that has my interest in Postgres 13 is the B-Tree deduplication effort.  B-Tree indexes are the default indexing method
in Postgres, and are likely the most-used indexes in production 
environments.
Any improvements to this part of the database are likely to have wide-reaching benefits.
Removing duplication from indexes keeps their physical size smaller,
reduces I/O overhead, and should help keep <code>SELECT</code> queries fast!</p>
<!--endteaser-->

<blockquote>
<p>This post is part of the series <a href="https://blog.rustprooflabs.com/2018/06/pg-series-toc"><em>PostgreSQL:  From Idea to Database</em></a>.</p>
</blockquote>
<p>There is a good summary of the what and how of this improvement
in <a href="https://www.cybertec-postgresql.com/en/b-tree-index-deduplication/">Laurenz Albe's post</a> from early in June 2020, presumably using
Pg13 Beta1.  <a href="https://www.highgo.ca/2020/07/06/features-in-pg13-deduplication-in-b-tree-indexes/">Hamid Akhtar's post in July</a>
covered this feature using Pg13 Beta2 and a different approach including
a look at the performance using <code>EXPLAIN</code>.
This post takes yet another look at this improvement using Pg13 Beta3.
I intend to see how well this improvement pans out on a data set I use in production.  For that task my go-to is
<a href="https://www.openstreetmap.org/">OpenStreetMap</a> data loaded to Postgres/PostGIS <a href="https://blog.rustprooflabs.com/2020/01/postgis-osm-load-2020">using osm2pgsql</a>.</p>
<h2>Install Postgres 13 Beta 3</h2>
<p>The first step is to install the two versions of Postgres (12 and 13beta3) 
on a single Ubuntu 18 host.  In the past when I have tested pre-production
releases I have built Postgres from source instead of using <code>apt</code>.
This time around I decided to use <code>apt install</code>, so am including the
basic process for that.</p>
<p>The advised way to install PostgreSQL is from the pgdg (PostgreSQL Global Development Group) repositories,
see <a href="https://wiki.postgresql.org/wiki/Apt">the Postgres wiki</a> for more.
To enable the beta versions, the line needed in <code>/etc/apt/sources.list.d/pgdg.list</code> is:</p>
<pre><code>deb http://apt.postgresql.org/pub/repos/apt/bionic-pgdg main 13
</code></pre>
<p>With that in place, update the sources and install Postgres and PostGIS.</p>
<pre><code>sudo apt update
# Postgres 12
sudo apt install postgresql-12 postgresql-12-postgis-3
# Postgres 13 (Currently Beta)
sudo apt install postgresql-13 postgresql-13-postgis-3
</code></pre>
<p>On Ubuntu, installing multiple versions will create multiple instances running on different ports. The test server I'm using to write this post
currently has three versions of Postgres installed, only two are currently running. Postgres 12 was installed first so "won" the default port of 5432.  Postgres 11 was installed second and was assigned 5433, and Pg13 beta 3 was installed last and was assigned port 5434.  The <code>pg_lsclusters</code> is avaiable on Debian/Ubuntu hosts as part of the wrapper around <code>pg_ctl</code>.</p>
<pre><code>sudo -u postgres pg_lsclusters
Ver Cluster Port Status Owner    Data directory              Log file
11  main    5433 down   postgres /var/lib/postgresql/11/main /var/log/postgresql/postgresql-11-main.log
12  main    5432 online postgres /var/lib/postgresql/12/main /var/log/postgresql/postgresql-12-main.log
13  main    5434 online postgres /var/lib/postgresql/13/main /var/log/postgresql/postgresql-13-main.log
</code></pre>
<blockquote>
<p>This post does not use Postgres 11 beyond this example.</p>
</blockquote>
<p>When working with multiple versions installed it is helpful to verify
versions match what you expect them to be.
First, the port 5432 for Postgres 12 version.</p>
<pre><code>psql -d pgosm -p 5432 -c "select version();"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚                                   version                                    â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ PostgreSQL 12.4 (Ubuntu 12.4-1.pgdg18.04+1) on x86_64-pc-linux-gnu, compiledâ€¦â”‚
â”‚â€¦ by gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0, 64-bit                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p>Now, port 5434 for Postgres 13 version.</p>
<pre><code>psql -d pgosm -p 5434 -c "select version();"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚                                   version                                    â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ PostgreSQL 13beta3 (Ubuntu 13~beta3-1.pgdg18.04+1) on x86_64-pc-linux-gnu, câ€¦â”‚
â”‚â€¦ompiled by gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0, 64-bit                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h2>Create Indexes</h2>
<p>Both versions of Postgres were loaded with the same Colorado OpenStreetMap
data loaded <a href="https://blog.rustprooflabs.com/2020/01/postgis-osm-load-2020">using osm2pgsql</a>.
The osm2pgsql does not create any B-Tree indexes on its own, only the GIST
indexes on geometries.
For this post, we examine B-Tree index sizes created on four columns:
<code>osm_id</code>, <code>highway</code>, <code>waterway</code>, and <code>natural</code>.
Looking at the stats on the <code>public.planet_osm_line</code> table I can make 
a couple guesses where we will and will not see gains based on <code>n_distinct</code>.
I can guess we will not see major gains in the
<code>osm_id</code>, there is only a small amount of duplication in those values.
The other three columns (<code>highway</code>, <code>natural</code> and <code>waterway</code>) have a small number of distinct values
and varying amounts of <code>NULL</code> values.  These three columns would all
be candidates for partial indexes to avoid indexing the <code>NULL</code> values,
thus reducing the size of the created index.
I have hopes to see the benefits in Postgres 13 on these columns, possibly
making the use of partial indexes less frequent.</p>
<pre><code> SELECT attname, n_distinct, null_frac
    FROM pg_catalog.pg_stats
    WHERE tablename = 'planet_osm_line'
        AND attname IN ('osm_id', 'highway', 'waterway', 'natural')
;

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ attname  â”‚ n_distinct â”‚ null_frac  â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ osm_id   â”‚  -0.833553 â”‚          0 â”‚
â”‚ highway  â”‚         26 â”‚ 0.41616666 â”‚
â”‚ natural  â”‚          4 â”‚      0.994 â”‚
â”‚ waterway â”‚          9 â”‚     0.6663 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h3>Index on <code>osm_id</code></h3>
<p>First up is the <code>osm_id</code> column, a nearly unique set of positive and 
negative values.  The index to create in both versions:</p>
<pre><code>CREATE INDEX ix_osm_line_osm_id ON public.planet_osm_line (osm_id);
</code></pre>
<p>The following query is used throughout to report out index sizes.
The query itself will not be repeated, as only the filter would change.</p>
<pre><code>SELECT ai.schemaname AS s_name, ai.relname AS t_name,
        ai.indexrelname AS index_name,
        pg_size_pretty(pg_relation_size(quote_ident(ai.schemaname)::text || '.' || quote_ident(ai.indexrelname)::text)) AS index_size,
        pg_relation_size(quote_ident(ai.schemaname)::text || '.' || quote_ident(ai.indexrelname)::text) AS index_size_bytes
    FROM pg_catalog.pg_stat_all_indexes ai
    WHERE ai.indexrelname LIKE 'ix_osm_line%'
    ORDER BY index_name
;
</code></pre>
<p>Due to the low number of duplicates, it is no surprise that the
show only a tiny reduction in the size.  The reduction
would not be detectable from only the <code>index_size</code> column (in MB), the
<code>index_size_bytes</code> shows the slight reduction in size (29,515,776 bytes vs. 29,384,704 bytes).</p>
<p>Pg 12.</p>
<pre><code>â”Œâ”€[ RECORD 1 ]â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ s_name           â”‚ public             â”‚
â”‚ t_name           â”‚ planet_osm_line    â”‚
â”‚ index_name       â”‚ ix_osm_line_osm_id â”‚
â”‚ index_size       â”‚ 28 MB              â”‚
â”‚ index_size_bytes â”‚ 29515776           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p>Pg 13.</p>
<pre><code>â”Œâ”€[ RECORD 1 ]â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ s_name           â”‚ public             â”‚
â”‚ t_name           â”‚ planet_osm_line    â”‚
â”‚ index_name       â”‚ ix_osm_line_osm_id â”‚
â”‚ index_size       â”‚ 28 MB              â”‚
â”‚ index_size_bytes â”‚ 29384704           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h3>Index on <code>highway</code></h3>
<p>The <code>highway</code> data in the <code>planet_osm_line</code> data is a good example of when
a
<a href="https://www.postgresql.org/docs/current/indexes-partial.html">partial index</a>
might typically be a good idea to minimize index size.  My hunch (and hope)
is that the de-duplication will make a partial index here a moot point by reducing the size required to index a large number of <code>NULL</code> values.</p>
<p>Create two indexes, one partial covering the non-<code>NULL</code> values and one full
index on the entire table.</p>
<pre><code>CREATE INDEX ix_osm_line_highway_partial 
    ON public.planet_osm_line (highway)
    WHERE highway IS NOT NULL;
CREATE INDEX ix_osm_line_highway_full
    ON public.planet_osm_line (highway);
</code></pre>
<p>The two indexes in Postgres 12, notice the partial index cuts out about 1/3 of the size from the full index.</p>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”…</code></pre></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.rustprooflabs.com/2020/09/postgres-beta3-btree-dedup">https://blog.rustprooflabs.com/2020/09/postgres-beta3-btree-dedup</a></em></p>]]>
            </description>
            <link>https://blog.rustprooflabs.com/2020/09/postgres-beta3-btree-dedup</link>
            <guid isPermaLink="false">hacker-news-small-sites-24395825</guid>
            <pubDate>Mon, 07 Sep 2020 00:39:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Introduction to Computational Thinking]]>
            </title>
            <description>
<![CDATA[
Score 328 | Comments 37 (<a href="https://news.ycombinator.com/item?id=24395700">thread link</a>) | @guiambros
<br/>
September 6, 2020 | https://mitmath.github.io/18S191/Fall20/ | <a href="https://web.archive.org/web/*/https://mitmath.github.io/18S191/Fall20/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<!-- Content appended here -->
<div>
<p>Welcome to the new course <strong>MIT 18.S191</strong>, the debut edition, <strong>Fall 2020</strong>!</p>
<p> This is an introductory course on Computational Thinking. We use the <a href="http://www.julialang.org/">Julia programming language</a> to approach real-world problems in varied areas applying data analysis and computational and mathematical modeling.  In this class you will learn computer science, software, algorithms, applications, and mathematics as an integrated whole.</p>
<p>Topics include:</p>
<ul>
<li><p>Image analysis</p>
</li>
<li><p>Particle dynamics and ray tracing</p>
</li>
<li><p>Epidemic propagation</p>
</li>
<li><p>Climate modeling</p>
</li>
</ul>
<h2 id="professors"><a href="#professors">Professors</a></h2>
<p><a href="http://math.mit.edu/~edelman">Alan Edelman</a>, <a href="http://sistemas.fciencias.unam.mx/~dsanders/">David P. Sanders</a>, <a href="https://www.3blue1brown.com/about">Grant Sanderson</a>, &amp; <a href="https://eapsweb.mit.edu/people/jars">James Schloss</a></p>
<h2 id="introduction_video"><a href="#introduction_video">Introduction video</a></h2>
<iframe id="course-intro" width="800" height="474" src="https://www.youtube.com/embed/vxjRWtWoD_w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h2 id="logistics"><a href="#logistics">Logistics</a></h2>
<p>Course materials will be published on the accompanying website: <a href="https://mitmath.github.io/18S191/Fall20/">https://mitmath.github.io/18S191/Fall20/</a></p>
<p>TR 2:30–3:30pm EST, online (Go to the lecture page on this site to stream it.)</p>
<ul>
<li><p>Tuesdays: Prerecorded videos, released on YouTube and played live on this site.</p>
</li>
<li><p>Thursdays: Live sessions (same YouTube link 2:30–3) and MIT-only discussion (3-3:30); link to follow</p>
</li>
</ul>
<p>Start date: September 1, 2020</p>
<p>Office hours TBD.</p>
<h3 id="discussion_forum_and_homework_submission"><a href="#discussion_forum_and_homework_submission">Discussion forum and homework submission</a></h3>
<ul>
<li><p><a href="https://discord.gg/Z5qnVf8">Discord</a>: discussion (we encourage you to hang out here during class!)</p>
</li>
<li><p><a href="https://piazza.com/class/kd33x1xnfyq3b1">Piazza</a>: (MIT only) allows for anonymity to other students, discussion</p>
</li>
<li><p><a href="https://canvas.mit.edu/courses/5637">Canvas</a>: (MIT only) homework submission. If you're a non-MIT student, please find a partner to cross-grade homeworks via Discord.</p>
</li>
</ul>
<h3 id="evaluation"><a href="#evaluation">Evaluation</a></h3>
<ul>
<li><p>12 weekly problem sets with equal weight; your lowest score will be dropped. </p>
</li>
<li><p>Released on Thursdays and due before the following Thursday's class. (No problem set during Thanskgiving week.)</p>
</li>
<li><p>No exams</p>
</li>
</ul>
<p>Problem sets consist of code. MIT students enrolled in the course must submit homeworks via Canvas. If you are not a student then we encourage you to join the Discord and find a cross-grading partner.</p>
<div>
  <p>
    Last modified: September 10, 2020. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </p>
</div>
</div><!-- CONTENT ENDS HERE -->

    </div></div>]]>
            </description>
            <link>https://mitmath.github.io/18S191/Fall20/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24395700</guid>
            <pubDate>Mon, 07 Sep 2020 00:08:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Elixir Is Erlang, not Ruby]]>
            </title>
            <description>
<![CDATA[
Score 409 | Comments 289 (<a href="https://news.ycombinator.com/item?id=24395695">thread link</a>) | @stanislavb
<br/>
September 6, 2020 | https://preslav.me/2020/09/06/elixir-is-not-ruby-elixir-is-erlang/ | <a href="https://web.archive.org/web/*/https://preslav.me/2020/09/06/elixir-is-not-ruby-elixir-is-erlang/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
              
              <p>Try to remember the first time you heard about this fascinating language called <a href="https://elixir-lang.org/">Elixir</a>. Chances are, you had by the time been developing software using Ruby. If that's the case, Elixir seems to have appeared out of nowhere until suddenly, it became the solution for all your previous problems. It is fast, clean, scales extremely well. It was <strong>almost</strong> like the Ruby you've always wanted to have, but never got.</p><p>I say <strong>almost</strong>, because as much as you want it to be, <strong>Elixir is not Ruby</strong>. The familiar syntax has definitely helped the language win the hearts of the broader developer community. Yet, under the hood, <strong>Elixir is all about Erlang.</strong> The <a href="https://elixir-lang.org/">Erlang</a> that everyone tells stories about, as if it were some mythical creature, but no one dares to touch.</p><figure><img src="https://preslav.me/content/images/2020/09/book-cover.png" alt="" width="500" height="700"><figcaption>Elixir for Non-Ruby Programmers</figcaption></figure><p>Why am I saying all of this? First, because I would love to see greater adoption of Elixir outside Ruby/Rails community. It was among that group of people where the idea first sparked, and I fully respect the fact. Nevertheless, I would love to see it growing out. As someone who discovered Elixir after years of doing Java, &nbsp;.NET and more recently, Go, I can say that there is enough goodness in it for everyone. Moreover, bringing people with different backgrounds (both technical and non-technical) will lead to an explosion of new ideas. Having more diverse minds will help us better understand and make use of the elephant in the room-Erlang.</p><p>This brings me to my second point. Erlang is not obscure at all, once you give it enough attention. It is actually oddly satisfying, once you start reading code written in it. I have had my few a-ha moments, when I figured the original inspiration for most constructs in Elixir. The point is, we shouldn't fear Erlang, but try to understand it. This applies both to when things go well, as well as when they go south.</p><p>My biggest hope of all is that by understanding Erlang well enough, the Elixir community will start looking for new uses for it. Uses that go beyond being a scalable replacement for Rails apps. More daring and more ambitious uses, where Erlang's resilience model can prove to be critical for the success of the mission. I'd love to see it used in transportation, space, as well as the domain that started it all- telecommunications.</p><blockquote>“Shoot for the moon. Even if you miss it you will land among the stars.”<br><strong>Les Brown</strong></blockquote>
                <section>
                  
                  <ul>
                      <li>
                        <a href="https://preslav.me/tag/elixir/" title="Elixir">Elixir</a>
                      </li>
                      <li>
                        <a href="https://preslav.me/tag/programming/" title="Programming">Programming</a>
                      </li>
                      <li>
                        <a href="https://preslav.me/tag/ruby/" title="Ruby">Ruby</a>
                      </li>
                      <li>
                        <a href="https://preslav.me/tag/erlang/" title="Erlang">Erlang</a>
                      </li>
                      <li>
                        <a href="https://preslav.me/tag/2-cents/" title="2 Cents">2 Cents</a>
                      </li>
                  </ul>
                </section>
            </div></div>]]>
            </description>
            <link>https://preslav.me/2020/09/06/elixir-is-not-ruby-elixir-is-erlang/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24395695</guid>
            <pubDate>Mon, 07 Sep 2020 00:07:39 GMT</pubDate>
        </item>
    </channel>
</rss>
