<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 10]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 10. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Sat, 03 Oct 2020 08:26:13 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-10.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Sat, 03 Oct 2020 08:26:13 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Why are secrets in Git such a threat?]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24649034">thread link</a>) | @oodelally
<br/>
October 1, 2020 | https://www.gitguardian.com/secrets-detection/secret-sprawl | <a href="https://web.archive.org/web/*/https://www.gitguardian.com/secrets-detection/secret-sprawl">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>What are secrets in the software development world?</p><div><p>In the common language, a secret can be any sensitive data that we want to keep private. When discussing secrets in the context of software development, secrets generally refer to digital authentication credentials that grant access to systems or data. These are most commonly API keys, usernames and passwords, or security certificates.</p><p>Secrets exist in the context of applications that are no longer standalone monoliths. Applications nowadays rely on thousands of independent building blocks: cloud infrastructure, databases, SaaS components such as Stripe, Slack, HubSpot…&nbsp;</p><p>Secrets are what tie together these different building blocks of a single application by creating a secure connection between each component.</p></div><p>What is secret sprawl?</p><div><p>Secret sprawl is the unwanted distribution of secrets like API keys and credentials through multiple systems.&nbsp;</p><p>In modern software development, secrets are regularly used by developers and applications. As a result they often get shared through different services like Slack or email and can be stored in multiple locations including different machines, git repositories or inside company wikis. This is a phenomenon we call secret sprawl.</p></div><p>What are some of the best practices to securely manage secrets like API keys?</p><div><p>Storing and managing secrets like API keys and other credentials can be challenging. Even the most careful policies can sometimes be circumvented in exchange for convenience. We have put together a helpful <a href="https://blog.gitguardian.com/secrets-api-management/" target="_blank">cheat sheet</a> and article explaining the <a href="https://blog.gitguardian.com/secrets-api-management/" target="_blank">best practices for managing secrets</a>.</p><p>As a minimum here are some points you should consider:</p></div><div><p>Never store unencrypted secrets in git repositories</p></div><div><p>Avoid git add * commands on git</p></div><div><p>Add sensitive files in .gitignore</p></div><div><p>Don’t rely on code reviews to discover secrets</p></div><div><p>Use automated secrets scanning on repositories</p></div><div><p>Don’t share your secrets unencrypted in messaging systems like Slack</p></div><div><p>Store secrets safely (method to be used depends on every use case)</p></div><div><p>Default to minimal permission scope for APIs</p></div><div><p>Whitelist IP addresses where appropriate</p></div><p>What are the threats associated with secret sprawl?</p><div><p>When secrets are sprawled through multiple systems it increases what is referred to as the ‘attack surface’. This is the amount of points where an unauthorized user could gain access to your systems or data. In the case of secret sprawl, each time a secret enters another system it is another point where an attacker could gain access to your secrets.</p><p>Most internal systems are not an appropriate place to store sensitive information, even if those systems are private. No company wants credit card numbers in plaintext in databases, PII in application logs, bank account credentials in a Google Doc. Secrets benefit from the same kind of protective measures.</p><p>As a general security principle, where feasible, data should remain safe even if it leaves the devices, systems, infrastructure or networks that are under organizations’ control, or if they are compromised. This helps prevent credential stealing, which is a well-known adversary technique described in the MITRE ATT&amp;CK framework:</p></div><p>“ Adversaries may search local file systems and remote file shares for files containing passwords. These can be files created by users to store their own credentials, shared credential stores for a group of individuals, configuration files containing passwords for a system or service, or source code/ binary files containing embedded passwords. ”<br></p><p>Secrets accessed by malicious threat actors can lead to information leakage and allow lateral movement or privilege escalation, as secrets very often lead to other secrets. Furthermore, once an attacker has the credentials to operate like a valid user, it is extremely difficult to detect the abuse and the threat can become persistent.<br></p><p>What are some of the biggest challenges associated with secret sprawl?</p><div><p>Because secrets tie together each component of an application, developers and ops need access to these secrets to build, connect, test and deploy applications. The result is that secrets need to be both tightly wrapped and also widely distributed. </p><p>Enforcing good security practices at the organization level is hard. Developers today can have large turnovers inside companies, be spread through many different teams and geographies. They have a growing number of technologies they must master and are under hard pressure due to shortened release cycles. This makes secret management very complicated and an ever changing challenge. </p><p>This is further complicated when VCS like git are introduced because secrets can be buried deep inside the history. The actual version of source code might look clean, while the history might present credentials that were added than later removed. Any secret reaching the Version Control System must be considered compromised and the presence of valid secrets in the git history presents a threat!</p></div></div></div>]]>
            </description>
            <link>https://www.gitguardian.com/secrets-detection/secret-sprawl</link>
            <guid isPermaLink="false">hacker-news-small-sites-24649034</guid>
            <pubDate>Thu, 01 Oct 2020 09:27:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why is it hard to detect API keys in source code?]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24649026">thread link</a>) | @oodelally
<br/>
October 1, 2020 | https://www.gitguardian.com/secrets-detection/how-to-detect-secrets-in-source-code | <a href="https://web.archive.org/web/*/https://www.gitguardian.com/secrets-detection/how-to-detect-secrets-in-source-code">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Why is it hard to detect secrets like API keys and other credentials?</p><div><p>Secrets detection is probabilistic—that is to say that it is not always possible to determine what is a true secret (or true positive). Because secrets share very few common, distinctive factors, decisions must be taken by aggregating weak signals to make strong predictions. </p><p>One of the most common factors is that almost all secrets are strings that look random. We call these strings high entropy strings. The issue is that this common factor is not very distinctive: 99% of strings that look random in source code aren’t secrets. They are for example database IDs or other types of false positives. </p><p>Of course, some secrets have fixed patterns: AWS keys often start with AKIA for example. But most secrets do not. The portions of code surrounding them are also very different, depending on what the secrets are used for and how they are used by the developers in the context of specific applications. Usernames and passwords can be used to authenticate in many different ways, and it is really hard to distinguish between real and fake credentials used as placeholders for example. </p><p>All this makes it extremely challenging to accurately capture all true secrets without also capturing false positives. At some point, a line in the sand needs to be drawn that considers the cost of a secret going undetected (a false negative) and compares it to the outcome that too many false positives would create. Different organizations with different people, cultures and workflows will draw different lines!</p></div><p>Why do code reviews fail at finding secrets in source code?</p><p>Code reviews are great overall for detecting logic flaws or maintaining certain good coding practices. But they are not adequate protection for detecting secrets, mostly for two reasons:<br></p><div><p>Reviews generally only consider the net difference between the current and proposed states. Not the entire history of changes. If a commit adds a secret and another one later deletes it, this has a zero net effect that is not of any interest to reviewers. But the vulnerability is there.</p></div><div><p>Reviewers prefer to focus on errors that cannot be automatically detected, like design flaws. As a general principle, security automation should be implemented wherever it can be, so that humans focus on where they bring the most value.</p></div><p>What is a "good" secrets detection algorithm?</p><div><p>Detecting secrets in source code is like finding needles in a haystack: there are a lot more sticks than there are needles, and you don’t know how many needles might be in the haystack. In the case of secrets detection, you don’t even know what all the needles look like!</p><p>Ideally, you want your detection system to achieve at the same time:</p></div><div><p>A low number of false alerts raised. We call this high precision. Precision answers the question: «What is the percentage of the secrets that you detect that are actual secrets?». This question is perfectly legitimate, especially in the context of security teams being overwhelmed with too many alerts.</p></div><div><p>A low number of secrets missed. This is what we call high recall. Considering that a single undetected credential can have a big impact for an organization, some organizations prefer to triage more false alerts but make sure they don’t miss a secret.</p></div><p>Balancing the equation to ensure that the algorithm captures as many secrets as possible without flagging too many false results is an intricate and extremely difficult challenge. Read more on evaluating <a href="https://blog.gitguardian.com/secrets-detection-accuracy-precision-recall-explained/" target="_blank">secrets detection algorithms</a>.<br></p><p>What is a false positive in secrets detection?</p><div><p>A false positive in secrets detection refers to when a secret candidate is wrongly marked as a true secret when it is in fact a non-sensitive string. </p><p>Typical examples of strings that can be mistaken for true secrets are:</p></div><div><p>UUIDs (Universally Unique Identifiers) that are used for example by databases as unique keys.</p></div><div><p>High entropy URLs or file paths. They often contain random strings that look like keys.</p></div><p>Examples of server-side hooks:<br></p><div><p>Test keys. Service Providers sometimes provide developers with a set of test credentials with a very restricted scope so developers can exercise parts of the API without charging their account</p></div><div><p>Public keys. As surprising as it is, a very small number of keys are really meant to be public (like Firebase keys, see this Stack Overflow <a href="https://stackoverflow.com/questions/37482366/is-it-safe-to-expose-firebase-apikey-to-the-public" target="_blank">conversation</a>).</p></div><p>Are secrets detection algorithms language-dependent?</p><p>Secrets detection is, for the most part, not language specific. Of course, there are some subtleties to take into account, like the way variables are assigned in any programming language. But there is no need to support all the different syntaxes in their greatest details. This means that the same algorithms can be applied to any project, in any programming language, without using things like Abstract Syntax Trees.<br></p></div></div>]]>
            </description>
            <link>https://www.gitguardian.com/secrets-detection/how-to-detect-secrets-in-source-code</link>
            <guid isPermaLink="false">hacker-news-small-sites-24649026</guid>
            <pubDate>Thu, 01 Oct 2020 09:26:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why automate secrets scanning throughout your SDLC?]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24649022">thread link</a>) | @oodelally
<br/>
October 1, 2020 | https://www.gitguardian.com/secrets-detection/secrets-detection-application-security | <a href="https://web.archive.org/web/*/https://www.gitguardian.com/secrets-detection/secrets-detection-application-security">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>My source code is private, so why is hardcoding credentials in git considered a bad practice?</p><div><p>While private repositories offer some level of protection for your code they still do not have adequate protection to store information as sensitive as secrets. </p><p>Imagine if there was a plain text file with all your credit card numbers within it, you hopefully wouldn’t put this into the companies git repository. Secrets are just as sensitive.</p><p>A few things to consider when storing secrets in private repositories:</p></div><div><p>Source code is made to be duplicated and distributed, therefore lives in multiple places. Source code is a leaky asset and you never know where it is going to end up. It can be cloned to a compromised workstation or server, intentionally or accidentally published in whole or in part, uploaded to your website, released to a customer, pasted in Slack, or end up in your package manager or mobile application...</p></div><div><p>It would just take one compromised developer account to compromise all the secrets they have access to.</p></div><div><p>Hardcoded credentials make it impossible to know what secrets a developer accessed, and very difficult to rotate keys.</p></div><p>Why automate secrets scanning throughout the Software Development Life Cycle (SLDC)?</p><p>While DevOps, Continuous Integration and Continuous Delivery speed up software development, they can also significantly increase security risks.<br></p><p>“DevOps is rapid and requires lots of small, iterative changes. But this increases complexity and opens up a new set of security problems. With DevOps, existing security vulnerabilities can be magnified and manifest themselves in new ways. The speed of software creation can mean new vulnerabilities are created unseen by developers. The solution is to build security monitoring into the DevOps process from the start.”<br></p><p>Greg Day, RSA Conference Organizer (<a href="https://www.securityroundtable.org/the-biggest-cybersecurity-risks-in-2020/" target="_blank">source</a>)<br></p><p>Security now needs to be part of the SDLC from the start, and part of every incremental change. This concept is called Shifting Left, a development principle which states that security should move from the right (or end) of the SDLC to the left (the beginning). A great deal of automation is needed in order to be able to cope with running security checks at each incremental change. &nbsp;Automation helps streamline the detection, alerting and remediation processes and workflows.<br></p><p>How does secrets detection compare with Static Application Security Testing (SAST)?</p><div><p>Secrets detection is often confused with SAST because both scan through static source code. </p><p>SAST is mostly testing control structure, input validation, error handling, etc. Such vulnerabilities like SQL injection vulnerabilities only express themselves the moment the code is deployed. Exposed secrets are unlike these vulnerabilities, because any secret reaching version control system must be considered compromised and requires immediate attention. This is true even if the code is never deployed. </p><p>Implementing secrets detection is not only about scanning the most actual version of your master branch before deployment. It is also about scanning through every single commit of your git history, covering every branch, even development or test ones.</p><p>To conclude, SAST is concerned only with the current version of a project, the version that is going to be deployed, whereas secrets detection is concerned with the entire history of the project.</p></div><p>What are git hooks?</p><div><p>Git hooks are scripts that are triggered by certain actions in the software development process, like committing or pushing. By automatically pointing out issues in code, they allow reviewers not to waste time on mistakes that can be easily diagnosed by a machine. </p><p>There are client-side hooks, that execute locally on the developers’ workstation, and server-side hooks, that execute on the centralized version control system.</p><p>Examples of client-side hooks:</p></div><p>Examples of server-side hooks:<br></p><p>What is a pre-commit hook?</p><div><p>"The pre-commit hook is run first when committing, before you even type in a commit message. It’s used to inspect the snapshot that’s about to be committed, to see if you’ve forgotten something, to make sure tests run, or to examine whatever you need to inspect in the code. </p><p>Exiting non-zero from this hook aborts the commit, although you can bypass it with <span>git commit --no-verify</span>. You can do things like check for code style (run lint or something equivalent), check for trailing whitespace, or check for appropriate documentation on new methods."</p></div><p>Source: <a href="https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks" target="_blank">git-scm</a><br></p><p>What is a pre-receive hook?</p><div><p>A pre-receive hook is a script that runs on the server. It performs checks on the content of the push; if it exits non-zero, the push is rejected. You can use this hook to do things like prevent a PR author from merging their own changes, or require commit messages to follow some specific guidelines. </p><p>Be careful when using pre-receive hooks as they are blocking: if the checks don’t pass, the server is not updated.</p></div><p>What is a post-receive hook?</p><p>"The post-receive hook runs after the entire process of pushing code to the server is completed and can be used to update other services or notify users. Examples include emailing a list, notifying a continuous integration server, or updating a ticket-tracking system – you can even parse the commit messages to see if any tickets need to be opened, modified, or closed. This script can’t stop the push process, but the client doesn’t disconnect until it has completed, so be careful if you try to do anything that may take a long time."<br></p><p>Source: <a href="https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks" target="_blank">git-scm</a><br></p><p>Unlike the pre-receive hook, post-receive is non-blocking.<br></p><p>Where in the DevOps pipeline to implement automated secrets scanning? Client-side or server-side?</p><div><p>The earlier a security vulnerability is uncovered, the less costly it is to correct. Hardcoded secrets are no exceptions. If the secret is uncovered after the secret reaches centralized version control server-side, it must be considered compromised, which requires rotating (revoking and redistributing) the exposed credential. This operation can be complex and typically involves multiple stakeholders.</p><p>Client-side secrets detection early in the software development process is a nice-to-have as it will prevent secrets entering the VCS earlier. </p><p>Server-side secrets detection is a must have:</p></div><div><p>&nbsp;Server-side is where the ultimate threat lies. From the server, the code can uncontrollably spread in a lot of different places, with the hardcoded secrets in it.</p></div><div><p>Implementing client-side hooks on an organization level is hard. This is something we have heard many application security professionals claim they are not confident to do, due to the difficulty to deploy and update this on every developer’s workstation.</p></div><div><p>Client-side hooks can and must be easy to bypass (remember secret detection is probabilistic). Usage of client side hooks largely comes down to the individual developers’ responsibility. Hence the need to have visibility over the later stages.</p></div><p>Should secrets detection be blocking or non-blocking in the SDLC?</p><div><p>From our experience, when trying to impose rules that are too constraining, people will bend them, often in an effort to collaborate better and do their job. Security must not be a blocker. It should allow flexibility and enable information to flow, yet enable visibility and control. </p><p>On one hand, security measures will be bypassed, sometimes for the worst. But on the other hand, it is also good sometimes that the developer can take the responsibility to bypass them. </p><p>Because even the best algorithms can fail and need human judgement. Secrets detection is probabilistic: algorithms achieve a tradeoff between not raising false alerts and not missing keys.</p></div></div></div>]]>
            </description>
            <link>https://www.gitguardian.com/secrets-detection/secrets-detection-application-security</link>
            <guid isPermaLink="false">hacker-news-small-sites-24649022</guid>
            <pubDate>Thu, 01 Oct 2020 09:25:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bare-metal programming the tinyAVR 0 microcontrollers]]>
            </title>
            <description>
<![CDATA[
Score 30 | Comments 17 (<a href="https://news.ycombinator.com/item?id=24648397">thread link</a>) | @unwind
<br/>
October 1, 2020 | https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers | <a href="https://web.archive.org/web/*/https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><section><div><h4>All you need is a text editor, a makefile and a USB-serial cable.</h4></div></section><section><div><p><img src="https://www.omzlo.com/uploads/std_attiny-406-macro.jpg" alt=""></p>

<h2 id="introduction">Introduction</h2>

<p>Are you an 8-bit or a 32-bit programmer? </p>

<p>At OMZLO, we have been mainly focussing our development efforts on newer 32-bit Arm-Cortex chips (STM32 and SAMD), which typically offer more RAM, more speed, more peripherals, at a similar or lower price-point than older 8-bit MCUs. But 8-bit MCUs are far from dead. Microchip has notably released a new series of chips, collectively branded as the "tinyAVR 0-series", which offer more modern peripherals than the older AVR chips at a very competitive price point. They seem like the perfect candidate for simple products that don't need all the features and fine-tuning capabilities of the newer 32-bit MCUs. 8-bit MCUs are also substantially simpler to program, which translates into faster development time. </p>

<p>Thanks to the success of the Arduino UNO, there are tons of tutorials online that explain how to program 8-bit Atmega328 microcontrollers and their cousins like the Attiny85 using direct register access, without the Arduino language or any vendor IDE such as Atmel Studio. Just google "atmega328 blinky". All you need is an AVR C-compiler, a text editor, <a href="https://www.nongnu.org/avrdude/">avrdude</a>, and an AVR programmer. Some <a href="https://www.instructables.com/id/Getting-Started-With-the-ATMega328P/">resources</a> even show how to also build the electronics needed to get a basic atmega328 running on a breadboard. However, it's hard to find the same information for these newer "tinyAVR 0" chips.</p>

<p>Of course, Microchip offers all the tools necessary to program these newer "TinyAVR" MCUs with their windows-only IDE. There are also "Arduino cores" for some of these newer "TinyAVR" MCUs that let you program them with the Arduino IDE. But again, if you like to write code for MCUs in "baremetal" style, with your favorite text editor, a makefile, and a c-compiler, there are few resources available online. </p>

<p>In this blog post, we will describe how to program a <a href="https://www.ladyada.net/learn/proj1/blinky.html">blinky</a> firmware on an <strong>Attiny406</strong>, from the ground up, using the simplest tools. Most of the things described here can be easily transposed to other TinyAVR MCUs. Our approach is generally guided toward macOS or Linux users, but should also be applicable in an MS-Windows environment with a few minor changes.</p>

<h2 id="hardware">Hardware</h2>

<p>We decided to play with the <a href="https://www.microchip.com/wwwproducts/en/ATTINY406">Attiny406</a>, with a view of using it in the future to replace the Attiny45 we currently use on the <a href="https://www.omzlo.com/articles/the-piwatcher">PiWatcher</a>, our Raspberry-Pi watchdog. The Attiny406 has 4K of flash space, 256 bytes of RAM, and can run at 20Mhz without an external clock source.</p>

<p>One of the most important differences between the new TinyAVR MCUs and the older classic AVR MCU like the Attiny85 is that the newer chips use a different programming protocol called UPDI, which requires only 3 pins, as opposed to the 6-pin ISP on the classic AVRs.</p>

<p>A little research shows that programming TinyAVRs with UPDI can be achieved with a simple USB-to-serial cable and a resistor, thanks to a python tool called <a href="https://github.com/mraardvark/pyupdi">pyupdi</a>, which suggests the following connection diagram for firmware upload:</p>
<pre>                        Vcc                     Vcc
                        +-+                     +-+
                         |                       |
 +---------------------+ |                       | +--------------------+
 | Serial port         +-+                       +-+  AVR device        |
 |                     |      +----------+         |                    |
 |                  TX +------+   4k7    +---------+ UPDI               |
 |                     |      +----------+    |    |                    |
 |                     |                      |    |                    |
 |                  RX +----------------------+    |                    |
 |                     |                           |                    |
 |                     +--+                     +--+                    |
 +---------------------+  |                     |  +--------------------+
                         +-+                   +-+
                         GND                   GND
</pre>
<h3 id="shematic">shematic</h3>

<p>We created a minimalistic breakout board for the Attiny406. The board can be powered by 5V through USB or a lower 3.3V through dedicated VCC/GND pins. An LED and a button were also fitted on the board. For testing purposes, we decided to embed the 4.7K resistor needed for the UPDI programming directly in the hardware (i.e. resistor <em>R2</em>). 
This gives us the following schematic:</p>

<p><img src="https://www.omzlo.com/uploads/attiny406-schematic.png" alt="schematic"></p>

<h3 id="board">Board</h3>

<p>The resulting breakout board is tiny and fits conveniently on a small breadboard. The design files are <a href="https://aisler.net/p/SIEGFIYL">shared on aisler.net</a>.</p>

<p><img src="https://www.omzlo.com/uploads/std_attiny-406-breadboard.jpg" alt=""></p>

<p>Programming the Attiny406 on the board with a USB-serial cable is done by connecting the headers on the board edge:</p>

<p><img src="https://www.omzlo.com/uploads/attiny406-prog.png" alt=""></p>

<h2 id="software">Software</h2>

<h3 id="pyudpi">pyudpi</h3>

<p>We installed <strong>pyupdi</strong> following the instructions provided on <a href="https://github.com/mraardvark/pyupdi">their webpage</a>. </p>

<p>We connected our USB-Serial cable to the board with the 4 dedicated UPDI pins available on the board. Our USB-Serial converter shows up as the file <code>/dev/tty.usbserial-FTF5HUAV</code> on a MacOS system.</p>

<p>To test that the programmer recognizes the Attiny406, you can issue a command similar to the following, adapting the path for the USB-serial converter to your setup:</p>
<pre>pyupdi -d tiny406 -c /dev/tty.usbserial-FTF5HUAV -i
</pre>
<p>This should result in the following output if all goes well:</p>
<pre>Device info: {'family': 'tinyAVR', 'nvm': 'P:0', 'ocd': 'D:0', 'osc': '3', 'device_id': '1E9225', 'device_rev': '0.1'}
</pre>
<h3 id="the-c-compiler">The C compiler</h3>

<p>The typical <strong>avr-gcc</strong> available on macOS with <a href="https://brew.sh/">homebrew</a> did not seem to recognize the Attiny406 as a compiler target, so we went off to install the avr-gcc compiler provided by Microchip, which is available <a href="https://www.microchip.com/mplab/avr-support/avr-and-arm-toolchains-c-compilers">here</a>. Downloading the compiler requires you to create an account on the Microchip website, which is a bit annoying. </p>

<p><img src="https://www.omzlo.com/uploads/avr-toolchain.png" alt="AVR toolchain link"></p>

<p>Once downloaded, we extracted the provided archive in a dedicated directory. The <code>bin</code> directory in the archive should be added to the <code>PATH</code> variable to make your life easier. Assuming the downloaded compiler is stored in the directory <code>$HOME/Src/avr8-gnu-toolchain-darwin_x86_64</code>, the PATH can be altered by adding the following line to your <code>.bash_profile</code> file:</p>
<pre>export PATH=$PATH:$HOME/Src/avr8-gnu-toolchain-darwin_x86_64/bin/
</pre>
<p>Newer Attiny MCUs are not supported out of the box by the Microchip <strong>avc-gcc</strong> compiler. You need to download a dedicated <em>Attiny Device Pack</em> from <a href="http://packs.download.atmel.com/">their website</a>, as shown below:</p>

<p><img src="https://www.omzlo.com/uploads/microchip-pack-repository.png" alt="AVR toolchain link"></p>

<p>The resulting downloaded <em>Device Pack</em> is named <code>Atmel.ATtiny_DFP.1.6.326.atpack</code> (or similar depending on versioning). Though the extension is <code>.atpack</code>, the file is actually a zip archive. We changed the extension to <code>.zip</code> and extracted the package in the directory <code>$HOME/Src/Atmel.ATtiny_DFP.1.6.326</code> next to the compiler files. </p>

<h3 id="c-program">C program</h3>

<p>We created the following program that blinks the LED on pin PB5 of our Attiny board at a frequency of 1Hz.</p>
<pre><span>#include &lt;avr/io.h&gt;
#include &lt;util/delay.h&gt;
</span>
<span>int</span> <span>main</span><span>()</span> <span>{</span>
    <span>_PROTECTED_WRITE</span><span>(</span><span>CLKCTRL</span><span>.</span><span>MCLKCTRLB</span><span>,</span> <span>0</span><span>);</span> <span>// set to 20Mhz (assuming fuse 0x02 is set to 2)</span>

    <span>PORTB</span><span>.</span><span>DIRSET</span> <span>=</span> <span>(</span><span>1</span><span>&lt;&lt;</span><span>5</span><span>);</span>
    <span>for</span> <span>(;;)</span> <span>{</span>
        <span>PORTB</span><span>.</span><span>OUTSET</span> <span>=</span> <span>(</span><span>1</span><span>&lt;&lt;</span><span>5</span><span>);</span>
        <span>_delay_ms</span><span>(</span><span>500</span><span>);</span>
        <span>PORTB</span><span>.</span><span>OUTCLR</span> <span>=</span> <span>(</span><span>1</span><span>&lt;&lt;</span><span>5</span><span>);</span>
        <span>_delay_ms</span><span>(</span><span>500</span><span>);</span>
    <span>}</span>
<span>}</span>
</pre>
<p>The code looks very similar to what you would see on a classic AVR "blinky" program. One visible change is the use of structures to access various registers of the MCU: e.g instead of setting bits in <code>PORTB</code>, you access <code>PORTB.DIRSET</code>.</p>

<p>The other visible change is the clock setup code <code>_PROTECTED_WRITE(CLKCTRL.MCLKCTRLB, 0)</code>. Out of the box, at reset, the Attiny406 runs at 3.33Mhz, which corresponds to a base frequency of 20Mhz with a 6x clock divider applied. To enable the full 20Mhz speed, the register <code>CLKCTRL.MCLKCTRLB</code> is cleared. Because this register needs to be protected against accidental changes, the Attiny406 requires a specific programming sequence to modify it. Fortunately, this is natively offered by the macro <code>_PROTECTED_WRITE</code>. More details are available in the <a href="http://ww1.microchip.com/downloads/en/DeviceDoc/ATtiny406-DataSheet-DS40001976B.pdf">Attiny406 datasheet</a>.</p>

<p>In comparison with an STM32 or a SAMD21, the code is blissfully simple. </p>

<h3 id="makefile">Makefile</h3>

<p>We assume the following directory structure where:</p>

<ul>
<li><code>Src/Atmel.ATtiny_DFP.1.6.326/</code> is the location of the Microchip <em>Device Pack</em></li>
<li><code>Src/attiny406-test/</code> is the directory where the code above is stored in a file called <code>main.c</code></li>
</ul>

<p>Compiling the code can be done by issuing the following command within <code>attiny406-test/</code> directory,:</p>
<pre>avr-gcc -mmcu=attiny406 -B ../Atmel.ATtiny_DFP.1.6.326/gcc/dev/attiny406/ -O3 -I ../Atmel.ATtiny_DFP.1.6.326/include/ -DF_CPU=20000000L -o attiny406-test.elf main.c
</pre>
<p>An <code>-O</code> optimization flag is required to make the <code>_delay_ms()</code> function calls work successfully, as well as defining the variable F_CPU to reflect the expected chip clock speed. The rest of the parameters provide the location of the Attiny406 device-specific files we previously extracted from the <em>Device Pack</em>.</p>

<p>Uploading the firmware to the MCU requires a conversion to the intel HEX format and a call to the <strong>pyupdi</strong> tool. To address all these steps, we created a simple Makefile.</p>
<pre><span>OBJS</span><span>=</span>main.o
<span>ELF</span><span>=</span><span>$(</span>notdir <span>$(</span>CURDIR<span>))</span>.elf  
<span>HEX</span><span>=</span><span>$(</span>notdir <span>$(</span>CURDIR<span>))</span>.hex
<span>F_CPU</span><span>=</span>20000000L


<span>CFLAGS</span><span>=</span><span>-mmcu</span><span>=</span>attiny406 <span>-B</span> ../Atmel.ATtiny_DFP.1.6.326/gcc/dev/attiny406/ <span>-O3</span>
CFLAGS+<span>=</span><span>-I</span> ../Atmel.ATtiny_DFP.1.6.326/include/ <span>-DF_CPU</span><span>=</span><span>$(</span>F_CPU<span>)</span>
<span>LDFLAGS</span><span>=</span><span>-mmcu</span><span>=</span>attiny406 <span>-B</span> ../Atmel.ATtiny_DFP.1.6.326/gcc/dev/attiny406/
<span>CC</span><span>=</span>avr-gcc
<span>LD</span><span>=</span>avr-gcc

all:    <span>$(</span>HEX<span>)</span>  

<span>$(</span>ELF<span>)</span>: <span>$(</span>OBJS<span>)</span>
                <span>$(</span>LD<span>)</span> <span>$(</span>LDFLAGS<span>)</span> <span>-o</span> <span>$@</span> <span>$(</span>OBJS<span>)</span> <span>$(</span>LDLIBS<span>)</span>

<span>$(</span>HEX<span>)</span>: <span>$(</span>ELF<span>)</span>
                avr-objcopy <span>-O</span> ihex <span>-R</span> .eeprom <span>$&lt;</span> <span>$@</span>

flash:  <span>$(</span>HEX<span>)</span>
                pyupdi <span>-d</span> tiny406 <span>-c</span> /dev/tty.usbserial-FTF5HUAV <span>-f</span> attiny406-test.hex

read-fuses:
                pyupdi <span>-d</span> tiny406 <span>-c</span> /dev/tty.usbserial-FTF5HUAV <span>-fr</span>

clean:
                <span>rm</span> <span>-rf</span> <span>$(</span>OBJS<span>)</span> <span>$(</span>ELF<span>)</span> <span>$(</span>HEX<span>)</span>
</pre>
<p>To compile the code, we simply type <code>make</code>. Uploading is done with <code>make flash</code>. This Makefile can be further enhanced as needed.</p>

<h2 id="conclusion">Conclusion</h2>

<p>With the right tools, baremetal programming on the new TinyAVR MCUs is as simple as on its older AVR cousins. </p>

<p>If you have programming tips for the AVRTiny, please share them with us on <a href="https://twitter.com/OmzloElec">on Twitter</a> or in the comments below.</p>
</div></section><section><div><h4>Comments</h4><div><div><p>Hi, </p>

<p>It's great that …</p></div></div></div></section></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers">https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers</a></em></p>]]>
            </description>
            <link>https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers</link>
            <guid isPermaLink="false">hacker-news-small-sites-24648397</guid>
            <pubDate>Thu, 01 Oct 2020 07:37:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Worst Decision of My Career]]>
            </title>
            <description>
<![CDATA[
Score 19 | Comments 12 (<a href="https://news.ycombinator.com/item?id=24648256">thread link</a>) | @fribmendes
<br/>
October 1, 2020 | https://subvisual.com/blog/posts/the-worst-decision-of-my-career/ | <a href="https://web.archive.org/web/*/https://subvisual.com/blog/posts/the-worst-decision-of-my-career/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><article><section><div><div><div><div><div><div>
<p>This is a reflection on software development and complexity. Let's start with
some quotes to make me look smart:</p>
<blockquote>
<p>A complex system that works is invariably found to have evolved from a simple
system that worked. A complex system designed from scratch never works and
cannot be patched up to make it work. You have to start over with a working
simple system -- <em>Gall's law</em></p>
</blockquote>
<p>I only came across this quote recently, but I think that it summarizes what
I've learned these past seven years. Another idea that I've found in a few books
is that programming in isolation is problem-solving, but software engineering
is all about managing complexity. This distinction between programming and
software engineering makes sense to me, probably because of my mental models.
I'm sure we can all disagree on this, but that's not the topic of today.
Complexity is.</p>
<h2>Complexity</h2>
<p>Almost everyone I know in the software industry wants to build products that
solve real and challenging problems. Change the world! The last thing you want
is to build another CRUD application. They are fine, but it gets boring after
a while.</p>
<p>Most of us will get bored without novelty. That's why the software industry has
a recycling mechanism to keep things fresh and interesting for us: every once
in a while a new, or different, language/framework will rise and light the path
for a brighter future, overthrowing the existing standard, demanding that we
learn it to be on top of the game.</p>
<p>We change our tools, but we keep building the same things over and over. I've
been doing this long enough to see that we are just going in circles. Let's be
honest, most of us aren't building life-changing products that wouldn't have
been possible ten years ago, so let's not jump straight into another shiny
technology that just came out and is going to solve all of our problems.</p>
<h2>Solutions, solutions, solutions</h2>
<blockquote>
<p>For a while, the solution to every problem I encountered was a Rails app. --
<em>this one is mine</em></p>
</blockquote>
<p>Over the years, I've seen companies rewrite their products to change languages,
frameworks, or architectures because they were told that the change would make
their product better, run faster, and scale. By the way, you're only a true
Ruby developer after you've heard the question "but does it scale?" at least
one hundred times (kidding, but it'll happen).</p>
<p>Think about it, how many times were you sold a new programming language,
technology, or a concept like microservices, serverless, event sourcing, clean
architecture, micro frontends. There are many preachers in the software world.</p>
<p>At Subvisual, we started using Elixir a lot these past years, so I've learned
a bit about the history of Erlang. If you don't know, Erlang uses the Actor
Model, and the interesting part is that the designers of Erlang only learned
about the Actor Model after having designed Erlang. This is important because
the designers of Erlang didn't start with the intent of applying the Actor
Model; it came as a solution to fault-tolerant distributed programming.</p>
<blockquote>
<p>If all you have is a hammer, everything looks like a nail <em>Abraham Maslow</em></p>
</blockquote>
<p>The designers of Erlang picked the right tool for the job and most of us want
to do the same. Unfortunately, there's usually more than one "tool" that would
be "right," but to know which ones, you need to know what "job" you're solving.
All of us <strong>should</strong> know this, but I keep learning about teams that fail to do
it. There are so many companies, with a handful of experienced developers, that
don't know what they are building, don't have a clear business yet, but their
product is already built on complex architectures and technologies such as
microservices, Kubernetes or event-sourcing.</p>
<h2>Improving</h2>
<p>Every project we start from scratch is an opportunity to do it right. We'll
think to ourselves: <em>This time I won't fall into the same traps! I have learned
my lessons, I've studied the books, and I even met some of my gurus that wrote
them! This time I'll follow "industry standards"!</em></p>
<p><em>I am the "architect"; I will design the perfect system! Without me, none of
this will be possible. Those mindless programmers have no idea what they are
doing. I, and only I, am the true heir of Martin Fowler!</em></p>
<p>This was a bit dramatic, but I needed a break from all of that whining. I know
it's easy to fall into these traps. It's even easier when your company raised
money, and everyone is expecting you to deliver the absolute best product ever
because they are paying you for it! This is why your starting team is so
important; they have to withstand the pressure. They must know that to build
the grand vision, they have to go one step at a time.</p>
<h2>The decision</h2>
<p>I've made many bad decisions, and I've been fortunate enough to suffer the
consequences of those decisions. A lot of developers don't get to experience
consequences, so they never learn.</p>
<p>So what was the worst decision of my career? I don't know. But the title of
this blog post is inspired by something an old colleague said. At the time, we
were working for a product company that reached for event-driven architecture
and event-sourcing too soon. They knew little about their market, and the
choices the software team made were crippling their ability to change. Business
rules were almost set in stone. Migrating data was a pain and the source of
many bugs. And unfortunately, because the team didn't have experience with
event-sourcing, the event store, which kept the state of all services, was also
being used as an event-bus to communicate between services. Because of that, it
was possible to couple one service to the internal state of another, which
happened a lot. This almost invisible coupling made everything worse.</p>
<p>What did we do against such an unpredictable system? We took it apart: merging
services that were too coupled; defining clear boundaries between services;
moving some services away from the event-store into a traditional database;
making some communication channels synchronous; writing integration and
end-to-end tests.</p>
<p>When we were finished it was still an unnecessarily complex system, but it was
one that we could change with some confidence. After that, we defined
a long-term plan for the product's architecture and technology, but we didn't
implement it. We waited for the right moment when something was starting to
slow us down to make a small step in that direction. When the business goals
changed, we changed our long-term plan, and once again made small steps in that
direction when we felt the need for it.</p>
<p>Eventually, complexity found its way again into the codebase, but it was fine
because the codebase was evolving slowing, adding and removing complexity when
necessary.</p>
<h2>Making the right call</h2>
<p>Was that the worst decision of his career? I don't think so. The issue with him
going for event-sourcing and event-driven architecture was that it wasn't the
right moment, but my colleague didn't know that. He thought the goals for the
next years were well defined, but unfortunately, they never are.</p>
<p>Should you use microservices or event-sourcing? Maybe, it depends on the
context and the client. The decisions I make are the best that I can with the
information that I have. For instance, when I'm part of the team that's
starting a product, the technology we pick will depend on how we'll hire: if
you want to build an office in Portugal, we have to make sure we have
developers for that technology available. We have to think things through, and
some things you only learn from experience. This applies to the systems we
design as well. I've seen enough people design around what they believe the
product will become in two years to know that those designs always fail to
accommodate the changes that will come. So we design systems for small,
incremental changes. Do the smallest thing that will get us started and
doesn't compromise our ability to change once we know what the business needs.</p></div></div></div></div></div></div></section></article></div></div></div>]]>
            </description>
            <link>https://subvisual.com/blog/posts/the-worst-decision-of-my-career/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24648256</guid>
            <pubDate>Thu, 01 Oct 2020 07:18:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[You're Allowed to Write Slow Rust Code]]>
            </title>
            <description>
<![CDATA[
Score 26 | Comments 36 (<a href="https://news.ycombinator.com/item?id=24647910">thread link</a>) | @ingve
<br/>
September 30, 2020 | https://blog.jonstodle.com/youre-allowed-to-write-slow-rust-code/ | <a href="https://web.archive.org/web/*/https://blog.jonstodle.com/youre-allowed-to-write-slow-rust-code/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="blog-post">
    
    
    <article>
        <p>There's a thing that comes up from time to time in the Rust community: people wanting to optimize their code. Make the code run faster. Make it allocate less memory. These are worthy goals, but maybe not necessary for all projects. Just because your program is written in Rust, it doesn't have to be optimized to the moon and back to do it's job. In a lot of cases, you'll be fine using <code>.clone()</code>.</p>
<p>There's a fantastic quote from a discussion on the <a href="https://users.rust-lang.org/">Rust user forums</a> which I always think of when these discussions emerge:</p>
<blockquote>
<p>Just because Rust allows you to write super cool non-allocating zero-copy algorithms safely, doesn't mean every algorithm you write should be super cool, zero-copy and non-allocating.<br>
-- <a href="https://users.rust-lang.org/t/feeling-rust-is-so-difficult/29962/15">trentj on The Rust Programming Language Forum</a></p>
</blockquote>
<p>Of course there's a time and place for <em>super cool no-allocating zero-copy algorithms</em>, but very often it's not the time, nor the place. You can write fantastically effective code with Rust, but <strong>you don't have to</strong>! A lot of the time it's entirely fine to just write code that works with minimal effort. You don't have to spend 4 days trying to figure out how lifetimes work just to avoid calling <code>.clone()</code> a few times.</p>
<p>Don't spend time trying to make micro optimizations now. Take the easy route. You'll probably come back to that piece of code in a few months time, smile, and change that <code>.clone()</code> to something more efficient.</p>
<p>Happy coding!</p>
<p><small>NB: Watch this <a href="https://youtu.be/rAl-9HwD858">video by Jon Gjengset</a> if you want a good, pragmatic walk through of lifetimes in Rust.</small></p>

    </article>
</div></div>]]>
            </description>
            <link>https://blog.jonstodle.com/youre-allowed-to-write-slow-rust-code/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24647910</guid>
            <pubDate>Thu, 01 Oct 2020 06:19:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Running GitHub Actions for Certain Commit Messages]]>
            </title>
            <description>
<![CDATA[
Score 20 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24647722">thread link</a>) | @kiyanwang
<br/>
September 30, 2020 | https://ryangjchandler.co.uk/articles/running-github-actions-for-certain-commit-messages | <a href="https://web.archive.org/web/*/https://ryangjchandler.co.uk/articles/running-github-actions-for-certain-commit-messages">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div>
            
                <section>
        
        <p>A quick look at how you can configure your GitHub Actions workflows to only run when a certain phrase is present in the commit message.</p>
                    <small>Published 3 days ago</small>
                            <span>|</span>
                <small>Updated 3 days ago</small>
                                                <p><span>
    Tooling
</span>
 
                                     <span>
    GitHub Actions
</span>
 
                            </p>
                        <article>
            <p>I'm going to be honest with you all for a second. I write a lot of <code>wip</code> commits. These commits are normally small changes that I want to push up to GitHub so that:</p>
<ol>
<li>I don't lose things if anything goes wrong and my backup hasn't picked it up.</li>
<li>If I can't describe the change I have just made.</li>
<li>If I'm demonstrating something to somebody on a pull-request.</li>
</ol>
<p>The problem is, my actions are setup to run on <code>push</code>, so every single <code>wip</code> commit gets run through the CI process, whether it be running tests, linting or formatting.</p>
<p>After doing some research, I found a way of preventing these from running on every single commit.</p>
<pre><code data-lang="yml"><span>jobs:</span>
  <span>format:</span>
    <span>runs-on:</span> <span>ubuntu-latest</span>
    <span>if:</span> <span>"! contains(github.event.head_commit.message, 'wip')"</span>
</code></pre>
<p>Now, whenever I push a <code>wip</code> commit or any commit that contains the word <code>wip</code>, it will be marked as skipped inside of GitHub actions.</p>
<p>You could also flip the logic and perhaps do something like:</p>
<pre><code data-lang="yml"><span>jobs:</span>
  <span>format:</span>
    <span>runs-on:</span> <span>ubuntu-latest</span>
    <span>if:</span> <span>"contains(github.event.head_commit.message, '[build]')"</span>
</code></pre>
<p>Any commit that contains <code>[build]</code> will now trigger these jobs, everything else will be skipped.</p>
<p>You can thank me later! 😉</p>

        </article>
    </section>
        </div>
    </div></div>]]>
            </description>
            <link>https://ryangjchandler.co.uk/articles/running-github-actions-for-certain-commit-messages</link>
            <guid isPermaLink="false">hacker-news-small-sites-24647722</guid>
            <pubDate>Thu, 01 Oct 2020 05:41:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ontario doctors sign letter to Premier advising against sweeping lockdowns]]>
            </title>
            <description>
<![CDATA[
Score 103 | Comments 148 (<a href="https://news.ycombinator.com/item?id=24645821">thread link</a>) | @mrfusion
<br/>
September 30, 2020 | https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html | <a href="https://web.archive.org/web/*/https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div>



<div>
    
        <p>Published Sept. 30, 2020 9:40 a.m. ET</p>
        <p>Updated  Sept. 30, 2020 7:26 p.m. ET</p>
    
</div>



  


    
        
        
    
    
    <amp-iframe resizable="" width="16" height="17" layout="responsive" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation" allowfullscreen="" frameborder="0" src="https://ampvideo.ctvnews.ca/content/ctvnews/en/local/ottawa/2020/9/30/1_5126193.vidi.root-responsivegrid-vidicomponent.html?preventDefault=true">
        
        <p>Click to Expand</p>
    </amp-iframe>





    <p>OTTAWA -- 	Twenty-one Ontario doctors have signed a joint letter to Premier Doug Ford, urging him not to issue a new lockdown this fall because of rising COVID-19 case numbers.</p><p>	Daily numbers of new cases have risen dramatically in recent days, with Ontario recording <a href="https://toronto.ctvnews.ca/new-covid-19-cases-in-ontario-reach-highest-mark-ever-1.5122863" target="_blank">700 new cases of COVID-19 on Monday</a> – the highest number of new cases recorded in a single day.</p><p>	However, the 21 doctors who signed the letter to the premier say another provincewide lockdown—similar to what was in place in the spring—would not be helpful.</p><ul>	<li>		<a href="#Full letter"><i>Read the full letter below</i></a></li></ul><p>	"We are writing this letter in support of the governments’ plan to use a tactical localized approach, rather than sweeping new lockdown measures, to deal with the increasing COVID case numbers in Ontario," the letter says.&nbsp;</p><p>	"Lockdowns have been shown not to eliminate the virus. While they slow the spread of the virus, this only lasts as long as the lockdown lasts. This creates a situation where there is no way to end the lockdown, and society cannot move forward in vitally important ways including in the health sector, the economy and other critically important instrumental goods including education, recreation, and healthy human social interactions."</p><p>	Speaking on Newstalk 580 CFRA's "The Morning Rush with Bill Carroll" in Ottawa, CTV's infectious disease specialist Dr. Neil Rau—a signatory of the letter—pointed to two data points to consider when looking at the spread of COVID-19.</p><p>	"We're reacting to increased aggregate case numbers, but the percentage positive is not really as bad as it used to be," he said. "We're testing more people, so we're finding more cases […] We're driving our numbers up, it's worse than it was in the summer, but it's not what it was last winter and life has to go on."</p><ul>	<li>		<a href="https://www.iheartradio.ca/580-cfra/podcasts/the-morning-rush-dr-neil-rau-interview-why-we-don-t-want-another-lockdown-1.13612912?mode=Article" target="_blank"><strong>LISTEN NOW: "The Morning Rush": Dr. Neil Rau - Why we don't want another lockdown</strong></a></li></ul><p>	Monday's 700 cases in Ontario came from 41,111 total tests, for a positive percentage rate of 1.7 per cent. On April 24, <a href="https://toronto.ctvnews.ca/highest-number-of-new-covid-19-cases-in-a-single-day-reported-by-ontario-health-officials-1.4910216" target="_blank">the previous watermark of 640 new cases in a single day</a>, the result came from 12,295 tests, for a positive percentage rate of 5.2 per cent.</p><p>	Testing capacity was lower in the spring than it is now, and testing criteria has changed over time; however, Dr. Rau and the other signatories of the letter said the increasing case numbers are not leading to unmanageable levels of hospitalizations.</p><p>	"In Ontario and other parts of the world, such as the European Union, increasing case loads are not necessarily translating into unmanageable levels of hospitalizations and ICU admissions," the letter says.</p><p>	The letter also points to other health impacts linked to the lockdown.</p><p>	"Hard data now exist showing the significant negative health effects shutting down society has caused. Overdoses have risen 40% in some jurisdictions. Extensive morbidity has been experienced by those whose surgery has been cancelled, and the ramifications for cancer patients whose diagnostic testing was delayed has yet to be determined," the letter states.</p><p>	"Economic harms are health harms," Dr. Rau told CFRA. "It sounds horrible to say, but it's true. Health is wealth. We all know this."</p><h2>	<strong>LETTER POINTS TO DISAGREEMENT AMONG HEALTH PROFESSIONALS</strong></h2><p>	Other Ontario health professionals have been arguing for increased restrictions as cases rise.</p><p>	Last week, the Ontario Hospital Association released a letter signed by 38 health professionals which <a href="https://ottawa.ctvnews.ca/ontario-hospital-association-calls-for-return-to-restrictions-on-non-essential-businesses-1.5119832" target="_blank">called for immediate restrictions to be re-imposed on non-essential businesses</a>, such as gyms, dine-in restaurants and bars, nightclubs, and theatres. It also calls on restrictions on other places where people can gather, such as places of worship.</p><p>	The letter from the OHA said regions where the speed of transmission was underestimated are “now facing the consequence of increased hospitalization rates, including a rise in intensive care unit (ICU) admissions and more deaths.”</p><p>	Hospitalizations in Ontario have been increasing, but have not yet reached the same level that was seen in the spring.</p><p>	According to <a href="https://covid-19.ontario.ca/data" target="_blank">data from the Ontario government</a>, there were 128 people in hospital in Ontario with COVID-19 complications on Monday—the day 700 new cases were recorded—up from 65 a week before; however, on April 24—when 640 new cases were recorded—government data shows that there were 910 people in hospital. The peak for hospitalizations in Ontario came in May, when there were days when more than 1,000 people were hospitalized. That number steadily decreased from May through the summer before it began going up again in September.</p><p>	Speaking on CTV Morning Live Ottawa, infectious disease specialist Dr. Abdu Sharkawy suggested t<a href="https://ottawa.ctvnews.ca/video?clipId=2045684" target="_blank">emporary restrictions on gathering would help curb the spread of COVID-19</a>.&nbsp;</p><p>	"We don't want to see our hospitals overwhelmed," he said. "We're still waiting for flu season and a whole bunch of other respiratory viruses to hit us and our capacity to be challenged. We don't want that to happen. We need everybody to try and simplify their lives and minimize anything that's non-essential."</p><p>	Dr. Sharkawy said regions where cases are rapidly rising may need to impose new restrictions to get the spread of the virus under control.</p><p>	"I think it's abundantly clear that, particularly in hot spots like Ottawa, Toronto and Peel region, the situation is not well controlled," he said. "Sometimes you need blunt instruments, even if they're temporary in nature, to make sure that you curtail the spread of this virus because it gets away from us a lot more quickly than many of us can anticipate sometimes."</p><p>	Dr. Sharkawy suggested hot spots follow the lead of Quebec, which imposed <a href="https://montreal.ctvnews.ca/life-in-the-red-zone-here-s-what-you-can-and-can-t-do-1.5125093" target="_blank">harsh restrictions on three areas in the province</a>, including Montreal and Quebec City, banning private gatherings and closing bars and restaurant dining rooms.</p><p>	"I think we should do it now," Dr. Sharkawy said of Ontario. "I think we all need to adopt an attitude and an approach that recognizes that we have to do what's absolutely necessary to keep everybody safe."</p><h2>	<strong>FULL LETTER FROM ONTARIO DOCTORS TO THE PREMIER</strong></h2><p>	Dear Premier Ford,</p><p>	We are writing this letter in support of the governments’ plan to use a tactical localized approach, rather than sweeping new lockdown measures, to deal with the increasing COVID case numbers in Ontario. Lockdowns have been shown not to eliminate the virus. While they slow the spread of the virus, this only lasts as long as the lockdown lasts. This creates a situation where there is no way to end the lockdown, and society cannot move forward in vitally important ways including in the health sector, the economy and other critically important instrumental goods including education, recreation, and healthy human social interactions.</p><p>	In Ontario the increase in cases at this time are in people under 60 years of age who are unlikely to become very ill. At the peak of the pandemic in Ontario in mid-April, 56% of cases were in ≥60 year olds, now in Sept only 14% of cases are in ≥60 year olds. In Ontario and other parts of the world, such as the European Union, increasing case loads are not necessarily translating into unmanageable levels of hospitalizations and ICU admissions. This is not a result of a lag in reporting of severe and fatal cases. While we understand the concerns that these cases could spill into vulnerable communities, we also need to balance the actual risk. As the virus circulates at manageable levels within the community, we need to continue the gains we have made in the protection of the vulnerable in long-term care and retirement institutions, and continue to educate other people about their individual risk, so that they can observe appropriate protective measures.</p><p>	Lockdowns have costs that have, to this point, not been included in the consideration of further measures. A full accounting of the implications on health and well-being must be included in the models, and be brought forward for public debate. Hard data now exist showing the significant negative health effects shutting down society has caused. Overdoses have risen 40% in some jurisdictions. Extensive morbidity has been experienced by those whose surgery has been cancelled, and the ramifications for cancer patients whose diagnostic testing was delayed has yet to be determined. A huge concern is the implication of closure of schools, and the ongoing reluctance we have seen in the large urban centers of sending children back to the classroom due to safety concerns. Global data clearly now show that children have an extremely low risk of serious illness, but they are disproportionately harmed by precautions. Children’s rights to societal care, mental health support and education must be protected. This cannot be achieved with ongoing or rotating lockdown.</p><p>	The invitation and involvement of other health experts to advise the government’s response beside individuals in Public Health and Infectious Diseases in addition to leaders in the business, securities and arts communities is essential. We also call for increased open debate, in the public forum, that hears voices from outside the medical and public health communities, in order to consider all points of view from society. This is a fundamental principle upon which democratic societies are built. All stakeholders should have an equal right to participation in public discourse when it comes to setting such fundamental and sweeping societal interventions.</p><p>	All have the right to feel their voices have been heard, and moreover to ensure factual credible data is openly debated, in contrast to the personal and political slants that have had apparent significant impacts on the management of the virus to date. Our society has borne enormous pain over the past 6 months. It’s time to do something different.</p><p>	Sincerely,</p><p>	Jane Batt MD, PhD, FRCPC. Respirologist, Associate Professor, Department of …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html">https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html</a></em></p>]]>
            </description>
            <link>https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24645821</guid>
            <pubDate>Thu, 01 Oct 2020 00:23:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A housing industry of endless one-offs is holding our society back]]>
            </title>
            <description>
<![CDATA[
Score 62 | Comments 133 (<a href="https://news.ycombinator.com/item?id=24645525">thread link</a>) | @jseliger
<br/>
September 30, 2020 | https://blokable.com/news/the-housing-market-is-building-snowflakes-an-endless-series-of-one-offs/ | <a href="https://web.archive.org/web/*/https://blokable.com/news/the-housing-market-is-building-snowflakes-an-endless-series-of-one-offs/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><h2><strong>By Aaron Holm and Nelson Del Rio,<br>Co-CEOs, Blokable Inc.</strong></h2><h2>The following post breaks down the high cost of building housing as part of a series exploring the root causes of the U.S. housing crisis and how private and public sector collaboration can chart a different course.&nbsp;</h2><figure><img width="1200" height="800" src="data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjA0OCAxMzY0IiB3aWR0aD0iMjA0OCIgaGVpZ2h0PSIxMzY0IiBkYXRhLXU9IiUyRndwLWNvbnRlbnQlMkZ1cGxvYWRzJTJGMjAyMCUyRjA5JTJGQmxvZy0yLWltYWdlLTIwNDh4MTM2NC5qcGciIGRhdGEtdz0iMjA0OCIgZGF0YS1oPSIxMzY0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==" data-spai="1" alt="" sizes="(max-width: 1200px) 100vw, 1200px"></figure><p>In mid-2018 we met with Washington State Governor Jay Inslee to discuss the insatiable need for affordable housing in the Pacific Northwest. We gave him a tour of Blokable’s prototype factory in Vancouver, Wash., and shared our R&amp;D work to reimagine how housing is built. Afterward, we drove up to Seattle in a rented Nissan Maxima, wondering aloud what it would cost to build our rental car from scratch in a driveway — exactly how nearly all housing is built today.&nbsp;&nbsp;&nbsp;</p><p>We would have to design, engineer, and fabricate all of the car’s systems and components, specify and source the materials, identify the regulatory requirements and work with different agencies to test our systems and sign off on safety and environmental requirements, create a working set of engineering drawings to track the build, order the equipment necessary to complete the assembly, and hire at least a small team of people to put the car together. A car that retails for just under $35,000 would cost tens of millions of dollars to build, and there would be no guarantees of quality and reliability.&nbsp;</p><p>Every time a new residential real estate project appears on the landscape, it’s a bespoke process that can directly involve hundreds of people, including a developer and a general contractor, various architects and engineers, and the investors who are funding the project. For each participant in the process, the singular goal is to extract a profit from that specific patch of dirt, and their profits are circumscribed by the value of the land, the applicable building regulations, and the prevailing labor and capital costs.&nbsp;</p><p>The result of this one-off approach is soaring project costs that make housing simply unaffordable to most people in many parts of the U.S.. Conventional housing development is like building a snowflake every time. And if the glut of empty luxury condo towers in downtown San Francisco is any indication, many of these snowflake projects are indeed melting as soon as they touch the ground.&nbsp;</p><p>How did this one-off approach to development take shape? Everything from ovens to stereos were all once clunky prototypes built as one-offs in the same location as their eventual use. Over time with innovation, standardization, and market demand, they evolved into products. In the early days of the automobile and aviation industries, prototypes and projects were built at high cost and yielded low quality results that resembled the current market products in their most basic functionality: they could drive and fly. Early automobile and airplane prototypes were unsafe and expensive. But through investment in product design, engineering, prototyping, and manufacturing, these prototypes eventually evolved into products. Increasingly stable designs and specifications enabled greater standardization, repeatability, and mass production resulting in lower costs, higher quality, and broader distribution.&nbsp;</p><p>Coupled with clear market demand, this standardization led to the formation of entire industries to build roads and airports, manufacture tires, test safety under federal guidelines, and so on. If you visit the automotive and aviation manufacturing hubs around Detroit and Seattle you’ll find vibrant supply chains competing for business and investing in innovation to provide better products and services. Continuously improving engines, seats, windshields, and bathrooms. All of this activity creates ever-improving products and fierce competition for performance, quality, and price.&nbsp;&nbsp;</p><p>If we compare product manufacturing in the automotive and aviation industries to how we build housing — whether it is an Accessory Dwelling Unit (ADU), single family home, multi-family apartment building, or a high rise — the steps in the process are largely the same. To simplify the housing development process, let’s assume the land is entitled, there are no environmental mitigation issues, financing is secured, and development will not be opposed by regulatory agencies or local city council. What does it take to build a snowflake?</p><figure><img width="1200" height="800" src="data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjA0OCAxMzY0IiB3aWR0aD0iMjA0OCIgaGVpZ2h0PSIxMzY0IiBkYXRhLXU9IiUyRndwLWNvbnRlbnQlMkZ1cGxvYWRzJTJGMjAyMCUyRjA5JTJGQmxvZy0yLWJvZHktaW1hZ2UtMjA0OHgxMzY0LmpwZyIgZGF0YS13PSIyMDQ4IiBkYXRhLWg9IjEzNjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+PC9zdmc+" data-spai="1" alt="Housing creation is an endless series of one-offs" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption><em>Conventional housing development is like building a snowflake every time.</em></figcaption></figure><p>Let’s look at the absolute bare minimum of steps in the process from the perspective of the developer. Broadly speaking, the developer identifies and ties up the site, conducts preliminary site review and discussions with impacted parties, manages entitlements, financing, and the project approval process for the site, oversees all building and owns the project along with any equity investors they have brought in to finance the deal. With respect to the building process itself, the developer will partner with an architect to work up a site massing and design that will satisfy the intended program — the mix of units, floorplans, common spaces, revenue generating and non-revenue generating space — as well as the code requirements for the building and the site. Once the basic architecture is in place, the project heads over to engineers, which are typically separate firms with separate contracts, costs, regulatory oversight, and liability, to specify structural engineering, MEP (mechanical, electrical, plumbing), and site and offsite engineering.&nbsp;</p><p>With the initial design and engineering complete, the developer sends a construction set of drawings to a general contractor (GC) for final pricing. The GC will take the specifications and build out a schedule and budget to determine how much work they intend to do themselves (self-perform) vs. how much work they will contract out to sub-contractors (subs). The GC will oversee all of the work necessary to build this custom-designed and engineered project on-site, including civil, foundation, pulling utilities, framing, rough-in and waterproofing, siding and cladding, roofing, windows and doors, MEP, paint and trim, fixtures and finishes, and ultimately certificate of occupancy.&nbsp; It is important to note that at this stage the GC’s quote is just an estimate. We haven’t built anything yet.</p><p>With approved drawings, financing, budget, and schedule in hand, the developer can start the snowflake build. Every step listed in the paragraph above must be completed by the corresponding specialists, and getting the work done on schedule is much more of an art than a science. Each contractor and subcontractor has their own contractual obligations, risks, and liability. Getting to the certificate of occupancy means the developer has navigated the process and can now bring in buyers or renters to start paying off the decades of debt that were secured to finance the build.&nbsp; When the developer moves on to the next project, all of the steps must be performed again, without exception. The finished project may be beautiful, energy efficient, or last 100 years, but it will still be a one-off.&nbsp;&nbsp;</p><p>All of this work is done for every project every time, regardless of size. A home or apartment is not more complicated than a car or plane, and while the unique attributes of individual sites and real estate do add challenges to repeatability, it is possible to decouple the building from the dirt.</p><p>We can’t be surprised that building costs for new housing continue to escalate. We can’t be surprised that it’s even more expensive to create affordable housing. Yet we continue to shrug our shoulders and say, “That’s just how it is.” We dig around the edges of the problem, foraging for scraps by looking for cost savings in commodities, underpaying labor, cutting corners on quality and performance, and building on cheaper land further and further from work and community.&nbsp;</p><p>Housing development is antiquated and the only group benefitting is the real estate development and building industry, who in fact have every incentive to restrict the supply. We’re making Version 1 of the oven, stereo, car, and plane over and over again, in a housing market that is chronically under-served. The failure to build adequate housing has generational effects on health, education, and opportunity.&nbsp;</p><p>There has to be a better way. We have to re-imagine the housing market and build better. More on that to come in our next post.</p></div></div></div>]]>
            </description>
            <link>https://blokable.com/news/the-housing-market-is-building-snowflakes-an-endless-series-of-one-offs/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24645525</guid>
            <pubDate>Wed, 30 Sep 2020 23:48:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[DigitalOcean's Hacktoberfest Is Hurting Open Source]]>
            </title>
            <description>
<![CDATA[
Score 671 | Comments 308 (<a href="https://news.ycombinator.com/item?id=24643894">thread link</a>) | @domenicd
<br/>
September 30, 2020 | https://blog.domenic.me/hacktoberfest/ | <a href="https://web.archive.org/web/*/https://blog.domenic.me/hacktoberfest/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>For the last couple of years, <a href="https://www.digitalocean.com/">DigitalOcean</a> has run
<a href="https://hacktoberfest.digitalocean.com/">Hacktoberfest</a>, which purports to “support open source” by giving free
t-shirts to people who send pull requests to open source repositories.</p>

<p>In reality, <strong>Hacktoberfest is a corporate-sponsored distributed denial of service attack against the open source
maintainer community</strong>.</p>

<p>So far today, on <a href="https://github.com/whatwg/html/pulls?q=is%3Apr+is%3Aclosed+label%3Aspam">a single repository</a>, myself
and fellow maintainers have closed 11 spam pull requests. Each of these generates notifications, often email, to the 485
watchers of the repository. And each of them requires maintainer time to visit the pull request page, evaluate its
spamminess, close it, tag it as spam, lock the thread to prevent further spam comments, and then report the spammer to
GitHub in the hopes of stopping their time-wasting rampage.</p>

<p>The rate of spam pull requests is, at this time, around four per hour. <em>And it’s not even October yet in my timezone.</em></p>

<p><img src="https://blog.domenic.me/images/hacktoberfest-spam-listing.png" alt="A screenshot showing a spam query for the whatwg/html repository, which is at this time up to 14 spam PRs"></p>

<p>Myself and other maintainers of the whatwg/html repository are not alone in suffering this deluge.
<a href="https://twitter.com/gravitystorm/status/1311386082982924289">My tweet</a> got commiseration from
<a href="https://mobile.twitter.com/gravitystorm/status/1311386082982924289">OpenStreetMap, phpMyAdmin</a>,
<a href="https://mobile.twitter.com/ulmerleben/status/1311378655231332355">PubCSS</a>,
<a href="https://mobile.twitter.com/JakeDChampion/status/1311389420638138370">GitHub, the Financial Times</a>,
<a href="https://twitter.com/slicknet/status/1311377444188770312">ESLint</a>, a
<a href="https://mobile.twitter.com/zekjur/status/1311411780162326531">computer club website</a>, and
<a href="https://mobile.twitter.com/juliusvolz/status/1311412919196844038">a conference website</a>, just within the first couple
of hours. Since then a dedicated account “<a href="https://twitter.com/shitoberfest">@shitoberfest</a>” has arisen to document the
barrage. Some <a href="https://github.com/search?q=is%3Apr+%22improve+docs%22+created%3A%3E2020-09-29&amp;type=Issues">cursory</a>
<a href="https://github.com/search?q=is%3Apr+label%3Ainvalid+created%3A%3E2020-09-29&amp;type=Issues">searches</a> show thousands of
spam pull requests, and rising.</p>

<p>DigitalOcean seems to be aware that they have a spam problem. Their solution, per their
<a href="https://hacktoberfest.digitalocean.com/faq">FAQ</a>, is to put the burden solely on the shoulders of maintainers. If we go
out of our way to tag a contribution as spam, then… we slightly decrease the chance of the spammer getting their free
t-shirt. In reality, the spammer will just keep going, submitting more pull requests to more repositories, until they
finally find a repository where the maintainer doesn’t bother to tag the PR as spam, or where the maintainer isn’t
available during the seven-day window DigitalOcean uses for spam-tracking.</p>

<p>To be clear, myself and my fellow maintainers did not ask for this. This is not an opt-in situation. If your open source
project is public on GitHub, DigitalOcean will incentivize people to spam you. There is no consent involved. Either we
contribute to DigitalOcean’s marketing project, or,
<a href="https://twitter.com/SudoFox/status/1311431141702819840">they suggest, we should quit open source</a>.</p>

<p>Hacktoberfest does not support open source. Instead, it drives open source maintainers even closer to
<a href="https://www.google.com/search?q=open+source+burnout">burnout</a>.</p>

<p><img src="https://blog.domenic.me/images/hacktoberfest-spam-pr.png" alt="A screenshot of a spam PR which adds the heading &quot;Great Work&quot; to the HTML Standard README"></p>

<h2 id="what-can-we-do">What can we do?</h2>

<p>My most fervent hope is that DigitalOcean will see the harm they are doing to the open source community, and put an end
to Hacktoberfest. I hope they can do it as soon as possible, before October becomes another lowpoint in the hell-year
that is 2020. In 2021, they could consider relaunching it as an opt-in project, where maintainers consent on a
per-repository basis to deal with such t-shirt–incentivized contributors.</p>

<p>To protect ourselves, maintainers have a few options. First, you can take the feeble step of ensuring that any spam
against your repositories doesn’t contribute to the spammer’s “t-shirt points”, by tagging pull requests with a “spam”
label, and <a href="https://twitter.com/MattIPv4/status/1311390498888781824">emailing hacktoberfest@digitalocean.com</a>.
DigitalOcean themselves, however, admit that
<a href="https://twitter.com/MattIPv4/status/1311390054334554113">this won’t stop the problem they’ve unleashed on us</a>. But
maybe it will contribute to the <a href="https://github.com/MattIPv4/hacktoberfest-data">metrics</a> they collect, which last year
showed that “only” 3,712 pull requests were labeled as spam by project maintainers.</p>

<p>If you’re comfortable cutting off genuine contributions from new users, you can try enabling GitHub’s
<a href="https://docs.github.com/en/free-pro-team@latest/github/building-a-strong-community/limiting-interactions-in-your-repository">interaction limits</a>.
However, <del>you have to do this every 24 hours, and</del> it has the drawback of also disabling issue creation and
comments. <ins>Update: GitHub has made the limit configurable, and has
<a href="https://twitter.com/github/status/1311772722234560517">a nice cheeky announcement tweet</a> zooming in on the “1 month”
option.</ins></p>

<p>Another promising route would be if GitHub would cut off DigitalOcean’s API access, as
<a href="https://twitter.com/__agwa/status/1311399074814472194">Andrew Ayer has suggested</a>. It’s not clear whether DigitalOcean
is committing a terms of service violation that would support such measures. But they’re certainly making GitHub a
less-pleasant place to be, and I hope GitHub can think seriously about how to discourage such corporate-sponsored
attacks on the open source community.</p>

<p>Finally, and most importantly, we can remember that this is how DigitalOcean treats the open source maintainer
community, and stay away from their products going forward. Although we’ve enjoyed using them for hosting the
<a href="https://whatwg.org/">WHATWG</a> standards organization, this kind of behavior is not something we want to support, so
we’re starting to investigate alternatives.</p>

  </div>

</article></div>]]>
            </description>
            <link>https://blog.domenic.me/hacktoberfest/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24643894</guid>
            <pubDate>Wed, 30 Sep 2020 21:10:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Defer Reference Implementation for C]]>
            </title>
            <description>
<![CDATA[
Score 72 | Comments 50 (<a href="https://news.ycombinator.com/item?id=24643034">thread link</a>) | @cyber1
<br/>
September 30, 2020 | https://gustedt.gitlabpages.inria.fr/defer/ | <a href="https://web.archive.org/web/*/https://gustedt.gitlabpages.inria.fr/defer/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="text-1-1">
<p>
In the following code snippet we
have one <i>guarded block</i> and three <i>deferred statements</i>.
</p>

<div>
<pre>guard {
  <span>void</span> * <span>const</span> <span>p</span> = malloc(25);
  <span>if</span> (<span>!</span>p) <span>break</span>;
  <span>defer</span> <span>free</span>(p);

  <span>void</span> * <span>const</span> <span>q</span> = malloc(25);
  <span>if</span> (<span>!</span>q) <span>break</span>;
  <span>defer</span> <span>free</span>(q);

  <span>if</span> (mtx_lock(&amp;mut)==thrd_error) <span>break</span>;
  <span>defer</span> <span>mtx_unlock</span>(&amp;mut);

  <span>// </span><span>all resources acquired</span>
}

</pre>
</div>

<p>
The idea is that we indicate with a <b><code>defer</code></b> keyword that the
statement, e.g a call to <b><code>free</code></b>, is only to be executed at the end of
the guarded block, and, that we want this action to happen
unconditionally in which way ever the guarded block is left.  For the
three deferred statements together it says that they should be
executed in the inverse order than they were encountered. So the
control flow for this code example can be visualized as follows:
</p>



<p><img src="https://gustedt.gitlabpages.inria.fr/defer/defer-image.png" alt="defer-image.png">
</p>

<p>
Here, the dashed lines represent circumstancial control flow that
might arise when some resources are not available or when the
execution is interrupted by a signal.
</p>

<p>
This new technique has at least two advantages to commonly used <code>C</code> or
<code>C++</code> techniques:
</p>

<dl>
<dt>proximity</dt><dd>The cleanup code (<code>free</code> or <code>mtx_unlock</code>) is coded
close to the place where its need arises.</dd>

<dt>visibility</dt><dd>The cleanup code is not hidden in some previously
defined function (such as for <code>atexit</code> handlers) or
constructor/destructor pairs (<code>C++</code>)</dd>
</dl>


<p>
For normal control flow (without intermediate <b><code>return</code></b>, <code>exit</code>, …)
code with similar properties can be coded with existing tools. The
above is equivalent to something like the following
</p>

<div>
<pre>{
   <span>void</span> * <span>const</span> <span>p</span> = malloc(25);
   <span>if</span> (<span>!</span>p) <span>goto</span> <span>DEFER0</span>;
   <span>if</span> (<span>false</span>) {
     <span>DEFER1</span>:
       free(p);
       <span>goto</span> <span>DEFER0</span>;
   }

   <span>void</span> * <span>const</span> <span>q</span> = malloc(25);
   <span>if</span> (<span>!</span>q) <span>goto</span> <span>DEFER1</span>;

   <span>if</span> (<span>false</span>) {
     <span>DEFER2</span>:
       free(q);
       <span>goto</span> <span>DEFER1</span>;
   }

   <span>if</span> (mtx_lock(&amp;mut)==thrd_error) <span>goto</span> <span>DEFER2</span>;

   <span>if</span> (<span>false</span>) {
     <span>DEFER3</span>:
       mtx_unlock(&amp;mut);
       <span>goto</span> <span>DEFER2</span>;
   }

   <span>// </span><span>all resources acquired</span>

   <span>goto</span> <span>DEFER3</span>;
   <span>DEFER0</span>:;
}

</pre>
</div>

<p>
Here, the <b><code>if(false)</code></b> clauses guarantee that the deferred statements
are jumped over when they are first met, and the labels and <b><code>goto</code></b>
statements implement the hops from back to front to execute the
deferred statements eventually.
</p>

<p>
Obviously, most <code>C</code> programmers would not code like this but they
would prefer to write down a <i>linearization</i> of the above, which is a
quite common idiom for cleanup handling in <code>C</code>:
</p>

<div>
<pre>{
   <span>void</span> * <span>const</span> <span>p</span> = malloc(25);
   <span>if</span> (<span>!</span>p) <span>goto</span> <span>DEFER0</span>;

   <span>void</span>*<span>const</span> <span>q</span> = malloc(25);
   <span>if</span> (<span>!</span>q) <span>goto</span> <span>DEFER1</span>;

   <span>if</span> (mtx_lock(&amp;mut)==thrd_error) <span>goto</span> <span>DEFER2</span>;

   <span>// </span><span>all resources acquired</span>

   mtx_unlock(&amp;mut);

 <span>DEFER2</span>:
   free(q);
 <span>DEFER1</span>:
   free(p);
 <span>DEFER0</span>:;
}

</pre>
</div>

<p>
This has the advantage of only making the circumstantial control flow
explicit (with three *=goto=) but it does that for the price of
proximity; the cleanup code is far from the place where its need
arises.
</p>

<p>
Nevertheless, even this linearization needs some form of naming
convention for the labels. For more complicated code the maintenance
of these jumps can be tricky and prone to errors.  This shows another
advantage of the <a href="#org63e5a5c"><code>defer</code></a> approach:
</p>

<dl>
<dt>maintainability</dt><dd>The cleanup specification is not dependent on
arbitrary naming such as labels (<code>C</code>) or RAII classes
(<code>C++</code>) and does not change when <a href="#org63e5a5c"><code>defer</code></a> or <b><code>break</code></b>
statements are added or removed.</dd>
</dl>

<p>
Another important property that is much more difficult to implement in
<code>C</code> (and that needs <b><code>try/catch</code></b> blocks in <code>C++</code>) is that <b>all</b> exits
from the guarded block are detected and acted upon: <b><code>break</code></b>, <b><code>return</code></b>,
<a href="#org18b8cc4"><code>thrd_exit</code></a>, <a href="#org9db677d"><code>exit</code></a>, <a href="#org7f6c106"><code>panic</code></a>, or
an interruption by a signal. That is, unless there are nasal deamons
flying around, we have a forth important property
</p>


<dl>
<dt>robustness</dt><dd>Any deferred statement is guaranteed to be executed
eventually.</dd>
</dl>

<p>
This is different from <code>C++</code>'s handling of destructors, which are only
guaranteed to be executed if there is a <b><code>try/catch</code></b> block underneath.
</p>

<p>
This principle of deferred execution extends to nested
guarded blocks in a natural way, even if they are stacked in
different function calls.
</p>
</div></div>]]>
            </description>
            <link>https://gustedt.gitlabpages.inria.fr/defer/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24643034</guid>
            <pubDate>Wed, 30 Sep 2020 19:55:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Progress in Biology Is Slow – Here's How We Can Speed It Up]]>
            </title>
            <description>
<![CDATA[
Score 37 | Comments 16 (<a href="https://news.ycombinator.com/item?id=24642489">thread link</a>) | @ashwal
<br/>
September 30, 2020 | https://adamashwal.com/irreducible | <a href="https://web.archive.org/web/*/https://adamashwal.com/irreducible">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

			<p>Living is great and I'd prefer to do more of it. Unfortunately, progress towards immortality has been rather slow — for all of our technological progress in the last century we've only picked up a few extra years of life. An incorrect framing of the problem has led to slow progress, but with a little bit of mind shift we can choose much better strategies for learning about and manipulating biology.</p>
<h3 id="what-s-the-status-quo-">What's the status quo?</h3>
<p>There's been a subset of humans over the last 100 years who also would like to live longer and healthier lives and have invested considerable time and energy into this problem. They've mostly failed as is evidence by the fact if you make it out of childhood, keep a good BMI, stop smoking, and exercise, you'll make it, at best, <a href="https://ourworldindata.org/life-expectancy">a decade longer than someone in the 1700s</a> (provided you escaped the trough of death that was childhood). In that same 100 years we took our first flight on Earth and then landed on the moon. In that same 100 years we went from 0 transistors per chip to 50,000,000,000. In that same 100 years we invented the Cool Ranch Dorito. So why did we succeed in so many other places but have failed in the most important? Why don't we live extremely long, healthy, and happy lives?</p>
<h3 id="why-has-this-approach-worked-in-other-areas-">Why has this approach worked in other areas?</h3>
<p>Some problems are, in retrospect, clearly easier than others. But if you had surveyed the leading minds in 1900 which would be easier: splitting the atom, sending a probe outside the solar system, or living to 90, it's hard to imagine it would be the last option they would have chosen. Yet here we are.</p>
<p>There are problems that on the face of it seem of similar difficulty to an outsider but are magnitudes (and magnitudes and magnitudes) harder to solve. What leads to this difference can be summarized as <em>computational reducibility</em>. As way of example, take the planet. Think of every atom on Earth- the rocks, the trees, that one ex who still drifts in and out of memory. If I want to know where this sphere will be in the universe tomorrow or 1,000 years from now and I don't have a notion of basic physics I may think this intractable. There's so much going on! How could one possibly think to describe how all these atoms would move through space and time? But it turns out to be fairly trivial - all those atoms can be reduced to a single point, a center of mass, and then calculations of momentum can be easily computed and trajectory projected forward. There are many systems that allow for these "shortcuts" where the dimensionality can be collapsed and the useful information is still present. A bridge builder doesn't need to think of each atom in a brick, or even really the brick; it's enough to think of the collection of bricks and how they are arranged.</p>
<h3 id="engineering-takes-place-in-reducible-places">Engineering takes place in reducible places</h3>
<p>Engineering, generally, is the practice of working on problems that are tractable. Given constraints on energy and time, only hard problems that have been sufficiently reduced are tractable and as such are the ones that are worked on. Oftentimes science leads us to new ways of reducing the complexity of problems (think Newton and his equations in the previous example). This isn't a law, just a result of how resources are allocated. </p>
<h3 id="is-biology-reducible-">Is Biology Reducible?</h3>
<p>We haven't seen progress in biology because it is stubborn to attempts to reduce it. Darwin was one of the last great reducers - able to collapse the high dimensional problem of evolution into a few axioms. But even with natural selection in hand, the resolution of claims is not particularly specific. Hypotheses are hard to prove or disprove given the near impossibility of running a counterfactual and it mostly serves as a post-hoc description (large proboscis <a href="https://en.wikipedia.org/wiki/Xanthopan">moths</a> notwithstanding). Perhaps if we were luckier we could  have lived in a universe that allowed us to use natural selection to know the structure of cells and animals without having to go look (this is a little true - something I'll explore in a future post), similar to knowing the position of the planets a millennia in advance.</p>
<p>What is it about biology that makes it irreducible but splitting the atom was something we accomplished 80 years ago? Spitting in the face of entropy is hard and the number of problems that need to be solved by a biological system are vast. The components of that system are not elegant fundamental laws of the universe but artisanal components created by random search through a loosely constrained fitness space. Even highly conserved pathways still exist in a unique context of the whole organism. </p>
<h3 id="biology-s-drunken-walk">Biology's Drunken Walk</h3>
<p>Biology is constantly transitioning from current state to a future state where some future branch of the evolutionary tree has higher fitness but the potential branch space is massive and the "choice" of which branch is picked is a random process. For example, in a scenario where the environment is slowly acidifying, any given bacteria has many solutions to survive. While aesthetically they could be vastly different (off the top of my head: changes to cell membranes, additional transmembrane proton pumps, neutralizing organelles, heat shock proteins, etc), which one ends up being dominant for a given bacteria is a random mutation. Given enough bacteria "searching" the solution space, you'll likely see many solutions.</p>
<p>Crucially, because in any scenario there is a one-to-many relationship between a problem and solutions, you can't extrapolate which solutions an organism possesses based on reasoning. You can’t postdict, you have to go look.</p>
<h3 id="so-what-do-we-do-">So, what do we do?</h3>
<p>So biology suffers from low reducibility - we aren’t able to summarize systems allowing us to make inferences cheaply. In the instance of disease this prevents both easy understanding of the disease state, i.e. what is going wrong, and prevents easy drug design, i.e. which node in the system do I push on in order to reverse the disease state. Right now, drug discovery is a lot of serendipity and a lot of pretending we know enough to pick targets. Unsurprisingly, this mostly fails.</p>
<p>There is another way. Currently, we brute force biology via Grad student search and it’s remarkably slow. A small number of underpowered, poorly done experiments makes up the bulk of what is produced. A model organism is chosen, an intervention is proposed, a measurement is done, and a paper is written: repeat ad nauseam. </p>
<p>But if an infinite number of monkeys can write Shakespeare, an infinite number of mice can allow us a way forward.</p>
<p>If we care about blood pressure, for example, why have we not given every drug, at every dosage, every regiment, and in every combination to a mouse and actually seen what happens?
We <em>do</em> have high throughput screening, mostly in individual cells or enzymes, but this is mostly garbage owing to the information decay from cell to whole organism (something I will expound on in a future post). Is my proposed solution expensive? Yes! Combinatorial explosions are the opposite of computational reducibility. But my point is that we can't just hope to have cheaper solutions in the future – that is the ostrich approach to progress. And we spent $288,100,000,000 to get to the moon.</p>
<p>The problem is also not as intractable as it may first seem. How do we test a large number of drugs on a large number of mice? Drive the cost down on any marginal mouse. Recent advances in machine learning allow automation of those pesky variable costs. Image recognition and classification are now good enough to track a mouse and its movements automatically - there is no need to babysit mice and manually classify behavior. With the state of every mouse known, simple robotics, e.g. food/medication administration and outcome measurements, become possible. The simplest experiments are possible today, and with a concerted effort, the realm of possible can grow. </p>
<p>There will always be innovations in new measurement techniques, new ways of peering into the system. Biologists generally fail in scaling these new techniques at a detriment to our ability to control biology. By adding scalability as an important aspect of innovation we can unlock so much more with what we already have today. </p>
<p>Importantly, this isn't just limited to a causal inference between a drug and a disease. We're getting very good at measuring the state of systems, just pick your favorite "–ome". It's not hard to squint and see that large numbers of interventions, on large numbers of model organisms, with large readouts of state will approach full 4D models of organic systems.  </p>
<p>There are not going to be any shortcuts with biology. The sooner we recognize this, the sooner we can start building systems that operate at the scale needed to bring useful inference, drug discovery, and network topology into the 21st century. </p>
<p>Thank you to Aubree for her feedback on commas, words, and ideas.</p>


			
			
		

			
			




		</div></div>]]>
            </description>
            <link>https://adamashwal.com/irreducible</link>
            <guid isPermaLink="false">hacker-news-small-sites-24642489</guid>
            <pubDate>Wed, 30 Sep 2020 19:10:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Energy Efficiency across Programming Languages [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 173 | Comments 141 (<a href="https://news.ycombinator.com/item?id=24642134">thread link</a>) | @Malfunction92
<br/>
September 30, 2020 | https://greenlab.di.uminho.pt/wp-content/uploads/2017/10/sleFinal.pdf | <a href="https://web.archive.org/web/*/https://greenlab.di.uminho.pt/wp-content/uploads/2017/10/sleFinal.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://greenlab.di.uminho.pt/wp-content/uploads/2017/10/sleFinal.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24642134</guid>
            <pubDate>Wed, 30 Sep 2020 18:43:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why is Snowflake so Valuable?]]>
            </title>
            <description>
<![CDATA[
Score 225 | Comments 145 (<a href="https://news.ycombinator.com/item?id=24641481">thread link</a>) | @malisper
<br/>
September 30, 2020 | https://www.freshpaint.io/blog/why-is-snowflake-so-valuable | <a href="https://web.archive.org/web/*/https://www.freshpaint.io/blog/why-is-snowflake-so-valuable">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Two weeks ago, Snowflake had the largest IPO ever for a software company. On the first day of trading, the stock price doubled, giving Snowflake a market cap of over $60 billion. Snowflake became all that everyone was talking about. There was so much hype, my mom, who doesn't even know what Snowflake is, decided to invest in Snowflake.</p><p>At first glance, the valuation of $60 billion seems absurd. Not only is Snowflake not profitable, having a net loss of $349 million in 2019, but their revenue is also small relative to their valuation. Their revenue over the last 12 months was $403 million. Based on that, their market cap is around 150x their revenue. So why exactly is their market cap so high? Besides revenue and profitability, there are a number of other metrics that investors look at. Compared to similar software businesses, Snowflake has god-like metrics. In this post, I'm going to go through a number of metrics from Snowflake's S-1, explain what the metrics mean, and give context on just how exceptional Snowflake's metrics are.</p><p>The first metric that stands out is Snowflakes 121% year over year growth. To give you a sense of how fantastic it is, you can compare it to the growth rates of other companies at IPO. <a href="https://techcrunch.com/2013/08/24/how-fast-should-you-be-growing/">Institutional Venture Partners looked at how quickly Internet and software companies were growing at IPO</a>.</p><figure><p><img src="https://uploads-ssl.webflow.com/5dad2b1e508f04886d0245c3/5f74c44d240d4de932d463c4_growth1.jpeg" alt="growth1"></p></figure><p>Based on their numbers, only 25% of companies that are between $75M-$500M in annual revenue are growing at just over 60% year over year. Snowflake's growth rate is nearly twice that! Snowflake's revenue was also $403M for the last 12 months, which would put them at the high end of that range. Not only are they growing significantly faster than other companies, but they are already significantly larger!</p><p>For companies that sell software to businesses an important number to look at is their retention rate. For Snowflake, their net retention rate is 158%. You may be asking, how do they have a retention rate over 100%? That's because companies that use Snowflake pay more for Snowflake over time. For every $1 of revenue Snowflake received from their customers a year ago, that same pool of customers are now paying $1.58. That means Snowflake could acquire no new customers, and they would still be doubling revenue every 18 months.</p><p>To compare that to other companies, <a href="https://www.lennyrachitsky.com/p/what-is-good-retention-issue-29">Lenny Rachitsky surveyed a number of industry experts on what they thought good and great net retention would be</a>. On average, for enterprise subscription software, the category Snowflake would fall under, the experts surveyed said 110% net retention would be good and 130% net retention would be great. He also pulled the net retention rates for a number of public companies:</p><ul role="list"><li><strong>Alteryx</strong>: 135% (<a href="https://docs.google.com/spreadsheets/d/1ogELPtct2s6jj9wRBpOgqGUE5k4cO73XXcvqmEZE89M/edit#gid=0">source</a>)</li><li><strong>Fastly</strong>: 130% (<a href="https://www.saastr.com/5-interesting-learnings-from-fastly-as-it-gets-ready-to-ipo/">source</a>)</li><li><strong>Okta</strong>: 124% (<a href="https://docs.google.com/spreadsheets/d/1ogELPtct2s6jj9wRBpOgqGUE5k4cO73XXcvqmEZE89M/edit#gid=0">source</a>)</li><li><strong>Anaplan</strong>: 124% (<a href="https://www.key.com/kco/images/Public_SaaS_Company_Retention_Metrics_2019.pdf">source</a>)</li><li><strong>Workday</strong>: 100%+ (<a href="https://www.workday.com/content/dam/web/en-us/documents/investor/workday-financial-analyst-day-2018.pdf">source</a>)</li><li><strong>ServiceNow</strong>: 97% (<a href="https://www.publiccomps.com/tickers/NOW">source</a>)</li></ul><p>You will notice that Snowflake's net retention rate of 158% is significantly higher than all of these!</p><p>Net promoter score (NPS) is a way of measuring customer satisfaction. It's convenient because it's easy to calculate and boils down customer satisfaction to a single number, making it easy to compare NPS between different companies. To compute NPS, a company performs a survey of their customers. The company asks "On a scale of 0 to 10, how likely are you to recommend this company’s product or service to a friend or a colleague?". Anyone that answers 9 or 10 is deemed a "promoter" of the product, while anyone that answers 6 or less is considered to be a "detractor". To compute the NPS, you take the percentage of users that are promoters and subtract the percentage of users that are detractors. This results in a number between -100 and 100 which is the NPS.</p><p>To give some examples, an NPS of 0 means your company has just as many promoters as detractors, whereas an NPS of 100 means everyone loves your product. Here are the NPS's for a few well known companies according to <a href="https://www.satmetrix.com/infographic/2020-us-consumer-benchmarks/">satmetrix</a>:</p><ul role="list"><li>Apple - 62</li><li>AirBnB - 43</li><li>AT&amp;T - 20</li></ul><p>What is Snowflake's NPS score? It's 71. According to NPS, Snowflake's customers like Snowflake more than Apple's customers like Apple.</p><p>The average Snowflake customer pays Snowflake $165k a year. Average contract value (ACV) is a useful metric to look at because it gives you a rough sense of how efficient a sales team is. To elaborate, there's a similar amount of work needed for a sales person to sell a $50k deal as there is for a sales person to sell a $100k deal. This is because most of the work a sales person does such as giving a demo, dealing with the paper process, and negotiating, have to happen regardless of how much the contract actually winds up being. Closing a $100k deal brings in twice as much revenue as a $50k deal does, but it's not usually twice as much work.</p><p>For this reason, higher average contract values are usually better. According to <a href="https://dskok-wpengine.netdna-ssl.com/wp-content/uploads/2020/08/2020-KBCM-SaaS-Survey.pdf">KBCM Technology Group</a>, 29% of software businesses have an ACV between $50k-$250k, and only 9% have an ACV greater than $250k. That gives Snowflake a relatively healthy ACV.</p><p>Not only does Snowflake have a high ACV, but their high end contract values are also really high. Snowflake has 56 customers that pay them over $1M a year. They also have Capital One, their largest customer, account for 11% of their 2019 revenue with approximately $29M.</p><p>Snowflake's net loss of $349M during 2019 does not sound good, but it does not tell the full story. There are two issues with using profit as a metric when looking at a subscription software business:</p><ul role="list"><li>Due to the generally accepted accounting principles (GAAP), a customer can give Snowflake money, but Snowflake can't count that money as revenue until a later date, even though that money is in Snowflake's bank account.</li><li>A net loss doesn't tell you where the money is coming in and where the money is going to.</li></ul><p>Addressing each of these points:</p><h3>Revenue under GAAP</h3><p>Under GAAP, Snowflake doesn't count revenue when a customer pays them, but instead when a customer uses the Snowflake product. To be more specific when a company pays Snowflake, they are purchasing a number of "Snowflake credits". When you are using the Snowflake product you exchange your credits for Snowflake compute time.</p><p>As an example, let's say a customer purchases one year of Snowflake credits for $100k. That money winds up in Snowflake's bank account, but Snowflake cannot count that $100k as revenue until Snowflake delivers the service, which is when the customer uses their credits for compute time. If the customer uses a quarter of the Snowflake credits within three months after purchases them, under GAAP, Snowflake would count that as $25k in revenue over those three months.</p><p>You can see the impact this has by looking at the $688M Snowflake has in "remaining performance obligations". This $688M is money that Snowflake's customers have either already given to Snowflake or have contractually committed to giving to Snowflake, but it won't show up as revenue until the customers use the credits they purchased.</p><h3>Where's the money going?</h3><p>The other thing to look at is what they are spending the money on. Snowflake is making money on every sale. They have a gross profit of 62% so they have no problem running the product. The thing is they are spending A LOT on sales and marketing. Their total net loss for the last six months was $171M while during the same time period they spent $191M on sales and marketing. If they wanted to, they could stop investing in growth, lay off their entire sales and marketing team, and they would become profitable. In other words, Snowflake could be profitable if they wanted to, but they are intentionally choosing not to, and are instead focusing on growing the customer base.</p><p>Not that they should. If we assume the $191M they spent in sales and marketing over the last six months was responsible for the $81M increase in revenue in the last six months compared to the prior six months, every $1 Snowflake invests in sales and marketing results in $.42 of revenue. With Snowflake's 158% net retention rate, that customer will pay back the initial investment in sales and marketing after around two years and then return many times the investment in the years following that.</p><p>In practice, Snowflake probably sees a return on investment much sooner. A company pays for Snowflake well before Snowflake is able to count that money as revenue.</p><p>So does it make sense for Snowflake's market cap to be 150x their revenue. I definitely think it's high, but I it's certainly plausible. They're growing faster, have higher customer satisfaction, and just have phenomenal metrics across the board when compared to similar businesses.</p><p>‍</p></div></div>]]>
            </description>
            <link>https://www.freshpaint.io/blog/why-is-snowflake-so-valuable</link>
            <guid isPermaLink="false">hacker-news-small-sites-24641481</guid>
            <pubDate>Wed, 30 Sep 2020 17:50:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Save Millions with QSBS and Section 1045 Rollovers]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24641348">thread link</a>) | @ankit77
<br/>
September 30, 2020 | https://goyalpost.com/2020/09/29/how-to-save-millions-with-qsbs-and-section-1045-rollovers/ | <a href="https://web.archive.org/web/*/https://goyalpost.com/2020/09/29/how-to-save-millions-with-qsbs-and-section-1045-rollovers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>Qualified Small Business Stock (QSBS) is some of the most tax-advantaged stock you can hold, yet few people know about it. If you’re a founder, early employee, or investor, you can potentially save millions of dollars by understanding the implications of QSBS.</p>



<p>In this article I will discuss the following:</p>



<ol><li>History of QSBS</li><li>QSBS Tax Savings</li><li>Requirements for QSBS</li><li>Section 1045 Rollover (applicable if you don’t meet the five year holding period, explained below)</li></ol>



<p>I recently sold shares in a company I co-founded and spoke to more than two dozen accountants in the process. I’m summarizing my findings and experiences in the article below. As such, the information in this article is not formal tax advice. I recommend you speak with your accountant prior to making any decisions.&nbsp;</p>



<h2>History of QSBS</h2>



<p>Section 1202 is the section of the tax code that outlines the QSBS tax exclusion. It was added to the tax code in 1993 to encourage individuals to invest in new ventures, far before the creation of Silicon Valley as we know it today. The act, however, failed to provide the intended incentive of spurring investments in new ventures.</p>



<p>Over the past three decades, a number of tailwinds have propelled QSBS back into the limelight. Congress reduced the tax on long-term capital gains in 1997, increased the tax savings of QSBS incrementally until 2010, and finally reduced corporate taxes from 35% to a flat 21% in 2017. These three forces have made QSBS far more relevant today than in the past.</p>



<h2>QSBS Tax Savings</h2>



<p><strong>Under Section 1202, your gains from selling QSBS may be eligible for up to 100% exclusion from federal and state taxes. This exclusion is limited to the </strong><strong><em>greater</em></strong><strong> of $10 million or 10 times your cost basis during a liquidity event.</strong></p>



<p>For instance, the excludable amount for a founder may be on $10 million of gain, while the exclusion for a VC may be much greater. If, for instance, a VC invests $20 million, the VC may obtain an exclusion for $200 million of gain. See the “Scenario Table” in the Appendix for more examples. Please also note that if you end up selling your shares in multiple tranches over multiple years, the excludable amount might vary. Reference<a href="https://www.thetaxadviser.com/issues/2018/nov/qualified-small-business-stock-more-attractive.html"> this</a> article for further details.</p>



<p><strong>If you qualify for a QSBS tax exclusion, you are 100% exempt from federal taxes. The current federal tax rate is 23.8% (20% federal + 3.8% medicare). This means you can save 23.8% in long-term capital gains that you would have been subject to otherwise. Depending on which state you live in (</strong><strong><em>not</em></strong><strong> which state the company is incorporated), you may qualify for state-level exclusion as well.</strong> States typically fall into one of four buckets:</p>



<ol><li>States with no individual income tax or no capital gains tax. These states are QSBS compliant by default.</li><li>States that follow the federal tax code and waive state taxes if an individual meets the federal-level QSBS requirements. These states are also QSBS compliant.&nbsp;</li><li>States that have their own QSBS exclusion statues.</li><li>States that do not recognize QSBS in any way, shape, or form (California notably falls in this bucket).&nbsp;</li></ol>



<p>I’ve provided a chart of applicable QSBS treatment in each of the 50 states and District of Columbia in the Appendix.</p>



<h2>Requirements for QSBS</h2>



<p><strong>For your stock to qualify as QSBS, you must meet certain requirements at the time of your stock issuance, and others during your entire holding period of the stock. If you sell QSBS, you must report the entire gain as a long-term gain on your Schedule D, and enter the allowable exclusion as a loss below the entry for the gain.</strong></p>



<h3>Requirements that must be met on the date of issuance:</h3>



<ul><li>Corporation issuing the stock must be a domestic C-Corp (and the stock must be issued after August 9, 1993)</li><li>You must acquire your stock directly from the company for money, property, or services. The only exception is if you acquire the stock by gift or inheritance. In this case, you are treated as having acquired the stock in the same manner as the original owner.<ul><li>Note: do not contribute the stock to a family LLC, limited partnership/trust, or to an LLC organized to manage the sale of your stock. This will disqualify the stock as QSBS.</li></ul></li><li><strong>Corporation must have assets of $50M or less at the time you receive your shares (or exercise your options).</strong><ul><li>Note: this is a continuous requirement and if at any point the assets of a corporation exceed $50M, the corporation can never again issue QSBS (even if the assets are below $50M on the date of the subsequent issuance).</li><li>Note: the assets of the corporation must not exceed $50M even after taking into account amounts the corporation received in the current issuance. If, for instance, a company has $40M in the bank and is raising a $20M Series B, none of the newly issued Series B stock will be QSBS.</li></ul></li><li>You must determine your stock issuance date. This is critical for three reasons:<ul><li><strong>Starts the clock for purposes of the five-year holding period requirement. In order to be eligible for the tax exemption outlined above, you must have held on to your stock for a minimum of five years.</strong> If you do not meet this minimum requirement, you can employ a Section 1045 rollover (described below) to extend your holding period.<ul><li><strong>Note: this requirement is yet another reason why you should early exercise your options and file an 83(b). Early exercise allows you to 1) start the one year holding period for long-term capital gains treatment, and 2) start the five year holding period for Section 1202. If you don’t file an 83(b) election, the clock on long-term capital gain only begins when your shares vest – so if there are multiple vesting dates, you will have multiple clocks to monitor for long-term capital gain – and, if eligible, QSBS. An unexercised option or warrant is not considered QSBS, even if the underlying stock would meet the definition of QSBS.</strong></li><li><strong>Note: for stock acquired through the exercise of an option, the company must pass the “$50M asset test” on the date of your exercise, not on the date of your grant.</strong> Similarly, for stock acquired through the vesting of RSUs, the company must pass the “$50M asset test” on vesting, and your five year holding period begins on vesting, not on grant.</li><li>Note: if the stock was received as a gift, inheritance, or as a distribution from a partnership, the acquisition date is the date on which the transferor acquired the stock.</li></ul></li><li>Determines whether gain from the sale of the QSB stock is eligible for a 50%, 75%, or 100% federal tax exclusion.<ul><li>50% federal tax exclusion for stock issued before February 18, 2009</li><li>75% federal tax exclusion for stock issued between February 18, 2009 and September 27, 2010</li><li>100% federal tax exclusion for stock issued after September 27, 2010</li></ul></li><li>Marks the date on which the company must have $50M or less in assets.&nbsp;</li></ul></li></ul>



<h3>Requirements that must be met during the shareholder’s holding period:</h3>



<ul><li>Corporation must be a C-corp for the entire holding period.</li><li><strong>The corporation must be an “active business” during the entire period you held your stock. This means that at least 80% (by value) of the assets in your corporation must be used to pursue business in industries </strong><strong><em>other</em></strong><strong> than the industries below. Note that if your business provides a service, then it most likely does not qualify as a qualified small business.&nbsp;</strong><ul><li>Health, law, non-software engineering (civil, electrical, etc), architecture, accounting, actuarial science, performing arts, consulting, athletics, financial services, or brokerage.</li><li>Banking, insurance, financing, leasing, investing, or similar business.</li><li>Farming.</li><li>Mining or natural resource production or extraction.</li><li>Operating a hotel, restaurant, or similar business.</li></ul></li><li>Cash held for burn requirements generally qualify under this “active business” requirement. However, after two years, technically no more than 50% of the corporation’s assets can qualify under this exemption. While startups usually satisfy this requirement, it isn’t always clear how to apply the rule, especially if the startup retains significant cash following an investment (in other words, overfunded startups sitting on cash). If you’re like most startups and you’re burning cash to fund business operations you will most likely pass this check.</li><li>If the corporation bought back 5% or more of its stock in the year before or after your stock issuance, your stock will not qualify as QSBS.&nbsp;</li></ul>



<h2>Section 1045 Rollover</h2>



<p><strong>In order to be eligible for preferential tax treatment under Section 1202, you must satisfy the requirements above and have held onto your stock for at least five years. If you have not met the five year minimum, however, you can employ a Section 1045 rollover to extend your holding period.</strong></p>



<p><strong>If you have held onto your QSBS for at least six months, you can sell your QSBS and roll the proceeds of the sale into another QSBS issuer without recognizing a gain under Section 1045.</strong> This is a similar concept to a Section 1031 exchange in real estate. <strong>Per Section 1045, you have 60 days from the date of the sale of your original QSBS to roll the sale proceeds into new QSBS.</strong> In general, you should roll all of the proceeds from your sale of original QSBS into the new QSBS. If you take any cash off the table after the initial sale, that amount would be subject to capital gains tax. <strong>The cost basis of the new QSBS is the same as the cost basis of the original QSBS, and the holding period from the original QSBS is counted towards the holding period of the new QSBS.</strong></p>



<p>Consider the following scenario: you acquire 5M shares of QSBS from “Company A” on January 1st, 2010 for $0.00001 per share. Your original cost basis is $50 (5M * $0.00001). Assume you then sell these shares in Company A for $4M on January 1st, 2012, and then reinvest this $4M to purchase 2,000 shares of QSBS in “Company B”. Your holding period will pick up from where it left off and the cost basis for your new shares in Company B will be the same as the original cost basis for your shares in Company A ($50). If you then sell your QSBS in Company B at any point after January 1, …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://goyalpost.com/2020/09/29/how-to-save-millions-with-qsbs-and-section-1045-rollovers/">https://goyalpost.com/2020/09/29/how-to-save-millions-with-qsbs-and-section-1045-rollovers/</a></em></p>]]>
            </description>
            <link>https://goyalpost.com/2020/09/29/how-to-save-millions-with-qsbs-and-section-1045-rollovers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24641348</guid>
            <pubDate>Wed, 30 Sep 2020 17:39:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Failed Promise of Web Components]]>
            </title>
            <description>
<![CDATA[
Score 330 | Comments 200 (<a href="https://news.ycombinator.com/item?id=24640151">thread link</a>) | @lemonberry
<br/>
September 30, 2020 | https://lea.verou.me/2020/09/the-failed-promise-of-web-components/ | <a href="https://web.archive.org/web/*/https://lea.verou.me/2020/09/the-failed-promise-of-web-components/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<div>

			
<p>Web Components had so much potential to empower HTML to do more, and make web development more accessible to non-programmers and easier for programmers. Remember how exciting it was every time we got new shiny HTML elements that actually <em>do stuff</em>? Remember how exciting it was to be able to do sliders, color pickers, dialogs, disclosure widgets straight in the HTML, without having to include any widget libraries?</p>



<p>The promise of Web Components was that we’d get this convenience, but for a much wider range of HTML elements, developed much faster, as nobody needs to wait for the full spec + implementation process. We’d just include a script, and boom, we have more elements at our disposal!</p>



<p>Or, that was the idea. Somewhere along the way, the space got flooded by JS frameworks aficionados, who revel in complex APIs, overengineered build processes and dependency graphs that look like the roots of a banyan tree.</p>



<figure><img src="https://live.staticflickr.com/2025/32441377780_e3acf6de12_b.jpg" alt=""><figcaption>This is what the roots of a Banyan tree look like. <a href="https://www.flickr.com/photos/79721788@N00/32441377780/">Photo by David Stanley on Flickr (CC-BY)</a>. </figcaption></figure>



<p>Perusing the components on <a href="https://www.webcomponents.org/">webcomponents.org</a> fills me with anxiety, and I’m perfectly comfortable writing JS — I write JS for a living! What hope do those who can’t write JS have? Using a custom element from the directory often needs to be preceded by a ritual of npm flugelhorn, import clownshoes, build quux, all completely unapologetically because “here is my truckload of dependencies, yeah, what”. Many steps are even omitted, likely because they are “obvious”. Often, you wade through the maze only to find the component doesn’t work anymore, or is not fit for your purpose.</p>



<p>Besides setup, the main problem is that HTML is not treated with the appropriate respect in the design of these components. They are not designed as closely as possible to standard HTML elements, but <em>expect</em> JS to be written for them to do anything. HTML is simply treated as a shorthand, or worse, as merely a marker to indicate where the element goes in the DOM, with all parameters passed in via JS. I recall <a href="https://adactio.com/articles/12839#webcomponents">a wonderful talk by Jeremy Keith</a> a few years ago about this very phenomenon, where he discussed <a href="https://shop.polymer-project.org/">this e-shop Web components demo by Google</a>, which is the poster child of this practice. These are the entire contents of its <code>&lt;body&gt;</code> element:</p>



<pre><code>&lt;body&gt;
	&lt;shop-app unresolved=""&gt;SHOP&lt;/shop-app&gt;
	&lt;script src="node_assets/@webcomponents/webcomponentsjs/webcomponents-loader.js"&gt;&lt;/script&gt;
	&lt;script type="module" src="src/shop-app.js"&gt;&lt;/script&gt;
	&lt;script&gt;window.performance&amp;&amp;performance.mark&amp;&amp;performance.mark("index.html");&lt;/script&gt;
&lt;/body&gt;</code></pre>



<p>If this is how Google is leading the way, how can we hope for contributors to design components that follow established HTML conventions?</p>



<p>Jeremy criticized this practice from the aspect of backwards compatibility: when JS is broken or not enabled, or the browser doesn’t support Web Components, the entire website is blank. While this is indeed a serious concern, my primary concern is one of <strong>usability</strong>: <strong>HTML is a lower barrier to entry language</strong>. Far more people can write HTML than JS. Even for those who do eventually write JS, it often comes after spending years writing HTML &amp; CSS.</p>



<p>If components are designed in a way that requires  JS, this excludes thousands of people from using them. And even for those who <em>can</em> write JS, HTML is often easier: you don’t see many people rolling their own sliders or using JS-based ones once <code>&lt;input type="range"&gt;</code> became widely supported, right?</p>



<p>Even when JS is unavoidable, it’s not black and white. A well designed HTML element can reduce the amount and complexity of JS needed to a minimum. Think of the <code><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/dialog">&lt;dialog&gt;</a></code> element: it usually does require *some* JS, but it’s usually rather simple JS. Similarly, the <code>&lt;video&gt;</code> element is perfectly usable just by writing HTML, and has a comprehensive JS API for anyone who wants to do fancy custom things.</p>



<p>The other day I was looking for a simple, dependency free, tabs component. You know, the canonical example of something that is easy to do with Web Components, the example 50% of tutorials mention. I didn’t even care what it looked like, it was for a testing interface. I just wanted something that is small and works like a normal HTML element. Yet, it proved so hard I ended up writing my own!</p>



<h3>Can we fix this?</h3>



<p>I’m not sure if this is a design issue, or a documentation issue. Perhaps for many of these web components, there are easier ways to use them. Perhaps there are vanilla web components out there that I just can’t find. Perhaps I’m looking in the wrong place and there is another directory somewhere with different goals and a different target audience. </p>



<p>But if not, and if I’m not alone in feeling this way, we need a directory of web components with strict inclusion criteria:</p>



<ul><li><strong>Plug and play.</strong> No dependencies, no setup beyond including one <code>&lt;script&gt;</code> tag. If a dependency is absolutely <em>needed</em> (e.g. in a map component it doesn’t make sense to draw your own maps), the component loads it automatically if it’s not already loaded.</li><li>Syntax and API follows <a href="https://www.smashingmagazine.com/2017/02/designing-html-apis/"><strong>conventions established by built-in HTML elements</strong></a> and anything that <em>can</em> be done without the component user writing JS, <em>is</em> doable without JS, per <a href="https://www.w3.org/2001/tag/doc/leastPower.html">the W3C principle of least power</a>.</li><li><strong>Accessible by default</strong> via sensible ARIA defaults, just like normal HTML elements.</li><li><strong>Themable</strong> via <code><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/::part">::part()</a></code>, selective inheritance and custom properties. Very minimal style by default. Normal CSS properties should just “work” to the the extent possible.</li><li><strong>Only one component of a given type</strong> in the directory, that is <strong>flexible</strong> and <strong>extensible</strong> and continuously iterated on and improved by the community. Not 30 different sliders and 15 different tabs that users have to wade through. No branding, no silos of “component libraries”. Only elements that are designed as closely as possible to what a browser would implement in every way the current technology allows.</li></ul>



<p>I would be up for working on this if others feel the same way, since that is not a project for one person to tackle. <em>Who’s with me?</em></p>



<p><strong>UPDATE:</strong> Wow this post blew up! Thank you all for your interest in participating in a potential future effort. I’m currently talking to stakeholders of some of the existing efforts to see if there are any potential collaborations before I go off and create a new one. <a href="https://twitter.com/leaverou">Follow me on Twitter to hear about the outcome</a>!</p>

		</div><!-- .entry-content -->

	</div></div>]]>
            </description>
            <link>https://lea.verou.me/2020/09/the-failed-promise-of-web-components/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24640151</guid>
            <pubDate>Wed, 30 Sep 2020 15:59:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Garry Tan on Posterous, Palantir, YC, Initialized and Influencer Investing]]>
            </title>
            <description>
<![CDATA[
Score 24 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24639871">thread link</a>) | @rayshan
<br/>
September 30, 2020 | https://www.acquired.fm/episodes/how-yc-rewrote-the-seed-playbook-with-garry-tan-hn | <a href="https://web.archive.org/web/*/https://www.acquired.fm/episodes/how-yc-rewrote-the-seed-playbook-with-garry-tan-hn">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><strong>Transcript:</strong> <em>(disclaimer: may contain unintentionally confusing, inaccurate and/or amusing transcription errors)</em><strong><br></strong></p><div><p>David: Hello, Acquired LPs. We are coming at you today with a very special episode with Garry Tan of Initialized Fame and before that—Y Combinator, Posterous, and has a long and very illustrious career here in Silicon Valley. We are going to talk about the evolution of early-stage investing.</p><p>Garry has seen it all. I think he started back in the early days when early-stage investing meant a $2 million Series A at a $5 million poster. Maybe that was even high. </p><p>Garry: Yeah, it’s a while.</p><p>David: Goodness, things have changed. I'm so excited to have you here. Thanks for joining us and we can't wait to dive in.</p><p>Garry: Yeah, thank you for having me. Big fan of the show and it means a lot to me that you'd have me.</p><p>David: Likewise that you would come on. Let's just dive right in. We're going to weave in the story of Initialized along the way, but we thought maybe we'd start way back in those prehistoric days of what life is like?</p><p>Garry: So, Arthur Rock.</p><p>David: We've already done that on the Sequoia episodes. Not that far back but when was it that you started Posterous? Was it in 2005?</p><p>Garry: It was 2008, actually. </p><p>David: It was 2008. It’s later than I thought.</p><p>Garry: 2005 I was still at Palantir, so I had just designed the logo for Palantir and built one of the major product teams. Before that, I was Stanford Computer Engineering, and the crazy story for me is that friends of mine were starting a company. And I was the lowest of the low PM at Microsoft.</p><p>Ben: What that’s like?</p><p>Garry: Great place to start, great reach. Friends of mine were starting a company with Peter Thiel. They flew me down to San Francisco to have dinner with Peter right when he wrote the $500,000 check to Facebook. He said, “Garry, what are you doing at Microsoft? You're wasting your time.” I said, “I wanted to work at a startup, but they weren't startups in 2003, 2004.”</p><p>He said, “I'm so sure this is the right thing. You need to quit your job.” He asked me how much a year I made. I told him it was $70,000. He said, “Well, how about this? I'll write you a personal check from my bank account to yours. This is your risk opportunity. Quit your job.” I said, “Thank you very much, Mr. Thiel, but I might make it to level 60 next year,” and I got on a plane and went back to Seattle. That company turned into Palantir. </p><p>David: Oh my gosh.</p><p>Garry: I ended up joining a year later, they had hired away some of my closest friends who were way smarter than me. Bob McGrew, who now runs stuff over at OpenAI. Just so many smart people you get to meet in Silicon Valley over time. Once they hired away people smarter than, I was like, I need to quit my job at Microsoft. At the moment, when your friends are starting a business and you don't know anything about startups, tech, or how these things are funded. You say, well, I have a real job, and you say no.</p><p>David: We're talking about this with Kevin and Julia Hartz on the Eventbrite episode. I imagine you're right out of college. Your parents were probably (if you even told them about this) like, no way would they let you do this.</p><p>Garry: They’re like don’t do it. That seems unsafe. Yeah. The immigrant mentality for sure.</p><p>Ben: Garry, that's an amazing story, and it is absolutely one of survivorship bias because I want to share my story. I had a friend who was starting a YC Company. I had just moved to Seattle, and I was about to start my job at Microsoft. This friend lobbied and lobbied and lobbied. They’d come and get me to co-found the company with them.</p><p>For years, it looked like a huge mistake that I said, are you kidding me? I'd have to give back my signing bonus. I just moved here. This person told me something very similar that they would help definitely pay back the signing bonus. [...] your risk thing, they’d pay for my move. But as years have gone by, that company sold for exactly the preference. It would have been completely awash.</p><p>Garry: Yeah. That's tough.</p><p>Ben: For a while I was like, wow, that's one of several examples in my life where I really blew it on joining something early. Sometimes, not that my choice was the right choice, but unless it's Peter Thiel calling, the story doesn't always end the way you’re—</p><p>Garry: Yeah. These things are crazy risky. I often think about, wow, he was willing to pay that much upfront for an engineer. The rest of my career—even as an investor today—is now actually the inversion of that. Which is now I realize actually, it's the software engineers, designers, product people, and the builders who create the future. That's why we're able to do early-stage at all is that we look a lot more like them at that stage and so they'll pick us.</p><p>On the flip side, because I can still code a little bit and I still do design. I'm probably better at marketing now than I was 10 years ago.</p><p>David: You've diversified your skillset.</p><p>Garry: Yeah. That's right. We look more like them. Then that's the cool thing. I think that fits with the overall series. It's like we're talking about the traitorous eight—sending real typewritten letters across the country to financiers who were totally different from them. Now, what we're talking about—what you guys do and what we're doing—is we're no different. I think that time was for venture capitalists to say we're set apart, we're different.</p><p>David: We're in this ivory tower.</p><p>Garry: Yeah. The ivory tower is different. There's actually a ritualistic aspect that I was talking with Geoff Lewis at Bedrock. He has this theory that is really interesting. There is something to be said for I'm walking up the steps of Sequoia, this is what legends before me did, and I can be a part of that.</p><p>Now it's Zoom. If anything, now it's flipped. Now it's about the one on one conversation that you can have right here. That's why you have the rise of influencer investing. I know, we'll talk about it later, but that's part of the reason why I think YouTube is so important, for me anyway. I'm investing very deeply into it because I think it's a very interesting innovation in the course of how ventures are created.</p><p>David: VCs historically take a long time to catch on. YouTube was founded right around this time that we're talking about, and here we are in 2020 and people only just were starting to catch on.</p><p>Garry: 2005, yeah.</p><p>Ben: Garry, you talked about your time as a builder, and we're going to put a pin in this influencer investing and definitely come back to it because I think it'll be a nice way to round out the full story. Take us through founding Posterous, leaving Palantir, how you raised money for that, and how you went about getting enough proof that there's a there, there to invest more of your time.</p><p>Garry: There's nothing quite like seeing a super early-stage startup for your own eyes. Actually, I've always been really thankful for my time working with Stephen Cohen, Joe Lonsdale, and Alex Karp—just the founders of Palantir. Being able to build software from scratch. The more subtle interesting thing that I feel like I learned was how important it is to basically continue to hire people way smarter than you actually.</p><p>The cult making and the mythmaking of the startup very early are really underplayed. I don't feel like people talk about it enough, and Palantir, I think remains very good at it. The only cult that was stronger than our cult was the Facebook cult (I think). But it's interesting to see. Years later, that's an order of magnitude bigger as a company, which is fascinating to me. I think that actually is directly proportional. How strong your cult will result in how big your company ends up being.</p><p>Assuming you're in the right market and 10 other things that you need to survive. You need to be one of those survivors. A lot of things have to break your way.</p><p>Ben: What's an example of something that was done at Palantir to help build that cult brand?</p><p>Garry: Honestly, I think the simplest thing was even just trying to get the smartest, most capable, and hardworking people, which sounds really stupid simple. It seems like everyone should do that, but honestly, people just don't. When you think about hiring, the mistake that a lot of founders make—and honestly, I made this mistake at some level too when I worked on Posterous—was who can I get? And that's the wrong question.</p><p>You should start with who is the smartest person I know, and it doesn't matter where they're at. Because if I get them, then our self-fulfilling prophecy becomes destiny. If I don't, then it doesn't. I'm not doing myself or them a favor by not going after them. We would just go. </p><p>People would pass out yellow legal pads, and we would force everyone to step away from their computers, and it'd be like, write down the names of 20 people who were the smartest people you've ever met in any walk of life. It didn't have to be engineering, design, or whatever. It was just, who are the smartest people you know in your life. We take it into a backroom and cross-reference it and then those are like our hit list. It's like let's go get those people. We're going to take them to dinner, we're going to take them to lunch, we're going to meet them, we're going to chop down the tree, and we're going to go get them.</p><p>David: That mindset leads to doing things like what Peter offered to you. I can't believe I've never thought of that before. Yeah, you were ungettable. You were at Microsoft, but what if you just offered to personally cover the gap in your salary? It didn't work for you then, but a certain number of people, that's going to work with. And if you're not in that mindset of okay, I don't even go to try to get this person. Well then, you don't know, but once you're like, no, I'm going to try and get them. Well, what can I do?</p><p>Garry: Yeah. It just compounds from there because smart people want to work with smart people. I think that is testament and credit to what they've been able to build over the years. That becomes a self-fulfilling …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.acquired.fm/episodes/how-yc-rewrote-the-seed-playbook-with-garry-tan-hn">https://www.acquired.fm/episodes/how-yc-rewrote-the-seed-playbook-with-garry-tan-hn</a></em></p>]]>
            </description>
            <link>https://www.acquired.fm/episodes/how-yc-rewrote-the-seed-playbook-with-garry-tan-hn</link>
            <guid isPermaLink="false">hacker-news-small-sites-24639871</guid>
            <pubDate>Wed, 30 Sep 2020 15:37:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Coinbase post was 100% right. Here's what you can do about it]]>
            </title>
            <description>
<![CDATA[
Score 30 | Comments 20 (<a href="https://news.ycombinator.com/item?id=24638995">thread link</a>) | @ihm
<br/>
September 30, 2020 | https://parametricity.com/posts/2020-power/ | <a href="https://web.archive.org/web/*/https://parametricity.com/posts/2020-power/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			<p>Recently there was a minor controversy over a blogpost from Coinbase’s <a href="https://blog.coinbase.com/coinbase-is-a-mission-focused-company-af882df8804">CEO</a>
discouraging employees from thinking about politics, and encouraging them to focus on profit-making.
I really appreciated this post, because I think it can be very clarifying for those employees who think their
company can be a pure force for good. The Coinbase blogpost gives the honest truth that in the final calculus,
corporations whose primary goal is profit-maximization can only incidentally and in small ways advance any other
aim.</p>
<p>At the same time, there is a lot of promise for good in the actual technology underlying the crypto industry. This post will argue</p>
<ol>
<li><strong>If work is organized through for-profit corporations, there will be strong pressure from management and capital to
distort positive aspects of the technology in favor of profit-making</strong></li>
<li>The <strong>naive utopianism</strong> which has sustained the industry so far and helped produce some good technology <strong>is no match for this pressure</strong></li>
<li><strong>The only realistic way to combat this</strong> distorting pressure and focus on the socially-beneficial aspects of the technology <strong>is for workers</strong>
(the other human factor of production apart from management and capitalists) <strong>to use their collective power to push back</strong>.</li>
</ol>
<p>This argument is not surprisingly somewhat controversial among executives and VCs, but happily that same group loves
free speech and hates cancel-culture, so I assume I will not be attacked for expressing it.</p>
<h2 id="what-is-the-web-and-what-should-it-be">What is the web and what should it be?</h2>
<p>What should the web be? It should be a medium for personal communication, organization for people in projects, art and creative expression, etc. that is oriented around the goals of individual and collective flourishing. That would be ideal.</p>
<p>In reality, it is not operated that way. It – like almost everything in our society – is operated as a profit-extraction machine in the service of a small group of people.
This machine started off performing the above functions (of course, already distorted by its origins as a technology for the military and academia).
Soon, an industry of for-profit companies sprung up around developing the internet. Many of these companies became large enough to have a decisive influence on what the web would be.</p>
<p>As the years went by, the parts of it that did not contribute much to profit generation (like weird personal websites) were scrapped under the influence of these companies,
and new pieces which enhanced profit generation were added. It happened this way because there is enormous pressure from management and investors to focus on profit-generation
to the exclusion of all else.</p>
<p>Of course, because these modifications were made primarily in pursuit of profit with other reasons being secondary, this resulted in a bunch of unintended (or un-“cared about”) negative consequences like</p>
<ul>
<li>creepy data-collection and surveillance which is used to manipulate us into buying things</li>
<li>right-wing radicalization which has resulted in widespread political violence</li>
<li>distribution of misinformation</li>
<li>an enormous carbon footprint</li>
<li>collective billions in wasted minutes spent scrolling because we were tricked by the algorithm to stay on just a bit longer</li>
</ul>
<p>All of these things are natural results of a system that cannot “see” the concept of social good and can only see profits.
These problems just do not matter to the system, and only could in the event that they interfered with profit-generation.</p>
<h2 id="how-to-push-back">How to push back</h2>
<p>Let’s say you are a worker at a crypto company who sees this history and want to make sure that in crypto, socially useful aspects of
the technology are prioritized rather than those that make the most profit. What should you do?</p>
<p>First, I think it is necessary to deal with the most popular response, which is a utopian faith that the decentralized
nature of some crypto technology will inherently stop bad things from happening.</p>
<h3 id="crypto-utopianism">Crypto-utopianism</h3>
<p>There’s a lot of utopianism in crypto. This ranges from the lowest Bitcoin-booster all the way up to people like Vitalik.</p>
<p>This quote is somewhat <a href="https://www.coindesk.com/this-political-conversation-with-vitalik-buterin-shows-how-ethereum-could-change-the-world">examplary</a>:</p>
<blockquote>
<p>These three white men talked about the protests erupting across the United States. To his credit, the Russian-Canadian Buterin spoke broadly instead of attempting to comment on inequality in American politics. He said the current generation is facing a global “crisis of legitimacy,” concerning both corporations and “many types of governments.”</p>
</blockquote>
<blockquote>
<p>“The challenge here is can we create systems that allow some groups of people to cooperate without that downside of a centralized or trusted actor having to be in the middle,” Buterin said.</p>
</blockquote>
<p>Crypto-utopianism usually includes a suggestion that crypto will replace existing systems (which are bad because they’re “centralized” or some other vague reason) and be better because they are “decentralized” (whatever that might mean).</p>
<p>These analyses give crypto projects a sense of grandeur and importance which is motivating to developers and investors alike. Unfortunately, if your goal is to push back against the existing profit-maximizing power structures, you need to have a better analysis than “centralized bad, decentralized good”. It sounds good, but it doesn’t actually constitute a plan to defeat the Facebooks of the world or even to resist pressure from investors.</p>
<p>The crypto-utopian prescription for all that ails the web is some new technology.
Technology is great and a necessary part of the puzzle, but unless Glenn Weyl has a way of forcing Facebook to use quadratic voting for corporate governance, it’s not useful yet.
<strong>The existing power structure can always pick-and-choose the new technology that best advances its goals.</strong>
If you have goals which are distinct from theirs, you need to build power that is capable of a serious challenge.</p>
<h3 id="what-kind-of-power-do-they-have">What kind of power do they have?</h3>
<p>What kind of power does the existing “power structure” consisting of senior management and capitalists have?
It has</p>
<ul>
<li><a href="https://www.theverge.com/2019/11/25/20983053/google-fires-four-employees-memo-rebecca-rivers-laurence-berland-union-busting-accusation-walkout">the ability to fire people</a> that won’t comply with its goals</li>
<li>the ability to withhold funding from companies that won’t comply with its goals (this is kind of another version of the previous)</li>
<li><a href="https://parametricity.com/posts/2020-power/news.ycombinator.com">public platforms</a> to spread <a href="https://blog.coinbase.com/coinbase-is-a-mission-focused-company-af882df8804">messaging that argues in favor of its goals</a></li>
<li>control of platforms which allows them to <a href="https://itsgoingdown.org/on-facebook-banning-anarchist-and-antifascist-pages-the-digital-censorship-to-come/">censor unfavorable currents in the culture at large</a></li>
<li>endless legal resources to neutralize those that won’t comply with its goals</li>
<li>in some cases, relationships with law enforcement that can be used against those that won’t comply with its goals
and surely much more.</li>
</ul>
<p>Profit-maximization is the “prime directive” of the existing power structure and as a result challenging it inevitably provokes a fight.
This fight is usually waged (on their end) by making use of all the above tools and more.</p>
<h3 id="what-kind-of-power-do-you-have">What kind of power do you have?</h3>
<p>What kind of power do you, a worker at a crypto company, have to resist all of the above? On your own, basically none. If you
make too much trouble, you will simply be fired and then you’re out of luck. Some small number of people can try starting a company (which I have done)
but you’re still subject to some of the same kinds of retaliation as funding can be withheld, which either “fires” the company or redirects
it toward whatever profit-maximization opportunity is available. And even if you have an independent revenue stream, you will be competing
against firms that are willing to do whatever it takes to maximize profits.</p>
<p>As a group however, workers have an enormous amount of power. It is sometimes considered controversial to say so, but VCs and senior
management cannot actually do anything without workers. As such, if workers organize together into a strong company or industry-wide union,
they can make demands of the existing power structure and refuse to participate in the production process (i.e., strike) if those demands are not met.
This power can be augmented with legal resources, platforms of their own, etc.</p>
<p>If you are interested in building power with your fellow workers to advance your own goals rather than those of the profit-maximizers,
both within your company and across your industry, I recommend reaching out to <a href="https://techworkerscoalition.org/">Tech Workers Coalition</a>
who can provide further guidance. You can also get in touch with or <a href="https://act.dsausa.org/donate/membership2020">join your local DSA</a> to
build your analysis, help push more broadly for a world beyond profit maximization, and get support.</p>

		</div></div>]]>
            </description>
            <link>https://parametricity.com/posts/2020-power/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24638995</guid>
            <pubDate>Wed, 30 Sep 2020 14:22:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Revisiting a 'Smaller Rust']]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24638129">thread link</a>) | @gbrown_
<br/>
September 30, 2020 | https://without.boats/blog/revisiting-a-smaller-rust/ | <a href="https://web.archive.org/web/*/https://without.boats/blog/revisiting-a-smaller-rust/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
			<p>A bit over a year ago, I wrote some <a href="https://without.boats/blog/notes-on-a-smaller-rust">notes on a “smaller Rust”</a> - a higher level language
that would take inspiration from some of Rust’s type system innovations, but would be simpler by
virtue of targeting a domain with less stringent requirements for user control and performance.
During my time of unemployment this year, I worked on sketching out what a language like that would
look like in a bit more detail. I wanted to write a bit about what new conclusions I’ve come to
during that time.</p>
<h2 id="the-purpose-for-our-language">The purpose for our language</h2>
<p>Re-reading my previous post, I’m struck by how vague my statement of purpose for this language is.
My entire blog post is really focused on differentiating the language from <em>Rust</em>, and I frame the
discussion in terms of what I would remove from Rust, and how the language would not support certain
use cases of Rust. This isn’t really surprising: I was working on Rust, and I never had taken the
time to think of this hypothetical language in itself the way I have now.</p>
<p>The goal of this design was to create a language that could compete as an “application programming
language.” The design goals of this language were:</p>
<ol>
<li>It should try not to be notably hard to learn. To the extent possible, it should be familiar to
most programmers. Since I’m comitting by the exercise to trying to apply ownership and borrowing
to the application domain, it will necessarily contain some features most programmers find
pretty novel (like Rust “lifetimes”). But in general, we will try to reduce the onboarding ramp
and simplify things.</li>
<li>It should typecheck and compile quickly. It should not have bad batch compilation performance,
and it should be designed with incremental recompilation in mind, to enable a good experience for
users who integrate their compiler into their development environment (with a full IDE or even
just with a plugin for a text editor). I didn’t even mention this concern in the previous post.
As others have discussed elsewhere; Rust’s poor compile times are not the result of its advanced
type system, but of a combination of other factors. Some are essential, like the runtime
guarantees it makes (e.g. monomorphization) whereas others are accidental, like some aspects of
its module system. None of these factors would be essential for our language, so we would
carefully avoid these pitfalls.</li>
<li>It should have a runtime which suits it well to the major use cases for application programming
languages today. This means mainly being well suited to the developing for the web, both
front-end and back-end. (Being well-suited to the mobile platforms is unrealistic for a language
not sponsored by those platform developers, unfortunately.) Being well-suited to CLIs would also
be beneficial.</li>
</ol>
<p>I want to focus the rest of this post on my thoughts for evolving Rust’s ownership and borrowing
system, but before I do that I want to briefly touch on other design decisions that fell out of this
thought process:</p>
<ul>
<li>I would target WASM, and only WASM, for this language. WASM with reference types is suitable as
an environment for application programming (with shims for future extensions like properly
integrated garbage collection). This way the language designers can piggy back on the work being
done at many companies to establish WASM as a good shared VM platform, instead of being
responsible for things like platform compatibility or using the very slow LLVM. Targeting WASM
would also mean easier FFI integration into other languages that run on the same VM as WASM; that
is, other languages targeting WASM (like Rust) and JavaScript.</li>
<li>I would explore control-flow-capturing closures as a core language abstraction, similar to Kotlin.
As I wrote in <a href="https://without.boats/blog/the-problem-of-effects">an earlier blog post</a> inspired by the design on this hypothetical
language, I think these are a great way to integrate effects well with higher order function
abstractions.</li>
<li>I would provide syntactic sugar for <code>Result</code> and <code>Option</code> as the way to handle null and errors,
similar to Swift.</li>
<li>As I wrote in a previous blog post, I would provide green threads as the sole concurrency model,
with language or standard library provided channels and cells (discussed later) as the way of
sharing data between threads. How these green threads are mapped to CPUs is a matter for the
runtime you choose to run the compiled WASM in.</li>
<li>I didn’t get to the point of designing a polymorphism system; I would probably start with a
strenuous comparison of Rust’s traits and Go’s interfaces, and (knowing the other features of the
language) try to figure out what from Rust’s traits is unimportant.</li>
<li>I would be hope the language could avoid macros, which (in the case of pattern based macros) add a
second meta language to the language that advanced users need to understand, and in all cases
substantially complicate compilation.</li>
</ul>

<p>But now onto the meat of this post: the ownership and borrowing model. In my previous post I made
some points that I largely agree with still, but would probably reframe. Here’s what I wrote:</p>
<blockquote>
<p>Rust works because it enables users to write in an imperative programming style, which is the
mainstream style of programming that most users are familiar with, while avoiding to an impressive
degree the kinds of bugs that imperative programming is notorious for. As I said once, pure
functional programming is an ingenious trick to show you can code without mutation, but Rust is an
even cleverer trick to show you can just have mutation.</p>
<p>…</p>
<p><strong>Resource acquisition is initialization:</strong> Objects should manage conceptual resources like file
descriptors and sockets, and have destructors which clean up resource state when the object goes
out of scope. It should be trivial to be confident the destructor will run when the object goes
out of scope. This necesitates most of ownership, moving, and borrowing.</p>
<p><strong>Aliasable XOR mutable:</strong> The default should be that values can be mutated only if they are not
aliased, and there should be no way to introduce unsynchronized aliased mutation. However, the
language should support mutating values. The only way to get this is the rest of ownership and
borrowing, the distinction between borrows and mutable borrows and the aliasing rules between
them.</p>
<p>In other words, the core, commonly identified “hard part” of Rust - ownership and borrowing - is
essentially applicable for any attempt to make checking the correctness of an imperative program
tractable. So trying to get rid of it would be missing the real insight of Rust, and not building
on the foundations Rust has laid out.</p>
</blockquote>
<p>I still think this is Rust’s “secret sauce” and it does mean what I said: the language would have to
have ownership and borrowing. But what I’ve realized since is that there’s a very important
distinction between the cases in which users <em>want</em> these semantics and the cases where they largely
get in the way. This distinction is between types which represent <em>resources</em> and types which
represent <em>data</em>.</p>
<p>In this mental model, resources are types which represent “a thing” - something with an identity and
a state which can change with time as the program executes. In Rust, almost everything is a
resource: a String is a resource a HashMap is a resource, most user types are resources. In
contrast, data types are just “information” - a fact, which has no meaningful identity, contains no
state that evolves over time, etc. In Rust, types like integers, <code>&amp;str</code>, and so on - which all
implement <code>Copy</code> - are data types. (However, a mutable reference to those types is a resource: more
on this later.)</p>
<p>In Rust, only types which can be cloned by a mempcy can implement <code>Copy</code>. This is because Rust is
designed to encourage treating all heap memory as a <em>resource</em>, the management of which the end
user can control by selecting when the type representing that memory is dropped. This is very
valuable in the domains which Rust is intended to target. However, for higher level applications
that most programmers write, control over heap memory is not <em>usually</em> important. This is what users
mean when they want to “turn off the borrow checker” - they want to let a garbage collector figure
it out for them when this bit of data is freed, because to them it is “just data” and not a
resource.</p>
<p>This hypothetical language would lean into that distinction. Using persistent data structures (like
those from Clojure) and garbage collection, the set of types which could be treated as data types
would not be restricted in this language. The string type would be a data type, rather than
a resource; a dynamically sized array of data types would be a data type as well, as would a map
with keys and values that are data types.</p>
<p>Meanwhile, types representing IO objects would always be resource types. Collections containing
resource types would also be resource types. Composite types (like structs and enums) which contain
a resource type would also have to be a resource type. There would be an easy way to convert data
types to fully owned resource types as well; in the case of persistent data structures, converting
a data type to a resource type would be the point at which the “copy on write” operation occurs.
As a result users can use ownership semantics for things which impact global and external state
(like IO) and for cases where they know it will be an important performance optimization.</p>
<p>And the difference in how the language treats data and resources would be identical to the
difference between how Rust treats Copy and non-Copy types. Only resources would have affine
“ownership” semantics - in which moving them invalidates the previous binding. Data types would have
the standard non-linear semantics users are familiar with from most languages. This means that
writing algorithms using data types would be functionally the same as writing algorithms in other
imperative languages, easing the onboarding of users to the language and limiting their errors
related to linear types to areas where they are certain to care.</p>
<h2 id="borrowing-and-the-two-reference-types">Borrowing and the two reference types</h2>
<p>The previous discussion covers the ground of …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://without.boats/blog/revisiting-a-smaller-rust/">https://without.boats/blog/revisiting-a-smaller-rust/</a></em></p>]]>
            </description>
            <link>https://without.boats/blog/revisiting-a-smaller-rust/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24638129</guid>
            <pubDate>Wed, 30 Sep 2020 12:38:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Ruby One-Liners Cookbook]]>
            </title>
            <description>
<![CDATA[
Score 174 | Comments 29 (<a href="https://news.ycombinator.com/item?id=24637797">thread link</a>) | @asicsp
<br/>
September 30, 2020 | https://learnbyexample.github.io/learn_ruby_oneliners/one-liner-introduction.html | <a href="https://web.archive.org/web/*/https://learnbyexample.github.io/learn_ruby_oneliners/one-liner-introduction.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
        <!-- Provide site root to javascript -->
        

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        

        <!-- Set the theme before any content is loaded, prevents flash -->
        

        <!-- Hide / unhide sidebar before it is displayed -->
        

        <nav id="sidebar" aria-label="Table of contents">
            
            
        </nav>

        <div id="page-wrapper">

            <div class="page">
                
                
                

                
                
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                

                <div id="content">
                    <main>
                        
<p>This chapter will give an overview of <code>ruby</code> syntax for command line usage and some examples to show what kind of problems are typically suited for one-liners.</p>
<h2><a href="#why-use-ruby-for-one-liners" id="why-use-ruby-for-one-liners">Why use Ruby for one-liners?</a></h2>
<p>I assume you are already familiar with use cases where command line is more productive compared to GUI. See also this series of articles titled <a href="https://sanctum.geek.nz/arabesque/series/unix-as-ide/">Unix as IDE</a>.</p>
<p>A shell utility like <code>bash</code> provides built-in commands and scripting features to make it easier to solve and automate various tasks. External *nix commands like <code>grep</code>, <code>sed</code>, <code>awk</code>, <code>sort</code>, <code>find</code>, <code>parallel</code> etc can be combined to work with each other. Depending upon your familiarity with those tools, you can either use <code>ruby</code> as a single replacement or complement them for specific use cases.</p>
<p>Here's some one-liners (options will be explained later):</p>
<ul>
<li><code>ruby -e 'puts readlines.uniq' *.txt</code> — retain only one copy if lines are duplicated from the given list of input file(s)</li>
<li><code>ruby -e 'puts readlines.uniq {|s| s.split[1]}' *.txt</code> — retain only first copy of duplicate lines using second field as duplicate criteria</li>
<li><code>ruby -rcommonregex -ne 'puts CommonRegex.get_links($_)' *.md</code> — extract only the URLs, using a third-party <a href="https://github.com/talyssonoc/CommonRegexRuby">CommonRegexRuby</a> library</li>
<li><a href="https://stackoverflow.com/questions/63954081/bash-remove-duplicate-of-key-values-with-preserving-order">stackoverflow: merge duplicate key values while preserving order</a> — a recent Q&amp;A that I answered with a simpler <code>ruby</code> solution compared to <code>awk</code></li>
</ul>
<p>The main advantage of <code>ruby</code> over tools like <code>grep</code>, <code>sed</code> and <code>awk</code> includes feature rich regular expression engine, standard library and third-party libraries. If you don't already know the syntax and idioms for <code>sed</code> and <code>awk</code>, learning command line options for <code>ruby</code> would be the easier option. The main disadvantage is that <code>ruby</code> is likely to be slower compared to those tools.</p>
<h2><a href="#command-line-options" id="command-line-options">Command line options</a></h2>
<table><thead><tr><th><strong>Option</strong></th><th><strong>Description</strong></th></tr></thead><tbody>
<tr><td><code>-0[octal]</code></td><td>specify record separator (<code>\0</code>, if no argument)</td></tr>
<tr><td><code>-a</code></td><td>autosplit mode with <code>-n</code> or <code>-p</code> (splits <code>$_</code> into <code>$F</code>)</td></tr>
<tr><td><code>-c</code></td><td>check syntax only</td></tr>
<tr><td><code>-Cdirectory</code></td><td>cd to directory before executing your script</td></tr>
<tr><td><code>-d</code></td><td>set debugging flags (set <code>$DEBUG</code> to true)</td></tr>
<tr><td><code>-e 'command'</code></td><td>one line of script. Several <code>-e</code>'s allowed. Omit [programfile]</td></tr>
<tr><td><code>-Eex[:in]</code></td><td>specify the default external and internal character encodings</td></tr>
<tr><td><code>-Fpattern</code></td><td><code>split()</code> pattern for autosplit (<code>-a</code>)</td></tr>
<tr><td><code>-i[extension]</code></td><td>edit <code>ARGV</code> files in place (make backup if extension supplied)</td></tr>
<tr><td><code>-Idirectory</code></td><td>specify <code>$LOAD_PATH</code> directory (may be used more than once)</td></tr>
<tr><td><code>-l</code></td><td>enable line ending processing</td></tr>
<tr><td><code>-n</code></td><td>assume <code>'while gets(); ... end'</code> loop around your script</td></tr>
<tr><td><code>-p</code></td><td>assume loop like <code>-n</code> but print line also like <code>sed</code></td></tr>
<tr><td><code>-rlibrary</code></td><td>require the library before executing your script</td></tr>
<tr><td><code>-s</code></td><td>enable some switch parsing for switches after script name</td></tr>
<tr><td><code>-S</code></td><td>look for the script using PATH environment variable</td></tr>
<tr><td><code>-v</code></td><td>print the version number, then turn on verbose mode</td></tr>
<tr><td><code>-w</code></td><td>turn warnings on for your script</td></tr>
<tr><td><code>-W[level=2|:category]</code></td><td>set warning level; 0=silence, 1=medium, 2=verbose</td></tr>
<tr><td><code>-x[directory]</code></td><td>strip off text before #!ruby line and perhaps cd to directory</td></tr>
<tr><td><code>--jit</code></td><td>enable JIT with default options (experimental)</td></tr>
<tr><td><code>--jit-[option]</code></td><td>enable JIT with an option (experimental)</td></tr>
<tr><td><code>-h</code></td><td>show this message, <code>--help</code> for more info</td></tr>
</tbody></table>
<p>This chapter will show examples with <code>-e</code>, <code>-n</code>, <code>-p</code> and <code>-a</code> options. Some more options will be covered in later chapters, but not all of them are discussed in this book.</p>
<h2><a href="#executing-ruby-code" id="executing-ruby-code">Executing Ruby code</a></h2>
<p>If you want to execute a <code>ruby</code> program file, one way is to pass the filename as argument to the <code>ruby</code> command.</p>
<pre><code>$ echo 'puts "Hello Ruby"' &gt; hello.rb
$ ruby hello.rb
Hello Ruby
</code></pre>
<p>For short programs, you can also directly pass the code as an argument to the <code>-e</code> option.</p>
<pre><code>$ ruby -e 'puts "Hello Ruby"'
Hello Ruby

$ # multiple statements can be issued separated by ;
$ ruby -e 'x=25; y=12; puts x**y'
59604644775390625
$ # or use -e option multiple times
$ ruby -e 'x=25' -e 'y=12' -e 'puts x**y'
59604644775390625
</code></pre>
<h2><a href="#filtering" id="filtering">Filtering</a></h2>
<p><code>ruby</code> one-liners can be used for filtering lines matched by a regexp, similar to <code>grep</code>, <code>sed</code> and <code>awk</code>. And similar to many command line utilities, <code>ruby</code> can accept input from both <code>stdin</code> and file arguments.</p>
<pre><code>$ # sample stdin data
$ printf 'gate\napple\nwhat\nkite\n'
gate
apple
what
kite

$ # print all lines containing 'at'
$ # same as: grep 'at' and sed -n '/at/p' and awk '/at/'
$ printf 'gate\napple\nwhat\nkite\n' | ruby -ne 'print if /at/'
gate
what

$ # print all lines NOT containing 'e'
$ # same as: grep -v 'e' and sed -n '/e/!p' and awk '!/e/'
$ printf 'gate\napple\nwhat\nkite\n' | ruby -ne 'print if !/e/'
what
</code></pre>
<p>By default, <code>grep</code>, <code>sed</code> and <code>awk</code> will automatically loop over input content line by line (with <code>\n</code> as the line distinguishing character). The <code>-n</code> or <code>-p</code> option will enable this feature for <code>ruby</code>. As seen before, the <code>-e</code> option accepts code as command line argument. Many shortcuts are available to reduce the amount of typing needed.</p>
<p>In the above examples, a regular expression (defined by the pattern between a pair of forward slashes) has been used to filter the input. When the input string isn't specified in a conditional context (for example: <code>if</code>), the test is performed against global variable <code>$_</code>, which has the contents of the input line (the correct term would be input <strong>record</strong>, see <a href="https://learnbyexample.github.io/learn_ruby_oneliners/record-separators.html#record-separators">Record separators</a> chapter). To summarize, in a conditional context:</p>
<ul>
<li><code>/regexp/</code> is a shortcut for <code>$_ =~ /regexp/</code></li>
<li><code>!/regexp/</code> is a shortcut for <code>$_ !~ /regexp/</code></li>
</ul>
<p><code>$_</code> is also the default argument for <code>print</code> method, which is why it is generally preferred in one-liners over <code>puts</code> method. More such defaults that apply to the <code>print</code> method will be discussed later.</p>
<blockquote>
<p><img src="https://learnbyexample.github.io/learn_ruby_oneliners/images/info.svg" alt="info"> See <a href="https://ruby-doc.org/core-2.7.1/doc/globals_rdoc.html">ruby-doc: Pre-defined global variables</a> for documentation on <code>$_</code>, <code>$&amp;</code>, etc.</p>
</blockquote>
<p>Here's an example with file input instead of <code>stdin</code>.</p>
<pre><code>$ cat table.txt
brown bread mat hair 42
blue cake mug shirt -7
yellow banana window shoes 3.14

$ # same as: grep -oE '[0-9]+$' table.txt
$ ruby -ne 'puts $&amp; if /\d+$/' table.txt
42
7
14
</code></pre>
<blockquote>
<p><img src="https://learnbyexample.github.io/learn_ruby_oneliners/images/info.svg" alt="info"> The <a href="https://github.com/learnbyexample/learn_ruby_oneliners/tree/master/example_files">learn_ruby_oneliners repo</a> has all the files used in examples.</p>
</blockquote>
<h2><a href="#substitution" id="substitution">Substitution</a></h2>
<p>Use <code>sub</code> and <code>gsub</code> methods for search and replace requirements. By default, these methods operate on <code>$_</code> when the input string isn't provided. For these examples, <code>-p</code> option is used instead of <code>-n</code> option, so that the value of <code>$_</code> is automatically printed after processing each input line.</p>
<pre><code>$ # for each input line, change only first ':' to '-'
$ # same as: sed 's/:/-/' and awk '{sub(/:/, "-")} 1'
$ printf '1:2:3:4\na:b:c:d\n' | ruby -pe 'sub(/:/, "-")'
1-2:3:4
a-b:c:d

$ # for each input line, change all ':' to '-'
$ # same as: sed 's/:/-/g' and awk '{gsub(/:/, "-")} 1'
$ printf '1:2:3:4\na:b:c:d\n' | ruby -pe 'gsub(/:/, "-")'
1-2-3-4
a-b-c-d
</code></pre>
<p>You might wonder how <code>$_</code> is modified without the use of <code>!</code> methods. The reason is that these methods are part of Kernel (see <a href="https://ruby-doc.org/core-2.7.1/Kernel.html">ruby-doc: Kernel</a> for details) and are available only when <code>-n</code> or <code>-p</code> options are used.</p>
<ul>
<li><code>sub(/regexp/, repl)</code> is a shortcut for <code>$_.sub(/regexp/, repl)</code> and <code>$_</code> will be updated if substitution succeeds</li>
<li><code>gsub(/regexp/, repl)</code> is a shortcut for <code>$_.gsub(/regexp/, repl)</code> and <code>$_</code> gets updated if substitution succeeds</li>
</ul>
<blockquote>
<p><img src="https://learnbyexample.github.io/learn_ruby_oneliners/images/info.svg" alt="info"> This book assumes you are already familiar with regular expressions. If not, you can check out my free <a href="https://learnbyexample.github.io/Ruby_Regexp/">Ruby Regexp</a> book.</p>
</blockquote>
<h2><a href="#field-processing" id="field-processing">Field processing</a></h2>
<p>Consider the sample input file shown below with fields separated by a single space character.</p>
<pre><code>$ cat table.txt
brown bread mat hair 42
blue cake mug shirt -7
yellow banana window shoes 3.14
</code></pre>
<p>Here's some examples that is based on specific field rather than the entire line. The <code>-a</code> option will cause the input line to be split based on whitespaces and the field contents can be accessed using <code>$F</code> global variable. Leading and trailing whitespaces will be suppressed and won't result in empty fields. More details is discussed in <a href="https://learnbyexample.github.io/learn_ruby_oneliners/field-separators.html#default-field-separation">Default field separation</a> section.</p>
<pre><code>$ # print the second field of each input line
$ # same as: awk '{print $2}' table.txt
$ ruby -ane 'puts $F[1]' table.txt
bread
cake
banana

$ # print lines only if last field is a negative number
$ # same as: awk '$NF&lt;0' table.txt
$ ruby -ane 'print if $F[-1].to_f &lt; 0' table.txt
blue cake mug shirt -7

$ # change 'b' to 'B' only for the first field
$ # same as: awk '{gsub(/b/, "B", $1)} 1' table.txt
$ ruby -ane '$F[0].gsub!(/b/, "B"); puts $F * " "' table.txt
Brown bread mat hair 42
Blue cake mug shirt -7
yellow banana window shoes 3.14
</code></pre>
<h2><a href="#begin-and-end" id="begin-and-end">BEGIN and END</a></h2>
<p>You can use a <code>BEGIN{}</code> block when you need to execute something before input is read and a <code>END{}</code> block to execute something after all of the input has been processed.</p>
<pre><code>$ # same as: awk 'BEGIN{print "---"} 1; END{print "%%%"}'
$ # note the use of ; after BEGIN block
$ seq 4 | ruby -pe 'BEGIN{puts "---"}; END{puts "%%%"}'
---
1
2
3
4
%%%
</code></pre>
<h2><a href="#env-hash" id="env-hash">ENV hash</a></h2>
<p>When it comes to automation and scripting, you'd often need to construct commands that can accept input from user, file, output of a shell command, etc. As mentioned before, this book assumes <code>bash</code> as the shell being used. To access environment variables of the shell, you can call the special hash variable <code>ENV</code> with the name of the environment variable as a string key.</p>
<pre><code>$ # existing environment variable
$ # output shown here is for my machine, would differ for you
$ ruby -e 'puts ENV["HOME"]'
/home/learnbyexample
$ ruby -e 'puts ENV["SHELL"]'
/bin/bash

$ # defined along with ruby command
$ # note that the variable is placed before the shell command
$ word='hello' ruby -e 'puts ENV["word"]'
hello
$ # the input characters are preserved as is
$ ip='hi\nbye' ruby -e 'puts ENV["ip"]'
hi\nbye
</code></pre>
<p>Here's another example when a regexp is passed as an environment variable content.</p>
<pre><code>$ cat word_anchors.txt
sub par
spar
apparent effort
two spare computers
cart part tart mart

$ # assume 'r' is a shell variable that has to be passed to the ruby command
$ r='\Bpar\B'
$ rgx="$r" ruby -ne 'print if /#{ENV["rgx"]}/' word_anchors.txt
apparent effort
two spare computers
</code></pre>
<blockquote>
<p><img src="https://learnbyexample.github.io/learn_ruby_oneliners/images/info.svg" alt="info"> As an example, see my repo <a href="https://github.com/learnbyexample/command_help/blob/master/ch">ch: command help</a> for a practical shell script, where commands are constructed …</p></blockquote></main></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://learnbyexample.github.io/learn_ruby_oneliners/one-liner-introduction.html">https://learnbyexample.github.io/learn_ruby_oneliners/one-liner-introduction.html</a></em></p>]]>
            </description>
            <link>https://learnbyexample.github.io/learn_ruby_oneliners/one-liner-introduction.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24637797</guid>
            <pubDate>Wed, 30 Sep 2020 11:57:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Spotify’s Failed Squad Goals]]>
            </title>
            <description>
<![CDATA[
Score 24 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24637656">thread link</a>) | @cocoflunchy
<br/>
September 30, 2020 | https://www.jeremiahlee.com/posts/failed-squad-goals/?repost | <a href="https://web.archive.org/web/*/https://www.jeremiahlee.com/posts/failed-squad-goals/?repost">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
<!-- Spread 1 -->
<section>
<section>
<video autoplay="true" muted="true" loop="true">
<source srcset="https://www.jeremiahlee.com/posts/failed-squad-goals/cover.png" media="(prefers-reduced-motion: reduce)">
<source src="https://www.jeremiahlee.com/posts/failed-squad-goals/cover.webm" type="video/webm">
<source src="https://www.jeremiahlee.com/posts/failed-squad-goals/cover.m4v" type="video/mp4">
<img src="https://www.jeremiahlee.com/posts/failed-squad-goals/cover.png" alt="Taylor Swift and her squad walking away from an explosion in front of the Spotify office at Birger Jarlsgatan.">
</video>
</section>
<section>
<div>

<p><b>Spotify</b> doesn’t use <em>“the Spotify model”</em><br>and neither should you.</p>
<p>By Jeremiah Lee</p>
<p>Sunday, April 19, 2020 • <a href="https://anchor.fm/jeremiah-oral-lee/episodes/Spotifys-Failed-SquadGoals-edia0p">Listen</a> •&nbsp;<a href="https://oyomy.fr/2020/04/l-echec-du-modele-spotify/" hreflang="fr">En français</a> •&nbsp;<a href="https://jp.quora.com/q/agile/Spotify%E3%81%AF-Spotify%E3%83%A2%E3%83%87%E3%83%AB-%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E3%81%84%E3%81%AA%E3%81%84" hreflang="ja">日本語で</a> • <a href="https://flavioclesio.com/2020/05/18/o-modelo-de-squadgoals-do-spotify-falhou/" hreflang="pt-br">Português (Brasil)</a></p>
</div>
</section>
</section>
<!-- Spread 2 -->
<section>
<section>
<div>
<p>Of all the allures of startup culture, few are more desireable than the speed and nimbleness of a small team. Maintaining that feeling as a company grows is a challenge. In 2012, Spotify shared its way of working and suggested it had figured it out.<sup><a href="#1">1</a></sup></p>
<p>I was excited to see <em>the Spotify model</em> in action when I interviewed for a product management role at its Stockholm headquarters in 2017. However, the recruiter surprised me before the first interview. She cautioned me to not expect Spotify to be an <a href="https://en.wikipedia.org/wiki/Agile_software_development">Agile</a> utopia.</p>
<p>I joined the company after it had tripled in size to 3,000 people over 18 months. I learned the famed squad model was only ever aspirational and never fully implemented. I witnessed organizational chaos as the company’s leaders incrementally transitioned to more traditional management structures.</p>
<p>When I asked my coworkers why the content was not removed or updated to reflect reality, I never got a good answer. Many people ironically thought the posts were great for recruiting. I no longer work at Spotify, so I am sharing my experience to set the record straight. The Spotify squad model failed Spotify and it will fail your company too.</p>
</div>
</section>
<section>
<div>
<h3>But you don’t have to take my word for it.</h3>
<p>The co-author of the Spotify model<sup><a href="#2">2</a></sup> and multiple agile coaches who worked at Spotify have been telling people to not copy it for years. Unfortunately, truth doesn’t spread as quickly or as widely as an idea people want to believe in.</p>
<blockquote>
<p>“Even at the time we wrote it, we weren’t doing it. It was part ambition, part approximation. People have really struggled to copy something that didn’t really exist.”</p>
</blockquote>
<p>—Joakim Sundén, agile coach at Spotify&nbsp;2011–2017<sup><a href="#4">4</a></sup></p>
<blockquote>
<p><em>“It worries me when people look at what we do and think it’s a framework they can just copy and implement.</em> … We are really trying hard now to emphasize we have problems as well. It’s not all ‘shiny and everything works well and all our squads are super amazing’.”</p>
</blockquote>
<p>—Anders Ivarsson, co-author of the Spotify&nbsp;whitepaper<sup><a href="#3">3</a></sup></p>
</div>
</section>
</section>
<!-- Spread 3 -->
<section>
<section>
<div>
<h2>Recap</h2>
<p>You can read and watch the <a href="#1">original content</a> in less than 30 minutes or <a href="#spread-4">skip to the next section</a> if you are familiar. Here is a brief summary.</p>
<p>Spotify had teams it called <em>squads</em> because it sounded cooler (not joking). A group of teams were organized into a department called a <em>tribe</em>. Each team was intended to be an autonomous mini-startup, with a product manager acting as mini-CEO for a feature area. The teams had designers and software engineers with a range of specializations. The intent was that a team should have every skill necessary without needing to rely on another team for success.</p>
<p>Product managers had a traditional management structure. A product manager for a team reported to their department’s product director (<em>“tribe lead”</em>). Same for designers. Software engineers, however, were managed outside of the team structure.</p>
</div>
</section>
<section>
<div>
<p><em>“Chapter leads”</em> managed software engineers specializing in a specific type of software development across the department. For example, all of the software engineers working on backend APIs across all the teams within the department would have one manager and all of the Android mobile engineers in the department would have a different manager. The intent was to allow engineers to be moved between teams within the department to best meet business requirements without them having to change managers.</p>
<figure>
<video autoplay="true" muted="true" loop="true">
<source src="https://www.jeremiahlee.com/posts/failed-squad-goals/scaling-spotify-squads.webm" type="video/webm">
<source src="https://www.jeremiahlee.com/posts/failed-squad-goals/scaling-spotify-squads.m4v" type="video/mp4">
<img src="https://www.jeremiahlee.com/posts/failed-squad-goals/scaling-spotify-squads.png" alt="Spotify matrix organization diagram. A department is called a tribe and has multiple squads, another name for a team. Each squad has a product owner, another name for a product manager. Software engineers of the same discipline across multiple teams are called a chapter and managed by a chapter lead, another name for an engineering manager.">
</video>
<figcaption>Spotify tried a long-lived matrix team structure with unusual word choices.<sup><a href="#1">1</a></sup> It did not work well.</figcaption>
</figure>
</div>
</section>
</section>
<!-- Spread 4 -->
<section id="spread-4">
<section>

</section>
<section id="matrix-management-sucks">
<div>
<h3>Matrix management solved the wrong problem</h3>
<p>The “full stack” agile team worked well, but the matrix management of software engineers introduced more problems than it solved.</p>
<ul>
<li><p>Teams at Spotify were rather long-lived. The benefit of not having to change manager when moving to another team was limited.</p></li>
<li><p>Engineering managers in this model had little responsibility beyond the career development of the people they managed. Even then, their ability to help with interpersonal people skills development was limited. An engineering manager would not manage enough of the other people on the team or be involved enough in the daily context to independently assess conflict within the team.</p></li>
</ul>
</div>
</section>
</section>
<!-- Spread 5 -->
<section>
<section>
<div>
<ul>
<li><p>Without a single engineering manager responsible for the engineers on a team, the product manager lacked an equivalent peer—the mini-CTO to their mini-CEO role. There was no single person accountable for the engineering team’s delivery or who could negotiate prioritization of work at an equivalent level of responsibility.</p><p>When disagreements within the engineering team arose, the product manager needed to negotiate with all of the engineers on the team. If the engineers could not reach a consensus, the product manager needed to escalate to as many engineering managers as there were engineering specializations within the team. A team with backend, Web app, and mobile app engineers would have at least 3 engineering managers who might need to get involved. If those engineering managers could not reach a consensus, a single team’s issue would have to escalate to the department’s engineering director.</p></li>
</ul>
</div></section>
<section id="matrix-management-sucks">
<div>
<blockquote>
<p>“Chapter leads are servant-leaders who help you grow as an individual. They don’t really work with any team. They have direct reports on all the teams. They don’t have really any accountability for the delivery. They aren’t taking that responsibility. It’s easy to see the product owner as the manager for the team.”</p>
</blockquote>
<p>—Joakim Sundén, agile coach at Spotify<sup><a href="#4">4</a></sup></p>
<p><strong>Learn from Spotify’s mistakes:</strong></p>
<ul>
<li>A product—design—engineering team typically contains more engineers than designers or product managers. Having a single engineering manager for the engineers on the team creates an accountable escalation path for conflict within the team.</li>
<li>Product managers should have an equivalent peer for engineering. Product managers should be accountable for the prioritization of work. Engineering managers should be accountable for the engineers’ execution, which includes being able to negotiate speed and quality tradeoffs with the product manager.</li>
</ul>
</div>
</section>
</section>
<!-- Spread 6 -->
<section>
<section id="too-much-autonomy">
<div>
<h3>Spotify fixated on team autonomy</h3>
<p>When a company is small, teams have to do a wide range of work to deliver and have to shift initiatives frequently. As a company grows from startup to scale-up, duplicated functions across teams move to new teams dedicated to increasing organization efficiency by reducing duplication. With more teams, the need for a team to shift initiative decreases in frequency. Both of these changes allow for teams to think more deeply and long term about the problems they are scoped to solve. Faster iteration, however, is not guaranteed. Every responsibility a team cedes to increase its focus becomes a new cross-team dependency.</p>
<p>Spotify did not define a common process for cross-team collaboration. Allowing every team to have a unique way of working meant each team needed a unique way of engagement when collaborating. Overall organization productivity suffered.</p>
<p>The Spotify model was documented when Spotify was a much smaller company. It was supposed to be a multiple part series, according to Anders Ivarsson. Autonomy made the first cut, but the parts on alignment and accountability were never completed.</p>
</div>
</section>
<section>
<div>
<p><strong>Learn from Spotify’s mistakes:</strong></p>
<ul>
<li>Autonomy requires alignment. Company priorities must be defined by leadership. Autonomy does not mean teams get to do whatever they want.</li>
<li>Processes for cross-team collaboration must be defined. Autonomy does not mean leaving teams to self-organize every problem.</li>
<li>How success is measured must be defined by leadership so people can effectively negotiate cross-team dependency prioritization.</li>
<li>Autonomy requires accountability. Product management is accountable for value. The team is accountable for delivering ‘done’ increments. Mature teams can justify their independence with their ability to articulate business value, risk, learning, and the next optimal move.<sup><a href="#6">6</a></sup></li>
</ul>
</div>
</section>
</section>
<!-- Spread 7 -->
<section>
<section id="too-much-autonomy">
<div>
<blockquote>
<p>“If I were to do one thing differently, I would say we should not be focusing so much on autonomy.</p>
<p>“Every time you have a new team, they have to reinvent the wheel in how they should be working. Maybe, just maybe, we should have a ‘minimum viable agility’. You start with that. You are free to opt out, but people shouldn’t have to opt-in all the time.</p>
<p>“At what point do you start inserting this process? Probably when it’s too late.”</p>
</blockquote>
<p>—Joakim Sundén, agile coach at Spotify<sup><a href="#4">4</a></sup></p>
</div>
</section>
<section>
<div>
<blockquote>
<p>“Henrik Kniberg talked about how we're not that good at large initiatives and we’re still not that good at large initiatives.</p>
<p>“If you have inconsistent ways of working, it’s more difficult for people to move. If it’s more difficult for people to move, it’s more likely you have inconsistent ways of working. It will just reinforce until all of a sudden, you’re not really working for the same company anymore. You’re working for these kind of weird subcultures.”</p>
</blockquote>
<p>—Jason Yip, agile coach at Spotify<br>2015–time of writing<sup><a href="#5">5</a></sup></p>
</div>
</section>
</section>
<!-- Spread 8 -->
<section>
<section id="collaboration-is-a-competency">
<div>
<h3>Collaboration was an assumed competency</h3>
<p>While Spotify gave teams control over their way of working, many people did not have a basic understanding of Agile practices. This resulted in teams iterating through process tweaks in blind hope of finding the combination that would help them improve their delivery. People lacked a common language to effectively discuss the process problems, the education to solve them, and the experience to evaluate performance. It was not really <em>agile</em>. It was just <em>not-waterfall</em>.</p>
<p><em>“Agile coaches”</em> were internal consultants Spotify provided teams to teach and suggest process improvements. While well-intentioned, there were not enough coaches to help every team. A coach’s engagement with a team was rarely long enough to span a project’s completion to help a team evaluate performance. More so, they were not accountable for anything.</p>
</div>
</section>
<section>
<div>
<blockquote>
<p>“Control without competence is chaos.”</p>
</blockquote>
<p>—L. David Marquet, <a href="https://www.indiebound.org/book/9781591846406"><cite>Turn the Ship Around!</cite></a></p>
<p><strong>Learn from Spotify’s …</strong></p></div></section></section></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.jeremiahlee.com/posts/failed-squad-goals/?repost">https://www.jeremiahlee.com/posts/failed-squad-goals/?repost</a></em></p>]]>
            </description>
            <link>https://www.jeremiahlee.com/posts/failed-squad-goals/?repost</link>
            <guid isPermaLink="false">hacker-news-small-sites-24637656</guid>
            <pubDate>Wed, 30 Sep 2020 11:35:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Six Figures in 6 days]]>
            </title>
            <description>
<![CDATA[
Score 587 | Comments 274 (<a href="https://news.ycombinator.com/item?id=24637405">thread link</a>) | @brunojppb
<br/>
September 30, 2020 | https://tr.af/6 | <a href="https://web.archive.org/web/*/https://tr.af/6">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-lax-preset="fadeIn">
        <p>This title is a little misleading.</p>

<p>Sure, it took me 6 days to amass 3,626 sales for a total of $101,528, but like any overnight success, it was years in the making. In my case, about 7 years.</p>

<p>In 2013, the jailbreaking days of iOS were in full effect. Inspired, I created and sold an icon set titled 'Greyish HD' on the Cydia store for $0.99. It was the first few dollars I ever made on the internet. I think I made a total of $17, and it was magical.</p>

<p>It wasn't about the $17, obviously; it was how I earned it. I spent time creating something that I thought was cool, then hit publish. After a few days, random strangers I've never met before decided to spend their hard-earned cents on my icon set, oftentimes while I was working on something else entirely. Without realizing it, I just discovered digital content leverage: <strong>create something once with sufficient effort, then sell it repeatedly with minimal effort.</strong></p>

<p>Fast forward 7 years later, minus 6 days. I saw some people sharing screenshots of their iPhones after discovering that iOS 14 now allows you to add custom icons to your home screen using the Siri Shortcuts app. This was the first time you can really customize iOS, and it was catching on. </p>

<p>This is something people have been doing for years on jailbroken iOS and Android devices, except this time, you can do it natively on iOS.</p>

<p>As soon as I noticed the hype, I put together some <a href="https://icons.tr.af/">icons</a> in my own style, downloaded some widgets, and tried it all out. I thought it looked cool, so I shared a screenshot of it on Twitter. Right away, people started asking about the icons in the screenshot. So I quickly packaged them, uploaded them to <a href="https://gumroad.com/">Gumroad</a>, and embedded them on a <a href="https://notion.so/">Notion</a> site using <a href="https://super.so/">Super</a>. All of this took about two hours.</p>

<p>The next day, the tweet had hundreds of retweets, thousands of likes, and over 100k impressions. The day after that, almost a million. The next thing I knew, it was everywhere. My icons got published on notable tech sites like <a href="https://www.cultofmac.com/723490/ios-14-iphone-home-screen-customization/">Cult of Mac</a>, <a href="https://www.imore.com/people-are-making-real-money-selling-icon-sets-ios-14-home-screens">iMore</a>, <a href="https://allthingst3ch.com/ios14homescreen">AllThingsTech</a>, and <a href="https://gridfiti.com/aesthetic-ios-home-screen-ideas">Gridfiti</a>. I think at this time I was around the $6k mark in sales.</p>

<p>Then, MKBHD happened. He shared a <a href="https://youtu.be/cH66LWWluVE">video</a> about all of this—using my icons for his setup, and linked them in the description. The next thing I knew, I was making $28 what felt like every 28 seconds. My phone turned into the ultimate dopamine dispenser (if it wasn't already). I had to disable notifications.</p>

<p>The day after, sales jumped from $6k to about $40k, and during the time of this writing, sales are at $116,147 from 4,188 customers.</p>

<p>All from one. Single. <a href="https://twitter.com/traf/status/1307707156788060160">Tweet</a>.</p>

<p>The right content, posted at the right time, can create unimaginable results. Although there's likely plenty of other variables that went into making this work, there are a few key insights that I think increased my odds.</p>

<h3>Publish often</h3>

<p>Continually putting things out into the world creates proof of work. The difference between where you are and where you want to be could be as little as sharing what you build, or even what you know.</p>

<p>There's only so much we can control once pushing something out into the world, but publishing often will increase your odds at finding something that sticks. They say that fortune favors the bold. In the internet age, the bold are those that aren't afraid to publish their work for the world to see. The internet is a never-ending stream of content, the idea of being annoying or over-sharing is only an idea that you invent to stop you from sharing. </p>

<h3>Act immediately</h3>

<p>I could have easily ignored the requests for the icons, or directed people to icon sets that already existed, but that would be a missed opportunity. Since I've done similar icon sets so many times already in the past, I was ready for it. Acting quickly is crucial if you plan on riding the wave. </p>

<h3>Be transparent</h3>

<p>I shared <a href="https://twitter.com/traf/status/1308158718975111175">this tweet</a> the day after posting my home screen with details of all the hype, and it created even more hype. It brought another 540,000 impressions and added fuel to an already growing fire. <strong>Share what works, and share what doesn't.</strong> Transparency is visibility.</p>

<h3>Have a clear schedule</h3>

<p>A lot of this was happening on a Monday morning. If I were working a more traditional job, or had time commitments that I had previously scheduled, I wouldn't have been able to act on my ideas as quickly as I did. I get that not everyone has the privilege of a flexible schedule, but almost anyone can start to move towards it if they want it badly enough. Freedom of a clear schedule allowed me to take action as soon as the inspiration hit, which was incredibly important to maximize exposure. Inspiration is a productivity-multiplier, but it's perishable—so act quickly.</p>

<h3>Charge more</h3>

<p>If I would have asked anyone what to price these at, most would have said $2, or maybe $5. One thing I knew for sure was that the people most likely to buy these were not the same people who were jailbreaking their iOS devices in the past. These are first time iOS customizers, so there's no notion of what an iOS icon set should be priced at.</p>

<h3>Do it for the art</h3>

<p>If I would have done this exclusively for the goal of making money, I'm convinced it wouldn't have worked nearly as well as it did. I see copycats showing up everywhere. They see success, and try to replicate exactly how it happened, but it won't work. At least not nearly as well. The reasons for this post is not to teach you how to sit on the edge of your seat, foaming at the mouth at the chance to exploit the next big internet trend, but it's to push you to keep working at the things you enjoy, share them with the world, and letting the internet do the rest.</p>

<h3>Aftermath</h3>

<p>So what came of all this? I suspect this is why most of you are here reading this, so here's a recap of everything that came from last week in numbers, from the time of this writing:</p>

<h4>Short-term metrics:</h4>

<ul><li>4,188 total icon sales</li><li>$116,147 in revenue</li><li>97% profit margins</li><li>6.2m impression</li><li>216,800 visitors</li></ul>

<h4>Long-term metrics:</h4>

<ul><li>5,216 email subscribers</li><li>4,620 Twitter followers</li><li>180 <a href="https://super.so/">Super</a> customers</li></ul>

<h3>What's next?</h3>

<p>In the same way that creating icon sets 7 years ago prepared me for last week, this whole experiment has likely prepared me for what's to come years from now.</p>

<p>The best part about this is the freedom its bought me, to keep building things that'll create even more freedom. <strong>Spending time on things that will buy you time is always a good use of it.</strong> When people buy into what ever it is you're selling, they're not only giving you money, but they're contributing to your future freedom.</p>

<p>The market will decide a huge part of what comes after sharing something, so continually increase your odds by building, publishing, then repeating. Try many things once to figure out what you want to do twice. Figure out what you've got stored in your brain that can be of value to others, then share it with the world. You might be surprised of what comes of it.</p>

<p>Thanks for reading.</p>

<p>PS. For proof that I did this in 2013, here's my <a href="https://deviantart.com/jtraf">DeviantArt profile</a>.<br>PPS. For details about the Notion/Super/Gumroad combo: <a href="https://www.makerpad.co/deep-dives/the-community-creator-how-to-run-a-low-cost-membership-community-or-creator-business-with-circle-memberspace-gumroad-notion-super-so">Makerpad deep-dive</a>.</p>

<p>— <a href="https://twitter.com/traf">@traf</a></p>
      </div></div>]]>
            </description>
            <link>https://tr.af/6</link>
            <guid isPermaLink="false">hacker-news-small-sites-24637405</guid>
            <pubDate>Wed, 30 Sep 2020 10:59:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[It Is Never a Compiler Bug Until It Is]]>
            </title>
            <description>
<![CDATA[
Score 236 | Comments 128 (<a href="https://news.ycombinator.com/item?id=24636326">thread link</a>) | @nullc
<br/>
September 30, 2020 | http://r6.ca/blog/20200929T023701Z.html | <a href="https://web.archive.org/web/*/http://r6.ca/blog/20200929T023701Z.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p>Last week I was trying to add some <a href="https://github.com/bitcoin-core/secp256k1/pull/822/commits/aa833603a6b4c947c21da04aeac40d80444ebcc1#diff-b04459e37839cd223176618536295715R425">testing code to libsecp256k1</a> and I was pulling out my hair trying to get it to work.
No amount of <code>printf</code> was working to illuminate what I was doing wrong.
Finally, out of desperation, I thought I would do a quick check to see if there are any compiler bugs related to <code>memcmp</code>, and lo and behold, I found <a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95189">GCC bug #95189: memcmp being wrongly stripped like strcmp</a>.</p><p>Honestly this was a pretty horrifying bug to read about.
Under some circumstances GCC 9 and 10 will cause <code>memcmp</code> to return an incorrect value when one of the inputs is statically known array that contains <code>NULL</code> bytes.
As I rushed to <a href="https://gist.githubusercontent.com/roconnor/2b8e22e829ed80088ed6690cc3c7f3a8/raw/455571a6d9053c597c1585debe6f9dbd6af85071/gistfile1.txt">recompile my computer system using GCC 8</a>, I contemplated the vast consequences of such a bug could be, and pondered how it was possible that computers could function at all.</p><p>However over the week, with the help of my colleagues, we managed to get a better understanding of the scope of the bug.
The bug can only convert non-zero values to zero values.
The static array needs to have a <code>NULL</code> byte within the first 4 bytes.
Most importantly, the <code>memcmp</code> result must not immediately be compared to <code>0</code> for equality or inequality, or any equivalent test.
A different code path is taken in the compiler in that case.
That explained why computers were still functioning.
I expect the vast majority of the uses of <code>memcmp</code> does an immediate test for equality with <code>0</code>.</p><p>I still wondered though, how much code was being affected. My colleague Tim suggested that it would be possible to instrument GCC to emit a message when it was about to miscompile a program.
Together we came up with <a href="https://gcc.gnu.org/bugzilla/attachment.cgi?id=49276&amp;action=diff">a patch</a> to GCC 9 and 10 that would print a debugging message.
Once again, I recompiled my entire system, to see what GCC was miscompiling.
This is what I found:</p><ul>
<li><a href="https://github.com/unicode-org/icu/blob/4fb47b12a70737ee12326220e71c2d73c5ec658f/icu4c/source/common/uniset_props.cpp#L709">https://github.com/unicode-org/icu/blob/4fb47b12a70737ee12326220e71c2d73c5ec658f/icu4c/source/common/uniset_props.cpp#L709</a></li>
<li><a href="https://github.com/xiph/flac/blob/ce6dd6b5732e319ef60716d9cc9af6a836a4011a/src/flac/decode.c#L1310">https://github.com/xiph/flac/blob/ce6dd6b5732e319ef60716d9cc9af6a836a4011a/src/flac/decode.c#L1310</a></li>
<li><a href="https://github.com/torvalds/linux/blob/fb0155a09b0224a7147cb07a4ce6034c8d29667f/drivers/atm/zatm.c#L1172">https://github.com/torvalds/linux/blob/fb0155a09b0224a7147cb07a4ce6034c8d29667f/drivers/atm/zatm.c#L1172</a></li>
<li><a href="https://github.com/nss-dev/nss/blob/1f3746f5107535a47bb4e3969f561e1bd1314bab/gtests/pk11_gtest/pk11_chacha20poly1305_unittest.cc#L425">https://github.com/nss-dev/nss/blob/1f3746f5107535a47bb4e3969f561e1bd1314bab/gtests/pk11_gtest/pk11_chacha20poly1305_unittest.cc#L425</a></li>
<li><a href="https://github.com/GNOME/glib/blob/010569b3734f864fcf584f771915b78bd391eb5f/glib/tests/refstring.c#L70">https://github.com/GNOME/glib/blob/010569b3734f864fcf584f771915b78bd391eb5f/glib/tests/refstring.c#L70</a></li>
<li><a href="https://github.com/heimdal/heimdal/blob/7ae2dfd853c87f9cbecb6f399de4dad09bad4606/lib/gssapi/krb5/arcfour.c#L390">https://github.com/heimdal/heimdal/blob/7ae2dfd853c87f9cbecb6f399de4dad09bad4606/lib/gssapi/krb5/arcfour.c#L390</a>, <a href="https://github.com/heimdal/heimdal/blob/7ae2dfd853c87f9cbecb6f399de4dad09bad4606/lib/gssapi/krb5/arcfour.c#L661">https://github.com/heimdal/heimdal/blob/7ae2dfd853c87f9cbecb6f399de4dad09bad4606/lib/gssapi/krb5/arcfour.c#L661</a>, <a href="https://github.com/heimdal/heimdal/blob/7ae2dfd853c87f9cbecb6f399de4dad09bad4606/lib/gssapi/krb5/arcfour.c#L1279">https://github.com/heimdal/heimdal/blob/7ae2dfd853c87f9cbecb6f399de4dad09bad4606/lib/gssapi/krb5/arcfour.c#L1279</a></li>
<li><a href="https://github.com/zeromq/libzmq/blob/22d218a182855f28038e865cb75bf5897ff0c786/tests/test_mock_pub_sub.cpp#L203">https://github.com/zeromq/libzmq/blob/22d218a182855f28038e865cb75bf5897ff0c786/tests/test_mock_pub_sub.cpp#L203</a></li>
<li><a href="https://github.com/pigoz/mplayer-svn/blob/8d651873a9eb193f5155ffb51ece206f187cf00f/sub/sub_cc.c#L391-L412">https://github.com/pigoz/mplayer-svn/blob/8d651873a9eb193f5155ffb51ece206f187cf00f/sub/sub_cc.c#L391-L412</a></li>
</ul>
<p>On my entire system I only found 10 lines of code that were miscompiled.
Three lines are tests.
All of the lines could be rewritten as a comparison to 0.
None of the lines looked that serious.
I am not sure which one is the worse: the reduced message integrity code(?) from some ARCFOUR implementation or the something something from an ATM driver?</p><p>The mplayer miscompilation is the most mysterious.
The code surrounding that function all appears to be immediately compare <code>memcmp</code> with <code>0</code>.
And given that my debug message refused to point to exactly what line is being miscompiled in that function, I fear some set of optimizations has happened to allow this code to be miscompiled in some way.</p><p>With more hardware I could do <a href="https://hydra.nixos.org/jobset/nixpkgs/trunk#tabs-jobs">a more thorough investigation</a> of the consequences of this GCC bug.
Until then I am going to stick with GCC 8 until GCC 9 and 10 have a new point releases.

</p></div></div>]]>
            </description>
            <link>http://r6.ca/blog/20200929T023701Z.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24636326</guid>
            <pubDate>Wed, 30 Sep 2020 07:01:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Understanding How UUIDs Are Generated]]>
            </title>
            <description>
<![CDATA[
Score 119 | Comments 43 (<a href="https://news.ycombinator.com/item?id=24636204">thread link</a>) | @aryamansharda
<br/>
September 29, 2020 | https://digitalbunker.dev/2020/09/30/understanding-how-uuids-are-generated/ | <a href="https://web.archive.org/web/*/https://digitalbunker.dev/2020/09/30/understanding-how-uuids-are-generated/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="mainEntityOfPage">
<h2><strong>Introduction</strong></h2>
<p>You’ve likely used UUIDs in projects before and assumed them to be unique. Today, we’ll take a look at the main aspects of the implementation and understand why UUIDs are <em>practically </em>unique, though an incredibly small potential for duplication exists.&nbsp;</p>
<p>The modern day implementation of UUIDs can be tied back to <a href="https://tools.ietf.org/html/rfc4122" target="_blank" rel="noreferrer noopener">RFC 4122 </a>which introduced 5 different approaches for generating these identifiers. We’ll take a look at each one and we’ll step through the implementation details of Version 1 &amp; Version 4 in a moment.&nbsp;</p>
<h2><strong>Background</strong></h2>
<p>UUIDs, or universally unique IDentifiers, are simply a 128 bit number used to uniquely identify items in software development. Their canonical textual representation is as a series of 32 hexadecimal characters which are separated into five groups by hyphens in the form 8-4-4-4-12.&nbsp;</p>
<p>For example,&nbsp;</p>
<p><code>3422b448-2460-4fd2-9183-8000de6f8343</code></p>
<p>Embedded inside this seemingly random series of hexadecimal characters is information about the UUID implementation.&nbsp;</p>
<p><code>xxxxxxxx-xxxx-<strong>M</strong>xxx-<strong>N</strong>xxx-xxxxxxxxxxxx</code></p>
<p>The values in the M and N locations uniquely identify the UUID version and variant respectively.</p>
<h4><strong>Version</strong></h4>
<p>The version number is identified by looking at the most significant 4 bits of the value in the M position.&nbsp;</p>
<p>The following table lists the currently defined versions:</p>
<div><figure><img src="https://lh3.googleusercontent.com/1ePxhp-OP2JdmdHVLd3rbfDc9dcNP5J9QQncT0bYPdJ5W3_2pAHfGZ9a7Q7m3mav6VA1syO22nTIavepVicTMW9YxqiuGcSePUOanJeyz67K_bZwDbd_KO25oLgPirr53qj1wZq9" alt="" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure></div>
<h4><strong>&nbsp;Variant</strong></h4>
<p>The variant field determines the layout of the information embedded in the UUID. The interpretation of all other bits in the UUID depends on the value of the variant.&nbsp;</p>
<p>We identify the variant by considering the first 1-3 most significant bits of N.&nbsp;</p>
<div><figure><img src="https://lh3.googleusercontent.com/FeXxFJcHdVVJA7IorpHtQx67F3eiSkMJVAr2Q5E4t3IierXggi-DR1uthMBOgRBXRP4JzcdRMuHbVqMMkCWZ47fo7sUO8sDBQcwgALwr45qLdpC9bSUBoXKQF9Bhdnnc9kN-g8p9" alt="" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure></div>
<p>The most common implementation nowadays is Variant 1 where the MSB0&nbsp; is fixed as 1 and MSB1 is fixed as 0. This means that considering our wildcard values – the bits marked with an <code>x</code> above – the only possible values are 8, 9, A, or B.</p>
<blockquote><p><strong>Quick Reminder:&nbsp;</strong></p><p>1 0 0 0 = 8&nbsp;</p><p>1 0 0 1 = 9</p><p>1 0 1 0 = A</p><p>1 0 1 1 = B</p></blockquote>
<p>So, if you ever see a UUID with these values in the N position, you’ll know it’s the common Variant 1 type.&nbsp;</p>
<h4><strong>Version 1 (Time Based + Unique or Random Host Identifier)&nbsp;</strong></h4>
<p>In this version, the UUID is generated by taking the current timestamp and some identifying property of the device generating the UUID – most commonly, the MAC address (also called the node ID).&nbsp;</p>
<p>This UUID is generated by concatenating the 48 bit MAC address, a 60 bit-timestamp, and a 14 bit “uniquifying” clock sequence, along with the 6 reserved bits for version and variant to generate a unique UUID.&nbsp;</p>
<blockquote><p>The clock sequence is simply a value incremented every time the clock is modified.&nbsp;</p></blockquote>
<p>The timestamp used in this version is the number of 100 nanosecond time intervals since October 15, 1582 – the date of Gregorian reform to the Christian calendar.&nbsp;</p>
<blockquote><p>You may be familiar with Unix systems and time since epoch. This is simply just a different Day 0. There are several algorithms online that allow you to convert one time representation to the other, so we won’t go over this here.</p></blockquote>
<p>Though this implementation seems fairly straightforward and robust, because it reveals the MAC address of the machine it was generated on, this approach is not suitable for all use cases.&nbsp; Especially, when security is a major concern. Instead, some implementations will use 6 random bytes sourced from a cryptographically secure random number generator as a replacement for the node ID.&nbsp;</p>
<p>&nbsp;To assemble a Version 1 UUID, we’ll do the following steps:&nbsp;</p>
<ol><li>Take the low 32 bits of the current UTC timestamp. This will be the first 4 bytes / 8 hex characters of our UUID [TimeLow]</li><li>Take the middle 16 bits of the current UTC timestamp. These will be the following 2 bytes / 4 hex characters. [TimeMid]</li><li>The next 2 bytes / 4 hex characters will concat the 4 bit UUID version with the remaining high 12 bits of the current UTC timestamp (which is 60 bits in total). [TimeHighAndVersion]</li><li>Now, the next 1-3 bits will specify the variant of the UUID version. The remaining bits will contain the clock sequence which is meant to contribute some small amount of randomness to this implementation. In doing so, it helps avoid collisions in the event multiple UUID generators are running on the same system, a system clock for a UUID generator is set backwards, or the system clock doesn’t advance fast enough. [ClockSequenceHiAndRes &amp;&amp; ClockSequenceLow]</li><li>The final 6 bytes / 12 hex characters / 48 bits are the “node id” which is most commonly the MAC address of the issuing device. [NodeID]</li></ol>
<p>Putting everything together, our Version 1 UUID is generated by concatenating:</p>
<p><code>TimeLow + TimeMid + TimeHighAndVersion + (ClockSequenceHiAndRes &amp;&amp; ClockSequenceLow) + NodeID&nbsp;</code></p>
<p>Due to this implementation’s reliance on the clock, there are some edge cases we’ll need to handle. Firstly, to minimize correlation across systems, the clock sequence is defaulted to a random number – this will only happen once in the lifetime of a system. This has the added benefit of allowing us to support node identifiers that may move from system to system as the initial value of the clock sequence is completely agnostic of the node identifier.&nbsp;</p>
<p>Remember that the main purpose of the clock sequence is to introduce some amount of randomness into the equation. The bits allocated for the clock sequence help us extend the timestamp and account for situations where multiple UUIDs are generated before the processor clock advances. This helps us avoid the potential creation of duplicates when the clock is set backwards in time (device is powered off) or the node ID changes. If the clock is set backwards, or might have been set backwards (e.g., while the system was powered off), and the UUID generator can not be sure that no UUIDs were generated with timestamps larger than the value to which the clock was set, then the clock sequence has to be changed. If the previous value of the clock sequence is known, it can just be incremented; otherwise it should be set to a random or high-quality pseudo-random value.</p>
<h4><strong>Version 2 (Distributed Computing Environment Security)</strong></h4>
<p>The main difference between this version and Version 1 is that this implementation uses some identifier specific to the system in place of the “randomness” generated by using the least significant bits of the clock sequence. This value is often just the current user’s ID. This version is less common and only a small deviation from Version 1, so we won’t explore it any further.&nbsp;</p>
<h4><strong>Version 3 (Name-based + MD5 Hash)</strong></h4>
<p>In the event that you want to have unique identifiers for information within a namespace or for “nameable” information more generally, UUID version 3 and version 5 are your preferred options.&nbsp;</p>
<p>They’ll encode any “nameable” entity (website, DNS information, plain text, etc) into the UUID value. The main takeaway here is that for the same namespace and text, the generated UUID will be the same.&nbsp;</p>
<p>Take note that the namespace itself is a UUID.</p>
<pre><code>let namespace = “digitalbunker.dev”
let namespaceUUID = UUID3(.DNS, namespace)

// Ex: 
UUID3(namespaceUUID, “/category/things-you-should-know-1/”) 
4896c91b-9e61-3129-87b6-8aa299028058

UUID3(namespaceUUID, “/category/things-you-should-know-2/”) 
29be0ee3-fe77-331e-a1bf-9494ec18c0ba

UUID3(namespaceUUID, “/category/things-you-should-know-3/”) 
33b06619-1ee7-3db5-827d-0dc85df1f759
</code></pre>
<p>In this implementation, the UUID of the namespace is transformed to a string of bytes, concatenated with the input name, then hashed with MD5, yielding the 128 bits for the UUID. Then, we’ll overwrite some of those bits to accurately reflect the version and variant information leaving the rest unchanged.&nbsp;</p>
<p>It’s also important to understand that neither the namespace nor the inputted name can be generated from the UUID. This is a one-way operation. The only exception here would be through a brute force approach if one of the values (namespace or text) were known by the attacker.</p>
<p>For Version 3 and Version 5, as long as you use the same inputs, the generated UUID will be deterministic.&nbsp;</p>
<h4><strong>Version 4 (PRNG)</strong></h4>
<p>This is probably the simplest implementation of the bunch.</p>
<p>As we’ve now seen, 6 bits are reserved for the version and variant information leaving us 122 bits free to decide. This version simply generates all 128 random bits and then fills in the values for the version and variant information as a secondary step.&nbsp;</p>
<p>UUIDs of this variety rely heavily on the quality of the PRNG in use (pseudo-random number generator). If the PRNG is lacking a sophisticated algorithm or the correct seed and initialization values, the likelihood of a duplicate can increase. To better understand how computers generate random numbers, <a href="https://digitalbunker.dev/2020/09/08/how-do-computers-generate-random-numbers/" target="_blank" rel="noreferrer noopener">check out my previous article.</a>&nbsp;</p>
<p>Version 4 is what you’ll find most commonly implemented in modern programming languages.</p>
<p>It’s implementation is reasonably simple.&nbsp;</p>
<ol><li>Generate 128 random bits</li><li>Now, we’ll need to overwrite some of these bits with the correct version and variant information&nbsp;<ol><li>Take the 7th byte and perform an AND operation with <code>0x0F</code> to clear out the high nibble. Then, OR it with <code>0x40</code> to set the version number to 4.&nbsp;</li><li>Next, take the 9th byte and perform an AND operation with <code>0x3F</code> and then OR it with <code>0x80</code>.&nbsp;</li></ol></li><li>Convert the 128 bits to hexadecimal representation and insert the hyphens to achieve the canonical text representation.&nbsp;</li></ol>
<h4><strong>Version 5 (Name-based + SHA-1 Hash)</strong></h4>
<p>This version is no different than Version 3 with the exception that the <code>SHA-1</code> hashing algorithm is used here in place of <code>MD5</code>. This version is preferred to Version 3 (SHA-1 &gt; MD5).&nbsp;</p>
<h2><strong>In Practice</strong></h2>
<p>One of the notable benefits of UUIDs is that their uniqueness does not depend on a central authority or coordination between different systems. Anyone can create UUIDs with reasonable assurance that a duplicate value does not exist and will conceivably not be made in the future.&nbsp;</p>
<p>This has the added benefit of allowing UUIDs generated by independent parties to be combined into a single database or moved across databases with a trivial probability of duplication / collision.&nbsp;</p>
<p>Due to this uniqueness, you can use UUIDs as primary keys in databases, unique filenames for uploaded files, unique names for any web resource, or …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://digitalbunker.dev/2020/09/30/understanding-how-uuids-are-generated/">https://digitalbunker.dev/2020/09/30/understanding-how-uuids-are-generated/</a></em></p>]]>
            </description>
            <link>https://digitalbunker.dev/2020/09/30/understanding-how-uuids-are-generated/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24636204</guid>
            <pubDate>Wed, 30 Sep 2020 06:31:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Apple’s T2 security chip jailbreak]]>
            </title>
            <description>
<![CDATA[
Score 699 | Comments 345 (<a href="https://news.ycombinator.com/item?id=24636166">thread link</a>) | @Yeri
<br/>
September 29, 2020 | https://reportcybercrime.com/hackers-jailbreak-apples-t2-security-chip-powered-by-bridgeos/ | <a href="https://web.archive.org/web/*/https://reportcybercrime.com/hackers-jailbreak-apples-t2-security-chip-powered-by-bridgeos/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>The Apple T2 security chip has finally been jailbroken! Here’s all you need to know about it.&nbsp; &nbsp; &nbsp;</p>
<h2><span id="The_Apple_T2_Security_chip_now_has_a_jailbreak"><strong>The Apple T2 Security chip now has a jailbreak</strong><span></span></span></h2>
<p>The <a href="https://yalujailbreak.net/checkra1n-jailbreak-ios-14/">latest update of checkra1n</a> adds support for bridgeOS – the operating system that powers the Apple T2 security chip.</p>
<p>For what it’s worth, the T2 chip is not A10 per se but it is derived from the Apple A10 Fusion architecture.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p>
<p><strong>bridgeOS</strong> is a proprietary operating system created by Apple for its hardware. It is responsible for operating the Touch Bar and managing secure data.&nbsp;&nbsp;&nbsp;</p>
<p>Here’s what hacker Jamie Bishop had to say about this development.&nbsp;&nbsp;</p>
<blockquote data-width="550" data-dnt="true">
<p lang="en" dir="ltr">With <a href="https://twitter.com/checkra1n?ref_src=twsrc%5Etfw" target="_blank" rel="nofollow noopener noreferrer">@checkra1n</a> 0.11.0, you can now jailbreak the T2 chip in your Mac. An incredible amount of work went into this and it required changes at multiple levels.</p>
<p>There’s too many people to tag, but shoutout to everyone who worked on getting this incredible feature shipped.</p>
<p>— Jamie Bishop (@jamiebishop123) <a href="https://twitter.com/jamiebishop123/status/1308355178307948545?ref_src=twsrc%5Etfw" target="_blank" rel="nofollow noopener noreferrer">September 22, 2020</a></p>
</blockquote>
<p>Since checkra1n is still in beta, there are a few issues you need to be aware of. Firstly, you might have to reconnect your device after jailbreaking for bootstrap upload.</p>
<p>Secondly, macOS takes over the USB connection and blocks communication after bootup.&nbsp;</p>
<p>If you are interested in jailbreaking the T2 chip, download checkra1n jailbreak v0.11 from this <a href="https://yalujailbreak.net/checkra1n-jailbreak-ios-14/">link</a>.&nbsp;</p>
<h2><span id="What_can_a_bridgeOS_jailbreak_be_used_for"><strong>What can a bridgeOS jailbreak be used for?&nbsp;</strong><span></span></span></h2>
<p>The T2 security processor and the Touch Bar can run while the operating system is shutdown.</p>
<p>Apparently, jailbreak tweak developers could develop tweaks for the Touch Bar if it gets Substrate support in the future.</p>
<p>At present, there are no publicly dumped headers available for bridgeOS. It lacks a MobileSubstrate port too. However, that could change in the future because it shares some of the components of watchOS and iOS frameworks.</p>
<p>Once we get Substrate working, <strong>tweaking and theming could become possible.</strong></p>

<p>The ability to exploit the T2 processor could also allow you to bypass the anti-repair mechanism built into the Touch Bar. Further, it may allow hackers to get rid of the password or unlock MDM-locked systems.&nbsp;</p>
<p>As far as the OS goes, we could also add secure boot certificates like Microsoft’s secure boot signing or a self-signed Linux certificate.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p>
<p>It will definitely be interesting to see what the future holds for the <em>T2 security chip</em> following the release of checkra1n.&nbsp; &nbsp; &nbsp;</p>
<p>Don’t forget to follow us on Twitter and Facebook for the latest jailbreak news and updates.&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p>
</div></div>]]>
            </description>
            <link>https://reportcybercrime.com/hackers-jailbreak-apples-t2-security-chip-powered-by-bridgeos/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24636166</guid>
            <pubDate>Wed, 30 Sep 2020 06:19:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Elements of Programmnig]]>
            </title>
            <description>
<![CDATA[
Score 32 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24635947">thread link</a>) | @todsacerdoti
<br/>
September 29, 2020 | http://www.pathsensitive.com/2020/09/book-review-elements-of-programmnig.html | <a href="https://web.archive.org/web/*/http://www.pathsensitive.com/2020/09/book-review-elements-of-programmnig.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div>

<p>The C++ STL may be the most impressive achievement in language standard libraries. Where most programmers are stuck complaining that their language’s default strings aren’t performant enough, about every standard C++ function for strings actually runs on arbitrary character sequences. Design your own container? <span>std::find_if</span> works just as well as for the built-ins. And it does this while often being more performant than the code you’d write yourself.</p>

<p>Alex Stepanov is the man who made this happen, and Elements of Programming (EOP) is his 200-page paean to his method for writing generic code. He’s a believer that programming can be turned from an art to a rigorous discipline based on mathematics, and I’ve long admired him for his deep knowledge and impact. Indeed, he’s spent years at  #1 on my list of people I’d like to have lunch with. (Hey readers — can anyone help?)</p>

<p>And now, I’m about to ruin all that by writing a negative review.</p>

<p>EOP has a strong beginning and a strong finish. The first chapter explains core programming language concepts such as types and state — using non-standard terminology, but probably intentionally, judging by the explanation’s lucidity. This culminates in his big idea of being able to write programs on any type that offers a bundle of operations and properties called a concept, explained in this book over a decade before they entered the C++ standard. The afterword is a few pages of reflection on the power of this approach.</p>

<p>Between them is 11 chapters where he plays the same game: define a new abstraction and then show a bunch of functions that can be written using it. And unfortunately, these functions and abstractions are largely not very interesting.</p>

<p>There’s a famous site called Project Euler, where users write code to solve mathy problems such as “In a modified version of the board game Monopoly, on what three squares is a player most likely to land?” My former programming-contest coach advocated against using it to practice, because “It’s not really programming, and it’s not really math.”</p>

<p>I think this is an apt description of EOP, particularly the first half. This starts from Chapter 2, which is about cool algorithms that involve applying some function to itself repeatedly (iteration). One of my <a href="https://www.cs.cmu.edu/~cdm/pdf/lect-05.pdf">favorite lectures</a> in undergrad was on this topic, and yet I still couldn’t enjoy this chapter, as I know no application of these algorithms outside of niche mathematical topics. This gets taken up another notch in Chapter 5, where, in about 5 pages, he goes from explaining commutativity to defining the algebraic structures of monoids, groups, and rings, all the way up to algebraic modules (no relation to software modules). I cannot fathom these explanations being useful to someone who does not already know these concepts, and certainly not to someone who already does. And while I do know many uses of groups in software engineering — as an abstraction of the idea of an invertible operation — he actually spends the remainder of this chapter considering generalizations of the greatest common divisor function.</p>

<p>I slogged through these chapters, excited for the second half of the book, which focused largely on iterators and containers, things more relevant to typical software engineering. Yet after encountering endless listings of variations of list-copy functions, I found myself no more fulfilled, and soon regressed to skimming through the pages.</p>

<p>Aside from my long-term goal to find all the good writing on software design, I had a short-term goal when reading this book: a student wanted me to teach a lesson based on it. But, halfway through the book, my deadline was fast approaching, and I hadn’t found any material useful enough for a software design lesson for experienced engineers.</p>

<p>I then noticed all the chapters were generated by the deeper principle of coming up with a good abstraction to write generic functions against. I got the idea that maybe I could use EOP as a problem book, telling them to look at the descriptions of generic functions, and then come up with both the code and the abstractions they can be written against. Alas, the topic selection is not suitable for this purpose. One section of the book, for example, deals with computing integer sequences by matrix exponentiation. Asking students to come up with this themselves would be too familiar for many who have taken a linear algebra course, and impossible for those who haven’t. I did design a lesson where students come up with their own abstractions for generic programming problems, but I used examples completely unrelated to the book.</p>

<p>I asked the friend who recommended EOP what he got out of the book, and his first answer was a technique for elegantly expressing state machines using goto’s. I similarly loved that part, but, alas, that was the only concrete thing I got out of this book. I’ll explain it at the end of this review and spare you the other 200 pages.</p>

<p>I have an undergraduate degree in mathematics and have authored several papers on generic programming, so I knew I was reading it for others’ benefit. Still, I don’t think my opinion would be changed were this not the case, and I’d really like help understanding the viewpoint of the many readers who did thoroughly enjoy it. Instead, I find myself agreeing with this Amazon reviewer, although I have too much admiration for Stepanov to contemplate a 1-star rating:</p>

<blockquote>If you've ever written a generic function, you already know that the type parameters must obey a set of preconditions. This book lays out a big pile of definitions for types, numbers, algebraic structures, iterators, and such, so as to bamboozle people easily impressed by mathematical notation. It does so sloppily and writes some trivial generic algorithms in C++. To whatever extent one might accomplish something interesting with this topic, this book doesn't. Avoid.</blockquote>

<p>As I read other material by Stepanov, I mourn for the book that could have been. Stepanov clearly cares about these abstractions and algorithms, to the point where he wrote a <a href="https://www.amazon.com/Mathematics-Generic-Programming-Alexander-Stepanov/dp/0321942043">second book</a> on largely the same material, with a chattier exposition and chapters more explicitly focusing on pure math. How different would it be had he managed to transmit this appreciation to me? The day after finishing, I watched <a href="https://www.youtube.com/watch?v=W2tWOdzgXHA&amp;t=1344s">this talk</a> by a close colleague of Stepanov. “In this menu, you can select a bunch of rows and drag them somewhere else,” he explained over animated slides. “How many of you could implement this in one line?” It made me want to open section 10.4 on “rotation algorithms” again.</p>

<p>I’ve started watching <a href="https://www.youtube.com/playlist?list=PLHxtyCq_WDLXryyw91lahwdtpZsmo4BGD">a seminar</a> he gave at Amazon. I’m only a few lectures in, but I’m already enthralled by his high teaching ability. I feel like I’m there with him working through problems. I feel like I’ve learned a great secret as he tells the story of how he invented “regular types,” something used throughout EOP but never motivated. To be honest, I still don’t know what this lecture series is about, but nonetheless expect to recommend it when I’m done.</p>

<p>In short, Stepanov has given many gifts to the world of programming, and EOP is not one of them.</p>

<p><b>Overall rating</b>: <span>Not recommended</span></p>

<p>With a smattering of exceptions, EOP neither teaches abstractions useful in everyday programming, nor teaches you the skills to invent your own.</p>

<h2><b>Addendum</b>: State machines by goto’s</h2>

<blockquote>“The fastest way to go from one place in code to another is goto.”</blockquote><p>

— <a href="https://www.youtube.com/watch?v=sp_IBYVqMeQ&amp;list=PLHxtyCq_WDLXryyw91lahwdtpZsmo4BGD">Alexander Stepanov</a></p><p>Many iterative algorithms can be described as state machines: first it looks for an X, then it does Y with it, then it looks for another X, and so on. Rather than trying to massage the cycles in the state machine into structured loops, Stepanov advocates a style using goto’s, with one goto-label per machine state.</p>

<p>In searching for an example to best illustrate this, I wanted something where the code was under 40 lines (which ruled out Stepanov’s examples), understandable with little context or knowledge of C++, and which was not equivalent to a regular expression. And so:</p>

<p>In my defunctionalization talk, I showed that many state machines are derived from recursive functions, being turned into iterative traversals by creating a state for each point in the program between recursive calls. For that talk, I demonstrated this in full for the example of printing a binary tree. It turns out that adding parentheses makes this derivation substantially harder, as an arbitrary number of close-parentheses may need to be printed after processing a node. And that difficulty comes from trying to massage a state machine into a loop.</p>

<p>In this case, seeing as I came up with this example by starring with a recursive function, the recursive version is quite simple:</p>

<div><pre><span>void</span> <span>print_tree_rec</span><span>(tree</span> <span>*</span><span>t)</span> <span>{</span>
  <span>if</span> <span>(t</span> <span>!=</span> <span>NULL)</span> <span>{</span>
    <span>printf(</span><span>"("</span><span>);</span>
    <span>print_tree_rec(t</span><span>-&gt;</span><span>left);</span>
    <span>printf(</span><span>" %d "</span><span>,</span> <span>t</span><span>-&gt;</span><span>element);</span>
    <span>print_tree_rec(t</span><span>-&gt;</span><span>right);</span>
    <span>printf(</span><span>")"</span><span>);</span>
  <span>}</span>
<span>}</span>
</pre></div>


<p>But, for other state machines, the recursive version is not so easy. For example, Dijkstra created the “shunting yard” algorithm for parsing an arithmetic expression all the way back in 1961, yet I’m not aware of the recursive equivalent being discovered <a href="https://www.brics.dk/RS/07/7/BRICS-RS-07-7.pdf">until 2007</a>, using the technique of refunctionalization.</p>

<p>Here’s the state machine:</p><p><span id="docs-internal-guid-068c5e82-7fff-aeae-61d2-74453ca23aeb"><span><span><img height="566" src="https://lh4.googleusercontent.com/bfYT7CeU0eo8KAoMt8gB5Y4FDgD7Y1UlUJYSNI3ndYZ6jQpZv5Nwpyqyqoqffi_tgSRBYslU93EZy_nCil9ub6FZraaRGOBoABGPk0i9DoNbgr1fWAnkQzXMxPvLvEYFSDRWZ1RL" width="572"></span></span></span></p>

<p>A confession: the first time I thought about how to make this recursive function function iterative, I didn’t get it, and had to look it up. The solution is to merge the “Next from stack” state in the diagram with its successors, resulting in a solution with two nested while-loops, at the cost of some duplicated code.</p>

<p>However, the version based on goto’s reads off this diagram rather nicely. One C++-ism in this code to note: while the stack s is initialized to NULL, the  <span>push()</span> and  <span>pop()</span> calls can actually change it.</p>

<!--HTML generated using hilite.me--><div><pre><span>void</span> <span>print_tree</span><span>(tree</span> <span>*</span><span>t)</span> <span>{</span>
  <span>tree</span> <span>*</span><span>cur</span> <span>=</span> <span>t;</span>
  <span>stack</span> <span>*</span><span>s</span> <span>=</span> <span>NULL;</span>
  
<span>begin_print_node:</span>
  <span>if</span> <span>(cur</span> <span>==</span> <span>NULL)</span> <span>{</span>
    <span>goto</span> <span>dispatch_stack_frame;</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>printf(</span><span>"("</span><span>);</span>
    <span>push(LEFT_CHILD,</span> <span>cur,</span> <span>s);</span>
    <span>cur</span> <span>=</span> <span>cur</span><span>-&gt;</span><span>left;</span>
    <span>goto</span> <span>begin_print_n…</span></pre></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.pathsensitive.com/2020/09/book-review-elements-of-programmnig.html">http://www.pathsensitive.com/2020/09/book-review-elements-of-programmnig.html</a></em></p>]]>
            </description>
            <link>http://www.pathsensitive.com/2020/09/book-review-elements-of-programmnig.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24635947</guid>
            <pubDate>Wed, 30 Sep 2020 05:28:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[From zero to main(): How to write a bootloader from scratch]]>
            </title>
            <description>
<![CDATA[
Score 323 | Comments 44 (<a href="https://news.ycombinator.com/item?id=24635383">thread link</a>) | @tigerlily
<br/>
September 29, 2020 | https://interrupt.memfault.com/blog/how-to-write-a-bootloader-from-scratch | <a href="https://web.archive.org/web/*/https://interrupt.memfault.com/blog/how-to-write-a-bootloader-from-scratch">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

        <p>This is the third post in our <a href="https://interrupt.memfault.com/blog/tag/zero-to-main">Zero to main() series</a>,
where we bootstrap a working firmware from zero code on a
cortex-M series microcontroller.</p>

<p>Previously, <a href="https://interrupt.memfault.com/blog/zero-to-main-1">we wrote a startup file to bootstrap our C environment</a>, and <a href="https://interrupt.memfault.com/blog/how-to-write-linker-scripts-for-firmware">a linker
script to get the right data at the right addresses</a>. These two will allow us to
write a monolithic firmware which we can load and run on our microcontrollers.</p>

<p>In practice, this is not how most firmware is structured. Digging through vendor
SDKs, you’ll notice that they all recommend using a <em>bootloader</em> to load your
applications. A bootloader is a small program which is responsible for loading
and starting your application.</p>

<!-- excerpt start -->
<p>In this post, we will explain why you may want a
bootloader, how to implement one, and cover a few advanced techniques you may
use to make your bootloader more useful.
<!-- excerpt end --></p>

<p>Like Interrupt? <a href="http://eepurl.com/gpRedv" target="_blank">Subscribe</a> to get our latest posts straight to your mailbox.</p>



<h2 id="why-you-may-need-a-bootloader">Why you may need a bootloader</h2>

<p>Bootloaders serve many purposes, ranging from security to software architecture.</p>

<p>Most commonly, you may need a bootloader to load your software. Some
microcontrollers like Dialog’s
<a href="https://www.dialog-semiconductor.com/products/connectivity/bluetooth-low-energy/smartbond-da14580-and-da14583">DA14580</a>
have little to no onboard flash and instead rely on an external device to store
firmware code. In that case, it is the bootloader’s job to copy code from
non-executable storage, such as a SPI flash, to an area of memory that can be
executed from, such as RAM.</p>

<p>Bootloaders also allow you to decouple parts of the program that are mission
critical, or that have security implications, from application code which
changes regularly.  For example, your bootloader may contain firmware update
logic so your device can recover no matter how bad a bug ships in your
application firmware.</p>

<p>Last but certainly not least, bootloaders are an essential component of a
trusted boot architecture.  Your bootloader can, for example, verify a
cryptographic signature to make sure the application has not been replaced or
tampered with.</p>

<h2 id="a-minimal-bootloader">A minimal bootloader</h2>
<p>Let’s build a simple bootloader together. To start, our bootloader must do two
things:</p>

<ol>
  <li>Execute on MCU boot</li>
  <li>Jump to our application code</li>
</ol>

<p>We’ll need to decide on a memory map, write some bootloader code, and update our
application to make it bootload-able.</p>

<h3 id="setting-the-stage">Setting the stage</h3>

<p>For this example, we’ll be using the same setup as we did in our previous Zero
to Main posts:</p>
<ul>
  <li>Adafruit’s <a href="https://www.adafruit.com/product/3505">Metro M0 Express</a> as our
development board,</li>
  <li>a simple <a href="https://www.adafruit.com/product/2764">CMSIS-DAP Adapter</a></li>
  <li>OpenOCD (the <a href="https://github.com/arduino/OpenOCD">Arduino fork</a>) for
programming</li>
</ul>

<h3 id="deciding-on-a-memory-map">Deciding on a memory map</h3>

<p>We must first decide on how much space we want to dedicate to our bootloader.
Code space is precious - your application may come to need more of it - and you
will not be able to change this without updating your bootloader, so make
this as small as you possibly can.</p>

<p>Another important factor is your flash sector size: you want to make sure you
can erase app sectors without erasing bootloader data, or vice versa.
Consequently, your bootloader region must end on a flash sector boundary
(typically 4kB).</p>

<p>I decided to go with a 16kB region, leading to the following memory map:</p>

<div><div><pre><code>        0x0 +---------------------+
            |                     |
            |     Bootloader      |
            |                     |
     0x4000 +---------------------+
            |                     |
            |                     |
            |     Application     |
            |                     |
            |                     |
    0x30000 +---------------------+
</code></pre></div></div>

<p>We can transcribe that memory into a linker script:</p>

<div><div><pre><code>/* memory_map.ld */
MEMORY
{
  bootrom  (rx)  : ORIGIN = 0x00000000, LENGTH = 0x00004000
  approm   (rx)  : ORIGIN = 0x00004000, LENGTH = 0x0003C000
  ram      (rwx) : ORIGIN = 0x20000000, LENGTH = 0x00008000
}

__bootrom_start__ = ORIGIN(bootrom);
__bootrom_size__ = LENGTH(bootrom);
__approm_start__ = ORIGIN(approm);
__approm_size__ = LENGTH(approm);
</code></pre></div></div>

<p>Since linker scripts are composable, we will be able to <code>include</code> that memory
map into the linker scripts we write for our bootloader and our application.</p>

<p>You’ll notice that the linker script above declares some variables. We’ll need
those for our bootloader to know where to find the application. To make them
accessible in C code, we declare them in a header file:</p>

<div><div><pre><code><span>/* memory_map.h */</span>
<span>#pragma once
</span>
<span>extern</span> <span>int</span> <span>__bootrom_start__</span><span>;</span>
<span>extern</span> <span>int</span> <span>__bootrom_size__</span><span>;</span>
<span>extern</span> <span>int</span> <span>__approm_start__</span><span>;</span>
<span>extern</span> <span>int</span> <span>__approm_size__</span><span>;</span>
</code></pre></div></div>

<h3 id="implementing-the-bootloader-itself">Implementing the bootloader itself</h3>
<p>Next up, let’s write some bootloader code. Our bootloader needs to start
executing on boot and then jump to our app.</p>

<p>We know how to do the first part from our previous post: we need a valid stack
pointer at address <code>0x0</code> , and a valid <code>Reset_Handler</code>  function setting up our
environment at address <code>0x4</code>. We can reuse our previous startup file and linker
script, with one change: we use  <code>memory_map.ld</code> rather than define our own
<code>MEMORY</code> section.</p>

<p>We also need to put our code in the <code>bootrom</code> region from our memory rather than
the <code>rom</code> region in our previous post.</p>

<p>Our linker script therefore looks like this:</p>

<div><div><pre><code>/* bootloader.ld */
INCLUDE memory_map.ld

/* Section Definitions */
SECTIONS
{
    .text :
    {
        KEEP(*(.vectors .vectors.*))
        *(.text*)
        *(.rodata*)
        _etext = .;
    } &gt; bootrom
  ...
}
</code></pre></div></div>

<p>To jump into our application, we need to know where the <code>Reset_Handler</code> of the
app is, and what stack pointer to load.  Again, we know from our previous post
that those should be the first two 32-bit words in our binary, so we just need
to dereference those addresses using the <code>__approm_start__</code> variable from our
memory map.</p>

<div><div><pre><code><span>/* bootloader.c */</span>
<span>#include &lt;inttypes.h&gt;
#include "memory_map.h"
</span>
<span>int</span> <span>main</span><span>(</span><span>void</span><span>)</span> <span>{</span>
  <span>uint32_t</span> <span>*</span><span>app_code</span> <span>=</span> <span>(</span><span>uint32_t</span> <span>*</span><span>)</span><span>__approm_start__</span><span>;</span>
  <span>uint32_t</span> <span>app_sp</span> <span>=</span> <span>app_code</span><span>[</span><span>0</span><span>];</span>
  <span>uint32_t</span> <span>app_start</span> <span>=</span> <span>app_code</span><span>[</span><span>1</span><span>];</span>
  <span>/* TODO: Start app */</span>
  <span>/* Not Reached */</span>
  <span>while</span> <span>(</span><span>1</span><span>)</span> <span>{}</span>
<span>}</span>
</code></pre></div></div>

<p>Next we must load that stack pointer and jump to the code. This will require a
bit of assembly code.</p>

<p>ARM MCUs use the <a href="http://www.keil.com/support/man/docs/armasm/armasm_dom1361289882044.htm"><code>msr</code> instruction
</a> to
load immediate or register data into system registers, in this case the MSP
register or “Main Stack Pointer”.</p>

<p>Jumping to an address is done with a branch, in our case with a <a href="http://www.keil.com/support/man/docs/armasm/armasm_dom1361289866466.htm"><code>bx</code>
instruction</a>.</p>

<p>We wrap those two into a <code>start_app</code> function which accepts our <code>pc</code> and <code>sp</code> as
arguments, and get our minimal bootloader:</p>

<div><div><pre><code><span>/* app.c */</span>
<span>#include &lt;inttypes.h&gt;
#include "memory_map.h"
</span>
<span>static</span> <span>void</span> <span>start_app</span><span>(</span><span>uint32_t</span> <span>pc</span><span>,</span> <span>uint32_t</span> <span>sp</span><span>)</span> <span>__attribute__</span><span>((</span><span>naked</span><span>))</span> <span>{</span>
    <span>__asm</span><span>(</span><span>"           </span><span>\n</span><span>\
          msr msp, r1 /* load r1 into MSP */</span><span>\n</span><span>\
          bx r0       /* branch to the address at r0 */</span><span>\n</span><span>\
    "</span><span>);</span>
<span>}</span>

<span>int</span> <span>main</span><span>(</span><span>void</span><span>)</span> <span>{</span>
  <span>uint32_t</span> <span>*</span><span>app_code</span> <span>=</span> <span>(</span><span>uint32_t</span> <span>*</span><span>)</span><span>__approm_start__</span><span>;</span>
  <span>uint32_t</span> <span>app_sp</span> <span>=</span> <span>app_code</span><span>[</span><span>0</span><span>];</span>
  <span>uint32_t</span> <span>app_start</span> <span>=</span> <span>app_code</span><span>[</span><span>1</span><span>];</span>
  <span>start_app</span><span>(</span><span>app_start</span><span>,</span> <span>app_sp</span><span>);</span>
  <span>/* Not Reached */</span>
  <span>while</span> <span>(</span><span>1</span><span>)</span> <span>{}</span>
<span>}</span>
</code></pre></div></div>

<blockquote>
  <p>Note: hardware resources initialized in the bootloader must be de-initialized
before control is transferred to the app. Otherwise, you risk breaking
assumptions the app code is making about the state of the system</p>
</blockquote>

<h3 id="making-our-app-bootloadable">Making our app bootloadable</h3>

<p>We must update our app to take advantage of our new memory map. This is again
done by updating our linker script to include <code>memory_map.ld</code> and changing our
sections to go to the <code>approm</code> region rather than <code>rom</code>.</p>

<div><div><pre><code>/* app.ld */
INCLUDE memory_map.ld

/* Section Definitions */
SECTIONS
{
    .text :
    {
        KEEP(*(.vectors .vectors.*))
        *(.text*)
        *(.rodata*)
        _etext = .;
    } &gt; approm
  ...
}
</code></pre></div></div>

<p>We also need to update the <a href="https://developer.arm.com/docs/dui0552/latest/the-cortex-m3-processor/exception-model/vector-table"><em>vector
table</em></a>
used by the microcontroller. The vector table contains the address of every
exception and interrupt handler in our system. When an interrupt signal comes
in, the ARM core will call the address at the corresponding offset in the vector
table.</p>

<p>For example, the offset for the Hard fault handler is <code>0xc</code>, so when a hard
fault is hit, the ARM core will jump to the address contained in the table at
that offset.</p>

<p>By default, the vector table is at address <code>0x0</code>, which means that when our chip
powers up, only the bootloader can handle exceptions or interrupts! Fortunately, ARM
provides the <a href="https://developer.arm.com/docs/dui0552/latest/cortex-m3-peripherals/system-control-block/vector-table-offset-register">Vector Table Offset
Register</a>
to dynamically change the address of the vector table. The register is at
address <code>0xE000ED08</code> and has a simple layout:</p>

<div><div><pre><code>31                                  7              0
+-----------------------------------+--------------+
|                                   |              |
|              TBLOFF               |   Reserved   |
|                                   |              |
+-----------------------------------+--------------+
</code></pre></div></div>

<p>Where <code>TBLOFF</code> is the address of the vector table. In our case, that’s the start
of our text section, or <code>_stext</code>. To set it in our app, we add the following to
our <code>Reset_Handler</code>:</p>

<div><div><pre><code><span>/* startup_samd21.c */</span>
<span>/* Set the vector table base address */</span>
<span>uint32_t</span> <span>*</span><span>vector_table</span> <span>=</span> <span>(</span><span>uint32_t</span> <span>*</span><span>)</span> <span>&amp;</span><span>_stext</span><span>;</span>
<span>uint32_t</span> <span>*</span><span>vtor</span> <span>=</span> <span>(</span><span>uint32_t</span> <span>*</span><span>)</span><span>0xE000ED08</span><span>;</span>
<span>*</span><span>vtor</span> <span>=</span> <span>((</span><span>uint32_t</span><span>)</span> <span>vector_table</span> <span>&amp;</span> <span>0xFFFFFFF8</span><span>);</span>
</code></pre></div></div>

<p>One quirk of the ARMv7-m architecture is the alignment requirement for the
vector table, as specified in section B1.5.3 of the <a href="https://static.docs.arm.com/ddi0403/eb/DDI0403E_B_armv7m_arm.pdf">reference
manual</a>:</p>

<blockquote>
  <p>The Vector table must be naturally aligned to a power of two whose alignment value is greater than or equal
to (Number of Exceptions supported x 4), with a minimum alignment of 128 bytes.The entry at offset 0 is
used to initialize the value for SP_main, see The SP registers on page B1-8. All other entries must have bit
[0] set, as the bit is used to define the EPSR T-bit on exception entry (see Reset behavior on page B1-20 and
Exception entry behavior on page B1-21 for details).</p>
</blockquote>

<p>Our SAMD21 MCU has 28 interrupts on top of the 16 system reserved exceptions,
for a total of 44 entries in the table. Multiply that by 4 and you get 176. The
next power of 2 is 256, so our vector table must be 256-byte aligned.</p>

<h3 id="putting-it-all-together">Putting it all together</h3>

<p>Because it is hard to witness the bootloader execute, we add a print line to
each of our programs:</p>

<div><div><pre><code><span>/* boootloader.c */</span>
<span>#include &lt;inttypes.h&gt;
#include "memory_map.h"
</span>
<span>static</span> <span>void</span> <span>s…</span></code></pre></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://interrupt.memfault.com/blog/how-to-write-a-bootloader-from-scratch">https://interrupt.memfault.com/blog/how-to-write-a-bootloader-from-scratch</a></em></p>]]>
            </description>
            <link>https://interrupt.memfault.com/blog/how-to-write-a-bootloader-from-scratch</link>
            <guid isPermaLink="false">hacker-news-small-sites-24635383</guid>
            <pubDate>Wed, 30 Sep 2020 03:19:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Nobody seems to give a crap about Google’s monopoly on search, not even Google]]>
            </title>
            <description>
<![CDATA[
Score 25 | Comments 8 (<a href="https://news.ycombinator.com/item?id=24635322">thread link</a>) | @pcr910303
<br/>
September 29, 2020 | https://thejollyteapot.com/2020/09/29/nobody-seems-to-give-a-crap-about-google-s-monopoly-on-search-not-even-google | <a href="https://web.archive.org/web/*/https://thejollyteapot.com/2020/09/29/nobody-seems-to-give-a-crap-about-google-s-monopoly-on-search-not-even-google">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <p><a href="https://thejollyteapot.com/2020/09/29/nobody-seems-to-give-a-crap-about-google-s-monopoly-on-search-not-even-google">29 September 2020</a></p>
<p>Quick summary of the situation, from the <a href="https://www.android.com/choicescreen/">Choice Screen webpage on the Android website</a>:</p>
<blockquote>
<p>On August 2, 2019, following the European Commission’s July 2018 Android decision, Google announced that it would implement a choice screen for general search providers on all new Android phones and tablets shipped into the European Economic Area (<span>EEA</span>) where the Google Search app is pre-installed. This updated Help Center article describes a modified choice screen design that was developed in consultation with the European Commission.</p>
<p>The choice screen will appear during initial device setup and will feature multiple search providers, including Google. An illustrative version of the choice screen follows. Providers may vary by country. […]</p>
<p>Eligible search providers will need to fill out an application form and can bid for inclusion based on an auction. The auction process is explained in greater detail below.</p>
</blockquote>
<p>Considering how much this farce has been <a href="https://www.neowin.net/news/duckduckgo-blasts-googles-android-choice-screen-says-it-incentivizes-ads">covered</a> <a href="https://techcrunch.com/2020/07/30/googles-no-choice-screen-on-android-isnt-working-says-ecosia-querying-the-eus-approach-to-antitrust-enforcement/">already</a>, I will not repeat what <a href="https://www.businessinsider.com/google-android-choice-screen-eu-duckduckgo-2020-9?IR=T">everybody knows</a>. Instead I will just point out how Google announced the result of the auctions <a href="https://www.android.com/choicescreen-winners/">on the Android website</a>: four sentences, two footnotes, a table listing the<span></span> <span>“</span>winners” per country, a title that reads do-not-give-a-shit, and that’s it.</p>
<p>Also, I don’t think I have ever heard of — <em>check notes</em> — PrivacyWall and info.com.<span></span> <span>’</span>What are they?’, you may ask: they are search engines apparently, and they are the only two search engine options offered to Android users in France that are not Google or Bing. And yes, in case you’re wondering, <a href="https://www.youtube.com/watch?v=nfHuZ5qrYX4">Bing is still around</a>. Ecosia, DuckDuckGo, barely appear on the list.</p>
<p>If anything, this whole thing will reinforce Google’s monopoly on search. Users will see this<span></span> <span>’</span>choice screen’ and think:<span></span> <span>“</span><span>OK</span>, so two shady things I don’t know, Bing (laughs), and yep, Google. Why would they even ask me to choose?”</p>
<p>It’s like asking kids who have only ever watched the movie <em>Ratatouille</em> if they want to watch Ratatouille again, or another movie from this list: <a href="https://www.imdb.com/title/tt2120120/?ref_=fn_al_tt_1"><em>Pixels</em></a> (even the kids would know it sucks), <em><a href="https://www.imdb.com/title/tt0257778/?ref_=ttls_li_tt">The Hunchback of Notre Dame <span>II</span>: The Secret of the Bell</a></em> (you’re lucky if the kids even know about the first one), and <em><a href="https://www.imdb.com/title/tt0083630/?ref_=nv_sr_srsg_0">The Beastmaster</a></em>.</p>

          
          <p>
            <a href="https://thejollyteapot.com/tagged/technology">Technology</a>
          </p>

          
          


      

          <a href="https://thejollyteapot.com/2020/08/18/samsung-commits-to-deliver-three-years-of-android-updates-for-its-phones">
            <h5>Previous post</h5>
            <span>Samsung commits to deliver three years of Android updates for its phones</span>
            
          </a>

          <a href="https://thejollyteapot.com/2020/10/1/looking-back-at-my-short-list-of-apple-wishes-from-april">
            <h5>Next post</h5>
            <span>Looking back at my short list of Apple wishes from April</span>
          </a>

        </div></div>]]>
            </description>
            <link>https://thejollyteapot.com/2020/09/29/nobody-seems-to-give-a-crap-about-google-s-monopoly-on-search-not-even-google</link>
            <guid isPermaLink="false">hacker-news-small-sites-24635322</guid>
            <pubDate>Wed, 30 Sep 2020 03:07:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I tracked and analyzed 13,159 Hacker News posts. Here's what I learned]]>
            </title>
            <description>
<![CDATA[
Score 31 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24634991">thread link</a>) | @mellosouls
<br/>
September 29, 2020 | https://www.ankle.io/posts/hacker-news-analysis | <a href="https://web.archive.org/web/*/https://www.ankle.io/posts/hacker-news-analysis">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2 id="motivation"><a href="#motivation" aria-label="motivation permalink"></a>Motivation</h2>
<p>Hitting the front page of Hacker News is an incredible source of short-term traffic to your website or article.</p>
<p>According to this <a href="https://thehftguy.com/2017/09/26/hitting-hacker-news-front-page-how-much-traffic-do-you-get" rel="nofollow">article</a>, in 2017, being on the front page brings a sudden influx of between 10k and 30k visitors to your website/article, something many content marketers, product people, and entrepreneurs undoubtedly salivate over, including me.</p>
<p>I occasionally post to Hacker News (with random things and personal stuff) but have never reached the front page before. Therefore, I decided to dig a little deeper into the mechanics of Hacker News and the best approach to take if the sole goal is to hit the front page.</p>
<p>Obviously, a critical part of hitting the front page is submitting an interesting article which resonates with the HN audience. But beyond this, there are other factors at play—timing, title, upvotes, etc.</p>
<h2 id="methodology-a-unique-approach"><a href="#methodology-a-unique-approach" aria-label="methodology a unique approach permalink"></a>Methodology: A unique approach</h2>
<p>I am not the only person to want to know the best approach to hitting the front page. A few people have analyzed data from <a href="https://console.cloud.google.com/marketplace/details/y-combinator/hacker-news" target="_blank" rel="nofollow noopener noreferrer">Google’s Hacker News Dataset</a> on BigQuery to find the best time to submit to Hacker News.</p>
<p>The problem with this approach is that BigQuery doesn’t know how long a post spent on the front page, nor how long it was there for or what position it reached. It can only provide the total number of upvotes.</p>
<p>With this in mind, I wrote a small NodeJS script to poll Hacker News every 2 minutes. Using a Postgres database, I stored every post, user, and upvote over two weeks.</p>
<p>With this information, I compiled a list of useful information and insights below.</p>
<p>Note: To filter out posts that are either flagged or transiently featured, I set an arbitrary duration of 60+ minutes as a time requirement in the top for a post to be considered a “front page post.”</p>
<h2 id="limitations"><a href="#limitations" aria-label="limitations permalink"></a>Limitations</h2>
<p>Obviously, the lack of data (13,159 posts) and time spent collecting data (2 weeks) is the most significant limitation of my results. Nonetheless, I believe the results are still relatively useful, at least for the next few weeks or months.</p>
<h2 id="results"><a href="#results" aria-label="results permalink"></a>Results</h2>
<h3 id="basic-stats"><a href="#basic-stats" aria-label="basic stats permalink"></a>Basic stats</h3>
<ul>
<li>I tracked a total of 13,159 posts over two weeks.</li>
<li>Of these, 1,073 made the front page for more than 60 minutes.</li>
<li>That’s a hit rate of 8.15%</li>
<li>Frontpage posts stayed at the top for an average of 483.3 minutes (8 hours)</li>
<li>30% of posts reaching the front page were posted by users who posted more than once over the two weeks.</li>
</ul>
<p><strong>Takeaway:</strong> In my opinion, more posts reach the front page than expected and for longer than expected.</p>
<h3 id="types-of-posts-breakdown"><a href="#types-of-posts-breakdown" aria-label="types of posts breakdown permalink"></a>Types of posts breakdown</h3>

<h3 id="hit-rate--of-different-post-types-making-the-front-page"><a href="#hit-rate--of-different-post-types-making-the-front-page" aria-label="hit rate  of different post types making the front page permalink"></a>Hit rate % of different post types making the front page</h3>

<p><strong>Takeaway:</strong> Interestingly, “Show HN” posts make the front page by proportion very slightly less than “Links,” which goes against previous thought that “Show HN” posts perform better due to an algorithm bias. That being said, there may well be an algorithm bias, just many “Show HN” posts are simply not worth upvoting.</p>
<h3 id="when-is-the-best-day-to-post-to-hacker-news-in-2020"><a href="#when-is-the-best-day-to-post-to-hacker-news-in-2020" aria-label="when is the best day to post to hacker news in 2020 permalink"></a>When is the best day to post to Hacker News in 2020?</h3>

<p>The chart above shows that you should post on the weekend to stand the best chance of reaching the front page of Hacker News. The difference from the worst to the best day is only 3.2%, so there’s not a lot in it.</p>
<p>The weekend likely gives you a higher chance to reach the front page because fewer posts are submitted, probably due to lower traffic.</p>
<p><strong>Takeaway</strong>: If your goal is to reach the front page regardless of traffic, posting on the weekend is your best bet. However, because the difference in chance is relatively low throughout the week, posting on a more popular day might be a better risk-reward ratio based on the considerable difference in the traffic you’d likely receive.</p>
<h3 id="when-is-the-best-time-to-post-to-hacker-news-in-2020"><a href="#when-is-the-best-time-to-post-to-hacker-news-in-2020" aria-label="when is the best time to post to hacker news in 2020 permalink"></a>When is the best time to post to Hacker News in 2020?</h3>

<p>Taking a more granular look at the combined hours of all the posts submitted, the chart above highlights a much more significant difference (10.9%) between the chance of reaching the front page of Hacker News.</p>
<p>The best time to post to Hacker News is between 6 am—12 pm UTC (11 pm—5 am PDT). Again, the reason is likely because this is when the website traffic is at it’s lowest, and therefore the competition is relatively low.</p>
<p>I imagine the US is responsible for most of Hacker News’s web traffic, which lines up with the results above. Therefore, if you are posting a link for a US-specific audience to HN, it’s probably best to pick a different time like 4 pm UTC (9 am PDT), or midnight UTC (5 pm PDT).</p>
<p><strong>Takeaway:</strong> If your link requires a US-specific audience, post at 4 pm UTC (9 am PDT / 12 pm EDT), or midnight UTC (5 pm PDT). Otherwise, post between 6 am—12 pm UTC (11 pm—5 am PDT) for the best chance of reaching the front page of Hacker News.</p>
<h3 id="when-in-the-best-day-and-time-to-post-to-hacker-news-in-2020"><a href="#when-in-the-best-day-and-time-to-post-to-hacker-news-in-2020" aria-label="when in the best day and time to post to hacker news in 2020 permalink"></a>When in the best day and time to post to Hacker News in 2020?</h3>
<p>Combining both days and hours into a total weekly picture is even more evident when it is best to post. However, please note, the lack of data makes this chart vulnerable to skewed results.</p>

<p>The chart above illustrates wild fluctuations between the different hours of the week. The lowest chance of reaching the front page is 1-2%, and the highest is 18%-27%. That’s a massive difference.</p>
<p>Looking at the chart, below are generally the best days &amp; times to post to Hacker News:</p>
<ul>
<li><strong>Monday:</strong> 1 am—4 pm UTC (6 pm—9 am PDT)</li>
<li><strong>Tuesday, Wednesday &amp; Thursday:</strong> 12 am—6 am &amp; 9 am—12 pm UTC (5 pm—11 pm &amp; 2 am—5 am PDT)</li>
<li><strong>Friday:</strong> 5 am—12 pm UTC (10 pm—5 am PDT)</li>
<li><strong>Saturday:</strong> 4 am—9 am UTC (9 pm—2 am PDT)</li>
<li><strong>Sunday:</strong> 3 am—8 am &amp; 10 am—6 pm UTC (8 pm—1 am &amp; 3 am—11 am PDT)</li>
</ul>
<p>And here are the best times specifically (bear in mind these may not be the case week-in-week-out because the amount of data limits the results):</p>
<ul>
<li><strong>Monday:</strong> 12 pm UTC (5 am PDT)</li>
<li><strong>Tuesday:</strong> 9 am UTC (2 am PDT)</li>
<li><strong>Wednesday:</strong> 10 am UTC (3 am PDT)</li>
<li><strong>Thursday:</strong> 12 pm UTC (5 pm PDT)</li>
<li><strong>Friday:</strong> 8 am UTC (1 am PDT)</li>
<li><strong>Saturday:</strong> 9 am UTC (2 am PDT)</li>
<li><strong>Sunday:</strong> 5 am OR 1 pm (10 pm OR 6 am PDT)</li>
</ul>
<p><strong>Takeaway:</strong> It’s pretty clear that the best times to post on Hacker News to reach the front page are usually around 6 am UTC (11 pm PDT), 9 am UTC (2 am PDT), or 12 pm UTC (5 am PDT), when the US is asleep.</p>
<p>However, given that 8 hours is the average time spent on the front page, it’s probably best to post at 12 pm UTC on Monday, Thursday, Saturday or Sunday to maximize traffic for when the US wakes up.</p>
<h3 id="how-many-votes-does-a-post-need-to-reach-the-front-page-of-hacker-news-in-2020"><a href="#how-many-votes-does-a-post-need-to-reach-the-front-page-of-hacker-news-in-2020" aria-label="how many votes does a post need to reach the front page of hacker news in 2020 permalink"></a>How many votes does a post need to reach the front page of Hacker News in 2020?</h3>
<p>5.22 is the average number of votes a post needed within an average time of 27.1 minutes to reach the front page for the first time.</p>
<p>Below is a chart of box plots to illustrate the variation of votes required over 24 hours.</p>

<p>Interestingly, this graph clearly shows every post, regardless of time, required at least three upvotes, and for 75% of posts, they never needed more than eight votes to reach the front page of Hacker News.</p>
<p>Expectedly, the range of upvotes required is lower for the first half of the day than the second half, roughly matching up to the timings above.</p>
<p>In some cases, posts required a lot more votes to reach the front page. Presumably, because either the time it took to get these votes was over more time, or there was a lot of competition.</p>
<p><strong>Takeaway:</strong> The votes required to be seen on the front page is relatively small. For the best chance of getting on the front page, you’ll need between 4-6 votes in about 30 minutes, posting in the first half of the day (12 am—12pm UTC).</p>
<h3 id="how-much-karma-do-you-need-to-reach-the-front-page-of-hacker-news-in-2020"><a href="#how-much-karma-do-you-need-to-reach-the-front-page-of-hacker-news-in-2020" aria-label="how much karma do you need to reach the front page of hacker news in 2020 permalink"></a>How much karma do you need to reach the front page of Hacker News in 2020?</h3>
<p>Finally, I thought it would be interesting to see the relationship between karma and reaching the front page of Hacker News.</p>
<p>This first pie chart demonstrates the proportion of posts being posted by users with varying levels of karma.</p>

<p>Surprisingly, over 50% of posts are submitted by users with over 1,000 karma points. However, the largest single segment is by users with little to no karma.</p>

<p>The chart above shows that newer users (0-50 karma) make the front page proportionally less than more experienced users. Interestingly, really experienced users (over 10,000 karma) reached the front page significantly more than experienced users.</p>
<p><strong>Takeaway:</strong> It’s hard to say whether karma has anything to do with rankings. Likely, users with more karma are just better at posting content that resonates with the Hacker News audience.</p></div></div>]]>
            </description>
            <link>https://www.ankle.io/posts/hacker-news-analysis</link>
            <guid isPermaLink="false">hacker-news-small-sites-24634991</guid>
            <pubDate>Wed, 30 Sep 2020 01:43:17 GMT</pubDate>
        </item>
    </channel>
</rss>
