<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 10]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 10. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Sat, 10 Oct 2020 04:28:06 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-10.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Sat, 10 Oct 2020 04:28:06 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Yaan LeCun Spring 2020 DL Course (Videos, Slides, Jupyter Notebooks)]]>
            </title>
            <description>
<![CDATA[
Score 34 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24715307">thread link</a>) | @eugenhotaj
<br/>
October 7, 2020 | https://atcold.github.io/pytorch-Deep-Learning/ | <a href="https://web.archive.org/web/*/https://atcold.github.io/pytorch-Deep-Learning/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    <!-- Provide site root to javascript -->
    

    <!-- Work around some values being stored in localStorage wrapped in quotes -->
    

    <!-- Set the theme before any content is loaded, prevents flash -->
    

    <!-- Hide / unhide sidebar before it is displayed -->
    

    <nav id="sidebar" aria-label="Table of contents" aria-hidden="false">
        
        
    </nav>

    <div id="page-wrapper">
        <div class="page">
            

            <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
            

            <div id="content">
                <main>
                    <div class="page">
                      
                      <p><strong>DS-GA 1008 Â· SPRING 2020 Â· <a href="http://cds.nyu.edu/">NYU CENTER FOR DATA SCIENCE</a></strong></p>



<h2 id="description">Description</h2>

<p>This course concerns the latest techniques in deep learning and representation learning, focusing on supervised and unsupervised deep learning, embedding methods, metric learning, convolutional and recurrent nets, with applications to computer vision, natural language understanding, and speech recognition. The prerequisites include: <a href="https://cds.nyu.edu/academics/ms-curriculum/">DS-GA 1001 Intro to Data Science</a> or a graduate-level machine learning course.</p>

<h2 id="lectures">Lectures</h2>

<p><strong>Legend</strong>: ğŸ–¥ slides, ğŸ““ Jupyter notebook, ğŸ¥ YouTube video.</p>

<table>
<!-- =============================== HEADER ================================ -->
  <thead>
    <tr>
      <th>Week</th>
      <th>Format</th>
      <th>Title</th>
      <th>Resources</th>
    </tr>
  </thead>
  <tbody>
<!-- =============================== WEEK 1 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week01/01">â‘ </a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week01/01-1">History and motivation</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1Q7LtZyIS1f3TfeTGll3aDtWygh3GAfCb">ğŸ–¥ï¸</a>
        <a href="https://www.youtube.com/watch?v=0bMe_vCZo30">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week01/01-2">Evolution and DL</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week01/01-3">Neural nets (NN)</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/01-tensor_tutorial.ipynb">ğŸ““</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/02-space_stretching.ipynb">ğŸ““</a>
        <a href="https://www.youtube.com/watch?v=5_qrxVq1kvc">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== WEEK 2 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week02/02">â‘¡</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week02/02-1">SGD and backprop</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1w2jV_BT2hWzfOKBR02x_rB4-dfVUI6SR">ğŸ–¥ï¸</a>
        <a href="https://www.youtube.com/watch?v=d9vdh3b787Y">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week02/02-2">Backprop in practice</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week02/02-3">NN training</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/01%20-%20Spiral%20classification.pdf">ğŸ–¥</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/04-spiral_classification.ipynb">ğŸ““</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/05-regression.ipynb">ğŸ““</a>
        <a href="https://www.youtube.com/watch?v=WAn6lip5oWk">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== WEEK 3 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week03/03">â‘¢</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-1">Parameter transformation</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=18UFaOGNKKKO5TYnSxr2b8dryI-PgZQmC">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/FW5gFiJb-ig">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-2">CNN</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3">Natural signals' properties</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/02%20-%20CNN.pdf">ğŸ–¥</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/06-convnet.ipynb">ğŸ““</a>
        <a href="https://youtu.be/kwPWpVverkw">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== WEEK 4 ================================ -->
    <tr>
      <td rowspan="1"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week04/04">â‘£</a></td>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week04/04-1">1D convolutions</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/07-listening_to_kernels.ipynb">ğŸ““</a>
        <a href="https://youtu.be/OrBEon3VlQg">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== WEEK 5 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week05/05">â‘¤</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week05/05-1">Optimisation I</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1pwlGN6hDFfEYQqBqcMjWbe4yfBDTxsab">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/--NZb480zlg">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week05/05-2">Optimisation II</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week05/05-3">CNN, autograd</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/03-autograd_tutorial.ipynb">ğŸ““</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/extra/b-custom_grads.ipynb">ğŸ““</a>
        <a href="https://youtu.be/eEzCZnOFU1w">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== WEEK 6 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week06/06">â‘¥</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week06/06-1">CNN applications</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1opT7lV0IRYJegtZjuHsKhlsM5L7GpGL1">ğŸ–¥ï¸</a>
        <a href="https://drive.google.com/open?id=1sdeVBC3nuh5Zkm2sqzdScEicRvLc_v-F">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/ycbMGyCPzvE">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week06/06-2">RNNs and attention</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week06/06-3">Training RNNs</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/08-seq_classification.ipynb">ğŸ““</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/09-echo_data.ipynb">ğŸ““</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/04%20-%20RNN.pdf">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/8cAffg2jaT0">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== WEEK 7 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week07/07">â‘¦</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1">Energy-Based Models</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/tVwV14YkbYs">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-2">SSL, EBM</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-3">Autoencoders</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/05%20-%20Generative%20models.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/10-autoencoder.ipynb">ğŸ““</a>
        <a href="https://youtu.be/bggWQ14DD9M">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== WEEK 8 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week08/08">â‘§</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week08/08-1">Contrastive methods</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1Zo_PyBEO6aNt0GV74kj8MQL7kfHdIHYO">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/ZaVP2SY23nc">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week08/08-2">Regularised latent</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week08/08-3">Training VAEs</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/05%20-%20Generative%20models.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/11-VAE.ipynb">ğŸ““</a>
        <a href="https://youtu.be/7Rb4s9wNOmc">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== WEEK 9 ================================ -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week09/09">â‘¨</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week09/09-1">Sparsity</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1wJRzhjSqlrSqEpX4Omagb_gdIkQ5f-6K">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/Pgct8PKV7iw">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week09/09-2">World model, GANs</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week09/09-3">Training GANs</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/05%20-%20Generative%20models.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/pytorch/examples/tree/master/dcgan">ğŸ““</a>
        <a href="https://youtu.be/xYc11zyZ26M">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== WEEK 10 =============================== -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week10/10">â‘©</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week10/10-1">CV SSL I</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=16lsnDN2HIBTcRucbVKY5B_U16c0tNQhR">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/0KeR6i1_56g">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week10/10-2">CV SSL II</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week10/10-3">Predictive Control</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/09%20-%20Controller%20learning.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/14-truck_backer-upper.ipynb">ğŸ““</a>
        <a href="https://youtu.be/A3klBqEWR-I">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== WEEK 11 =============================== -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week11/11">â‘ª</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week11/11-1">Activations</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/file/d/1AzFVLG7D4NK6ugh60f0cJQGYF5OL2sUB">ğŸ–¥ï¸</a>
        <a href="https://drive.google.com/file/d/1rkiZy0vjZqE2w7baVWvxwfAGae0Eh1Wm">ğŸ–¥ï¸</a>
        <a href="https://drive.google.com/file/d/1tryOlVAFmazLLZusD2-UfReFMkPk5hPk">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/bj1fh3BvqSU">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week11/11-2">Losses</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week11/11-3">PPUU</a></td>
      <td>
        <a href="http://bit.ly/PPUU-slides">ğŸ–¥ï¸</a>
        <a href="http://bit.ly/PPUU-code">ğŸ““</a>
        <a href="https://youtu.be/VcrCr-KNBHc">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== WEEK 12 =============================== -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week12/12">â‘«</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-1">DL for NLP I</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/file/d/149m3wRavTp4DQZ6RJTej8KP8gv4jnkPW/">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/6D4EWKJgNn0">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-2">DL for NLP II</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-3">Attention &amp; transformer</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/10%20-%20Attention%20%26%20transformer.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/15-transformer.ipynb">ğŸ““</a>
        <a href="https://youtu.be/f01J0Dri-6k">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== WEEK 13 =============================== -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week13/13">â‘¬</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week13/13-1">GCNs I</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/file/d/1oq-nZE2bEiQjqBlmk5_N_rFC8LQY0jQr/">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/Iiv9R6BjxHM">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week13/13-2">GCNs II</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week13/13-3">GCNs III</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/11%20-%20GCN.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/16-gated_GCN.ipynb">ğŸ““</a>
        <a href="https://youtu.be/2aKXWqkbpWg">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== WEEK 14 =============================== -->
    <tr>
      <td rowspan="3"><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week14/14">â‘­</a></td>
      <td rowspan="2">Lecture</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week14/14-1">Structured Prediction</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/file/d/1qBu-2hYWaGYEXeX7kAU8O4S2RZ1hMjsk/">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/gYayCG6YyO8">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week14/14-2">Graphical methods</a></td></tr>
    <tr>
      <td rowspan="1">Practicum</td>
      <td><a href="https://atcold.github.io/pytorch-Deep-Learning/en/week14/14-3">Regularisation and Bayesian</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/07%20-%20Regularisation.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/12-regularization.ipynb">ğŸ““</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/08%20-%20Bayesian%20NN.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/13-bayesian_nn.ipynb">ğŸ““</a>
        <a href="https://youtu.be/DL7iew823c0">ğŸ¥</a>
      </td>
    </tr>
  </tbody>
</table>

<h2 id="people">People</h2>

<table>
  <thead>
    <tr>
      <th>Role</th>
      <th>Photo</th>
      <th>Contact</th>
      <th>About</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Instructor</td>
      <td><img src="https://atcold.github.io/pytorch-Deep-Learning/images/Yann.png" width="100" height="100"></td>
      <td><a href="https://twitter.com/ylecun">Yann LeCun</a><br>yann@cs.nyu.edu</td>
      <td>Silver Professor in CS at NYU<br>and Turing Award winner</td>
    </tr>
    <tr>
      <td>Instructor</td>
      <td><img src="https://avatars1.githubusercontent.com/u/2119355" width="100" height="100"></td>
      <td><a href="https://twitter.com/alfcnz">Alfredo Canziani</a><br>canziani@nyu.edu</td>
      <td>Asst. Prof. in CS at NYU</td>
    </tr>
    <tr>
      <td>Assistant</td>
      <td><img src="https://pbs.twimg.com/profile_images/1186879808845860864/czRv3g1G_400x400.jpg" width="100" height="100"></td>
      <td><a href="https://twitter.com/marikgoldstein">Mark Goldstein</a><br>goldstein@nyu.edu</td>
      <td>PhD student in CS at NYU</td>
    </tr>
    <tr>
      <td>Webmaster</td>
      <td><img src="https://pbs.twimg.com/profile_images/673997980370927616/vMXf545j_400x400.jpg" width="100" height="100"></td>
      <td><a href="https://twitter.com/ebetica">Zeming Lin</a><br>zl2799@nyu.edu</td>
      <td>PhD student in CS at NYU</td>
    </tr>
  </tbody>
</table>

<!--
|Grader|<img src="https://st3.depositphotos.com/13159112/17145/v/450/depositphotos_171453724-stock-illustration-default-avatar-profile-icon-grey.jpg" width="100" height="100">|Serkan Karakulak <br>sk7685@nyu.edu|
|Grader|<img src="https://st3.depositphotos.com/13159112/17145/v/450/depositphotos_171453724-stock-illustration-default-avatar-profile-icon-grey.jpg" width="100" height="100">|Raghav Jajodia <br>rj1408@nyu.edu|
|Grader|<img src="https://st3.depositphotos.com/13159112/17145/v/450/depositphotos_171453724-stock-illustration-default-avatar-profile-icon-grey.jpg" width="100" height="100">|Priyank Pathak <br>pp1953@nyu.edu|
|Grader|<img src="https://st3.depositphotos.com/13159112/17145/v/450/depositphotos_171453724-stock-illustration-default-avatar-profile-icon-grey.jpg" width="100" height="100">|Chiao-Hsun Wang <br>chw371@nyu.edu|
|Grader|<img src="https://st3.depositphotos.com/13159112/17145/v/450/depositphotos_171453724-stock-illustration-default-avatar-profile-icon-grey.jpg" width="100" height="100">|Pedro Vidal<br>pmh314@nyu.edu|
|Grader|<img src="https://st3.depositphotos.com/13159112/17145/v/450/depositphotos_171453724-stock-illustration-default-avatar-profile-icon-grey.jpg" width="100" height="100">|Bixing Yan <br>by783@nyu.edu|
-->

<h2 id="disclaimer">Disclaimer</h2>

<p>All other texts found on this site are lecture notes taken by students of the New York University during lectures given by Yann Le Cun, Alfredo Canziani, Ishan Misra, Mike Lewis and Xavier Bresson.
Thus the texts in English were written by about 130 people, which has an impact on the homogeneity of the texts (some write in the past tense, others in the present tense; the abbreviations used are not always the same; some write short sentences, while others write sentences of up to 5 or 6 lines, etc.).
It is possible that there may be some omissions: typing errors, spelling mistakes, etc. If you notice any, we invite you to submit a PR on the <a href="https://github.com/Atcold/pytorch-Deep-Learning/pulls">GitHub directory of the site</a> specifying with an <code>[EN]</code> that it concerns the English translation.</p>

<p>Wishing you a deep reading !</p>

                    </div>
                </main>

                <nav aria-label="Page navigation">
                    <!-- Mobile navigation buttons -->
                    <a rel="prev" href="https://atcold.github.io/pytorch-Deep-Learning" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i></i>
                    </a>

                    <a rel="next" href="https://atcold.github.io/pytorch-Deep-Learning/en/about/" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>

                    
                </nav>
            </div>
        </div>

        <nav aria-label="Page navigation">
                <a href="https://atcold.github.io/pytorch-Deep-Learning" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                    <i></i>
                </a>

                <a href="https://atcold.github.io/pytorch-Deep-Learning/en/about/" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                    <i></i>
                </a>
        </nav>

    </div>

    
    
    

    

    

</div>]]>
            </description>
            <link>https://atcold.github.io/pytorch-Deep-Learning/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24715307</guid>
            <pubDate>Thu, 08 Oct 2020 03:13:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[New York City thinks up to half of restaurants will close permanently [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 76 | Comments 120 (<a href="https://news.ycombinator.com/item?id=24715150">thread link</a>) | @bookofjoe
<br/>
October 7, 2020 | https://www.osc.state.ny.us/files/reports/osdc/pdf/nyc-restaurant-industry-final.pdf | <a href="https://web.archive.org/web/*/https://www.osc.state.ny.us/files/reports/osdc/pdf/nyc-restaurant-industry-final.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.osc.state.ny.us/files/reports/osdc/pdf/nyc-restaurant-industry-final.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24715150</guid>
            <pubDate>Thu, 08 Oct 2020 02:47:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I built an app to fix my depression]]>
            </title>
            <description>
<![CDATA[
Score 64 | Comments 39 (<a href="https://news.ycombinator.com/item?id=24715148">thread link</a>) | @zoozla
<br/>
October 7, 2020 | https://blog.elifiner.com/i-built-an-app-to-fix-my-depression/ | <a href="https://web.archive.org/web/*/https://blog.elifiner.com/i-built-an-app-to-fix-my-depression/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://blog.elifiner.com/i-built-an-app-to-fix-my-depression/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24715148</guid>
            <pubDate>Thu, 08 Oct 2020 02:47:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Recycling was a lie to sell more plastic, recycling industry veteran says]]>
            </title>
            <description>
<![CDATA[
Score 691 | Comments 308 (<a href="https://news.ycombinator.com/item?id=24714880">thread link</a>) | @vivekd
<br/>
October 7, 2020 | https://www.cbc.ca/documentaries/the-passionate-eye/recycling-was-a-lie-a-big-lie-to-sell-more-plastic-industry-experts-say-1.5735618 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/documentaries/the-passionate-eye/recycling-was-a-lie-a-big-lie-to-sell-more-plastic-industry-experts-say-1.5735618">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Less than 10 per cent of the plastics weâ€™ve used have been recycled. A new documentary reveals why</p><div><figure><div><p><img alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5755241.1602170985!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/157672506.jpg"></p></div><figcaption>trash on the beach<!-- --> <!-- -->(Getty Images)</figcaption></figure><p><span><p>Although our landfills and oceans are full of it, we are as dependent as ever on plastic. And since COVID-19, it's gotten worse.&nbsp;</p>  <p>Last year, Canada announced it was working on a ban of single-use plastics, which was initally&nbsp;<a href="https://www.cbc.ca/news/canada/toronto/single-use-plastics-covid-1.5683617">sidelined by the pandemic</a>. Recently, the government announced that <a href="https://www.cbc.ca/news/politics/single-use-plastics-1.5753327">many single-use plastics will be banned</a> by the end of 2021. At the same time, <a href="https://www.cbc.ca/news/canada/toronto/single-use-plastics-covid-1.5683617">CBC News reports</a> our single-use plastic use increased by 250 to 300 per cent as people tossed their personal protective equipment and stopped using reusable bags and containers over fears they would spread the virus.</p>  <p>What makes our lives convenient is also burying us. <a href="https://www.cbc.ca/passionateeye/episodes/plastic-wars"><strong><em>Plastic Wars</em></strong></a>, presented by <em>The Passionate Eye</em>, looks at the mounting crisis and how the industry has spent millions promoting recycling â€” just to sell more plastic.</p>  <h2>Less than 10% of the plastics we've used have been recycled</h2>  <p>Although activists sounded the alarm about plastic waste in the 1970s, the documentary claims from 1990 to 2010, plastic production more than doubled. We've been sorting our trash for decades, believing it would be recycled. But the truth is the vast majority of the plastic we use won't be. Over the last seven decades, <a href="https://www.oecd.org/environment/waste/policy-highlights-improving-plastics-management.pdf">less than 10 per cent of plastic waste has been recycled</a>.&nbsp;</p>  <p>That's because, says David Allaway, from the Oregon Department of Environmental Quality, the conversation has been almost exclusively about recycling and not reducing and reusing.</p>  <p><span><span><div><div role="button" tabindex="0" title="Plastic Wars: Recycling"><div><p><img src="https://thumbnails.cbc.ca/maven_legacy/thumbnails/1002/935/PlasticWars_Recycling_2500kbps_620x350_1755680835594.jpg" alt=""></p></div></div></div><span>Even as the plastic crisis worsens, the demand for plastic grows and plastic production is rapidly expanding. One issue? Only focusing on recycling, and not reducing the amount of plastic that we use.<!-- --> <!-- -->1:06</span></span></span></p>  <h2>Recycling logo was used as a green marketing tool, says industry expert</h2>  <p>In the '80s, the industry was at the centre of an environmental backlash. Fearing an outright ban on plastics, manufacturers looked for ways to get ahead of the problem. They looked at recycling as a way to improve the image of their product and started labeling plastics with the now ubiquitous chasing-arrows symbol with a number inside.&nbsp;</p>  <p>According to Ronald Liesemer, an industry veteran who was tasked with overseeing the new initiative, "Making recycling work was a way to keep their products in the marketplace."&nbsp;</p>  <p>Most consumers might have assumed the symbol meant the product was recyclable. But according to experts in the film, there was no economically viable way to recycle most plastics, and they have ultimately ended up in a landfill. This included plastic films, bags and the wrapping around packaged goods, as well as containers like margarine tubs.<br> "Our own customers â€¦ they would flat out say, 'It says it's recyclable right on it,'" says Coy Smith, former board member of the National Recycling Coalition. "And I'd be like, 'I can tell you, I can't give this away. There's no one that would even take it if I paid them to take it.'" He believes manufacturers used the symbol as a green marketing tool.</p>  <p>"If the public thinks that recycling is working, then they're not going to be as concerned about the environment," says Larry Thomas, another top industry official interviewed in <a href="https://www.cbc.ca/passionateeye/episodes/plastic-wars"><em><strong>Plastic Wars</strong></em></a>.</p>  <p>According to Lewis Freeman, a former vice-president with the Society of the Plastics Industry, many in the industry had doubts about recycling from the start. "There was never an enthusiastic belief that recycling was ultimately going to work in a significant way," he says.</p>  <p>Yet the plastic industry spent millions on ads selling plastics and recycling to consumers.</p>  <h2>Lots of our plastic was shipped to China, then Southeast Asia, for 'recycling'</h2>  <p>To solve the plastic waste problem, many recyclers started selling their product to China in the 1990s. According to recycling broker Sunil Bagaria, China took waste that North American recyclers couldn't use. "As long as it remotely resembled plastic, they wanted it," he says.</p>  <p>But they used the good stuff and disposed of the rest. And because of a growing plastic waste problem in that country, China finally stopped taking most imported plastic waste in 2018.</p>  <p>"We never asked the question, 'Are they doing it the right way? Are we damaging the environment more in the name of recycling?'" says Bagaria.</p>  <p>Now, Southeast Asian countries like Indonesia have picked up the plastic waste market. And although some North American plastics recyclers are following up to ensure their products are in fact being recycled, plastic waste is now a growing problem there, too.&nbsp;</p>  <p>In <a href="https://www.cbc.ca/passionateeye/episodes/plastic-wars"><em><strong>Plastic Wars</strong></em></a>, local activist Yuyan Ismawati visits a rural community where locals scour through a huge field of plastic waste for items of value and burn the rest. This creates health problems for the residents in addition to destroying the surrounding environment. "We are struggling to clean up the modern debris and modern litter in Indonesia, the additional burden of waste from overseas â€” I don't know how we are going to handle it," says Ismawati. "Americans need to know that your waste ended up here."</p>  <h2>Production of plastics expected to triple by 2050</h2>  <p>In 2020, roughly 60 years after concerns about plastic waste were first raised, the focus is still on the consumer to recycle, says Allaway, and not on the environmental impact of the product and overproduction by the industry.</p>  <p><span><span><div><div role="button" tabindex="0" title="Plastic Wars: Full Impact"><div><p><img src="https://thumbnails.cbc.ca/maven_legacy/thumbnails/1004/887/PlasticWars_FullImpact_2500kbps_620x350_1755684419745.jpg" alt=""></p></div></div></div><span>Consumers are constantly told that they should do their part to reduce plastic waste, but in reality, consumers have the lowest amount of leverage in reducing waste - it's plastic producers that should be reporting their full environmental impacts.<!-- --> <!-- -->1:56</span></span></span></p>  <p>According to <a href="https://www.cbc.ca/passionateeye/episodes/plastic-wars"><em><strong>Plastic Wars</strong></em></a> the problem is only going to get worse. By 2050, it's estimated the global production of plastic will triple. As the oil and gas industry â€” which provides the source materials for plastics â€” &nbsp;faces a future of declining demand for fuel, it has turned to other markets.&nbsp;</p>  <p>The stakes are high, says Annie Leonard, executive director of Greenpeace USA. "This is their lifeline," she says. "They are going to double down on single-use plastic like we have never seen. So we're heading towards a real battle.... This is the big war."&nbsp;</p>  <p>Watch <a href="https://www.cbc.ca/passionateeye/episodes/plastic-wars"><em><strong>Plastic Wars</strong></em></a> on <em>The Passionate Eye</em>.</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/documentaries/the-passionate-eye/recycling-was-a-lie-a-big-lie-to-sell-more-plastic-industry-experts-say-1.5735618</link>
            <guid isPermaLink="false">hacker-news-small-sites-24714880</guid>
            <pubDate>Thu, 08 Oct 2020 02:01:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Reverse Engineering the Japanese Sentence]]>
            </title>
            <description>
<![CDATA[
Score 37 | Comments 32 (<a href="https://news.ycombinator.com/item?id=24714136">thread link</a>) | @sova
<br/>
October 7, 2020 | https://japanesecomplete.com/reverse-engineer/ | <a href="https://web.archive.org/web/*/https://japanesecomplete.com/reverse-engineer/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://japanesecomplete.com/reverse-engineer/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24714136</guid>
            <pubDate>Wed, 07 Oct 2020 23:43:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A B.C. research project gave homeless people $7500 each. Results were surprising]]>
            </title>
            <description>
<![CDATA[
Score 25 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24713991">thread link</a>) | @cpncrunch
<br/>
October 7, 2020 | https://www.cbc.ca/news/canada/british-columbia/new-leaf-project-results-1.5752714 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/canada/british-columbia/new-leaf-project-results-1.5752714">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>The results of a B.C. research project that gave thousands of dollars&nbsp;to homeless people&nbsp;are in and, according to one researcher, could challenge stereotypes about people "living on the margins."</p><div><figure><div><p><img alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5756225.1602196801!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/strathcona-park.jpg"></p></div><figcaption>A Vancouver-based research project gave homeless individuals cash and tracked their progress for a year. The 50 cash transfer recipients not only found stable housing, they freed up space in shelters and, according to project data, saved the shelter system $8,100 per person over those 12 months.<!-- --> <!-- -->(Ben Nelms/CBC)</figcaption></figure><p><span><p>The results of a B.C. research project that gave thousands of dollars&nbsp;to homeless people&nbsp;are in and, according to one researcher, could challenge stereotypes about people "living on the margins."</p>  <p><a href="https://forsocialchange.org/impact">The&nbsp;New Leaf&nbsp;project </a>is&nbsp;a joint study started in 2018 by&nbsp;Foundations for Social Change, a Vancouver-based charitable organization, and the University of British Columbia. After giving homeless Lower Mainland residents cash payments of $7,500, researchers checked on them&nbsp;over a year to see how they were faring.</p>  <p>All 115 participants, ranging in age between 19 and 64,&nbsp;had been homeless&nbsp;for at least six months and were not struggling&nbsp;with serious substance use or mental health issues.&nbsp;Of those,&nbsp;50 people were chosen at random to be given the cash, while the others formed a control group that did not receive any money.</p>  <p>"I had no expectations and really high hopes," said Claire Williams, CEO of&nbsp;Foundations for Social Change, on CBC's&nbsp;<em>The Early Edition</em>&nbsp;on Tuesday.</p>  <p>What researchers found after 12 months, she said, was&nbsp;"beautifully surprising."</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5756210.1602196627!/cumulusImage/httpImage/image.jpg_gen/derivatives/original_300/strathcona-park.jpg 300w,https://i.cbc.ca/1.5756210.1602196627!/cumulusImage/httpImage/image.jpg_gen/derivatives/original_460/strathcona-park.jpg 460w,https://i.cbc.ca/1.5756210.1602196627!/cumulusImage/httpImage/image.jpg_gen/derivatives/original_620/strathcona-park.jpg 620w,https://i.cbc.ca/1.5756210.1602196627!/cumulusImage/httpImage/image.jpg_gen/derivatives/original_780/strathcona-park.jpg 780w,https://i.cbc.ca/1.5756210.1602196627!/cumulusImage/httpImage/image.jpg_gen/derivatives/original_1180/strathcona-park.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5756210.1602196627!/cumulusImage/httpImage/image.jpg_gen/derivatives/original_780/strathcona-park.jpg"></p></div><figcaption>Strathcona Park is the site of Vancouver's latest tent city, where hundreds of people without homes erected tents this summer.<!-- --> <!-- -->(Ben Nelms/CBC)</figcaption></figure></span></p>  <h2>Budget breakdown</h2>  <p>Not only did those who received the money spend fewer days homeless than those in the control group, they had also moved into stable housing after an average of three months, compared to those in the control group, who took an average of five months.</p>  <p>Those who received the money also managed it well over the course of a year.</p>  <p>"We saw people retain&nbsp;over $1,000 for 12 months, which is remarkable in the Lower Mainland," said Williams.</p>  <p>On average, cash recipients spent 52 per cent&nbsp;of their money&nbsp;on food and rent, 15 per cent&nbsp;on other items such as medications and bills, and 16 per cent on clothes and transportation.&nbsp;</p>    <p>Almost 70 per cent of people who received the payments were food secure after one month. In comparison, spending on alcohol, cigarettes and drugs went down, on average, by 39 per cent.</p>  <p>Too often people dismiss the idea of giving homeless people money because they assume it will be mismanaged, Williams said.</p>  <p>"It challenges stereotypes we have here in the West about how to help people living on the margins," she said.&nbsp;</p>  <h2>New beginnings</h2>  <p>Ray, whose last name project researchers did not release&nbsp;for privacy reasons, was living in an emergency shelter before receiving money from the New Leaf project.</p>  <p>He said the money helped him get housing and take a computer class he needed to work toward&nbsp;his goal of becoming a frontline worker for people with substance addictions.</p>  <p><em><strong>WATCH | Ray, a study participant, says what the money meant for him:</strong></em></p>  <p><span><span><iframe src="https://www.youtube.com/embed/icWhu5icsJ0" frameborder="no" title="YouTube content" allowfullscreen=""></iframe></span></span></p>  <p>"I kind of want to give back where I've came from," said Ray. "I might one day be that important person that has a powerful voice... a&nbsp;seed can grow into an oak tree."</p>  <p>According to Williams, providing people like Ray the cash they need to get ahead&nbsp;also helps Canadian taxpayers.</p>  <p>She said it costs, on average, $55,000&nbsp;annually for social and health services for one homeless individual.&nbsp;According to study data, the project saved the shelter system approximately $8,100 per person for a total of roughly $405,000 over one year for all 50.</p>    <p>"The common belief is that the status quo is cheap... in fact, it is incredibly&nbsp;expensive," said Williams.</p>  <p>According to the 2018 B.C. Homeless Count, there are about 7,600 homeless people living in the province â€” meaning a group of 115 study participants is relatively small.</p>  <p><strong>To hear the complete interview with&nbsp;Claire Williams, CEO of&nbsp;Foundations for Social Change, on <em>The Early Edition, </em><a href="https://www.cbc.ca/listen/live-radio/1-91-the-early-edition/clip/15801752-cash-homeless-key-getting-people-living-streets">tap here.</a></strong></p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/canada/british-columbia/new-leaf-project-results-1.5752714</link>
            <guid isPermaLink="false">hacker-news-small-sites-24713991</guid>
            <pubDate>Wed, 07 Oct 2020 23:21:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Lisp and Haskell (2015)]]>
            </title>
            <description>
<![CDATA[
Score 69 | Comments 56 (<a href="https://news.ycombinator.com/item?id=24712207">thread link</a>) | @dunefox
<br/>
October 7, 2020 | https://markkarpov.com/post/lisp-and-haskell.html | <a href="https://web.archive.org/web/*/https://markkarpov.com/post/lisp-and-haskell.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <div>
          

<p><a href="https://markkarpov.com/tag/haskell.html">haskell</a></p><p>
  <em>
    Published on October 23, 2015, last updated November 23, 2019
  </em>
</p>

  <p>Lisp and Haskell are arguably some of the more peculiar languages out there.
It is always interesting to compare languages, so let me entertain you with
a story how I finally decided which of them is better.</p>
<p>When I first found out about Common Lisp it took my breath away. Seriously,
Lisp has consistent syntax, good design, and unique metaprogramming
capabilities. After Common Lisp, I learned a few other languages, some of
them out of necessity, others because of curiosity: Python, JavaScript,
Prolog, Clojure, and Haskell. I also was doing C and C++ in the past, but I
donâ€™t touch them now. Until recently I considered Common Lisp the best
language I know, and probably the most powerful language in existence.</p>
<p>The fact is, I know what Common Lisp is and what it can do, but the days
when I actually hacked Lisp (more or less) regularly are long gone, and Iâ€™m
mainly doing Haskell these days.</p>
<h2 id="goodbye-lisp">Goodbye, Lisp</h2>
<p>Today I actually have had a chance to compare my productivity with Common
Lisp and Haskell. I decided to spend a few hours on my open source projects.
First, I refactored <a href="https://github.com/mrkkrp/megaparsec">Megaparsec</a>, and that was nice and easy,
but I didnâ€™t notice this because Iâ€™m already used to the level of efficiency
Haskell gives me.</p>
<p>Next, a user of one of my Common Lisp libraries opened an issue asking to
improve one thing a bit. I estimated the required work in 15 minutes of time
and started Common Lisp hacking for the first time in a couple of months.</p>
<p>It took about 1 hour to write about 20 lines of trivial code. Of course one
might say that I just forgot the details. Yet, from my point of view the
real reasons are:</p>
<ul>
<li>
<p>Common Lisp is dynamically typed and the compiler cannot help you when you
write your code. (Well, it can help you a bit, making sure that your code
is syntactically correct and all your declared variables are used for
something.)</p>
</li>
<li>
<p>Common Lisp mixes functional code with code that has side effects. To
write idiomatic Common Lisp, you usually have to mix functional code with
not-so-functional approaches. See how this works below.</p>
</li>
<li>
<p>Common Lispâ€™s standard library (the functions that are available to you as
part of the ANSI Common Lisp standard) is quite poor by modern standards.
A lot of useful functions are missing. There are libraries, but Iâ€™ll get
to them.</p>
</li>
</ul>
<p>Itâ€™s essential for that library I was working on to have minimal
dependencies, so I came up with this function in bare Common Lisp to add
padding to every line of text except for the first line:</p>
<div><pre><code><span>(</span><span>defun</span><span> add-text-padding </span><span>(str &amp;key padding newline)</span>
<span>  </span><span>"Add padding to text STR. Every line except for the first one, will be</span>
<span>prefixed with PADDING spaces. If NEWLINE is non-NIL, newline character will</span>
<span>be prepended to the text making it start on the next line with padding</span>
<span>applied to every single line."</span>
<span>  (</span><span>let</span><span> ((str (</span><span>if</span><span> newline</span>
<span>                 (</span><span>concatenate</span><span> 'string (</span><span>string</span><span> </span><span>#\N</span><span>ewline) str)</span>
<span>                 str)))</span>
<span>    (</span><span>with-output-to-string</span><span> (s)</span>
<span>      (</span><span>map</span><span> 'string</span>
<span>           (</span><span>lambda</span><span> (x)</span>
<span>             (</span><span>princ</span><span> x s)</span>
<span>             (</span><span>when</span><span> (</span><span>char=</span><span> x </span><span>#\N</span><span>ewline)</span>
<span>               (</span><span>dotimes</span><span> (i padding)</span>
<span>                 (</span><span>princ</span><span> </span><span>#\S</span><span>pace s))))</span>
<span>           str))))</span>
</code></pre></div>
<p>In case you donâ€™t speak Common Lisp, let me highlight some parts of the
code:</p>
<ul>
<li>
<p><code>concatenate</code> needs to know the type of its output, so we pass it a symbol
specifying type of desired result as the first argument.</p>
</li>
<li>
<p><code>(string #\Newline)</code> constructs a line containing a single newline
character. There is no syntax in Common Lisp to write something like
<code>"\n"</code>. The alternative approach would be <code>(format nil "~%")</code>. There is no
syntax for all other special characters if you want to put them into
string. To be fair, you have multi-line string literals without funny
escaping instead, which is vital for doc-strings and the like.</p>
</li>
<li>
<p><code>(map 'string â€¦)</code> is used to loop through characters in a string. Note
that here we use <code>map</code> function as a helper for a rather imperative
procedureâ€”printing results to new string using a temporarily created
stream <code>s</code> (with the help of <code>with-output-to-string</code>). But thatâ€™s
idiomatic in Common Lisp.</p>
</li>
</ul>
<p>When I ran this in the REPL, I got the following:</p>
<div><pre><code><span>; SLIME 2015-10-18</span>
<span>CL-USER&gt; (asdf:load-system :unix-opts)</span>
<span>T</span>
<span>CL-USER&gt; (</span><span>in-package</span><span> :unix-opts)</span>
<span>#&lt;PACKAGE </span><span>"UNIX-OPTS"</span><span>&gt;</span>
<span>OPTS&gt; (</span><span>defvar</span><span> *foo* </span><span>(</span><span>format</span><span> </span><span>nil</span><span> </span><span>"first line~%second line~%third line"</span><span>))</span>
<span>*FOO*</span>
<span>OPTS&gt; *foo*</span>
<span>"first line</span>
<span>second line</span>
<span>third line"</span>
<span>; compiling (DEFUN ADD-TEXT-PADDING ...)</span>
<span>OPTS&gt; (add-text-padding *foo* :padding </span><span>10</span><span>)</span>
<span>; Evaluation aborted on #&lt;TYPE-ERROR expected-type: CHARACTER datum: NIL&gt;.</span>
</code></pre></div>
<p>The debugger popped up and told me in plain English:</p>
<blockquote>
<p>The value NIL is not of type CHARACTER.</p>
</blockquote>
<p>It is difficult to argue with, <code>nil</code> is definitely not a character. But why
the heck do I get this? Can you tell? Please try as hard as you can! <em>(The
answer is at the end of the blog post.)</em></p>
<p>I decided that I wonâ€™t hack Common Lisp anymore. Thatâ€™s great and expressive
language, but I want to write in something Iâ€™m efficient with.</p>
<h2 id="productivity-of-haskell-programmer">Productivity of Haskell programmer</h2>
<p>I use <a href="https://gnu.org/software/emacs/">Emacs</a> for almost everything that is
related to text. One package I love in particular is
<a href="http://www.flycheck.org/">Flycheck</a>. When I edit Haskell source code,
Flycheck is running GHC with <code>-Wall</code> flag and
<a href="https://github.com/ndmitchell/hlint">HLint</a> in the background and displays
warnings and errors interactively underlining my source code. This is a
convenient feature for any language, but only Haskell with its type system
takes this sort of tool to its limits.</p>
<p>In fact, this non-stop interactive conversation with compiler is the most
efficient programming workflow Iâ€™ve ever used. Combined with the fact that
<em>if your code compiles, it probably works</em>, Haskell must be the most
efficient (with respect to human resources) programming language in
existence just because of the static type system that works as a powerful
ally for the programmer. Of course, bugs can live in Haskell code too, but
Iâ€™m not saying we should abandon writing tests.</p>
<h2 id="problems-of-common-lisp">Problems of Common Lisp</h2>
<p>Speaking of tests, recently I discovered that Zach Beane AKA Xach, an
Ã¼ber-level Common Lisp hacker <a href="http://xach.livejournal.com/278047.html?thread=674335#t674335">doesnâ€™t usually write tests</a>.
FYI, he is the author of <a href="https://www.quicklisp.org/beta/">Quicklisp</a>, that is something like (but
not quite) Cabal or Stack. Quicklisp is de-facto the only widely used
library manager in Common Lisp world, and so itâ€™s written in Common Lisp and
<a href="https://github.com/quicklisp/quicklisp-client">doesnâ€™t have any tests</a>. It is a wonder for me how it
works. Usually when a project is big enough I start to have doubts whether
all parts of it still work after some changes, so I cannot imagine you can
do a thing like Quicklisp without tests and be confident about the result.</p>
<p>But you know what, Lisp, and its most advanced dialect (IMO), Common Lisp is
really cool. If you donâ€™t believe me, you can read <a href="http://www.paulgraham.com/avg.html">Paul Graham</a> at any
time. The author can tell you what a great language Common Lisp is on
many-many pages. I donâ€™t remember where I read this, but he has something
like â€œThere is the problem of lacking libraries, but on a big enough project
benefits of the language itself outweigh the lack of libraries.â€</p>
<p><em>Well, take any high-level language like Python, which have all the nice
libraries, and for project of any size it will be better than Common Lisp.
Macros are missing, but you can live without macros after all.</em></p>
<p>Common Lisp doesnâ€™t have enough high-quality, actively maintained libraries.
The fact is, there are some pearls like <a href="https://github.com/fukamachi/caveman">caveman</a> or
<a href="https://github.com/stumpwm/stumpwm">stumpwm</a>, but most libraries donâ€™t look good enough. Sometimes you
start thinking that if you want to end up with a great project youâ€™ll need
to write your own libraries (which youâ€™ll probably do, like many people
before you, not that it has improved the situation though).</p>
<p>Another problem is that some widely-used Common Lisp libraries have no
documentation at all. If youâ€™re to understand how to use them, <em>read the
source code</em>. I can name a couple of them, but I donâ€™t want to do so,
because I donâ€™t think itâ€™s polite. Iâ€™ve opened an issue on GitHub of one
quite popular library, asking the maintainer to write documentation. After 6
months itâ€™s still not written (strange, right?). In my opinion, this is not
a serious approach to maintaining your code.</p>
<p>When I was interested in Common Lisp, I had an idea of a pet project to help
me remember all sorts of French words and verbs in particular. Of course I
wanted to do the whole thing decently, even though itâ€™s console app, it
should have decent interface and work smoothly in general. I succeeded, but
I had to do a lot more than I would need to do if I wrote it in, say Python.
This is how (in retrospect I understand) less powerful Python would be
better fit for this (or almost any) project.</p>
<h2 id="the-curse-of-dynamic-languages">The curse of dynamic languages</h2>
<p>There is a blog post called <a href="https://existentialtype.wordpress.com/2011/03/19/dynamic-languages-are-static-languages/"><em>Dynamic Languages are Static
Languages</em></a>. In short, the author makes the point that
dynamic langauges are static languages but with one huge type including all
possible values. Here is a paragraph I find important:</p>
<blockquote>
<p>And this is precisely what is wrong with dynamically typed languages:
rather than affording the <em>freedom</em> to ignore types, they instead impose
the <em>bondage</em> of restricting attention to a <em>single</em> type! Every single
value has to be a value of that type, you have no choice! Even if in a
particular situation we are absolutely certain that a particular value is,
say, an integer, we have no choice but to regard it as a value of the â€œone
true typeâ€ that is <em>classified</em>, not typed, as an integer. Conceptually,
this is just rubbish, but it has serious, tangible penalties. For one, you
are depriving yourself of the ability to state and enforce the <em>invariant</em>
that the value at a particular program point must be an integer. For
another, you are imposing a serious bit of run-time overhead to represent
the class itself (a tag of some sort) and to check and remove and apply
the class tag on the value each time it is used.</p>
</blockquote>
<p>The lack of the power to express meaning of your program on type level is
another downside of Lisp. (You can add types in Common Lisp too, but thatâ€™s
used solely for optimization. Common Lisp can be almost as fast â€¦</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://markkarpov.com/post/lisp-and-haskell.html">https://markkarpov.com/post/lisp-and-haskell.html</a></em></p>]]>
            </description>
            <link>https://markkarpov.com/post/lisp-and-haskell.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24712207</guid>
            <pubDate>Wed, 07 Oct 2020 20:13:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Designing a New Rust Class at Stanford: Safety in Systems Programming]]>
            </title>
            <description>
<![CDATA[
Score 199 | Comments 46 (<a href="https://news.ycombinator.com/item?id=24711314">thread link</a>) | @ksml
<br/>
October 7, 2020 | https://reberhardt.com/blog/2020/10/05/designing-a-new-class-at-stanford-safety-in-systems-programming.html | <a href="https://web.archive.org/web/*/https://reberhardt.com/blog/2020/10/05/designing-a-new-class-at-stanford-safety-in-systems-programming.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <div>
                <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Writing quality software is hard. Sometimes, software breaks in entertaining
ways. However, when software runs everything from personal assistants like
Alexa and Google Home to banking to elections, some bugs can be much more
severe.</p>

<p>This past quarter, Armin Namavari and I tried teaching a class about how to
write software that sucks just a <em>little</em> less. We focused on common problems
in computer systems caused by certain kinds of silly (but very serious)
mistakes, such as issues of memory safety and thread safety. The core theme of
the class was, <em>What are common problems with systems programming right now?
How are people responding to those issues? How do those measures fall short?</em>
We wanted students to be aware of problems that have plagued the industry for
decades, and we wanted to teach students how to use tools and mental models
that people have developed to combat those issues. However, these tools are
imperfect, and we also wanted students to experience and understand the
limitations of such tools to be better aware of what to watch out for when
building systems.</p>

<!--more-->

<p>In particular, we focused on teaching the Rust programming language as a way to
build better habits and combat mistakes endemic to C- and C++-based software.
In many ways, Rust <em>requires</em> good practices, and it has an educational
compiler with helpful error messages that help students learn. Additionally, we
looked at how lessons from Rust can be applied to write better code in C++, and
we taught students about tools that can be used to detect common mistakes
before they become a problem.</p>

<p>In contrast with a typical security class, we aimed to build a robust software
engineering aspect into the course, giving code-heavy assignments and trying to
improve studentsâ€™ processes rather than merely giving awareness about common
problems. Our goal with this class was to train students to be better software
developers, regardless of what programming language they end up using.</p>

<p>I think the class went quite well, and student evaluations were extremely
positive. Even before the quarter ended, students told us that the class was
extremely helpful for implementing and debugging assignments in other classes.
We hope to teach the class again this coming fall, and are looking for input on
how it might be improved.</p>

<p>This blog post aims to be a summary of what we did, why we did it, and what we
are thinking about changing for the future. Itâ€™s long, but written so you can
skip around to whatever is interesting to you. Hereâ€™s an outline:</p>

<!--* [What is systems programming?](#what-is-systems-programming)-->
<ul>
  <li><a href="#what-is-safety-and-why-should-we-care">What is safety, and why should we care?</a></li>
  <li><a href="#imagining-safety-education">Imagining safety education</a>
    <ul>
      <li><a href="#should-there-be-a-safety-class">Should there be a â€œsafety classâ€?</a></li>
      <li><a href="#how-would-this-be-different-from-a-security-class">How would this be different from a security class?</a></li>
      <li><a href="#how-does-one-teach-safe-programming">How does one teach safe programming?</a></li>
    </ul>
  </li>
  <li><a href="#summary-of-the-class">Summary of the class</a>
    <ul>
      <li><a href="#lectures">Lectures</a></li>
      <li><a href="#assignments">Assignments</a></li>
    </ul>
  </li>
  <li><a href="#survey-results">Survey results</a></li>
  <li><a href="#takeaways">Takeaways</a>
    <ul>
      <li><a href="#is-there-a-place-for-a-safety-class-in-a-cs-curriculum">Is there a place for a safety class in a CS curriculum?</a></li>
      <li><a href="#whats-it-like-for-students-to-learn-rust-in-a-short-time-frame">Whatâ€™s it like for students to learn Rust in a short time frame?</a></li>
      <li><a href="#should-we-try-to-incorporate-rust-into-the-stanford-core-curriculum">Should we try to incorporate Rust into the Stanford core curriculum?</a></li>
      <li><a href="#should-this-be-a-standalone-class-or-should-it-remain-an-addon-to-cs-110">Should this be a standalone class, or should it remain an addon class to CS 110?</a></li>
    </ul>
  </li>
  <li><a href="#general-teaching-lessons-learned">General teaching lessons learned</a></li>
</ul>

<p>Major thanks go to Armin Namavari for being a wonderful co-instructor, Sergio
Benitez for giving an excellent guest lecture, Will Crichton for providing
feedback and guidance in designing the class, Jerry Cain for giving us the
opportunity to teach and giving encouragement throughout, and Rakesh Chatrath,
Jeff Tucker, Vinesh Kannan, John Deng, and Shiranka Miskin for reviewing drafts
of this post.</p>

<!--
## What is systems programming?

In this class, we wanted to focus on safety in systems programming &mdash; but what
*is* systems programming, anyways?

"Systems programming" is a frequently-used term that I have never heard a clear
definition for, despite specializing in this track in undergrad. While it is
used in many different ways, I think about it like this: **Systems programming
is when you spend more time thinking about hardware than humans.** An
application programmer must think, *what does a user of my software need, and
how can I implement that in code?* An application programmer might work with
different programming languages and libraries, but rarely needs to think about
exactly what the hardware is doing under the hood. By contrast, a systems
programmer must think, *how can I teach this hardware new tricks in order to do
the things we need to do?* A systems programmer spends much more time thinking
about when memory is allocated, when a processor might switch contexts, when
data might be passed over a network, etc.
-->

<!-- TODO: picture of restaurant -->

<!--
As an application developer, you spend more time implementing business logic using whatever programming languages and tools you've decided to use. You're trying to design something that fits within the physical constraints of whatever you're running on. As a systems programmer, you spend much more time thinking about when memory gets allocated, when a processor switches contexts, when data gets passed over a network, etc.

Humans are still important (I'll admit I deemphasized humans partially for the alliteration), but the focus is less on end users and more on supporting the application developers and the application software that builds on top

Diagram: trying to show humans walking on top, hardware on bottom, and application developers standing on top of the systems developers
Diagram: floor is programming languages, libraries, and abstractions
Have some plumbers, carpenters, etc under the floor, and application devs servicing customers on top

If a computer were a restaurant, systems programmers would be the plumbers etc, and application programmers would be the chefs, waiters, etc

Examples of systems software: the code generating directions for Google Maps, Google Chrome, the infrastructure that does transcription for Siri (not the AI algorithms, but everything in between your phone and the algorithms), banking infrastructure, car firmware, etc.
-->

<h2 id="what-is-safety-and-why-should-we-care">What is safety, and why should we care?</h2>

<p>Safety is an unfortunately vague term lacking a great definition, but for our
purposes, weâ€™ll say <strong>safety is about avoiding harmful mistakes.</strong> I view
safety as being concerned with the subset of potentially serious bugs: if a
button on a website renders as purple instead of blue, thatâ€™s a bug we might
not care much about, but if bank account software allows users to withdraw the
same $1000 multiple times, or if autonomous vehicle software can fail under
certain circumstances, thatâ€™s a more concerning problem.</p>

<p>Safety is particularly relevant in systems programming because systems
programming is <em>hard</em>. Systems programming often involves pushing the limits of
what hardware can do, and often involves reasoning about the state of multiple
threads sometimes even distributed across thousands of machines. Additionally,
for performance and historical reasons, the majority of systems software is
written in C or C++, which are <em>notoriously</em> difficult to use correctly.
Reasoning about pointers and memory is hard, and C and C++ do little to help.
C/C++â€™s weak type systems and poorly defined
specifications mean they will happily accept <a href="https://www.radford.edu/ibarland/Manifestoes/whyC++isBad.shtml">clearly broken code with no
sensible
interpretation</a>.
Even worse, there are countless minefields where the languagesâ€™ poor designs
are just begging for mistakes to happen. Simple functions like <code>strcpy</code>, which
copies a string from one place in memory to another, are extremely easy to
misuse and have been the cause of countless <a href="https://pointerless.wordpress.com/2012/02/26/strcpy-security-exploit-how-to-easily-buffer-overflow/">security
vulnerabilities</a>.
The <code>strncpy</code> function was introduced to address the weaknesses of <code>strcpy</code>,
yet <a href="https://devblogs.microsoft.com/oldnewthing/20050107-00/?p=36773"><code>strncpy</code> turns out to be almost just as
bad</a>. Even
<a href="https://stackoverflow.com/questions/7459630/how-can-a-format-string-vulnerability-be-exploited"><code>printf</code> can lead to security
vulnerabilities</a>
when called the wrong way.</p>

<figure>
    <a href="https://reberhardt.com/blog/images/designing-cs-110l/strcpy.png">
        <img src="https://reberhardt.com/blog/images/designing-cs-110l/strcpy.png" alt="">
    </a>
    
</figure>

<p>Also, as systems software provides the foundation on which other software runs,
itâ€™s particularly important to get right. Many real-world examples demonstrate
the severe impact of the aforementioned issues.  One of my favorite examples is
presented in <a href="http://www.autosec.org/pubs/cars-usenixsec2011.pdf">Comprehensive Experimental Analyses of Automotive Attack
Surfaces</a>.  Itâ€™s a great
read, but as a summary, the authors bought a popular car and attempted to find
as many ways as possible to remotely hijack the car without having physical
access. They examined vectors such as wireless key fobs, Bluetooth, and even
the tire pressure monitoring system (which uses wireless signals to transmit
information from sensors in the tires). Every vector was found to be
exploitable, many of them trivially so. For example, the Bluetooth software had
â€œover 20 calls to <code>strcpy</code>, none of which were clearly safe.â€ The authors only
looked at the first instance of <code>strcpy</code>, and found that it copies data to the
stack when handling a Bluetooth configuration command without checking the
length of the string. This results in a trivially exploitable buffer overflow
that allows a paired device to execute arbitrary code in the media system.
Since the subsystems in most cars lack isolation, compromising one subsystem
(such as the media player) can result in the compromise of the entire car. In
2015, researchers demonstrated this, <a href="https://www.wired.com/2015/07/hackers-remotely-kill-jeep-highway/">remotely killing a Jeep that was driving
on the
highway</a>.</p>

<p>This isnâ€™t just a problem with the automotive industry. <a href="https://blog.zimperium.com/whatsapp-buffer-overflow-vulnerability-under-the-scope/">Professional</a>
<a href="https://www.theregister.com/2019/08/06/qualcomm_android_security_patches/">programmers</a>
<a href="https://www.biometricupdate.com/202006/acronis-reports-critical-flaws-in-geovision-biometric-devices-man-in-the-middle-attack-risks">across</a>
<a href="https://www.zdnet.com/article/critical-security-flaw-schneider-industrial-software-power-plants-vulnerabilty/">many</a>
<a href="https://blog.zecops.com/vulnerabilities/youve-got-0-click-mail/">industries</a>
<a href="https://www.theverge.com/2017/5/12/15630354/nhs-hospitals-ransomware-hack-wannacry-bitcoin">regularly</a>
<a href="https://tools.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-sdwanbo-QKcABnS2">make</a>
<a href="https://threatpost.com/netgear-zero-day-takeover-routers/156744/">simple</a>
<a href="https://gadgets.ndtv.com/mobiles/news/samsung-critical-bug-fix-skia-sve-2020-16747-zero-click-vulnerability-2224867">but</a>
<a href="https://redmondmag.com/articles/2020/07/16/cisa-windows-server-dns-vulnerability.aspx">serious</a>
<a href="https://threatpost.com/google-squashes-high-severity-flaws-in-chrome-browser/154424/">mistakes</a>.</p>

<p>This seems like something we should be talking about. Would you hand a
chemistry student a bunch of volatile chemicals that regularly explode in
professional labs without a robust discussion of safety?  Probably not. Yet
thatâ€™s effectively what our curriculums are doing. Weâ€™re handing students a
series of tools that professionals routinely shoot themselves in the foot with,
and we arenâ€™t having a substantial discussion of precautions we can take to
avoid potentially life-threatening mistakes.</p>

<figure>
    <a href="https://reberhardt.com/blog/images/designing-cs-110l/chem-lab.png">
        <img src="https://reberhardt.com/blog/images/designing-cs-110l/chem-lab.png" alt="In many ways, our CS curriculums are like inviting students into a chem lab without any discussion of safety.">
    </a>
    
    <figcaption>In many ways, our CS curriculums are like inviting students into a chem lab without any discussion of safety.</figcaption>
    
</figure>

<p>One might argue that the perils of strcpy are nothing like the dangers of a
fully-stocked chemistry lab; thereâ€™s no danger of students dying in front of
the computer here. (Well, we hope.) However, I argue that we deal with dangers
on a much larger scale.  One line of code can easily affect millions (or
billions) of people, and the impacts of our code can be much greater than we
realize, even when we arenâ€™t working on software for cars (which <a href="https://www.safetyresearch.net/blog/articles/toyota-unintended-acceleration-and-big-bowl-%E2%80%9Cspaghetti%E2%80%9D-code">weâ€™ve killed
people
with</a>)
or medical devices (which <a href="https://hackaday.com/2015/10/26/killed-by-a-machine-the-therac-25/">weâ€™ve also killed people
with</a>). It
may seem that the worst-case bugs in a file sharing server would simply prevent
users from sharing files, but one such bug led to the <a href="https://www.telegraph.co.uk/technology/2018/10/11/wannacry-cyber-attack-cost-nhs-92m-19000-appointments-cancelled/">significant disruption
of the National Health Service in the
UK</a>.
Non-critical emergencies had to be refused. It may seem that the worst-case
bugs in a web application library would simply take down some websites, but one
such bug led to the <a href="https://www.csoonline.com/article/3444488/equifax-data-breach-faq-what-happened-who-was-affected-what-was-the-impact.html">exfiltration of extremely sensitive data on nearly every
American adult with a credit
history</a>.</p>

<!--We've spent a lot of effort over the last three decades finding ways to improve
the safety of code. We have linters, which can detect unsafe patterns in code
(e.g. calls to `strcpy`, or use of uninitialized memory). We have fuzzers and
sanitizers, which can stress test our code and identify memory errors and data
races. Most promising (in my opinion), we have new progamming languages such as
Rust and Swift which are safer, competitive on performance, and showing
potential for replacing C and C++ in the settings that those languages have
typically dominated. These languages can prevent entire classes of mistakes
that keep recurring in C and C++ codebases even with the use of other safety
tools. Linters, sanitizers, fuzzers, and static analyzers all help to catch
mistakes, but they don't *prevent* them, and they can't catch everything. While
these new languages are not a panacea and have their own problems, they make
massive strides towards squashing issues we haven't been able to address via
other means. Despite having some of the best security and development practices
in the world, Google Chrome (written in C++) is still plagued with memory
errors that would have been entirely prevented with Rust. Recently, they found
that [70% of security vulnerabilities were caused by memory
errors](https://www.chromium.org/Home/chromium-security/memory-safety).-->

<p>Precautions and safety measures <em>do</em> exist, but people arenâ€™t using them. Part
of this may be because the tooling isnâ€™t good enough or easy enough to use.
Part of this may be because there hasnâ€™t been enough time to see mass adoption.
But I think part of this may also be because of a lack of education and
awareness surrounding these issues. We can teach C and C++ and hope that
students will learn good habits and learn how to use static analyzers,
sanitizers, fuzzers, and safer languages on the job, but then have we not
failed them as educators? Seeing that software engineers keep making pretty
basic mistakes with critical impact, it seems that something is wrong and we
should be trying to do more.</p>

<h2 id="imagining-safety-education">Imagining safety education</h2>

<p>So, we should talk more about safety. But how should we go about it?</p>

<h3 id="should-there-be-a-safety-class">Should there be a â€œsafety classâ€?</h3>

<p>In planning this class, we couldnâ€™t find any other programming safety class out
there. Is that because no one has thought to do it yet, or is it because it is
better to teach safety in context alongside more central material?</p>

<p>Particularly because â€œsafetyâ€ is so broad, it does seem helpful
to cover best practices and helpful tricks/tools while introducing new
material. However, there were two reasons we felt it might make sense to teach
a class entirely focused on safety.</p>

<p>First, teaching a separate safety class gives us room to experiment with
teaching new material that would be difficult to integrate into existing</p></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://reberhardt.com/blog/2020/10/05/designing-a-new-class-at-stanford-safety-in-systems-programming.html">https://reberhardt.com/blog/2020/10/05/designing-a-new-class-at-stanford-safety-in-systems-programming.html</a></em></p>]]>
            </description>
            <link>https://reberhardt.com/blog/2020/10/05/designing-a-new-class-at-stanford-safety-in-systems-programming.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24711314</guid>
            <pubDate>Wed, 07 Oct 2020 18:47:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Google Interview Questions Deconstructed: The Knightâ€™s Dialer]]>
            </title>
            <description>
<![CDATA[
Score 67 | Comments 100 (<a href="https://news.ycombinator.com/item?id=24711094">thread link</a>) | @thanato0s
<br/>
October 7, 2020 | https://alexgolec.dev/google-interview-questions-deconstructed-the-knights-dialer/ | <a href="https://web.archive.org/web/*/https://alexgolec.dev/google-interview-questions-deconstructed-the-knights-dialer/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<p>This is the second in a series of posts in which I share my advice for candidates interviewing for tech companies, drawing on my experience as an engineer and interviewer at Google. If you havenâ€™t already, take a look at the <a href="https://alexgolec.dev/introducing-google-interview-questions-deconstructed/">introduction</a> to this series.</p><p><em>Before I start, a disclaimer: while interviewing candidates is one of my professional responsibilities, this blog represents my personal observations, my personal anecdotes, and my personal opinions. Please donâ€™t mistake this for any sort of official statement by or about Google, Alphabet, or any other person or organization.</em></p><p>This was the first problem I used during my interviewing career, and it was also the first to leak and get banned. I like it because it hits number of sweet spots:</p><ul><li>Itâ€™s easy to state and understand.</li><li>It has a number of solutions, each requiring varying degrees of algorithms and data structures knowledge. Also, a little bit of insight goes a long way.</li><li>Each solution can be implemented in relatively few lines, making it perfect for a time-constrained environment.</li></ul><p>If youâ€™re a student or otherwise applying to tech jobs, my hope is that youâ€™ll come away from reading this with a better understanding of what to expect from interview problems. If youâ€™re an interviewer, Iâ€™d like to share my thought process and stylistic approach to interviewing, the better to inform others and solicit comments.</p><p>Note Iâ€™ll be writing code in Python. I like Python because itâ€™s easy to learn, compact, and has an absolutely massive standard library. Candidates like it, too: even though we impose no language constraints, 90% of people I interview use Python. Also I use Python 3 because câ€™mon, itâ€™s 2018.</p><p><em>Join <a href="https://discord.gg/cKFppJ">our Discord</a> to discuss this problem with the author and the community! </em></p><p>Imagine you place a knight chess piece on a phone dial pad. This chess piece moves in an uppercase â€œLâ€ shape: two steps horizontally followed by one vertically, or one step horizontally then two vertically:</p><figure><img src="https://alexgolec.dev/content/images/2020/08/1-pE4b3hqGDv7pKivQTQZyPw.png" alt="Image for post"><figcaption>Pay no attention to the poorly-redacted star and pound keys</figcaption></figure><p>Suppose you dial keys on the keypad using only hops a knight can make. Every time the knight lands on a key, we dial that key and make another hop. The starting position counts as being dialed.</p><p>How many distinct numbers can you dial in N hops from a particular starting position?</p><p>Every interview I conduct basically breaks down into two parts: first we find an algorithmic solution and then the candidate implements it in code. I say â€œweâ€ find a solution because Iâ€™m not a mute spectator: 45 minutes is not a lot of time to design and implement anything under the best circumstances, never mind under pressure. I let candidates take the lead in the discussion, generating ideas, solving instances of the problem, etc., but Iâ€™m more than happy to give a nudge in the right direction. The better the candidate, the fewer hints I tend to have to give, but I have yet to see a candidate who required no input from me at all.</p><p>I should underscore this, because itâ€™s important: as an interviewer, Iâ€™m not in the business of sitting back and watching people fail. I want to write as much positive feedback as I can, and I try to give you opportunities to allow me to write good things about you. Hints are my way of saying â€œokay, Iâ€™m gonna give this bit to you, but only so you can move on and show me what youâ€™ve got on the other parts of the question.â€</p><p>With that being said, your first action after hearing the question should be stepping up to the whiteboard and solving small instances of the problem by hand. <em>Never dive right into code!</em> Solving small instances lets you spot patterns, observed and edge cases, and also helps crystallize a solution in your head. As an example, suppose you start on 6 and have two hops to make. Your sequences will beâ€¦</p><ul><li>6â€“1â€“8</li><li>6â€“1â€“6</li><li>6â€“7â€“2</li><li>6â€“7â€“6</li><li>6â€“0â€“4</li><li>6â€“0â€“6</li></ul><p>â€¦for a total of six sequences. If youâ€™re following along, try taking a pencil and paper and deriving these. This doesnâ€™t translate well into a blog post, but trust me when I say thereâ€™s something magical about working out a problem by hand that leads to many more insights than just staring at it and thinking quietly.</p><p>With all that said, you may have a solution forming in your head. But before we get thereâ€¦</p><p>One of the surprises I had when I started using this problem is how often candidates get stuck on computing the keys to which we can hop from a given position, also known as the neighbors. My advice is: when in doubt, write an empty placeholder and ask the interviewer if you can implement it later. This problemâ€™s complexity does not lie in the neighbor computation; Iâ€™m paying attention to how well you count full numbers. Any time spent on neighbor computation is effectively wasted.</p><p>I would accept â€œletâ€™s assume thereâ€™s a function that gives me the neighborsâ€ along with the following stub. Of course, Iâ€™ll probably ask you to double back and implement this later, but only if we have time. You can simply write a stub like this and move on:</p><pre><code>def neighbors(position):
	...</code></pre><p>Also, you donâ€™t really lose much by asking to use a stub: if the questionâ€™s complexity is elsewhere Iâ€™ll allow it. If not, Iâ€™ll ask you to actually implement it. I donâ€™t mind when candidates donâ€™t realize where the complexity of a question lies, especially in the early stages when they might not have fully explored the problem.</p><p>As for the neighbors function here, given that it never changes you can simply create a map and return the appropriate value:</p><pre><code>NEIGHBORS_MAP = {
    1: (6, 8),
    2: (7, 9),
    3: (4, 8),
    4: (3, 9, 0),
    5: tuple(),  # 5 has no neighbors
    6: (1, 7, 0),
    7: (2, 6),
    8: (1, 3),
    9: (2, 4),
    0: (4, 6),
}
def neighbors(position):
    return NEIGHBORS_MAP[position]</code></pre><p>Anyway, on to the solution. Perhaps youâ€™ve already noticed this problem can be solved by enumerating all possible numbers and counting them. You can use recursion to generate these values:</p><pre><code>def yield_sequences(starting_position, num_hops, sequence=None):
    if sequence is None:
        sequence = [starting_position]
    
    if num_hops == 0:
        yield sequence
        return

    for neighbor in neighbors(starting_position):
        yield from yield_sequences(
            neighbor, num_hops - 1, sequence + [neighbor])

def count_sequences(starting_position, num_hops):
    num_sequences = 0
    for sequence in yield_sequences(starting_position, num_hops):
        num_sequences += 1
    return num_sequences</code></pre><p>This works, and itâ€™s a common starting point I saw in interviews. Notice, however, that we generate the numbers and never actually use them. This problem asks for the <em>count</em> of numbers, not the numbers themselves. Once we count a number we never revisit it. As a general rule of thumb, I recommend paying attention to when your solution computes something it doesnâ€™t use. Often you can cut it out and get a better solution. Letâ€™s do that now.</p><p>How can we count phone numbers without generating them? It can be done, but not without an additional insight. Notice how the count of numbers that can be generated from a given starting position in N hops is equal to the sum of the counts of hops that can be generated starting from each of its neighbors in N-1 hops. Stated mathematically as a recurrence relation, it looks like this:</p><figure><img src="https://alexgolec.dev/content/images/2020/08/1-mcwSdrDe69X5FDegPmfgHg.png" alt="Image for post"></figure><p>This is intuitively obvious when you consider what happens with one hop: 6 has 3 neighbors (1, 7, and 0) and in zero hops you can reach one number for each, so you can only dial three numbers.</p><p>How does one arrive at this insight, you might ask? If youâ€™ve studied recursion, this should become evident after some exploration on the whiteboard. Many candidates whoâ€™ve practiced recursion immediately notice this problem breaks down into smaller subproblems, which is a dead giveaway. If youâ€™re in an interview with me and you canâ€™t seem to arrive at this insight, I will usually give hints to help get you there, up to and including outright giving it away if prodding fails.</p><p>Once you have this insight in hand, you can already move forward and solve this problem again. There are a number of implementations that use this fact, but letâ€™s start with the one I see most often in interviews: the naive recursive approach:</p><pre><code>from neighbors import neighbors                                 
                                                                
def count_sequences(start_position, num_hops):                  
    if num_hops == 0:                                           
        return 1                                                
                                                                
    num_sequences = 0                                           
    for position in neighbors(start_position):                  
        num_sequences += count_sequences(position, num_hops - 1)
    return num_sequences                                        
                                                                
if __name__ == '__main__':                                      
    print(count_sequences(6, 2))                                </code></pre><p>Thatâ€™s it! Combine this with a function to compute the neighbors and youâ€™ve produced a working solution! At this point, you should pat yourself on the back. If you scroll down youâ€™ll notice weâ€™ve still got a lot of ground to cover, but this point is a milestone. Producing any working solution already sets you apart from a surprising number of candidates.</p><p>This next question is one youâ€™re going to be hearing a lot from me: what is the Big-O complexity of this solution? For those who donâ€™t know, Big-O complexity is (informally) a sort of shorthand for the rate at which the amount of computation required by a solution grows as a function of the size of the input. For this problem, the size of the input is the number of hops. If youâ€™re interested in the proper mathematical definition, you can read more <a href="https://en.wikipedia.org/wiki/Big_O_notation" rel="noopener nofollow">here</a>.</p><p>For this implementation, every call to <code>count_sequences()</code> recursively calls <code>count_sequences()</code> at least twice, because each key has at least two neighbors. Since we recurse a number of times equal to the â€¦</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://alexgolec.dev/google-interview-questions-deconstructed-the-knights-dialer/">https://alexgolec.dev/google-interview-questions-deconstructed-the-knights-dialer/</a></em></p>]]>
            </description>
            <link>https://alexgolec.dev/google-interview-questions-deconstructed-the-knights-dialer/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24711094</guid>
            <pubDate>Wed, 07 Oct 2020 18:22:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[WSL2 â€“ Installation Tutorial for Graphical Windows Subsystem on Linux]]>
            </title>
            <description>
<![CDATA[
Score 91 | Comments 85 (<a href="https://news.ycombinator.com/item?id=24711054">thread link</a>) | @todsacerdoti
<br/>
October 7, 2020 | https://l-o-o-s-e-d.net/wsl2 | <a href="https://web.archive.org/web/*/https://l-o-o-s-e-d.net/wsl2">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div>
          <div>
            <p><h2>WSL2</h2></p>

            

            <div>
              <p>06:00pm | 10/05/2020<br>Daniel Tompkins</p>
              


 
            </div>

            <p>
              <h3>Microsoft ğŸ’” Linux</h3>
            </p>

            <div>
              <p>In 2001, Microsoft's former CEOâ€” Steve Ballmerâ€” was <a target="_blank" href="https://www.theregister.com/2001/06/02/ballmer_linux_is_a_cancer/">quoted</a> by the online tech news publication, <em>The Register</em>, saying:</p>
              <p>Linux is a cancer that attaches itself in an intellectual property sense to everything it touches</p>
              <p>Fast-forward 15 years into the futureâ€” at Microsoft's developer conference, <em>Build 2016</em>â€” and Gates' tech behemoth reveals a sudden volte-face. The current CEO, Satya Nadella, announces Windows Subsystem for Linux. With WSL, Microsoft is taking some of the most popular Linux distributions and making them available within Windows through the Microsoft Store.</p>
              <p>According to a <a target="_blank" href="https://w3techs.com/technologies/overview/operating_system">W<sup>3</sup>Techs survey</a>, Unix operating systems (the under-pinning OS of Linux, as well as MacOS) make up 71% of the Web, the remaining 29% being Windows. Additionally, every Android phone, tablet and smart TV runs on a modified version of the Linux kernel. So, I guess if you can't beat 'em, join 'em?</p>
            </div>

            <a target="_blank" href="https://l-o-o-s-e-d.net/assets/img/wsl2/w3techs_webservers.jpg">
              <p><img alt="W3Techs Survey on Web OS's, Unix: 71%, Windows: 29%" data-src="assets/img/wsl2/w3techs_webservers.jpg" src="https://l-o-o-s-e-d.net/assets/img/wsl2/w3techs_webservers.jpg">
              </p>
            </a>

            <p>Whether or not its because of Microsoft's good graces or some ulterior motive, I know having an easily accessible Unix-type environment available on Windows has been a godsend for me and for so many other developers.</p>

            <p>
              <h3>What's so great about WSL?</h3>
            </p>

            <div>
              <p>Before WSL, developers running Windows had two options: 1) a virtual machine (VM), or 2) dual-booting. Running a virtual machine uses up more resources than WSL. It can also be difficult to integrate hardware and files between the host machine and the VM. Dual-booting allows for a full-fledged install on a separate disk partition; but it requires a restart any time you want to switch between OS's.</p>
              <p>Windows Subsystem on Linux doesn't integrate with the host's hardware perfectlyâ€” for example, NVIDIA is still working on <a target="_blank" href="https://developer.nvidia.com/cuda/wsl">CUDA drivers</a> that will take advantage of GPU resources from within WSL. However, for Linux developers who are frequently running CAD software or Adobe Suite (which are <a target="_blank" href="https://appdb.winehq.org/objectManager.php?iId=17&amp;sClass=application">difficult-to-impossible</a> to install on Linux), WSL can be a fantastic partner.</p>
            </div>

            <p>
              <h3>WSL1 vs WSL2</h3>
            </p>

            <div>
              <p>More recently, Microsoft announced WSL2â€” an update that allows for a more complete Linux kernel to run on a Windows machine. This made it much easier to install a variety of software that had been difficult to run on the previous, WSL1. WSL2 is very similar to running a virtual machine (in fact it uses Microsoft's hyperV virtual machines).</p>
              <p>However, using WSL2 (as opposed to installing a Linux distro through VirtualBox, or another VM manager) provides some minor performance benefits since Microsoft has optimized it to integrate with Windows' services. If you want, I recommend reading Microsoft's own WSL1-vs-WSL2 <a target="_blank" href="https://docs.microsoft.com/en-us/windows/wsl/compare-versions">feature comparison</a> docs.</p>
            </div>

            <p>
              <h3>Alright, so how do I install WSL2?</h3>
            </p>

            <div>
              <p>Microsoft has clean, straight-forward <a target="_blank" href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">installation documentation</a> for WSL and WSL2. You can refer to that tutorial if you get stuck, or just follow the steps outlined below. Before starting, make sure you update your Windows 10 installation with the most recent build.</p>
              <p>I'll also be going one-step further, and showing you how to run a Linux GUI using WSL2 and VcXsrv (display forwarding). If you're more of a visual-learner, I've also included an installation speedrun <a target="_blank" href="https://www.youtube.com/embed/gtXIzVM5wZE">video</a> that follows the same steps outlined below (<em>edit: I forgot step 11 in the video, and it's a critical one! Make sure you do that!</em>).</p>
            </div>

            <div>
              <p><b>1. Enable WSL Feature</b></p>
              <p>First you need to enable the Windows Subsystem on Linux feature by right-clicking on Powershell from the start menu and clicking "Run as Administrator".</p>
              <p>Then, paste the following command and hit "Enter"â€” don't close the Powershell!</p>
              <pre><code>dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart</code></pre>
            </div>

            <div>
              <p><b>2. Enable WSL2 Virtual Machine Feature</b></p>
              <p>After the last command is finished, paste the following command in the same Administrator-level shell, and hit "Enter" to enable the WSL2 VM. Again, keep this shell open.</p>
              <pre><code>dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart</code></pre>
            </div>

            <div>
              <p><b>3. Download and Install the WSL2 Linux Kernel Update</b></p>
              <p>Click <a target="_blank" href="https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi">here</a> to download the Microsoft executable for installing the WSL2 Linux Kernel update. Once it's finished downloading, double-click the executable and follow the installation steps. This part's pretty straightforward</p>
            </div>

            <div>
              <p><b>4. Set WSL2 as Default Version</b></p>
              <p>Copy and paste the following command in Powershell to set WSL2 to be the default version:</p>
              <pre><code>wsl --set-default-version 2</code></pre>
            </div>

            <div>
              <p><b>5. Install Ubuntu 20.04 from the Microsoft Store</b></p>
              <p>Click the start menu and open the Microsoft Store. Search for "Ubuntu 20.04" and install this Linux distro. If you want to use another distro, that's fine; but Ubuntu 20.04 is compatible with the Regolith Linux desktop GUI we'll be installing in just a bit.</p>
            </div>

            <div>
              <p><b>6. Ubuntu 20.04 Initial Setup</b></p>
              <p>Once Ubuntu is done installing, click "Launch" to initiate first-time installation setup. You'll be prompted to put in a username and password.</p>
            </div>

            <div>
              <p><b>7. Make sure You're Using WSL2</b></p>
              <p>At this point, it might be a good idea to double-check that WSL is using version 2 by default. Open a command prompt (or use Powershell if it's still open) to paste in the following command:</p>
              <pre><code>wsl --list --verbose</code></pre>
            </div>

            <div>
              <p><b>8. Download and Install VcXsrv</b></p>
              <p>VcXsrv is an X Server that we'll use to view the GUI from WSL2.</p>
              <p>There are a few other display-forwarding servers available (like <a target="_blank" href="https://sourceforge.net/projects/xming/">Xming</a>), but I've found VcXsrv works the best. Download the executable <a target="_blank" href="https://sourceforge.net/projects/vcxsrv/">here</a> and click through the installation steps.</p>
            </div>

            <div>
              <p><b>9. Install Regolith Desktop</b></p>
              <p>I have <a target="_blank" href="https://l-o-o-s-e-d.net/regolith">Regolith Desktop</a> installed on one of my PCs, and it's fantastic. It's preconfigured to use the i3 window manager which I find incredibly efficient for its tiling and hotkey features.</p>
              <p>A <em>loosed</em> reader, Rodrigo, asked me use Regolith for the tutorial; but if you want to install a different GUI you can! To install Regolith Desktop, open your fresh Ubuntu install, and paste in the following lines:</p>
              <pre><code>sudo add-apt-repository ppa:regolith-linux/release</code></pre>
              <pre><code>sudo apt install regolith-desktop i3xrocks-net-traffic i3xrocks-cpu-usage i3xrocks-time</code></pre>
              <p>It's a lot of packages, so it'll take some time.</p>
            </div>

            <div>
              <p><b>10. Change the "Mod" Key</b></p>
              <p>Regolith, or rather i3-wm, uses the Super (Windows) key as the hotkey prefix by default. Since you're running this GUI within Windows, you'll run into a lot of overlap between Windows' and i3-wm's preconfigured shortcuts.</p>
              <p>For this reason, I recommend swapping the Super key for the Alt key. To  change the Mod key mapping use the Vim or Nano text editors to open the configuration file located at:</p>
              <pre><code>vim /etc/regolith/i3/config</code></pre>
              <p>On line 42 and 43, you'll find the Mod key assignment. Switch "Mod1" and "Mod4" and you'll be good to go! Your edited lines should look like this:</p>
              <pre><code>set_from_resource $mod i3-wm.mod Mod1</code></pre>
              <pre><code>set_from_resource $alt i3-wm.alt Mod4</code></pre>
              <p>If you're using Vim, hit "Escape" and type ":wq", then hit "Enter" to write and quit the file. You can check out the <a target="_blank" href="https://regolith-linux.org/regolith-site-r13/docs/howto/super-to-alt/">official Regolith tutorial</a> on making these changes if you get stuck.</p>
            </div>

            <div>
              <p><b>11. Export DISPLAY parameter</b></p>
              <p>Another critical edit (that I forgot to put in the videoâ€” oops ğŸ™ƒ) is to export the DISPLAY variable. Since WSL2 is a VM, it has it's own IP address (which can change at each startup). As a result, you'll need to add a couple lines to your bash profile for VcXsrv to connect to WSL2.</p>
              <p>To open your ".bashrc" with Vim:</p>
            </div>
            <div>
              <pre><code>vim ~/.bashrc</code></pre>
              <p>Press and hold Shift then press "G" to jump to the bottom of the file. On two new lines, paste in the following code:</p> 
              <pre><code>export DISPLAY=$(awk '/nameserver / {print $2; exit}' /etc/resolv.conf 2&gt;/dev/null):0<br>export LIBGL_ALWAYS_INDIRECT=1</code></pre>
            </div>

            <div>
              <p><b>12. Open and Configure VcXsrv</b></p>
              <p>Click the start menu and type in "Xlaunch" then hit "Enter" to run VcXsrv. Click the "One window without titlebar" option (you can explore the others later, if you want) and click next. Leave it on "Start no client" and click next. Then, in the "Additional parameters" input, add "-ac" and click next. I recommend clicking "Save configuration" for ease of use.</p>
              <p>At this point, you should have a black screen waiting to accept a display input.</p>
            </div>

            <p><b>13. Run Regolith Desktop</b></p>
            <p>The last thing to do is run the magic line:</p>
            <div>
              <pre><code>i3-gnome-flashback-session</code></pre>
              <p>You should then see a graphical Regolith Desktop appear in the VcXsrv window! Huzzah! Feel free to play around with your new graphical WSL2 setup. To see an overview of the available shortcuts, use "Alt+Shift+?" to bring up the help menu. You can find more help in Regolith's official <a target="_blank" href="https://regolith-linux.org/docs/">documentation</a>.</p>
            </div>

            <a target="_blank" href="https://l-o-o-s-e-d.net/assets/img/wsl2/regolith_screenshot.jpg">
              <p><img alt="Regolith Desktop with Windows 10 Taskbar and i3 tiling running on WSL2" data-src="assets/img/wsl2/regolith_screenshot.jpg" src="https://l-o-o-s-e-d.net/assets/img/wsl2/regolith_screenshot.jpg">
              </p>
            </a>

            <p>
              </p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://l-o-o-s-e-d.net/wsl2">https://l-o-o-s-e-d.net/wsl2</a></em></p>]]>
            </description>
            <link>https://l-o-o-s-e-d.net/wsl2</link>
            <guid isPermaLink="false">hacker-news-small-sites-24711054</guid>
            <pubDate>Wed, 07 Oct 2020 18:18:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Translation Units Considered Harmful?]]>
            </title>
            <description>
<![CDATA[
Score 102 | Comments 51 (<a href="https://news.ycombinator.com/item?id=24710612">thread link</a>) | @ingve
<br/>
October 7, 2020 | https://cor3ntin.github.io/posts/translation_units/ | <a href="https://web.archive.org/web/*/https://cor3ntin.github.io/posts/translation_units/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <p>Let say you have some struct <code>square</code> you want to compute the area of.</p>
<p><code>struct square { int width; }</code></p>
<p>You could of course do that:</p>
<p><code>int area(square s) { return s.width * s.width; }</code></p>
<p>But, your friend Tony told you to use more functions, so instead you do that</p>
<div><pre><code data-lang="cpp"><span>int</span> <span>area</span>(square s) { <span>return</span> width(s) * width(s); }
<span>int</span> <span>width</span>(square s) { <span>return</span> s.width; }
</code></pre></div><p><code>area</code> being the function you really care about it is defined first - after all, code reads from top to bottom.</p>
<p>As you may have guessed from the lack of <code>;</code> after the structâ€™s closing bracket, the above code is written in D.
I figure my readership isnâ€™t really into D, so maybe you would prefer some <strong>Rust</strong>?</p>
<div><pre><code data-lang="rs"><span>pub</span><span> </span><span>fn</span> <span>area</span>(square: <span>Square</span>)<span> </span>-&gt; <span>i32</span> {<span> </span><span>return</span><span> </span>width(s)<span> </span>*<span> </span>width(s)<span> </span>}<span>
</span><span></span><span>pub</span><span> </span><span>fn</span> <span>width</span>(square: <span>Square</span>)<span> </span>-&gt; <span>i32</span> {<span> </span><span>return</span><span> </span>s.width<span> </span>}<span>
</span><span></span><span>pub</span><span> </span><span>struct</span> <span>Square</span><span> </span>{<span> </span>width: <span>i32</span> }<span>
</span></code></pre></div><p>You can even compute the area of you square <strong><em>at scale</em></strong> with go</p>
<div><pre><code data-lang="go"><span>func</span> <span>Area</span>(s square) <span>int</span> { <span>return</span> <span>width</span>(s) * <span>width</span>(s); }
<span>func</span> <span>width</span>(s square) <span>int</span> { <span>return</span> s.width }
<span>type</span> square <span>struct</span> { width  <span>int</span> }
</code></pre></div><p>Or even <strong>Swift</strong>ly.</p>
<div><pre><code data-lang="swift"><span>func</span> <span>area</span>(s: Square) -&gt; <span>Int</span> { <span>return</span> width(s:s) * width(s:s); }
<span>func</span> <span>width</span>(s: Square) -&gt; <span>Int</span> { <span>return</span> s.width }
<span>struct</span> <span>Square</span> { <span>var</span> <span>width</span>:<span>Int</span> = <span>0</span>; }
</code></pre></div><p>But of course, <em>you</em> will worry about the overhead and will want the language the most performant (thatâ€™s not a word).
Eager to please and impress, let me copy the D code and add that oh-so-important semi-colon.</p>
<div><pre><code data-lang="cpp"><span>struct</span> <span>square</span> { <span>int</span> width; };
<span>int</span> <span>area</span>(square s)  { <span>return</span> width(s) * width(s); }
<span>int</span> <span>width</span>(square s) { <span>return</span> s.width; }
</code></pre></div><p>Thatâ€™s nice, isnâ€™t it?  Interesting how most languages look alike.
Hum, wait, that doesnâ€™t work???!!!</p>
<p><code>error: 'width' was not declared in this scope</code></p>
<p>But, you stupid thing, itâ€™s <em>RIGHT THERE</em>.
I declared everything in the global scope like a maniac, canâ€™t you see?</p>
<p>Alas, the standard makes the compiler blind.</p>
<blockquote>
<p>In the definition of a function that is a member of namespace N, a name used after the functionâ€™s declarator-id23 shall be declared before its use in the block in which it is used or in one of its enclosing blocks ([stmt.block]) or shall be declared before its use in namespace N or, if N is a nested namespace, shall be declared before its use in one of Nâ€™s enclosing namespaces.</p>
</blockquote>
<p>Of course, this makes no sense, a compiler can really easily parse the declaration independently of the definition, as
proven by other languages. Or you know, C++ classes. (imagine replacing a big namespace with a class full of static methods and nested types)
Unless of course, itâ€™s a performance thing.
But, you are a very great engineer, so you wouldnâ€™t let a source file grow above a few hundred lines of code, would you?
I bet your code is beautiful, like this small self-contained super useful program</p>
<div><pre><code data-lang="cpp"><span>#include</span> <span>&lt;iostream&gt;</span><span>
</span><span></span><span>int</span> <span>main</span> () {
    std::cout &lt;&lt; <span>"Hello world</span><span>\n</span><span>"</span>;
}
</code></pre></div><p>Which on my system expands to about <em>33000</em> lines of code. The freaking thing. But more on that later.</p>
<p>Letâ€™s go back to square one.
C++, in its infinite wisdom, lets us forward-declare functions, so we can write this:</p>
<div><pre><code data-lang="cpp"><span>struct</span> <span>square</span> { <span>int</span> width; };
<span>int</span> <span>width</span>(<span>const</span> square&amp; s);
<span>int</span> <span>area</span>(<span>const</span> square&amp; s)  { <span>return</span> width(s) * width(s); }
<span>int</span> <span>width</span>(<span>const</span> square&amp; s) { <span>return</span> s.width; }
</code></pre></div><p>Which is nice and dandy, if you squint.</p>
<p>Besides requiring you to get the exact declaration of functions perfectly right - which is hard to maintain, lots of entities are not forward-declarable,
notably type alias, templated types, etc.
Which is an odd limitation given that where forward declaring a function require you
to know the precise signature, for types you are merely trying to introduce a name.</p>
<h2 id="noexcept">noexcept</h2>
<p>You will notice that <code>area</code> never throws.
That is, there is no subexpression of <code>area</code> that can throw, ever.</p>
<p>You can check that it does not.</p>
<p><code>static_assert(noexcept(area(square{})));</code></p>
<p>Inevitably,  that fails.
<code>error: static assertion failed</code>.
We indeed forgot to tell the compiler that our function could not throw.</p>
<div><pre><code data-lang="cpp"><span>int</span> <span>width</span>(<span>const</span> square&amp; s) <span>noexcept</span>;
<span>int</span> <span>area</span>(<span>const</span> square&amp; s) <span>noexcept</span> { <span>return</span> width(s) * width(s); }
<span>int</span> <span>width</span>(<span>const</span> square&amp; s) <span>noexcept</span> { <span>return</span> s.width; }
</code></pre></div><p>Notice that we need to add <code>noexcept</code> on all declarations, including the forward declarations.
And, you can lie to the compiler pretty easily.</p>
<div><pre><code data-lang="cpp"><span>int</span> <span>area</span>(<span>const</span> square&amp; s) <span>noexcept</span> {
    <span>return</span> width(s) * width(s);
}

<span>int</span> <span>width</span>(<span>const</span> square&amp; s) {
    <span>throw</span> <span>42</span>;
}
</code></pre></div><p>The above code will <code>std::terminate()</code>, you know that the compiler knows that, everybody knows that.</p>
<p>Soâ€¦what functions should be marked <code>noexcept</code>?
Itâ€™s pretty simple actually. All the functions that can not throw.
That is the functions that:</p>
<ul>
<li>Donâ€™t contain a <code>throw</code> exception</li>
<li>Donâ€™t call non-noexcept functions</li>
</ul>
<p>Notice the double (triple?) negative.</p>
<p>So you, as a developer striving to mark all function that can be <code>noexcept</code> as such,
have to walk the call tree recursively until you can ascertain that the call chain will never throw
or actually might (because one callee does throw, or is at a C interface boundary, etc).
One argument against exceptions is that it makes reasoning about control flow harder:
Exceptions more or less force you to reason about the control flow of the whole program at every time.
<code>noexcept</code> is supposed to solve that, but, to put that <code>noexcept</code> keyword confidently, you still need
to do that analyze. The chances you get it wrong are high.
If you write generic code, you will have to tell the compiler that a symbol is noexcept
if all of itâ€™s subexpression is noexcept manually.</p>
<p>And the compiler can not trust you that the function will indeed not throw, so implementers will inject calls to <code>std::terminate</code>
here and there, negating somewhat the performance benefits of marking the function <code>noexcept</code> in the first place.</p>
<p>Letâ€™s rewrite our code using lambda instead</p>
<div><pre><code data-lang="cpp"><span>auto</span> width = [](<span>const</span> square&amp; s) -&gt; <span>int</span> {
    <span>return</span> s.width;
};
<span>auto</span> area = [](<span>const</span> square&amp; s) -&gt; <span>int</span> {
    <span>return</span> <span>width</span>(s) * width(s);
};
</code></pre></div><p>Of course, lambdas cannot be forward declared.
So I had to reorganize the code.</p>
<p>And now, despite the lack of <code>noexcept</code> keyword,
<code>static_assert(noexcept(area(square{})));</code> passes.</p>
<p><strong>What is happening?</strong></p>
<p>It turns out that the compiler is pretty good at knowing which functions are <code>noexcept</code>.
In the case of lambdas, the definition will always be visible to the compiler before any invocation,
so it can implicitly mark it no except and do the work for us. This allowed as part of C++20.</p>
<h3 id="what-does-noexcept-even-mean">What does noexcept even mean?</h3>
<p>Iâ€™m not saying that <code>noexcept</code> would not be necessary in an ideal world, because it has more than one meaning
and people use it differently. Notably, <code>noexcept</code> might mean:</p>
<ul>
<li>Do not generate exception handling code for this function</li>
<li>This function does not throw</li>
<li>This function will <em>never</em> throw</li>
</ul>
<p>The first statement is a request for the compiler, the second is an assertion for both the compiler and human readers,
while the last one is exclusively for people.</p>
<p>So <code>noexcept</code> would remain interesting at API boundary as a contract between people even if the compiler could decide
for itself whether the function was actually non-throwing.</p>
<h2 id="transaction_safe">transaction_safe</h2>
<p>The Transactional Memory TS defines the notion of <em>transaction safe expression</em> as follow:</p>
<blockquote>
<p>An expression is transaction-unsafe if it contains any of the following as a potentially-evaluated subexpression (3.2[basic.def.odr]):</p>
</blockquote>
<blockquote>
<ul>
<li>an lvalue-to-rvalue conversion (4.1 [conv.lval]) applied to a volatile glvalue</li>
<li>an expression that modifies an object through a volatile glvalue</li>
<li>the creation of a temporary object of volatile-qualified type or with a subobject of volatile-qualified type</li>
<li>a function call (5.2.2 expr.call) whose postfix-expression is an id-expression that names a non-virtual
function that is not transaction-safe</li>
<li>an implicit call of a non-virtual function that is not transaction-safe</li>
<li>any other <strong>call of a function, where the function type is not â€œtransaction_safe functionâ€</strong></li>
</ul>
</blockquote>
<p>(Emphasis mine)</p>
<p>The details are not important, but, basically, a <code>transaction_safe</code> safe expression is one that doesnâ€™t touch volatile objects.
And only call functions with the same properties.
Thatâ€™s probably upward of 99% of functions - I suspect the very terrible default exists for compatibility reasons.
The important part is that you have to tag all your functions or hope that the property holds true recursively.
(Like <code>noexcept</code>, you can lie, by marking a function <code>transaction_safe</code> even if a callee is not itself <code>transaction_safe</code>, opening the door to UB).
An issue that seems to hold this TS back.</p>
<h2 id="constexpr">constexpr</h2>
<p><code>constexpr</code> functions are a bit different. The compiler knows what functions are candidate <code>constexpr</code>.
Most of the time it will constant evaluate them regardless of whether they are actually marked as such.
The keyword is required to ensure that the compiler will actually do the constant evaluation when it can and, most importantly,
because removing the constexpr-ness of a function may be a source breaking change - (if that function is called during the evaluation of a <code>constexpr</code> variable).
By its very nature, <code>constexpr</code> implies that <code>constexpr</code> functions are defined somewhere is the TU. And everything not defined in the TU cannot be constant-evaluated.
<a href="https://wg21.link/p1235">A proposal for C++20 proposes to make it implicit in some cases</a></p>
<p>For now, we are left with the following code, and it is on you to use the appropriate qualifiers.</p>
<div><pre><code data-lang="cpp"><span>constexpr</span> <span>int</span> <span>width</span>(square s) <span>noexcept</span> transaction_safe;
<span>constexpr</span> <span>int</span> <span>area</span>(square s) <span>noexcept</span> transaction_safe  { <span>return</span> width(s) * width(s); }
<span>constexpr</span> <span>int</span> <span>width</span>(square s) <span>noexcept</span> transaction_safe { <span>return</span> s.width; }
</code></pre></div><p>As of C++20, <code>constexpr</code> functions can throw. The committee is also considering making <code>new</code> expressions
<code>noexcept</code> by 23 or 26 so we are slowly getting to a place where 95%+ of functions will be both <code>constexpr</code> and <code>noexcept</code>
eligible and will have to be marked manually.</p>
<p><strong>Is there a better way ?</strong></p>

<p>A source file and its included headers form a translation unit.
Multiple translations units form a program.</p>
<p>Sounds simple enough right?
Itâ€™s actually <em>simpler</em> than right.</p>
<p>Headers and sources files are a bit of a lie we tell ourselves.
As far as I can tell, the term â€œheaderâ€ only appear in the standard as to name the â€¦</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://cor3ntin.github.io/posts/translation_units/">https://cor3ntin.github.io/posts/translation_units/</a></em></p>]]>
            </description>
            <link>https://cor3ntin.github.io/posts/translation_units/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24710612</guid>
            <pubDate>Wed, 07 Oct 2020 17:34:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Generalizing 'jq' and Traversal Systems using optics and standard monads]]>
            </title>
            <description>
<![CDATA[
Score 142 | Comments 78 (<a href="https://news.ycombinator.com/item?id=24710565">thread link</a>) | @todsacerdoti
<br/>
October 7, 2020 | https://chrispenner.ca/posts/traversal-systems | <a href="https://web.archive.org/web/*/https://chrispenner.ca/posts/traversal-systems">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
        <p>Hi folks! Today I'll be chatting about <strong>Traversal Systems</strong> like <strong>jq</strong> and <strong>XPath</strong>; we're going to discover which properties make them useful, then see how we can replicate their most useful behaviours in Haskell using (almost entirely) pre-ols!existing standard Haskell tools! Let's go!</p>
<h2 id="whats-a-traversal-system">What's a Traversal System?</h2>
<p>First off I'll admit that "Traversal System" is a name I just came up with, you probably won't find anything if you search for it (unless this post really catches on ğŸ˜‰).</p>
<p>A <strong>Traversal System</strong> allows you dive deeply into a piece of data and may allow you to fetch, query, and edit the structure as you go while maintaining references to other pieces of the structure to influence your work. The goal of most Traversal Systems is to make this as painless and concise as possible. It turns out that this sort of thing is <strong>incredibly useful</strong> for manipulating JSON, querying HTML and CSS, working with CSVs, or even just handling standard Haskell Records and data-types.</p>
<p>Some good examples of existing <strong>Traversal Systems</strong> which you may have heard of include the brilliant <a href="https://stedolan.github.io/jq/">jq</a> utility for manipulating and querying JSON, the <strong>XPath</strong> language for querying XML, and the <a href="https://github.com/noprompt/meander">meander</a> data manipulation system in Clojure. Although each of these systems may appear drastically different at a glance, they both <em>accomplish many of the same goals</em> of manipulating and querying data in a concise way.</p>
<p>The similarities between these systems intrigued me! They seem so similar, but yet still seem to share very little in the way of structure, syntax, and prior art. They re-invent the wheel for each new data type! Ideally we could recognize the useful behaviours in each system and build a generalized system which works for any data type.</p>
<p>This post is an attempt to do exactly that; we'll take a look at a few things that these systems do well, then we'll re-build them in Haskell using standard tooling, all the while abstracting over the type of data!</p>
<h2 id="optics-as-a-basis-for-a-traversal-system">Optics as a basis for a traversal system</h2>
<p>For any of those who know me it should be no surprise that my first thought was to look at optics (i.e. Lenses and Traversals). In general I find that optics solve a lot of my problems, but in this case they are particularly appropriate! Optics inherently deal with the idea of diving deep into data and querying or updating data in a structured and compositional fashion.</p>
<p>In addition, optics also allow abstracting over the data type they work on. There are pre-existing libraries of optics for working with JSON via <a href="https://hackage.haskell.org/package/lens-aeson"><code>lens-aeson</code></a> and for html via <a href="https://hackage.haskell.org/package/taggy-lens"><code>taggy-lens</code></a>. I've written optics libraries for working with <a href="https://hackage.haskell.org/package/lens-csv">CSVs</a> and even <a href="https://hackage.haskell.org/package/lens-regex-pcre">Regular Expressions</a>, so I can say confidently that they're a brilliantly adaptable tool for data manipulation.</p>
<p>It also happens that optics are well-principled and mathematically sound, so they're a good tool for studying the properties that a system like this may have.</p>
<p>However, optics themselves don't provide everything we need! Optics are rather obtuse, in fact I wrote <a href="https://leanpub.com/optics-by-example">a whole book</a> to help teach them, and they lack clarity and easy of use when it comes to building larger expressions. It's also pretty tough to work on one part of a data structure while referencing data in another part of the same structure. My hope is to address some of these short comings in this post.</p>
<p>In this particular post I'm mostly interested in explaining a framework for traversal systems in Haskell, we'll be using many standard <a href="https://hackage.haskell.org/package/mtl"><strong>mtl</strong></a> Monad Transformers alongside a lot of combinators from the <a href="https://hackage.haskell.org/package/lens"><strong>lens</strong></a> library. You won't need to understand any of these intimately to get the <em>gist</em> of what's going on, but I won't be explaining them in depth here, so you may need to look elsewhere if you're lacking a bit of context.</p>
<h2 id="establishing-the-problem">Establishing the Problem</h2>
<p>I'll be demoing a few examples as we go along so let's set up some data. I'll be working in both <strong>jq</strong> and <strong>Haskell</strong> to make comparisons between them, so we'll set up the same data in both <strong>JSON</strong> and Haskell.</p>
<p>Here's a funny lil' company as a JSON object:</p>
<div id="cb1"><pre><code><span id="cb1-1"><a href="#cb1-1"></a><span>{</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>    <span>"staff"</span><span>:</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>      <span>[</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>        <span>{</span> <span>"id"</span><span>:</span> <span>"1"</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>        <span>,</span> <span>"name"</span><span>:</span> <span>"bob"</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>        <span>,</span> <span>"pets"</span><span>:</span> <span>[</span></span>
<span id="cb1-7"><a href="#cb1-7"></a>              <span>{</span> <span>"name"</span><span>:</span> <span>"Rocky"</span></span>
<span id="cb1-8"><a href="#cb1-8"></a>              <span>,</span> <span>"type"</span><span>:</span> <span>"cat"</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>              <span>}</span><span>,</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>              <span>{</span> <span>"name"</span><span>:</span> <span>"Bullwinkle"</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>              <span>,</span> <span>"type"</span><span>:</span> <span>"dog"</span></span>
<span id="cb1-12"><a href="#cb1-12"></a>              <span>}</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>            <span>]</span></span>
<span id="cb1-14"><a href="#cb1-14"></a>        <span>}</span><span>,</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>        <span>{</span> <span>"id"</span><span>:</span> <span>"2"</span></span>
<span id="cb1-16"><a href="#cb1-16"></a>        <span>,</span> <span>"name"</span><span>:</span> <span>"sally"</span></span>
<span id="cb1-17"><a href="#cb1-17"></a>        <span>,</span> <span>"pets"</span><span>:</span> <span>[</span></span>
<span id="cb1-18"><a href="#cb1-18"></a>              <span>{</span> <span>"name"</span><span>:</span> <span>"Inigo"</span></span>
<span id="cb1-19"><a href="#cb1-19"></a>              <span>,</span> <span>"type"</span><span>:</span> <span>"cat"</span></span>
<span id="cb1-20"><a href="#cb1-20"></a>              <span>}</span></span>
<span id="cb1-21"><a href="#cb1-21"></a>            <span>]</span></span>
<span id="cb1-22"><a href="#cb1-22"></a>        <span>}</span></span>
<span id="cb1-23"><a href="#cb1-23"></a>      <span>]</span><span>,</span></span>
<span id="cb1-24"><a href="#cb1-24"></a>    <span>"salaries"</span><span>:</span> <span>{</span></span>
<span id="cb1-25"><a href="#cb1-25"></a>        <span>"1"</span><span>:</span> <span>12</span><span>,</span></span>
<span id="cb1-26"><a href="#cb1-26"></a>        <span>"2"</span><span>:</span> <span>15</span></span>
<span id="cb1-27"><a href="#cb1-27"></a>    <span>}</span></span>
<span id="cb1-28"><a href="#cb1-28"></a><span>}</span></span></code></pre></div>
<p>And here's the same data in its Haskell representation, complete with generated optics for each record field.</p>
<div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1"></a><span>data</span> <span>Company</span> <span>=</span> <span>Company</span> {<span> _staff ::</span> [<span>Employee</span>]</span>
<span id="cb2-2"><a href="#cb2-2"></a>                       ,<span> _salaries ::</span> <span>M.Map</span> <span>Int</span> <span>Int</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>                       } <span>deriving</span> <span>Show</span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span>data</span> <span>Pet</span> <span>=</span> <span>Pet</span> {<span> _petName ::</span> <span>String</span></span>
<span id="cb2-5"><a href="#cb2-5"></a>               ,<span> _petType ::</span> <span>String</span></span>
<span id="cb2-6"><a href="#cb2-6"></a>               } <span>deriving</span> <span>Show</span></span>
<span id="cb2-7"><a href="#cb2-7"></a><span>data</span> <span>Employee</span> <span>=</span> <span>Employee</span> {<span> _employeeId ::</span> <span>Int</span></span>
<span id="cb2-8"><a href="#cb2-8"></a>                         ,<span> _employeeName ::</span> <span>String</span></span>
<span id="cb2-9"><a href="#cb2-9"></a>                         ,<span> _employeePets ::</span> [<span>Pet</span>]</span>
<span id="cb2-10"><a href="#cb2-10"></a>                         } <span>deriving</span> <span>Show</span></span>
<span id="cb2-11"><a href="#cb2-11"></a></span>
<span id="cb2-12"><a href="#cb2-12"></a>makeLenses '<span>'Company</span></span>
<span id="cb2-13"><a href="#cb2-13"></a>makeLenses '<span>'Pet</span></span>
<span id="cb2-14"><a href="#cb2-14"></a>makeLenses '<span>'Employee</span></span>
<span id="cb2-15"><a href="#cb2-15"></a></span>
<span id="cb2-16"><a href="#cb2-16"></a><span>company ::</span> <span>Company</span></span>
<span id="cb2-17"><a href="#cb2-17"></a>company <span>=</span> <span>Company</span> [ <span>Employee</span> <span>1</span> <span>"bob"</span> [<span>Pet</span> <span>"Rocky"</span> <span>"cat"</span>, <span>Pet</span> <span>"Bullwinkle"</span> <span>"dog"</span>] </span>
<span id="cb2-18"><a href="#cb2-18"></a>                  , <span>Employee</span> <span>2</span> <span>"sally"</span> [<span>Pet</span> <span>"Inigo"</span> <span>"cat"</span>]</span>
<span id="cb2-19"><a href="#cb2-19"></a>                  ] (M.fromList [ (<span>1</span>, <span>12</span>)</span>
<span id="cb2-20"><a href="#cb2-20"></a>                                , (<span>2</span>, <span>15</span>)</span>
<span id="cb2-21"><a href="#cb2-21"></a>                                ])</span></code></pre></div>
<h2 id="querying">Querying</h2>
<p>Let's dive into a few example queries to test the waters! First an easy one, let's write a query to find all the pets owned by any of our employees.</p>
<p>Here's how it looks in <strong>jq</strong>:</p>
<pre><code>$ cat company.json | jq '.staff[].pets[] | select(.type == "cat")'
{
  "name": "Rocky",
  "type": "cat"
}
{
  "name": "Inigo",
  "type": "cat"
}</code></pre>
<p>We look in the <code>staff</code> key, then <em>enumerate</em> that list, then for each staff member we enumerate their cats! Lastly we filter out anything that's not a cat.</p>
<p>We can recognize a few hallmarks of a <strong>Traversal System</strong> here. <strong>jq</strong> allows us to "dive" down deeper into our structure by providing a path to where we want to be. It also allows us to <strong>enumerate</strong> many possibilities using the <code>[]</code> operator, which will forward <strong>each</strong> value to the rest of the pipeline one after the other. Lastly it allows us to <strong>filter</strong> our results using <code>select</code>.</p>
<p>And in Haskell using optics it looks like this:</p>
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1"></a><span>&gt;&gt;&gt;</span> toListOf (staff <span>.</span> folded <span>.</span> employeePets <span>.</span> folded <span>.</span> filteredBy (petType <span>.</span> only <span>"cat"</span>)) company</span>
<span id="cb4-2"><a href="#cb4-2"></a>[ <span>Pet</span> {_petName <span>=</span> <span>"Rocky"</span>, _petType <span>=</span> <span>"cat"</span>}</span>
<span id="cb4-3"><a href="#cb4-3"></a>, <span>Pet</span> {_petName <span>=</span> <span>"Inigo"</span>, _petType <span>=</span> <span>"cat"</span>}</span>
<span id="cb4-4"><a href="#cb4-4"></a>]</span></code></pre></div>
<p>Here we use "toListOf" along with an optic which "folds" over each staff member, then folds over each of their pets, again filtering for "only" cats.</p>
<p>At a glance the two are extremely similar!</p>
<p>They each allow the <em>enumeration</em> of multiple values, in <strong>jq</strong> using <code>[]</code> and in optics using <code>folded</code>.</p>
<p>Both implement some form of <strong>filtering</strong>, <strong>jq</strong> using <code>select</code> and our optics with <code>filteredBy</code>.</p>
<p>Great! So far we've had no trouble keeping up! We're already starting to see a lot of similarities between the two, and our solutions using optics are easily generalizable to any data type.</p>
<p>Let's move on to a more complex example.</p>
<h2 id="keeping-references">Keeping references</h2>
<p>This time we're going to print out each pet and their owner!</p>
<p>First, here's the <strong>jq</strong>:</p>
<div id="cb5"><pre><code><span id="cb5-1"><a href="#cb5-1"></a>$ <span>cat</span> join.json <span>|</span> <span>jq</span> <span>'</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span>    .staff[] </span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span>  | .name as $personName </span></span>
<span id="cb5-4"><a href="#cb5-4"></a><span>  | .pets[] </span></span>
<span id="cb5-5"><a href="#cb5-5"></a><span>  | "\(.name) belongs to \($personName)"</span></span>
<span id="cb5-6"><a href="#cb5-6"></a><span>'</span></span>
<span id="cb5-7"><a href="#cb5-7"></a><span>"Rocky belongs to bob"</span></span>
<span id="cb5-8"><a href="#cb5-8"></a><span>"Bullwinkle belongs to bob"</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span>"Inigo belongs to sally"</span></span></code></pre></div>
<p>Here we see a new feature in <strong>jq</strong> which is the ability to maintain <strong>references</strong> to a part of the structure for later while we continue to dig deeper into the structure. We're grabbing the name of each employee as we enumerate them and saving it into <code>$personName</code> so we can refer to this later on. Then we enumerate each of the pets and use string interpolation to describe who owns each pet.</p>
<p>If we try to stick with optics on their own, well, it's possible, but unfortunately this is where it all starts to break down, look at this absolute mess:</p>
<div id="cb6"><pre><code><span id="cb6-1"><a href="#cb6-1"></a><span>owners ::</span> [<span>String</span>]</span>
<span id="cb6-2"><a href="#cb6-2"></a>owners <span>=</span> </span>
<span id="cb6-3"><a href="#cb6-3"></a>  company <span>^..</span> </span>
<span id="cb6-4"><a href="#cb6-4"></a>    (staff <span>.</span> folded <span>.</span> reindexed _employeeName selfIndex <span>&lt;.</span> employeePets <span>.</span> folded <span>.</span> petName) </span>
<span id="cb6-5"><a href="#cb6-5"></a>    <span>.</span> withIndex </span>
<span id="cb6-6"><a href="#cb6-6"></a>    <span>.</span> to (\(eName, pName) <span>-&gt;</span> pName <span>&lt;&gt;</span> <span>" belongs to "</span> <span>&lt;&gt;</span> eName)</span>
<span id="cb6-7"><a href="#cb6-7"></a></span>
<span id="cb6-8"><a href="#cb6-8"></a><span>&gt;&gt;&gt;</span> owners</span>
<span id="cb6-9"><a href="#cb6-9"></a>[ <span>"Rocky belongs to bob"</span></span>
<span id="cb6-10"><a href="#cb6-10"></a>, <span>"Bullwinkle belongs to bob"</span></span>
<span id="cb6-11"><a href="#cb6-11"></a>, <span>"Inigo belongs to sally"</span></span>
<span id="cb6-12"><a href="#cb6-12"></a>]</span></code></pre></div>
<p>You can bet that nobody is calling that "easy to read". Heck, I wrote a book on optics and it still took me a few tries to figure out where the brackets needed to go!</p>
<p>Optics are great for handling a <em>single</em> stream of values, but they're much worse at more complex expressions, especially those which require a reference to values that occur <em>earlier</em> in the chain. Let's see how we can address those shortcomings as we build our <strong>Traversal System</strong> in Haskell.</p>
<p>Just for the <strong>jq</strong> aficionados in the audience I'll show off this alternate version which uses a little bit of <em>magic</em> that <strong>jq</strong> does for you.</p>
<div id="cb7"><pre><code><span id="cb7-1"><a href="#cb7-1"></a> $ <span>cat</span> company.json <span>|</span> <span>jq</span> <span>'.staff[] | "\(.pets[].name) belongs to \(.name)"'</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span>"Rocky belongs to bob"</span></span>
<span id="cb7-3"><a href="#cb7-3"></a><span>"Bullwinkle belongs to bob"</span></span>
<span id="cb7-4"><a href="#cb7-4"></a><span>"Inigo belongs to sally"</span></span></code></pre></div>
<p>Depending on your experience may be less <strong>magical</strong> and more <strong>confusing</strong> ğŸ˜¬. Since the final expression contains an <strong>enumeration</strong> (i.e. <code>\(.pets[].name)</code>) <strong>jq</strong> will expand the final term once for each value in the enumeration. This is really cool, but unfortunately a bit "less principled" and tough to understand in my opinion.</p>
<p>Regardless, the behaviour is the same, and we haven't replicated it in Haskell satisfactorily yet, let's see what we can do about that!</p>
<h2 id="monads-to-the-rescue-again">Monads to the rescue (again...)</h2>
<p>In Haskell we love our <strong>embedded DSLs</strong>; if you give a Haskeller a problem to solve, you can bet that 9 times out of 10 they'll solve it with a custom monad and an DSL ğŸ˜‚. Well, I'm sorry to tell you that I'm no different!</p>
<p>We'll be using a monad to address the readability problem of the last optics solution, but the question is... <em>which</em> monad?</p>
<p>Since all we're doing at the moment is <strong>querying</strong> data, we can make use of the esteemed <strong>Reader Monad</strong> to provide a context for our query.</p>
<p>Here's what that last query looks like when we use the <a href="https://hackage.haskell.org/package/mtl-2.2.2/docs/Control-Monad-Reader.html"><code>Reader</code></a> monad with the relatively â€¦</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://chrispenner.ca/posts/traversal-systems">https://chrispenner.ca/posts/traversal-systems</a></em></p>]]>
            </description>
            <link>https://chrispenner.ca/posts/traversal-systems</link>
            <guid isPermaLink="false">hacker-news-small-sites-24710565</guid>
            <pubDate>Wed, 07 Oct 2020 17:29:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Competition and Quarter-Life Crises]]>
            </title>
            <description>
<![CDATA[
Score 83 | Comments 84 (<a href="https://news.ycombinator.com/item?id=24710496">thread link</a>) | @arvarik
<br/>
October 7, 2020 | https://www.arvarik.com/competition-quarter-life-crisis | <a href="https://web.archive.org/web/*/https://www.arvarik.com/competition-quarter-life-crisis">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p>Growing up in an Indian household and in one of the most
<a href="https://en.wikipedia.org/wiki/New_Jersey#Demographics">diverse states/towns</a> in the US, itâ€™s not easy to avoid
the competitive stress that derives from first generation families.</p><p>From test scores, which classes I take, SAT scores, college admissions, sports performance, internships received etc.,
the first 22 years of my life have been filled with constant competition with others. As a result Iâ€™ve turned out to
be a pretty competitive person - if there was anything that I liked, I not only wanted to be good at it, but I also wanted
to be better at it than others.</p><p>Something happened right around when I graduated college and joined the workforce. My Saturdays were totally free and
werenâ€™t consumed by an all-day track meet where I would compete in the 800m and gruel over two laps trying to edge out
other people over the finish line. After work, my weekday nights consisted of me making dinner instead of living in the
library and taking caffeine pills to study for a test that Iâ€™ve been procrastinating to study for. </p><p>For the first time in my life there was no clear next step in what I had to do, and there was no clear competition that
I could engage in with peers. It was a dramatic change and something that I refer to as my quarter life crisis. Itâ€™s not
well documented in many places, but itâ€™s a concept that I think is
<a href="https://en.wikipedia.org/wiki/Quarter-life_crisis">picking up steam</a> in recent years.</p><h3>What does a Quarter-Life Crisis look like?</h3><p>I think thereâ€™s one song that really encapsulates this feeling, The Beatlesâ€™
<a href="https://www.youtube.com/watch?v=8scSwaKbE64">Nowhere Man</a>. The opening lines go:</p><blockquote><p>He's a real nowhere man.
Sitting in his nowhere land.
Making all his nowhere plans for nobody</p></blockquote><p>In fact, the song is what inspired me to name my blog post <strong>Nowhere Plans</strong>. John Lennon was 25 when he wrote this,
and I think he really captures the general feeling of this time in oneâ€™s life.</p><p>Back when I was in college and high school I was really competitive with other people, whether in academics or sports,
and I felt I had a strong purpose - anything that I did was to get further in these pursuits. It was a nice and easy
framework to live life by. Decision making was incredibly easy.</p><p>Fast forward to work-life ~&gt; I felt that I couldnâ€™t really relate much to my work peers, and all my college and high
school friends were vibing and doing their own thing in their respective companies. I spent my nights binge watching
shows, my weekends binge watching movies, and my free time working or hanging with peers talking about the binge
watched shows/movies. </p><p>The <a href="https://en.wikipedia.org/wiki/Seattle_Freeze">isolating</a> and incredibly gray city of Seattle didnâ€™t help not
feeling like a nowhere man in nowhere land.</p><p>Maybe Iâ€™ll get a cat? Will that help find some purpose to be doing something? Iâ€™ll download some dating apps and try to
get a girlfriend, hopefully that should bring me closer to my next stage in life of marriage right? Should I go to grad
school? I have to, no, I need to do these things now! ğŸ˜¬</p><p>I was truly lost in what my next steps in life were going to be, and felt that I was just aimlessly swimming around in a sea of uncertainty.</p><h3>Swimming Out</h3><p>When 2019 had come, I knew I wanted to do something different. I made a list of things that were important to me and that I
wanted to achieve. Visiting friends more, learning to play the piano, getting back into running races, fixing my posture, volunteering etc. </p><p>But the biggest shift in my thinking during this time was to try to bring back competition in my life. No, not competition
with others like I had previously experienced all my life, but instead with something much easier to think about. Myself. </p><p>Thereâ€™s a saying that you should â€œCompare yourself to who you were yesterday, not to who someone else is todayâ€. People always
say that youâ€™ll never find happiness comparing yourself to others, and that competitive stress is really bad for you. While that
is true, I also donâ€™t think many people advocate for competition against yourself. Itâ€™s possibly because of the stress and
anxiety induced from competition, but if youâ€™re anything like me, itâ€™s necessary to find some purpose and bring back a fire
in you to go about and live life.</p><p>In 2019, I had worked on things I had never done in my life in order to be a better person than who I was in years past.
Iâ€™ll probably never be as fit as I was in college on the varsity XC/T&amp;F teams, but hey, in 2019 I
set <a href="https://www.strava.com/athletes/19875553">personal records</a> in
the 10 mile and marathon (granted it was my only time running them but thatâ€™s besides the point). </p><p>I also had an entirely different view in perceiving others. Anyone who has spent 5 min on LinkedIn can describe this
feeling - seeing other people who are really successful and do the things that youâ€™ve always want to do is disheartening to
say the least. You feel like somewhere along your life path you made a wrong turn which didnâ€™t lead you to where that other
person is. But, at the end of the day, thatâ€™s just life - thereâ€™s always going to be someone better than you at everything
you do, you just have to find solace in that youâ€™re better than who you used to be.</p><h3>Where are you now?</h3><p>This past year has been wild. It felt like a decade fit into 9 months so far, but with all the major events and uncertainty
happening, Iâ€™ve never felt more certain about myself than I have now.</p><p>I try to focus on things that I really enjoy, and try to be better than my former self. I stopped going to my piano teacher
because of COVID, but still try to play the pieces that I remember learning every now and then while also trying to learn
some more music theory. Writing and deep diving on topics that I find interesting have always been a favorite past-time of
mine, why not start a blog and have others critique me to become better? And finally, if Iâ€™m going to make a career out of
my current profession, why not be the best that I can be in more ways than programming. Iâ€™ve engaged in more leadership
opportunities to mentor younger engineers and also participate in the daunting interviewing process but this time as an interviewer.</p><p>Through this process Iâ€™ve grown to know that I really enjoy helping and teaching others - itâ€™s something that has helped me
envision what I want to be doing as I grow.</p><p>I donâ€™t advise many people to live life with this much analysis on your past self, but if this story resonated with you I
encourage to try it out! Itâ€™s helped me get out of a quarter life crisis and will hopefully keep me going until my
inevitable mid-life crisis. Stay tuned for that post.</p></section></div>]]>
            </description>
            <link>https://www.arvarik.com/competition-quarter-life-crisis</link>
            <guid isPermaLink="false">hacker-news-small-sites-24710496</guid>
            <pubDate>Wed, 07 Oct 2020 17:22:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I wrote a new tool in Rust to convert M4Aâ€™s to MP3â€™s]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 16 (<a href="https://news.ycombinator.com/item?id=24709396">thread link</a>) | @WardyJP
<br/>
October 7, 2020 | http://johnward.net/2020/10/i-wrote-a-new-tool-in-rust-to-convert-m4as-to-mp3s/ | <a href="https://web.archive.org/web/*/http://johnward.net/2020/10/i-wrote-a-new-tool-in-rust-to-convert-m4as-to-mp3s/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>http://johnward.net/2020/10/i-wrote-a-new-tool-in-rust-to-convert-m4as-to-mp3s/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24709396</guid>
            <pubDate>Wed, 07 Oct 2020 15:46:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Rust and Raspberry Pi Tide Clock]]>
            </title>
            <description>
<![CDATA[
Score 94 | Comments 13 (<a href="https://news.ycombinator.com/item?id=24708345">thread link</a>) | @zdw
<br/>
October 7, 2020 | https://thefuntastic.com/blog/rust-tide-clock | <a href="https://web.archive.org/web/*/https://thefuntastic.com/blog/rust-tide-clock">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-v-10b1bd0a="" data-v-6f4ff34b=""><p><em>In this part 1 of 2 posts, I share the process of a heartwarming maker project built on top of Raspberry Pi and Rust. It's more a story than a how-to guide, but provides an interesting chronology of problems encountered. In part 2 I'll be getting technical and discussing Rust in-depth. Source code for this project can be found on <a href="https://github.com/thefuntastic/rust-tide-clock" target="_blank" rel="nofollow noopener noreferrer">Github</a></em></p>
<p>I had the good fortune to spend my summer with Alice's family who had recently moved to a seaside town. Tim, the patriarch of the family, was distraught to learn that, out of his impressive array of nautical implements, his tide clock readings were never accurate. This is the story of how we built him a surprise 60th birthday present he'd never forget: </p>
<p><img src="https://thefuntastic.com/blog/2020-09-25-Tide-Clock-12.jpg" title="Picture completed Tide-Clock"></p>
<h2 id="the-problem-with-tides">The Problem with Tides</h2>
<p>Most people understand the moons gravitational pull causes a regular ebb and flow in water levels. Those concerned with the sea will likely recite 6 hours and 10-ish minutes as the duration between low and high tide. Consider a wristwatch or wall clock, most mechanical timepieces already track a daily period of 12 hours. With a small adjustment to gearing ratios, it would be easy enough to build a clock that reports instead the tidal cycle of 12 hours and 25 minutes. Indeed this is how most ornamental tide clocks work. </p>
<p><img src="https://thefuntastic.com/blog/2020-09-Wikipedia-Tide-clock.jpg" title="Picture of tide clock - Wikipedia"></p>
<p>I was surprised, however, to learn <a href="http://www.coastalwiki.org/wiki/Tidal_asymmetry_and_tidal_basin_morphodynamics" target="_blank" rel="nofollow noopener noreferrer">tides can be asymmetric</a>! Instead of six hours, it might take the tide seven hours to come in and five hours to go out again. Local environmental factors like the shallowness of an estuary basin can have a pronounced effect on tidal regularity. In extreme places, like the Gulf of Mexico, this can be enough to reduce the regular cycle from four tides a day to just two. </p>
<p><img src="https://thefuntastic.com/blog/2020-09-25-Tide-Graph.png" title="WorldTides.info graph of tide station in the Gulf of Mexico showing asymmetry changing to produce single daily tide"></p>
<p>A disaster for the clockmaker! This asymmetry also changes with the lunar cycle, so it's not even possible for a mechanical clock to be consistently wrong. It makes intuitive sense if you think about it. Water is "stuff". When the tide changes that stuff has to go somewhere. If something makes it hard for stuff to move about, like say sandbars in a shallow basin, it's going to make the stuff pile up. The bigger the change in stuff, like say near spring tide, the more piling up is going to happen. If that stuff is still hanging about when the tide changes, the net effect is going to be an asymmetric tide.</p>
<h2 id="making-with-embedded-devices">Making with Embedded Devices</h2>
<p>To my mind, this was the perfect application for an internet-of-things powered device. By outsourcing the problem and fetching data from an API it meant the source of truth would always be accurate. Furthermore, a digital display could accurately visualise the asymmetric ebbs and floods.</p>
<h3 id="on-choosing-raspberry-pi">On choosing Raspberry Pi</h3>
<p>Initially, I considered using an Arduino, as I already had one knocking about my toolbox, together with a compatible LED screen. Ultimately I chose the path of least resistance because the Raspberry Pi comes with an integrated WiFi chip. Unlike the Arduino, the Pi is a mini-computer running an entire operating system. This is the sledgehammer approach, but has advantages considering the project would eventually live in the hands of someone unfamiliar with embedded tech. If the WiFi connection were to drop or need changing, logging onto a desktop would be a much friendlier experience. </p>
<p><img src="https://thefuntastic.com/blog/2020-09-Tide-Clock-21.jpg" title="Raspbian Desktop Interface running on Raspberry Pi"></p>
<p>Having settled on a Raspberry Pi build, the next job was to find a suitable display. Considering tides change relatively slowly, it was tempting to use an e-ink display for crazy power efficiency. That might be necessary if the clock was battery-powered, however, the need to keep the Pi running meant we were already committed to a plug-in power supply. In the end, I choose the 128x32 pixel <a href="https://thepihut.com/collections/waveshare/products/128x32-2-23inch-oled-display-hat-for-raspberry-pi" target="_blank" rel="nofollow noopener noreferrer">Waveshare 1305</a>. Its convenient "hat" form factor meant no soldering. Also, OLED looks crazy good compared to the standard LED screen I already had. </p>
<p><img src="https://thefuntastic.com/blog/2020-09-Tide-Clock-3.jpg" title="Waveshare OLED screen shown attached to Raspberry Pi"></p>
<h3 id="on-choosing-rust">On Choosing Rust</h3>
<p>At this point, I was still unsure of which programming language to use. Whilst the Raspberry Pi can run anything that compiles to Linux, communication to the screen happens through the Pi's <code>GPIO</code> pins (<a href="https://en.wikipedia.org/wiki/General-purpose_input/output" target="_blank" rel="nofollow noopener noreferrer">General Purpose Input/Output</a>). All information is transmitted by setting pins high and low and feels reminiscent of working on Arduino and other embedded platforms. Even though the screen is just 128x32 (aka 4096) pixels, that's far more destinations than the Pi's 40 <code>GPIO</code> pins can individually address. Fortunately protocols, in this case <code>SPI</code>, exist to pack data into compressed blocks which can be sent over the limited bandwidth of the IO pins. </p>
<p><img src="https://thefuntastic.com/blog/2020-09-Tide-Clock-31.jpg" title="Raspberry Pi 3 GPIO Pins"></p>
<p>The screen manufacturer-provided <a href="https://www.waveshare.com/wiki/2.23inch_OLED_HAT#Demo_codes" target="_blank" rel="nofollow noopener noreferrer">3 code samples</a>: 1 written in python and 2 written in C. Python was my immediate choice, given the online nature of the project, however, I simply couldn't get it to work. The two C samples were curious. One was built on top of a bring-your-own driver for the embedded Broadcom chip that controls <code>GPIO</code> pins. The other was written on top of <a href="https://github.com/WiringPi/WiringPi" target="_blank" rel="nofollow noopener noreferrer"><code>wiringPi</code></a>, which ships with Raspbian (aka the Pi flavoured Linux OS) and seems to be the blessed path for doing IO. However, I was saddened to learn this open source project was largely the efforts of a single person who has since <a href="http://wiringpi.com/wiringpi-deprecated/" target="_blank" rel="nofollow noopener noreferrer">stepped down as a maintainer</a> due to open source burnout. It's a worrying trend I'm seeing a lot lately. </p>
<p><img src="https://thefuntastic.com/blog/2020-09-Tide-Clock-4.jpg" title="Waveshare OLED screen showing Hello World"></p>
<p>Despite the above, the current <code>wiringPi</code> sample still works well. In theory I'm sure it would have been absolutely possible to complete the rest of the project in C. As a language though, C tends to come batteries-not-included. The prospect of stumbling my way through image processing, data parsing, fetching URLs and date-time munging did not fill me with joy, especially given my rudimentary C experience. I'd much rather be building sand-castles in a play pit that didn't require me to build my shovel first.  </p>
<p>I've been "Rust-curious" for a long time, and it's done a great job of establishing itself as <strong>an alternative for workloads where C was historically the only viable candidate </strong>(high performance or memory-constrained). That it does so without forsaking a first-class developer experience is one of the many reasons it's become such <a href="https://insights.stackoverflow.com/survey/2020#technology-most-loved-dreaded-and-wanted-languages" target="_blank" rel="nofollow noopener noreferrer">a beloved language</a>. Through the package manager, <code>cargo</code>, I'd have a thriving ecosystem of 3rd party libraries (aka crates) within arms reach. Indeed, I quickly found <code>rppal</code> (<a href="https://github.com/golemparts/rppal" target="_blank" rel="nofollow noopener noreferrer">Raspberry Pi Peripheral Access Layer</a>), a Rust crate to manage <code>GPIO</code>.</p>
<pre>

<span>[</span><span>dependencies</span><span>]</span>
<span>image</span> <span>=</span> <span>"0.23.8"</span>
<span>chrono</span> <span>=</span> <span>"0.4"</span>
<span>serde</span> <span>=</span> <span>{</span> <span>version</span> <span>=</span> <span>"1.0"</span><span>,</span> <span>features</span> <span>=</span> <span>[</span><span>"derive"</span><span>]</span> <span>}</span>
<span>serde_json</span> <span>=</span> <span>"1.0"</span>
<span>toml</span> <span>=</span> <span>"0.5"</span>
<span>ordered-float</span> <span>=</span> <span>"2.0"</span>
<span>reqwest</span> <span>=</span> <span>{</span> <span>version</span> <span>=</span> <span>"0.10"</span><span>,</span> <span>features</span> <span>=</span> <span>[</span><span>"json"</span><span>]</span> <span>}</span>
<span>tokio</span> <span>=</span> <span>{</span> <span>version</span> <span>=</span> <span>"0.2"</span><span>,</span> <span>features</span> <span>=</span> <span>[</span><span>"full"</span><span>]</span> <span>}</span>
<span>simple-error</span> <span>=</span> <span>"0.1.9"</span>

<span>[</span><span>target.'cfg(target_arch="arm")'.dependencies</span><span>]</span>
<span>rppal</span> <span>=</span> <span>"0.9.0"</span></pre>
<p>There were also plenty of reasons <strong>not</strong> to choose Rust. Scant few weeks remained until the birthday party; if we were going to pull off the surprise it meant sticking to a very aggressive timeline. <code>rppal</code> is not a port of <code>wiringPi</code>, the <code>SPI</code> protocol implementation details might differ in subtle but fundamental ways, enough to bork the entire endeavour. Rust's borrow checker is infamously unforgiving to the uninitiated (it's a bit of an arsehole really). If I was being a responsible lead I'd probably command the troops to trudge on with C. But hack projects really should be about personal edification. So sod it, how hard could it be? </p>
<h2 id="programming-the-app">Programming the App</h2>
<p>Lets just say I got a beating, the likes of which demand a doughnut-shaped cushion afterwards. Rust's learning curve is notoriously steep, and hoping to grok it on such a tight schedule was perhaps optimistic. To Rust's credit, there are plenty of escape hatches to get yourself out of (or into) trouble. This is useful when writing your own code, but by consuming 3rd party libraries you're expected to be more fluent with "idiomatic Rust". </p>
<p><em>(Stay tuned for part 2 where I'll discuss my technical first impressions of Rust)</em>.</p>
<p>Idiomatic understanding is difficult to rush, as it requires a breadth of exposure. I'd fare better now, but as my first-touch point I simply couldn't figure out how I was meant to consume the <code>rppal</code> library. Combing the changelog revealed a big refactor towards a <strong>more</strong> idiomatic and Rust-like API. By reverting to an earlier <strong>less</strong> idiomatic version I'd found a cheeky get out of jail card. Breakthrough, at last, a single pixel signalled business time. </p>
<p><img src="https://thefuntastic.com/blog/2020-09-Tide-Clock-5.jpg" alt="The happiest pixel" title="Displaying single pixel"></p>
<h3 id="interface-and-ui">Interface and UI</h3>
<p>Now that drawing was possible, the next question was a matter of <em>what</em> to draw? In answer, I cracked open a pixel editor and performed a design sprint in miniature. Several iterations later I had a single image that served as my "design document". </p>
<p><img src="https://thefuntastic.com/blog/2020-09-Tide-Clock-6.jpg" title="Designing in the pixel editor"></p>
<p>That's a great example of why this project was so much fun. The problem space left room to flex muscles in every layer of abstraction, whilst being constrained enough to avoid becoming an onerous chore. Font rendering is another example. By using the old school technique of copying slices from a sprite sheet I didn't have to bother myself with font files or font rendering libraries.   </p>
<p><img src="https://thefuntastic.com/blog/2020-09-Tide-Clock-7.jpg" title="Designing the font. First on paper, then on screen"></p>
<p>To help me copy slices I relied on the <code>image</code> crate. Again there was a high degree of fumbling to get my head around the API, but once I did I was very impressed by the quality of the library. Even if the ecosystem is still technically maturing, the quality already on display announces Rust's arrival as a serious contender. </p>
<pre>

 
<span>pub</span> <span>fn</span> <span>new</span><span>(</span><span>)</span> <span>-&gt;</span> <span>Font5</span> <span>{</span>
    <span>let</span> p <span>=</span> <span>Path</span><span>::</span><span>new</span><span>(</span><span>"resources/Font-5px.png"</span><span>)</span><span>;</span>

    <span>let</span> img <span>=</span> <span>image<span>::</span></span><span>open</span><span>(</span>p<span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>.</span><span>to_rgb</span><span>(</span><span>)</span><span>;</span>

    <span>let</span> <span>mut</span> faces <span>=</span> <span>HashMap</span><span>::</span><span>new</span><span>(</span><span>)</span><span>;</span>

    faces<span>.</span><span>insert</span><span>(</span><span>'1'</span><span>,</span> img<span>.</span><span>view</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>,</span> <span>5</span><span>)</span><span>.</span><span>to_image</span><span>(</span><span>)</span><span>)</span><span>;</span>
    faces<span>.</span><span>insert</span><span>(</span><span>'2'</span><span>,</span> img<span>.</span><span>view</span><span>(</span><span>2</span><span>,</span> <span>0</span><span>,</span> <span>3</span><span>,</span> <span>5</span><span>)</span><span>.</span><span>to_image</span><span>(</span><span>)</span><span>)</span><span>;</span>
    faces<span>.</span><span>insert</span><span>(</span><span>'3'</span><span>,</span> img<span>.</span><span>view</span><span>(</span><span>6</span><span>,</span> <span>0</span><span>,</span> <span>3</span><span>,</span> <span>5</span><span>)</span><span>.</span><span>to_image</span><span>(</span><span>)</span><span>)</span><span>;</span>
    <span>...</span>
    faces<span>.</span><span>insert</span><span>(</span><span>'X'</span><span>,</span> img<span>.</span><span>view</span><span>(</span><span>95</span><span>,</span> <span>6</span><span>,</span> <span>3</span><span>,</span> <span>5</span><span>)</span><span>.</span><span>to_image</span><span>(</span><span>)</span><span>)</span><span>;</span>
    faces<span>.</span><span>insert</span><span>(</span><span>'Y'</span><span>,</span> img<span>.</span><span>view</span><span>(</span><span>99</span><span>,</span> <span>6</span><span>,</span> <span>3</span><span>,</span> <span>5</span><span>)</span><span>.</span><span>to_image</span><span>(</span><span>)</span><span>)</span><span>;</span>
    faces<span>.</span><span>insert</span><span>(</span><span>'Z'</span><span>,</span> img<span>.</span><span>view</span><span>(</span><span>103</span><span>,</span> <span>6</span><span>,</span> <span>3</span><span>,</span> <span>5</span><span>)</span><span>.</span><span>to_image</span><span>(</span><span>)</span><span>)</span><span>;</span>

    <span>Font5</span> <span>{</span> faces <span>}</span>
<span>}</span></pre>
<p>Up till now, I had been doing all the development directly on a Raspberry Pi 3. It worked well enough, but I was sorely missing the quality of life features I could enjoy in a full developer environment. The <code>image</code> crate was convenient enough that I started using it for other â€¦</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://thefuntastic.com/blog/rust-tide-clock">https://thefuntastic.com/blog/rust-tide-clock</a></em></p>]]>
            </description>
            <link>https://thefuntastic.com/blog/rust-tide-clock</link>
            <guid isPermaLink="false">hacker-news-small-sites-24708345</guid>
            <pubDate>Wed, 07 Oct 2020 14:06:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Complete AWS Lambda Handbook for Beginners]]>
            </title>
            <description>
<![CDATA[
Score 127 | Comments 34 (<a href="https://news.ycombinator.com/item?id=24708237">thread link</a>) | @maridashbird
<br/>
October 7, 2020 | https://dashbird.io/blog/complete-aws-lambda-handbook-beginners-part-1/ | <a href="https://web.archive.org/web/*/https://dashbird.io/blog/complete-aws-lambda-handbook-beginners-part-1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div><p>Welcome to the Serverless world. One of the first things youâ€™ll hear about is AWS Lambda - and youâ€™ll continue to keep hearing about it! While architecture can be serverless without Lambdas involved, itâ€™s very often the key component within a serverless application. In the first post of this 3-part AWS Lambda Handbook series, we run through what is AWS Lambda, dialling back to basics with the various terminology, how to create a Lambda function and how to run it.&nbsp;</p>
<blockquote>
<p>Read <a href="https://dashbird.io/blog/complete-aws-lambda-handbook-beginners-part-2/">Part 2</a> and <a href="https://dashbird.io/blog/complete-aws-lambda-handbook-beginners-part-3/">Part 3</a></p>
</blockquote>
<h2 id="what-is-aws-lambda-and-what-does-it-do">What is AWS Lambda, and what does it do?</h2>
<p>AWS Lambda is an event-driven serverless compute platform, spinning up the service in response to an event - find out more about Lambda triggers in <a href="https://dashbird.io/blog/complete-guide-lambda-triggers-design-patterns-part-1/">part 1</a> and <a href="https://dashbird.io/blog/complete-guide-lambda-triggers-design-patterns-part-2/">part 2</a> of our Complete Guide to Lambda Triggers series. Your code simply sits there as a file while AWS keeps a lookout for the trigger event youâ€™ve set. When that event occurs, your code is executed and the required operations are carried out. Itâ€™s deemed â€˜serverlessâ€™ because the server doesnâ€™t exist until the user goes out to look for it - this is the epitome of <a href="https://dashbird.io/blog/what-is-faas-function-as-a-service/">Function-as-a-Service (FaaS)</a>.</p>
<p>Another bonus to Lambda is itâ€™s auto-scalability managed by AWS, meaning you donâ€™t need to think about infrastructure. The service will automatically accommodate growing needs and likewise, will scale down to conserve resources. All of this makes AWS Lambda a great solution to reduce waste of resources and budget.&nbsp;</p>
<h2 id="aws-lambda-definitions-explained">AWS Lambda Definitions Explained</h2>
<p>Before getting into how to set up and configure Lambda, below are definitions and terminology commonly used and spoken about.</p>
<p>Lambda Function: a group of related statements that perform a specific task in your application. It consists of code and any dependencies that are associated with it. Each Lambda function has its associated configuration information (name, description, entry point, and resource requirements).</p>
<p>The function itself has the following important aspects associated with it:</p>
<ol>
<li>
<p>Trigger: A set of activities which invokes the function (runs the code you provide). The activity could be anything like a new object coming to your S3 bucket, a website or a service going down, an API call, etc.</p>
</li>
<li>
<p>The actual function: This is the run-time code that constitutes the function. AWS supports Python, Node.js, C#, Go and Java8 as runtime environments.&nbsp;</p>
</li>
<li>
<p>Resources: Each function can be assigned certain Roles, which grants the function certain privileges such as reading S3 bucket contents, writing results to a database and so on.</p>
</li>
</ol>
<p><img src="https://dashbird.io/images/blog/2020-05-vacation-buffer/1aws-lambda-api-gateway-trigger.png" alt="AWS Lambda anatomy" title="AWS Lambda anatomy"></p>
<p>The triggers are shown to the left, and in this case an API gateway trigger is active. The resources are shown on the right, which in this case, are CloudWatch Logs and DynamoDB.</p>
<p>Event Sources: an entity that publishes events. An event source can be an AWS service or developer-created application that produces events that trigger a function to run.</p>
<p>Invocation: an invocation is called up to execute a specific Lambda function. These are triggers for the code of the function to start running. Invocations can be either <a href="https://docs.aws.amazon.com/lambda/latest/dg/invocation-options.html">synchronous or asynchronous</a>.</p>
<p>Event Source Mapping: a configuration of AWS services in which an event source is tied to a specific Lambda function. It enables automatic invocation of a Lambda function when specific events occur.</p>
<p>Lambda Execution Model: When you create a Lambda function, you can specify configuration information, such as the amount of memory and maximum execution time that you allow for your function. When that function is invoked, AWS Lambda launches an <a href="https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html">Execution Context</a> based on the configuration settings you have provided.</p>
<p>Cold Starts: A cold start happens when a Lambda function is invoked after not being used for an extended period of time, which results in increased invocation latency (more on this later).</p>
<h2 id="aws-lambda-configuration-elements">AWS Lambda Configuration Elements</h2>
<p>A Lambda function consists of the code and associated dependencies, and it also has configuration information within it. An API is also provided so you can update some of the configuration data. Lambda function configuration information comes with these critical elements:</p>
<ul>
<li>
<p>Calculating the required resources: specifying the amount of memory that you wish to allocate for your Lambda function. AWS Lambda allocates CPU power in proportion to the memory by the same ratio as a general-purpose AWS EC2 instance type, like an M3 type.&nbsp;</p>
</li>
<li>
<p>Maximum execution time (timeout): specified to prevent the Lambda function from running non-stop. Since youâ€™re paying for the AWS resources that are used to run your Lambda function, this is particularly important. Upon reaching the timeout, AWS Lambda is terminating the execution of your Lambda function. The recommended setting is valued upon the expected execution time.</p>
</li>
<li>
<p>IAM role (execution role): the role that AWS Lambda performs on your behalf when executing a Lambda function.</p>
</li>
<li>
<p>Handler name: the method of entry point that runs your code with any event source dependencies included as a part of your Lambda function. You will be able to discover more details, and the quality features of monitoring and debugging AWS Lambda using this.&nbsp;</p>
</li>
</ul>
<h2 id="creating-a-simple-aws-lambda-function">Creating a Simple AWS Lambda Function</h2>
<p>Letâ€™s create a simple Lambda function that is invoked by an API call, i.e. we generate a URL, which when entered in the browser would invoke the function. Our input would be passed into the function via this URL and the output would be returned and shown in the browser.</p>
<p>Step 1: Creating the function</p>
<p>In the Lambda console panel, click on create function. Give your function a name, in our case, it is DemoFunction. Also select the runtime as Python3, as we will be using that particular language for this example. Lastly, give your functionâ€™s role a name and, from Policy Templates, select Simple Microservice permissions.</p>
<p><img src="https://dashbird.io/images/blog/2020-05-vacation-buffer/2creating-new-aws-serverless-function.png" alt="AWS Lambda Author From Scratch" title="AWS Lambda Author From Scratch"></p>
<p>Click on Create Function and you will be taken to the next screen where you can provide the actual code. We are authoring this API from scratch, but there are tons of templates from Amazon repository that you can explore.</p>
<p>The next page will have an inline text editor with a simple python function in there. Replace that with the following content:</p>
<pre><code>import json

print('Loading function')

def lambda_handler(event, context):

&nbsp;&nbsp;&nbsp;&nbsp;firstName = event['first']

&nbsp;&nbsp;&nbsp;&nbsp;lastName = event['last']

&nbsp;&nbsp;&nbsp;&nbsp;return 'Greetings, ' + firstName + ' ' + lastName +'!' 
</code></pre><p>The first line is for parsing the JSON using the JSON library in Python. The lambda_handler function gets the event as one of its parameters; this event brings along a set of data. The first and second line inside the function extracts whatever data is labeled first and second, and stores them into the respective variables.</p>
<p>The last line returns a message back and thatâ€™s what we will see in our browser.</p>
<p><img src="https://dashbird.io/images/blog/2020-05-vacation-buffer/3creating-new-aws-serverless-function.png" alt="Creating AWS Lambda function" title="Creating AWS Lambda function"></p>
<p>We can add an API Gateway trigger right here, but for the sake of clarity, letâ€™s do it separately. For now, we can click Save and move into the testing phase.</p>
<p>Step 2: Testing your function</p>
<p>To test your function, just click on the top right corner where it says â€˜TestEventâ€™, then click on Configure Test Event.</p>
<p><img src="https://dashbird.io/images/blog/2020-05-vacation-buffer/4creating-new-aws-serverless-function.png" alt="Testing AWS Lambda function" title="Testing AWS Lambda function"></p>
<p>Here we will have our first encounter with a JSON payload. In the template TestEvent.</p>
<p><img src="https://dashbird.io/images/blog/2020-05-vacation-buffer/5test-new-aws-serverless-function.png" alt="Testing AWS Lambda JSON function" title="Testing AWS Lambda JSON function"></p>
<p>Replace the fileâ€™s content with the following lines:</p>
<pre><code>{

&nbsp;&nbsp;"first": "Jane",

&nbsp;&nbsp;"last": "Doe"

}
</code></pre><p>Now that we have saved the test event. Click on Test in the previous menu. Upon successful execution you should see:</p>
<p><img src="https://dashbird.io/images/blog/2020-05-vacation-buffer/6creating-new-aws-serverless-function.png" alt="AWS Lambda function output" title="AWS Lambda function output"></p>
<p>Step 3: Setting up a Trigger</p>
<p>As mentioned before, our user would <a href="https://dashbird.io/blog/what-are-aws-lambda-triggers/">invoke the function</a> by accessing a certain URL. To enable that go to the API Gateway Console under your AWS Services and click on Get Started or New API option.</p>
<p>Letâ€™s create one from scratch:</p>
<p><img src="https://dashbird.io/images/blog/2020-05-vacation-buffer/7creating-new-aws-serverless-function.png" alt="AWS Lambda API creation" title="AWS Lambda API creation"></p>
<p>Our API is named dashbird-api. After clicking on Create API. You will get the resources that the API has access to (listed in the next menu):</p>
<p><img src="https://dashbird.io/images/blog/2020-05-vacation-buffer/8creating-new-aws-serverless-function.png" alt="AWS Lambda API resources" title="AWS Lambda API resources"></p>
<p>Since there are no resources, we just get a forward-slash. But you can create a new resource by using the Actions drop-down and picking Create Resource.</p>
<p><img src="https://dashbird.io/images/blog/2020-05-vacation-buffer/9creating-new-aws-serverless-function.png" alt="AWS Lambda child resources" title="AWS Lambda child resources">
In the resource list, you can select this new resource (named greetings), click on actions and select Create Method. Our HTTP request method is going to be a GET request since our aim is to get an appropriate response from invoking the function.</p>
<p><img src="https://dashbird.io/images/blog/2020-05-vacation-buffer/91creating-new-aws-serverless-function.png" alt="AWS Lambda GET request" title="AWS Lambda GET request"></p>
<p>The method will have a Lambda integration option, select that and then enter the function name chosen by you earlier Step 2. Also, from Step 2â€™s screenshot, make note of the functionâ€™s <a href="https://dashbird.io/knowledge-base/aws-cloud/arn-amazon-resource-names/">ARN</a> (top-right corner), it has the string eu-central-1 indicating the region it is in. Make sure that the same region is selected for the Lambda region also, as shown above. It would then ask permission for invoking the function; grant that and now we are ready for the final modification.</p>
<p>The GET method execution is explained in this diagram:</p>
<p><img src="https://dashbird.io/images/blog/2020-05-vacation-buffer/92-GET-method-execution-aws-serverless.png" alt="AWS Lambda modify method" title="AWS Lambda modify method">
We still need to make sure that the input parameters are passed on correctly. For that we need to modify the Integration Request stage from above. You can click on it to make modifications:</p>
<p><img src="https://dashbird.io/images/blog/2020-05-vacation-buffer/93-GET-method-execution-aws-serverless.png" alt="AWS Lambda body mapping" title="AWS Lambda body mapping"></p>
<p>Leave everything as it is, except at the very bottom of the menu where you will find the Body Mapping Template here we get to describe our input template.The template is going to be of type application/json :</p>
<pre><code data-lang="{">
&nbsp;&nbsp;&nbsp;&nbsp;"first": "$input.params('first')",

&nbsp;&nbsp;&nbsp;&nbsp;"last": "$input.params('last')"

}
</code></pre><p>The dollar sign and the input.params() part act as a placeholder and helps us define the structure of a proper request. Now we can save our changes, and click on Actions and select Deploy API option. It will ask for a stage name; give it a suitable name (in our case it is called prod). All is set! We can now run this function in real-time.</p>
<h2 id="running-the-function">Running the Function&nbsp;</h2>
<p>The function can be invoked using a unique URL associated with it. In the API console, where we first selected Resources, select Stage submenu instead. Then drop down to greetings and then to the GET option.</p>
<p><img src="https://dashbird.io/images/blog/2020-05-vacation-buffer/94-GET-method-execution-aws-serverless.png" alt="AWS Lambda invoke URL" title="AWS Lambda invoke URL"></p>
<p>It will give you an invoke URL, which you can click on for the function to run. However, on the first try you might get an error message if you didnâ€™t give any input. You can rectify this by modifying the URL like this:</p>
<p><a href="https://.........amazonaws.com/prod/greetings?first=John&amp;last=Doe">https://.........amazonaws.com/prod/greetings?first=John&amp;last=Doe</a></p>
<p>Adding â€¦</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dashbird.io/blog/complete-aws-lambda-handbook-beginners-part-1/">https://dashbird.io/blog/complete-aws-lambda-handbook-beginners-part-1/</a></em></p>]]>
            </description>
            <link>https://dashbird.io/blog/complete-aws-lambda-handbook-beginners-part-1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24708237</guid>
            <pubDate>Wed, 07 Oct 2020 13:57:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Lessons learned from onboarding emails with no HTML styling]]>
            </title>
            <description>
<![CDATA[
Score 165 | Comments 166 (<a href="https://news.ycombinator.com/item?id=24707994">thread link</a>) | @pau_alcala
<br/>
October 7, 2020 | https://blog.palabra.io/great-onboarding-plain-text | <a href="https://web.archive.org/web/*/https://blog.palabra.io/great-onboarding-plain-text">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p><em>By <a href="https://www.linkedin.com/in/naara-abril-taker/">Abril Taker</a> - Content Creator at <a href="https://www.palabra.io/?utm_medium=best-onboarding&amp;utm_source=blog">Palabra</a></em></p><p>Great onboarding emails don't need to have impressive design. They should provide a clear path for new users to follow to get as much value from your product and as quickly as possible. While creating <a href="https://www.palabra.io/?utm_medium=best-onboarding&amp;utm_source=blog">Palabra's</a> onboarding, we went through our favorite plain text emails. Here's what we learned.</p><h2>Plain text is the way to go ğŸš€</h2><p>Writing HTML emails takes a lot of time, even with image based builders. As an early stage company, we simply didn't have enough time to design and mark our emails.</p><p>We also have <a href="https://blog.palabra.io/plain-text-engagement">a lot of reasons to go with plain text</a> instead of image-based. It helps deliverability, accesibility and looks much more real and important than ad-looking emails.</p><p>That's why we decided to go with plain text emails all the way. We explored different email sequences that used plain text (or simple styling) to understand what they did best. And where better to start than our very own inbox?</p><p>We noticed a few of the onboarding email sequences we got were really helpful for us as users. They kept simple a simple design and used mostly text to share their best features. </p><p>And we found some awesome examples of onboarding sequences that use little to no HTML styling:</p><ul><li>Notion's awesome and personal onboarding (simple styling).</li><li>Superhuman's daily bits of information on its greatest features (only text and images).</li><li><a href="http://zest.is/">Zest.is</a> drips using only plain text and images or gifs (our personal favorite).</li></ul><p>Here's what we discovered.</p><h2>Welcome emails are more than a confirmation</h2><p>Every SaaS company must start their onboarding sequences with a welcome message that is sent when a user joins the platform. This is a must by now, since everyone who signs up will expect some sort of confirmation of their transaction.</p><p>I mean, it is essentially a welcome message, but it can be so much more.</p><p><span>
      <span></span>
  <img alt="Image" title="Image" src="https://blog.palabra.io/static/7cbe707b1b06d73390376c23df182939/f0157/01.png" srcset="https://blog.palabra.io/static/7cbe707b1b06d73390376c23df182939/5243c/01.png 240w,https://blog.palabra.io/static/7cbe707b1b06d73390376c23df182939/ab158/01.png 480w,https://blog.palabra.io/static/7cbe707b1b06d73390376c23df182939/f0157/01.png 809w" sizes="(max-width: 809px) 100vw, 809px" loading="lazy">
    </span></p><p>For example, SuperHuman sends you a warm welcome message that immediately teaches you how to use their Command.</p><p>Another excellent example of welcome email is from Zest:</p><p><span>
      <span></span>
  <img alt="Image" title="Image" src="https://blog.palabra.io/static/484d998ef2ecfebe45feeffb359a512e/7d769/02.png" srcset="https://blog.palabra.io/static/484d998ef2ecfebe45feeffb359a512e/5243c/02.png 240w,https://blog.palabra.io/static/484d998ef2ecfebe45feeffb359a512e/ab158/02.png 480w,https://blog.palabra.io/static/484d998ef2ecfebe45feeffb359a512e/7d769/02.png 960w,https://blog.palabra.io/static/484d998ef2ecfebe45feeffb359a512e/b1884/02.png 994w" sizes="(max-width: 960px) 100vw, 960px" loading="lazy">
    </span></p><p>We love these simple but catchy lines. They nail the exact effect that plain text should achieve: make you feel among friends.</p><p>Both Superhuman and Zest suggest a next step you should follow after receiving that first email. This gives a clear path for people to follow if they want to get value from their products. And that's the best thing you can do with a welcome message.</p><h2>A name and a face</h2><p>This is a great tip to increase engagement. I always feel awkward when I donâ€™t know who is writing on the other side. Is it the CEO? Someone from Sales? Is it a super intelligent baby? Who knows.</p><p><span>
      <span></span>
  <img alt="Image" title="Image" src="https://blog.palabra.io/static/e6bb721d1b9ff6f37091aedd39fcc4d5/0248a/03.png" srcset="https://blog.palabra.io/static/e6bb721d1b9ff6f37091aedd39fcc4d5/5243c/03.png 240w,https://blog.palabra.io/static/e6bb721d1b9ff6f37091aedd39fcc4d5/ab158/03.png 480w,https://blog.palabra.io/static/e6bb721d1b9ff6f37091aedd39fcc4d5/0248a/03.png 658w" sizes="(max-width: 658px) 100vw, 658px" loading="lazy">
    </span></p><p>We can see this information clearly in Notionâ€™s emails. Ivan is not only a name, he is Notionâ€™s Co-founder. Itâ€™s flattering to receive a direct message from a co-founder, it also gives the impression of commitment from the very roots of the company. </p><p>At <a href="https://www.palabra.io/?utm_medium=best-onboarding&amp;utm_source=blog">Palabra</a> we do the very same thing. Our emails are sent by Paula or Karen, who are the founders (and the heart) of this project. </p><p>A photo is not a requirement in itself, yet, the clearer the image we have of the person who sends and receives the emails, the more engagement we can generate.</p><p>Even if you have hundreds of people working in your business, you can create an identity to address your users.</p><h2>Emojis in the subject (use with caution)</h2><p>This point is more to talk about email subjects, a very important topic that sometimes is forgotten.</p><p>If every email is a gift to your users, the subject is the wrapping paper. You want it to be shining, flashy, stunning so the reader has no other option than to open the email.</p><p>Definitely, most of the plain text onboarding sequences than we observed have emojis (at some stage) in their subjects. </p><p>Remember: emojis are important, but they have to reinforce the idea of the text in the subject. Otherwise youâ€™re gonna look cu-cu or, even worse, desperate.</p><p>Zest win the contest of better subjects seding thing like: </p><ul><li>you here -&gt; ğŸ’—</li><li>make yourself at home ğŸ‹</li></ul><p>You can also include them in the body of the email to generate a greater visual impact and a neatear appearance.</p><h2>Email 'til you make it</h2><p>Yes, we received tons of emails per week. In fact, Superhuman mentioned it in their onboarding email sequence.</p><p><span>
      <span></span>
  <img alt="Image" title="Image" src="https://blog.palabra.io/static/21d1789e9a7a13f7b48e1446974657ed/a9fc9/04.png" srcset="https://blog.palabra.io/static/21d1789e9a7a13f7b48e1446974657ed/5243c/04.png 240w,https://blog.palabra.io/static/21d1789e9a7a13f7b48e1446974657ed/ab158/04.png 480w,https://blog.palabra.io/static/21d1789e9a7a13f7b48e1446974657ed/a9fc9/04.png 701w" sizes="(max-width: 701px) 100vw, 701px" loading="lazy">
    </span></p><p><em>(this is simply genius)</em></p><p>But preciscely for that reason you have to be present. The first days are critical to impress your users.</p><p>Zest has the strategy of sending an email the first day a user joins. And then three more during the second day, another 4 days after and another one 10 days after.</p><p>Imagine someone youâ€™re dating sends you 4 emails in 10 days (well, in that case, youâ€™re probably Meg Ryan, so maybe itâ€™s not so bad).</p><p>This could sound excessive, but it can take a while until people understand your value. Just make sure the value you're providing is clear, and that each email you send has a reason to be in people's inbox.</p><h2>Thank yous matter</h2><p>Far from recommending you stalk your users, we want to encourage you to use emails as a tool for a meaningful exchange of information. You can learn one thing or two about your own service or product.</p><p>The Superhuman sequence puts feedback as a priority, using sentences like:</p><p>â€œ<strong>We love hearing your feedback: please reply to this email and say hello :)â€</strong></p><p><strong>â€œWe love hearing from you! Please reply and let us know what you think ğŸ˜ƒâ€</strong></p><p>In the email sequence that we mentioned from Zest, at the 10 day after the user joins, they ask for the thoughts and feelings about the platform and for the likes and dislikes.</p><p>A â€œthank youâ€ at the end of every email leaves a good impression. Of course. Itâ€™s also a good idea to make a special thanking email. When a business is growing, every user is something to thank, so let them know that in your own words (or emojis!).</p><p>If you read this far, ping us at <a href="https://twitter.com/palabraio">Twitter</a> and tell us who sent you your favorite onboarding emails.</p><hr><p>Hope you enjoyed this post! If you are curious about what you can do with Palabra or would just like to try it go ahead and <a href="https://www.palabra.io/?utm_medium=best-onboarding&amp;utm_source=blog">create an account here</a>.</p></section></div>]]>
            </description>
            <link>https://blog.palabra.io/great-onboarding-plain-text</link>
            <guid isPermaLink="false">hacker-news-small-sites-24707994</guid>
            <pubDate>Wed, 07 Oct 2020 13:35:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Making the Monty Hall problem weirder but obvious]]>
            </title>
            <description>
<![CDATA[
Score 160 | Comments 208 (<a href="https://news.ycombinator.com/item?id=24707305">thread link</a>) | @dyno-might
<br/>
October 7, 2020 | https://dyno-might.github.io/2020/09/17/making-the-monty-hall-problem-weirder-but-obvious/ | <a href="https://web.archive.org/web/*/https://dyno-might.github.io/2020/09/17/making-the-monty-hall-problem-weirder-but-obvious/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
    <div>
        







<div>
    <div>
        <div>
            
            <p><strong>Sep 17, 2020</strong></p>
            
            



<p>The <a href="https://en.wikipedia.org/wiki/Monty_Hall_problem">Monty Hall problem</a> is famously unintuitive. This post starts with an extreme version where the solution is blindingly obvious. We then go through a series of small changes. It will be clear that these donâ€™t affect the solution. At the end, we arrive at the classic Monty Hall problem.</p>

<p>For reference, the <a href="https://en.wikipedia.org/wiki/Monty_Hall_problem">classic formulation</a> goes:</p>

<blockquote>
  <p>Suppose youâ€™re on a game show, and youâ€™re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows whatâ€™s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, â€œDo you want to pick door No. 2?â€ Is it to your advantage to switch your choice?</p>
</blockquote>

<p>Intuitively, many people guess it doesnâ€™t matter if you switch. But it does. You get the car 2/3 of the time if you switch, and 1/3 of the time if you donâ€™t. Why?</p>



<p>Hereâ€™s our first game.</p>

<ol>
  <li>There are 10 doors. A car is randomly placed behind one, and goats behind the other 9.</li>
  <li>You pick one door.</li>
  <li>You get two options:
    <ul>
      <li>Option A: You get whatever is behind the door you picked.</li>
      <li>Option B: You get whatever is behind all of the other 9 doors.</li>
    </ul>
  </li>
</ol>

<p><img src="https://dyno-might.github.io/img/monty-hall/game1.png">
</p>

<p>Thereâ€™s nothing mysterious here. You should choose option B. Thereâ€™s only a 10% chance you picked the right door, so thereâ€™s a 90% chance the car is behind one of the others.</p>



<p>Now, we slightly update the game (new part in bold).</p>

<ol>
  <li>There are 10 doors. A car is randomly placed behind one, and goats behind the other 9.</li>
  <li>You pick one door.</li>
  <li><strong>Monty says â€œHey! I promise you that there is a goat behind at least 8 of the other 9 doors!â€</strong></li>
  <li>You get two options:
    <ul>
      <li>Option A: You get whatever is behind the door you picked.</li>
      <li>Option B: You get whatever is behind all of the other 9 doors.</li>
    </ul>
  </li>
</ol>

<p><img src="https://dyno-might.github.io/img/monty-hall/game2.png">
</p>

<p>Montyâ€™s statement changes nothing. You donâ€™t need to rely on his <a href="https://en.wikipedia.org/wiki/Monty_Hall#/media/File:Monty_hall_abc_tv.JPG">trustworthy looks</a>. You already <em>knew</em> there were at least 8 goats! Option B still gets you the car 90% of the time.</p>



<p>Letâ€™s update the game again (new part in bold).</p>

<ol>
  <li>There are 10 doors. A car is randomly placed behind one, and goats behind the other 9.</li>
  <li>You pick one door.</li>
  <li><strong>Monty looks behind the other 9 doors. He chooses 8 with goats behind them, and opens them.</strong></li>
  <li>You get two options:
    <ul>
      <li>Option A: You get whatever is behind the door you picked.</li>
      <li>Option B: You get whatever is behind all of the other 9 doors.</li>
    </ul>
  </li>
</ol>

<p><img src="https://dyno-might.github.io/img/monty-hall/game3.png">
</p>

<p>The key insight is this: When Monty shows you that 8 of the 9 other doors contain goats, you havenâ€™t learned anything relevant to your decision. You <em>already knew there were at least 8 goats behind the other doors</em>! So this is just like game 2. Option B still gets you the car 90% of the time.</p>

<p>Want more intuition? Suppose you picked door 3. Imagne Monty walking past the doors, opening doors 1, 2, 4, 5, 6, <strong>skipping 7</strong>, then opening 8, 9, and 10. Doesnâ€™t door 7 seem special?</p>



<p>Letâ€™s make another change. Finally, we arrive at a game very similar to Monty Hall.</p>

<ol>
  <li>There are 10 doors. A car is randomly placed behind one, and goats behind the other 9.</li>
  <li>You pick one door.</li>
  <li>Monty looks behind the other 9 doors. He chooses 8 of them with goats behind them, and opens them.</li>
  <li>You get two options:
    <ul>
      <li>Option A: You get whatever is behind the door you picked.</li>
      <li>Option B: You get whatever is behind <strong>the other closed door</strong>.</li>
    </ul>
  </li>
</ol>

<p><img src="https://dyno-might.github.io/img/monty-hall/game4.png">
</p>

<p>The only difference with Game 3 is that option B doesnâ€™t get you the 8 visible goats. Since you donâ€™t care about goats, this makes no difference. This is still just like the game 3. You get the car 90% of the time by switching.</p>



<p>Here is the last game. We just change the number of doors from 10 to 3.</p>

<ol>
  <li>There are <strong>3</strong> doors. A car is randomly placed behind one, and goats behind the other <strong>2</strong>.</li>
  <li>You pick one door.</li>
  <li>Monty looks behind the other <strong>2</strong> doors. He chooses one <strong>1</strong> of them with a goat behind it, and opens it.</li>
  <li>You get two options:
    <ul>
      <li>Option A: You get whatever is behind the door you picked.</li>
      <li>Option B: You get whatever is behind the other closed door.</li>
    </ul>
  </li>
</ol>

<p><img src="https://dyno-might.github.io/img/monty-hall/game5.png">
</p>

<p>Of course, you still want to choose option B. The chance of success is now 2/3 instead of 9/10. This game is exactly Monty Hall, so weâ€™re done.</p>



<ul>
  <li>
    <p>Itâ€™s important that Monty looked behind the doors before choosing which to open. This is where peopleâ€™s intuition usually fails. If he had chosen a door at random â€” <em>in a way that he risked possibly exposing a car</em>, then the situation would be different. (In that case, thereâ€™s no advantage or harm in switching.) But he doesnâ€™t choose the door at random. He deliberately chooses to show you goats. Since this is always possible, it tells you nothing. I think this is the crux of what makes this problem unintuitive. Many people intuitively think it doenâ€™t matter if you switch. And that <em>would be correct</em> if the door had been opened at random!</p>
  </li>
  <li>
    <p>It might be helpful to draw a diagram of the relationship of the different games, starting with classic Monty Hall and ending with the extreme version.</p>
  </li>
</ul>

<blockquote>
  <p>Game 5 (Classic Monty Hall)<br>
â€ƒâ†“<br>
â€ƒâ†“ (Use 10 doors instead of 3)<br>
â€ƒâ†“ <br>
Game 4<br>
â€ƒâ†“<br>
â€ƒâ†“ (If you switch, get the contents of <em>all</em> other doors, not just the other closed door.)<br>
â€ƒâ†“<br>
Game 3<br>
â€ƒâ†“<br>
â€ƒâ†“ (Monty promises 8 goats behind the other doors instead of showing you.)<br>
â€ƒâ†“<br>
Game 2<br>
â€ƒâ†“<br>
â€ƒâ†“ (Monty doesnâ€™t bother promsising.)<br>
â€ƒâ†“<br>
Game 1 (Dyno MightÂ© Monty Hall)</p>
</blockquote>

<ul>
  <li>
    <p>There are <a href="https://marginalrevolution.com/marginalrevolution/2019/09/the-intuitive-monty-hall-problem.html">some</a> <a href="https://twitter.com/jben0/status/1174180200072011776">other</a> <a href="https://statmodeling.stat.columbia.edu/2019/09/19/alternative-more-intuitive-formulation-of-monte-hall-problem/">attempts</a> at <a href="https://math.stackexchange.com/questions/96826/the-monty-hall-problem/3360686#3360686">variants</a> of the Monty Hall problem, also intended to be more intuitive. These involve switching the doors for â€œboxersâ€.</p>
  </li>
  <li>
    <p>Monty Hall was actually named â€œMonteâ€ at birth! Given that <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo simulations</a> are often used for exploring the Monty Hall problem, thatâ€™s either a tragedy for puns or a miracle for confused students.</p>
  </li>
</ul>

        </div>

        

        
        
    </div>
</div>


    </div>
</section></div>]]>
            </description>
            <link>https://dyno-might.github.io/2020/09/17/making-the-monty-hall-problem-weirder-but-obvious/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24707305</guid>
            <pubDate>Wed, 07 Oct 2020 11:57:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Design an Algorithm (2018)]]>
            </title>
            <description>
<![CDATA[
Score 60 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24706841">thread link</a>) | @rohithkp
<br/>
October 7, 2020 | https://www.adamconrad.dev/blog/how-to-design-an-algorithm/ | <a href="https://web.archive.org/web/*/https://www.adamconrad.dev/blog/how-to-design-an-algorithm/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">

    <article>

        

        <section>

            <p>If you missed my <a href="https://www.adamconrad.dev/blog/why-hiring-is-broken-and-how-im-dealing-with-it">previous article</a>, Iâ€™m going to spend a series of articles providing notes as I audit <a href="http://www3.cs.stonybrook.edu/~skiena/373/">Steven Skienaâ€™s CSE 373 Analysis of Algorithms class</a>.</p>

<p>In the first lecture, Skiena mentions you should take a data structures course and a linear algebra course before studying this material.</p>

<p>For professional (read: practical) purposes, weâ€™re obviously not starting from scratch, but peaking at the syllabus I do believe we can incorporate data structures by implementing them in JavaScript (this is, after all, a front-end blog) along the way.</p>

<p>And as much as foundational linear algebra will help, there simply wonâ€™t be anything in a technical interview that would warrant deep study anyway, so we can safely skip this prerequisite.</p>

<h2 id="what-is-an-algorithm">What is an algorithm?</h2>

<p>An algorithm is <strong>an instruction set.</strong> Kind of like a recipe for a food dish. And just like that recipe for cauliflower rice you found on some random blog, you can translate that recipe into any language you want.</p>

<p>Algorithms are the same way: they are language-agnostic and can be expressed in a human-readable form or machine-readable. The simpler the idea, the easier itâ€™s going to be to express in English. The more complex or nuanced the algorithm is, the more likely youâ€™ll want to lean on a machine language like Python or JavaScript.</p>

<p>For practical purposes, think of the high-level overview of your algorithm as something youâ€™ll explain to an interviewer in English before you dive into the code, but understand that in order to prove your chops as a programmer the code will have to be the primary source for explaining and validating the algorithms you design.</p>

<p>The two defining characteristics of an algorithm that separate an algorithm from other instructions are:</p>

<ol>
  <li><strong>Itâ€™s correct.</strong> Has anyone ever received credit for implementing an algorithm in a tech interview that didnâ€™t produce the correct result every single time?</li>
  <li><strong>Itâ€™s efficient.</strong> Weâ€™d like our algorithms to run sometime before we get old.</li>
</ol>

<p>Now, <em>technically</em>, programs donâ€™t have to run correctly to be acceptable. Programs that run instructions that are <em>mostly</em> correct are known as <em>heuristics</em>. These will become important when studying approximation algorithms later, but for now, assume that correctness is a requirement.</p>

<h3 id="how-do-you-prove-an-algorithm-is-correct">How do you prove an algorithm is correct?</h3>

<p>Proofs were not my strong suit in high school or college. For some reason, it never really clicked for me because the steps in a proof never seemed to line up with the logic my brain used to jump from one step to the next.</p>

<p>Luckily, the easiest way to prove correctness is to prove something <strong>isnâ€™t correct.</strong></p>

<p>Wait, what? How does proving the opposite help us here?</p>

<p>Well, when weâ€™re trying to figure out a correct and efficient algorithm to solve a problem, we can narrow the scope of possible choices by eliminating the ones that are demonstratively incorrect. Proving by counterexample can be far easier than other methods.</p>

<p>As a trivial example, suppose we have a total <em>T = 6</em> we want to strive for by adding up numbers in a valid set like <em>S = [1,2,3]</em>. It might seem like a very simple algorithm that will solve this problem is to pick numbers from left to right until we reach the total <em>T</em>. Even if you add in a big number at the beginning like <em>S = [5,1,2,3]</em> this works since we can just scrap the last two numbers.</p>

<p>Whatâ€™s a counterexample that wouldnâ€™t work?</p>

<p>How about <em>S = [5,2,4]</em>.</p>

<p>First, we pick 5, which is less than 6. There are no other numbers in our set that could add up to equal 6 after already picking 5, but there <em>is</em> a valid configuration that would still satisfy <em>T</em> (2 and 4). Thatâ€™s proof by counterexample that our algorithm was not correct. Itâ€™s also a pretty simple counterexample. <strong>Counter-examples can be useful in the real-world to quickly help you assess if the path youâ€™re going down is a good one or not.</strong></p>

<p>If you can come up with a relatively simple counterexample (simple meaning it should only require a handful of variables or items) you know that your algorithm is dead on arrival and youâ€™ll need to try something else. If you canâ€™t, youâ€™re probably on the right track, but that doesnâ€™t mean your algorithm is definitively correct.</p>

<h3 id="what-techniques-are-used-to-prove-correctness">What techniques are used to prove correctness?</h3>

<p>One way to prove correctness is induction. <strong>Proof by induction indicates that if we can solve for a base case <em>and</em> the general case for <code>n+1</code>, we know weâ€™ve provided a correct answer for all possible inputs.</strong></p>

<p>There are two important connections to make here about induction which are useful in a professional setting:</p>

<ol>
  <li><strong>Proof by induction is a mathematical form of recursion.</strong> Recursion is a fundamental concept in programming which allows a function to call itself. It allows us to split up large problems into smaller ones.</li>
</ol>

<p>The classic example here is the Fibonacci sequence (a sequence of integers where the current number is the sum of the previous two numbers). To calculate Fibonacci for a value <em>n</em> in the sequence, you <em>could</em> count up all of the previous numbers manually for each input of <em>n</em>, but that would not only be slow and laborious, it would also be difficult to express as a program.</p>

<p>Another way would be to count a few base cases (n = 0 and n = 1) to get the counting started, and then continuously call a <code>Fibonacci</code> function with the summed values from the previous step. Recursion is what allows us to accomplish this in code. It is the programming strategy for tackling induction, which is the mathematical strategy for proving statements for algorithms which operate on our sets of data.</p>

<ol>
  <li><strong>Proof by induction is useful for summation.</strong> If youâ€™re adding up a lot of inputs together, and you need to prove it will work for all cases, even ones larger than the set you have defined, it can be proven with induction.</li>
</ol>

<p>But are you ever going to need to formally prove something at work or in an interview? Absolutely not. <strong>But you will need to test your code, and tests are a form of proof.</strong></p>

<p>So while a formal mathematical proof of induction is likely way more rigorous than you will ever need to showcase in a professional setting, it does set the tone that you canâ€™t simply write code and have people assume what you wrote is correct. It needs to be tested somehow, so if you have the mindset that your algorithm needs to be proven correct in some form, youâ€™re on the right track to writing quality code.</p>

<h3 id="and-how-do-you-prove-something-is-efficient">And how do you prove something is efficient?</h3>

<p>If weâ€™ll primarily be using code to express our algorithms, and tests to prove their correctness, Big O notation will be used to prove our algorithms are efficient.</p>

<p>Weâ€™ll cover Big O in a later post in this series, but the important thing to remember now is that in general, youâ€™re going to want to strive for things that take a reasonable amount of time on large data.</p>

<p>For example, if something you design takes an <code>n!</code> factorial amount of time, anything over a measly 30 items and youâ€™re dealing with numbers larger than the number of stars in the known universe. You <em>probably</em> want something that runs a bit faster than that.</p>

<h3 id="the-big-picture-on-the-properties-of-algorithms">The big picture on the properties of algorithms</h3>

<p>Most CS courses (and most schools) only ever care about these two things. If your teachers and TAs can successfully run your program within a reasonable amount of time, you get an A. Real life doesnâ€™t give you an A for these two things because <em>you donâ€™t work in a vacuum</em> as you do on a problem set or exam. So what things are missing from the real world picture?</p>

<ul>
  <li><strong>Orthogonality:</strong> Is your code dependent on other stuff? Are you writing stateful or functional code? Since most coding whiteboard problems are isolated and self-contained, you usually canâ€™t test for this, so make sure you present a portfolio of real-world projects and open source code to demonstrate this</li>
  <li><strong>Readability:</strong> You can write the hackiest crap to get an algorithm to work, but in the real world other people canâ€™t read or use that code, and thatâ€™s a fail. Make sure if you have time and your code is correct and efficient, to <em>refactor</em> to demonstrate you can write readable, reusable code that is DRY (donâ€™t repeat yourself) and orthogonal (or at least that it can be written as part of an orthogonal system)</li>
</ul>

<h3 id="the-first-step-in-designing-an-algorithm">The first step in designing an algorithm</h3>

<p>So now that we know what defines an algorithm and what is required to prove itâ€™s worth using to solve our problems, the next step is to decide how we will design our algorithms. Modeling a problem means knowing the objects youâ€™re dealing with, and there are two classes of objects we will cover:</p>

<h4 id="combinatorial-objects">Combinatorial objects</h4>

<p>A <em>combinatorial object</em> is just a fancy way of saying â€œwhat kinds of things can I use to count with?â€ Since machines are just big 0 and 1 factories, combinatorial objects are the way for us to collect and organize all of the 0 and 1 math our machines are performing thousands upon millions of instructions per second. What kinds of things are we talking about?</p>

<ul>
  <li><strong>Permutations:</strong> reorderings of a set. Colloquial terms for this include words like <em>arrangement</em>, <em>tour</em>, <em>ordering</em>, and/or <em>sequence</em>.</li>
  <li><strong>String:</strong> sequence of characters or patterns. Think of strings like permutations but with letters instead of numbers. Words like <em>text</em>, <em>character</em>, <em>pattern</em>, <em>label</em>, <em>sentence</em> are key insights that youâ€™re dealing with string data.</li>
  <li><strong>Subsets:</strong> portions of a set. If you see words like <em>cluster</em>, <em>collection</em>, <em>committee</em>, <em>group</em>, <em>packaging</em>, or <em>selection</em>, youâ€™ve probably got a subset.</li>
  <li><strong>Points:</strong> locations in space. Words like <em>node</em>, <em>site</em>, <em>position</em>, <em>record</em>, or <em>location</em> are all references to <em>points</em>.</li>
  <li><strong>Graphs:</strong> nodes with vertices to connect them and give them direction. We mentioned this much earlier in this article. Words like <em>network</em>, <em>circuit</em>, <em>web</em>, and <em>relationship</em> all describe graphs.</li>
  <li><strong>Trees:</strong> graphs that flow in one direction and donâ€™t end up where they started (acyclic). When theyâ€™re perfectly balanced, they literally look like a Christmas tree. Words like <em>hierarchy</em>, <em>dominance relationship</em>, <em>ancestor/descendent relationship</em>, <em>taxonomy</em> are all indicators youâ€™re dealing with trees in your problem.</li>
  <li><strong>Polygon:</strong> â€¦</li></ul></section></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.adamconrad.dev/blog/how-to-design-an-algorithm/">https://www.adamconrad.dev/blog/how-to-design-an-algorithm/</a></em></p>]]>
            </description>
            <link>https://www.adamconrad.dev/blog/how-to-design-an-algorithm/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24706841</guid>
            <pubDate>Wed, 07 Oct 2020 10:23:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Developing with Squeak on a Cellphone]]>
            </title>
            <description>
<![CDATA[
Score 39 | Comments 27 (<a href="https://news.ycombinator.com/item?id=24706567">thread link</a>) | @tonyg
<br/>
October 7, 2020 | https://eighty-twenty.org/2020/10/07/developing-with-squeak-on-a-cellphone | <a href="https://web.archive.org/web/*/https://eighty-twenty.org/2020/10/07/developing-with-squeak-on-a-cellphone">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p><strong>Part of a series:</strong> <a href="https://eighty-twenty.org/tag/squeak-phone">#squeak-phone</a></p>

<hr>

<p>One <em>lovely</em> thing about working in Smalltalk is the effortlessness of
development.</p>

<p>I started off developing the code on my desktop machine, and
occasionally testing it on the phone itself. I just <code>rsync</code> the image
and changes files back and forth. This lets me pick up exactly where I
left off on the other device each time I move over.</p>

<p>However, <em>developing</em> on the phone was challenging because of the lack
of a keyboard (though Iâ€™ll post soon about an on-screen keyboard Iâ€™ve
written). So I installed RFB (from
<a href="http://source.squeak.org/ss/">here</a>) into my image on the desktop,
and tested it. Then I saved the image and <code>rsync</code>ed it to the phone as
usual, and presto, I can develop and test interactively on the phone
itself:</p>

<p><a href="https://eighty-twenty.org/images/bootstrapping-a-cellphone-20201007/vnc-to-phone.jpg"><img src="https://eighty-twenty.org/images/bootstrapping-a-cellphone-20201007/vnc-to-phone-640.jpg" alt="Using VNC to develop on the phone itself"></a>
Using VNC to develop on the phone itself</p>

<p>There were a couple of things I had to do to get this to work:</p>

<ul>
  <li>
    <p>Use this version of RFBServer: <a href="http://source.squeak.org/ss/">http://source.squeak.org/ss/</a></p>
  </li>
  <li>
    <p>Change <code>AllowTcpForwarding no</code> to <code>yes</code> in <code>/etc/ssh/sshd_config</code>
on the phone and then <code>service sshd restart</code></p>
  </li>
  <li>
    <p>Use <code>ssh -L 5900:localhost:5900 pm</code> to log into the phone (thatâ€™s
the green-screen transcript in the background in the picture above)</p>
  </li>
</ul>

  </div></div>]]>
            </description>
            <link>https://eighty-twenty.org/2020/10/07/developing-with-squeak-on-a-cellphone</link>
            <guid isPermaLink="false">hacker-news-small-sites-24706567</guid>
            <pubDate>Wed, 07 Oct 2020 09:31:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[BitSyntax for Smalltalk]]>
            </title>
            <description>
<![CDATA[
Score 31 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24706342">thread link</a>) | @tonyg
<br/>
October 7, 2020 | https://eighty-twenty.org/2020/10/07/bit-syntax-for-smalltalk | <a href="https://web.archive.org/web/*/https://eighty-twenty.org/2020/10/07/bit-syntax-for-smalltalk">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p><strong>Part of a series:</strong> <a href="https://eighty-twenty.org/tag/squeak-phone">#squeak-phone</a></p>

<hr>

<h3 id="hand-written-binary-parsingunparsing-sucks">Hand-written binary parsing/unparsing sucks</h3>

<p>As Iâ€™ve been working on a <a href="https://eighty-twenty.org/tag/squeak-phone/">mobile Smalltalk system</a>, Iâ€™ve found myself needing to decode
and encode a number of complex telephony packet
formats<sup id="fnref:telephony-complex"><a href="#fn:telephony-complex">1</a></sup> such as the following, an incoming SMS
delivery message containing an SMS-DELIVER TPDU in
<a href="https://en.wikipedia.org/wiki/GSM_03.40">GSM 03.40</a> format,
containing seven-bit (!)
<a href="https://en.wikipedia.org/wiki/GSM_03.38">GSM 03.38</a>-encoded text:</p>

<pre><code>02 01 ffff
01 28 07911356131313f3
04 0b911316325476f8 000002909021044480 0ec67219644e83cc6f90b9de0e01
</code></pre>

<p>It turns out there are a <em>plethora</em> of such binary formats needed to
get a working cellphone.</p>

<p>I started off hand-rolling them, but it quickly became too much, so
I <s>borrowed liberally</s> <strong>stole</strong> from Erlang, and implemented
<a href="https://squeaksource.com/BitSyntax.html">BitSyntax for Smalltalk</a>. (After all, I am
already using
<a href="https://tonyg.github.io/squeak-actors/">Erlang-influenced actors</a> for
the Smalltalk system daemons!)</p>

<p>Iâ€™ve
<a href="https://docs.racket-lang.org/bitsyntax/">done this before, for Racket</a>,
and there are plenty of other similar projects for e.g.
<a href="https://github.com/squaremo/bitsyntax-js#readme">JavaScript</a> and
<a href="https://bitstring.software/documentation/">OCaml</a>.</p>

<p>Every language needs a BitSyntax, it seems!</p>

<h3 id="what-does-bitsyntax-do">What does BitSyntax do?</h3>

<p>The BitSyntax package includes a <code>BitSyntaxCompiler</code> class which
interprets <code>BitSyntaxSpecification</code> objects, producing reasonably
efficient Smalltalk for decoding and encoding binary structures,
mapping from bytes to instance variables and back again.</p>

<p>The interface to the compiled code is simple. After compiling a
<code>BitSyntaxSpecification</code> for the data format above, we can analyze the
example message straightforwardly:</p>

<figure><pre><code data-lang="smalltalk"><span></span><span>parsedMessage</span> <span>:=</span> <span>SmsIncoming</span> <span>loadFrom:</span> (<span>ByteArray</span> <span>fromHex:</span>
    <span>'02 01 ffff</span>
<span>     01 28 07911356131313f3</span>
<span>     04 0b911316325476f8 000002909021044480 0ec67219644e83cc6f90b9de0e01'</span>)</code></pre></figure>

<p>and, if we wish, serialize it again:</p>

<figure><pre><code data-lang="smalltalk"><span></span><span>serializedBytes</span> <span>:=</span> <span>ByteArray</span> <span>streamContents:</span> [<span>:</span><span>w</span> <span>|</span> <span>parsedMessage</span> <span>saveTo:</span> <span>w</span>]</code></pre></figure>

<h3 id="how-does-it-work">How does it work?</h3>

<p>Syntax specifications are built using an embedded domain-specific
language (EDSL).</p>

<p>For example, for the above data format, we would supply the following
spec for class <code>SmsIncoming</code>:</p>

<figure><pre><code data-lang="smalltalk"><span></span>        (<span>1</span> <span>byte</span> <span>&gt;&gt;</span> <span>#msgType</span>)<span>,</span>
        (<span>1</span> <span>byte</span> <span>&gt;&gt;</span> <span>#type</span>)<span>,</span>
        (<span>2</span> <span>bytesLE</span> <span>&gt;&gt;</span> <span>#simIndex</span>)<span>,</span>
        (<span>1</span> <span>byte</span> <span>&gt;&gt;</span> <span>#id</span>)<span>,</span>
        ((<span>1</span> <span>byte</span> <span>storeTemp:</span> <span>#payloadLength</span> <span>expr:</span> <span>'payload size'</span>)<span>,</span> <span>'payloadLength'</span> <span>bytes</span>)
            <span>&gt;&gt;&gt;</span> <span>#payload</span> <span>&lt;&lt;&lt;</span>
            (
                (<span>SmsAddress</span> <span>codecCountingOctets</span> <span>&gt;&gt;</span> <span>#smscAddress</span>)<span>,</span>
                (<span>SmsPdu</span> <span>codecIncoming</span> <span>&gt;&gt;</span> <span>#tpdu</span>))</code></pre></figure>

<p>along with appropriate specs for <code>SmsAddress</code> and <code>SmsPdu</code> (omitted
for space reasons here) and the following for the <code>SmsPdu</code> subclass
<code>SmsPduDeliver</code>:</p>

<figure><pre><code data-lang="smalltalk"><span></span>        (<span>1</span> <span>bit</span> <span>boolean</span> <span>&gt;&gt;</span> <span>#replyPath</span>)<span>,</span>
        (<span>1</span> <span>bit</span> <span>boolean</span> <span>&gt;&gt;</span> <span>#userDataHeaderIndicator</span>)<span>,</span>
        (<span>1</span> <span>bit</span> <span>boolean</span> <span>&gt;&gt;</span> <span>#statusReportIndication</span>)<span>,</span>
        (<span>2</span> <span>bits</span>)<span>,</span>
        (<span>1</span> <span>bit</span> <span>boolean</span> <span>&gt;&gt;</span> <span>#moreMessagesToSend</span>)<span>,</span>
        (<span>2</span> <span>bits</span> <span>=</span> <span>0</span>)<span>,</span>

        (<span>SmsAddress</span> <span>codecCountingSemiOctets</span> <span>&gt;&gt;</span> <span>#originatingAddress</span>)<span>,</span>
        (<span>1</span> <span>byte</span> <span>&gt;&gt;</span> <span>#protocolIdentifier</span>)<span>,</span>
        (<span>1</span> <span>byte</span> <span>&gt;&gt;</span> <span>#dataCodingScheme</span>)<span>,</span>
        ((<span>7</span> <span>bytes</span>
                <span>transformLoad:</span> [<span>:</span><span>v</span> <span>|</span> <span>'self class decodeSmscTimestamp: '</span><span>,</span> <span>v</span>]
                <span>save:</span> [<span>:</span><span>v</span> <span>|</span> <span>'self class encodeSmscTimestamp: '</span><span>,</span> <span>v</span>])
            <span>&gt;&gt;</span> <span>#serviceCentreTimeStamp</span>)<span>,</span>
        (((<span>1</span> <span>byte</span> <span>&gt;&gt;</span> <span>#itemCount</span>)
            <span>transformLoad:</span> [<span>:</span><span>v</span> <span>|</span> <span>'self userDataOctetsFor: '</span><span>,</span> <span>v</span>]
            <span>save:</span> [<span>:</span><span>v</span> <span>|</span> <span>'itemCount'</span>])
                <span>storeTemp:</span> <span>#userDataLength</span> <span>expr:</span> <span>'userData size'</span>)<span>,</span>
        (<span>#userDataLength</span> <span>bytes</span> <span>&gt;&gt;</span> <span>#userData</span>)</code></pre></figure>

<p>These are non-trivial examples; the simple cases are simple, and the
complex cases are usually possible to express without having to write
code by hand. The EDSL is extensible, so more combinators and parser
types can be easily added as the need arises.</p>

<h3 id="how-do-i-get-it">How do I get it?</h3>

<p>Load it into an up-to-date trunk Squeak image:</p>

<figure><pre><code data-lang="smalltalk"><span></span>(<span>Installer</span> <span>squeaksource</span> <span>project:</span> <span>'BitSyntax'</span>)
    <span>install:</span> <span>'BitSyntax-Core'</span><span>;</span>      <span>"the compiler and EDSL"</span>
    <span>install:</span> <span>'BitSyntax-Examples'</span><span>;</span>  <span>"non-trivial examples"</span>
    <span>install:</span> <span>'BitSyntax-Help'</span><span>.</span>      <span>"user guide and reference"</span></code></pre></figure>

<p>You can also visit the <a href="https://squeaksource.com/BitSyntax.html">project page</a> directly.</p>

<p>The package <code>BitSyntax-Help</code> contains an extensive manual written for
Squeakâ€™s built-in documentation system.</p>

<p>Enjoy!</p>


  </div></div>]]>
            </description>
            <link>https://eighty-twenty.org/2020/10/07/bit-syntax-for-smalltalk</link>
            <guid isPermaLink="false">hacker-news-small-sites-24706342</guid>
            <pubDate>Wed, 07 Oct 2020 08:33:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Crouching T2, Hidden Danger]]>
            </title>
            <description>
<![CDATA[
Score 217 | Comments 109 (<a href="https://news.ycombinator.com/item?id=24705645">thread link</a>) | @xrayarx
<br/>
October 6, 2020 | https://ironpeak.be/blog/crouching-t2-hidden-danger/ | <a href="https://web.archive.org/web/*/https://ironpeak.be/blog/crouching-t2-hidden-danger/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="info"><div><p><h4><br><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" width="25" height="20"><path fill="currentcolor" d="M448 0H64C28.7.0.0 28.7.0 64v288c0 35.3 28.7 64 64 64h96v84c0 7.1 5.8 12 12 12 2.4.0 4.9-.7 7.1-2.4L304 416h144c35.3.0 64-28.7 64-64V64c0-35.3-28.7-64-64-64zm16 352c0 8.8-7.2 16-16 16H288l-12.8 9.6L208 428v-60H64c-8.8.0-16-7.2-16-16V64c0-8.8 7.2-16 16-16h384c8.8.0 16 7.2 16 16v288z"></path></svg></span>Crouching T2, Hidden Danger
<span>Mon Oct 5, 2020</span></h4></p></div></div><div id="features"><div><p><h4>Crouching T2, Hidden Danger
<span>Mon Oct 5, 2020</span></h4><hr><br></p><p><strong>Letâ€™s talk about that thing nobodyâ€™s talking about.
Letâ€™s talk about a vulnerability thatâ€™s exposing 2018-2020 Macs while most are declining to act nor report about the matter.
Oh, and did I mention itâ€™s unpatchable?</strong></p><p><strong>Buckle up buckaroo, weâ€™re in for a wild ride.</strong></p><p>Skip to <a href="#security-issues">#security-issues</a> for the technical mumbo-jumbo.</p><h2 id="preface">Preface</h2><h3 id="attribution">Attribution</h3><p>The following post is an industry analysis of the code and research performed by <a href="https://twitter.com/axi0mx/">twitter.com/axi0mx</a>, <a href="https://twitter.com/h0m3us3r/">twitter.com/h0m3us3r</a>, <a href="https://twitter.com/aunali1/">twitter.com/aunali1</a>, <a href="https://twitter.com/mcmrarm/">twitter.com/mcmrarm</a> and <a href="https://twitter.com/su_rickmark/">twitter.com/su_rickmark</a> who poured endless hours of work into this, allowing companies and users to understand their risks concerning this issue.</p><h3 id="intel-vs-silicon">Intel vs Silicon</h3><p>This blog post only applies to macOS systems with an Intel processor and the embedded T2 security chip.
Apple silicon systems will run completely on a set of Apple-designed ARM processors and mighth have a different topology, e.g. based on the A12.
Since the A12 chip seems to have fixed this issue (to be confirmed), itâ€™s highly likely the new Apple Silicon machines will not be vulnerable.
And while the new upcoming Intel Macs at the end of year will probably receive a new hardware revision of the T2 chip (e.g. based on the A12), we are still stuck with this vulnerability on Macs between 2018 and 2020.</p><h3 id="so-about-this-t2-thing">So about this T2 thing</h3><p>In case you are using a recent macOS device, you are probably using <a href="https://support.apple.com/en-us/HT208862">the embedded T2 security chip</a> which runs <em>bridgeOS</em> and is actually based on watchOS. This is a custom ARM processor designed by Apple based on the A10 CPU found in the iPhone 7.
The T2 chip contains a <em>Secure Enclave Processor</em> (SEP), much like the A-series processor in your iPhone will contain a SEP.</p><p>While newer Macs and/or Apple Silicon (including the dev kit) will use a more recent A-series processor such as the one found in the recent iPhone (A12), current Macs still use the A10.</p><p>It performs a predefined set of tasks for macOS such as audio processing, handling I/O, functioning as a <a href="https://en.wikipedia.org/wiki/Hardware_security_module">Hardware Security Module</a> for e.g. Apple KeyChain or 2FA, hardware accelerating media playback, whitelisting kernel extensions, cryptographic operations and <strong>ensuring the operating system you are booting is not tampered with</strong>.
The T2 chip runs its own firmware called <em>bridgeOS</em>, which can be updated when you install a new macOS version. (ever notice the screen flickering? thatâ€™s the display driver being interrupted and possibly updated.)</p><p><em>Edit</em>: I first mentioned the iPad Pro to be impacted by the T2 vulnerability, but while it could suffer from the same vulnerability, it does not contain a T2 chip.</p><h3 id="the-macos-boot-sequence">The macOS boot sequence</h3><p>So letâ€™s focus on the boot image verification on macOS. What exactly happens when you press that power button?
<a href="https://eclecticlightdotcom.files.wordpress.com/2018/08/bootprocess.png">Thereâ€™s also a visual representation for any <em>conaisseurs</em></a>.
For the enthusiasts, I personally find <a href="https://michaellynn.github.io/2018/07/27/booting-secure/">Booting Secure by mikeymikey</a> a more in-depth description.</p><ol><li><p>The T2 chip is fully booted and stays on, even if your Mac device is shutdown.</p></li><li><p>The press of the power button or the opening of the lid triggers the System Management Controller (SMC) to boot.</p></li><li><p>The SMC performs a Power-On-Self-Test (POST) to detect any EFI or hardware issues such as bad RAM and possibly redirect to Recovery.</p></li><li><p>After those basic sanity checks, the T2 chip is triggered and I/O connectors are setup. (USB, NVMe, PCIe, â€¦) It will use NVMe and PCIe to talk to NAND storage.</p></li><li><p>The applicable boot disk is selected and a disk encryption password is asked if enabled to mount <a href="https://en.wikipedia.org/wiki/Apple_File_System">APFS</a> volumes possibly via FileVault2 disk encryption.</p></li><li><p><code>/System/Library/CoreServices/boot.efi</code> is located on your System APFS volume and <a href="https://support.apple.com/en-us/HT208330">depending on your secure boot settings</a> is validated.</p></li><li><p><em>boot.efi</em> is ran which loads the Darwin kernel <em>(throwback to BSD)</em> (or Boot Camp if booting Microsoft Windows) &amp; IODevice drivers. If a kernel cache is found in <code>/System/Library/PrelinkedKernels/prelinkedkernel</code>, it will use that.</p></li><li><p>Any User Approved Kernel Extensions are initialized &amp; added to the kernel space -if- they are approved by the T2 chip.
<em>This will go away with System Extensions</em>.</p></li></ol><h3 id="macos-security-features">macOS security features</h3><p>So Apple has a couple of tricks up its sleeve to limit the attack surface of any potential security vulnerabilities. A small summary of related measures since macOS Big Sur on Intel processors:</p><ul><li><p><em>System Integrity Protection</em> (SIP): a read-only <code>/System</code> partition so the base install of macOS (including the kernel) cannot be tampered with.</p></li><li><p><em>System Extensions</em>: a move to away from Kernel Extensions, getting external code out of the Kernel framework-wise.</p></li><li><p><em>Secure Boot</em>: verifies the signature validity of the operating system on disk.</p></li><li><p><em>Filesystem seals</em>: every byte of data is compared to a hash in the filesystem metadata tree, recursively verifying integrity.</p></li></ul><h3 id="apple-marketing">Apple marketing</h3><p>As you probably all already know, Apple pushes forward privacy &amp; security as important weapons in todays world of technology.
They tout their devices as highly secure and vouch to handle your personal data using a privacy-centric approach.
While there have been mistakes made in the past (who can blame them?), Apple has been generally quick to fix any security issues that were disclosed to <a href="https://support.apple.com/en-gb/HT201220">their responsible disclosure program</a> or in public.</p><h2 id="security-issues">Security issues</h2><h3 id="jailbreaking">Jailbreaking</h3><h3 id="the-core-problem">The core problem</h3><p>The mini operating system on the T2 (<em>SepOS</em>) suffers from a security vulnerable also found in the iPhone 7 since it contains a processor based on the iOS A10. Exploitation of this type of processor for the sake of installing homebrew software is very actively discussed in the <a href="https://reddit.com/r/jailbreak/">/r/jailbreak</a> subreddit.</p><p>So using the <a href="https://checkm8.info/">checkm8 exploit</a> originally made for iPhones, the checkra1n exploit was developed to build a semi-tethered exploit for the T2 security chip, exploiting a flaw. This could be used to e.g. circumvent activation lock, allowing stolen iPhones or macOS devices to be reset and sold on the black market.</p><p>Normally the T2 chip will exit with a fatal error if it is in DFU mode and it detects a decryption call, but thanks to the <a href="https://github.com/windknown/presentations/blob/master/Attack_Secure_Boot_of_SEP.pdf">blackbird vulnerability</a> by team Pangu, we can completely circumvent that check in the SEP and do whatever we please.</p><p>Since sepOS/BootROM is <em>Read-Only Memory</em> for security reasons, interestingly, Apple cannot patch this core vulnerability without a new hardware revision.
This thankfully also means that this is not a persistent vulnerability, so it will require a hardware insert or other attached component such as a malicious USB-C cable.</p><h3 id="debugging">Debugging</h3><p>Every Apple iDevice (which includes the T2 and the Watch, via a port under the band) ships with a firmware recovery USB interface called Device Firmware Update (DFU), which is triggered when the device is not be able to boot or by pressing a particular set of buttons when turned on. It is always available because it is code run from SecureROM. This is the mode in which checkm8 runs.</p><p>Apple also leaves the ability to access various debug functionality which is disabled on production devices unless a special boot payload is used which runs in DFU. Since Apple is the only one who can sign code for DFU, they can demote any device they like, including the most recent A14 processors.
But since the checkm8 vulnerability runs so early in the boot process, we too can demote the T2 into DFU mode.
Without checkm8, we would not be able to run unsigned code in DFU and thus not be able enable debug interfaces. Once the debug interface is enabled Apple uses specialized cables with simian names (see Chimp, Kanzi, Gorilla).</p><h3 id="impact">Impact</h3><p>Once you have access on the T2, you have full <code>root</code> access and kernel execution privileges since the kernel is rewritten before execution.
Good news is that if you are using FileVault2 as disk encryption, they do not have access to your data on disk <em>immediately</em>.
They can however inject a keylogger in the T2 firmware since it manages keyboard access, storing your password for retrieval or transmitting it in the case of a malicious hardware attachment.</p><p>The functionality of locking an Apple device remotely (e.g. via MDM or FindMy) can be bypassed (<em>Activation Lock</em>).</p><p>A firmware password does not mitigate this issue since it requires keyboard access, and thus needs the T2 chip to run first.</p><p>Any kernel extension could be whitelisted since the T2 chip decides which one to load during boot.</p><p>If the attack is able to alter your hardware (or sneak in a malicious USB-C cable), it would be possible to achieve a semi-tethered exploit.</p><p>While this may not sound as frightening, be aware that this is a perfectly possible attack scenario for state actors.
I have sources that say more news is on the way in the upcoming weeks. I quote: <em>be afraid, be very afraid</em>.</p><h2 id="exploitation">Exploitation</h2><pre><code># install devtools
$ xcode-select --install

# check the script &amp; install homebrew
$ /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)"

# install packages
$ brew install libplist automake autoconf pkg-config openssl libtool llvm libusb

# git clone, autogen.sh, make &amp; make install
# https://github.com/sbingner/ldid
# https://github.com/libimobiledevice/libusbmuxd
# https://github.com/libimobiledevice/libimobiledevice
# https://github.com/libimobiledevice/usbmuxd

# Run checkra1n and wait for T2 boot. It will stall when complete.
# TODO describe the checkra1n exploitation 

# Unplug and replug the usb connection. Checkra1n should now send the overlay.
# TODO describe the usb debug mode &amp; overlay

# Bring up a proxy to dropbear
$ iproxy 2222 44 &amp;

# Connect to T2 &amp; enjoy
$ ssh -p 2222 root@127.0.0.1
</code></pre><h2 id="responsible-disclosure">Responsible Disclosure</h2><p>Iâ€™ve reached out to Apple concerning this issue on numerous occasions, even doing the dreaded cc <em>tcook@apple.com</em> to get some exposure.
Since I did not receive a response for weeks, I did the same to numerous news websites that cover Apple, but no response there as well.
In hope of raising more awareness (and an official response from Apple), I am hereby disclosing almost all of the details.
You could argue Iâ€™m not following responsible disclosure, but since this issue has been known since 2019, I think â€¦</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ironpeak.be/blog/crouching-t2-hidden-danger/">https://ironpeak.be/blog/crouching-t2-hidden-danger/</a></em></p>]]>
            </description>
            <link>https://ironpeak.be/blog/crouching-t2-hidden-danger/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24705645</guid>
            <pubDate>Wed, 07 Oct 2020 05:52:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[VSCode on Google Colab]]>
            </title>
            <description>
<![CDATA[
Score 193 | Comments 49 (<a href="https://news.ycombinator.com/item?id=24705599">thread link</a>) | @amitness
<br/>
October 6, 2020 | https://amitness.com/vscode-on-colab/ | <a href="https://web.archive.org/web/*/https://amitness.com/vscode-on-colab/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<header>

<p>
<span>

2 minute read
</span>
</p>
</header>
<section itemprop="text">
<p>I recently discovered a way to set up VSCode on Google Colab and use it as an editor to write code and run experiments on the Colab VM.</p>
<p>With this setup, you can still prototype in the Colab Notebook while also using VSCode for all the advantages of a full-fledged code editor. Here is how you can replicate my setup.</p>
<h2 id="approach-1-python-package">Approach 1: Python Package</h2>
<p>In this setup, we use the <a href="https://github.com/abhishekkrthakur/colabcode">colab-code</a> package that automates all the manual setup steps previously described in the <strong>Approach 2</strong> section of this blog post. You can make a copy of this <a href="https://colab.research.google.com/github/abhishekkrthakur/colabcode/blob/master/colab_starter.ipynb">notebook</a> directly to get started.</p>
<ol>
<li>
<p>First, install the <code>colab-code</code> package using the following command:</p>

</li>
<li>
<p>Now, import <code>ColabCode</code> class from the package and specify the port and password.</p>
<div><div><pre><code> <span>from</span> <span>colabcode</span> <span>import</span> <span>ColabCode</span>
 <span>ColabCode</span><span>(</span><span>port</span><span>=</span><span>10000</span><span>,</span> <span>password</span><span>=</span><span>"password123"</span><span>)</span>
</code></pre></div> </div>
<p>You can also use it directly with the default port and without any password as shown below.</p>
<div><div><pre><code> <span>from</span> <span>colabcode</span> <span>import</span> <span>ColabCode</span>
 <span>ColabCode</span><span>()</span>
</code></pre></div> </div>
</li>
<li>
<p>You will get the ngrok URL in the output. Click the link and a login page will open in a new tab.</p>
<p><img src="https://amitness.com/images/colab-code-step-1.png" alt="Generated NGROK URL"></p>
</li>
<li>
<p>Type the password you had set in step 2 and click submit. If the page gets stuck for more than 4-5 seconds, refresh the page and you should be redirected to the editor.</p>
<p><img src="https://amitness.com/images/colab-code-step-2.png" alt="Authenticating with password in VSCode"></p>
</li>
<li>
<p>Now you will get access to the editor interface and can use it to work on python files.</p>
<p><img src="https://amitness.com/images/colab-code-step-3.png" alt="VSCode Interface"></p>
</li>
</ol>
<h2 id="approach-2-manual-setup">Approach 2: Manual Setup</h2>
<p>I have described the setup steps in detail below. After going through all the steps, please use this <a href="https://colab.research.google.com/drive/1yvUy5Gn9lPjmCQH6RjD_LvUO2NE0Z7RM?usp=sharing">colab notebook</a> to try it out directly.</p>
<ol>
<li>
<p>First, we will install the <a href="https://github.com/cdr/code-server">code-server</a> package to run VSCode editor as a web app. Copy and run the following command on colab to install <code>code-server</code>.</p>
<div><div><pre><code> !curl -fsSL https://code-server.dev/install.sh | sh
</code></pre></div> </div>
</li>
<li>
<p>After the installation is complete, we will expose a random port <code>9000</code> to an external URL we can access using the <code>pyngrok</code> package. To install <code>pyngrok</code>, run</p>
<div><div><pre><code> <span>!</span>pip <span>install</span> <span>-qqq</span> pyngrok
</code></pre></div> </div>
</li>
<li>
<p>Then, run the following command to get a public ngrok URL. This will be the URL we will use to access VSCode.</p>
<div><div><pre><code> <span>from</span> <span>pyngrok</span> <span>import</span> <span>ngrok</span>
 <span>url</span> <span>=</span> <span>ngrok</span><span>.</span><span>connect</span><span>(</span><span>port</span><span>=</span><span>9000</span><span>)</span>
 <span>print</span><span>(</span><span>url</span><span>)</span>
</code></pre></div> </div>
</li>
<li>
<p>Now, we will start the VSCode server in the background at port 9000 without any authentication using the following command.</p>
<div><div><pre><code> !nohup code-server --port 9000 --auth none &amp;
</code></pre></div> </div>
</li>
<li>
<p>Now, you can access the VSCode interface at the URL you got from step 3. The interface and functionality are the same as the desktop version of VSCode.</p>
</li>
</ol>
<p><img src="https://amitness.com/images/colab-vscode.png" alt="Example of a running instance of VSCode server"></p>
<h2 id="usage-tips">Usage Tips</h2>
<ol>
<li>
<p>You can switch to the dark theme by going to the bottom-left corner of the editor, clicking the <strong>settings icon</strong>, and then clicking â€˜<strong>Color Theme</strong>â€™.</p>
<p><img src="https://amitness.com/images/colab-dark-theme-step-1.png" alt="Switching to dark theme on VSCode"></p>
<p>A popup will open. Select <strong>Dark (Visual Studio)</strong> in the options and the editor will switch to a dark theme.
<img src="https://amitness.com/images/colab-dark-theme-step-2.png" alt="Theme selection interface on VSCode"></p>
</li>
<li>
<p>All the keyword shortcuts of regular VSCode works with this. For example, you can use <code>Ctrl + Shift + P</code> to open a popup for various actions.</p>
<p><img src="https://amitness.com/images/vscode-ctrl-shift-p.png" alt="Action popup in VSCode"></p>
</li>
<li>
<p>To open a terminal, you can use the shortcut <code>Ctrl + Shift + `</code>.</p>
<p><img src="https://amitness.com/images/vscode-terminal.png" alt="Opening integrated terminal in VSCode"></p>
</li>
<li>
<p>To get python code completions, you can install the Python(<code>ms-python</code>) extension from the extensions page on the left sidebar.</p>
<p><img src="https://amitness.com/images/vscode-code-completions.png" alt="Installing extensions in VSCode"></p>
</li>
<li>
<p>The Colab interface is still usable as a notebook and regular functions to upload and download files and mount with Google Drive. Thus, you get the benefits of both a notebook and a code editor.</p>
</li>
</ol>
<h2 id="references">References</h2>
<ul>
<li><a href="https://github.com/cdr/code-server/blob/v3.5.0/doc/FAQ.md">Code-Server FAQs</a></li>
<li><a href="https://pyngrok.readthedocs.io/en/latest/">pyngrok - a Python wrapper for ngrok</a></li>
</ul>
</section>



</div></div>]]>
            </description>
            <link>https://amitness.com/vscode-on-colab/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24705599</guid>
            <pubDate>Wed, 07 Oct 2020 05:38:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Dissertation: Marine Insurance in the Netherlands 1600-1870 (2009) [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 24 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24705036">thread link</a>) | @johntfella
<br/>
October 6, 2020 | https://research.vu.nl/ws/portalfiles/portal/42182698/complete+dissertation.pdf | <a href="https://web.archive.org/web/*/https://research.vu.nl/ws/portalfiles/portal/42182698/complete+dissertation.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>Ã†}Ã©Ã”Ã˜(OÃ¾`oÃÅ“Â®Ã¼ÃÂ®â€šÃ¢yÂ¯Ã«Ã…3WU^ 3Â«ÃÃ¿Ã¨Ãƒ_Â¿O|@ï¿½â€“kÂ»
Ã–Ã¡â€¡EÃ’Â¼Ã²Ã£Â¦Â½ÃÃµÂ½Ã¤Â¡=Â£Ã¿`Ã—Å Ã«Ã·PÂ²oÂ»Ã¦RÂ²Ã³JÃâ€šÃÂ®;_Â£Ã¬jÂ±}Ã¸Ã¿Ã«Ã‘ÃÅ“ÃÃ¼Â¯V9/Å¾ÃŠÃ¬tÂ®ï¿½YyÂ±Ãï¿½&gt;Æ’uÅ½Å“ÃÃ›Â³Ã²/68Ã‘Ã§RÂ§S|Â¸Ã˜ Ã•ÂºÃÂ¿Ã³Â¹jm)Ã¡Â§Â³lÃ©Â®Ãª'[?@Â·	ÃœÂ«Ã¼2Ã¶Â©ï¿½Å½ÃŸâ€œÃ€=Ã£=|Ã´"AÂ£Â²Ã§qÂ¬Â¥Ã°DÃ´Ã¤n
wDoÃ–Ã™*Ã«f'Å½gÂ«Â®ÂµÃ¨ÃªK6Ã˜r<iÂ©Â¶Â¯b|[=sÂ£kÂ¯Ã¸Ã¤Â¢Ã‹gÅ½w3 â€Â²h;t="" bÂ¨Â®1m0^jâ€°i$rjËœ="" Ã%="Â©Ã‡" Ã¥]Ã‡wlÃŠÃ·â€¹$â€ â€â€¡Ã†Ã„llâ€™Ã¸ï¿½Ã±+Ã‹Ã¢kÅ¡Ã€Ã§Ã¡Å¡4Ã‡Â¯Âªs,xÃŸÅ“:qâ€˜b<veiÂ©Ë†_oÂ´Â¿!ï¿½Ã°Â»Ã~ï¿½8ÂªgÂ¢eÂ³tÅ qÃ²Â¯Ã”@bpÃ¦p="Å¡Æ’BÂ¢â€Ã=â€š])\Â¬0ï¿½Å¡â€)g8fÃ•M\ï¿½Â©Ã“ÂºzKÂ«Ã»%cÃŸÂ´Ã¹}-Ã—ÃŒMÃµvâ„¢Ã¹Ã‹5Æ’F[Kq#Ã±ÃˆÃ’$Â²Â¢Â§Ã®;ÃvÃŒ({Â´z9Å Ã‹Â£â€¹EÃ‘VÂ¬Ã–Ã¨Â£drm]Â³{Ã›Â¼Å½ÃšÃ¹&quot;Ã¢Ã«Â¾â€¹JÃ±B1Ã“Ã‹w">6-LXÂ±Â·Ã‹ÃŠâ€&gt;Ã‚Ãº.JeÃ¡9Â¨evâ€ºÃŠ2Ã»Ãœâ€™3Ã‰QJÃ¥Â¶â€¦g+Ã–Ã§U]&lt;Ã·Ã¥Æ’Â·=Mg&amp;=6~7Å¡&lt;{dÃ’Ã'Â¾Ã¸Å½Ã¬Ã¹Yg3|Å¸Å“Ã¹l[Å¾Ã½c0Â¥Ã®Ã•Â£[ÂºÃ®Â¿}Ã‰Ã‡Â®)TyÃ€ËœX7Â¢Â¹Âª@`Ã¦:ÃƒÃÃ&amp;â€¦Ãµ
e:zÃ‚$@FÂ²Jt(â€™Å“kJuâ„¢ï¿½Ã¶,
Ã†zÃTÃ¯Å¾ÃŸ/f+LÂ¡â€“E.â€™1	6Â­[hÂ¬	
Ã–&amp;Â¹VÂ²5Ã½EÃ¬Ã²Ã¨Â¹bË†$Â¬Å½Â¹Ã¾5Ã½Â§Cï¿½EÃ²gÂ©Ã©uÃŒâ‚¬Â©CVâ€¡%TÃ­Ã°â€˜_ï¿½VÃ…+ï¿½ÃÃ´Â»â€¢Ã¢â€ºJ{fÂ·Ã¼BÃ¥;KFI\Â¢^Ã¾-Ã“Ã‹Ã–Ã®yqÃ~Å’â€šQ!YÃ‚&nbsp;
Ã‰ÂªÃªaÃ³Ãšâ€°Tâ€˜QtÂ¬Ã›â€¦Oâ€9^5RÆ’ï¿½Ã„zdW"Ã‡ÃšqÂ©-]IÃ—$+FÂªFÂ­Ã­â€¦Ã¨W
Ã¡Ã´gÂ¹Ã¾Fkâ„¢Ã˜Ã¼â€œÂ©e!Â­ÃƒmÃ½â€˜Å¡k@â€™â€š#Ã¡DÃ½zÂ¥ÃÃªXY&lt;&amp;Â©Â¡SÂ¾ÂªÃ0Ã‚â€EXEâ€˜Ã0ÃŠÃ¾9Lr"+"Â¡Â«IÃ£â€˜zQÃ„ï¿½aÃºbË†Ãµ)Â¤â€”Âº&amp;k'%â„¢.ÃºÃÂ¸Ã¸iÂ»Â°ÃªIâ€Ã‚IÃ¤u%Å’Â¹x5Â·Â¦Â¨Ã¯_Ã…ï¿½Ã¹/Â­â€¦Â¼â€°cÃ¦
mÃ·}ÃGÂ·DNÂ±Å¾â€˜Eâ€“Â¬HAÃ®
â€”My?Ã±c1ÃÃ†ÃjV5Ã&amp;ÃuÃ½Ã¦Ã°eâ€™â€¢Ã†Ã¹;Å¾ÃªÃ¦ÃÃ«mÃ…Ë†Â½Ã›"Ã„ÃºÃºÃ¼k.Ã—Â¯ï¿½Â·ihÂ£DÃ–iâ€¦Â·Ãœï¿½Ã¯Ãºâ€˜Ã²Æ’Ãª(Ã¬Å’D~Ã–Â¤;Å¾Â¹Â¾Â§Ã£Ã‰kÃÃ®ï¿½Â¥}"â„¢4WPm.Ã¯Ã¨Â¿Ã…Â¬Ã±Â·Ã­Ã°tCÂ®ÃŒÃ‘Ã¤Â¦Ã¢=â€¹&amp;Âµâ‚¬â€ZYZÃ£(Â´Ã·Å½Ãœ'FÃ¨|ÂªÃ‚[Ã·c6tÃÃª\oÃ•Â»&lt;Ãºï¿½Ã™Å½Ã¶
Ã‹VWÃ§tÃ‰Ã­ÃÃš;Â¿zÃ­fvÃ;}Ã¢Ã‹Â¿â€ºÃ“Ã»v'Å¾Å¾ï¿½Â¾Ã´?["Ã¬wâ€¡Ã©nÃ›hÃ˜ÃÃƒâ‚¬RÃœÃ–F,â€¹Â²â„¢Â´#-/Ã’Â®2Ã²â€“#2&gt;Ã…Å¾ÃœÂ«Â¬Â¸Å¡â€¡Â¥eeÃŠscÃ„Â¶fâ€¢6Ã¹Ã•ÂªÅ KÂ½=7ÃœKdÂ¾\ÃˆÃ¿Â£â€ Â´_ÃšÃ„qÅ½ÃˆQÂ¨%6Ãœï¿½Qu(,â€HTÂ§ÃˆÃ¥Å Ã½ 	ï¿½tâ€“QÂ¦_\Â¹HÃ¢Â¤IÃ¢{Â¤Â­QÅ’xJ6$ï¿½Ã°Ãº\PÃ‚Ã¬ÂºÅ’Â£Â¦\ZÃ©I?Ë†	Ëœ5Ã†PÃŸÃ…3â€šlâ€™|Â¾ï¿½ï¿½?$â€ºÂ£QÃ”Å ï¿½Ã‡Ã¤Ã¤Â¦:Ã‹(1Â¡â‚¬pÃ”â€¦jWF^Ã¬n
`Ã CÃ°â€ºÅ’[â€Ã‡Ã0Ã†+Ã³&nbsp;dPÃ§ï¿½Ã”XoGÃ’&nbsp;dï¿½2MÂ¾7Ã¦ÃL@Ã¯@Ã«â€Â¾Ã«Â±Ã›â€œÂ¼Ã¢Â·Ã€VÂ«â€¹'O.Wâ€˜8Â¥+Â¿.Â¶Ã›=Ã¨ï¿½ÃÃ£Ã7Dâ€¡oÂ²ï¿½8Â¯uÂ¹Å Ã½â€Ã›Ã¥Ã£	"ï¿½ÃŒË†Ã¥Ã¦Â¢Â¡_Â¼â€”Ã»e~HÃ¯Â¢Mâ€ï¿½Ã˜Ã­â€°Ã™Ã™â€Ã•nÅ¸Ã¢Wâ„¢Ã¤Â¼6;;Ã›Ã§Ã³Ã±Ã’Ã·Ã§.!EÃ´(Å !â€“dgÃ§P*jËœÃ¼Â¤#/ï¿½dâ€šÂ¨'â€°â€™â€°zÂ¥ Â«5â€“Â¾ÃSÂ²Ã£qÃ´	[!zâ€¡ÃÃ‹ÃÂ£ÂºÃ„KÃ’Oâ€ÃƒC%ggâ€°8Ã¨â€¹ruPÃ³Ã¼Ã²[Å“Â²&nbsp;ï¿½4vÂ¹^Ã‰7Â¤ÃÃ©Â¨!(Â«Ã¥Â¼OqÃ‘ÃÃº4rË†&lt;Ã¢0Ã—â€â€™â€œÃ¢Â¬Â­Â­uÃ¦)Â¢#GS5â€Æ’UÃ¬WÃ¹UÃ¬â€¡Sâ€Ã¦Ã”Ã–JÂ¯Â­V+Â²y&amp;Ã‚adÃÂ¼8ÃŠQÂ¡â€˜G!.V'B$Ã‰7Â«u	
?ÂµKÃŠ)Âº(\â€ºKÃ¥Å¾Ã–ÂªSRâ€šRNËœÂ¬V[
Ã†zÂ°vâ€œ.Ëœkâ€˜	c!;8Â§UÃƒQGÃ­Ã±(Å â€š!Ã§Â¬VÃRDâ€¡â€šBâ€ Ã‚dÂ¤"â€™Ã¥KIÆ’Â¯!Ã†H5LÃŸ"oÅ Ã‚Â¤Tâ€“\Â¬NJÅ¾aÃ˜%E`$7qÃ¸Ã„Ã”"Â¬QÂ´R:bÂ¤ÃBoÃ‹iÃ²ÃŠCÃ¤ÃŠU:Â¤VÆ’rÅ½oÂ©9Ã Ã–GÂ«Â¦D|].Å¡JÂ¥Âªâ€\ÃÂªÃ„Ã£0K6JÅ¾â€“Â«CJa*â€¦@$Ã·}Â¹Å“SUk'Ã¹Ã©â€¡I&gt;u}H/â‚¬Ã”l&lt;Å 5Â¤aPï¿½CÂ²/LKÃ³;Â¤4hÂ»Ãƒ0Ã†,Ã·Â©R"!I-Ã²ÃŠ@ï¿½[Â½Å¾â€¢Ã¡?Ã‰"Ã’QtbiÃ“ H@Â¦â€:yRÃ¢â€¹â€šQ.zÂ½
rÃ‹â€™â€œÂ¼Â°Kâ€˜Â¬Â§â€šÃ€ï¿½\EJ4Â²TÂ¡Ã7$+Â¤oÃ‰â€™&lt;Ã”Ã†rUÃ´ÃµÃƒ&amp;C\?Ã†yÃ­â€”Â¾mÃâ€
E+;4â€¡rÃ¾Ã‡Â©Ã‹N&amp;SRir	Â±NÆ’Âª[)â€¦Â£â€¦ÃœÂ§â€šÃ»8Â¹Ã Â¤uÅ“â€“oÃ®T.KÂ«6m{zï¿½Â¢Ã˜Æ’;Â½WfÃÂ«Â¨=JfÂºÃ’â€ºH(Ã«Ã…Ã±Â¤
Ã¹Ã &gt;B+.â€¢zEÃPÃŸâ€jâ€š2Ã…Ã½â€¡Â²ÂµPSÆ’Ã€E
â€ÃºFw1Q7vâ€¹$ï¿½Â¦6tâ€Â¼ï¿½Ãˆ&nbsp;Â¾ÃŠÅ¸Ã¾Ã¤Ã¿Ã“Å¸Ã¼Ã¿OÃ¾Ã¤ï¿½Â¿$â€Â¼â€¡Ã Ãiâ€œb4ÂµÃ–Ãµ&nbsp;c[Ã•nÃƒÅ’6ÃÃhÂ²Å Ã¹Ã ?Ã‚Ãâ€¡mÃ¼yÃgÅ¡]/Ã­Ã“niÃƒÂ¥â‚¬Ã‰Ã&nbsp;Ã¿-ÃœgÂªÃ³Ã‡'qÃ›O3Ã )Ã˜&nbsp;Ã¦%Â±Â±o^4Â£Å¾Å¸Ã®Ã³Â­Ãv6Â»Ã¢â€˜Ã²Ãµâ‚¬Ã£oÅ¸0Ãƒï¿½ÃˆÂ¯NÃ•Ãœâ€œ8)Ã§Ã¤Ã˜â€œÂ¦GÃ³ÃÃœaÃï¿½Â§i3Â¹ÂµÃ1[Â¶Ã›Ã§â‚¬ZÃU-kÂ¨'Ã„pï¿½&nbsp;Â¦hÃšâ„¢Ã¹Ã“Ã™Ëœ@.â€”ÃŒÃ·hÂ®Ã‚Â¸Â¨Â­ÃšnËœÃ…Ã›5X:Ã¤Â§;Z3HÅ“Â¸â€°Ã¾Ã®â€¡CÃ£&lt;ï¿½XÃ»Â®Ã¸ÃƒÃÃ¸Ã¿
0$ï¿½*Å’
endstream
endobj
824 0 obj
&lt;&gt;
endobj
825 0 obj
&lt;&gt;/Font&lt;&gt;/ProcSet[/PDF/Text]/ExtGState&lt;&gt;&gt;&gt;/Type/Page&gt;&gt;
endobj
826 0 obj
&lt;&gt;
endobj
827 0 obj
&lt;&gt;
endobj
828 0 obj
&lt;&gt;stream
hÃÂ´xy|UÂºvÂµ!Ã•=Â¢RÂ¶â€ *Â­Å Â¢â€šÅ Â¢ÃˆÂ¨pAâ€¡-Ã¬[Ã‚â€“ï¿½Ã¬I'ï¿½Â­Ã·Ã®ÃšÂºâ€“Ãâ€œÃNVÂ²ï¿½Iâ€ U@Ã€eTdÃ”AÂ·qÃ†Ã‘9ÃÃ”ÃŒwÃ¯Â©ÃÃœÃŸ|ÃŸÂ½Ã¿}Â¿Ã¤â€”Â¤;Ã•Ã¯yÃ{Å¾Ã·yÅ¸Ã§hï¿½qÂ·!ï¿½_Â°pÃ¡Ã²Ã…Ã‹Â§Ã_Â°Â½$Â· eqÃÃ£)Ã©â„¢Â¥Â¹Ã›â€¹Ã•Ã¿&gt;%5Ã‘Â¤qQ*Ã¡^%Ã¥+Ã¯ï¿½Ã¿Ã´oÃ¦xpÃ¹ÃÃ¨Ã¡Â»Â¢Â§Ã¯~Ã­Ã¾Ã­ÃŸN@Ã¢5Å¡qwLÂ¸Ã¶Â§5iYÂ¥Ã¹Ã™â€¹f=Ã³tÃ¬Ã—Â³O-((Â¬(ÃÃÃŒ2$?=cÃ†Å’Ã©ÃªÃÂ§â€™Ã§Ã¯*Ã˜â€˜Å¾Â¼ÂºÂ¢Ã„ï¿½Å¾Wâ€™Â¼$gAqaAÃ±vCÃºÂ®'â€™Ã§Ã§Ã¦&amp;Â§Â¨(INI/I/.Æ’oÅ½=?â€“\rvIrzÂ¶!+Â½8y{rqzf6RÅ“Â¾+Ã™PÂ¼}WzÃÃ¶Ã¢Å“Ã¤Ãµ?Ã¿Ã­eÃ†Ã¿Â²^rv~2Å’â€¢Â¼6?[}ÂµÃšÃŸ,IÃÅ¾Â¿Ã«IÂ¥ Â¶ÃŠÃâ€šÃ’|CqvzÃ‰Ã¿ï¿½Ã² Ã¸â€¦Ã¼AÂ¢Ã²â€šÃ¼Â¨AzÃ=
Æ’ :Ds;â€šÅ’Â¿
Ã¹â€¢FC!Å¡â€°Ë†â€ D4IË†fÂ¢yÃ‘Ãœï¿½h0Ds/â€šË†f2Â¢IÃ–hÃ®@4SÃ¤]â„¢tÂ§Gï¿½â„¢Ã±Å¡â€¡ÃDÃ³â€š|Å’ w!&lt;ÂªGÃ¤~â„¢ï¿½#Ã¯!H2â€šÂ¼â‚¬hÃ®Cï¿½?!ÃˆÃÃ²â€šÃ¹+â€šhÃÂ£Ë†&amp;A^Dï¿½Â¹rAÅ¾Gï¿½Ã‰Â»
Â©@Ã¸Ã¾Ã“â€™ï¿½ FÂ©Gï¿½&amp;iCï¿½7Ã¤Â¹Ë†  Ãˆ5Ã¹AÂ¾F4ï¿½!Å¡Ã§
Â«Ã‘Ã„'Ã¯Ã®BÃº5Ã»â€˜Ã—Ã„â€œ@â€“!Ã‹â€˜ï¿½4Â¯hNÃÂ¶Ã Â¶Å¾8qÃœ3Ã£ÃªBOkÃ¿Â¨{RÂ·IWÂ®{Ã¯WÃ“oÃ´Å½	Å½Ã±+Å¸Â½Ã³â€˜Â»VÃŸÃ½Ã¤â€â€¦*Â±]Ã˜â„¢{LzpÃ¯Ã°}â€¹â€œÅ¾Å¡Ë†O&lt;â€Ã›â€°â€˜Ã»Ã?Ã@Å¾Â£Ã–QWÅ’{pdÃ’Å¸â€œwLIÅ¾2Ã¸Ã°Â±Â©Ã¡GÂª]Ã½1ï¿½ËœÃ¶Ã­Ã´cï¿½g?Ã¾ï¿½'f&lt;Ã¡zâ€™Ã²ÃºÅ’Ã‰3Â®<uÃ¾Ã”ÂµÂ§Ã›gb3ï¿½=Ã³Ã‰Â³Â§fmÅ¸Ãµâ€”Ã™Ã±Â³Â¥__|Â¾Ã³ÃÂ¯Â¿x8â€ºÃ“4Ã·ï¿½Â¹Ã»ï¿½Ãy=Ã³ï¿½Â¼d hx0Â¼pÃºÃ‚="" _="">Â»hÃ­Â¢Ã—Ã\\Â±Ã¸â€¡%Â·/ï¿½Ã²ÂªÃ¸Ãªâ€¢ÃŸLÃ¹Ã?â€“}Â¸Â¼qEÃ­JÃ‹JÃ¿ÃŠÃ«Â«ÃUÃ‡SÅ¾KÃ™Â·ÃºÃ¦ZrÃÅ ÃµSÃ—Ã¯ÃŸÂ°iÃ£Ã¸MwnÂ¦Â¶LÃœÂ¶ÃÂ±ÃµÂ£Ã”ï¿½cÃ®Ã‘Ã¨wÂ£Ã¸sÃ²hÅ“{\â€Â½Â¹Ã²Ã¯,
$Ã¥Â´^â„¢Ã¼Ã±Ã Â¨â€™Â¬Ã—ÃµÃ…ÃŸ}ÃšÂ¿Â£Â©Â±Â¿Ã\Â¾FÂ£Ã‡Ã´Ãª_Å Ãº*1Â±Ã”}Å“:Â§Ãâ€”Ã£Ã€Ã”zQÃ»zLÃ½Ãœâ€°Â¯zÃ§Ã•Â¢UÃ¥Ã™9dAAU&amp;mÃâ„¢X Ã‚â€™P+SGÅ½wÃ¯%â€¡Ã“rÅ¸JÃ‹Vf+Iâ€ÃÃ©r8qâ€¡Ã‡"ï¿½
z8Â«@â„¢7Ã¼â€_j$ÂªÃ¶ÃÃ+Å¾&nbsp; 
"'ZIÅ“Ã€Â¹yÃa)+ÂµÃ›uGUu1â€˜8Ã™&gt;Ã®Ã“Å’~
â€RÂ½â€”Ã„noâ€¹Ã#â‚¬Ã‡Â¿[Ã¥&gt;%~Ã–â€dEÃ¿Ã½Å¡Ã·FÃÃ¶4â€Hâ€˜Ã‹SÂ¦Ã}"yÃ‡DÃ‹Â°NÃœ)Â¹|$Ë†GÃ´Ã™â€¹{~Ã¦Â¨ÃŸq?}CÂ¼ÃÅ¸Ã²Ã¢&lt;&amp;ÂµÂ¢â€2VÃ¥m&amp;Å¸UJâ€ Â£Ã­Ã¯Â¿fl5NÃŒ{Ã˜&nbsp;UJÃ´7~Ã¾Ãœ&amp;O~{J=Ã•Ã®Å½â€œÃÃš&nbsp;7,SÃ˜:Â®/pÃ‚DÂ«Ã„	&lt;Ãs+Ã­â€“Ã‚Ã­â€2ÂµÂ±Â¬ï¿½Â£Â¶Ã‚Å ~UyÂ£@!Ã°Ã‡6Â­Ã¹Mnâ€¦'h&amp;eÂº2\Aâ€[ÃÃ•v%Ã«
Ã«21E)Å½Dï¿½Â¾9Ã¸.,Â»Å ]Ã¼Q)Ã–Æ’Ã›ï¿½Ã®:x`4Ã»Ã”ÃƒÃ¶Ã™ÃÃ Ã‰Â£Â¿ÃƒGÃ“N,|aÃÃŠ-â€™cXï¿½Â»d&amp;Ã â€”DÅ¸â€”Ã¬hÃš;0JÅ“Ã¨^Å¸Â³Ã¨Â¥EÃ«Â­â€ï¿½chÃ®â€™h?Ob?3LYÅ’+Ã¨Ã³Ã›Ã–â„¢|5&amp;2hÂ©Uâ€¢fâ€¹iÃ§yÃ›â€¢Ã˜Ã &gt;M4j@Ã£Â»qÃ‘UÃ·Ãº=Rï¿½ÃªÃ–Ã‰E*ZÃ°,ÃºÅ½Ã¼Â£ÃœF6J.ï¿½â€¹pÂ°Å’ÃIÃ™LJÃŸÃŸoÂ§]Ã°Â¨x.$Â³Ã°ËœmÃŠ&lt;Â´JdÃ¤Ã«Â¨Ã—#)Ã°Â²vÃ±IÂ½Â²/kÂ¼Ë†&amp;FÃ‡WG@Ã¼pTÃ›
Ã¢"Å¡â€˜Ã£__}ï¿½Ã Ã±Ã£qÃ‘Ã™Ã—Ã¿Ã¾Ã¬â€˜Ã£Ã»)IvÃ‹Å“Ã¨Ã°&amp;1'rÅ“nWÃ¾Ãºï¿½/;wâ€¦3Â¨Ã”ÃƒÃ¶s|DwÃ¥Ãw,/Ã¬$Ã›KjÅ "+Â¹%Ë†Â©$Ã‰-ÃŠ[oÂ¾Ã„Â¼`Â·Ã“&lt;Ã‹â€œLÂ¬â€¦Ã›%OÃ´Z6wÂ¬&amp;Â§Ã¤Ã¤Pi[ï¿½)Â«Ã±â€¦Â­Ã«Â¦â€˜YÂ§ï¿½gÃ´sÂ½â€™Â¦&lt;Â§ÃŒT2â€`â€™Ã²0â‚¬b0ÃŒ;Aï¿½Ã²xLÃ‰Â¢Ã¾â€˜2NÂ¹WyÃµKÃ€â‚¬
 &lt;Ã¦Æ’	`Ã…Â£Å Eâ„¢<kyÂ¥â€sâ€°Â£â€“Ã‘(Â¾oÂ·qÃˆ8aÃ¸Ã“Ã¿Ã”ï¿½vÃ¤?â€˜Â¬Ã¶Ã¨Â°Ã¨Â¿Âº4rÂ¶â€”â€™Â¼Â¢â€¡+ciÂ¬ÃˆÃ²Å’sÂ·Â½dÃÃ®Ã„ÃÃÂ¡Ã¾jÃƒ)Ã‹Ã»Æ’Ã¸'Ã¼â€˜Ã¯Â¿#â€ jzÂ©Å½â€šÅ¡bÂ¯at Â¬,Â´Ã‰iâ€™Ã Ã±Ã°Ã–Ã€Â·Ã™Â¾Ã¡vÂ°Â¤ï¿½Ã¥xÅ¾bÂ­Ã±xo;Ãvï¿½{+tÅ¾Ã–Â¸â€šËœâ€”â€™ï¿½[@Â¥n*]Â±uÃkÂ¯Â§â€™Â¹}#ewâ€°d0â€°ï¿½dÃµdâ€œ"yÃÃ€Ã¸+Â Ã©3l8="Ã–Â°ï¿½SÃ®Ã§Ã¬$Ã;Â¬f&quot;" exÅ½Â¥Â°-Â¥zÂ§Ã€Ã‰Â¤Ã§cÂ©6Ã¼'Â®â€˜Ã¨â€°Ã¢â€¦sÃ”Ã¡â€˜Ã°Ã¡Ã‹Ã¸Ã…gÃ’Ã‰Ã£+^Ã±="">Nâ€.a+Ã‹Ã§9&amp;Ã2â€“kÃÃ’'â€˜hcÃ€a(Â±JÅ“Wâ€°_Â©Ã”c[BÃ®ï¿½â€¦NRIÃ¾&nbsp;GÃ§Â¶Â¸ÃƒÃ„Ã…â€œÃ…Ã©Ã½Tg^Ã›fÂ­nÂ«iKIâ„¢â€˜Â·"Ã§UÃ‚PÃ¨Ã›({
ÂªÃ…Ã¹^Ã¾/~&lt;Ã±Ã¦ï¿½l$sÃ¼p0/Ã¬Ã˜Ã‘Ã€{	d]Ã‚Å’7Ã¯Å’;Â¸Ã”Â­ÃœÃ†â€“â€œÂ®Å¡Â©"ÃŠP+Ãƒ;)ÃŒXÂ¡uJÂ¬LÃ¶KÂ£â€¡â€¡Ã½	â€œÂ¼Â¢Ã¬Â£Â°Æ’
Z/#:Ã‰&lt;Â´Å“
Ã´R _Ã›jâ€ÃˆÃÂ®Hp/Ã‘Ãb/oÂ¦Ã¶ÃŠ;WÃ£Ãksâ€uÂ¼â€°Â¬Ã¤ï¿½\%a@]<kÃƒÂ°â€¢zâ€¡ÃˆÃºÃˆ@dzÃ½ï¿½^_,Â¬Å¸Ã‚"uz+:ÃˆjÂ´Â¬jÂªÂ¡Å¡Â¸sÂ§Ã°7Â´'Ã„Â£Â©ÃˆÃâ€šÃ¼ÃšÃ­dÃš*ÃƒÃ¶,Å Â¾â‚¬Å½Âºï¿½aj â€º$â€¡â€”&\Â¬hÂ³pÂ¥vÂ¶â‚¬Ã—afnÃ“6fÃ‘â€œÃ‘Ã‡Â·Ã‹Ã¢:="" +ÃˆlreÂ­9ï¿½$1Â¬@="" ÃƒÅ¡9Å Â³f)z[&Ã§â€™xâ€°oÂ¤â€œ<Â¼$Ë†Ã†poÃ·{â€¡Ã‚ÂºÅ¡Ã="" ÃœÂ½dÂ»dÃ‹ÃÃŠy6Å¸Â´Ã°â€¹â€˜xÂ¬nÃiâ€¢k]Ã£'ÃƒÂ¤oÂ¿Ã¯="" &l="" â€™â€¡jÃ‘ÃºxÃ‰nÃ¾-qÃ”ÂµsÃŸÃ }aÃ¬Ã¯Ã’Â¾Ã¾5wÃm="" k6Ã®Ã„â€¢â€œÃšd%t3Ã“Â¨Ã¹Ã¡\ÃœÃ¯nfÃªs*Å’â€ Â´Â¶ï¿½cÃ¹xÂ¿Ã…Å’oÃšÅ¾vf$.7gÃ£nzb}~ï¿½Ã‡â€œ0Â¹1Ã´Ã…â€¦Â¯Ã±s[ÂºsÃ‰dï¿½ÃŒfÃ€Ãµ#Ã€1r82Ã¡Â¯â€”Ã€Å½kÃ˜Ã•Â¿â€šâ‚¬^yÂ©5Ã°Å’Â¥Å“Ã˜Å’:8Å½Â¦jÂµÂ´ÃˆyiÃ¿qÃ©Ã¤Ãˆâ‚¬7aâ€™gÂ­sÂ½Ã–Ã‹â€°vÂ²-c="" 0Ãï¿½Ã»Â¸Â¿v(Ã±ÃšÃ˜Ã½Ã¯Ãº?Ã½Ã·ï¿½)Ã¿Ã 6-vÂ²@yÃºÃ¢eï¿½Ã·:="" e|3ÃºÃ·Ã…Ã€Ã™ÂµÂ§Ã“Ã—a4â€¦ï¿½uÂµâ€lâ€¡ÃŒzÃ‡Ã¸Æ’\ï¿½Â¨Ã¯hjÃ›Å¸Ã–Â¸eÃ½ÃªÅ â€ÃµÃ¶]Ã„Â°Â¹iÂ±vmÃ™ÂªuÃ”Â®Â´â€šÂ¥="" Ã¢Ã¥Â«Â´â€“wÃ±â€œÃ¯Ã¢%ÃšcÂ¼Æ’^Ãºh?ÃˆÃ½^ï¿½Ã›ÂºÃ—Ã—mtÃ”Ã“Ã¦="" Â®Ã¨Â­Ã¨â€˜.Â©dweÃŠÂ¼\Ã™Â½Ã–â‚¬â€°Ã£â‚¬hÃµâ„¢Ã†Ã’Ã¢ï¿½#â€“="" cÃ¹Ã™â‚¬Ã‰Å’Â¯Ã™Â²Â©Â¤â€t8urâ€ Å“Ã‡Ãº}="">Â¯7arsÃwÃ¯Ã¿â€Å¸_ÃŸâ€”E&amp;Â¦Ã»)Ã1p&lt;Ã®Ã¬ZÃ¤}	zQÂ¸O/Âµ_PÃ‘ÂªÃ´Â¡NÅ½wQâ€°&nbsp;Ã•Â¼nï¿½Ã·ZÃ´=Ã­{Ãº
[Ã³7ZvUnÂ²Ã¬Â¡â€¢Ã›â€¢Ã¹â€ºÂ®9Ã»nÃœY0Â¤Å¸Ã”Ã²Ã¤Ã€&lt;Ã²Ã€3Â§Ã—Â§Ã«\&gt;Ã§!Â¡&nbsp;Ã‡ÃƒÃºh/5Ã­Ã»%Ã«qÂ°Ã¡m0Ã±â€º?â€™^Â¯Ã¤â€”qï¿½5`Â§ï¿½Ã­$wÃ¬ZÂ³&lt;Ã³ÃÃYâ€¦Ã'Ã£Â¿Â©Ã¯Ã†ï¿½VÃºÂ§Â²â€™Ã›Â¿XurÃ‡&nbsp;ÃÃ£tÅ 4aÂ·ÃšiZâ€šÃªâ€šÃºÃ¶Ã‘Â·\Yï¿½+â€“(Â§?BÂº\Â¬â€œÃÃ©ï¿½-Ã Ã±â€°^9Â¼Ã¿Ã¤Â¹ÃˆÃ‡ÂºÂ¡Ã:Â¯Ã¾OÂ³â€˜Ã²Â²Ã¨â€¹â€˜	@g#Ã˜Â¡Â¨Ã°ÃºaÃ®Â¼Ã§0=Â¾v"Ë†ÃŠÂ¢$CÃ¨CHÃÂ¤3â€¹-Ã‹Ã™Ã©Å’Ã¦ â€“Â¨Ã Ã±ï¿½
hCï¿½Ã‹Ã¢Â¡&lt;iU	.Ã¦Â¹)Ã¸FkÂ©}Ã¼ÂªÂ¢Â¥Â¬Ã)Â¬Â¬Ã³2.Ã‘DXÃœe\â€¢ÂµJ0'L6[Å½`8V
Ã„Ë†ï¿½NhW{e&amp;5IÂ«Ã„syi`Â«Å“6H*,Tâ€˜ÃŠÅ“"Ã©?$â€¦Ã·AJÃ±Â¨Ã“Â­M+Â»aZeÂ¨Ãje).Ã€Ã¼pË†ÃšÃºÅ Â«
Ã’V2Â¿Â¶Ã¡Ã°V&gt;Â¼iÃŸÅ¸SÃ‡ÃŠAgâ‚¬Â¨â€˜â€š*lâ‚¬=Â¶|ËœÃ‘Â¨Å“/\â€°â€¹6Å¸Â¾UÃ¸S^Ã«
Ã•â€œÂ¨_â‚¬[oRÂ»ÃA:VÂ°â€¢Â¥Ã‹Â¬Â±Â­Ã“â€AÃÃ˜CÃ®â€¦ÃªÃ‡ZF9Ã„9â€¦Â¸]Ã«`8Â©â€Â¡.â€™â€¦_Ã›-\Ã uÃPÃ™â€¦Ãº Ã·Å’Ã‘EZSÃ˜Ã’Ã¢Â¥0â€™Å KÂªlÂ¬;Ã‘Ã†ï¿½=Å¸Â²â€°svÃ¢Ãš*NÂ¹Ã‹Å’â€”Ã±|â€¢ï¿½ÃˆEiÅ½WWÅ’;Ã¼â€“Ã”Â´Ã§TMÃ‚Â¤Ã˜Â±Ã¬Ã‘z8â€˜ï¿½<mrz{xÅ orÃŸâ‚¬Â´qÃ°â€œÂ  Ã‰ï¿½Ã¤Ã­Ãš="" nÅ ="" 7Â°â€“jrÅ’Ã§ï¿½Å¡="Â¤Ã§ÃºÂ³RKÃ¯Ã‘â‚¬JÃ0Â»=Zâ„¢â€”2-sy*Ã€Æ’â€â€˜6M8ËœaÃ¡PÃŸÂ¥">Ã†ÃºNFÅ¸Ã—Â³"cï¿½Ã¨Â·Â£Å¡Â²Ã“Ã­Â¸â€¦SÅ¾Â³Â§â€™Ã•lâ€¢9Å¸0Â£Â¬ÂºÅ½QÃ«`ÃÃ7Â¤==#*Â»â€™Â¤Ã¦Â¬Â®Sï¿½Ã¦â„¢â€šâ€ Ã¢j@Ã¸qÃ¬-Å¸Å¾Æ’Â¤Ë†Ã–â€“Â¼Â®Ã¨Ã‚yÂ¾Ãª/â€Â­Â­sIÃ­%Ã¼Å¾â‚¬Wâ€šbÃ¡"Ëœ\Ã›ÃÃÃ·	ËœÃ«	Å |â€™Â²AkÃ£â€šÂ¯&amp;Ã­NÂ«â€˜ÃŒAâ„¢Â±uï¿½"Å“jÂ¾sR{Ã¿)Â¿ÃŠjâ€™Å (Â¡Xr7ZÃ¥ÂªoÂ¢Â°&gt;wâ‚¬IÃ®ZwXÃ“	Sâ€šâ€š
mÂ¨gÃ®Â±ï¿½eÃ¬$kÂ°Ã‘Â¬â€œâ€¡eâ„¢rwUNEÃºÂ¦Â©Å½rÃ–Â§LÅ¸Ã„Ã»Ã˜wâ€Ã™ÃˆÂ¬v%LbÃ•Â¶HÃ¼Ã”Â²$Ã­Ã–}Å¡Ã¡Ãï¿½Ã¢*
ÃœÂ¯ohâ€™Æ’Â°U$â€“ â€“
C2Â©\Ã†ÃšMÂ®Ëœ8Pâ€ºâ‚¬U5EÃšÃ‘Ãª4PZ#g6Ã›HsUÅ½=â€¹(Ã˜YÃ—]@Ã­.}Ã£
~hÃÃ±Ã®&gt;rh`0Ã”CÃ®2tRm%
Â¹Â¾*Ã‰!Ã‚Ã—Ã¤â€¦Â¢â€š
Ã§ÃÃ°Ã &gt;NÂ¶;Xâ€“Ã¡HÃšÃÂ±Å’Ã›Ã¡â€º.ÃŸÃ’Â¸â„¢XÂ¸Â¼lÃ§jÃ³Ã¶Â¢Ã¥Ã‹Ã±9Ãkâ€¡Ã’ÃˆÃµ#Å¸â€#.Å“wÂ§|â€™,AÂ²Ã²Ã°Ã„Â¯`â€”$Ã·jB`Ã˜Ã–Ã…ï¿½OÃ€|Â½ÃŸÃÃ¹â€šlÃ”9Â»â€¦Â¬â€šÅ¡Â¶pâ„¢â€“hY{Fjj=Ã¡Ã©Ã qÂµh%ÂµÅ’Ã¥hee&nbsp;Ã‘L9Æ’Ã›Ã€Ã­Ã•Â£Ã¦Â¡
Ã !Ã“ï¿½ÃÃ‡qÂ²â€¹p10;ÃŠÃ®(Â¨(â€¡&amp;ï¿½Æ’5Ã Y	BÃÃ‡ÂºÂ¬Å“Ã‹UÂ¹RÂ¹ÃŸi*Ã™Â¸AIp:tVGâ€¢Â­â€™0Â¡Å’Å¾2\Ã’MCÃ–â€¢Å¡Ãªï¿½Ã»cÃ¨â€™Ã‡PÃ¬"â€¹Ã¡Ã¬ÂªmÃ¶RrMÃ¯5Â¹FWÃ—Â«Ã·ÃŸï¿½K~ï¿½_=B QÂ¢cDyL6Ã©?`#Â»Ã»Ã€ÃŒnÂ°&gt;â€™Ã“q~&lt;Ã½)Ã–%`Â¹Å¾_.Â¬Ã¤7Ã©J-L	iQÃ¡Ã…Ã†ZQâ€¦Ã—QÂ©Â½Ã¯â€Ã¯VÃ»Ãœ:Ã“"Ã”Ã¨
â€¦9
 Ãš6Â­Ã¨Â¸Æ’Ã„ï¿½ÂºÃ’e*P}Ã°Ã¹&amp;â€œÃ‹Ãšâ€Ã–`
 Â°~&gt;Ã¨Ã§E"Ã–ÃŠÃZ/Â¬Uâ€¦YÅ¡ÃˆÃ²Â°Â¡Ã›Ã”Â¼Ã£Â´Â±Ã‡Ã Ã‘â€°Ãšï¿½Â»â€¡.Ã¢Â­Ã¦Â½Â¥-Ã¤Å¾Ã¢Ã”â€ â€Ã¹Ã–RÂ±Å’â€ ÃˆQÃ³Ãâ€ !ï¿½
Â¬DÃ‘Â®â‚¬Ã•(PÃ“Â´XÃ¿e&nbsp;Ã‘7Ã–Ã‰uC]Ã¶ÃŠNÂ²Ã‘RWËœâ€¡UÃQÃ¢ÃŒ6Apâ€™â€°WÃ¡Â¹.Ãª#Â±Ã­RÃ e&gt;xâ€¹â€šï¿½&nbsp;YÃŸ.~[Æ’Ã—zÃ¤Â²Ã½WÃBÃºÃŸÃ†â€“dÂ¥AÃºwÃ±Ã¬`aÃ‘&amp;Ã‘=ï¿½Ã´HÅ¾Â§}IxÃŠe+Ã«Â¢mDÂ©Z.:Ã†p09Ã¹Â¤Ã”ÃœÃµÂºWe.9Ã–ajÂ¹ÃŠPÂ»Ã•&nbsp;ÃœW=xÅ¸OÃ«
pÂ®tâ€šx%Ã™kÃ•AqÃ¤Â°4Â±0ÂµÂ©Ãƒm'Ã¾FÃ¦ Ã‚ÂªPGÂµâ€Ã³(Â³Ã{Pâ€¡Ã½Ã”Ã•Ã·Ã4u5-ï¿½nÃ¨Ã‹Ã‘:ZÃ¡Ã£ï¿½\9Â¬1/5V&gt;~Å’
!â„¢Ã‰hï¿½Ã§(Ã¥Â²)qJRÂµCÃ§Â²Ãš9\=Ã±(Ã›ï¿½k{Ã€3â€˜Ã¼Â½Â°\ÃªÂ£â€ÃÂ°'Â¢/Ãª]nÃC6Â¡MM|Â¶H	VÃ±Ã¥b|Â¾+]Ã§Å¾Ã‹Å¡H3kÂ·Tâ€¦PRÃ‚eÂ°eÃ5@ÃšmË†Hï¿½Ã»ÃºÃ¶%Lzw$Ã®Â¤ÃºÃªÅ¡Ã·Ã£Ã¯Â¤Ã­Å¸Ã›JÅ ZÃ¬ÂµÃ³Â­ÃÃµÃ§Ã±Swz3YcÃ˜Ã®YMâ€Ã¬bK+Ã›Ã¡Ã¨Ã¡a9ÃÆ’JÂ¡Ã›Ã—Â»WÂ°Ã¬Â£Ã¶â€º;s7Ã Â¯l(JÃhÃª:Bâ€šï¿½Ã£Â¸O
ÂµÂ¤Ã =^A'x}ngMÃ»ï¿½Ã•ÃT{Â¹Â§Â¸/Â³â€“â€œFs&amp;Â³UÃ’%Â³=Â¹â€˜Ã] Â¥Â¼Â¸/f~4=Ã˜â€¹`SÃ´9Ã½Ã\|Â°Å¾Â±Ã–P
UGVÃƒÂ¤Ã¡Â¬Ãª}IÂ¢Ã yâ„¢Ã³|Ãt5Ã´Â¥Å’â„¢Â³Å ï¿½ï¿½Â¯^#Uï¿½8UÂ¨F'Ë†yâ„¢2â€°lÃ‚Ã’uÃ¸2-Â¶Eâ„¢Â¦5Â±ÃŒÃ’&gt;6Å Â°Æ’Å ]ï¿½Â·AHÃ¬Ã…Ã†F!Ã°Ã“Ã›Ã¼Â¤]~Â¸Å½â€”Å câ€˜Å Ã¼,HÅ“Ã„	Â«Ã±Ã˜\VÂ¤Â¡ÂµÂ³UeÂ¥:\ÂºÃ‚Ã¬&gt;ï¿½Ã˜â€“/ï¿½PÂµÃœÃ©~|Å¸6Ã˜Ã¯vvS.oâ€¢Â§XÃ‡Â¢["ZÃ„xIdÅ Â¯e{Ã¶Ã Ã·q1@ÃŠ+2DÂ©ÃÃ€Ã£Ã‰Âµâ„¢Å Å“Ã§XÃ‰ÂºUÃ©ï¿½ÃŒÃ¿MÃ­ï¿½ï¿½Ã‡Ã½ÃŸ
pâ€”Â¥_wâ€šÅ¾}Â´qÃ‚â€¦Æ’Ã›ÃŸÂ³;Å ÃaÃ`7Â´ÂºG.|8ji7Â¶PXGqGFhkCÂ¹/Â³Â®ÂºIgÂ­Â¥[ÃªÃ°Ã†ÃºÃ—Nâ€™XÃÃ=Â¶ÃŠ:ÂªÂ¥Ã„Å¸Ã¥ÂµÃªâ‚¬Ã¥â€ºK8VÃ‡â€¡Ã­oÃµÃ¡^v8X7Ã‹â€˜Å’3Ã«Ã¤hZÂ°yu^Kâ„¢/ï¿½ÃˆÃŒÂ±â€”QÂ¥Ã¥eÂ¹Ã™Ã¸Â¦Å¡Å“Ã–RÃ«Ã¨.<rÃ‘Ã¬Ã”7Â§Ã Ã›6Ã§ï¿½xwÃ†Ã®Ã¦Ã®j Ã‹Â®Ã¨sÂ·â€qâ„¢ï¿½Â¬Ã£aâ€œ1pbt9_â‚¬6â€¹â€“="" Ã§Ã§<ï¿½Ã™$â€™Ã¨Ã·Ã²="">Zâ€¡uWÂ´Ã¶W%Ã¿jÃ™}Â´_3Ã¼8Ã¹yÃ¨Â²Ã¨&lt;Â½Ã¿Ã„~ÂªÃ»@Ã°Ãµ#Ã¸â€”Ã©Ã§Â¶Ã­'n\Ã˜J,Yf^Â¸â€™ÃšÅ“Ã¢\Â±Ã¼Ã˜Ã‹%dÃ¶Ã‘3Â¶~Ã‚Â»OÂ¿9Ã¹ËœyÂ´Ã¡Ã‚Ã ~jdÂº#Ã¢ÃŒÃµâ€¢Ã»Ã–Ãšâ€â€¢Â¨Ã„NÃˆNOwÆ’
cÅ½Ã¼2ËœÃ±9vÃ’qâ€°~Ã·oâ€¦vÃ“Ã…â€“E(Ã‹Ã†Ã¦â‚¬+FÃ‡g Å¸â€ºÃ¶c#6Â¦fÅ’v_-ÃœVÃ€}Â¥Ã¯ÃÂ²Âµ|Ã†ï¿½?Ã¶YÂ¤&lt;â€™Ã“BÂµedÃ¹Å¸%ÃŠvÂ²â€“Â²EÂ®_Ã¸Ã´Â»Ã¢1AF#&gt;Ã–mzÂ¹{Ã³Â»zÃœÂ©â€”Â­Ã®Ã‘{ÂµxÂ¡ï¿½3Ã›+Â©Â¼ÂªÃƒNÂ¢Â¨Râ€Ã­Tbt|ÃŒÂ·5ï¿½sÂ¤Ã ÃˆËœsÆ’ÃŸÃ·hÂ±wÃ¿?Ã›Â·Â¾KÃcÅ“x+â‚¬Æ’LEÂ«Â¸Å¡0Ã…Ã½Ã²Ã¹DÃ­Ã»ÂµUK)eÂ²vÂ¦Â¶â€šÂ§!?Ã¯Dï¿½,T&nbsp;FÂ­Kâ€š*-Â¸W:sÂ¾=v Ã¹Â¨Drâ€™Ã›ÃjÃ†ÃŸÂ¨&amp;Ã„ï¿½Fâ€¹EÂ¾tÅ¸â€¡Ã†Ãœ'Â¸=NÃ‚[Ã˜NaÃ¯:Â²2Å“Ã©â€2ÂºAÃ±Ã“â€˜Â£d[KWÂ¨â€œh[*jÂ©Ã‚â€“K+Â±Â§Â£Â¾ï¿½Â§"Â©Â½k6ÃŒ)[Âµâ„¢Ã„.nhÃšÃ¸?A'Å“â€¦Ã¿Â³4Ã¶TÃ¶â€¹ÃÃ­Â®X{Ã‹â€EÃ§G4]`5Ë†Â³Ã¢@A4EÃ¯QÃ•Â¶k9[RÂ´Ã¦â€”QY&lt;6Â³â€šhÂ­ÃŸaÃµS&gt;[Ã·Å’â€ Â­Â¾Ã’$Ã©qeÃŠÅ¾â€¢:â€¡Ã¤qÃºÂ¡Ã˜â€&lt;Ã¥Â¯Ã½lÂ´;Nâ‚¬Ã»Å¡Å¡tï¿½@Ã˜Ã›DÃ”Â£Ã¿}|Â¹RÃ™
ÃƒÃºnÃ˜1]QBdX^saÂºGÃ‘Ã•â€ºâ€Â»\e:uÃ•gï¿½Â£]WÃ¸Â»ÃŠÃVÂ¼Â¶NÂ¬%[QYâ€š.&amp;Â±Ã”rvÃ¿Ã˜Â­Ã¸Â¬ÂºÃ¾hÃ’Ã—Ã—Ë†Ã¡Ãâ‚¬(Ãšï¿½Ã­iÃ›Â¨uÃ«lÂ¯Ã¬Ã‚_dÃ´o Ã—Å“Â¯|Æ’Â¸~Ã¬Ã¢P#Ã¥	q+Ëœ}IÂ¬&nbsp;j_ÃÃ®ÃŠmYÃ‹â€°â€“}Ã°}Â«P/(Â¹[Ã¿AÃ“ï¿½Â½â€°Â·jâ€“Ã¬ÂºÅ“zÂ¤Ã¨ENtâ€¹"Â¸Ã¯Ã«Â¤Â®AÃƒÃ¾BÂ¢Ã‚l1[Â©iÃŠÃ§â€ â€&gt;1aÃ‘Ã°7Â«Ã£Â¢Ã©&nbsp;GÃ9Â¬ÃœÃ£ÃÃ”Ã¸Ã¬Â¶Â¢uÃÂ£â€š;vIp&amp;Â·Ã¶KÃ—Â®Â¶4&amp;LÃ²IP	ÂµÂ«3Ã™â€¦vqQM_%eÃšÂ»,7|T~a0â€Å¸Ã—â€¦XÃÃ«"hâ€“Â¡iÃŠjJÃ‹+pÃ©Å“0ogdH:Â¤ÃJâ€ºYâ€ºÂ½Ã°y%Ã‡iÃŒÃ¿ÃµKÃŠ2gâ€¢.Ã‡d2Ã¥Y(Ãƒ@ÃŒâ€”Ã€vWï¿½ÃÃ©Â·gâ€ ÃƒÂ°i$Ã‘Â»â€¡ï¿½Ã¤Â´Ã”Ã¢{Ã„&nbsp;Ã Â£&lt;-'ÃÃ´`Â»Â®6Â¢Ã·ÂµÅ¾ZÂ¨Ã½dÅ¸LHÂ¢Ã“)QÅ¾ÃªÂ«Å ÃˆÃ“%â€š,Â¶7g|ÃœzUÃ±GÃ¾Xn,Ã¿Ã–HÃ°Å¾Ã‹Ã¿5Â§$Â±2â€¡Â¡-Â¹Â°qï¿½jÃŸaÃ»Å Tâ€ Â¨!â€ºÃHÂ£/&amp;LÂºÃ±FÂ°Ã£uxoÃ{GÃ±Å¸â€”Å¾}Â¬â€”dÃ…xÂ¬Ã¿Â£Ã¡Æ’Ã­Ã„ÃÂ§Ã´ï¿½ÃYÂ¯ÃºÃ¦Y+Ã˜Å“Â©UPCÃ…(kÃ¡Ã´p=Â·
ÃŠ
QIÃŒÃ€Ã¿â€™Â¾=&lt;Ã‚W
SgÃƒÃ«Å¾Ã‡â€¢;Ã¦Ã®|nvÃ©Ã°
ï¿½Z0wÃ–MÃ¸Bh&amp;^Å¸_$EË†GGÃ­Ã…j&nbsp;Â°fg:Å¾Vï¿½Âº#â€”Â¬,IÂµÂ§â€¢â€¢nÂ±ÃÂ£Ã•oÃœÃ„Ã»5G~Ë†â€“ÂµÃ…EÂ§WÃ«ÃÆ’ÃŸ]Ã½Ã¢,%A,Â³Â¢9ËœÃ„JP|Ã“ÂºÂ¥[g.SÃ¢â€°WRÃ¼â€”Ss&gt;Â¯Ã±Â§qÃ°0	ÃŒâ€¹â€°ï¿½ï¿½go;@Ã˜Â¶gsxÂ·j9<bï¿½Å“$â€šÃŒ{ttâ€”lmÃ‚Ãƒï¿½Â¿Ã¬Å“Ã›Ã…â€˜Å½|ÃiÃ³fÃ¿Ã„â€“ÃŒyÃ»fÅ Ã¦Â©-k7pÃ¾#oÃšÃ¸Â´~ewi1Â¹Ã¦Ã¬â€¡Ã v"1Âªqï¿½Ã¡Ã¦]Â±3xÃ¨fÃ´Ã¥Â±Ã°pÃ´=Â¶eÂ£â€Â²edguâ€Â¯Â 4Ã“ eÂ·="" Ã±ï¿½Ã´Ã…Â¡Ã‹Ã§&]Â¤Ã»Ã§oÂ©Â·="" Ãµ]zÃ¨Â¶^[zÅ Ã¼pÂ¦â€šÃ”)zÃ‚0â€¢-z*Ã="" â€¢uÂ¥x_!.n'Ã‘ÃzÂ¦dÂ¢Ã„jiâ„¢tâ€ÃŸ(Ã’c="" ÃµnpÅ¸pâ‚¬Ã¶ÃˆÃµÃ¡â‚¬.Ã˜0Ã¬Ãcâ‚¬Â¸ÃŸÃ¯j9fyÂ·Ã¯Â©9Ã¸Å’Ãœ_o_oÂ¾ÂºqvÃšbÃ£ÃpkenfÃªâ€ºqÃ¾Ã¦Ã–bÃ‹lÂ«wÂ±â„¢}Ã Ã½^p`0Â£xÃ¸6Ã˜oÃÃ–Æ’el'ÃŸei1mlÂ«Ã¾`:Â¼Å“ÃŒjbï¿½?iâ€ "â€¦â€”Ã™`xâ‚¬="" Ã«$Å¾iÂºÅ“Â«zÃ¤Ã¾"="" ]â€™%â€¢Â±+lÃ“Â­ÃŒ#Ã¨@Ãâ„¢#ca%*wÃÃ†Ã•*6mÃ£ÃœjÂªÂ¤Â®n1tÂ¨Â¥%ÃÂ¨â€â€ºÃ•Ãœ%="" â‚¬ÃŸÃ±g="" ,Ã„Å¸ÃÃƒ{Ë†Å¸7="" qÃ‡ÃtÃ‰Ã¯â€“h="" *+Ã˜Ã‹="" koÃ„fâ„¢="" Â«xleÂ¥iÃ–jnÃ‡â€¢mÃ¤+â€°Ã¢|i6rÂ¹ÃÃ¢fÃ¾ËœÂ®â„¢Ã»Ã®~n[Ã‘mï¿½plÂ¨Ãš[&yÂ½Ã‘,@ÃŸÃ­Ã¶hÂ¢Ãªa[Ëœ#Ã½xï¿½Ã»s81dÂ¸wÅ¡(â€ jâ€¹â€¦Â¸Ã§Ã‹#lÃ³\â€¡#Å¸Ã–qâ€“dÃ†a$â€šÃ‡="" Ã¥Ã´Ã·Ã¾dÂ¶hÃ¦Ã¾Â¬Â¾="" Ã¹Ã¶hÅ½&Ã¨â€¢yÅ“2ï¿½3â€™Â¼Æ’)"rÃ‘Ã˜)Å½Ã‘mÃ~Ã©Â£Ãâ€¡â€š1Ã¦uÃ‰â€¡â€¦Â¬Â¿="" ÃÂ¦="ï¿½T^Ã¬Ã†Â´ÃµÂ¿uÂ»FÂ¨=lï¿½Ã…W&quot;Y}yâ€šC&nbsp;Â¡Ã¨Å¡Ã¤$ÃXÂ¦ÃÃ¬â€˜v_aÂ¦Âªp0" Â·[Ãâ€Â©â„¢Å¡Ã­Â¥="" ÃŒtk"fÂµo="" `k-Ã="" fÂ²Â®vjÂ¯Â³Â­Â²ÃÂ°wwgfÂ jrÃ…dâ€º*â€™(â€™uÃ˜7ÃÅ½<@Â°="" :Ã¬+ÂºjÃ¹â€¹pilÃâ€¦}cÂ°.â€šâ€°aÃ¹Ã‡b-#ÃÂ­Å“f;ÃƒÂ¶rÃŠ"*Â¦Ã¡â€°qÂ´zÃÆ’Ã‘Â»#Å¡#?fwÃœË†â€¹ÃsÃµÃ«â„¢rÆ’â€°4fm2Â½@,]Ãzd#ÂµÃ¬ÃŒÃ®Ã¡â€Ã¿rÃŸÂµoï¿½ÃÅ¾kÂ¸h|y.5Â£â€ºÃªÃŸÃ–Â°Ã…_Â Â£â€°â€¢â€°f1^dÃ…â€šÃ•Ã±="">ÃÃ£t14Ãƒï¿½Ã•.â€¡â€¹Â³&amp;vdÃÃ­}â€“PÅ¾ÃŠ^Â¾ï¿½Å¡Â¿dÃ—c3pe|ÃÂ¬7â€˜Ã_â€°Â»Â¯CÃÃ´IÃ«Ã°U*({eÂ¿Å¸	â€ qï¿½Â¤Ã­Â©Ãµ5â€™Æ’Â¨Wï¿½cWq&lt;Â¦vÃÃ¢TSÃŒÂ«Â¸Â¨Ã¢Â±kÂ²hwÃˆâ€“	Ã¥Ã‘Â£Ã°â€/FÃ€Ã€&nbsp;Ã 7Â¢ÃŸË†8Ã¸â€¹Â¾Ë†SÃ–Ã±[ÃˆÂ¶Â¤Ëœ\â€°Ã†JrÂ«Â¹[Â»Â¥Ã/Ãµ7ÃÂ²Ã˜Ã­ÃšÃ˜DÃŸâ€â€“Ã™â€šmPÃ“Â½Ã‡Æ’':Ã±Â­Oâ€™BdÃ´6Â´ÃÃƒ[)â€œ6â€œSÅ¾â€¦Ã¡
Ã™Ã²Ã²7hÅ’fÃ•&lt;Ã Ã˜Ã¯Ã®â€¢Â¾Ã¸Ã­Â¾Â¦[PiSÃƒ9Ã‰44ÃÅ½PÃ¼HÃ©â€¦Ã®a@sÃ‚xDjÃ˜KBÂ½Â¢&nbsp;&gt;Ã¨Ã¥Ã¡Æ’Ã¥;Ã˜Ã…â€¹Â·â€¢Ã…Â¶Ã¶Ãï¿½Â¡{Ã¥Ã²ÃŠ")T
nÃ“Å¡iÂ®Å¡Ã¼;Å ÃšYÃ‰5Ã”)$â€¦Ã±!OÃƒ&gt;Ã²([Â´Ã¢ÃšÃ¯Ã²LvÃÃŒ#Å’Ã…Å½ï¿½4ÃµÅ¾lÃ°â„¢6PUÂ¢B,ÃƒÃ§Â²*j`Ã„ï¿½dÃ°gÃ½:VÃ‰â€¦Ãâ€¹ ï¿½Ã„Ãª1Å’Ã¡Ã†Câ€ ^â€œÂ¾Ã¹bHÂ½uQÃ¥Dâ€¡:irÅ¡SÃ­Ã©Â¡B,Â¨
Ã¢WÂ´5(
Ã¢ÃuÃ¾fÂ¸Å½Ã§Å¸sÅ¡VOÃ¬Ã©'6XTjÃ¼WÃ„Ã“Ã¨Ã¾&nbsp;Â­:ÃšjIyÂ¥OTtJÃÃ7;`:`:Ã€gÃºâ€”BÃŠkÃ…HÃ«Ã®RCÂ©SÃ—cÃÂ½'â€ºÃˆ)Â©ÃˆK55^Â¾Ã¸Ã–Ã»ÃƒTÃ·QÃŸÃ¨Ã›Â­Ã²Ã·ÃƒkÃxÃœ#Ã”â€œ-BÃ€ÃŸIÅ’Â¢Â¿4Â«z&nbsp;Ã•YÃ¬Â¬Ã©VÃµZCÃâ‚¬â€”Ã‰Æ’Ã¨`Ã€R*PÃ’6Â·Ã²rdÃ•Ë†ï¿½Ã“ÃÃ â€Å ï¿½/ï¿½Â¬â€Ã â€°Ã†Ã«â€¢Â»9e	o'mÅ“Ã“QAÃ¬PÂ©ï¿½Ã¾Ã¥pÂ½Ã’Ã‡Ã¯wÃ—ÃœÂºRKÃ«â€™â€ Å¡Ã­Ã•â€90uÂ¿Ã”Â¤Â­Â«Ã¥ÃŒÃµTÂ­
Â¼Â¬ËœFï¿½ÃÃƒUâ€“Ã•cWâ€°EÂ·Å“Ã€&nbsp;tÃ­ÃƒÃ½Ã1VÂ¹,Â´$_nÂ©Â¤Ã¬!EÃ§Ã­Â¡Ã—Nï¿½Ã‡ÃÃ‹ï¿½ÂºÃšÃ†VoËœh+sÂ§z1Dâ€œâ€“,6Ã«KLâ€¡Ã†nsâ€¦ËœÃÂ¨ÃµÃ˜LvÃŠnVâ€“)Ã–Ã’*ï¿½ÃdeÂªË†Å QÃ´u	ÃŒÂ¨ÃƒÃ›Ã¼Â¾fÃ˜=Âª
SÃ“Wâ€¹eÃÃÂ¾2/ÃÃ“Â·PÃ¨%Ã»Ã‘vÃ”oIY_KÂµâ€ºÃŒÂ¼9~0Ã†&nbsp;â€¦ï¿½AÃ¯Ã—cÃ‹YE,Ã‡Ã.SÂ¹ï¿½UÂºXMÃ‚Ã§Â«&nbsp;Å½Ã€
[Ã¶
Æ’Ã„â€°Ã¯ÃÃ¼u
Â¸Ã¾ÃÃ—RÃÃ!Ã°kÃ‰'xÃÃÅ½Vyâ€œ`)Tâ€°â€°Å¡vÂ­PVÃšseâ€œX*ÃªÃ˜a7ÃˆmÃ€?Ã—Â¾'ï¿½iÂµÃ¸^9Ã˜CÅ¾EÃ½"Ã¬ï¿½VÃµÃ¦ÃˆAÃšÂ·Â³ï¿½ÃÃÃ¼Ã¥Ë†Â¡mh/Ã„aÂ·Ã‡V$ÂªÅ¡Ã§ÃW!ÃŸuÃ‡Â¯^4â€˜Ã¨Ã•Ã˜ÃxsÂ©Â¾â€“vÃ‰Ã•â€Â¥Ë†Ã2Â«ÃvÃ«ÃªÂ©HÂ½3Ã°â€™oÂ¢ÃƒÂ¼Ã‰ PBÂ¾[â„¢fÃ‚+X%`YHfÂ»Ã¬EÃ©Ã„:â€Â»Â¤rÂ¼Å¸Ã´KÅ¸]Ã«%LÅ ]Ã©Â·ï¿½]nDsÃ¬ÃµoÃÃ›/â‚¬ÃŒÂ¼F^9Lâ€ -_)â€ Â¦Â­Â¸GÂ¹(9u6Ã™Ã£Ã°~]@Â¦Ã¤Ë†~oï¿½'&amp;ï¿½wÆ’Âµ:â€¢Ã¬neÃ‘Ã¼UÃ­|^yâ€°Â·ï¿½yÅ“ÃUI,Fï¿½|Ã¬Ëœ]Ãªï¿½OÃ­&nbsp;tÃœÃ–X[ÃÂ«ÃBÂ©Ã‹Â¯Dâ€¹Ã­ÃµÃ»Â©fÃŒÃ…ÃŸÃ“Â¾#ï¿½â€¡Ã¶Ã¢Â­Rc7Ã„Â¾Ã¯Å¸ÃÃ‘Â¤5â€ºU&amp;Ã˜!â€ºâ€¢Â·rkT4Å Ã…irËœÂ§*S5Ã”Â¯?Â°Æ’â€¦e7Ã§
Ã†ÃŠÂ¶{;Ë†Ã¨tÃ½QÃ¯ï¿½Ã‘qTÃ™ÂºÂ¡Ãª0
Â¨iï¿½Â«Ëœ*â€šâ„¢1Æ’â€°&amp;ï¿½Ã™`c[Â¶qï¿½eIâ€“Â¬Â¬ÃœARÃ§Ã®
ÃUï¿½â€œÃ”
Â­Å“â€œÃ¥,gÃ£Ã†6â€“Ëœ4Ã†30Ã£Ã¡Â´Â¦|Ã¯ÃœSÃÂ²gxâ€”Â·Ã®Â½Ã¯ÃÂ¼ÂµÃÃ½Ã©Â¥&gt;}Ã&gt;Ã»|;|ÃŸ^bâ€Q)ï¿½Â¶Ãï¿½YÂ£,Ã¦vÃS|KÃ§Ã¨Ã.|â€¦aÂ±Ã³(â€™Â½Â¥Ã¦Â¦QÅ dÅ¡ï¿½Ã Ã‰(wJÅ“FÂ°XÂ¨Ã±Ã©ï¿½Ã¦3Â½Ã³*Sâ€º{Ã¸YÆ’RÂ¼GÃ«Ã‡Ã­6oï¿½Å’\/9nÃ½Ã~â€šÃ¡Â¼Â¡NÂ¼3FÂµÃ‡â€šï¿½Â¦â€š^ÂºÂ¨Â°.ÃªÃ³SÂ¾j'Ãºï¿½Â®Â¦Âºr7Ã©T;Ã®Â³mDÂ¹ÂµÂ¶â€GazKI*Ãºâ€Â¹ KÃ”â€¦UÃ€aqÂ¢Â£Ã‘Â®â€°2FÃ¹Ã¹UÂ¶Ãši3jaÃ˜Ã†Â§Ãâ€“;bÅ’Mb{
â€“AÃ§Ã¹EÃ­ÃˆÃ¤oFÃ¢vÃ½	_Å’ï¿½&lt;â€œ[Ã¢Ã¼â€˜sGÃ‰Â½Ã¯Â´Â¾Ã„Â¢â€¹ÃÂ½IÂ¼=Ã§Ã©Ã€bÃ¼Ã‰â„¢Å ;gâ€œÃ³Â¬â„¢Âµ~ÃµÂ¾pÃÃ©"Ã¥Æ’ÃŸÃ—Å’Ã£Ã®Ã¹Ã‚Â¢&nbsp;Ã¡Â£Ã¾Ã¯OÂ¼EÅ¾~Ã¯Â¤Â¶&amp;Ã aA.lz4GÂ¨Ja="Qw8MUÃ±/â‚¬Ã¿ÃÂ²oA,Â¨Â¾ÃÃ¬0fÃ½;ï¿½xÂ¨Ã¤`j9Âº|â€¢Â¦Ã’Ã‹hÃ•,Ã‘Ã½Â£Â¼)Â¸?â€tz'Ã¶Ã†â€Â¸W0A"PZxËœÃ•ÃƒÃeÂ²Ã¤Ã£Ë†%Ã¶J)Ã‘â€¡|Â£Ã¼Ã§_ÃµÂ£.Ã¤Â¸mâ€°Â¡0ÃŒ@,cÃ;Â±ÃˆÃ½Ã—Ã‰ÃÂ¹Å¡Ã‡Â¬!Ã¼0ï¿½Ã¦	Â·Ã§Q9U%d^Ã™Ã«KÃ°ï¿½Â¼SAJ9=Â¹5ÃŠyÃ…Æ’Ã â€˜Â»lÃ /qÃ™Ãâ€°â€¡bmÃ’Â©^Â¶Ã‘Âºâ€ .Â±Â¦Â²Â¯Ã™ÃÃ“Ã®Â¥Â¦Ë†#Ëœ6â€°Â¼ï¿½GC-Ã·Ã«,]â€™Ã
2Ã€*Â¦â€ -ï¿½Ã¶|Â¡Â£UÃ…ZÃ¤Ã7Ã¸ï¿½Ã­{Â¯Ã´Ã¦Ã¡Â²,DÃ‚ZÂ¤Dlâ€¦â€ºÂ®Â·â€°Ã‚Ã­9ÃœÃDÃ­I~ÃªÂ¿â€œï¿½$Ã¸Â«|â€“Ãµf&amp;QÂª6UÃhfÂ±Ã’QÃœÂ´Ã’nÃ‚Ã—ÃŠÃ¹u;|Ã^,Ã¾Â¢ÃiÅ’ÃµÂµÃ†Ã†q]\w;(Ã¡Æ’^Â«Â£Â­Â´Â¨Æ’Ã´Ã—Ã­zÃnrÃ·Ãrk=Å¡pÂ¾Oï¿½Ã–Ã›â€ Â¹Ã®mÂ´{ÃšÅ’Ã¨sÂ©Â»iS%W_M(Æ’â€¹Â¾Â´xï¿½ï¿½~ÃÅ’Â¾ÂºÃ†
ÃŠIÃ€Â­]â€¡vâ€š[Â°aÃ¥Ã¶â€š&gt;b(Ã»â€¢Ã¶Â§pÃ•Æ’Ã´Å 'â€¹Â®Ã´fÂ£ï¿½#Ã‘Bâ€ºâ€˜vÂ¿Âº Å Lâ€¹Å¾Ã…dÃ§Â¾â€¹Ã¥
Å¸Ã‹AÃ€Å’Å“Ã³Ã£oÂ«-%Â»+ZÃ’Ã—cÃ²â€[â€Wâ€¢:_KHÃ¿X;:yÃ¿ï¿½Yâ€˜Ã¸Ã®Ã¨ï¿½â‚¬Â¸zrÃ’Ã„Ã·Ã ÃºÂ·UÂ£eÂ½dÃ®Â¶Â¡â€”Ã‚%Å¾Ã¡â€™nÂ´Â¼ÃƒÃœÃŸâ€¦Ã­Ã©{Ã»Ã¼ÃÃÂºâ€™Nb,#Â°Ã‘]Â±Å¡ï¿½
â‚¬Ã¬Q"HÂ¶ï¿½
k~Ã„&lt;Â´Ã›"&amp;
BvVÂ£ÂµX,Â¬Ã1ÃÂ­N-Ã‡__[â€”Å¸Kâ€¢Â¥Â¬Ã…^	Â¥Ã¶[Ã“&amp;ÃŠ{uÃ¨â€¡â€ºÃÃÅ¾ï¿½=2?gÃ©ÃŠâ€¢ï¿½;â€¹â€°â€š=L?Ã›â€¡:EÃ¯Â¨3dÂ«fÃˆ%zAFâ€¢Â£:&gt;ZÃœÃ¥Â°Ã¹Å“Â¤Ã—iw9qâ€¡â€œ6Ã¸Ã‰Â¢ÃÃ£Ã•Ã¯Ã€Ã´Ã%Ã‚Ã»â€8Ev
Ã¬Å½ÃŒâ€“Ã¿Ã“Â¤Ã½5=Ã³ÃœJÃdcÃ­Â¶â€ XRÃ«QÃ™Â°qâ€ºÃœÃŒÃ¸Ã‘6ÃŒÂ¶EyÅ RÂ«Ã–kÃ¨*vÂºÃ¬Ã”â€â€dÃ)Ã‰Ã°Ã¿HJrÃªÂ§RDÂ¬Â°G'Ã¯ï¿½Ã»Ã£"wÅ â€¡Ã¹Â»Ã¼Ã·G[â€¡7â€œwâ€¦ÃŸt~â€Ã²Â£x&gt;Â«p7Ã§%Ã¼&lt;ÃŸ5Å’ï¿½Ãg}Ã¦&lt;Ã‚PHÂ¿Â´pÂ½~*LÅ =tÃ¥Â¶!</bï¿½Å“$â€šÃ¬{ttâ€”lmÃ¢Ã£ï¿½Â¿Ã¬Å“Ã»Ã¥â€˜Å¾|Ã®iÃ³fÃ¿Ã¤â€“Ã¬yÃ»fÅ¡Ã¦Â©-k7pÃ¾#oÃºÃ¸Â´~ewi1Â¹Ã¦Ã¬â€¡Ã v"1Âªqï¿½Ã¡Ã¦]Â±3xÃ¨fÃ´Ã¥Â±Ã°pÃ´=Â¶eÂ£â€Â²edguâ€Â¯Â 4Ã³></rÃ±Ã¬Ã´7Â§Ã Ã»6Ã§ï¿½xwÃ¦Ã®Ã¦Ã®j></mrz{xÅ¡orÃŸâ‚¬Â´qÃ°â€œÂ ></kÃ£Â°â€¢zâ€¡Ã¨ÃºÃ¨@dzÃ½ï¿½^_,Â¬Ã¿Ã¢"uz+:Ã¨jÂ´Â¬jÂªÂ¡Å¡Â¸sÂ§Ã°7Â´'Ã¤Â£Â©Ã¨Ã®â€šÃ¼ÃºÃ­dÃº*Ã£Ã¶,Å¡Â¾â‚¬Å¾Âºï¿½aj></kyÂ¥â€sâ€°Â£â€“Ã±(Â¾oÂ·qÃ¨8aÃ¸Ã³Ã¿Ã´ï¿½vÃ¤?â€˜Â¬Ã¶Ã¨Â°Ã¨Â¿Âº4rÂ¶â€”â€™Â¼Â¢â€¡+ciÂ¬Ã¨Ã²Å“sÂ·Â½dÃ­Ã®Ã¤Ã®Ã½Â¡Ã¾jÃ£)Ã«Ã»Æ’Ã¸'Ã¼â€˜Ã¯Â¿#â€ jzÂ©Å¾â€šÅ¡bÂ¯at></uÃ¾Ã´ÂµÂ§Ã»gb3ï¿½=Ã³Ã©Â³Â§fmÃ¿Ãµâ€”Ã¹Ã±Â³Â¥__|Â¾Ã³Ã½Â¯Â¿x8â€ºÃ³4Ã·ï¿½Â¹Ã»ï¿½Ã¡y=Ã³ï¿½Â¼d></iÂ©Â¶Â¯b|[=sÂ£kÂ¯Ã¸Ã¤Â¢Ã«gÅ¾w3></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://research.vu.nl/ws/portalfiles/portal/42182698/complete+dissertation.pdf">https://research.vu.nl/ws/portalfiles/portal/42182698/complete+dissertation.pdf</a></em></p>]]>
            </description>
            <link>https://research.vu.nl/ws/portalfiles/portal/42182698/complete+dissertation.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24705036</guid>
            <pubDate>Wed, 07 Oct 2020 03:30:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[USB3: Why it's a bit harder than USB2]]>
            </title>
            <description>
<![CDATA[
Score 231 | Comments 145 (<a href="https://news.ycombinator.com/item?id=24704298">thread link</a>) | @panic
<br/>
October 6, 2020 | https://lab.ktemkin.com/post/why-is-usb3-harder/ | <a href="https://web.archive.org/web/*/https://lab.ktemkin.com/post/why-is-usb3-harder/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    

    <p>A few people on twitter have asked me to explain why the USB3 winds up being much harder to implement than USB2.
The answer is more than will fit in a single tweet, so I thought I'd put a quick-but-rough answer, here. This is
by no means comprehensive; consider it <del>a longer tweet</del> what a tweet would be given I had more than 240 characters and a proclivity to babble. (I do.)</p>
<p>A lot of the challenges come from the way we work around <em>physical-layer</em> limitations. Put poetically, physics gives
us lots of little obstacles we have to work around in order to talk at 5 billion transfers per second (5GT/s).</p>
<h5 id="its-hard-to-establish-common-dc-operating-conditions-on-both-sides-of-a-link">It's hard to establish common â€œDC operating conditionsâ€ on both sides of a link.</h5>
<p>It's not trivial to get the same bias voltages â€“ and common grounds â€“ across a long motherboard or down a cable â€“ and when you're operating at really high frequencies, you're a lot more sensitive to changes in your operating environment. In USB3, we work around this by <em>capacitively isolating</em> both sides of the link from each other â€“ in short, we use capacitors to ensure only signal <em>changes</em> are carried across the link, which means that both sides can establish their own local operating conditions.</p>
<figure>
    <img src="https://lab.ktemkin.com/post-media/why-is-usb3-harder/circuit.png" alt="diagram showing the transmitter is connected to the receiver through a pair of AC coupling capacitors"> <figcaption>
            <h4>From the USB3.2 specification: diagram showing how signals are isolated</h4>
        </figcaption>
</figure>

<p>This puts some requirements on the digital protocols used to exchange data. Because data currents are exchanged as the relevant capacitors charge and discharge, <em>capacitive coupling</em> only works when those capacitors have room to charge and discharge. <strong>This means our data must be DC-balanced; we have to spend as much time charging those capacitors as we do discharging them</strong>. In digital terms, this means we have to encode the data in a way that sends the same amount of <code>1</code>s and <code>0</code>s.</p>
<h5 id="its-hard-to-establish-a-common-clock-across-both-sides-of-a-link">It's hard to establish a â€œcommon clockâ€ across both sides of a link.</h5>
<p>When sending serial data, you typically have two challenges: you need to make sure both sides are sampling the data <em>at the same rate</em>, and that both sample clocks are <em>synchronized enough</em> that you're sampling at the right point. Many high-speed protocols deal with this using a technique called <em>clock recovery</em>, which essentially means that each receiver looks at the data it receives and tries to figure out what the clock that produced it looks like.</p>
<p>If both sides have agreed on a clock rate, this can be simple, in theory: if the receiver sees a change in its
received data, it can infer that that changed happened <em>on an active edge of the transmitter's clock</em>, and so it can start to figure out how to align its internal clock with the transmitter's.</p>
<p>This introduces another protocol requirement: <strong>for <em>clock recovery</em> to work, the data has to change frequently enough that the two sides can keep synchronized</strong>. At 5GT/s and high data throughputs, there's not much time for clocks to become synchronized when a packet is received; accordingly, it's important that data is encoded with lots of transitions, even when the line is idle.</p>
<p><strong>To ensure both <em>DC-Balance</em> and <em>sufficient transition density</em>, USB3 uses a method of encoding called 8b10b encoding.</strong>
In this encoding scheme, every single byte of data is transmitted as ten bits, with encodings chosen so that:</p>
<ul>
<li>A typical data byte can be transmitted <em>either</em> as a code with <em>one more one than zero</em>, or <em>one more zero than one</em>.
This allows the transmitter to choose between the two encodings, in order to keep the data stream at 50% ones.</li>
<li>Every valid encoding has sufficient <em>transition density</em> to ensure that it's useful for clock recovery.</li>
</ul>
<p>I won't go into more 8b10b background here, but you can read about the typical IBM implementation <a href="https://en.wikipedia.org/wiki/8b/10b_encoding">on wikipedia</a>.</p>
<h5 id="its-hard-to-run-both-sides-of-the-link-at-the-same--clock-rate-">It's hard to run both sides of the link at the same <em>clock rate</em>.</h5>
<p>Even with successful <em>clock recovery</em>, it's difficult to have both sides of the link produce and consume data at
the same rate. Each side's internal logic is running off of its own <em>clock source</em>; and every clock has a bit of deviation from its nominal frequency. For the protocol to function despite these differences, the USB3 specification allows each clock to deviate from its nominal value by up to a certain <em>tolerance</em>; and specifies a method for compensating for this tolerance. This technique is appropriately named <em>clock tolerance compensation</em>, or CTC.</p>
<p><strong>To compensate for mismatches in sender/receiver clock rates, USB3 requires senders to periodically insert filler data into their transmitted data-stream</strong>. Receivers can then discard this data; allowing a brief pause in which the slower
side of the link can â€œcatch upâ€. For this to be useful, the filler data (called â€˜skip setsâ€™) must be sent regularly;
which means additional logic on the transmitter side for insertion, and additional logic on the receiver side for
removal.</p>
<h5 id="its-hard-to-deal-with-varying-electrical-properties-of-different-transmitters-receivers-and-cables">It's hard to deal with varying electrical properties of different transmitters, receivers, and cables.</h5>
<p>When operating at very high frequencies, all of the little non-idealities along your transmission path really add up. At slower data rates, there's plenty of time for digital signals to â€œsettleâ€ after a change; making the non-ideal properties of your transmission lines less important. The faster your data gets, the more important it is for your data
to reach a â€œreadableâ€ value quickly.</p>
<p>To help with this, most high-speed receivers employ a technique called <em>receiver equalization</em>, which uses analog hardware
to help reshape signal transitions, so they can be more reliably sampled. Equalization helps to â€œcancel outâ€ some of the ways the non-ideal transmission path adversely affects the signal.</p>
<figure>
    <img src="https://lab.ktemkin.com/post-media/why-is-usb3-harder/eye.png" alt="diagram showing a variety of slow rises and falls; illustrating that the physical link slows transitions"> <figcaption>
            <h4>From the USB3.2 specification: an 'eye diagram', which shows an overlay of many rising and falling transitions, illustrating how non-ideal properties affect the link.</h4>
        </figcaption>
</figure>

<p>Since every transmission path is different â€“ due to different transmitter, receiver, and cable properties â€“ it's impossible to create a single â€œone size fits allâ€ equalizer. Instead, each USB3 equalizer needs to be tuned to its transmission path via a process called <em>link training</em>.</p>
<p><strong>At the start of each USB3 communication, link partners repeatedly exchange collections of known data called <em>training sets</em>, which give the opportunity for each side to tune their equalizer.</strong> Training sets include both sets of data chosen to have high transition density and sets designed to include a wide range of â€œnormally-distributedâ€ data.</p>
<p>During a few milliseconds of data exchange â€“ an eternity in fast-protocol terms â€“ both sides of the link gradually
tweak their equalizer settings until they're clearly seeing the expected values from the other side.</p>
<h5 id="its-hard-not-to-generate-harmful-interference">It's hard not to generate harmful interference.</h5>
<p>USB3 has a very high transition rate â€“ it easily qualifies as high radio-frequency signaling â€“ and its link
often tends to exchange repeating data. This has a nasty side effect: even a well-functioning link can act as an
antenna; unintentionally emitting RF that can interfere with nearby systems. The more repeating elements this signaling
has, the more troublesome the interference tends to be.</p>
<p><strong>To reduce the amount of harmful interference generated, USB3 links use a technique called <em>scrambling</em>, in which data is XOR'd with a fixed pattern before transmission.</strong> The receiver is then capable of applying the same transform to <em>descramble</em> the data stream, recovering the relevant data.</p>
<p>You can think of scrambling as being very similar to encryption â€“ except everyone knows the key. Once data is scrambled, it looks a lot more like â€œrandom numbersâ€ than the pre-scrambling data â€“ and accordingly, it's a lot less likely to
generate troublesome interference. Once the scrambled data travels the link, it can be <em>descrambled</em> by the receiving end â€“ a process similar to decryption â€“ restoring the original data stream.</p>
<h5 id="in-summary">In summaryâ€¦</h5>
<p>In summary, before you can even exchange meaningful data, the digital side of your device needs:</p>
<ul>
<li><strong>8b10b encoding and decoding hardware</strong>, so the data exchanged is <em>DC-balanced</em> and contains sufficient transitions as to allow <em>clock recovery</em>;</li>
<li><strong>Clock Tolerance Compensation hardware</strong>, which allows the two sides to communicate even with slightly-varying clock frequencies;</li>
<li>Hardware to orchestrate <strong>link training</strong> and <strong>receiver equalization</strong>, which helps to deal with non-ideal transmission properties;</li>
<li><strong>Scrambling</strong> and <strong>descrambling</strong> hardware, which help to reduce harmful interference.</li>
</ul>
<p>This omits a few minor things, such as USB3's <em>Low Frequency Periodic Signaling</em>; but these are the major components.</p>
<h5 id="oh-and-one-more-thing-its-hard-to-get-good-resources">Oh, and one more thing: it's hard to get good resources.</h5>
<p>Finally, ignoring all the physical layer challenges associated with bringing a link up, there's one more major obstacle: it's hard to get good resources for working with USB3:</p>
<ul>
<li>Most hardware enabling custom USB designs is expensive; and <a href="https://lab.ktemkin.com/post/ab07-usb3fmc-wtf/">still rife with issues</a>.</li>
<li>Most USB3 tooling is <a href="https://www.totalphase.com/products/beagle-usb5000-v2-ultimate/">very expensive</a>, and still rife with issues.</li>
<li>There's very little documentation in support of the specification; and what documentation exists still hasn't been
used enough to <a href="https://lab.ktemkin.com/post/mindshare-usb3/">identify all of its errors</a>.</li>
</ul>
<p>Hopefully, at some point, I'll have built enough tooling to change this.</p>


  </article></div>]]>
            </description>
            <link>https://lab.ktemkin.com/post/why-is-usb3-harder/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24704298</guid>
            <pubDate>Wed, 07 Oct 2020 01:11:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Thoughts on Meaning and Writing]]>
            </title>
            <description>
<![CDATA[
Score 24 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24703646">thread link</a>) | @exolymph
<br/>
October 6, 2020 | https://dormin.org/2020/10/06/thoughts-on-meaning-and-writing/ | <a href="https://web.archive.org/web/*/https://dormin.org/2020/10/06/thoughts-on-meaning-and-writing/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-870">

	
	<!-- .entry-header -->


			<div>

			
<p>I was once at a dinner party and someone was telling me about her recent trip to Arizona. One of the highlights was visiting a biodome project where scientists had attempted to achieve a totally self-sustained structure in preparation for the colonization of other planets. I told her that Steve Bannon used to run that place. She didnâ€™t believe me. I obnoxiously pulled out my phone and showed her Bannonâ€™s Wikipedia page.</p>



<p>Steve Bannon has lived a fascinating life. Thatâ€™s not an evaluation of his politics or morality, itâ€™s a statement of fact. Witness Bannonâ€™s life in bullet points:</p>



<ul><li>Born in 1953 in Norfolk, Virginia to a telephone lineman and a housewife</li><li>Attended military prep school and then Virginia Tech for a degree in Urban Planning, was elected president of the student body, worked in a junk yard</li><li>Served in the navy for seven years in the Pacific fleet</li><li>While in the navy, earned a Masters in National Security Studies from Georgetown</li><li>After leaving the navy, earned an MBA from Harvard</li><li>Got a job at Goldman Sachs as an investment banker in the mergers &amp; acquisitions division, worked way up to a Vice President position</li><li>Left Goldman Sachs with some colleagues to launch Bannon &amp; Co., a boutique investment bank specializing in media, nabbed a small slice of syndication rights to mega-hit tv show <strong>Seinfeld</strong>, still receives residual payments from the show to this day</li></ul>



<ul><li>While running Bannon &amp; Co., became the Acting Director of Biosphere 2 in Arizona, a project to design a self-sustaining habitat in preparation for the colonization of other planets</li><li>Dove into film production, executive produced 18 Hollywood films</li><li>Wrote and directed 12 documentaries</li><li>With an investment from Goldman Sachs, founded Internet Gaming Media, a World of Warcraft gold mining operation</li><li>Married and divorced three times, has three daughters</li><li>Charged with misdemeanor domestic violence, battery &amp; assault, and dissuading a witness based on accusations from his wife</li><li>Co-founded Government Accountability Institute, a conservative think tank</li><li>Was senior vice president of Cambridge Analytica, the company known for its influence in the 2016 presidential election and Brexit referendum</li><li>On the founding board of Breitbart News, served as its Editor-in-Chief and eventual Executive Chair, hosted its radio show</li><li>Became Chief Executive of Trumpâ€™s presidential campaign 88 days before Election Day, was considered the only person on Trumpâ€™s staff (including Trump himself) who thought he could win</li><li>Is generally considered the ideological mastermind behind the â€œTrumpismâ€ ideology â€“ populist, anti-globalist, culturally conservative, economically liberal</li><li>Served as Chief Strategist in Trumpâ€™s White House</li><li>After leaving the White House, built an international â€œinfrastructureâ€ for Trumpism by supporting the formation of a dozen significant political parties across Europe</li><li>Formed a partnership with outlaw Chinese billionaire Guo Wengui, pronounced the formation of an independent Chinese state to overthrow the current communist regime</li><li>Recently arrested for alleged fraud and money laundering connected to the We Build the Wall campaign, currently awaiting trial</li></ul>



<p>Again, Iâ€™m not making any moral judgement on Bannon or any of particular actions. Iâ€™m just amazed that he has done <em>so much</em>. Almost every one of these bullet points would be one of the most significant events in a normal personâ€™s life.</p>



<p>Arnold Schwarzeneggerâ€™s life in bullet points is also incredible:</p>



<ul><li>Born in 1947 in rural Austrian poverty</li><li>Father was a physically abusive (willing) ex-Nazi wounded at Stalingrad who supposedly resented his son for suspected illegitimacy</li><li>Brother died in car crash while drunk driving, left behind a three-year-old son whom Schwarzenegger would later support</li><li>Started lifting weights at age 14 or 15</li><li>Served in the Austrian army for mandatory one year of service</li><li>While in the army, won Junior Mr. Europe weightlifting contest, was arrested and imprisoned for a week for going AWOL to attend the competition</li><li>Moved to London in his late teens to live and train with a weightlifting judge</li><li>At age 20, became the youngest Mr. Universe winner ever, first won the amateur contest, then won the professional contest three years in a row</li><li>Moved to Los Angeles, most likely lived as an illegal immigrant for years</li><li>First won Mr. Olympia at age 23, youngest ever; won contest six more times; became known as one of the greatest bodybuilders of all time</li><li>Attended classes at three different colleges to attain a degree in Business Administration and Marketing</li><li>Appeared in a handful of action films, was told by agents that his body was too â€œweird,â€ accent too thick, and name too long</li><li>Started a surprisingly successful bricklaying business and a mail-order business with another bodybuilder, also invested in numerous real estate companies</li><li>Became a millionaire by age 30</li><li>Starred in <strong>Pumping Iron</strong> and then <strong>Conan the Barbarian</strong>, breakthrough roles</li><li>Starred in <strong>Terminator, Predator, Commando, Running Man, Total Recall, Kindergarten Cop, Terminator 2, True Lies, </strong>and became known as one of the great action stars ever</li><li>Invested in Planet Hollywood, a shopping mall in Ohio, a restaurant, Dimensional Fund Advisors, a movie production company, numerous fitness magazines, and a fitness competition</li><li>Appointed Chairman of President Clintonâ€™s Council of Physical Fitness and Sports</li><li>Ran for Governor of California (the most populous and wealthiest state in the wealthiest country in the world) and won despite having no political experience</li><li>Won re-election as a Republican during the height of anti-Bush sentiment</li><li>Went back to acting and continues to star in movies to this day</li><li>Married and divorced a Kennedy, had four children with her, plus another child with his housekeeper</li><li>Current net worth estimated at $100-200 million, may have peaked at $800 million before the divorce</li></ul>



<p>Iâ€™ve read a lot of Wikipedia pages and I find they all end up as bullet points in my head. Thatâ€™s kind of how I think about everythingâ€¦ as lists of the most important things. Everything else fades away over time.</p>



<p>As you can tell from this blog, Iâ€™ve spent a lot of time thinking about <em>great</em> people. Napoleon, Cortes, Genghis Khan, Mark Schilling, Hideo Kojima, and Tommy Wiseau are not all good people, but they are <em>great</em>. Each one could easily earn a bullet point list like Bannonâ€™s and Schwarzeneggerâ€™s. Theyâ€™ve all built lives full of notable activities and accomplishments which will live on in history in one way or another.</p>



<p>I donâ€™t think Iâ€™ll live on in history for very long after I die except for boring stuff like my tax records. And Iâ€™m fine with that. Itâ€™s not clear that living a historically noteworthy life is desirable in and of itself.</p>



<p>But I do want a noteworthy life on my own terms. I want my life to resemble one of these bullet point lists within the reasonable bounds of what I can experience and achieve given my abilities, resources, and will. Especially when Iâ€™m older, I want to be able to sit at a computer and type out the actions, events, and people that I remember and be proud of the list before me.</p>



<p>This is my personal heuristic for <em>meaning</em> in life. Both in the short and long term I try to do things which one day could be put on a bullet point list about me. Or maybe I just try to do things Iâ€™ll <em>remember</em>.</p>



<p>Pick a random year from your life and try to write a bullet point list of things that happened that year. If I do that for any year in the past decade, I usually getâ€¦</p>



<ul><li>The girl I was dating at the time (if any)</li><li>Places I traveled</li><li>The job I had (though I rarely remember any particular thing I did in that year on the job)</li><li>Significant money I made or lost</li><li>Significant events experienced with friends</li><li>Itâ€™s a bit harder to pinpoint specific years, but I can always remember the general era when I experienced particular passion projects/activities, like movies/tv shows/books/video games/athletics, etc. For instance, I distinctly remember playing <em>Skyrim</em> in college and <em>Faster than Light</em> while at the office in 2015.</li></ul>



<p>So what I tend to remember are relationships, work, money, and passion projects. I donâ€™t think thatâ€™s uncommon, though the weight I give each one might be. Regardless, these are the things that matter to me. They should be my major goals in life. They are what Iâ€™ll look back on and remember when Iâ€™m 30, 40, 50, and close to death. Hopefully.</p>



<p>Thereâ€™s this Anthony Bourdain quote I like:</p>



<blockquote><p>â€œI understand thereâ€™s a guy inside me who wants to lay in bed, smoke weed all day, and watch cartoons and watch old movies. My whole life is a series of stratagems to avoid and outwit that guy.â€</p></blockquote>



<p>I very much sympathize. Though my version of that is sitting on the couch, watching old episodes of <strong>Archer</strong>, and playing my hundredth <strong>Crusader Kings 2</strong> campaign.</p>



<p>I think what Bourdainâ€™s struggle comes down to is finding meaning every day. Itâ€™s the easiest thing in the world to let each day go by filled with nothing but obligations and short term stimulation. Nothing long-term is gained, nothing is remembered, and when you wake up the next morning youâ€™re a day older with nothing important added to your existence. This will be 99.9% of the days in your life.</p>



<p>Resisting this reality is literally a constant challenge. One way I try to resist is to consider which specific actions which will result in discrete memorable experiences. That is, I try to do things Iâ€™ll remember even if I donâ€™t remember the day itself. For instance, what am I more likely to remember doing tonight: playing another three out of literally thousands of hours of <strong>Crusader Kings 2</strong>, or watching a new movie?</p>



<p>The movie, of course. Even if the movie is bad, or boring, or forgettable, I will still vaguely recall having seen it years from now, but there is almost no chance Iâ€™ll remember those random three hours of <strong>Crusader Kings 2</strong> regardless of how much I enjoyed them. </p>



<p>Alternatively, you can deal with Bourdainâ€™s problem by trying to make small contributions to greater acts of meaning each day. Iâ€™ve kept an IMDB account since I was â€¦</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dormin.org/2020/10/06/thoughts-on-meaning-and-writing/">https://dormin.org/2020/10/06/thoughts-on-meaning-and-writing/</a></em></p>]]>
            </description>
            <link>https://dormin.org/2020/10/06/thoughts-on-meaning-and-writing/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24703646</guid>
            <pubDate>Tue, 06 Oct 2020 23:25:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I will never buy another CyberPower UPS]]>
            </title>
            <description>
<![CDATA[
Score 25 | Comments 20 (<a href="https://news.ycombinator.com/item?id=24703593">thread link</a>) | @monstermunch
<br/>
October 6, 2020 | https://blog.networkprofile.org/cyberpower-ups-avoid/ | <a href="https://web.archive.org/web/*/https://blog.networkprofile.org/cyberpower-ups-avoid/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://blog.networkprofile.org/content/images/size/w300/2020/10/2020-10-06-16.50.41-1.JPG 300w,
                            https://blog.networkprofile.org/content/images/size/w600/2020/10/2020-10-06-16.50.41-1.JPG 600w,
                            https://blog.networkprofile.org/content/images/size/w1000/2020/10/2020-10-06-16.50.41-1.JPG 1000w,
                            https://blog.networkprofile.org/content/images/size/w2000/2020/10/2020-10-06-16.50.41-1.JPG 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://blog.networkprofile.org/content/images/size/w2000/2020/10/2020-10-06-16.50.41-1.JPG" alt="Cyberpower UPS's - Try something else...">
            </figure>

            <section>
                <div>
                    <p>This is now the fourth time I have come across these issues with a Cyberpower UPS. It means your UPS really isn't a UPS, so you may want to think twice about picking one up</p><p>First thing to keep in mind is that these are line interactive UPS's, that means the input power isn't going through the batteries at all like with a double conversion UPS, but its just passing right through to your devices while maybe doing some minor filtering and surge protection. These devices simply do not need batteries in them to provide output power</p><h2 id="issue-1">Issue 1</h2><p>What happens is that the UPS will work fine, until it can no longer charge the batteries. When that happens, the UPS will both shut down output power (Even though the power from the wall is fine) and continue to try and charge the batteries while giving you an error code E24 that says "Internal Fault - CONTACT SUPPORT" in the manual </p><p>If you do contact support, they tell you that the UPS needs to be replaced entirely. What it really means is that you have bad batteries, and for around $40 you can be back in business.</p><p>They also claim that the reason it shutdown output power is for safety reasons, but then why does it try and charge the batteries still? Your guess is as good as mine. Even worse, when you turn the UPS off and take the batteries out, they are burning hot, because the UPS is trying its best to charge bad batteries.</p><h2 id="issue-2">Issue 2</h2><p>The second issue with them is that when the batteries are clearly bad and cannot supply a load, but still give a nominal 12v charge, the UPS thinks its fine. Even if it fails the self test, it reports that is has succeeded.</p><p>I have a small USB LED Light that probably doesn't even draw 1w. If I initiate a self test, the UPS just shuts off completely, and when powered back on, reports everything is great, and the software says the test passed. Awesome!</p><figure><img src="https://blog.networkprofile.org/content/images/2020/10/2020-10-06-16.34.55.JPG" alt="" srcset="https://blog.networkprofile.org/content/images/size/w600/2020/10/2020-10-06-16.34.55.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2020/10/2020-10-06-16.34.55.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2020/10/2020-10-06-16.34.55.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2020/10/2020-10-06-16.34.55.JPG 2400w" sizes="(min-width: 720px) 720px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2020/10/2020-10-06-16_46_30-PowerPanel--Personal-_-CyberPower-Systems.png" alt="" srcset="https://blog.networkprofile.org/content/images/size/w600/2020/10/2020-10-06-16_46_30-PowerPanel--Personal-_-CyberPower-Systems.png 600w, https://blog.networkprofile.org/content/images/2020/10/2020-10-06-16_46_30-PowerPanel--Personal-_-CyberPower-Systems.png 861w" sizes="(min-width: 720px) 720px"></figure><p>Every other UPS I come across would fail the self test while NOT shutting down output power, and it would alert you that you need new batteries.</p><p>Watch this video: </p><figure></figure><p>In the end of the video you can see the UPS lost connection, but that doesn't even matter as the Cyberpower software marks the test complete before that even happens, as you can see in the screenshot I posted above the video.</p><p>This issue is compounded by the fact that the UPS completely yet again turns itself off. That means if you had a PC plugged into the UPS, and another plugged into the wall and had a very minor power dip, the one directly in the wall would probably remain on, and the one plugged into the UPS would be turned off.</p><p>Doesn't that defeat the entire point of a UPS? If you have one of these units, you need to be prepared that you will lose power unexpectedly at some point. The fact you can't test the batteries without losing power means that you will either need to shutdown your computer/servers/etc every 2 weeks and test, or just replace the batteries WELL ahead of schedule, say every 2 years just to avoid losing power.</p><p>A better solution would be to toss the UPS, and get a better one </p><p>I have found this issue in every single Cyberpower UPS I have ever used which is 4 units in the PFCLCD series and AVRLCD series</p><p>Just avoid them entirely, and go with a different brand.</p>
                </div>
            </section>



        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://blog.networkprofile.org/cyberpower-ups-avoid/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24703593</guid>
            <pubDate>Tue, 06 Oct 2020 23:19:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[DOMPurify bypass: XSS via HTML namespace confusion]]>
            </title>
            <description>
<![CDATA[
Score 159 | Comments 80 (<a href="https://news.ycombinator.com/item?id=24703230">thread link</a>) | @fanf2
<br/>
October 6, 2020 | https://research.securitum.com/mutation-xss-via-mathml-mutation-dompurify-2-0-17-bypass/ | <a href="https://web.archive.org/web/*/https://research.securitum.com/mutation-xss-via-mathml-mutation-dompurify-2-0-17-bypass/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1639">
	<!-- .entry-header -->

	
	<div>
		
<p>In this blogpost Iâ€™ll explain my recent bypass in <a href="https://github.com/cure53/DOMPurify/">DOMPurify</a> â€“ the popular HTML sanitizer library. In a nutshell, DOMPurifyâ€™s job is to take an untrusted HTML snippet, supposedly coming from an end-user, and remove all elements and attributes that can lead to Cross-Site Scripting (XSS).</p>



<p>This is the bypass:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f81387a87475379968139" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>form</span><span>&gt;</span></p><p><span>&lt;</span><span>math</span><span>&gt;</span><span>&lt;</span><span>mtext</span><span>&gt;</span></p><p><span>&lt;</span><span>/</span><span>form</span><span>&gt;</span><span>&lt;</span><span>form</span><span>&gt;</span></p><p><span>&lt;</span><span>mglyph</span><span>&gt;</span></p><p><span>&lt;</span><span>style</span><span>&gt;</span><span>&lt;</span><span>/</span><span>math</span><span>&gt;</span><span>&lt;</span><span>img </span><span>src </span><span>onerror</span><span>=</span><span>alert</span><span>(</span><span>1</span><span>)</span><span>&gt;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0003 seconds] -->




<p>Believe me that thereâ€™s not a single element in this snippet that is superfluous ğŸ™‚ </p>



<p>To understand why this particular code worked, I need to give you a ride through some interesting features of HTML specification that I used to make the bypass work.</p>



<h2>Usage of DOMPurify</h2>



<p>Letâ€™s begin with the basics, and explain how DOMPurify is usually used. Assuming that we have an untrusted HTML in <code>htmlMarkup</code> and we want to assign it to a certain <code>div</code>, we use the following code to sanitize it using DOMPurify and assign to the <code>div</code>:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f81387a8747b020708679" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>div</span><span>.</span><span>innerHTML</span><span> </span><span>=</span><span> </span><span>DOMPurify</span><span>.</span><span>sanitize</span><span>(</span><span>htmlMarkup</span><span>)</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0001 seconds] -->




<p>In terms of parsing and serializing HTML as well as operations on the DOM tree, the following operations happen in the short snippet above:</p>



<ol><li><code>htmlMarkup</code> is parsed into the DOM Tree.</li><li>DOMPurify sanitizes the DOM Tree (in a nutshell, the process is about walking through all elements and attributes in the DOM tree, and deleting all nodes that are not in the allow-list).</li><li>The DOM tree is serialized back into the HTML markup.</li><li>After assignment to <code>innerHTML</code>, the browser parses the HTML markup again.</li><li>The parsed DOM tree is appended into the DOM tree of the document.</li></ol>



<p>Letâ€™s see that on a simple example. Assume that our initial markup is <code>A&lt;img src=1 onerror=alert(1)&gt;B</code>. In the first step it is parsed into the following tree:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-1-1024x104.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-1-1024x104.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-1-300x30.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-1-768x78.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-1-1536x155.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-1-1320x134.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-1.png 1956w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Then, DOMPurify sanitizes it, leaving the following DOM tree:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-2-1024x107.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-2-1024x107.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-2-300x31.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-2-768x80.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-2-1536x161.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-2-1320x138.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-2.png 1952w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Then it is serialized to:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		
<!-- [Format Time: 0.0001 seconds] -->




<p>And this is what <code>DOMPurify.sanitize</code> returns. Then the markup is parsed again by the browser on assignment to innerHTML:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-3-1024x107.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-3-1024x107.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-3-300x31.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-3-768x80.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-3-1536x161.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-3-1320x138.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-3.png 1952w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The DOM tree is identical to the one that DOMPurify worked on, and it is then appended to the document.</p>



<p>So to put it shortly, we have the following order of operations: <strong>parsing â¡ï¸ serialization â¡ï¸ parsing</strong>. The intuition may be that serializing a DOM tree and parsing it again should always return the initial DOM tree. But this is not true at all. Thereâ€™s even <a href="https://html.spec.whatwg.org/multipage/parsing.html#serialising-html-fragments:escapingString-3:~:text=It%20is%20possible%20that%20the%20output,not%20return%20the%20original%20tree%20structure">a warning in the HTML spec</a> in a section about serializing HTML fragments:</p>



<blockquote><p>It is possible that the output of this algorithm [serializing HTML], if parsed with an HTML parser, will not return the original tree structure. <strong>Tree structures that do not roundtrip a serialize and reparse step can also be produced by the HTML parser itself</strong>, although such cases are typically non-conforming.</p></blockquote>



<p>The important take-away is that serialize-parse roundtrip is not guaranteed to return the original DOM tree (this is also a root cause of a type of XSS known as <strong>mutation XSS</strong>). While usually these situations are a result of some kind of parser/serializer error, there are at least two cases of spec-compliant mutations.</p>



<h2>Nesting FORM element</h2>



<p>One of these cases is related to the FORM element. It is quite special element in the HTML because it cannot be nested in itself. The specification is explicit that<a href="https://html.spec.whatwg.org/#the-form-element"> it cannot have any descendant that is also a FORM</a>:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-4-1024x279.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-4-1024x279.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-4-300x82.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-4-768x209.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-4-1536x419.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-4-1320x360.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-4.png 1936w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>This can be confirmed in any browser, with the following markup:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f81387a87483005405282" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>form1</span><span>&gt;</span></p><p><span>INSIDE_FORM1</span></p><p><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>form2</span><span>&gt;</span></p><p><span>INSIDE_FORM2</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0001 seconds] -->




<p>Which would yield the following DOM tree:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-5-1024x80.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-5-1024x80.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-5-300x24.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-5-768x60.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-5-1536x121.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-5-1320x104.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-5.png 1960w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The second <code>form</code> is completely omitted in the DOM tree just as it wasnâ€™t ever there.</p>



<p>Now comes the interesting part. If we keep reading the HTML specification, it actually gives <a href="https://html.spec.whatwg.org/multipage/parsing.html#serialising-html-fragments:the-script-element-4:~:text=DOM.-,For%20example%2C%20consider%20the%20following%20markup%3A,%3Cform">an example</a> that with a slightly broken markup with mis-nested tags, it is possible to create nested forms. Here it comes (taken directly from the spec):</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f81387a8748a843288668" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>"outer"</span><span>&gt;</span><span>&lt;</span><span>div</span><span>&gt;</span><span>&lt;</span><span>/</span><span>form</span><span>&gt;</span><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>"inner"</span><span>&gt;</span><span>&lt;</span><span>input</span><span>&gt;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0001 seconds] -->




<p>It yields the following DOM tree, which contains a nested form element:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-6-1024x141.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-6-1024x141.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-6-300x41.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-6-768x106.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-6-1536x211.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-6-1320x182.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-6.png 1948w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>This is not a bug in any particular browser; it results directly from the HTML spec, and is described in the algorithm of parsing HTML. Hereâ€™s the general idea:</p>



<ul><li>When you open a <code>&lt;form&gt;</code> tag, the parser needs to keep record of the fact that it was opened with a <strong>form element pointer</strong> (thatâ€™s how itâ€™s called in the spec). If the pointer is not <code>null</code>, then <code>form</code> element cannot be created.</li><li>When you end a <code>&lt;form&gt;</code> tag, the form element pointer is always set to <code>null</code>. </li></ul>



<p>Thus, going back to the snippet:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f81387a8748c911235721" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>"outer"</span><span>&gt;</span><span>&lt;</span><span>div</span><span>&gt;</span><span>&lt;</span><span>/</span><span>form</span><span>&gt;</span><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>"inner"</span><span>&gt;</span><span>&lt;</span><span>input</span><span>&gt;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0001 seconds] -->




<p>In the beginning, the form element pointer is set to the one with <code>id="outer"</code>. Then, a <code>div</code> is being started, and the <code>&lt;/form&gt;</code> end tag set the form element pointer to <code>null</code>. Because itâ€™s <code>null</code>, the next form with <code>id="inner"</code> can be created; and because weâ€™re currently within <code>div</code>, we effectively have a <code>form</code> nested in <code>form</code>.</p>



<p>Now, if we try to serialize the resulting DOM tree, weâ€™ll get the following markup:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f81387a87490157897589" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>"outer"</span><span>&gt;</span><span>&lt;</span><span>div</span><span>&gt;</span><span>&lt;</span><span>form </span><span>id</span><span>=</span><span>"inner"</span><span>&gt;</span><span>&lt;</span><span>input</span><span>&gt;</span><span>&lt;</span><span>/</span><span>form</span><span>&gt;</span><span>&lt;</span><span>/</span><span>div</span><span>&gt;</span><span>&lt;</span><span>/</span><span>form</span><span>&gt;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0001 seconds] -->




<p>Note that this markup no longer has any mis-nested tags. And when the markup is parsed again, the following DOM tree is created:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-7-1024x101.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-7-1024x101.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-7-300x30.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-7-768x76.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-7-1536x151.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-7-2048x202.png 2048w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-7-1320x130.png 1320w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>So this is a proof that serialize-reparse roundtrip is not guaranteed to return the original DOM tree. And even more interestingly, this is basically <strong>a spec-compliant mutation</strong>.</p>



<p>Since the very moment I was made aware of this quirk, Iâ€™ve been pretty sure that it must be possible to somehow abuse it to bypass HTML sanitizers. And after a long time of not getting any ideas of how to make use of it, I finally stumbled upon another quirk in HTML specification. But before going into the specific quirk itself, letâ€™s talk about my favorite Pandoraâ€™s box of the HTML specification: foreign content.</p>



<h2>Foreign content</h2>



<p>Foreign content is a like a Swiss Army knife for breaking parsers and sanitizers. I used it in my <a href="https://research.securitum.com/dompurify-bypass-using-mxss/">previous DOMPurify bypass</a> as well as in <a href="https://research.securitum.com/html-sanitization-bypass-in-ruby-sanitize-5-2-1/">bypass of Ruby sanitize library</a>.</p>



<p>The HTML parser can create a DOM tree with elements of three namespaces:</p>



<ul><li>HTML namespace (<code>http://www.w3.org/1999/xhtml</code>)</li><li>SVG namespace (<code>http://www.w3.org/2000/svg</code>)</li><li>MathML namespace (<code>http://www.w3.org/1998/Math/MathML</code>)</li></ul>



<p>By default, all elements are in HTML namespace; however if the parser encounters <code>&lt;svg&gt;</code> or <code>&lt;math&gt;</code> element, then it â€œswitchesâ€ to SVG and MathML namespace respectively. And both these namespaces make foreign content.</p>



<p>In foreign content markup is parsed differently than in ordinary HTML. This can be most clearly shown on parsing of <code>&lt;style&gt;</code> element. In HTML namespace, <code>&lt;style&gt;</code> can only contain text; no descendants, and HTML entities are not decoded. The same is not true in foreign content: foreign contentâ€™s <code>&lt;style&gt;</code> can have child elements, and entities are decoded.</p>



<p>Consider the following markup:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f81387a87492909307993" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;style&gt;</span><span>&lt;a&gt;</span><span>ABC&lt;/style&gt;</span><span>&lt;</span><span>svg</span><span>&gt;</span><span>&lt;</span><span>style</span><span>&gt;</span><span>&lt;</span><span>a</span><span>&gt;</span><span>ABC</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0020 seconds] -->




<p>It is parsed into the following DOM tree</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-8-1024x206.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-8-1024x206.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-8-300x60.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-8-768x154.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-8-1536x308.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-8-1320x265.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-8.png 1962w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><strong>Note:</strong> from now on, all elements in the DOM tree in this blogpost will contain a namespace. So <code>html style</code> means that it is a <code>&lt;style&gt;</code> element in HTML namespace, while <code>svg style</code> means that it is a <code>&lt;style&gt;</code> element in SVG namespace.</p>



<p>The resulting DOM tree proves my point: <code>html style</code> has only text content, while <code>svg style</code> is parsed just like an ordinary element.</p>



<p>Moving on, it may be tempting to make a certain observation. That is: if we are inside <code>&lt;svg&gt;</code> or <code>&lt;math&gt;</code> then all elements are also in non-HTML namespace. But this is not true. There are certain elements in HTML specification called <strong>MathML text integration points</strong> and <strong>HTML integration point</strong>. And the children of these elements have HTML namespace (with certain exceptions Iâ€™m listing below).</p>



<p>Consider the following example:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f81387a87496587360392" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>math</span><span>&gt;</span></p><p><span>&lt;style&gt;</span><span>&lt;/style&gt;</span></p><p><span>&lt;</span><span>mtext</span><span>&gt;</span><span>&lt;style&gt;</span><span>&lt;/style&gt;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0001 seconds] -->




<p>It is parsed into the following DOM tree:</p>



<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-9-1024x138.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-9-1024x138.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-9-300x40.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-9-768x104.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-9-1536x207.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-9-1320x178.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-9.png 1956w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Note how the <code>style</code> element that is a direct child of <code>math</code> is in MathML namespace, while the <code>style</code> element in <code>mtext</code> is in HTML namespace. And this is because <code>mtext</code> is <strong>MathML text integration points</strong> and makes the parser switch namespaces. </p>



<p>MathML text integration points are:</p>



<ul><li><code>math mi</code></li><li><code>math mo</code></li><li><code>math mn</code></li><li><code>math ms</code></li></ul>



<p>HTML integration points are:</p>



<ul><li><code>math annotation-xml</code> if it has an attribute called <code>encoding</code> whose value is equal to either <code>text/html</code> or <code>application/xhtml+xml</code></li><li><code>svg foreignObject</code></li><li><code>svg desc</code></li><li><code>svg title</code></li></ul>



<p>I always assumed that all children of MathML text integration points or HTML integration points have HTML namespace by default. How wrong was I! The HTML specification says that children of MathML text integration points are by default in HTML namespace with two exceptions: <code>mglyph</code> and <code>malignmark</code>. And this only happens if they are a direct child of MathML text integration points.</p>



<p>Letâ€™s check that with the following markup:</p>



<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-5f81387a87498853088976" data-settings=" minimize scroll-mouseover">
		
			
			
			
			<div>
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>math</span><span>&gt;</span></p><p><span>&lt;</span><span>mtext</span><span>&gt;</span></p><p><span>&lt;</span><span>mglyph</span><span>&gt;</span><span>&lt;</span><span>/</span><span>mglyph</span><span>&gt;</span></p><p><span>&lt;</span><span>a</span><span>&gt;</span><span>&lt;</span><span>mglyph</span><span>&gt;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
		</div>
<!-- [Format Time: 0.0001 seconds] -->




<figure><img src="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-10-1024x168.png" alt="" srcset="https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-10-1024x168.png 1024w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-10-300x49.png 300w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-10-768x126.png 768w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-10-1536x252.png 1536w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-10-1320x217.png 1320w, https://research.securitum.com/wp-content/uploads/sites/2/2020/10/image-10.png 1974w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Notice that <code>mglyph</code> that is a direct child of <code>mtext</code> is in MathML namespace, while the one that is a child of <code>html a</code> element is in HTML namespace.</p>



<p>Assume that we have a â€œcurrent elementâ€, and weâ€™d like determine its namespace. Iâ€™ve compiled some rules of thumb:</p>



<ul><li>Current element is in the namespace of its parent unless conditions from the points below are met.</li><li>If current element is <code>&lt;svg&gt;</code> or <code>&lt;math&gt;</code> and parent is in HTML namespace, then current element is in SVG or MathML namespace respectively.</li><li>If parent of current element is an HTML integration point, then current element is in HTML namespace unless itâ€™s <code>&lt;svg&gt;</code> or <code>&lt;math&gt;</code>.</li><li>If parent of current element is an MathML integration point, then current element is in HTML namespace unless itâ€™s <code>&lt;svg&gt;</code>, <code>&lt;math&gt;</code>, <code>&lt;mglyph&gt;</code> or <code>&lt;malignmark&gt;</code>.</li><li>If current element is one of <code>&lt;b&gt;, &lt;big&gt;, &lt;blockquote&gt;, &lt;body&gt;, &lt;br&gt;, &lt;center&gt;, &lt;codeâ€¦</code></li></ul></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://research.securitum.com/mutation-xss-via-mathml-mutation-dompurify-2-0-17-bypass/">https://research.securitum.com/mutation-xss-via-mathml-mutation-dompurify-2-0-17-bypass/</a></em></p>]]>
            </description>
            <link>https://research.securitum.com/mutation-xss-via-mathml-mutation-dompurify-2-0-17-bypass/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24703230</guid>
            <pubDate>Tue, 06 Oct 2020 22:43:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Planning to Explore via Self-Supervised World Models]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24703113">thread link</a>) | @headalgorithm
<br/>
October 6, 2020 | https://ramanans1.github.io/plan2explore/ | <a href="https://web.archive.org/web/*/https://ramanans1.github.io/plan2explore/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div max-width="100%">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  <title>Planning to Explore via Self-Supervised World Models</title>
  <meta name="HandheldFriendly" content="True">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="referrer" content="no-referrer-when-downgrade">

  <meta property="og:site_name" content="Planning to Explore">
  <meta property="og:type" content="video.other">
  <meta property="og:title" content="Planning to Explore via Self-Supervised World Models">
  <meta property="og:description" content="Sekar, Rybkin, Daniilidis, Abbeel, Hafner, Pathak. Planning to Explore via Self-Supervised World Models. ICML 2020.">
  <meta property="og:url" content="https://ramanans1.github.io/plan2explore/">
  <meta property="og:image" content="https://ramanans1.github.io/plan2explore/resources/setting.png">
  <meta property="og:video" content="https://www.youtube.com/v/GftqnPWsCWw">

  <meta property="article:publisher" content="https://github.com/ramanans1">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Planning to Explore via Self-Supervised World Models">
  <meta name="twitter:description" content="Sekar, Rybkin, Daniilidis, Abbeel, Hafner, Pathak. Planning to Explore via Self-Supervised World Models. ICML 2020.">
  <meta name="twitter:url" content="https://ramanans1.github.io/plan2explore/">
  <meta name="twitter:image" content="https://ramanans1.github.io/plan2explore/resources/setting.png">
  <!-- <meta name="twitter:label1" content="Written by" />
  <meta name="twitter:data1" content="Deepak Pathak" /> -->
  <!-- <meta name="twitter:label2" content="Filed under" />
  <meta name="twitter:data2" content="" /> -->
  <meta name="twitter:site" content="@pathak2206">
  <meta property="og:image:width" content="1600">
  <meta property="og:image:height" content="900">

  
  <meta name="twitter:card" content="player">
  <meta name="twitter:image" content="https://ramanans1.github.io/plan2explore/resources/setting.png">
  <meta name="twitter:player" content="https://www.youtube.com/embed/GftqnPWsCWw?rel=0&amp;showinfo=0">
  <meta name="twitter:player:width" content="640">
  <meta name="twitter:player:height" content="360">



      <br>
      <center><span>Planning to Explore via Self-Supervised World Models</span></center><br>
      
      
      <center><span><a href="https://icml.cc/Conferences/2020">International Conference on Machine Learning (ICML), 2020</a></span></center>

      

      <center>
      <iframe width="768" height="432" max-width="100%" src="https://www.youtube.com/embed/GftqnPWsCWw?rel=0" frameborder="0" allowfullscreen=""></iframe></center>
      

      <p>
        Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards.
      </p>
      <br><hr>

      <center></center>
      <p>
        The agent first leverages planning to explore in self-supervised manner, without task-specific rewards, to learn a global world model. After the exploration phase, it receives reward functions to adapt to multiple tasks, such as standing, walking, running, and using either zero or few tasks-specific interactions.
      </p>
      <center><a href="https://ramanans1.github.io/plan2explore/resources/setting.png"><img src="https://ramanans1.github.io/plan2explore/resources/setting.png" width="600px"></a><br></center>
      <hr>


      <center></center><br>
      <center><a href="https://ramanans1.github.io/plan2explore/resources/method.png"><img src="https://ramanans1.github.io/plan2explore/resources/method.png" width="800px"></a><br></center>
      <br><hr>


            <center id="sourceCode"></center>
            <p>
            We have released our implementation in Tensorflow on the github page. Try our code!
            </p>
            
            <br><hr>

            <center id="sourceCode"></center>
            
            <br><hr>

              <center></center>
              <table>
              <tbody><tr>
              <td>
              <!-- <p style="margin-top:4px;"></p> -->
              <a href="https://arxiv.org/pdf/2005.05960.pdf"><img src="https://ramanans1.github.io/plan2explore/resources/thumbnail.png"></a>
              <center>
              <span><a href="https://arxiv.org/pdf/2005.05960.pdf">[Paper]</a>
              <span><a href="https://arxiv.org/abs/2005.05960">[ArXiv]</a>
              <!-- <span style="font-size:20pt"><a href="resources/slides.pdf">[Slides]</a></span>
              <span style="font-size:20pt"><a href="resources/poster.pdf">[Poster]</a></span> -->
              </span></span></center>
              </td>
              <td>
              </td>
              <td>
              <!-- <p style="margin-top:4px;"></p> -->
              <p><b><span>Citation</span></b><br><span>&nbsp;<br></span> <span>Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak. <b>Planning to Explore via Self-Supervised World Models.</b> ICML 2020.</span></p>
              <!-- <p style="margin-top:20px;"></p> -->
              <span>[Bibtex]</span>
              </td>
              </tr>
              <tr>
              <td>
              </td>
              <td>
              </td>
              <td>
                <div id="plan2explore2019_bib">
<pre xml:space="preserve">@inproceedings{sekar2020planning,
    title={Planning to Explore
    via Self-Supervised World Models},
    author={Ramanan Sekar and Oleh Rybkin
    and Kostas Daniilidis and Pieter Abbeel
    and Danijar Hafner and Deepak Pathak},
    year={2020},
    Booktitle={ICML}
}</pre>
                </div>
                </td>
                </tr>
            </tbody></table>
          <br><hr>
      


</div></div>]]>
            </description>
            <link>https://ramanans1.github.io/plan2explore/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24703113</guid>
            <pubDate>Tue, 06 Oct 2020 22:29:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The discovery of a new taste cell]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24702624">thread link</a>) | @flummox
<br/>
October 6, 2020 | https://neo.life/2020/10/taste-2-0-is-here/ | <a href="https://web.archive.org/web/*/https://neo.life/2020/10/taste-2-0-is-here/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                        
<p>Years ago, <a href="https://www.mykeeper.com/profile/JohnBardi/">my father</a> took a huge slurping bite of what he thought was a fat navel orange and immediately gagged. Spitting it out, he later recalled, it was the most poisonous piece of fruit heâ€™d ever tastedâ€”or so he thought. <em>Whatâ€™s wrong with this orange</em>? he asked his companion. <em>Itâ€™s actually a grapefruit</em>, she said. And with that, the horrible taste of the rotten fruit turned delicious in his mouth, almost like the flick of a switch.</p>



<p>For some time after I heard this story, I wondered whether it would it be possible to do the same thing deliberately. If the brain can be fooled into shaping our experience of taste by accident, could you also trick it on purposeâ€”by taking a pill or painting a few drops of solution on your tongue before a meal? Can science turn the experience of eating the worst Soylent Green into one of the best crÃ¨me brÃ»lÃ©es ever?</p>



<p>This anecdote is germane to a group of researchers at the University of Buffalo in upstate New York. They recently discovered <a href="https://journals.plos.org/plosgenetics/article/authors?id=10.1371/journal.pgen.1008925">a new type of taste cell</a>â€”a discovery that has somewhat upended what we thought we knew about the human taste system.</p>



<h3>A taste of evolution</h3>



<p>The taste system helps accomplish one of lifeâ€™s oldest needs: analyzing chemicals on the tongue and detecting flavors that indicate whether something is food or foul. Even the most ancient prokaryotic organisms to emerge from the primordial ooze on the tangled banks of wherever needed to find food, and the prevailing idea in biology today is that the modern mammalian taste system evolved to serve that most fundamental of needs.</p>



  




<p>We are adept at detecting certain flavors of things in our environment that contain the nutrients we need and at detecting more abhorrent flavors of things that would make us sick. We detect sweet things quite well because sugars are a key chemical by-product of carbohydrates, which we typically consume as a basic food in our diet. We need sodium and chloride to maintain cardiovascular homeostasis, so we detect salt quite well.</p>



<p>And in genomic tribute to our forgotten, foraging past, humans are able to taste bitter flavors with finely honed accuracy. The human genome has no fewer than 25 different subtypes of <em>Tas2r</em> genes to detect bitter tastesâ€”less than frogs, which have 50 such genes, or mice, which have 35, but far more than chickens, which have only three bitter taste receptors.</p>



<p>â€œThe taste system is just absolutely required for all organisms to be able to consume the nutrients that they need and avoid the things that would make them sick,â€ says Kathryn Medler, a biologist at the University of Buffalo in New York who led the team that discovered the new taste cell.</p>



<p>As a fundamental tool for survival today, taste seems almost unnecessary for the modern humanâ€”at least anywhere one can drive past a thousand restaurants to a 24-hour grocery store bursting with food choices. We did not evolve with such abundanceâ€”but rather in a wild, dangerously fluctuating world full of frequent famine and strange, bitter things that may be toxic.</p>



<p>As a result, says Paul Breslin, a nutritional scientist at Rutgers University and researcher at the Monell Chemical Senses Center in Philadelphia, â€œweâ€™re a food chemistry analytical machine.â€</p>



<h3>Cats, cows, koalas, and dolphins</h3>



<p>Consider the cat. Cats are carnivores, and they have no need in their diet for carbohydrates or sweet things, except perhaps the occasional non-critter bon-bon. This isnâ€™t merely a question of preferenceâ€”itâ€™s evolution. Cats actually donâ€™t even have sugar receptor genes.</p>



<p>â€œThey donâ€™t even taste itâ€”theyâ€™re taste-blind to sweet,â€ says University of Miami biologist Steve Roper, another expert in the field who published a review on the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5958546/pdf/nihms942554.pdf">current state of knowledge on taste buds</a> a few years ago. If a cat licks something sweet, Roper added, itâ€™s because they detect the fat in that food and not the sugar.</p>



<figure><blockquote><p>What would it be like to eat meat with a catâ€™s tongue?</p></blockquote></figure>



<p>Cats do have a highly developed sense of savory tasteâ€”dubbed <em>umami</em> in Japanese (delicious savory taste)â€”and this makes sense because they are consuming protein all the time. What would it be like to eat meat with a catâ€™s tongue? Itâ€™s impossible for science to say for certain, but one can imagine for a sensory system specifically tuned to taste savory flesh, it must be delicious.</p>



<p>On the other hand, you would never want to eat a salad with a cowâ€™s tongue because cows eat grass all dayâ€”something they would never be able to do if they were actually able to taste it the way we do. Unlike humans, with our dozens of bitter receptors, cows are highly insensitive to bitter foods. They still detect it, but apparently not so well.</p>



<p>â€œIn some respects, the taste system is like a mirror of [our] environment,â€ says Maik Behrens, a biologist at the Leibniz Institute for Systems Biology in Freising, Germany. He has studied the taste systems of multiple animals, publishing a paper a few years ago that explored the ability of <a href="https://dx.doi.org/10.1093/molbev/msu254">chickens and frogs</a> to taste bitter flavors.</p>



<p>Lots of creatures in the animal kingdom have reached the point over time where they have more or less one thing in their diet. Koalas are known to mostly eat eucalyptus, pandas are primarily bamboo eaters, and sea lions are bona fide pescatarians. According to experts, you see the same trend in lots of animals like these: a refinement of their taste systems to suit their niche.</p>



<p>â€œThatâ€™s why we taste bitter better than bottlenose dolphins,â€ which actually have no taste receptors at all, Behrens says. â€œApparently they simply swallow their fish without tasting a lot.â€</p>



<p>If you want to enjoy some sushi, donâ€™t try it with a dolphinâ€™s tongue</p>



<h3>That ape is alive and well within us</h3>



<p>If an animalâ€™s sense of taste is informed by whatâ€™s to eat in the environment in which they live, how they find that food in the first place matters. The more an animal forages, the more receptors it will need to detect potentially dangerous bitters.</p>



<p>â€œIf you donâ€™t know what youâ€™re going to eat,â€ says Breslin, â€œyou have to engage in this process of evaluation.â€</p>



<p>Foodies often tout the learned aspect of taste, a sense of refinement-through-experience. But fundamentally, our taste preferences are hard coded from the beginning. There is evidence that fetuses taste foods<em> in utero</em>, and newborn infants will coo at sweet and pucker at sour flavors placed in their mouths.</p>



<p>â€œWe love sugar for the same reason the chimpanzees doâ€”that other apes do,â€ Breslin says. â€œJust like the gorillas, chimpanzees, bonobos, and orangutans, which are all largely fruit eaters, we were meant to be sugar eaters.â€</p>



<p>â€œThat ape is alive and well within us,â€ he added.</p>



<p>Reached for comment on the discovery of the new type of taste cell, Medler explained how it did not behave the way we expect taste cells to behave.</p>



<p>Decades ago, there was thought to be a single type of taste cell that responded to all tastes. But over the years, that concept was slowly refined as we discovered different taste cells. Scientists then classified taste cells into different types, but the sense was still that a single cell would detect a single taste.</p>



<p>â€œThatâ€™s how it has stood for many years,â€ says Roper, the biologist from the University of Miami.</p>



<p>But what Medler and her colleagues recently discovered was a subset of these cells that were actually broadly responsive to four of the five basic taste qualities and seem to be just as important for the perception of those tastes.</p>



<p>â€œItâ€™s a very elegant study,â€ says Julie Mennella, a biologist at Monell Chemical Senses Center who studies taste also but was not involved in Medlerâ€™s research. The study involved characterizing isolated batches of broadly receptive taste cells for their electrical signaling as well as extensive behavioral experiments in which mice placed in special cages with different bottles of solution were trained to seek out certain bottles that offered tastes they like. Medlerâ€™s team showed that when mice lose their broadly responsive taste cells, the animals basically behave like they canâ€™t taste bitter, sweet, and umami.</p>



<h3>The quest for new tastes</h3>



<p>Not everyone agrees with the current breakdown of taste into five key flavors, and some scientists have proposed that additional tastes could exist, and some of these potential new tastes have been identified.</p>



<p>â€œThere are quite a number of additional candidates out there,â€ says Purdue University nutritional scientist Richard Mattes, who coined the word <em>oleogustus</em> (fatty taste) to describe one of these proposed new sensory flavors. Calcium, carbon dioxide, starch, and short-chain carbohydrates also have all been variously proposed as possible new tastes. â€œBut they are by no means accepted broadly in the scientific community,â€ Mattes says.</p>



<p>The evidence for fat as a taste is overwhelming, Mattes says. There is at least one receptor found on human taste cells, called CD36, that can detect fat. There is also a mechanism for it to transduce signals to the brain, and these signals seem to be tuned specifically for fat. There are also sensory experiments where people can sensitively detect fat on their tongues.</p>



<p>â€œItâ€™s not just sour, salty, bitter, or umamiâ€”people can, under the right testing conditions, say, â€˜Ohâ€”no! This is not like any of those others,â€ Mattes says. â€œIn my mind, itâ€™s a pretty strong case.â€</p>



<p>The evolutionary argument is that humans have the ability to detect a specific type of fat known as free fatty acids, which are present in rancid food. The food industry has been aware of this for decades and goes to great lengths to eliminate free fatty acids from food. The reason why olive oil is cold pressed, for instance, is to reduce the temperature-dependent oxidation of the oil, which produces these free fatty acids and fouls the taste of the oil.</p>



<p>The last time a new flavor was added to the list was 20 years ago, when umami was added after the discovery of its receptorâ€”but that was almost a century â€¦</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://neo.life/2020/10/taste-2-0-is-here/">https://neo.life/2020/10/taste-2-0-is-here/</a></em></p>]]>
            </description>
            <link>https://neo.life/2020/10/taste-2-0-is-here/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24702624</guid>
            <pubDate>Tue, 06 Oct 2020 21:41:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Quest for the Whistler Button]]>
            </title>
            <description>
<![CDATA[
Score 76 | Comments 20 (<a href="https://news.ycombinator.com/item?id=24702444">thread link</a>) | @josecastillo
<br/>
October 6, 2020 | https://newscrewdriver.com/2020/10/06/quest-for-the-whistler-button/ | <a href="https://web.archive.org/web/*/https://newscrewdriver.com/2020/10/06/quest-for-the-whistler-button/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>Iâ€™m a fan of physical, tactile buttons that provide visual feedback. I realize the current trend favors capacitive touch, but I love individual buttons I can find by feel. And one of the best looking buttons Iâ€™ve seen was from the 1992 movie <em><a rel="noreferrer noopener" href="https://www.imdb.com/title/tt0105435/" target="_blank">Sneakers</a></em>. When the blind character Whistler used a Braille-labeled device to add a sound effect representing the â€œthumpâ€ sound of a car going over seams of a concrete bridge.</p>



<p>They were only on screen for a few seconds, but I was enamored with the black buttons, each with a corresponding red LED. The aesthetics reminded me of <em>2001</em>, like the eye of HAL in a mini monolith. Or maybe Darth Vader, if the Sith lord were a button. When I first watched the movie many years ago, I thought they were neat and left it at that. But in recent years Iâ€™ve started building electronics projects. So when I rewatched the movie recently and saw them again, I decided to research these buttons.</p>



<p>The first step is to determine if they were even a thing. All we saw was the front control panel of an unknown device. It was possible the buttons and LEDs were unrelated components sitting adjacent to each other on the circuit board, and only visually tied together by pieces of plastic custom-made for the device. So the first step was to find that device. There was a label at the bottom of the panel below Whistlerâ€™s hand, but due to the shallow depth of field I could only make out the end as â€œâ€¦ 2002 digital samplerâ€. Time to hit the internet and see if anyone recognized the machine.</p>



<p>The first step is <a href="https://www.imdb.com/title/tt0105435/trivia" target="_blank" rel="noreferrer noopener">the Trivia section</a> of the movieâ€™s page on Internet Movie Database where people contribute random and minute pieces of information. Firearms enthusiasts can usually be counted on to name specific guns used in a film, and automotive enthusiasts frequently contribute make and model of cars as well.</p>



<p>Sadly, the electronics audio enthusiasts have not felt fit to contribute to this page, so I went elsewhere on the internet trying various keyword combinations of â€œSneakersâ€, â€œWhistlerâ€, â€œsamplerâ€, etc. The answer was found in <a rel="noreferrer noopener" href="https://hackaday.com/2017/09/15/sneakers-a-love-fest/#comment-4015147" target="_blank">a comment to a Hackaday post about the movie</a>. Iâ€™ve complained a lot about the general quality of internet comments, but this time one personâ€™s nitpicking correction is my rare nugget of gold.</p>



<p>Whistlerâ€™s device is a <a rel="noreferrer noopener" href="https://encyclotronic.com/synthesizers/sequential-circuits/prophet-2002-r469/" target="_blank">Sequential Circuits Prophet 2002 Digital Sampler rack</a>. As befitting the movie character, the samplerâ€™s control panel had Braille labels covering the default text. But otherwise it appears relatively unmodified for the movie. I wish the pictures were higher resolution, but their arrangement strongly implies the button and LED are part of a single subcomponent. The strongest evidence came from the presence of four vertical axis buttons, rotated 90 degrees from the rest.</p>



<blockquote><p><em>Aside: On the far right of the control panel, we can see a sign of the era, a 3.5â€³ floppy drive for data storage.</em></p></blockquote>



<p>Encouraged by this find, I started searching for Prophet 2002 buttons. I quickly found an eBay community offering replacement parts for Sequential Circuits products <a rel="noreferrer noopener" href="https://www.ebay.com/itm/Sequential-Circuits-Prophet-VS-Prophet-2002-Black-Switch-with-led/224163004481" target="_blank">including these buttons</a>. Whatâ€™s intriguing to me is that these are sold in â€œNewâ€ condition, not surplus or salvaged from old units. Iâ€™m optimistically interpreting this as a hint these buttons might still be in production, decades after the Prophet 2002 was released in 1985.</p>



<p>Thanks to those eBay listings, I have seen a picture of the component by itself and it is exactly what I hoped it would be: the buttonâ€™s exterior surface, the electric switch itself, and the LED are integrated into a single through-hole component. Given the tantalizing possibility it is still in active production and something I can buy for my own projects, I went next to electronics supplier Digi-Key.</p>



<p>Digi-Key carries 305,212 components under its â€œ<a rel="noreferrer noopener" href="https://www.digikey.com/en/products/category/switches/15" target="_blank">Switches</a>â€ section, not practical for individual manual review. Fortunately there are subsections and I first tried â€œ<a href="https://www.digikey.com/en/products/filter/tactile-switches/197">Tactile Switches</a>â€ (5721 items) because those buttons look like theyâ€™d give a good tactile response. In the movie we also heard a satisfying click when the button was pressed, but I donâ€™t know if that was added later by the filmâ€™s sound mixer.</p>



<p>Within the â€œTactile Switchesâ€ section, I aggressively filtered by the most optimistic wish they are active and in stock:</p>



<ul><li>Part Status: Active</li><li>Stocking Options: In Stock</li><li>Illumination: Illuminated</li><li>Illuminator: LED, Red</li></ul>



<p>That dropped it to 76 candidates. Almost all of them carried their illumination under the button instead of adjacent to it. The closest candidate is a JF Series switch by NKK Switches, the <a rel="noreferrer noopener" href="https://www.digikey.com/en/products/detail/nkk-switches/JF15RP3HC/2104244" target="_blank">JF15RP3HC which has a Digi-Key part number 360-3284-ND</a>.</p>



<figure><img data-attachment-id="23048" data-permalink="https://newscrewdriver.com/nkk-jf15rp3hc-illuminated-switch/" data-orig-file="https://newscrewdriver.files.wordpress.com/2020/10/nkk-jf15rp3hc-illuminated-switch.jpg" data-orig-size="948,643" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nkk-jf15rp3hc-illuminated-switch" data-image-description="" data-medium-file="https://newscrewdriver.files.wordpress.com/2020/10/nkk-jf15rp3hc-illuminated-switch.jpg?w=300" data-large-file="https://newscrewdriver.files.wordpress.com/2020/10/nkk-jf15rp3hc-illuminated-switch.jpg?w=700" src="https://newscrewdriver.files.wordpress.com/2020/10/nkk-jf15rp3hc-illuminated-switch.jpg?w=948" alt="" srcset="https://newscrewdriver.files.wordpress.com/2020/10/nkk-jf15rp3hc-illuminated-switch.jpg 948w, https://newscrewdriver.files.wordpress.com/2020/10/nkk-jf15rp3hc-illuminated-switch.jpg?w=150 150w, https://newscrewdriver.files.wordpress.com/2020/10/nkk-jf15rp3hc-illuminated-switch.jpg?w=300 300w, https://newscrewdriver.files.wordpress.com/2020/10/nkk-jf15rp3hc-illuminated-switch.jpg?w=768 768w" sizes="(max-width: 948px) 100vw, 948px"></figure>



<p>It is a more modern and refined variant of the same concept. The button is sculpted, and the illuminated portion sits flush with the surroundings. This would be a great choice if I was updating the design, but I am chasing a specific aesthetic and this switch does not look like a monolith or Vader.</p>



<p>So that wasnâ€™t too bad, but Iâ€™m not ready to stop. Peer to â€œTactile Switchesâ€ are several other subsections worth investigating. I next went to â€œ<a rel="noreferrer noopener" href="https://www.digikey.com/en/products/filter/pushbutton-switches/199" target="_blank">Pushbutton Switches</a>â€ (175,722 items) and applied the following filters. Again starting with the optimistic wish they are active and in stock:</p>



<ul><li>Part Status: Active</li><li>Stocking Options: In Stock</li><li>Type: Keyswitch, Illuminated</li><li>Illumination Type, Color: LED, Red</li></ul>



<p>That filter cut the number of possibilities from 175,722 down to 21 which felt like an overly aggressive shot in the dark, and I expected I would have to adjust the search. But it wouldnâ€™t hurt to take a quick look over those 21 and my eyes widened when I saw that list. Most of the 21 results had a very similar aesthetic and would make an acceptable substitute, but that would not be necessary because I saw the Omron B3J-2100.</p>



<figure><img data-attachment-id="23053" data-permalink="https://newscrewdriver.com/pushbutton-switches-illuminated-led-red/" data-orig-file="https://newscrewdriver.files.wordpress.com/2020/10/pushbutton-switches-illuminated-led-red.jpg" data-orig-size="1639,925" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pushbutton-switches-illuminated-led-red" data-image-description="" data-medium-file="https://newscrewdriver.files.wordpress.com/2020/10/pushbutton-switches-illuminated-led-red.jpg?w=300" data-large-file="https://newscrewdriver.files.wordpress.com/2020/10/pushbutton-switches-illuminated-led-red.jpg?w=700" src="https://newscrewdriver.files.wordpress.com/2020/10/pushbutton-switches-illuminated-led-red.jpg?w=1024" alt="" srcset="https://newscrewdriver.files.wordpress.com/2020/10/pushbutton-switches-illuminated-led-red.jpg?w=1024 1024w, https://newscrewdriver.files.wordpress.com/2020/10/pushbutton-switches-illuminated-led-red.jpg?w=150 150w, https://newscrewdriver.files.wordpress.com/2020/10/pushbutton-switches-illuminated-led-red.jpg?w=300 300w, https://newscrewdriver.files.wordpress.com/2020/10/pushbutton-switches-illuminated-led-red.jpg?w=768 768w, https://newscrewdriver.files.wordpress.com/2020/10/pushbutton-switches-illuminated-led-red.jpg 1639w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Yes, Iâ€™ve hit the jackpot! Even if that isnâ€™t precisely the correct replacement for a Prophet 2002 sampler, it has the right aesthetics: a dark angular block with the round LED poking out. But now that Iâ€™ve found the component, I can perform web searches with its name to confirm that others have also decided <a rel="noreferrer noopener" href="https://www.gearslutz.com/board/electronic-music-instruments-and-electronic-music-production/871589-few-words-refurbishing-sequential-digital-gear.html" target="_blank">Omron B3J is the correct replacement</a>.</p>



<figure><img data-attachment-id="23056" data-permalink="https://newscrewdriver.com/omron-b3j-list-of-models/" data-orig-file="https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-list-of-models.jpg" data-orig-size="1482,906" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="omron-b3j-list-of-models" data-image-description="" data-medium-file="https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-list-of-models.jpg?w=300" data-large-file="https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-list-of-models.jpg?w=700" src="https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-list-of-models.jpg?w=1024" alt="" srcset="https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-list-of-models.jpg?w=1024 1024w, https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-list-of-models.jpg?w=150 150w, https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-list-of-models.jpg?w=300 300w, https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-list-of-models.jpg?w=768 768w, https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-list-of-models.jpg 1482w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><a rel="noreferrer noopener" href="https://omronfs.omron.com/en_US/ecb/products/pdf/en-b3j.pdf" target="_blank">Omronâ€™s B3J datasheet</a> showed a list of models, where we can see variations on this design. The button is available in multiple colors, including this black unit and the blue also used by the Prophet 2002. The number and color of LEDs add to the possible combinations, from no LEDs (a few blue examples on a Prophet 2002 have no lights) to two lights in combinations of red, green, or yellow.</p>



<p>Sure, these switches are more expensive than the lowest bidder options on Amazon. But the price premium is a small price to pay when Iâ€™m specifically seeking this specific aesthetic. When I want the look that started me on this little research project, only the <a href="https://www.digikey.com/en/products/detail/omron-electronics-inc-emc-div/B3J-2100/700006" target="_blank" rel="noreferrer noopener">Omron B3J-2100</a> will do. And yeah, Iâ€™m going to call them â€œWhistler buttonsâ€.</p>



<figure><img data-attachment-id="23051" data-permalink="https://newscrewdriver.com/omron-b3j-2100/" data-orig-file="https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-2100.jpg" data-orig-size="954,955" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="omron-b3j-2100" data-image-description="" data-medium-file="https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-2100.jpg?w=300" data-large-file="https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-2100.jpg?w=700" src="https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-2100.jpg?w=954" alt="" srcset="https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-2100.jpg 954w, https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-2100.jpg?w=150 150w, https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-2100.jpg?w=300 300w, https://newscrewdriver.files.wordpress.com/2020/10/omron-b3j-2100.jpg?w=768 768w" sizes="(max-width: 954px) 100vw, 954px"></figure>



<p>[Follow-up: This post became more popular than I had expected, and Iâ€™m glad I <a href="https://newscrewdriver.com/2020/10/07/a-delight-for-the-button-connoisseur/">made a lot of fellow button enthusiasts happy</a>.]</p>
			</div></div>]]>
            </description>
            <link>https://newscrewdriver.com/2020/10/06/quest-for-the-whistler-button/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24702444</guid>
            <pubDate>Tue, 06 Oct 2020 21:20:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[â€œA server hall draws as much electricity as half a nuclear power plant createsâ€]]>
            </title>
            <description>
<![CDATA[
Score 47 | Comments 71 (<a href="https://news.ycombinator.com/item?id=24702206">thread link</a>) | @draugadrotten
<br/>
October 6, 2020 | https://tekdeeps.com/a-server-hall-draws-as-much-electricity-as-half-a-nuclear-power-plant-creates/ | <a href="https://web.archive.org/web/*/https://tekdeeps.com/a-server-hall-draws-as-much-electricity-as-half-a-nuclear-power-plant-creates/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://tekdeeps.com/a-server-hall-draws-as-much-electricity-as-half-a-nuclear-power-plant-creates/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24702206</guid>
            <pubDate>Tue, 06 Oct 2020 20:51:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hootsuite employee fired after speaking out about company's ICE deal]]>
            </title>
            <description>
<![CDATA[
Score 64 | Comments 22 (<a href="https://news.ycombinator.com/item?id=24702114">thread link</a>) | @foofoo55
<br/>
October 6, 2020 | https://bc.ctvnews.ca/hootsuite-employee-fired-after-speaking-out-about-company-s-ice-deal-1.5135073 | <a href="https://web.archive.org/web/*/https://bc.ctvnews.ca/hootsuite-employee-fired-after-speaking-out-about-company-s-ice-deal-1.5135073">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p>VANCOUVER -- 
	A Hootsuite employee who publicly criticized the company's decision to work with U.S. Immigration and Customs Enforcement has lost her job.</p>
<p>
	Sam Anderson said she was let go on Monday, less than two weeks after she spoke out about Hootsuite's three-year contract with ICE, which has since been called off.</p>
<p>
	"I'm not sure what I can and can't say about my departure, but I assume it's fair to say (and also probably obvious) that it was not my decision to leave," Anderson wrote on Twitter.</p>
<p>
	Anderson said she wouldn't be commenting further on her firing because "this has been an exhausting week and a half and I need to focus my energy on finding a new job."</p>
<p>
	Before going public with her concerns, Anderson was a senior training specialist with Hootsuite, according to her LinkedIn profile.</p>
<p>
	Anderson slammed the ICE deal in a series of tweets on Sept. 23, alleging that more than 100 Hootsuite employees had raised similar concerns about working with the government agency.</p>
<div data-attribute="embed_code">
<!--startPolopolyEmbed-->	<blockquote>
		<p dir="ltr" lang="en">
			As of yesterday morning I am no longer employed by Hootsuite. Iâ€™m not sure what I can and canâ€™t say about my departure, but I assume itâ€™s fair to say (and also probably obvious) that it was not my decision to leave.</p>
		â€” Sam | abolish the police (@samelaanderson) <a href="https://twitter.com/samelaanderson/status/1313515970195984384?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>
<!--endPolopolyEmbed--></div>
<p>
	"That we are eagerly accepting money from an organization that is allegedly subjecting its female detainees to forced hysterectomies, that has a documented history of locking children in cages, that tears families apart and destroys lives is devastating and disgusting," she wrote at the time.</p>
<p>
	Hootsuite issued a statement the next day announcing that it was backing out of the ICE contract.</p>
<p>
	Asked about Anderson's departure on Tuesday, the company said it doesn't discuss employee status for privacy reasons, but "supports differences of thoughts and opinions within the company and firmly believes in engaging dialogue."</p>
<p>
	"We deeply value the trust of our employees, partners and customers. To that end we must be unequivocal in upholding our confidentiality obligations," Hootsuite said in an email statement.</p>
<p>
	Last month, CEO Tom Keiser shed some light on the company's original response to employee concerns about the ICE contract, which Anderson said they first learned about in June.</p>
<p>
	In a written statement, Keiser said the internal strife initially led Hootsuite to form a committee to "consider all points of view" on the deal, but that management decided to press forward with the contract anyway.</p>
<p>
	That changed in September following a "broad emotional and passionate reaction from our people," the CEO added.</p>
<p>
	"I â€“ and the rest of the management team â€“ share the concerns our people have expressed. As a result, we have decided to not proceed with the deal with ICE," Keiser said.</p>
<p>
	Immigration and Customs Enforcement prompted widespread outrage last year after it was revealed that six migrant children had died in federal custody over a period of months.</p>
<p>
	There has been more anger in recent weeks after several women alleged they were given hysterectomies without their consent while detained by ICE.</p>
                                              </div></div>]]>
            </description>
            <link>https://bc.ctvnews.ca/hootsuite-employee-fired-after-speaking-out-about-company-s-ice-deal-1.5135073</link>
            <guid isPermaLink="false">hacker-news-small-sites-24702114</guid>
            <pubDate>Tue, 06 Oct 2020 20:40:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Clarifying exceptions and visualizing tensor operations in deep learning code]]>
            </title>
            <description>
<![CDATA[
Score 57 | Comments 12 (<a href="https://news.ycombinator.com/item?id=24701739">thread link</a>) | @parrt
<br/>
October 6, 2020 | https://explained.ai/tensor-sensor/index.html | <a href="https://web.archive.org/web/*/https://explained.ai/tensor-sensor/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">






<p><a href="http://parrt.cs.usfca.edu/">Terence Parr</a></p>

<p>(Terence teaches in <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">University of San Francisco's MS in Data Science program</a>. You might know Terence as the creator of the ANTLR parser generator.)</p>






<p>	Most people solve deep learning problems using high-level libraries such as <a href="https://keras.io/">Keras</a> or <a href="https://www.fast.ai/">fastai</a>,  which makes sense. These libraries hide a lot of implementation details that we either don't care about or can learn later.  To truly understand deep learning, however, I think it's important at some point to implement your own network layers and training loops. For example, see my recent article called <a href="https://explained.ai/rnn/index.html">Explaining RNNs without neural networks</a>. If you're comfortable building deep learning models while leaving some of the details a bit fuzzy, then this article is not for you.  In my quirky case, I care more about learning something deeply than actually applying it to something useful, so I go straight for the details. (I guess that's why I work at a university, not in industry Ã°Å¸Ëœâ‚¬.)  This article is in response to a pain point I experienced during an obsessive coding and learning burn through the fundamentals of deep learning in the isolation of Covid summer 2020.</p>

<center>
<a href="https://explained.ai/tensor-sensor/images/teaser.png">
<img src="https://explained.ai/tensor-sensor/images/teaser.png" width="40%" url="images/teaser.png">
</a>
</center>
One of the biggest challenges when writing code to implement deep learning networks, particularly for us newbies, is getting all of the tensor (matrix and vector) dimensions to line up properly. It's really easy to lose track of tensor dimensionality in complicated expressions involving multiple tensors and tensor operations.  Even when just feeding data into predefined <a href="https://www.tensorflow.org/">Tensorflow</a> network layers, we still need to get the dimensions right. When you ask for improper computations, you're going to run into some less than helpful exception messages.  To help myself and other programmers debug tensor code, I built a new library called <a href="https://github.com/parrt/tensor-sensor">TensorSensor</a> (<span>pip install tensor-sensor</span>).  TensorSensor clarifies exceptions by augmenting messages and visualizing Python code to indicate the shape of tensor variables (see figure to the right for a teaser). It works with <a href="https://www.tensorflow.org/">Tensorflow</a>, <a href="https://pytorch.org/">PyTorch</a>, and <a href="https://numpy.org/">Numpy</a>, as well as higher-level libraries like <a href="https://keras.io/">Keras</a> and <a href="https://www.fast.ai/">fastai</a>.

<p><i>TensorSensor is currently at 0.1b1 so I'm happy to receive issues created at the</i> <a href="https://github.com/parrt/tensor-sensor">repo</a> <i>or direct email</i>.</p>



<h2 id="sec:1.1">Isolating issues in tensor code is maddening!</h2>


<p>Even for experts, it can be hard to quickly identify the cause of an exception in a line of Python code performing tensor operations.  The debugging process usually involves adding a print statement in front of the offending line to emit the shape of each tensor operand.  That requires editing the code to create the debugging statement and rerunning the training process. Or, we can manually click or type commands to request all operand shapes using an interactive debugger. (This can be less practical in an IDE like PyCharm where executing code in debug mode seems to be much slower.)  The following subsections illustrate the anemic default exception messages and my proposed TensorSensor approach, rather than a debugger or print statements.</p>



<h3 id="sec:1.1.1">Debugging a simple linear layer</h3>


<p>Let's look at a simple tensor computation to illustrate the less-than-optimal information provided by the default exception message. Consider the following simple NumPy implementation for a hardcoded single (linear) network layer that contains a tensor dimension error.</p>


<p>import numpy as np

n = 200                          # number of instances
d = 764                          # number of instance features
n_neurons = 100                  # how many neurons in this layer?

W = np.random.rand(d,n_neurons)  # Ooops! Should be (n_neurons,d) &lt;=======
b = np.random.rand(n_neurons,1)
X = np.random.rand(n,d)          # fake input matrix with n rows of d-dimensions

Y = W @ X.T + b                  # pass all X instances through layer</p>


<p>Executing that code triggers an exception whose important elements are:</p>

<p>...
---&gt; 10 Y = W @ X.T + b
	
ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 764 is different from 100)</p>


<p>The exception identifies the offending line and which operation (<span>matmul</span>: matrix multiply) but would be more useful if it gave the complete tensor dimensions. Also, the exception would be unable to distinguish between multiple matrix multiplications occurring in one line of Python.</p>

<p>Next, let's see how TensorSensor makes debugging that statement much easier. If we wrap the statement using a Python <span>with</span> statement and <span>tsensor</span>'s <span>clarify()</span>, we get a visualization and an augmented error message. </p>


<p>import tsensor
with tsensor.clarify():
    Y = W @ X.T + b</p>


<p>
<a href="https://explained.ai/tensor-sensor/images/numpy-mm-py.svg">
<img nocenter="true" src="https://explained.ai/tensor-sensor/images/numpy-mm-py.svg" url="images/numpy-mm-py.svg">
</a>
</p>

<p>...
ValueError: matmul: Input operand ...
Cause: @ on tensor operand W w/shape (764, 100) and operand X.T w/shape (764, 200)</p>


<p>It's clear from the visualization that <span>W</span>'s dimensions should be flipped to be <span>n_neurons x d</span>; the columns of <span>W</span> must match the rows of <span>X.T</span>. You can also checkout a <a href="https://explained.ai/tensor-sensor/images/numpy-mm.png">complete side-by-side image</a> with and without <span>clarify()</span> to see what it looks like in a notebook.</p>

<p>The <span>clarify()</span> functionality incurs no overhead on the executing program until an exception occurs. Upon exception, <span>clarify()</span>:</p>
<ol>
<li> Augments the exception object's message created by the underlying tensor library.</li>
<li> Gives a visual representation of the tensor sizes involved in the offending operation; only the operands and operator involved in the exception are highlighted, while the other Python elements are de-highlighted.</li>
</ol>
<p>TensorSensor also clarifies tensor-related exceptions raised by PyTorch and TensorFlow. Here are the equivalent code snippets and resulting augmented exception error messages (<span>Cause: @ on tensor ...</span>) and visualization from TensorSensor:</p>
<center>
<table>
<thead>
<tr>
<th>PyTorch</th><th>TensorFlow</th>
</tr>
</thead>
<tbody>
<tr>
<td>

<p>import torch
W = torch.rand(d,n_neurons)
b = torch.rand(n_neurons,1)
X = torch.rand(n,d)
with tsensor.clarify():
    Y = W @ X.T + b</p>

</td><td>

<p>import tensorflow as tf
W = tf.random.uniform((d,n_neurons))
b = tf.random.uniform((n_neurons,1))
X = tf.random.uniform((n,d))
with tsensor.clarify():
    Y = W @ tf.transpose(X) + b</p>

</td>
</tr>
<tr>
<td>

<a href="https://explained.ai/tensor-sensor/images/mm.svg">
<img nocenter="true" src="https://explained.ai/tensor-sensor/images/mm.svg" url="images/mm.svg">
</a>



<p>RuntimeError: size mismatch, m1: [764 x 100], m2: [764 x 200] at /tmp/pip-req-build-as628lz5/aten/src/TH/generic/THTensorMath.cpp:41
Cause: @ on tensor operand W w/shape [764, 100] and operand X.T w/shape [764, 200]</p>

</td><td>
<img src="https://explained.ai/tensor-sensor/images/tf-mm.svg" nocenter="true">

<p>InvalidArgumentError: Matrix size-incompatible: In[0]: [764,100], In[1]: [764,200] [Op:MatMul]
Cause: @ on tensor operand W w/shape (764, 100) and operand tf.transpose(X) w/shape (764, 200)</p>
</td>
</tr>
</tbody>
</table>
</center>
<p>The PyTorch message does not identify which operation triggered the exception, but TensorFlow's message does indicate matrix multiplication. Both show the operand dimensions. These default exception messages are probably good enough for this simple tensor expression for a linear layer. Still, it's easier to see the problem with the TensorSensor visualization.</p>

<p>You might be wondering, though, why tensor libraries don't generate a more helpful exception message that identified the names of the Python variables involved in the offending subexpression.  It's not that the library authors couldn't be bothered. The fundamental issue is that Python tensor libraries are wrappers around extremely efficient cores written in C or C++. Python passes, say, the data for two tensors to a C++ function, but not the associated tensor variable names in Python space. An exception caught deep in C++ has no access to the local and global variable spaces in Python, so it just throws a generic exception back over the fence.  Because Python traps exceptions at the statement level, it also cannot isolate the subexpression within the statement.  (To learn how TensorSensor manages to generate such specific messages, check out <b>Section</b> <i>Key TensorSensor implementation Kung Fu</i> below.)</p>



<h3 id="sec:1.1.2">Debugging a complex tensor expression</h3>


<p>That lack of specificity in default messages makes it hard to identify bad subexpressions within more complicated statements that contain lots of operators. For example, here's a statement pulled from the guts of a Gated Recurrent Unit (GRU) implementation:</p>


<p>h_ = torch.tanh(Whh_ @ (r*h) + Uxh_ @ X.T + bh_)</p>


<p>It doesn't matter what it's computing or what the variables represent, just that they are tensor variables. There are two matrix multiplications, two vector additions, and even a vector element-wise modification (<span>r*h</span>).  Without augmented error messages or visualizations we wouldn't know which operator and operands caused an exception. To demonstrate how TensorSensor clarifies exceptions in this case, we need to give some fake definitions for the variables used in the statement (the assignment to <span>h_</span>) to get executable code:</p>


<p>nhidden = 256
Whh_ = torch.eye(nhidden, nhidden)  # Identity matrix
Uxh_ = torch.randn(d, nhidden)
bh_  = torch.zeros(nhidden, 1)
h = torch.randn(nhidden, 1)         # fake previous hidden state h
r = torch.randn(nhidden, 1)         # fake this computation
X = torch.rand(n,d)                 # fake input

with tsensor.clarify():
    h_ = torch.tanh(Whh_ @ (r*h) + Uxh_ @ X.T + bh_)</p>


<p>Again, you can ignore the actual computation performed by the code to focus on the shape of the tensor variables.  </p>

<p>For most of us, it's impossible to identify the problem just by looking at the tensor dimensions and the tensor code.  The default exception message is helpful of course, but most of us will still struggle to identify the problem.  Here are the key bits of the default exception message (note the less-than-helpful reference to the C++ code):</p>

<p>---&gt; 10     h_ = torch.tanh(Whh_ @ (r*h) + Uxh_ @ X.T + bh_)
RuntimeError: size mismatch, m1: [764 x 256], m2: [764 x 200] at /tmp/pip-req-build-as628lz5/aten/src/TH/generic/THTensorMath.cpp:41
</p>


<p>What we need to know is which operator and operands failed, then we can look at the dimensions to identify the problem.  Here is TensorSensor's visualization and augmented exception message:</p>

<p>
<a href="https://explained.ai/tensor-sensor/images/torch-gru.svg">
<img nocenter="true" src="https://explained.ai/tensor-sensor/images/torch-gru.svg" url="images/torch-gru.svg">
</a>

</p><p>---&gt; 10 h_ = torch.tanh(Whh_ @ (r*h) + Uxh_ @ X.T + bh_)
RuntimeError: size mismatch, m1: [764 x 256], m2: [764 x 200] at â€¦</p></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://explained.ai/tensor-sensor/index.html">https://explained.ai/tensor-sensor/index.html</a></em></p>]]>
            </description>
            <link>https://explained.ai/tensor-sensor/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24701739</guid>
            <pubDate>Tue, 06 Oct 2020 20:01:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Simulating Machines in Clojure]]>
            </title>
            <description>
<![CDATA[
Score 159 | Comments 17 (<a href="https://news.ycombinator.com/item?id=24701737">thread link</a>) | @stopachka
<br/>
October 6, 2020 | https://stopa.io/post/255 | <a href="https://web.archive.org/web/*/https://stopa.io/post/255">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><span><p><a href="https://twitter.com/stopachka/status/1295411936625074178" target="_blank">My cofounder Joe and I recently finished SICP.</a> It was a mind-bending experience: you start from just 3 concepts, and you recursively build up algebraic equation solvers, circuit simulators, 4 interpreters, and a compiler. </p><p>At some point you experience a visceral feeling: If you were dropped in a forestâ€¦you could create your own computer. The project that contributed most significantly to this feeling was creating a machine simulator.</p><p>We diverged from the book by writing the simulator in Clojure rather than Scheme. Immutable data structures and higher-level concepts available to us in Clojure compressed the solution, to the point where I think you can build your own in a few days worth of hacking.</p><p>This essay will guide you through doing just that: letâ€™s build a machine simulator, over a good few days worth of hacking! I hope this inspires you to play with Clojure and to take a deeper look at SICP. </p><p>Before we simulate general machines, letâ€™s think about a concrete machine. <strong>How could we create a machine that could figure out factorials?</strong> </p><p>If we were writing code, factorial could look something like this:</p><pre><code><span>(</span><span>defn</span><span> factorial [n]</span>
<span>  (</span><span>loop</span><span> [res </span><span>1</span>
<span>         counter </span><span>1</span><span>]</span>
<span>    (</span><span>if</span><span> (</span><span>&gt;</span><span> counter n)</span>
<span>      res</span>
<span>      (</span><span>recur</span>
<span>        (</span><span>*</span><span> counter res)</span>
<span>        (</span><span>inc</span><span> counter)))))</span></code></pre><p>Letâ€™s see if we could build factorials using <em>physical</em> devices.</p><p>Well, we need a way to keep track of <code value="counter">counter</code>, <code value="res">res</code>, and <code value="n">n</code>. To do that, weâ€™ll need a device that stores information. </p><h2>Bulbs</h2><p>Imagine a device that has some light bulbs inside of it. </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwNDUyLTFlNzkzODgwLTA3ZTgtMTFlYi04MjI4LWQwYzE4ZTcwNTZhYS5wbmc" alt="image"></span></p><p>We can say that if a light bulb is <em>on,</em> that represents the number 1, and if a light bulb is <em>off</em> that represents the number 0. </p><p>If we had a bunch of light bulbs in the device, we could interpret the state of these bulbs as larger and larger binary numbers. The light bulbs in the device I just showed you for example, would represent â€œ10101â€, which is binary for â€œ21â€.</p><h2>Incoming Current</h2><p>Now, imagine that at all times there are a bunch of other wires connected to this device. These wires carry â€œnewâ€ charges for the light bulbs, but with a twist: the incoming charges <em>do not</em> affect the light bulbs inside just yet.</p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwNjIxLTVhMTQwMjgwLTA3ZTgtMTFlYi05ZjM2LTU2MDFkZTIyOWUwOS5wbmc" alt="image"></span></p><p>Notice how the <em>incoming charge</em> for the â€œaâ€ light bulb is â€œoffâ€, but the bulb inside is still on. Conversely, the incoming charge "b" is "on", but the bulb is off. If our device can do this, it means that whatever the charges for the light bulbs are inside is a <em>stored value</em>. Very cool! </p><h2>Save</h2><p>Now, we need these incoming wires to do something at some point. What if this device had a â€œsaveâ€ button. </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwNjQ1LTYzMDRkNDAwLTA3ZTgtMTFlYi04NjVjLWE3YTg3OTE5Njg3ZC5wbmc" alt="image"></span></p><p>Once we pressed â€œsaveâ€, the incoming current would transfer inside the box:  </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwNjU0LTY3Yzk4ODAwLTA3ZTgtMTFlYi04MjU1LTExNmEzMGUxNTcwMy5wbmc" alt="image"></span></p><p>Here, light bulb â€œaâ€ changes from â€œonâ€ to â€œoffâ€, and the light bulb "b" changed from "on" to "off".</p><p>Great, now we have a way to â€œsaveâ€ new numbers inside! </p><h2>Outgoing current</h2><p>We also need a way to share the state of whatâ€™s inside to other devices.  All weâ€™d need to do to make that work, is to have a bunch of wires that leave our device, which carry the sames charges as the light bulbs: </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUxNzQ2LWY5ODVjNTAwLTA3ZTktMTFlYi05YzczLTY4ZWYwNzc3OGJlMi5wbmc" alt="image"></span></p><p>Now, if we hooked those outgoing charges to some other device, that device would receive the â€œnumberâ€ that was stored in this one. </p><h2>Registers</h2><p>What we just described is analogous to a computerâ€™s <em>register</em> (1). Registers let us store and share information. </p><p>Now, we could use three of registers to store the value of <code value="res">res</code> <code value="counter">counter</code> and <code value="n">n</code>.</p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwODUxLWIxYjI2ZTAwLTA3ZTgtMTFlYi04NzRjLTQxODgyODAxNTQ2OC5wbmc" alt="image"></span></p><p>Next up, weâ€™ll need a device that that can â€œaddâ€ two registers. Imagine a device that had two registerâ€™s worth of incoming wires, and one registerâ€™s worth of outgoing wires: </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwODY5LWI4ZDk3YzAwLTA3ZTgtMTFlYi05ZDM0LTQwMDE0YmQ5NDAyOC5wbmc" alt="image"></span></p><h2>Adder</h2><p>If the device could connect those incoming wires in such a way, that the outgoing wires represented the â€œadditionâ€ of those registers, weâ€™d have an â€œadderâ€ device! </p><p>In the example above, the left register represents â€œ10101â€ (21), and the right represents â€œ00001â€ (1). The output wires are charged as â€œ10110â€â€¦which represent 22!</p><p>Similarly, we could have a device that has two registerâ€™s worth input wires, and one registerâ€™s worth of output wires: </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwODkyLWMxMzFiNzAwLTA3ZTgtMTFlYi05OTQ1LTI4ZDljNmU4N2IzNy5wbmc" alt="image"></span></p><h2>Multiplier</h2><p>If we could connect the incoming wires in such a way, that the outgoing wires represent the result of a multiplication, boom we would have a multiplying device! </p><p>The left register above represents â€œ00101â€ (5), the right register represents â€œ00010â€ (2), and the charge of the outgoing wire is â€œ01010â€ (10). Nice! That gives us a multiplier machine. </p><h2>Comparator</h2><p>We need one more device. Imagine a device that takes two registerâ€™s worth of input wires, and only has <em>one</em> output wire: </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwOTE1LWNhMjI4ODgwLTA3ZTgtMTFlYi05Y2RiLWM2ZDEwZTYxNzc1Yi5wbmc" alt="image"></span></p><p>If we could combine the input wires in such a way, that the output wire was â€œonâ€ when the left register was bigger, and off otherwise, we could use this as a comparator machine! </p><p>If we had all these machines, we can wire them in such a way, that lets us compute factorials: </p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUwOTg1LWRlZmYxYzAwLTA3ZTgtMTFlYi05MDRjLTE2NmUzNTIyMjJkOS5wbmc" alt="image"></span></p><p>Here, we wired the output wires of <code value="res">res</code> and <code value="counter">counter</code> to the <code value="*">*</code> machine. We wired the output wires of the <code value="*">*</code> machine, to <em>be</em> the input wires of <code value="res">res</code>. </p><p>This way, if we press â€œAâ€, we would â€œstoreâ€ the result of multiplying <code value="counter">counter</code> with <code value="res">res</code>! </p><p>Similarly, we wired up the output wires of <code value="counter">counter</code> and a register that keeps the value <code value="1">1</code>, to the <code value="+">+</code> machine. We wired the output wires of the <code value="+">+</code> machine, to <em>be</em> the input wires of <code value="counter">counter</code>. </p><p>Now, If we pressed â€œBâ€, <code value="counter">counter</code> would be stored with the result of adding <code value="1">1</code>! </p><p>Next up, we also wired up <code value="counter">counter</code> and <code value="n">n</code> with the <code value=">">&gt;</code> machine. If we hooked up a light bulb to the output wire of the <code value=">">&gt;</code> machine for example, then whenever it was on, we would know that <code value="counter">counter</code> was larger than <code value="n">n</code>. </p><p>Weâ€™ve just drawn out the â€œdata pathâ€ of our machine. </p><h2>Manual Recipe</h2><p>Letâ€™s remember our code for factorial: </p><pre><code><span>  (</span><span>loop</span><span> [res </span><span>1</span>
<span>         counter </span><span>1</span><span>]</span>
<span>    (</span><span>if</span><span> (</span><span>&gt;</span><span> counter n)</span>
<span>      res</span>
<span>      (</span><span>recur</span>
<span>        (</span><span>*</span><span> counter res)</span>
<span>        (</span><span>inc</span><span> counter))))</span></code></pre><p>imagine if we had our â€œdata pathâ€ machine. What would happen if we followed the following recipe:</p><ol><li>Take a look at the output of the <code value=">">&gt;</code> machine. </li><li>If the light bulb connected to the <code value=">">&gt;</code>  machine is on, <strong>stop</strong></li></ol><p><em>Otherwiseâ€¦</em></p><ol><li>"Press A". This will update <code value="res">res</code>  with the result of the <code value="*">*</code>  machine on <code value="res">res</code> and <code value="counter">counter</code> </li><li>â€œPress Bâ€œ. This will update <code value="counter">counter</code> with the result of the <code value="+">+</code>  machine on <code value="1">1</code> and <code value="counter">counter</code></li><li>Go back up to the start of the recipe</li></ol><p><strong>If we did this over and over again, once the light bulb connected to the output of the</strong>  <strong><code value=">">&gt;</code></strong> <strong>machine turns on,</strong> <strong><code value="res">res</code></strong> <strong>would contain the result of factorial!</strong> </p><h2>Automation</h2><p>Pretty cool, but this kind of manual work would be annoying. If you look at these instructions though, thereâ€™s a pretty significant insight: <em>all of those instructions are simple: "look at charge of light bulb", "press button..."</em></p><p>In fact, theyâ€™re so simple that we could wire up a machine that goes through that recipe! Imagine if we created a machine that could â€œpressâ€ buttons for us, depending on whether the output wire of the <code value=">">&gt;</code> machine is on:</p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUxMDIwLWU5MjExYTgwLTA3ZTgtMTFlYi04MDJlLTI4YzIyNjZiNzQwOC5wbmc" alt="image"></span></p><p>We would be able to automate computing factorials ğŸ™‚</p><h2>Balls and Hills</h2><p>Now, at this stage, you may be wondering: exactly <em>how</em> would <code value="*">*</code> produce output wires that represent the multiplication? How would <code value="+">+</code> work, and how would the <code value="controller">controller</code> move along? </p><p>If you think about it, these can all be reduced to very simple machines. They donâ€™t even necessarily have to be electronic: </p><p>Imagine you had a ball rolling down some hill. You could construct something like the <code value=">">&gt;</code> machine, by putting <code value="res">res</code> and <code value="counter">counter</code> on a scale: based on whatâ€™s bigger, the ball would take a different path</p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUxMDQ4LWY4MDdjZDAwLTA3ZTgtMTFlYi04NTg3LWRiYjE2ZDU3NGZkMy5wbmc" alt="image"></span></p><p>With sufficient energy, space, time, and ingenuity you really could build all of this with a ball on a hill. Now, you wouldnâ€™t necessarily do that (2), but you can imagine how the electronic parts that make up our machines are similarly simple, logical machines: <em>turn on if off, turn off if on, etc</em>. These logical machines are called â€œlogic gatesâ€. You can look them up, but hopefully Iâ€™ll have an essay for you about these machines soon ğŸ™‚. </p><p>Now, we drew out our machine and saw how we could build them with simple devices. How could we simulate these machines? </p><p>To simulate these machines, we need to transform our <em>picture descriptions</em> into something that computers can manipulate. Computers can manipulate text much better: letâ€™s create a <em>language</em> for describing these machines. </p><p>If we remember the pictures again:</p><p><span><img src="https://stopa.io/api/image/aHR0cHM6Ly91c2VyLWltYWdlcy5naXRodWJ1c2VyY29udGVudC5jb20vOTg0NTc0Lzk1MjUxMTg3LTJmNzY3OTgwLTA3ZTktMTFlYi05ZTNlLTg3Zjg0NDkxZjQxYS5wbmc" alt="image"></span></p><p>we could transform them into a language that looks like this:</p><pre><code><span>(</span><span>def</span><span> factorial-instructions</span>
<span>  '(</span>
<span>     start</span>
<!-- -->
<span>     (</span><span>test</span><span> (</span><span>op</span><span> &gt;) (</span><span>reg</span><span> counter) (</span><span>reg</span><span> n))</span>
<span>     (</span><span>branch</span><span> (</span><span>label</span><span> done))</span>
<!-- -->
<span>     (</span><span>assign</span><span> res (</span><span>op</span><span> *) (</span><span>reg</span><span> counter) (</span><span>reg</span><span> res))</span>
<span>     (</span><span>assign</span><span> counter (</span><span>op</span><span> +) (</span><span>reg</span><span> counter) (</span><span>const</span><span> </span><span>1</span><span>))</span>
<span>     (</span><span>goto</span><span> (</span><span>label</span><span> start))</span>
<!-- -->
<span>     done))</span></code></pre><p>When the <code value="test">test</code> instruction runs, we run the <code value=">">&gt;</code> machine with <code value="counter">counter</code> and <code value="n">n</code>.</p><p>Our <code value="branch">branch</code> instruction checks if the <code value="test">test</code> instruction said <code value="yes">yes</code>. If it did, it moves to <code value="done">done</code>. Otherwise it no-ops and the machine moves forward by one.</p><p>After that, our <code value="(assign res">(assign res</code> expression is analogous to â€œpress Aâ€. <code value="(assign counter">(assign counter</code> is analogous to â€œpress Bâ€, and <code value="(goto (label start)">(goto (label start)</code> is analogous to the arrow bringing us back to the start.</p><p>With this textual representation, we can build an interpreter and simulate our machine. Letâ€™s do this! </p><p>What does the state of our machine look like in Clojure? Well, how do we represent most things in Clojure? With maps!  Letâ€™s represent the state of our machine as a map:</p><pre><code><span>(</span><span>def</span><span> ex-machine-state-v0</span>
<span>  {</span><span>:registry-map</span><span> {'n </span><span>10</span><span> 'res </span><span>1</span><span> 'counter </span><span>1</span><span>}</span>
<span>   </span><span>:label-&gt;idx</span><span> {'start </span><span>0</span><span> 'done </span><span>5</span><span>}})</span></code></pre><p><code value="registry-map">registry-map</code> could keep a mapping of registers to values. 
<code value="labelâ†’idx">labelâ†’idx</code> could keep a mapping of labels to their <code value="idx">idx</code> in the instruction list</p><p>With this, we can get the most foundational part of our language to work: We use <code value="(constâ€¦">(constâ€¦</code>  <code value="(reg...">(reg...</code> and <code value="(labelâ€¦">(labelâ€¦</code> all over the place.</p><ol><li>If our machine sees <code value="(const 1)">(const 1)</code>, it should return the actual value <code value="1">1</code></li><li>If our machine sees <code value="(reg foo)">(reg foo)</code>, it should look up whatever is in the <code value="foo">foo</code> register, and return that </li><li>If our machine sees <code value="(label foo)">(label foo)</code>, it should return the correct index in our instruction list.</li></ol><p>Letâ€™s write this out in Clojure:</p><pre><code><span>(</span><span>def</span><span> tag first) </span><span>; (tag '(const 1)) =&gt; const</span>
<span>(</span><span>defn</span><span> tag-of? [sym s] (</span><span>=</span><span> sym (</span><span>tag</span><span> s))) </span><span>; (tag-of? 'const '(const 1)) =&gt; true</span>
<!-- -->
<span>(</span><span>defn</span><span> parse-primitive [{</span><span>:keys</span><span> [registry-map label-&gt;idx] </span><span>:as</span><span> machine-state}</span>
<span>                       prim-exp]</span>
<span>  (</span><span>condp</span><span> tag-of? prim-exp</span>
<span>    'const</span>
<span>    (</span><span>second</span><span> prim-exp)</span>
<span>    'reg</span>
<span>    (</span><span>gâ€¦</span></code></pre></span></p></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://stopa.io/post/255">https://stopa.io/post/255</a></em></p>]]>
            </description>
            <link>https://stopa.io/post/255</link>
            <guid isPermaLink="false">hacker-news-small-sites-24701737</guid>
            <pubDate>Tue, 06 Oct 2020 20:01:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Celebrities Explain DevOps]]>
            </title>
            <description>
<![CDATA[
Score 28 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24700712">thread link</a>) | @jacksonpollock
<br/>
October 6, 2020 | https://cto.ai/blog/celebrities-explain-devops/ | <a href="https://web.archive.org/web/*/https://cto.ai/blog/celebrities-explain-devops/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<h3 id="david-hasselhoff-flavor-flav-and-carole-baskin-help-simplify-aws-kubernetes-and-docker"><em>David Hasselhoff, Flavor Flav, and Carole Baskin help simplify AWS, Kubernetes, and Docker</em></h3><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><p>ğŸ‘‹ &nbsp;Hey all you cool cats and kittens! ğŸ˜º </p><p>You probably thought this day would never come.</p><p>You might be thinking, have DevOps tools reached this level of adoption? That even the some of the biggest personalities on TV are excited by them.</p><p>Well, the day has come. It's today. And the proof is in the pudding (and the videos below).</p><p>In an effort to bring some lighthearted fun to the complex and serious nature of DevOps, we asked three of our favorite celebrities to explain some of the most popular technologies used in DevOps.</p><p>The Guinness World Record holder as The Most Watched Man on TV, <strong>David 'The Hoff' Hasselhoff</strong>,<strong> </strong>shares with us what Docker means to him.</p><p><strong>Flavor Flav</strong> (yeahhh boy!) of Public Enemy and Flavor of Love teaches us a thing or two about Kubernetes.</p><p>And<strong> Carole Baskin</strong> of Big Cat Rescue and Tiger King shares her excitement of AWS.</p><p>All are in support of our mission to simplify the snowballing complexity of the DevOps universe.</p><p>Please enjoy thoroughly:</p><!--kg-card-begin: embed--><figure><iframe width="480" height="270" src="https://www.youtube.com/embed/QxvmO-QlxJQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><!--kg-card-end: embed--><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><p><em>Love this video? Love DevOps? Come join <a href="https://w.cto.ai/community">The Ops Community on Slack</a> and trade tips and tricks on workflow automation with us!</em></p>
			</div></div>]]>
            </description>
            <link>https://cto.ai/blog/celebrities-explain-devops/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24700712</guid>
            <pubDate>Tue, 06 Oct 2020 18:15:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How I remember what I learn]]>
            </title>
            <description>
<![CDATA[
Score 517 | Comments 121 (<a href="https://news.ycombinator.com/item?id=24700647">thread link</a>) | @flreln
<br/>
October 6, 2020 | https://vasilishynkarenka.com/learning/ | <a href="https://web.archive.org/web/*/https://vasilishynkarenka.com/learning/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://vasilishynkarenka.com/content/images/size/w300/2020/09/IMG_1517.jpg 300w,
                            https://vasilishynkarenka.com/content/images/size/w600/2020/09/IMG_1517.jpg 600w,
                            https://vasilishynkarenka.com/content/images/size/w1000/2020/09/IMG_1517.jpg 1000w,
                            https://vasilishynkarenka.com/content/images/size/w2000/2020/09/IMG_1517.jpg 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://vasilishynkarenka.com/content/images/size/w2000/2020/09/IMG_1517.jpg" alt="How to remember what you learn">
            </figure>

            <section>
                <div>
                    <p><em>â€œI donâ€™t remember a damn thing.â€</em></p><p>The book I held my hands was full of highlights. It seemed like Iâ€™ve got all colors of the rainbow on a page. Apparently, this didnâ€™t help. When I tried recalling ideas from the book, I didnâ€™t hear a thing. Just. Silence.</p><p>Terrified, I started questioning how much I <em>really</em> know. If I forget everything I read, I canâ€™t apply my knowledge to the problem at hand. I canâ€™t transfer it. And without transfer, knowledge is very much like music for deaf ears.</p><p>I quickly did the math. I was planning to invest in learning a few hours a day for the next ~75 years of my life. Staring at the number of potentially wasted hours, I knew exactly what I had to do.</p><hr><p>In the past six months, Iâ€™ve devoured dozens of books, research papers, and studies on how people learn. As a result, Iâ€™ve designed a learning process that works for me. Itâ€™s not perfect, but an order of magnitude better than what I had before. </p><p>In this work, I outline my workflow so that you can try it out. It applies to any subject or discipline, from programming to economics. If you stumble upon something where it doesnâ€™t work, let me know.</p><blockquote><em>Make it time-based, take regular breaks, and learn what youâ€™re curious about.</em></blockquote><p>The most important thing is that my learning is time-based, not goal-based. Setting learning goals such as â€œread X pages todayâ€ is a way to fail because you set up the wrong incentives. When you plan to read X pages by lunch, you canâ€™t help but begin optimizing for the goal, which leads to focusing on speed instead of understanding. And when you donâ€™t have those â€œahaâ€ moments, it is hard to remember what you learn.</p><p>Itâ€™s also important to not overload yourself and take breaks. I do 3h learning sessions every day split into 30 min intervals with 5 min breaks. Breaks help to fall back into the diffuse mode of thinking and get access to a broader set of neural networks in my head. They also warm up my body, and I feel better after moving around for a few minutes.</p><p>As for material, I learn what Iâ€™m interested in. First, because life is <a href="http://www.paulgraham.com/vb.html">too short</a> to do things that you donâ€™t love. Second, Iâ€™ve found that studying stuff I genuinely like awakens my curiosity. And curiosity is essential to develop mastery because mastery is about depth and breadth of knowledge. </p><p>For example, if youâ€™re learning JavaScript and youâ€™re curious about it, youâ€™ll go and figure out how JS runtime environment works in Chrome even though the tutorial doesnâ€™t cover it. Just because youâ€™re interested. But if youâ€™re not curious, then youâ€™ll just memorize the tutorial, and your knowledge will be shallow.</p><h2 id="how-my-learning-session-works">How my learning session works</h2><blockquote>Clean up working memory, apply metacognition, and "siege" the thing with questions to improve understanding.</blockquote><p>When I learn, I always have two devices on my desk. I have my laptop with the study material (ie, a book, a video, an article) on the right, and I have my iPad with a text editor open on the left.</p><p>This is how my current setup looks like:</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/09/IMG_2658.jpg" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/09/IMG_2658.jpg 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/09/IMG_2658.jpg 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/09/IMG_2658.jpg 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/09/IMG_2658.jpg 2400w" sizes="(min-width: 720px) 720px"><figcaption>Morning learning session.</figcaption></figure><p>When I begin learning, I set a timer for 30 minutes and create three files in Drafts:</p><ol><li>A file with a timestamp where my random thoughts go.</li><li>A file with a timestamp where I think about the subject.</li><li>A file with questions.</li></ol><p><strong>The first file is a mind dump.</strong> When I start learning, I immediately begin thinking about things. Itâ€™s almost as if my brain wakes up and starts throwing ideas, tasks, and memories at me. I suspect this comes from the associative memory because I present myself with many triggers when Iâ€™m learning; words and sentences that bear special meaning to me and invoke these ideas.</p><p>But hereâ€™s the problem. If I donâ€™t write thoughts down, I canâ€™t focus. My working memory is overloaded with todos, ideas, and emotions. Youâ€™ve probably experienced this for yourself â€“ your mind is running too fast, and you canâ€™t really concentrate on what youâ€™re learning. Having this â€œdumpâ€ file is immensely useful to a) free up my working memory to focus on my learning instead of thinking about these things, and b) store these thoughts somewhere safe to go back to them later and take action.</p><p><strong>The second file is where I write about what Iâ€™m learning.</strong> Folks in the kitchen call it metacognition, which means thinking about thinking. Metacognition is the single best trick Iâ€™ve found to improve understanding, and I will write more about it in the future. Whenever I donâ€™t understand something or see that my understanding is shallow, I begin writing in the first person. It looks like this: â€œSo Peter explains that there are four characteristics of a monopoly, but I donâ€™t really understand why branding is one of them; why so?â€</p><p>Itâ€™s also important to note that I donâ€™t write in a usual sentence-paragraph manner. Instead, I write every thought on a new line. I donâ€™t even put dots at the end of the sentences. This helps me to focus on understanding instead of nitty-gritty styling and typos. The â€œenterâ€ key on a keyboard serves as the â€œend of thoughtâ€ symbol and helps formulate ideas more clearly.</p><p>Another important idea is that my editor is plain text. Iâ€™ve found it incredibly liberating to operate in a plain text environment where you donâ€™t have incentives to color, underline, bold, italicize, or do some other weird things with the text youâ€™re writing. Instead of choosing the right font for my heading, I can focus on meaning instead. Also, my plain text app is way faster than all feature-rich text editors, and Iâ€™ve found it essential for a thought input environment to be fast. Otherwise, I can't think.</p><p>Hereâ€™s a fragment from my learning of React yesterday:</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/09/IMG_D7F6BD12F133-1.jpeg" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/09/IMG_D7F6BD12F133-1.jpeg 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/09/IMG_D7F6BD12F133-1.jpeg 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/09/IMG_D7F6BD12F133-1.jpeg 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/09/IMG_D7F6BD12F133-1.jpeg 2400w" sizes="(min-width: 720px) 720px"><figcaption>Metacognition notes from studying React.js.</figcaption></figure><p>Youâ€™re probably thinking that itâ€™s quite a bit of writing. It is. For an hour-long learning session, I usually do about 500-1000 words in this file. But itâ€™s worth every character, and hereâ€™s why:</p><ul><li>When I apply metacognition, I understand things <em>way</em> better than when I donâ€™t. Iâ€™ve tested many different learning modes and found metacognition to perform at least 2x better based on my later ability to recall and transfer knowledge. Also, thereâ€™s <a href="https://academicpublishingplatforms.com/downloads/pdfs/ati/volume18/201607070302_09_ATI_Vol11_Issue2_2015_Todorova_and_Karamanska_Study_motivation_satisfaction_students_e-learning_pp.82-89.pdf">some</a> <a href="http://www.csun.edu/science/ref/reasoning/how-students-learn/1.html">research</a> on metacognition as well.</li><li>Having a file with my thinking about the subject keeps my working memory clean. I donâ€™t feel overloaded as I usually feel after reading many articles at one go. Youâ€™ve probably experienced this yourself; your brain is almost melting after an hour of scrolling through the web. Thatâ€™s because you present yourself with too much information without really making sense of it. After a few months of applying metacognitive practices, I realized that I canâ€™t go back. It just feels so strange to experience that cognitive load again.</li><li>Metacognition improves remembering through elaboration and interleaving. When Iâ€™m writing my thoughts in the file, I canâ€™t help but begin connecting them with other ideas on that topic because of associative memory. And interleaving leads to mastery.</li><li>(Speculation) Training metacognition improves my ability to transform vague notions and thoughts that I have during the day into specific words that I can write down for later analysis. This one is particularly interesting to me, but thereâ€™s no evidence besides my own experiments. And I might be biased because Iâ€™ve come up with this method.</li></ul><p>Moreover, I type 2-3x faster than most people because I use <a href="https://barehands.substack.com/p/how-to-type-3x-faster">shortcuts</a>. So itâ€™s not that bad.</p><p><strong>The third file is questions. </strong>Whenever I stumble upon something that I donâ€™t understand, I try to break it down into a set of simple questions. Each question in the group takes on a small part of the problem. If the concept is particularly challenging, I try to â€œsiegeâ€ it with questions from many many different angles and break it down even further.</p><p>When Iâ€™m beginning a new session, I always start from the previous oneâ€™s questions file. I only look at questions and answer them before Iâ€™m beginning new learning. This doesnâ€™t sound like very much fun, but itâ€™s actually pretty interesting to explain stuff to yourself if you do it out loud. Answering questions improves my understanding and helps to connect ideas together. And yes, answering questions counts as learning â€“ probably the most efficient learning you could be doing.</p><p><em>I'm not going into much detail on questions because Michael Nielsen has done a phenomenal job describing it <a href="http://augmentingcognition.com/ltm.html">here</a>.</em></p><h2 id="what-happens-after-the-session">What happens after the session</h2><blockquote>Write a dense summary, provoke elaboration, interleaving, and transfer, and choose what to never forget.</blockquote><p>After the session is done, and my three files are full of information, I begin the recap process.</p><p><strong>First, I write a three to five sentence-long summary of what Iâ€™ve just studied.</strong> Here I try to distill the materialâ€™s core idea and compress the whole thing into a maximally dense chunk. When Iâ€™m summarizing, my laptop is closed. Not looking at the text helps to â€œcompressâ€ the idea to its core and make a small â€œhookâ€ to my memory to later see what the whole book was about.</p><p>Hereâ€™s how my summary note looks like: </p><figure><img src="https://vasilishynkarenka.com/content/images/2020/09/IMG_9771D059CFD1-1.jpeg" alt="Recap of studying React.js." srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/09/IMG_9771D059CFD1-1.jpeg 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/09/IMG_9771D059CFD1-1.jpeg 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/09/IMG_9771D059CFD1-1.jpeg 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/09/IMG_9771D059CFD1-1.jpeg 2400w" sizes="(min-width: 720px) 720px"><figcaption>Recap of studying React.js.</figcaption></figure><p>Very often, what Iâ€™m writing in the summary section is not what the text was about, but what it means to me. In other words, if both of us read this text and wrote a summary of it, mine would be very different than yours.</p><p>After Iâ€™m done with the summary, I write down the answers to three questions:</p><ol><li>What are the key ideas? </li><li>How can I apply this knowledge that I learned? </li><li>How do these ideas relate to what I already know?</li></ol><p><strong>The first question speaks for itself.</strong> I try to remember what I just read and write down as many ideas as I can bring back. When I began applying the metacognition trick that I mentioned earlier, I noticed a 3x increase in the number of concepts I could recall. And as I speculate that long-term memory recall is influenced by initial interleaving and recall, this might actually help to improve your long-term memory.</p><p><strong>The second question is about transfer.</strong> The sole purpose of learning is to apply the knowledge that we learn. Without application, â€¦</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://vasilishynkarenka.com/learning/">https://vasilishynkarenka.com/learning/</a></em></p>]]>
            </description>
            <link>https://vasilishynkarenka.com/learning/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24700647</guid>
            <pubDate>Tue, 06 Oct 2020 18:06:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Best of Both Worlds: A New Take on Metalâ€“Plastic Hybrid 3D Printing]]>
            </title>
            <description>
<![CDATA[
Score 22 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24700553">thread link</a>) | @rustoo
<br/>
October 6, 2020 | https://www.waseda.jp/top/en/news/73810 | <a href="https://web.archive.org/web/*/https://www.waseda.jp/top/en/news/73810">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-type="narrow">
              <div>
                
<h5>Scientists develop a novel and surprisingly simple method to print 3D structures made of metal and plastic, paving the way for 3D electronics</h5>
<p><strong>Current 3D printers employ either plastic or metal only, and the conventional method to coat 3D plastic structures with metal is not environment-friendly and yields poor results. Now, scientists from Waseda University, Japan, have developed a metalâ€“plastic hybrid 3D printing technique that produces plastic structures with a highly adhesive metal coating on desired areas. This approach extends the use of 3D printers to 3D electronics for future robotics and Internet-of-Things applications.</strong></p>
<p><a href="https://www.waseda.jp/top/en/assets/uploads/2020/10/Infographic_Oct_01_2020.jpg"><img src="https://www.waseda.jp/top/en/assets/uploads/2020/10/Infographic_Oct_01_2020-2000x1125.jpg" alt="" width="2000" height="1125" srcset="https://www.waseda.jp/top/en/assets/uploads/2020/10/Infographic_Oct_01_2020-2000x1125.jpg 2000w, https://www.waseda.jp/top/en/assets/uploads/2020/10/Infographic_Oct_01_2020-610x343.jpg 610w, https://www.waseda.jp/top/en/assets/uploads/2020/10/Infographic_Oct_01_2020-768x432.jpg 768w" sizes="(max-width: 2000px) 100vw, 2000px"></a></p>
<p>Three-dimensional (3D) printing technology has evolved tremendously over the last decade to the point where it is now viable for mass production in industrial settings. Also known as â€œadditive manufacturing,â€ 3D printing allows one to create arbitrarily complex 3D objects directly from their raw materials. In fused filament fabrication, the most popular 3D printing process, a plastic or metal is melted and extruded through a small nozzle by a printer head and then immediately solidifies and fuses with the rest of the piece. However, because the melting points of plastics and metals are very different, this technology has been limited to creating objects of either metal or plastic onlyâ€”until now.</p>
<p>In a recent study published in <a href="https://www.sciencedirect.com/science/article/pii/S2214860420309283?via%3Dihub">Additive Manufacturing</a>, scientists from Waseda University, Japan, developed a new hybrid technique that can produce 3D objects made of both metal and plastic. <a href="http://www.umeshin.mmech.waseda.ac.jp/en/">Professor Shinjiro Umezu</a>, who led the study, explains their motivation: â€œEven though 3D printers let us create 3D structures from metal and plastic, most of the objects we see around us are a combination of both, including electronic devices. Thus, we thought weâ€™d be able to expand the applications of conventional 3D printers if we managed to use them to create 3D objects made of both metal and plastic.â€</p>
<p>Their method is actually a major improvement over the conventional metallization process used to coat 3D plastic structures with metal. In the conventional approach, the plastic object is 3D-printed and then submerged in a solution containing palladium (Pd), which adheres to the objectâ€™s surface. Afterwards, the piece is submerged in an electroless plating bath that, using the deposited Pd as a catalyst, causes dissolved metal ions to stick to the object. While technically sound, the conventional approach produces a metallic coating that is non-uniform and adheres poorly to the plastic structure.</p>
<p>In contrast, in the new hybrid method, a printer with a dual nozzle is used; one nozzle extrudes standard melted plastic (acrylonitrile butadiene styrene, or ABS) whereas the other extrudes ABS loaded with PdCl2. By selectively printing layers using one nozzle or the other, specific areas of the 3D object are loaded with Pd. Then, through electroless plating, one finally obtains a plastic structure with a metallic coating over selected areas only.</p>
<p>The scientists found the adhesion of the metal coating to be much higher when using their approach. Whatâ€™s more, because Pd is loaded in the raw material, their technique does not require any type of roughening or etching of the ABS structure to promote the deposition of the catalyst, unlike the conventional method. This is especially important when considering that these extra steps cause damage not only to the 3D object itself, but to the environment as well, owing to the use of toxic chemicals like chromic acid. Lastly, their approach is entirely compatible with existing fused filament fabrication 3D printers.</p>
<p>Umezu believes that metalâ€“plastic hybrid 3D printing could become very relevant in the near future considering its potential use in 3D electronics, which is the focus of upcoming Internet-of-Things and artificial intelligence applications. In this regard, he adds: â€œOur hybrid 3D printing method has opened up the possibility of fabricating 3D electronics so that devices and robots used in healthcare and nursing care could become significantly better than what we have today.â€</p>
<p>This study hopefully paves the way for hybrid 3D printing technology that will enable us to get the best of both worldsâ€”metal and plastic combined.</p>
<h3>Reference</h3>
<h4>Authors</h4>
<p>Jing Zhan<sup>(a)</sup>, Takayuki Tamura<sup>(b)</sup>, Gyotong Ri<sup>(b)</sup>, Zhenghao Ma<sup>(b)</sup>, Michinari Sone<sup>(c)</sup>, Masahiro Yoshino<sup>(c)</sup>, Shinjiro Umezu<sup>(b)</sup>, and Hirotaka Sato<sup>(a)</sup></p>
<h4>Title of original paper</h4>
<p>Metal-Plastic Hybrid 3D Printing Using Catalyst-Loaded Filament and Electroless Plating</p>
<h4>Journal</h4>
<p>Additive Manufacturing</p>
<h4>DOI</h4>
<p>10.1016/j.addma.2020.101556</p>
<h4>Affiliations</h4>
<ul>
<li>a: School of Mechanical and Aerospace Engineering, Nanyang Technological University</li>
<li>b: Department of Modern Mechanical Engineering, Waseda University</li>
<li>c: Research and Development div., Yoshino Denka Kogyo, Inc.</li>
</ul>
<p><img src="https://www.waseda.jp/top/en/assets/uploads/2020/10/umezu.jpg" alt="" width="1144" height="817" srcset="https://www.waseda.jp/top/en/assets/uploads/2020/10/umezu.jpg 1144w, https://www.waseda.jp/top/en/assets/uploads/2020/10/umezu-610x436.jpg 610w, https://www.waseda.jp/top/en/assets/uploads/2020/10/umezu-768x548.jpg 768w" sizes="(max-width: 1144px) 100vw, 1144px"></p>
              </div>
                          </div></div>]]>
            </description>
            <link>https://www.waseda.jp/top/en/news/73810</link>
            <guid isPermaLink="false">hacker-news-small-sites-24700553</guid>
            <pubDate>Tue, 06 Oct 2020 17:56:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Q3 Linux touchpad update: Multitouch gesture test packages now ready]]>
            </title>
            <description>
<![CDATA[
Score 413 | Comments 133 (<a href="https://news.ycombinator.com/item?id=24700537">thread link</a>) | @wbharding
<br/>
October 6, 2020 | https://bill.harding.blog/2020/10/06/q3-linux-touchpad-like-macbook-update-multitouch-gesture-test-packages-are-ready/ | <a href="https://web.archive.org/web/*/https://bill.harding.blog/2020/10/06/q3-linux-touchpad-like-macbook-update-multitouch-gesture-test-packages-are-ready/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="comments">

		<div id="respond">
		<h3 id="reply-title">Leave a Reply <small></small></h3><form action="https://bill.harding.blog/wp-comments-post.php" method="post" id="commentform" novalidate=""><p><span id="email-notes">Your email address will not be published.</span> Required fields are marked <span>*</span></p><p><label for="comment">Comment</label> </p><p><label for="author">Name <span>*</span></label> </p>
<p><label for="email">Email <span>*</span></label> </p>
<p><label for="url">Website</label> </p>
<p> <label for="wp-comment-cookies-consent">Save my name, email, and website in this browser for the next time I comment.</label></p>
<!-- Anti-spam plugin wordpress.org/plugins/anti-spam/ --><div><p><label>Current ye@r <span>*</span></label>
					
					
				  </p>

</div><!--\End Anti-spam plugin --></form>	</div><!-- #respond -->
	
</div></div>]]>
            </description>
            <link>https://bill.harding.blog/2020/10/06/q3-linux-touchpad-like-macbook-update-multitouch-gesture-test-packages-are-ready/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24700537</guid>
            <pubDate>Tue, 06 Oct 2020 17:54:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A quick introduction to data parallelism in Julia]]>
            </title>
            <description>
<![CDATA[
Score 155 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24700436">thread link</a>) | @amkkma
<br/>
October 6, 2020 | https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/ | <a href="https://web.archive.org/web/*/https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> <p>If you have a large collection of data and have to do similar computations on each element, <a href="https://en.wikipedia.org/wiki/Data_parallelism">data parallelism</a> is an easy way to speedup computation using multiple CPUs and machines as well as GPU(s). While this is not the only kind of parallelism, it covers a vast class of compute-intensive programs. A major hurdle for using data parallelism is that you need to unlearn some habits useful in sequential computation (i.e., patterns result in mutations of data structure). In particular, it is important to use libraries that help you describe <em>what</em> to compute rather than <em>how</em> to compute. Practically, it means to use generalized form of map and reduce operations and learn how to express your computation in terms of them. Luckily, if you already know how to write <a href="https://docs.julialang.org/en/v1/manual/arrays/#Generator-Expressions">iterator comprehensions</a>, there is not much more to learn for accessing a large class of data parallel computations.</p>  <p>This introduction primary focuses on the Julia packages that I (Takafumi Arakaki <strong><code>@tkf</code></strong>) have developed. As a result, it currently focuses on thread-based parallelism. There is simple distributed computing support. GPU support is a frequently requested feature but <a href="https://github.com/JuliaFolds/Transducers.jl/issues/236">it hasn't been implemented yet</a>. See also <a href="https://juliafolds.github.io/data-parallelism/explanation/libraries/">other parallel-computation libraries in Julia</a>.</p> <p>Also note that this introduction does not discuss how to use threading primitives such as <a href="https://docs.julialang.org/en/v1/base/multi-threading/"><code>Threads.@spawn</code></a> since it is too low-level and error-prone. For data parallelism, a higher-level description is much more appropriate. It also helps you write more reusable code; e.g., using the same code for single-threaded, multi-threaded, and distributed computing.</p>   <h2 id="getting_julia_and_libraries"><a href="#getting_julia_and_libraries">Getting <code>julia</code> and libraries</a></h2> <p>Most of the examples here may work in all Julia 1.x releases. However, for the best result, it is highly recommended to get the latest released version (1.5.2 as of writing). You can download it at <a href="https://julialang.org/">https://julialang.org/</a>.</p> <p>Once you get <code>julia</code>, you can get the dependencies required for this tutorial by running <code>using Pkg; Pkg.add(["Transducers", "ThreadsX", "OnlineStats", "FLoops", "MicroCollections", "BangBang", "Plots", "BenchmarkTools"])</code> in Julia REPL.</p> <p>If you prefer using exactly the same environment used for testing this tutorial, run the following commands</p> <pre><code>git <span>clone</span> https://github.com/JuliaFolds/data-parallelism
<span>cd</span> data-parallelism
julia --project</code></pre> <p>and then in the Julia REPL:</p> <pre><code><span>julia&gt;</span><span> <span>using</span> Pkg
</span>
<span>julia&gt;</span><span> Pkg.instantiate()</span></code></pre> <h2 id="starting_julia"><a href="#starting_julia">Starting <code>julia</code></a></h2> <p>To use multi-threading in Julia, you need to start it with multiple execution threads. If you have Julia 1.5 or higher, you can start it with the <code>-t auto</code> (or, equivalently, <code>--threads auto</code>) option:</p> <pre><code>$ julia -t auto
               _
   _       _ _(_)_     |  Documentation: https://docs.julialang.org
  (_)     | (_) (_)    |
   _ _   _| |_  __ _   |  Type "?" for help, "]?" for Pkg help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 1.5.2 (2020-09-23)
 _/ |\__'_|_|_|\__'_|  |  Official https://julialang.org/ release
|__/                   |

julia&gt; Threads.nthreads()  # number of core you have
8</code></pre> <p>The command line option <code>-t</code>/<code>--threads</code> can also take the number of threads to be used. In older Julia releases, use the <code>JULIA_NUM_THREADS</code> environment variable. For example, on Linux and macOS, <code>JULIA_NUM_THREADS=4 julia</code> starts <code>juila</code> with 4 execution threads.</p> <p>For more information, see <a href="https://docs.julialang.org/en/v1/manual/multi-threading/#Starting-Julia-with-multiple-threads">Starting Julia with multiple threads</a> in the Julia manual.</p> <h3 id="starting_julia_with_multiple_worker_processes"><a href="#starting_julia_with_multiple_worker_processes">Starting <code>julia</code> with multiple worker processes</a></h3> <p>A few examples below mention <a href="https://docs.julialang.org/en/v1/stdlib/Distributed/">Distributed.jl</a>-based parallelism. Like how multi-threading is setup, you need to setup multiple worker processes to get speedup. You can start <code>julia</code> with <code>-p auto</code> (or, equivalently, <code>--procs auto</code>). Distributed.jl also lets you add worker processes after starting Julia with <a href="https://docs.julialang.org/en/v1/stdlib/Distributed/#Distributed.addprocs"><code>addprocs</code></a>:</p> <pre><code><span>using</span> Distributed
addprocs(<span>8</span>)</code></pre> <p>For more information, see <a href="https://docs.julialang.org/en/v1/manual/distributed-computing/#Starting-and-managing-worker-processes">Starting and managing worker processes</a> section in the Julia manual.</p> <h2 id="mapping"><a href="#mapping">Mapping</a></h2> <p>Mapping is probably the most frequently used function in data parallelism. Recall how Julia's sequential <code>map</code> works:</p> <pre><code>a1 = map(string, <span>1</span>:<span>9</span>, <span>'a'</span>:<span>'i'</span>)</code></pre>
<pre><code>9-element Array{String,1}:
 "1a"
 "2b"
 "3c"
 "4d"
 "5e"
 "6f"
 "7g"
 "8h"
 "9i"</code></pre>
<p>We can simply replace it with <a href="https://github.com/tkf/ThreadsX.jl"><code>ThreadsX.map</code></a> for thread-based parallelism (see also <a href="https://juliafolds.github.io/data-parallelism/explanation/libraries/">other libraries</a>):</p>
<pre><code><span>using</span> ThreadsX
a2 = ThreadsX.map(string, <span>1</span>:<span>9</span>, <span>'a'</span>:<span>'i'</span>)
<span>@assert</span> a1 == a2</code></pre>

<p>Julia's standard library Distributed.jl contains <a href="https://docs.julialang.org/en/v1/stdlib/Distributed/#Distributed.pmap"><code>pmap</code></a> as a distributed version of <code>map</code>:</p>
<pre><code><span>using</span> Distributed
a3 = pmap(string, <span>1</span>:<span>9</span>, <span>'a'</span>:<span>'i'</span>)
<span>@assert</span> a1 == a3</code></pre>

<div><div><p>ğŸ”¬ Test Code</p>
<pre><code><span>using</span> Test
    <span>@testset</span> <span>begin</span>
        <span>@test</span> a1 == a2
        <span>@test</span> a1 == a3
    <span>end</span></code></pre></div> <div><p>â˜‘ Pass</p>
<pre><code>Test Summary: | Pass  Total
test set      |    2      2
</code></pre></div></div>
<div><p>ğŸ’¡ Note</p>
<p>In the above example, the inputs (<code>1:9</code> and <code>'a':'i'</code>) are too small for multi-threading to be useful.  In this tutorial, almost all examples except "Practical example" are toy examples that are designed to demonstrate how the functions work.  It is an "exercise" for the reader to try a larger input sizes and see on what size multi-threading becomes useful.</p></div>
<h3 id="practical_example_stopping_time_of_collatz_function"><a href="#practical_example_stopping_time_of_collatz_function">Practical example: Stopping time of Collatz function</a></h3>
<p>As a slightly more "practical" example, let's play with the <a href="https://en.wikipedia.org/wiki/Collatz_conjecture">Collatz conjecture</a> which states that recursive application the <em>Collatz function</em> defined as</p>
<pre><code>collatz(x) =
    <span>if</span> iseven(x)
        x Ã· <span>2</span>
    <span>else</span>
        <span>3</span>x + <span>1</span>
    <span>end</span></code></pre>

<p>reaches the number 1 for all positive integers.</p>
<p>I'll skip the mathematical background of it (as I don't know much about it) but let me mention that there are plenty of fun-to-watch explanations in YouTube :)</p>
<p>If the conjecture is correct, the number of iteration required for the initial value is finite.  In Julia, we can calculate it with</p>
<pre><code><span>function</span> collatz_stopping_time(x)
    n = <span>0</span>
    <span>while</span> <span>true</span>
        x == <span>1</span> &amp;&amp; <span>return</span> n
        n += <span>1</span>
        x = collatz(x)
    <span>end</span>
<span>end</span></code></pre>

<p>Just for fun, let's plot the stopping time of the initial values from 1 to 10,000:</p>
<pre><code><span>using</span> Plots
plt = scatter(
    map(collatz_stopping_time, <span>1</span>:<span>10_000</span>),
    xlabel = <span>"Initial value"</span>,
    ylabel = <span>"Stopping time"</span>,
    label = <span>""</span>,
    markercolor = <span>1</span>,
    markerstrokecolor = <span>1</span>,
    markersize = <span>3</span>,
    size = (<span>450</span>, <span>300</span>),
)</code></pre>
<p><img src="https://juliafolds.github.io/data-parallelism/assets/tutorials/quick-introduction/code/output/collatz_stopping_time_scatter.png" alt=""></p><p>We can easily parallelize <code>map(collatz_stopping_time, 1:10_000)</code> and get a good speedup:</p>
<pre><code><span>julia&gt;</span><span> Threads.nthreads()  
</span>4

<span>julia&gt;</span><span> <span>using</span> BenchmarkTools
</span>
<span>julia&gt;</span><span> <span>@btime</span> map(collatz_stopping_time, <span>1</span>:<span>100_000</span>);
</span>  18.116 ms (2 allocations: 781.33 KiB)

<span>julia&gt;</span><span> <span>@btime</span> ThreadsX.map(collatz_stopping_time, <span>1</span>:<span>100_000</span>);
</span>  5.391 ms (1665 allocations: 7.09 MiB)</code></pre>
<h2 id="iterator_comprehensions"><a href="#iterator_comprehensions">Iterator comprehensions</a></h2>
<p>Julia's <a href="https://docs.julialang.org/en/v1/manual/arrays/#Generator-Expressions">iterator comprehension syntax</a> is a powerful tool for composing mapping, filtering, and flattening. Recall that mapping can be written as an array or iterator comprehension:</p>
<pre><code>b1 = map(x -&gt; x + <span>1</span>, <span>1</span>:<span>3</span>)
b2 = [x + <span>1</span> <span>for</span> x <span>in</span> <span>1</span>:<span>3</span>]         
b3 = collect(x + <span>1</span> <span>for</span> x <span>in</span> <span>1</span>:<span>3</span>)  
<span>@assert</span> b1 == b2 == b3
b1</code></pre>
<pre><code>3-element Array{Int64,1}:
 2
 3
 4</code></pre>
<p>The iterator comprehension can be executed with threads by using <a href="https://github.com/tkf/ThreadsX.jl"><code>ThreadsX.collect</code></a>:</p>
<pre><code>b4 = ThreadsX.collect(x + <span>1</span> <span>for</span> x <span>in</span> <span>1</span>:<span>3</span>)
<span>@assert</span> b1 == b4</code></pre>

<div><div><p>ğŸ”¬ Test Code</p>
<pre><code><span>using</span> Test
    <span>@testset</span> <span>begin</span>
        <span>@test</span> b1 == b2 == b3
    <span>end</span></code></pre></div> <div><p>â˜‘ Pass</p>
<pre><code>Test Summary: | Pass  Total
test set      |    1      1
</code></pre></div></div>
<p>Note that more complex composition of mapping, filtering, and flattening can also be executed in parallel:</p>
<pre><code>c1 = ThreadsX.collect(y <span>for</span> x <span>in</span> <span>1</span>:<span>3</span> <span>if</span> isodd(x) <span>for</span> y <span>in</span> <span>1</span>:x)</code></pre>
<pre><code>4-element Array{Int64,1}:
 1
 1
 2
 3</code></pre>
<p><a href="https://juliafolds.github.io/Transducers.jl/dev/reference/manual/#Transducers.dcollect"><code>Transducers.dcollect</code></a> is for using iterator comprehensions with a distributed backend:</p>
<pre><code><span>using</span> Transducers
c2 = dcollect(y <span>for</span> x <span>in</span> <span>1</span>:<span>3</span> <span>if</span> isodd(x) <span>for</span> y <span>in</span> <span>1</span>:x)
<span>@assert</span> c1 == c2</code></pre>

<div><div><p>ğŸ”¬ Test Code</p>
<pre><code><span>@test</span> c1 == c2 == [<span>1</span>, <span>1</span>, <span>2</span>, <span>3</span>]</code></pre></div> </div>
<h2 id="pre-defined_reductions"><a href="#pre-defined_reductions">Pre-defined reductions</a></h2>
<p>Functions such as <code>sum</code>, <code>prod</code>, <code>maximum</code>, and <code>all</code> are the examples of <em>reduction</em> (aka <a href="https://en.wikipedia.org/wiki/Fold_(higher-order_function)"><em>fold</em></a>) that can be parallelized.  They are very powerful tools when combined with iterator comprehensions.  Using ThreadsX.jl, a sum of an iterator created by the comprehension syntax</p>
<pre><code>d1 = sum(x + <span>1</span> <span>for</span> x <span>in</span> <span>1</span>:<span>3</span>)</code></pre>
<pre><code>9</code></pre>
<p>can easily be parallelized by</p>
<pre><code>d2 = ThreadsX.sum(x + <span>1</span> <span>for</span> x <span>in</span> <span>1</span>:<span>3</span>)</code></pre>
<pre><code>9</code></pre>
<div><div><p>ğŸ”¬ Test Code</p>
<pre><code><span>@test</span> d1 == d2</code></pre></div> </div>
<p>For the full list of pre-defined reductions and other parallelized functions, type <code>ThreadsX.</code> and press <kbd>TAB</kbd> in the REPL.</p>
<h3 id="practical_example_maximum_stopping_time_of_collatz_function"><a href="#practical_example_maximum_stopping_time_of_collatz_function">Practical example: Maximum stopping time of Collatz function</a></h3>
<p>We can use <code>maximum</code> to compute the maximum stopping time of Collatz function on a given the range of initial values</p>
<pre><code>max_time = ThreadsX.maximum(collatz_stopping_time, <span>1</span>:<span>100_000</span>)</code></pre>
<pre><code>350</code></pre>
<div><div><p>ğŸ”¬ Test Code</p>
<pre><code><span>@test</span> max_time == <span>350</span></code></pre></div> </div>
<p>We get a speedup similar to the <code>map</code> example above:</p>
<pre><code><span>julia&gt;</span><span> <span>@btime</span> maximum(collatz_stopping_time, <span>1</span>:<span>100_000</span>)
</span>  17.625 ms (0 allocations: 0 bytes)
350

<span>julia&gt;</span><span> <span>@btime</span> ThreadsX.maximum(collatz_stopping_time, <span>1</span>:<span>100_000</span>)
</span>  5.024 ms (1214 allocations: 69.17 KiB)
350</code></pre>
<h3 id="onlinestatsjl"><a href="#onlinestatsjl">OnlineStats.jl</a></h3>
<p><a href="https://github.com/joshday/OnlineStats.jl">OnlineStats.jl</a> provides a <a href="https://joshday.github.io/OnlineStats.jl/latest/stats_and_models/">very rich</a> and <a href="https://joshday.github.io/OnlineStats.jl/latest/collections/">composable</a> set of reductions.  You can pass it as the first argument to <a href="https://github.com/tkf/ThreadsX.jl#onlinestatsjl"><code>ThreadsX.reduce</code></a>:</p>
<pre><code><span>using</span> OnlineStats: Mean
e1 = ThreadsX.reduce(Mean(), <span>1</span>:<span>10</span>)</code></pre>
<pre><code>Mean: n=10 | value=5.5</code></pre>
<div><div><p>ğŸ”¬ Test Code</p>
<pre><code><span>using</span> OnlineStats; <span>@test</span> e1 == fit!(Mean(), <span>1</span>:<span>10</span>)</code></pre></div> </div>
<div><p>ğŸ’¡ Note</p>
<p>While OnlineStats.jl often does not provide the fastest way to compute the given statistics when all the intermediate data can fit in memory, in many cases you don't really need the absolute best performance. However, it may be worth considering other ways to compute statistics if ThreadsX.jl + OnlineStats.jl becomes the bottleneck.</p></div>
<h2 id="manual_reductions"><a href="#manual_reductions">Manual reductions</a></h2>
<p>For non-trivial parallel computations, you need to write a custom reduction.  <a href="https://github.com/JuliaFolds/FLoops.jl">FLoops.jl</a> provides a concise set of syntax for writing custom reductions.  For example, this is how to compute sums of two quantities in one sweep:</p>
<pre><code><span>using</span> FLoops

<span>@floop</span> <span>for</span> (x, y) <span>in</span> zip(<span>1</span>:<span>3</span>, <span>1</span>:<span>2</span>:<span>6</span>)
    a = x + y
    b = x - y
    <span>@reduce</span>(s += a, t += b)
<span>end</span>
(s, t)</code></pre>
<pre><code>(15, -3)</code></pre> <div><div><p>ğŸ”¬ Test Code</p>
<pre><code><span>@test</span> (s, t) == (<span>15</span>, -<span>3</span>)</code></pre></div> </div>
<p>In this example, we do not initialize <code>s</code> and <code>t</code>; but it is not a typo.  In parallel sum, the only reasonable value of the initial state of the accumulators like <code>s</code> and <code>t</code> is zero.  So, <code>@reduce(s += a, t
+= b)</code> works as if <code>s</code> and <code>t</code> are initialized to appropriate type of zero.  However, since there are many zeros in Julia (<code>0::Int</code>, <code>0.0::Float64</code>, <code>(0x00 + 0x00im)::Complex{UInt8}</code>, ...), <code>s</code> and <code>t</code> are undefined if the input collection (i.e., the value of <code>xs</code> in <code>for
x in xs</code>) is empty.</p>
<p>To control the type of the â€¦</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/">https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/</a></em></p>]]>
            </description>
            <link>https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24700436</guid>
            <pubDate>Tue, 06 Oct 2020 17:44:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Gradient Boosted Decision Trees]]>
            </title>
            <description>
<![CDATA[
Score 141 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24700250">thread link</a>) | @simonwardjones
<br/>
October 6, 2020 | https://www.simonwardjones.co.uk/posts/gradient_boosted_decision_trees/ | <a href="https://web.archive.org/web/*/https://www.simonwardjones.co.uk/posts/gradient_boosted_decision_trees/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

<article>
  <div>
    <div>
      
      
      
      <p>What is a <code>gradient boosted decision tree</code>? ğŸ¤·â€â™‚ï¸</p>
<p>This article is the fifth in a series covering fundamental machine learning algorithms. Each post will be split into two parts</p>
<ol>
<li><a href="#the-idea-and-key-concepts"><strong>The idea and key concepts</strong></a>
- Most people should be able to follow this section and learn how the algorithm works</li>
<li><a href="#the-maths"><strong>The maths</strong></a>
- This is for the interested reader and will include detailed mathematical derivations followed by an implementation in Python</li>
</ol>
<p>Click</p>
<ul>
<li><a href="https://www.simonwardjones.co.uk/posts/linear_regression/">here</a> if you missed <code>From zero to Linear Regression</code></li>
<li><a href="https://www.simonwardjones.co.uk/posts/logistic_regression/">here</a> if you missed <code>From zero to Logistic Regression</code></li>
<li><a href="https://www.simonwardjones.co.uk/posts/decision_trees/">here</a> if you missed <code>From zero to Decision Tree</code></li>
<li><a href="https://www.simonwardjones.co.uk/posts/random_forests/">here</a> if you missed <code>From zero to Random Forest</code></li>
</ul>
<p>Great if you have already read these!</p>
<hr>
<h2 id="the-idea-and-key-concepts">The idea and key concepts</h2>
<p>In the last post we talked about <code>underfitting</code>, <code>overfitting</code>, <code>bias</code> and <code>variance</code>. We explained how a <code>random forest</code> uses the average output of multiple trees to reduce the chance of overfitting without introducing bias by oversimplifying (such as using only one tree but restricting the depth).</p>
<p><code>Gradient boosting</code> is a machine learning technique for regression and classification where multiple models are trained <code>sequentially</code> with each model trying to learn the mistakes from the previous models. The individual models are known as <code>weak learners</code> and in the case of <code>gradient boosted decision trees</code> the individual models are decision trees.</p>
<p>In order to give intuition it is easiest to consider first the case of regression. Imagine we are again trying to predict house prices in a desirable area of north London. With training data that looks like the following</p>





<table <thead="">
<tbody><tr>
<th></th>
<th>House size ğŸ </th>
<th>Garden size ğŸŒ³</th>
<th>Garage? ğŸš™</th>
<th>True House Price ğŸ’°</th>
</tr>

</tbody><tbody>
<tr>
<td>1</td>
<td>1000</td>
<td>700</td>
<td>Garage</td>
<td>Â£1m</td>
</tr>
<tr>
<td>2</td>
<td>770</td>
<td>580</td>
<td>No Garage</td>
<td>Â£0.75m</td>
</tr>
<tr>
<td>3</td>
<td>660</td>
<td>200</td>
<td>Garage</td>
<td>Â£0.72m</td>
</tr>
</tbody>
</table>

<p><strong>Initial prediction $f_0$</strong></p>
<p>We can make an initial prediction for each of the house prices based on an initial model, letâ€™s call this initial model $f_0$. Often this model is very simple - just using the mean of the target variable in the training data. The following table shows the initial predictions as well as the <code>errors</code> $e_1$ (also known as the <code>residuals</code>) defined for each sample as $e_1 = y - f_0$ where $y$ is the true value and $f_0$ is our initial prediction</p>





<table <thead="">
<tbody><tr>
<th></th>
<th>True House Price ğŸ’°</th>
<th>Initial Prediction $f_0$</th>
<th>Error $e_1$</th>
</tr>

</tbody><tbody>
<tr>
<td>1</td>
<td>Â£1m</td>
<td>Â£0.82m</td>
<td>Â£(1m - 0.82) = Â£0.18m</td>
</tr>
<tr>
<td>2</td>
<td>Â£0.75m</td>
<td>Â£0.82m</td>
<td>Â£(0.75m - 0.82m) = -Â£0.07m</td>
</tr>
<tr>
<td>3</td>
<td>Â£0.72m</td>
<td>Â£0.82m</td>
<td>Â£(0.72m - 0.82m) = -Â£0.1m</td>
</tr>
</tbody>
</table>

<p><strong>Predicting the error</strong></p>
<p>Our initial prediction isnâ€™t very accurate as it is just the mean house price of the training data! In order to improve this we introduce another model $f_1$ trying to predict the error $e_1$ from the sample feature values. In gradient boosted decision trees this model is itself a decision tree. So now we can predict what the error $e_1$ will be for each sample using $f_1$</p>





<table <thead="">
<tbody><tr>
<th></th>
<th>True House Price ğŸ’°</th>
<th>Initial Prediction $f_0$</th>
<th>Error $e_1$</th>
<th>Predicted Error $f_1$</th>
</tr>

</tbody><tbody>
<tr>
<td>1</td>
<td>Â£1m</td>
<td>Â£0.82m</td>
<td>Â£(1m - 0.82) = Â£0.18m</td>
<td>Â£0.17m</td>
</tr>
<tr>
<td>2</td>
<td>Â£0.75m</td>
<td>Â£0.82m</td>
<td>Â£(0.75m - 0.82m) = -Â£0.07m</td>
<td>Â£-0.09m</td>
</tr>
<tr>
<td>3</td>
<td>Â£0.72m</td>
<td>Â£0.82m</td>
<td>Â£(0.72m - 0.82m) = -Â£0.1m</td>
<td>Â£-0.1m</td>
</tr>
</tbody>
</table>

<p><strong>Updating our prediction using the error prediction</strong></p>
<p>For the first house our initial prediction $f_0$ was Â£0.82m (using the mean) and as we actually know the true value we can see this gave an error $e_1$ of 0.18m. We then trained $f_1$ - a decision tree - to predict the error $e_1$ for each sample. In practise this is only a prediction of the error so it wont be exactly equal, in this toy example our $f_1$ model predicted an error of Â£0.17m. We could now combine the two models into a new second prediction called $F_1$ by adding the predicted error $f_1$ to the initial prediction $f_0$ as in the table below</p>





<table <thead="">
<tbody><tr>
<th></th>
<th>True House Price ğŸ’°</th>
<th>Initial Prediction $f_0$</th>
<th>Predicted Error $f_1$</th>
<th>Prediction $F_1 =f_0 + f_1$</th>
</tr>

</tbody><tbody>
<tr>
<td>1</td>
<td>Â£1m</td>
<td>Â£0.82m</td>
<td>Â£0.17m</td>
<td>Â£0.99m</td>
</tr>
<tr>
<td>2</td>
<td>Â£0.75m</td>
<td>Â£0.82m</td>
<td>-Â£0.09m</td>
<td>Â£0.73m</td>
</tr>
<tr>
<td>3</td>
<td>Â£0.72m</td>
<td>Â£0.82m</td>
<td>-Â£0.1m</td>
<td>Â£0.71m</td>
</tr>
</tbody>
</table>

<p><strong>Additive model</strong></p>
<p>Now we have a second prediction $F_1$ we can continue in a sequential manner, again calculating the error of our second prediction $e_2$ and training a tree $f_2$ to predict this second error. Then once again we add this second predicted error to the second prediction to get a third prediction $F_2 = F_1 + f_2$ and so on. As the models are summed together this approach is known as an <code>aditive model</code>. In general we have
$$F_m =  F_{m-1} + f_m$$
Where the next prediction $F_m$ is made up of the current prediction $F_{m-1}$ and the prediction of the error $f_m \sim e_m =y - F_{m-1}$ at this stage. In general the number of <code>weak learners</code> is a <code>hyper parameter</code> you have to choose.</p>
<p><strong>learning rate</strong></p>
<p>We can think of each individual <code>weak learner</code> $f_m$ as stepping our predictions closer to the true target values $y$. To reduce the variance and overfitting rather than stepping the whole predicted error we can instead add only a fraction of the step controlled by the learning rate. So rather than
$$F_m =  F_{m-1} + f_m$$
In gradient boosting we use
$$F_m =  F_{m-1} + (\text{learning rate}*f_m)$$
This process requires more steps but reduces the variance and overfitting overall.</p>
<p><strong>Summary of the algorithm</strong></p>
<ol>
<li>Make initial model $f_0$ (often the mean of y)</li>
<li>Train decision tree model $f_1$ on the error $e_1 = y - f_0$ where y is the true value</li>
<li>Calculate new prediction $F_1 = f_0 + \eta * f_1$ where $\eta$ is the learning rate</li>
<li>Repeat 2, 3 as many times as chosen where in general
<ol>
<li>Train model $f_m$ on the error $e_m = y - F_{m-1}$</li>
<li>Calculate new prediction as $F_{m-1} + \eta * f_m$</li>
</ol>
</li>
</ol>
<p>In short gradient boosting uses an initial prediction and then sequentially updates this prediction by fitting a model to the error at that stage.</p>
<p>In the following section we explore the mathematical details and extend the algorithm to the classification setting. We also cover the intuition behind gradient boosting as gradient descent.</p>
<hr>
<h2 id="the-maths">The maths</h2>
<p><strong>Why is it called gradient boosting?</strong></p>
<p>In general in <code>supervised learning</code> we aim to find a model $F$ to fit the data such that the predicted value $\hat{y}_i$ for the $j$th training example $\mathbf{x}_i$ is approximately equal to the $j$th target value $y_i$ or equivalently</p>
<p>
    $$
\hat{y}_i=F(\mathbf{x}_i)\sim y_i \quad\forall j \in {1,\dots,n} 
$$
</p><p>
Where n is the number of training samples.</p>
<p>Equivalently we aim to minimise a loss function $\mathcal{L(y, \hat{y})}$ which tells us how badly the model $\hat{y}$ currently fits the data $y$.</p>
<p>In a <code>parametric</code> setting (e.g. logistic regression) the model can be written as

</p><p>
    $$
\hat{y}_i=F_{\mathbf{\theta}}(\mathbf{x}_i)
$$
</p>
<p>Where the subscript $\mathbf{\theta}$ indicates the models dependence on the parameters. We can also write the loss in terms of $\mathbf{\theta}$ as $\mathcal{L(y, \hat{y}(\mathbf{\theta})})$. In this setting we update the model parameters using gradient descent. That is we iteratively update the model parameters by stepping the parameters in the direction of the negative gradient of the loss function with respect to the parameters (where $\eta$ is the learning rate).</p>

<p>
    $$
\mathbf{\theta}^{m+1} = \mathbf{\theta}^{m} - \eta* \frac{\partial\mathcal{L}}{\partial{\mathbf{\theta}^m}}
$$
</p>
<p>Instead of differentiating the loss with respect to $\mathbf{\theta}$ we can differentiate with respect to the prediction $\hat{y}$ directly. If we think about gradient descent ideally we would update $\hat{y}$ as follows to reduce the cost function</p>

<p>
    $$
\hat{y}_i \to \hat{y}_i - \eta\frac{\partial\mathcal{L}}{\partial{\hat{y}_i}}
$$
</p>
<p>Equivalently we update $F_{m-1}$ by adding another â€œdelta modelâ€ $f_{m+1}$</p>

<p>
    $$
\hat{y}_i = F_m(\mathbf{x}_i) + f_{m+1}(\mathbf{x}_i) \quad\forall j \in {1,\dots,n} 
$$
</p>
<p>Where $\eta$ is the learning rate and

</p><p>
    $$
f_{m+1}(\mathbf{x}_i)= -\eta\frac{\partial\mathcal{L}}{\partial{\hat{y}_i}}
$$
</p>
<p>In practise we cannot set this delta model exactly so we train a model on the data to fit

</p><p>
    $$
 - \eta\frac{\partial\mathcal{L}}{\partial{\hat{y}_i}}$$
</p><p>
In general this gradient can be fitted with any model but gradient boosted decision trees use a decision tree - hence the name! Note each tree will have itâ€™s own Loss $\mathcal{L}^{f_{m+1}}$ separate to the global loss $\mathcal{L}$.</p>
<p><strong>Key Point</strong></p>
<p>The gradient boosted decision tree is not trained on the residuals at each step. Rather it is trained on the negative gradient of the loss function evaluated using the prediction of the current step - which happens to be the residual for some common cost functions.</p>
<h3 id="regression">Regression</h3>
<p>In the case of regression we define the loss function as the mean square error</p>
<p>$$
\mathcal{L}(\hat{y}) = \frac{1}{2n}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2
$$
hence
$$
-\eta\frac{\partial\mathcal{L}}{\partial{\hat{y}_i}} = \frac{\eta}{n}(y_i-\hat{y}_i)
$$</p>
<p>How the process looks:</p>
<p>We fit $f_0(x)\sim y$ then $F_0(x) = f_0(x)$<br>
We fit $f_1(x)\sim (y-F_0(x))$ then $F_1(x) = F_0(x) + \eta f_1(x)$<br>
We fit $f_2(x)\sim (y-F_1(x))$ then $F_2(x) = F_1(x) + \eta f_2(x)$<br>
We fit $f_3(x)\sim (y-F_2(x))$ then $F_3(x) = F_2(x) + \eta f_3(x)$<br>
â€¦<br>
We fit $f_m(x)\sim (y-F_{m-1}(x))$ then $F_m(x) = F_{m-1}(x) + \eta f_m(x)$</p>
<p>Then predictions $\hat{y} = F_m(x)$</p>
<h4 id="binomial-classification">Binomial Classification</h4>
<p>Suppose our iterative model was $\hat{y}_i = F_m(x_i)$ where the $\hat{y}_i$ directly represented the probability $x_i$ is in class 1. i.e. $P(x_i \in C_1)$ where $C_1$ represents class 1.</p>
<p>In this case the delta model doesnâ€™t make sense as we would be directly adding to a probability value. As in logistic regression it is often the case to fit the model to a transformation of probability.</p>
<p>We define a model
$$
\hat{y}\sim F(x)
$$
where
$$
\hat{p} = \frac{1}{1+e^{-\hat{y}}}
$$
so
$$
\hat{y} = \log\left(\frac{\hat{p}}{1-\hat{p}}\right)
$$</p>
<p>where $\hat{p}$ represents the probability of being in class 1, $\hat{y}$ is sometimes known as the logit.</p>
<p>Note $\hat{p}\in[0,1],\quad \hat{y}\in(-\infty,\infty),\quad y\in{0,1}$</p>
<p>Hence in the classification setting the gradient boosted decision tree predicts $\hat{y}$ as a sum of multiple delta models. The probability values are then calculated by transforming $\hat{y}$ using the sigmoid function (a.k.a the expit function).</p>
<p>We will use the following fact later on</p>

<p>
    $$
\begin{align}
\hat{p} &amp;= \frac{1}{1+e^{-\hat{y}}} \quad so \\
\hat{p} &amp;= â€¦</p></div></div></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.simonwardjones.co.uk/posts/gradient_boosted_decision_trees/">https://www.simonwardjones.co.uk/posts/gradient_boosted_decision_trees/</a></em></p>]]>
            </description>
            <link>https://www.simonwardjones.co.uk/posts/gradient_boosted_decision_trees/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24700250</guid>
            <pubDate>Tue, 06 Oct 2020 17:26:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Eliminating Task Processing Outages by Replacing RabbitMQ with Apache Kafka]]>
            </title>
            <description>
<![CDATA[
Score 93 | Comments 61 (<a href="https://news.ycombinator.com/item?id=24699534">thread link</a>) | @sciurus
<br/>
October 6, 2020 | https://doordash.engineering/2020/09/03/eliminating-task-processing-outages-with-kafka/ | <a href="https://web.archive.org/web/*/https://doordash.engineering/2020/09/03/eliminating-task-processing-outages-with-kafka/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
					<p><span>Scaling backend infrastructure to handle hyper-growth is one of the many exciting challenges of working at DoorDash. In mid 2019, we faced significant scaling challenges and frequent outages involving </span><a href="https://en.wikipedia.org/wiki/Celery_(software)"><span>Celery</span></a><span> and </span><a href="https://www.rabbitmq.com/"><span>RabbitMQ</span></a><span>, two technologies powering the system that handles the asynchronous work enabling critical functionalities of our platform, including order checkout and Dasher assignments.&nbsp;</span></p>
<p><span>We quickly solved this problem with a simple, </span><a href="https://kafka.apache.org/"><span>Apache Kafka</span></a><span>-based asynchronous task processing system that stopped our outages while we continued to iterate on a robust solution. Our initial version implemented the smallest set of features needed to accommodate a large portion of existing Celery tasks. Once in production, we continued to add support for more Celery features while addressing novel problems that arose when using Kafka. </span></p>
<h2><span>The problems we faced using Celery and RabbitMQ</span></h2>
<p><span>RabbitMQ and Celery were mission critical pieces of our infrastructure that powered over 900 different asynchronous tasks at DoorDash, including order checkout, merchant order transmission, and Dasher location processing. The problem DoorDash faced was that RabbitMQ was frequently going down due to excessive load. If task processing went down, DoorDash effectively went down and orders could not be completed, resulting in revenue loss for our merchants and Dashers, and a poor experience for our consumers. We faced issues on the following fronts:</span></p>
<ul>
<li><b>Availability:</b><span> Outages caused by demand reduced availability.&nbsp;</span></li>
<li><b>Scalability:</b><span> RabbitMQ could not scale with the growth of our business.&nbsp;</span></li>
<li><b>Observability:</b><span> RabbitMQ offered limited metrics and Celery workers were opaque.&nbsp;</span></li>
<li><b>Operational efficiency:</b><span> Restarting these components was a time-consuming, manual process.&nbsp;</span></li>
</ul>
<h3><span>Why our asynchronous task processing system wasnâ€™t highly available</span></h3>
<p><span>This biggest problem we faced were outages, and they often came when demand was at its peak. RabbitMQ would go down due to load, </span><a href="https://www.rabbitmq.com/connections.html#high-connection-churn"><span>excessive connection churn</span></a><span>, and other reasons. Orders would be halted, and weâ€™d have to restart our system or sometimes even bring up an entirely new broker and manually </span><a href="https://en.wikipedia.org/wiki/Failover"><span>failover</span></a><span> in order to recover from the outage.</span></p>
<p><span>On diving deeper into the availability issues, we found the following sub-issues:</span></p>
<ul>
<li><span>Celery allows users to schedule tasks in the future with a countdown or ETA. Our heavy use of&nbsp; these countdowns resulted in noticeable load increases on the broker. Some of our outages were directly related to an increase in tasks with countdowns. We ultimately decided to restrict the use of countdowns in favor of another system we had in place for scheduling work in the future.</span></li>
<li><span>Sudden bursts of traffic would leave RabbitMQ in a degraded state where task consumption was significantly lower than expected. In our experience, this could only be resolved with a RabbitMQ bounce. RabbitMQ has a concept of Flow Control where it will reduce the speed of connections which are publishing too quickly so that queues can keep up. Flow Control was often, but not always, involved in these availability degradations. When Flow Control kicks in, the publishers effectively see it as network latency. Network latency reduces our response times; if latency increases during peak traffic, significant slowdowns can result that cascade as requests pile up upstream.</span></li>
<li><span>Our python </span><a href="https://uwsgi-docs.readthedocs.io/en/latest/"><span>uWSGI</span></a><span> web workers had a feature called harakiri that was enabled to kill any processes that exceeded a timeout. During outages or slowdowns, harakiri resulted in a connection churn to the RabbitMQ brokers as processes were repeatedly killed and restarted. With thousands of web workers running at any given time, any slowness that triggered harakiri would in turn contribute even more to slowness by adding extra load to RabbitMQ.</span></li>
<li><span>In production we experienced several cases where task processing in the Celery consumers&nbsp; stopped, even in the absence of significant load. Our investigation efforts did not yield evidence of any resource constraints that wouldâ€™ve halted processing, and the workers resumed processing once they were bounced. This problem was never root caused, though we suspect an issue in the Celery workers themselves and not RabbitMQ.</span></li>
</ul>
<p><span>Overall, all of these availability issues were unacceptable for us as high reliability is one of our highest priorities. Since these outages were costing us a lot in terms of missed orders and credibility we needed a solution that would address these problems as soon as possible.</span></p>
<h3><span>Why our legacy solution did not scale&nbsp;</span></h3>
<p><span>The next biggest problem was scale. DoorDash is growing fast and we were quickly reaching the limits of our existing solution. We needed to find something that would keep up with our continued growth since our legacy solution had the following problems:&nbsp;</span></p>
<p><strong>Hitting the vertical scaling limit</strong></p>
<p><span>We were using the largest available single-node RabbitMQ solution that was available to us. There was no path to scale vertically any further and we were already starting to push that node to its limits.</span></p>
<p><strong>The High Availability mode limited our capacity&nbsp;</strong></p>
<p><span>Due to replication, the primary-secondary High Availability (HA) mode reduced throughput compared to the single node option, leaving us with even less headroom than just the single node solution. We could not afford to trade throughput for availability.</span></p>
<p><span>Secondly, the primary-secondary HA mode did not, in practice, reduce the severity of our outages. Failovers took more than 20&nbsp; minutes&nbsp; to complete and would often get stuck requiring manual intervention. Messages were often lost in the process as well.</span></p>
<p><span>We were quickly running out of headroom as DoorDash continued to grow and push our task processing to its limits. We needed a solution that could scale horizontally as our processing needs grew.</span></p>
<h3><span>How Celery and RabbitMQ offered limited observability</span></h3>
<p><span>Knowing whatâ€™s going on in any system is fundamental to ensuring its availability, scalability, and operational integrity.&nbsp;</span></p>
<p><span>As we navigated the issues outlined above, we noticed that :</span></p>
<ul>
<li><span>We were limited to a small set of RabbitMQ metrics available to us.</span></li>
<li><span>We had limited visibility into the Celery workers themselves.</span></li>
</ul>
<p><span>We needed to be able to see real-time metrics of every aspect of our system which meant the observability limitations needed to be addressed as well.&nbsp;</span></p>
<h3><span>The operational efficiency challenges</span></h3>
<p><span>We also faced several issues with operating RabbitMQ:</span></p>
<ul>
<li><span>We often had to failover our RabbitMQ node to a new one to resolve the persistent degradation we observed. This operation was manual and time consuming for the engineers involved and often had to be done late at night, outside of peak times.</span></li>
<li><span>There were no in-house Celery or RabbitMQ experts at DoorDash who we could lean on to help devise a scaling strategy for this technology.</span></li>
</ul>
<p><span>Engineering time spent operating and maintaining RabbitMQ was not sustainable. We needed something that better met our current and future needs.</span></p>
<h2><span>Potential solutions to our problems with Celery and RabbitMQ&nbsp;</span></h2>
<p><span>With the problems outlined above, we considered the following solutions:</span></p>
<ul>
<li><b>Change the Celery broker from RabbitMQ to Redis or Kafka. </b>This would allow us to continue using Celery, with a different and potentially more reliable backing datastore.</li>
</ul>
<ul>
<li><b>Add multi-broker support to our </b><a href="https://www.djangoproject.com/"><b>Django</b></a><b> app so consumers could publish to N different brokers based on whatever logic we wanted. </b>Task processing will get sharded across multiple brokers, so each broker will experience a fraction of the initial load.</li>
</ul>
<ul>
<li><b>Upgrade to newer versions of Celery and RabbitMQ. </b>Newer versions of Celery and RabbitMQ were expected to fix reliability issues, buying us time as we were already extracting components from our Django monolith in parallel.</li>
</ul>
<ul>
<li><b>Migrate to a custom solution backed by Kafka. </b>This solution takes more effort than the other options we listed, but also has more potential to solve every problem we were having with the legacy solution.</li>
</ul>
<p><span>Each option has its pros and cons:</span></p>
<table>
<tbody>
<tr>
<td><b>Option</b></td>
<td><b>Pros</b></td>
<td><b>Cons</b></td>
</tr>
<tr>
<td><span>Redis as broker&nbsp;</span></td>
<td>
<ul>
<li><span>Improved availability with ElasticCache and multi-AZ support</span></li>
<li><span>Improved broker observability with ElasticCache as the broker</span></li>
<li><span>Improved operational efficiency</span></li>
<li><span>In-house operational experience and expertise with Redis</span></li>
<li><span>A broker swap is straight-foward as a supported option in Celery</span></li>
<li><span>Harakiri connection churn does not significantly degrade Redis performance</span></li>
</ul>
</td>
<td>
<ul>
<li><span>Incompatible with Redis clustered mode</span></li>
<li><span>Single node Redis does not scale horizontally</span></li>
<li><span>No Celery observability improvements</span></li>
<li><span>This solution does not address the observed issue where Celery workers stopped processing tasks</span></li>
</ul>
</td>
</tr>
<tr>
<td><span>Kafka as broker</span></td>
<td>
<ul>
<li><span>Kafka can be highly available</span></li>
<li><span>Kafka is horizontally scalable</span></li>
<li><span>Improved observability with Kafka as the broker</span></li>
<li><span>Improved operational efficiency</span></li>
<li><span>DoorDash had in-house Kafka expertise</span></li>
<li><span>A broker swap is straight-foward as a supported option in Celery</span></li>
<li><span>Harakiri connection churn does not significantly degrade Kafka performance</span></li>
</ul>
</td>
<td>
<ul>
<li><span>Kafka is not supported by Celery yet&nbsp;</span></li>
<li><span>Does not address the observed issue where Celery workers stop processing tasks</span></li>
<li><span>No celery observability improvements</span></li>
<li><span>Despite in-house experience, we had not operated Kafka at scale at DoorDash.</span></li>
</ul>
</td>
</tr>
<tr>
<td><span>Multiple brokers</span></td>
<td>
<ul>
<li><span>Improved availability&nbsp;</span></li>
<li><span>Horizontal scalability</span></li>
</ul>
</td>
<td>
<ul>
<li><span>No observability improvements</span></li>
<li><span>No operational efficiency improvements</span></li>
<li><span>Does not address the observed issue where Celery workers stop processing tasks</span></li>
<li><span>Does not address the issue with harakiri-induced connection churn</span></li>
</ul>
</td>
</tr>
<tr>
<td><span>Upgrade versions</span></td>
<td>
<ul>
<li><span>Might improve the issue where RabbitMQ becomes stuck in a degraded state</span></li>
<li><span>Might improve the issue where Celery workers get stuck</span></li>
<li><span>Might buy us headroom to implement a longer term strategy</span></li>
</ul>
</td>
<td>
<ul>
<li><span>Not guaranteed to fix our observed bugs</span></li>
<li><span>Will not immediately fix our issues with availability, scalability, observability, and operational efficiency</span></li>
<li><span>Newer versions of RabbitMQ and Celery required newer versions of Python.</span></li>
<li><span>Does not address the issue with harakiri-induced connection churn</span></li>
</ul>
</td>
</tr>
<tr>
<td><span>Custom Kafka solution</span></td>
<td>
<ul>
<li><span>Kafka can be highly available</span></li>
<li><span>Kafka is horizontally scalable</span></li>
<li><span>Improved observability with Kakfa as the broker</span></li>
<li><span>Improved operational efficiency</span></li>
<li><span>In-house Kafka expertise</span></li>
<li><span>A broker change is â€¦</span></li></ul></td></tr></tbody></table></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://doordash.engineering/2020/09/03/eliminating-task-processing-outages-with-kafka/">https://doordash.engineering/2020/09/03/eliminating-task-processing-outages-with-kafka/</a></em></p>]]>
            </description>
            <link>https://doordash.engineering/2020/09/03/eliminating-task-processing-outages-with-kafka/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24699534</guid>
            <pubDate>Tue, 06 Oct 2020 16:26:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Teaching our five year old to code by cheating]]>
            </title>
            <description>
<![CDATA[
Score 119 | Comments 104 (<a href="https://news.ycombinator.com/item?id=24699448">thread link</a>) | @gregorymichael
<br/>
October 6, 2020 | https://baugues.com/cheat-code/ | <a href="https://web.archive.org/web/*/https://baugues.com/cheat-code/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<p>My wife and I became <a href="https://baugues.com/homeschool">reluctant homeschoolers</a> this year â€“ choosing to teach our five year old daughter without the our school's remote learning. Rachel teaches Reading, Writing, Arts, and Science. My job is Math, Chess, and Technology. </p><p>I started programming on a <a href="https://baugues.com/trs-80">TRS-80</a> when I was six or seven. Back then, the computer booted into BASIC, the most approachable programming language of all time. Hello World in BASIC looks something like:</p><pre><code>10 print "hello world" 
20 goto 10
</code></pre><p>Programming in BASIC was the most instant gratification you could get on a TRS-80. There were few games and no Internet. Had I been introduced to a different programming language at a different age, I'm not sure I would have taken to it.</p><p>That's been a problem I've been wrestling with when introducing our daughter, Emma, to programming. Modern developer environments have a lot of friction and overhead. We've played with Swift Playgrounds, which is great for introducing programming concepts, but feels like you're writing instructions inside a video game as opposed to harnessing the the raw power of code to control the computer.</p><p>A colleague recently introduced me to <a href="https://repl.it/talk/announcements/Announcing-Basic-Language-With-Graphics-Beta/31741">pg-basic on repl.it</a>, which recaptures the simplicity of writing BASIC on a TRS-80.</p><p>Emma and I are working on addition. She likes video games and coding, so I figured we could create a game to practice math. The general idea is: pick two numbers at random, ask her to add them, give her points if she gets it right. We did it in Python, as the code was't that dissimilar to its Basic equivalent. </p><p>Go ahead, run it. (And edit, if you wish.) </p><!--kg-card-begin: html--><!--kg-card-end: html--><p>I composed the code with her sitting next to me, asking for her suggestions along the way.</p><ul><li>"What should we name this variable?"</li><li>"How many points should you get when you get one right?"</li><li>"How many points do you need to win?"</li><li>"What should it say when you win?" </li></ul><p>Then I made her a deal: if she won the game two times, she could cheat and change the code. She loves cheating.</p><p>She quickly figured out she could change the lines that generate numbers to:</p><pre><code>lulu = 0
boonie = random.randrange(11)
</code></pre><p>Math problems got easier. Then she changed it to: </p><pre><code>lulu = 0
boonie = 0
</code></pre><p>Problems got <em>a lot</em> easier.</p><p>She still had to answer a bunch of questions to win the game, and typing zero and enter repeatedly is hard work, so she changed the looping condition to:</p><pre><code>while points &lt; 1:
</code></pre><p>It may be the first time in her short educational career when she's had control over the quiz, instead of the quiz having control over her.</p><p>Thinking back to how I started writing code, it was copying a few dozen lines of BASIC out of the back of <em><a href="http://games.datagrind.com/index.php?pageid=10">3-2-1 Contact</a></em>, getting it to run, and then tweaking it. Today, when I learn a new language or service, it's "copy, paste, edit."</p><p>Composing along side Emma and letting her edit seems to be a winning strategy. Yesterday, after making a modification, she thought for a few seconds, turned to look at me, and said, "... I can use code to do <em>anything</em>."</p><p>She's starting to get it.</p>
			</section></div>]]>
            </description>
            <link>https://baugues.com/cheat-code/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24699448</guid>
            <pubDate>Tue, 06 Oct 2020 16:22:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Try out the new Python 3.9 features in a Python sandbox]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24698705">thread link</a>) | @sunaden
<br/>
October 6, 2020 | https://deepnote.com/project/09e2609b-986b-40fa-9f56-fcbbc60eb61d | <a href="https://web.archive.org/web/*/https://deepnote.com/project/09e2609b-986b-40fa-9f56-fcbbc60eb61d">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><section><p><span>You can view this project and add comments, but can't make any changes.</span><span> <!-- -->You can try to<!-- --> <a>sign in</a> <!-- -->to request additional access.</span></p></section></div></div></div>]]>
            </description>
            <link>https://deepnote.com/project/09e2609b-986b-40fa-9f56-fcbbc60eb61d</link>
            <guid isPermaLink="false">hacker-news-small-sites-24698705</guid>
            <pubDate>Tue, 06 Oct 2020 15:26:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Time for a WTF MySQL Moment]]>
            </title>
            <description>
<![CDATA[
Score 321 | Comments 113 (<a href="https://news.ycombinator.com/item?id=24698660">thread link</a>) | @gbl08ma
<br/>
October 6, 2020 | https://gbl08ma.com/time-for-a-wtf-mysql-moment/ | <a href="https://web.archive.org/web/*/https://gbl08ma.com/time-for-a-wtf-mysql-moment/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content-holder" role="main">
	<div>
		<article id="post-21465">			
			
	<p>
				October 4, 2020 / gbl08ma / 0 Comments	</p>
	<p>Many people have been experiencing strange time perception phenomenon throughout 2020, but certain database management systems have been into time shenanigans for way longer. This came to my attention when a friend received the following exception in one of his projects (his popular Discord bot, <a href="https://accord.abcric.net/">Accord</a>), coming from the MySQL connector being used with EF Core:</p>
<pre>MySqlException: Incorrect TIME value: '960:00:00.000000'</pre>
<p>Not being too experienced with MySQL, as I prefer PostgreSQL for reasons that will soon become self-evident, for a brief moment I assumed the incorrection in this value was the hundreds of hours, as one could reasonably assume that maybe TIME values were capped at 24 hours, or that a different syntax was needed for values spanning multiple days, and that one would need to use, say, â€œ40:00:00:00â€ to represent 40 days. But reality turned out to be more complex and harder to explain.</p>
<p>With checking the documentation being the most natural next step, the MySQL documentation goes:</p>
<blockquote><p>MySQL retrieves and displays <code>TIME</code> values in <em><code>'hh:mm:ss'</code></em> format (or <em><code>'hhh:mm:ss'</code></em> format for large hours values).</p></blockquote>
<p>So far so good, our problematic TIME value respects this format, but the fact that <code>hh</code> and <code>hhh</code> are explicitly pointed out is already suspect (what about values with over 999 hours?). The next sentence in the documentation explains why, and left me with even more questions of the WTF kind:</p>
<blockquote><p><code>TIME</code> values may range from <code>'-838:59:59'</code> to <code>'838:59:59'</code>.</p></blockquote>
<p>Oooh Kaaayâ€¦ thatâ€™s an oddly specific range, but Iâ€™m sure there has to be a technical reason for it. 839 hours is 34.958(3) days, and the whole range spans exactly 6040798 seconds. The documentation also mentions the following:</p>
<blockquote><p>MySQL recognizes <code>TIME</code> values in several formats, some of which can include a trailing fractional seconds part in up to microseconds (6 digits) precision.</p></blockquote>
<p>Therefore, it also makes sense to point out that the whole interval spans <span id="display">6 040 798 </span>000 000 microseconds, but again, these seem like oddly specific numbers. They are not near any power of two, the latter being between 2<sup>42</sup> and 2<sup>43</sup>, so MySQL must be using some awkward internal representation format. But before we dive into that, let me just point out how bad this type is. It is the closest MySQL has to a time interval type, and yet it canâ€™t deal with intervals that are just a bit over a month long. How much is that â€œbitâ€? Not even a nice, rounded number of days, it seems.</p>
<p>To make matters worse, it appears that the most popular EF Core MySQL provider maps .NETâ€™s <code>TimeSpan</code> to <code>TIME</code> by default, despite the fact that&nbsp;<code>TimeSpan</code> can contain intervals in the dozens of millennia (it uses a 64 bit integer and has 10<sup>-8</sup> s precision) compared to TIMEâ€™s measly â€œa bit over two monthsâ€. This is an <a href="https://github.com/PomeloFoundation/Pomelo.EntityFrameworkCore.MySql/issues/1046">issue other people have run into</a>, and the discussion in that issue includes a â€œThis mimics the behavior of SQL Serverâ€ remark, which made me go check and, sure enough, SQL Serverâ€™s <code>time</code> is meant to encode a time of day and has a range of 00:00:00.0000000 through 23:59:59.9999999, something which overall makes more sense to me than MySQLâ€™s odd TIME range.</p>
<p>So letâ€™s go back to MySQL. What is the reasoning behind such an <em>interesting</em> range? The <a href="https://dev.mysql.com/doc/internals/en/date-and-time-data-type-representation.html">MySQL Internals Manual</a> says that the storage for the TIME type has changed with version 5.6.4, having gained support for fractional seconds in this version. It uses 3 bytes for the non-fractional type. Now, had they just used these 3 bytes to encode a number of seconds, they would have been able to support intervals spanning over 2330 hours, which would already be a considerable improvement over the current 838 hours maximum, even if still a bit useless when it comes to mapping a <code>TimeSpan</code> to it.</p>
<p>This means their encoding must be wasting bits, probably so it is easier to work withâ€¦ not sure in what circumstances exactly, but maybe it makes more sense if your database management system (and/or your conception of what the users will do with it) just loves strings, and you really want to speed up the hh:mm:ss representation. So, behold:</p>
<blockquote>
<pre>1 bit sign (1= non-negative, 0= negative)
1 bit unused (reserved for future extensions)
10 bits hour (0-838)
6 bits minute (0-59) 
6 bits second (0-59) 
---------------------
24 bits = 3 bytes</pre>
</blockquote>
<p>This explains everything, right? Well, look closely. 10 bits for the hourâ€¦ and a range of 0 to 838. I kindly remind you that 2<sup>10</sup> is 1024, not 838. The plot thickens. Iâ€™m not the first person to wonder about this, of course, <a href="https://stackoverflow.com/questions/39259910/why-is-mysqls-maximum-time-limit-8385959">this was asked on StackOverflow before</a>. The accepted answer in that question explains everything, but <em>it almost didnâ€™t</em>, as it initially dismisses the odd choice of 838 as â€œbackward compatibility with applications that were written a while agoâ€, and only later it is explained that this choice had to do with compatibility with MySQL versionâ€¦ 3, from the times when, you know, Windows 98 was a fresh operating system and Linux wasnâ€™t 10 years old yet.</p>
<p>In MySQL 3, the TIME type used 3 bytes as well, but they were used differently. One of the bits was used for the sign as well, but the remaining 23 bits were an integer value produced like this: Hours Ã— 10000 + Minutes Ã— 100 + Seconds; in other words, the two least significant decimal digits of the number contained the seconds, the next two contained the minutes, and the remaining ones contained the hours. 2<sup>23</sup> is 83888608, i.e. 838:86:08, therefore, the maximum valid time in this format is 838:59:59. This format is even less wieldy than the current one, requiring multiplication and division to do basically anything with it, except string formatting and parsing â€“ once again showing that MySQL places too much value on string IO and not so much on having types that are convenient for internal operations and non-string-based protocols.</p>
<p>MySQL developers had ample opportunities to fix this type, or at the very least introduce an alternative one that is free of this reduced range. They changed this type twice from MySQL 3 until now, but decided to retain the range every time, supposedly for compatibility reasons. I am struggling to imagine the circumstances where increasing the value range for a type can break compatibility with an application â€“ do types in MySQL have defined overflow behaviors? Is any sane person writing applications where they are relying on a database typeâ€™s intrinsic limits for validation? If yes, who looked at this awkward 838 hours range and thought of it as an appropriate limitation to carry unchanged into their applicationâ€™s data model? At this point, I donâ€™t even want to know.</p>
<p>Despite having changed twice throughout MySQLâ€™s lifetime, the TIME type is still quite an awkward and limited one. That unused, â€œreserved for future extensionsâ€ bit is, in my opinion, really the <em>piÃ¨ce de rÃ©sistance</em> here. Hereâ€™s hoping that one day it will be used to signify a â€œlegacyâ€ TIME value and that, by then, MySQL and/or MariaDB will have support for a proper type like <a href="https://www.postgresql.org/docs/current/datatype-datetime.html">PostgreSQLâ€™s INTERVAL</a>, which has a range of +/- 178000000 years and a very reasonable microsecond precision.</p>
	</article>		
	</div>
	</div></div>]]>
            </description>
            <link>https://gbl08ma.com/time-for-a-wtf-mysql-moment/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24698660</guid>
            <pubDate>Tue, 06 Oct 2020 15:23:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hiring for tech jobs has increased more than 100% in these Midwestern cities]]>
            </title>
            <description>
<![CDATA[
Score 118 | Comments 288 (<a href="https://news.ycombinator.com/item?id=24698449">thread link</a>) | @KaiserSanchez
<br/>
October 6, 2020 | https://www.purpose.jobs/blog/hiring-tech-jobs-has-increased-in-midwestern-cities | <a href="https://web.archive.org/web/*/https://www.purpose.jobs/blog/hiring-tech-jobs-has-increased-in-midwestern-cities">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p><img src="https://www.purpose.jobs/hubfs/social-suggested-images/www.michiganbusiness.org49d2d3globalassetsimagesnews1440-bannersdetroit-1440.jpg" alt="Hiring for Tech Jobs has Increased More than 100% in These Midwestern Cities">
</p></div><div>
<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p>When people think tech jobs, they tend to think Silicon Valley or New York City.</p>
<!--more-->
<p>They donâ€™t think about the Midwest, which is better known for rolling farmland and wide-open spaces than a booming tech scene where startups thrive.</p>
<p>But itâ€™s time to think again about the Midwest.&nbsp;</p>
<p><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-1-midwest.jpg?width=600&amp;name=img-1-midwest.jpg" alt="img-1-midwest" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-1-midwest.jpg?width=300&amp;name=img-1-midwest.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-1-midwest.jpg?width=600&amp;name=img-1-midwest.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-1-midwest.jpg?width=900&amp;name=img-1-midwest.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-1-midwest.jpg?width=1200&amp;name=img-1-midwest.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-1-midwest.jpg?width=1500&amp;name=img-1-midwest.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-1-midwest.jpg?width=1800&amp;name=img-1-midwest.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>This region comprises 19 percent of the <em>entire U.S. GDP</em>. Twenty-five percent of all computer science grads get their degrees in the Midwest. Forty-five percent of Fortune 500 countries are located here, as is 60 percent of all U.S. manufacturing.</p>
<p>And, as icing on the cake, seven of the top 10 <a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank">most affordable states</a> in the nation are in the Midwest.</p>
<p>What does that have to do with tech jobs? Well, increasingly, startup founders and investors are taking note of all those things the Midwest has to offer, as well as the excellent quality of life and affordable cost of living you can find in so many cities in the Heartland. Theyâ€™re realizing you donâ€™t have to be based in the Golden State or the Big Apple if you want your startup to succeed. You can be based in the Midwest and find just as much success.</p>
<div><p>So tech startups are booming in the Midwest. Donâ€™t believe us? The proof is in the numbers.</p></div>
<h2><span>In 3 of the Midwestâ€™s Top 10 Cities, Tech Hiring Is Up More than 100% In the Last 3 Years</span></h2>
<div><p>Weâ€™ll let the numbers tell the full story. These are the Midwestâ€™s top 10 cities in terms of growth and offerings for tech workers.&nbsp;</p></div>
<h3><span>Chicago: 8th in the U.S. for Net Tech Employment</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-2-chicago.jpg?width=600&amp;name=img-2-chicago.jpg" alt="img-2-chicago" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-2-chicago.jpg?width=300&amp;name=img-2-chicago.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-2-chicago.jpg?width=600&amp;name=img-2-chicago.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-2-chicago.jpg?width=900&amp;name=img-2-chicago.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-2-chicago.jpg?width=1200&amp;name=img-2-chicago.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-2-chicago.jpg?width=1500&amp;name=img-2-chicago.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-2-chicago.jpg?width=1800&amp;name=img-2-chicago.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p><a href="https://www.purpose.jobs/chicago" rel="noopener" target="_blank">Chicago</a> is the No. 1 city in the Midwest for growth in the tech sector, and it ranks eighth in the country for net tech employment. Currently, there are 344,146 people in Chicago working in tech jobs. The city saw nearly 18 percent growth in its net tech employment from 2010 to 2018, and from just 2017 to 2018, job posting in the tech sector increased by a whopping 73 percent.</p></div>
<h3><span>Detroit: 11th in the U.S. for Net Tech Employment</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-3-detroit.jpg?width=600&amp;name=img-3-detroit.jpg" alt="img-3-detroit" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-3-detroit.jpg?width=300&amp;name=img-3-detroit.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-3-detroit.jpg?width=600&amp;name=img-3-detroit.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-3-detroit.jpg?width=900&amp;name=img-3-detroit.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-3-detroit.jpg?width=1200&amp;name=img-3-detroit.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-3-detroit.jpg?width=1500&amp;name=img-3-detroit.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-3-detroit.jpg?width=1800&amp;name=img-3-detroit.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<p>Coming in just behind Chicago is <a href="https://www.purpose.jobs/detroit" rel="noopener" target="_blank">Detroit</a>, which ranks 11th in the U.S. for its net tech employment. 241,135 people work in the tech sector in Detroit, where net tech employment increased by 37.2 percent from 2010 to 2018. From 2017 to 2018, job postings in tech rose 41 percent, making Detroit a fantastic spot to look for a startup job.</p>
<p><em>Looking to get connected with top startups? Join the purpose.jobs talent community to start applying for Midwest startup jobs.</em> <!--HubSpot Call-to-Action Code --><span id="hs-cta-wrapper-6fe04a32-7b40-49d0-873c-8f8ae79aa1dd"><span id="hs-cta-6fe04a32-7b40-49d0-873c-8f8ae79aa1dd"><!--[if lte IE 8]><div id="hs-cta-ie-element"></div><![endif]--><a href="https://cta-redirect.hubspot.com/cta/redirect/2873777/6fe04a32-7b40-49d0-873c-8f8ae79aa1dd" target="_blank"><img id="hs-cta-img-6fe04a32-7b40-49d0-873c-8f8ae79aa1dd" src="https://no-cache.hubspot.com/cta/default/2873777/6fe04a32-7b40-49d0-873c-8f8ae79aa1dd.png" alt="Create a free profile."></a></span></span><!-- end HubSpot Call-to-Action Code --></p>

<h3><span>Minneapolis: Nearly 200,000 Tech Jobs and Growing</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-4-minn.jpg?width=600&amp;name=img-4-minn.jpg" alt="img-4-minn" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-4-minn.jpg?width=300&amp;name=img-4-minn.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-4-minn.jpg?width=600&amp;name=img-4-minn.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-4-minn.jpg?width=900&amp;name=img-4-minn.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-4-minn.jpg?width=1200&amp;name=img-4-minn.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-4-minn.jpg?width=1500&amp;name=img-4-minn.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-4-minn.jpg?width=1800&amp;name=img-4-minn.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p>Third in the Midwest for growth is Minneapolis, a thriving city many overlook, despite its tech workforce of 196,151 and growing. Minneapolis is ranked 14th in the U.S. overall for net tech employment, which increased 17 percent in the city from 2010 to 2018. Whatâ€™s even more impressive is that job postings in the tech sector increased 76 percent in Minneapolis from 2017 to 2018.</p></div>
<h3><span>Kansas City: Where New Tech Jobs Have Almost Doubled in Recent Years<br></span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-5-kc.jpg?width=600&amp;name=img-5-kc.jpg" alt="img-5-kc" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-5-kc.jpg?width=300&amp;name=img-5-kc.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-5-kc.jpg?width=600&amp;name=img-5-kc.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-5-kc.jpg?width=900&amp;name=img-5-kc.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-5-kc.jpg?width=1200&amp;name=img-5-kc.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-5-kc.jpg?width=1500&amp;name=img-5-kc.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-5-kc.jpg?width=1800&amp;name=img-5-kc.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p>Kansas City has more to boast about than its Super Bowl win. The city is home to 100,782 people who work in the tech sector, making it 24th in the U.S. for net tech employment. Kansas City also saw 17.3 percent growth in its net tech employment from 2010 to 2018, and 82 percent growth in its tech job posting just from 2017 to 2018, indicating that its rate of growth is ramping up even faster in recent years than over the last decade.</p></div>
<h3><span>Cincinnati: Nearly 100,000 Tech Workers and Steady Growth of New Jobs<br></span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-6-cinci.jpg?width=600&amp;name=img-6-cinci.jpg" alt="img-6-cinci" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-6-cinci.jpg?width=300&amp;name=img-6-cinci.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-6-cinci.jpg?width=600&amp;name=img-6-cinci.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-6-cinci.jpg?width=900&amp;name=img-6-cinci.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-6-cinci.jpg?width=1200&amp;name=img-6-cinci.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-6-cinci.jpg?width=1500&amp;name=img-6-cinci.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-6-cinci.jpg?width=1800&amp;name=img-6-cinci.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p>Fifth on the list of growing Midwest cities in <a href="https://www.purpose.jobs/cincinnati" rel="noopener" target="_blank">Cincinnati</a>, where 82,088 workers already have tech jobs. From 2010 to 2018, the city saw a 23.9 percent increase in its net tech employment, and job postings in the tech sector jumped up 41 percent just from 2017 to 2018. That lands Cincinnati 28th in the U.S. for net tech employment, and thereâ€™s plenty of opportunity here as the city continues to grow.</p></div>
<h3><span>Cleveland: Job Growth that Nearly Doubled in Just One Year</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-7-cleveland.jpg?width=600&amp;name=img-7-cleveland.jpg" alt="img-7-cleveland" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-7-cleveland.jpg?width=300&amp;name=img-7-cleveland.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-7-cleveland.jpg?width=600&amp;name=img-7-cleveland.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-7-cleveland.jpg?width=900&amp;name=img-7-cleveland.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-7-cleveland.jpg?width=1200&amp;name=img-7-cleveland.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-7-cleveland.jpg?width=1500&amp;name=img-7-cleveland.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-7-cleveland.jpg?width=1800&amp;name=img-7-cleveland.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p>Sixth in the Midwest is <a href="https://www.purpose.jobs/cleveland" rel="noopener" target="_blank">Cleveland</a>, an oft-overlooked Ohio metropolis that has plenty to offer tech workers â€”&nbsp;just ask the 76,698 workers who have tech jobs there. Cleveland saw 16.3 percent growth in its net tech employment from 2010 to 2018, which led to its 93 percent increase in tech job postings from 2017 to 2018. Of all the cities in the U.S., Cleveland ranks 29th for net tech employment, making it a place well worth considering whether youâ€™re looking for a tech job or hoping to found a startup in a new home.</p></div>
<h3><span>Indianapolis: More than 100 Percent Job Growth in One Year</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-8-indi.jpg?width=600&amp;name=img-8-indi.jpg" alt="img-8-indi" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-8-indi.jpg?width=300&amp;name=img-8-indi.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-8-indi.jpg?width=600&amp;name=img-8-indi.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-8-indi.jpg?width=900&amp;name=img-8-indi.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-8-indi.jpg?width=1200&amp;name=img-8-indi.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-8-indi.jpg?width=1500&amp;name=img-8-indi.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-8-indi.jpg?width=1800&amp;name=img-8-indi.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p><a href="https://www.purpose.jobs/indianapolis" rel="noopener" target="_blank">Indianapolis</a> is the first of our three Midwestern cities that increased their startup job growth more than 100 percent â€”&nbsp;the city saw a 121 percent increase in new tech job postings from 2017 to 2018, after 24.2 percent growth in net tech employment from 2010 to 2018. As of now, there are 74,615 people employed in the tech sector in Indy, and that number is only going up.</p></div>
<h3><span>Milwaukee: Tied for Highest Increase in New Tech Jobs in the Midwest</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-9-milwak.jpg?width=600&amp;name=img-9-milwak.jpg" alt="img-9-milwak" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-9-milwak.jpg?width=300&amp;name=img-9-milwak.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-9-milwak.jpg?width=600&amp;name=img-9-milwak.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-9-milwak.jpg?width=900&amp;name=img-9-milwak.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-9-milwak.jpg?width=1200&amp;name=img-9-milwak.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-9-milwak.jpg?width=1500&amp;name=img-9-milwak.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-9-milwak.jpg?width=1800&amp;name=img-9-milwak.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p>With an astonishing 137 percent increase in new tech job postings from 2017 to 2018, Milwaukee is one of the most promising spots in the Midwest for anyone looking for a tech position. The city currently boasts 71,755 tech workers after a 9.2 percent increase in net tech employment from 2010 to 2018. Sure, thatâ€™s slower growth over the course of the decade than some of the cities on our list, but the rate of new job postings in Milwaukee show this city is just getting started.</p></div>
<h3><span>Omaha: An Unlikely Hotspot for New Tech Job Postings</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-10-omaha.jpg?width=600&amp;name=img-10-omaha.jpg" alt="img-10-omaha" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-10-omaha.jpg?width=300&amp;name=img-10-omaha.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-10-omaha.jpg?width=600&amp;name=img-10-omaha.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-10-omaha.jpg?width=900&amp;name=img-10-omaha.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-10-omaha.jpg?width=1200&amp;name=img-10-omaha.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-10-omaha.jpg?width=1500&amp;name=img-10-omaha.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-10-omaha.jpg?width=1800&amp;name=img-10-omaha.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p>Like Milwaukee, Omaha also had a stunning 137 percent increase in new tech job postings from 2017 to 2018. While growth in net tech jobs in the city was only 10.7 percent from 2010 to 2018, all that seems to indicate is that tech workers are <em>just</em> starting to realize what Omaha has to offer. 37,508 tech workers live in the city now, but with such a marked increase in new tech jobs, we can only see that number going up.</p></div>
<h3><span>Des Moines: Second-Highest 10-Year Growth in the Midwest</span><a href="https://www.purpose.jobs/midwest-salary-report?utm_source=blog&amp;utm_medium=web&amp;utm_campaign=community" rel="noopener" target="_blank"><img src="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-11-des-moines.jpg?width=600&amp;name=img-11-des-moines.jpg" alt="img-11-des-moines" width="600" srcset="https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-11-des-moines.jpg?width=300&amp;name=img-11-des-moines.jpg 300w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-11-des-moines.jpg?width=600&amp;name=img-11-des-moines.jpg 600w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-11-des-moines.jpg?width=900&amp;name=img-11-des-moines.jpg 900w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-11-des-moines.jpg?width=1200&amp;name=img-11-des-moines.jpg 1200w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-11-des-moines.jpg?width=1500&amp;name=img-11-des-moines.jpg 1500w, https://www.purpose.jobs/hs-fs/hubfs/Hiring%20Tech/img-11-des-moines.jpg?width=1800&amp;name=img-11-des-moines.jpg 1800w" sizes="(max-width: 600px) 100vw, 600px"></a></h3>
<div><p>While Des Moines is 10th in the top 10 Midwestern cities, itâ€™s had the second-highest rate of growth in net tech employment from 2010 to 2018: 26.9 percent, behind only Detroit. Des Moines is currently home to 28,693 tech workers, and from 2017 to 2018, saw a 47 percent increase in new tech job postings.&nbsp;</p></div>
<h2><span>Midwestern Companies Are Hiring Tens of Thousands of Tech Workers Right Now</span></h2>
<div><p>In Chicago, Detroit, and Indianapolis alone, there are nearly 31,000 open tech positions at any given time. The Midwest is next for tech workers. Find out more with a free download of the Midwest Salary and Cost of Living Handbook.</p></div>
<p><!--HubSpot Call-to-Action Code --><span id="hs-cta-wrapper-d825b188-3a7f-4d00-ac4a-1ef02e65ec84"><span id="hs-cta-d825b188-3a7f-4d00-ac4a-1ef02e65ec84"><!--[if lte IE 8]><div id="hs-cta-ie-element"></div><![endif]--><a href="https://cta-redirect.hubspot.com/cta/redirect/2873777/d825b188-3a7f-4d00-ac4a-1ef02e65ec84" target="_blank"><img id="hs-cta-img-d825b188-3a7f-4d00-ac4a-1ef02e65ec84" height="709" width="1600" src="https://no-cache.hubspot.com/cta/default/2873777/d825b188-3a7f-4d00-ac4a-1ef02e65ec84.png" alt="New call-to-action"></a></span></span><!-- end HubSpot Call-to-Action Code --></p>

<p><strong><br></strong><strong><img src="https://www.purpose.jobs/hs-fs/hubfs/Christina%20headshot.png?width=120&amp;name=Christina%20headshot.png" alt="Christina headshot" width="120" srcset="https://www.purpose.jobs/hs-fs/hubfs/Christina%20headshot.png?width=60&amp;name=Christina%20headshot.png 60w, https://www.purpose.jobs/hs-fs/hubfs/Christina%20headshot.png?width=120&amp;name=Christina%20headshot.png 120w, https://www.purpose.jobs/hs-fs/hubfs/Christina%20headshot.png?width=180&amp;name=Christina%20headshot.png 180w, https://www.purpose.jobs/hs-fs/hubfs/Christina%20headshot.png?width=240&amp;name=Christina%20headshot.png 240w, https://www.purpose.jobs/hs-fs/hubfs/Christina%20headshot.png?width=300&amp;name=Christina%20headshot.png 300w, https://www.purpose.jobs/hs-fs/hubfs/Christina%20headshot.png?width=360&amp;name=Christina%20headshot.png 360w" sizes="(max-width: 120px) 100vw, 120px"></strong><em><strong>Christina Marfice</strong> is a born and raised Midwesterner who traveled the globe and came right back. She has been a journalist and freelance writer for almost ten years. In addition to her other projects, she explores startup strategies, business operations, and eCommerce topics for&nbsp;<a target="_blank" data-stringify-link="https://www.yesoptimist.com/" delay="150" data-sk="tooltip_parent" href="https://www.yesoptimist.com/" rel="noopener">Optimist</a>. She currently resides in Chicago with her two cats, Dumpling and Doughnut.</em></p></span>
</p>

</div></div>]]>
            </description>
            <link>https://www.purpose.jobs/blog/hiring-tech-jobs-has-increased-in-midwestern-cities</link>
            <guid isPermaLink="false">hacker-news-small-sites-24698449</guid>
            <pubDate>Tue, 06 Oct 2020 15:09:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Deploy to K8s without YAML using ShuttleOps]]>
            </title>
            <description>
<![CDATA[
Score 61 | Comments 52 (<a href="https://news.ycombinator.com/item?id=24698326">thread link</a>) | @gscho
<br/>
October 6, 2020 | https://go.shuttleops.io/no-code-docker-kubernetes | <a href="https://web.archive.org/web/*/https://go.shuttleops.io/no-code-docker-kubernetes">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
<div data-widget-type="custom_widget" data-x="0" data-w="12">
<p><span id="hs_cos_wrapper_module_159802898093264_" data-hs-cos-general-type="widget" data-hs-cos-type="rich_text"><h2><span>See How No-Code </span><span id="5f0fb456-fdf5-4e5b-993a-ec7087c62a86" data-renderer-mark="true" data-mark-type="annotation" data-mark-annotation-type="inlineComment" data-id="5f0fb456-fdf5-4e5b-993a-ec7087c62a86">Continuous Delivery<br></span><span>&nbsp;Can Accelerate Your Business</span></h2>
<h5>The same powerful drag-and-drop interface, true multicloud integration and security and compliance youâ€™ve come to expect from ShuttleOps, now with Docker and Kubernetes support. See how easy it is to onboard your application, your team, and scale your delivery. Get started today for free. No credit card required!&nbsp;</h5>

<p><span><!--HubSpot Call-to-Action Code --><span id="hs-cta-wrapper-fcb53eb1-7328-44f8-b128-f953ffc8bab9"><span id="hs-cta-fcb53eb1-7328-44f8-b128-f953ffc8bab9"><!--[if lte IE 8]><div id="hs-cta-ie-element"></div><![endif]--><a href="https://cta-redirect.hubspot.com/cta/redirect/5669359/fcb53eb1-7328-44f8-b128-f953ffc8bab9"><img id="hs-cta-img-fcb53eb1-7328-44f8-b128-f953ffc8bab9" src="https://no-cache.hubspot.com/cta/default/5669359/fcb53eb1-7328-44f8-b128-f953ffc8bab9.png" alt="Get Started"></a></span></span><!-- end HubSpot Call-to-Action Code --></span></p></span></p>

</div><!--end widget-span -->
</div><!--end row-->
</div></div>]]>
            </description>
            <link>https://go.shuttleops.io/no-code-docker-kubernetes</link>
            <guid isPermaLink="false">hacker-news-small-sites-24698326</guid>
            <pubDate>Tue, 06 Oct 2020 14:58:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[4x4 Macro Pad Kit]]>
            </title>
            <description>
<![CDATA[
Score 77 | Comments 39 (<a href="https://news.ycombinator.com/item?id=24697624">thread link</a>) | @0xC45
<br/>
October 6, 2020 | https://0xc45.com/blog/4x4-macro-pad/ | <a href="https://web.archive.org/web/*/https://0xc45.com/blog/4x4-macro-pad/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    <header>
        
    </header>
    <p>10/6/2020</p>
    <h2>Contents</h2>
    <ul>
        
        <li>
            <a href="https://0xc45.com/blog/4x4-macro-pad/#overview">Overview</a>
            
        </li>
        
        <li>
            <a href="https://0xc45.com/blog/4x4-macro-pad/#build-process">Build Process</a>
            
        </li>
        
        <li>
            <a href="https://0xc45.com/blog/4x4-macro-pad/#flash-firmware">Flash Firmware</a>
            
        </li>
        
        <li>
            <a href="https://0xc45.com/blog/4x4-macro-pad/#design-keycaps">Design Keycaps</a>
            
        </li>
        
        <li>
            <a href="https://0xc45.com/blog/4x4-macro-pad/#conclusion">Conclusion</a>
            
        </li>
        
        <li>
            <a href="https://0xc45.com/blog/4x4-macro-pad/#links">Links</a>
            
        </li>
        
    </ul>
    <section>
<h2 id="overview">Overview</h2>
<p>Last weekend, I built a 4x4 keyboard kit. By this point, many people are familiar with the growing (and outspoken) mechanical keyboard hobbyist community. However, this kit is a bit unique. It's not a full keyboard. Instead, it's a 4x4 "macro pad" intended for sending keyboard shortcut sequences such as muting my microphone, muting my audio, volume up, volume down, etc. Additionally, with some extra software such as AutoHotKey, vastly complex programs could be triggered with the press of a button.</p>
<h2 id="build-process">Build Process</h2>
<p>Overall, building the macro pad was a simple and straightforward process. The <a href="https://www.1upkeyboards.com/instructions-downloads/sweet-16-instructions/">kit's build guide</a> provides a nice set of instructions with pictures to explain things. However, unlike many (some?) keyboard kits, the Sweet 16 kit requires soldering a few smaller components, such as the diodes and microcontroller headers. Additionally, the kit requires soldering one "surface-mount" component, the reset switch.</p>
<p>The parts:
<img src="https://0xc45.com/blog/4x4-macro-pad/sweet16-parts.jpg" alt="Sweet16 Parts"></p>
<p>Completed build:
<img src="https://0xc45.com/blog/4x4-macro-pad/sweet16-solder-joints.jpg" alt="Sweet16 Solder Joints"></p>
<h2 id="flash-firmware">Flash Firmware</h2>
<p>To program my custom keymap (including multiple keypress macros), I used <a href="https://qmk.fm/">QMK firmware</a>, the most popular keyboard firmware project.</p>
<p>Using QMK, it's possible to create custom keycodes that, when pressed, trigger a sequence of inputs. So, by pressing one button on the macro pad (or keyboard), the firmware will submit an entire sequence of keycode presses.</p>
<p>To do this, I defined my custom keycodes in an enum:</p>
<pre><code><span>enum </span><span>macro_keycodes {
  MICMUTE = SAFE_RANGE,
  MACRO1,
  MACRO2,
  MACRO3,
  MACRO4,
  MACRO5,
  MACRO6,
  MACRO7,
  MACRO8
};
</span></code></pre>
<p>Next, I defined a "keymap" array. Each position in the array corresponds to a single button on the 4x4 macro pad:</p>
<pre><code><span>const </span><span>uint16_t PROGMEM keymaps[][MATRIX_ROWS][MATRIX_COLS] = {
  [</span><span>0</span><span>] = </span><span>LAYOUT_ortho_4x4</span><span>( </span><span>/* Base */</span><span>
    MICMUTE, KC_MUTE, KC_VOLD, KC_VOLU,
    XXXXXXX, XXXXXXX, XXXXXXX, XXXXXXX,
    MACRO1,  MACRO2,  MACRO3,  MACRO4,
    MACRO5,  MACRO6,  MACRO7,  MACRO8
  ),
};
</span></code></pre>
<p>Lastly, I implemented the <code>process_record_user</code> function to define what should happen when each custom keycode is pressed:</p>
<pre><code><span>bool </span><span>process_record_user</span><span>(uint16_t </span><span>keycode</span><span>, keyrecord_t *</span><span>record</span><span>) {
  </span><span>switch </span><span>(keycode) {
  </span><span>case</span><span> MICMUTE:
    </span><span>if </span><span>(record-&gt;event.</span><span>pressed</span><span>) {
      </span><span>SEND_STRING</span><span>(</span><span>SS_LCTL</span><span>(</span><span>SS_LALT</span><span>(</span><span>SS_LSFT</span><span>(</span><span>SS_TAP</span><span>(X_F10)))));
    }
    </span><span>break</span><span>;
  </span><span>case</span><span> MACRO1:
    </span><span>if </span><span>(record-&gt;event.</span><span>pressed</span><span>) {
      </span><span>SEND_STRING</span><span>(</span><span>SS_LCTL</span><span>(</span><span>SS_LALT</span><span>(</span><span>SS_LSFT</span><span>(</span><span>SS_TAP</span><span>(X_F1)))));
    }
    </span><span>break</span><span>;
  </span><span>/*
   * ... etc
   */
  </span><span>}
  </span><span>return </span><span>true
</span><span>}
</span></code></pre>
<p>As you can see, I have configured the <code>MICMUTE</code> button to send the entire sequence <code>CTRL+ALT+SHIFT+F10</code>. However, in practice, any arbitrary sequence could be sent for any button. And, that's only beginning to scratch the surface of the capabilities of the QMK firmware.</p>
<h2 id="design-keycaps">Design Keycaps</h2>
<p>For this "DIY" kit, it felt important to design my own icons. I'm no graphic designer, but it was kinda fun. To do this, I used "re-legendable" keycaps that snap together with a clear top. Then, I printed the icons on plain white paper, cut them out, and sandwiched each icon in the keycaps. Here's a photo of my efforts:</p>
<p><img src="https://0xc45.com/blog/4x4-macro-pad/sweet16-completed.jpg" alt="Sweet16 Completed"></p>
<h2 id="conclusion">Conclusion</h2>
<p>This was a pretty quick project, but I felt like it deserved a writeup nevertheless. As a relative beginner at soldering, this kit was a fantastic way to increase my skills and ability beyond the "absolute beginner" level required for most keyboard kits. Furthermore, the final product is quite useful and extensible. Beyond the specific purpose as a simple macro pad keyboard, this hardware is essentially a microcontroller connected to a set of buttons. There are numerous possible applications. It's ripe for hacking. This device could become a MIDI controller, home automation remote, or anything else my imagination might dream up. Until next time.</p>
<h2 id="links">Links</h2>
<ol>
<li>Sweeet 16 Macro Pad Kit: <a href="https://www.1upkeyboards.com/shop/keyboard-kits/macro-pads/sweet-16-macro-pad-black/">https://www.1upkeyboards.com/shop/keyboard-kits/macro-pads/sweet-16-macro-pad-black/</a></li>
<li>QMK Firmware: <a href="https://qmk.fm/">https://qmk.fm/</a></li>
<li>My Sweet 16 Keymap: <a href="https://github.com/0xC45/qmk-firmware/blob/master/keyboards/1upkeyboards/sweet16/keymaps/0xC45/keymap.c">https://github.com/0xC45/qmk-firmware/blob/master/keyboards/1upkeyboards/sweet16/keymaps/0xC45/keymap.c</a></li>
</ol>

    </section>
</article></div>]]>
            </description>
            <link>https://0xc45.com/blog/4x4-macro-pad/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24697624</guid>
            <pubDate>Tue, 06 Oct 2020 13:49:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On a Typical Day: Daniel Ek]]>
            </title>
            <description>
<![CDATA[
Score 44 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24697009">thread link</a>) | @tosh
<br/>
October 6, 2020 | https://www.theobservereffect.org/daniel.html | <a href="https://web.archive.org/web/*/https://www.theobservereffect.org/daniel.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><em>
									Welcome to the second interview on 'The Observer Effect'. We are lucky to have one
									of the most influential founders/CEOs in technology and media - Daniel Ek, Founder
									and CEO of Spotify. This interview was published on 4th October, 2020.

							</em></p><p><em>Daniel does things very differently from other business leaders and was generous to go
								deep with us on his leadership style, time management, decision making, Spotify's impact
								on the world and much, much more. Enjoy!
								</em>
							</p><p><b><a href="https://sriramk.com/">Sriram Krishnan</a></b><br>
								<em><strong>Letâ€™s start with the basics. Walk me through a typical day in the life of
										Daniel Ek.</strong></em>
							</p><p><strong>Daniel Ek</strong><br>
								So, this will sound incredibly lazy compared to some leaders. I wake up at around 6:30
								in the morning and spend some time with my kids and wife. At 7:30, I go work out. At
								8:30, I go for a walk â€“ even in the winter. Iâ€™ve found this is often where I do my best
								thinking. At 9:30, I read for thirty minutes to an hour. Sometimes I read the news, but
								youâ€™ll also find an ever-rotating stack of books in my office, next to my bed, on tables
								around the house. Books on history, leadership, biographies. Itâ€™s a pretty eclectic mix
								â€“ much like my taste in music. Finally, my â€œworkâ€ day really starts at 10:30.
							</p><p>
								Many people make big decisions early on in the day, I make them later in the day--at
								least later in the day here in Europe. Ironically, it's not actually because I'm more
								productive then, rather because we have so many of our staff in the US, and as a result,
								I've kind of primed myself to work that way.
							</p><p>
								So the earlier part of my day is focused on coaching, one-on-ones, and planning. Then, I
								typically tackle one topic a day which takes a lot of my time. That's my big thing for
								the day. Before we go into a live team discussion on that particular topic, I invest
								time to prepare beforehand â€“ reading and talking to members of the team who are either
								part of the decision-making process or who have insights and context. I sometimes even
								get external perspectives.
							</p><p>
								I also think about what my role is at that meeting. Sometimes I'm the approver. Other
								times, I'm supposed to come with a thoughtful perspective on whether an initiative makes
								sense or not.
							</p><p>
								Iâ€™ve found that creating this clarity of role for myself is critical. Itâ€™s something I
								challenge my direct reports to think about as they engage with their own teams. I remind
								them that all meetings are not the same. Even when we are meeting to discuss really,
								really complicated topics I always ask myself: â€œWhat am I going to do in this meeting?
								What does my involvement really need to be?â€
							</p><p>
								The truth is: it's entirely contextual. I find it crucial to be upfront about everyoneâ€™s
								role in different meetings, I think this is super, super important. Often that's my
								number one thing: to make sure I know what role I'm playing.</p><p>
								<b><i>Wow, okay, there are multiple things in there ranging from how you choose to spend
										your time to how you handle meetings. To work backwards, what makes a good
										meeting in your mind?
									</i></b></p><p>A great meeting has three key elements: the desired outcome of the meeting is clear ahead
								of time; the various options are clear, ideally ahead of time; and the roles of the
								participants are clear at the time.
							</p><p>
								I often find that meetings lack one of those elements. Sometimes they lack all those,
								which is when you have to say, â€œThis is a horrible meeting, let's end it and regroup so
								it can be more effective for everyone.â€
							</p><p>
								To clarify outcomes, options, and roles ahead of time, we sometimes rely upon a preread.
								Prereads are a great way to share context so that attendees can quickly get into the
								meat of the issue and not waste time getting everyone up to speed. What I find is when
								you use a tool like a Google Doc, you can take in a great deal of information by reading
								comments, assessing options, and understanding how opinions have evolved over time. With
								this uniform background and context, attendees can focus on discussing the matter at
								hand versus getting on the same page. When the latter happens, the meeting becomes an
								incredible waste of time.
							</p><p>
								I think that's the single largest source of optimization for a company: the makeup of
								their meetings. To be clear, it's not about fewer meetings because meetings serve a
								purpose. Rather, itâ€™s key to improve the meetings, themselves. A lot of my efforts focus
								on teaching people this framework. Ironically, I find that most people are just
								challenged by that stuff.
							</p><p>
								Candidly, thatâ€™s my role as leader: to coach others on how best to make use of their
								limited time. Not only is time the most precious resource the company has, itâ€™s also the
								most precious resource they have! Itâ€™s crucial that they approach the use of their time
								with a holistic perspective. By way of example, I had a recent call with one of my
								directors who had not taken a vacation in six months. Our conversation delved into why
								this person thought that they could not be away for two weeks, and me arguing for why
								the person had to take two weeks to recharge!
							</p><p>
								There is never enough time â€“ for work, for family and friends â€“ and it takes work to
								make the best use of it. It's all about fostering a holistic perspective in life.

							</p><p>
								<b><i>
										Thatâ€™s fascinating. Letâ€™s turn to your team.

										Your direct reports are highly accomplished people; what are the common mistakes
										you see executives at that level make when it comes to personal time management?
									</i>
							</b></p><div>
							<div><p>
								I donâ€™t think most executives dedicate enough time to thinking. They spend too much time
								in meetings. By the way, I will say as a caveat, I do know people who are incredibly
								organized and succeed with a lot of â€œdo time.â€ Shishir Mehrotra [Co-founder and CEO of
								Coda] is a great example. If you've seen the docs on how he organizes his time...
								</p><p>

								<b><i>Oh yeah, he has a lot of very well-organized docs! [laughs]</i></b></p></div><p>
								He is a source of inspiration. For a while, I tried to mimic his style because I was so
								impressed with his thinking behind it. But in the end, it just wasnâ€™t for me. It
								actually drove me nuts. <i>[Sriram laughs]</i>
								But I respect him. I would say he's a highly effective executive. His system works for
								him. It's not one size fits all. Some of my direct reports thrive on lots of meetings.
								But, in general, I would say the largest mistake is that they conflate meetings with
								productivity. Often fewer meetings and better decisions drive the business forward.

							</p>
							<h2 id="opencalendar"><b>On Creating an Open Calendar</b></h2>
							<p>


								<b><i>This dovetails nicely with something that fascinates many of your colleagues: how
										do you have so much open time on your calendar?

										This drastically differs from your typical â€œsuccessful CEOâ€ who is booked from
										8:30am to 6pm. Walk us through your calendar and how you manage to create this
										open space.
									</i></b>

							</p>

							<p>

								My friends know me well! I do keep a lot of open time. I understand this comes from a
								place of privilege and Iâ€™m very lucky to have this flexibility.
							</p>
							<p>

								I feel like synchronous time is very costly; asynchronous time is better. I know there
								are some leaders who prefer to have all executive decisions travel through them. But
								then, you have to wait until the leader has availability to review things. Sometimes you
								run into delays in that process.
							</p>
							<p>
								I typically don't have more than really three or four meetings per day. There are
								exceptions; when I travel, I book in a lot more and I don't keep to my normal schedule.
								That said, most of the time it's three or four meetings a day.

							</p>
							<p>
								My way is to plan long term and do so ahead of time so that people better understand the
								direction in which they're going. You have to be incredibly crisp and clear when doing
								that. For instance, right now we're finalizing our five year plans and long range
								planning. These are actual, real targets fueled by real insights. They are made up of
								lots of super-detailed quarterly and annual goals. I donâ€™t spend much time on the
								quarterly goals and instead focus on our so-called â€œbig rocks.â€
							</p>


							<h2 id="bets"><b>On Company Bets</b></h2>
							<p>
								At Spotify, we have something called â€œCompany Bets.â€ These are large-scale initiatives
								that we believe will have a significant impact on the business within a relatively short
								period of time. I find that these bets are a much better use of my time. Our Company
								Bets typically update every six months, so I'm not needed that much in between. This
								way, I can constantly be thinking: â€œWhere are we headed in the next six months?â€ Right
								now, I am thinking more about H2 2021. From a timeline perspective, that's the earliest
								place where I focus most of my time.
							</p>
							<p>
								Itâ€™s also my role to think far beyond that. For instance, Iâ€™m immersing myself in our
								2025 plans. I trust my team to manage the day-to-day, shorter-term initiatives and
								iterate as needed based on data and insights. Theyâ€™re the best at that and I appreciate
								that this then frees me up to think about the long term.
							</p>

							<h2 id="decisionmaking"><b>On Delegated Decision Making<br></b></h2>
							<p>
								<b><i>
										Your system reminds me of Jack [Dorsey] at Twitter a â€¦</i></b></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.theobservereffect.org/daniel.html">https://www.theobservereffect.org/daniel.html</a></em></p>]]>
            </description>
            <link>https://www.theobservereffect.org/daniel.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24697009</guid>
            <pubDate>Tue, 06 Oct 2020 12:32:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Cyclone Scheme]]>
            </title>
            <description>
<![CDATA[
Score 89 | Comments 29 (<a href="https://news.ycombinator.com/item?id=24696939">thread link</a>) | @andrenth
<br/>
October 6, 2020 | https://justinethier.github.io/cyclone/ | <a href="https://web.archive.org/web/*/https://justinethier.github.io/cyclone/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main_content_wrap">
      <section id="main_content">
      <p>Cyclone Scheme is a brand-new compiler that allows real-world application development using the R<sup>7</sup>RS Scheme Language standard. We provide modern features and a stable system capable of generating fast native binaries.</p>

<p><a href="https://github.com/justinethier/cyclone/raw/master/docs/research-papers/CheneyMTA.pdf">Cheney on the MTA</a> is used by Cycloneâ€™s runtime to implement full tail recursion, continuations, and generational garbage collection. In addition, the Cheney on the MTA concept has been extended to allow execution of multiple native threads. An on-the-fly garbage collector is used to manage the second-generation heap and perform major collections without â€œstopping the worldâ€.</p>



<ul>
  <li>Support for the majority of the Scheme language as specified by the latest <a href="https://justinethier.github.io/cyclone/docs/Scheme-Language-Compliance.html">R<sup>7</sup>RS standard</a>.</li>
  <li>New features from R<sup>7</sup>RS including libraries, exceptions, and record types.</li>
  <li>Built-in support for Unicode strings and characters.</li>
  <li>Hygienic macros based on <code>syntax-rules</code></li>
  <li>Low-level explicit renaming macros</li>
  <li>Guaranteed tail call optimizations</li>
  <li>Native multithreading support</li>
  <li>A foreign function interface that allows easy integration with C</li>
  <li>A concurrent, generational garbage collector based on Cheney on the MTA</li>
  <li>Includes an optimizing Scheme-to-C compiler,</li>
  <li>â€¦ as well as an interpreter for debugging</li>
  <li>A <a href="https://github.com/cyclone-scheme/cyclone-winds">Package Manager</a> and a growing list of packages.</li>
  <li>Support for <a href="https://justinethier.github.io/cyclone/docs/API.html#srfi-libraries">many popular SRFIâ€™s</a></li>
  <li>Online user manual and API documentation</li>
  <li>Support for Linux, Windows, FreeBSD, and Mac platforms.</li>
  <li>Known to run on x86-64, x86, and Arm (Raspberry Pi) architectures.</li>
</ul>



<p>There are several options available for installing Cyclone:</p>

<h2 id="docker">Docker</h2>
<p><img src="https://justinethier.github.io/cyclone/docs/images/docker-thumb.png" alt="Docker" title="Docker"></p>

<p>Cyclone can be run from a <a href="https://hub.docker.com/r/cyclonescm/cyclone">Docker Image</a>:</p>

<div><div><pre><code>docker run -it cyclonescm/cyclone bash
</code></pre></div></div>

<h2 id="homebrew">Homebrew</h2>
<p><img src="https://justinethier.github.io/cyclone/docs/images/homebrew-thumb.png" alt="Homebrew" title="Homebrew"></p>

<p>Mac (and Linux!) users wanting to use Homebrew can do the following.</p>

<p>Note if Homebrew is not already installed: follow the instructions at <a href="https://brew.sh/">https://brew.sh/</a> to install the homebrew package manager.</p>

<div><div><pre><code>brew tap cyclone-scheme/cyclone
brew install cyclone-scheme/cyclone/cyclone-bootstrap
</code></pre></div></div>

<h2 id="arch-linux">Arch Linux</h2>
<p><img src="https://justinethier.github.io/cyclone/docs/images/arch-linux-thumb.png" alt="Arch Linux" title="Arch Linux"></p>

<p>Arch Linux users can install using the <a href="https://aur.archlinux.org/packages/cyclone-scheme/">AUR</a>:</p>

<div><div><pre><code>git clone https://aur.archlinux.org/cyclone-scheme.git
cd cyclone-scheme
makepkg -si
</code></pre></div></div>

<h2 id="build-from-source">Build from Source</h2>
<p><img src="https://justinethier.github.io/cyclone/docs/images/build-thumb.png" alt="Build from Source" title="Build from Source"></p>

<p>To install Cyclone on your machine for the first time on Linux, Windows, FreeBSD, and for Mac users wanting to install without using Homebrew, use <a href="https://github.com/justinethier/cyclone-bootstrap"><strong>cyclone-bootstrap</strong></a> to build a set of binaries. Instructions are provided for Linux, Mac, Windows (via MSYS), and FreeBSD 12.</p>



<p>After installing you can run the <code>cyclone</code> command to compile a single Scheme file:</p>

<div><div><pre><code>$ cyclone examples/fac.scm
$ examples/fac
3628800
</code></pre></div></div>

<p>And the <code>icyc</code> command to start an interactive interpreter. Note you can use <a href="http://linux.die.net/man/1/rlwrap"><code>rlwrap</code></a> to make the interpreter more friendly, EG: <code>rlwrap icyc</code>:</p>

<div><div><pre><code>$ icyc

              :@
            @@@
          @@@@:
        `@@@@@+
       .@@@+@@@      
       @@     @@     Cyclone Scheme-&gt;C compiler
      ,@             http://justinethier.github.io/cyclone/
      '@
      .@
       @@     #@     (c) 2014-2019 Justin Ethier
       `@@@#@@@.     Version 0.11
        #@@@@@
        +@@@+
        @@#
      `@.
   
cyclone&gt; (write 'hello-world)
hello-world
</code></pre></div></div>

<p>Read the documentation below for more information on how to use Cyclone.</p>



<p><img src="https://justinethier.github.io/cyclone/docs/images/cyclone-winds-small.png" alt="Cyclone Winds" title="Cyclone Winds"></p>

<p>The <code>cyclone-winds</code> package manager provides the ability to install packaged libraries and programs for Cyclone. See the <a href="https://github.com/cyclone-scheme/cyclone-winds#cyclone-winds">cyclone-winds</a> site for more information.</p>



<ul>
  <li>
    <p>The <a href="https://justinethier.github.io/cyclone/docs/User-Manual">User Manual</a> covers in detail how to use Cyclone and provides information on the Scheme language features implemented by Cyclone.</p>
  </li>
  <li>
    <p>An <a href="https://justinethier.github.io/cyclone/docs/API">API Reference</a> is available for all libraries provided by Cyclone, including a complete alphabetical listing.</p>
  </li>
  <li>
    <p>If you need a resource to start learning the Scheme language you may want to try a classic textbook such as <a href="https://mitpress.mit.edu/sicp/full-text/book/book.html">Structure and Interpretation of Computer Programs</a>.</p>
  </li>
  <li>
    <p>Finally, this <a href="http://ecraven.github.io/r7rs-benchmarks/benchmark.html">benchmarks</a> page by <a href="https://github.com/ecraven">ecraven</a> compares the performance of Cyclone with other Schemes.</p>
  </li>
</ul>



<p>Cyclone provides several example programs, including:</p>

<ul>
  <li>
    <p><a href="https://github.com/justinethier/cyclone/blob/master/examples/tail-call-optimization.scm">Tail Call Optimization</a> - A simple example of Scheme tail call optimization; this program runs forever, calling into two mutually recursive functions.</p>
  </li>
  <li>
    <p><a href="https://github.com/justinethier/cyclone/blob/master/examples/threading">Threading</a> - Various examples of multi-threaded programs.</p>
  </li>
  <li>
    <p><a href="https://github.com/justinethier/cyclone/blob/master/examples/game-of-life">Game of Life</a> - The <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Conwayâ€™s game of life</a> example program and libraries from R<sup>7</sup>RS.</p>
  </li>
  <li>
    <p><a href="https://github.com/justinethier/cyclone/blob/master/examples/game-of-life-png">Game of Life PNG Image Generator</a> - A modified version of game of life that uses libpng to create an image of each iteration instead of writing it to console. This example also demonstrates basic usage of the C Foreign Function Interface (FFI).</p>
  </li>
  <li>
    <p>Finally, the largest program is the compiler itself. Most of the code is contained in a series of libraries which are used by <a href="https://github.com/justinethier/cyclone/blob/master/cyclone.scm"><code>cyclone.scm</code></a> and <a href="https://github.com/justinethier/cyclone/blob/master/icyc.scm"><code>icyc.scm</code></a> to create executables for Cycloneâ€™s compiler and interpreter.</p>
  </li>
</ul>



<ul>
  <li>
    <p><a href="https://justinethier.github.io/cyclone/docs/Writing-the-Cyclone-Scheme-Compiler-Revised-2017">Writing the Cyclone Scheme Compiler</a> provides high-level details on how the compiler was written and how it works.</p>
  </li>
  <li>
    <p>There is a <a href="https://justinethier.github.io/cyclone/docs/Development">Development Guide</a> with instructions for common tasks when hacking on the compiler itself.</p>
  </li>
  <li>
    <p>Cycloneâ€™s <a href="https://justinethier.github.io/cyclone/docs/Garbage-Collector">Garbage Collector</a> is documented at a high-level. This document includes details on extending Cheney on the MTA to support multiple stacks and fusing that approach with a tri-color marking collector.</p>
  </li>
  <li>
    <p>The garbage collector was subsequently enhanced to support <a href="https://justinethier.github.io/cyclone/docs/Garbage-Collection-Using-Lazy-Sweeping">Lazy Sweeping</a> which improves performance for a wide range of applications.</p>
  </li>
</ul>



<p>Copyright (C) 2014 <a href="http://github.com/justinethier">Justin Ethier</a>.</p>

<p>Cyclone is available under the <a href="http://www.opensource.org/licenses/mit-license.php">MIT license</a>.</p>

            <h2>Recent News</h2>
      
        <h4>
          <a href="https://justinethier.github.io/cyclone//2020/09/17/Released-Cyclone-Scheme-0.21.html">Released Cyclone Scheme 0.21</a>
        </h4>
        <span>September 17, 2020</span>
        <br>
        Various bug fixes and continuous integration support for FreeBSD.
      
        <h4>
          <a href="https://justinethier.github.io/cyclone//2020/08/14/Released-Cyclone-Scheme-0.20.html">Released Cyclone Scheme 0.20</a>
        </h4>
        <span>August 14, 2020</span>
        <br>
        We now have official support for calling Scheme from C.
      
        <h4>
          <a href="https://justinethier.github.io/cyclone//2020/08/03/Released-Cyclone-Scheme-0.19.html">Released Cyclone Scheme 0.19</a>
        </h4>
        <span>August  3, 2020</span>
        <br>
        This release improves error reporting and includes many bug fixes.
      


      </section>
    </div></div>]]>
            </description>
            <link>https://justinethier.github.io/cyclone/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24696939</guid>
            <pubDate>Tue, 06 Oct 2020 12:23:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why I am building permapeople.org]]>
            </title>
            <description>
<![CDATA[
Score 137 | Comments 141 (<a href="https://news.ycombinator.com/item?id=24696688">thread link</a>) | @roboben
<br/>
October 6, 2020 | https://permapeople.org/blog/2020/10/05/why-i-am-building-permapeople-org.html | <a href="https://web.archive.org/web/*/https://permapeople.org/blog/2020/10/05/why-i-am-building-permapeople-org.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><img src="https://permapeople.org/blog/assets/why-permapeople.jpg" alt="Two potatoes in an Ikea bag"></p>

<p>Four years ago my grandfather gave me two potatoes. I had no idea what to do, so I put them in bought soil in a big blue Ikea bag on the balcony and with a bit watering, they turned out great and I got hooked on growing food for my family and me. It is really magical if you think of how much you spare our planet with growing your own food: You need to get a job to make money so that you can spend that money on buying food which was produced and delivered close to you by large, complex and very inefficient industries. This system spends incredible amounts of resources (time, energy, labor) which you can save by simply growing your own food. And it doesnâ€™t stop with food only: People grow plants for medicinal uses, to help the nature and wildlife around them, or just for their own pleasure.</p>

<h2 id="the-problem">The Problem</h2>

<p>This season I really tried to scale up and created raised beds all over our small urban plot in Berlin, Germany. I wanted to do it sustainable and close to nature, so I read Toby Hemenwayâ€™s Gaiaâ€™s Garden, which is probably the most widely read book on permaculture. While it is a good base to start and there are a lot of resources around online, it is actually pretty hard to make all that info useful for my own garden. Most of the time I found myself random googling just to answer simple questions like</p>

<ul>
  <li>What plants in the herb layer are available in my zone?</li>
  <li>What are the best companion plants for Tomatoes?</li>
  <li>What is the best time to sow peas in the garden in my zone?</li>
</ul>

<p>I fell back to having a spreadsheet, a collection of browser bookmarks, and a few books to look up what plants I can grow and how they could fit together in my garden. This took me a lot of time, Iâ€™d rather spend in the garden.
After the garden was planned, the next challenge was where to find seeds, seedlings and plants to start the garden. Mostly I googled the plant name I wanted to buy and ordered in whatever shop came up but it would be so much easier to buy it directly from other fellow gardeners.
The season started and another thing I did was writing a diary of all my garden activities. The idea was to learn from past mistakes to grow better next year. It worked well for me but true learning comes from sharing experiences with others, which was not possible with that.
While this worked for this year, I wanted to have something better next year so I started building a platform around all these topics.</p>

<h2 id="a-platform-for-everyone">A Platform for Everyone</h2>

<p>Most of the resources about growing plants you find online are either anecdotal or very scientific. There is no place where a gardening enthusiast can share their experiences, see what other enthusiasts learned already, and collaborate on everything related to growing plants. I think to achieve that, we need:</p>

<p>A <strong>permaculture plant database</strong> which everyone can search easily by common permaculture plant attributes like Layer, preferred light and soil conditions, times when to plant and harvest and benefits for animals, human and the environment. In addition everyone can look up advanced topics like companion planting and guild design. To make this info useful, it needs to be verified by others through ratings, comments and linked sources. If someone could see that most people were successful with growing that specific variety of a plant in your area or that a certain guild really works for a lot of others, that would be a huge help for everyone.</p>

<p>A <strong>permaculture marketplace</strong> where people can share/trade/buy/sell seeds, plants and everything else they might need like equipment, books, courses. Others can use it to sell products from their permaculture gardens to make an income for themselves. Everything happens directly between fellow gardeners.</p>

<p>A <strong>permaculture garden log and planner</strong> where everyone can log their past garden activities, learn from each other and plan their next season or project. If this info is combined with all other gardeners, then it becomes citizen science and we can improve everyoneâ€™s gardening results. Imagine you could be notified when all the more advanced gardeners start their tomato seedlings in your area, so you could do that too.</p>

<h2 id="make-the-planet-a-better-place-for-real">Make the planet a better place (for real)</h2>

<p>There are many people who want to grow plants for many reasons but donâ€™t know how. There are also many people already growing a few plants in their garden and learned it the hard way. I believe we need a platform where they can come together and share their experiences and learn from one another to help improve the life of eveveryone. If we would all start growing a bit of our own food, we could help the planet and ourselves in so many impactful ways.</p>

<p>There is a lot to write about the implications of having such a platform, which I will do in future posts.</p>

<p>In the meantime, you can check out the plant database <a href="https://permapeople.org/database">here</a> and if you are interested, either <a href="https://permapeople.org/users/sign_up">sign up</a>, write me an email to hello at permapeople org or sign up for the newsletter where I am posting regular updates.</p>

<p>Thanks for reading ğŸŒ±âœŒï¸,</p>

<p>ben</p>

  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://permapeople.org/blog/2020/10/05/why-i-am-building-permapeople-org.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24696688</guid>
            <pubDate>Tue, 06 Oct 2020 11:42:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Chat bot powered by GPT-3]]>
            </title>
            <description>
<![CDATA[
Score 70 | Comments 79 (<a href="https://news.ycombinator.com/item?id=24695710">thread link</a>) | @piotrgrudzien
<br/>
October 6, 2020 | https://blog.quickchat.ai/post/knowledge-base-chat-bot/ | <a href="https://web.archive.org/web/*/https://blog.quickchat.ai/post/knowledge-base-chat-bot/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><figure><img src="https://blog.quickchat.ai/images/blog-post-1-bg.png" alt="Knowledge-base chat bot for SaaS product sales"></figure><section><div><p><em>Brief summary of our GPT-3 chat bot for SaaS product sales.</em></p><p>The most natural way for us to communicate is, well, <em>natural language</em>. Chat bots are nothing new but unless they meet a high-enough quality bar, they tend to be a step backwards rather than forward. We believe huge language models such as <a href="https://openai.com/blog/openai-api/">OpenAIâ€™s GPT-3</a> will form a foundation for a truly conversational human-computer interface. It is, however, a foundation rather than a solution in and of itself.</p><p>In this new paradigm, the big challenge becomes to ensure the chat bot strictly sticks to the topic it was designed for and provides accurate information - without depriving it of its creativity.</p><p>I will discuss this briefly in the context of what we refer to as <strong>knowledge-base chat bots</strong>. They are built to answer general questions and hold a conversation about a product, service or a topic delineated by a predetermined unstructured knowledge base.</p><p><img src="https://blog.quickchat.ai/images/zeroth_faster.gif" alt="Start a conversation - image" title="Start a conversation"></p><p>Our chat bot implementation approved by the OpenAI team (try it out live at <a href="https://itemsy.com/">itemsy.com</a>) is an expert on Itemsy - a software product for managing the content you read online. It relies on GPT-3 for its conversational capabilities.</p><p>Thanks to our <a href="https://quickchat.ai/">Quickchat</a> engine (on top of GPT-3), it makes full and accurate use of the Itemsy knowledge base it was provided with, focuses on the topic at hand and cannot be maneuvered away from it:</p><p><img src="https://blog.quickchat.ai/images/first_faster.gif" alt="Avoid off-topic conversations - image" title="Avoid off-topic conversations"></p><p>Ultimately, itâ€™s all about <em>conversation</em>. It requires context, needs to be unscripted, adaptive and creative. Youâ€™re still talking to a machine but this time language feels more like natural language. ğŸ™ƒ</p><p><img src="https://blog.quickchat.ai/images/second_faster.gif" alt="Creative conversation guided by the user - image" title="Creative conversation guided by the user"></p><p>Weâ€™re ready to work with you and launch conversational chat bots for a wide range of use cases. Reach out to us at <a href="https://quickchat.ai/">quickchat.ai</a>!</p><blockquote>â€” Dominik Posmyk (@dominikposmyk) <a href="https://twitter.com/dominikposmyk/status/1309497928810213376?ref_src=twsrc%5Etfw">September 25, 2020</a></blockquote></div></section></article><div><h2>Follow the Quickchat blog for product updates, user stories and technical posts about artificial intelligence.</h2><p>
<span>Please correct your email address</span></p></div></div>]]>
            </description>
            <link>https://blog.quickchat.ai/post/knowledge-base-chat-bot/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24695710</guid>
            <pubDate>Tue, 06 Oct 2020 08:22:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fooling Around with Foveated Rendering]]>
            </title>
            <description>
<![CDATA[
Score 172 | Comments 85 (<a href="https://news.ycombinator.com/item?id=24695275">thread link</a>) | @underanalyzer
<br/>
October 5, 2020 | https://www.peterstefek.me/focused-render.html | <a href="https://web.archive.org/web/*/https://www.peterstefek.me/focused-render.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
 <div>
  
  <p><label>Posted on <strong>28 September 2020</strong></label></p>
<p>Shadertoy is a wonderful tool which lets users create and share a type of program called a fragment shader online. The true magic of shadertoy is its community of very talented graphics programmers who build incredible works of art despite having access to only a sliver of the traditional graphics pipeline.  </p>
<p>Some of these shaders are very computationally intensive and even in a small window, they crawl along well below their intended 60 frames per second on my old laptop. Inspired by a technique in the VR community called Foveated Rendering, I decided to try to optimize these shaders by only rendering a fully detailed image within a small focal region. As you move away from the focal point the image quality decreases.   </p>
<p>This rendering scheme is motivated by biology. It turns out your eye notices more detail in the center of your vision than in the periphery. Some VR graphics programmers realized they could take advantage of this phenomenon to increase the effective resolution of images by increasing image quality towards the center of your vision. An in depth discussion of foveated rendering can be found in the â€œprevious work sectionâ€ of this <a href="https://ai.facebook.com/blog/deepfovea-using-deep-learning-for-foveated-reconstruction-in-ar-vr">paper</a>.  </p>
<p>I did not have the time, equipment or the background necessary to implement a full foveated rendering system but it was fun to fool around with the concept.  </p>
<p>Before diving into the technical details letâ€™s look at a simple shadertoy fragment shader.   </p>
<p><code>
void mainImage(out vec4 fragColor, in vec2 fragCoord)<br>
{
</code></p><p><code>
    // Normalized pixel coordinates (from 0 to 1)<br>
    vec2 uv = fragCoord/iResolution.xy;
<div><pre><span></span><span>//</span> <span>Output</span> <span>the</span> <span>pixel</span> <span>coordinates</span> <span>as</span> <span>a</span> <span>color</span> <span>to</span> <span>screen</span>
<span>//</span> <span>fragColor</span> <span>is</span> <span>a</span> <span>4</span> <span>vector</span> <span>of</span> <span>the</span> <span>form</span>
<span>//</span> <span>(</span><span>red</span><span>,</span> <span>green</span><span>,</span> <span>blue</span><span>,</span> <span>transparency</span><span>)</span>
<span>fragColor</span> <span>=</span> <span>vec4</span><span>(</span><span>uv</span><span>,</span> <span>0</span><span>.</span><span>0</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>);</span>
</pre></div>


</code></p><p><code>
}<br>
</code></p>
<p>This program runs once for each pixel on the screen. Each time it runs, we receive the input variable <code>fragCoord</code>. <code>fragCoord</code> is a 2d vector which contains the x and y coordinates of the pixel being drawn. We normalize those coordinates by dividing by <code>iResolution</code>, another 2d vector, which contains the width and height of the image. Finally we output a color to the screen, whose red and green channels are proportional to the x and y position of the pixel being drawn. The output of this shader looks like this:<br>
</p><p>
  <img src="https://www.peterstefek.me/images/focused-render/simple-shader-out.png" width="50%"> 
</p>
<p>Side note, why do these shader programs require their own language? Shaders are special because they run on the graphics card instead of the cpu. They are highly parallel. A helpful mental model might be imagining that each pixel is colored simultaneously. Therefore a lot of things that we take for granted in normal program languages such as liberally accessing memory and branching become much more difficult.  </p>
<p>In shadertoy shaders the bottleneck is always in the pixel rendering step. So to speed them up we want to only render a subset of the all the pixels on the screen. It seems like selectively rendering pixels should be as simple as adding a branch to the per pixel shader code that looks like:  </p>
<p><code>
void mainImage(out vec4 fragColor, in vec2 fragCoord) <br>
{<br>
</code></p><p><code>
    if (fragCoord is in the subset of pixels to render) {
      <p>
      ... do computationally intensive work 
      </p> 
    } else {
      <p>
      // return a black pixel<br>
      return vec4(0, 0, 0, 1); 
      </p> 
    } 
</code></p><p><code> 
}
</code></p>
<p>Unfortunately we cannot just use an if statement inside of the shader to save us from rendering all the pixels. Unlike normal programming languages, fragment shaders always execute both parts of each branch due to gpu limitations. So while our above code will still have to spend the sample amount of time evaluating compuationally intensive work.  </p>
<p>Luckily, it turns out that graphics drivers can selectively mark which pixels not to shade by writing their location to a special buffer called the stencil buffer. We can use this stencil buffer to only shade the subset of pixels we are interested in.</p>
<p>Once I could efficiently render a subset of the pixels, I needed to come up with a pre-generated sampling pattern. Most foveated rendering techniques seem to use a grid, but I decided to try a non uniform approach. Searching for some kind of optimal sampling pattern seemed like an interesting problem and if I was going to devote more time to this I'd explore options like <a href="https://blog.demofox.org/2018/01/30/what-the-heck-is-blue-noise/">blue noise</a>. However in the interest of time, I just decided fill in a small circle in the center and then use samples drawn from one low variance and one high variance gaussians centered at the middle of the screen to place the rest of the pixels. The final sampling pattern ended up looking like this:
</p><p>
  <img src="https://www.peterstefek.me/images/focused-render/final-sample-pattern.png" width="50%"> 
</p>
<p>Next, I needed a way to fill in all the missing pixels in the final image. The approach I took was pretty simple. I started by mapping each pixel in the final screen to its nearest neighbor. Since my sampling pattern was predetermined, I could create this map beforehand and pass it into the shader as a texture. Here's what this mapping looks like:<br>
</p><p>
  <img src="https://www.peterstefek.me/images/focused-render/nearest-mapping.png" width="50%"> 
</p>
<p>And hereâ€™s a gif of the mapping applied to a <a href="https://www.shadertoy.com/view/3lsSzf">shadertoy</a> created by the extremely talented <a href="https://www.iquilezles.org/">Inigo Quilez</a>:<br>
</p><p>
  <img src="https://www.peterstefek.me/images/focused-render/1-neighbor.gif"> 
</p>
<p>The above screen is 420x236 pixels and only 1/10th of those pixels are actually rendered. The focal point is directly in the center of the screen. Here's what the full resolution version looks like:</p>
<p>
  <img src="https://www.peterstefek.me/images/focused-render/original.gif"> 
</p>

<p>And here's what it looks like with only our sampling pixels:
</p><p>
  <img src="https://www.peterstefek.me/images/focused-render/sample-pixels.gif"> 
</p>
<p>One little improvement I tried was to make 4 different maps. The kth map mapped each pixel in the final image to its kth nearest sampled neighbor. I weighted each of neighbors by the inverse of their distance to the pixel in question. I actually even tried using some gradient descent based optimization to fine tune the weights but ended up seeing little improvement. It also seemed that increasing the number of maps beyond 4 did not improve things much either. Here's what the example from above looks like with weighted interpolation between the four closest neighbors of each pixel (we are still rendering only 1/10th of the total pixels):
</p><p>
  <img src="https://www.peterstefek.me/images/focused-render/4-neighbors.gif"> 
</p>
<p>Finally, here's the shader with 1/5th of the total pixels rendered (as opposed to 1/10th shown above):
</p><p>
  <img src="https://www.peterstefek.me/images/focused-render/1of5pixels.gif"> 
</p>
<p>Okay that's all cool but does this technique actually increase performance? I did not do a rigerous benchmark, but <a href="https://www.shadertoy.com/view/3l23Rh">this shader</a> goes from around 20-25 fps on my plugged in laptop to 60 fps when reduced to 1/5th of the total pixels. <a href="https://www.shadertoy.com/view/Ms2SD1">Another shader</a> went from around 15 fps to 60 fps.  </p>
<p>One last side note is that this method can be used with any 3d scene and is not exclusive to shader toys. I just chose to use them because they are always bottlenecked by the pixel rendering step and they are really pretty!</p>
<p>Further questions:</p>
<ul>
<li>How do we achive better temporal stability? (the <a href="https://ai.facebook.com/blog/deepfovea-using-deep-learning-for-foveated-reconstruction-in-ar-vr">paper</a> I mentioned earlier talks about this)</li>
<li>Can we dynamically change the sampling pattern to give us better results? For example what if we sampled along edges or areas where large amounts of motion is occuring? Of course to do this we would need to compute our nearest neighbor mappings on the fly (there are actually <a href="https://www.shadertoy.com/view/XtlGDS">some</a> <a href="https://www.shadertoy.com/view/ldl3W8">shadertoys</a> which already demonstrate capability).</li>
<li>How could this scheme improve if we had access to the internals of the 3d scene? For example, could we adjust our sampling pattern based on depth information?  </li>
<li>How does this actually look in VR?  </li>
</ul>
<p>Have questions / comments / corrections?<br>
Get in touch: <a href="mailto:pstefek.dev@gmail.com">pstefek.dev@gmail.com</a>   </p>
<p>Discussion on <a href="https://news.ycombinator.com/item?id=24695275">Hacker News</a></p>
 </div>
</div></div>]]>
            </description>
            <link>https://www.peterstefek.me/focused-render.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24695275</guid>
            <pubDate>Tue, 06 Oct 2020 06:44:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[File Corruption Is Attractive]]>
            </title>
            <description>
<![CDATA[
Score 25 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24694017">thread link</a>) | @sergioro
<br/>
October 5, 2020 | https://venam.nixers.net/blog/programming/2020/10/05/corruption-at-the-core.html | <a href="https://web.archive.org/web/*/https://venam.nixers.net/blog/programming/2020/10/05/corruption-at-the-core.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <article>
    <p><img src="https://venam.nixers.net/blog/assets/chaos1.jpg" alt="Chaos, an important theme in hermetism" loading="lazy"></p>

<p>We live in a world that is gradually and incessantly attracted by
over-rationality and order. In this article weâ€™ll burst the enchanted
bubble and embrace corruption and chaos â€” Weâ€™re going to discuss the
topic of image glitch art.</p>

<h2 id="wÌ¸hÌ¸aÌ·tÌ´Ì¶sÌ´-Ì¶aÌ´-Ì·gÌ·lÌ¸iÌ·tÌ´cÌµhÌµ">wÌ¸hÌ¸aÌ·tÌ´â€™Ì¶sÌ´ Ì¶aÌ´ Ì·gÌ·lÌ¸iÌ·tÌ´cÌµhÌµ</h2>

<p>Welcome to the land of creative destruction: image glitch art. Our story
starts with a simple idea: glitching a wallpaper to create a slideshow
of corrupted pictures.<br>
The unfortunate victim of our crime: The world (Right click <strong>&gt;</strong> View
image, while keeping the <strong>Control</strong> key pressed, to admire it in more
details while its still in its pristine form):</p>

<p><img src="https://venam.nixers.net/blog/assets/glitch_art/world_map.jpg" alt="World Map, nominal case" loading="lazy"></p>

<p>Before we begin, letâ€™s attempt to define what weâ€™re trying to do: What
is glitch art?<br>
Like any art movement, words can barely express the essence behind the
meaning, they are but fleeting and nebulous. Regardless, Iâ€™ll be an
infidel and valiantly express what I think glitch art is.</p>

<p>A glitch is a perturbation, a minor malfunction, a spurious signal. In
computers, glitches are predominantly accidental events that are
undesirable and could possibly corrupt data.<br>
Glitch art started as people developed a liking for such unusual events
and the effects glitches had on the media they were perturbing. Some started
to collect these glitches that happened naturally in the wild, and others
started to intentionally appropriate the effects by manually performing them.<br>
In the art scene, some started using image processing to â€œfakeâ€ true
glitching effects.</p>

<p>Glitches happen all the time and everywhere, information is never as
durable and reliable as we might like it to be, and living in a physical
world makes it even less so. Youâ€™ve probably encountered or heard of the
effect of putting a magnet next to anything electronic that hasnâ€™t been
rugged to withstand such scenario.<br>
Thatâ€™s why many techniques have been put in place to avoid glitches,
at all layers, from the hardware storage, to the software reading
and interpreting it. Be it error correcting codes (ECC) or error detection
algorithms, they are all enemies of glitch art and the chaos we like.</p>

<p>However, this niche aesthetic is more than a fun pass-time for computer
aficionado, there is a bigger picture. Similar to painters with
brushes on a canvas, we are offered a material, an object to work with
â€” a material made of bits and formatted in a specific way.<br>
Like any object, our medium has a form and meaning, it can move, it has
a size, it can be transferred, and interpreted â€” information theory
is the field interested in this.<br>
Like any object, our medium can be subject and react to deformations,
forces, and stressors. How it flows is what the field of rheology
is interested in (not to be confused with computational rheology, the
field of fluid simulation.) The medium fluidity can be studied to answer
questions such as: is it elastic, solid, viscous, or oily, how does it
respond, within the bound of information theory, to different types of
applied forces.</p>

<p>Here are some words you may encounter and that you definitely want
to know:</p>

<ul>
  <li>
    <p>Misregistration: Whenever a physical medium misread data because of
damages caused by scratches, dirt, smudges, gamma rays, or any other
treasures the universe throws at us.</p>
  </li>
  <li>
    <p>Datamoshing, Photomosh, Imagemosh: Abusing the format of a medium,
normally compression artefacts, to create glitches. For example, video
compression often use i-frames for fixed images and p-frame for the
movement/transition of pixels on that image. <a href="https://www.reddit.com/r/datamoshing/">Removing i-frames is a
common glitching method</a>.</p>
  </li>
  <li>
    <p>Databending: An idea taken from circuit bending, bending the circuit
board of a toy to generate weird sounds. Databending is about bending
the medium into another unrelated one, reinterpreting it as something
it is not meant to be.</p>
  </li>
</ul>

<p>Let me add that glitch art is vast and fascinating, this article is but a
glimpse into this space. If youâ€™re captivated as much as I am, please take
a look at <a href="http://gli.tc/h/0nline/">gli.tc</a> and <a href="https://beyondresolution.info/">Rosa Menkmanâ€™s Beyond
Resolution</a>. Images can be pleasantly
destroyed in a great number of ways to create masterpieces.</p>

<h2 id="iÌ·mÌ·aÌ·gÌ´eÌ´-Ì¸gÌ¸lÌ´iÌ´tÌ´cÌµhÌ¸-Ì´aÌ¶rÌµtÌµ">IÌ·mÌ·aÌ·gÌ´eÌ´ Ì¸GÌ¸lÌ´iÌ´tÌ´cÌµhÌ¸ Ì´AÌ¶rÌµtÌµ</h2>

<p>Before starting letâ€™s give some advices:</p>

<ul>
  <li>Back up your precious files before corrupting them.</li>
  <li>Any glitching techniques can be combined and/or applied multiple times.</li>
  <li>Sometimes too little has no effect, and sometimes too much can destroy
the file.</li>
  <li>Itâ€™s all about trials and errors, especially errors that result in
glitches.</li>
</ul>

<h3 id="Ì·hÌ·oÌµwÌ¶-ÌµtÌ¸oÌ´-Ì¶iÌ·nÌ¶dÌ¸uÌ·cÌ¶eÌµ-Ì¶aÌ¸-Ì¶gÌ¸lÌµiÌ·tÌ¶cÌ¸hÌ´">Ì·HÌ·oÌµwÌ¶ ÌµTÌ¸oÌ´ Ì¶IÌ·nÌ¶dÌ¸uÌ·cÌ¶eÌµ Ì¶AÌ¸ Ì¶GÌ¸lÌµiÌ·tÌ¶cÌ¸hÌ´</h3>

<p>Now itâ€™s time to think about how we can apply our mischievous little
stimuli, its size, the level or layer at which itâ€™ll be applied, and
the methodological recipe weâ€™ll concoct to poison our images.</p>

<p>Glitch artist Benjamin Berg classifies glitches into 3 categories:</p>

<ul>
  <li>Incorrect Editing: Editing a file using a software that
wasnâ€™t made to edit such file. Like editing an image file as if it
was a text file.</li>
  <li>Reinterpretation aka Databending: Convert or read a file as if it was
another type of medium. Like listening to an image file as if it was
an audio file (aka sonification).</li>
  <li>Forced errors, Datamoshing, and Misregistration: A software or hardware
bug to force specific errors in the file. This can be about the
corruption of specific bytes in the file to induce glitches, or
something happening accidentally like a device turning off when saving
a file.</li>
</ul>

<p>So letâ€™s get to work!</p>

<h3 id="mÌ·aÌµsÌµhÌ¸iÌ¶nÌ¶gÌ·-Ì´tÌ¶hÌ·eÌ·-Ì·dÌ¶aÌ¸tÌµaÌ¸-Ì¸rÌ·aÌ¸nÌ¶dÌ¶oÌ¸mÌ´lÌµyÌ·">MÌ·aÌµsÌµhÌ¸iÌ¶nÌ¶gÌ· Ì´TÌ¶hÌ·eÌ· Ì·DÌ¶aÌ¸tÌµaÌ¸ Ì¸RÌ·aÌ¸nÌ¶dÌ¶oÌ¸mÌ´lÌµyÌ·</h3>

<p>The easiest, but roughest, way to glitch a file is to put on our monkey
suit and overwrite or add random bytes in our image. As you would have
guessed, this isnâ€™t very efficient but half the time it does the trick
and forces errors.</p>

<p>This technique is better suited for stronger materials like images in
raw format â€” without metadata and headers. Weâ€™ll understand why in a bit.<br>
To convert the file to raw format, open it in GIMP, select <strong>Export As</strong>,
select the file by extension, and choose the raw type. For now, it doesnâ€™t
matter if you pick pixel-ordered or planar, but weâ€™ll come back to this
choice later because itâ€™s an important one.</p>

<p><img src="https://venam.nixers.net/blog/assets/glitch_art/gimp_saveas_raw.jpg" alt="GIMP process to save image as raw" loading="lazy"></p>

<figure><pre><code data-lang="shell">file world_map.data
<span># world_map.data: Targa image data - Map (771-3) 771 x 259 x 1 - 1-bit alpha "\003\003\003\003\003\003\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001\001"</span></code></pre></figure>

<p>You should also note the width and height of the image as it now doesnâ€™t
contain this information anymore, and weâ€™ll need those to reopen it in
GIMP. In our case it is <code>2000x1479</code>.</p>

<p>We now proceed to hand over the file to our least favorite
staff and let them have an anger tantrum at it. So what does
it look like, letâ€™s take a look at <a href="https://www.youtube.com/watch?v=oj6NMiuU0ys">the result our monkey
did</a>:</p>

<p><img src="https://venam.nixers.net/blog/assets/glitch_art/world_map_random_bytes.jpg" alt="World Map, monkey have been randomly mashing the
world" loading="lazy"></p>

<p>Not bad at all for something random, but we can do better.</p>

<h3 id="cÌ¸oÌ¶mÌ¸pÌ·rÌ¶eÌ·sÌ¸sÌµiÌ¸oÌ¶nÌ´-Ì¶dÌµeÌµfÌ¶oÌ¶rÌ´mÌ·aÌ¶tÌ·iÌµoÌ¸nÌ·">CÌ¸oÌ¶mÌ¸pÌ·rÌ¶eÌ·sÌ¸sÌµiÌ¸oÌ¶nÌ´ Ì¶DÌµeÌµfÌ¶oÌ¶rÌ´mÌ·aÌ¶tÌ·iÌµoÌ¸nÌ·</h3>

<p>Some medium are more malleable when squished properly and squished
in different ways. The image sheds a lot of information and only the
essence stays. Thatâ€™s a form of databending.<br>
For example, increasing the compression of JPEG images can open the path
for glitches to happen more frequently. This is a key asset, especially
when trying to create errors related to the compression parameters within
the format of the file.</p>

<figure><pre><code data-lang="shell">convert <span>-quality</span> 2 world_map.jpg world_map_compressed.jpg</code></pre></figure>

<p><img src="https://venam.nixers.net/blog/assets/glitch_art/world_map_compressed.jpg" alt="World Map, compressed to extract its
essence" loading="lazy"></p>

<p>Keep this in your toolbox to use along with other techniques.</p>

<h3 id="gÌµeÌ´tÌµtÌ¶iÌ¸nÌµgÌ·-Ì´iÌ¸nÌµtÌ·iÌµmÌ·aÌ´tÌ¸eÌ´-Ì¸wÌ¶iÌµtÌ¸hÌ¶-Ì´tÌµhÌ¸eÌ¸-Ì´fÌ·oÌ´rÌ¶mÌ¸aÌ´tÌ·">GÌµeÌ´tÌµtÌ¶iÌ¸nÌµgÌ· Ì´IÌ¸nÌµtÌ·iÌµmÌ·aÌ´tÌ¸eÌ´ Ì¸WÌ¶iÌµtÌ¸hÌ¶ Ì´TÌµhÌ¸eÌ¸ Ì´FÌ·oÌ´rÌ¶mÌ¸aÌ´tÌ·</h3>

<p>We want to corrupt in the most efficient way possible, to create
attractive chaos from the smallest change possible. To do that we have to
get intimate with the medium, to understand its deepest secrets, tickle
the image in the right places. This is what we previously referred to
as imagemoshing.</p>

<p>Thereâ€™s a panoply of image formats, and they all are special in their
own ways. However, thereâ€™s still some commonality:</p>

<ul>
  <li>Header, Footer, and Metadata: If the format contains these extra
information, be it extraneous or essential, what they represent, and
how they affect the rest of the image.</li>
  <li>Compression: The format can either be compressed or not. When it is
compressed, there can be extra bits of information to help other software
uncompress the image data.</li>
  <li>How the data is laid out: Usually, the image color information is
decomposed into its components such as HSL, RGB, or others. These
components then need to be represented in the image data, either in
an interleaved or planar manner. Planar refers to writing components
independently in the data (<em>ex:</em> all R, then all G, then all B),
while interleaved refers to having them joined non-contiguously in an
alternate sequence (<em>ex:</em> RGB, then RGB, then RGB..).</li>
</ul>

<p>Manipulating these to our advantage can lead to wonderful glitches. For
example, in our previous raw image example â€” an image bare of header,
footer, and without compression â€” the pixels were interleaved which
gave rise to the effect weâ€™ve seen, namely shifts and changes in some
colors. Having them in planar form wouldâ€™ve led to different glitches
in separate color channels.</p>

<h3 id="rÌµeÌ·iÌ·nÌ´tÌ¶eÌ·rÌ¶pÌ¸rÌ´eÌ¸tÌ¸aÌ·tÌ¶iÌ·oÌµnÌ´-ÌµaÌ¸sÌµ-ÌµrÌ¸iÌ·cÌµhÌ¸-Ì¸tÌ·eÌµxÌµtÌ´-Ì´aÌ´kÌ·aÌ¸-Ì·wÌ¶oÌ´rÌµdÌ´pÌ´aÌ¸dÌµ-Ì·eÌµfÌ¸fÌ´eÌ¶cÌ¶tÌ¶">RÌµeÌ·iÌ·nÌ´tÌ¶eÌ·rÌ¶pÌ¸rÌ´eÌ¸tÌ¸aÌ·tÌ¶iÌ·oÌµnÌ´ ÌµAÌ¸sÌµ ÌµRÌ¸iÌ·cÌµhÌ¸ Ì¸TÌ·eÌµxÌµtÌ´ Ì´AÌ´KÌ·AÌ¸ Ì·WÌ¶oÌ´rÌµdÌ´PÌ´aÌ¸dÌµ Ì·EÌµfÌ¸fÌ´eÌ¶cÌ¶tÌ¶</h3>

<p>Letâ€™s give this a try with the well-known WordPad effect, which is about
databending an image into rich text: opening the image in WordPad and
saving it.<br>
Keep in mind that this only works with raw images as itâ€™s highly
destructive and otherwise could break fragile key info in the header
and footer. So letâ€™s reuse our interleaved raw image of earlier but also
get a planar one.</p>

<p>This is our results for interleaved:</p>

<p><img src="https://venam.nixers.net/blog/assets/glitch_art/world_map_wordpad.interleaved.corrupt.jpg" alt="World Map, WordPad effect interleaved" loading="lazy"></p>

<p>And for planar:</p>

<p><img src="https://venam.nixers.net/blog/assets/glitch_art/world_map_wordpad.planar.corrupt.jpg" alt="World Map, WordPad effect planar" loading="lazy"></p>

<p>Technically, what happens is that during the bending and interpretations
as rich text, some bytes are inserted in some â€¦</p></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://venam.nixers.net/blog/programming/2020/10/05/corruption-at-the-core.html">https://venam.nixers.net/blog/programming/2020/10/05/corruption-at-the-core.html</a></em></p>]]>
            </description>
            <link>https://venam.nixers.net/blog/programming/2020/10/05/corruption-at-the-core.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24694017</guid>
            <pubDate>Tue, 06 Oct 2020 01:43:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Donâ€™t Find Mentors. Find Your Future Self]]>
            </title>
            <description>
<![CDATA[
Score 38 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24693763">thread link</a>) | @jdcampolargo
<br/>
October 5, 2020 | https://www.juandavidcampolargo.com/blog/future-self | <a href="https://web.archive.org/web/*/https://www.juandavidcampolargo.com/blog/future-self">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div data-layout-label="Post Body" data-type="item" data-updated-on="1601739309102" id="item-5f789898afa8642a86474931"><div><div><div data-block-type="2" id="block-ba447757312844ef3468"><div><p>Back in 2018, I was fifteen years old, and I wanted to do something over the summer. I could get a job, work on my businesses, sit around, watch Netflix, or work for free at tech startups.&nbsp;</p><p>I chose the latter.</p><p>Why would I work for free in a startup? It wasnâ€™t for free; I had to pay for the train tickets and food. I didnâ€™t care. I knew that being out there meeting founders and investors would be the best way to learn.&nbsp;</p><p>I wanted to be involved in the startup world by creating, developing, and investing in them. I believe thatâ€™s the way how we can change the world and have a positive impact.&nbsp;</p><p>But how in the world would I find those startups?</p><p>I sent 500+ emails, called companies, asked family friends, and did everything I could, but I got annoyed. So I was like, â€œScrew it, Iâ€™m just going to go and ask them.â€</p><p>I went to this startup incubator called Techstars at 1871 and asked if they were hiring. And that I was willing to work for free or â€œvolunteer.â€</p><p>At first, the founders looked at me like, â€œKid, how old even are you?â€ Most founders looked the other way, but two guys were like, â€œDamn, letâ€™s find something for this kid to do.â€</p><p>And thatâ€™s how one of my best summers started.</p><p>The founders may have thought a fifteen-year-old wonâ€™t be able to do much, but I literally told them, â€œIâ€™m willing to do anything, I just want to learn and spend as much time as possible at the incubator.â€ It felt like I was learning by osmosis.&nbsp;&nbsp;</p><p>I would go with the founders to meetings with investors and mentors and take notes, find venture capitalists interested in investing, work on hardware, and anything they told me to do.&nbsp;</p><p>I worked with three startups that developed an AI fitness assistant, solar drones, and a real estate crowdfunding site.&nbsp;</p><p>That summer showed me the world of startups. I met people like the governor, billionaires, and overall people who were heavily involved in the startup world.&nbsp;</p><p>I also experienced the startup world, which Iâ€™m sure will pay off in the future.&nbsp;</p><p>But most importantly, I met Paul, the founder who gave me the opportunity.&nbsp;</p><p>I always kept him posted on my progress in my projects and would ask how his company was doing. And Thanksgiving, Christmas, and Happy New Year messages as well.&nbsp;</p><p>Well, it turns out that Paul studied in the same college and did the same major. Iâ€™ve talked to him multiple times in the last few weeks and it reminded me of the <strong>importance of having people who were once where you are.</strong></p><p>Some people called people like Paul â€œmentors,â€ but that word is misleading because 1) itâ€™s overused and over-hyped 2) itâ€™s transactional and self-centered.&nbsp;</p><p>How do I call people like Paul? I call them, <strong>â€œMy Future Selves.â€</strong></p><p>They often can give that little push you need. And you get to cheat because you get to talk to your future self. Whatâ€™s even better is that you can always do things differently and learn from their mistakes.&nbsp;</p><p>Studying engineering isnâ€™t the easiest thing in the world and when you bomb a test, you can feel like â€œYou donâ€™t have what it takes.â€ My first chemistry exam? Hmmm. One to forget. I like to do as best as I can and not doing as expected frustrates me, especially when I need to get a good GPA to transfer to the engineering school.&nbsp;</p><p>I felt like I was the only one, but as I talked to â€œMy Future Selfâ€ or Paul, I realized I wasnâ€™t the only one and he too didnâ€™t do great on his first chemistry exam. But he improved and could transfer to the engineering school.&nbsp;</p><p>He advised me on how to approach studying and how to approach the test and my grades have improved.&nbsp;</p><p>Sometimes, you can also lose sight of the big picture of what you want. Future selves remind you to keep focused not by scolding you but by asking you simple questions like, â€œWhat are your future plans after college?â€ or â€œDo you like what youâ€™re studying?â€</p><p>In my case, Paul knows Iâ€™m into startups and <a href="https://www.juandavidcampolargo.com/blog/ambition" target="_blank">eating the world</a>, so he helps me with choosing a path that aligns with my interests and with my goals.&nbsp;</p><p>Looking back, that was not a normal thing for a teen to do. Yes, you can say it, â€œI was weird.â€&nbsp;</p><p>Well, not really. I knew <strong>who</strong> I was and <strong>where</strong> I wanted to go. Thatâ€™s how going up to startup founders asking for a job could become your summer.&nbsp;</p><h2>How To Find Your Future Self</h2><p>If I hadnâ€™t talked to Paul a few times since college started, Iâ€™d honestly be down and not very excited about the possibilities. <strong>Future selves can you show a path that gets you excited to work harder and more ambitiously.&nbsp;</strong></p><p>If youâ€™re interested in finding a possible future self. Iâ€™ve learned a few lessons that can be helpful.&nbsp;</p><p><strong><em>Avoid being artificial. </em></strong>Weâ€™ve all heard the talk, â€œGet a mentor and blah blah blah.â€ Sure, but that canâ€™t the only reason. You should be genuine. And please please, donâ€™t be asking people to be your mentor.&nbsp;</p><p>How can you be more genuine?</p><ul data-rte-list="default"><li><p>Make yourself useful to them</p></li><li><p>Get a job or internship at that personâ€™s company or lab</p></li><li><p>Ask unique and interesting questions</p></li><li><p>Just DM or email them</p></li></ul><p><strong><em>Understand their motivations. </em></strong>Keep this question in mind, â€œWhy would someone want to mentor you?â€</p><p>It usually happens when a high-potential person approaches them, and they can help him/her reach that potential.&nbsp; And if they can see their advice pays off 10x in you. If your future self tells you to walk 5 steps, they want to see you walk 50 steps.&nbsp;&nbsp;</p><p><strong><em>Donâ€™t find mentors. </em></strong>This one will be the hardest to understand, but the most powerful. I donâ€™t ask people, â€œDo you want to be my mentor?â€</p><p>Most people will say â€œNo.â€ Instead, the way you find mentors or future selves is by:</p><p>1) Finding people who are where you want to be.&nbsp;</p><p>2) Asking great questions and becoming genuinely interested in their work, and finally,&nbsp;</p><p>3) Keeping in contact with them and asking questions and/or advice when you need it. Or if you find something useful for them, this is when you help them.</p><p>Iâ€™m grateful to Paul and many of my other future selves because they allow me to learn from their mistakes, pick a unique path for me, and give you the little push and fresh perspective when you most need it.&nbsp;</p><p>Thank you, Paul, and thank you to all my future selves!</p></div></div></div></div></div></div></div>]]>
            </description>
            <link>https://www.juandavidcampolargo.com/blog/future-self</link>
            <guid isPermaLink="false">hacker-news-small-sites-24693763</guid>
            <pubDate>Tue, 06 Oct 2020 00:52:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Oswald Spengler â€“ an intellectual life]]>
            </title>
            <description>
<![CDATA[
Score 51 | Comments 41 (<a href="https://news.ycombinator.com/item?id=24693655">thread link</a>) | @objections
<br/>
October 5, 2020 | https://engelsbergideas.com/portraits/oswald-spengler-an-intellectual-life/ | <a href="https://web.archive.org/web/*/https://engelsbergideas.com/portraits/oswald-spengler-an-intellectual-life/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://engelsbergideas.com/portraits/oswald-spengler-an-intellectual-life/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24693655</guid>
            <pubDate>Tue, 06 Oct 2020 00:30:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Probability and Statistics with Applications to Computing [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 70 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24693589">thread link</a>) | @ArtWomb
<br/>
October 5, 2020 | https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf | <a href="https://web.archive.org/web/*/https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><i d]g(*hÂªÂ®jcÅ¡Â»â€¢ÃŸÃ•]Â¿Â¿}[xÃ¯^Ã®r)Å f3@="Ã¯JÂ§â€œcccâ€“/_^RRbsâ€”Â¬mÃˆ" ;Ã„Ã§ÃeÃ¡Ãš?Â§â€¹â‚¬Ã‘="" p8]xÂ±<Ã…<Â­}ÃŸÃ´â€¡Ã™â€¦â€¹!kim2Å â€¹â€¹m1cÃ†Â¡â€“â€ºÃÂ·yyybÂ¡'ï¿½ï¿½aÃ€â€¡kdÂ¡8Ã»Ã»Â£â€¡="" Ã¶Ã˜Â½+ÃªÃ¦ï¿½dÃ´fÃ”Ã :qÃ euÃŠju;~Ã—Â¨)?l7Â¥="" Ã‚="" Ã–Â®[bcÂ£vciqÃ²ÃŠ!Ã¡2ÃœÃ‘ÃƒrÃ¸="" ÃŒ|Ã±ÃÃª`â€°-Â»ï¿½8&7Å¾Â¢ÃƒÃ½="" Ã½Â­Â¼â€_Ã¶Ã‡Å’)Å Ã‡3â„¢.dÂ¢Ã§lÂ¥&nÅ“xÃ»Ã¶Ã­'oÅ¾Ëœo)Â­ÃŠï¿½â€œ?yÃ‰Å vw-jÃ¾b7rï¿½Ã…Â»s="" â€ ï¿½Ã§h<bÂ¦iÃ¬Â²Ã“â€a-Ã¥pÃ‹mÃ‰Â¬y-g="" "Ã¦Â³gÃ6nÃœhÂ¡ï¿½\]Â°xÃÃ½Ãºzï¿½]gÃ³â€¢â€ ="" mÃŠ`4ÃÃ®â€¡qp;iÃ¬1â„¢Ã¬Ã´Å¸Ã¿|Ã¥Ã¤Ã¤Ã‘Â«wÂ¯#f,yÂ²Ã¤Ã¢Ã…â€¹.mqÃ•Â´Ë†ï¿½Â£`â€¹Ã½ÃƒÅ½Ã°Â¢ÃŸ7aÃ¯Ã‹Ã™)Ã‘sÃ”Å’hÅ½,ÃšxÃ•Ã–Ã²(jÂ®aÂ¬jÃ‹kÂ²yÅ“uÃ«Ã–Ã™bÃÂ¡c-7Å¸oÃ³Ã³Ã³Ã·Ã¬Ã™3yÃ²Ã¤&mÅ¡Ã¸Ã¸xâ€œÃ‰d"Ã‘ï¿½nwâ€¹qÃ©Å’Ã¡ÃƒÂ¼Â²â€“nÃ›~Ã°@ÃŒÃ™Ã“ÃŠâ€¹Ã§Ã•Â¹Ã•9Ã”'ï¿½+Ã¶Ã®Å½=""><b nÂ¸1Â Â±ÃœÂ®3Ã”%Ã•e!ï¿½jÂ¡Â«Å’ÃÂ±ï¿½Ã†cvâ€°ynglbpaï¿½kÂ¯â€Â«cÃ²,Â´Ã¥ï¿½Â£(Å ÃœÃŠ"Ã^Â¯iÃ®2Â¢^?Ã¥â€˜Ã—ueâ€°ï¿½mâ€Ã‰Ã^ÃsÃ?Â¿}Ã‡vÃ¸â€“.Ëœ_`6zÃÃ‘#ÃÂ¶ÃoÂ°Ã®Å½;Ã®pÂ©d^Â¯Ã´Â«eÃ…\Ã‹Â¶Ã£Ã›ÃÅ¸|Ã½Â±uÃ±&Ã—â„¢Ã‹b9%â€¦n)Ã˜Ã¤zÂµ(Â¶Â¼'Â§Â oÃ½ÃšÃ¢Ã­wÂ¬Å¾k="" Ã¼auÂ¸â€°;Ã¯pvâ€â€œï¿½@&Ã«.â€˜tâ€°Âº="" â€¦Â°Ã±ÃÃ¯Â¬xÂ±Ã¢\Ã¥Â¤eÃ­Å’="" nÃ¼ï¿½?Ã¾xeâ€™Ã%buÃ”Â¥Å½Â°j+râ€¦Â¤b_â€ºÃ€ÃœuÃ˜.wÃªâ€°Ë†sÃ¦Ãimzua@jÃ‘ÃˆÅ“Â¡Å½Ãª.c!ËœÃœï¿½Ã›Ã›â€¡Æ’Ãº=""><s â€˜hÂªÃ·*^Ã­Ã•ÂªjÅ¾i[jÂ¯Å¸!Â±Â¼Å¡gyÃ¤1<`Ã€Ã•="" Ãœ8(Â²gp!z$_ÃµÃ´Å’Ã»kÃâ€uqï¿½Ã Ã¢tâ€¡Ã‘h6:p<â€°Ã£Ã—Ã®Å’ï¿½Ã€Ã$bxÃ€%ÂµÂ¨Ëœâ€˜}h="" !="" lâ€˜"â€â€¡b!qÃ‚hË†@b"ï¿½pÂ¼zeâ€šâ€˜Å’â€¦ic"ï¿½(Âµ="â„¢41" Â¡Â¤!Â½Å lÃ€ÃˆÃŒÂ«fip7Ã±Â¼ÃªÃ¾Â¿Ã»Ã‰â€¡Â½sÃ¬Å¾Å¾ï¿½Â kÃ¡uï¿½Â¤Ã‘Â¼j9Ã”3Ã®â€°â€“Ã¯Ã¼Ã¿Â¥="" wÃ¼Â¤Ã°Ãº+nÅ¸fÃ­Ã•cÃ.Ã·Å“Ã·Ã¡Ã’â€¢â€“sâ€šhhÂ¯râ€šÃuiâ€™x5â€“4Â©Â¼ÃŠÂ«ï¿½&{Â¯="" bÃšÃ˜â„¢eÃ¶Â«ï¿½â€“Â®Ã²Â®6<uÂºqÂµÃ½v="" Ã¯â€°oÃˆcoÅ“uoÃ­Â·Ã¨s="" â€°Ã§u&Ã»ï¿½oï¿½Ã´ÃªËœwÅ¡]tc4gËœwiiÂ´â€™gzâ€¢'Ã±ÃªÂ±ï¿½mÃ¯="" â€¹Å¸:fbÂªÃ°*="" ;kÂ¯Ã²Â¬Â½ÃŠË†?Ã@Å¡Â¾vÃ§Ã¹Ã³*d^sÅ½iâ€˜fÃªÃ uÂ¸â„¢ÃÂ«ÃªÃ”Ã•Â«Æ’sxvÂ®x5ï¿½<Ã²ÃˆcÂ¸ÃˆÃ’"Ãâ€¹Ã¼Ãˆâ€Âª\Ã¡Ã¥pÃ½0Ã¦y0Ã£e~Â¤|&Â®]Æ’Å“.ÂªÃ“Ã»Â¤ï¿½Ã¨â€ºa%â€ hÃ°â€°="" Å¡d!Â¨_<â€fÆ’Ã»â„¢="" Ãeâ„¢a`l%Â¤:iÃ‚qÃ¦hâ€™Ã†h\rÃ±â€°Å’bÂ¬Å½&â€Ã‡â€œÃ†hhliÃÂ«Ãâ„¢3Ã¢kÃ»Â¾5<Â½j}ÂºÂ©Ã«ï¿½Â»Ã¤knÃ¹}qÃ¥Ã»i="" Â¯ÃÂµÅ¾uÃ|ÃŸÂ·_Ã¡="" iÆ’Â©Â¡Â¼'ï¿½ï¿½^Ã¥="" Ã¦uâ€ qÂ¤Â©ÃÂ«bÃgÃ¯â€ºÃ–8Ã±Ã…dkÃ¤gÃÃªâ‚¬ï¿½^Â½urÃ¥%ï¿½'[-8t8!="" fâ‚¬,Â¯Â¾Ã¿iÃ¥qï¿½Ã‡oÃ­Â²iÂ¯ÂªÃ¼Ã©hrÃ¤Ã•pÂ¯*Å“dzuÃc%Ã·tÃºï¿½ÃŸÃµk^Â­"ï¿½h0jÃªubÅ¡*Â«Ã Â¾kÅ¸Â¬Â°Ã´Ã®ï¿½Ã…â€â€Ã„Ã³ÂªÅ $sÂ¯â€™â€h7dÃ©Â¿ï¿½â€™x^{dÃ®uÂ±Âºaj^="" Â½Å o2Ã‰#ï¿½<Ã²Ã8Æ’Ã‰Ã›_pj="" n="" â€¹Â¥,jÂªÃ¥Â ="CÃ«Ã¥" Ã…="" â€°Â¨Ã—dq#Ã²Â¹zâ€¡p="Ã•Â«Â¼Dâ€šÃº3#Â¡ÃŠk!â€™HWaÂ²Ã§Ã’Ãâ€&nbsp;Å’Ãº{6!" Â¬yÃŒ.mË†2Â¦Ã©aï¿½?ÂªÃ±*Â«Ã±Â¼j}Â¶Â¥Ã­wÂºgÃ™Â­Â¿ÂºpÂ¾Â¬â‚¬Ã²*=""><i â€¹b#Ã…Ã„(cÅ’##eÆ’*mÃ¼â€œrÅ â€°â€¹txÆ’Æ’Ã @1Â¸Ãœâ€¢bÃ©Ã®Ã‰Ã¥Ã“Ã ÃºÃ Â¸â€¹iÂ§rÅ’Â¤Ã pmÃ†]Å â„¢Ãšx="" â€¡Ã®-&ï¿½Â©="" bâ€19[khyâ€3Æ’Â§Ã‡\â€¹6â€"'?Å’â€œtÃ Â´1%â€¢â€“Ã¼ejq9â€ºkï¿½rÂ¬Æ’#="" Ã…x="" Å½Â¯Â´98Ã—*="" Â¥x="Ã—Ã‚fb&quot;#EÃŸÂ¥" Ã³jÂ¸ÃŸÃ‚@â‚¬Ã20gÃŒ5Å Ã€Ã‚â€¹â€°Â´Ã‡k&â€ [Â¶Ã¢Â§9p="" m8le="v]ÃÂ­â‚¬$=Ë†Ã¶+Â¤" =""><i ÃŠn<yÂ¾"+xÃ’o%bwï¿½oÂ v!Ã¦Ã‡="" <aÃ«Ã‚Ã“jâ€¡ï¿½Â´â€¦'Â `xz(hÅ¾,="" bÃ”a{Ã±Â¬gâ€¡Â¡â€°Â£:="" ovâ€°ÃŒcoÂºÂ´Ã—ofâ€¡!(â€ºÃxzâ‚¬ÂµÂµÃjkkÃ½Ã½Ã½Â±.Ã¬gÂ£iÃ“ffÂ£Ã¡Ã§Å¸opÂ¨ÃšpzÂ¸pÂ¡Ã‰h"Å¸ï¿½`ÃƒÃÃ‘Â±Â Â°Ã Ã¾Æ’Ã»Â¯xï¿½Â²Ã¡="" Â®Ãcï¿½jkÅ¸<~Â¼lÃ®Â¤Ã”0u=""><i Â¼,<-;l]?hï¿½="xÃ™â€°'Â¥`Ãï¿½Â¦Ã½â€ºÃºÆ’vÃ“7Â±TÅ¾?;ÂºMb7wE62" gâ€Ã”j<bâ€”.kwgÂº+ÃšÃºÂ©ÃœcxÃšxÃ§Ã¸0eÃ§="" c@1ÃªËœ4â€¢Ã©oÃŠÃ°3vÃ²Ã•Â·Ã±sÃ…Â´â€Ã·â€˜="" rd&Ë†zhï¿½pf9ï¿½eÃ€5ÃŠâ€œÅ¡&Ã¬)Â¢â‚¬k@yp[qhÂ¤="" Ãœâ€¦â€¹Ã¬ÃÃ‡f4Â°ikahh#&Â¶*Ã„_Ã€$cpÃ¥!Ã•Â®hÃ˜Ã‚}Ë†Ã–="" zÂ¾ÃœsÃ¯Â spd;Â¨Ã‚l=""><i Ã‚'@ÃˆÃ¢Â°jiÃ°$*Ë†_Â³zÃ¡pÃ½Ã›aâ€šÃ¯Ã€â€œj0]Ãâ€œÂ ="" cuwaâ€Â¢Â Â²="" Ã¥r4ujÃŸÂ§ÂªÃ„sjÃ·mÃ°Â¤(Ã˜u<5jâ‚¬Â½ÃÃ´4Ã€ï¿½eÃ¼Ã½fÃ²$="Â¬-Ã±Ã·7" Å¾ÃÂ®ÃÃœÃ¡jÃµÂ»â€¢.Ã†2ÃœÃŒxÃ£uÃ¬|yÂ£Â¦ÃªÂ¹Ã¶Ã£xzÃ³<Ã»Â¡Ã›4Ãˆ;5ÃŠÃ·Å¾fwÃŠâ‚¬gjÃ€,cÃ¢d#â€¡5<â€¡e\Â·Ã¾&.Ã–Ã¼â„¢Â¦z3="" y3eÂ¯Ã±Â§Ã‰&&lÃ¡jojÃ›;Â±mÃ½Â¹Ãƒâ€ hÆ’Ã |Â²=""><i Ã¢Ã¯a5Ãâ€œÂ¢="" ÃÃ™Ã™Ã»Â¡i:aaaÂ¹Â¹Â¹Ã¿f<uÅ’Â¢Ãœ`ÂºË†Â§â€fyaâ€¢xjÂ¦="" â€šÃ(xwwwÃ©Ã’Â¥Ã¥Ã‹â€”c3Â»Ã»Ã»Ã¸Ã¡#Â­â€š]Ã€â€œÂ¨`Ã—Ã±dÃˆxr%Ã¢Å¡Ã…il0Ã¥oâ€”Ã°â€Ã’Ã¡iuï¿½pâ€¦$<Â¿eÃ¼2x=""><s kâ„¢-sÃ²Å¡Ã¦s\Ã¿h%=""><i Æ’Æ’â€¦="" Â¨="" Ë†fÃ„Æ’Ã«Å¡Ã¡Ã¿nÃ«1Å¡$â€¢Ã¡,â€”ÃÃÅ’â€¡Ã€'Å¸ntÃœÃ‹Ã ="" ÃƒÂ´3<Ã¢Ã¹Ë†Â²Ã—ï¿½ï¿½Â¶Ã‚Ë†ÂºÃ—#â‚¬e0â€™â€™ï¿½cp0â€¢Ã±="" ËœÂ¬Â»Ã`="Ã†`+Å’Ã€v3Fï¿½Â«Ã0E|Ã·2Â¸Ã`ÃŒÃ°Ã®Ã”Ã¹HQÃ²98â‚¬â€¦~Ã˜v`Â¦Ã­Ã‚Ã€nÂ³Â­Ã¬Ã›Â¼k4UÃ¼kÂ¤wÃ¡Â³Å’â€¦ï¿½Â·Ã€Ã°Â®Â¹"><i><s Å’Â¦Ãœ7Ã‹Ãˆsï¿½\Â â€“xÅ Âµ(`="" Ãï¿½Ã¿Ã¨zÃ°Ãº~Ã¬Ã¿tz~tÃ™Ã‹cï¿½Â¢Â ÂºÅ“j%ï¿½Ã¹Â¾Ã…â€¦Å’â€¹*ï¿½ÃˆrÃ²â€@Ë†|)pÃ¹Ã€ÃœÃÃ–4Ã­Ãˆâ€˜#.Å“<y2Â¿ÃŒÃˆÃˆ0="" ÃƒwÂ¿ÃÃ½Â­aÂ¢="" Ã˜="" Âª="" 0="" [ï¿½o@vrâ€˜Ã•jÃÂ£ï¿½â€“Â«â€™Ã›Ã¼Å¾Â´Ã±Â³="" Ã‡ï¿½(Â¸ï¿½="" cÃ†Ã¥e{ÂµÃ€dfbÃ¿r="" sË†Â¥Ã¡Ã…="" â€š="" â€”="" .0Ã„rÅ Â°}Â²;â€“Â¬Â¶â€¦a$`Ã˜Â°j="" ÃœÃ‘y{â„¢Âª[zÂ·="" -yqÃ“pË†Ã—Ã‹â€ rcl*ovÅ½Â£Â´;vÂ®hbÂ¨rÂ¹ÃÃÃ‰Ã‰yÂ»vÃ­Ã”Â©sgÃÅ¡ÂµcÃ‡Ã“4ubÂ ÃŠÃ·Ã‚b]2â€°j`Â¢Â¡ro,xpÃ¥â€¢w~Ã¶Ã™gÂ¹Â¹Â¹gâ€¢Ã·Ã²Â§:dÃ¼#ÃšÂ¦mâ€ºÂ¶mÃ›nÃšÂ´Ã©<{Ã»Ã•<Ã˜Ã³Ã¸Ã¿yyyÃ¼Ã·Âª=""><b><b ï¿½bÃ‰Ã¶vï¿½jÂ§Ã…ÂºdÃ€ï¿½8Ã‡gÃ….k\â€ Â¨â€¡Å’="" =""><i Â¬Å½qÃ”Â¥wÂ´Ã©:â€¡Ãµï¿½Æ’Ã³Ã²_fÃ§:Â§uÃ’Â¶Å¡Âª7Â½Â¾Ã¢frÃ¸Â¹Ã—eÃ¸Å Â«Ã¨5Ã±!_Â¢â€˜q<dÃ±Ã¡eÃ…=""><s â€Ã»Â·tb!aÃˆÃ§Ã¹*`lze;â€¹Ã—5Ã‘rÆ’Ã°Â¬ÂºÃÃŒzÃ’Ã Â¸gxlÂ¼Ã€diyÃ»-yÂ­;o="" Ãâ€ iÃ‚+jâ€“Â¶Ã¢o\^Ã¥ÃŠuÃ‚â€œo*1`hÅ¾sÃ›â‚¬="" â€“Â©eÃŠvie;Â¿Â¤Ã·â€Ã¡Â£Â©Ã–hâ„¢c|Ã‰Ã™Â¨bÂ£pâ€¦edï¿½-Ã­Â¨r9Â¹Ãœ=""><i â€™Ã®Ã£qÅ¡Æ’Ã¬}="" Â¯5Ã€7-}Â£Ã°gzfÃ¨Ãµ4nÂ¼Â¦kxËœâ„¢~Å“criÂ³Â¯Ãˆï¿½g@Ã…Â´)Ã®1â€°Ã¾_Â¼alsbÃ·Ë†k=""><b Â­ÃŠÃ«%jtï¿½="" Ã•Ã§Â¬,p1ÃÃ¯Â¾Ã»n]l="" hjâ€šâ€šâ€šÂ Ã›yÃ›Â¶mÂ _Â®Ã¬Â½Ã±Ã‚Â¸="" ="" Â¯ï¿½Ã¶whï¿½Ã†Ã«Â Å“Ã Ã·gÃ‚nj2}Â²?Ã¡Â§](â€pÃ†a`â€ cÂªÃ²Ã°â€ºÃ±Ã¾@Å½d="" tâ€“Ã¸,utyÃŠ="" ÃˆÃºÃ¬izzj)Ã°â€œâ€™Â§iyâ„¢ybâ€“Ã˜luyâ€“Å ="" Å¸=""><b><i o*dÂ´,wÃgÂ·Â®Ã€+Å“Â¶*â€šâ€¹f\yâ€="" Â«'yÃ¨ÃÃ¿Â¥Â¨Â¿Â©Å“Ã¿â€ºÂ´ÃÃ¿roÃ«â€ºÂ®Â·5Â¾wÃ«eqÃÃ½yâ€#ÃŒâ€”Ã¹Ã‚â€¹nÃ§Ã…Â¦pÃ¶Ã£yÃºï¿½Ã›,Ã’ÃœÂ­ÃÃ¼nÂ³yÃ–ÃÂ²Å½Ãœ="" 7ÃsÃ¤="" Â£Ã­bÃ®&2xÂ«Ãâ€°zqÃ¶ÃšÂ¢qÅ’+Â¡:Ã¬â€¦]â€!Ã”â€œÃœ{ÃˆÅ½8^Ã‡=""><s><b Ã7ËœÃ³Â¸Ã£Ã‘Ã‡Ã†a$211yÂ¼hqbbâ€ Ã©pÃ˜1*Å“Ë†â€œg0^3gÃ:tÃ„Â¾Ã½Ã»Â¢Â¢Â¢ÃŠï¿½^ï¿½Â³Â©iâ€œÂ¦Ã¤ÃªccgggËœÃÃ¹â€™Ã¼Ã„Ã„â€Â·xq="" â€Ã…â€°*)â€™Å¡Â·}Ã»Å½vÃŸ?zÃ”Ã¨;w$â€™|xâ€“hpÃk(Ã¿="" 9Ã¹0Å’Ã—Ã¶mÃ›Ã«7hï¿½y="" Ã#Ã€"rÂ©â€Ã£Ã¢zÂ·jmhË†="" Ã†hÃ™Â²Ã¥Â¹sÃ§ï¿½Â¥Â¬Ã¢Ã|Â¦Ã²â€¡ÃŒ,yï¿½Ã¬Å½â€¡gÂ³Ã¦Ã!uâ€¢Âªuâ€”Ã½ÂµÃ¬ÃÂ»w"4Ã©Ã¸@Ã€-Ã»[-Ã¿Ã·?rimÂ¿Ã¾Ã½Ã°Ã="" ÂºtÃ1bÆ’_Â§Ã†Ã¼Ã©uÃ€"â€”â€â€¢â€œ~Ã›?jÃÃ¥â‚¬Ãâ€ºÃœÃºÂ®ÃƒÃ—ÃŸÅ½Ã»Ã›Ã³Ë†kÃ°Ã«Ã¨Ã´\â€°Â¬@Â®Â¬Â°Ã½vÃ€ÃœÃ”cxÃ`j="" Â»Ã‚@Â´5fÂ¨bâ‚¬Å¾a="Ã•â‚¬Ã¾&nbsp;Â¾" Â§h3ËœÃ§â€ºâ‚¬Å Â½lhÃ“="" Ã€o`Ã¢â‚¬s!Â½cqÃ³ËœxÃ`Ã›Ëœ="Ã¯ï¿½Â²ÃŒP'Ë†" Â¡Ã•ngÃ¬kvpÂ±Å¸â€˜Ã·Å¸_Ã”Ã²â€¹ÃÂ ÃŒÂ²Ã…Ã–3Å½=""><s><u><i sÃs[Å½="" Â´â€š="" mpnÂ¬iÂ©Ã¸8Â¦Â¸_ÂªsÃ·w4Â´:Â®Æ’vaÃ¦="" Ã’.vâ€ºbÃ³:!Â®â€ ?jÆ’zÃ˜Ã…yrÃ¬Ã„Ã¡Ã¨Â§Ã£kÃ£Å¾uÃ‹ÃµfÃ‹Â¸Â yzÃ¯Ã¢nÃ‹="" Â«="" nÃ©Ã¬ÃŠÃï¿½Â¢â€¹Â¿Â¿Ã™="" q="" gwhÃšÃ¬Â£Ã¥zâ€¢Ã â€¹â€Â²â€œÂ¹Â­ÃÃ£^yË†="" tÂ­Å½Â»xhu="" Å 5Â¦cÂ·â€º=""><u><i hÃ¦Ã¨Ã™ÃŸ&Å Ãª="" Â¦mÂ¤â€™)â€™ï¿½#cmâ€º5kËœÃ¬Ã‘kldÂºÂ°)u+Â¡btâ€¢ï¿½="" ~ytÂ¨Ã–="Ãâ€°zUiÃ¨%ï¿½Â´ÂºTâ€°â€šÅ¾â€¦" Æ’nÃ´htâ€)wÂ«ÃŒhÃ…ÃÂ»wcÂ·Ã‘%Ã„Ã¥ï¿½Ã€="" Ã•@="" Ã¨ÃbÃ¨Ã¾ÃÃ›jÃ¨Ã¼q4txâ€°ï¿½Ë†Ã¯:%.Â¨Â¬{Ã‹b)Ã†lyjâ€˜Ã£Â wÃ‘:Â¥]uÂº{i9#nÃ¨ÃÂ«bÃ.Ã¨â€™Ãiwa'ÃªuÂ¥Â¡â€”4Ã’Ã¨rÂ¥ÃÅ“xÂ vÃ¯$â€ ^onÃ¼Â»Â·â€¹6'gÂ®â‚¬ÃµdjrwdÅ½lâ‚¬$Ë†Â°cÃ„hÃ‹Â¤ÂµÃ¡*Ã¼Å’Â§dâ€˜jâ€œcÂ¶jÃˆxÂ¥ï¿½Ã°Ã’â€œ|â‚¬â€1â€š$ÃŒ*f4="" |xÃ¡'Ã´#rbÅ½Â¶p#Ã¥Â¡â€¹deÃ+â€ _="" tpâ€â€¦nÃ…%Å’hÂµ*kkË†2wÂ½Ã˜Ë†:Ë†ÃÃÃ¥Ã§"#Â­â€n`Â­Ã¤uqsâ€“Ã¢u@Ã·2Ã¢â€^ÃË†zÅ¾Ã¶tyÂ§Ã¸=""><a Ã¤â€¹Å¸Â­9ï¿½1Ã¯.â€œ}(â€”xâ€™dÅ½|wÃ-Å’ÃŠÃƒâ€œ4Ã¿dÂ¦Â¼[b="" â€ Â©Ã¹o0â€¦Å“lyÂ®hÂ§.Ã£.ÃÃºâ€¹ï¿½!Ã–Â©â‚¬w\Ã€Ã¯ÃvÃ²zvÃ•cÂ¸â€¡,jÃ§Ã¾Â»?ÃœÃ´%]Â£uÃ˜ÃÃ²'v~tÃ˜ÂµËœmÂ¬ÃºÃfÂ¼Ã…="" ÃšÂ¡Â¨y8jÅ½Â§Â¸.=""><u Ã½â€¦ï¿½Ã®Ã˜Â¿?v9Ã¥$;bii"iÃšÃŸÃŸï¿½Ã¬dÂ¹Ã‚e$&&Å Â¤*Â¡Ã—(â€¢j;;;Ã¬bÃŠÃ¨Ã‘lÃšÂ´i$iy?tÃ—Â®]Ã˜yÃ²Ã†Ã™Ã™Ã™Ã‘Ã‘q$u="" Ã½Â¥Â­Â­mdÂ¦ad$v="" Ã¥mxx8$ÃÃœÃœ,â€ ÂªÃ±Ã±Ã±Ã°Ã¦'nÅ“Ã€Ãâ€™7Ã‘Ã‘Ã‘l6ÂºÂªÃºknn.Å’#gÅ½`â€”pÃÂ°Â©Ãœ"uï¿½Â }Ã¨Ã¤Ã¤â€ï¿½"Ã¬quï¿½Ë†1Ã¸Ã¸Ã¸Ëœâ„¢â„¢uuuaâ€”pÃtvvÅ¡Å¡Å¡Å Ã‘ujiia_ï¿½ÃªÃ»|ï¿½]Ã¾â€”Â¶Â²Â²Â¢Â®1Å¡Å¾Å¾Â¨!!!Å“â€¹"Ã˜zjjÅ Â·Â·Ã·ÃšÂµkÂ¿ÃºÃª+Ã¸aÃŸÂ¾}Ã¼Â·eÃ³Ã³Ã³Æ’Ã´Ãpâ€uâ€¢Â­Â¥\wwÃ‡9="" Ã¯â€¹="" Â¾xÃ—]w-[Â¶Å’Ã¿Ã•cccÃ…Ã«ï¿½ÃºÃ«;pÃÃ‘#!!Ã¡â„¢gÅ¾Â¹kÃ°kpÅ¾â€˜Ã¤Ã¤Ã¤@Ãºvu="" Ã„n|wÂ¬yÂ³â€ iÅ Ã¢3lfÃ¥Ã¾Ã½Ã»â€¦uâ€¢Ã_Â Ã“eÂ¢Â£Â£Æ’[!t*â€¢cyÃ´Ã‘g_Ã½ÃµÃ—^{Ã­Â§?Ã½Â©Ã¦â€”<Ã—Â£`Ã·Ã€Ã¡Å j:44â€Ã˜i:{Ã¶Ã¬ÂªuÂ«4bÂ¢Ã¸="" ï¿½="" Ã€-Â Âªâ€Ã¾2<<ÃŒ6Ã¢vÃ¼Ã â€ºnÃ®ÃœÂ¹pÃ¾|Ã°ÃÃ‘â€¹hÃw0â€Ã^â€šï¿½="ÃŠ-$Â¶â€¢H!â€ÂªÃÃ»cS5Â¹Â¥&nbsp;!**jÃâ€šÂ£â€ºË†X">ÃˆOâ€™Ã‘rÃš(ï¿½ÃŒÃ¼IYÃ¦Ã¦tÃ²Ã†tÃšÂ²:Â»Â·ï¿½0Ãˆï¿½Ã¾ÃªÃƒ-â€¢Â£|Ã’+eÂ©â€ºÂ¼9V9-â€šÃsm5â€”'Â«Ã¯Â­SÃ¯oVÂ¿?Ã‡8Ã—^6ÃÃ­Ã›+56Ã©/Ã“Ã¯Â¬06â€¡â„¢Ã§â€¢ÃµhkÃ†â€™â€šuÂ¾Â³Ã¦Ã†RÃ•ÃÂµÃŠÃ§Ë†Â«Æ’â€Â¤TÃ‚Ã£â€šÂª1&gt;Ã´ Usâ€°ÃªfÃ‚\gÃ¾Ã†TÃ²Ã…Ã±Â¤Â¹Â®4
ÃŸQÆ’^Ã¦HÂ©9brvg]Ã¢Lâ€¡Ã¿Lâ€¡ÃÂ¨Ã„â€¡[Å¾Ã”XÅ“â€â€šjm~,=;â€“UÃÃ•`Â­Ã¢Xv7ËœÃ“Â²}Ã‹Æ’Ã±Ã±ÃˆÃ½aÂ¬Â¯6Â©
Ã¯C\^Å¡Â¯ï¿½3kÅ“M}LÂ½OÅ¸
&gt;1rb&amp;Ã¸Ã„lÃ â€°Ã™`#Â¥Â¨ÃµÂ·`Ã•WÂ»&nbsp;Â¬ÂªÃ«ÂºÂµÂ¼Bï¿½QcÂ¥]Â¬
4YÃÂªÂ»kâ‚¬!Â«jÃÂ¼ÃªÂ½Â»w^Ã’Â¯ÃšÃÃ•!Ã¦ËœÂ¶ÃÂ¯ÃºEâ€™Ã©'â€°&amp;Ã½Ã¡Â¿ÃŒÂª0vÅ Ã¶.Ã€Â¼Ã€ÂªÃšÂ¼*5CÃÃ†0bkrÅ¡ÃŠÂ±Â½ÃQ#â€™hvQF+1Â½Â¹4Â«Â¿%uYâ€œry.mu8KÃ½Âºâ€¹vÃ•â€.U&nbsp;yÃ•ksÂ´9%eÅ“OÃ‘Å¡uÅ¸kÃŸÃÂ¨Â®Â©!Â¨p/
6Æ’Ë†Ã¨oÅ¸Å½Ã¾Ã¯Â¯Â¡ÃšÃŸÅ¾N|Ã¿ÃÃ¸Ã¿Ã¾rÃ¾~^Ã½Â¿[Ã¹Ã©Ã£â„¢G3Ã²GsÃ²â€¡Â³Â²{Ã“ÃÃ­Ã–Â¨Ã°Ã†Â¨Ã Æ’UÃ¹Å¸Ã{Ã¾gï¿½:Å¾^â€¢~rSÃ²Ã¹Ã‰gâ€ï¿½Â¯4_â€¡Å½LÃ¼Ãªï¿½Ã‘jpÃ°lAÂ«Â¥11AU_Ã–Ã—Ã½ÂºÂ»Ëœâ€¦Ã Ã€Å Ã‰â€¦Ã›Â¬:3Â¥Å’â€¢Ã’pÃ¬â€™Ã”&amp;pZK Â«Ã¦Ã†CVÃÂ¼*`UrZ(!)Å’â‚¬Æ’Â¬ÂºÃ­Ã—Ã£â€œâ€¦\Ãµ#Â©UÂ´k5Â°ÂªÃ¯Ã³QÃˆÃ.fâ‚¬UÂ¯DÃƒâ€˜7boDÃÅ½2Ëœ/OzÂ³yU"Â²~Â±Ã¸ÃµkÃµuâ‚¬ÂµÂ±â€œÃ®=Â¿Å¾UÃµÃ«ÃŸcÃ­Â©Ã¾rW^ÃµÃ¶Ã­Ã›333Ã»Ã¶Â«baÂ¿ÃªSÂ¤_ÃµÃƒD^ï¿½â€°â€“UuÂ£Ã¬_hÂ¸xï¿½UÆ’VEÃ²Âªâ€UÃ“Ã¢Â¹Po9	Â¼ÃŠÂ¸!Ads,Â§$ï¿½_â€˜ÃRâ€“Â¥Â¦\zÃ¯Å¡Ã³Â¿_|_cdUËœWÂ¥]Å¡Â¬â€ Â¬*&nbsp;"Â¥Â¿Å’EÂ­ÃÃ¶4hÃµÃ¶Ã²`Ã³Â½â€¦Â¶?Ã»ÃoÂ¦Ã¿Ã¾Ã—Â¹Â¿&gt;Ã¿Ã¾Ã±Ã¸ÃŸÂ¿YzÃ»ÃŸ?,Ã¿hÃ¼Ã‘Ã¬Å½Ã"&amp;7G7Ã‡]jÃ¿Ã¡Ã“ï¿½o&gt;Ã¬Ã¹Ã¢AÃ»â€œÃ‹Ã¢OoÂ½v_Ã°pâ€œÂ»Â®fM	h3Ã¢jÃ°Â¯?Â¼Ã´â€“Â¼6VÂ¡Ã¡â€¢Ã·6â€Ãµ6Ã Â»YEpÅ“Â£@L*ÃœfÃ•Qâ€Uâ€œÃ™%)Meâ€°Â¨ÃÃ’sÃ¢Â«â€¹]4Â«â€™Rï¿½Ãâ€ Â¢zâ€¹ÃÃ¡Â½Ãâ€°Ã²Ã’Ã
&amp;Â¿p7Ãµ6ÃœÃ&lt;ÃÃ™,ÃŸÃƒÃ¤Â¢Â·Ã—coÂ½ï¿½4ËœÂ¨.|Â³Â¬Âº_Ãƒâ€¦Â®Ã£ÃºÂ«.tÃµVÃ—q]Â·ÃeU+++Ã€ÂªgÃÅ“Ã‘Â³Âª~Ã½AÃ–/Â²ÃªÂ¥Ã½YuÂ¼,y=Ã¤Ã”â€¦Ã°SÂ«Ã¡Â°Ar&gt;Ã´TÂ´Â½ï¿½ï¿½:GÃ¬9Â®â€šxÃ¸yC\&nbsp;SJÂ°KnÂ´sYâ€™Svâ€gvâ€oIÂ¼g
ÃÃâ€™1ÃŒkÃ²&lt;Ã‹â€œ"HÂ©aÂ¤Ã”Â¨Â¢Gâ€¹=Â³ Â¦&amp;/Å½UÃ—PÅ’m(Nj&amp;$Ã¶Ã³â€šÃ§{Ã¼z4Ã‚()5CLÃiÂ£Ã¦Ã²Ã’ÃÂ«cWGÂ±Ã§Ã•I=ÃµÂ¨VÃƒ)CMâ€Â¡&amp;Ã¢0ï¿½Â¸Ãï¿½_Å¸(iÂ¦N&nbsp;h_Ã”Â®/â€¢Ãœ8ï¿½ÃŸÅ“ Å¾â€¢Bâ‚¬ï¿½Ã•Â®â€œoÅ“/Â¿Â¹RÃ¶Ã¾Ã¹Ã²ÃµÃ’Â¶Ã¥ï¿½Å’&gt;#Â¦ÃÃ‰Â©â€”Â¦KÂ®/Ã¥_Å¡)XÃ¬.mÂ¡Å’Â¶â€™Fxâ€¢Å¡fÂ¢Å¡â€¡Å½Ãµ7.â€œdÅ¸WÃ‡-kÂ¢g:Ã£Ã¤Ã•â€¦rZÃ¸Ã™$â€,!)KPâ„¢ÃQ3Â©Ã°â€”Â¹ÃµrÂ½Ãªâ€¹puâ€¦ÃC&nbsp;6â€“Ã·P3ÃÃ“Q2-ï¿½Ã±ÃŸfâ€,||Pq\Ã€&nbsp;ÂªÃ“â€”ï¿½â€šÃªÃ³dï¿½Â«yËœÂ³iï¿½â€œâ€°ï¿½Â½1Ã‹Ã«Ã”DÃ Ã±Â©Ã€Ã£Ã£`Ã»4TÅ [eoËœUÃ·Ã—Â¸Ã—Â­Ã¥â€šÃ¼RÂ· ÃˆÃ¨pUWWWÃ›Ã˜Ã˜Ã¨Yu7Â«&gt;Ã»Ã²Ã™Â£â€¡Ë†Â·dÃ•â„¢Ã™Ã™{wÃ®,-.Â¾â€UÃ›w|â‚¬Â¡&gt;Ã€XÃ£Å¾Â£YÃ•ÃºÃ¥yÃ•hmMZ@YR`cÂ©Ã§Ã¬WuÃ¨Ã¡@VÂ¥VÃÃ‡Ã”Ã€Ã˜)vH:*ï¿½hÂ£'u5&amp;Â¶Ã—%kâ€Â¸Ã³CÂ¸ÃiAeLÂ´Ã¥*iÂ«Ã–â€“Å¾Ã«+Â»ÂµZÃ±pÂ«ÃªÃªYÃŠYeÃ•Ã»Å¾{Â°TkÂ»Â©Â¼ï¿½=ÃÅ¡ÃŸkÃ½Ã¦ÃƒÃ¾&gt;SÃ¿Ã´lÃ´Ã©ZÃ‡Â£Ã¹WwÃ¿Ã£â€ºÃ‰Å¸Å¾MÃ¼Ã¸Ã¹Ã¸Ã‡ÂºLAÂ´Ã»Sm0vÃ<ziÃ»Ã¶iÃ—Â³gÃŠ Ã‰Å¸="lÃ»Ã¢Â¾Ã¸Ã£Ã›Ã¼â€¡[ÃœKÃµÃ³2Ã†Å’â€">Ã‘JÂ»Â¨Â¦Ãœ]Â¯ÂºÂ·AÃœÅ“$Å’â€¹Ã‹{Â«Â²ÂµÂ¬Å¡/Â©*kKEÃºUÂ£&amp;Ã¤	Ã‡Ã†Â§5â€¢Â¡Â¬ÃÃˆMï¿½Ã—Ã¸#Ã½ÂªÂ²7ÃˆÂªÂ¸Ã§Â¬ÃºBYÃŠÂªÃÃ/eÃ•gÃ“ï¿½Â¨3Â¨PÃÂ«gfJÃ	â€¢Â½QVÃ5X
Â­Ã˜Ã›Â¬ÂºÂ·yjÃ¯Â¤Â¿Ã½KiÃÂ©Ã¶Å¾ÃŸÃÃNÃÂªÃºÃµÂ»^Â»YÃµÃ‹/&gt;GÃ»UÃ»ÃºÃ€Ã™\X\Â¼}Ã³Ã¦Ã™Â³g_ZÃœÃŸÃŒÃ¼4Å¡Â½Ã½$ÃÃ¤AÅ“Qsâ‚¬Ã±Ã®Â¼ÂªÂ®Ãâ€ Â¹iÃµVâ€ºW-Ã‡Ã²*ÃÃ•Â­Â°_UÃ‰Â¬
Â¨-â€ UM&lt;Ã„Ã•1ÃƒÃ¢PÂµ JÃ†HÃªlHÃªÂ¨OÃ£VGqâ€º3)F24â€šÃ¼Å“MÃ¢Å¸UuÃ¹Â½uÃ¢ÃƒKU[â€œâ€iiË†|Ã6UToÂ«Â¢zÂ»}78Ã|gNÃ´Ã­Ã“Ã¾&gt;Ã—Ã¼Ã´Ã•Ã˜Â£%Ã¹â€¹Å Ã¯k~Ã¾jÃ²Ã§gâ€œÃ½pÃ´Ã©rÃ§Æ’Â©6Ã„pÃ‘Ã›QÃ¡Ã1Ã¡â€¡â€ºÃ²o&gt;|Â®Â·Å¸ÃŸ}|â€¹Â½i]SÃ‚Â§i}ZRÂ½1
'Â¡Ã…Ã“hÃ¹6Â«6Ã®ÃŒÃ²c@Â½ï¿½Ã©Ã€Â­Â¨c/G!cwâ€™9Ã¸TÃ6Â«Ã†Ã“sÂ»Â½4"'ÂµÃAPÃ¥	YÃ·"Â«b^dUDoÃ·Â²jÂ®Â»Ã‰fÃ”â„¢â€ºÃ‘â€”Â¢Ã\â€°<s-Ã¼ÃŒ(ÂµpÃ¾Â°Ãª?[oÂ¶Â·yÃµÃµfÂ¨Ã©:Ã[â€umllï¿½Â«â€šÃ«yuÂ¿Ã¾Ã«_fuÂ¹Â¬mÂ¤4y9Ã¨Ã¤r(ÃœkÃ¡Â§Â§â€šodÃ˜Å¾Ã¶Â´>Ã£og&nbsp;â€¹Â«ÃšÃ´
Ãï¿½â€¡Â¤@'BÂ²Â·Ãœ23Ã‚5-Ã„Â»0ÃMÅ Â°ÂªË†jAÃ‰Ã´(â€°â€¡Ã¾Â·Ã‰"Âªï¿½â€šiSï¿½Sï¿½â€¦aÃ¤Ã†â‚¬Ã€Ëœâ„¢ÃŸPÂ¯Â¨
Å¾VÂºOÂ·{â€¹Ã½EUÃ‰Â­Ã„AeV;+eÂº#ri0tÂ±?Â²â€”ï¿½Ã™Ã½RÅ ÂºYÂ°Ã¸PU_6Ã€)ï¿½VÃ¤Å“ÃˆÃ¨k Â©!]V.ÃµlNglÃdÂ®Å½Â¡#ÃƒÃ¸â€â„¢6Ã¢Ã†DÃ®Ã–lÃ–Â¥Â³YGÃ²gdÃ„1u\Ëœâ€2!"^Ã‰ÃœËœJÂ¾0â€™:-/`W6â€¢Ã®gÃƒKBÃ°Â¯Ã­Ã­Â¨)Ã©Ã§fÃtD.Ã¶M)CdÂ´LQUÅ½Ã±fÃ§â€¢gÃ°â€°Â¸Ã¶ÂºÃ Q)P]')ÃÂ§6/Â©6^â€“Â¢ÃÃ¬Ã¤Â´HzN&nbsp;Å“aÃšUoÃ–YgFÃÃ°.Å Ã³/Ã€Ã¸Ã¦Ã‡xÃ¯â‚¬ÃªvFÂ§Ã“Â¦ÂºSUh@5Ã„Ã‰Â±g7bxÅ“Ã¶?:Ã¢lÃˆÃ¯Â¨Ã†Ã§Â¨Ã†ÃŸ@Ã±Ã¦XUÃ·Ã²pÂ¯{Ã€Ãqï¿½{Ã WÂ»Âµhâ€ºUuYÃÃ—Å“<yrÃÂª;yÃ•gh^uzxx,vÂ½{Ã§ÃÃ¢kyÂµÂ£Â«cÃ„Â¿â€œdÃ¿ï¿½â„¢Ã·Ã·0ÃÃ¨^Å“qgÂ°â€˜â€”ÃƒÂ¶Â·ÃštÃÃ™asyâ‚¬#xÂ¯gÂ½>ÃÂ¦rtrQ\`ib`ÃsÃºÃ›uÂ³ï¿½Â¨â„¢Ã°TÃ–Ã¤a9Ã±rÃ´â€&lt;`EÂ°:Â¼&gt;Â¶1Â¹&gt;sa{q,im<eÂº=Â«ï¿½qÃ” n?Ã—ï¿½Ã¿\Ã©Ã5Ã‚Ã¥Ã™ÃŠÂ³jÃ²(ï¿½2-ï¿½Ã½s="" Âªyâ‚¬â€“jÂ¬Ã³;ÃÂªÃ«Â½Å“ï¿½Â¾Â¦Â«#Ã_Ãœw|Ã½aÃ‡Ã—ï¿½;Ã¯ÃiÃ|Â¸Â®Ã¸Ã¶cÃ•wÃ·~Ã·tÃµÃ©â€¢Ã¶â€ºÃƒÃ‚â€ºË†Ã»Ã™ï¿½1ÃšÂ¸zwiÃ°Ã…cÃ©gÃ·doÂ®Ã°="" ï¿½Â²7Ã”ÃµÃ»Ã«â€“â€Âµg%Å’Â³r:Ã•Ã†[Â«â€”hÂ·v="" Â·="" â€“Â®oÃ Ã‡deÂªÂºr;uÃ¯ÃŒaâ€“tÅ’iqâ€¹1Ã§â€¡"f%1b2â€™w-ÃÃ‰Â«Ã¦$ÃˆÂ¾="" â‚¬tÃ¬Ã›Â®Ã¤Ã”]v-|}vu1]ï¿½8u1@[â€¹8Â½vjÅ¸Ã°Ã›Â±Ãª~Â±â€œâ€“uuÃÂº_Ã<Â¥kÃ´Â±â€”uaÃ¬Â¤guÃ½ÃºÂ½Â¯â€”Ã¤u!Â«^Ã®cÂ¼â€¢oÂ½â€uâ€˜Ã¹ÂªÃ½\Ã¦â€œhÂ£Ã»qfwÃ¢Å’="">@Ã¦Â«rÃ¼Å’|_nÃ¢â€˜Ã¼Ã¢tÂ°Ã˜Â¯ÃšBtÃ¤A`EÂ­)5Â°*bï¿½â€ºÃÃ†Ë†ËœÃ©zxa4d}"Ã‘[ÃŒâ€¦Ã‘Ã¸ÂµÃ±Â¤â€¹c)Â£â€™Å“Å½Å¡Â¢.VIO=~Â©Ã»BÃ©Â½ÃµÅ ÃÃ‰ÃŠ))yÂ¼Ã¨-4@ÃµÂµTÃ’Ã•Ã›3Ã¼/(Â¾Ã¹Â°Ã³Ã«Ã‡]Â·&amp;DÃ¯?Â¹ÃšÃ¾Ã­SÂ¨Â·ÃŸ&lt;Ã®Ã¹hSqC#â€Ã€Ë†Ã^Ã“Â´^Ã¦?\Â½Ã½Ã´Å½Ã¨Ã‘&amp;ocÂ°qS]Â¡â€”Âµ ÃŸÃ–[&nbsp;Ã­SbÃª
4Å“Â¼}aÂ¯Ã¦â€¢Æ’xÃÂ«vÃ€â€“UÂ¨Â·Ã“Ã­	Ã§Â£â€”â€¡Ã‚Ã¹Â±Â­â€¢Ã‰Å“XÂ¼ï¿½WÃNÃ¨Â¬Ã·TC`Â»Vâ€™Ã‡/Â³ÃªN^5Ã¶EVÃq7Â¾Ë†Ã¨Ã­Ã…ÃˆÃ“Ã«Ã¡Â§7BO
S~#VÃ•ï¿½Â¶Â·YuoÂ½â„¢nÃ‹KÃµVÂ·ÃLÃÂªÃºÃµâ€¡]Ã»Â°Ãªâ€šB"Ã¾EVUâ€”Ã Ã¦Ã½ï¿½ÃÅ¸{9Ã¬Ã”hÃ€Ã±PÃ«Ã®â€“Â§|lN\
Â´7v4Ã5#ÃÃ‡Ã«Ã«XÅ½Â³Ã¥-3ÃƒÃ½Â½
cï¿½etKÃ›Â¢â€¦hMHÃ¶(Ã„@!Â¸*cËœÃ±Ë†6Â¤Ã”Â¨ÂªÂ´ÃˆÃª,â‚¬rÃ‘Ã´Å“Ã˜ÃšÃ¼8&gt;)dDÃ¬8,vÃ’Ë†Ãœâ€Ã¤Ã˜Ã¦Â²â€Ã¦Â²t!9Â¥Â¿9bÂ®Ã‡Ã·lï¿½Ã¿PkÅ’â€Å¡'Â«ÃSÃ’Ã³@Ã°Â©dÃ€jâ€œ1qÃ²Ã™ÃÃ¸Â®Bo}Â©ÂªÂ¾|Fâ„¢Â¾2Å’]ÃÃuÂ¥
pÆ’MC\ ÃÃ¹Ã°Ã¹â€Ã•QÃ¬Â²:qZÅ¾&gt;3Ã„-Ã«k$Âªâ€ºKÃ»Ã¢â€”5Qâ€¹}ËœQf7Â³Å½vâ€¦u,Eï¿½ÂµÂ°tÂº'U*k2â€ Z#f:=Ã‡eÃÃ²,ï¿½ï¿½Ã…Â«Hm!$Â³KÃ’xDLgÂ½Ã»PÂ«ï¿½Å cÃŸXFÃÅ½Â§eÆ’â‚¬<wÂ¥â€¡sÃ‚Â©Ã™Å Å¡mv%Â¥yÃ¤Ã‡Ã€ï¿½ÂªÂºÂ½o {ï¿½â€Ã¾â€ "Â â€Å½Â½Â¶1Â ÂºÃ¯Ã·92Ã¨sÂ´Ã—Ã»hÅ¸Ã§â€˜="">ÃŸÃ“
Ã‘fÃ•â€”Å½kÃœÃ=Ã 5`Â­ Â£Â¬ÂªÃ•d=Â«Ã‚ÃµKÂ¬zg_VÃ­VÅ Ã¸Ã¯'Ã˜ÃÅ 6Â¸â€ºÃŠoÃ†*
Ã‘
Ã â€”gÂ»JÃŸÃ›:ÃÃ±Ã«FZÂ¨P{%Ã”Â¯â€Oâ€¦Â±Ã¾Ã¸â€â‚¬ÂºbÃ·Ã¾fÃ«ï¿½Ã›Ãz{Ã”â€OÃ´Å“81-|TÃªÂ£Ã²C5Ã¢Ã°IÃ”XfÂ¾Â³Â¢â€°Â½8?Ã›â€¢ÃšQâ€œpÂµï¿½^2)+Â¼:WpsÂ¥hkÂºtFQÂ¡Ã¡â€™'â€U;Â£`Ã—ÃªÂ¼Â¼vâ€#	Ã–Ã•Ã®Ã†ï¿½AÃ¶G7xÅ¸Ã|zWxsâ€ Â·Ã•Ã—|sÅ¡Ã¿Ã‘5Ã‘ï¿½$Å¸Ãz[â€šÂ¥+CÂ¼+C-â€”â€¡ZÂ¶Ãºâ€º7Â¸Ã¯Ãr?ÂºÃ…}zÆ’sÃ¯4@[ï¿½1Ã§eÂµsmHÃ $Â¥MÅ Â©c&lt;ÃŠ|gÃ…ÂµÂ¥â€™Ã·Ã\Ãâ€“tÂ³J{ÃªÂ·â€œÂª
zÅ¾â€Å“7*â€°Å¸Ã¯â€¹XÃ¬V"Ã¹DÃ€ÂªË†Y7Ã¬Å¸Å Ã‡VJÃ·Ã Ã™Ã°lÂ¤Ã•ÃÂ¤Ã”Â¤_AÃ–Ã­WÃÃ?Â¥Ã›Â¯ÃªjÃ¬Yu1Ã´$Ë†Å¡â‚¬l.â€¡Å¾\
99VÃ²Ã¦YuÂ¿Â©Ã´Â¿Â¾YU;Â¡Â¬Âª[â€œÂ¦gUÃ½ÃºÂ½Â¯ï¿½UÂ«ÂªÂ¸ÃŒ{7c&nbsp;ÃÃâ€¹5Â¼mÃÃ kÃ¨Ã£`Â¢Ã›Ã‹ÃŒÂ½M~Ã‰|&nbsp;Â·Â¥â€°Ã¾\â€šÃÃ›Ã¾f[ÃƒÂ¥2%â€“â€“Ã…ÃˆÃ…ÃsÂ°Ãˆï¿½Â·Â¯Z4Ã˜:,Å½â€˜Dï¿½ÂµÃ….
`Vâ€ Ã£.Å½%Å’Â·eÃ€hÃÃ•Ã*nÅ“z[Â¼1Q:!!Â·ï¿½'â€¦U:Â¦Ã«Ã›z{IÂ°Â½Â½2Ã†Ã¹Ã¨&amp;Ã¿Â³Ã»PoÂ¯ï¿½Â¶lÃ¶6ÃŸâ„¢oÃ½Ã¸Â¦Ã¸â€¹â€¡PoÅ¸^]QCÂ±EÃµvÃ¨Ã­ Ã·Ã¶ÃÃ›Â¦'Ã—Ã™Â·ÃÃ•ÃKËœÂ²ÃšyY
Ã[hÃ¥!Â­Å¾R'ZÂ«â€“Tâ€+Ã…Ã¯Å¸Ã‹_Ã•Ã¤5Ã£Â»Ã«Ã°Â¨Â·â„¢Ã¤4Â¨Â·SJÃŒBÃ¸b_poSLÃ‘Ã›â€™mÂ½Â¥eÃ‡Â·Ã—Â¹
Ã²Ã­ZlÃ¸â€¢Ã®â€¢)!HÂ¿ÂªvÃ¨Ã¼s+`8Â¹fGoÂµ7Ã¹HÂ¿Âªyï¿½Â³YÂ¶Â»Ã±RÃ”Ã›Ã³;zÂ«Â¦Â¼yV}Eoâ€nÃ°Ã«4Â«Ã®Â½ÃƒÃ—:oooÂ¯gUÃ½ÃºÂ£Â­ï¿½UÃ¥Â«'MÃ»â€º&lt;6pÃ¼lÃ°â€°AÂ¿Â£AÃ‡ï¿½ÃOxZÅ¾Ã´Â±&gt;Ã¥g{&amp;Ã€Â­ÃÅ¾â€š
B5  1Å¾Â¶Ã¸Ã‘"%Ãˆ&gt;ÃŒÃ™Â»$ÃZYkÂ¡bâ€ºÂ³Å Ã¬
0Å¾Â¹Q@â€šÃ¼Ã‹â€™Ã¼;Ã«,Ã¹ÃÂ¥	aÃ‰Â¡Ã¤Â´Â°ÂªÃ´JF5Ãƒ*Å’Ã¨mÂ²Ã®oï¿½)Å¾VrxCqRc1Å½Æ’Ã‡â€°Â¨Ëœâ€°Ã«Â¸Ãœu\Ã¦!Â¥%Â·Ã³EÂ¤<iuÅ¾Ë†\Â¤`dï¿½hbg$â€˜2*^Ã‰(pÃjâ€ â€¦â€°s=!Ã³Â½!Ã£mÃ±ï¿½ÂµÂ¥ÃÂ´Ã¨'aÃo?Ã›Â±Ã˜Â²Ã:Â¥Ë†Ã«Â©(zÃsw4Â¥,0Ã•Ã’Ã—â€â€š|Å¸<Ã™ÃŠhÂ¹mÃˆdfiuâ€“Ëœâ€%$e*j1â‚¬Â£â€¡eÅ½] Ã¬Ã¢Â¬Ã†â€™Â´â€ Ã¢va*Å¸ÃœÃhÃ•ÃƒÂ¶ï¿½tÃ›uÂ¥Ã‡k)â€¢â€jlÃ…'â€sï¿½Å¾wÃ—â€ºuÃ•â„¢â€¢&zdfÃ¸dflÅ Ã®2sÃ’Ãµ="" u6a^ÃºÃ›Ã¸ÃšÅ“vÂ³:er="">Ã–Ã­uÂ¤Ã‡Ã«Hâ€¡Ã§Ã¡NÃ·CÃÃÂ§eÃ‚VEG'â‚¬â€¢
â€¦JÂ¥Ãºâ€¢Â¬ÂºÅ¸{ÃŠÂªÂ¿8ÃªMÂªÂ¾Ã”Ã©N+Ãˆ(Â«M1Â°6_sÃ¢Ã„â€°Ã¦Ã¦f=Â«Ã®fUÂ¡pÃ¦Ã¬Ã™Â»wÃ¯Â¾â€šU/amÂ¯FÅ¾Â¹Å’4â€¢_â€°2ï¿½Ã¸Å¸Ã±Â°â€¦Â³Ã©uÃ‚Â§=ckÂ®Fx&nbsp;KÃ¹Â¿bÂ¬Â³ÃUÃ…Â¶Ã¨mÂ²T0mIÂ©â€˜â€Å’HdlMt[M`_Â³Â§Ã›XÅ“Ã„-Ãƒ5RZÃŠÃ’â€ Ã¸Â±Â½â€˜+Å¡Ã¨Ã™Â®â€ÃÂºÃ¬6jÂ¡â€šVÂ¤Ã¦Ã¥Â¯Od^YÃˆÃËœÃŒâ€ºVÃ Ã˜Ã„^%Å“Gï¿½z&nbsp;â€°ÂªÃ§dÂ´â€¢^ÃšrcAï¿½Â»W{X7Yï¿½Â¯Ã—?Â¾ÃšpyÂ´qÂµÂ«qÂ½Å¸}kï¿½Ã½Ã¡ï¿½Ã†'Ã—?|Â¿Ã±Ãº4{Â­â€¡Â³Â¦bÂ¯Ãµ6^Ã¨n\Ã­Â®Â¿4Ãztâ€¦Ã±Ã°Ã­Ã¦yÃšÂ¢Â¢v[Â¯Ã·EÃ”	!eÂ¬â€¢Â¬i&amp;MJ[Ã“9â€”Ã§2Vâ€¡3â€ 5%];&amp;Ã€Â²Ãª1)wDsÂ¶;hÂ¶Ã‹Â¿Â·)&lt;#v15VbÃ¦Ã‡Ã’Â²Â°bÂªWoâ€œMoâ€œâ€¢ËœÃ¢Xâ„¢\Å¡RÅ¡&nbsp;[Â¬[Â®Ã¿Â¢/%;Â¹Ëœ;â„¢F9â€ºÃŒâ€ Å“QÃ“&lt;Ã˜Ã'OÃ…Â¿YV}ï¿½i5{â€ºUÃ‘Â£ÃºRÂ³Ã®â€”â€“Ã’Å¾SmÃ¬Â¤gUÃ½ÃºÂ½Â¯_`Ã•Â¥Â¥[Â·nÃ­ÃƒÂªï¿½=MÃŒÃ·ÃƒÃ&nbsp;z{#Ã†p+Ã²Ã“Ã›Ã€Ã‹ÃX'
BÃµVgÃ´|Ã¨Ã¶Ã˜&nbsp;Â·Ã¹&nbsp;Â·Ã¾Ã¸x_Nâ„¢S/Ã‡BÃ…Â±â€™T;UÃ bÂ¨â„¢PoÃ³Â¢Â»Ã½:Ã«Æ’8Ã¸xÃ¶sÂ½Mâ€œÃ†,Ã¶GÂ®GOÃˆâ€œ:Ëœ92*Â´Â¢aP9&nbsp;Â·kÃ£Ã¹cÃ¢Â²!Ã”Ã›Â±Ã§zKâ€ºSÃ/ÃÃu"e-JÃ¦ÃºÃ«Ã¡%DoÂ¯4lÂ²ï¿½Ãâ€šï¿½wWÃ˜@iÅ¸\oÃ¼Ã JÃ£Ã•	Ã¶â€¦NÃZ/{MÃ„Â¶qÂµÂ§Ã¾ÃšÃ³Ã‘eÃºÃ½MhApV@RÃªÂ´Â¸Ã•[&nbsp;Ã­Ãƒ-â€¢3Å Â²+Yâ€”ÃÂ¦Â¯Â¨3Â¸ &nbsp;*Ã‘Å¡Â®Â·QÂ¡ÃN*"Ã¦ÂºÂ¡Ã¡dG}tS)Â¼ï¿½Ã€aË†ï¿½GufÂ¼Â¢Ã–Â¥ï¿½kÃÃ‹Â±lÂ©p%&amp;Â±
FÃµÂ¶pÂ§=jÃ›X{â„¢Ã¯gÂ¯ï¿½â€˜Ã€Â«
Ã´6ÃÃ‰4Ã“Ãh.Ã¤Ã„JÃ¨IÃ°Ã•Ã›rÃ¾dÃ•7Ã’Â¬ÂºkZÃKÃ«ÃÂ´Â¬jiiill|ÃºÃ´i=Â«ÃªÃ—dÃ½Ã«Â¬ÃšÃ–Ã–[â€4ÃªuTÃ£wTÃ£Â«ÃŠÃ·Ë†Â¿Ã™GÃ£Â£nfÃ‡&lt;,NxYï¿½Ã”&amp;XÆ’ï¿½UÂ¶Å tÂ³Ë†pÂµ*Ã‚Zâ€°Â©Â¦Ã¥Ã‰foÂ§FÂ¼iÃ‡RQcFHvLÃµÃŒÅ’Ã°ÃŠÅ½Ã²"Â§;wÃ•[P3ï¿½
cÃ±Ã±ÃÃ¥IAÃ‰!Ã„â€0bJ$=;PQcÃÃ2Ã¯nÂ´hÂ­tcÃ¦GÃ—Ã¦%Â°
â‚¬Â¾IÃ©~ÃƒB[ï¿½Ã^Ã…qS1Â¼Å 4&gt;1Â­ï¿½Ã—ÃŸÃ¬?!wÃ«kÃ…Ã¤&lt;)5Â«Â¿%|\Ã¦5!Ã·RÂ·â€ â€™mÂ£ÃˆÂªÃ³eÃ•ÃŠÅ¡ï¿½ lRÃ¡
Ã¶ËœÃ”_Ã…Ã†J)%â€™Âªâ€™vfÃŠÂ°Ãˆ\Ã¦&gt;,Ã¶ÃªnË†WAï¿½B8ÃTÂ¦Â·Ã“Ã¸0Å¡Ã‚-Kk!$(jÂ¼yÃ–Ã ]Â¦ÂµÃ’â€¹UÂ¢Â±KÃ½Â¤4Ã›Ã®sEï¿½MmÂ¾_e
â€ â€
Å¾K(\â€“TTÅ’
 gÂ¸Ã‹Ã¨Â¦LÃ“vÂ¦)1Ã•&gt;#Ãœ-9Ã˜=%Ã˜eÃ‡Å¾(Ã°Ãm!Ã¢&amp;
@#Ã„Ã™Â¼Â°â‚¬RmÃÃ¸XÅ¸Ã²Â°&lt;Ã©d~Â¢ÃœÃ±Ë†Ã‚Ã£ï¿½Ã’Ã£ï¿½ÃŒÃ½ï¿½Ã‚Ã­&nbsp;ÃœÃ«Â¤BÃ”ÂªÃ¬Ã¬zÆ’Â¬ÃºÃÅ½k|ï¿½XÂ·YÃ•Ã‹Ã‹KÃÂª/Â¬_bÃ•{Ã»Â³jÂ»ËœÂ¿kÂ³~z=Ã²Ã´Â¥Ãˆ3â€˜Â§â€¦Â¾Â§ÃmÅ’Â¶ï¿½Ã¤&gt;Â¸Å¡Â³Â«nYâ€˜nÂ¹QÂ¹Q&gt; 0Â¨Ã‰wÂ¿Ã€Ã ÃIiâ€“Ã‰Ã¤Â´pRj#/LVÃ£Ã•QÃ¯~Ã‰â„¢Ã¹	ÃµEâ€°l|RCaZg}ÃŒtGÃˆÃ’@ÃˆLgTwcÅ ËœW^'+Ã·Ãœ n}wq<urâ€“ÃŸ[_>Ãˆ)Ws+4-0Ë†iÂ©Å¡â€˜â€˜.j*ÃÃ·â€™Â§â€tÃ³,(iÂ·â€“+Ã¯Â®â€œÃ®^$_Â¨YTÂ²â€“â€Â¬ÂµÂ¡ÃšÃ›Â«Ã”Ã»â€ºâ€Ã»â€Ã·Â«Â¡Ãµâ„¢â€™uÂ®ï¿½Â¹Â¨Â»Ã¦Ã¢ÃµÃ¶Ã‚Â­Ã•Â²Ã‹Â³Ã¥Â³R2M
)Ã£Â­U jiï¿½ÃÃ¥Æ’lÃ‚Â·lEâ€œÂº6â€˜p^ï¿½Â¨nÃVÃ’â€¹ÃÃ©	mÃ•Ã™brÂ¶ï¿½â€9,ï¿½Ã©Ã°Ãº&nbsp;dâ€ #â€“n	Â¨Ã‚Å }JFÅ“â‚¬Ã¬ÃÃ•
Ã$Ã»
2_5~gÃÃŸÃÃ€Å¡tÃ­Â¼Â¿Mâ‚¬Â³nÃ“@'â€œH'Ã£â€°&nbsp;Ã£ jÅ¡;Ã¨Ã¸Å’Ã¿Ã±Â¡Bâ€UÃ›ÃŸÂ«Â¾~Â³ÂªÃ®QÃ•Â­ÃmVÃ•ï¿½ï¿½tÃÂºuc'â€UAÃ¬Â¤gUÃ½ÃºÂ½Â¯WÂ±ÂªLXÃµÃÂ­[Â³/eÃ•Ã¶Ã.Ã³rÃˆÃ©ï¿½Ë†Ã“kâ€˜Â§Â¯Dï¿½Â¹qÅ¡Ã¡yÃ†Ã“Ã¨Â­Ã±Nd0
Ã…Ã•Ã»ï¿½)Â«Â®@oÂ³Â¡ÃzÃ¦FÃ¹Ã‡Ã¹Â°Ã±Å½@d@Ã”JÂ²/KË†Â¦dâ€â€˜RÂ£â„¢â€¦!ï¿½
Ã®Rz Ã[Ã³@Â½Ã…Ã•Â¤Ã·5GÃvÅ¸â„¢ï¿½c:Ã«Ã’$dËœÂ²Ã¬iÃˆÂ¹0Å¡â€”Ã†RGEEÂ½
Ã¥Æ’MÃ¥ÃªfToI#ÃUgâ€¢â€¢Ã«câ€â€¦ÃŠÂ´Ë†&gt;+Â¡/Â«ÂªoÂ­@Â½Â½sÂ±jÂ¹Â»ÃªÂ­Â¢nkÂ¬Ã¦ÃÂ¨Â·wÃ—Â¨Ã—ÃV/ÃŠÃ«Ã Ã€Â¾vÃ¦â€šÅ“Â¹Â¨Â¬Ã™Â©Âº}Â¡Ã¼Ã¦rÃ™Ã–$aÂ²â€¢&gt;%FÂ¯Â«FÃÃ·oÂ©Tsâ€°Æ’Ã‚p+~mÂ·6n0Â±Â¯)OI/TÃ’QÃƒÃ‰lÃ”Ã›Å’	YÃÃ›1Â©â€”â€ÃPâ€C3ÂªTï¿½ÃVÂ¥Ã‡IiÅ½Ãï¿½â€“]
Ã¦MÃ¥.Ã¨â€^Ã¬Â¦Ã“eï¿½&amp;UÃ‘ÂªÂ³Gâ€œ4WÃƒÃ‰mÂ½=Ã´vÃ–Ã¯xÃ©ÃÂ³ÃªÃ«4Â«ÃªÃ¶FÃ½Â³ÃƒÂ´Æ’Ã¼Ã´Â¬Âª_Ã€ÃµkXÂµÂ»0qÃ€Ã³HÂ¿Ãâ€˜&gt;ÃŸ#ÃƒÃ¾G;&lt;{Â²58Ã¤l|Ã˜Ã•Ã´Â¨Â»Ã¹1Oâ€¹ÃVÂ§|Qbu@Kâ€šMBÅ“ÃÂ±Â½lâ€¹Å¾F+Ã•â‚¬j_â€œ-Ã—2-Ã”1)Ã5=ÃŒÂ¥"Ã…Â©â€œe3Ã˜bÃ™Ã“`Ã‹%Ã˜â€“&amp;Ã‚*Ã„l@yRÂ«ÃÂ½â€œeÃ—Ã›Â¾Ã–BÂ¿Æ’5ï¿½Ã¨Ã‰Ãˆ
Â§eÃ‡2rbYâ€¦QbÂªKoâ€œÃ¥Ã—Âºâ€¡Ã£Ã˜UÃ¯Ã’ÃÃ¨Ã’Ã‡uÃ¤Ã™Ãµ7;Ji!Å“Ã¢<iuxÃ—k-pÃ’Ë†Ã¬â€¡Ãâ€šÂ¿ÃµÃ¨Â®h%fhÂ­Ã„ÃœvÂ¨zÃ <"â€ â€“Â§jï¿½[g]â‚¬Â²&xï¿½Ã¯Â®8 Â³Â¹Ã­ynâ‚¬â€¦Ã¥t@Â¦Ãœ2\sÅ½sÅ¡Ã„f:,Ãªâ€¹axÃ¾)Â©Â¶Ã«io4Ã–="" â€ Â½Å“Ã®Ã˜ÃÂ²Ã­iÂ°ï¿½Ã‘Â­Ã«Å Ã="" Â¸pâ„¢sâ€˜kyÃ€sÂ£fÂ¹iÂ¨Â¶ÃÃµÂ¶}lâ€¹^Å½eâ€œewï¿½u[Âµ="" !ÃÂ»bÃ½tÂ»sâ€˜Â«bwsÃ°2â€š3ÃÃÃÃÃ¶Å’Â¯ÃµiÃ°"Æ’â€”ÃšÃ•Ã¬ËœÂ½Ã‰Â±bÂ»cbÂ·Æ’rÂ·Æ’bÃ—bÃ§bÃ·="" Â±Ã°ï¿½Â°ÃªÂ«yÂ·yÃµÃµÂ§Ã•Ã¬="" â‚¬qvÅ¡Â¬ï¿½ï¿½ÃµÂ¬Ãºâ€¹Â¬zÃ¿yu19Ã†z5Ã¬Ã”rÃ˜Ã¶Ã”ÃÃ¯sÂ®vvâ€ Â¯Ã‚uÃ„ï¿½Å¡eâ€¹opÃÅ Ã°ÃŠï¿½Ã¶Â¦Ã§:u7Ëœï¿½Ëœï¿½oÂ²(m'Â¦â€â€™Â¢jd5nâ€™j="" jf|m="">Ãµ@cÃ¥'KhÃ‘#bÃ¿Â¹nÃŸ)eï¿½Å Ã‹'Ã¤J)Yâ€™ÂªÃœ)%Ã¶Â¼:fe8vBâ€“Ã–]WÃœSâ€¡Ã¯kÃ„Ã·Â³Ã‹Â´4'$Â¥Ã‹Æ’sÂ¥ÃƒÃU Ã â„¢â€“ï¿½/Å¸-Â¼Â¾TpmÂ¡hÂ±â€¹&lt;#Â¡Mâ€¹Ã©s
ÃªÃº(Ã¡Ã†rÃ‘ÃµÃ…â€™Ã«KÃ…K]U3:Ã²WÃ•Ã“bÃªRÃ¡ÃªBÃâ€¢Â¹Ã¬ÂµÃ‘ÃœÂ±VÃ’pÃ˜Du3qÂ¨â€°Â°Ã]^ï¿½Ã¯fâ€¢Ãv&amp;Å¾Å ZÃ¬ï¿½Ã¤%Ë†F
ÃDÃ¤~Eâ€“â‚¬Å’Ã¸MÃˆ]Ãâ€˜QÃƒËœÃ¹8Å“JÃ¤%â€ â€“ENÃƒÃ°*\:XÃ hâ€ºÃ±+Ã­Kâ€œâ€¹Ã¢Ã€Ã¶ÃUï¿½WÆ’*xÂ©CÅ“LÃ¼Å’ÃƒÅ’â€ Å½M86ÃªlÃœÃ§XVÂ®Ã¸MXÃµï¿½&gt;^ÃÂ¬Ãº
Â£4vÃ’Â³Âª~Ã½â€ºÂ­WÂ³ÃªÃ¢+YÂµÂ»â€°Â¹tÃ•Ã›ÃµË†Ã“Ã§Ã‚NQÃOÂ»[Ã€â€“Â¨Ã­Ã»(wÃÃ¬ÃªÂ®â€ ÂºÃ”Ã¤Ã›Ã„Âºfâ€ {Ã§cÂ¼JÃ¬ï¿½ÃÂ©i*Â·-Å½â€¹"Â¥â€”'F7â€Ã¸(YÅ½<r@ul;Ã–!zÃ‹ÃŒoâ€˜Ã—fÅ’Ã‹Ã¼Ã¦z|Ã‡eÃï¿½Ãµ â€šÅ )%Â»Â­:gÂ®Â³Â¬â€°yÅ½â€¦ï¿½kÂ»ÃµÂ¶rjâ€ _Ã•Ã¤oÂ·fzÂ ÃÃ)â€°wÃ¦Â¶Ãµvnkâ€¦Ãƒ.vrÂ¶&Ã‹Ã?ÃµÃ¶Ãª|Ã‰Â¼â€š:#â€ ÃµÂ½sÂ¢Ãª="" ueÂ ="" Ã¨Ã­Ã¥Â³Ã™Â«Ãªm="" Â°ÃªmÃ…Â¶Ã6â€“ÂªÃªÃ±Â½ï¿½Ã…Â½qÃ‹ÃªÃˆâ€¦Â¾'Ã¨Â­Ã•[r&Â¿"sxâ€¢4*ÃµÅ¡ï¿½Â»Ã´Â·Â¸Ã³*Â£â„¢Ã¹iË†ÃbÃ¨9Â°cÅ â€Å qÃ¬:Ã«`cÂ»ÃŒâ€leq@oÃ·Å½â€ºâ€¡zÃ«oÂ¿k:ÂªÂ·~Ã†)Ã#Ë†ÃÅ½Ã®Ã¨Â­Å â€â€¡Ã´Â¿1vÃ•-ï¿½Ã–mv}ï¿½â€ â€¹Â½ÃÂªhÃƒÃ…Â®fu4.Â¬="" 4vÃÂªÃºÃµg[ÃyÃµoÃ»Ã©Ã«Â¯Ã¿)vÃ­(hÃ¬v;Ã”Ã¥}Â¸Ã‹Ã«Ã°â‚¬Ã¯â„¢Ã‡!Ã·3Ã¯yÅ¾zÃÃÃ Â Â£Ã¡!'Ã£#.ï¿½xï¿½#Ã„Ãº<Ã‡`oÅ“hr_d,Â­6â€¢Ãldsrâ€ yjï¿½="" Ã–Ã‡="">ÃÃÃ¤XgÃ‡)3Ã§â€YÂ´TXÃ–Ã¦[`ÃœsÂ£Â¼Ã³c|Å Ã¢Â¼Â©YÃÃ¼JÂ«â€“
Ã«â€“
â€ºÂ¢M+Ã‰Â¦Â®ÃˆÂ­*=%"EÃ‚Å’Ãœ0Nâ„¢Â§Ëœj'Â¯Â±TÃ–Å¡+j,Ã›hÃ¶Â²'Â·&lt;FÃ‘Â©-AmtWÃƒEÃ†pâ€¢3Ãœ5Â®Å ZW)Ã•â€ºÆ’OÃ¥â€“&amp;qÃ°i<blÃ|rqÃ¾Ã–mlÃ±vÃ¹Ã‹k\dtâ€”6Ã˜Ãm4')ÃÂ©â€¢Ã¤]wËœtwË†ebâ„¢Ãˆ5 Â´Å¡Ã‰ï¿½Â¡Ã§`hÃ™ÃÃ”â€”[Ã”ff7â€”3ÃŒ%Ã•â€“ÃzÅ½wyr`i|piÂ¼1Ã–Â¯0â€ Â¸9q="">â‚¬ÃÃ™xnÂ¹%Ã¸Ã˜Xb6Â§Ã”â€š]bVgâ€¦ÃµÂ±Ã…xmÂ·Â¦Ã®Â¢T[XÃ´^XOÃ‹Ã Ev5=Ãªh|Ã˜Ã†Ã°pÂ¾ÃµÂ¾Ã³{Â­ÃZÅ“ÃŸÃ£9Â¾Ã‡s=Â®Ã,Â«ÃªvdÃ¨vÃ€Ã­Ã§Â°wZ
Â¿Tï¿½QcÂ¥]1Â°Å¾U_Ã‚ÂªÃ“Ã/Â³ÂªË†Â¿iÂµÃ‡ÃÂ©Â»â€¹Â¡'9Å¾'Å“,NÃ»ÃšÅ“	Â°G=Ã`1pÃ„Å½Å Â«	Ã¾Å½INÃâ€¹ÃŠ4Â»Ã´PÃ·Â¬ZÂ®CrÃÃŸË†Â·*Å Æ’MÅ¡Â¥Ã±uEÅ¾Å¡5â€”Ã IJÃ…Ã’â€˜Å½*@Â¬5Â¹â€°Ãâ€Ã¨^Â®ÃÂ¤ÃœmÂ¬ÃSÃ•ÃŠ-ÃTÂ¦Ã±ÃŠsÃºÅ¡Ã£fÂ»BB&amp;dÂ±ÃÃµY
@ÃœÃ…,â€šÂ£Âµe~Ã¾\WÃšâ€4Â¿Â¯ÃeÃ…0Â¿Ã¼Ã¢XÃšÃ†TÃŠÃºdÃšÂ´Â¬Ã¦xd@&nbsp;gÃ›Ã±â€ºÃ“iÃ«â€œÃ©[Â³Â©â€¹ÂªÃ¼1AÃ¥Å¸4ÃŒÂ«{FQÂ¸6â€˜Â¸6Å¾pn yÂ°	Ã€)Ã˜Â¥}HÃˆâ€6Ëœw2Ã¥Ã•%ÃªÃ–Ã¸Ã™Ã®Ã Â³=Ã¾-1
4gï¿½`iDSIÂ¦Ë†5Ã€sÃ™wÃ”Â»sÃŠ"9	5y1Â´Ã¬hÂ´hÅ¸ËœÃ•LpjgÅ¡!Â¬jW
0Ã/	Å“Ã -Ã“Ã®Å’j(Ã’WÃ®kgbgÃÃ¯wtÃ„Ã¯Ã¨Ã˜&gt;GÃ•^GTÃ¹qoâ€“U_gÂ°Ã”Â«cÂ§Ã—,ÂµÃ·Å¾ÃŸÃ–Ã–VÃÂªÃºÃµÂ»^Ã»Â°Ãªâ€¦Ã¡Â¾Ã_`UÃ°FÃ‹Â©=Ã¯b1ÃªÃ­JÃ˜Â©Â¹ï¿½â€œ$Ã—â€œ.â€“Â§}aKâ€Ã€UÂ¤Ã˜lÃ—Ãµ Ã[â‚¬Â«Ã’jÂ³â€™â€¡Â´ÃÅ“(Ã·ÃºbxÃÂ­dZ0Ã­
bÃ‚	Â¸&lt;6Å SÃ®"Â¥Y7â€Ã¸â€œÃ“Ã¢Ã¨9Q5Ë†Ã2râ€œxâ€¢CÂ­Å¾â€œ
7Ã€}uâ€˜ÃebzkEÃ¶ï¿½ f^Ã´vDÃŸÃ‰ÃŒUÃ’ï¿½â€z[S&gt;"ÃŒ[PÂ¥Å½Â´Ã·7â€º*&amp;$Ã¸ÂµÃ±TÂ¨Â·iÃ£Ã‚Å Â½Â­\Ã¬.zÂ»1â€¢&gt;ÃwÅ½Ã°ÂªFÃ¹â€¢Ãƒ-Â¤&gt;qÂ®+Ã¨Ã­Ã…Ã‘Ã„UZ_}%&nbsp;` Â¶ZÂ·IÃ„@Â©HÃ‰(cÃ¦Tï¿½gÂ»Ã½TlÂ¬Â¸*S@ÃŠâ€oâ€4niÅ¡Â¤:\#tz+Â¯Ãµl(â€°aÃ¤Ã„Æ’ 
5Å“Â¬Jï¿½Â¨HÅ½â€™m:XÂ«â€“:ï¿½Â¨Â©Ã£â€”=v'wgTaÂ°Â´m8Ã©Â²m8Ã©cgËœÃ¤xzÃÃ¯Ã¨Â¨Ã¿Ã‘To=ï¿½tUÂ¾IVÃ•-~ï¿½Ã¡{Â§Æ’Ã©Ã–â€ºÂ¡zÂ«Ã›pÂ±kï¿½`U###=Â«ÃªÃ—gÃ­ÃÂªÃxeUÂµZâ€”UÃ¥;Â¬ÂªÃˆKPÂºTxâ€{ÃªÃ¶&gt;,r;Ã¨|Ã²mÃ“co[ï¿½|Ã‡Ã¦4 Ã–ï¿½XÂ»Ëœ&nbsp;UÃÃ‡aUÂ°Ãµ)â€º3ï¿½â€ oÃƒâ€Ã£Å’Ã“â€SÅ’Â§yÂ¤PÃ¨Ã§câ€º`â€”fâ€ºnÅ¸eÅ¸Ã©ï¿½ÃªÅ¡}Ã=â‚¬:Ã¥Ã‡Â¸â€”$Â¸â€¢$Â¸â€”&amp;zâ€%yâ€™Ã½Ã¹â€“%â€“%pAÃ„Ã¤ï¿½Å Ã¤Ã°ÃŠÃ”ÃÃªlÃ¿Å¡|_fÂ¡Â³Ã€Â¯&amp;/Ë†â€“Qï¿½â€°Â¡eÃ…Ã’sQF1Ã‚XÃ¡Â¬â€šË†ÂºBÂ¸Ã«â€¹Ã‚Ã«â€¹"â„¢Ã¹Â°Ã²â€Yx6Â®Â¾(ÂªÂ¡8|Â²Â¾&lt; Å¡USWÃ‰*GÂ¾0Å’YZâ€ºÅ½`)bï¿½Â®FiEÃ»Oâ€˜ÃšÃˆpbrDej05Ã“â€¡â€“Ã£IÃ‹Ã±Â¨ÃŠÃ°,Ã‡Ã¹@Ã·ÃŒswÃŸÃŸ$Ã·ÃŒâ€”Å“(Ã°L3ÃƒÃ­ÃOÂµM
Â±I
Â±Å Ã·â€¦Â¯I$RÃŠâ€š
/tJ=Æ’P*ÃŒÂ¥Âºâ€ºâ€ÃªlrÃ„Ã‘Ã¨ï¿½ï¿½ÃAÃ‹SÂ²-Ãmr|â€”Ã«Ã´.Ã›Ã¡]Â¶Ã½;Â§cÃ²7Ã‡ÂªÂ»`Tï¿½u`ÃÂªÃ‚Wï¿½kÃœoZï¿½â€“UÃ‘Xâ€ºÂ¯Ã‘Â³ÃªV]Å¸Ã–Â¨_ÃÂª2â€UgÃ‚-fï¿½Oï¿½ï¿½Ã“LÃâ€°:Ã·Ã£fÂ°DÃŸwÂ´XÂ»8oGÃ‘â€šâ€fÆ’pMqcÃ¤Ã™ÂªÃ˜Ã¦ï¿½uâ€,Ã»Â¼hhÃƒXÃšPÃ¢*Â¤XÃ•Ã¤{â€™b(aÃ  ÃÂ²ÃÃ§(FQÃ«Kâ€N=N	Å½[Å¡Ã’â€OSâ€ Eï¿½3ï¿½ÃÃ²Ã€NÂ¼ËœÅ“ÃŸFÃâ€œÃ“Ã²Å’|ÃŸÃŸÅ“6)ï¿½UÃ³3:kÃŠ`
â‚¬]Â¼Ã”ï¿½YÃ–DÅ¸Ã‚hZÃºQÃ¶$Â¨y%â€¹Â½â€°0_&nbsp;â€°YÃ¬ÃƒjÃ¸EÃ Ã³Ã oAËœ4*ÃŠ:7~n |Â¶3Â¦Â»Â®Â¤â€¹	Mâ€œ:kÃ›Ã¡Â¤chï¿½$Â§Ã§Å Ã‰J&amp;nDÃ¢BÂ»Ã¾	%ï¿½[Å¡Ã•\Å¾Ã’TÅ¡\_ï¿½ÃFÃ·Ã©kÂ¶Ã«Ã£ZÃˆÃï¿½RÂ³bÂµÂ­Ã¥â€¢Â©aÂ¥â€°MÃ¥Å½(Â«Ã²*lâ€˜Â»&amp;ÃŸÃœÂ´Ã§=ÂªhÃ Ã¤Â³câ‚¬â€ VÃ¬;Æ’Ã³Ã«ogÃ¨ms&amp;ÃˆÃ¦tÂ·74@SyQyÃ®Ãµ8Ãœâ„¢'Â«Â½ÃƒÃ Ã¿Ã±Ã—Â°Ãª~ÃSÂ¿XÂ«Ã¿Ã’Ã¦Â©WÃ—ÃªÃ¯Â½Ã§Ã—Â³Âª~Ã½ÃÃ—Â¾Â¬ÃšÃ»Ã‹Â¬ÃšÃ‰Â©ï¿½Ã·9&gt;t|*Ã¨Ã„BÃˆÃ‰Ã‰&nbsp;Ã§Ã£Å½Ã¦'Â½ï¿½Â³X`fÃ¬dÃ¶\oaÃ˜Ã«cÃ¯Ã«(Â§â€ºâ€”Ã„Ã›&amp;Ã¸Â¹gEÂ¸4Ã¢mâ‚¬ÃÃŠÃ¨â€“Â¤tÂ§ÃœÂ¨`|Â¼_alxKâ€¦C+Ã‰Å¡â€“Ã­Wï¿½â€¹Â¦dâ€#zMÃ‹Ã†Â²
Â£Â»ÃÃ”BÂ¨Â·Ã­Â¬@&gt;Ã¨-Â·4]JÃ‡Å½Ã‹|gÂºÂ¼Ã‡Ãšâ€š:Ã«pÃ¢Âª&lt;&nbsp;Â·
Â¨Â·mâ€R57Â­Ã„Ã´7Ã¥vÃ•â€“vÃ—Ã¡â€¡ZÃ²â€”Â¶Ãµvï¿½[Ã‰ï¿½Fâ€â€¹Ã½Ã±PoÃ•Ëœâ€¦ÃÃ¸ÃÂ¦R&nbsp;Â·Â½
@Å Ã±mÂ©@oÃ»#Â¦Ã˜vÂ¡â€¹Yâ€Ã¨mï¿½VoeÂ´\IU^G]Ã‚Â¸ÃœÃ¨Â­Å $$Ã¡Â¸eÃœÃ²&gt;Â¥Â±8YQÃ«&gt;Ã€Â³Ã©Ã¥ZÂ·TÃ€Â±YËœÃª,Jâ€˜Ã“Ã‚*SÃ‚Ã°	â€˜Â­$ÃˆÂª]uf
%Å½Ã¨Ã&gt;4Â¯Ã“5Ã¾}aÃ–Ã¼vÃ©Â¯Å½ÃxÃ™Å“â€°Â·?Ã•Æ’Ã¨mÂªÂ·Ã®â€¡;Ë†Â¹râ€¦RÂ®Ã¬PÃ¼jVÃo8ÂªÂ·ÂºÅ½Ã«Ã»
GÃ˜Â«Â·ÂºÅ½Ã«ÂºzÂ«:ï¿½Â²ÃªÂ©SÂ§Ã€Ã£ÃµÂ¬Âª_â€Â¥Ã‹Âª?Â¾ÃˆÂªÃ½Ã½Ã½[[[â€”/]Ã’Â¼ËœWâ€¢Ã‰d
â€UÂ¥Â¹Ã±Â§bÂ·Æ’`+&lt;ÂµÂ¸Â°?Ã¶gÂ£Ãƒoâ„¢Ã»Â³Ã…Ã±Â¿XÅ¾|Ã‡ÃºÃ”{vg8t2:xÃŠÃ•Ã´ËœÂ»Ã¹qÃ‹â€œÂ§ÃÃÃxXz[Ã»Ã˜ËœÃšâ€ºâ€šP9ÃœÃ•IÃ®XF{XGÂ¹Ã›E{8`Â¼Ã¢|Q{Ã’Ã¤`g MÂ©Ãn)ÃÅ¾Ã©Â¡^aÃYÃ¡Â¾Ã™â€˜Â¾Â¹Ã‘Â¾Ã¹ÃŸÃ‚XÃŸÂ¢8Â¿Â¬?&gt;&gt;Ã© Æ’nEÂ¥â€°Ã¡Ã¥â€°Ã¡â€Â¤`ÃƒHiÃÂ£â€°â€IJï¿½&amp;Â¥Ã†ï¿½Ã’bÃˆ`Â§cÂªÃ’1â€XpDl&nbsp;Ã¿â€LxÃ‡VeÃ„R2Ã _ï¿½]â€¦&lt;Å’Å’~IZ4Ã²"IKC+SBâ€°)Â¡Ã‰!\p9.Â¨,	Ã¸Ã€'cÆ’
0AÃ¹1ï¿½yÃ‘Â¹QÃ¾9Q~9QÃ9Q Â¬EmBÂ¡iRÂ°3Ã¢\Ã§â€ÃµqÅ ÃµvÅ’Ã±tÅ’Ã¶Â°â€¹tÂ³ï¿½pÂµ	uÂ¶sï¿½N,Ã›â€°TÃ„=Ã‰ÃÃ¦4â‚¬}@â‚¬RÃ¾Â»Ëœu2&gt;Ã¬`tÃˆÃÃ &nbsp;Ã­Ã©Ã·Â¬NÂ½kzÃ¬ï¿½TÂ³Â·Ã«Ã¬ÃŸiÂ°â€¡iÃ·Ã‹Ã¦mâ€“ÃƒQÂ¹Ã¨ÃÂ°ÃªÂ®Ã¸u:Ã ^=Â®q?AF^kc`â€Uï¿½?ÃÃ¥rÃµÂ¬ÃºÃÂ²ÂªBÃ„ÂµÃ·?6pl):Â­q=jkOÂ¢â€”Ã¥	Ã°Ã«Ã¤Â«Ã£ï¿½Â¶Ã£Â¶dï¿½Â¼Ã©Ã›ÂµTXP2-Ã¼Å“Ã’BaÃªÂ¿ï¿½cÃ‘FÂ·(Ã€Â¸fGÃºâ‚¬sWâ‚¬Ã±Ã§â€:Âµâ€™l+Ã“Â¼KÂ°Ã¡Ã„â€`pÃŠÃˆÃ©Ã¡Ã¤tGÃ…Â´â€™<zÃ˜v}ÃÃ–] Ãï¿½%Ã‘Â¬â€šÃ¤Ã†Ã¢dÅ“(â„¢ajÂ¡Ã«xâ€ºÃ d15Â©â€¢Ëœ#"eâ€¹Ãˆ9jÃ¶="" btÃ”Ã“Ë†kÂ£Ã‰iÃ¹Ã­ÂµÂ¹Ã“Ã­ÃgÂ»fÂ»â€šzâ€º2ÃškÅ ;kâ€¹;jkÂºxeÅ¡Ã–Ã„yuÃ€Ã™Â®Ã 9uÂ Å¡â€¡cfÃµâ€¢vÃ”="" 4'ÃvÃºÃtÃºÅ½jcÃ¤Ã´|Â¤="" }Ã‡Ã½Å’â€™%&gÅ hbÂ¿"Â£Â«!`hÃ Ã˜ÃŸÃ¢(Â¥ï¿½Å¸ÂªÂ¾(Â¹Â®0Â©Â¡Â£dÃšÂ©Ã˜2â€ m]q="" Â¢="" Â°3Â·2Ã°Â²Ã„ï¿½Â¢Â¸pvÂ©Æ’Â²Ã†nÃ5Â¥â€¡yi)uÃ‡ï¿½Ãvkâ‚¬{Â¦ï¿½Ã€iÃ‡ï¿½Ã­Å’Â§Ã•)Â«â€œ="" ÃÃƒ="Å¾â€¡Ã›ÃÃ¶8Ã”Ã¡vHâ„¢ï¿½Âº;ÃÂ«Ãªï¿½Ã«}Ã¬7Ã©ooÂ°Â®Ã‘Ã‡ÃÃ¦)Ã”XÃ·Å¾Â°*Ë†ï¿½lllÃ´Â¬Âª_Â¿Ã«ÃµkXÂµÆ’S;Ã¥ul<ï¿½ÃªÃ­TÃÃ±Ã¡â‚¬cxÃ‡Â£v&amp;Ã‡Ã-NxÃ©zN:â€¡ÃªÂ¸-Ã…xÂ¹Â°â€œ1ÃŒÃ‹qâ€“Â±^Â®Â¹Ã‘Ã¶Â¼JÃ˜Ã¥Ã”JÂ²ÃŠï¿½qÃ‹Å½Ã°ÃCÃ´V@Â¶k*Â·Â¯HÃ¶ÃƒÃ‡â€!Â¥â€ ÃÃ«Ã±tÂ¢HiÃ*Ã”[EÂ­ÃÂ±ÂºBÂ¢Â·)]ï¿½Â¾Ãƒbâ€”1Â©K'DDIÃ‘ÃªÂ­â€â€™=," â€˜uÂ°Ã’eÃ”b9Â­Â Â»!cÂº#Ã•Ã›Â®ÃºÃœÂ Â·ÃŒÂ¢Å½z|w}Ã¾Â¨8Ã•Ã›yuÃ ï¿½7Â­Â£Â¶Ã¨mgmÂ¡Å¡Ã´vÂºÃo#Å’ï¿½tâ€¢ÃˆÂ«Ã³Â ÃrwÃœ&ÂµzkÃŒp5yâ€Ã®Ã£:â€°ÂªÃ‚Å â€œÃ«Ã |Ã¹dnidwÂ½â€¢Å c!Â­Â¶Â­Ã‰Â£d@ï¿½g(5Â´ÃµÂ¶0.â€_iÃ“Ãâ€z[wlÅ¸Ã¥Â¥Â«Â·Â¸@Â§Ãº,t="" 'Âµ!â€œï¿½ÃiÃ‹sÂ±Â¶'â‚¬Ã’Âª<+Â½Ã­t="Â¤Â¬â‚¬Â¬ÂªhÃ¯PÃˆÃ¥Â¿Å¾UuÃ—_sÂ²Ãª.Ã‡ÃµÃ—iVEMÃ—QV577Ã—Â³Âª~Ã½Â¡Ã–.VÃ½aâ€¡UAÃ”Ã›Ã—Ã—Â·Â¹ÂµuÃ¥Ã²eâ€UÃ›ÃšÃštYË†Â³(Ã‹Â·ï¿½Ã¯Ã²ÃÃ¥â‚¬Ã„Ã­" Ã›Ã©="Â»Â£oÃºâ€œÃ‰Ã¡?â„¢yÃ‹Ã´Ã¨Å¸Ãï¿½Ã½Ã…Ã²Ã¸Ã›Ã–'ÃŸ$ef;ÃÃªÅ’Â¤Y=-Å½{[Â¡Ã’}ÃšÃŸÃ®ÃŒÃ}#Ãª?`Ã©nâ€ ÃŒÃ›5vÃªdÃ¬Ã¬qï¿½)!Å½)!NÃ©Â¡ÃÃ©aÃÃ¡pÂ¾ï¿½Ã€Å“HÃÃœ(Â¯Â¼hÃ¯|8hÃƒÂ»0Ã–Â»(ÃÂ§8ÃÂ·Ã«[Ã¯Ã Â±Ã¬Ã„Ã€Â²D8Â¼" lÃaÃ‰`wÃ€Å“,ÂºÃÅ¸Æ’â€°Ãˆgd#Ã€Ã â€ _rÅ¾;gÃ‹p&Ã…Ã‡Ã»#5Â½Â°ÃŸÂ¿(pÂ³o="">bÂ¥Å¾Ã£â€¢
~*ï¿½Ã¬Ã·Â¬pÂ·ÃŒpÃ—Ã´0Ã¨Ã¨â€¹Ã†Â´IHâ€¢ Ã¢
â€¢6ÃÃ›
qHÂ¶Å’Ã²0Ã=Ã‚Ã•4ÃŒÃ™$Ã„^Ã˜Ã¸#MÂ©^VÂ§Ãr_HÂ©&amp;G Â¥Â²;sÃ€Ã¦Ã”{V'ÃŸÂµ&lt;Ã±Â¶Ã™Â±Â¿Ã¾sÂ¢Ã±Å¸Ã©6oÃ—Ã˜Â¾MÂ³yâ€ºfÃ½ÂºÃaâ„¢HÃÃÃ™Â¥Ã¼uÂ¬ÂºÅ¸[Ã‹?Â¿Zï¿½ÂµÃ†JÃšÃÃ—MÃ–Â³ÃªÂ¿ÃˆÂªBÂ¾:Ã˜\Ã£Ã‹Mâ€¡Ã½Ã¡Ã‡jÃ§#VG^Â¨vÂ°9Ã­gk&nbsp;Ã£ï¿½Ã¡jÃ¦lÃB4oÂ®0Ã'YUÂ¦[+jÃŒTï¿½â€“ÃµÃ…â€°Ã¾Å¾Ã°Â²Ã…Â«Ã§ÃšJÂ¶VÃ™P2ÃœÃ€Ã€Ã‡ï¿½Ãƒâ€¦GÂ£fÃ³*Å“Ã“uÃ–â„¢Â·3Â­â€º	Å¾Â´Ã¬xX'Å¸â€ºÃ„Ã†G)â„¢Æ’&lt;Ã›Å¾}WÆ’â€¡ï¿½Ã“Zâ„¢(Â¢`Â»Ã¼G%.C9#NPâ€˜+Â¡Â¤Â¶Â³Ã¢F$cmncR;FÃHmÂ£fK)yRjÂ®Â²6
|rÂ¼ÃcBÃ¦Â®Ã¸wÂ°â€™Â¤Ã”9#Â­Â·)lTÃª:"qjÃµPÃ”bÂ¥â€aeF+â€“Â¶ÃÃÅ¡Ã‹â€œÂ¹eIï¿½Ã…Ã©Â¼Å ÃˆvÃ¬mogÃ™Ã²*Â½Ã«
Â£Ã«â€¹Ã‚Dâ€ºÃ:â€¹â€“Â»Ã”Â£*=Å’ËœÃ›rÂ¡Z`1xvIÂ¾â€¢i^&lt;Â¢M;x^,3Ã™Â²"Ã…1'Ãš59ÃˆDMÂ¨Ã–gWÅ“P4?hâ‚¬vÃ„Â«Â®Ã¦'Â¼-Å½â€¹Ã*ÃÂµï¿½Ã­vÂ°ÃÃ¥â‚¬,Â£PÃˆâ€¢]Â¿Â«Â¾Â¢=ÂªÂºÂµÃºÂºÃ“jtkÃµuÃ’Â´Â±â€œÅ¾UÃµÃ«ÃŸoÃ½VmgÃ—Å½xUÃ»BÂ¥Â¸:Ã wÂ´ÃÃ®Ë†ï¿½!Ã”[xQoÂ¡SÃÂ²Ã£Ã ï¿½Ã¨-ï¿½kyï¿½9Â»Ã”Â¤ Ãâ€ Å¾gÃ™Ã2BDÃÂµÃ‚xfF@Â½%Â¦:Â·Ã‘Â¬Z*Ã¬Hi%Ã°N&gt;zNÂ½M	Â£eâ€°ÃˆvHÂ½â€¡Â¹Å“aÃ‹)Ã³Â£Ã§`kÃ³Ã£jrqÅ“Â²Ë†Â®â€”AÂ¾Ã­@â€¹};Ã‹GPÃ´VBï¿½Uq|Ã†eN<o)' Ã¦hÂ«sÂºÂ¢ÂµzÃ›Ã•â‚¬â€¢Ã“Ã“!u"zÃ›yÅ¸8Ã–Ã¦1.Ã³Ëœï¿½Â»="" Âµ*krÂ¥â€emrs0ÃÃ›aâ€°Ã«ï¿½ÃÂ§ï¿½Å½â€œtÂ¥="" Â ÃÂ¦Â½Ã¥Ã­Ã¨msÂ fÂ Ã€a]="" ÃfÂ¾Ã†Â¾â„¢Ã Ã´â€“ï¿½â€™t[w7â‚¬Â§iywÃ¨mjbnl)Ã‡Æ’ÃˆÂª$="">Â°Ã§KJÃ·UYâ€šÃ§Ã´â€“[nUâ€“Ã¤â€â€°ÃªÃ­vÃ¸â€ºÂ¤ï¿½tÃªÃ³Âº_c]ÃƒIOÃ‹â€œ.Ã¦'Â¢Â¬Å½IÃœJâ€¢Âºâ€™Â½u&gt; Â¯Ãˆ2Â«|Â£Â¬ÃºÅ i5Â¯Ã™pÂ¡Ã•Ã›Â½Ã€zVÃ•Â¯?Ã¸zÂ«Ã¶Ã¶Ãµmll\Â¾|yxxx/Â«ÃŠÃ›ÃšÃ™XÅ½Ã­;Â§wÃÃ¦Â»`9Â¾kwlâ€ºUÂ·Ã·Â¡?â„¢~Ã‹Ã¬(â€™f=Ã±Å½ÃµÂ©w!Â´â€ÃjtË†â€“Âµ9â€[Â·}Æ’C Â·Å¡â€š`â€ÃQÃ®ÃˆÃ#:Ã«XÂ¬Â¯Ã­6ÂºÃº;â‚¬Ã¤â€â€šf]C]Ã“Ã‚ÃœÃ’aÃ0Ã€CÃ·lÃ„&gt;'ÃŠ372,Å“Â¾AÂ²lâ€¢Ã›Ã™Ã˜Ã­ï¿½â€¦Æ’9ï¿½Ã­_â€â€¦Ã¬â€°Ã¼~}@aÅ“_Ã¡ÃWï¿½Ã¯Â¾Ã¸nâ€œÂ¢Â½Ãâ€™.;Ã’#+Ã‚â€¦</o)'></zÃ¸v}Ã­Ã¶]></blÃ½|rqÃ¾Ã¶mlÃ±vÃ¹Ã«k\dtâ€”6Ã¸Ã®m4')Ã­Â©â€¢Ã¤]wËœtwË†ebâ„¢Ã¨5></r@ul;Ã¶!zÃ«Ã¬oâ€˜Ã—fÅ“Ã«Ã¼Ã¦z|Ã§eÃ¡ï¿½Ãµ></iuxÃ—k-pÃ²Ë†Ã¬â€¡Ã¡â€šÂ¿ÃµÃ¨Â®h%fhÂ­Ã¤Ã¼vÂ¨zÃ <"â€ â€“Â§jï¿½[g]â‚¬Â²&xï¿½Ã¯Â®8></urâ€“ÃŸ[_></iuÅ¾Ë†\Â¤`dï¿½hbg$â€˜2*^Ã©(pÃ°jâ€ â€¦â€°s=!Ã³Â½!Ã£mÃ±ï¿½ÂµÂ¥Ã°Â´Ã¨'aÃ­o?Ã»Â±Ã¸Â²Ã°:Â¥Ë†Ã«Â©(zÃ°sw4Â¥,0ÃµÃ²Ã—â€â€š|Ã¿<Ã¹ÃªhÂ¹mÃ¨dfiuâ€“Ëœâ€%$e*j1â‚¬Â£â€¡eÅ¾]></wÂ¥â€¡sÃ¢Â©Ã¹Å¡Å¡mv%Â¥yÃ¤Ã§Ã ï¿½ÂªÂºÂ½o></eÂº=Â«ï¿½qÃ´></yrÃ¯Âª;yÃµgh^uzxx,vÂ½{Ã§Ã®Ã¢kyÂµÂ£Â«cÃ¤Â¿â€œdÃ¿ï¿½â„¢Ã·Ã·0Ã¾Ã¨^Å“qgÂ°â€˜â€”Ã£Â¶Â·ÃºtÃ¾Ã¹asyâ‚¬#xÂ¯gÂ½></s-Ã¼Ã¬(ÂµpÃ¾Â°Ãª?[oÂ¶Â·yÃµÃµfÂ¨Ã©:Ã°[â€umllï¿½Â«â€šÃ«yuÂ¿Ã¾Ã«_fuÂ¹Â¬mÂ¤4y9Ã¨Ã¤r(Ã¼kÃ¡Â§Â§â€šodÃ¸Å¾Ã¶Â´></ziÃ»Ã¶iÃ—Â³gÃª></u></a></i></u></i></u></s></b></s></i></b></b></i></s></i></b></b></s></i></i></s></i></i></i></i></i></s></b></i></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf">https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf</a></em></p>]]>
            </description>
            <link>https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24693589</guid>
            <pubDate>Tue, 06 Oct 2020 00:17:45 GMT</pubDate>
        </item>
    </channel>
</rss>
