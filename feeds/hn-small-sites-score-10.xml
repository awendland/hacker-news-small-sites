<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 10]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 10. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 04 Nov 2020 16:37:04 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-10.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Wed, 04 Nov 2020 16:37:04 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[The Untimely Demise of Workstations]]>
            </title>
            <description>
<![CDATA[
Score 27 | Comments 14 (<a href="https://news.ycombinator.com/item?id=24977652">thread link</a>) | @ingve
<br/>
November 3, 2020 | https://deprogrammaticaipsum.com/the-untimely-demise-of-workstations/ | <a href="https://web.archive.org/web/*/https://deprogrammaticaipsum.com/the-untimely-demise-of-workstations/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<p>Last month’s news that <a href="https://arstechnica.com/information-technology/2020/10/ibm-to-split-into-two-companies-by-the-end-of-2021/">IBM would do a Hewlett-Packard</a> and divide into two—an IT consultancy and a buzzword compliance unit—marks the end of “business as usual” for yet another of the great workstation companies.</p>
<p>A quick aside on computing history. You can imagine personal computing being driven by two distinct schools of thought. The “top down” school, represented by research-led organisations including Xerox PARC, Bell Labs,academia and the military, asked “what would the world be like if everyone had their own minicomputer”? They took large, time-sharing systems like UNIX and installed them first under, then on, employees’ desks for their own personal use.</p>
<p>The “bottom up” school was made up of hobbyists who asked “can we make an interesting computer out of inexpensive components”? Thus companies like Apple and MITS in the US, Acorn and Sinclair Radionics in the UK, and others took chips that were usually used as peripherals controllers in “real” computers and built interactive programming systems around them. The microcomputer revolution came from the bottom-up school, as they made home computing affordable. The workstation revolution came from the top-down school, as they made powerful on-demand computing feasible.</p>
<p>The two schools came into very close proximity in the 1980s, when the Motorola 68000 family of CPUs (along with the 68881/68882 FPU and 68851 MMU) were the processors of choice in everything from entry-level PCs like the Atari 520ST, through games consoles like the Sega Mega Drive (Genesis in the US), to the most expensive UNIX workstations from NeXT Computer, Sun Microsystems, and Apollo Computer.</p>
<p>But then the workstation makers invested heavily in their own CPU architectures based on RISC design principles and again the two diverged. The workstation market became highly differentiated: RS/6000 from IBM (later PowerPC), Alpha from Digital Equipment Corp, MIPS from, well, MIPS, SPARC from Sun, PA-RISC from HP. The software on these workstations, while superficially very similar, was also differentiated and surprisingly incompatible. Take a program from HP-UX and you’ll have difficulty running it on NeXTSTEP, unless the authors shared the source code and used the nascent GNU autotools to support portable building. As Yoda said: begun, the <a href="https://www.livinginternet.com/i/iw_unix_war.htm">UNIX wars</a> have.</p>
<p>Of course we know that the (desktop) computing world today is mostly Intel and that workstations are mostly fancy PCs, rather than bespoke designs by vertically-integrated companies, Apple being the two trillion dollar outlier. How we got here was that the commodity parts got good enough that there was no evident advantage to workstation-grade hardware. A high-end PC could easily run a workstation OS like System V UNIX (Solaris was an early example), BSD (386BSD which later became FreeBSD, or NeXTSTEP) or Windows NT.</p>
<p>Along the way, the workstation companies consolidated (Apollo and eventually DEC got absorbed into HP; MIPS into SGI) or disappeared altogether (Sun became Oracle Hardware; SGI went bankrupt and sold its assets to sgi; Symbolics did similar—incidentally Symbolics was the first company with a .com domain). IBM long ago stopped even making its own brand PCs, and the news of its split means that there are now very few workstation companies trading in the same form they had “back in the day”. The only ones I can think of that have not had major changes to their corporate structures are Xerox and Sony, whose management may not even have known that they sold workstations.</p>
<p>What’s got lost alongside the death of the workstation is the business model where you sell expensive computers as part of an integrated solution into a particular vertical market, where that expensive solution will cost a lot less than cobbling something together out of cheap PCs. Why? I think people have a lower expectation and higher pain threshold when using computers now; they expect an amount of friction based on their own experience and translate that expectation into realms where it doesn’t belong. As I described way back in issue 2, <a href="https://deprogrammaticaipsum.com/the-various-meanings-of-quality/" target="_blank" rel="noopener noreferrer">computing is a lemon market</a>.</p>
<p>Organisations would go to the workstation vendors because they solved particular problems very well. If you’re in AI, you need Symbolics. Computer graphics, SGI. Telecoms, that’d be Sun. If you want to write software in Ada for the military-industrial complex, you’ll be buying a Rational workstation. Yes, the first IDE was a completely integrated package of hardware and software. And, of course, Apple for Desktop Publishing, the Mac being a workstation of sorts itself. People would buy computers <em>because</em> applications like AutoCAD, Quark or Mathematica ran well on them. They wouldn’t buy the computer then browse the App Store to see whether it could do anything useful.</p>
<p>And the strange thing is that catering to those vertical markets with integrated solutions is easier than ever now. The wide availability of free software means that the basic job of “being a desktop computer” is taken care of at zero cost, so business can focus on contributing valuable bespoke behaviour. And hardware costs are lower than ever: the availability of high-capability SoCs and single-board computers like the Raspberry Pi and Rock64 should make it a no-brainer to sell the computers as accessories for the applications, not the other way around.</p>
<p>In high-tech domains, an engineer could readily have a toolchest of suitable computers in the same way that a mechanic has different tools for their tasks. This one has an FPGA connected by both PCI-E and JTAG to allow for quick hardware prototyping. This one is connected to a high-throughput GPU for visualisations; that one to a high-capacity GPU for scientific simulations.</p>
<p>The general purpose hardware vendors want us to believe that an okay-at-anything computer is the best for everything: you don’t need a truck, so here’s a car. But when you’re hauling a ton of goods, you’ll find it cheaper and more satisfying to shell out more for a truck. Okay-at-anything is good for nothing.</p>
<p>Cover photo by <a href="https://unsplash.com/@serejahh">Serhii Butenko</a> on <a href="https://unsplash.com/photos/zx2Vc1zPDIs">Unsplash</a>.</p>
	</div></div>]]>
            </description>
            <link>https://deprogrammaticaipsum.com/the-untimely-demise-of-workstations/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24977652</guid>
            <pubDate>Tue, 03 Nov 2020 08:34:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Learning Rust as a Gopher, part 3]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24977373">thread link</a>) | @BookPage
<br/>
November 2, 2020 | https://levpaul.com/posts/rust-lesson-3-and-4/ | <a href="https://web.archive.org/web/*/https://levpaul.com/posts/rust-lesson-3-and-4/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody"><p>Hello and welcome to the third post in my series about learning Rust. In case you want to hit it from the start, <a href="https://levpaul.com/posts/rust-lesson-1/">here’s</a> a link to the first one! This entire series covers my journey from being a completely land-locked Gopher to becoming (hopefully) a hardened Rustacean, able to skitter the hazardous seabed of any application safely.</p><p><em>Warning: Incoming opinion monologue; feel free to skip to <a href="#give-me-the-lessons">The Lesson Review</a> review if that’s what you’re after</em></p><h4 id="how-rust-is-makin-me-feel">How Rust is Makin' Me Feel</h4><p>Let me begin not by thwacking loosely about in concepts I know little yet of, as there’s plenty of time for that, but instead allow me to paint the landscapes I see for both Rust and Go. At quite the visceral it occurred to me that these modern languages are built not just to make our coding lives more delightful - but instead to shepherd us into particular engineering and organizational goals.</p><p>The needs for new general purpose languages no longer stem from the “simple” software problems. Take the problems of using variables (Assembly), using custom data types (C), cross-platform, cross-architecture compatibility (JVM languages) or even being dead simple (Python). Today’s languages can instead choose features from all the preceding and <em>then</em> apply engineering and organizational direction.</p><h4 id="the-coercion-of-go">The Coercion of Go</h4><p>I’ve been writing in Go as a hobby since 2014 but professionally only since 2018. As a hobbyist I’ll admit I used to think “Go is <em>opinionated</em>”. That seemed cool, because opinions mean something! Right? But <em>why</em> was it opinionated Levi, WHY? WAKE UP MAN!</p><p>Well today I have a completely different look on it. Go is simply a language that is very <em>safe</em> to share across engineers. This is because engineers don’t need to make a lot of decisions when they use Go. If an application has had its design completed in theory then in Go it often really is just a matter of whacking out the code to make it a reality.</p><p>Just look at how much of Go’s development tooling Google owns. With Java, I remember choosing between Ant or Maven for your build tooling. Go doesn’t let you chose. The closest we got to having a choice was with <code>dep</code> for dependency management. But finally Google caved and <code>gomod</code> was brought about as the standard. Go does its best to take choices away from you. You don’t need to chose between Tomcat or Jetty - the Go <code>net/http</code> package will handle 10kRPS for you no problem. Hell I’ve looked at apps serving 50kRPS, whilst <em>logging</em> a third of said requests simply using <code>fmt.Println</code>. You just don’t need to stray far from the standard library to scale and that is a huge plus of the language. (A large part of this is also comes from the fact that webservers haven’t changed a heck of a lot in the past 10 years so Go didn’t have to “keep up” - on the other hand my experience with http2 in Go has been far from ideal).</p><h5 id="_anyone_-can-pick-up-your-code-and-fix-it"><em>Anyone</em> can pick up your code and fix it</h5><p>This is by far the biggest selling point of using Go <em>in a company</em>. If you need to hire help you can find basically <em>anyone</em> with backend experience, and they will pick up 90% of Go in a week or two. You don’t need to worry at all if they have experience with Struts, JUnit or Spring - what’s in the standard library is plenty. I mean it. Do they need to know about passing pointers or values? Not really - general software engineering practices like peer review and simple unit testing will uncover those types of issues with ease.</p><p>Now on the other hand - who in their right mind would hire <em>me</em> to join their Rust team right now? Nobody - because I would be a gigantic liability to that team.</p><h5 id="gos-purpose-is-for-dev-shops-to-crank-out-web-services-that-are-scalable-easy-to-develop-and-do-not-require-mission-critical-performance">Go’s purpose is for dev shops to crank out web services that are scalable, easy to develop and do not require mission critical performance.</h5><h4 id="rust-no-rust">Rust no Rust</h4><p>I’m a noob. 100%. But even so, in my feeble mind I can already see what Rust is. I see a very sharp knife; but this knife is completely and utterly shrouded and encased in tamper-proof, child-proof, thief-proof hardened and sealed plastic shells. Yes shells as in the plural of shell. These shells are even adult-proof too, where the adult is a generic engineer trained generally in other languages only. The compiler is the packaging, and it will let you wield the knife when it knows exactly what your action plan is. -But oh no, not just any plan will do, your plan <em>must</em> adhere to each and every rule and regulation from the Knife Safety Measurement Act of 1938 and its associated amendments!! (This may not be strictly true as I’ve heard about an <code>unsafe</code> keyword).</p><p><em>But why so much plastic broseidon?</em> You know, and I know that it’s to keep mild-minded people like myself exactly, from nicking fingers with that very sharp knife. Warping back to a meta-level, those fingers don’t even necessarily belong to me the coder, but to the end users of the code. It is no secret to anybody even slightly interested in Rust that a major driving factor for the language was to be able to replace C++ code with something <em>as</em> efficient but much less susceptible to security exploits. Thus, the safety plastic aims not to protect individual coders, but the <em>coding organization</em>.</p><p>Okay, you’ve probably heard enough of my opinion, let’s move on before this analogy implodes and actually hurts someone.</p><hr><h2 id="give-me-the-lessons">Give Me The Lessons!</h2><h4 id="3-common-programming-conceptshttpsdocrust-langorgbookch03-00-common-programming-conceptshtml">3. <a href="https://doc.rust-lang.org/book/ch03-00-common-programming-concepts.html">Common Programming Concepts</a></h4><p>One of the first ‘huh’ moments in this lesson was this compiler message:</p><div><pre><code data-lang="rust"><span>compiler</span><span> </span><span>error</span>:
<span>For</span><span> </span><span>more</span><span> </span><span>information</span><span> </span><span>about</span><span> </span><span>this</span><span> </span><span>error</span><span>,</span><span> </span><span>try</span><span> </span><span>`</span><span>rustc</span><span> </span><span>--</span><span>explain</span><span> </span><span>E0384</span><span>`</span><span>.</span><span>
</span></code></pre></div><p>Naturally I ran the command listed, which took me to a <code>less</code> window (buffer?) containing the following:</p><div><pre><code data-lang="fallback">An immutable variable was reassigned.
Erroneous code example:
'''
fn main() {
    let x = 3;
    x = 5; // error, reassignment of immutable variable
}
'''
By default, variables in Rust are immutable. To fix this error, add the keyword
`mut` after the keyword `let` when declaring the variable. For example:
'''
fn main() {
    let mut x = 3;
    x = 5;
}
'''
</code></pre></div><p>…and this slightly let me down. There isn’t a whole lot of information in this “explanation”. I proceeded to allow <code>rustc</code> to “explain” some more random error codes to me, most of them seemed also to be quite small or to have been deprecated. I am hoping either A) I don’t have to use this feature much or B) I can make rustc/cargo/intellij just tell me the detailed stuff by default.</p><hr><div><pre><code data-lang="rust"><span>const</span><span> </span><span>MAX_POINTS</span>: <span>u32</span> <span>=</span><span> </span><span>100_000</span><span>;</span><span> </span><span>// wtf is this ugly numeric shit
</span></code></pre></div><p>For some reason this irked me when I first saw it (the comment taken verbatim from my lesson notes). On second look it actually seems really, really helpful for readability.</p><hr><h4 id="shadowinghttpsdocrust-langorgbookch03-01-variables-and-mutabilityhtmlshadowing"><a href="https://doc.rust-lang.org/book/ch03-01-variables-and-mutability.html#shadowing">Shadowing</a></h4><p>This seems like a really handy trick. You get to write code as if your variable was immutable, but the compiler does the switching for you. What is really messing my head up though is that you learn about Shadowing before you learn about Ownership. So does my naive understanding of shadowing change after this? I don’t think so … at least. Here’s something I wrote to verify my learnings:</p><div><pre><code data-lang="rust"><span>fn</span> <span>main</span><span>()</span><span> </span><span>{</span><span>
</span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>s</span><span> </span><span>=</span><span> </span><span>String</span>::<span>from</span><span>(</span><span>"hello"</span><span>);</span><span>
</span><span>    </span><span>let</span><span> </span><span>r1</span><span> </span><span>=</span><span> </span><span>&amp;</span><span>mut</span><span> </span><span>s</span><span>;</span><span>
</span><span>    </span><span>let</span><span> </span><span>r1</span><span> </span><span>=</span><span> </span><span>&amp;</span><span>mut</span><span> </span><span>s</span><span>;</span><span>
</span><span>    </span><span>println</span><span>!</span><span>(</span><span>"{}"</span><span>,</span><span> </span><span>r1</span><span>);</span><span>
</span><span></span><span>}</span><span>
</span></code></pre></div><p>…and it works. It also answers a question I had when originally learning ownership. For some reason when reading the examples in the book I came away thinking Rust could infer when immutable references' scopes end, but not mutable ones. I was puzzled by this and had a follow-up item but this little example proves I was wrong. Happy days!</p><hr><h5 id="small-nit">Small Nit?</h5><p>The <a href="https://doc.rust-lang.org/book/ch03-02-data-types.html#invalid-array-element-access">Invalid array element access example</a> didn’t work - it was supposed to produce a runtime error but instead it failed to compile:</p><div><pre><code data-lang="rust"><span>   </span><span>Compiling</span><span> </span><span>variables</span><span> </span><span>v0</span><span>.</span><span>1.0</span><span> </span><span>(</span><span>/</span><span>home</span><span>/</span><span>levi</span><span>/</span><span>rustprojs</span><span>/</span><span>variables</span><span>)</span><span>
</span><span></span><span>error</span>: <span>this</span><span> </span><span>operation</span><span> </span><span>will</span><span> </span><span>panic</span><span> </span><span>at</span><span> </span><span>runtime</span><span>
</span><span> </span><span>-</span>-&gt; <span>src</span><span>/</span><span>main</span><span>.</span><span>rs</span>:<span>5</span>:<span>19</span><span>
</span><span>  </span><span>|</span><span>
</span><span></span><span>5</span><span> </span><span>|</span><span>     </span><span>let</span><span> </span><span>element</span><span> </span><span>=</span><span> </span><span>a</span><span>[</span><span>index</span><span>];</span><span>
</span><span>  </span><span>|</span><span>                   </span><span>^^^^^^^^</span><span> </span><span>index</span><span> </span><span>out</span><span> </span><span>of</span><span> </span><span>bounds</span>: <span>the</span><span> </span><span>len</span><span> </span><span>is</span><span> </span><span>5</span><span> </span><span>but</span><span> </span><span>the</span><span> </span><span>index</span><span> </span><span>is</span><span> </span><span>10</span><span>
</span><span>  </span><span>|</span><span>
</span><span>  </span><span>=</span><span> </span><span>note</span>: <span>`</span><span>#[deny(unconditional_panic)]</span><span>`</span><span> </span><span>on</span><span> </span><span>by</span><span> </span><span>default</span><span>
</span></code></pre></div><p>I don’t even know if I should call this a nit or just straight up be impressed. Did Rust <em>evolve</em> to the point the <em>book</em> can no longer trick me into making a runtime panic?! This is some straight-jacket level packaging I swear to god. Much applause.</p><hr><h4 id="_a-smol-walk-in-the-woods_"><em>A smol walk in the woods</em></h4><p>Having picked up many a good pointer during this lesson I figured I had bumped myself up a couple of notches. Maybe white-belt, double-yellow-tip or something along those lines… “<em>Let’s go for a wander</em>” I thought to myself with quiet confidence. Looking left, and then looking right, under the shelter of a single raised eye-brow I chose to descend toward the belly of Rust.</p><p><em><strong><code>Ctrl + *click*</code></strong></em></p><p>I chose a simple avenue. I chose something concrete to all beginners. I chose the pinnacle of <code>Hello_World</code>…</p><p>I chose to dive into <code>println!</code></p><p>… and dive I did. Straight into the ground after clanking my head into a hard iron post of this macro. My eyes but glimpsed Sauron directly and from then on and always, I am blind:</p><div><pre><code data-lang="rust"><span>#[macro_export]</span><span>
</span><span></span><span>#[stable(feature = </span><span>"rust1"</span><span>, since = </span><span>"1.0.0"</span><span>)]</span><span>
</span><span></span><span>#[allow_internal_unstable(print_internals, format_args_nl)]</span><span>
</span><span></span><span>macro_rules</span><span>!</span><span> </span><span>println</span><span> </span><span>{</span><span>
</span><span>    </span><span>()</span><span> </span><span>=&gt;</span><span> </span><span>(</span><span>$crate</span>::<span>print</span><span>!</span><span>(</span><span>"\n"</span><span>));</span><span>
</span><span>    </span><span>(</span><span>$($arg</span>:<span>tt</span><span>)</span><span>*</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>({</span><span>
</span><span>        </span><span>$crate</span>::<span>io</span>::<span>_print</span><span>(</span><span>$crate</span>::<span>format_args_nl</span><span>!</span><span>(</span><span>$($arg</span><span>)</span><span>*</span><span>));</span><span>
</span><span>    </span><span>})</span><span>
</span><span></span><span>}</span><span>
</span></code></pre></div><p>What in God’s sweet name is this acrid assault on all of my senses?</p><p>I am going to be honest here. There is no way in hell I will understand this macro by the end of my twentieth lesson. Will I?
I’m not sure actually. I guess there’s hope? Chapters <a href="https://doc.rust-lang.org/book/ch10-00-generics.html">10</a>, <a href="https://doc.rust-lang.org/book/ch14-00-more-about-cargo.html">14</a> and <a href="https://doc.rust-lang.org/book/ch19-00-advanced-features.html">19</a> all look they will be mandatory. <em>Hoping Intensifies … ?</em></p><hr><h4 id="expressions-versus-statements">Expressions versus Statements</h4><blockquote><p>If you add a semicolon to the end of an expression, you turn it into a statement, which will then not return a value. Keep this in mind as you explore function return values and expressions next.</p></blockquote><p>This was a mind fuck - about 10 minutes before reading this I thought to myself that semi-colons seemed optional and kind of pointless in rust. Boy was I well outside the woods.</p><p>A later thought did have me wondering though; do Rustaceans really just write expressions at the end of their getter functions or is it more common to explicitly <code>return</code>?</p><hr><blockquote><p>In Rust, the idiomatic comment style starts a comment with two slashes, and the comment continues until …</p></blockquote></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://levpaul.com/posts/rust-lesson-3-and-4/">https://levpaul.com/posts/rust-lesson-3-and-4/</a></em></p>]]>
            </description>
            <link>https://levpaul.com/posts/rust-lesson-3-and-4/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24977373</guid>
            <pubDate>Tue, 03 Nov 2020 07:46:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Nassim Taleb vs. Nate Silver: who is right about election forecasting?]]>
            </title>
            <description>
<![CDATA[
Score 70 | Comments 101 (<a href="https://news.ycombinator.com/item?id=24976175">thread link</a>) | @probe
<br/>
November 2, 2020 | http://quant.am/statistics/2020/10/11/taleb-silver-feud/ | <a href="https://web.archive.org/web/*/http://quant.am/statistics/2020/10/11/taleb-silver-feud/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
<p>Perhaps lost in the whirlwind of presidential name-calling, a lesser-known multi-year old feud has resurfaced on Twitter this election season. Nate Silver is the founder of <a href="https://fivethirtyeight.com/">FiveThirtyEight</a> and is a popular statistician frequently called upon by media members to give commentary and expertise on election forecasting. Nassim Taleb is a statistician/quant turned philosopher, perhaps most well known for authoring the book “The Black Swan”. He is second most well known for calling people names on Twitter. In this 2018 instance he seemed to take issue with FiveThirtyEight’s election forecasts, saying that <a href="https://twitter.com/nntaleb/status/1059202026184282113">“klueless Nate Silver” “doesn’t know how math works”</a>, among a host of other insults. Silver responded that Taleb was an <a href="https://twitter.com/NateSilver538/status/1062782704159256576">“intellectual-yet-idiot”</a>, an phrase coined by Taleb himself. Ouch. A litany of statisticans, mathematicians, and data scientists came out of the woodwork to take sides. Taleb himself <a href="https://twitter.com/nntaleb/status/1314902682570764288">doubled down</a> on Oct. 10, 2020, again calling Silver “totally clueless”.</p>

<p>In this article I take a look behind the mathematical premises of Taleb’s arguments, and give intuitive explanations of why or why not they hold up. In short, while Taleb’s math is sound, he still manages to miss the mark by ignoring the nuance of Silver’s forecasts.</p>

<p>Taleb’s main gripe is that forecasters change their opinion too much over time. Take a look at FiveThirtyEight’s forecast from the 2016 presidential election, where the probability of Clinton winning peaked at 90%, and hit a low of 50%. 
<img src="http://quant.am/assets/2016election.png" alt="2016 election"></p>

<p>Taleb insists that Clinton never should’ve received a probability of winning of 90%. Even if polls were heavily in favor of Clinton at the time, he says Silver should’ve taken into account the uncertainty that polls would change over the next few months leading up to the election, or the possibility of major news breaking. If Silver had factored in the “unknown unknowns” his forecast should’ve been closer to 50%. In essence, this single number should reflect all <em>current and future uncertainty</em>. Taleb constructs this argument by way of quantitative finance, which perhaps led to him and traditional statisticians talking past each other. In the following sections I step through his argument in intuitive terms.</p>


<p>A well known truth to economists, quants, and traders: if I tell you a number, I must be willing to transact at that number. If I tell you the fair value of this house is $500,000, I must be willing to buy AND sell at that price. Otherwise, the number I gave you is meaningless. Likewise, if I tell you the probability of Biden winning this election is 73%, I must be willing to pay $0.73 to make the following wager: if Biden wins I receive $1, if he loses I receive $0.</p>

<p>This is important because it turns the predicted probabilities into a tradeable financial instrument known as a binary option. If the prediction is 50%, I can buy the option at $0.50. If the prediction moves to 65%, I can now sell the option at $0.65, turning a $0.15 profit.</p>

<p>This brings us to another important principle known as the <em>no-arbitrage condition</em>. If the election predictions are accurate, there should be no way for a trader to make guaranteed money by trading this option. To give an illustrative example, let’s say that we live in an unchanging world where the probability of Candidate A winning is static at 50%. If a pollster does not report a static forecast day after day, he will create an arbitrage opportunity. We will sell when the prediction is above 50%, and buy below. 
<img src="http://quant.am/assets/arbitrage-pollster.jpeg" alt="Arbitrage condition"></p>

<p>OK, so now we’ve established that if an arbitrage condition exists, then the pollster is wrong and should not have made that prediction in the first place. Still, it’s not obvious that there’s an arbitrage condition within Silver’s predictions yet (remember, the trader does have access to an oracle, and only has the same information available to him as the pollster). There’s two more building blocks that we need in order to establish an arbitrage condition.</p>


<p>To paraphrase Taleb, if I tell you an event has a 0% chance of occurring, I cannot change my mind and tell you tomorrow it now has a 50% chance of occurring. Otherwise I shouldn’t have told you it has a 0% chance in the first place. Probability and confidence are inextricably linked, and the number a pollster predicts should encapsulate both. To go to the other extreme, if the uncertainty is extremely high (and therefore confidence low), <em>it does not matter what the polls today are saying</em>. I should give both candidates a 50% chance of winning, because I am admitting the extremely likely possibility that an external event will happen that will invalidate today’s polls. To put it in technical terms, maximum uncertainty implies maximum entropy, and the maximum entropy distribution on the [0, 1] interval is the uniform distribution, which has a mean at .5. The following figure (from <a href="https://arxiv.org/pdf/1703.06351.pdf">this paper</a>) shows the relationship between probability (x-axis) and volatility (y-axis) under a specific option pricing formulation.
<img src="http://quant.am/assets/confidence-probability.png" alt="Confidence vs volatility"></p>

<p>At this point we are suspecting something doesn’t look right with FiveThirtyEight’s predictions, as they seem to have both high volatility and high probability, which contradict each other. Where is the threshold though? How can we prove that the volatility is too high?</p>


<p>Now we’re going to go a little bit technical and show that a no-arbitrage condition was likely violated. The basic construction is as follows:</p>

<ol>
  <li>In order to satisfy the no-arbitrage condition, all information must be “priced in” into the pollster’s current prediction.</li>
  <li>Therefore, the time series of predictions must be a martingale.</li>
  <li>Martingales cannot show trending or mean-reverting behavior, therefore Silver’s predictions violated the martingale property, and therefore the no-arbitrage condition.</li>
</ol>

<p>The definition of a martingale is a stochastic process \(X_1, X_2, ... X_t\) that satisfies</p><p>

\[E[X_{t+1} | X_1, ... ,X_t] = X_t\]

</p><p>To quote <a href="https://www.researchgate.net/profile/Christopher_Wlezien/publication/344419648_Information_incentives_and_goals_in_election_forecasts/links/5f73c994a6fdcc0086484861/Information-incentives-and-goals-in-election-forecasts.pdf">Andrew Gelman</a>,</p>

<blockquote>
  <p>In non-technical terms, the martingale property says that knowledge of the past will be of no use in predicting the future…One implication of this is
that it should be unlikely for forecast probabilities to change too much during the campaign (Taleb, 2017). Big events can still lead to big changes in the forecast: for example, a series of polls with Biden or Trump doing much better than before will translate into an inference that public opinion has shifted in that candidate’s favor. The point of the martingale property is not that this cannot happen, but that the possibility of such shifts should be anticipated in the model, to an amount corresponding to their prior probability. If large opinion shifts are allowed with high probability, then there should be a correspondingly wide uncertainty in the vote share forecast a few months before the election, which in turn will lead to win probabilities closer to 50%.</p>
</blockquote>

<p>In other words, all information is already priced into the current market. If it were not so, a trader could make money by taking advantage of the information that is not priced in already. So last thing we need to check: is it likely that Silver’s predictions have the martingale property? It is not in dispute that the answer is no…it shows clear mean-reversion behavior and can be validated by a statistical test of the martingale hypothesis (for example, a <a href="http://www.planchet.net/EXT/ISFA/1226.nsf/9c8e3fd4d8874d60c1257052003eced6/35822efeb009804cc1257afe006b0063/$FILE/11park.pdf">Kolmogorov-Smirnov test</a>). It seems that Taleb’s math is sound here. So where did he go wrong?</p>


<p>I believe Nate Silver is answering a subtly different question with his election forecasts. Each data point that Silver produces is answering the question: <em>if the election were to happen today</em>, what is the probability of each candidate winning? I argue that this is a valid and useful formulation. To put it slightly differently: if the question is “Who will win the election on Nov. 3?”, which of the following answers is more satisfying?</p>

<ul>
  <li>“If nothing else changes between now and the election, Joe Biden has a 85% chance of winning.” (Silver’s argument)</li>
  <li>“I dunno, anything could happen between now and the election, I give neither candidate chances much more than 50%.” (Taleb’s argument)</li>
</ul>

<p>It is a valid criticism that perhaps Silver is not very clear on explaining what his numbers represent, and therefore the media misreports his predictions. Still, I wager that most people would find the first answer more useful. In this interpretation, the “financial instrument” is a binary option that expires every day. Thefore the time series of Silver’s predictions is not interpretable as a martingale, as it strings together the price of a completely different instrument every day.</p>

<p>It is also a valid criticism that Silver’s predictions prior to Nov. 3 mean absolutely nothing, whereas in the Taleb formulation it has a natural interpretation as the betting odds for each candidate. Silver has explicitly stated that he only judges his models based on his finalized prediction. To that end, his models are extremely well calibrated, i.e., when he says something has a 50% chance of happening it actually does happen 50% of the time.
<img src="http://quant.am/assets/538-calibration.png" alt="538 calibration"></p>

<p>In conclusion, Taleb and Silver should be having a philosophical debate on what pollsters’ numbers actually mean, and stay away from the useless distraction of calling each other names on Twitter.</p>

<h3 id="update-10122020">Update (10/12/2020)</h3>
<p>Andrew Gelman, Aubrey Clayton, Dhruv Madeka and many other statisticians respond and give their thoughts: <a href="https://statmodeling.stat.columbia.edu/2020/10/12/more-on-martingale-property-of-probabilistic-forecasts-and-some-other-issues-with-our-election-model/">https://statmodeling.stat.columbia.edu/2020/10/12/more-on-martingale-property-of-probabilistic-forecasts-and-some-other-issues-with-our-election-model/</a></p>

		</div></div>]]>
            </description>
            <link>http://quant.am/statistics/2020/10/11/taleb-silver-feud/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24976175</guid>
            <pubDate>Tue, 03 Nov 2020 04:04:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[EME, CDM, AES, CENC, and Keys – Building Blocks of DRM]]>
            </title>
            <description>
<![CDATA[
Score 78 | Comments 19 (<a href="https://news.ycombinator.com/item?id=24975487">thread link</a>) | @jayjohn436
<br/>
November 2, 2020 | https://ottverse.com/eme-cenc-cdm-aes-keys-drm-digital-rights-management/ | <a href="https://web.archive.org/web/*/https://ottverse.com/eme-cenc-cdm-aes-keys-drm-digital-rights-management/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<figure>
<img src="https://i0.wp.com/ottverse.com/wp-content/uploads/2020/08/eme-cdm-cenc-featured-image.png?resize=678%2C381&amp;ssl=1" alt="eme cdm cenc keys" title="eme-cdm-cenc-featured-image" data-src="https://i0.wp.com/ottverse.com/wp-content/uploads/2020/08/eme-cdm-cenc-featured-image.png?resize=678%2C381&amp;ssl=1" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">
</figure>


<p><strong>Anyone trying to understand DRM (Digital Rights Management) will be confronted with acronyms such as AES, CDM, CENC, EME, etc. This can get very confusing for a newcomer, but understanding them is important to get a good understanding of DRM. In this article, let’s take a gentle tour of the building blocks of DRM:- EME, CDM, AES, CENC, and the use of Keys &amp; Key Servers.</strong></p>








<h2 id="simplified-architecture-of-a-drm-system"><span id="Simplified_Architecture_of_a_DRM_System"></span>Simplified Architecture of a DRM System<span></span></h2>



<p>As we saw&nbsp;<a href="https://ottverse.com/what-is-drm-digital-rights-management/">in the previous article</a>,&nbsp;<strong>DRM is a combination of encryption and business rules to control access and consumption of digital content.</strong></p>



<p>Simply put, DRM is a system that,</p>



<ul><li>provides the tools and infrastructure to enable a content provider to encrypt their content, and</li><li>build an ecosystem around the encrypted content so that the content provider can control who/what can decrypt and consume their content.</li></ul>



<p><a href="https://ottverse.com/what-is-drm-digital-rights-management/">In the previous article of the series</a>, we saw Ram and Shyam sending coded messages to each other. At the same time, Hari maintained the codebooks and decided who got to read/write the notes – remember?</p>



<figure><img data-attachment-id="156" data-permalink="https://ottverse.com/with-drm/" data-orig-file="https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/with-drm.png?fit=1316%2C878&amp;ssl=1" data-orig-size="1316,878" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="with-drm" data-image-description="" data-medium-file="https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/with-drm.png?fit=300%2C200&amp;ssl=1" data-large-file="https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/with-drm.png?fit=1024%2C683&amp;ssl=1" loading="lazy" width="1024" height="683" src="https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/with-drm.png?resize=1024%2C683&amp;is-pending-load=1#038;ssl=1" alt="aes-cenc-cdm-eme-keys" data-recalc-dims="1" data-lazy-srcset="https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/with-drm.png?resize=1024%2C683&amp;ssl=1 1024w, https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/with-drm.png?resize=300%2C200&amp;ssl=1 300w, https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/with-drm.png?resize=768%2C512&amp;ssl=1 768w, https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/with-drm.png?resize=1200%2C801&amp;ssl=1 1200w, https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/with-drm.png?w=1316&amp;ssl=1 1316w" data-lazy-sizes="(max-width: 1000px) 100vw, 1000px" data-lazy-src="https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/with-drm.png?resize=1024%2C683&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Now, let’s take this simple system and replace it with the technology needed to secure and distribute video. What do we get?</p>



<figure><img data-attachment-id="138" data-permalink="https://ottverse.com/step-0/" data-orig-file="https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0.png?fit=1396%2C818&amp;ssl=1" data-orig-size="1396,818" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="step-0" data-image-description="" data-medium-file="https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0.png?fit=300%2C176&amp;ssl=1" data-large-file="https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0.png?fit=1024%2C600&amp;ssl=1" loading="lazy" width="1024" height="600" src="https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0.png?resize=1024%2C600&amp;is-pending-load=1#038;ssl=1" alt="aes-cenc-cdm-eme-keys" data-recalc-dims="1" data-lazy-srcset="https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0.png?resize=1024%2C600&amp;ssl=1 1024w, https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0.png?resize=300%2C176&amp;ssl=1 300w, https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0.png?resize=768%2C450&amp;ssl=1 768w, https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0.png?resize=1200%2C703&amp;ssl=1 1200w, https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0.png?w=1396&amp;ssl=1 1396w" data-lazy-sizes="(max-width: 1000px) 100vw, 1000px" data-lazy-src="https://i2.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0.png?resize=1024%2C600&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Let’s describe what we have here. There is a movie that we want to send to an authenticated user securely.</p>



<p>So,</p>



<ol><li>we ask a DRM company’s server for a codebook to encrypt our video,</li><li>then, we encrypt the video using that codebook</li><li>we send the movie to the user.</li><li>the user then asks the DRM company’s server for the codebook to unlock the video (decrypt it)</li><li>and then he watches the movie!</li></ol>



<p>Fantastic!</p>



<p>Is this all there is to know about DRM for video?</p>



<p>Nope! What we have here is a simple, toy-example of how to transfer movies securely using DRM. It captures the essence of DRM perfectly but wouldn’t work well in the real world.</p>



<p>In the rest of this article, let’s take each piece of this simple system, re-think it, re-design it, and see how it fits within the world of video delivery and DRM, shall we?</p>



<h2 id="step-0-lets-move-to-adaptive-bitrate-streaming"><span id="Step_0_Let%E2%80%99s_Move_to_Adaptive_Bitrate_Streaming"></span>Step 0: Let’s Move to Adaptive Bitrate Streaming<span></span></h2>



<p>Before we talk about the order, let’s modify our example to suit the ABR (<strong>A</strong>daptive&nbsp;<strong>B</strong>it<strong>R</strong>ate) model of video delivery.</p>



<p><strong>ABR Refresher:</strong>&nbsp;in ABR, a movie is encoded into different bitrate-resolution combinations&nbsp;<em>(a.k.a ladder)</em>&nbsp;and then split into&nbsp;<strong>chunks or segments</strong>. Each chunk represents a few seconds of video and it is independently decodable.</p>



<p><strong>“Packaging”</strong>&nbsp;refers to chunking or breaking up a movie into small pieces and describing it in a manifest or playlist document. When the user wants to play the movie, he needs to refer to this manifest.</p>



<p>Depending on the available bandwidth, the player requests a chunk/segment of a particular bitrate&nbsp;<em>(rendition, or rung of the ladder)</em>&nbsp;and a CDN (Content Delivery Network) responds with the requested chunk.</p>



<p>Popular methods of video delivery using ABR are MPEG DASH and HLS. For a deeper understanding, please refer to our articles on&nbsp;<a href="https://ottverse.com/what-is-ott-video-streaming/">OTT</a>&nbsp;and&nbsp;<a href="https://ottverse.com/what-is-abr-video-streaming/">ABR</a>&nbsp;video streaming.</p>



<p>Let’s change our block digram to reflect ABR video delivery.</p>



<figure><img data-attachment-id="139" data-permalink="https://ottverse.com/step-0-with-abr/" data-orig-file="https://i0.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0-with-abr.png?fit=1476%2C868&amp;ssl=1" data-orig-size="1476,868" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="step-0-with-abr" data-image-description="" data-medium-file="https://i0.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0-with-abr.png?fit=300%2C176&amp;ssl=1" data-large-file="https://i0.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0-with-abr.png?fit=1024%2C602&amp;ssl=1" loading="lazy" width="1024" height="602" src="https://i0.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0-with-abr.png?resize=1024%2C602&amp;is-pending-load=1#038;ssl=1" alt="aes-cenc-cdm-eme-keys" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0-with-abr.png?resize=1024%2C602&amp;ssl=1 1024w, https://i0.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0-with-abr.png?resize=300%2C176&amp;ssl=1 300w, https://i0.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0-with-abr.png?resize=768%2C452&amp;ssl=1 768w, https://i0.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0-with-abr.png?resize=1200%2C706&amp;ssl=1 1200w, https://i0.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0-with-abr.png?w=1476&amp;ssl=1 1476w" data-lazy-sizes="(max-width: 1000px) 100vw, 1000px" data-lazy-src="https://i0.wp.com/ottverse.com/wp-content/uploads/2020/10/step-0-with-abr.png?resize=1024%2C602&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>The only changes here are the packaging and CDN-based delivery steps. That’s all.</p>



<p>Okay, let’s move on and start with the encryption process.</p>



<h2 id="step-1-video-encryption"><span id="Step_1_Video_Encryption"></span>Step 1: Video Encryption<span></span></h2>



<p>The whole idea of encryption is to ensure that when someone intercepts our data, they should not read it or watch it in the case of video.</p>



<p><strong>Encryption refresher:</strong>&nbsp;–&nbsp;<em>encryption is a technique used to keep data confidential and prevent unauthorized people from reading it. Encryption uses a “key” to convert input data (plaintext) into an alternate form called ciphertext. It is almost impossible to convert the ciphertext back to plaintext without the key.</em></p>



<p><em>However, practically speaking, decryption without the key is possible, and encryption algorithms are designed make reverse-engineering extremely expensive – in terms of time, money, and computing resources needed.</em></p>



<p>One of the most popular encryption techniques is the “Advanced Encryption Standard” or “AES” for short. It is also called Rijndael (after its inventor) and was established by the U.S. National Institute of Standards and Technology (NIST) in 2001 to encrypt electronic data.</p>



<p>Some important points to remember about AES:-</p>



<ul><li>It’s a&nbsp;<strong>symmetric-key algorithm</strong>: encryption and decryption are performed using the same key.</li><li>It has three variants based on the key-length: 128, 192, and 256 bits. The longer the key, the harder it is to crack.</li><li>Cracking the AES-128 without the key would require a “billion times a billion years” and a super-computer (<a href="https://www.eetimes.com/how-secure-is-aes-against-brute-force-attacks/" target="_blank" rel="noopener">source</a>).</li></ul>



<p>If you are interested in going deep into the AES standard, look at the&nbsp;<a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard" target="_blank" rel="noopener">AES’s Wikipedia page</a>.&nbsp;<em>I am not an expert in cryptography and won’t be able to do justice to the AES.</em></p>



<p><strong>Note:</strong>&nbsp;Please remember that&nbsp;<strong>encryption is not encoding, and decryption is not decoding in the video space</strong>. For videos, encoding and decoding are words used to refer to compression and decompression, respectively. To learn more about encoding, decoding, and video codecs, please read our articles on&nbsp;<a href="https://ottverse.com/need-for-video-compression/">the need for compression</a>&nbsp;and a&nbsp;<a href="https://ottverse.com/what-is-a-video-codec/">simple introduction to video codecs</a>.</p>



<h3 id="is-aes-128-the-only-encryption-technique"><span id="Is_AES128_The_Only_Encryption_Technique"></span>Is AES-128 The Only Encryption Technique?<span></span></h3>



<p>No, it isn’t, and let’s think about the implication of this for a minute.</p>



<p>If a content provider decides to engage with three different DRM companies, and all three use different encryption techniques, then it means that the content provider needs to encrypt their videos three times, resulting in a waste of storage space and other resources.</p>



<p>That is why the CENC specification came into being – to reduce this encryption-driven fragmentation of the market and to reduce storage requirements.</p>



<p>Let’s learn about this next.</p>



<h3 id="cenc-or-common-encryption"><span id="CENC_or_Common_Encryption"></span>CENC or Common Encryption<span></span></h3>



<p><strong>Actually, before we dive into CENC, let’s step back and take a look at the state of OTT streaming protocols and CMAF in particular.</strong></p>



<p>There are primarily two protocols in use today – MPEG-DASH and HLS.&nbsp;<em>There are others such as MSS (Microsoft Smooth Streaming) and HDS, but, we’ll leave them aside for this discussion.</em></p>



<p>MPEG-DASH uses the&nbsp;<code>mp4</code>&nbsp;container format for its videos and HLS uses the MPEG-TS (<code>ts</code>) container for its files. If a content provider uses both MPEG-DASH and HLS, then they need to store a copy of their videos in both&nbsp;<code>mp4</code>&nbsp;and&nbsp;<code>ts</code>&nbsp;file formats.</p>



<p>Now, let’s add the DRM encryption problem to it. If our three hypothetical DRM providers use three different encryption standards, then a content providers needs to store&nbsp;<code>2 * 3</code>&nbsp;… six copies of each video! What a waste of storage space!!</p>



<p><strong>To combat the first problem posed by video streaming protocols, the&nbsp;<a href="https://mpeg.chiariglione.org/standards/mpeg-a/common-media-application-format" target="_blank" rel="noopener">CMAF</a>&nbsp;specification was created</strong> which said that videos can be stored in the&nbsp;<strong>fragmented mp4</strong>&nbsp;container format (<code>fmp4</code>). With support from both MPEG-DASH and HLS, you can now create only one set of videos, store it in&nbsp;<code>fmp4</code>&nbsp;format, and use a common set of files for both protocols. </p>



<p><strong>Just make sure you create two manifests (sigh!).</strong></p>



<h3><span id="How_About_Unifying_the_Encryption"></span><strong>How About Unifying the Encryption?</strong><span></span></h3>



<p>We still need to store multiple copies of each file if different DRM technologies use different encryption standards, right?</p>



<p>For this purpose, the MPEG developed the&nbsp;<a href="https://www.iso.org/standard/68042.html" target="_blank" rel="noopener">CENC or Common Encryption specification</a>, specifying that videos can be encrypted using either&nbsp;<code>cenc</code>&nbsp;(AES-128 CTR) or&nbsp;<code>cbcs</code>&nbsp;(AES-128 CBC).&nbsp;<em>CTR stands for Counter; and CBC stands for Cipher Block Chaining.</em></p>



<p>The implication of CENC is that a content provider needs to encrypt his videos only once and any decryption module can decrypt it.&nbsp;<em>Note: Exposing the encryption algorithm is not a problem as long as the keys are strongly protected.</em></p>



<p><strong>Well, CENC might sound like a magic wand for DRM-unification, but it is not.</strong></p>



<p>There are three primary DRM technologies in the market – Apple FairPlay, Google Widevine, and Microsoft PlayReady.</p>



<ul><li>Apple FairPlay supports only AES-CBC&nbsp;<code>cbcs</code>&nbsp;mode.</li><li>HLS supports only AES-CBC&nbsp;<code>cbcs</code>&nbsp;mode (irrespective of CMAF)</li><li>Widevine and PlayReady support both AES-128 CTR&nbsp;<code>cenc</code>&nbsp;or AES-128 CBC&nbsp;<code>cbcs</code>&nbsp;modes.</li><li>MPEG-DASH with CMAF supports both AES-128 CTR&nbsp;<code>cenc</code>&nbsp;or AES-128 CBC&nbsp;<code>cbcs</code>&nbsp;modes.</li><li>MPEG-DASH without CMAF supports only AES-128 CTR&nbsp;<code>cenc</code>&nbsp;mode.</li></ul>



<p>As you can see, the CMAF and CENC specs have lead to confusion and fragmentation in the streaming space. </p>



<p><strong>A possible convergence point is the universal use of CMAF and AES-CBC&nbsp;cbcs&nbsp;mode, but, how will these impact legacy devices that support only CTR or only MPEG-TS?</strong></p>



<p>That’s a discussion for another time.</p>



<h2 id="step-2-key-keyid-and-the-license-server"><span id="Step_2_Key,_KeyID,_and_the_License_Server"></span>Step 2: Key, KeyID, and the License Server<span></span></h2>



<p>By now, we have established that we will be encrypting or videos using AES-128 bit encryption. At this stage, a few questions that come up are –</p>



<ol><li>Where do we get the AES-128 Encryption Keys?</li><li>How do we associate an Encryption Key with a movie?</li><li>Where do we store the Encryption Keys?</li></ol>



<p>Let’s answer them one at a time.</p>



<h3 id="where-do-we-get-the-aes-128-bit-encryption-keys"><span id="Where_do_we_get_the_AES128_bit_encryption_keys"></span>Where do we get the AES-128 bit encryption keys?<span></span></h3>



<p>Any content provider can generate the encryption keys manually using specialized software. Alternatively, several DRM vendors provide the necessary tools and software to generate these keys.</p>



<h3 id="how-do-we-associate-an-encryption-key-with-a-movie"><span id="How_do_we_associate_an_encryption_key_with_a_movie"></span>How do we associate an encryption key with a movie?<span></span></h3>



<p>Let’s understand the “why” first. When you go to a hotel, you ask the receptionist for the keys to a particular room by mentioning the room number – right? You’re providing the association here between a key and a room by telling her the room number.</p>



<p>Similarly, when we encrypt a movie with a particular key, we need to create that association and provide that to the DRM license server&nbsp;<em>(our receptionist, if you will)</em>.</p>



<p>In DRM, a “<strong>KeyID</strong>” provides the association between an encryption key and a movie. It is a&nbsp;<strong>unique</strong>&nbsp;string of characters generated at the time of creating an encryption key for a particular movie.</p>



<p><em>And finally,</em></p>



<h3 id="where-do-we-store-the-encryption-key--its-keyid"><span id="Where_do_we_store_the_Encryption_Key_its_KeyID"></span>Where do we store the Encryption Key &amp; its KeyID?<span></span></h3>



<p><strong>The Encryption Key and the KeyID are stored in a secure server (Key Store) that works alongside a DRM license server</strong>.</p>



<p>When a client needs to play an encrypted movie, it requests the DRM license server for the decryption key by providing that …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ottverse.com/eme-cenc-cdm-aes-keys-drm-digital-rights-management/">https://ottverse.com/eme-cenc-cdm-aes-keys-drm-digital-rights-management/</a></em></p>]]>
            </description>
            <link>https://ottverse.com/eme-cenc-cdm-aes-keys-drm-digital-rights-management/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24975487</guid>
            <pubDate>Tue, 03 Nov 2020 01:38:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Sales as a Core Competency in Your Company]]>
            </title>
            <description>
<![CDATA[
Score 18 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24975300">thread link</a>) | @neinasaservice
<br/>
November 2, 2020 | https://21-lessons.com/sales-as-a-core-competency-in-your-company/ | <a href="https://web.archive.org/web/*/https://21-lessons.com/sales-as-a-core-competency-in-your-company/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1361">
			
		<!-- .entry-media -->
	

	<div>

		<!-- .entry-header -->

		<div>
			
<p>Currently, you might not be actively selling (as in approaching leads). Or the product sells itself right now (people get in touch with you and buy).&nbsp;</p>



<p>You feel weird about cold-calling, approaching strangers about your offerings. It’s a valid concern.&nbsp;</p>



<p>When you look at your current customer base and revenues: Can you predict when you make a sale? Can you be confident if somebody will follow through with the purchase?&nbsp;</p>



<p>If you don’t have Sales People on staff, this is a challenge. Why should you care, though?</p>



<p>For starters, you might need to plan revenue for the next few months, to hire a new employee, or invest in that new project you’ve been anxious to kick off.</p>



<p>Whatever the motivation is in the end, you need to predict incoming revenue. And for that, you need to sell. As always, there are multiple approaches to this.</p>



<p>As a starting point, you can start to work off all inbound sales inquiries. Your Advantage: No cold outreach to anyone. You focus solely on incoming requests and work them off.&nbsp;</p>



<p>You increase your odds of closing deals with a well-defined sales process. A sales process helps you confidently walk a prospect through each step and increase the chance to become a paying customer.</p>



<p>It still might not allow you to increase revenue as you need it, but you can more effectively predict incoming revenue. This circumstance is already worth a lot because it provides you with a lot more financial stability.&nbsp;</p>



<p>Another circumstance should also make this process more comfortable for you: You are already selling to those who gave you permission. You are allowed to sell. These prospects got in touch with you because they need something from you. Now it’s on you to professionally handle the request and walk them through the process.</p>



<p>With this approach, you’re slowly building Sales Competency in your organization for a more stable revenue foundation.</p>

					</div><!-- .entry-content -->

		
			</div><!-- .entry-inner -->
</article></div>]]>
            </description>
            <link>https://21-lessons.com/sales-as-a-core-competency-in-your-company/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24975300</guid>
            <pubDate>Tue, 03 Nov 2020 01:07:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Leaving OCaml]]>
            </title>
            <description>
<![CDATA[
Score 191 | Comments 117 (<a href="https://news.ycombinator.com/item?id=24974907">thread link</a>) | @rbanffy
<br/>
November 2, 2020 | https://blog.darklang.com/leaving-ocaml/ | <a href="https://web.archive.org/web/*/https://blog.darklang.com/leaving-ocaml/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://blog.darklang.com/content/images/size/w300/2020/11/skeleton-camel.jpg 300w,
                            https://blog.darklang.com/content/images/size/w600/2020/11/skeleton-camel.jpg 600w,
                            https://blog.darklang.com/content/images/size/w1000/2020/11/skeleton-camel.jpg 1000w,
                            https://blog.darklang.com/content/images/size/w2000/2020/11/skeleton-camel.jpg 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://blog.darklang.com/content/images/size/w2000/2020/11/skeleton-camel.jpg" alt="Leaving OCaml">
            </figure>

            <section>
                <div>
                    <p>I built the first demo of Dark in Python, in about two weeks. A few months later when I started productizing it, I rebuilt it in OCaml. Back in 2017, when I was considering the language and platform to use for Dark, OCaml was extremely compelling:</p><ul><li>it's a high-level language with static types, so easy to make large scale changes as we figure out what the language/product was</li><li>you mostly model data with sum types, which in my mind are the best way to model data</li><li>it's very similar to the language I wanted to build (in particular, we could reuse built-in immutable data structures for Dark's values)</li><li>it had a reputation for being high-performance, which meant that we could write an interpreter for Dark and not have it be terribly slow (vs writing an interpreter in python, which might be too slow)</li></ul><p>Unfortunately, as we've built Dark we've run into significant problems that have made it challenging to build in OCaml.</p><h2 id="lack-of-libraries">Lack of libraries</h2><p>When you bet on an off-mainstream language, one of the things you accept is that many libraries are not going to be available. When there is a small community, often there aren't enough people working in the language to make important libraries. This is especially true if few people are building business applications.</p><p>In OCaml there are many high quality libraries, especially for data structures and data manipulation. The annual<a href="https://opensource.janestreet.com/core/"> Jane Street code dump</a> has been quite useful and very high quality. However, we really felt the lack of several libraries. The most obvious of these is that we had to build a <a href="https://github.com/darklang/dark/blob/main/backend/libexecution/unicode_string.mli">Unicode string library</a> ourselves (built on top of the <a href="https://erratique.ch/software/uuseg">very impressive OCaml Unicode libraries</a> built by <a href="https://erratique.ch/contact.en">Daniel Bünzli</a>), but we needed many more libraries than that.</p><p>The lack of an SDK for Google Cloud has affected us greatly. When you're searching for product-market fit, you do the simplest, easiest thing. If you lack a good SDK for your cloud provider, the simplest, easiest thing is often a terrible architectural choice. We've built our own queue on top of our database rather than using the production-quality cloud queues available on GCP. Similarly, we barely use the Cloud Storage (GCP's version of S3), because we initially put things in the database <a href="https://blog.darklang.com/evolving-darks-tracing-system/">because it was easier</a>. We've built 3 services, 2 <a href="https://github.com/darklang/dark/tree/main/containers/stroller">in</a> <a href="https://github.com/darklang/dark/tree/main/containers/queue-scheduler">Rust</a>, and 1 in <a href="https://github.com/darklang/dark/tree/main/containers/postgres-honeytail">Go</a>, to workaround the challenges we've faced.</p><p>The biggest challenge here is our use of Postgres. Postgres is a great database and we're big fans, but Cloud SQL is not a great hosted database. GCP's position is that Cloud SQL is there to tick a box and we should be using Cloud Spanner. I would love to switch to Cloud Spanner, but we have no driver for it in OCaml. Given the Postgres driver in OCaml is not particularly mature, it's hard to expect that a Cloud Spanner driver would exist, and indeed it doesn't. We've had to contribute to the <a href="https://github.com/mmottl/postgresql-ocaml/commit/81a4ae5240decd8f483a90568257cfbc1558c7ed">OCaml Postgres driver</a>, and some parts of our codebase have been <a href="https://github.com/darklang/dark/blob/main/backend/libbackend/serialize.ml#L226">well and truly mangled</a> when working around features not supported in that driver.</p><p>We've also suffered from a lack of a high-level, production web stack (there are <a href="https://github.com/anmonteiro/ocaml-h2">low-level stacks with good reputations</a> that I've struggled to use, and a <a href="https://github.com/oxidizing/sihl">few</a> <a href="https://github.com/reason-native-web/morph">new</a> ones out there that look good), in particular lacking a user authentication module. We've been using <a href="https://auth0.com/">Auth0</a> to work around this for now, which has more moving pieces than I'd like, and a shockingly high cost (our 7000 users, most of whom never log in, costs us over $500/mo).</p><p>We've worked around other missing vendor SDKs by calling their HTTP endpoints directly and that's been mostly fine. However, for libraries like encryption we don't have that option - we <a href="https://github.com/darklang/dark/pull/1455/files">hacked around a missing encryption library</a>, but decided not to ship it to production until we audited it for security (which was never actually worth the cost).</p><p>At CircleCI, we bet on Clojure. That was also a non-mainstream language, but its ability to call Java SDKs meant we had a mature cloud library, which was essential for building CircleCI. Of course, in OCaml we could call C libraries (and <a href="https://github.com/darklang/dark/pull/1841">even Rust libraries</a>, perhaps), but it doesn't match having native libraries we can call directly.</p><h2 id="learnability">Learnability</h2><p>I'm mostly in the camp that anyone can learn any language, but I saw a team struggle with OCaml, and for good reason. Language tutorials are extremely poor in OCaml compared to other languages; they're mostly lecture notes from academic courses.</p><p>The compiler isn't particularly helpful, certainly compared to Rust or Elm (both of which have been in our stack at one point). Often it gives no information about an error. Syntax errors typically say "Syntax error"; though it will try to give a good error for a mismatched brace, often incorrectly. Type errors can be a real burden to read, even after 3 years of experience with it.</p><p>The docs in OCaml are often challenging to find. The <a href="https://ocaml.janestreet.com/ocaml-core/latest/doc/base/index.html">Jane Street docs</a> have improved significantly in the last few years, but it can be a challenge to even figure out what functions are available in a particular module for most libraries. Compare to the excellent <a href="https://docs.rs/">docs.rs</a> in Rust, which has comprehensive API docs for every package in Rust.</p><p>One of the ways I personally struggled in OCaml is around <code>Lwt</code>. Lwt is (one of!) OCaml's async implementations. I couldn't figure it out several years ago and so just built a single-threaded server. The amount of workarounds and downtime we've suffered from that single decision is immense. A tutorial around building high-performance (or even medium performance!) web servers would be very valuable. </p><p>Tooling is something I read would be good in OCaml. I remember reading there was a debugger that could go back in time! I don't know where that's gone but I've never heard of anyone using it.</p><p>We have struggled to make editor tooling work for us. This is partially because we also use ReasonML and this seems to break things. Unfortunately, this is common in programming, but even more so in small communities: you might be the first person to ever try to use a particular configuration.</p><p>Finally, the disconnect between the various tools is immense. You need to understand Opam, Dune, and Esy, to be able to get something working (you could also do it without Esy and just rely on Opam, but that's much worse). I talked about a bunch of these challenges <a href="https://blog.darklang.com/first-thoughts-on-rust-vs-ocaml/">here</a>.</p><h2 id="language-problems">Language problems</h2><p>Multicore is coming Any Day Now™️, and while this wasn't a huge deal for us, it was annoying. </p><h2 id="minor-annoyances">Minor annoyances</h2><p>One of my biggest annoyances was how often OCaml folks talk about Fancy Type System problems, instead of how to actually build products and applications. In other communities for similar languages (ReasonML, Elm, F#), people talk about building apps and solving their problems. In OCaml, it feels like people spend an awful lot of time discussing Functors. It's not quite at the level that I perceived in the Haskell world, but it pointed out that the people building the core of the ecosystem do not have the same problems that I do (which is building web-y stuff).</p><p>I honestly think OCaml was a great choice at the start. Being able to quickly and safely make large-scale changes to your app is something that staticly-typed functional languages excel at. I'm happy that we made the choice, and in retrospect, it still seems like the best choice of those we had at the time.</p><p>I'm working on building the next version of the backend. We have about 20k lines to be replaced, and they'll be rewritten in a new language while keeping the semantics the same. I plan to leave keep the frontend in ReasonML: it doesn't suffer from the same library problems as it can interface nicely to JS, and it's nearly 50k lines of code so it would be a much bigger undertaking.</p><p>Read <a href="https://blog.darklang.com/new-backend-fsharp/">the followup</a> to see what we picked!</p><hr><p><em><em>You can sign up for Dark </em></em><a href="https://darklang.com/signup" rel="noopener nofollow"><em><em>here</em></em></a><em><em>. For more info on Dark, follow our </em></em><a href="https://blog.darklang.com/rss" rel="noopener nofollow"><em><em>RSS</em></em></a><em><em>, follow </em></em><a href="https://twitter.com/darklang" rel="noopener nofollow"><em><em>us</em></em></a><em><em> (or </em></em><a href="https://twitter.com/paulbiggar" rel="noopener nofollow"><em><em>me</em></em></a><em><em>) on Twitter, join our </em></em><a href="https://darklang.com/slack-invite" rel="noopener nofollow"><em><em>Slack Community</em></em></a><em><em>, or watch our </em></em><a href="https://github.com/darklang/dark" rel="noopener nofollow"><em><em>GitHub repo</em></em></a><em><em>.</em></em></p>
                </div>
            </section>



        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://blog.darklang.com/leaving-ocaml/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24974907</guid>
            <pubDate>Tue, 03 Nov 2020 00:03:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[DKIM: Show Your Privates]]>
            </title>
            <description>
<![CDATA[
Score 90 | Comments 42 (<a href="https://news.ycombinator.com/item?id=24972609">thread link</a>) | @ryan-c
<br/>
November 2, 2020 | https://rya.nc/dkim-privates.html | <a href="https://web.archive.org/web/*/https://rya.nc/dkim-privates.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody"> <p>If you’re like most people, there’s a good chance that it’s been years since you’ve sent an email that wasn’t cryptographically signed. You don’t use PGP, you say? Well, even if <em>you</em> are not signing your email, your provider is almost certainly doing it for you. Plausible deniability has been tossed aside in the name of stopping spam, but it doesn’t have to be.</p> <p><span data-title="DomainKeys Identified Mail"><abbr title="DomainKeys Identified Mail">DKIM</abbr></span>, originally standardized in 2007 by <a href="http://tools.ietf.org/html/rfc4871.html">RFC 4871</a>, now has near universal<a href="#id4" id="id1">[1]</a> adoption. To quote the RFC, the goal behind the protocol is to “permit a signing domain to assert responsibility for a message, thus protecting message signer identity and the integrity of the messages they convey”. It’s one of several technologies used prevent the sender identity information in email from being spoofed<a href="#id5" id="id2">[2]</a>. Anti-spam systems use it to help determine whether to consider the reputation of a domain name when making a processing decision.</p> <p>While <abbr title="DomainKeys Identified Mail">DKIM</abbr> was designed to be useful for spam prevention, the cryptographic signatures it uses have quietly made a property called “<a href="https://en.wikipedia.org/wiki/Non-repudiation">non-repudiation</a>” the new normal for email. The term is used in in contract law — for example if someone claims “that’s not my signature”, they could be said to be “repudiating” the authenticity of the document. In the case of email, the impact is that if you have a copy of an email in its original format including full headers (for example, an email spool dump) you can check the signature. The extent to which this is a reliable means of verification varies depending on the circumstances — keys short enough to be cracked used to be common, and in some cases straight-up theft of the private keys is plausible.</p> <p>Meanwhile, secure messaging tools like <a href="https://en.wikipedia.org/wiki/Off-the-Record_Messaging">OTR</a> and its successors have taken the approach of explicitly providing “deniable encryption”. The <a href="https://signal.org/blog/simplifying-otr-deniability/#potential-simplifications-and-improvements">state of the art</a> allows a sender, given a recipient’s public key, to craft a fake transcript apparently between the two of them that will pass cryptographic checks. This is generally fine for users of these apps because they know what they said. To the best of my knowledge, there is nowhere this creates a legal “get out of jail free” card. All it really does is ensure the users of these tools aren’t <em>reducing</em> their deniability by using the tool. This is an issue where <abbr title="DomainKeys Identified Mail">DKIM</abbr> really fails its users, and I’m apparently not the only one that feels this way.</p> <blockquote> <p>Apropos of nothing, I really wish Gmail would start publishing its expired DKIM secret keys.</p> <p>—<a href="https://twitter.com/matthew_d_green/status/1323011619069321216">Matthew Green</a></p> </blockquote> <p>A little over three years ago, I started doing exactly that for my domain. Since then, I’ve had a <a href="https://gist.github.com/ryancdotorg/a8f565b9e4f0902eb7b5cd4cdefeea0f">key rotation script</a> running every day, generating a new key and adding the appropriate record (called a “selector”).</p> <div><pre><span></span><code>20170829-<wbr>b29b2444f764c222c3faf5c.<wbr>_domainkey.<wbr>ryanc.<wbr>org.<wbr> 5 <wbr>IN <wbr>TXT <wbr>"<wbr>v=<wbr>DKIM1;<wbr>t=<wbr>s;<wbr>h=<wbr>sha256;<wbr>p=<wbr>MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDkOSIRW7R8a3e0J0lZqbBJSpHJYPk043/<wbr>OB3lcT2apKtnu7MLjIRqUAgRyYSVAGC10ID2Qlxmy1Ji3EBRB1qI2IsNKgC2C4qzGxx54ShpVR/<wbr>8yY9Qy1eyNtTF5Y/<wbr>XSoLWoRVO1oly+<wbr>WL+<wbr>4O2TRuyujEwoZcFUwXzuuuqJtzbI17wIDAQAB"<wbr>
</code></pre></div> <p>Each selector remains live for seven days, then is “revoked” by publishing an update blanking the public key portion of the record.</p> <div><pre><span></span><code>20170829-<wbr>b29b2444f764c222c3faf5c.<wbr>_domainkey.<wbr>ryanc.<wbr>org.<wbr> 5 <wbr>IN <wbr>TXT <wbr>"<wbr>v=<wbr>DKIM1;<wbr>t=<wbr>s;<wbr>p=<wbr>"<wbr>
</code></pre></div> <p>Once another three days pass, the minimal set of <a href="https://en.wikipedia.org/wiki/RSA_(cryptosystem)">RSA</a> parameters needed to recreate the public and private keys are published in the selector’s “notes” field.</p> <div><pre><span></span><code>20170829-<wbr>b29b2444f764c222c3faf5c.<wbr>_domainkey.<wbr>ryanc.<wbr>org.<wbr> 5 <wbr>IN <wbr>TXT <wbr>"<wbr>v=<wbr>DKIM1;<wbr>t=<wbr>s;<wbr>p=<wbr>;<wbr>n=<wbr>e:<wbr>AQAB,<wbr>p:<wbr>6o/<wbr>8upWykC5USot9Q2o5M89EO1qA7J/<wbr>ao/<wbr>FPc2TUJKat+<wbr>z4JXde2HWW/<wbr>8D3LJR4hGwSpgwLMq9drTzdjbzFTkQ=<wbr>=<wbr>,<wbr>q:<wbr>+<wbr>RTTux+<wbr>yMx0LPyXDkAQiEBcOt8xYrr60s1sXO/<wbr>5nQSQSZBlLtRJKHQpz65MnIxlOCB+<wbr>1umqLW8q78hHC3Asxfw=<wbr>=<wbr>"<wbr>
</code></pre></div> <p>The format here is non-standard, as a full RSA private key with all of the redundant data it includes would exceed the 255 character limit for strings stored in DNS<a href="#id6" id="id3">[3]</a>. A small Python script is enough to reconstitute everything, though.</p> <table><thead><tr><th colspan="2"><a href="https://rya.nc/dkim-privates_/attach/dkim-private.py" download="">dkim-private.py</a></th></tr></thead><tbody><tr><td unselectable="on">1</td><td> <code><span>import</span> <span>gmpy2</span><span>,</span> <span>sys</span><span>,</span> <span>dns.resolver</span></code> </td></tr><tr><td unselectable="on">2</td><td> <code><span>from</span> <span>Cryptodome.PublicKey</span> <span>import</span> <span>RSA</span></code> </td></tr><tr><td unselectable="on">3</td><td> <code><span>from</span> <span>base64</span> <span>import</span> <span>b64decode</span> <span>as</span> <span>b64d</span></code> </td></tr><tr><td unselectable="on">4</td><td> <code></code> </td></tr><tr><td unselectable="on">5</td><td> <code><span>def</span> <span>decode_dkim_private</span><span>(</span><span>txt</span><span>):</span></code> </td></tr><tr><td unselectable="on">6</td><td> <code>    <span>params</span> <span>=</span> <span>dict</span><span>()</span></code> </td></tr><tr><td unselectable="on">7</td><td> <code>    <span># Parse the DKIM selector record.</span></code> </td></tr><tr><td unselectable="on">8</td><td> <code>    <span>for</span> <span>key</span><span>,</span> <span>_</span><span>,</span> <span>val</span> <span>in</span> <span>map</span><span>(</span><span>lambda</span> <span>x</span><span>:</span> <span>x</span><span>.</span><span>partition</span><span>(</span><span>'='</span><span>),</span> <span>txt</span><span>.</span><span>split</span><span>(</span><span>';'</span><span>)):</span></code> </td></tr><tr><td unselectable="on">9</td><td> <code>        <span>if</span> <span>key</span> <span>==</span> <span>'n'</span><span>:</span></code> </td></tr><tr><td unselectable="on">10</td><td> <code>            <span>for</span> <span>k</span><span>,</span> <span>v</span> <span>in</span> <span>map</span><span>(</span><span>lambda</span> <span>x</span><span>:</span> <span>x</span><span>.</span><span>split</span><span>(</span><span>':'</span><span>),</span> <span>val</span><span>.</span><span>split</span><span>(</span><span>','</span><span>)):</span></code> </td></tr><tr><td unselectable="on">11</td><td> <code>                <span>params</span><span>[</span><span>k</span><span>]</span> <span>=</span> <span>int</span><span>.</span><span>from_bytes</span><span>(</span><span>b64d</span><span>(</span><span>v</span><span>),</span> <span>'big'</span><span>)</span></code> </td></tr><tr><td unselectable="on">12</td><td> <code>    <span># Compute rest of RSA keypair parameters (if possible).</span></code> </td></tr><tr><td unselectable="on">13</td><td> <code>    <span>if</span> <span>all</span> <span>(</span><span>k</span> <span>in</span> <span>params</span> <span>for</span> <span>k</span> <span>in</span> <span>(</span><span>'e'</span><span>,</span> <span>'p'</span><span>,</span> <span>'q'</span><span>)):</span></code> </td></tr><tr><td unselectable="on">14</td><td> <code>        <span>params</span><span>[</span><span>'n'</span><span>]</span> <span>=</span> <span>params</span><span>[</span><span>'p'</span><span>]</span> <span>*</span> <span>params</span><span>[</span><span>'q'</span><span>]</span></code> </td></tr><tr><td unselectable="on">15</td><td> <code>        <span>phi</span> <span>=</span> <span>(</span><span>params</span><span>[</span><span>'p'</span><span>]</span> <span>-</span> <span>1</span><span>)</span> <span>*</span> <span>(</span><span>params</span><span>[</span><span>'q'</span><span>]</span> <span>-</span> <span>1</span><span>)</span></code> </td></tr><tr><td unselectable="on">16</td><td> <code>        <span>params</span><span>[</span><span>'d'</span><span>]</span> <span>=</span> <span>int</span><span>(</span><span>gmpy2</span><span>.</span><span>invert</span><span>(</span><span>params</span><span>[</span><span>'e'</span><span>],</span> <span>phi</span><span>))</span></code> </td></tr><tr><td unselectable="on">17</td><td> <code>        <span>rsa</span> <span>=</span> <span>map</span><span>(</span><span>lambda</span> <span>x</span><span>:</span> <span>params</span><span>[</span><span>x</span><span>],</span> <span>'nedpq'</span><span>)</span></code> </td></tr><tr><td unselectable="on">18</td><td> <code>        <span>return</span> <span>RSA</span><span>.</span><span>construct</span><span>(</span><span>tuple</span><span>(</span><span>rsa</span><span>))</span></code> </td></tr><tr><td unselectable="on">19</td><td> <code>    <span>else</span><span>:</span></code> </td></tr><tr><td unselectable="on">20</td><td> <code>        <span>return</span> <span>None</span></code> </td></tr><tr><td unselectable="on">21</td><td> <code></code> </td></tr><tr><td unselectable="on">22</td><td> <code><span>if</span> <span>__name__</span> <span>==</span> <span>'__main__'</span> <span>and</span> <span>len</span><span>(</span><span>sys</span><span>.</span><span>argv</span><span>)</span> <span>==</span> <span>3</span><span>:</span></code> </td></tr><tr><td unselectable="on">23</td><td> <code>    <span>domain</span> <span>=</span> <span>sys</span><span>.</span><span>argv</span><span>[</span><span>1</span><span>]</span></code> </td></tr><tr><td unselectable="on">24</td><td> <code>    <span>selector</span> <span>=</span> <span>sys</span><span>.</span><span>argv</span><span>[</span><span>2</span><span>]</span></code> </td></tr><tr><td unselectable="on">25</td><td> <code>    <span>for</span> <span>answer</span> <span>in</span> <span>dns</span><span>.</span><span>resolver</span><span>.</span><span>query</span><span>(</span><span>selector</span> <span>+</span> <span>'._domainkey.'</span> <span>+</span> <span>domain</span><span>,</span> <span>'TXT'</span><span>):</span></code> </td></tr><tr><td unselectable="on">26</td><td> <code>        <span>txt</span> <span>=</span> <span>str</span><span>(</span><span>answer</span><span>)</span><span>.</span><span>strip</span><span>(</span><span>'"'</span><span>)</span></code> </td></tr><tr><td unselectable="on">27</td><td> <code>        <span>print</span><span>(</span><span>decode_dkim_private</span><span>(</span><span>txt</span><span>)</span><span>.</span><span>exportKey</span><span>()</span><span>.</span><span>decode</span><span>())</span></code> </td></tr></tbody></table><p>An example run:</p> <div><pre><span></span><code><span>$</span> ./dkim-private.py <span>'ryanc.org'</span> <span>'20170829-b29b2444f764c222c3faf5c'</span>
<span>-----BEGIN RSA PRIVATE KEY-----</span>
<span>MIICXQIBAAKBgQDkOSIRW7R8a3e0J0lZqbBJSpHJYPk043/OB3lcT2apKtnu7MLj</span>
<span>IRqUAgRyYSVAGC10ID2Qlxmy1Ji3EBRB1qI2IsNKgC2C4qzGxx54ShpVR/8yY9Qy</span>
<span>1eyNtTF5Y/XSoLWoRVO1oly+WL+4O2TRuyujEwoZcFUwXzuuuqJtzbI17wIDAQAB</span>
<span>AoGBAKClArD7PzExKGJcIQqHIjqEzdfVdbVfyc+JfUiX72h2bE78wzXDUIUMYnrs</span>
<span>nJ7gJeaO5ycG5ST29sQtAkVRwn1KTLaU9fYmGpbkKyOWWfmztppZIvwi9l4tU5h2</span>
<span>GJVw+HbhcWO6tYbTqR9Bc8IelXyVibwmJwImr0AoD8sBLryhAkEA6o/8upWykC5U</span>
<span>Sot9Q2o5M89EO1qA7J/ao/FPc2TUJKat+z4JXde2HWW/8D3LJR4hGwSpgwLMq9dr</span>
<span>TzdjbzFTkQJBAPkU07sfsjMdCz8lw5AEIhAXDrfMWK6+tLNbFzv+Z0EkEmQZS7US</span>
<span>Sh0Kc+uTJyMZTggftbpqi1vKu/IRwtwLMX8CQFT/ABGMlTvxzdGFYkq/fyLrBEqN</span>
<span>rRIRiuTFWIj0DHuLepgEDtjWhcN5T2f6vFYi6NQliFdU+F18ngICjCGKukECQHse</span>
<span>ClIyJpkRQB/kgLfM8zFU1FeRUDx/0z3cRq3G4C7Yr6Z+wmcsNSoJoqbMw8mblnB5</span>
<span>jBAq3dtvaFsM4G53se0CQQC9ocR9eQdXvq5ibwZAmgYcMLEaq7NeX//l6zdxLd52</span>
<span>NcVcuaAUzf5KdTRwA9gJ4Qdzwntc+UB2ElpI2AOj7AFV</span>
<span>-----END RSA PRIVATE KEY-----</span>
</code></pre></div> <p>When I originally set this up, I was a bit concerned that I’d run into issues with filtering systems trying to validate my sent emails significantly after delivery. Per the RFC:</p> <blockquote> A signer should not sign with a private key when the selector containing the corresponding public key is expected to be revoked or removed before the verifier has an opportunity to validate the signature. The signer should anticipate that verifiers may choose to defer validation, perhaps until the message is actually read by the final recipient. In particular, when rotating to a new key pair, signing should immediately commence with the new private key and the old public key should be retained for a reasonable validation interval before being removed from the key server.</blockquote> <p>In the process of writing this up, I went through the 24 months of query logs I have. With very few exceptions (most of which were probably my own testing) there were no lookups against selectors other than on the day they were being used, so this doesn’t seem to be a problem in practice.</p> <p>I alluded to it earlier, but I want to be clear — publishing <abbr title="DomainKeys Identified Mail">DKIM</abbr> private keys like this mainly addresses leaks as a threat model. In a legal dispute, mail server logs and/or stored mail can be subpoenaed if the authenticity of messages is in question. Even in my case, where I have my own mail server on dedicated hardware with full disk encryption at an undisclosed location, most mail I send will be delivered to a server operated by a third party with no incentive to alter logs at the behest of the recipient.</p> <p>It would make for a fascinating experiment for one of the privacy focused email providers to try deploying a key management strategy similar to the one I’ve described in this post.</p> <table id="id4"> <colgroup><col><col></colgroup> <tbody> <tr><td><a href="#id1">[1]</a></td><td>I can’t find any recent public data on this, but Google reported that 87.6% of non-spam emails received by Gmail users had valid DKIM signatures as of Febuary 2016. <a href="https://security.googleblog.com/2013/12/internet-wide-efforts-to-fight-email.html">https://security.googleblog.com/2013/12/internet-wide-efforts-to-fight-email.html</a></td></tr> </tbody> </table> <table id="id5"> <colgroup><col><col></colgroup> <tbody> <tr><td><a href="#id2">[2]</a></td><td>The core protocol behind email, <span data-title="Simple Mail Transfer Protocol"><abbr title="Simple Mail Transfer Protocol">SMTP</abbr></span> was designed in the early eighties, and one of the terms it uses is “envelope sender”. This is apt because it originally was not much harder to fake than the return address on a physical envelope.</td></tr> </tbody> </table> <table id="id6"> <colgroup><col><col></colgroup> <tbody> <tr><td><a href="#id3">[3]</a></td><td>The DNS standards provide for storing values longer than 255 characters in a TXT record by simply storing multiple strings in the record, but such records can be annoying to work with in some software.</td></tr> </tbody> </table> </div></div>]]>
            </description>
            <link>https://rya.nc/dkim-privates.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24972609</guid>
            <pubDate>Mon, 02 Nov 2020 20:10:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Remember What You've Read?]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24972568">thread link</a>) | @victorbreder
<br/>
November 2, 2020 | https://breder.org/3/ | <a href="https://web.archive.org/web/*/https://breder.org/3/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>



<p>2020-11-01</p>

<p>Of the books that you have read in full more than a year ago, how much of their content can you remember on top of your head right now? As discussed in the <a href="https://freakonomics.com/podcast/nsq-books-influence/">No Stupid Questions Podcast</a>, we are more likely to remember how a book made us feel, where we bought that book, or even some special circumstance related to it (such as being gifted), than <em>its actual main points</em>.</p>

<p>While remembering plot points is arguably not important for books read for entertainment (the quote "I envy the ones who have not read book X, because they get to experience it for the first time" comes to mind), I would argue that the whole point of reading non-fiction books is to become more knowledgeable. This requires us, at the very least, to retrieve the facts we've learned and, hopefully, being able to articulate them with other facts we know or come to know.</p>

<p>Upon reading the <a href="https://www.amazon.com/dp/B06WVYW33Y">How to Take Smart Notes</a> book by Sonke Ahrens, which outlines the <em>Zettelkasten</em> method of note-taking for academic and non-fiction writing, and the <a href="https://www.amazon.com/dp/B07K6MF8MD">Ultralearning</a> book by Scott Young, which lays out the author's principles for mastery of a given subject through intense effort and focus, I've come to believe that the most important thing to remember what we've read is <em>writing</em>.</p>

<p>As the research into deliberate practice by Anders Ericsson has shown, we're terrible judges for how well we are learning something. We usually equate ease with performing well, so activities that require less effort, such as passively rereading, <em>feel</em> more productive than activities that require more effort, such as testing yourself on what you've learned. Systematic testing afterwards show that the former kind of practice, the <em>effortful</em> practice, performs much better than the latter.</p>

<p>I would argue that while reading through a 200-page book may <em>feel</em> productive, the upside afterwards, for which we set retrieval as the lowest bar, may be small, or smaller than it can be if we adopt some complementary techniques. By simply passively reading, we may fall prey to the <em>illusion of fluency</em>, which means that, while the information is still fresh in our minds, we <em>feel</em> like we master it.</p>

<p>The antidote to the illusion of mastery is, of course, <em>testing</em> yourself. But we can't make this too hard (or we will likely end up not doing it at all or for long). The simplest form of testing is, after you read a chapter, to <em>retrieve</em> from memory the gist of it and the most important points <em>for you</em>, ultimately <em>writing those down in your own words</em>.</p>

<p>By retrieving from memory instead of looking up in the book we practice <em><a href="https://en.wikipedia.org/wiki/Active_recall">active recall</a></em>, which doubles as a self-testing method and a way to strengthen our ability to retrieve that information later. If we fail to recall the main points of a chapter after we've just read it, we likely weren't paying that much attention and we won't be able to recall anything at all in the future.</p>

<p>By focusing on the points that are personally important <em>to you</em>, we ensure that we are not simply becoming able to summarize everything about a book (like a encyclopedia that can be looked up), but we are compounding upon our existing knowledge in ways that interest us and that are more likely to be useful for us in the future.</p>

<p>Finally, and arguably the most important, by writing down in our own words, instead of highlighting or copying quotes, we ensure that we are actively engaged in the concepts we are reading and we test that we are able to articulate those concepts in a coherent manner. By doing this, it is very hard to fool ourselves about our level of competency in that subject (or at least harder than it would be by doing all of this in our own mind).</p>

<p>So what do you remember from this blog post? How this may be important to you by shaping your actions in the future? Don't skip writing it down. :)</p>

</div></div>]]>
            </description>
            <link>https://breder.org/3/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24972568</guid>
            <pubDate>Mon, 02 Nov 2020 20:07:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My Thoughts on Monorepo]]>
            </title>
            <description>
<![CDATA[
Score 37 | Comments 17 (<a href="https://news.ycombinator.com/item?id=24971288">thread link</a>) | @shekhargulati
<br/>
November 2, 2020 | https://shekhargulati.com/2020/11/02/my-thoughts-on-monorepo/ | <a href="https://web.archive.org/web/*/https://shekhargulati.com/2020/11/02/my-thoughts-on-monorepo/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-6395">
	<!-- .entry-header -->

	
	
	<div>
		
<p>Before we start let me give some context on my background so that you can better understand my thoughts on Monorepo.&nbsp;</p>



<p>I head technology at an IT services organization. Most of the products that I build are using Microservices architecture, have multiple frontends(web and mobile). The biggest product that I recently built had close to 30 microservices, 1 web client written in React,&nbsp; and native mobile app built using React Native. These numbers are nowhere near the numbers big product companies have shared.&nbsp;</p>



<blockquote><p>I prefer Macroservices over Microservices. I think most products don’t need more than 10 microservices.&nbsp;</p></blockquote>



<p>The reason I am clearly specifying I belong to the IT services world is because most of the stuff we consume on software development is written by engineers and senior tech people at the product companies. The stuff they write and share is based on the real problems and challenges they face at work. There are times when those problems resonate with problems other software engineers face at their work but there are times they are solutions to the problems we don’t have. So, we have to look at these solutions from the lens of our problems.</p>



<blockquote><p>The post is based on my experience building software, leading and managing software delivery teams, and learning from the great articles written by engineers using Monorepos. Please refer to the references section for good resources on monorepos.</p></blockquote>



<p>Let’s get back to the topic at hand.</p>



<h2>So, what is a Monorepo?</h2>



<p>A monorepo is a software development strategy where a single version control repository has source code for multiple projects, libraries, and applications irrespective of their programming language. Also, the organizations using Monorepo strategy often use a common build tool (like Bazel, Pants, Buck) to manage all the source code. Some of the popular examples of organizations that employ monorepo strategy are Google, Facebook, Twitter, Microsoft, and Uber.&nbsp;</p>



<p>The alternative to monorepo is polyrepo/multirepo. In multirepo, you have a separate version control repository for each component. This is the common strategy used by most organizations to structure their code. This in my view has been largely driven by Microservices architecture style and small modules movement.</p>



<p>As mentioned in the paper[1] (Advantages and Disadvantages of a Monolithic Repository – A case study at Google), Monorepos have following properties:</p>



<ul><li><strong>Centralization</strong>: The codebase is contained in a single repo encompassing multiple projects.</li><li><strong>Visibility</strong>: Code is viewable and searchable by all engineers in the organization.</li><li><strong>Synchronization</strong>: The development process is trunk-based; engineers commit to the head of the repo.</li><li><strong>Completeness</strong>: Any project in the repo can be built only from dependencies also checked into the repo. Dependencies are unversioned; projects must use whatever version of their dependency is at the repo head.</li><li><strong>Standardization</strong>: A shared set of tooling governs how engineers interact with the code, including building, testing, browsing, and reviewing code.</li></ul>



<p>My understanding is that to successfully use monorepo you will have to satisfy all the properties. Otherwise, you will not get benefits intended from monorepo.&nbsp;</p>



<h2>Advantages of Monorepos</h2>



<p>There are valid reasons why many big product organizations prefer Monorepo. Following are the main reasons:</p>



<h3>Reason 1: Simplified dependency management</h3>



<p>Monorepos make dependency management simple by:</p>



<ol><li>You can easily depend on other projects/modules in a monorepo without the need for artifact management tools like Nexus, Artifactory etc.</li><li>You avoid diamond dependency problem. Diamond dependences occur when a project has two dependencies which depend on the same underlying library. When a developer upgrades a dependency, they run the risk of breaking a diamond in the dependency graph.</li><li>It is easier to keep all dependencies on the same version by using a centralized way to manage version numbers.</li></ol>



<p>This is simplified further by using a single build tool. I have not used Bazel, Bucks, or Pants. I was watching a talk on Twitter monorepo journey where they talked about Gradle being too slow for their use case. For the size of applications I have built Gradle has worked just fine.</p>



<h3>Reason 2: Code sharing and reusability</h3>



<p>The second big benefit of Monorepo is that developers can share code across projects. It is easier to enforce best practices across the code base by using monorepo. Another related point is that with monorepo we don’t end up creating silos. This is important in an enterprise setup because it leads to passing the buck and bugs falling through the cracks of the boundaries. In my experience with multirepo setup people only care about their Microservice running fine. They miss the point that value is achieved by integrating the software and collaboration. In IT service organizations where there is more bureaucracy and uneven distribution of skilled developers the problem scales very quickly with multirepo setup. Yes, I know it is a culture problem but most IT service organizations can’t burn investor dollars to build the culture.</p>



<h3>Reason 3: Atomic changes</h3>



<p>This I didn’t realize before I read literature on Monorepo. There is a lot of benefit in seeing related changes in a single commit. If you are working on a story that requires changes in multiple components then in a multirepo scenario you will have to see changes in multiple repositories and merge the PRs in some sequence so that you are in a healthy state. WIth monorepo you save the pain of trying to coordinate commits across multiple repositories. Also, this leads to better code reviews as all the changes are in one place.</p>



<h3>Reason 4: Large-scale code refactoring</h3>



<p>This is related to reason 3. With a monorepo, you can refactor the API and all of its callers in one commit. You see all the usages of an API at a single place and it is much easier to do than with multirepo where you might not even have all the code checked out. In my experience with multirepo setup most developers don’t keep all the repos updated with the upstream changes. Monorepos enables continuous improvement on global level that multirepo you do at local level.</p>



<h3>Reason 5: Less bureaucracy</h3>



<p>With some organizations I have worked at, you have to create ServiceNow tickets to create a repository. It can take a couple of days before you get your empty repository. With monorepo you don’t have to go through this pain.&nbsp;</p>



<h2>Disadvantages of Monorepo</h2>



<p>Nothing comes for free. There are always trade offs involved. Your job as a software engineer is to figure out if advantages weigh more than trade offs or not.&nbsp;</p>



<p>In my view following are the downsides of monorepo:</p>



<ol><li>Monorepos could slow down developers because of slow build times, poor tooling, and merge conflicts.&nbsp;<ol><li>Most developers still in 2020 struggle to cleanly merge code.&nbsp;</li><li>Git is slow for projects with large numbers of files and history.</li></ol></li><li>There is cognitive overhead involved as developers have to get comfortable with a much larger code base than they would have with multirepo setup.</li><li>To do monorepo well require investment in tooling that most organization non-tech leadership will fail to understand</li></ol>



<h2>So, what’s my view on monorepos?</h2>



<p>Before I talk about my views on Monorepo let’s understand three main constraints of IT services organization.&nbsp;</p>



<ol><li>We work with multiple customers so we can’t keep code of all customers in the same repository even when we host their code in our version control for obvious reasons. Also, we can’t give access to all our repositories to all our developers because of security and IP related issues. So, we will keep our discussion focused on how to manage repos for a single customer.</li><li>IT services organizations have a high ratio of junior(&lt; 5 years) to senior engineers(&gt; 10 years) somewhere in the range of 10:1 to 100:1 or may be higher in bigger IT service organizations. The reason I am bringing this point is that monorepos requires discipline and it is tough to achieve without senior engineers driving it using a well-defined process.</li><li>People come and go at a faster rate.</li></ol>



<p>Given the above two constraints and the disadvantages of monorepos it might seem that monorepos will not work for us. But, I see real problems faced by software delivery teams that can be solved by monorepos.</p>



<p>We build products for different customers. These products usually follow Microservice architecture, have multiple frontends – web and mobile, functional tests, scripts for deployment automation. In the multirepo strategy, you will create at minimum 5 repositories – 1 for backend with all microservices, 1 for SPA frontend, 1 or 2 repo for mobile depending on whether you are building pure native or using some native framework like React Native or Flutter, 1 for functional tests, 1 for deployment automation scripts.&nbsp; More often than not your team will use one repo per Microservice then only god knows how many repos you end up creating.</p>



<blockquote><p>Let me tell you a real story. I was once working with a client that had more than 1000 repositories in their version control system. They were using the Gitlab version control platform. They had 5 products and each product was made up of multiple Microservices. The first question I asked them was to help us understand which all services and their respective code repositories were part of product A. Their chief architect had to spent a day figuring out all the repositories that made the product A. After spend a day still she was not sure if she has covered all the services.</p></blockquote>



<p>Let’s discuss problems that I face with software delivery teams using the multirepo strategy. Just to reiterate these problems are in the context of a single customer.</p>



<ol><li>Lack of accountability: Humans are good at creating boundaries and silos. They don’t care what happens outside those boundaries. They don’t care about the bigger picture.</li><li>Version drift. 10 different versions of Spring Boot, three different JDK versions, …</li></ol></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://shekhargulati.com/2020/11/02/my-thoughts-on-monorepo/">https://shekhargulati.com/2020/11/02/my-thoughts-on-monorepo/</a></em></p>]]>
            </description>
            <link>https://shekhargulati.com/2020/11/02/my-thoughts-on-monorepo/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24971288</guid>
            <pubDate>Mon, 02 Nov 2020 18:24:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Darklang: Leaving OCaml]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24969352">thread link</a>) | @areski
<br/>
November 2, 2020 | https://blog.darklang.com/leaving-ocaml/ | <a href="https://web.archive.org/web/*/https://blog.darklang.com/leaving-ocaml/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://blog.darklang.com/content/images/size/w300/2020/11/skeleton-camel.jpg 300w,
                            https://blog.darklang.com/content/images/size/w600/2020/11/skeleton-camel.jpg 600w,
                            https://blog.darklang.com/content/images/size/w1000/2020/11/skeleton-camel.jpg 1000w,
                            https://blog.darklang.com/content/images/size/w2000/2020/11/skeleton-camel.jpg 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://blog.darklang.com/content/images/size/w2000/2020/11/skeleton-camel.jpg" alt="Leaving OCaml">
            </figure>

            <section>
                <div>
                    <p>I built the first demo of Dark in Python, in about two weeks. A few months later when I started productizing it, I rebuilt it in OCaml. Back in 2017, when I was considering the language and platform to use for Dark, OCaml was extremely compelling:</p><ul><li>it's a high-level language with static types, so easy to make large scale changes as we figure out what the language/product was</li><li>you mostly model data with sum types, which in my mind are the best way to model data</li><li>it's very similar to the language I wanted to build (in particular, we could reuse built-in immutable data structures for Dark's values)</li><li>it had a reputation for being high-performance, which meant that we could write an interpreter for Dark and not have it be terribly slow (vs writing an interpreter in python, which might be too slow)</li></ul><p>Unfortunately, as we've built Dark we've run into significant problems that have made it challenging to build in OCaml.</p><h2 id="lack-of-libraries">Lack of libraries</h2><p>When you bet on an off-mainstream language, one of the things you accept is that many libraries are not going to be available. When there is a small community, often there aren't enough people working in the language to make important libraries. This is especially true if few people are building business applications.</p><p>In OCaml there are many high quality libraries, especially for data structures and data manipulation. The annual<a href="https://opensource.janestreet.com/core/"> Jane Street code dump</a> has been quite useful and very high quality. However, we really felt the lack of several libraries. The most obvious of these is that we had to build a <a href="https://github.com/darklang/dark/blob/main/backend/libexecution/unicode_string.mli">Unicode string library</a> ourselves (built on top of the <a href="https://erratique.ch/software/uuseg">very impressive OCaml Unicode libraries</a> built by <a href="https://erratique.ch/contact.en">Daniel Bünzli</a>), but we needed many more libraries than that.</p><p>The lack of an SDK for Google Cloud has affected us greatly. When you're searching for product-market fit, you do the simplest, easiest thing. If you lack a good SDK for your cloud provider, the simplest, easiest thing is often a terrible architectural choice. We've built our own queue on top of our database rather than using the production-quality cloud queues available on GCP. Similarly, we barely use the Cloud Storage (GCP's version of S3), because we initially put things in the database <a href="https://blog.darklang.com/evolving-darks-tracing-system/">because it was easier</a>. We've built 3 services, 2 <a href="https://github.com/darklang/dark/tree/main/containers/stroller">in</a> <a href="https://github.com/darklang/dark/tree/main/containers/queue-scheduler">Rust</a>, and 1 in <a href="https://github.com/darklang/dark/tree/main/containers/postgres-honeytail">Go</a>, to workaround the challenges we've faced.</p><p>The biggest challenge here is our use of Postgres. Postgres is a great database and we're big fans, but Cloud SQL is not a great hosted database. GCP's position is that Cloud SQL is there to tick a box and we should be using Cloud Spanner. I would love to switch to Cloud Spanner, but we have no driver for it in OCaml. Given the Postgres driver in OCaml is not particularly mature, it's hard to expect that a Cloud Spanner driver would exist, and indeed it doesn't. We've had to contribute to the <a href="https://github.com/mmottl/postgresql-ocaml/commit/81a4ae5240decd8f483a90568257cfbc1558c7ed">OCaml Postgres driver</a>, and some parts of our codebase have been <a href="https://github.com/darklang/dark/blob/main/backend/libbackend/serialize.ml#L226">well and truly mangled</a> when working around features not supported in that driver.</p><p>We've also suffered from a lack of a high-level, production web stack (there are <a href="https://github.com/anmonteiro/ocaml-h2">low-level stacks with good reputations</a> that I've struggled to use, and a <a href="https://github.com/oxidizing/sihl">few</a> <a href="https://github.com/reason-native-web/morph">new</a> ones out there that look good), in particular lacking a user authentication module. We've been using <a href="https://auth0.com/">Auth0</a> to work around this for now, which has more moving pieces than I'd like, and a shockingly high cost (our 7000 users, most of whom never log in, costs us over $500/mo).</p><p>We've worked around other missing vendor SDKs by calling their HTTP endpoints directly and that's been mostly fine. However, for libraries like encryption we don't have that option - we <a href="https://github.com/darklang/dark/pull/1455/files">hacked around a missing encryption library</a>, but decided not to ship it to production until we audited it for security (which was never actually worth the cost).</p><p>At CircleCI, we bet on Clojure. That was also a non-mainstream language, but its ability to call Java SDKs meant we had a mature cloud library, which was essential for building CircleCI. Of course, in OCaml we could call C libraries (and <a href="https://github.com/darklang/dark/pull/1841">even Rust libraries</a>, perhaps), but it doesn't match having native libraries we can call directly.</p><h2 id="learnability">Learnability</h2><p>I'm mostly in the camp that anyone can learn any language, but I saw a team struggle with OCaml, and for good reason. Language tutorials are extremely poor in OCaml compared to other languages; they're mostly lecture notes from academic courses.</p><p>The compiler isn't particularly helpful, certainly compared to Rust or Elm (both of which have been in our stack at one point). Often it gives no information about an error. Syntax errors typically say "Syntax error"; though it will try to give a good error for a mismatched brace, often incorrectly. Type errors can be a real burden to read, even after 3 years of experience with it.</p><p>The docs in OCaml are often challenging to find. The <a href="https://ocaml.janestreet.com/ocaml-core/latest/doc/base/index.html">Jane Street docs</a> have improved significantly in the last few years, but it can be a challenge to even figure out what functions are available in a particular module for most libraries. Compare to the excellent <a href="https://docs.rs/">docs.rs</a> in Rust, which has comprehensive API docs for every package in Rust.</p><p>One of the ways I personally struggled in OCaml is around <code>Lwt</code>. Lwt is (one of!) OCaml's async implementations. I couldn't figure it out several years ago and so just built a single-threaded server. The amount of workarounds and downtime we've suffered from that single decision is immense. A tutorial around building high-performance (or even medium performance!) web servers would be very valuable. </p><p>Tooling is something I read would be good in OCaml. I remember reading there was a debugger that could go back in time! I don't know where that's gone but I've never heard of anyone using it.</p><p>We have struggled to make editor tooling work for us. This is partially because we also use ReasonML and this seems to break things. Unfortunately, this is common in programming, but even more so in small communities: you might be the first person to ever try to use a particular configuration.</p><p>Finally, the disconnect between the various tools is immense. You need to understand Opam, Dune, and Esy, to be able to get something working (you could also do it without Esy and just rely on Opam, but that's much worse). I talked about a bunch of these challenges <a href="https://blog.darklang.com/first-thoughts-on-rust-vs-ocaml/">here</a>.</p><h2 id="language-problems">Language problems</h2><p>Multicore is coming Any Day Now™️, and while this wasn't a huge deal for us, it was annoying. </p><h2 id="minor-annoyances">Minor annoyances</h2><p>One of my biggest annoyances was how often OCaml folks talk about Fancy Type System problems, instead of how to actually build products and applications. In other communities for similar languages (ReasonML, Elm, F#), people talk about building apps and solving their problems. In OCaml, it feels like people spend an awful lot of time discussing Functors. It's not quite at the level that I perceived in the Haskell world, but it pointed out that the people building the core of the ecosystem do not have the same problems that I do (which is building web-y stuff).</p><p>I honestly think OCaml was a great choice at the start. Being able to quickly and safely make large-scale changes to your app is something that staticly-typed functional languages excel at. I'm happy that we made the choice, and in retrospect, it still seems like the best choice of those we had at the time.</p><p>I'm working on building the next version of the backend. We have about 20k lines to be replaced, and they'll be rewritten in a new language while keeping the semantics the same. I plan to leave keep the frontend in ReasonML: it doesn't suffer from the same library problems as it can interface nicely to JS, and it's nearly 50k lines of code so it would be a much bigger undertaking.</p><p>Read <a href="https://blog.darklang.com/new-backend-fsharp/">the followup</a> to see what we picked!</p><hr><p><em><em>You can sign up for Dark </em></em><a href="https://darklang.com/signup" rel="noopener nofollow"><em><em>here</em></em></a><em><em>. For more info on Dark, follow our </em></em><a href="https://blog.darklang.com/rss" rel="noopener nofollow"><em><em>RSS</em></em></a><em><em>, follow </em></em><a href="https://twitter.com/darklang" rel="noopener nofollow"><em><em>us</em></em></a><em><em> (or </em></em><a href="https://twitter.com/paulbiggar" rel="noopener nofollow"><em><em>me</em></em></a><em><em>) on Twitter, join our </em></em><a href="https://darklang.com/slack-invite" rel="noopener nofollow"><em><em>Slack Community</em></em></a><em><em>, or watch our </em></em><a href="https://github.com/darklang/dark" rel="noopener nofollow"><em><em>GitHub repo</em></em></a><em><em>.</em></em></p>
                </div>
            </section>



        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://blog.darklang.com/leaving-ocaml/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24969352</guid>
            <pubDate>Mon, 02 Nov 2020 15:48:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Cellular immunity to SARS-CoV2 found at 6 months in non-hospitalised individuals]]>
            </title>
            <description>
<![CDATA[
Score 177 | Comments 132 (<a href="https://news.ycombinator.com/item?id=24968034">thread link</a>) | @ageitgey
<br/>
November 2, 2020 | https://www.uk-cic.org/news/cellular-immunity-sars-cov-2-found-six-months-non-hospitalised-individuals | <a href="https://web.archive.org/web/*/https://www.uk-cic.org/news/cellular-immunity-sars-cov-2-found-six-months-non-hospitalised-individuals">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><img alt="UK-CIC logo" data-entity-type="file" data-entity-uuid="8a702f0f-0ed4-43c4-b10a-d98dba34a018" src="https://www.uk-cic.org/sites/default/files/inline-images/UK-CIC%20logo%20square%20300pix.jpg">Cellular (T cell) immunity against SARS-CoV-2 is likely to be present within most adults six months after primary infection, a new pre-print on bioRxiv suggests. The research from the UK Coronavirus Immunology Consortium (UK-CIC), Public Health England and Manchester University NHS Foundation Trust&nbsp;demonstrates robust T cell responses to SARS-CoV-2 virus peptides at this timepoint in all participants following asymptomatic or mild/moderate COVID-19 infection.</p>

<p>A key question is whether previous infection with SARS-CoV-2 results in immunity to reinfection, and if so for how long. The immune system is extremely complex and there are many different potential routes whereby it can generate immunity to a disease post-infection. This study examines the role of T cells in contributing to immunity against SARS-CoV-2 at six months post infection.</p>

<p>As part of UK-CIC, researchers from the University of Birmingham, Public Health England, Manchester University NHS Foundation Trust (MFT) and&nbsp;NIHR Manchester Clinical Research Facility collected serum and blood samples from a cohort of more than 2,000 clinical and non-clinical healthcare workers including 100 individuals who tested sero-positive for SARS-CoV-2 in March/April 2020 (average age 41 (range 22–65); 23 men, 77 women). All 100 individuals experienced either mild/moderate symptoms or were asymptomatic (56 versus 44 people) and none were hospitalised for COVID-19. Serum samples were collected monthly to measure antibody levels, and blood samples were taken after six months to assess the cellular (T cell) response. A range of analyses were carried out to assess different aspects of the T cell response including the magnitude of response and the response to different proteins from SARS-CoV-2. Carrying out these cellular analyses is much more complex than antibody studies – but this study of 100 individuals is one of the largest in the world to date in this field.</p>

<p>T cell responses were present in all individuals at six months after SARS-CoV-2 infection. The cellular immune response was directed against a range of proteins from the virus, including the Spike protein that is being used in most vaccine studies. However, comparable immunity was present against additional proteins, such as nucleoprotein, which suggests that these may be of value for incorporation in future vaccine protocols. This indicates that a robust cellular memory against the virus persists for at least six months.</p>

<p>The size of T cell response differed between individuals, being considerably (50%) higher in people who had experienced symptomatic disease at the time of infection six months previously. Further research will be needed to determine the significance of this finding. It is possible that heightened cellular immunity might provide increased protection against re-infection in people with initial symptomatic infection, or that asymptomatic individuals are simply able to fight off the virus without the need to generate a large immune response.</p>

<p>Antibodies are also a crucial component of immune defence and cellular immunity was strongly correlated with the peak level of the antibody response. Furthermore, larger cellular responses appeared to protect against antibody ‘waning’ over time, again suggesting the need to ensure that cellular immune responses are elicited in vaccine regimens.</p>

<p>Overall, these findings indicate a robust cellular (T cell) immune response against SARS-CoV-2 at six months post-infection. These findings will feed not only into our understanding of how immunity to SARS-CoV-2 works but also help inform future vaccine strategies. Further research is now needed to assess whether this immune response is maintained over the longer term and to better understand how strength of cellular immune response corresponds to likelihood of reinfection.</p>



<p><b>Professor Paul Moss, UK Coronavirus Immunology Consortium lead from University of Birmingham, said:</b></p>

<blockquote>
<p>“Understanding what constitutes effective immunity to SARS-CoV-2 is extremely important, both to allow us to understand how susceptible individuals are to reinfection and to help us develop more effective COVID-19 vaccines.</p>

<p>“To our knowledge, our study is the first in the world to show robust cellular immunity remains at six months after infection in individuals who experienced either mild/moderate or asymptomatic COVID-19. Interestingly, we found that cellular immunity is stronger at this time point in those people who had symptomatic infection compared with asymptomatic cases. We now need more research to find out if symptomatic individuals are better protected against reinfection in the future.</p>

<p>“Our knowledge of SARS-CoV-2 infection is increasing all the time. While our findings cause us to be cautiously optimistic about the strength and length of immunity generated after SARS-CoV-2 infection, this is just one piece of the puzzle. There is still a lot for us learn before we have a full understanding of how immunity to COVID-19 works. While we increase our understanding, whether we think we have previously had COVID-19 or not, we all should still follow Government guidelines on social distancing to ensure we play our part in minimising the spread of COVID-19 within our communities.”</p>
</blockquote>



<p><b>Dr Shamez Ladhani, Consultant epidemiologist at Public Health England and the study’s author, said:</b></p>

<blockquote>
<p>“Cellular immunity is a complex but potentially very significant piece of the COVID-19 puzzle, and it’s important that more research be done in this area. However, early results show that T-cell responses may outlast the initial antibody response, which could have a significant impact on COVID vaccine development and immunity research.</p>

<p>“Our thanks go to the more than 2,000 staff who have volunteered to provide monthly blood samples since the beginning of the pandemic. Recruiting donors so early in 2020 allowed us significantly longer follow-up than many similar studies have achieved so far.”</p>


</blockquote>

<p><b>Professor Fiona Watt, Executive Chair of the Medical Research Council, part of UKRI, said: </b></p>

<blockquote>
<p>“This study shows the benefit of funding world-leading immunologists through the UK Coronavirus Immunology Consortium. Researchers investigated whether previous infection with SARS-CoV-2 results in immunity to reinfection. They found that a robust cellular memory against the virus persists for at least six months. This is promising news – if natural infection with the virus can elicit a robust T cell response then this may mean that a vaccine could do the same.”</p>
</blockquote>



<p><strong>Dr Shazaad Ahmad, Consultant Virologist at Manchester University NHS Foundation Trust (MFT) and Principal Investigator for the study at MFT, said:&nbsp;</strong></p>

<blockquote>
<p>“As one of the leading NHS trusts for research and innovation, known for our strong track record of recruiting to studies, we were selected to rapidly enlist a cohort of healthcare workers to take part in this important research – and 1,200 MFT staff swiftly answered the call.</p>

<p>“The study was delivered at the NIHR Manchester Clinical Research Facilities at Manchester Royal Infirmary and Wythenshawe Hospital (both part of MFT), which provide dedicated research space and highly-trained staff. I would like to say how grateful I am to my colleagues for continuing to deliver and participate in this study, as without them we would not be able to report the findings, which could have a huge global impact."<br>
&nbsp;</p>


</blockquote>

<p>Please note, this paper is a pre-print reporting preliminary data that has not yet been peer-reviewed.</p>

<p>----------Ends----------</p>

<p><u><strong>Notes for editors</strong></u><br>
This press release reports on findings in the following pre-print which is <a href="http://biorxiv.org/cgi/content/short/2020.11.01.362319">available to read on bioRxiv</a>:<br>
Zuo <em>et al. </em>2020 Robust SARS-CoV-2-specific T-cell immunity is maintained at 6 months following primary infection<br>
Journalists - please contact the press team for a copy of the preprint, or use the link above to find it directly.&nbsp;</p>

<p>The UK Coronavirus Immunology Consortium brings together UK 19 immunology centres of excellence to research how the immune system interacts with SARS-CoV-2 to help us improve patient care and develop better diagnostics, treatments and vaccines against COVID-19. It is jointly funded by UK Research and Innovation (UKRI) and National Institute for Health Research (NIHR) and supported by the British Society for Immunology.<br>
Website: <a href="https://www.uk-cic.org/news/www.uk-cic.org">www.uk-cic.org</a>&nbsp;<br>
Twitter: <a href="https://www.uk-cic.org/news/www.twitter.com/UKCICstudy">@UKCICstudy</a><br>
Email: <a href="mailto:uk-cic@immunology.org">uk-cic@immunology.org</a></p>

<p><strong>Press contacts</strong><br>
Gabriela De Sousa<br>
Email: <a href="mailto:g.desousa@immunology.org%C2%A0">g.desousa@immunology.org&nbsp;</a><br>
Jennie Evans<br>
Tel: 07703 807 444<br>
Email: <a href="mailto:j.evans@immunology.org">j.evans@immunology.org</a></p>
</div></div>]]>
            </description>
            <link>https://www.uk-cic.org/news/cellular-immunity-sars-cov-2-found-six-months-non-hospitalised-individuals</link>
            <guid isPermaLink="false">hacker-news-small-sites-24968034</guid>
            <pubDate>Mon, 02 Nov 2020 13:45:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Is a billion dollars worth of server lying on the ground?]]>
            </title>
            <description>
<![CDATA[
Score 315 | Comments 307 (<a href="https://news.ycombinator.com/item?id=24966028">thread link</a>) | @george3d6
<br/>
November 2, 2020 | https://cerebralab.com/Is_a_billion-dollar_worth_of_server_lying_on_the_ground | <a href="https://web.archive.org/web/*/https://cerebralab.com/Is_a_billion-dollar_worth_of_server_lying_on_the_ground">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
        <p>Published on: 2020-11-02</p>
        
<p><em>Note: Some details of the stories in this article are slightly altered to protect the privacy of the companies I worked for</em></p>
<p>It's somewhat anecdotal, but in my work, I often encounter projects that seem to use highly inefficient infrastructure providers, from a cost perspective.</p>
<p>I usually point out that, based on a fairly unbiased hardware comparison, that they could save over half their budget by migrating, and am usually met with a series of almost canned answer about migrations being too difficult due to x,y,z.</p>
<h2>I - A representative comparison</h2>
<p>I will pick one of the "expensive" and one of the "cheap" server providers, chosen simply based on the fact that I've worked with them a lot, and compare two of their high~ish end servers.</p>
<p>I'm going to take 1 example from a cheap server provider OVH and a somewhat worst machine from AWS.</p>
<p>OVH offers <a href="https://us.ovh.com/us/order/dedicated/#/dedicated/configure-hg?product=~(dc~(gra~1)~planCode~%271901bhg~option~(~(planCode~%27cpu-2x6132-dual-2018v1~family~%27cpu~quantity~1)~(planCode~%27ram-768g-2666-dual-2018v1~family~%27ram~quantity~1)~(planCode~%27disk-960ssd-sata-2019~family~%27disk~quantity~8)))">this machine</a>:</p>
<ul>
<li>2x Intel Xeon Scalable Gold 6132 - 28c/56t - 2.6/3.7 GHz</li>
<li>768GB RAM DDR4 ECC 2666MHz</li>
<li>960GB SATA SSD</li>
<li>3 Gb/s internal and 1 Gb/s external free bandwidth</li>
</ul>
<p>For $15,800/year (though it can be paid monthly)</p>
<p>For a close comparison, AWS offers their <a href="https://aws.amazon.com/ec2/pricing/on-demand/">4.16xlarge</a>. I'll try to figure out the exact hardware specs on <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/memory-optimized-instances.html">this description</a>:</p>
<blockquote>
<p>R4 instances feature up to 64 vCPUs and are powered by two AWS-customized Intel XEON processors based on E5-2686v4 that feature high-memory bandwidth and larger L3 caches to boost the performance of in-memory applications.</p>
</blockquote>
<p>So basically let's call it 2x E5-2686v4, though the "real" E5-2686v4 seems to have more cores (both real and virtual) than the AWS version, I'll give AWS the benefit of the doubt and say that their version is more or less the same. I'll also assume AWS's RAM is the same 2666MHz DDR4 EEC2 (basically the best you can get right now) though they don't specify this, but I'll be generous here.</p>
<p>So we have:</p>
<ul>
<li>2x Intel Xeon E5-2686v4 - 36c/72t - 2.3/3.0 GHz</li>
<li>488GB RAM DDR4 ECC 2666MHz</li>
<li>Storage paid for separately</li>
<li>Bandwith paid for separately</li>
</ul>
<p>For $37,282/year (paid hourly) or $25,771/year (paid upfront)</p>
<p>The OVH server has more memory, it comes with 1TB of very fast storage, and adding more storage is much cheaper than AWS EBS prices (+ you get the option for NVME SSDs connected via PCIe on all servers).</p>
<p>I chose two processors which are <em>fairly</em> similar but running a comparison is still hard. Unlike e.g. RAM, processors are much more synergistic, you can't just look at parameters like nr cores, cache size, and frequency to figure out how well they perform.</p>
<p>Still, these two seem to be pretty close on those parameters and when looking at <a href="https://www.cpubenchmark.net/compare/Intel-Xeon-E5-2686-v4-vs-Intel-Xeon-Gold-6132/2870vs3227">the benchmarks</a>. It seems that the Gold 6132 is marginally better than the E5-2684-v4. Granted, benchmarking server CPUs is hard, but still, I think it's fair to say that the former has at least a tiny advantage, even if somehow the E5-2684 performs worst of benchmarks than on "real tasks".</p>
<p>So we have 2 servers:</p>
<ul>
<li>Both are in Paris</li>
<li>One has 768GB of RAM the other 488GB (58% of the first one) with the same specs</li>
<li>One has an extremely to slightly better CPU (let's say 10% better, a bit bellow the ~17% claimed by the benchmarks)</li>
<li>One has free bandwidth, the other one charges you for every single Kb of communication with the outside world (though at a fairly small sum)</li>
<li>One comes with 1TB of very fast NVME storage, for the other one you have to pay extra</li>
<li>One is paid monthly, the other one yearly or hourly.</li>
</ul>
<p>If the first server, the one that is better in literally every way, costs ~16k/year... how much should the other one cost? Well, maybe 10, maybe 12, maybe 14?</p>
<p>I don't know, but the answer certainly shouldn't be "Almost twice as much at 26k/year", that's the kind of answer that indicates something is broken.</p>
<p>In a worst-case scenario, AWS is ~1.6x times as expensive, but again, that's paid yearly. If we compare paid monthly to paid hourly (not exactly fair) we get 37k vs 16k, if we do some <a href="https://aws.amazon.com/ebs/pricing/">napkin math calculations</a> for equivalent storage cost (with equivalent speed via guaranteed iops) we easily get to ~3k/year extra. We have a 40k vs 16k difference, the AWS machine with the worst specs is 250% more expensive.</p>
<p>But whether the worst AWS machine is 160% or 250% as expensive as the OVH one is not the relevant question here, the question is why are we seeing those astronomical differences in terms of cost to being with.</p>
<p>We should consider there are hosting providers cheaper than OVH (e.g. scaleway, potentially online.net, and other such providers you never heard of). On the flip side of the coin, there are server providers such as digital oceans, GC, and Azure that can be more expensive than AWS.</p>
<p>Why?</p>
<h2>II - Vendor lock-in hypothesis</h2>
<p>The easiest thing to do here is to cry vendor lock-in.</p>
<p>The story goes that you end up using firebase for authentication, then you hire a sysadmin / DevOps guy that knows GC to create your infrastructure there. Then you make use of some fancy google ML service that integrates seamlessly with the GC storage... so on and so forth, until it would cost you a lot more manpower to move away from GC than to pay them a bit extra for whatever compute or storage you could get for less elsewhere.</p>
<hr>
<p>This is compounded by the fact that most of the time startups are oblivious to the cost of these services.</p>
<p>I switched my personal "infrastructure" from AWS since it ended up costing me over $100/month to maintain. Nowadays I pay $23/month and get a lot more leeway out of my current setup. But I haven't done that with some startups I've worked with or advised, even though the cost savings could have one or two additional zeros added to them. Why?</p>
<p>I can often call the shots regarding hardware at the startups I've worked with, yet I usually can't argue against using AWS or GC... because often enough, the first hit is free. AWS, GC, and Azure are throwing out 10k$ worth of credits like candy, and topping that off with 50-200k$ worth of credit for startups that they think have potential. The catch here is that the credits expire in 1 year, and once that year is done many are probably locked into the vendor.</p>
<p>The startup model is one of exponential growth, most fail and the winners have dozens or hundreds of millions from investments. So what is one or two hundred thousand a year on an IaaS bill?</p>
<p>Well, the answer is almost nothing. I believe the standard AWS offering for free credits is something like 100k$/year. So assuming a startup that uses that for a year gets 10mil in investment, it costs them 1% of their budget a year to maintain that.</p>
<p>The problem is that investment reflects future potential worth, a startup receiving a 10 mil investment is probably operating at a small fraction of the capacity those investors hope it will reach. For the shares to be worth 5x time that original investment, the company might have to scale its operations 20x or 50x, or 100x.</p>
<p>This becomes a problem since you can't run on investment forever, and scaling up 20x suddenly turns that 100k into 2 million a year spent on servers.</p>
<p>Of course, this is just a hypothetical, the numbers here are stand-ins to make a point, not a case-study. From my own experience, that the lock-in funnel looks something like:</p>
<ol>
<li>Free credits, let's use {expensive infrastructure provider}.</li>
<li>Loads of investment money, let's not waste time switching away from {expensive infrastructure provider}, it's &lt; 1% of our yearly budget.</li>
<li>Turns out that once the company grew, {expensive infrastructure provider} now consists of a double-digit percentage of our yearly expenditure, but it's too late to switch now.</li>
</ol>
<hr>
<p>This situation is exacerbated by consolidation (big fish buys little fish). I vividly remember a situation where I found an optimal hardware+software combination for a data processing platform, I think a conservative estimate would be that it was ~5 times cheaper than the vendor lock-in alternative being used at the time.</p>
<p>This happened to make it worth the switch since the startup lacked a generous credit offer for Google cloud. But, as soon as it was "consolidated", I was forced to switch the whole system back to Google cloud, granted a much better GC setup, but one that still involved costs ~2-3x times greater than the original solution.</p>
<p>Why? Well, boils down to the parent company using Google cloud, all their employees knowing how to work with GC, all their contracts having weird security-related clauses composed by many lawyers based on "official security audits" ran on their GC infrastructure, and so on.</p>
<p>However, this leads nicely into my second hypothesis.</p>
<h2>III - Employee lock-in hypothesis</h2>
<p>Employees end up deciding most of what a company is using internally, including infrastructure providers.</p>
<p>People aggregate <a href="https://slatestarcodex.com/2014/09/30/i-can-tolerate-anything-except-the-outgroup/">along weird lines</a>, to the extent that it wouldn't surprise me if a CTO hired initial engineers that favored his preferred infrastructure provider, even if he didn't actively seek that trait out.</p>
<p>Once the first few employees are fans of a given infrastructure provider, it starts making it into the job specs, because onboarding someone familiar with AWS when you use Azure is a huge pain in the ass. All other things being equal you'd rather have someone familiar with the technology you are already using.</p>
<p>This is compounded by the kind of employees that permeate a given field. If you are developing mobile apps or web apps, for example, it's likely that many engineers you will find will be familiar with Heroku and Digital Oceans. If you are developing whatever the heck people use C# for, I'd bet you'll find people that know how to use Azure. If you are doing machine learning, most people will know a thing or two about google cloud's offers regarding TPUs.</p>
<p>More broadly, this leaves no room for people that want to have a "multi-cloud" infrastructure or use a very little known platform. Either you get engineers that are very versed in the subject, but that will cost extra. Or you consign to having a few experts on the subject handle everything, with the rest of the team having no idea how to boot up a new VM without calling someone up.</p>
<p>Of …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://cerebralab.com/Is_a_billion-dollar_worth_of_server_lying_on_the_ground">https://cerebralab.com/Is_a_billion-dollar_worth_of_server_lying_on_the_ground</a></em></p>]]>
            </description>
            <link>https://cerebralab.com/Is_a_billion-dollar_worth_of_server_lying_on_the_ground</link>
            <guid isPermaLink="false">hacker-news-small-sites-24966028</guid>
            <pubDate>Mon, 02 Nov 2020 09:09:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Build a Collaborative Chatbot with Google Sheets and TensorFlow]]>
            </title>
            <description>
<![CDATA[
Score 93 | Comments 14 (<a href="https://news.ycombinator.com/item?id=24966013">thread link</a>) | @jonathanbgn
<br/>
November 2, 2020 | https://jonathanbgn.com/nlp/2020/09/29/chatbot-universal-sentence-encoder.html | <a href="https://web.archive.org/web/*/https://jonathanbgn.com/nlp/2020/09/29/chatbot-universal-sentence-encoder.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><img src="https://jonathanbgn.com/assets/images/taipei.jpg" alt="Taipei"></p>

<p>Currently living in Taiwan, I recently joined the <a href="https://github.com/taiwangoldcard/taiwan-bot">Taiwan Bot 🤖</a> project along with <a href="https://www.linkedin.com/in/shawn-lim-0a307550">Shawn</a> and <a href="https://erickhun.com/about/">Eric</a>. The idea is to build a go-to assistant to help foreigners answer their questions about moving to, working, and living in Taiwan (pro-tip: ask the bot where to find cheese or chocolate).</p>

<p>Building a functional and useful chatbot is a non-trivial project. Fortunately, there has been impressive progress in the fields of machine learning and <strong>natural language processing (NLP)</strong> in the past few years. Moreover, the democratization and open-source sharing of cutting-edge deep learning models from research work at large tech companies like Google or Facebook is making it possible for anyone to implement the latest state-of-the-art solutions.</p>

<p>The <a href="https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html">Universal Sentence Encoder</a>, recently released by Google AI, is one of these new models available via <a href="https://tfhub.dev/google/universal-sentence-encoder/4">Tensorflow Hub</a>. Trained in a <strong>multi-tasking</strong> fashion, the model can encode sentences into meaningful continuous representations that work well on a range of different tasks. It is thus ideal for <strong>transfer learning</strong> and performs competitively with more complex models like <a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a>. Moreover, it can run much faster than BERT or other similar <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">Transformer</a> models and is thus more applicable to real-world problems. There is even a <a href="https://tfhub.dev/google/universal-sentence-encoder-lite/2">Lite version</a> of the model, small enough to run in Javascript on the client-side.</p>

<h2 id="the-project">The project</h2>

<p>Despite being an amazing place to live, <strong>Taiwan is still misunderstood by most foreigners</strong>. We think that a fun and approachable chatbot could help people understand a lot more about all the great things this place has to offer, as well as answer most of the questions they might have about living here.</p>

<p>We decided to start with a limited scope first and to focus on answering practical questions about moving to and living in Taiwan. Specifically, we chose to focus on visa issues and the recently created <a href="https://taiwangoldcard.com/">Gold Card program</a>. We plan to expand the bot capabilities in future versions.</p>

<p>When it comes to chatbots, there are a lot of ways to go, and many tools and libraries out there to help you make your plan a reality. However, being just a small team of 3 doing this in our spare time, we didn’t have enough resources and time to build something very sophisticated. We also didn’t want to spend a lot of time to compile a large training dataset. So we looked for the best way to build a system that would be:</p>

<ul>
  <li>🧩 Easy and quick to build</li>
  <li>⚡️ Lightweight and runnable on a small server</li>
  <li>🔧 Iterable and easy to improve</li>
  <li>🧠 Focused on finding relevant answers</li>
</ul>

<p>We chose to build our bot with Microsoft’s <a href="https://github.com/microsoft/botframework-sdk">Bot Framework SDK</a> for easy development, user management and to be able to easily publish it to multiple platforms like Messenger or Line, the most popular messaging platform in Taiwan. <strong>The only thing remaining was to build the brain behind the messages.</strong></p>

<h2 id="understanding-the-meaning-behind-a-question">Understanding the meaning behind a question</h2>

<p>The main challenge when building a bot is <strong>relevancy</strong>, and this starts by having a clear understanding of what the user’s intention is. There are many approaches possible to make sense of what the user wants. At the most simple, one could simply look for some keywords such as <code>hello</code>, <code>restaurant</code>, or <code>visa</code>. However, this doesn’t take at all into account all the nuances of the language.</p>

<p>We didn’t have the resources to build a full-scale bot that could recognize the user intention among thousands of possibilities, yet we wanted to create something that could be relevant enough so that people would find it useful. So we needed to find an ideal middle ground between complexity and performance.</p>

<p>One of the most important concepts in NLP is one of <strong>distributed representations</strong>, inspired by the linguistic field of <a href="https://en.wikipedia.org/wiki/Distributional_semantics">distributional semantics</a>. The core idea is to encode linguistic items (words, sentences) into <strong>embeddings</strong> (vectors in a large dimensional space) such that items with similar properties should be closer in the resulting space.</p>

<blockquote>
  <p>You shall know a word by the company it keeps.</p>

  <p><em>- John Rupert Firth (1957)</em></p>
</blockquote>

<p>For example, similar words will cluster together in the vector space:</p>

<p><img src="https://jonathanbgn.com/assets/images/word-embeddings.png" alt="Word Embeddings"></p>

<p><em>Image from <a href="https://blog.tensorflow.org/2020/08/introducing-semantic-reactor-explore-nlp-sheets.html">TensorFlow Blog</a></em></p>

<p>You could do the same as the above but with sentences, effectively encoding them into large vectors which can be compared between themselves using <strong>similarity functions</strong>. Hence sentences with similar vector representations are sentences with similar meaning, topic, syntax…</p>

<h2 id="encoding-questions-with-the-universal-sentence-encoder">Encoding questions with the Universal Sentence Encoder</h2>

<p>The <a href="https://arxiv.org/abs/1803.11175">Universal Sentence Encoder</a> is a powerful Transformer model (in its large version) allowing to extract <a href="https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture">embeddings</a> directly from sentences instead of from individual words. It already powers some impressive Google projects such as <a href="https://books.google.com/talktobooks/">Talk to Books</a> or <a href="https://google.github.io/mysteryofthreebots/">Mystery of the Three Bots</a>.</p>

<p>For our chatbot project, we are first using the model to encode all the questions that we think users would want to ask to the bot. This can be done in a few lines of code thanks to the convenient TensorFlow Hub library:</p>

<div><div><pre><code><span>import</span> <span>tensorflow</span> <span>as</span> <span>tf</span>
<span>import</span> <span>tensorflow_hub</span> <span>as</span> <span>tfhub</span>

<span>model</span> <span>=</span> <span>tfhub</span><span>.</span><span>load</span><span>(</span><span>"https://tfhub.dev/google/universal-sentence-encoder/4"</span><span>)</span>

<span>questions</span> <span>=</span> <span>[</span> <span>...</span> <span>]</span>  <span># questions most likely to be asked to the bot
</span><span>answers</span> <span>=</span> <span>[</span> <span>....</span> <span>]</span>  <span># all answers to the questions above
</span>
<span>batch_size</span> <span>=</span> <span>10</span>
<span>embeddings</span> <span>=</span> <span>[]</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>0</span><span>,</span> <span>len</span><span>(</span><span>questions</span><span>),</span> <span>batch_size</span><span>):</span>
    <span>embeddings</span><span>.</span><span>append</span><span>(</span><span>model</span><span>(</span><span>questions</span><span>[</span><span>i</span><span>:</span><span>i</span><span>+</span><span>batch_size</span><span>]))</span>
<span>questions_embeddings</span> <span>=</span> <span>tf</span><span>.</span><span>concat</span><span>(</span><span>embeddings</span><span>,</span> <span>axis</span><span>=</span><span>0</span><span>)</span>
</code></pre></div></div>

<p>Then whenever a user asks a question, we can just extract its embedding and find the most similar question in our database of embeddings. In our case we use a simple vector dot product as a similary function:</p>

<div><div><pre><code><span>def</span> <span>find_best_answer</span><span>(</span><span>question</span><span>:</span> <span>str</span><span>)</span> <span>-&gt;</span> <span>str</span><span>:</span>
    <span>embedding</span> <span>=</span> <span>model</span><span>([</span><span>question</span><span>,])</span>
    <span># compute dot product with each question:
</span>    <span>scores</span> <span>=</span> <span>questions_embeddings</span> <span>@</span> <span>tf</span><span>.</span><span>transpose</span><span>(</span><span>embedding</span><span>)</span>

    <span>return</span> <span>answers</span><span>[</span><span>np</span><span>.</span><span>argmax</span><span>(</span><span>tf</span><span>.</span><span>squeeze</span><span>(</span><span>scores</span><span>).</span><span>numpy</span><span>())]</span>
</code></pre></div></div>

<h2 id="google-sheets-as-a-collaborative-database">Google Sheets as a collaborative database</h2>

<p>We built our dataset using a simple Google spreadsheet with 2 columns: questions and answers. Whenever a user asks a question, we just find the most relevant question and return the appropriate answer.</p>

<p><img src="https://jonathanbgn.com/assets/images/taiwan-bot-database.png" alt="Questions Answers Dataset"></p>

<p>This approach, while relatively simple, is a flexible enough for efficiently working together. Querying the data is done once during startup with a few lines of code:</p>

<div><div><pre><code><span>client</span> <span>=</span> <span>gspread</span><span>.</span><span>authorize</span><span>(</span>
    <span>ServiceAccountCredentials</span><span>.</span><span>from_json_keyfile_dict</span><span>(</span><span>SERVICE_ACCOUNT_INFO_DICT</span><span>,</span>
        <span>[</span><span>'https://spreadsheets.google.com/feeds'</span><span>,</span><span>'https://www.googleapis.com/auth/drive'</span><span>])</span>
<span>)</span>
<span>sheet</span> <span>=</span> <span>client</span><span>.</span><span>open</span><span>(</span><span>SPREADSHEET_FAQ_FILE</span><span>).</span><span>worksheet</span><span>(</span><span>SPREADSHEET_SHEET_NAME</span><span>)</span>
<span>questions</span> <span>=</span> <span>list</span><span>(</span><span>map</span><span>(</span><span>str</span><span>.</span><span>strip</span><span>,</span> <span>sheet</span><span>.</span><span>col_values</span><span>(</span><span>1</span><span>)[</span><span>1</span><span>:]))</span>
<span>answers</span> <span>=</span> <span>list</span><span>(</span><span>map</span><span>(</span><span>str</span><span>.</span><span>strip</span><span>,</span> <span>sheet</span><span>.</span><span>col_values</span><span>(</span><span>2</span><span>)[</span><span>1</span><span>:]))</span>
</code></pre></div></div>

<p>Here is an example of a conversation with the bot:</p>

<p><img src="https://jonathanbgn.com/assets/images/taiwan-bot-conversation.jpg" alt="Conversation with the bot"></p>

<h2 id="continuous-improvement">Continuous improvement</h2>

<p>We did our best to think about what would be the most commonly asked questions but, of course, we cannot predict everything people will ask. This is why if you ask a question that is not present in our database, the bot can answer with something completely unrelated. To prevent this, we built a small logging system to be able to track the questions asked to the bot and which question it thought was the most similar (along with the similarity score).</p>

<p>For example, here is what happened behind the scenes during the small conversation above. The first column is the user message. The second column is the most similar question (as based on the embeddings similarity). The third column is the best answer and the last column the computed similarity score. If the similarity score is not good enough, the bot will answer with a generic reply <em>” Sorry, I cannot help with that yet “</em>.</p>

<p><img src="https://jonathanbgn.com/assets/images/taiwan-bot-logs.png" alt="Conversation Logs"></p>

<p>This logging system will also help us improve our answers as more people use the bot and new edge cases are found. Still, no chatbot is perfect, and we think the bot will be most useful in a context where humans can take over when the bot fails. For example, on Slack, we added the bot to a general FAQ channel where people can get assistance from both the bot and humans for more specific information.</p>

<p><img src="https://jonathanbgn.com/assets/images/taiwan-bot-slack.png" alt="Chatbot on Slack"></p>

<h2 id="conclusion">Conclusion</h2>

<p>Building an effective chatbot doesn’t have to be a complex project. As long as the scope is relatively narrow, it is possible to use a general encoder model like the Universal Sentence Encoder to build something useful. The hard part is collecting enough questions/answers for the bot to be able to answer most questions. It is also important to regularly monitor what users are asking and complement new data whenever the bot can’t find a relevant answer.</p>

<p>If you are also living or considering to move to Taiwan, you can <a href="https://m.me/thetaiwanbot">chat with Taiwan bot on Messenger here</a>!</p>

<h3 id="read-next">Read next</h3>

<p><a href="https://jonathanbgn.com/nlp/2020/08/30/gpt2-gpt3-creativity.html">Unleash GPT-2 (and GPT-3) Creativity through Decoding Strategies</a></p>

<p><a href="https://jonathanbgn.com/speech/2020/10/31/emotion-recognition-transfer-learning-wav2vec.html">Detecting Emotions from Voice with Very Few Training Data</a></p>

  </div>
  
  
</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://jonathanbgn.com/nlp/2020/09/29/chatbot-universal-sentence-encoder.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24966013</guid>
            <pubDate>Mon, 02 Nov 2020 09:05:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[You might not need to store plaintext email addresses]]>
            </title>
            <description>
<![CDATA[
Score 242 | Comments 155 (<a href="https://news.ycombinator.com/item?id=24965671">thread link</a>) | @danielskogly
<br/>
November 2, 2020 | https://blog.klungo.no/2020/11/01/you-might-not-need-to-store-plaintext-emails/ | <a href="https://web.archive.org/web/*/https://blog.klungo.no/2020/11/01/you-might-not-need-to-store-plaintext-emails/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Earlier this year, when I went from having only Facebook-login on <a href="https://wishy.gift/">Wishy.gift</a> to allow registrations with email address and password, one of my concerns was how to implement this is a way that protects the data and privacy of my users. I don’t have any ads or analytics on the site, the users can select whatever display name they want, and I never stored the email addresses I got from Facebook when a user registered or logged in - only a hashed<sup><a href="#fn1" id="fnref1">[1]</a></sup> version of the ID. Email addresses and passwords, on the other hand, are a whole other beast, and the consequences of a database breach much worse.</p>
<p>Considering that the only kind of emails I ever need to send out are transactional - no newsletters or other kinds of notifications - the only thing I need to store them for are as identifiers, and can safely be hashed.</p>
<p>For every transactional email I need to send out - registration, account recovery, and email change verification - the user always initiates this by submitting their email address, and it will at that time be available to the backend to perform the needed action.</p>
<p>In conclusion, if you only use email addresses for transactional emails, you might be able to only store hashed versions of them. For <a href="https://wishy.gift/">Wishy.gift</a> I use SHA512 with a fixed salt, and this has been working perfectly since implementation in June.</p>
<p>Thank you for reading this! I would love to hear your thoughts and ideas too. Join the discussion on <a href="https://news.ycombinator.com/item?id=24959734">Hacker News</a>, or feel free to email me at <code>daniel</code> at the domain this blog is on.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>I discovered that, even though the ID was unique to my FB-app, it was still possible to go to <a href="http://facebook.com/%7Bid%7D">facebook.com/{id}</a> and be redirected to the user’s FB-profile. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
</div></div>]]>
            </description>
            <link>https://blog.klungo.no/2020/11/01/you-might-not-need-to-store-plaintext-emails/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24965671</guid>
            <pubDate>Mon, 02 Nov 2020 08:11:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[As an older guy I've finally figured out weight-loss]]>
            </title>
            <description>
<![CDATA[
Score 47 | Comments 112 (<a href="https://news.ycombinator.com/item?id=24965631">thread link</a>) | @RikNieu
<br/>
November 2, 2020 | https://www.riknieu.com/the-best-way-to-lose-weight/ | <a href="https://web.archive.org/web/*/https://www.riknieu.com/the-best-way-to-lose-weight/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<p>This post isn't really product or development related, but I think it applies to lots of us desk-job types. Hopefully it'll help someone else who's struggling just as much as I was.</p><h2 id="first-my-results-and-progress-so-far">First my results and progress so far</h2><p>I started getting serious about my diet at the end of September 2020. I weighed about 80kg/176lbs. </p><p>Today, on 1 Nov 2020, I weighed about 74kg/163lbs. </p><figure><img src="https://www.riknieu.com/content/images/2020/11/Screenshot-2020-11-01-at-17.09.12.png"><figcaption>My weight-loss chart 20 Sept - 1 Nov</figcaption></figure><p>That's a loss of 6kg/13lbs in a bit over a month! Insane! I would never have dreamt it possible for a middle-aged guy like me, who doesn't exercise too much and sits behind a computer all day to pull something like that off.</p><p>My journey is far from complete, but the progress I've had up to now is exciting enough to share. This would likely(hopefully) be the first in a series of posts detailing my weight-loss journey.</p><h2 id="the-backstory">The backstory</h2><p>I've never had a flat stomach in my entire life. Not when I was a kid, not when I was a young adult, and certainly not now, in my almost middle-age.</p><p>And I'm not even talking about having that shredded, six-pack look like the guys on the MensHealth covers, I'm just talking about a regular, flat tummy that doesn't bulge out in convex shape when caught from the side. I've always hovered between almost obese to dad-bod at best.</p><p>I've been self-conscious about my weight since forever. Being the fat kid in the 80s, when most kids weren't overweight yet, left it's mark on my psyche and self-esteem. I got noticed and bullied for it, often. It also didn't help that my skin was pale enough to trip the sun when I took my shirt off on the beach.</p><p>All jokes aside, being able to swim in public without a shirt on has just never been in the realms of conceivability for me. The shame is too great.</p><p>Society, my community and my peers have made it very clear, since a young age, that nobody wants to see that shit. In fact, I don't think I've been in any form of public water without a shirt on since the age of 10. Maybe even earlier.</p><h2 id="when-the-dough-starts-to-rise">When the dough starts to rise</h2><p>I've learned to live with it, of course. The pale skin, the belly, it's not that bad. I mean, I've still managed to convince a poor, unsuspecting soul to be my wife. I still had fun outdoors. And getting older means nobody really cares about my appearance anymore, at least not to my face. Being doughy is not ideal but it's not that big a deal either.</p><p>I always assumed it's just the genetic cards I've been dealt, my unique biology or some sort of weird hormonal thing. This made sense to me because most of my extended family are overweight, and dangerously so. Maybe we're just built to look like this.</p><p>This idea might have a sliver of truth to it - I personally know people who look fit and fabulous without any seeming effort and who can eat whatever they like. </p><p>I, however, seem to gain weight by just looking at delicious food. But if I'm honest, declaring it impossible to slim down due to my genes might have just been a convenient excuse. </p><p>On my 38th birthday this year, I reflected on this, and the fact that my 40s were rapidly coming into view. I can see it emerging just over the horizon, arms and legs pumping as it's sprinting towards me. Soon.</p><p>I wasn't concerned with becoming unattractive with age(I've never been considered attractive anyway) but using my family history as a rough model for extrapolation suggested that the physical shape I was in then was past the point of good-as-it-would-ever-be. Other thoughts circled around, about getting older, about my losing my health, mobility and vitality. </p><p>And the signs were certainly there. My weight and pant size had already started creeping up over the last couple of years. </p><p>I wasn't obviously overweight, don't get me wrong. Most people would have considered my weight quite normal, perhaps even slim when compared to present society. </p><p>But the trend in measurements suggested that my future would probably involve an ever-increasing waistline, with the eventual addition of tent-like shirts, feebly trying to conceal the obvious. </p><p>Yep, the coming years would see me getting rounder, wrinklier and unhealthier. Not paler though, that wouldn't be possible. This bun could could never be baked, my genes won't allow for it.</p><h2 id="middle-age-is-looming-let-s-set-a-crazy-goal-">Middle-age is looming - let's set a crazy goal!</h2><p>To my credit though, I did realise that being middle-aged didn't mean it's game-over yet. Perhaps I could still steer this ship around, if I just put my mind to it?</p><p>I won't ever become that tasty, handsome dish that turns heads, but perhaps I could aim for a relatively healthy, strong, utilitarian body? Like Sean Connery(RIP) in those first James Bond films. Old-school fit.</p><p>And with that musing, on the eve of my biological new year, I made the resolution to give it one, last, genuine shot. </p><p>I would do my best to attempt a solid, last-ditch effort in getting my stomach flat. My goal is cultivating an appearance that I won't necessarily want to show off, but that I'd at least not feel the need to cover up and hide.</p><p>The idea is too sport, by the end of this year or early the next, a completely flat stomach without having to suck it in. And I would endeavour to keep it that way, perpetually, for the foreseeable future.</p><p>This will be hard, and I know it. I've been down this road before. Though I was comfortable plump my whole life, I am no stranger to diets and exercise. I know them well, begrudgingly.</p><h2 id="the-revolving-door-of-fad-diets">The revolving door of fad diets </h2><p>In fact, I've tried almost every diet you can think of - keto, vegetarian, calorie counting, Fit-For-Life, caveman, paleo diet, slow carb... you name it, I probably tried it. And I've had various levels of success, but none left me with consistent, long-lasting results. Never mind washboard abs.</p><p>At best I'd maybe lose a couple of kilos, hit the inevitable plateau, and then give up out of frustration and resentment. Or I'd start feeling feeble and sick, suffer from weak concentration, or just feel generally miserable.</p><p>There was also the issue of inevitable birthdays and celebrations interfering, where you didn't want to be a party-pooper, or deny yourself life's simple pleasures either, but which then would usually result in me falling off the bus. </p><p>And the cheat days too. Not often, maybe once or twice on weekends. Those, I told myself, would be needed to keep myself sane and motivated, to keep the cravings at bay. But they were a trap. Cheat day's may not add more weight, but they certainly stop progress dead in its tracks.</p><p>Then there's exercise. Although my foray into the myriad of options available were not as extensive as with diets(due to some personal health limitations), my results - or lack thereof - were similar.</p><p>I tried to do some form of exercise at least 3 times a week. These included rollerblading with the wife, bodyweight routines, weight lifting, martial arts, walks and even the infernal torture that is running. </p><p>Exercise seemed to have <em>some</em> effect on sharpening my curves, but had no measurable effect on the scale.</p><p>Safe to say, it was pretty daunting to decide to commit to yet another version of this whole rigmarole again. When you feel you've tried everything before, you're skeptical this time would be different. Which diet would I be following this time, which training schedule?</p><p>I could feel that "it's just genetics" excuse looming just behind my shoulder, smugly waiting to tell me, "I told you so. It's pointless."</p><p>But this is what I wanted to do, so I decided to break it down and approach it from first principles, or at the very least keep it dead simple.</p><h2 id="best-way-of-losing-weight-science-simplicity-patience-and-discipline">Best way of losing weight? Science, simplicity, patience and discipline</h2><p>First principles suggested I stop faffing around with diets that operate on clever narratives, assumptions and "magic" rules, and just look at what the science said. And the science was pretty consistent - calories matter above all else. </p><p>Most studies suggested that for most people, the amount of calories consumed determines your weight. Hormonal factor exist, but they're either negligible or for a very, very, VERY small part of the population. You're likely not it.</p><p>Exercise does help, but in the modern world with its convenient packaged foods one can easily consume way more calories in a single day than you could realistically burn off in a week of exercising. And if you're not cognisant of the calories in your meals and snacks, you won't even know it.</p><p>Now, of course I've tried calorie counting before, and it worked alright, but still lead to the inevitable plateau and quit cycle. This was something I'd need to be careful of. I'd need to be strict to the max.</p><p>I decided I was going to be <em>extremely</em> disciplined and committed. The only bending of the rules would be cheat meals(not days) on birthdays. Not weekly, like I weaselled about with before. Not cheating otherwise at all.</p><p>And for exercise it would be just as simple. I would do weight training 3x a week, ala <a href="https://startingstrength.com/">Starting Strength</a>, because it's the only form of exercise I actually enjoy doing. That's it. Beyond that would just be whatever recreational activities we did with friends over weekends.</p><h3 id="calories-energy-in-energy-out">Calories - energy in, energy out</h3><p>I'm not going to give a lecture on calories and the intricacies behind them in this post, you can look it up yourself. </p><p>The <em>only</em> thing you need to know to successfully lose weight is that you need to consume less calories than your body requires to maintain its current weight. That's it.</p><p>"Not all calories are equal!", I hear some of you seethe, and yes, I've considered that too. I took it into account for this experiment as well. It will be addressed below.</p><h2 id="weight-loss-plan">Weight-loss plan</h2><h4 id="5-steps-calculate-adjust-simplify-measure-patience">5 Steps - calculate, adjust, simplify, measure, patience</h4><p><strong>Calculate</strong> - Google and find a free calorie calculator online, enter your particular details, and calculate the amount of daily calories you need to maintain your current weight. </p><p><strong>Adjust</strong> - To lose weight you need to reduce the amount of daily calories you consume. So you would take the amount you calculated above and just consume less calories than that every day. </p><p>How much less? That depends on how much you have to lose. If you're very …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.riknieu.com/the-best-way-to-lose-weight/">https://www.riknieu.com/the-best-way-to-lose-weight/</a></em></p>]]>
            </description>
            <link>https://www.riknieu.com/the-best-way-to-lose-weight/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24965631</guid>
            <pubDate>Mon, 02 Nov 2020 08:02:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Supabase.js 1.0 Released]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24965071">thread link</a>) | @awalias
<br/>
November 1, 2020 | https://supabase.io/blog/2020/10/30/improved-dx | <a href="https://web.archive.org/web/*/https://supabase.io/blog/2020/10/30/improved-dx">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><header><p><time datetime="2020-10-30T00:00:00.000Z">October 30, 2020  · 3 min read</time></p><div><p><a href="https://github.com/kiwicopple" target="_blank" rel="noreferrer noopener"><img src="https://avatars2.githubusercontent.com/u/10214025?s=400&amp;u=c6775be2ae667e2acae3ccd347fed62bb3f5b3e7&amp;v=4" alt="Paul Copplestone"></a></p></div></header><section><p>Today we're releasing <a href="https://github.com/supabase/supabase-js" target="_blank" rel="noopener noreferrer">supabase-js</a> version 1.0, and it comes with some major Developer Experience improvements.</p><h3>New Docs</h3><p>Before digging into the improvements, we're excited to point out our new <a href="https://supabase.io/docs/client/supabase-client">developer docs</a>. While they're still a work in progress, here are some things we think you'll like:</p><ul><li>The <a href="https://supabase.io/docs/client/supabase-client">Reference Docs</a> are auto-generated from our Typescript definitions and then enriched with examples. This forces us to document our code and makes it easier to keep everything in sync.</li><li>We added placeholders for the other languages that the community is developing. They have already started with Python, C#, Dart, Rust, and Swift. Expect to see the docs filling up soon!</li><li>We've added sections for all of the open source tools we use, including <a href="https://supabase.io/docs/postgres/server/about">Postgres</a>, <a href="https://supabase.io/docs/postgrest/server/about">PostgREST</a>, <a href="https://supabase.io/docs/gotrue/server/about">GoTrue</a>, and <a href="https://supabase.io/docs/realtime/server/about">Realtime</a>. We'll be filling these with lots of valuable information including self-hosting, benchmarks, and simple guides.</li></ul><h3>Errors are returned, not thrown</h3><p>We attribute this improvement to community feedback. This has significantly improved the developer experience. Previously we would throw errors:</p><div><div><div><div><p><span>try</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>const</span><span> </span><span>{</span><span> body </span><span>}</span><span> </span><span>=</span><span> supabase</span><span>.</span><span>from</span><span>(</span><span>'todos'</span><span>)</span><span>.</span><span>select</span><span>(</span><span>'*'</span><span>)</span><span></span></p><p><span></span><span>}</span><span></span></p><p><span></span><span>catch</span><span> </span><span>(</span><span>error</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>console</span><span>.</span><span>log</span><span>(</span><span>error</span><span>)</span><span></span></p><p><span></span><span>}</span></p></div></div></div></div><p>And now we simply return them:</p><div><div><div><div><p><span>const</span><span> </span><span>{</span><span> data</span><span>,</span><span> error </span><span>}</span><span> </span><span>=</span><span> supabase</span><span>.</span><span>from</span><span>(</span><span>'todos'</span><span>)</span><span>.</span><span>select</span><span>(</span><span>'*'</span><span>)</span><span></span></p><p><span></span><span>if</span><span> </span><span>(</span><span>error</span><span>)</span><span> </span><span>console</span><span>.</span><span>log</span><span>(</span><span>error</span><span>)</span><span></span></p></div></div></div></div><p>After testing this for a while we're very happy with this pattern. Errors are handled next to the offending function. Of course you can always rethrow the error if that's your preference.</p><h3>We created <code>gotrue-js</code></h3><p>Our goal for <code>supabase-js</code> is to tie together many sub-libaries. Each sub-library is a standalone implementation for a single external system. This is one of the ways we support existing open source tools.</p><p>To maintain this philosophy, we created <a href="https://github.com/supabase/gotrue-js" target="_blank" rel="noopener noreferrer"><code>gotrue-js</code></a>, a library for Netlify's GoTrue auth server. This libary includes a number of new additions, including third-party logins.</p><p>Previously:</p><div><div><div><div><p><span>const</span><span> </span><span>{</span><span></span></p><p><span>  body</span><span>:</span><span> </span><span>{</span><span> user </span><span>}</span><span>,</span><span></span></p><p><span></span><span>}</span><span> </span><span>=</span><span> </span><span>await</span><span> supabase</span><span>.</span><span>auth</span><span>.</span><span>signup</span><span>(</span><span></span></p><p><span>  </span><span>'someone@email.com'</span><span>,</span><span></span></p><p><span>  </span><span>'password'</span><span></span></p><p><span></span><span>)</span></p></div></div></div></div><p>Now:</p><div><div><div><div><p><span>const</span><span> </span><span>{</span><span> user</span><span>,</span><span> error </span><span>}</span><span> </span><span>=</span><span> </span><span>await</span><span> supabase</span><span>.</span><span>auth</span><span>.</span><span>signUp</span><span>(</span><span>{</span><span></span></p><p><span>  email</span><span>:</span><span> </span><span>'someone@email.com'</span><span>,</span><span></span></p><p><span>  password</span><span>:</span><span> </span><span>'password'</span><span></span></p><p><span></span><span>}</span><span>)</span></p></div></div></div></div><h3>Enhancements and fixes</h3><ul><li>Native Typescript. All of our libraries are now natively built with Typescript: <a href="https://github.com/supabase/supabase-js" target="_blank" rel="noopener noreferrer"><code>supabase-js</code></a>, <a href="https://github.com/supabase/postgrest-js" target="_blank" rel="noopener noreferrer"><code>postgrest-js</code></a>, <a href="https://github.com/supabase/gotrue-js" target="_blank" rel="noopener noreferrer"><code>gotrue-js</code></a>, and <a href="https://github.com/supabase/realtime-js" target="_blank" rel="noopener noreferrer"><code>realtime-js</code></a>.</li><li>Better realtime scalability: we only generate one socket connection per Supabase client. Previously we would create a connection for every subscription.</li><li>We've added support for OAuth providers.</li><li>60% of minor bugs outstanding for <code>supabase-js</code> have been <a href="https://github.com/supabase/supabase-js/pull/50" target="_blank" rel="noopener noreferrer">solved</a>.</li><li>You can use <code>select()</code> instead of <code>select(*)</code></li></ul><h3>Breaking changes</h3><p>We've bumped the major version because there are a number of breaking changes. We've detailed these in the <a href="https://github.com/supabase/supabase-js/releases/tag/v1.0.1" target="_blank" rel="noopener noreferrer">release notes</a>, but here are a few to be aware of:</p><ul><li><code>signup()</code> is now <code>signUp()</code> and <code>email</code> / <code>password</code> is passed as an object</li><li><code>logout()</code> is now <code>signOut()</code></li><li><code>login()</code> is now <code>signIn()</code></li><li><code>ova()</code> and <code>ovr()</code> are now just <code>ov()</code></li><li><code>body</code> is now <code>data</code></li></ul><p>Previously:</p><div><div><div><div><p><span>const</span><span> </span><span>{</span><span> body </span><span>}</span><span> </span><span>=</span><span> supabase</span><span>.</span><span>from</span><span>(</span><span>'todos'</span><span>)</span><span>.</span><span>select</span><span>(</span><span>'*'</span><span>)</span></p></div></div></div></div><p>Now:</p><div><div><div><div><p><span>const</span><span> </span><span>{</span><span> data </span><span>}</span><span> </span><span>=</span><span> supabase</span><span>.</span><span>from</span><span>(</span><span>'todos'</span><span>)</span><span>.</span><span>select</span><span>(</span><span>)</span></p></div></div></div></div><h3>Upgrading</h3><p>We have documented all of the changes in the <a href="https://github.com/supabase/supabase-js/releases/tag/v1.0.1" target="_blank" rel="noopener noreferrer">release notes</a>. </p><p>To summarise the steps:</p><ol><li>Install the new version: <code>npm install @supabase/supabase-js@latest</code></li><li>Update all your <code>body</code> constants to <code>data</code></li><li>Update all your <code>supabase.auth</code> functions with the new <a href="https://supabase.io/docs/client/auth-signup">Auth interface</a></li></ol><h3>Get started</h3><ul><li>Start using Supabase today: <a href="https://app.supabase.io/" target="_blank" rel="noopener noreferrer">app.supabase.io</a></li><li>Make sure to <a href="https://github.com/supabase/supabase" target="_blank" rel="noopener noreferrer">star us on GitHub</a></li><li>Follow us <a href="https://twitter.com/supabase_io" target="_blank" rel="noopener noreferrer">on Twitter</a></li><li>Become a <a href="https://github.com/sponsors/supabase" target="_blank" rel="noopener noreferrer">sponsor</a></li></ul></section></article></div>]]>
            </description>
            <link>https://supabase.io/blog/2020/10/30/improved-dx</link>
            <guid isPermaLink="false">hacker-news-small-sites-24965071</guid>
            <pubDate>Mon, 02 Nov 2020 05:36:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What Does It Take to Resolve a Hostname]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24965056">thread link</a>) | @todsacerdoti
<br/>
November 1, 2020 | https://venam.nixers.net/blog/unix/2020/11/01/resolving-a-hostname.html | <a href="https://web.archive.org/web/*/https://venam.nixers.net/blog/unix/2020/11/01/resolving-a-hostname.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <article>
    <p><img src="https://venam.nixers.net/blog/assets/resolving_hostname/What_Does_It_Take_To_Resolve_A_Hostname.jpg" alt="slide1"></p>

<ul>
  <li><a href="#resolving-a-name-is-complex">Resolving A Name Is Complex</a></li>
  <li><a href="#nih">NIH</a></li>
  <li><a href="#historic">Historic</a></li>
  <li><a href="#resolver3">resolver(3)</a></li>
  <li><a href="#gethostbyname3-and-getaddrinfo3">gethostbyname(3) and getaddrinfo(3)</a></li>
  <li><a href="#nss5">nss(5)</a></li>
  <li><a href="#resolvconf8">resolvconf(8)</a></li>
  <li><a href="#caching">Caching</a></li>
  <li><a href="#how-to-debug">How To Debug</a></li>
  <li><a href="#big-picture">Big Picture</a></li>
  <li><a href="#references">References</a></li>
</ul>

<p>Can also be found in presentation format <a href="https://youtu.be/Hd8Nc8ZRkNM">here</a></p>



<p><img src="https://venam.nixers.net/blog/assets/resolving_hostname/What_Does_It_Take_To_Resolve_A_Hostname2.jpg" alt="slide2"></p>

<p>Resolving a domain name is complex.  It’s not limited to the DNS, the
Domain Name System — A decentralized and hierarchical system to associate
names and other information to IP addresses.<br>
It’s not something we, as users, usually pay attention to.  We notice
it only when we’re facing an issue. It normally works out of the box
but really nobody get the crux.<br>
You search online for clarifications but they barely help and add more
confusion.</p>

<p>Here are some schemas trying to decipher the mystery that domain name
resolution came to be.</p>

<p><img src="https://venam.nixers.net/blog/assets/resolving_hostname/What_Does_It_Take_To_Resolve_A_Hostname3.jpg" alt="slide3"></p>

<p>One, two, and three, I think you get me, it is not easy.  It’s never as
simple as taking a hostname as a string, getting the DNS address in the
<code>/etc/resolv.conf</code> config, then sending a request to the DNS on port 53
to be greeted back with the IP.<br>
Behind the scene there are ton of files and libraries involved, all of
this to get a domain name solved.</p>

<p>So in this talk we’ll try to create some order to try to understand thing
as an end-user. Let’s make sense and reason behind this mess even if I
have to say, I don’t get it much myself.<br>
I can’t assess I haven’t made mistakes but if I did, please correct me,
that would be great!</p>



<p><img src="https://venam.nixers.net/blog/assets/resolving_hostname/What_Does_It_Take_To_Resolve_A_Hostname4.jpg" alt="slide4"></p>

<p>Let’s start with the misfits, the ones that don’t follow the rules,
the not-invented-here syndrome found within our tools.<br>
When it comes to DNS resolution, there’s no one-size fit all
solution. Obviously, many of us don’t want to deal with all the
complexity, so we say, “let’s pack these bytes ourselves, and forget
the hassle”.<br>
That’s pure heresy though. We’d prefer everything to work the same way,
so that it’s easier to follow. It would be preferable that they all use
the same lib, to all have the same behavior. That is, in our case to rely
on the C standard lib, or the POSIX API our savior.</p>

<p>In all cases, let’s note some software that don’t rely on it, as we said,
all the misfits.</p>

<ul>
  <li>The ISC/BSD BIND tools: from host, to dig, to drill, to nslookup,
and more, used for debugging chores.</li>
  <li>Firefox/Chrome/Chromium: There are the browsers, because they are one
of a kind, bypassing libc and POSIX mechanism, implementing their own
DNS API for performance reasons and perfectionism.</li>
  <li>Any applications needing advanced DNS features, other than simple name to IP.</li>
  <li>Language that don’t wrap around a libc: The Go programming language
comes to mind. It implements it’s own resolver API.</li>
</ul>

<p>Fortunately, I can ease your mind by letting you know that all
of these will at least respect <code>/etc/resolv.conf</code> and <code>/etc/hosts</code>
configurations. Files that we’ll see in the next sections.</p>



<p><img src="https://venam.nixers.net/blog/assets/resolving_hostname/What_Does_It_Take_To_Resolve_A_Hostname5.jpg" alt="slide5"></p>

<p>I’ve taken a look at over a dozen different technologies and I think the
best way to understand them is through their archaeologies. There’s a
lot that can be explained about DNS resolution simply based on all the
historic reasons.<br>
The main thing you need to understand, is that there’s not a single
clean library call to resolve a hostname. Standards and new specs have
pilled up over the years, with some software that haven’t followed,
but risking to disappear.</p>

<p>Overall, libc and POSIX provide multiple resolution APIs:</p>

<ul>
  <li>There’s the historic, low level one provided by ISC/BSD BIND resolver
implementation within libc. Accessed though <code>libresolv/resolv.h</code>
incantation.</li>
  <li>The <code>gethostbyname(3)</code> and related functions, implementing an obsolete
POSIX C specification.</li>
  <li>The <code>getaddrinfo(3)</code>, that is the modern POSIX C API for name
resolution.</li>
</ul>

<p>All these combinations, ladies and gentlemen, are the standard ways
to resolve a name.<br>
Newer applications will use <code>getaddrinfo</code> while older ones will use
<code>gethostbyname</code>. Both of these 2 will often rely on something called
NSS and another part to manage <code>resolv.conf</code> access.</p>

<p>Now let’s dive into each of these and you’ll get them like a breeze.</p>



<p><img src="https://venam.nixers.net/blog/assets/resolving_hostname/What_Does_It_Take_To_Resolve_A_Hostname6.jpg" alt="slide6"></p>

<p>The resolver layer is the oldest and most stable in our quest. It
originates from 1983, today almost 37 years ago, at Berkeley university.</p>

<p>It comes from a project called BIND, Berkely Internet Name Domain, which
was sponsored by a DARPA grants. And like the Berkeley socket that gave
rise to the internet, it has now turned into much much pain.<br>
It was the very first implementation of the DNS specifications. It got
released in BSD4.3 and today the BIND project is maintained by the
Internet Systems Consortium, aka ISC.</p>

<p>It not only offers servers and clients, and the debug tools which we
mentioned earlier, but also offers a library called “libbind”. This
library is the defacto implementation, the standard resolver, the one
of a kind. It is initially based on all the original RFC discussions,
namely RFC 881, 882, and 883.<br>
The BSD people wrote technical papers assessing its feasibility, and
went on recommending and implementing it within BSD.</p>

<p>At that point BIND wasn’t a standard yet, it was an optionally-compiled
code for those who wanted to get their feet wet, those who wanted to
try DNS.<br>
Then it got part of the C standard library interface through <code>resolver</code>,
<code>libresolv</code>, <code>-lresolv</code>, <code>resolv.h</code>, and closed the case</p>

<p>If you take a look at most Unix-like systems today, from MacOS, to
OpenBSD, to Linux, and company, you’ll see clearly in <code>resolv.h</code>, the
copyright going back to 1983, to that very date. But obviously, it depends
on the choice of the implementer, a case by case</p>

<p>So then the code diverged, there’s the libresolv provided
by the C standardization and the libbind provided by the BIND
implementation. However, most Unix only add small specific changes to
their needs. For example, resolver in glibc is baselined off libbind
from BIND version 8.2.3.</p>

<p>This layer is normally used for low level DNS interactions because it’s
missing the goodies we’ll see later in this presentation.</p>

<p>Now let’s talk about environments and configurations.</p>

<p>The resolver configuration file</p>

<p>The resolver configuration files were mentioned in BIND first release,
in section 4.2.2.2 of “The Design and Implementation of ‘Domain Name
Resolver’” by Mark Painter based on RFC883, part of the DNS RFC series.</p>

<p>This particular file being <code>/etc/resolv.conf</code>, you’ll see it hardcoded in
<code>resolv.h</code> and if that file is missing, it’ll fall back to the localhost
as the DNS, just to be safe.<br>
Additionally, there’s <code>/etc/host.conf</code>, according to the manpage also
“the resolver configuration file”, it’s so appropriately named. It’s a
conf that dictates the working of <code>/etc/hosts</code>, the “static table lookup
for hostsnames”.</p>

<p>So what’s in these files.<br>
<code>resolv.conf</code> takes care of how to resolve names and which <code>nameserver</code>
to use for that, while <code>hosts</code> simply has a list of known host aliases,
ip + name, as simple as that.</p>

<p>Within <code>resolv.conf</code> you can also have a <code>search</code> list for domains.
That’s if a name you’re searching for doesn’t have the minimum number
of dots in it then it’ll add one of these TLD to it, top-level-domains,
and keep searching until it finds something that fits.<br>
This can also be manipulated in an environment variable <code>LOCALDOMAIN</code>.</p>

<figure><pre><code data-lang="shell"><span>$ </span><span>echo</span> <span>'example www.example.com'</span> <span>&gt;</span> ./host_aliases
<span>$ HOSTALIASES</span><span>=</span><span>"./host_aliases"</span> getent hosts example
93.184.216.34   www.example.com</code></pre></figure>

<p>There can also be a sortlist IP netmask, for when there’s many results
to match but you don’t want to give priority to the cloud VPS that lives
only for cash.</p>

<p>Finally, there’s the <code>option</code> field, also overriden on the command
line by the <code>RES_OPTIONS</code> environment variable. It manipulates the minimum
number of dots we mentioned and also if you want can set debug as enabled.</p>

<p>Meanwhile, the <code>hosts</code> file is but a key-value db, simply made of domain
names and IPs.</p>

<p>Its config also lets you change the order of results and for the rest
you have <code>host.conf</code> to consult.</p>

<p>So remember, that all of these are mostly used everywhere because it’s
the lowest layer.  So it’s used by libbind and libresolv but also the
custom NIH syndrome</p>

<p>Alright, so far that’s all classic clean stuff. Let’s move on to the
next sections, you’ll scratch your head until there’s no dandruff.</p>



<p><img src="https://venam.nixers.net/blog/assets/resolving_hostname/What_Does_It_Take_To_Resolve_A_Hostname7.jpg" alt="slide7"></p>

<p>The C library POSIX specs create a superset over the C standard
library. They add a few simpler calls to resolve hostnames and make it
easy. These focus on returning A and AAAA records only, ipV4 and ipV6
respsectively.<br>
There’s <code>gethostbyname(3)</code> which is deprecated, and there’s the newer
<code>getaddrinfo(3)</code> defined in IEEE Std 1003.1g-2000, which mainly adds
RFC3493 aka ipV6 is now supported.  So applications are recommended to
use this updated version unless they want to divert from mainland.</p>

<p>There are functions to resolve IP addresses to host names, but let’s
focus only on name to ip for today, I know it’s lame.</p>

<p>Apart from ipV6 support being added, some internal structures have been
updated as they weren’t so safe between subsequent calls and thus could
be your demise and your fall.</p>

<p>Obviously they both return different structures.</p>

<p><code>hostent</code> struct is returned to <code>gethostbyname</code> function caller.
while <code>getaddrinfo</code> returns an <code>addrinfo</code> structure.
Both being defined in the <code>netdb.h</code> header.</p>

<figure><pre><code data-lang="c"><span>struct</span> <span>hostent</span> <span>{</span>
	<span>char</span>  <span>*</span><span>h_name</span><span>;</span>            <span>/* official name of host */</span>
	<span>char</span> <span>**</span><span>h_aliases</span><span>;</span>         <span>/* alias list */</span>
	<span>int</span>    <span>h_addrtype</span><span>;</span>        <span>/* host address type */</span>
	<span>int</span>    <span>h_length</span><span>;</span>          <span>/* length of address */</span>
	<span>char</span> <span>**</span><span>h_addr_list</span><span>;</span>       <span>/* list of addresses */</span>
<span>}</span></code></pre></figure>

<figure><pre><code data-lang="c"><span>struct</span> <span>addrinfo</span> <span>{</span>
	<span>int</span>              <span>ai_flags</span><span>;</span>
	<span>int</span>              <span>ai_family</span><span>;</span>
	<span>int</span>              <span>ai_socktype</span><span>;</span>
	<span>int</span>              <span>ai_protocol</span><span>;</span>
	<span>socklen_t</span>        <span>ai_addrlen</span><span>;</span>
	<span>struct</span> <span>sockaddr</span> <span>*</span><span>ai_addr</span><span>;</span>
	<span>char</span>            <span>*</span><span>ai_canonname</span><span>;</span>
	<span>struct</span> <span>addrinfo</span> <span>*</span><span>ai_next</span><span>;</span>
<span>};</span></code></pre></figure>

<p>Some libc implementations will get fancy and add their own modified
versions of <code>gethostbyname</code>. For instance in glibc they add support for
ipV6 in their modified <code>gethostbyname2</code> for backward compatibility.</p>

<p>Regarding configuration files, <code>getaddrinfo</code> will consult <code>/etc/gai.conf</code>
which takes care of the precedence of the addresses returned in the
results. And now, you’re going to brandish your …</p></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://venam.nixers.net/blog/unix/2020/11/01/resolving-a-hostname.html">https://venam.nixers.net/blog/unix/2020/11/01/resolving-a-hostname.html</a></em></p>]]>
            </description>
            <link>https://venam.nixers.net/blog/unix/2020/11/01/resolving-a-hostname.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24965056</guid>
            <pubDate>Mon, 02 Nov 2020 05:28:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Rendering photo-realistic glass in the browser]]>
            </title>
            <description>
<![CDATA[
Score 375 | Comments 105 (<a href="https://news.ycombinator.com/item?id=24965005">thread link</a>) | @anonytrary
<br/>
November 1, 2020 | https://domenicobrz.github.io/webgl/projects/SSRefractionDepthPeeling/ | <a href="https://web.archive.org/web/*/https://domenicobrz.github.io/webgl/projects/SSRefractionDepthPeeling/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://domenicobrz.github.io/webgl/projects/SSRefractionDepthPeeling/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24965005</guid>
            <pubDate>Mon, 02 Nov 2020 05:13:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why Leaders Need to Learn the Skill of Writing]]>
            </title>
            <description>
<![CDATA[
Score 118 | Comments 29 (<a href="https://news.ycombinator.com/item?id=24964730">thread link</a>) | @mooreds
<br/>
November 1, 2020 | https://fromthegreennotebook.com/2020/10/03/why-leaders-need-to-learn-the-skill-of-writing/ | <a href="https://web.archive.org/web/*/https://fromthegreennotebook.com/2020/10/03/why-leaders-need-to-learn-the-skill-of-writing/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

	<section id="primary">
		<main id="main" role="main">

		
			<article id="post-6068">
	
	
		

	<div>
		<p><img data-attachment-id="6071" data-permalink="https://fromthegreennotebook.com/2020/10/03/why-leaders-need-to-learn-the-skill-of-writing/shutterstock_55915930-e1415052560114-2/" data-orig-file="https://i0.wp.com/fromthegreennotebook.com/wp-content/uploads/2020/10/shutterstock_55915930-e1415052560114-2.jpg?fit=3862%2C1974&amp;ssl=1" data-orig-size="3862,1974" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="shutterstock_55915930-e1415052560114-2" data-image-description="" data-medium-file="https://i0.wp.com/fromthegreennotebook.com/wp-content/uploads/2020/10/shutterstock_55915930-e1415052560114-2.jpg?fit=300%2C153&amp;ssl=1" data-large-file="https://i0.wp.com/fromthegreennotebook.com/wp-content/uploads/2020/10/shutterstock_55915930-e1415052560114-2.jpg?fit=723%2C369&amp;ssl=1" loading="lazy" src="https://i0.wp.com/fromthegreennotebook.com/wp-content/uploads/2020/10/shutterstock_55915930-e1415052560114-2.jpg?resize=715%2C365&amp;ssl=1" alt="" width="715" height="365" srcset="https://i0.wp.com/fromthegreennotebook.com/wp-content/uploads/2020/10/shutterstock_55915930-e1415052560114-2.jpg?w=3862&amp;ssl=1 3862w, https://i0.wp.com/fromthegreennotebook.com/wp-content/uploads/2020/10/shutterstock_55915930-e1415052560114-2.jpg?resize=300%2C153&amp;ssl=1 300w, https://i0.wp.com/fromthegreennotebook.com/wp-content/uploads/2020/10/shutterstock_55915930-e1415052560114-2.jpg?resize=1024%2C523&amp;ssl=1 1024w, https://i0.wp.com/fromthegreennotebook.com/wp-content/uploads/2020/10/shutterstock_55915930-e1415052560114-2.jpg?resize=768%2C393&amp;ssl=1 768w, https://i0.wp.com/fromthegreennotebook.com/wp-content/uploads/2020/10/shutterstock_55915930-e1415052560114-2.jpg?resize=1536%2C785&amp;ssl=1 1536w, https://i0.wp.com/fromthegreennotebook.com/wp-content/uploads/2020/10/shutterstock_55915930-e1415052560114-2.jpg?resize=2048%2C1047&amp;ssl=1 2048w, https://i0.wp.com/fromthegreennotebook.com/wp-content/uploads/2020/10/shutterstock_55915930-e1415052560114-2.jpg?w=1446&amp;ssl=1 1446w, https://i0.wp.com/fromthegreennotebook.com/wp-content/uploads/2020/10/shutterstock_55915930-e1415052560114-2.jpg?w=2169&amp;ssl=1 2169w" sizes="(max-width: 715px) 100vw, 715px" data-recalc-dims="1"></p>
<p>By Joe Byerly</p>
<p>Anyone who has worked directly for a battalion commander or above probably has experience writing “ghost notes.” These are emails a subordinate writes and addresses for their boss to send to other people. Ghost notes can be weekly or monthly sitreps, updates on an ongoing situation or emails asking for additional resources. No matter the type, they are the “easy button” for the commander because all they have to do is hit “send.”</p>
<p>Recently, I worked for a senior Army leader who encouraged his subordinate commanders to own their communications—meaning, write their own emails. As I reflected on his guidance, I realized there are benefits to communications ownership. I witnessed many of these benefits firsthand as I watched him communicate with senior military leaders, senior civilian leaders and his own commanders.<span data-ez-name="fromthegreennotebook_com-medrectangle-3"></span></p>

<p><strong>Greatness and Writing</strong></p>
<p>One of the best ways to work through a problem is to write it down. Throughout history, leaders who found themselves in tough situations sat alone with their thoughts and worked through them using pen and paper.<span data-ez-name="fromthegreennotebook_com-medrectangle-4"></span></p>
<p>Marcus Aurelius, who served as Roman emperor for almost two decades, wrote his <em>Meditations</em> to work through daily leadership challenges, wars and a pandemic. In the week leading up to the D-Day landings, Gen. Dwight Eisenhower wrote himself letters to help work through risks, opportunities and necessities of operations.</p>
<p>Both Marcus and Eisenhower used writing to achieve clarity of thought. This point is underscored by author Stephen King, who has said writing is “refined thinking.” In our minds, our thoughts are clear, but real clarity doesn’t come until those thoughts are solidified in writing. The process of framing an email, capturing important points and discarding nonessential elements helps us gain more clarity.</p>
<p><strong>Sound Authentic</strong></p>
<p>Over the years, I have worked under multiple commanders while in staff positions, and the best ones never let me draft their intent for operations orders. They wanted to own those. At the time, I didn’t understand it—thinking it was one more staff drill I could handle for them. But as I gained experience, I realized they wanted that section of the operations order to reflect their voice.<span data-ez-name="fromthegreennotebook_com-box-4"></span></p>
<p>We all have a voice when we write. This voice is our certificate of authenticity. When commanders write their own correspondence, their voice comes through. When someone else drafts an email, using words and phrases the commander wouldn’t normally use, others can tell someone else wrote it.</p>
<p>Authenticity in communications is important for two reasons. First, subordinates will know if the intent, the guidance, the policy, etc., is the commander’s, or if it is another product produced by staff. They are more likely to follow it and adhere to it when they know it comes directly from the commander’s mind and is not a draft by a random staff officer.</p>
<p>Also, commanders who write their own communications tend to reinforce the message by repeating what they wrote. The senior Army leader I worked for occasionally wrote guidance that he sent out in an email. He then repeated key words and phrases from the document in meetings, during battlefield circulation and in one-on-one discussions. Everyone knew he wrote the email because he owned it and talked about it; his guidance didn’t become memorandums left on a bulletin board in a headquarters.</p>
<p>The second reason authenticity in communications is important is that it signals leader involvement in an issue. I have learned that many senior leaders can tell when a subordinate commander’s email is authentic or a staff-produced ghost note.</p>
<p>Every time there was a change or inflection point in the strategic situation, my boss would provide a one- or two-page update to his commanders. He always wrote these himself, for the reasons mentioned above. I found out from those commanders’ staff members that their bosses read these emails because they knew it was from him, and that if he took the time to write it, they should take the time to read it.</p>
<p>Communication can be frustrating. Sometimes it is like tapping out a song you have in your head and expecting another person to immediately know the tune. It is hard to convey an idea in your head to someone who may not have the same background or experiences as you, or who wasn’t in the same room when you had a conversation.</p>
<p>Communication is a skill that takes practice. We need repetition. Leaders who write their own emails gain needed communication experience when it matters. I have also learned that speed comes with practice. I can write in hours what used to take days.</p>
<p><strong>Honing the Skill</strong></p>
<p>I recognize that commanders have a lot on their plates, and it isn’t feasible for them to spend hours writing and responding to emails. There are many ghost notes best produced by staff in the interest of time. However, when it comes to communicating up or down the chain of command on key issues, or writing guidance on important topics, it is best for commanders to own those.</p>
<p>Great leaders are also great communicators, but the ability to communicate effectively and efficiently takes time to develop. By owning communications, commanders refine and hone this skill. They also have an opportunity to work through problems and refine their thinking through the process of writing. Finally, they gain authenticity in their communications—an important factor in ensuring that “message sent” is “message received.”</p>
<p>This article was first published in <a href="https://www.ausa.org/articles/leaders-subject-write-your-own-emails">ARMY Magazine</a> and reprinted with their permission.</p>



	





		
		
					</div><!-- .entry-content -->

	<!-- .entry-footer -->

</article><!-- #post-## -->

			
<!-- #comments -->

		
		</main><!-- #main -->
	</section><!-- #primary -->

	<!-- #secondary --></div></div>]]>
            </description>
            <link>https://fromthegreennotebook.com/2020/10/03/why-leaders-need-to-learn-the-skill-of-writing/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24964730</guid>
            <pubDate>Mon, 02 Nov 2020 04:03:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Names are not type safety]]>
            </title>
            <description>
<![CDATA[
Score 256 | Comments 121 (<a href="https://news.ycombinator.com/item?id=24963821">thread link</a>) | @azhenley
<br/>
November 1, 2020 | http://lexi-lambda.github.io/blog/2020/11/01/names-are-not-type-safety/ | <a href="https://web.archive.org/web/*/http://lexi-lambda.github.io/blog/2020/11/01/names-are-not-type-safety/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section role="main">
        <!-- Main column -->
        <div>



          <article>
  <header>
    
    
  </header>

<p>Haskell programmers spend a lot of time talking about <em>type safety</em>. The Haskell school of program construction advocates “capturing invariants in the type system” and “making illegal states unrepresentable,” both of which sound like compelling goals, but are rather vague on the techniques used to achieve them. Almost exactly one year ago, I published <a href="http://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/">Parse, Don’t Validate</a> as an initial stab towards bridging that gap.</p>

<p>The ensuing discussions were largely productive and right-minded, but one particular source of confusion quickly became clear: Haskell’s <code>newtype</code> construct. The idea is simple enough—the <code>newtype</code> keyword declares a wrapper type, nominally distinct from but representationally equivalent to the type it wraps—and on the surface this <em>sounds</em> like a simple and straightforward path to type safety. For example, one might consider using a <code>newtype</code> declaration to define a type for an email address:</p>

<div>
 <div>
  <pre><span></span><span>newtype</span> <span>EmailAddress</span> <span>=</span> <span>EmailAddress</span> <span>Text</span>
</pre></div>

</div>

<p>This technique can provide <em>some</em> value, and when coupled with a smart constructor and an encapsulation boundary, it can even provide some safety. But it is a meaningfully distinct <em>kind</em> of type safety from the one I highlighted a year ago, one that is far weaker. On its own, a newtype is just a name.</p>

<p>And names are not type safety.</p>
<!-- more-->



<p>To illustrate the difference between constructive data modeling (discussed at length in my <a href="http://lexi-lambda.github.io/blog/2020/08/13/types-as-axioms-or-playing-god-with-static-types/">previous blog post</a>) and newtype wrappers, let’s consider an example. Suppose we want a type for “an integer between 1 and 5, inclusive.” The natural constructive modeling would be an enumeration with five cases:</p>

<div>
 <div>
  <pre><span></span><span>data</span> <span>OneToFive</span>
  <span>=</span> <span>One</span>
  <span>|</span> <span>Two</span>
  <span>|</span> <span>Three</span>
  <span>|</span> <span>Four</span>
  <span>|</span> <span>Five</span>
</pre></div>

</div>

<p>We could then write some functions to convert between <code>Int</code> and our <code>OneToFive</code> type:</p>

<div>
 <div>
  <pre><span></span><span>toOneToFive</span> <span>::</span> <span>Int</span> <span>-&gt;</span> <span>Maybe</span> <span>OneToFive</span>
<span>toOneToFive</span> <span>1</span> <span>=</span> <span>Just</span> <span>One</span>
<span>toOneToFive</span> <span>2</span> <span>=</span> <span>Just</span> <span>Two</span>
<span>toOneToFive</span> <span>3</span> <span>=</span> <span>Just</span> <span>Three</span>
<span>toOneToFive</span> <span>4</span> <span>=</span> <span>Just</span> <span>Four</span>
<span>toOneToFive</span> <span>5</span> <span>=</span> <span>Just</span> <span>Five</span>
<span>toOneToFive</span> <span>_</span> <span>=</span> <span>Nothing</span>

<span>fromOneToFive</span> <span>::</span> <span>OneToFive</span> <span>-&gt;</span> <span>Int</span>
<span>fromOneToFive</span> <span>One</span>   <span>=</span> <span>1</span>
<span>fromOneToFive</span> <span>Two</span>   <span>=</span> <span>2</span>
<span>fromOneToFive</span> <span>Three</span> <span>=</span> <span>3</span>
<span>fromOneToFive</span> <span>Four</span>  <span>=</span> <span>4</span>
<span>fromOneToFive</span> <span>Five</span>  <span>=</span> <span>5</span>
</pre></div>

</div>

<p>This would be perfectly sufficient for achieving our stated goal, but you’d be forgiven for finding it odd: it would be rather awkward to work with in practice. Because we’ve invented an entirely new type, we can’t reuse any of the usual numeric functions Haskell provides. Consequently, many programmers would gravitate towards a newtype wrapper, instead:</p>

<div>
 <div>
  <pre><span></span><span>newtype</span> <span>OneToFive</span> <span>=</span> <span>OneToFive</span> <span>Int</span>
</pre></div>

</div>

<p>Just as before, we can provide <code>toOneToFive</code> and <code>fromOneToFive</code> functions, with identical types:</p>

<div>
 <div>
  <pre><span></span><span>toOneToFive</span> <span>::</span> <span>Int</span> <span>-&gt;</span> <span>Maybe</span> <span>OneToFive</span>
<span>toOneToFive</span> <span>n</span>
  <span>|</span> <span>n</span> <span>&gt;=</span> <span>1</span> <span>&amp;&amp;</span> <span>n</span> <span>&lt;=</span> <span>5</span> <span>=</span> <span>Just</span> <span>$</span> <span>OneToFive</span> <span>n</span>
  <span>|</span> <span>otherwise</span>        <span>=</span> <span>Nothing</span>

<span>fromOneToFive</span> <span>::</span> <span>OneToFive</span> <span>-&gt;</span> <span>Int</span>
<span>fromOneToFive</span> <span>(</span><span>OneToFive</span> <span>n</span><span>)</span> <span>=</span> <span>n</span>
</pre></div>

</div>

<p>If we put these declarations in their own module and choose not to export the <code>OneToFive</code> constructor, these APIs might appear entirely interchangeable. Naïvely, it seems that the newtype version is both simpler and equally type-safe. However—perhaps surprisingly—this is not actually true.</p>

<p>To see why, suppose we write a function that consumes a <code>OneToFive</code> value as an argument. Under the constructive modeling, such a function need only pattern-match against each of the five constructors, and GHC will accept the definition as exhaustive:</p>

<div>
 <div>
  <pre><span></span><span>ordinal</span> <span>::</span> <span>OneToFive</span> <span>-&gt;</span> <span>Text</span>
<span>ordinal</span> <span>One</span>   <span>=</span> <span>"first"</span>
<span>ordinal</span> <span>Two</span>   <span>=</span> <span>"second"</span>
<span>ordinal</span> <span>Three</span> <span>=</span> <span>"third"</span>
<span>ordinal</span> <span>Four</span>  <span>=</span> <span>"fourth"</span>
<span>ordinal</span> <span>Five</span>  <span>=</span> <span>"fifth"</span>
</pre></div>

</div>

<p>The same is not true given the newtype encoding. The newtype is opaque, so the only way to observe it is to convert it back to an <code>Int</code>—after all, it <em>is</em> an <code>Int</code>. An <code>Int</code> can of course contain many other values besides <code>1</code> through <code>5</code>, so we are forced to add an error case to satisfy the exhaustiveness checker:</p>

<div>
 <div>
  <pre><span></span><span>ordinal</span> <span>::</span> <span>OneToFive</span> <span>-&gt;</span> <span>Text</span>
<span>ordinal</span> <span>n</span> <span>=</span> <span>case</span> <span>fromOneToFive</span> <span>n</span> <span>of</span>
  <span>1</span> <span>-&gt;</span> <span>"first"</span>
  <span>2</span> <span>-&gt;</span> <span>"second"</span>
  <span>3</span> <span>-&gt;</span> <span>"third"</span>
  <span>4</span> <span>-&gt;</span> <span>"fourth"</span>
  <span>5</span> <span>-&gt;</span> <span>"fifth"</span>
  <span>_</span> <span>-&gt;</span> <span>error</span> <span>"impossible: bad OneToFive value"</span>
</pre></div>

</div>

<p>In this highly contrived example, this may not seem like much of a problem to you. But it nonetheless illustrates a key difference in the guarantees afforded by the two approaches:</p>

<ul>
 <li>
  <p>The constructive datatype captures its invariants in such a way that they are <em>accessible</em> to downstream consumers. This frees our <code>ordinal</code> function from worrying about handling illegal values, as they have been made unutterable.</p></li>
 <li>
  <p>The newtype wrapper provides a smart constructor that <em>validates</em> the value, but the boolean result of that check is used only for control flow; it is not preserved in the function’s result. Accordingly, downstream consumers cannot take advantage of the restricted domain; they are functionally accepting <code>Int</code>s.</p></li></ul>

<p>Losing exhaustiveness checking might seem like small potatoes, but it absolutely is not: our use of <code>error</code> has punched a hole right through our type system. If we were to add another constructor to our <code>OneToFive</code> datatype,<sup><a href="#2020-11-01-names-are-not-type-safety-footnote-1-definition" name="2020-11-01-names-are-not-type-safety-footnote-1-return">1</a></sup> the version of <code>ordinal</code> that consumes a constructive datatype would be immediately detected non-exhaustive at compile-time, while the version that consumes a newtype wrapper would continue to compile yet fail at runtime, dropping through to the “impossible” case.</p>

<p>All of this is a consequence of the fact that the constructive modeling is <em>intrinsically</em> type-safe; that is, the safety properties are enforced by the type declaration itself. Illegal values truly are unrepresentable: there is simply no way to represent <code>6</code> using any of the five constructors. The same is not true of the newtype declaration, which has no intrinsic semantic distinction from that of an <code>Int</code>; its meaning is specified extrinsically via the <code>toOneToFive</code> smart constructor. Any semantic distinction intended by a newtype is thoroughly invisible to the type system; it exists only in the programmer’s mind.</p>

<h2 id="revisiting-non-empty-lists">Revisiting non-empty lists</h2>

<p>Our <code>OneToFive</code> datatype is rather artificial, but identical reasoning applies to other datatypes that are significantly more practical. Consider the <code>NonEmpty</code> datatype I’ve repeatedly highlighted in recent blog posts:</p>

<div>
 <div>
  <pre><span></span><span>data</span> <span>NonEmpty</span> <span>a</span> <span>=</span> <span>a</span> <span>:|</span> <span>[</span><span>a</span><span>]</span>
</pre></div>

</div>

<p>It may be illustrative to imagine a version of <code>NonEmpty</code> represented as a newtype over ordinary lists. We can use the usual smart constructor strategy to enforce the desired non-emptiness property:</p>

<div>
 <div>
  <pre><span></span><span>newtype</span> <span>NonEmpty</span> <span>a</span> <span>=</span> <span>NonEmpty</span> <span>[</span><span>a</span><span>]</span>

<span>nonEmpty</span> <span>::</span> <span>[</span><span>a</span><span>]</span> <span>-&gt;</span> <span>Maybe</span> <span>(</span><span>NonEmpty</span> <span>a</span><span>)</span>
<span>nonEmpty</span> <span>[]</span> <span>=</span> <span>Nothing</span>
<span>nonEmpty</span> <span>xs</span> <span>=</span> <span>Just</span> <span>$</span> <span>NonEmpty</span> <span>xs</span>

<span>instance</span> <span>Foldable</span> <span>NonEmpty</span> <span>where</span>
  <span>toList</span> <span>(</span><span>NonEmpty</span> <span>xs</span><span>)</span> <span>=</span> <span>xs</span>
</pre></div>

</div>

<p>Just as with <code>OneToFive</code>, we quickly discover the consequences of failing to preserve this information in the type system. Our motivating use case for <code>NonEmpty</code> was the ability to write a safe version of <code>head</code>, but the newtype version requires another assertion:</p>

<div>
 <div>
  <pre><span></span><span>head</span> <span>::</span> <span>NonEmpty</span> <span>a</span> <span>-&gt;</span> <span>a</span>
<span>head</span> <span>xs</span> <span>=</span> <span>case</span> <span>toList</span> <span>xs</span> <span>of</span>
  <span>x</span><span>:</span><span>_</span> <span>-&gt;</span> <span>x</span>
  <span>[]</span>  <span>-&gt;</span> <span>error</span> <span>"impossible: empty NonEmpty value"</span>
</pre></div>

</div>

<p>This might not seem like a big deal, since it seems unlikely such a case would ever happen. But that reasoning hinges entirely on trusting the correctness of the module that defines <code>NonEmpty</code>, while the constructive definition only requires trusting the GHC typechecker. As we generally trust that the typechecker works correctly, the latter is a much more compelling proof.</p>



<p>If you are fond of newtypes, this whole argument may seem a bit troubling. It may seem like I’m implying newtypes are scarcely better than comments, albeit comments that happen to be meaningful to the typechecker. Fortunately, the situation is not quite that grim—newtypes <em>can</em> provide a sort of safety, just a weaker one.</p>

<p>The primary safety benefit of newtypes is derived from abstraction boundaries. If a newtype’s constructor is not exported, it becomes opaque to other modules. The module that defines the newtype—its “home module”—can take advantage of this to create a <em>trust boundary</em> where internal invariants are enforced by restricting clients to a safe API.</p>

<p>We can use the <code>NonEmpty</code> example from above to illustrate how this works. We refrain from exporting the <code>NonEmpty</code> constructor, and we provide <code>head</code> and <code>tail</code> operations that we trust to never actually fail:</p>

<div>
 <div>
  <pre><span></span><span>module</span> <span>Data.List.NonEmpty.Newtype</span>
  <span>(</span> <span>NonEmpty</span>
  <span>,</span> <span>cons</span>
  <span>,</span> <span>nonEmpty</span>
  <span>,</span> <span>head</span>
  <span>,</span> <span>tail</span>
  <span>)</span> <span>where</span>

<span>newtype</span> <span>NonEmpty</span> <span>a</span> <span>=</span> <span>NonEmpty</span> <span>[</span><span>a</span><span>]</span>

<span>cons</span> <span>::</span> <span>a</span> <span>-&gt;</span> <span>[</span><span>a</span><span>]</span> <span>-&gt;</span> <span>NonEmpty</span> <span>a</span>
<span>cons</span> <span>x</span> <span>xs</span> <span>=</span> <span>NonEmpty</span> <span>(</span><span>x</span><span>:</span><span>xs</span><span>)</span>

<span>nonEmpty</span> <span>::</span> <span>[</span><span>a</span><span>]</span> <span>-&gt;</span> <span>Maybe</span> <span>(</span><span>NonEmpty</span> <span>a</span><span>)</span>
<span>nonEmpty</span> <span>[]</span> <span>=</span> <span>Nothing</span>
<span>nonEmpty</span> <span>xs</span> <span>=</span> <span>Just</span> <span>$</span> <span>NonEmpty</span> <span>xs</span>

<span>head</span> <span>::</span> <span>NonEmpty</span> <span>a</span> <span>-&gt;</span> <span>a</span>
<span>head</span> <span>(</span><span>NonEmpty</span> <span>(</span><span>x</span><span>:</span><span>_</span><span>))</span> <span>=</span> <span>x</span>
<span>head</span> <span>(</span><span>NonEmpty</span> <span>[]</span><span>)</span>    <span>=</span> <span>error</span> <span>"impossible: empty NonEmpty value"</span>

<span>tail</span> <span>::</span> <span>NonEmpty</span> <span>a</span> <span>-&gt;</span> <span>[</span><span>a</span><span>]</span>
<span>tail</span> <span>(</span><span>NonEmpty</span> <span>(</span><span>_</span><span>:</span><span>xs</span><span>))</span> <span>=</span> <span>xs</span>
<span>tail</span> <span>(</span><span>NonEmpty</span> <span>[]</span><span>)</span>     <span>=</span> <span>error</span> <span>"impossible: empty NonEmpty value"</span>
</pre></div>

</div>

<p>Since the only way to construct or consume <code>NonEmpty</code> values is to use the functions in <code>Data.List.NonEmpty.Newtype</code>’s exported API, the above implementation makes it impossible for clients to violate the non-emptiness invariant. In a sense, values of opaque newtypes are like <em>tokens</em>: the implementing module issues tokens via its constructor functions, and those tokens have no intrinsic value. The only way to do anything useful with them is to “redeem” them to the issuing module’s accessor functions, in this case <code>head</code> and <code>tail</code>, to obtain the values contained within.</p>

<p>This approach is significantly weaker than using a constructive datatype, since it is theoretically possible to screw up and accidentally provide a means to construct an invalid <code>NonEmpty []</code> value. For this reason, the newtype approach to type safety does not on its own constitute a <em>proof</em> that a desired invariant holds. However, it restricts the “surface area” where an invariant violation can occur to the defining module, so reasonable confidence the invariant really does hold can be achieved by thoroughly testing the module’s API using fuzzing or property-based testing techniques.<sup><a href="#2020-11-01-names-are-not-type-safety-footnote-2-definition" name="2020-11-01-names-are-not-type-safety-footnote-2-return">2</a></sup></p>

<p>This tradeoff may not seem all that bad, and indeed, it is often a very good one! Guaranteeing invariants using …</p></article></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://lexi-lambda.github.io/blog/2020/11/01/names-are-not-type-safety/">http://lexi-lambda.github.io/blog/2020/11/01/names-are-not-type-safety/</a></em></p>]]>
            </description>
            <link>http://lexi-lambda.github.io/blog/2020/11/01/names-are-not-type-safety/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24963821</guid>
            <pubDate>Mon, 02 Nov 2020 00:27:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Microservices – architecture nihilism in minimalism's clothes]]>
            </title>
            <description>
<![CDATA[
Score 226 | Comments 151 (<a href="https://news.ycombinator.com/item?id=24963742">thread link</a>) | @zdw
<br/>
November 1, 2020 | https://vlfig.me/posts/microservices | <a href="https://web.archive.org/web/*/https://vlfig.me/posts/microservices">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Some recent <a href="https://twitter.com/gergelyorosz/status/1247132806041546754" target="_blank" rel="nofollow noopener noreferrer">backtracking</a> <a href="https://twitter.com/copyconstruct/status/1247130488667394049" target="_blank" rel="nofollow noopener noreferrer">from</a> what we have been calling “Microservices” has sparked anew the debate around that software architecture pattern. It turns out that for increasingly more software people, having a backend with (<a href="https://www.infoq.com/presentations/monzo-microservices/" target="_blank" rel="nofollow noopener noreferrer">sometimes several</a>) hundreds of services wasn’t that great an idea after all. The debate has <a href="https://riak.com/posts/technical/microservices-please-dont/" target="_blank" rel="nofollow noopener noreferrer">been going on for a while</a> and much has already been said, but there are still a couple of things I’d like to say.</p>
<p><strong>TL;DR</strong> “Microservices” was a good idea taken too far and applied too bluntly. The fix isn’t just to dial back the granularity knob but instead to 1) focus on the split-join criteria as opposed to size; and 2) differentiate between the project model and the deployment model when applying them.</p>
<p>I explain. Allow me to rant a bit first.</p>


<p>There were three main reasons for the initial success of <em>microservices</em> as an architectural pattern for software: 1) forced modularisation, 2) weakened dependencies, and 3) an excuse for having no architecture. In order:</p>
<ol>
<li>In the monoliths of old, you could in theory enforce module boundaries. You could say that your <code>acme-helpers</code> or <code>acme-data-tools</code> could not depend on <code>acme-domain</code>, say, and you even had some tooling to enforce that, but it was fallible. Especially in big companies where these monoliths spanned more than a team’s cognitive horizon, violations of those boundaries were often a simple <code>import</code> away, and of course rife. Angry architects saw in microservices the promise of making those a thing of the past: now the developer is forced to only deal with the API. Codebases parted ways and calls were made to go down the network stack and back.</li>
<li>
<p>So then, one wouldn’t depend on a fellow service at build time, only at runtime. Great. Method calls became http calls. “Now we don’t need to care about dependencies” — actual people said this, as if the dependency wasn’t fundamental and instead just an accidental artifact of the build setup. Everybody brushed up on their HTTP and different server and client implementations, read all about REST and Soap (and RPC, RMI and CORBA while at it) and merrily created a layer of indirection between modules — now <em>services</em> — that was <em>very</em> loose. Typed APIs, granular network policies and contract testing came much later.</p>
<p>It felt liberating until the complexities of API versioning, delivery semantics, error propagation, distributed transaction management and the sprawl of client code in all callers of a service began to show up. This was a gigantic <strong>shift right</strong>, but hey, the build process was simpler.</p>
</li>
<li>
<p>More insidious perhaps was the validation that “doing microservices” brought to organisations that lacked a thesis about how their architecture should be. There was now a sanctioned answer to most architectural dilemmas: another microservice. Another entry in the service catalog for any and all interested parties to call. This ecology of interacting parties, each acting in their own interest for the common good spoke to an underlying, tacit belief that the emergent mesh of services would approximate the latent natural architecture of the domain.</p>
<p>So soft and convenient was the lure of not having to draw hard architectural lines that we got lazy where we weren’t and accepted our lazyness where we already were. If you didn’t subscribe to that belief, the problem was you and your lack of understanding of complex systems, you objectivist cretin.</p>
</li>
</ol>
<p>Yes, there was real pain in managing monoliths and sure, many systems were too monolithic (i.e. had deployables too large) but the zealotry of a newfound purity swung the pendulum too far, as they always do. Not only do we not need to run so many services so small, we also don’t benefit from isolating their codebases so much. To summarise:</p>
<ol>
<li>having a big flat permissive build is no good reason to split deployables;</li>
<li>weakening dependencies between different parts of our systems is a “shift-right” loan with high interest; and</li>
<li>having a ready answer when the thinking gets tough is a soothing lie that just moves complexity about. There is no substitute to the effortful application of cognitive power to a problem.</li>
</ol>

<p>Two things: focus on the right criteria for splitting a service instead of on its size, and apply those criteria more thoughtfully.</p>
<h2 id="size-is-not-the-answer"><a href="#size-is-not-the-answer" aria-label="size is not the answer permalink"></a>Size is not the answer</h2>
<p>The <em>micro</em> in microservices ought to be at best a prediction, never a goal. We may predict services <em>will be</em> micro but they don’t <em>have to be</em>. <a href="https://kalele.io/microservices-and-microservices/" target="_blank" rel="nofollow noopener noreferrer">Vaugh Vernon is right</a> when he speaks about “cohesion for a reason”.</p>
<p>There should be no prescribed <em>a priori</em> granularity of services. There <em>is</em> no prescribed size of a service. There are instead <strong>good and bad reasons to split</strong> parts of a software system.</p>
<p>So the heuristic is:</p>
<div data-language="text"><pre><code>                      Start one, split with a reason.</code></pre></div>
<p>Conversely, if a reason ceases to exist, consider joining them.</p>
<h2 id="the-missing-hinge"><a href="#the-missing-hinge" aria-label="the missing hinge permalink"></a>The missing hinge</h2>
<p>There are however different realms in which “software systems” exist: they exist both as artifacts we interact with and as artifacts computers interact with. Code and binary. We organise them in different ways: the <strong>project model</strong> (repositories, projects, modules and their dependencies) and the <strong>deployment model</strong> (what production environments look like and how deployables run in them).</p>
<figure>
    <span>
      <a href="https://vlfig.me/static/8123a14a0e4efcacfddc6f26f7ad3e78/6246a/monolith.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="A monolithic setup where one big repo builds one single big deployable." title="A monolithic setup where one big repo builds one single big deployable." src="https://vlfig.me/static/8123a14a0e4efcacfddc6f26f7ad3e78/6a068/monolith.jpg" srcset="https://vlfig.me/static/8123a14a0e4efcacfddc6f26f7ad3e78/09b79/monolith.jpg 240w,
https://vlfig.me/static/8123a14a0e4efcacfddc6f26f7ad3e78/7cc5e/monolith.jpg 480w,
https://vlfig.me/static/8123a14a0e4efcacfddc6f26f7ad3e78/6a068/monolith.jpg 960w,
https://vlfig.me/static/8123a14a0e4efcacfddc6f26f7ad3e78/644c5/monolith.jpg 1440w,
https://vlfig.me/static/8123a14a0e4efcacfddc6f26f7ad3e78/6246a/monolith.jpg 1463w" sizes="(max-width: 960px) 100vw, 960px" loading="lazy">
  </a>
    </span>
    <figcaption><p>A monolithic setup where one big repo builds one single big deployable.</p></figcaption>
  </figure>
<p>In the process of going from coarse to granular (i.e. from monolith to microservices) however, little attention was paid to the difference — and possible indirection — between those two models. The hammer hit both fairly indiscriminately and made us split codebases because of runtime concerns and split deployables due to project concerns.</p>
<figure>
    <span>
      <a href="https://vlfig.me/static/fd7e19a50516d91d0b3a84dd16e262ca/6c0a0/stiff-1-1.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="A set of repositories each building their own self-contained independent service." title="Excessive mirroring between the project and deployment models." src="https://vlfig.me/static/fd7e19a50516d91d0b3a84dd16e262ca/6a068/stiff-1-1.jpg" srcset="https://vlfig.me/static/fd7e19a50516d91d0b3a84dd16e262ca/09b79/stiff-1-1.jpg 240w,
https://vlfig.me/static/fd7e19a50516d91d0b3a84dd16e262ca/7cc5e/stiff-1-1.jpg 480w,
https://vlfig.me/static/fd7e19a50516d91d0b3a84dd16e262ca/6a068/stiff-1-1.jpg 960w,
https://vlfig.me/static/fd7e19a50516d91d0b3a84dd16e262ca/6c0a0/stiff-1-1.jpg 1062w" sizes="(max-width: 960px) 100vw, 960px" loading="lazy">
  </a>
    </span>
    <figcaption><p>Excessive mirroring between the project and deployment models.</p></figcaption>
  </figure>
<p>Much like stiffness in a part of the human spine can result in pain in another, <strong>stiffness in our build DAGs is causing excessive mirroring between our project and deployment models</strong>; between our repositories and our services; between the way we organise our code and the way our services run. That mirroring is on the one hand preventing us from shifting left concerns about the relationships between modules that have often been made weak and fragile runtime dependencies, while on the other hand encouraging us to have more services than what the runtime reality would call for. That brings pain.</p>
<figure>
    <span>
      <a href="https://vlfig.me/static/1f3476e5c0b1320f4dd42084cfc102b9/dc6ba/hinge.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="The build DAG as a hinge between the project model and the deployment model." title="A build DAG mediates the project and the deployment models, absorbing the impedance mismatch between what is human friendly and what is machine-friendly." src="https://vlfig.me/static/1f3476e5c0b1320f4dd42084cfc102b9/6a068/hinge.jpg" srcset="https://vlfig.me/static/1f3476e5c0b1320f4dd42084cfc102b9/09b79/hinge.jpg 240w,
https://vlfig.me/static/1f3476e5c0b1320f4dd42084cfc102b9/7cc5e/hinge.jpg 480w,
https://vlfig.me/static/1f3476e5c0b1320f4dd42084cfc102b9/6a068/hinge.jpg 960w,
https://vlfig.me/static/1f3476e5c0b1320f4dd42084cfc102b9/dc6ba/hinge.jpg 1335w" sizes="(max-width: 960px) 100vw, 960px" loading="lazy">
  </a>
    </span>
    <figcaption><p>A build DAG mediates the project and the deployment models, absorbing the impedance mismatch between what is human friendly and what is machine-friendly.</p></figcaption>
  </figure>
<p>Central to resolving this stiffness is the realisation that the build flow, at least conceptually, is a DAG – Directed Acyclic Graph – where the nodes are <em>jobs</em> and <em>versioned artifacts</em> and the edges connect either a job to a versioned artifact (“produces”) or a versioned artifact to a jobs (“dependency_of”). Deployables are by definition the versioned artifacts that are consumed by the deployment jobs.</p>
<figure>
    <span>
      <a href="https://vlfig.me/static/e029adab126c3c2556e80d1647accb16/50066/dag.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="A graph of two types of nodes, jobs and versioned artifacts, connected by edges 'dependency_of' and 'produces'. Versioned artifacts can be further specialised." title="A graph of two types of nodes, jobs and versioned artifacts, connected by edges 'dependency_of' and 'produces'. Versioned artifacts can be further specialised." src="https://vlfig.me/static/e029adab126c3c2556e80d1647accb16/6a068/dag.jpg" srcset="https://vlfig.me/static/e029adab126c3c2556e80d1647accb16/09b79/dag.jpg 240w,
https://vlfig.me/static/e029adab126c3c2556e80d1647accb16/7cc5e/dag.jpg 480w,
https://vlfig.me/static/e029adab126c3c2556e80d1647accb16/6a068/dag.jpg 960w,
https://vlfig.me/static/e029adab126c3c2556e80d1647accb16/50066/dag.jpg 1384w" sizes="(max-width: 960px) 100vw, 960px" loading="lazy">
  </a>
    </span>
    <figcaption><p>A graph of two types of nodes, jobs and versioned artifacts, connected by edges ‘dependency_of’ and ‘produces’. Versioned artifacts can be further specialised.</p></figcaption>
  </figure>
<p>For too long we overlooked how much a flexible and frictionless build DAG allows us to improve our architecture on both sides. With moderately rich build patterns we can have our code where its intent is clearer and more constraints can be validated at build time and still have it deployed into its simplest viable form, running where its execution is cheaper, faster and safer.</p>
<figure>
    <span>
      <a href="https://vlfig.me/static/08e9b2a03b92afa1c2b43018fb141eb0/a18e1/wedding-altar.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="Cheesy cisgender, neurotypical westernised image of a wedding altar with the build dag marrying developer effectiveness with mechanical sympathy." title="Sorry. I had to." src="https://vlfig.me/static/08e9b2a03b92afa1c2b43018fb141eb0/a18e1/wedding-altar.jpg" srcset="https://vlfig.me/static/08e9b2a03b92afa1c2b43018fb141eb0/09b79/wedding-altar.jpg 240w,
https://vlfig.me/static/08e9b2a03b92afa1c2b43018fb141eb0/7cc5e/wedding-altar.jpg 480w,
https://vlfig.me/static/08e9b2a03b92afa1c2b43018fb141eb0/a18e1/wedding-altar.jpg 612w" sizes="(max-width: 612px) 100vw, 612px" loading="lazy">
  </a>
    </span>
    <figcaption><p>Sorry. I had to.</p></figcaption>
  </figure><p>.</p>
<h3 id="why-so-stiff-bro"><a href="#why-so-stiff-bro" aria-label="why so stiff bro permalink"></a>Why so stiff, bro?</h3>
<p>I’m not sure what the historically accurate account is that would explain the excessive simplicity of build patterns across the industry. I do know from experience that too many practices make do with very simple and linear flows where one repository builds independently one and only one service. Regardless of the legitimate argument about code duplication and its tradeoffs, there seems to be an aversion to build-time internal dependencies, even when these bring in clearly desirable data or logic such as message format definitions.</p>
<p>I suspect it might have something to do with how very few CI tools support composition natively (i.e. the outputs of jobs being able to be the inputs of others), how fallible semantic versioning in practice is and the difficulty of automating deterministic version propagation.</p>
<p>By that I mean keeping local copies in sync with CI, builds repeatable, and new upstream versions automatically used by their downstream dependents. It isn’t trivial and requires some versioning and build-fu that, to my knowledge, most practices end up shortcutting to either sacrifice repeatability by using <code>latest</code> or stifling the flow by requiring repeated manual work. Hence the pressure to have a simple build setup.</p>
<p>The exact cause is unimportant though. What is important is that overcoming this is crucial.</p>

<p>Many criteria for splitting or joining software systems, ranging from the social (teams, bounded contexts) to the mechanical (cpu or io boundedness) have been put forth, and they all make some form of sense. However, most of them are either a good reason to split projects or modules, or a good reason to split deployables, rarely both. Keeping that in mind will help us apply them more effectively.</p>
<p>Below are a few possible criteria and some comments about their application. I’m not trying to be exhaustive, just illustrating the kind of reasoning makes sense to me.</p>
<h2 id="runtime-deployment-side-criteria"><a href="#runtime-deployment-side-criteria" aria-label="runtime deployment side criteria permalink"></a>Runtime, deployment side criteria</h2>
<ul>
<li><strong>Different Runtime</strong> – If a part of the codebase compiles to a different runtime it becomes a different deployable and we call it a different service.</li>
<li><strong>Elasticity Profile</strong> – Some parts of the system may have a spikier load profile. It might pay off to have them scale in and out separately from the rest.</li>
<li><strong>Load Type</strong> – Some parts of a generally latency-oriented io-bound system may generate occasional peaks of cpu-bound load which can hurt response times. It might be better to put them in a …</li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://vlfig.me/posts/microservices">https://vlfig.me/posts/microservices</a></em></p>]]>
            </description>
            <link>https://vlfig.me/posts/microservices</link>
            <guid isPermaLink="false">hacker-news-small-sites-24963742</guid>
            <pubDate>Mon, 02 Nov 2020 00:16:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Collision]]>
            </title>
            <description>
<![CDATA[
Score 153 | Comments 24 (<a href="https://news.ycombinator.com/item?id=24963131">thread link</a>) | @vijayr02
<br/>
November 1, 2020 | https://fiftytwo.in/story/collision/ | <a href="https://web.archive.org/web/*/https://fiftytwo.in/story/collision/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-v-154a69ab=""><div data-v-154a69ab=""><p>
              T
            </p> <div data-v-154a69ab=""><p>hat November evening, the people of Charkhi Dadri town, about 100km west of Delhi, were done with the day’s work out in the fields. Some of them heard the sky detonate with thunder and erupt into flames. On the ground, doors and windows crumbled. Glass shards flew through the air.</p><p>Mistaking it for an earthquake, residents streamed out of their homes, only to find over 500 tonnes of material pouring out from the sky—what an <em>India Today</em> story vividly described as “the equivalent of 600 Maruti cars”. The planes plunged into the mustard and cotton fields, miraculously hitting no one on the ground. There were rumours of a few survivors found in the wreckage, but none were ever brought to a hospital.&nbsp;</p></div></div><p>At about 9pm, as Prannoy Roy finished reading for <em>The News Tonight</em>, the daily English bulletin on Doordarshan, wire services broke the story of a mid-air collision just outside Delhi. Most journalists had already left for the day, and the crowd at the studios of NDTV, a private news production company, was thinning. Writer and film-maker Natasha Badhwar, then a 25-year-old cameraperson two years out of Jamia Millia Islamia University, quickly volunteered herself for the story. Along with reporter Radhika Bordia and camera assistant Kanan Patra, she got into an office vehicle and headed towards Charkhi Dadri. As they approached the site, they saw a massive inferno raging in the distance, ringed by smaller fires. </p><p>They got out of the car and began to walk through the debris and darkness. When Badhwar struck something, the group stopped and switched on the camera light. A stiff body swam into view, dead, but not burnt or broken. The man looked in death as he might have in life. How do you film a dead body, Badhwar wondered. “There was a sense of horror I could not shut out,” she said. “There was a sense of incredulity: How could this happen? How could two planes collide?”</p><p>Through the night, journalists arrived to scenes of damage and disarray. Passports, bags of food, toys, slippers, wallets, wrenched-open suitcases and airplane seats were flung around. Disembodied human limbs lay scattered like confetti. The full horror would not be apparent until sunrise, but the acrid stench of burning fuselage and singed remains had already begun choking the air.</p><p>At one point, the grandson of a prominent politician waltzed in,<a href="" onclick="return!1"><sup id="2pzmabfu9t5s">[4]</sup></a> chaperoned by a group of sidekicks and handlers. He had no reason to be there but quickly became the centre of attention. When asked what he was doing there, he smarmily replied, “Plane curiosity.”&nbsp;</p><p>Through the night, villagers tried to find survivors as emergency services arrived. By the morning, several bodies were moved into a school which had been converted to a makeshift morgue. Debris had scattered over a radius of several kilometres, complicating search and rescue operations. </p><p>KPS Nair was among the first team of investigators and diplomats to reach the scene as dawn broke. Then a Deputy Director with the Directorate General of Civil Aviation (DGCA), Nair had been in a meeting on air safety when news filtered in the previous evening. The irony was inescapable. “My first thought was, my god!” he recalled over the phone in June 2020. “Why do such things happen in spite of the efforts of human beings to make better flying machines and systems?”</p><p>As lead investigator, his first impulse was to locate the “black boxes”, vermilion-coloured crash-proof casings that held the cockpit voice recorder (CVR) and the flight data recorder (FDR). The CVR, which saved the last 30 minutes of cockpit conversation, could provide precious insight into what the crew had said to each other. The FDR would provide readings on altitude, speed, flight path and engine power to help build a picture of both planes’ vital parameters. “This,” Nair explained, “is the most important tool in the hands of an investigator from the accident site.”</p><div data-v-154a69ab=""><p>Both black boxes were found before nightfall on the day following the accident. The cockpit of the Saudi aircraft was stuck deeper in the ground, irrevocably damaged, none of its instruments readable. All three of its altimeters were destroyed in the fire.&nbsp;</p><p>The Kazakh plane had suffered less damage and yielded four altimeters. Two of them, curiously, showed different readings: 4,443m (14,576 feet) and 4,540m (14,895 feet). The other two were unreadable.&nbsp;</p></div><p>Nair scoured the fields, analysing the debris scattered across two sites seven kilometres apart, searching for clues in the carcasses of the two planes. “We approach any accident with an open mind,” he said. “We ask, did everyone involved apply the necessary procedures? The contributing factors we look at include the ATC communications, pilot error, engineering and maintenance aspects.” </p><p>AK Chopra, then head of air safety for the North region at the DGCA, also reached the site with his team. Chopra sensed something puzzling, even counter-intuitive. The frontal structure of the Kazakh flight was mostly intact, which indicated that this hadn’t been a head-on collision. If the Kazakh plane was supposed to have been higher (15,000 feet), why did a primary assessment suggest that it had hit the Saudi plane from below? </p><p>By mid-afternoon, just 19 bodies had been claimed from the morgue. Rescue workers tried to pick through the debris, retrieving limbs and luggage. Badhwar even encountered the studio in-charge from Jamia’s Mass Communication Research Centre, mourning the loss of a younger brother who had been on the flight.&nbsp;</p><p>For at least 15 hours after the crash, there was little sense of sanctity about the site, which was only cordoned off much later.<a href="" onclick="return!1"><sup id="z5s3bz6hivmy">[5]</sup></a> Some people rifled through the wreckage and corpses, trying to make off with the spoils. But for the most part, people were kind and helpful, bringing sheets from their homes for the bodies, volunteering tractors in search efforts, and offering tea to families. </p><p>Over time, the horror of losing a loved one morphed into the numbing pain of dealing with paperwork. Relatives scrambling to apply for compensation and obtain death certificates shuttled from one office to another. The Saudis announced compensation of up to GBP 12,000 for the families of passengers, and some compensation<a href="" onclick="return!1"><sup id="k4yf7bl2nlj4">[6]</sup></a> for the villagers in whose fields the flights had crashed.&nbsp;</p><p>In the end, 94 bodies were charred beyond recognition or totally mutilated. They lay piled up in the morgue, their limbs on ice. Many of the dead were from mofussil areas. Their families would hear about the tragedy only much later, particularly since some didn’t even know their relatives were on board.<a href="" onclick="return!1"><sup id="xjunwbq7ccht">[7]</sup></a> Others, perhaps, never found out at all, given the underhand manner in which agents and touts recruited people for jobs abroad.<a href="" onclick="return!1"><sup id="am1u2ycez0x2">[8]</sup></a></p><p>After 15 days in the morgue, the unclaimed bodies were divided in proportion to the Hindus and Muslims on board according to the passenger manifest, a compromise between community leaders following an initial dispute. First, everyone went to the Muslim burial ground; later they went to the crematorium. Badhwar, still processing the disaster she had covered, attended the mass funerals and filmed them. “The smell of burning flesh still haunted me,” she said. “I felt I had some undone mourning to do.”</p></div></div>]]>
            </description>
            <link>https://fiftytwo.in/story/collision/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24963131</guid>
            <pubDate>Sun, 01 Nov 2020 22:33:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Algebraic Effects for React Developers]]>
            </title>
            <description>
<![CDATA[
Score 137 | Comments 94 (<a href="https://news.ycombinator.com/item?id=24962842">thread link</a>) | @reesew
<br/>
November 1, 2020 | https://reesew.io/posts/react-algebraic-effects | <a href="https://web.archive.org/web/*/https://reesew.io/posts/react-algebraic-effects">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><blockquote>
<p>It’s in words that the magic is—Abracadabra, Open Sesame, and the rest—but the magic words in one story aren’t magical in the next.
The real magic is to understand which words work, and when, and for what; the trick is to learn the trick.</p>
<p>- John Barth, <em>Chimera</em></p>
</blockquote>
<p>It’s been quite some time since Hooks were officially stabilized in <a href="https://reactjs.org/blog/2019/02/06/react-v16.8.0.html" target="_blank" rel="nofollow noopener noreferrer">React 16.8</a>, and with them came a fundamentally different way of understanding the way our applications work.
This is both a blessing and a curse: Hooks are much closer to the React programming model and help avoid a certain class of subtle and confusing bugs, but some developers have also expressed concerns that <a href="https://jaredpalmer.com/blog/react-is-becoming-a-black-box" target="_blank" rel="nofollow noopener noreferrer">React is becoming a black box</a>.
These concerns are completely valid; Hooks can often seem “magical,” since most of the complexities are hidden away in React’s internals.</p>
<p>Much of that “magical” feeling is simply due to the fact that Hooks are based on some <a href="https://reactjs.org/docs/hooks-faq.html#what-is-the-prior-art-for-hooks" target="_blank" rel="nofollow noopener noreferrer">prior art</a> and programming language research that many developers simply aren’t familiar with.
Understanding some of the motivations and inspirations for Hooks can help build a mental model for what’s happening behind the scenes.
While there are several sources of influence on the original Hooks proposal, arguably the most important is the notion of algebraic effects.</p>
<blockquote>
<p>❗❗ Note that this article is <em>not</em> an introduction into how to <em>use</em> Hooks or how Hooks work internally.
This is merely a way to <em>think</em> about Hooks. For more information about how to use them, I suggest
starting with <a href="https://reactjs.org/docs/hooks-intro.html" target="_blank" rel="nofollow noopener noreferrer">the docs</a>.</p>
</blockquote>
<p>Before diving into the details of algebraic effects, let’s first take a step back.</p>
<h2 id="why-do-we-need-hooks"><a href="#why-do-we-need-hooks" aria-label="why do we need hooks permalink"></a>Why Do We Need Hooks?</h2>
<p>Class components seemed to be working well enough, why add <em>another</em> way of writing components that, at least at face value, do the same thing?</p>
<p><img src="https://reesew.io/media/DysfunctionalClassComponents.png" alt="First tweet by @rickhanlonii: &quot;The existence of functional components implies the existence of dysfunctional components&quot;. Reply tweet by @sebmarkbage: &quot;We call those 'class components'.&quot;"></p>
<p>One of React’s core principles is the idea that an application’s user interface is a pure function of that application’s state.
Here, “state” can refer to any combination of local component state and global state, such as a Redux store.
When that state changes and propagates through your component tree, the output represents your new UI after that state change.
This is, of course, an abstraction over the nuts and bolts of how that update actually happens, since React handles the actual reconciliation and DOM updates that are necessary, but this core principle means, at least in theory, that our UI is always synchronized with our data.</p>
<p>Of course, this isn’t always true.
Class components expose certain scenarios that allow us to ignore changes in state if we don’t effectively handle those state changes in our lifecycle methods.
Dan Abramov wrote an <a href="https://overreacted.io/writing-resilient-components/#dont-stop-the-data-flow-in-side-effects" target="_blank" rel="nofollow noopener noreferrer">excellent article</a> on some common pitfalls related to this that’s worth a read for more detail.
In short, class components use different lifecycle methods to handle side effects, but that maps side effects to DOM operations, <em>not</em> state changes.
This means that while the visual elements of our UI may respond to state changes, our side effects might not.</p>
<p>Because class components have to do these internal updates to synchronize their internal state when props change, they are by definition <em>impure</em>.
<em>But wait</em>, you say, <em>I thought we said that UI was a <em>pure</em> function of state</em>.</p>
<p><em>Precisely</em>.
This is where Hooks come into play.</p>
<p>Hooks represent a different way of thinking about effects.
Instead of thinking about the entire lifecycle of a component, Hooks allow us to narrow our focus to only the current state.
We can then <em>declare</em> the states in which we want our effects to run, ensuring that those state changes are reflected in our effects.
Of course, an “effect” can be many things, from handling state with <code>useState</code>, making network requests or manually updating the DOM with <code>useEffect</code>, or calculating expensive callback functions with <code>useCallback</code>.</p>
<p>But how do we reason about those side effects within a pure function?
I’m glad you asked!</p>
<h2 id="an-introduction-to-algebraic-effects"><a href="#an-introduction-to-algebraic-effects" aria-label="an introduction to algebraic effects permalink"></a>An Introduction to Algebraic Effects</h2>
<p><em>Algebraic effects</em> are a generalized approach to reasoning about computational effects in pure contexts by defining an <em>effect</em>, a set of operations, and an effect <em>handler</em>, which is responsible for handling the semantics of how to implement effects.<sup id="fnref-1"><a href="#fn-1">1</a></sup>
Algebraic effects generalize over a whole host of potential uses, like input and output, handling state, <code>async</code>/<code>await</code>, and many more.</p>
<p>This is a little abstract, so let’s write some code to see how this works in practice.
Unfortunately, JavaScript doesn’t actually support algebraic effects, although React might mimic them internally.
While there are a few different languages<sup id="fnref-2"><a href="#fn-2">2</a></sup> that support algebraic effects, we’re going to use <a href="https://github.com/matijapretnar/eff" target="_blank" rel="nofollow noopener noreferrer">Eff</a>, a functional programming language designed specifically around algebraic effects.
Don’t worry, <em>most</em> people won’t know Eff, so I’ll explain some syntax as we go along<sup id="fnref-3"><a href="#fn-3">3</a></sup>.</p>
<p>A common use case for algebraic effects is handling stateful computations.
Remember that effects are just in an interface with a set of operations.
In Eff, we defined effects with the <code>effect</code> keyword and a type signature:</p>
<!---
I'm using OCaml tags in the markdown here because there's obviously
no highlighting options for Eff.
-->
<div data-language="ocaml"><pre><code>


<span>type</span> user <span>=</span> string <span>*</span> int

effect <span>Get</span><span>:</span> user
effect <span>Set</span><span>:</span> user <span>-&gt;</span> unit</code></pre></div>
<p>Once we’ve defined what effects our effects will look like, we can define how our effects are handled by using the <code>handler</code> keyword.</p>
<div data-language="ocaml"><pre><code><span>let</span> state <span>=</span> handler
  <span>|</span> y <span>-&gt;</span> <span>fun</span> currentState <span>-&gt;</span> <span>(</span>y<span>,</span> currentState<span>)</span>
  <span>|</span> effect <span>Get</span> k <span>-&gt;</span> <span>(</span><span>fun</span> currentState <span>-&gt;</span> <span>(</span>continue k currentState<span>)</span> currentState<span>)</span>
  <span>|</span> effect <span>(</span><span>Set</span> newState<span>)</span> k <span>-&gt;</span> <span>(</span><span>fun</span> <span>_</span> <span>-&gt;</span> <span>(</span>continue k <span>(</span><span>)</span><span>)</span> newState<span>)</span>
<span>;</span><span>;</span></code></pre></div>
<p>Hmm, this looks a little trickier — let’s break it down a bit.
We have a <code>handler</code> with three branches, and all of them return a function.
That function will be used to handle some effect (or lack thereof).</p>
<p>The first branch, <code>y -&gt; fun currentState -&gt; (y, currentState)</code>, represents <em>no</em> effect, which happens when we reach the end of the block we’re handling (which we’ll see shortly). <code>y</code> here is the return value of the function, so this simply returns a tuple of the inner return and the state.</p>
<p>The second and third branches match our effects, but there’s a suspicious argument <code>k</code>.
<code>k</code> here is a <em>continuation</em>, which represents the rest of the computation <em>after</em> where we perform an effect.</p>
<div><p>GOTO, but better</p><div><blockquote>
<p>At my heart, I am something like the goto instruction; my creation sets the label, and my methods do the jump. However, this is a really powerful kind of goto instruction. If your hair is turning green at this point, don’t worry as you will probably only deal with users of continuations, rather than with the concept itself.</p>
</blockquote><p>This little gem comes from the GNU Smalltalk <a href="https://www.gnu.org/software/smalltalk/manual-base/html_node/Continuation.html" target="_blank" rel="nofollow noopener noreferrer"><code>Continuation</code> documentation</a>.
For some of you, the reference to <code>GOTO</code> might make you a little nauseated, but there’s a reason that continuations still have their place as a control flow, which is about <em>context</em>.
One of the more treacherous aspects of <code>GOTO</code> is getting plopped into an invalid context, but with continuations, you’re really storing an in-flight <em>process</em>, so the variables, pointers, and so on will all be valid.<sup id="fnref-5"><a href="#fn-5">5</a></sup></p></div></div>
<p>Because continuations represent the entire process in action, they’re essentially a snapshot of the call stack at the time of the effect.
When we get to an effect, it’s almost as if we hit a giant pause button on the computation until we properly handle the effect.
Calling <code>continue k</code><sup id="fnref-4"><a href="#fn-4">4</a></sup> is like hitting the play button again.</p>
<p>Alright, I think we’re ready to see our effect handlers in action.
Right now, we have a user in state; let’s wish them well on their birthday:</p>
<div data-language="ocaml"><pre><code><span>let</span> celebrate <span>=</span> <span>with</span> state handle
  <span>let</span> <span>(</span>name<span>,</span> age<span>)</span> <span>=</span> perform <span>Get</span> <span>in</span>

  print<span>_</span>string <span>"Happy Birthday, "</span><span>;</span>
  print<span>_</span>string name<span>;</span>
  print<span>_</span>endline <span>"!"</span><span>;</span>

  perform <span>(</span><span>Set</span> <span>(</span>name<span>,</span> age<span>+</span><span>1</span><span>)</span><span>)</span><span>;</span>
  perform <span>Get</span>
<span>;</span><span>;</span>

celebrate<span>(</span><span>(</span><span>"Henry"</span><span>,</span> <span>39</span><span>)</span><span>)</span><span>;</span><span>;</span></code></pre></div>
<p>When we start off this computation, we first <code>Get</code> our user from state, which runs the second branch in our handler.
At this point, we’ve hit the pause button, so the function has stopped running while we get this from state.
The handler gives us back a function, which calls <code>continue k currentState</code>, resuming our computation with the value of <code>currentState</code>.</p>
<p>This same flow happens every time we <code>perform</code> an effect.
Hit pause, do some work, hit play.</p>
<p><img src="https://reesew.io/media/CelebrateToBeContinued.png" alt="Our code for the celebrate function, overlaid with the &quot;To Be Continued&quot; meme from Jojo's Bizarre Adventure."></p>
<!-- markdownlint-disable MD033 -->
<figcaption>I'm so sorry</figcaption>
<p>And here, dear reader, is where the power of algebraic effects really shines.
You see, it doesn’t really <em>matter</em> how we hold state.
Sure, right now it’s just an object in memory, but what if it was in a database?
What if it was stored in a browser’s <code>localStorage</code>?
As far as <code>celebrate</code> knows, these are <em>all the same</em>.
If we wanted, we could swap out our <code>state</code> handler with a <code>redisState</code> handler that stored state in a key-value store.</p>
<p>In JavaScript, your code has to be aware of what’s synchronous and what’s not.
If this were to change in the future, and state was handled asynchronously, we would need to start handling Promises, which would require changes across <em>everything</em> that touches this function.
But with algebraic effects, instead of maintaining a running process that holds a reference to a <em>different</em> process, we can simply stop the current process altogether until our effects are finished.</p>
<p>Of course, state isn’t the only thing that we can handle with algebraic effects.
Let’s say we have some network request we want to make or cleanup we want to execute, but we only want to do it <em>after</em> our function is done.
We’ll call it a <code>Defer</code> effect.</p>
<div data-language="ocaml"><pre><code>effect <span>Defer</span><span>:</span> <span>(</span>unit <span>-&gt;</span> unit<span>)</span> <span>-&gt;</span> unit

<span>let</span> defer <span>=</span> handler
    <span>|</span> y <span>-&gt;</span> <span>fun</span> <span>(</span><span>)</span> <span>-&gt;</span> <span>(</span><span>)</span>
    <span>|</span> effect <span>(</span><span>Defer</span> effectFunc<span>)</span> k <span>-&gt;</span>
        <span>(</span><span>fun</span> <span>(</span><span>)</span> <span>-&gt;</span>
            continue k <span>(</span><span>)</span><span>;</span>
            effectFunc <span>(</span><span>)</span>
        <span>)</span>
<span>;</span><span>;</span></code></pre></div>
<p>Notice that <code>continue k ()</code> doesn’t have to be the last part of the handler, as it was in our <code>state</code> handler.
We can call continuations <em>whenever</em> we want and however many times we want — remember, they’re just representations of a process.</p>
<p>To make sure this works as intended, let’s make a quick sketch of how this might work in practice:</p>
<div data-language="ocaml"><pre><code><span>let</span> runWithCleanup <span>=</span> <span>with</span> defer handle
    print<span>_</span>endline <span>"Starting our computation"</span><span>;</span>
    perform <span>(</span><span>Defer</span> <span>fun</span> <span>(</span><span>)</span> <span>-&gt;</span> print<span>_</span>endline <span>"Running cleanup"</span><span>)</span><span>;</span>
    
    print<span>_</span>endline <span>"Finishing computation"</span>
<span>;</span><span>;</span>

runWithCleanup<span>(</span><span>)</span><span>;</span><span>;</span></code></pre></div>
<p>When we run this, we get the following in our terminal:</p>
<div data-language="shell"><pre><code>$ eff defer.eff</code></pre></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://reesew.io/posts/react-algebraic-effects">https://reesew.io/posts/react-algebraic-effects</a></em></p>]]>
            </description>
            <link>https://reesew.io/posts/react-algebraic-effects</link>
            <guid isPermaLink="false">hacker-news-small-sites-24962842</guid>
            <pubDate>Sun, 01 Nov 2020 21:51:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Winamp for Windows 10]]>
            </title>
            <description>
<![CDATA[
Score 301 | Comments 171 (<a href="https://news.ycombinator.com/item?id=24962823">thread link</a>) | @zindera
<br/>
November 1, 2020 | http://www.mywinamp.com/winamp-for-windows-10-download/ | <a href="https://web.archive.org/web/*/http://www.mywinamp.com/winamp-for-windows-10-download/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>



<p>Last updated: November 01, 2020<br>
File Size: 22.00 MB</p>
<p>Winamp has remained a firm favourite for many tech geeks and music lovers alike. It had appeared to lose its way for a while, as it was slow in coming forward with newer versions, which could be supported by more modern software. This all looks set to change and could be the revival that Winamp truly deserves. The new update, BETA version 5.8, seems to have found its way onto the internet, hinting that this could be a step towards this fan favourite being supported by Windows 10. Although its Belgian owners, Radionomy, are yet to make a formal release statement.</p>
<p><a href="http://www.mywinamp.com/wp-content/uploads/2015/07/Winamp-Windows-101.jpg"><img loading="lazy" src="http://www.mywinamp.com/wp-content/uploads/2015/07/Winamp-Windows-101-300x225.jpg" alt="Download Winamp for Windows 10" width="300" height="225" srcset="http://www.mywinamp.com/wp-content/uploads/2015/07/Winamp-Windows-101-300x225.jpg 300w, http://www.mywinamp.com/wp-content/uploads/2015/07/Winamp-Windows-101.jpg 1024w, http://www.mywinamp.com/wp-content/uploads/2015/07/Winamp-Windows-101-541x406.jpg 541w" sizes="(max-width: 300px) 100vw, 300px"></a></p>
<p>Support for Windows Audio seems to be one of the key developments and this change allows for complete compatibility with Windows 10. No doubt, it will have the usual teething problems, but as has been shown before, it’s the fans and enthusiasts who have helped to push the software in the right direction when it comes to tweaks and apps. Winamp has launched their own forum for just this purpose and it’s great to see fans new and old being able to discuss both their excitement and, as to be expected at this early stage, some of their frustrations.</p>
<p>The goal of this new set of developers has been clearly defined as, the intention to make Winamp the player of today and provide an up to date and complete listening experience. One of the best things they’ve done so far is to remove any old pro licences, making Winamp 100% free to use again. There are also some other current fixes which have been included in the new version, these fixes include, resolved – slow loading issue, improved – updated scroll bar and buttons, fixed – various memory leaks. There are tons more additions and you can see the complete list of updates, fixes and resolutions within the new BETA version.</p>
<p>The go-to functions of customisable skins, visualisation, plugins and the ability to design and make your media player unique to you, are all still high on the agenda and remain some of the software’s key features. The fact that it can now be supported by Windows 10 has also opened up other new avenues. There are more advanced video plugins which can now be supported whereas previously there were some issues with this. There is no multinational version available with Lang packs as yet, but they’re working on the language packs, with Spanish and Polish currently available on the BETA version.</p>
<p>For more avid users of Winamp, they’ll be pleased to know the CD playback and ripping functions now use native Windows API instead of Sonic. The AAC decoder now uses Media foundation, Vista and higher and the H.264 decoder is now also using Media foundation, Vista and higher. These technical changes demonstrate that Winamp is taking itself seriously and has finally understood there is still a huge legion of fans who will willingly support and promote this unequalled media player. There is still nothing as flexible, yet as technically delightful as Winamp when it comes to creating and building your own individual home for music.</p>
<p>Back in October 2018, Radionomy’s CEO, Alexandre Saboundjan hinted that we could expect to see a version 6 available in 2019. 5 months into the year there have been no further corporate updates, including whether or not it will be able to support newer services or how it might integrate with the big players such as Apple Music and Spotify. However, the fact that the owners of Winamp are starting to understand the true potential behind this awesome media player can surely be nothing but good a sign for hardcore Winamp enthusiasts.</p>
<p>The beauty of Winamp is that you experience all of your music in a single place and this makes it a singular experience. There’s no need to go switching between platforms and getting lost in remembering where dedicated playlists are saved. The visualization aspect of Winamp and its spectrum analyser has always allowed people to bring their music to life. To see and feel it. It’s fantastic to see the BETA version available, and the input this is allowing fans and users alike to generate.</p>
<p>We’ve touched on some of the updates but there are a few others that deserve a mention. You can slow down the pop-up buttons, so they don’t overtake the screen and they’ve also improved the browse path and edit title functions in Editor. The OpenMPT-base module player has also been improved and they’ve replaced the MikMod player. The added functionality which can now be supported by Windows 10 is set to continue and we can expect to see more and advances over the coming months.</p>
<p>With only the BETA version being released so far, it’s safe to say, this is a work in progress, but I for one look forward to being a part of the continuing journey to restoring Winamp to its former and well-deserved glory.</p>
<p>Here you can find the Winamp for Windows 10 (Windows media player) with contain required update patches. Windows 10 had backward compatibility with Winamp skins and popular plugins. Compatibility Winamp updates for security&nbsp;support are have already installed. Winamp works perfectly well with Windows 10.</p>
<p><a href="http://www.mywinamp.com/downloads/Winamp-5.666-MULTI.zip">Download Winamp for Windows 10</a></p>
<p>Winamp Essentials Pack 5.6 &amp; 5.7 <a href="http://www.mywinamp.com/winamp-essentials-pack/">Download page</a></p>
 </div></div>]]>
            </description>
            <link>http://www.mywinamp.com/winamp-for-windows-10-download/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24962823</guid>
            <pubDate>Sun, 01 Nov 2020 21:48:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[ARM ecosystem disintegration and the rise of RISC-V]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24962692">thread link</a>) | @ingve
<br/>
November 1, 2020 | https://www.reflectionsofthevoid.com/2020/11/arm-ecosystem-disintegration-and-rise.html | <a href="https://web.archive.org/web/*/https://www.reflectionsofthevoid.com/2020/11/arm-ecosystem-disintegration-and-rise.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-body-5235483455562909851" itemprop="description articleBody"><p>
#ARM acquisition by #Nvidia is making people uneasy.&nbsp;</p><div><p>And the early sign of the unravelling of the #ARM ecosystem start to appear:&nbsp;<a href="https://www.servethehome.com/impact-of-marvell-thunderx3-general-purpose-skus-canceled/" target="_blank">ThunderX3 general-purpose ARM CPU has been cancelled</a>.</p><p>One would ask why spending $$ to build a better product and increase its number of consumers if, for that, it will have to use the Nvidia IP and compete directly against the IP owner. <br>If you combine this with the difficult viability of putting together a general-purpose #ARM alternative to #Intel / #AMD as #ARM vendors are effectively competing on cost with much lower volumes. </p><p>We start to understand why Marvell decided to shift toward the much <a href="https://seekingalpha.com/article/4374263-marvell-technology-group-ltd-mrvl-ceo-matt-murphy-presents-deutsche-bank-2020-virtual">more trendy IPU/PDU/Smartnic market</a>. </p><p>On the other hand, I think we will see an acceleration of RISC-V adoption. Eating away at the traditional #ARM market share. This will be driven by the large scale edge deployment of #riscv sees chips with a RISC-V core and an #NPU (neural processing unit). These chips can be churned out at incredibly cheap cost, less than $10, and these will become ubiquitous really rapidly. </p><p>It might take 10-15 years but ultimately this will seal the fate of the ARM franchise. </p><p><a href="https://1.bp.blogspot.com/-QxQI2JLX7HU/X58CBfN4RAI/AAAAAAAAWi4/0TkyizvfCecI4ADDf-D3N7CjCRZmtZnqACLcBGAsYHQ/s680/ElBuUPDXUAI1ePq.png" imageanchor="1"><img data-original-height="655" data-original-width="680" src="https://1.bp.blogspot.com/-QxQI2JLX7HU/X58CBfN4RAI/AAAAAAAAWi4/0TkyizvfCecI4ADDf-D3N7CjCRZmtZnqACLcBGAsYHQ/s320/ElBuUPDXUAI1ePq.png" width="320"></a></p><br></div>

</div></div>]]>
            </description>
            <link>https://www.reflectionsofthevoid.com/2020/11/arm-ecosystem-disintegration-and-rise.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24962692</guid>
            <pubDate>Sun, 01 Nov 2020 21:33:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Flutter Web: A Fractal of Bad Design]]>
            </title>
            <description>
<![CDATA[
Score 120 | Comments 77 (<a href="https://news.ycombinator.com/item?id=24962504">thread link</a>) | @adrian_mrd
<br/>
November 1, 2020 | https://hugotunius.se/2020/10/31/flutter-web-a-fractal-of-bad-design.html | <a href="https://web.archive.org/web/*/https://hugotunius.se/2020/10/31/flutter-web-a-fractal-of-bad-design.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main"><article>
  
  <div><p>The web has a long and rich history dating back to the nineties at CERN. Back then <a href="https://twitter.com/timberners_lee">Tim Berners-Lee</a> laid the foundation of HTML that is still around today. There have been attempts to replace it with varying success but none have been successful, for good reason. HTML and the later invention of CSS are a remarkably powerful set of tools to build all kinds of experiences on the web. People are still trying to replace HTML, which brings us to the topic of this post: Flutter Web.</p>

<p><a href="https://flutter.dev/web">Flutter Web</a> is part of Google’s <a href="https://flutter.dev/">Flutter</a> framework for building cross platform UI. Hailed by many developers as the best thing since sliced bread, my opinion of it lacks the rose coloured glasses. I haven’t looked at Flutter for other platforms than web so I cannot comment on it other than that the general principle of Flutter is a terrible idea. Flutter works by throwing away the native UI toolkits provided by the platform and rendering everything from scratch using OpenGL et al. This translates extremely poorly to the web platform in particular. It’s worth noting that Flutter for Web is currently in beta and the problems I am about to detail could be addressed. However, I believe these issues are fundamental to Flutter’s design choices so I feel confident in my criticism.</p>

<h2 id="semantic-html">Semantic HTML</h2>

<p>Anyone learning HTML these days would have encountered the term “Semantic HTML” because it is such an important part of modern web. <a href="https://www.petelambert.com/">Pete Lambert</a> describes why this is important in the excellent blog post <a href="https://www.petelambert.com/journal/html-is-the-web">HTML is the Web</a>. In short, the visible portion of a website, i.e. presentation, is only half the story. To take an example Pete used, a <code>div</code> with an <code>onClick</code> handler might be clickable and can be styled to look like a button, but that doesn’t make it a button. The semantic structure of a document matters because that’s how machines, not humans, understand the web. A <code>div</code> with an <code>onClick</code> handler doesn’t look like a link or a button to a screen reader, search engine crawler, or accessibility extension, it looks like a <code>div</code>.</p>

<p>Most importantly, semantic HTML is key for accessibility and other tools that let a user experience the web as they wish.</p>

<h2 id="a-fractal-of-bad-design">A Fractal of Bad Design</h2>

<p>Does Flutter Web generate semantic HTML? Not even close. It generates a patchwork of <code>canvas</code> elements, custom elements, and a few other HTML elements. In the demo app <a href="https://gallery.flutter.dev/#/reply">Reply</a>, how many buttons and links are there? If you guessed <strong>zero</strong>, congratulations you are as jaded and cynical as me. Let me reiterate that, an email app with no buttons and links! Because there are no links in particular, features like cmd/ctrl clicking to open a new tab, hovering links to see the URL, and using the context menu do not work.</p>

<p>I use the browser extension <a href="https://vimium.github.io/">Vimium</a> to navigate the web, it’s an amazingly powerful tool that relies on, you guessed it, semantic HTML. Does it work on pages built with Flutter Web? Fuck no. It doesn’t work because it tries to find things that are semantically clickable, like <code>button</code> or <code>a</code> elements, of which, as we have established, Flutter Web generates none. Vimium works on almost all websites I use because most developers, thankfully, don’t just stick <code>onClick</code> handlers on <code>div</code>s. However, whatever crazy shit Flutter Web does, doesn’t look clickable. Things being clickable that don’t look clickable is a good proxy for poor accessibility. For example, screen readers can navigate by landmarks, such as headings, <strong>links</strong>, forms, and other semantic elements, does any of this work with Flutter Web? Fuck no.</p>

<p>Even worse, unless you use special “selectable” text, Flutter Web doesn’t even support selecting text. No joke, their own code examples have a “copy all” button to get around this.</p>

<p><a href="https://hugotunius.se/img/flutter-web-a-fractal-of-bad-design/code-example.png"><img src="https://hugotunius.se/img/flutter-web-a-fractal-of-bad-design/code-example.png?1604348346" alt="Screenshot of Flutter's Navigation Bar component with preview, code, and &quot;copy all&quot; button"></a></p>

<p>How did anyone look at this and say “yeah nah, selecting text isn’t an important use case on the web”? Why does selecting text matter you ask? Some people use it to aid with reading by selecting the text they are currently reading, other people use it to(and I know this is wild) copy parts of the text, people with dyslexia use tools that read out selected portions of the text to help them read. Does any of this work with Flutter’s default, unselectable text? Fuck no!</p>

<p>While we are talking about dyslexia, another useful feature that help people who suffer from it is the ability to change the fonts of web pages to one they find easier to read, such as <a href="https://www.opendyslexic.org/">OpenDyslexic</a>. There are many tools that help with this and they all rely on the ability to inject custom CSS in a web page, to my surprise this actually looked to work when I tried it on some of the Flutter Web demos. However, looks deceive and while the font does apply it causes text to get cut off in almost all instances because of the terrible HTML Flutter generates. For example, in the “Reply” email client here’s the tag for the word “Reply” in the upper right</p>

<figure><pre><code data-lang="html"><span>&lt;p</span> <span>style=</span><span>"font-size: 18px; font-weight: normal; font-family: WorkSans_regular, -apple-system, BlinkMacSystemFont, sans-serif; color: rgb(255, 255, 255); letter-spacing: 0px; position: absolute; white-space: pre-wrap; overflow-wrap: break-word; overflow: hidden; height: 27px; width: 54px; transform-origin: 0px 0px 0px; transform: matrix(1, 0, 0, 1, 59, 3.5); left: 0px; top: 0px;"</span><span>&gt;</span>REPLY<span>&lt;/p&gt;</span></code></pre></figure>

<p>Cleaned up a bit and these are the attributes</p>

<figure><pre><code data-lang="plain">font-size: 18px;
font-weight: normal;
font-family: WorkSans_regular, -apple-system, BlinkMacSystemFont, sans-serif; color: rgb(255, 255, 255);
letter-spacing: 0px;
position: absolute;
white-space: pre-wrap;
overflow-wrap: break-word;
overflow: hidden;
height: 27px;
width: 54px;
transform-origin: 0px 0px 0px;
transform: matrix(1, 0, 0, 1, 59, 3.5);
left: 0px;
top: 0px;</code></pre></figure>

<p>Flutter Web generates absolutely positioned fixed sized HTML which instead of adopting to the text layout specified by the browser just cuts the text off.</p>

<p>Another useful feature that is common in accessibility extensions is making links stand out more. This works by injecting global CSS rules that target <code>a</code> tags on the page.</p>

<p><a href="https://hugotunius.se/img/flutter-web-a-fractal-of-bad-design/link-underlines.png"><img src="https://hugotunius.se/img/flutter-web-a-fractal-of-bad-design/link-underlines.png?1604348346" alt="Screenshot of links with link emphasis turned on which underlines them in multiple colours"></a></p>

<p>Does this work with Flutter Web (this questions is starting to feel redundant because the answer is almost always “No”)? Fuck no! In general, users use custom stylesheets for a multitude of reasons not limited to accessibility. As a “fun” exercise try this <a href="https://ssb22.user.srcf.net/css/">low vision stylesheet</a> on one of the <a href="https://gallery.flutter.dev/">Flutter Web demos</a> and see how readable the content is.</p>

<p>Another one of the <a href="https://gallery.flutter.dev/#/fortnightly">demos</a> in the Flutter Web Gallery is a news site. On news sites, I like to use the built-in “reader mode” in my browser to get a reading experience that is free from clutter and better suites me. For example, when reading in bed late at night (which I know I shouldn’t do) it’s nice to have a soft, dark mode experience instead of the glaring black text on pure white that many publications use. Does reader mode work with Flutter Web website? Nope. It doesn’t work because, you guessed it, it relies on semantic HTML.</p>

<p><a href="https://hugotunius.se/img/flutter-web-a-fractal-of-bad-design/enable-accessibility.png"><img src="https://hugotunius.se/img/flutter-web-a-fractal-of-bad-design/enable-accessibility.png?1604348346" alt="Screenshot of Firefox accessibility inspector showing a single button with the text &quot;Enable accessibility&quot;"></a></p>

<p>Like any good writer, I’ve saved the best for last: the screen reader experience with Flutter Web. When you first focus on  one of the Flutter Web demos with a screen reader you are greeted with a “button” that says “Enable accessibility”. Admittedly, when you click this button there is a resemblance of screen reader content but it’s terrible. In the “Reply” app things are read out with unnecessarily high detail in the list view, things that can be clicked aren’t identified as such, you cannot even get to the menu as far as I can tell, and as previously identified there are almost no landmarks which are used for navigation.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Flutter is a misguided attempt to achieve the impossible: quality cross platform experiences. Flutter Web in particular is fundamentally flawed and needs to be rebuilt from the ground up if it has any hopes of being viable tech that generates semantic, accessible, and modern web experiences. I have serious doubt that when Flutter Web leaves beta any of this will be addressed properly, unless the whole approach is reconsidered. If you see Flutter Web, turn around and run in the opposite direction.</p>

<p>Developers, designers, and product people all love cross platform solutions because it saves them time and energy while achieving the “same” outcome as the costlier alternatives. Flutter Web nicely illustrates that the outcomes aren’t the same, the visible part of a product is only one part of the puzzle.</p>

<p>I’m merely an accessibility novice and I didn’t even mention SEO in this post. I didn’t stop writing because I stopped finding flaws, but because this post was getting too long and I have other things to do. I’m sure accessibility users and experts can find even more issues than I have presented here(feel free to DM me on <a href="https://twitter.com/k0nserv">Twitter</a> and I’ll include them).</p>

<p>To end I’d like to leave you with a <a href="https://www.youtube.com/watch?v=mRNX6XJOeGU">quote</a> from <a href="https://www.imdb.com/title/tt0107290/characters/nm0000156?ref_=tt_cl_t3">Dr. Ian Malcolm</a>.</p>

<blockquote>
  <p>Your scientists were so preoccupied with whether or not they could, they didn’t stop to think if they should.</p>
</blockquote>
</div>
  
    
</article>
</div></div>]]>
            </description>
            <link>https://hugotunius.se/2020/10/31/flutter-web-a-fractal-of-bad-design.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24962504</guid>
            <pubDate>Sun, 01 Nov 2020 21:06:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Firearms by the Numbers]]>
            </title>
            <description>
<![CDATA[
Score 26 | Comments 8 (<a href="https://news.ycombinator.com/item?id=24962427">thread link</a>) | @lettergram
<br/>
November 1, 2020 | https://austingwalters.com/firearms-by-the-numbers/ | <a href="https://web.archive.org/web/*/https://austingwalters.com/firearms-by-the-numbers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://austingwalters.com/firearms-by-the-numbers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24962427</guid>
            <pubDate>Sun, 01 Nov 2020 20:53:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Open letter to founders: entrepreneurship is not physics]]>
            </title>
            <description>
<![CDATA[
Score 46 | Comments 46 (<a href="https://news.ycombinator.com/item?id=24962232">thread link</a>) | @rjyoungling
<br/>
November 1, 2020 | https://www.younglingfeynman.com/essays/physicsenvy | <a href="https://web.archive.org/web/*/https://www.younglingfeynman.com/essays/physicsenvy">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-yui_3_17_2_1_1570824304254_14431"><div><p><em>TLDR: In areas like classical physics where degrees of freedom are low, a comparatively small amount of data can enable you to accurately predict how the macro will react. E.g. I don’t need to know the quantum state of every single subatomic particle in a baseball in order to calculate how long it takes to reach the floor if I drop it. I need merely two variables (the downward acceleration caused by Earth’s gravitational field and the height from the ball to the floor), resulting in a simple yet accurate model. In highly complex systems such as business, things like butterfly effects can cause massive distortions rendering our models flawed. Pretending, as we (correctly) do in physics, that there’s a near-perfect bijection between our simple model and reality often causes major mistakes. We must caution against an over-reliance on numbers and the easily measurable because for us, the valuable and the (easily) measurable aren’t always synonymous. Until we have better models or are able to collect enough of the right data points (big and minute), it’s often preferable, as well as faster, to simply try things out on a small scale in the real world and use a feedback loop to iterate.</em></p><blockquote><p>The specific anticipative understanding of the conditions of the uncertain future defies any rules and systematization. It can be neither taught nor learned. If it were different, everybody could embark upon entrepreneurship with the same prospect of success. What distinguishes the successful entrepreneur and promoter from other people is precisely the fact that he does not let himself be guided by what was and is, but arranges his affairs on the ground of his opinion about the future. He sees the past and the present as other people do; but he judges the future in a different way. In his actions he is directed by an opinion about the future which deviates from that held by the crowd.</p><p>- Ludwig von Mises (1949).</p></blockquote><p>It used to be a very contrarian thing to say that business plans have virtually no value.</p><p>That’s not true anymore.</p><p>Most people now agree that business plans are mostly useless.</p><p>But if we think about it… that seems extremely counterintuitive, doesn’t it?</p><p>We have more ways to capture data, visualize data, and use data to guide decisions.</p><p>How on earth is it possible that with such an abundance of data, we don’t see an extremely high correlation between being highly data-driven, doing years of market research, using complex mathematics to make projections and a successful business outcome?&nbsp;<em>Or if anything an inversely proportionate relationship?</em></p><p>It just doesn’t seem to add up…</p><p>I think the answer can be found in long- and medium-range forecasting in meteorology.</p><blockquote><p><em>…a forecast for 5 days out is typically less reliable than a forecast for the next day. This occurs since small changes and small size phenomena are more likely to influence observed weather events as time advances (Butterfly Effect). It is more difficult to analyze phenomena as the size gets smaller, thus it is difficult to know how the extreme multitude of tiny phenomena will impact observed weather as time moves forward. (</em><a href="http://www.theweatherprediction.com/hardtoforecast/" target="_blank"><em>Haby, 2019</em></a><em>)</em></p></blockquote><p>In order to perfectly predict the weather, you’d need incredible amounts of data and an equally overwhelming number of data points and then somehow synthesize that into an accurate prediction.</p><p>That’s hard.</p><p>But when you’re dealing with business plans which involve competitors, the pace of innovation, markets (which are made up of many individuals each with their own mind), and are often projecting many, many years into the future… we’re probably dealing with an order of magnitude more difficulty.</p><p>It quickly becomes an exercise of the imagination of the author as it starts to resemble a fantasy novel behind a pseudo-scientific facade.</p><p>It’s like trying to predict the exact place on the floor a crumpled up piece of paper will land if it’s thrown…</p><p>If that piece of paper is a hundred dollar bill…</p><p>And it’s placed in the middle of Times Square…</p><p>The day before NY’s Eve…</p><p>With elementary Newtonian physics, we can just calculate how far a person will throw it and then we’ll have our answer.</p><p>Except…</p><p>Who’s gonna throw a hundred dollar bill away?</p><p>Someone will find it and might use it to buy some fireworks of a sketchy dude who only sells snakes and sparklers.</p></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1570824304254_23343"><div><p><em>Always laugh at this scene because we, as entrepreneurs, have a tendency to almost fight the market about what they want to buy, because we’re so invested in what we want to sell.</em></p><p>Now our fireworks fella has the $100 bill, gets into a cab, leaves NYC and drops the bill while trying to hand it over.</p><p>If we somehow had all the information in the universe, we might be able to calculate the exact place that bill will touch the floor*1.</p><p>But we don’t.</p><p><em>And Big Data too but more on that later.</em></p><p>There are just too many degrees of freedom.</p><p><em>Think of degrees of freedom as ‘moving parts’. In statistics, it’s the number of variables that are allowed to vary.</em></p><p><em>If you have a set of five numbers that average out to a certain integer, then four numbers are free to change but the last one can not because that one is needed to create the right average. So you have n-1 = 4 degrees of freedom in this example.</em></p><p><em>In physics (and biomechanics), it’s the minimum number of variables required to completely describe how something can move.</em></p><p><em>Your arm (excluding your hand) has seven degrees of freedom. An iPhone that’s wirelessly charging has two (x and y axes).</em></p><p>The way I see it, there are two ways to view or use complicated mathematical models on reality, be it in economics or in our entrepreneurial world.</p><p>Option 1: Employ those select few people who truly and therefore deeply understand the possibilities and limitations of mathematical tools.</p><p>Think of people with Ph.D.’s or more generally, anyone who has a deep understanding from first principles.</p><p>Option 2: Have a childlike perspective and ask why a lot. Eventually, you might realize that the people who don’t belong in the first category don’t really understand the foundation.</p><p>Therefore, any mistake in the foundation renders everything that’s built on top of it useless.</p><p>They have enough understanding to make it seem like they’re knowledgeable but too little to actually understand what they’re doing, resulting in overconfidence in their models.*2</p><p>One such mistake can be seen in standard economic theory.</p><p>I’ll borrow an example given by&nbsp;<a href="https://twitter.com/ole_b_peters?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor" target="_blank">Ole Peters</a>&nbsp;(of the&nbsp;<a href="http://www.lml.org.uk/" target="_blank">London Mathematical Laboratory</a>) which he gives in his lecture,&nbsp;<a href="https://youtu.be/f1vXAHGIpfc" target="_blank">Time for a Change: Introducing irreversible time in economics</a> (Peters, 2012).</p><p>Suppose you have a coin and $100 and you’re going to play a game of heads or tails.</p><p>If it lands on heads you’ll win 50%, if it lands on tails you’ll lose 40%.</p><p>The chance that it lands on heads or tails is P(heads) = P(tails)=0.5, so this seems like a good bet.</p><p>Some quick back of the envelope mathematics suggests that the</p><p>Expected Value = win 50% at 0.5 probability — lose 40% at 0.5 probability</p><p>= (50%*.5)–(40%*.5)=25%-20% = +5%</p><p>We’ll likely end up with more money than we started with. Sweet! But before we start picking out brand new cars with our imaginary winnings… let’s double-check to see if the math works out.</p><p>If we play enough times, the number of heads will start to roughly equal the number of tails.</p><p>Let’s see what happens when we play four times and we get HHTT:</p><p>$100*1.5*1.5*0.6*0.6 = $81</p><p>Hmm... That’s weird.</p><p>If we play 100 times we get $100*1.5⁵⁰*0.6⁵⁰=$0.51</p><p>In fact, the more we play, the closer we get to zero.</p><p>If we play N times, we should get half of that as heads and half of that as tails:</p><p>$100*1.5^(n/2)*0.6^(n/2)</p><p>= $100*(1.5*0.6)^(n/2)</p><p>= $100*0.9^(n/2)</p><p>This implies that if we play long enough we’ll eventually lose all of our money.</p><p><em>This isn’t completely accurate because I make it seem like order doesn’t matter. It doesn’t in mathematics because we won’t run out of small integers. In the real world, the smallest unit of money is one cent, so if you lose that bet there’s no bouncing back. So in my example, 50 losses followed by 50 wins will get you to the same outcome as the reverse, while in reality, you’d lose your last penny after you’d lose for the 19th time in a row, starting with $100. (N&gt;Log0.6 ($0.01/$100) = 18.03).</em></p><p><em>Turns out that if we lose 40% we have $100*0.6 = $60. In order to break even, we need to win at least 66,66..%, $60*1.66.. = $100.</em></p><p>And our imaginary cars?!</p><p>Suppose we had a group of 10.000 people, each with $100, playing this game once.</p><p>The group then has $1.000.000 collectively.</p><p>With the probability of both heads and tails being 50%, suppose 5000 people win and 5000 people lose.</p><p>That means 5000 people now have $150 and 5000 people have $60.</p><p>This gives us 5000*$150 = $750.000</p><p>And 5000*$60 = $300.000</p><p>Giving us a total of $1.050.000.</p><p>And tada… we’ve got our +5% Expected Value back. *3</p><p><em>As it turns out, it’s not just the odds of the bet that matters but also the bet size. Imagine a scenario where if you win you win 300% but if you lose, you lose 100%. Betting all your capital is not a good strategy because while you’ll grow fast when you win, all it takes is one loss to lose everything. On the other side of that extreme is betting the smallest amount possible, one cent. Now you won’t lose much but it’ll take forever to make some serious money. And as a wise woman once said: ‘’Ain’t nobody got time for that!’’.</em></p><p><em>So the ideal bet size is somewhere in that $0.01 — $100 range. In order to determine the right % of your capital that you should bet in order to grow as quickly as possible and not so aggressively that you lose it all, there’s something called the Kelly Criterion. It suggests that the ideal percentage of our capital that we should be betting is: Kelly % = (50/100)-((100–50)/300)*100% = 33% of our capital. If you’re interested you can&nbsp;</em><a href="https://blogs.cfainstitute.org/investor/2018/06/14/the-kelly-criterion-you-dont-know-the-half-of-it/" target="_blank"><em>read more about it here</em></a><em>.</em></p><p><em>Quick side note, in our original bet (40% when we lose a bet, 50% when we win a bet) the Kelly Criterion was 25%. Betting all our capital (100%) for exceeds our KC which is why we go broke in our time perspective.</em></p><p>This is this essence of our little conundrum, winning or losing depends …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.younglingfeynman.com/essays/physicsenvy">https://www.younglingfeynman.com/essays/physicsenvy</a></em></p>]]>
            </description>
            <link>https://www.younglingfeynman.com/essays/physicsenvy</link>
            <guid isPermaLink="false">hacker-news-small-sites-24962232</guid>
            <pubDate>Sun, 01 Nov 2020 20:29:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The missing explanation of ZK-SNARKs: Part 1]]>
            </title>
            <description>
<![CDATA[
Score 72 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24962049">thread link</a>) | @gbrown_
<br/>
November 1, 2020 | https://www.cryptologie.net/article/507/the-missing-explanation-of-zk-snarks/ | <a href="https://web.archive.org/web/*/https://www.cryptologie.net/article/507/the-missing-explanation-of-zk-snarks/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>

<p>What are ZK-SNARKs and how do they work? This is a question I’ve had for years, and always felt like the resources I found gave no clear intuition as to how all of that stuff worked. So today, after a breakthrough in my own understanding, I thought it would be good to re-share what I’ve learned in a more understandable picture. Something that tells you what is the right way of thinking about these things, and what are the gaps that you can fill for yourself if you want to.</p>
<h2>Getting terminology out of the way</h2>
<p>The first part of the question is pretty easy to answer. ZK-SNARKs, no matter what their funny name might imply, are simply <strong>zero-knowledge proofs</strong> that are:</p>
<ul>
<li><strong>non interactive</strong></li>
<li><strong>general purpose</strong></li>
<li>and <strong>succinct</strong></li>
</ul>
<p>Huh, what are all these words? you ask, intimidated by their vagueness.</p>
<p>Well first, <strong>zero-knowledge proofs</strong> are cryptographic proofs that you know something, without revealing the something (zero-knowledgeness). That “something” is usually called the <strong>witness</strong>, but this detail doesn’t matter much. There are a lot of resources about zero-knowledge proofs so I won’t explain much about how they work, or what their exact cryptographic properties are (completeness, soundness, zero-knowledgeness).
Zero-knowledge proofs are often seen used to prove that you know the discrete logarithm in some base of an element of some group (e.g. what is $x$ in $g^x \mod p$), or similarly-limited statements.<br>
“<em>Limited yes, but still useful!</em>” yells Schnorr, inventor of the Schnorr signature scheme which is fabricated by taking a zero-knowledge proof of the knowledge of a discrete logarithm, and making it <strong>non-interactive</strong>. A zero-knowledge proof or <strong>ZKP</strong> is an interactive protocol between a prover (who knows the <strong>witness</strong>) and a verifier (who has to be convinced). An interactive protocol sucks in the real world, as it often limits the number of potential use-cases of the primitive, and slows down protocols depending on the number of round trips that need to happen between the prover and the verifier. Fortunately, some ZKPs can be constructed without interaction with a verifier. In other words, a prover can simply create a proof, and that proof can be verified by anyone at any point in time later without further help from the prover. When ZKPs are made non-interactive, we simply call them non-interactive zero-knowledge proofs or <strong>NIZKs</strong>. I talk more about <a href="https://www.cryptologie.net/article/193/schnorrs-signature-and-non-interactive-protocols/">the link between signatures and zero-knowledge proofs here</a>.<br>
ZKPs and NIZKs can also be constructed on much more general statements like “I know an input to some function such that the execution gives some output”, or more specifically “I know $a$ in $f(a, b) = c$”. If this still doesn’t make sense think about the usual example given to illustrate general-purpose ZKPs: “I know the solution of the sudoku”. <br>
We’re almost there: <strong>ZK-SNARKs</strong> are general-purpose non-interactive zero-knowledge proofs, and more! They are also <strong>succinct</strong>, meaning that the proofs they produce are small in size and are fast to verify, which makes them so special they deserve to be called ZK-SNARKs. Not every modern proof systems deserve that special classification, for example STARKs don’t :(</p>
<p>As a recap:</p>
<ul>
<li>zero-knowledge proof: a cryptographic proof that you know something, without revealing the something</li>
<li>non interactive: a proof that was constructed without the help of a verifier</li>
<li>general purpose: a proof of a more general statement, like the knowledge of secret inputs or outputs of a program</li>
<li>succinct: a small proof that is fast to verify</li>
</ul>
<p>But not only did you ask, what are ZK-SNARKs, but you also asked about how they work.</p>
<h2>The actual stuff</h2>
<p>And oh boy, this is a complex subject to answer. First and foremost, there are many many schemes, too many of them, and so I’m not sure exactly how to answer that question. But I have some idea of how some of them work, and so I imagine that most of them follow that pattern, or improve on it, so let me explain…</p>
<p>There’s two parts to your typical ZK-SNARK: </p>
<ol>
<li>The proving system, which I'll explain in this post.</li>
<li>The translation or compilation of a program to something the proving system can prove, which I'll explain in part 2 of this post.</li>
</ol>
<p>The first part is not too hard to understand, while the second sort of requires a graduate course into the subject…</p>
<h3>Proving your knowledge of a constrained polynomial</h3>
<p>Here it is, remember that one: ZK-SNARKs are all about proving that you know some polynomial $f(x)$ that has some roots.
By roots I mean that the verifier has some values in mind (e.g. $1$ and $2$) and the prover must prove that the secret polynomial they have in mind evaluates to $0$ for this values (e.g. $f(1) = f(2) = 0$).
By the way, a polynomial that has 1 and 2 as roots (in our example) can be written as $f(x)=(x-1)(x-2)h(x)$ for some polynomial $h(x)$. (If you’re not convinced try to evaluate that at $x=1$ and $x=2$.)
So we say that the prover must prove that they know an $f(x)$ and $h(x)$ such that $f(x) = t(x)h(x)$ for some target polynomial $t(x) = (x-1)(x-2)$ (in the example that $1$ and $2$ are the roots that the verifier wants to check).</p>
<p>But that’s it, that’s what ZK-SNARKs proving systems usually provide: something to prove that you know some polynomial. I’m repeating this because the first time I learned about that it made no sense to me: how can you prove that you know some secret input to a program, if all you can prove is that you know a polynomial. Well, that’s why part 2 of this explanation is so difficult: it’s about translating a program into a polynomial. But more on that later.</p>
<p>Back to our proving system, how does one prove that they know such a function $f(x)$? Well they just have to prove that they know an $h(x)$ such that you can write $f(x)$ as $f(x) = t(x)h(x)$. Ugh… Not so fast here. We’re talking about <strong>zero-knowledge</strong> proofs right? How can we prove this without giving out $f(x)$? Well, by using three tricks!</p>
<ol>
<li><strong>homomorphic commitments</strong></li>
<li><strong>bilinear pairings</strong></li>
<li><strong>different polynomials evaluate to different values most of the time</strong></li>
</ol>
<p>So let's go through each of them shall we?</p>
<h3>Homomorphic commitments</h3>
<p>The first trick is to use <strong>commitments</strong> to hide the values that we’re sending to the prover. But not only do we hide them, we also want to allow the <strong>verifier</strong> to perform some operations on them so that they can verify the proof. Specifically verify that if the prover commits on their polynomial $f(x)$ as well as $h(x)$, then we have
$$
com(f(x)) = com(t(x)) com(h(x)) = com(t(x)h(x))
$$ </p>
<p>where $com(t(x))$ is computed by the verifier as these are the known constraints on the polynomial.
These operations are called <strong>homomorphic operations</strong> and we can’t perform them if we use hash functions as commitment mechanisms. Instead, we can simply “hide the values in the exponent” (e.g. for a value $v$ then send the commitment $g^v \mod{p}$) as these are commitments that allow for these homomorphic operations. (To convince yourself, observe that if $a = bc$ then $g^a = g^b g^c = g^{b+c}$. </p>
<p>Wait, this is not what we wanted… we wanted $g^a = g^{bc}$.</p>
<h3>Bilinear pairings</h3>
<p>$g^a = (g^b)^c = g^{bc}$ gets us there, but only if $c$ is a known value and not a commitment (e.g. $g^c$). Unfortunately this is a limitation for our proving protocol, as there will be multiplication operations between commitments. This is where <strong>bilinear pairings</strong> can be used to unblock us, and this is the <em>sole reason</em> why we use bilinear pairings in a ZK-SNARK (really just to be able to multiply the values inside the commitments).
I don’t want to go too deep into what bilinear pairings are, but just know that it is just another tool in our toolkit that:</p>
<ul>
<li>Takes two values of our group (the values generates by $g$ raised to different powers modulo $p$) and place them in another group.</li>
<li>By moving stuff from one group to the other, <strong>we can multiply things that couldn't be multiplied previously</strong>.</li>
</ul>
<p>So using $e$ as the typical way of writing a bilinear pairing, we have $e(g_1, g_2) = h_3$ and we can use it to perform multiplications hidden in the exponent via this one equation:</p>
<p>$$
e(g^b, g^c) = e(g)^{bc}
$$</p>
<p>But no more about bilinear pairings! Again that’s the only reason why we use these in ZK-SNARKs. It’s just a trick to make our homomorphic commitments more homomorphic, to allow us to do:</p>
<p>$$
com(f(x)) = com(t(x)) com(h(x)) = com(t(x)h(x))
$$</p>
<h3>Where does the succinctness comes from?</h3>
<p>Finally, the <strong>succinctness</strong> of ZK-SNARKs come from the fact that two functions that differ will evaluate to different points most of the time.
What this means for us is that if my $f(x)$ is not really equal to $t(x)h(x)$, meaning that I don’t have a polynomial $f(x)$ that really has the roots we’ve chosen with the verifier, then evaluating $f(x)$ and $t(x)h(x)$ at a random point $r$ will not give out the same result (most of the time). In other words for almost all $r$, $f(r) \neq t(r)h(r)$.</p>
<p>This is known as the <strong>Schwartz-Zippel lemma</strong>, which I pictured in the following illustration.</p>
<p><img alt="schwartz zippel lemma" src="https://www.cryptologie.net/upload/Screen_Shot_2020-11-01_at_11.48_.47_AM_.png"></p>
<p>Knowing this, it is enough to prove that $com(f(r)) = com(t(r)h(r))$ for some random point $r$. This is why ZK-SNARKs are so small; by comparing points in a group you end up comparing entire polynomials!</p>
<p>But this is also why there is a “trusted setup” needed before most ZK-SNARKs can work. If a prover learns the random point $r$, then they can forge bad polynomials that will verify. So a trusted setup is about:</p>
<ol>
<li>creating a random value $r$</li>
<li>committing different exponentiation of it $g^r, g^{r^2}, g^{r^3}, \ldots$ so that they can be used by the prover to compute their polynomial without knowing the point $r$</li>
<li>destroying the value $r$</li>
</ol>
<p>Does the second point makes sense? If my polynomial as the prover is $f(x) = x^2 + x$ then all I have to do is compute $g^{r^2} g^r$ to obtain a commitment of my polynomial evaluated at that random point $r$.</p>
<p>Next, I'll write the second part of this blogpost and you'll have to wait until I'm done.</p>
</article></div>]]>
            </description>
            <link>https://www.cryptologie.net/article/507/the-missing-explanation-of-zk-snarks/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24962049</guid>
            <pubDate>Sun, 01 Nov 2020 20:03:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Notion's Onboarding Experience: A Case of Simplicity and Delightfulness]]>
            </title>
            <description>
<![CDATA[
Score 33 | Comments 8 (<a href="https://news.ycombinator.com/item?id=24961797">thread link</a>) | @moeminm
<br/>
November 1, 2020 | https://blog.moeminmamdouh.com/notions-onboarding-experience-a-case-of-simplicity-and-delightfulness | <a href="https://web.archive.org/web/*/https://blog.moeminmamdouh.com/notions-onboarding-experience-a-case-of-simplicity-and-delightfulness">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1604258661806/KVY5KlQih.png?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div><div><div><p>Subscribe to <!-- -->my<!-- --> newsletter and never miss <!-- -->my<!-- --> upcoming articles</p></div></div></div><div itemprop="text"><p>Notion’s onboarding experience is simply great. The use of simple language and illustrations allows the user to feel immersed, and unlike most onboardings, this one actually feels like something you want to complete.</p>

<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1604257176685/iDr0l8w8s.png?auto=format&amp;q=60" alt="https___bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com_public_images_b0900c00-f378-440a-8fc4-91135e388bf3_551x730.png"></p>
<p>Right off the bat, you’re greeted with clear questions, descriptions, and illustrations. One of the mistakes a lot of people make when choosing/making illustrations is the lack of relevance. The illustration may look pretty, but it can do more harm than good if it’s irrelevant to your main headline.</p>
<p>They tell you explicitly at the top of the page that each choice comes with a personalized onboarding experience.</p>
<p><code>We’ll streamline your setup experience accordingly.</code></p>
<p>It makes sense. People using Notion for their own personal use will have their own way of using Notion, while teams will more likely require more collaboration.</p>
<p>Let’s choose For <em>myself</em>.</p>

<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1604257276107/NtsQ5huBR.png?auto=format&amp;q=60" alt="Mask Group 1.png"></p>
<p>Once you click ‘Take me to Notion’, you aren’t just kept waiting. They provide you with feedback. ‘Getting ready’. This is a concept called Idleness Aversion and it’s present in hundreds of apps. Uber being one of the famous examples all around the internet.</p>
<p>When you call in an Uber, you’re provided with real-time graphics of where the driver currently is. This is much better practice than just showing a static image of a car. This simply removes the anxiety of not knowing where the driver is and whether you’re going to be late or not.</p>
<p>This applies to any loading message you receive, it gives you feedback so you don’t have to wait around not knowing what’s going on.</p>

<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1604257330845/qp10RA2O2.png?auto=format&amp;q=60" alt="https___bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com_public_images_75f6a6ab-0b11-40f2-89fb-93d7a0f961e5_1900x977.png"></p>
<p><code>We know trying out a new platform can be scary. Here are a few templates you can choose from to test out Notion with.</code></p>
<p>Everything else is blurred out to keep you focused on what Notion is trying to accomplish here. After you’ve selected a template, chances are, you still need some training to do. Here’s where the ‘Getting Started’ page comes in handy.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1604257365105/t29bWYhSi.png?auto=format&amp;q=60" alt="https___bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com_public_images_a3c14591-7737-47f6-9df7-d8cde125098e_1900x977.png"></p>
<p>The <em>Getting Started</em> page contains 4 tutorials to help you get started and a link at the end to their Youtube channel with more than 50 tutorials to help you even more.</p>

<p>Their Personal Onboarding experience is that simple. They use simple language, simple illustrations, and simple design to guide you through the most important stage of any online service. It’s a make-or-break point and Notion’s got it right.</p>
</div></div></section></div>]]>
            </description>
            <link>https://blog.moeminmamdouh.com/notions-onboarding-experience-a-case-of-simplicity-and-delightfulness</link>
            <guid isPermaLink="false">hacker-news-small-sites-24961797</guid>
            <pubDate>Sun, 01 Nov 2020 19:24:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Cowering to Genocide: Uighur Persecution and the World’s Last Hope]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24961697">thread link</a>) | @willbobaggins
<br/>
November 1, 2020 | https://www.willdjarvis.com/2020/10/31/cowering-to-genocide-uighur-persecution-and-the-worlds-last-hope/ | <a href="https://web.archive.org/web/*/https://www.willdjarvis.com/2020/10/31/cowering-to-genocide-uighur-persecution-and-the-worlds-last-hope/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-127">
		
	
	<div>
		<div>
<div>
<div>
<div>
<p>If you could stop a genocide-would you?&nbsp;</p>
<p>In western China, right now, more than a million men, women, and children sit in reeducation camps. Mothers separated from children, forced marriages, forced sterilizations-Margaret Atwood’s fiction made a reality.&nbsp;</p>
<p>What’s to be done? Have we learned nothing? From Rwanda, from the Holocaust, from the Holomodor? The cat’s out of the bag, and the End of history is over. Finally, we can acknowledge that the future is not just greyed out liberal democracy, 2% a year GDP growth, easing ourselves into decadence. In reality, our dark 20th century never really left us after all.&nbsp;</p>
<p>Modernity and the belief in bad people are incompatible.&nbsp; The enlightenment has swept conflict under the rug and right out the Overton window. Hobbes, banished from the lexicon. To even conceive a world with people who are themselves, <i>bad,</i> is a heretical assumption. To understand the aspects of human nature that are deeply problematic is to understand the true nature of reality. American culture lulled to sleep by narcotics and video games, our senses dulled by ultra-palatable foods and 2-hour delivery. In some sense, things are better than they ever have been before; in another sense, they’re much worse. Something is rotten in the state of Denmark. Americans are depressed, fat, and lead lives bereft of meaning. We are reduced to larping real revolutionary change on the streets, with bats, shields, and batons. A simulacra of revolution, a cry for meaning in a world gone mad.&nbsp;</p>
<p>I recall a cool October night in 2018. Joe Biden spoke softly to a crowd I was a part of. Bathed in dim light, he rambled. He wove soft, slow, soliloquies on American Greatness. He remarked on the magnificence of our landmass. That we are surrounded by vast oceans, blessed by great mineral reserves, possessing unrivalled food production capability, the mightiest navy the world has ever seen. The Zeihan-Esque supposition? Geography is destiny. Our geopolitical rivals? Nothing to worry about. China-why worry about China…&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; “c’mon man.”&nbsp;</p>
<p>This theory is seductive because it is easy. It’s a denial of agency-the idea that nothing can be done to challenge us. This makes our position precarious. You can’t fix problems you can’t see. If, like Biden says-we’re the greatest country in the world, why can’t we stop Uighur persecution in Xinjiang? The reason, I posit here, is that for all our technical capability, we lack the will and imagination to see it through.&nbsp;</p>
<p>In the farthest reaches of eastern North Carolina sits a massive dune of sand. Jockey’s Ridge rises out of the warm waters of the Pamlico Sound. Here, a century ago, two men from Ohio made history with the first powered flight.&nbsp; Wilbur and Orville Wright were bicycle mechanics-yet they were the first to fly. Imagine the expectations for a bicycle mechanic today. Produce a plane-are you kidding me? <i>That’s for the experts, the technocrats.</i> The institutionalization of science, of technology, of politics, has eroded our capability to achieve the American greatness we, as a people, are destined for. Science, once&nbsp; a process of obtaining truth-has become a bloated bureaucratic institution, a tool for politicians. . When our ruling class says “trust science” they don’t mean “trust the process of inquiry”, they mean “trust our experts.” Somehow, we have gone from a culture that encourages bicycle mechanics to invent an airplane to American institutions that can no longer prevent planes many times more advanced from diving <a href="https://en.wikipedia.org/wiki/Boeing_737_MAX"><u>straight into the ground</u></a>. (Really like this bit.)</p>
<p>The belief, the conceit, is that only experts can manipulate reality. But while experts have gotten us into this mess, they won’t get us out. This is not to denigrate experts, but the bureaucracies in which they work have become sick-sick enough that our planes fall out of the sky, and our factories that produce life-saving PPE have been shipped off our continent long ago. Their bureaucracies and their selection mechanisms have become corrupt, and sclerotic. What is to be done?</p>
<h3>Stopping a genocide.&nbsp;</h3>
<p>American military might is now too far behind to stop the genocide. Our carriers can be shot out of the water, with advanced surface to surface Chinese missiles. Tariffs won’t work either-just look at North Korea. but with a little imagination-a plan from me, a 26-year-old technologist emerges.&nbsp;</p>
<p>First, we need to visualize the problem as a chessboard. Each player has strategic pieces-some are stronger than others. Here, we want to understand how to incentivize the PRC to close the camps and allow Xinjiang a level of autonomy it rightly deserves. Bludgeoning them into submission won’t work. Attacking the PRC, whether rhetorically or physically will get us nowhere. The only way for us to avert a genocide is to alter the incentive structure, in such a way that it is easy and graceful for the PRC to change course. Attacking will just lead to path dependence for the CCP.&nbsp;</p>
<p>First, our strengths. Currently, America’s most powerful institutions are our worldwide entertainment and corporations. Hollywood, the NBA, Disney. These institutions have soft power that matters to a lot of people overseas. When I worked in China, I was always amazed at how popular the NBA was. The finals played on almost every TV and billboard I saw. With over 600 million viewers in China, the NBA is a cultural behemoth. It is important to note that China is a rising power, with a burgeoning middle class. This middle class finally has time to consume entertainment-and what a luxury it is.&nbsp;</p>
<p>America has a monopoly on the best basketball played anywhere. It’s quality is unsurpassed. Paragraph 1.1 of the Wikipedia page on Olympic basketball is aptly entitled “American Dominance.” A monopoly gives you market power. You can squash competitors, and you can force people to obey your will. In this way, the NBA has power, and I propose that it use it to solve a genocide.&nbsp;</p>
<p>The plan is quite simple. Adam Silver contacts the CCP, and requests that they start treating Uighurs decently, with a list of specific demands- let everyone go, and start over as if nothing ever happened. There is no need for this to be violent: everyone who was involved (police officers, bureaucrats, administrators) will be given new positions elsewhere and taken care of appropriately. The camps will be demolished, and each individual sent home. Mr Silver will do this respectfully, there is no need to be ugly, and without disrespect to the CCP’s power in mainland China.</p>
<p>If they refuse, the commissioner’s next move will be to offer alternatives.<i> A full-scale PR nightmare. We will publicly embrace the CCP, and Chinese people. No hate for either (or Xi himself), but we will pack our stands with Uighur refugees, and activists. We will put #freeuighurs on the court at every game, and we will continue to do this until the situation resolves to our liking.</i>&nbsp;</p>
<p>The CCP can try to ban the NBA, but it will face serious backlash. The party has experienced this before. When things are very unpopular, they try to fix them. 600 million fans of the NBA <i>want </i>to watch basketball, and even only a minority are diehard fans, those diehard fans will make their displeasure known, and it would hardly prove expedient to throw half the country into camps if they disagree with you.&nbsp;</p>
<p>As I like to say, “hell hath no fury like a consumer scorned.”&nbsp;</p>
<p>The worst-case scenario here is that China really does successfully ban the league. I find this eventuality unlikely, but this is where the State Department enters. We can organize a group that writes an insurance policy for the NBA-you lose revenue, we’ll cover your costs. Putting this together, we could make it a no-lose situation for the NBA.&nbsp;</p>
<p>This strategy, if implemented, has a high chance of success- in fact, it may have the highest chance of success that currently exists in the problem space. Tariffs don’t work, sanctions don’t work, military force won’t work, but maybe, just maybe, asking nicely, while carrying cultural clout of American basketball might.</p>
</div>
</div>
</div>
</div>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

				
</article></div>]]>
            </description>
            <link>https://www.willdjarvis.com/2020/10/31/cowering-to-genocide-uighur-persecution-and-the-worlds-last-hope/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24961697</guid>
            <pubDate>Sun, 01 Nov 2020 19:10:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[SQL X-to-Y]]>
            </title>
            <description>
<![CDATA[
Score 62 | Comments 28 (<a href="https://news.ycombinator.com/item?id=24961028">thread link</a>) | @panda17
<br/>
November 1, 2020 | https://www.damirsystems.com/sql-x-to-y/ | <a href="https://web.archive.org/web/*/https://www.damirsystems.com/sql-x-to-y/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-2056">

<div>
<p>What is the correct way to model one-to-one, one-to-many, many-to-many? Where do foreign keys go? What criteria to use? How to decide? I see these questions repeated on StackOverflow time and again.</p>
<p>The particular implementation should be based on <em>logic</em>. In most cases, <em>verbalizing</em> the constraint in <em>natural language</em> reveals the correct solution.</p>
<p>The following example uses two entities: <em>thing</em> <strong>T</strong>, and <em>category</em> <strong>C</strong>. Defining relationship with four most common multiplicities results in <tt>4x4=16</tt> possible constraints.</p>
<pre>Term                    Multiplicity
------------------------------------
is     in exactly one       [1]
is     in at most one       [0,1]
is     in at least one      [1,*]
may be in more than one     [0,*]
</pre>
<h2>Easy</h2>
<p>The following 11 cases can be declaratively defined in SQL using standard constraints <tt>(FK, PK, AK)</tt>. No need for triggers, deferred constraints, stored procedures, nor special application code.</p>
<hr>
<p><strong>T</strong> <tt>|-[1]------------[1]-|</tt> <strong>C</strong></p>
<p>1. Each thing <em>is</em> in <em>exactly one</em> category;<br>
for each category: <em>exactly one</em> thing <em>is</em> in that category.</p>
<pre>thing {T, C}
   PK {T}
   AK {C}
</pre>
<hr>
<p><strong>T</strong> <tt>|-[0,1]----------[1]-|</tt> <strong>C</strong></p>
<p>2. Each thing <em>is</em> in <em>exactly one</em> category;<br>
for each category: <em>at most one</em> thing <em>is</em> in that category.</p>
<pre>thing {T, C}
   PK {T}
   AK {C}

   FK {C} REFERENCES category {C}


category {C}
      PK {C}
</pre>
<hr>
<p><strong>T</strong> <tt>|-[1]----------[0,1]-|</tt> <strong>C</strong></p>
<p>3. Each thing <em>is</em> in <em>at most one</em> category;<br>
for each category: <em>exactly one</em> thing <em>is</em> in that category.</p>
<pre>thing {T}
   PK {T}


category {C, T}
      PK {C}
      AK {T}

      FK {T} REFERENCES thing {T}
</pre>
<hr>
<p><strong>T</strong> <tt>|-[0,*]----------[1]-|</tt> <strong>C</strong></p>
<p>4. Each thing <em>is</em> in <em>exactly one</em> category;<br>
for each category: <em>more than one</em> thing <em>may be</em> in that category.</p>
<pre>thing {T, C}
   PK {T}

   FK {C} REFERENCES category {C}


category {C}
      PK {C}
</pre>
<hr>
<p><strong>T</strong> <tt>|-[1]----------[0,*]-|</tt> <strong>C</strong></p>
<p>5. Each thing <em>may be</em> in <em>more than one</em> category;<br>
for each category: <em>exactly one</em> thing <em>is</em> in that category.</p>
<pre>thing {T}
   PK {T}


category {C, T}
      PK {C}

      FK {T} REFERENCES thing {T}
</pre>
<hr>
<p><strong>T</strong> <tt>|-[0,*]--------[0,*]-|</tt> <strong>C</strong></p>
<p>6. Each thing <em>may be</em> in <em>more than one</em> category;<br>
for each category: <em>more than one</em> thing <em>may be</em> in that category.</p>
<pre>thing {T}
   PK {T}


category {C}
      PK {C}


thing_category {T, C}
            PK {T, C}

           FK1 {T} REFERENCES thing    {T}
           FK2 {C} REFERENCES category {C}
</pre>
<hr>
<p><strong>T</strong> <tt>|-[0,*]--------[0,1]-|</tt> <strong>C</strong></p>
<p>7. Each thing <em>is</em> in <em>at most one</em> category;<br>
for each category: <em>more than</em> one thing <em>may be</em> in that category.</p>
<pre>thing {T}
   PK {T}


category {C}
      PK {C}


thing_category {T, C}
            PK {T}

           FK1 {T} REFERENCES thing    {T}
           FK2 {C} REFERENCES category {C}
</pre>
<hr>
<p><strong>T</strong> <tt>|-[0,1]--------[0,*]-|</tt> <strong>C</strong></p>
<p>8. Each thing <em>may be</em> in <em>more than one</em> category;<br>
for each category: <em>at most one</em> thing <em>is</em> in that category.</p>
<pre>thing {T}
   PK {T}


category {C}
      PK {C}


thing_category {T, C}
            PK {C}

           FK1 {T} REFERENCES thing    {T}
           FK2 {C} REFERENCES category {C}
</pre>
<hr>
<p><strong>T</strong> <tt>|-[0,1]--------[0,1]-|</tt> <strong>C</strong></p>
<p>9. Each thing <em>is</em> in <em>at most one</em> category;<br>
for each category: <em>at most one</em> thing <em>is</em> in that category.</p>
<pre>thing {T}
   PK {T}


category {C}
      PK {C}


thing_category {T, C}
            PK {T}
            AK {C}

           FK1 {T} REFERENCES thing    {T}
           FK2 {C} REFERENCES category {C}
</pre>
<hr>
<p><strong>T</strong> <tt>|-[1,*]--------[0,*]-|</tt> <strong>C</strong></p>
<p>10. Each thing <em>may be</em> in <em>more than one</em> category;<br>
for each category: <em>at least one</em> thing <em>is</em> in that category.</p>
<pre>thing {T}
   PK {T}


category {C, T}
      PK {C}

      FK {T} REFERENCES thing {T}


thing_cat {T, C}
       PK {T, C}

      FK1 {T} REFERENCES thing    {T}
      FK2 {C} REFERENCES category {C}


VIEW thing_category {T, C}
AS
category  {T, C}
UNION
thing_cat {T, C}
</pre>
<hr>
<p><strong>T</strong> <tt>|-[0,*]--------[1,*]-|</tt> <strong>C</strong></p>
<p>11. Each thing <em>is</em> in <em>at least one</em> category;<br>
for each category: <em>more than one</em> thing <em>may be</em> in that category.</p>
<pre>thing {T, C}
   PK {T}

   FK {C} REFERENCES category {C}


category {C}
      PK {C}


thing_cat {T, C}
       PK {T, C}

      FK1 {T} REFERENCES thing    {T}
      FK2 {C} REFERENCES category {C}


VIEW thing_category {T, C}
AS
thing {T, C}
UNION
thing_cat {T, C}
</pre>
<h2>Not Easy</h2>
<p>Cases from 12 to 16 require use of triggers, stored procedures, deferred constraints, or application code to enforce the constraint.</p>
<p>12. <strong>T</strong> <tt>|-[1,*]----------[1]-|</tt> <strong>C</strong><br>
Each thing <em>is</em> in <em>exactly one</em> category;<br>
for each category: <em>at least one</em> thing <em>is</em> in that category.</p>
<p>13. <strong>T</strong> <tt>|-[1]----------[1,*]-|</tt> <strong>C</strong><br>
Each thing <em>is</em> in <em>at least one</em> category;<br>
for each category: <em>exactly one</em> thing <em>is</em> in that category.</p>
<p>14. <strong>T</strong> <tt>|-[1,*]--------[0,1]-|</tt> <strong>C</strong><br>
Each thing <em>is</em> in <em>at most one</em> category;<br>
for each category: <em>at least one</em> thing <em>is</em> in that category.</p>
<p>15. <strong>T</strong> <tt>|-[0,1]--------[1,*]-|</tt> <strong>C</strong><br>
Each thing <em>is</em> in <em>at least one</em> category;<br>
for each category: <em>at most one</em> thing <em>is</em> in that category.</p>
<p>16. <strong>T</strong> <tt>|-[1,*]--------[1,*]-|</tt> <strong>C</strong><br>
Each thing <em>is</em> in <em>at least one</em> category;<br>
for each category: <em>at least one</em> thing <em>is</em> in that category.</p>
<h3>The Problem</h3>
<p>The main problem is lack of assertions (DB-wide constraints) in main SQL implementations. SQL standard actually defines them (<tt>CREATE ASSERTION</tt>), but no luck yet. Hence, not every business constraint can be elegantly defined in SQL. Often some creativity, compromise, and awkwardness is required.</p>
<hr>
<p>Note:</p>
<pre>All attributes (columns) NOT NULL

PK = Primary Key
AK = Alternate Key (Unique)
FK = Foreign Key
</pre>
</div>

</article></div>]]>
            </description>
            <link>https://www.damirsystems.com/sql-x-to-y/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24961028</guid>
            <pubDate>Sun, 01 Nov 2020 17:34:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Danish military intelligence uses XKEYSCORE to tap cables in co-op with the NSA]]>
            </title>
            <description>
<![CDATA[
Score 655 | Comments 214 (<a href="https://news.ycombinator.com/item?id=24960994">thread link</a>) | @XzetaU8
<br/>
November 1, 2020 | https://www.electrospaces.net/2020/10/danish-military-intelligence-uses.html | <a href="https://web.archive.org/web/*/https://www.electrospaces.net/2020/10/danish-military-intelligence-uses.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-body-7080478453343339184" itemprop="description articleBody">
<p>
Last August, it came out that a whistleblower accused the Danish military and signals intelligence service (<a href="https://en.wikipedia.org/wiki/Danish_Defence_Intelligence_Service" target="_blank"><i>Forsvarets Efterretningstjeneste</i></a> or FE) of unlawful activities and deliberately misleading the intelligence oversight board. </p><p>

Meanwhile, the Danish press was able to paint a surprisingly comprehensive and detailed picture of how the FE cooperated with the NSA in <a href="#cabletapping"><b>cable tapping</b></a> on Danish soil.</p><p>

It was further revealed that the Americans provided Denmark with a sophisticated <a href="#spysystem"><b>new spy system</b></a> which includes the NSA's data processing system <a href="#xkeyscore"><b>XKEYSCORE</b></a>.</p><p>

A Danish paper also disclosed that the accusation of <a href="#unlawful"><b>unlawful collection</b></a> came from a young FE employee who reminds of Edward Snowden. A newly established investigation commission now has to clarify whether he was driven by fears or by facts.</p><p><a href="https://1.bp.blogspot.com/-FEhoYi_a0EI/X3lHJbfeSlI/AAAAAAAAEyk/urOidzARWswgJstbeNFiU8CRo38JQMufACPcBGAYYCw/s800/fe-xks-header.jpg"><img alt="" data-original-height="420" data-original-width="800" src="https://1.bp.blogspot.com/-FEhoYi_a0EI/X3lHJbfeSlI/AAAAAAAAEyk/urOidzARWswgJstbeNFiU8CRo38JQMufACPcBGAYYCw/s800/fe-xks-header.jpg" width="600"></a></p>
<p><span size="2">
The Sandagergård complex of the FE on the island of Amager, where a new<br> 
data center was built for its deployment of the XKEYSCORE system<br>
</span>
</p>


<p><span size="+2">Cable tapping</span></p><p>

In an extensive piece from September 13, the renowned Danish newspaper <a href="https://www.berlingske.dk/samfund/et-pengeskab-paa-kastellet-har-i-aartier-gemt-paa-et-dybt-fortroligt" target="_blank"><i>Berlingske</i></a> (founded in 1749) describes how the FE, in cooperation with the NSA, started to tap an international telecommunications cable in order to gather foreign intelligence.</p><p>
  
  In the mid-1990s, the NSA had found out that somewhere under Copenhagen there was a backbone cable containing phone calls, e-mails and text messages from and to countries like China and Russia, which was of great interest for the Americans.</p><p>
  
Tapping that cable, however, was almost impossible without the help of the Danes, so the NSA asked the FE for access to the cable, but this request was denied, according to Berlingske.</p><p>
  
  
  <b>Agreement with the United States</b></p><p>
  
  The US government did not give up, and in a letter sent directly to the Danish prime minister Poul Nyrup Rasmussen, US president Clinton asked his Danish colleague to reconsider the decision. And Nyrup, who was a sworn supporter of a close relationship with the US, said yes. </p><p>
  
  The cooperation was laid down in a document, which, according to Berlingske, all Danish defense ministers had to sign "so that any new minister could see that his predecessor - and his predecessors before his predecessors - with their signatures had been part of this small, exclusive circle of people who knew one of the kingdom's biggest secrets."</p><p> 
    
  The code name for this cooperation is not known, but it's most likely part of the NSA's umbrella program <a href="https://www.electrospaces.net/2014/10/the-german-operation-eikonal-as-part-of.html#rampart-a">RAMPART-A</a>. Under this program, which started in 1992, foreign partners <a href="https://s3.amazonaws.com/s3.documentcloud.org/documents/1200864/tssinframpartaoverview-v1-0-redacted-information.pdf" target="_blank">provide</a> access to high-capacity international fiber-optic cables, while the US provides the equipment for transport, processing and analysis:</p><p><a href="https://1.bp.blogspot.com/-VQv_x5OW4VM/VD2MIOzUWkI/AAAAAAAAB8o/rxZgVvQ4hmI/s1600/rampart-a-2010-diagram.jpg" imageanchor="1" target="_blank"><img src="https://1.bp.blogspot.com/-VQv_x5OW4VM/VD2MIOzUWkI/AAAAAAAAB8o/rxZgVvQ4hmI/s1600/rampart-a-2010-diagram.jpg" title="Slide from an NSA presentation about the RAMPART-A program" width="450"></a></p>
   
  <p>

  <b>Agreement with a cable operator</b></p><p>
  
To make sure that tapping the cable was as legal as possible, the government asked approval of the private Danish company that operated the cable. The company agreed, but only when it was approved at the highest level, and so the agreement was signed by prime minister Rasmussen, minister of defense Hækkerup and head of department Troldborg. </p><p>
  
  Because the cable contained international telecommunications it was considered to fall within the FE's foreign intelligence mandate. The agreement was prepared in only one copy, which was shown to the company and then locked in a safe at the FE's headquarters at the <a href="https://web.archive.org/web/20200811152740/https://fe-ddis.dk/om-os/Organisation/lokaliteter-i-dk/Pages/Kastellet.aspx" target="_blank">Kastellet</a> fortress in Copenhagen, according to Berlingske.</p><p>

 This Danish agreement is very similar to the <a href="http://download.krone.at/pdf/VertragWZschwarz.pdf" target="_blank">Transit Agreement</a> between the German foreign intelligence service BND and Deutsche Telekom, in which the latter agreed to provide access to international transit cables at its switching center in Frankfurt am Main. The BND then tapped these cables with help from the NSA under <a href="https://www.electrospaces.net/2015/05/new-details-about-joint-nsa-bnd.html">operation Eikonal</a> (2004-2008).</p><p>
  
  
    <b>Processing at Sandagergård</b></p><p>
  
  Berlingske reported that the communications data that were extracted from the backbone cable in Copenhagen were sent from the Danish company's technical hub to the <a href="https://web.archive.org/web/20200811153920/https://fe-ddis.dk/om-os/Organisation/lokaliteter-i-dk/Pages/Sandagergaard.aspx" target="_blank">Sandagergård complex</a> of the FE on the island of <a href="https://en.wikipedia.org/wiki/Amager" target="_blank">Amager</a>. The US had paid for a cable between the two locations.</p><p>
  
  At Sandagergård, the "NSA made sure to install the technology that made it possible to enter keywords and translate the huge amount of information, so-called raw data from the cable tapping, into "readable" information."</p><p>
  
The filter system was not only fed by keywords from the FE, but the NSA also provided "the FE with a series of keywords that are relevant to the United States. The FE then reviews them - and checks that there are basically no Danes among them - and then enters the keywords" according to sources cited by Berlingske.</p><p>
  
  Besides this filtering with keywords and selectors, the FE and the NSA will also have used the metadata for <a href="https://edwardsnowden.com/docs/doc/B17-TDB-Knowledge-Sharing.pdf" target="_blank">contact-chaining</a>, which means reconstructing which phone numbers and e-mail addresses had been in contact with each other, in order to create social network graphs - something the sources apparently didn't want to disclose to Berlingske.</p><p><a href="https://1.bp.blogspot.com/-zFJ49f56CXY/X5PXqIaouqI/AAAAAAAAEzY/TJwURx3Jn6oCxN9s4I3LqKKu1EhtLPEAACPcBGAYYCw/s1351/dk-cables.JPG"><img alt="" data-original-height="830" data-original-width="1351" src="https://1.bp.blogspot.com/-zFJ49f56CXY/X5PXqIaouqI/AAAAAAAAEzY/TJwURx3Jn6oCxN9s4I3LqKKu1EhtLPEAACPcBGAYYCw/s1351/dk-cables.JPG" width="550"></a></p>
  <p><span size="2">
  Map of the current backbone cables around the Danish capital Copenhagen<br>
  and the Sandagergård complex of the FE on the island of Amager<br>
  (source: <a href="https://live.infrapedia.com/app" target="_blank">Infrapedia</a> - click to enlarge)<br>
</span>
</p>

  <p>
  
      <b>Trusted partners</b></p><p>
  
  Part of the agreement between the US and Denmark was that "the USA does not use the system against Danish citizens and companies. And the other way around". Similar words can be found in an <a href="https://www.documentcloud.org/documents/1200860-odd-s3-overviewnov2011-v1-0-redacted-information.html" target="_blank">NSA presentation</a> from 2011: "No US collection by Partner and No Host Country collection by US" - although this is followed by "there ARE exceptions!"</p><p>
  
  The latter remark may have inspired Edward Snowden to accuse the NSA of abusing these cooperations with foreign partner agencies to spy on European citizens, but as a source told Berlingske: </p><p>
  
  "I can not at all imagine in my imagination that the NSA would betray that trust. I consider it completely and utterly unlikely. If the NSA had a desire to obtain information about Danish citizens or companies, the United States would simply turn to [the domestic security service] PET, which would then provide the necessary legal basis."</p><p>
  
The source also said that "the NSA wanted to jump and run for Denmark. The agency did everything Denmark asked for, without discussion. The NSA continuously helped Denmark - because of this cable access. [...] Denmark was a very, very close and valued partner."</p><p>
    
This close and successful cooperation was apparently one of the reasons for the visit of president Bill Clinton to Denmark in July 1997, according to Berlingske.</p><p><a href="https://1.bp.blogspot.com/-1-v3T8EQqk0/X3gIQLWgChI/AAAAAAAAEyE/j-yGk3qiStcS3Y692bc25UT6WG1_B_2BACLcBGAsYHQ/s0/clinton-rasmussen.jpg"><img alt="" src="https://1.bp.blogspot.com/-1-v3T8EQqk0/X3gIQLWgChI/AAAAAAAAEyE/j-yGk3qiStcS3Y692bc25UT6WG1_B_2BACLcBGAsYHQ/s0/clinton-rasmussen.jpg" width="550"></a></p>
  <p><span size="2">
Danish prime minister Poul Nyrup Rasmussen and US president Bill Clinton<br>
  during his visit to Denmark in July 1997 (photo: Linda Kastrup)<br>
</span>
</p>

<p><span size="+2">A new spy system</span></p><p>
  
  In the wake of the FE scandal even more recent developments have been revealed: a <a href="https://www.dr.dk/nyheder/indland/ny-afsloering-fe-masseindsamler-oplysninger-om-danskere-gennem-avanceret-spionsystem" target="_blank">report</a> by the Danish broadcaster DR from September 24, 2020 provides interesting details about how the Americans provided Denmark with a sophisticated new "spy system".</p><p>
  
After the FE got a new head of procurement in 2008, NSA employees frequently traveled to Denmark for quite some time to build the necessary hardware and install the required software for the new system, which DR News describes as extremely advanced. It also has a special internal code name, which the broadcaster decided not to publish. It's also this new system through which the alleged illegal collection of Danish data took place.</p><p>
  
  According to DR News, the NSA technicians were also involved in the construction of a new data center at the FE's <a href="https://web.archive.org/web/20200811153920/https://fe-ddis.dk/om-os/Organisation/lokaliteter-i-dk/Pages/Sandagergaard.aspx" target="_blank">Sandagergård complex</a> on Amager that was specifically built to house the new spy system, which was taken into use somewhere between 2012 and 2014. The cooperation between the FE and the NSA on this specific system was based upon a Memorandum of Understanding (MoU) signed by then FE chief <a href="https://da.wikipedia.org/wiki/Thomas_Ahrenkiel" target="_blank">Thomas Ahrenkiel</a>.</p><p>

  
  
  <b>Filter systems</b></p><p>
  
The DR News <a href="https://www.dr.dk/nyheder/indland/ny-afsloering-fe-masseindsamler-oplysninger-om-danskere-gennem-avanceret-spionsystem" target="_blank">report</a> also goes into more detail about the interception process. It says that first, the intelligence service identifies a data stream that may be interesting, after which they "mirror" the light that passes through the particular fiber-optic cables. In this way, they copy both metadata and content, like text messages, chat conversations, phone calls and e-mails, and send them to the FE's data center at Sandagergård.</p><p>

  
According to DR News, the FE tried to develop a number of filters to ensure that data from Danish citizens and companies is sorted out and not made searchable by the new spy system. The former Danish minister of defense Claus Hjort Frederiksen recently <a href="http://www.weekendavisen.dk/2020-37/samfund/landsskadeligt" target="_blank">said</a> that there was indeed an attempt to develop such filters, but at the same time he admitted that there can be no guarantee that no Danish information will pass through.</p><p><span size="+2">XKEYSCORE</span></p><p>

  DR News also reported that the heart of the new spy system is formed by <a href="https://en.wikipedia.org/wiki/XKeyscore" target="_blank">XKEYSCORE</a>, which was developed by the NSA and the existence of which was first <a href="https://www.theguardian.com/world/2013/jul/31/nsa-top-secret-program-online-data" target="_blank">revealed</a> by The Guardian in June 2013.</p><p> 
  
  The NSA's British counterpart GCHQ incorporated XKEYSCORE in its own system for processing bulk internet data codenamed <a href="https://en.wikipedia.org/wiki/Tempora" target="_blank">TEMPORA</a> and it can be assumed that the other <a href="https://www.electrospaces.net/2014/09/nsas-foreign-partnerships.html#2ndparty">Second Party</a> partners (also known as the Five Eyes) also use this system, whether or not under a different codename.</p><p><a href="https://1.bp.blogspot.com/-t6LWjnPz3Wo/X3LLi61rZzI/AAAAAAAAExc/-F5vREaIi4oBCAZ_LNhvOOJFVnHnLlFbgCLcBGAsYHQ/s0/xks%2Bintro.JPG" target="_blank"><img alt="" src="https://1.bp.blogspot.com/-t6LWjnPz3Wo/X3LLi61rZzI/AAAAAAAAExc/-F5vREaIi4oBCAZ_LNhvOOJFVnHnLlFbgCLcBGAsYHQ/s0/xks%2Bintro.JPG" width="500"></a></p>
  <p>
  
From the Snowden documents we know that the NSA also provided XKEYSCORE to some of its <a href="https://www.electrospaces.net/2014/09/nsas-foreign-partnerships.html#3rdparty">Third Party</a> partners: the German <a href="https://www.electrospaces.net/2016/09/secret-report-reveals-german-bnd-also.html">foreign intelligence service BND</a> and domestic security service BfV, the Swedish signals intelligence service FRA and the Japanese Directorate for SIGINT. It is new though that the Danish military intelligence service FE uses the system too.</p><p>

  
Some press reports seem to suggest that these partner agencies "gain access to XKEYSCORE" as if it would allow them to connect to a huge global mass surveillance system. The latter may be the case for the NSA's Second Party partners, but the Third Party partners are using XKEYSCORE only to process and analyze data from their own tapping points and are not able to access data from Five Eyes collection platforms.</p><p>
  
  Likewise, NSA analysts using XKEYSCORE don't have direct …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.electrospaces.net/2020/10/danish-military-intelligence-uses.html">https://www.electrospaces.net/2020/10/danish-military-intelligence-uses.html</a></em></p>]]>
            </description>
            <link>https://www.electrospaces.net/2020/10/danish-military-intelligence-uses.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24960994</guid>
            <pubDate>Sun, 01 Nov 2020 17:30:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[C/C++ vs. Rust Performance]]>
            </title>
            <description>
<![CDATA[
Score 101 | Comments 146 (<a href="https://news.ycombinator.com/item?id=24960108">thread link</a>) | @krizhanovsky
<br/>
November 1, 2020 | http://tempesta-tech.com/blog/fast-programming-languages-c-c++-rust-assembly | <a href="https://web.archive.org/web/*/http://tempesta-tech.com/blog/fast-programming-languages-c-c++-rust-assembly">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div> <p><i>Posted on October 28, 2020</i></p><p>This article isn't about which programming language is better, instead it discusses the most powerful tool set for development of the fastest server-side system software, such as database engines and HTTPS servers. There are several specific properties of this type of software: </p><ul><li>Relatively large code base, 100,000 lines of C or C++ code and more. While it's possible to write particular, the most 'hot' functions, in Assembly language, it's impractical to write the whole program in Assembly. </li><li>Databases and web servers are mission-critical software - we all got used that our Linux systems with MySQL and Nginx processes work for months and years. There are <a href="https://github.com/tempesta-tech/tempesta/wiki/High-availability">simple high availability</a> best practices mitigating the downtime due to possible crashes, but they're the subject for another article. Meantime, it's worth mentioning that if you really-really care about high availability, then you should build you infrastructure with an assumption that any component of your system may crash at any time, just like <a href="https://lwn.net/Articles/801871/">Facebook does this</a> -the company deploys the recent versions of the Linux kernel as soon as they're available. </li></ul><p>We've been developing the <a href="http://tempesta-tech.com/c++-services">fastest software in C, C++, and Assembly</a> for ages. It's not a surprise that since Rust is <a href="https://en.wikipedia.org/wiki/Rust_(programming_language)">"focused on performance"</a> we're very interested in it. With a bit of skepticism though. Just remember the rise of Java programming language: there were a lot of reports that the JIT compilation produced code faster than C++. Now it's hard to find a case, when C++ is slower than Java, see for example the <a href="https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/java.html">benchmark game</a>. It's also worth mentioning that the memory garbage collection (GC) in Java leads to high tail latencies and it's hard or even impossible to do anything with the problem. Golang can not be considered for high performance programming also due to the GC. </p><h2>C or C++? Or both of them?</h2><p> The C programming language dominates in the system programming. An operating system kernel is an example of one of the most sophisticated system software, not only because it deals with hardware directly, but also due to strict performance requirements. The Linux and FreeBSD kernels are written in C, as well as the other UNIX'es and Windows kernels. Let's start the discussion from this bright example of high-performance system software. </p><h3>C++ for operating systems kernel development</h3><p> FreeBSD <a href="https://forums.freebsd.org/threads/driver-building-in-c.35701/">has been supporting C++ modules</a> for a while. While the Linux kernel never supported C++, there was <a href="https://pdos.csail.mit.edu/papers/click:tocs00/paper.pdf">the Click modular router</a> written in C++ and working as a Linux kernel module. If you're interested in C++ applicability for the operating systems kernel development, then you can find quite good discussions in the <a href="https://wiki.osdev.org/C++">C++</a> and <a href="https://wiki.osdev.org/C++_Bare_Bones">Bare bones</a> articles. However, there are fundamental reasons against using C++ for operating system kernel development: </p><ul><li>You do not have <code>libstdc++</code> with <a href="https://en.wikipedia.org/wiki/Run-time_type_information">RTTI</a> and exceptions in the kernel space. Actually, <code>dynamic_cast</code> isn't so frequently used and there are a lot of C++ projects compiled without RTTI. If you need exceptions, then you have to port them into the kernel. <code>libstdc++</code> uses basic C allocations, so it must be significantly reworked for the kernel. </li><li>You can't use the STL and Boost libraries and, in fact, all kernels already have their own libraries. C++ introduces filesystem, threading and networking libraries, which are senseless in an OS kernel. From the other hand, the modern OSes provide advanced synchronization primitives, which are still not available in standard C++ (e.g. there is still no read-write spinlocks in C++). </li><li>The Linux kernel provides number of memory allocators (SLAB, page, <code>vmalloc()</code>, <code>kmalloc()</code>, and so on), thus you have to use <a href="https://en.cppreference.com/w/cpp/language/new"><code>placement new</code></a> and/or just use the C functions for memory allocation and freeing. Aligned memory is crucial for the performance, but you need to write special wrappers to get aligned memory with <code>new</code>. </li><li>Strong type safety isn't so comfortable for system programming when raw memory pointers are frequently casted to some data structures. This is debatable though: while some people are uncomfortable with frequent <code>reinterpret_cast&lt;Foo *&gt;(ptr)</code> instead of short <code>(Foo *)ptr</code>, the others are good with more typing and more type safety. </li><li>C++ <a href="https://en.wikipedia.org/wiki/Name_mangling">name mangling</a>, required for namespaces and function overloading, makes function hard to call from Assembly, so you need to use <code>extern "C"</code>. </li><li>You have to make special code sections for static objects constructors and destructors, <code>.ctor</code> and <code>.dtor</code> correspondingly. </li><li>C++ exceptions can not cross <i>context</i> boundaries, i.e. you can not throw an exception in one thread and catch it in another. The operating system kernel deals with much more complex context model: there are kernel threads, user space processes entering into the kernel, deferred and hardware interruptions. The contexts can preempt each other in voluntarily or cooperative manner, so exception handling of current context could be preempted by another context. There are also memory management and contexts switching code which could conflict with exception handling code. Just like for RTTI, it's possible to implement the mechanism in kernel, but the current standard library can not be used. </li><li>While Clang and G++ support <code>__restrict__</code> extension, the official C++ standard <a href="https://www.quora.com/Why-doesnt-C++-have-an-equivalent-of-Cs-restrict-specifier">does not support</a> it. </li><li>Variable length arrays (VLA) are <a href="https://lwn.net/Articles/749064/">discouraged</a> in the Linux kernel, they are still handy in some scenarios, but are <a href="https://groups.google.com/g/comp.std.c++/c/K_4lgA1JYeg?pli=1">completely unavailable in C++</a>. </li></ul><p> Thus, with C++ in the kernel space you basically have only templates, classes inheritance and some syntax sugar like lambda functions. Since system code is quite rarely requires complicated abstractions and inheritances, then does it still have sense to make effort to use C++ in the kernel space? </p><h3>C++ exceptions</h3><p> This is one of the most <a href="https://herbsutter.com/2010/03/13/trip-report-march-2010-iso-c-standards-meeting/">debatable</a> C++ feature and it deserves a separate chapter. For example, the MySQL project, following to the <a href="https://google.github.io/styleguide/cppguide.html#Exceptions">Google coding style</a>, <a href="https://dev.mysql.com/doc/dev/mysql-server/latest/PAGE_CODING_GUIDELINES.html">doesn't use exceptions</a>. The Google coding style provides the good lists of pros and cons of using exceptions. Here we focus on performance aspects only. </p><p>Exceptions can improve performance when we have to handle error codes in too may places, e.g. (let the functions be inlined and very small) </p><pre><code>
        if (func_1())
            return -EINVAL;
        if (func_2())
            return -EINVAL;
        ....
        if (func_100())
            return -EINVAL;
    </code></pre><p> The problem with the code is that there are extra conditional jumps. Modern CPU are pretty good with branch prediction, but it still hurts performance. In C++ we can just write </p><pre><code>
        try {
            func_1();
            func_2();
            ...
            func_100();
        } catch (...) {
            return -EINVAL;
        }
    </code></pre><p> , so there are no extra conditions in the <i>hot</i> path. However, this isn't for free: most of the functions in your C++ code have to have extra epilogues with a table of exceptions, which these functions can catch, and an appropriate cleanup table. The function epilogues aren't executed in normal workflow, but they increase the size of code causing extra pollution in the CPU instruction cache. You can find great details about C++ exception handling internals in the <a href="https://monoinfinito.wordpress.com/series/exception-handling-in-c/">Nico Brailovsky's blog</a>. </p><h3>Is C++ still good?</h3><p> Yes, it is. Firstly, not the whole code actually must be as fast as possible and in most of the places we don't need custom memory allocation and don't care about exceptions overhead. The most of the projects are developed in the user space and benefit, especially the new ones, from relatively rich C++ standard and Boost libraries (not so rich as Java's though). </p><p>Secondly, the killing feature of C++ is that <b>it is C</b>. If you don't want to use exceptions or RTTI, then you can just switch the features off. Most of C programs can be just compiled with a C++ compiler with very small changes or without any changes at all. As an example, we need only this trivial change </p><pre><code>
    $ diff -up nbody.c nbody-new.cc
        @@ -112,9 +112,9 @@ static void advance(body bodies[]){
             // ROUNDED_INTERACTIONS_COUNT elements to simplify one of the following
             // loops and to also keep the second and third arrays in position_Deltas
             // aligned properly.
        -    static alignas(__m128d) double
        -      position_Deltas[3][ROUNDED_INTERACTIONS_COUNT],
        -      magnitudes[ROUNDED_INTERACTIONS_COUNT];
        +    static double
        +      position_Deltas[3][ROUNDED_INTERACTIONS_COUNT] __attribute__((aligned(16))),
        +      magnitudes[ROUNDED_INTERACTIONS_COUNT] __attribute__((aligned(16)));

             // Calculate the position_Deltas between the bodies for each interaction.
             for(intnative_t i=0, k=0; i &lt; BODIES_COUNT-1; ++i)
    </code></pre><p> to compile <a href="https://benchmarksgame-team.pages.debian.net/benchmarksgame/program/nbody-clang-8.html">the C program</a> with G++ compiler. The modern C++ compilers provide C compatibility extensions like the <code>__restrict__</code> keyword. You always can write the most performance critical code of a C++ program in C style. If you don't like the <a href="https://250bpm.com/blog:8/">STL containers with an overhead</a>, then you can use <a href="https://www.boost.org/doc/libs/1_74_0/doc/html/intrusive.html">Boost.intrusive</a> or even port a similar container from the Linux kernel or other fast C project -in most of the cases this won't be painful. See for example how a hash table from PostgreSQL, HTrie from <a href="https://github.com/tempesta-tech/tempesta/tree/master/tempesta_db">Tempesta DB</a>, and the Linux kernel read/write spinlocks (all are written in C) were used in a C++ <a href="https://github.com/tempesta-tech/blog/tree/master/htrie">benchmark</a>. </p><p>The last thing which must be mentioned about development of high performance programs in C++ is <a href="https://en.wikipedia.org/wiki/Template_metaprogramming">template metaprogramming</a>. It's very exciting about the modern C++ standards that using templates you can write quite sophisticated logic which is fully computed in the compile time and costs nothing in the run time. </p><h2>GOTO - the power of C</h2> <p><b>A professional tool must allow you to work with it in the most efficient way.</b> The goal of the high-level and high-performance programming languages is to generate the most efficient machine code. Each hardware architecture supports <i>jumps</i>, which means that you can jump to any address by any condition. The closest abstraction for the jumps in the C and C++ programming languages is <code>goto</code> operator. It's not so flexible …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://tempesta-tech.com/blog/fast-programming-languages-c-c++-rust-assembly">http://tempesta-tech.com/blog/fast-programming-languages-c-c++-rust-assembly</a></em></p>]]>
            </description>
            <link>http://tempesta-tech.com/blog/fast-programming-languages-c-c++-rust-assembly</link>
            <guid isPermaLink="false">hacker-news-small-sites-24960108</guid>
            <pubDate>Sun, 01 Nov 2020 15:33:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[You might not need to store (plaintext) emails]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24959734">thread link</a>) | @danielskogly
<br/>
November 1, 2020 | https://blog.klungo.no/2020/11/01/you-might-not-need-to-store-plaintext-emails/ | <a href="https://web.archive.org/web/*/https://blog.klungo.no/2020/11/01/you-might-not-need-to-store-plaintext-emails/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Earlier this year, when I went from having only Facebook-login on <a href="https://wishy.gift/">Wishy.gift</a> to allow registrations with email address and password, one of my concerns was how to implement this is a way that protects the data and privacy of my users. I don’t have any ads or analytics on the site, the users can select whatever display name they want, and I never stored the email addresses I got from Facebook when a user registered or logged in - only a hashed<sup><a href="#fn1" id="fnref1">[1]</a></sup> version of the ID. Email addresses and passwords, on the other hand, are a whole other beast, and the consequences of a database breach much worse.</p>
<p>Considering that the only kind of emails I ever need to send out are transactional - no newsletters or other kinds of notifications - the only thing I need to store them for are as identifiers, and can safely be hashed.</p>
<p>For every transactional email I need to send out - registration, account recovery, and email change verification - the user always initiates this by submitting their email address, and it will at that time be available to the backend to perform the needed action.</p>
<p>In conclusion, if you only use email addresses for transactional emails, you might be able to only store hashed versions of them. For <a href="https://wishy.gift/">Wishy.gift</a> I use SHA512 with a fixed salt, and this has been working perfectly since implementation in June.</p>
<p>Thank you for reading this! I would love to hear your thoughts and ideas too. Join the discussion on <a href="https://news.ycombinator.com/item?id=24959734">Hacker News</a>, or feel free to email me at <code>daniel</code> at the domain this blog is on.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>I discovered that, even though the ID was unique to my FB-app, it was still possible to go to <a href="http://facebook.com/%7Bid%7D">facebook.com/{id}</a> and be redirected to the user’s FB-profile. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
</div></div>]]>
            </description>
            <link>https://blog.klungo.no/2020/11/01/you-might-not-need-to-store-plaintext-emails/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24959734</guid>
            <pubDate>Sun, 01 Nov 2020 14:39:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: A JavaScript SDK to reduce video streaming costs]]>
            </title>
            <description>
<![CDATA[
Score 68 | Comments 50 (<a href="https://news.ycombinator.com/item?id=24959519">thread link</a>) | @Anil1331
<br/>
November 1, 2020 | https://api.peervadoo.com/test | <a href="https://web.archive.org/web/*/https://api.peervadoo.com/test">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                <div>
                    <p><label for="url">Url:</label></p><!--<input type="text" class="form-control" id="url", placeholder="Please enter video url", value="https://d34mbcqhk5ge1k.cloudfront.net/index.m3u8">-->
                    </div>
                </div></div>]]>
            </description>
            <link>https://api.peervadoo.com/test</link>
            <guid isPermaLink="false">hacker-news-small-sites-24959519</guid>
            <pubDate>Sun, 01 Nov 2020 13:57:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fast Inverse Square Root]]>
            </title>
            <description>
<![CDATA[
Score 215 | Comments 71 (<a href="https://news.ycombinator.com/item?id=24959157">thread link</a>) | @timhh
<br/>
November 1, 2020 | https://timmmm.github.io/fast-inverse-square-root/ | <a href="https://web.archive.org/web/*/https://timmmm.github.io/fast-inverse-square-root/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">


  <p>The famous <em>fast inverse square root</em> is some mysterious code <a href="https://www.beyond3d.com/content/articles/8/">not written</a> by programming legend John Carmack to calculate a fast approximation of $1/\sqrt{x}$:</p>

  <pre>  // Code from Quake 3 Arena.
  float Q_rsqrt( float number )
  {
    long i;
    float x2, y;
    const float threehalfs = 1.5F;

    x2 = number * 0.5F;
    y  = number;
    i  = * ( long * ) &amp;y;                       // evil floating point bit level hacking
    i  = 0x5f3759df - ( i &gt;&gt; 1 );               // what the fuck?
    y  = * ( float * ) &amp;i;
    y  = y * ( threehalfs - ( x2 * y * y ) );   // 1st iteration
  //	y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed

    return y;
  }
  </pre>

  <p>Games calculate square roots and inverse square roots all the time to find the lengths of vectors and <a href="https://www.khanacademy.org/computing/computer-programming/programming-natural-simulations/programming-vectors/a/vector-magnitude-normalization">to normalise them</a>, but using the <a href="https://en.cppreference.com/w/cpp/numeric/math/sqrt"><code>sqrt()</code></a> function can be very slow. The code above finds an approximate result much more quickly through some integer magic. I was curious how it works but the <a href="https://en.wikipedia.org/wiki/Fast_inverse_square_root">Wikipedia page</a> describing it is <em>so badly written</em> that it was easier to figure it out from scratch. Here's what I found.</p>

<h2>The Basic Idea</h2>

  <p>The code works by computing an initial approximation to $1/\sqrt{x}$ (which can also be written as $x^{-1/2}$) and then refining it using one or two iterations of <a href="https://en.wikipedia.org/wiki/Newton%27s_method">the Newton-Raphson method</a>. If you aren't familiar with the Newton-Raphson method, don't worry - it isn't very complicated and also isn't the clever bit so you can ignore it if you like. There are a ton of good visual explanations on the web (e.g. <a href="http://amsi.org.au/ESA_Senior_Years/SeniorTopic3/3j/3j_2content_2.html">this one</a>) so I won't go into it here.</p>

  <p>The clever line is this one:</p>

  <pre>i  = 0x5f3759df - ( i &gt;&gt; 1 );               // what the fuck?</pre>

  <p>The heart of the method is bit manipulation of floating point numbers, so we need to know how they work. But the method is simpler to explain using a weird number format that nobody uses.</p>

<h2>Real number systems</h2>

  <p>Suppose we have 32 bits with which to represent a <a href="https://www.britannica.com/science/real-number">real number</a> $x$.</p>

  <img src="https://timmmm.github.io/fast-inverse-square-root/diagrams_p1.svg">

  <p>We can use the upper bit for the sign (0 for positive numbers, 1 for negative numbers), and the remaining bits to represent a <em>positive</em> real number. Using a dedicated bit to represent sign is called <a href="https://en.wikipedia.org/wiki/Signed_number_representations#Signed_magnitude_representation">sign and magnitude</a>. Since we cannot take the square root of a negative number we can assume the sign bit is always 0.</p>

  <p>We can interpret the next 8 bits as the integral part of a value, and the remaining bits are the fractional part. In this case the value is <code>10001010.0100001</code> in binary (we can elide trailing zeros just like with decimal numbers). This corresponds to a decimal value of $138.2578125.$ We can use this value which I will denote $f$ to represent numbers in a few different ways. If we treat the 31-bit value as an unsigned integer $u$ (i.e. $u=\mathrm{0b01000101001000010000000000000000}=1159790592$) then</p>

  $$f = \frac{u}{2^{23}}$$

  <h3>Fixed point</h3>

  <p>We can just set $x=f$, i.e. we consider <code>01000101<wbr>00100001<wbr>00000000<wbr>00000000</code> to represent the number $138.2578125.$ This format is called <a href="https://en.wikipedia.org/wiki/Fixed-point_arithmetic">fixed point</a>, because the decimal point is always always fixed at the same place. It has some niche uses but isn't commonly used because of its major weakness: the range of values it can represent is very limited: only up to 128.9999999 in this example.</p>

  <h3>Logarithmic Number System</h3>

  <p>We can increase the range of values that we can represent by exponentiating the fixed point value, i.e. $f=2^{x-127}$. So, instead of considering <code>01000101<wbr>00100001<wbr>00000000<wbr>00000000</code> to represent the value $138.2578125$ we say it represents the number $2^{138.2578125 - 127} = 2448.7.$ This is called the <a href="https://en.wikipedia.org/wiki/Logarithmic_number_system">Logarithmic Number System</a> (LNS). The $^{-127}$ offset is used so we can represent numbers smaller than $1.$ Now we can represent numbers up to $2^{128.9999999}.$ We do lose some nice properties though, the most critical is that addition and subtraction are difficult to implement with this number system.</p>

  <h3>Floating point</h3>

  <p>To make addition and subtraction easier to implement we can treat the integral and fractional parts of $f$ differently, let's call them $f_e$ and $f_m$. We make the integral part logarithmic, and the fractional part linear using $x=2^{f_e - 127} \times (1 + f_m)$. In our example $2^{138 - 127} \times 1.2578125 = 2576.$ This means the exponent of $2$ is always integral, which computers can easily deal with. This format is called <a href="https://en.wikipedia.org/wiki/Floating-point_arithmetic">floating point</a> because the decimal point floats around (don't worry about it). Virtually all code uses this format for real numbers, and almost all processors have hardware support for it. This is also why real numbers in C are <code>float</code>.</p>

  <p>To summarise, the same bit pattern can mean three different numbers depending on the number format we are using:</p>

  <table>
    <thead>
      <tr>
        <th>Bit pattern</th>
        <th>Number format</th>
        <th>Value</th>
      </tr>
  </thead>
    <tbody>
      <tr>
        <td><code>01000101<wbr>00100001<wbr>00000000<wbr>00000000</code></td>
        <td>Fixed point</td>
        <td>$\textcolor{blue}{138}.\textcolor{red}{2578125}$</td>
      </tr>
      <tr>
        <td><code>01000101<wbr>00100001<wbr>00000000<wbr>00000000</code></td>
        <td>LNS</td>
        <td>$2^{\textcolor{blue}{138}.\textcolor{red}{2578125} - 127} = 2448.7$</td>
      </tr>
      <tr>
        <td><code>01000101<wbr>00100001<wbr>00000000<wbr>00000000</code></td>
        <td>Floating point</td>
        <td>$2^{\textcolor{blue}{138} - 127} \times 1.\textcolor{red}{2578125} = 2576$</td>
      </tr>
    </tbody>
  </table>

<h2>Calculating the inverse square root using LNS numbers</h2>

  <p>If we are using LNS numbers, we do not need a trick to calculate the inverse square root - we can easily calculate it exactly. Recall that the LNS interpretation of an unsigned integer $u$ is</p>

  $$2^{u/2^{23} - 127}$$

  <p>The inverse square root of a value $2^x$ is</p>

  $$(2^x)^{-1/2} = 2^{-x/2}$$

  <p>So to find the unsigned integer $q$ that would give the inverse square root, we need to solve</p>

  $$2^{q/2^{23} - 127} = 2^{-(u/2^{23} - 127)/2}$$

  <p>Simplifying this gives</p>

  $$q/2^{23} - 127 = -(u/2^{23} - 127)/2$$
  $$q/2^{23} - 127 = -(u/2^{23})/2 + 127/2$$
  $$q/2^{23} = 190.5 - (u/2^{23})/2$$
  $$q = 190.5 \times 2^{23} - u/2$$

  <p>In code form this is</p>

  <pre>q = 1598029824 - (u/2);</pre>

  <p>Or if I convert that constant to <a href="https://www.mathsisfun.com/hexadecimals.html">hex</a>, and use a bit shift to divide by 2</p>

  <pre>q = 0x5F400000 - (u &gt;&gt; 1);</pre>

  <p>Look familiar? It's pretty close to the line we saw before! "But", I imagine you ask, "this is for these weird LNS numbers! What about actual floating point numbers?"</p>

<h2>Using floating point numbers</h2>

  <p>The formula above calculates an <em>exact</em> inverse square root for LNS numbers. The trick is that floating point numbers are actually pretty similar to LNS numbers! They never differ by more than a factor of $\frac{2}{e\log 2} = 1.0615.$ The following graph shows the value of an LNS number when reinterpreted as a float. In other words the value we get when we take the bit pattern of an LNS number and work out what value those bits represent if they were a floating point number. As you can see they are pretty close!</p>

  <!-- Error is "plot (1+x) / (2^x) from 0 to 1" -->

  
  

  <p>Another way to look at the difference between LNS and floating point is to vary $f$ and plot the LNS and floating point results.</p>

  <p><label>
      
      Log scale
    </label>
  </p>

  
  

  <p>You can see how LNS is fully logarithmic between integer powers of 2, but floating point is linear between those points. All this is just to say that LNS and floating point are pretty similar, so the exact inverse square root of an LNS number is still pretty close to the inverse square root of a floating point number! But why does the code use <code>0x5f3759df</code> instead of <code>0x5F400000</code>?</p>

<h2>Minimising the error</h2>

  <p>We have a calculation that gives an exact result with LNS numbers, and an approximate result with floating point. How good is the approximation? We can plot the ratio of the approximate output to the correct output for different inputs. A log scale is used so that being out by a constant factor (e.g. $\times 2$ or $\div 2$) looks the same. The green line shows the result after the initial approximation, the blue line shows the result after one iteration of Newton-Raphson. Closer to 1 (the red line) is better.</p>

  

  <table>
    <colgroup>
      <col span="1">
      <col span="1">
      <col span="1">
    </colgroup>
    <tbody>
      <tr>
        <td>Subtractant:</td>
        <td>
          
        </td>
        <td>
          <span id="sub_value"></span> <code id="sub_value_hex"></code>
        </td>
      </tr>
    </tbody>
  </table>

  

  

  
  
  <span>
    
    
  </span>

  

  

  <p>Using a subtractant (a word I just made up) of $190.5$ (<code>0x5F400000</code>) you can see that the initial approximation always gives a value that is too high (the green line is above the red line). This is because floating point numbers are always greater than the corresponding LNS number. The estimate is somewhere between $1$ and $1.09$ times the actual answer. If we could multiply that output by a constant factor of around $0.96$ then we could balance the error above and below $1$, which would reduce the maximum error.</p>

  <p>This is easy to do just by reducing the subtractant by a small amount. I could go into detail, but you can easily see the effect by sliding the slider above. See how small you can make the maximum error, and then click the "Carmack" button to see the value Quake 3 uses.</p>

  <p>It's pretty close to optimal! But... maybe there's a way we can do better. Notice that the output after one iteration of Newton-Raphson is still always too small (the blue line is below the red line). Is there a way we could multiply that by some constant factor slightly more than $1$? Yes! All we have to do is modify the <code>0.5</code> in the original code a little.

  

  </p><p>Try playing around with the "One half" slider. In fact, while we're at it, what happens if we change the <code>1.5</code> in the code too and feed the whole thing into a hacked together <a href="https://timmmm.github.io/fast-inverse-square-root/solver.rs">optimisation program that I wrote</a>? We can do even slightly better! Click the extra buttons to see the optimisation results when two or three parameters are allowed to change.</p>

  <h2>Conclusion</h2>

  <p>That's basically it. To summarise, using LNS numbers we can calculate an exact solution using <code>0x5F400000</code>. LNS numbers are pretty close to floating point numbers, so the same code gives an approximate solution for floating point. But floating point numbers are always greater than LNS numbers so there is a bias in the approximation that can be corrected by subtracting a small amount from <code>0x5F400000</code>.</p>

  <p>Let me know if anything wasn't clear or I made a mistake. Read on for the answers to a few extra questions I'm pretending you have asked.</p>

  <h3>Can you calculate the error analytically and solve for an exact optimum?</h3>

  <p>Yes! I started doing this, but because there are 3 or 4 parts to …</p></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://timmmm.github.io/fast-inverse-square-root/">https://timmmm.github.io/fast-inverse-square-root/</a></em></p>]]>
            </description>
            <link>https://timmmm.github.io/fast-inverse-square-root/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24959157</guid>
            <pubDate>Sun, 01 Nov 2020 12:58:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Adventures in hardware, part 8 – PS2 Keyboard Typewriter]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24958794">thread link</a>) | @axarydax
<br/>
November 1, 2020 | http://jborza.com/hardware/2020/10/31/hardware-adventures-8-ps2-typewriter.html | <a href="https://web.archive.org/web/*/http://jborza.com/hardware/2020/10/31/hardware-adventures-8-ps2-typewriter.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    

<div><div><pre><code>What will we do with a drunken keyboard?
What will we do with a drunken keyboard?
What will we do with a drunken keyboard?
Early in the morning!
</code></pre></div></div>
<p><em>A sea shanty from early 19th century</em></p>

<p>After I implemented a display of memory-mapped display in the 
<a href="http://jborza.com/hardware/2020/10/25/hardware-adventures-7-lcd-controller.html">Adventures in hardware, part 6 - Stopwatch memory mapped LCD controller</a> I wanted to play with the LCD display a bit more. I also got a PS2 keyboard, and it felt that these two go together.</p>

<p>So join me in attempting something that might be impractical in actual engineering scenarios: an electronic typewriter, where the words you type are shown on the screen.</p>

<p><img src="http://jborza.com/assets/hw8-typewriter-700.jpg" alt="preview"></p>

<p><em>Preview of the final project</em></p>

<h2 id="handling-the-ps2-protocol">Handling the PS2 protocol</h2>

<p>The transmitting device (keyboard or mouse) triggers the clock at range 10-16.7 kHz, then these bits come over the data pin, on the each falling edge of the PS2 clock:</p>
<ul>
  <li>1 start bit</li>
  <li>8 data bits</li>
  <li>1 parity bit</li>
  <li>1 stop bit</li>
</ul>

<p>This <a href="https://students.iitk.ac.in/eclub/assets/tutorials/keyboard.pdf">assignment</a> describes a simple approach to collecting the data, triggered by the <code>@negedge clk</code> and just collecting 8 data bits and ignoring others in an 11-state state machine.</p>

<p><img src="http://jborza.com/assets/hw8-scope-ps2.jpg" alt="oscilloscope with PS2"></p>

<p><em>PS2 clock pin waveform captured on an oscilloscope</em></p>

<p>It was a bit of a nuisance to try to capture the PS2 clock on oscilloscope. Again, reading the specs more correctly could have left me less surprised, as it explicitly states that the clock signal is generated only when there is some data coming down the wire.</p>

<h3 id="ps2-receiver">PS2 receiver</h3>

<p>If we want to play nice with the rest of the system, we should encapsulate the protocol into a PS2 receiver module with the following interface (as per this <a href="http://www.zaphinath.com/ps2-receiver-module-for-vhdl/">article</a>):</p>

<div><div><pre><code><span>module</span> <span>ps2_rx</span> <span>(</span>
	<span>input</span> <span>wire</span> <span>clk</span><span>,</span> <span>reset</span><span>,</span>
	<span>input</span> <span>wire</span> <span>ps2d</span><span>,</span> <span>//PS2 data
</span>	<span>input</span> <span>wire</span> <span>ps2c</span><span>,</span> <span>//PS2 clock
</span>	<span>input</span> <span>wire</span> <span>rx_en</span><span>,</span> <span>//receiver enable
</span>	<span>output</span> <span>reg</span> <span>rx_done_tick</span><span>,</span> <span>//signal that data is available
</span>	<span>output</span> <span>wire</span> <span>[</span><span>7</span><span>:</span><span>0</span><span>]</span> <span>dout</span> <span>//PS2 scancode
</span><span>);</span>
</code></pre></div></div>

<blockquote>
  <p>Note that I didn’t come up with the PS2 receiver code on my own, but looking at various implementations online showed me different approaches to handling the input and the states.</p>
</blockquote>

<p>We can then connect this module to the rest of the system by using the <code>rx_done_tick</code> signal. In this case I directly move the received scancode through a converter into ASCII code  (by the following <a href="https://github.com/jborza/fpga-ps2-typewriter/blob/master/key2ascii.v">Verilog module</a>) into the display RAM, which is later shown on screen.</p>

<p>This is what happened when I typed “hello” on the keyboard:</p>

<p><img src="http://jborza.com/assets/hw8-hello.jpg" alt="first attempt"></p>

<p><em>The * symbol stands for unknown scancode</em></p>

<h3 id="ps2-make-and-break-codes">PS2 Make and break codes</h3>

<p>Hm, not really what I expected. Looking at PS2 protocol again, it operates with a concept of make (<em>press</em>) and break (<em>release</em>) codes, so a scancode for <code>H</code> (<code>0x48</code>) is sent first when it’s pushed, then the “break code” <code>0xF0</code> is sent when it’s released, followed. by the scancode of <code>H</code> (<code>0x48</code>) again.</p>

<blockquote>
  <p>This is a bit more complicated for special keys, such as alt, arrow keys, home, end, etc which have multi-word make codes.</p>
</blockquote>

<p>In case the <code>H</code> key is held, it generates code sequence <code>48 48 48 48 ... F0 48</code>.</p>

<p>Of course, when you hold <code>A</code> and press <code>B</code>, then release <code>B</code> and then release <code>A</code>, you’ll get a sequence of make codes for A, B, break code for B and break code for A.</p>

<table>
  <thead>
    <tr>
      <th>Key</th>
      <th>Make</th>
      <th>Break</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>A</td>
      <td>1C</td>
      <td>F0,1C</td>
    </tr>
    <tr>
      <td>B</td>
      <td>32</td>
      <td>F0,32</td>
    </tr>
    <tr>
      <td>Backspace</td>
      <td>66</td>
      <td>F0,66</td>
    </tr>
    <tr>
      <td>L Shift</td>
      <td>12</td>
      <td>F0,12</td>
    </tr>
    <tr>
      <td>Enter</td>
      <td>5A</td>
      <td>F0,5A</td>
    </tr>
    <tr>
      <td>Left ←</td>
      <td>E0,6B</td>
      <td>E0,F0,6B</td>
    </tr>
    <tr>
      <td>Numpad 4</td>
      <td>6B</td>
      <td>F0,6B</td>
    </tr>
    <tr>
      <td>Home</td>
      <td>E0,6C</td>
      <td>E0,F0,6C</td>
    </tr>
    <tr>
      <td>Numpad 7</td>
      <td>6C</td>
      <td>F0,6C</td>
    </tr>
    <tr>
      <td>Pause</td>
      <td>E1,1D,45,E1,9D,C5</td>
      <td>-None -</td>
    </tr>
  </tbody>
</table>

<p>In this simple typewriter case, let’s pretend that we’re not interested in the keyboard key released event.</p>

<p>It also so happens that the last word of multi-word make codes usually correspond to an alternate version of the key on the regular keyboard. So the left arrow ← has the same code <code>6B</code> as Keypad 4 <code>E0,6B</code>, which on my keyboard has a left arrow pictured on it.</p>

<p>It means that we probably can get away with interpreting just the last word of the make code and get the meaning of the most right, if we pretend the numeric part of the keyboard doesn’t exist.</p>

<h2 id="improvements">Improvements</h2>
<h3 id="handling-only-the-key-press-not-release">Handling only the key press, not release</h3>

<p>We should upgrade our keyboard driver by adding another module around it, that handles keypresses. It has a new output: makeBreak, which will output 1 for make and 0 for break code. Then in the top module we can handle these situation separately - by ignoring the scancode with the break code flag completely.</p>

<p>E0 seems to be followed by a single scan code, E1 seems to be followed by two scan codes.</p>

<blockquote>
  <p>There are some weird keys with long scancode sequence: Pause: <code>E1 1D 45 E1 9D C5</code>, Print screen: <code>e0 2a e0 37</code>. I’ll pretend these don’t exist.</p>
</blockquote>

<p>I named the new module <code>ps2_keypress_driver</code>, it actually ignores the events if they come in with the break codes.</p>

<p>It also advances the address of the byte being written into the memory, so now as we type <code>HELLO</code> it stores the corresponding ASCII codes <code>48 45 4C 4C 4F</code>, and they get displayed.</p>

<div><div><pre><code><span>//write current ascii code into $current_address and advance the address by 1
</span><span>write_address</span> <span>&lt;=</span> <span>current_address</span><span>;</span>				
<span>ram_in</span> <span>&lt;=</span> <span>ascii_scan_data</span><span>;</span>	
<span>current_address</span> <span>&lt;=</span> <span>current_address</span> <span>+</span> <span>1</span><span>;</span>
<span>we</span> <span>&lt;=</span> <span>1'b1</span><span>;</span>
</code></pre></div></div>
<h3 id="implementing-backspace">Implementing backspace</h3>

<p>Let’s say we also want to delete something we already typed on backspace key press.</p>

<p>The implementation ended up hacky - moving the “cursor” back and overwriting the memory with a blank space:</p>

<div><div><pre><code><span>if</span><span>(</span><span>ascii_scan_data</span> <span>==</span> <span>BACKSPACE</span><span>)</span> <span>begin</span>
    <span>write_address</span> <span>&lt;=</span> <span>current_address</span> <span>-</span> <span>1</span><span>;</span>
    <span>ram_in</span> <span>&lt;=</span> <span>SPACE</span><span>;</span>
    <span>current_address</span> <span>&lt;=</span> <span>current_address</span> <span>-</span> <span>1</span><span>;</span>
<span>end</span> 
</code></pre></div></div>

<h3 id="implementing-newline-enter">Implementing newline (enter)</h3>

<p>To make the virtual cursor move to a new line after pressing the <code>Enter</code> key, we basically want to advance the memory pointer to the nearest multiple of 16.</p>

<p>We can do that by dividing by 16, incrementing by one and multiplying back by 16.</p>

<div><div><pre><code><span>current_address</span> <span>&lt;=</span> <span>(((</span><span>current_address</span> <span>&gt;&gt;</span> <span>4</span><span>)</span> <span>+</span> <span>1</span><span>)</span> <span>&lt;&lt;</span> <span>4</span><span>);</span>
</code></pre></div></div>

<p>Note: I know that having multiple <code>if</code> statements to handle special key logic feels strange and kludgy, I feel it too and know that there must be a better way to structure the logic.</p>

<h2 id="the-complete-product-demonstration">The complete product, demonstration</h2>

<p><img src="http://jborza.com/assets/hw8-keyboard.gif" alt="The finished project"></p>

<h2 id="what-did-i-learn">What did I learn</h2>

<h3 id="active-high-vs-active-low">Active-high vs Active-low</h3>

<p>Check if your reset button is active-low or active-high logic (https://en.wikipedia.org/wiki/Logic_level#Active_state).
I have spent an hour looking at code, debugging various signals only to realize my reset button sends logical one if not pressed.</p>

<h3 id="two-vs-one-process-state-machines">Two vs one process state machines</h3>

<p>I learned about “two process” design method (as per https://www.gaisler.com/doc/vhdl2proc.pdf) - one sequential, one combinatorial vs “one process”, which does everything in … a single process. The <a href="https://forums.xilinx.com/t5/Synthesis/Differences-between-one-process-and-two-process-FSM/td-p/214607">opinions online differ</a>. Apparently the two process one makes sense if you come from electrical engineering background and appears more often in textbooks and articles.</p>

<p>I found the latter easier to debug, write and especially reason about what’s going to happen after every cycle / state change and eventually converted the two process starting sample code to a single process one.</p>

<h3 id="connectivity-warning--stuck-output">Connectivity warning / stuck output</h3>

<p>I again ran into a problem of the synthesizer optimized most of my logic away. While looking for the root cause, this warning helped:</p>

<blockquote>
  <p>Warning (12241): 1 hierarchies have connectivity warnings - see the Connectivity Checks report folder</p>
</blockquote>

<p>Connectivity checks was useful - it told me which port I missed to wire when connecting a module to the top module and it was stuck on GND.</p>

<h3 id="read-the-specs-in-more-detail">Read the specs in more detail</h3>

<p>I’m still missing out on the protocol / datasheet nuances and realize some important bits later. Compared to software, I find debugging hardware much harder and time consuming, so it’s best to avoid or catch the bugs early.</p>

<h3 id="making-smaller-gifs-with-ffmpeg">Making smaller gifs with ffmpeg</h3>

<p>I cut the gif palette size with <code>palettegen=max_colors=64</code> to improve the compression a bit.</p>

<div><div><pre><code>ffmpeg -ss 00:03 -t 00:03.7 -i input.mp4 -an -vf "transpose=2, crop=in_w-200:in_h-50:0:0, fps=11, scale=-1:360:flags=lanczos, split[s0][s1];[s0]palettegen=max_colors=64[p];[s1][p]paletteuse" -loop 1 output.gif
</code></pre></div></div>

<h2 id="whats-next">What’s next?</h2>

<p>An easy thing to extend the project would be to handle the <code>Shift</code> key and generate lowercase ASCII codes as well. Also, one could either follow up on the keyboard interface and do something with the text in the buffer when a certain key (<code>Enter</code>) gets pressed - for example send it over a serial port or control various peripherals (LEDs) based on control keys.</p>

<p>Or, as an exercise, add more bells and whistles, such as typing on the bottom line scrolling the entire buffer up on a keypress, moving the cursor with arrow keys, playing back the buffer in morse code over a speaker - realistically, I’m probably not going to do any of those.</p>

<p>What I’d like to do is to store the ASCII code of the last key pressed into a special buffer, memory mapped to a certain memory address, so the future software running on the board can pick it up, such as in the neat <a href="http://www.6502asm.com/">6502asm emulator</a>.</p>

<h2 id="the-code">The code</h2>

<p>Hosted with ❤️ on GitHub: https://github.com/jborza/fpga-ps2-typewriter</p>


  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>http://jborza.com/hardware/2020/10/31/hardware-adventures-8-ps2-typewriter.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24958794</guid>
            <pubDate>Sun, 01 Nov 2020 11:36:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Future of RLSL]]>
            </title>
            <description>
<![CDATA[
Score 66 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24958762">thread link</a>) | @ibobev
<br/>
November 1, 2020 | http://maikklein.github.io/rlsl-update3/ | <a href="https://web.archive.org/web/*/http://maikklein.github.io/rlsl-update3/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            
<div>
  
  <p><span>2020-10-22</span></p><ul>
  
      <li>
          <a href="#history">History</a>
          
      </li>
  
      <li>
          <a href="#rlsl-is-dead-long-live-rust-gpu">RLSL is dead, long live rust-gpu</a>
          
      </li>
  
  </ul>
  
<p>3 years ago I started a project called "Rust Like Shading Language" to bring Rust to the GPU. I was very unhappy with the current state of affairs of glsl and hlsl, and I imagined what shader programming would be like if we had proper tooling like formatting, compiler errors, proper type system, package manager, auto completion etc. After Vulkan introduced SPIR-V, I was anticipating the appearance of many new shading languages, but nothing really did and I decided to take the matters in my own hand.</p>
<p>Knowing nothing about compilers, I wrote a simple parser and type checker for a very simple language, and I could generate a triangle <img src="https://camo.githubusercontent.com/fcf8368d94842046bcb1856ecd69386109ce672f/687474703a2f2f692e696d6775722e636f6d2f50515a634c36772e6a7067" alt="triangle"> But I realized that writing your own compiler is quite the undertaking and I was thinking about using rustc instead.</p>
<p>After a few days of head scratching I could render a triangle again going from <code>Rust -&gt; HIR (High level IR) -&gt; SPIR-V</code>. While translating HIR to SPIR-V was simple, it was also time consuming. I decided to try out MIR (Mid level IR). After a few weeks of head scratching I finally could render the triangle again, and a few days later I could also generate the default shader from shadertoy.</p>

<p>I expected to run into major blocking issues. While there were certainly some challenges, I always found a way to hack around them. And I just kept on adding features constantly:</p>
<ul>
<li>Support for cargo and crates.io</li>
<li>Simple math library</li>
<li>Closures</li>
<li>Complex branches (especially with <code>Try/?</code>)</li>
<li>Simple loops</li>
<li>Enums (Option<t> in shader)</t></li>
<li>Computer shaders</li>
<li>Full GPU/CPU testing</li>
<li>Render fragement shaders on the CPU and GPU with the same code</li>
</ul>

<p>By that time I was working on slightly more complex shaders like raymarching, and I ran into a small slump. The ecosystem was quite immature at that time and debugging incorrect codegen was time consuming. I really needed a way to automate the debugging process.</p>
<p>I was also a University student, and I spent way too much time working on RLSL than I should have. I had a really hard time justifying my work on RLSL compared to focusing on my studies.</p>
<p>A few months later, I saw a post from <a href="https://www.embark-studios.com/">Embark Studios</a> about Rust. Long story short, I applied, got the Job, moved from Germany to Sweden (Stockholm). Everyone at work was really interested in bringing Rust to the GPU as well. It just wasn't the right time to work on it professionally.</p>
<p>I also struggled to have a good work life balance while working on RLSL and <a href="https://github.com/MaikKlein/ash">ash</a> in my spare time.</p>

<p>Fast forwarding 1.5 years, we now have <a href="https://github.com/EmbarkStudios/rust-gpu">rust-gpu</a>. See the <a href="https://github.com/EmbarkStudios/rust-gpu/releases/tag/v0.1">announcement</a> for more details. <code>rust-gpu</code> will replace <code>RLSL</code>. <code>rust-gpu</code> is written by some extremely talented colleagues of mine, and I couldn't be happier with the work they have done. I am collaborating with the team although I am not actively committing code right now. I am extremely excited to be writing shaders in Rust soon and I hope you are as well.</p>
<p>I want to thank you all for the support you have given me over the years. I can't tell you how much all of the kind comments helped me going though some of my harder times. Thank you ❤️.</p>

</div>

            </div></div>]]>
            </description>
            <link>http://maikklein.github.io/rlsl-update3/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24958762</guid>
            <pubDate>Sun, 01 Nov 2020 11:29:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[7GUIs]]>
            </title>
            <description>
<![CDATA[
Score 497 | Comments 56 (<a href="https://news.ycombinator.com/item?id=24958725">thread link</a>) | @dsego
<br/>
November 1, 2020 | https://eugenkiss.github.io/7guis/tasks/ | <a href="https://web.archive.org/web/*/https://eugenkiss.github.io/7guis/tasks/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>The tasks were selected by the following criteria. The task set should be as small as possible yet reflect as many typical (or fundamental or representative) challenges in GUI programming as possible. Each task should be as simple and self-contained as possible yet not too artificial. Preferably, a task should be based on existing examples as that gives the task more justification to be useful and there already will be at least one reference implementation.</p><p>Below, a description of each task highlighted with the challenges it reflects and a screenshot of the resulting GUI application in Java/Swing is given.</p><p>For a live version of the tasks see my<!-- --> <a target="_blank" href="https://eugenkiss.github.io/7guis-React-TypeScript-MobX/">React/MobX</a> <!-- -->implementation.</p><h2>Counter</h2><p><em>Challenge:</em> Understanding the basic ideas of a language/toolkit.</p><p><img src="https://eugenkiss.github.io/7guis/static/counter.9cd92091.png"></p><p>The task is to build a frame containing a label or read-only textfield<!-- --> <em>T</em> and a button <em>B</em>. Initially, the value in <em>T</em> is “0” and each click of <em>B</em> increases the value in <em>T</em> by one.</p><p>Counter serves as a gentle introduction to the basics of the language, paradigm and toolkit for one of the simplest GUI applications imaginable. Thus, Counter reveals the required scaffolding and how the very basic features work together to build a GUI application. A good solution will have almost no scaffolding.</p><h2>Temperature Converter</h2><p><em>Challenges:</em> bidirectional data flow, user-provided text input.</p><p><img src="https://eugenkiss.github.io/7guis/static/tempconv.de9aff1f.png"></p><p>The task is to build a frame containing two textfields <em>T<sub>C</sub></em> <!-- -->and <em>T<sub>F</sub></em> representing the temperature in Celsius and Fahrenheit, respectively. Initially, both <em>T<sub>C</sub></em> and <em>T<sub>F</sub></em> are empty. When the user enters a numerical value into <em>T<sub>C</sub></em> the corresponding value in <em>T<sub>F</sub></em> is automatically updated and vice versa. When the user enters a non-numerical string into <em>T<sub>C</sub></em> the value in <em>T<sub>F</sub></em> is <em>not</em> updated and vice versa. The formula for converting a temperature <em>C</em> in Celsius into a temperature <em>F</em> in Fahrenheit is <em>C = (F - 32) * (5/9)</em> and the dual direction is <em>F = C * (9/5) + 32</em>.</p><p>Temperature Converter increases the complexity of Counter by having bidirectional data flow between the Celsius and Fahrenheit inputs and the need to check the user input for validity. A good solution will make the bidirectional dependency very clear with minimal boilerplate code.</p><p>Temperature Converter is inspired by the<!-- --> <a target="_blank" href="https://www.artima.com/pins1ed/gui-programming.html#32.4">Celsius/Fahrenheit converter</a> <!-- -->from the book <em>Programming in Scala</em>. It is such a widespread example—sometimes also in the form of a currency converter—that one could give a thousand references. The same is true for the Counter task.</p><h2>Flight Booker</h2><p><em>Challenge:</em> Constraints.</p><p><img src="https://eugenkiss.github.io/7guis/static/bookflight.a5434663.png"></p><p>The task is to build a frame containing a combobox <em>C</em> with the two options “one-way flight” and “return flight”, two textfields <em>T<sub>1</sub></em> and<!-- --> <em>T<sub>2</sub></em> representing the start and return date, respectively, and a button <em>B</em> for submitting the selected flight. <em>T<sub>2</sub></em> is enabled iff <em>C</em>’s value is “return flight”. When <em>C</em> has the value “return flight” and <em>T<sub>2</sub></em>’s date is strictly before <em>T<sub>1</sub></em>’s then <em>B</em> is disabled. When a non-disabled textfield <em>T</em> has an ill-formatted date then <em>T</em> is colored red and <em>B</em> is disabled. When clicking <em>B</em> a message is displayed informing the user of his selection (e.g. “You have booked a one-way flight on 04.04.2014.”). Initially, <em>C</em> has the value “one-way flight” and <em>T<sub>1</sub></em> as well as <em>T<sub>2</sub></em> have the same (arbitrary) date (it is implied that <em>T<sub>2</sub></em> is disabled).</p><p>The focus of Flight Booker lies on modelling constraints between widgets on the one hand and modelling constraints within a widget on the other hand. Such constraints are very common in everyday interactions with GUI applications. A good solution for Flight Booker will make the constraints clear, succinct and explicit in the source code and not hidden behind a lot of scaffolding.</p><p>Flight Booker is directly inspired by the<!-- --> <a target="_blank" href="http://blog.reactiveprogramming.org/?p=21">Flight Booking Java example in Sodium</a> <!-- -->with the simplification of using textfields for date input instead of specialized date picking widgets as the focus of Flight Booker is not on specialized/custom widgets.</p><h2>Timer</h2><p><em>Challenges:</em> concurrency, competing user/signal interactions, responsiveness.</p><p><img src="https://eugenkiss.github.io/7guis/static/timer.ed46b6b4.png"></p><p>The task is to build a frame containing a gauge <em>G</em> for the elapsed time <em>e</em>, a label which shows the elapsed time as a numerical value, a slider <em>S</em> by which the duration <em>d</em> of the timer can be adjusted while the timer is running and a reset button <em>R</em>. Adjusting<!-- --> <em>S</em> must immediately reflect on <em>d</em> and not only when<!-- --> <em>S</em> is released. It follows that while moving <em>S</em> the filled amount of <em>G</em> will (usually) change immediately. When <em>e ≥ d</em> is true then the timer stops (and <em>G</em> will be full). If, thereafter, <em>d</em> is increased such that <em>d &gt; e</em> will be true then the timer restarts to tick until <em>e ≥ d</em> is true again. Clicking <em>R</em> will reset <em>e</em> to zero.</p><p>Timer deals with concurrency in the sense that a timer process that updates the elapsed time runs concurrently to the user’s interactions with the GUI application. This also means that the solution to competing user and signal interactions is tested. The fact that slider adjustments must be reflected immediately moreover tests the responsiveness of the solution. A good solution will make it clear that the signal is a timer tick and, as always, has not much scaffolding.</p><p>Timer is directly inspired by the timer example in the paper<!-- --> <a target="_blank" href="http://cs.brown.edu/~sk/Publications/Papers/Published/ick-adapt-oo-fwk-frp/paper.pdf">Crossing State Lines: Adapting Object-Oriented Frameworks to Functional Reactive Languages</a>.</p><h2>CRUD</h2><p><em>Challenges:</em> separating the domain and presentation logic, managing mutation, building a non-trivial layout.</p><p><img src="https://eugenkiss.github.io/7guis/static/crud.515ce94b.png"></p><p>The task is to build a frame containing the following elements: a textfield<!-- --> <em>T<sub>prefix</sub></em>, a pair of textfields <em>T<sub>name</sub></em> and<!-- --> <em>T<sub>surname</sub></em>, a listbox <em>L</em>, buttons <em>B<sub>C</sub></em>,<!-- --> <em>B<sub>U</sub></em> and <em>B<sub>D</sub></em> and the three labels as seen in the screenshot. <em>L</em> presents a view of the data in the database that consists of a list of names. At most one entry can be selected in <em>L</em> at a time. By entering a string into <em>T<sub>prefix</sub></em> the user can filter the names whose surname start with the entered prefix—this should happen immediately without having to submit the prefix with enter. Clicking <em>B<sub>C</sub></em> <!-- -->will append the resulting name from concatenating the strings in<!-- --> <em>T<sub>name</sub></em> and <em>T<sub>surname</sub></em> to <em>L</em>.<!-- --> <em>B<sub>U</sub></em> and <em>B<sub>D</sub></em> are enabled iff an entry in <em>L</em> is selected. In contrast to <em>B<sub>C</sub></em>, <em>B<sub>U</sub></em> <!-- -->will not append the resulting name but instead replace the selected entry with the new name. <em>B<sub>D</sub></em> will remove the selected entry. The layout is to be done like suggested in the screenshot. In particular, <em>L</em> must occupy all the remaining space.</p><p>CRUD (Create, Read, Update and Delete) represents a typical graphical business application. The primary challenge is the separation of domain and presentation logic in the source code that is more or less forced on the implementer due to the ability to filter the view by a prefix. Traditionally, some form of MVC pattern is used to achieve the separation of domain and presentation logic. Also, the approach to managing the mutation of the list of names is tested. A good solution will have a good separation between the domain and presentation logic without much overhead (e.g. in the form of toolkit specific concepts or language/paradigm concepts), a mutation management that is fast but not error-prone and a natural representation of the layout (layout builders are allowed, of course, but would increase the overhead).</p><p>CRUD is directly inspired by the crud example in the blog post<!-- --> <a target="_blank" href="http://apfelmus.nfshost.com/blog/2012/03/29-frp-three-principles-bidirectional-gui.html">FRP - Three principles for GUI elements with bidirectional data flow</a>.</p><h2>Circle Drawer</h2><p><em>Challenges:</em> undo/redo, custom drawing, dialog control*.</p><p><img src="https://eugenkiss.github.io/7guis/static/circledraw.235dfd8b.png"></p><p>The task is to build a frame containing an undo and redo button as well as a canvas area underneath. Left-clicking inside an empty area inside the canvas will create an unfilled circle with a fixed diameter whose center is the left-clicked point. The circle nearest to the mouse pointer such that the distance from its center to the pointer is less than its radius, if it exists, is filled with the color gray. The gray circle is the selected circle <em>C</em>. Right-clicking <em>C</em> will make a popup menu appear with one entry “Adjust diameter..”. Clicking on this entry will open another frame with a slider inside that adjusts the diameter of <em>C</em>. Changes are applied immediately. Closing this frame will mark the last diameter as significant for the undo/redo history. Clicking undo will undo the last significant change (i.e. circle creation or diameter adjustment). Clicking redo will reapply the last undoed change unless new changes were made by the user in the meantime.</p><p>Circle Drawer’s goal is, among other things, to test how good the common challenge of implementing an undo/redo functionality for a GUI application can be solved. In an ideal solution the undo/redo functionality comes for free resp. just comes out as a natural consequence of the language / toolkit / paradigm. Moreover, Circle Drawer tests how dialog control*, i.e. keeping the relevant context between several successive GUI interaction steps, is achieved in the source code. Last but not least, the ease of custom drawing is tested.</p><p><small>* Dialog control is explained in more detail in the paper<!-- --> <a target="_blank" href="http://ceur-ws.org/Vol-610/paper11.pdf">Developing GUI Applications: Architectural Patterns Revisited</a> <!-- -->starting on page seven. The term describes the challenge of retaining context between successive GUI operations.</small></p><h2>Cells</h2><p><em>Challenges:</em> change propagation, widget customization, implementing a more authentic/involved GUI application.</p><p><img src="https://eugenkiss.github.io/7guis/static/cells.9544a72f.png"></p><p>The task is to create a simple but usable spreadsheet application. The spreadsheet should be scrollable. The rows should be numbered from 0 to 99 and the columns from A to Z. Double-clicking a cell <em>C</em> lets the user change <em>C</em>’s formula. After having finished editing the formula is parsed and evaluated and its updated value is shown in <em>C</em>. In addition, all cells which depend on <em>C</em> must be reevaluated. This process repeats until there are no more changes in the values of any cell (change propagation). Note that one should not just recompute the value of every cell but only of those cells that depend on another cell’s changed value. If there is an already provided spreadsheet widget it should not be used. Instead, another similar widget (like JTable in Swing) should be customized to become a reusable spreadsheet widget.</p><p>Cells is a more authentic and involved task that tests if a particular …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://eugenkiss.github.io/7guis/tasks/">https://eugenkiss.github.io/7guis/tasks/</a></em></p>]]>
            </description>
            <link>https://eugenkiss.github.io/7guis/tasks/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24958725</guid>
            <pubDate>Sun, 01 Nov 2020 11:20:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Selling Data to Hedge Funds (2017)]]>
            </title>
            <description>
<![CDATA[
Score 133 | Comments 55 (<a href="https://news.ycombinator.com/item?id=24958215">thread link</a>) | @r_singh
<br/>
November 1, 2020 | https://alternativedata.org/the-ultimate-guide-to-selling-data-to-hedge-funds/ | <a href="https://web.archive.org/web/*/https://alternativedata.org/the-ultimate-guide-to-selling-data-to-hedge-funds/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <h2><b>How to sell data to hedge funds</b></h2>

<p><img src="https://alternativedata.org/wp-content/uploads/2017/08/Crossing-the-Chasm-Selling-Data-to-Investors-01.png" alt="" width="5705" height="4298"></p><h3><b>Step 1: Know your audience</b></h3>
<p><span>Hedge funds are not all the same. Below are the main types.</span></p><h5>Quantitative Investing (a.k.a. Quant, Systematic, Algorithmic)</h5>
<p><span>Quant funds utilize automated trading strategies based on algorithms and data. They are not looking to be right on every trade. They just want to be right more than they’re wrong and trade a lot of securities.</span>

<span>They typically purchase data that</span></p><ul>
 	<li><span>Apply to 100s or 1,000s of securities</span></li>
</ul>
<ul>
 	<li><span>Has a long history of data they can backtest</span></li>
</ul>
<ul>
 	<li><span>Is published frequently. 5 years of historical data, published quarterly provides a weaker backtest than 5 years of historical data published daily</span></li>
</ul>
<p><span>The best part about selling to quant funds is that their business model is based on data, so they employ professionals to speak to data vendors, understand their data and make compelling proposals when valuable. </span>

<span>The difficult part about selling to quant funds is if your dataset does not meet the above attributes, you’re probably not going to sell them anything. But you’ll learn that pretty fast. </span>

<span>More Info:&nbsp;</span><a href="http://www.streetofwalls.com/finance-training-courses/quantitative-hedge-fund-training/quant-firms/"><span>Largest Quantitative Hedge Funds</span></a></p><h5>Fundamental Investing (a.k.a. Discretionary, Stock-Picking)</h5>
<h6><i><span>Platform Funds (a.k.a. Pod, Multi-Manager, Multi-Strategy)</span></i></h6>
<p><span>Platform Funds consist of many individual teams of PMs &amp; Analysts who share centralized resources such as assets under management, trading &amp; execution, compliance, data, office space, training and more. </span>

<span>Platform funds with discretionary strategies typically purchase data that</span></p><ul>
 	<li><span>Is highly correlated with a company KPI (“Key Performance Indicator”) or key investor question of specific securities</span></li>
</ul>
<ul>
 	<li><span>Has a history of data they can backtest</span></li>
</ul>
<ul>
 	<li><span>Is unique</span></li>
</ul>
<p><span>Since platform funds provide infrastructure shared among their investing teams (or “pods”), they often employ data sourcing professionals similar to Quant funds. These are also quick and pleasant conversations to have that require minimal sales infrastructure.</span>

<span>More info:&nbsp;</span><a href="https://www.wallstreetoasis.com/forums/what-ive-learned-about-hedge-fund-structure-and-compensation"><span>Multi-Manager Funds</span></a></p><h6><i><span>Long/Short Equity Hedge Funds</span></i></h6>
<p><span>Long/Short Equity Hedge funds pick stocks. They tend to purchase similar data to Platform Funds</span>

<span>The Long/Short Equity Hedge Funds who spend the most on data:</span></p><ul>
 	<li><span>Have a large amount of Assets Under Management (AUM)</span></li>
</ul>
<ul>
 	<li><span>Trade frequently. The more often they trade, the more data they want to look at. A rough proxy for this is “Turnover %” on a 13F database such as </span><a href="https://whalewisdom.com/"><span>Whale Wisdom</span></a></li>
</ul>
<ul>
 	<li><span>Make concentrated bets. The larger their positions, the more they can spend. A proxy for this is “% of Portfolio” on Whale Wisdom</span></li>
</ul>
<p><span>More Info: </span><a href="http://www.institutionalinvestorsalpha.com/profile/3287866/4689/hedge-fund-100-firm-profiles.html"><span>Institutional Investors Top 100 Hedge Funds</span></a><span> (also includes Quant and Platform funds)

</span></p><h6><i><span>Event Driven Funds</span></i></h6>
<p><span>These firms invest based on specific catalysts such as a merger, acquisition, bankruptcy, spinoff or legislation. If your dataset allows investors unique insights into key events, they may be a good match for Event Driven Funds.</span></p><h5><strong>Other Types of Funds</strong></h5>
<p><span>The above designations are neither mutually exclusive nor collectively exhaustive. Many funds are combinations of the above or something else entirely, including:</span></p><ul>
 	<li><span>Long Only/Mutual Funds: much longer term oriented investors with a different business model</span></li>
 	<li>Macro Funds: These firms invest across broader trends that affect a lot of stocks. If your data speaks to broader trends like inflation, currencies, weather, interest rates or global events they may be a good match for Macro Funds.</li>
</ul>
<ul>
 	<li><span>Credit Funds: invest in debt</span></li>
</ul>
<ul>
 	<li><span>Family Offices: manage funds of an individual family or group of families</span></li>
</ul>
<ul>
 	<li><span>Fund of Funds: invests in other funds</span></li>
</ul>
<ul>
 	<li><span>Sovereign Wealth Funds / Pension Funds: manages money of countries, endowments</span></li>
</ul>
<ul>
 	<li><span>Private Equity/Venture Capital: make large investments in mainly private companies</span></li>
</ul>
<h3><b>Step 2: Understand key use cases for your data&nbsp;</b></h3>
<p><span>For background on why hedge funds value alternative data, see </span><a href="http://mattturck.com/the-new-gold-rush-wall-street-wants-your-data/"><span>Matt Turck’s excellent piece on the subject</span></a><span>. High level, it helps to divide the institutional investor market into quantitative funds and discretionary funds, as the two have very different requirements and use cases for data.</span>

<span>To attract quant funds, your data should speak to a lot of companies and have a long time series. A good example is a panel of consumer transactions touching many public companies, that has a positive correlation with share prices. Once a quant fund has an understanding of your dataset, they can run a backtest to establish value. </span>

<span>To attract fundamental investors, it’s easier to start with a few case studies about specific public companies. Pick a few that your data can best speak to and run a correlation to their KPIs (e.g., Revenue, GMV, Gross Profit). The best companies are:</span></p><ul>
 	<li><span>Stock price is driven by a key metric or investor question your data can speak to</span></li>
</ul>
<ul>
 	<li><span>Large market cap / average trading volume</span></li>
</ul>
<ul>
 	<li><span>High volatility </span></li>
</ul>
<ul>
 	<li><span>Always nice: high hedge fund ownership (examples on </span><a href="https://insight.factset.com/hubfs/Hedge%20Fund%20Ownership/Hedge%20Fund%20Ownership%20Q4%202016_2.18.17.pdf"><span>page 5</span></a><span>)</span></li>
</ul>
<p><span>The right company will vary by dataset, but a few examples:</span></p><ul>
 	<li><span>Panel of Mobile App Usage: Correlate to observed app usage with reported DAUs for SnapChat, Facebook or Twitter</span></li>
</ul>
<ul>
 	<li><span>Panel of Consumer Transactions: Correlate to same store sales of retailers or GMV/sales of ecommerce companies </span></li>
</ul>
<ul>
 	<li><span>Social Sentiment: Correlate shifts in sentiment to revenue or share price of consumer / apparel brand companies</span></li>
</ul>
<h3><b>Step 3: Start with early adopters: quant funds and platform funds</b></h3>
<p><span>Both types of organizations employ teams of people looking to speak with data owners, can move quickly and offer you a price for your data. </span>

<b>You can speak to these people directly and don’t need to work with a data broker or other intermediary, who can demand a revenue share of 50% or more. </b></p><h3><b>Step 4: Sign up early adopters quickly with a limited distribution.</b></h3>
<p><span>Selling data is a multi-stage game. Price discovery and productization can take years. Don’t overthink your first contracts or hold out for the last dollar. Speak to a handful of early adopters, negotiate fair, 1 year contracts with a limited distribution, say 5-10 funds. You can decide the specific number based on conversations with the funds.</span>

<span>Do it quickly so you can then focus your your time and resources on understanding exactly how investors are using your data and determine your strategy when renewals come around.</span></p><h3><b>Step 5: Productize your data</b></h3>
<p><span>Quant, platform and other large hedge funds employ data teams who can extract value from data in nearly any format. Selling into a broader audience of funds requires additional QA and analysis investments into your data product. For more information on developing this team see our post on How to </span><a href="https://alternativedata.org/how-yipitdata-integrates-investment-analysts-data-analysts-and-engineers/"><span>Integrate Investment Analysts, Data Analysts and Engineers</span></a><span>.</span>

<span>Productizing your data means providing additional QA and analysis to enable you to understand it’s value and enable a fund without a large data team to extract value from it. </span>

<span>Productizing your data, regardless of your customer distribution, will make it easier to use by more individuals at your customers, increasing its value.</span></p><h3><b>Step 6: Determine the size of your eventual market</b></h3>
<p><span>Once your first year contract is up, you will need to decide whether you want to expand the size of your distribution. The more funds you sell to, the less valuable your data will be to your customers.</span>

<span>Reasons you may be able to increase your distribution and sell to more types of funds:</span></p><ul>
 	<li><span>It is the #1 dataset of its kind</span></li>
</ul>
<ul>
 	<li><span>Your dataset applies to lots and lots of companies</span></li>
</ul>
<ul>
 	<li><span>The data is granular and multi-dimensional. It’s more than a single datapoint and different types of investors use it to answer different types of questions</span></li>
</ul>
<ul>
 	<li><span>You can sell other services on top of the data</span></li>
</ul>
<ul>
 	<li><span>Compliance departments generally prefer datasets with a broader distribution</span></li>
</ul>
<ul>
 	<li><span>You can develop different data products for different customer types at different price points, so not everyone is getting the same experience </span></li>
</ul>
<ul>
 	<li><span>Diversify your revenue stream across more customers</span></li>
</ul>
<p><span>Reasons you may want to maintain a limited distribution: </span></p><ul>
 	<li><span>Reduce complexity</span></li>
</ul>
<ul>
 	<li><span>Reduce need for marketing, sales and customer service expenses</span></li>
</ul>
<ul>
 	<li><span>Enjoy higher ASP, higher margins</span></li>
</ul>
<ul>
 	<li><span>The primary use case of your data is a KPI estimate, which is more quickly commoditized than a granular dataset. Are you providing a revenue estimate or a way to understand an entire industry?</span></li>
</ul>
<h2><b>Frequently Asked Questions</b></h2>
<h3><b>How much money will I make?</b></h3>
<p><span>Reasons you may have unrealistic expectations of the value of your dataset:</span></p><ul>
 	<li><span>The value of your dataset will depend heavily on details such as accuracy, time series, compliance, release schedule</span></li>
</ul>
<ul>
 	<li><span>It will also depend on factors specific to the target company such as the existence of competitor datasets, the precision of sellside consensus, key investor questions, or the existence of legal/macro/regulatory overhangs</span></li>
</ul>
<ul>
 	<li><span>Investors will pay a large premium for the #1 dataset in a category. Are you #1?</span></li>
</ul>
<ul>
 	<li><span>Rumors of datasets commanding enormous premiums are more viral than ones about datasets nobody wants </span></li>
</ul>
<h3><b>What about contracts and compliance?</b></h3>
<p><span>Hedge fund customers will demand certain representations about your dataset. The </span><a href="http://yipitdata.com/agreements/msa"><span>YipitData Master Services Agreement</span></a><span> can give you a sense of what to expect. </span>

<span>You should consult a lawyer to help you with contracts relating to your specific dataset and to help you through your customer compliance reviews.</span><span>
</span><b></b></p><h3><b>What should I never do?</b></h3>
<p><span>Provide material, non-public information in violation of securities laws or personally identifiable information.</span>

<span>Provide misleading, doctored or “data-mined” historical correlations.</span>

<span>Conceal significant data outages or other issues that may affect your data's accuracy.</span></p><h2><b>CONCLUSION</b></h2>
<p><span>We think a reasonable go-to-market strategy for hedge funds is:</span></p><ul>
 	<li><span>Start with platform funds and quant funds</span></li>
</ul>
<ul>
 	<li><span>Set up one year contracts with a limited number (5-10) of buyers as soon as possible</span></li>
</ul>
<ul>
 	<li><span>Spend your next year productizing your data and learning about its use cases</span></li>
</ul>
<ul>
 	<li><span>Determine whether or not you want to expand the size of your distribution</span></li>
</ul>
<p><b>If you believe you have data that may be of interest to hedge funds, we would be happy to speak with you, and if you are interested, refer you to hedge funds who have expressed interest in alternative data. Email me at: </b><a href="mailto:jim@alternativedata.org"><b>jim@alternativedata.org</b></a>

___</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://alternativedata.org/the-ultimate-guide-to-selling-data-to-hedge-funds/">https://alternativedata.org/the-ultimate-guide-to-selling-data-to-hedge-funds/</a></em></p>]]>
            </description>
            <link>https://alternativedata.org/the-ultimate-guide-to-selling-data-to-hedge-funds/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24958215</guid>
            <pubDate>Sun, 01 Nov 2020 09:11:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building Containers Without Docker]]>
            </title>
            <description>
<![CDATA[
Score 110 | Comments 22 (<a href="https://news.ycombinator.com/item?id=24957887">thread link</a>) | @kiyanwang
<br/>
November 1, 2020 | https://blog.alexellis.io/building-containers-without-docker/ | <a href="https://web.archive.org/web/*/https://blog.alexellis.io/building-containers-without-docker/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
    <article>

        

        <section>
            <div><p>In this post I'll outline several ways to build containers without the need for Docker itself. I'll use <a href="https://github.com/openfaas/">OpenFaaS</a> as the case-study, which uses OCI-format container images for its workloads. The easiest way to think about OpenFaaS is as a CaaS platform for <a href="https://kubernetes.io/">Kubernetes</a> which can run microservices, and add in FaaS and event-driven tooling for free.</p>
<p>See also <a href="https://openfaas.com/">OpenFaaS.com</a></p>
<p>The first option in the post will show how to use the built-in buildkit option for Docker's CLI, then <a href="https://github.com/moby/buildkit">buildkit</a> stand-alone (on Linux only), followed by Google's container builder, <a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a>.</p>
<p>This post covers tooling which can build an image from a Dockerfile, and so anything which limits the user to only Java (jib) or Go (ko) for instance is out of scope. I'll then wrap things up and let you know how to get in touch with suggestions, feedback and your own stories around wants and needs in container tooling.</p>
<h2 id="sowhatswrongwithdocker">So what's wrong with Docker?</h2>
<p>Nothing as such, Docker runs well on armhf, arm64, and on <code>x86_64</code>. The main Docker CLI has become a lot more than build/ship/run, and also lugs around several years of baggage, it now comes bundled with Docker Swarm and EE features.</p>
<blockquote>
<p>Update for Nov 2020: anyone using Docker's set of official base-images should also read: <a href="https://inlets.dev/blog/2020/10/29/preparing-docker-hub-rate-limits.html">Preparing for the Docker Hub Rate Limits</a></p>
</blockquote>
<h3 id="alternativestodocker">Alternatives to Docker</h3>
<p>There are a few efforts that attempt to strip "docker" back to its component pieces, the original UX we all fell in love with:</p>
<ul>
<li>
<p><a href="https://github.com/docker/docker">Docker</a> - docker itself now uses containerd to run containers, and has support for enabling buildkit to do highly efficient, caching builds.</p>
</li>
<li>
<p><a href="https://podman.io/">Podman</a> and <a href="https://github.com/containers/buildah">buildah</a> combination - RedHat / IBM's effort, which uses their own OSS toolchain to generate OCI images. Podman is marketed as being daemonless and rootless, but still ends up having to mount overlay filesystems and use a UNIX socket.</p>
</li>
<li>
<p><a href="https://github.com/alibaba/pouch">pouch</a> - from Alibaba, pouch is billed as "An Efficient Enterprise-class Container Engine". It uses containerd just like Docker, and supports both container-level isolation with <a href="https://github.com/opencontainers/runc">runc</a> and "lightweight VMs" such as <a href="https://github.com/hyperhq/runv">runV</a>. There's also more of a <a href="https://github.com/alibaba/pouch/blob/master/docs/architecture.md">focus on image distribution and strong isolation</a>.</p>
</li>
<li>
<p>Stand-alone buildkit - buildkit was started by <a href="https://twitter.com/tonistiigi?lang=en">Tõnis Tiigi</a> from Docker Inc as a brand new container builder with caching and concurrency in mind. buildkit currently only runs as a daemon, but you will hear people claim otherwise. They are forking the daemon and then killing it after a build.</p>
</li>
<li>
<p><a href="https://github.com/genuinetools/img">img</a> - img was written by <a href="https://github.com/jessfraz">Jess Frazelle</a> and is often quoted in these sorts of guides and is a wrapper for buildkit. That said, I haven't seen traction with it compared to the other options mentioned. The project was quite active <a href="https://github.com/genuinetools/img/commits/master">until late 2018 and has only received a few patches since</a>. img claims to be daemonless, but it uses buildkit so is probably doing some trickery there. I hear that <code>img</code> gives a better UX than buildkit's own CLI <code>buildctr</code>, but it should also be noted that img is only released for <code>x86_64</code> and there are no binaries for armhf / arm64.</p>
</li>
</ul>
<blockquote>
<p>An alternative to <code>img</code> would be <code>k3c</code> which also includes a runtime component and plans to support ARM architectures.</p>
</blockquote>
<ul>
<li><a href="https://github.com/ibuildthecloud/k3c">k3c</a> - Rancher's latest experiment which uses containerd and buildkit to re-create the original, classic, vanilla, lite experience of the original Docker version.</li>
</ul>
<p>Out of all the options, I think that I like k3c the most, but it is very nascient and bundles everything into one binary which is likely to conflict with other software, at present it runs its own embedded containerd and buildkit binaries.</p>
<blockquote>
<p>Note: If you're a RedHat customer and paying for support, then you really should use their entire toolchain to get the best value for your money. I checked out some of the examples and saw one that used my "classic" blog post on multi-stage builds. See for yourself which style you prefer <a href="https://github.com/containers/buildah/blob/master/demos/buildah_multi_stage.sh">the buildah example</a> vs. <a href="https://blog.alexellis.io/mutli-stage-docker-builds">Dockerfile example</a>.</p>
</blockquote>
<p>So since we are focusing on the "build" piece here and want to look at relativelt stable options, I'm going to look at:</p>
<ul>
<li>buildkit in Docker,</li>
<li>buildkit stand-alone</li>
<li>and kaniko.</li>
</ul>
<p>All of the above and more are now possible since the OpenFaaS CLI can output a standard "build context" that any builder can work with.</p>
<h2 id="buildatestapp">Build a test app</h2>
<p>Let's start with a Golang HTTP middleware, this is a cross between a function and a microservice and shows off how versatile OpenFaaS can be.</p>
<pre><code>faas-cli template store pull golang-middleware

faas-cli new --lang golang-middleware \
  build-test --prefix=alexellis2
</code></pre>
<ul>
<li><code>--lang</code> specifies the build template</li>
<li><code>build-test</code> is the name of the function</li>
<li><code>--prefix</code> is the Docker Hub username to use for pushing up our OCI image</li>
</ul>
<p>We'll get the following created:</p>
<pre><code>./
├── build-test
│   └── handler.go
└── build-test.yml

1 directory, 2 files
</code></pre>
<p>The handler looks like this, and is easy to modify. Additional dependencies can be added through vendoring or <a href="https://blog.golang.org/using-go-modules">Go modules</a>.</p>
<pre><code>package function

import (
	"fmt"
	"io/ioutil"
	"net/http"
)

func Handle(w http.ResponseWriter, r *http.Request) {
	var input []byte

	if r.Body != nil {
		defer r.Body.Close()

		body, _ := ioutil.ReadAll(r.Body)

		input = body
	}

	w.WriteHeader(http.StatusOK)
	w.Write([]byte(fmt.Sprintf("Hello world, input was: %s", string(input))))
}
</code></pre>
<h3 id="buildthenormalway">Build the normal way</h3>
<p>The normal way to build this app would be:</p>
<pre><code>faas-cli build -f build-test.yml
</code></pre>
<p>A local cache of the template and Dockerfile is also available at <code>./template/golang-middleware/Dockerfile</code></p>
<p>There are three images that are pulled in for this template:</p>
<pre><code>FROM openfaas/of-watchdog:0.7.3 as watchdog
FROM golang:1.13-alpine3.11 as build
FROM alpine:3.12
</code></pre>
<p>With the traditional builder, each of the images will be pulled in sequentially.</p>
<p>The wait a few moments and you're done, we now have that image in our local library.</p>
<p>We can also push it up to a registry with <code>faas-cli push -f build-test.yml</code>.</p>
<p><img src="https://blog.alexellis.io/content/images/2020/01/seq.png" alt="seq"></p>
<h3 id="buildwithbuildkitanddocker">Build with Buildkit and Docker</h3>
<p>This is the easiest change of all to make, and gives a fast build too.</p>
<pre><code>DOCKER_BUILDKIT=1 faas-cli build -f build-test.yml
</code></pre>
<p>We'll see that with this approach, the Docker daemon automatically switches out its builder for buildkit.</p>
<p>Buildkit offers a number of advantages:</p>
<ul>
<li>More sophisticated caching</li>
<li>Running later instructions first, when possible - i.e. downloading the "runtime" image, before the build in the "sdk" layer is even completed</li>
<li>Super fast when building a second time</li>
</ul>
<p>With buildkit, all of the base images can be pulled in to our local library at once, since the FROM (download) commands are not executed sequentially.</p>
<pre><code>FROM openfaas/of-watchdog:0.7.3 as watchdog
FROM golang:1.13-alpine3.11 as build
FROM alpine:3.11
</code></pre>
<p>This option works even on a Mac, since buildkit is proxied via the Docker daemon running in the VM.</p>
<p><img src="https://blog.alexellis.io/content/images/2020/01/dkit.png" alt="dkit"></p>
<h3 id="buildwithbuildkitstandalone">Build with Buildkit standalone</h3>
<p>To build with Buildkit in a stand-alone setup we need to run buildkit separately on a Linux host, so we can't use a Mac.</p>
<p><code>faas-cli build</code> would normally execute or fork <code>docker</code>, because the command is just a wrapper. So to bypass this behaviour we should write out a build context, that's possible via the following command:</p>
<pre><code>faas-cli build -f build-test.yml --shrinkwrap

[0] &gt; Building build-test.
Clearing temporary build folder: ./build/build-test/
Preparing ./build-test/ ./build/build-test//function
Building: alexellis2/build-test:latest with golang-middleware template. Please wait..
build-test shrink-wrapped to ./build/build-test/
[0] &lt; Building build-test done in 0.00s.
[0] Worker done.

Total build time: 0.00
</code></pre>
<p>Our context is now available in the <code>./build/build-test/</code> folder with our function code and the template with its entrypoint and Dockerfile.</p>
<pre><code>./build/build-test/
├── Dockerfile
├── function
│   └── handler.go
├── go.mod
├── main.go
└── template.yml

1 directory, 5 files
</code></pre>
<p>Now we need to run buildkit, we can build from source, or grab upstream binaries.</p>
<pre><code>curl -sSLf https://github.com/moby/buildkit/releases/download/v0.6.3/buildkit-v0.6.3.linux-amd64.tar.gz | sudo tar -xz -C /usr/local/bin/ --strip-components=1
</code></pre>
<p>If you checkout the releases page, you'll also find buildkit available for armhf and arm64, which is great for multi-arch.</p>
<p>Run the buildkit daemon in a new window:</p>
<pre><code>sudo buildkitd 
WARN[0000] using host network as the default            
INFO[0000] found worker "l1ltft74h0ek1718gitwghjxy", labels=map[org.mobyproject.buildkit.worker.executor:oci org.mobyproject.buildkit.worker.hostname:nuc org.mobyproject.buildkit.worker.snapshotter:overlayfs], platforms=[linux/amd64 linux/386] 
WARN[0000] skipping containerd worker, as "/run/containerd/containerd.sock" does not exist 
INFO[0000] found 1 workers, default="l1ltft74h0ek1718gitwghjxy" 
WARN[0000] currently, only the default worker can be used. 
INFO[0000] running server on /run/buildkit/buildkitd.sock 
</code></pre>
<p>Now let's start a build, passing in the shrink-wrapped location as the build-context. The command we want is <code>buildctl</code>, buildctl is a client for the daemon and will configure how to build the image and what to do when it's done, such as exporting a tar, ignoring the build or pushing it to a registry.</p>
<pre><code>buildctl build --help
NAME:
   buildctl build - build

USAGE:
   
  To build and push an image using Dockerfile:
    $ buildctl build --frontend dockerfile.v0 --opt target=foo --opt build-arg:foo=bar --local context=. --local dockerfile=. --output type=image,name=docker.io/username/image,push=true
  

OPTIONS:
   --output value, -o value  Define exports for build result, e.g. --output type=image,name=docker.io/username/image,push=true
   --progress value          Set type of progress (auto, plain, tty). Use plain to show container output (default: "auto")
   --trace value             Path to trace file. Defaults to no tracing.
   --local value             Allow build access to the local directory
   --frontend value          Define frontend used for build
   --opt value               Define custom options for frontend, e.g. --opt target=foo --opt build-arg:foo=bar
   --no-cache                Disable cache for all the vertices
   --export-cache value      Export build cache, e.g. --export-cache type=registry,ref=example.com/foo/bar, or …</code></pre></div></section></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.alexellis.io/building-containers-without-docker/">https://blog.alexellis.io/building-containers-without-docker/</a></em></p>]]>
            </description>
            <link>https://blog.alexellis.io/building-containers-without-docker/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24957887</guid>
            <pubDate>Sun, 01 Nov 2020 07:54:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I No Longer Tell My Friends about Anki/SuperMemo]]>
            </title>
            <description>
<![CDATA[
Score 63 | Comments 83 (<a href="https://news.ycombinator.com/item?id=24957617">thread link</a>) | @kioleanu
<br/>
October 31, 2020 | https://www.masterhowtolearn.com/2020-10-31-why-i-no-longer-tell-my-friends-about-anki-supermemo/ | <a href="https://web.archive.org/web/*/https://www.masterhowtolearn.com/2020-10-31-why-i-no-longer-tell-my-friends-about-anki-supermemo/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p>This article <a href="https://www.lesswrong.com/posts/6NvbSwuSAooQxxf7f/beware-of-other-optimizing">Beware of Other-Optimizing</a> by Eliezer Yudkowsky is highly illuminating. I recommend you read it through the lens of “recommending Anki to a friend.” This article is about the problems of giving advice on how to learn.</p>
<h2>Spreading the “gospel” to the world</h2>
<p>If you deeply believe something, along the lines of “if everyone did it, the world would be much better off.” and have tried convincing other people to do that thing, then you will realize it’s almost impossible to change others’ opinions or behaviors.</p>
<p>Maybe you’re not that ambitious to convince everyone, so you start small: you share it with your friends, but then discovered nobody actually cares or realizes its significance. A mild version is like <a href="https://www.deathbulge.com/comics/206">showing your favorite TV show and they don’t care</a>:</p>
<p><span>
      <a href="https://www.masterhowtolearn.com/static/26c8c80614f9b9a1a0a8f6e2ff1e17c2/acb04/deathbulge.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="deathbulge" title="deathbulge" src="https://www.masterhowtolearn.com/static/26c8c80614f9b9a1a0a8f6e2ff1e17c2/1c72d/deathbulge.jpg" srcset="https://www.masterhowtolearn.com/static/26c8c80614f9b9a1a0a8f6e2ff1e17c2/a80bd/deathbulge.jpg 148w,
https://www.masterhowtolearn.com/static/26c8c80614f9b9a1a0a8f6e2ff1e17c2/1c91a/deathbulge.jpg 295w,
https://www.masterhowtolearn.com/static/26c8c80614f9b9a1a0a8f6e2ff1e17c2/1c72d/deathbulge.jpg 590w,
https://www.masterhowtolearn.com/static/26c8c80614f9b9a1a0a8f6e2ff1e17c2/acb04/deathbulge.jpg 750w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<p>My faith in evidence-based learning strategies is informed by my personal experience and <a href="https://www.masterhowtolearn.com/2020-02-25-how-to-learn-about-meta-learning-my-resource-list">my meta-learning list</a>. I wholeheartedly believe Spaced Repetition Software (SRS) like SuperMemo and Anki is the key to effective and efficient learning. If you believe education is the future, then the knowledge about evidence-based learning strategies is one big key to unlocking that future, both individually and collectively.</p>
<h2>“Doing this every day seems very tiring.”</h2>
<p>Two years ago I was doing my Anki reps. One friend glanced over and was interested in what I was doing.</p>
<p><span>
      <a href="https://www.masterhowtolearn.com/static/443ff24d4c08fb6e97f3db4438a49b2e/ee745/myTime.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="My time has come" title="My time has come" src="https://www.masterhowtolearn.com/static/443ff24d4c08fb6e97f3db4438a49b2e/1c72d/myTime.jpg" srcset="https://www.masterhowtolearn.com/static/443ff24d4c08fb6e97f3db4438a49b2e/a80bd/myTime.jpg 148w,
https://www.masterhowtolearn.com/static/443ff24d4c08fb6e97f3db4438a49b2e/1c91a/myTime.jpg 295w,
https://www.masterhowtolearn.com/static/443ff24d4c08fb6e97f3db4438a49b2e/1c72d/myTime.jpg 590w,
https://www.masterhowtolearn.com/static/443ff24d4c08fb6e97f3db4438a49b2e/ee745/myTime.jpg 660w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<p>We ate lunch together while I was giving her a little presentation. I talked <em>very enthusiastically</em> about spaced repetition, basic memory science and Anki operations. Of course I had to show her the infamous forgetting curve:</p>
<p><span>
      <a href="https://www.masterhowtolearn.com/static/2abe95310027f94ad36981994f56c65d/8aab1/forgetting-curve-wired-wozniak.webp" target="_blank" rel="noopener">
    <span></span>
  <img alt="forgetting-curve" title="forgetting-curve" src="https://www.masterhowtolearn.com/static/2abe95310027f94ad36981994f56c65d/5ca24/forgetting-curve-wired-wozniak.webp" srcset="https://www.masterhowtolearn.com/static/2abe95310027f94ad36981994f56c65d/cbe2e/forgetting-curve-wired-wozniak.webp 148w,
https://www.masterhowtolearn.com/static/2abe95310027f94ad36981994f56c65d/3084c/forgetting-curve-wired-wozniak.webp 295w,
https://www.masterhowtolearn.com/static/2abe95310027f94ad36981994f56c65d/5ca24/forgetting-curve-wired-wozniak.webp 590w,
https://www.masterhowtolearn.com/static/2abe95310027f94ad36981994f56c65d/8aab1/forgetting-curve-wired-wozniak.webp 630w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<p>She sounded excited about the possibility of finally mastering a second language. But at one point she was confused:</p>
<blockquote>
<p>“If I hadn’t learned it how could I recall it from memory?”</p>
</blockquote>
<p>So I explained that assessment <strong>for</strong> learning (testing as a means of learning) is different from assessment <strong>of</strong> learning (finding out what you’ve learned). The act of “recalling from memory” is itself a learning process. Then I was aware that her confusion was probably due to a common misconception about memory: human memory works like computer memory:</p>
<blockquote>
<p>The functional architecture of how humans forget, remember, and learn is <strong>unlike</strong> the corresponding processes in man-made devices […] We think of ourselves as working like computers, we become prone to assuming that exposing ourselves to information and procedures will lead to storage (i.e., recording) of such information or procedures in our memories—that the information will write itself in one’s memory.</p>
<p>If we think of human memory equals to memory in a computer, we are unlikely to appreciate that retrieving information from our memory increases the subsequent accessibility of that information, while retrieving information from computer memory leaves the status of that information unperturbed. <a href="https://www.taylorfrancis.com/books/e/9780203842539/chapters/10.4324%2F9780203842539-6">On the Symbiosis of Remembering, Forgetting, and Learning</a></p>
</blockquote>
<p>Side note: this story might give you the impression that I liked to show off what I know. No not really. I like to be convinced with evidence, and I thought others were the same. I was wrong. Also, I included the above quotes for the sake of convincing you, the reader. I didn’t obfuscate the subject matter further with phrases like assessment <strong>for</strong> learning and assessment <strong>of</strong> learning.</p>
<p>She looked… befuddled. Sort of like this:</p>
<p><span>
      <a href="https://www.masterhowtolearn.com/static/e459a547d1aa99ea034a06ea3f4f1f13/c08c5/AwkwardSmile.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="awkwardSmile" title="awkwardSmile" src="https://www.masterhowtolearn.com/static/e459a547d1aa99ea034a06ea3f4f1f13/1c72d/AwkwardSmile.jpg" srcset="https://www.masterhowtolearn.com/static/e459a547d1aa99ea034a06ea3f4f1f13/a80bd/AwkwardSmile.jpg 148w,
https://www.masterhowtolearn.com/static/e459a547d1aa99ea034a06ea3f4f1f13/1c91a/AwkwardSmile.jpg 295w,
https://www.masterhowtolearn.com/static/e459a547d1aa99ea034a06ea3f4f1f13/1c72d/AwkwardSmile.jpg 590w,
https://www.masterhowtolearn.com/static/e459a547d1aa99ea034a06ea3f4f1f13/c08c5/AwkwardSmile.jpg 640w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<p>I remember I was so excited and nervous at the same time that I was fumbling for words, trying to simplify it as much as I could. In retrospect, I was making things worse.</p>
<p>She’d listen to me quite intently, signaling that she was thinking and trying to figure it out, but I could tell she was still confused. And the more I explained, the deeper the rabbit hole went (like keep clicking hyperlinks in Wikipedia), and the more confused she became, so I stopped talking to stop making the whole situation uncomfortable. The following monologue is how I imagine what she was thinking at the time:</p>
<blockquote>
<p>“He’s so passionate about this stuff and so sure of himself that I guess he’s right.”</p>
</blockquote>
<blockquote>
<p>“I’m not really sure what he means. I do want to learn Japanese but the stuff he’s talking about is so confusing…”</p>
</blockquote>
<blockquote>
<p>“I was indeed interested in the beginning, but I don’t really care at this point. I’m just going to pretend I understand and end this whole conversation asap.”</p>
</blockquote>
<p>I will never forget what she said at one point,</p>
<blockquote>
<p>“Doing this every day seems very tiring.”</p>
</blockquote>
<p>I was like “Yeah…” I never followed up on her progress. I figured if she was truly interested, when she bumped into problem she would ask for my help. As expected, we never talked about it and I never mentioned Anki again.</p>
<p><span>
      <a href="https://www.masterhowtolearn.com/static/7a936abffcd1a452478dca75a9f1ec62/0b533/sadFrog.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="sadFrog" title="sadFrog" src="https://www.masterhowtolearn.com/static/7a936abffcd1a452478dca75a9f1ec62/0b533/sadFrog.png" srcset="https://www.masterhowtolearn.com/static/7a936abffcd1a452478dca75a9f1ec62/12f09/sadFrog.png 148w,
https://www.masterhowtolearn.com/static/7a936abffcd1a452478dca75a9f1ec62/e4a3f/sadFrog.png 295w,
https://www.masterhowtolearn.com/static/7a936abffcd1a452478dca75a9f1ec62/0b533/sadFrog.png 500w" sizes="(max-width: 500px) 100vw, 500px" loading="lazy">
  </a>
    </span></p>
<p>These has happened countless times. I usually look nonchalant on the outside but dying on the inside. People looked interested (probably due to my enthusiasm and it’s impolite to look otherwise) and then never actually bothered to use Anki. Some would actually try, make a few cards, do the reps for a few days and then say, “I tried and Anki doesn’t work.”</p>
<p>I’m probably over my head, but sometimes it feels like I’m personally attacked, that what I’m saying is not valuable. I understand it’s not true, but it took me years to study the learning science and to gain the experience with Anki/SuperMemo. The fact that they’re dismissing the software feels like they’re dismissing my knowledge and experience.</p>
<h2>The problems of giving advice on how to learn</h2>
<h3>#1. Nobody cares <em>that</em> much, alright?</h3>
<p>When I first discovered Anki I was like, “How come no one around me knows this?! I need to share this to everyone!” So I would tell my friends about Anki but nobody was interested. I’ve introduced Anki to people and every time it’s a very frustrating experience. (Surprise surprise, not SuperMemo. The learning curve of Anki is much lower. What chance do I have if I preached SuperMemo when they even think Anki is too hard to use?)</p>
<p>Update: I was never a missionary and randomly went out my way to tell people about Anki all the time like this:</p>
<p><span>
      <a href="https://www.masterhowtolearn.com/static/e160326dcc1a72ba87b79f26f888d457/737f1/thejenkinscomicVim.webp" target="_blank" rel="noopener">
    <span></span>
  <img alt="vim" title="vim" src="https://www.masterhowtolearn.com/static/e160326dcc1a72ba87b79f26f888d457/737f1/thejenkinscomicVim.webp" srcset="https://www.masterhowtolearn.com/static/e160326dcc1a72ba87b79f26f888d457/cbe2e/thejenkinscomicVim.webp 148w,
https://www.masterhowtolearn.com/static/e160326dcc1a72ba87b79f26f888d457/3084c/thejenkinscomicVim.webp 295w,
https://www.masterhowtolearn.com/static/e160326dcc1a72ba87b79f26f888d457/737f1/thejenkinscomicVim.webp 585w" sizes="(max-width: 585px) 100vw, 585px" loading="lazy">
  </a>
    </span></p>
<p><a href="https://thejenkinscomic.wordpress.com/">Source</a></p>
<p>Very early on I did give unsolicited advice once to my best friend by intentionally bringing up Anki. Then I did bring up Anki on multiple occasions, but only when the opportunity presented itself, like the story above.</p>
<p>People may be interested in how you’ve developed a skill or become fluent in a foreign language, but just not <strong>that</strong> interested. And they certainly don’t expect to be suddenly lectured on how learning and memory work.</p>
<p>I never liked following up to people with questions like “So how’s Anki? Have you used it?” It feels like pushing an agenda to them. Also, since most would not even bother buying and downloading the app, their response is usually, “I forgot about it. I’ll do it later.” and the conversation would end in an awkward tone.</p>
<h3>#2. I’m not sure if I actually want it, alright?</h3>
<p>Sometimes people keep lamenting “I want to learn X or I want to get better grades.” How many times do you hear people say they want to learn a second language? This is like that friend who keeps saying “I want to lose weight”. Probably after all, they’re just lamenting and not yet ready to put in the effort to change.</p>
<p>Trying to convince others to use Anki/SuperMemo is like trying to convince your friends to go to the gym regularly. You can talk about the benefits of exercising/weight-lifting, how good you’ll feel afterwards, how much more productive you’ll be and so on. But nothing will work if they don’t try it in the first place, and it doesn’t help that spaced repetition doesn’t work in the short-term since using Spaced Repetition Software is a life-long pursuit (just as learning is):</p>
<blockquote>
<p>This long term focus may explain why explicit spaced repetition is an uncommon studying technique: the pay-off is distant &amp; counterintuitive, the cost of self-control near &amp; vivid. <a href="https://www.gwern.net/Spaced-repetition">Gwern’s Spaced Repetition for Efficient Learning</a></p>
</blockquote>
<h3>#3. Why are you so hyped about it?</h3>
<p>It’s rare if people could understand and realize the significance and application of Spaced Repetition Software in a casual 10-min chat. It’s not about the complexity (it’s not rocket science after all), but rather it’s about awareness: problems with current learning approaches and how Anki/SuperMemo could solve the problems. In other words, if I don’t see the problems, why bother changing?</p>
<p>I have a friend who was learning German. She copied German vocabulary on one side and the equivalent English on the other in a notebook. I showed her my Korean Anki cards: “Take a look at these beautiful images! Gifs! Sentence cards! Audio clips! Mass Immersion Approach! (Former: All Japanese All The Time (AJATT)), Stephen Krashen’s Input Hypothesis!” Then I had another friend who was studying to become a nurse. I told him about Anki: “Image occlusion for anatomy!” It felt wrong of me to be hiding Anki (<a href="https://ankiweb.net/shared/info/1374772155">Image Occlusion</a> to be specific) from him. Deep down I want to shove their faces to this table:</p>
<p><span>
      <a href="https://www.masterhowtolearn.com/static/50f60f43bbd89353f63423035d211043/91608/UtilityAssessment.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="UtilityAssessment" title="UtilityAssessment" src="https://www.masterhowtolearn.com/static/50f60f43bbd89353f63423035d211043/fcda8/UtilityAssessment.png" srcset="https://www.masterhowtolearn.com/static/50f60f43bbd89353f63423035d211043/12f09/UtilityAssessment.png 148w,
https://www.masterhowtolearn.com/static/50f60f43bbd89353f63423035d211043/e4a3f/UtilityAssessment.png 295w,
https://www.masterhowtolearn.com/static/50f60f43bbd89353f63423035d211043/fcda8/UtilityAssessment.png 590w,
https://www.masterhowtolearn.com/static/50f60f43bbd89353f63423035d211043/efc66/UtilityAssessment.png 885w,
https://www.masterhowtolearn.com/static/50f60f43bbd89353f63423035d211043/c83ae/UtilityAssessment.png 1180w,
https://www.masterhowtolearn.com/static/50f60f43bbd89353f63423035d211043/91608/UtilityAssessment.png 1251w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<p>(Image source: <a href="https://journals.sagepub.com/stoken/rbtfl/Z10jaVH/60XQM/full">Improving Students’ Learning With Effective Learning Techniques</a>)</p>
<p>I was exaggerating but you get the point: it’s not possible to convey the significance and application of it all in a casual 10-min chat. With the benefit of hindsight, all my attempts could’ve been a lot better:</p>
<ul>
<li>Maybe I was so convinced and have so much faith in Spaced Repetition Software that I came off as condescending, giving off the impression like “you don’t know how to study; let me teach you.”</li>
<li>Maybe it’s the bold claims: “You’ll get 2x results with half the study time.” (How many are really true whenever you hear such claim?)</li>
<li>Maybe it’s the situation: all he or she wants is someone to listen about the difficulty of studying, not some real advice or suggestions.</li>
</ul>
<p>Here’s the guy from <a href="http://brianjx.altervista.org/">How I Passed the Demanding […] Italian Language Exam Without Going to Italy – Here’s a Hint: the 326,538 Flashcard Reviews Helped a Lot.</a></p>
<blockquote>
<p>Like many language teachers, V. had never heard of Anki (but she did know Reverso Context). I showed her how I studied Italian …</p></blockquote></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.masterhowtolearn.com/2020-10-31-why-i-no-longer-tell-my-friends-about-anki-supermemo/">https://www.masterhowtolearn.com/2020-10-31-why-i-no-longer-tell-my-friends-about-anki-supermemo/</a></em></p>]]>
            </description>
            <link>https://www.masterhowtolearn.com/2020-10-31-why-i-no-longer-tell-my-friends-about-anki-supermemo/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24957617</guid>
            <pubDate>Sun, 01 Nov 2020 06:33:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Visualizing Git Concepts with D3]]>
            </title>
            <description>
<![CDATA[
Score 258 | Comments 27 (<a href="https://news.ycombinator.com/item?id=24957280">thread link</a>) | @gilad
<br/>
October 31, 2020 | https://onlywei.github.io/explain-git-with-d3/ | <a href="https://web.archive.org/web/*/https://onlywei.github.io/explain-git-with-d3/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div id="ExplainGitCommit-Container">
      <p>
        We are going to skip instructing you on how to add your files for commit in this explanation.
        Let's assume you already know how to do that. If you don't, go read some other tutorials.
      </p>
      <p>
        Pretend that you already have your files staged for commit and enter <span>git commit</span>
        as many times as you like in the terminal box.
      </p>
      
    </div>
    <div id="ExplainGitTag-Container">
      <p>
        <span>git tag name</span> will create a new tag named "name".
        Creating tags just creates a new tag pointing to the currently checked out commit.
      </p>
      <p>
        Tags can be deleted using the command <span>git tag -d name</span> (coming soon).
      </p>
      <p>
        Type <span>git commit</span> and <span>git tag</span> commands
        to your hearts desire until you understand this concept.
      </p>
      
    </div>
    <div id="ExplainGitBranch-Container">
      <p>
        <span>git branch name</span> will create a new branch named "name".
        Creating branches just creates a new tag pointing to the currently checked out commit.
      </p>
      <p>
        Branches can be deleted using the command <span>git branch -d name</span>.
      </p>
      <p>
        Type <span>git commit</span> and <span>git branch</span> commands
        to your hearts desire until you understand this concept.
      </p>
      
    </div>
    <div id="ExplainGitCheckout-Container">
      <p>
        <span>git checkout</span> has many uses,
        but the main one is to switch between branches.<br>
        For example, to switch from master branch to dev branch,
        I would type <span>git checkout dev</span>.
        After that, if I do a git commit, notice where it goes. Try it.
      </p>
      <p>
        In addition to checking out branches, you can also checkout individual commits. Try it.<br>
        Make a new commit and then type <span>git checkout bb92e0e</span>
        and see what happens.
      </p>
      <p>
        Type <span>git commit</span>, <span>git branch</span>,
        and <span>git checkout</span> commands to your hearts desire
        until you understand this concept.
      </p>
      
    </div>
    <div id="ExplainGitCheckout-b-Container">
      <p>
        You can combine <span>git branch</span> and <span>git checkout</span>
        into a single command by typing <span>git checkout -b branchname</span>.
        This will create the branch if it does not already exist and immediately check it out.
      </p>
      
    </div>
    <div id="ExplainGitReset-Container">
      <p>
        <span>git reset</span> will move HEAD and the current branch back to wherever
        you specify, abandoning any commits that may be left behind. This is useful to undo a commit
        that you no longer need.
      </p>
      <p>
        This command is normally used with one of three flags: "--soft", "--mixed", and "--hard".
        The soft and mixed flags deal with what to do with the work that was inside the commit after
        you reset, and you can read about it <a href="http://git-scm.com/2011/07/11/reset.html">here</a>.
        Since this visualization cannot graphically display that work, only the "--hard" flag will work
        on this site.
      </p>
      <p>
        The ref "HEAD^" is usually used together with this command. "HEAD^" means "the commit right
        before HEAD. "HEAD^^" means "two commits before HEAD", and so on.
      </p>
      <p>
        Note that you must <b>never</b> use <span>git reset</span> to abandon commits
        that have already been pushed and merged into the origin. This can cause your local repository
        to become out of sync with the origin. Don't do it unless you really know what you're doing.
      </p>
      
    </div>
    <div id="ExplainGitRevert-Container">
      <p>
        To undo commits that have already been pushed and shared with the team, we cannot use the
        <span>git reset</span> command. Instead, we have to use <span>git revert</span>.
      </p>
      <p>
        <span>git revert</span> will create a new commit that will undo all of the work that
        was done in the commit you want to revert.
      </p>
      
    </div>
    <div id="ExplainGitMerge-Container">
      <p>
        <span>git merge</span> will create a new commit with two parents. The resulting
        commit snapshot will have the all of the work that has been done in both branches.
      </p>
      <p>
        If there was no divergence between the two commits, git will do a "fast-forward" method merge.<br>
        To see this happen, checkout the 'ff' branch and then type <span>git merge dev</span>.
      </p>
      
    </div>
    <div id="ExplainGitRebase-Container">
      <p>
        <span>git rebase</span> will take the commits on this branch and "move" them so that their
        new "base" is at the point you specify.
      </p>
      <p>
        You should pay close attention to the commit IDs of the circles as they move when you do this exercise.
      </p>
      <p>
        The reason I put "move" in quotations because this process actually generates brand new commits with
        completely different IDs than the old commits, and leaves the old commits where they were. For this reason,
        you never want to rebase commits that have already been shared with the team you are working with.
      </p>
      
    </div>
    <div id="ExplainGitFetch-Container">
      <p>
        <span>git fetch</span> will update all of the "remote tracking branches" in your local repository.
        Remote tracking branches are tagged in grey.
      </p>
      
    </div>
    <div id="ExplainGitPull-Container">
      <p>
        A <span>git pull</span> is a two step process that first does a <span>git fetch</span>,
        and then does a <span>git merge</span> of the remote tracking branch associated with your current branch.
        If you have no current branch, the process will stop after fetching.
      </p>
      <p>
        If the argument "--rebase" was given by typing <span>git pull --rebase</span>, the second step of
        pull process will be a rebase instead of a merge. This can be set to the default behavior by configuration by typing:
        <span>git config branch.BRANCHNAME.rebase true</span>.
      </p>
      
    </div>
    <div id="ExplainGitPush-Container">
      <p>
        A <span>git push</span> will find the commits you have on your local branch that the corresponding branch
        on the origin server does not have, and send them to the remote repository.
      </p>
      <p>
        By default, all pushes must cause a fast-forward merge on the remote repository. If there is any divergence between
        your local branch and the remote branch, your push will be rejected. In this scenario, you need to pull first and then
        you will be able to push again.
      </p>
      
    </div>
    <div id="ExplainGitClean-Container">
      <p>
        One simple example of the use of <span>git reset</span> is to completely restore your local repository
        state to that of the origin.<br>
        You can do so by typing <span>git reset origin/master</span>.
      </p>
      <p>
        Note that this won't delete untracked files, you will have to delete those separately with
        the command <span>git clean -df</span>.
      </p>
      
    </div>
    <div id="ExplainGitFetchRebase-Container">
      <p>
        Below is a situation in which you are working in a local branch that is all your own. You want to receive the latest code
        from the origin server's master branch. To update your local branch, you can do it without having to switch branches!
      </p>
      <p>
        First do a <span>git fetch</span>, then type <span>git rebase origin/master</span>!
      </p>
      
    </div>
    <div id="ExplainGitDeleteBranches-Container">
      <p>
        <span>git branch -d</span> is used to delete branches.
        I have pre-created a bunch of branches for you to delete in the playground below.
        Have at it.
      </p>
      
    </div>
    <div id="ExplainGitFree-Container">
      <p>
        Do whatever you want in this free playground.
      </p>
      
    </div>
  </div></div>]]>
            </description>
            <link>https://onlywei.github.io/explain-git-with-d3/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24957280</guid>
            <pubDate>Sun, 01 Nov 2020 04:43:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Toronto-area lawyer had to flee Canada after taking on the tow truck industry]]>
            </title>
            <description>
<![CDATA[
Score 427 | Comments 240 (<a href="https://news.ycombinator.com/item?id=24957200">thread link</a>) | @walterbell
<br/>
October 31, 2020 | https://www.ctvnews.ca/w5/this-toronto-area-lawyer-had-to-flee-the-country-after-taking-on-the-tow-truck-industry-1.5167869 | <a href="https://web.archive.org/web/*/https://www.ctvnews.ca/w5/this-toronto-area-lawyer-had-to-flee-the-country-after-taking-on-the-tow-truck-industry-1.5167869">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p>TORONTO -- 
	Lisa Carr never thought her work would lead to armed threats, a firebombing, a shooting and a conspiracy to kill her.</p>
<p>
	The Carr Law office is in a nondescript strip mall in Vaughan, Ont., north of Toronto. It’s closed now, after the litigation lawyer says police told her they could no longer protect her.</p>
<p>
	She was shuttled to another country where she spent five long months in hiding. Carr has never before told her story, but agreed to meet for an interview as part of a W5 investigation into the shady underbelly of an industry that forced her to give up her business and almost cost her, her life: The tow truck industry.</p>
<p>
	Over the last number of years, criminal elements have been battling for lucrative control of the major highways around the Toronto area. It has resulted in more than 50 arsons, multiple shootings and at least four murders.</p>
<p>
	So why is there so much violence over a couple of hundred-dollar tows at the side of the road? Because that one tow can net tens of thousands of dollars.</p>
<p>
	Here’s how it works: The tow truck driver gets a kickback from an unscrupulous auto body shop, which then submits wildly inflated repair fees to an insurance company.</p>
<p>
	<img alt="york police" src="https://www.ctvnews.ca/polopoly_fs/1.4954903!/httpImage/image.jpg_gen/derivatives/landscape_960/image.jpg"></p>
<p>
	The insurance industry estimates that fake repair bills tally up to $2 billion a year in Canada. And that’s why Carr was in the crosshairs. She was hired by an insurance company to challenge bogus claims.</p>
<p>
	Over the course of a number of months, Carr’s law firm was the target of increasingly violent attacks. First a firebombing and then her office was set on fire.</p>
<p>
	Months later, in broad daylight, a colleague leaving work had a gun put to her head and was told, “Stop suing our friends.” Shortly after that, again in broad daylight, someone opened fire through the front door of the busy office.</p>
<p>
	Carr says it is incredible no one was struck by the flurry of bullets.</p>
<p>
	“I looked down the hall and I saw my receptionist on her hands and knees surrounded by glass. And one of the other girls came running at me saying, ‘Shots fired, shots fired. Call 911.’”</p>
<p>
	While the violence surrounding the tow truck industry has made headlines in the Greater Toronto Area, the story that has never been told is that York Regional Police (YRP) uncovered a plot to kill Carr.</p>
<p>
	It was such a credible threat that they gave her an hour to pack up her belongings and leave her home. Carr and her husband were then whisked out of the country and spent five months in hiding.</p>
<p>
	Three separate police services – YRP, Toronto Police Service and Ontario Provincial Police – joined forces to launch Project Platinum to investigate the violence associated with the tow truck industry.</p>
<p>
	They carried out a series of raids this past spring, which netted dozens of high-powered weapons and led to the arrests of 35 people who face almost 500 charges, including the attempted murder of Carr.</p>
<p>
	<img alt="weapon" src="https://www.ctvnews.ca/polopoly_fs/1.4954983!/httpImage/image.jpg_gen/derivatives/landscape_960/image.jpg"></p>
<p>
	Now back in Canada, Carr says police have told her she is likely no longer in danger, but with one caveat.</p>
<p>
	“The police said we believe the risk is low. As long as you don't go back to work, as long as you don't restart the firm,” she says.</p>
<p>
	“So they have effectively ended my career. We lost everything. They won.”</p>
                                              </div></div>]]>
            </description>
            <link>https://www.ctvnews.ca/w5/this-toronto-area-lawyer-had-to-flee-the-country-after-taking-on-the-tow-truck-industry-1.5167869</link>
            <guid isPermaLink="false">hacker-news-small-sites-24957200</guid>
            <pubDate>Sun, 01 Nov 2020 04:20:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Simple Image Vectorization]]>
            </title>
            <description>
<![CDATA[
Score 128 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24957120">thread link</a>) | @thesephist
<br/>
October 31, 2020 | https://wordsandbuttons.online/simple_image_vectorization.html | <a href="https://web.archive.org/web/*/https://wordsandbuttons.online/simple_image_vectorization.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    
    <p>
Vectorization is when you take some minecraft-style raster image and make a crisp vector picture out of it.
    </p>
    <img src="https://upload.wikimedia.org/wikipedia/commons/6/6b/Bitmap_VS_SVG.svg">
    
    <p>
It's especially useful when you want to turn a satellite photo into a map. Or if you want to scan some blueprint and turn it into a CAD model. Or if you want to reissue an old game and you don't want to redraw all the artwork from scratch.
    </p>
    <p>
The algorithm I'm going to show you has nothing to do with all these things. It's a basic vectorization technique which, in its original form, has little to none applications in the industry.
    </p>
    <p>
On the plus side, it illustrates the approach rather well. It shows how things like bilinear interpolation, gradient descent, and parametric splines work together to solve a real-world problem. At the very least, it makes learning about all these things a little more compelling.
    </p>

    <h2>
An input image
    </h2>
    <p>
A raster image is essentially a rectangular table of things. If it's a full-color RGB, then it's a table of color pixels. Color pixels are the triplets of 8-bit integer values where each value represents an amount of red, green, and blue color.
    </p>
    <p>
Medical images, such as obtained from computed tomography, are usually the tables of 12-bit or 16-bit integers. It's not a color really since the values come from invisible X-ray radiation, but they are called gray values nevertheless.
    </p>
    <p>
Satellite images may have a lot of channels. Apart from the colors of the visible specter they may contain ultra-violet and infra-red luminosity. Channels may be represented by integers or floating point values.
    </p>
    <p>
Our image will be a simple gray-scale bitmap.
    </p>
    <canvas id="greyscale_canvas" width="640" height="640"></canvas>
    <p>
Technically, we can already turn it into vectors rather easily. Let's just agree on some threshold, and mark the contour of all the pixels that have the values exceeding this threshold.
    </p>
    <canvas id="greyscale_canvas_contour" width="640" height="640"></canvas>
    <p>
Well, it's simple, but it's not what we wanted. We want curves, not corners. And for that, we have to make our image less cornery.
    </p>
    <h2>
<span id="index_image_interpolation">Image interpolation</span>
    </h2>
    <p>
Let's say our image is not a table of values. Let's say we only know the values in the centers of the pixels, and we have to guess the values between them somehow.
    </p>
    <p>
This is called interpolation. The simplest case would be the nearest neighbor interpolation, where for every point on an image, the value is the value from the nearest pixel's center. But this simply turns it back into a table.
    </p>
    <p>
A little more advanced is the <span id="index_bilinear_interpolation">bilinear interpolation</span>. The value is the linear sum of the four neighboring values. It looks like this.
    </p>
    <div>
    <pre id="code_1">// pixel value with out of bounds checks
function pixel_in(pixels, i, j) {
    if(i &gt;= pixels.length)
        return pixel_in(pixels, pixels.length-1, j);
    if(i &lt; 0)
        return pixel_in(pixels, 0, j);
    if(j &gt;= pixels[0].length)
        return pixel_in(pixels, i, pixels[0].length-1);
    if(j &lt; 0)
        return pixel_in(pixels, i, 0);
    return pixels[i][j];
}

// linear interpolation
function value_in(pixels, x, y) {
    var j = Math.floor(x - 0.5);
    var tj = x - 0.5 - j;
    var i = Math.floor(y - 0.5);
    var ti = y - 0.5 - i;
    return pixel_in(pixels, i, j) * (1 - ti) * (1 - tj)
         + pixel_in(pixels, i, j+1) * (1 - ti) * (tj)
         + pixel_in(pixels, i+1, j+1) * (ti) * (tj)
         + pixel_in(pixels, i+1, j) * (ti) * (1 - tj);
}
    </pre>
    </div>
    <p>
If we darken the pixels where the interpolated value meets the threshold, we'll get some kind of a contour.
    </p>
    <canvas id="interpolation_canvas" width="640" height="640"></canvas>
    
    <p>
There are other methods. Plenty of them. But linear interpolation solves the cornery border problem just fine. Although, the border we see is just the borderline of some threshold. It's not a vector representation yet.
    </p>

    <h2>
Turning an interpolated image into a contour
    </h2>
    <p>
We can borrow an idea from the <a href="https://wordsandbuttons.online/the_simplest_possible_smooth_contouring_algorithm.html">simplest possible smooth contouring</a> algorithm. We'll build an initial border from the source pixels, and then we'll use our linearly interpolated image to find the best place to put each contour point so the image value will meet the threshold value.
    </p>
    <p>
When you have a <span id="index_distance_field">distance field</span>, it's easy. A distance field is when for any point in space you can tell how far it lies from the surface you want. It's basically a function from point in space to distance.
    </p>
    <p>
You take its gradient, take the difference between the value you have and the threshold value. Since it's the distance field, the value difference is exactly the distance you should move your point for. And the gradient is the exact opposite direction. You just inverse, multiply, add — and you're there.
    </p>
    <p>
Unfortunately, we don't have a distance field. We have a continuous image which only resembles one.
    </p>
    <p>
But the principle still works. If you traverse against the gradient, you will get closer to the threshold value. And the more the difference, the further you have to go. It's just you wouldn't always get there in one try.
    </p>
    <p>
So let's try several times then. Let's make an <a href="https://wordsandbuttons.online/interactive_introduction_to_iterative_algorithms.html">iterative algorithm</a> out of it.
    </p>
    <div>
    <pre id="code_2">// gradient
function gradient(pixels, x, y) {
    const eps = 1e-5;
    return [(value_in(pixels, x + eps, y) - value_in(pixels, x, y)) / eps,
            (value_in(pixels, x, y + eps) - value_in(pixels, x, y)) / eps];
}

// how far should you shift the point to meet the isoline
// if value_in were a distance function
function gradient_shift(pixels, threshold, x, y) {
    var g = gradient(pixels, x, y);
    var g_norm = Math.sqrt(g[0]*g[0] + g[1]*g[1]);
    var d = threshold - value_in(pixels, x, y);
    return [g[0] * d / g_norm / g_norm, g[1] * d / g_norm / g_norm];
}

// brings a point closer to the threshold isoline
function fit_point_better(pixels, threshold, point) {
    const ok_error = 1/255;
    if(Math.abs(value_in(pixels, point[0], point[1]) - threshold) &lt; ok_error)
        return point;
    gs = gradient_shift(pixels, threshold, point[0], point[1])
    var new_point = [point[0] + gs[0], point[1] + gs[1]];
    return fit_point_better(pixels, threshold, new_point);
}
    </pre>
    </div>
    <p>
We'll move our contour points against the gradient until we're close enough to the threshold
    </p>
    <canvas id="fitting_canvas" width="640" height="640"></canvas>
        
    <p>
That's good but we can do better. Let's make the contour smooth.
    </p>

    <h2>
Cubic splines
    </h2>
    <p>
All we have to do to make the contour smooth is to turn each line segment into a parametric cubic curve.
    </p>
    <p>
It's probably sounds more complicated than it is. A parametric cubic curve is just a pair of polynomials. If you have the points and partial derivatives in this points, you can get the coefficients for them from this pair of <a href="https://wordsandbuttons.online/programmers_introduction_to_linear_equations.html">linear systems</a>:
    </p>
    <div><p>
Px(t<sub>1</sub>)' = 3a<sub>x</sub>t<sub>1</sub><sup>2</sup> + 2b<sub>x</sub>t<sub>1</sub> + c = dx<sub>1</sub>/dt
<br>
Px(t<sub>1</sub>) = a<sub>x</sub>t<sub>1</sub><sup>3</sup> + b<sub>x</sub>t<sub>1</sub><sup>2</sup> + c<sub>x</sub>t<sub>1</sub> + d = x<sub>1</sub>
<br>
Px(t<sub>2</sub>) = a<sub>x</sub>t<sub>2</sub><sup>3</sup> + b<sub>x</sub>t<sub>2</sub><sup>2</sup> + c<sub>x</sub>t<sub>2</sub> + d = x<sub>2</sub>
<br>
Px(t<sub>2</sub>)' = 3a<sub>x</sub>t<sub>2</sub><sup>2</sup> + 2b<sub>x</sub>t<sub>2</sub> + c = dx<sub>2</sub>/dt
    </p><p>
    
Py(t<sub>1</sub>)' = 3a<sub>y</sub>t<sub>1</sub><sup>2</sup> + 2b<sub>y</sub>t<sub>1</sub> + c = dy<sub>1</sub>/dt
<br>
Py(t<sub>1</sub>) = a<sub>y</sub>t<sub>1</sub><sup>3</sup> + b<sub>y</sub>t<sub>1</sub><sup>2</sup> + c<sub>y</sub>t<sub>1</sub> + d = y<sub>1</sub>
<br>
Py(t<sub>2</sub>) = a<sub>y</sub>t<sub>2</sub><sup>3</sup> + b<sub>y</sub>t<sub>2</sub><sup>2</sup> + c<sub>y</sub>t<sub>2</sub> + d = y<sub>2</sub>
<br>
Py(t<sub>2</sub>)' = 3a<sub>y</sub>t<sub>2</sub><sup>2</sup> + 2b<sub>y</sub>t<sub>2</sub> + c = dy<sub>2</sub>/dt
    </p></div>
    <p>
The curve itself will then look like this.
    </p>
    <canvas id="cubic_canvas" width="640" height="640"></canvas>
    <p>
Even more, since we get to choose the parameter range, we can make it [0..1]. This greatly simplifies our system and makes it really easy to solve.
    </p>
    <p>
Here is the function that makes one array of polynomial coefficients from two pairs of point and tangent values.
    </p>
    <div>
    <pre id="code_3">// solver specific to [0..1] parametrized splines
function spline_for(p1, p1d, p2, p2d) {
//     A = [
//         [1, 0, 0, 0],
//         [0, 1, 0, 0],
//         [1, 1, 1, 1],
//         [0, 1, 2, 3]];
//     B = [p1, p1d, p2, p2d]
    return [
        p1,
        p1d,
        3*p2 - p2d - 3*p1 - 2*p1d,
        p2d + p1d - 2*p2 + 2*p1
    ];
}
    </pre>
    </div>
    <p>
The polynomial is then computed in every <i>t</i> with this function.
    </p>
    <div>
    <pre id="code_4">// polynomial
function polynomial_in_t(A, t){
    var pt = 0.0;
    for(var i = 0; i &lt; A.length; ++i){
        pt += A[i] * Math.pow(x, i);
    }
    return pt;
}
    </pre>
    </div>
    <p>
So for every line segment with tangents, we can make a parametric polynomial. There is one problem though. We don't have tangents.
    </p>
    <p>
We have the gradient, which is orthogonal to the tangent, but there are two possible tangents in every point. The tangent can be oriented left or right from the gradient.
    </p>
    <p>
But this is solvable. Let's just pick the direction we like and keep it consistent.
    </p>
    <p>
Let the curves that originally come from horizontally oriented segments always have both tangents that way that <i>dx &gt; 0</i>. And the ones that come from vertically oriented segments, will have <i>dy &gt; 0</i>.
    </p>
    <p>
It looks like we have enough parts to assemble an algorithm.
    </p>
    <h2>
Creating splines from the pixels
    </h2>
    <p>
Let's split our vectorization into two parts. First, we'll get points and tangents for every line segment from the pixels. Then we'll turn it all into polynomial splines.
    </p>
    <p>
The function that does the first part looks like this.
    </p>
    <div>
    <pre id="code_5">function turn_pixels_into_points_and_tangents(pixels, threshold) {
    var points = [];
    var tangents = [];

    // "horizontal" pieces
    for(var i = 0; i &lt;= pixels.length; i += 1) {
        var old_point = [];
        var old_tangent = [];
        for(var j = 0; j &lt;= pixels[0].length; j += 1) {
            // if right, left, top, and bottom pixels have a sign change,
            // there should be a spline there
            var sign_change_on_the_right  =
                (pixel_in(pixels, i-1, j+0) - threshold) *
                (pixel_in(pixels, i+0, j+0) - threshold) &lt; 0;
            var sign_change_on_the_left   =
                (pixel_in(pixels, i-1, j-1) - threshold) *
                (pixel_in(pixels, i+0, j-1) - threshold) &lt; 0;
            var sign_change_on_the_bottom =
                (pixel_in(pixels, i+0, j-1) - threshold) *
                (pixel_in(pixels, i+0, j+0) - threshold) &lt; 0;
            var sign_change_on_the_top    =
                (pixel_in(pixels, i-1, j-1) - threshold) *
                …</pre></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://wordsandbuttons.online/simple_image_vectorization.html">https://wordsandbuttons.online/simple_image_vectorization.html</a></em></p>]]>
            </description>
            <link>https://wordsandbuttons.online/simple_image_vectorization.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24957120</guid>
            <pubDate>Sun, 01 Nov 2020 03:57:17 GMT</pubDate>
        </item>
    </channel>
</rss>
